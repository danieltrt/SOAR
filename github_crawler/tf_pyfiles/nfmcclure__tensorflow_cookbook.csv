file_path,api_count,code
test_script.py,0,b'import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport requests\nimport jupyter\n'
01_Introduction/02_Creating_and_Using_Tensors/02_tensors.py,16,"b'# Tensors\n#----------------------------------\n#\n# This function introduces various ways to create\n# tensors in TensorFlow\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Introduce tensors in tf\n\n# Get graph handle\nsess = tf.Session()\n\nmy_tensor = tf.zeros([1,20])\n\n# Declare a variable\nmy_var = tf.Variable(tf.zeros([1,20]))\n\n# Different kinds of variables\nrow_dim = 2\ncol_dim = 3 \n\n# Zero initialized variable\nzero_var = tf.Variable(tf.zeros([row_dim, col_dim]))\n\n# One initialized variable\nones_var = tf.Variable(tf.ones([row_dim, col_dim]))\n\n# shaped like other variable\nsess.run(zero_var.initializer)\nsess.run(ones_var.initializer)\nzero_similar = tf.Variable(tf.zeros_like(zero_var))\nones_similar = tf.Variable(tf.ones_like(ones_var))\n\nsess.run(ones_similar.initializer)\nsess.run(zero_similar.initializer)\n\n# Fill shape with a constant\nfill_var = tf.Variable(tf.fill([row_dim, col_dim], -1))\n\n# Create a variable from a constant\nconst_var = tf.Variable(tf.constant([8, 6, 7, 5, 3, 0, 9]))\n# This can also be used to fill an array:\nconst_fill_var = tf.Variable(tf.constant(-1, shape=[row_dim, col_dim]))\n\n# Sequence generation\nlinear_var = tf.Variable(tf.linspace(start=0.0, stop=1.0, num=3)) # Generates [0.0, 0.5, 1.0] includes the end\n\nsequence_var = tf.Variable(tf.range(start=6, limit=15, delta=3)) # Generates [6, 9, 12] doesn\'t include the end\n\n# Random Numbers\n\n# Random Normal\nrnorm_var = tf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0)\n\n# Add summaries to tensorboard\nmerged = tf.summary.merge_all()\n\n# Initialize graph writer:\nwriter = tf.summary.FileWriter(""/tmp/variable_logs"", graph=sess.graph)\n\n# Initialize operation\ninitialize_op = tf.global_variables_initializer()\n\n# Run initialization of variable\nsess.run(initialize_op)'"
01_Introduction/03_Using_Variables_and_Placeholders/03_placeholders.py,5,"b'# Placeholders\n#----------------------------------\n#\n# This function introduces how to \n# use placeholders in TensorFlow\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Using Placeholders\nsess = tf.Session()\n\nx = tf.placeholder(tf.float32, shape=(4, 4))\ny = tf.identity(x)\n\nrand_array = np.random.rand(4, 4)\n\nmerged = tf.summary.merge_all()\n\nwriter = tf.summary.FileWriter(""/tmp/variable_logs"", sess.graph)\n\nprint(sess.run(y, feed_dict={x: rand_array}))'"
01_Introduction/04_Working_with_Matrices/04_matrices.py,12,"b'# Matrices and Matrix Operations\n#----------------------------------\n#\n# This function introduces various ways to create\n# matrices and how to use them in TensorFlow\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Declaring matrices\nsess = tf.Session()\n\n# Declaring matrices\n\n# Identity matrix\nidentity_matrix = tf.diag([1.0,1.0,1.0])\nprint(sess.run(identity_matrix))\n\n# 2x3 random norm matrix\nA = tf.truncated_normal([2,3])\nprint(sess.run(A))\n\n# 2x3 constant matrix\nB = tf.fill([2,3], 5.0)\nprint(sess.run(B))\n\n# 3x2 random uniform matrix\nC = tf.random_uniform([3,2])\nprint(sess.run(C))  # Note that we are reinitializing, hence the new random variables\n\n# Create matrix from np array\nD = tf.convert_to_tensor(np.array([[1., 2., 3.], [-3., -7., -1.], [0., 5., -2.]]))\nprint(sess.run(D))\n\n# Matrix addition/subtraction\nprint(sess.run(A+B))\nprint(sess.run(B-B))\n\n# Matrix Multiplication\nprint(sess.run(tf.matmul(B, identity_matrix)))\n\n# Matrix Transpose\nprint(sess.run(tf.transpose(C))) # Again, new random variables\n\n# Matrix Determinant\nprint(sess.run(tf.matrix_determinant(D)))\n\n# Matrix Inverse\nprint(sess.run(tf.matrix_inverse(D)))\n\n# Cholesky Decomposition\nprint(sess.run(tf.cholesky(identity_matrix)))\n\n# Eigenvalues and Eigenvectors\nprint(sess.run(tf.self_adjoint_eig(D)))'"
01_Introduction/05_Declaring_Operations/05_operations.py,10,"b'# Operations\n#----------------------------------\n#\n# This function introduces various operations\n# in TensorFlow\n\n# Declaring Operations\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Open graph session\nsess = tf.Session()\n\n# div() vs truediv() vs floordiv()\nprint(sess.run(tf.div(3, 4)))\nprint(sess.run(tf.truediv(3, 4)))\nprint(sess.run(tf.floordiv(3.0, 4.0)))\n\n# Mod function\nprint(sess.run(tf.mod(22.0, 5.0)))\n\n# Cross Product\nprint(sess.run(tf.cross([1., 0., 0.], [0., 1., 0.])))\n\n# Trig functions\nprint(sess.run(tf.sin(3.1416)))\nprint(sess.run(tf.cos(3.1416)))\nprint(sess.run(tf.tan(3.1416/4.)))\n\n# Custom operation\ntest_nums = range(15)\n\n\ndef custom_polynomial(x_val):\n    # Return 3x^2 - x + 10\n    return tf.subtract(3 * tf.square(x_val), x_val) + 10\n\nprint(sess.run(custom_polynomial(11)))\n\n# What should we get with list comprehension\nexpected_output = [3*x*x-x+10 for x in test_nums]\nprint(expected_output)\n\n# TensorFlow custom function output\nfor num in test_nums:\n    print(sess.run(custom_polynomial(num)))\n'"
01_Introduction/06_Implementing_Activation_Functions/06_activation_functions.py,15,"b""# Activation Functions\n#----------------------------------\n#\n# This function introduces activation\n# functions in TensorFlow\n\n# Implementing Activation Functions\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Open graph session\nsess = tf.Session()\n\n# X range\nx_vals = np.linspace(start=-10., stop=10., num=100)\n\n# ReLU activation\nprint(sess.run(tf.nn.relu([-3., 3., 10.])))\ny_relu = sess.run(tf.nn.relu(x_vals))\n\n# ReLU-6 activation\nprint(sess.run(tf.nn.relu6([-3., 3., 10.])))\ny_relu6 = sess.run(tf.nn.relu6(x_vals))\n\n# Sigmoid activation\nprint(sess.run(tf.nn.sigmoid([-1., 0., 1.])))\ny_sigmoid = sess.run(tf.nn.sigmoid(x_vals))\n\n# Hyper Tangent activation\nprint(sess.run(tf.nn.tanh([-1., 0., 1.])))\ny_tanh = sess.run(tf.nn.tanh(x_vals))\n\n# Softsign activation\nprint(sess.run(tf.nn.softsign([-1., 0., 1.])))\ny_softsign = sess.run(tf.nn.softsign(x_vals))\n\n# Softplus activation\nprint(sess.run(tf.nn.softplus([-1., 0., 1.])))\ny_softplus = sess.run(tf.nn.softplus(x_vals))\n\n# Exponential linear activation\nprint(sess.run(tf.nn.elu([-1., 0., 1.])))\ny_elu = sess.run(tf.nn.elu(x_vals))\n\n# Plot the different functions\nplt.plot(x_vals, y_softplus, 'r--', label='Softplus', linewidth=2)\nplt.plot(x_vals, y_relu, 'b:', label='ReLU', linewidth=2)\nplt.plot(x_vals, y_relu6, 'g-.', label='ReLU6', linewidth=2)\nplt.plot(x_vals, y_elu, 'k-', label='ExpLU', linewidth=0.5)\nplt.ylim([-1.5,7])\nplt.legend(loc='upper left')\nplt.show()\n\nplt.plot(x_vals, y_sigmoid, 'r--', label='Sigmoid', linewidth=2)\nplt.plot(x_vals, y_tanh, 'b:', label='Tanh', linewidth=2)\nplt.plot(x_vals, y_softsign, 'g-.', label='Softsign', linewidth=2)\nplt.ylim([-2,2])\nplt.legend(loc='upper left')\nplt.show()\n"""
01_Introduction/07_Working_with_Data_Sources/07_data_gathering.py,1,"b'# Data gathering\n#----------------------------------\n#\n# This function gives us the ways to access\n# the various data sets we will need\n\n# Data Gathering\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n\n# Iris Data\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nprint(len(iris.data))\nprint(len(iris.target))\nprint(iris.data[0])\nprint(set(iris.target))\n\n# Low Birthrate Data\nimport requests\n\nbirthdata_url = \'https://github.com/nfmcclure/tensorflow_cookbook/raw/master/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat\'\nbirth_file = requests.get(birthdata_url)\nbirth_data = birth_file.text.split(\'\\r\\n\')\nbirth_header = birth_data[0].split(\'\\t\')\nbirth_data = [[float(x) for x in y.split(\'\\t\') if len(x)>=1] for y in birth_data[1:] if len(y)>=1]\nprint(len(birth_data))\nprint(len(birth_data[0]))\n\n\n# Housing Price Data\nfrom keras.datasets import boston_housing\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\nhousing_header = [\'CRIM\', \'ZN\', \'INDUS\', \'CHAS\', \'NOX\', \'RM\', \'AGE\', \'DIS\', \'RAD\', \'TAX\', \'PTRATIO\', \'B\', \'LSTAT\', \'MEDV\']\nprint(x_train.shape[0])\nprint(x_train.shape[1])\n\n\n# MNIST Handwriting Data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\nprint(len(mnist.train.images))\nprint(len(mnist.test.images))\nprint(len(mnist.validation.images))\nprint(mnist.train.labels[1,:])\n\n# CIFAR-10 Image Category Dataset\n# The CIFAR-10 data ( https://www.cs.toronto.edu/~kriz/cifar.html ) contains 60,000 32x32 color images of 10 classes.\n# It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n# Alex Krizhevsky maintains the page referenced here.\n# This is such a common dataset, that there are built in functions in TensorFlow to access this data.\n\n# Running this command requires an internet connection and a few minutes to download all the images.\n(X_train, y_train), (X_test, y_test) = tf.contrib.keras.datasets.cifar10.load_data()\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(y_train[0,])  # this is a frog\n\n# Plot the 0-th image (a frog)\nfrom PIL import Image\nimg = Image.fromarray(X_train[0,:,:,:])\nplt.imshow(img)\n\n\n# Ham/Spam Text Data\nimport requests\nimport io\nfrom zipfile import ZipFile\n\n# Get/read zip file\nzip_url = \'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\'\nr = requests.get(zip_url)\nz = ZipFile(io.BytesIO(r.content))\nfile = z.read(\'SMSSpamCollection\')\n# Format Data\ntext_data = file.decode()\ntext_data = text_data.encode(\'ascii\',errors=\'ignore\')\ntext_data = text_data.decode().split(\'\\n\')\ntext_data = [x.split(\'\\t\') for x in text_data if len(x)>=1]\n[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]\nprint(len(text_data_train))\nprint(set(text_data_target))\nprint(text_data_train[1])\n\n\n# Movie Review Data\nimport requests\nimport io\nimport tarfile\n\nmovie_data_url = \'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\'\nr = requests.get(movie_data_url)\n# Stream data into temp object\nstream_data = io.BytesIO(r.content)\ntmp = io.BytesIO()\nwhile True:\n    s = stream_data.read(16384)\n    if not s:  \n        break\n    tmp.write(s)\nstream_data.close()\ntmp.seek(0)\n# Extract tar file\ntar_file = tarfile.open(fileobj=tmp, mode=""r:gz"")\npos = tar_file.extractfile(\'rt-polaritydata/rt-polarity.pos\')\nneg = tar_file.extractfile(\'rt-polaritydata/rt-polarity.neg\')\n# Save pos/neg reviews\npos_data = []\nfor line in pos:\n    pos_data.append(line.decode(\'ISO-8859-1\').encode(\'ascii\',errors=\'ignore\').decode())\nneg_data = []\nfor line in neg:\n    neg_data.append(line.decode(\'ISO-8859-1\').encode(\'ascii\',errors=\'ignore\').decode())\ntar_file.close()\n\nprint(len(pos_data))\nprint(len(neg_data))\nprint(neg_data[0])\n\n\n# The Works of Shakespeare Data\nimport requests\n\nshakespeare_url = \'http://www.gutenberg.org/cache/epub/100/pg100.txt\'\n# Get Shakespeare text\nresponse = requests.get(shakespeare_url)\nshakespeare_file = response.content\n# Decode binary into string\nshakespeare_text = shakespeare_file.decode(\'utf-8\')\n# Drop first few descriptive paragraphs.\nshakespeare_text = shakespeare_text[7675:]\nprint(len(shakespeare_text))\n\n\n# English-German Sentence Translation Data\nimport requests\nimport io\nfrom zipfile import ZipFile\nsentence_url = \'http://www.manythings.org/anki/deu-eng.zip\'\nr = requests.get(sentence_url)\nz = ZipFile(io.BytesIO(r.content))\nfile = z.read(\'deu.txt\')\n# Format Data\neng_ger_data = file.decode()\neng_ger_data = eng_ger_data.encode(\'ascii\',errors=\'ignore\')\neng_ger_data = eng_ger_data.decode().split(\'\\n\')\neng_ger_data = [x.split(\'\\t\') for x in eng_ger_data if len(x)>=1]\n[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)]\nprint(len(english_sentence))\nprint(len(german_sentence))\nprint(eng_ger_data[10])\n'"
02_TensorFlow_Way/01_Operations_as_a_Computational_Graph/01_operations_on_a_graph.py,6,"b""# Operations on a Computational Graph\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Create tensors\n\n# Create data to feed in\nx_vals = np.array([1., 3., 5., 7., 9.])\nx_data = tf.placeholder(tf.float32)\nm_const = tf.constant(3.)\n\n# Multiplication\nmy_product = tf.multiply(x_data, m_const)\nfor x_val in x_vals:\n    print(sess.run(my_product, feed_dict={x_data: x_val}))\n\n# View the tensorboard graph by running the following code and then\n#    going to the terminal and typing:\n#    $ tensorboard --logdir=tensorboard_logs\nmerged = tf.summary.merge_all()\nif not os.path.exists('tensorboard_logs/'):\n    os.makedirs('tensorboard_logs/')\n\nmy_writer = tf.summary.FileWriter('tensorboard_logs/', sess.graph)\n"""
02_TensorFlow_Way/02_Layering_Nested_Operations/02_layering_nested_operations.py,10,"b""# Layering Nested Operations\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a graph session\nsess = tf.Session()\n\n# Create the data and variables\nmy_array = np.array([[1., 3., 5., 7., 9.],\n                   [-2., 0., 2., 4., 6.],\n                   [-6., -3., 0., 3., 6.]])\nx_vals = np.array([my_array, my_array + 1])\nx_data = tf.placeholder(tf.float32, shape=(3, 5))\n\n# Constants for matrix multiplication:\nm1 = tf.constant([[1.], [0.], [-1.], [2.], [4.]])\nm2 = tf.constant([[2.]])\na1 = tf.constant([[10.]])\n\n# Create our multiple operations\nprod1 = tf.matmul(x_data, m1)\nprod2 = tf.matmul(prod1, m2)\nadd1 = tf.add(prod2, a1)\n\n# Now feed data through placeholder and print results\nfor x_val in x_vals:\n    print(sess.run(add1, feed_dict={x_data: x_val}))\n\n# View the tensorboard graph by running the following code and then\n#    going to the terminal and typing:\n#    $ tensorboard --logdir=tensorboard_logs\nmerged = tf.summary.merge_all()\nif not os.path.exists('tensorboard_logs/'):\n    os.makedirs('tensorboard_logs/')\n\nmy_writer = tf.summary.FileWriter('tensorboard_logs/', sess.graph)\n"""
02_TensorFlow_Way/03_Working_with_Multiple_Layers/03_multiple_layers.py,13,"b""# Working with Multiple Layers\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Create tensors\n\n# Create a small random 'image' of size 4x4\nx_shape = [1, 4, 4, 1]\nx_val = np.random.uniform(size=x_shape)\n\nx_data = tf.placeholder(tf.float32, shape=x_shape)\n\n# Create a layer that takes a spatial moving window average\n# Our window will be 2x2 with a stride of 2 for height and width\n# The filter value will be 0.25 because we want the average of the 2x2 window\nmy_filter = tf.constant(0.25, shape=[2, 2, 1, 1])\nmy_strides = [1, 2, 2, 1]\nmov_avg_layer= tf.nn.conv2d(x_data, my_filter, my_strides,\n                            padding='SAME', name='Moving_Avg_Window')\n\n# Define a custom layer which will be sigmoid(Ax+b) where\n# x is a 2x2 matrix and A and b are 2x2 matrices\ndef custom_layer(input_matrix):\n    input_matrix_sqeezed = tf.squeeze(input_matrix)\n    A = tf.constant([[1., 2.], [-1., 3.]])\n    b = tf.constant(1., shape=[2, 2])\n    temp1 = tf.matmul(A, input_matrix_sqeezed)\n    temp = tf.add(temp1, b) # Ax + b\n    return(tf.sigmoid(temp))\n\n# Add custom layer to graph\nwith tf.name_scope('Custom_Layer') as scope:\n    custom_layer1 = custom_layer(mov_avg_layer)\n\n# The output should be an array that is 2x2, but size (1,2,2,1)\nprint(sess.run(mov_avg_layer, feed_dict={x_data: x_val}))\n\n# After custom operation, size is now 2x2 (squeezed out size 1 dims)\nprint(sess.run(custom_layer1, feed_dict={x_data: x_val}))\n\nmerged = tf.summary.merge_all(key='summaries')\n\nif not os.path.exists('tensorboard_logs/'):\n    os.makedirs('tensorboard_logs/')\n\nmy_writer = tf.summary.FileWriter('tensorboard_logs/', sess.graph)\n"""
02_TensorFlow_Way/04_Implementing_Loss_Functions/04_loss_functions.py,25,"b""# Loss Functions\n#----------------------------------\n#\n#  This python script illustrates the different\n#  loss functions for regression and classification.\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n###### Numerical Predictions ######\nx_vals = tf.linspace(-1., 1., 500)\ntarget = tf.constant(0.)\n\n# L2 loss\n# L = (pred - actual)^2\nl2_y_vals = tf.square(target - x_vals)\nl2_y_out = sess.run(l2_y_vals)\n\n# L1 loss\n# L = abs(pred - actual)\nl1_y_vals = tf.abs(target - x_vals)\nl1_y_out = sess.run(l1_y_vals)\n\n# Pseudo-Huber loss\n# L = delta^2 * (sqrt(1 + ((pred - actual)/delta)^2) - 1)\ndelta1 = tf.constant(0.25)\nphuber1_y_vals = tf.multiply(tf.square(delta1), tf.sqrt(1. + tf.square((target - x_vals)/delta1)) - 1.)\nphuber1_y_out = sess.run(phuber1_y_vals)\n\ndelta2 = tf.constant(5.)\nphuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(1. + tf.square((target - x_vals)/delta2)) - 1.)\nphuber2_y_out = sess.run(phuber2_y_vals)\n\n# Plot the output:\nx_array = sess.run(x_vals)\nplt.plot(x_array, l2_y_out, 'b-', label='L2 Loss')\nplt.plot(x_array, l1_y_out, 'r--', label='L1 Loss')\nplt.plot(x_array, phuber1_y_out, 'k-.', label='P-Huber Loss (0.25)')\nplt.plot(x_array, phuber2_y_out, 'g:', label='P-Huber Loss (5.0)')\nplt.ylim(-0.2, 0.4)\nplt.legend(loc='lower right', prop={'size': 11})\nplt.grid()\nplt.show()\n\n\n###### Categorical Predictions ######\nx_vals = tf.linspace(-3., 5., 500)\ntarget = tf.constant(1.)\ntargets = tf.fill([500,], 1.)\n\n# Hinge loss\n# Use for predicting binary (-1, 1) classes\n# L = max(0, 1 - (pred * actual))\nhinge_y_vals = tf.maximum(0., 1. - tf.multiply(target, x_vals))\nhinge_y_out = sess.run(hinge_y_vals)\n\n# Cross entropy loss\n# L = -actual * (log(pred)) - (1-actual)(log(1-pred))\nxentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((1. - target), tf.log(1. - x_vals))\nxentropy_y_out = sess.run(xentropy_y_vals)\n\n# L = -actual * (log(sigmoid(pred))) - (1-actual)(log(1-sigmoid(pred)))\n# or\n# L = max(actual, 0) - actual * pred + log(1 + exp(-abs(actual)))\nx_val_input = tf.expand_dims(x_vals, 1)\ntarget_input = tf.expand_dims(targets, 1)\nxentropy_sigmoid_y_vals = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_val_input,\n                                                                  labels=target_input)\nxentropy_sigmoid_y_out = sess.run(xentropy_sigmoid_y_vals)\n\n# Weighted (softmax) cross entropy loss\n# L = -actual * (log(pred)) * weights - (1-actual)(log(1-pred))\n# or\n# L = (1 - pred) * actual + (1 + (weights - 1) * pred) * log(1 + exp(-actual))\nweight = tf.constant(0.5)\nxentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(logits=x_vals,\n                                                                    targets=targets,\n                                                                    pos_weight=weight)\nxentropy_weighted_y_out = sess.run(xentropy_weighted_y_vals)\n\n# Plot the output\nx_array = sess.run(x_vals)\nplt.plot(x_array, hinge_y_out, 'b-', label='Hinge Loss')\nplt.plot(x_array, xentropy_y_out, 'r--', label='Cross Entropy Loss')\nplt.plot(x_array, xentropy_sigmoid_y_out, 'k-.', label='Cross Entropy Sigmoid Loss')\nplt.plot(x_array, xentropy_weighted_y_out, 'g:', label='Weighted Cross Entropy Loss (x0.5)')\nplt.ylim(-1.5, 3)\n#plt.xlim(-1, 3)\nplt.grid()\nplt.legend(loc='lower right', prop={'size': 11})\nplt.show()\n\n# Softmax entropy loss\n# L = -actual * (log(softmax(pred))) - (1-actual)(log(1-softmax(pred)))\nunscaled_logits = tf.constant([[1., -3., 10.]])\ntarget_dist = tf.constant([[0.1, 0.02, 0.88]])\nsoftmax_xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=unscaled_logits,\n                                                              labels=target_dist)\nprint(sess.run(softmax_xentropy))\n\n# Sparse entropy loss\n# Use when classes and targets have to be mutually exclusive\n# L = sum( -actual * log(pred) )\nunscaled_logits = tf.constant([[1., -3., 10.]])\nsparse_target_dist = tf.constant([2])\nsparse_xentropy =  tf.nn.sparse_softmax_cross_entropy_with_logits(logits=unscaled_logits,\n                                                                  labels=sparse_target_dist)\nprint(sess.run(sparse_xentropy))"""
02_TensorFlow_Way/05_Implementing_Back_Propagation/05_back_propagation.py,19,"b""# Back Propagation\n#----------------------------------\n#\n# This python function shows how to implement back propagation\n# in regression and classification models.\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Regression Example:\n# We will create sample data as follows:\n# x-data: 100 random samples from a normal ~ N(1, 0.1)\n# target: 100 values of the value 10.\n# We will fit the model:\n# x-data * A = target\n# Theoretically, A = 10.\n\n# Create data\nx_vals = np.random.normal(1, 0.1, 100)\ny_vals = np.repeat(10., 100)\nx_data = tf.placeholder(shape=[1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[1], dtype=tf.float32)\n\n# Create variable (one model parameter = A)\nA = tf.Variable(tf.random_normal(shape=[1]))\n\n# Add operation to graph\nmy_output = tf.multiply(x_data, A)\n\n# Add L2 loss operation to graph\nloss = tf.square(my_output - y_target)\n\n# Create Optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.02)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Run Loop\nfor i in range(100):\n    rand_index = np.random.choice(100)\n    rand_x = [x_vals[rand_index]]\n    rand_y = [y_vals[rand_index]]\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    if (i+1)%25==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n        print('Loss = ' + str(sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})))\n\n# Classification Example\n# We will create sample data as follows:\n# x-data: sample 50 random values from a normal = N(-1, 1)\n#         + sample 50 random values from a normal = N(1, 1)\n# target: 50 values of 0 + 50 values of 1.\n#         These are essentially 100 values of the corresponding output index\n# We will fit the binary classification model:\n# If sigmoid(x+A) < 0.5 -> 0 else 1\n# Theoretically, A should be -(mean1 + mean2)/2\n\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Create data\nx_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3, 1, 50)))\ny_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\nx_data = tf.placeholder(shape=[1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[1], dtype=tf.float32)\n\n# Create variable (one model parameter = A)\nA = tf.Variable(tf.random_normal(mean=10, shape=[1]))\n\n# Add operation to graph\n# Want to create the operstion sigmoid(x + A)\n# Note, the sigmoid() part is in the loss function\nmy_output = tf.add(x_data, A)\n\n# Now we have to add another dimension to each (batch size of 1)\nmy_output_expanded = tf.expand_dims(my_output, 0)\ny_target_expanded = tf.expand_dims(y_target, 0)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Add classification loss (cross entropy)\nxentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output_expanded, labels=y_target_expanded)\n\n# Create Optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.05)\ntrain_step = my_opt.minimize(xentropy)\n\n# Run loop\nfor i in range(1400):\n    rand_index = np.random.choice(100)\n    rand_x = [x_vals[rand_index]]\n    rand_y = [y_vals[rand_index]]\n    \n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    if (i+1)%200==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n        print('Loss = ' + str(sess.run(xentropy, feed_dict={x_data: rand_x, y_target: rand_y})))\n\n# Evaluate Predictions\npredictions = []\nfor i in range(len(x_vals)):\n    x_val = [x_vals[i]]\n    prediction = sess.run(tf.round(tf.sigmoid(my_output)), feed_dict={x_data: x_val})\n    predictions.append(prediction[0])\n    \naccuracy = sum(x==y for x,y in zip(predictions, y_vals))/100.\nprint('Ending Accuracy = ' + str(np.round(accuracy, 2)))"""
02_TensorFlow_Way/06_Working_with_Batch_and_Stochastic_Training/06_batch_stochastic_training.py,16,"b""# Batch and Stochastic Training\n#----------------------------------\n#\n#  This python function illustrates two different training methods:\n#  batch and stochastic training.  For each model, we will use\n#  a regression model that predicts one model variable.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# We will implement a regression example in stochastic and batch training\n\n# Stochastic Training:\n# Create graph\nsess = tf.Session()\n\n# Create data\nx_vals = np.random.normal(1, 0.1, 100)\ny_vals = np.repeat(10., 100)\nx_data = tf.placeholder(shape=[1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[1], dtype=tf.float32)\n\n# Create variable (one model parameter = A)\nA = tf.Variable(tf.random_normal(shape=[1]))\n\n# Add operation to graph\nmy_output = tf.multiply(x_data, A)\n\n# Add L2 loss operation to graph\nloss = tf.square(my_output - y_target)\n\n# Create Optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.02)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nloss_stochastic = []\n# Run Loop\nfor i in range(100):\n    rand_index = np.random.choice(100)\n    rand_x = [x_vals[rand_index]]\n    rand_y = [y_vals[rand_index]]\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    if (i+1)%5==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n        temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n        print('Loss = ' + str(temp_loss))\n        loss_stochastic.append(temp_loss)\n        \n\n# Batch Training:\n# Re-initialize graph\nops.reset_default_graph()\nsess = tf.Session()\n\n# Declare batch size\nbatch_size = 20\n\n# Create data\nx_vals = np.random.normal(1, 0.1, 100)\ny_vals = np.repeat(10., 100)\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variable (one model parameter = A)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Add operation to graph\nmy_output = tf.matmul(x_data, A)\n\n# Add L2 loss operation to graph\nloss = tf.reduce_mean(tf.square(my_output - y_target))\n\n# Create Optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.02)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nloss_batch = []\n# Run Loop\nfor i in range(100):\n    rand_index = np.random.choice(100, size=batch_size)\n    rand_x = np.transpose([x_vals[rand_index]])\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    if (i+1)%5==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n        temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n        print('Loss = ' + str(temp_loss))\n        loss_batch.append(temp_loss)\n        \nplt.plot(range(0, 100, 5), loss_stochastic, 'b-', label='Stochastic Loss')\nplt.plot(range(0, 100, 5), loss_batch, 'r--', label='Batch Loss, size=20')\nplt.legend(loc='upper right', prop={'size': 11})\nplt.show()"""
02_TensorFlow_Way/07_Combining_Everything_Together/07_combining_everything_together.py,12,"b""# Combining Everything Together\n#----------------------------------\n# This file will perform binary classification on the\n# iris dataset. We will only predict if a flower is\n# I.setosa or not.\n#\n# We will create a simple binary classifier by creating a line\n# and running everything through a sigmoid to get a binary predictor.\n# The two features we will use are pedal length and pedal width.\n#\n# We will use batch training, but this can be easily\n# adapted to stochastic training.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Load the iris data\n# iris.target = {0, 1, 2}, where '0' is setosa\n# iris.data ~ [sepal.width, sepal.length, pedal.width, pedal.length]\niris = datasets.load_iris()\nbinary_target = np.array([1. if x==0 else 0. for x in iris.target])\niris_2d = np.array([[x[2], x[3]] for x in iris.data])\n\n# Declare batch size\nbatch_size = 20\n\n# Create graph\nsess = tf.Session()\n\n# Declare placeholders\nx1_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nx2_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables A and b (0 = x1 - A*x2 + b)\nA = tf.Variable(tf.random_normal(shape=[1, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\n\n# Add model to graph:\n# x1 - A*x2 + b\nmy_mult = tf.matmul(x2_data, A)\nmy_add = tf.add(my_mult, b)\nmy_output = tf.subtract(x1_data, my_add)\n\n# Add classification loss (cross entropy)\nxentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output, labels=y_target)\n\n# Create Optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.05)\ntrain_step = my_opt.minimize(xentropy)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Run Loop\nfor i in range(1000):\n    rand_index = np.random.choice(len(iris_2d), size=batch_size)\n    #rand_x = np.transpose([iris_2d[rand_index]])\n    rand_x = iris_2d[rand_index]\n    rand_x1 = np.array([[x[0]] for x in rand_x])\n    rand_x2 = np.array([[x[1]] for x in rand_x])\n    #rand_y = np.transpose([binary_target[rand_index]])\n    rand_y = np.array([[y] for y in binary_target[rand_index]])\n    sess.run(train_step, feed_dict={x1_data: rand_x1, x2_data: rand_x2, y_target: rand_y})\n    if (i+1)%200==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ', b = ' + str(sess.run(b)))\n        \n\n# Visualize Results\n# Pull out slope/intercept\n[[slope]] = sess.run(A)\n[[intercept]] = sess.run(b)\n\n# Create fitted line\nx = np.linspace(0, 3, num=50)\nablineValues = []\nfor i in x:\n  ablineValues.append(slope*i+intercept)\n\n# Plot the fitted line over the data\nsetosa_x = [a[1] for i,a in enumerate(iris_2d) if binary_target[i]==1]\nsetosa_y = [a[0] for i,a in enumerate(iris_2d) if binary_target[i]==1]\nnon_setosa_x = [a[1] for i,a in enumerate(iris_2d) if binary_target[i]==0]\nnon_setosa_y = [a[0] for i,a in enumerate(iris_2d) if binary_target[i]==0]\nplt.plot(setosa_x, setosa_y, 'rx', ms=10, mew=2, label='setosa')\nplt.plot(non_setosa_x, non_setosa_y, 'ro', label='Non-setosa')\nplt.plot(x, ablineValues, 'b-')\nplt.xlim([0.0, 2.7])\nplt.ylim([0.0, 7.1])\nplt.suptitle('Linear Separator For I.setosa', fontsize=20)\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.legend(loc='lower right')\nplt.show()"""
02_TensorFlow_Way/08_Evaluating_Models/08_evaluating_models.py,19,"b""# Evaluating models in TensorFlow\n#\n# This code will implement two models.  The first\n#  is a simple regression model, we will show how to\n#  call the loss function, MSE during training, and\n#  output it after for test and training sets.\n#\n# The second model will be a simple classification\n#  model.  We will also show how to print percent\n#  classified correctly during training and after\n#  for both the test and training sets.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Regression Example:\n# We will create sample data as follows:\n# x-data: 100 random samples from a normal ~ N(1, 0.1)\n# target: 100 values of the value 10.\n# We will fit the model:\n# x-data * A = target\n# Theoretically, A = 10.\n\n# Declare batch size\nbatch_size = 25\n\n# Create data\nx_vals = np.random.normal(1, 0.1, 100)\ny_vals = np.repeat(10., 100)\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Split data into train/test = 80%/20%\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n# Create variable (one model parameter = A)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Add operation to graph\nmy_output = tf.matmul(x_data, A)\n\n# Add L2 loss operation to graph\nloss = tf.reduce_mean(tf.square(my_output - y_target))\n\n# Create Optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.02)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Run Loop\nfor i in range(100):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = np.transpose([x_vals_train[rand_index]])\n    rand_y = np.transpose([y_vals_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    if (i+1)%25==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n        print('Loss = ' + str(sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})))\n\n# Evaluate accuracy (loss) on test set\nmse_test = sess.run(loss, feed_dict={x_data: np.transpose([x_vals_test]), y_target: np.transpose([y_vals_test])})\nmse_train = sess.run(loss, feed_dict={x_data: np.transpose([x_vals_train]), y_target: np.transpose([y_vals_train])})\nprint('MSE on test:' + str(np.round(mse_test, 2)))\nprint('MSE on train:' + str(np.round(mse_train, 2)))\n\n# Classification Example\n# We will create sample data as follows:\n# x-data: sample 50 random values from a normal = N(-1, 1)\n#         + sample 50 random values from a normal = N(1, 1)\n# target: 50 values of 0 + 50 values of 1.\n#         These are essentially 100 values of the corresponding output index\n# We will fit the binary classification model:\n# If sigmoid(x+A) < 0.5 -> 0 else 1\n# Theoretically, A should be -(mean1 + mean2)/2\n\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Declare batch size\nbatch_size = 25\n\n# Create data\nx_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(2, 1, 50)))\ny_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\nx_data = tf.placeholder(shape=[1, None], dtype=tf.float32)\ny_target = tf.placeholder(shape=[1, None], dtype=tf.float32)\n\n# Split data into train/test = 80%/20%\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n# Create variable (one model parameter = A)\nA = tf.Variable(tf.random_normal(mean=10, shape=[1]))\n\n# Add operation to graph\n# Want to create the operstion sigmoid(x + A)\n# Note, the sigmoid() part is in the loss function\nmy_output = tf.add(x_data, A)\n\n# Add classification loss (cross entropy)\nxentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output, labels=y_target))\n\n# Create Optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.05)\ntrain_step = my_opt.minimize(xentropy)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Run loop\nfor i in range(1800):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = [x_vals_train[rand_index]]\n    rand_y = [y_vals_train[rand_index]]\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    if (i+1)%200==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n        print('Loss = ' + str(sess.run(xentropy, feed_dict={x_data: rand_x, y_target: rand_y})))\n        \n# Evaluate Predictions on test set\ny_prediction = tf.squeeze(tf.round(tf.nn.sigmoid(tf.add(x_data, A))))\ncorrect_prediction = tf.equal(y_prediction, y_target)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nacc_value_test = sess.run(accuracy, feed_dict={x_data: [x_vals_test], y_target: [y_vals_test]})\nacc_value_train = sess.run(accuracy, feed_dict={x_data: [x_vals_train], y_target: [y_vals_train]})\nprint('Accuracy on train set: ' + str(acc_value_train))\nprint('Accuracy on test set: ' + str(acc_value_test))\n\n# Plot classification result\nA_result = -sess.run(A)\nbins = np.linspace(-5, 5, 50)\nplt.hist(x_vals[0:50], bins, alpha=0.5, label='N(-1,1)', color='blue')\nplt.hist(x_vals[50:100], bins[0:50], alpha=0.5, label='N(2,1)', color='red')\nplt.plot((A_result, A_result), (0, 8), 'k--', linewidth=3, label='A = '+ str(np.round(A_result, 2)))\nplt.legend(loc='upper right')\nplt.title('Binary Classifier, Accuracy=' + str(np.round(acc_value_test, 2)))\nplt.show()"""
03_Linear_Regression/01_Using_the_Matrix_Inverse_Method/01_lin_reg_inverse.py,7,"b""# Linear Regression: Inverse Matrix Method\n#----------------------------------\n#\n# This function shows how to use TensorFlow to\n# solve linear regression via the matrix inverse.\n#\n# Given Ax=b, solving for x:\n#  x = (t(A) * A)^(-1) * t(A) * b\n#  where t(A) is the transpose of A\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Create the data\nx_vals = np.linspace(0, 10, 100)\ny_vals = x_vals + np.random.normal(0, 1, 100)\n\n# Create design matrix\nx_vals_column = np.transpose(np.matrix(x_vals))\nones_column = np.transpose(np.matrix(np.repeat(1, 100)))\nA = np.column_stack((x_vals_column, ones_column))\n\n# Create b matrix\nb = np.transpose(np.matrix(y_vals))\n\n# Create tensors\nA_tensor = tf.constant(A)\nb_tensor = tf.constant(b)\n\n# Matrix inverse solution\ntA_A = tf.matmul(tf.transpose(A_tensor), A_tensor)\ntA_A_inv = tf.matrix_inverse(tA_A)\nproduct = tf.matmul(tA_A_inv, tf.transpose(A_tensor))\nsolution = tf.matmul(product, b_tensor)\n\nsolution_eval = sess.run(solution)\n\n# Extract coefficients\nslope = solution_eval[0][0]\ny_intercept = solution_eval[1][0]\n\nprint('slope: ' + str(slope))\nprint('y_intercept: ' + str(y_intercept))\n\n# Get best fit line\nbest_fit = []\nfor i in x_vals:\n  best_fit.append(slope*i+y_intercept)\n\n# Plot the results\nplt.plot(x_vals, y_vals, 'o', label='Data')\nplt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)\nplt.legend(loc='upper left')\nplt.show()"""
03_Linear_Regression/02_Implementing_a_Decomposition_Method/02_lin_reg_decomposition.py,8,"b""# Linear Regression: Decomposition Method\n#----------------------------------\n#\n# This function shows how to use TensorFlow to\n# solve linear regression via the matrix inverse.\n#\n# Given Ax=b, and a Cholesky decomposition such that\n#  A = L*L' then we can get solve for x via\n# 1) L*y=t(A)*b\n# 2) L'*x=y\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Create the data\nx_vals = np.linspace(0, 10, 100)\ny_vals = x_vals + np.random.normal(0, 1, 100)\n\n# Create design matrix\nx_vals_column = np.transpose(np.matrix(x_vals))\nones_column = np.transpose(np.matrix(np.repeat(1, 100)))\nA = np.column_stack((x_vals_column, ones_column))\n\n# Create b matrix\nb = np.transpose(np.matrix(y_vals))\n\n# Create tensors\nA_tensor = tf.constant(A)\nb_tensor = tf.constant(b)\n\n# Find Cholesky Decomposition\ntA_A = tf.matmul(tf.transpose(A_tensor), A_tensor)\nL = tf.cholesky(tA_A)\n\n# Solve L*y=t(A)*b\ntA_b = tf.matmul(tf.transpose(A_tensor), b)\nsol1 = tf.matrix_solve(L, tA_b)\n\n# Solve L' * y = sol1\nsol2 = tf.matrix_solve(tf.transpose(L), sol1)\n\nsolution_eval = sess.run(sol2)\n\n# Extract coefficients\nslope = solution_eval[0][0]\ny_intercept = solution_eval[1][0]\n\nprint('slope: ' + str(slope))\nprint('y_intercept: ' + str(y_intercept))\n\n# Get best fit line\nbest_fit = []\nfor i in x_vals:\n  best_fit.append(slope*i+y_intercept)\n\n# Plot the results\nplt.plot(x_vals, y_vals, 'o', label='Data')\nplt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)\nplt.legend(loc='upper left')\nplt.show()"""
03_Linear_Regression/03_TensorFlow_Way_of_Linear_Regression/03_lin_reg_tensorflow_way.py,9,"b""# Linear Regression: TensorFlow Way\n#----------------------------------\n#\n# This function shows how to use TensorFlow to\n# solve linear regression.\n# y = Ax + b\n#\n# We will use the iris data, specifically:\n#  y = Sepal Length\n#  x = Petal Width\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n\n# Declare batch size\nbatch_size = 25\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare model operations\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n# Declare loss function (L2 loss)\nloss = tf.reduce_mean(tf.square(y_target - model_output))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.05)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\nfor i in range(100):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = np.transpose([x_vals[rand_index]])\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n    if (i+1)%25==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n        print('Loss = ' + str(temp_loss))\n\n# Get the optimal coefficients\n[slope] = sess.run(A)\n[y_intercept] = sess.run(b)\n\n# Get best fit line\nbest_fit = []\nfor i in x_vals:\n  best_fit.append(slope*i+y_intercept)\n\n# Plot the result\nplt.plot(x_vals, y_vals, 'o', label='Data Points')\nplt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)\nplt.legend(loc='upper left')\nplt.title('Sepal Length vs Petal Width')\nplt.xlabel('Petal Width')\nplt.ylabel('Sepal Length')\nplt.show()\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title('L2 Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('L2 Loss')\nplt.show()\n"""
03_Linear_Regression/04_Loss_Functions_in_Linear_Regressions/04_lin_reg_l1_vs_l2.py,18,"b""# Linear Regression: L1 vs L2\n#----------------------------------\n#\n# This function shows how to use TensorFlow to\n# solve linear regression via the matrix inverse.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n\n# Declare batch size and number of iterations\nbatch_size = 25\nlearning_rate = 0.4 # Will not converge with learning rate at 0.4\niterations = 50\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare model operations\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n# Declare loss functions\nloss_l1 = tf.reduce_mean(tf.abs(y_target - model_output))\n\n# Declare optimizers\nmy_opt_l1 = tf.train.GradientDescentOptimizer(learning_rate)\ntrain_step_l1 = my_opt_l1.minimize(loss_l1)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec_l1 = []\nfor i in range(iterations):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = np.transpose([x_vals[rand_index]])\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step_l1, feed_dict={x_data: rand_x, y_target: rand_y})\n    temp_loss_l1 = sess.run(loss_l1, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec_l1.append(temp_loss_l1)\n    if (i+1)%25==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n\n\n# L2 Loss\n# Reinitialize graph\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare model operations\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n# Declare loss functions\nloss_l2 = tf.reduce_mean(tf.square(y_target - model_output))\n\n# Declare optimizers\nmy_opt_l2 = tf.train.GradientDescentOptimizer(learning_rate)\ntrain_step_l2 = my_opt_l2.minimize(loss_l2)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nloss_vec_l2 = []\nfor i in range(iterations):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = np.transpose([x_vals[rand_index]])\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step_l2, feed_dict={x_data: rand_x, y_target: rand_y})\n    temp_loss_l2 = sess.run(loss_l2, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec_l2.append(temp_loss_l2)\n    if (i+1)%25==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n\n\n# Plot loss over time\nplt.plot(loss_vec_l1, 'k-', label='L1 Loss')\nplt.plot(loss_vec_l2, 'r--', label='L2 Loss')\nplt.title('L1 and L2 Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('L1 Loss')\nplt.legend(loc='upper right')\nplt.show()\n"""
03_Linear_Regression/05_Implementing_Deming_Regression/05_deming_regression.py,12,"b""# Deming Regression\n#----------------------------------\n#\n# This function shows how to use TensorFlow to\n# solve linear Deming regression.\n# y = Ax + b\n#\n# We will use the iris data, specifically:\n#  y = Sepal Length\n#  x = Petal Width\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Set a random seed\ntf.set_random_seed(42)\nnp.random.seed(42)\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n\n# Declare batch size\nbatch_size = 50\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare model operations\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n# Declare Deming loss function\ndeming_numerator = tf.abs(tf.subtract(y_target, tf.add(tf.matmul(x_data, A), b)))\ndeming_denominator = tf.sqrt(tf.add(tf.square(A),1))\nloss = tf.reduce_mean(tf.truediv(deming_numerator, deming_denominator))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.15)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\nfor i in range(1000):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = np.transpose([x_vals[rand_index]])\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n    if (i+1)%50 == 0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n        print('Loss = ' + str(temp_loss))\n\n# Get the optimal coefficients\n[slope] = sess.run(A)\n[y_intercept] = sess.run(b)\n\n# Get best fit line\nbest_fit = []\nfor i in x_vals:\n  best_fit.append(slope*i+y_intercept)\n\n# Plot the result\nplt.plot(x_vals, y_vals, 'o', label='Data Points')\nplt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)\nplt.legend(loc='upper left')\nplt.title('Sepal Length vs Petal Width')\nplt.xlabel('Petal Width')\nplt.ylabel('Sepal Length')\nplt.show()\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title('L2 Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('L2 Loss')\nplt.show()\n"""
03_Linear_Regression/06_Implementing_Lasso_and_Ridge_Regression/06_lasso_and_ridge_regression.py,16,"b""# LASSO and Ridge Regression\n# \n# This function shows how to use TensorFlow to solve LASSO or \n# Ridge regression for \n# y = Ax + b\n# \n# We will use the iris data, specifically: \n#   y = Sepal Length \n#   x = Petal Width\n\n# import required libraries\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\n\n\n# Specify 'Ridge' or 'LASSO'\n#regression_type = 'LASSO'\nregression_type = 'Ridge'\n\n# clear out old graph\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n###\n# Load iris data\n###\n\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n\n###\n# Model Parameters\n###\n\n# Declare batch size\nbatch_size = 50\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# make results reproducible\nseed = 13\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare model operations\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n###\n# Loss Functions\n###\n\n# Select appropriate loss function based on regression type\n\nif regression_type == 'LASSO':\n    # Declare Lasso loss function\n    # Lasso Loss = L2_Loss + heavyside_step,\n    # Where heavyside_step ~ 0 if A < constant, otherwise ~ 99\n    lasso_param = tf.constant(0.9)\n    heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-50., tf.subtract(A, lasso_param)))))\n    regularization_param = tf.multiply(heavyside_step, 99.)\n    loss = tf.add(tf.reduce_mean(tf.square(y_target - model_output)), regularization_param)\n\nelif regression_type == 'Ridge':\n    # Declare the Ridge loss function\n    # Ridge loss = L2_loss + L2 norm of slope\n    ridge_param = tf.constant(1.)\n    ridge_loss = tf.reduce_mean(tf.square(A))\n    loss = tf.expand_dims(tf.add(tf.reduce_mean(tf.square(y_target - model_output)), tf.multiply(ridge_param, ridge_loss)), 0)\n    \nelse:\n    print('Invalid regression_type parameter value',file=sys.stderr)\n\n\n###\n# Optimizer\n###\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.001)\ntrain_step = my_opt.minimize(loss)\n\n###\n# Run regression\n###\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\nfor i in range(1500):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = np.transpose([x_vals[rand_index]])\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss[0])\n    if (i+1)%300==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n        print('Loss = ' + str(temp_loss))\n        print('\\n')\n\n###\n# Extract regression results\n###\n\n# Get the optimal coefficients\n[slope] = sess.run(A)\n[y_intercept] = sess.run(b)\n\n# Get best fit line\nbest_fit = []\nfor i in x_vals:\n  best_fit.append(slope*i+y_intercept)\n\n\n###\n# Plot results\n###\n\n# Plot regression line against data points\nplt.plot(x_vals, y_vals, 'o', label='Data Points')\nplt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)\nplt.legend(loc='upper left')\nplt.title('Sepal Length vs Pedal Width')\nplt.xlabel('Pedal Width')\nplt.ylabel('Sepal Length')\nplt.show()\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title(regression_type + ' Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Loss')\nplt.show()\n\n"""
03_Linear_Regression/07_Implementing_Elasticnet_Regression/07_elasticnet_regression.py,16,"b""# Elastic Net Regression\n#----------------------------------\n#\n# This function shows how to use TensorFlow to\n# solve elastic net regression.\n# y = Ax + b\n#\n# We will use the iris data, specifically:\n#  y = Sepal Length\n#  x = Pedal Length, Petal Width, Sepal Width\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\n\n###\n# Set up for TensorFlow\n###\n\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n###\n# Obtain data\n###\n\n# Load the data\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([[x[1], x[2], x[3]] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n\n###\n# Setup model\n###\n\n# make results reproducible\nseed = 13\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Declare batch size\nbatch_size = 50\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 3], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[3,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare model operations\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n# Declare the elastic net loss function\nelastic_param1 = tf.constant(1.)\nelastic_param2 = tf.constant(1.)\nl1_a_loss = tf.reduce_mean(tf.abs(A))\nl2_a_loss = tf.reduce_mean(tf.square(A))\ne1_term = tf.multiply(elastic_param1, l1_a_loss)\ne2_term = tf.multiply(elastic_param2, l2_a_loss)\nloss = tf.expand_dims(tf.add(tf.add(tf.reduce_mean(tf.square(y_target - model_output)), e1_term), e2_term), 0)\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.001)\ntrain_step = my_opt.minimize(loss)\n\n###\n# Train model\n###\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\nfor i in range(1000):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = x_vals[rand_index]\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss[0])\n    if (i+1)%250==0:\n        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n        print('Loss = ' + str(temp_loss))\n\n###\n# Extract model results\n###\n\n# Get the optimal coefficients\n[[sw_coef], [pl_coef], [pw_ceof]] = sess.run(A)\n[y_intercept] = sess.run(b)\n\n###\n# Plot results\n###\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title('Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Loss')\nplt.show()\n\n"""
03_Linear_Regression/08_Implementing_Logistic_Regression/08_logistic_regression.py,13,"b""# Logistic Regression\n#----------------------------------\n#\n# This function shows how to use TensorFlow to\n# solve logistic regression.\n# y = sigmoid(Ax + b)\n#\n# We will use the low birth weight data, specifically:\n#  y = 0 or 1 = low birth weight\n#  x = demographic and medical history data\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport requests\nfrom tensorflow.python.framework import ops\nimport os.path\nimport csv\n\n\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n###\n# Obtain and prepare data for modeling\n###\n\n# Set name of data file\nbirth_weight_file = 'birth_weight.csv'\n\n# Download data and create data file if file does not exist in current directory\nif not os.path.exists(birth_weight_file):\n    birthdata_url = 'https://github.com/nfmcclure/tensorflow_cookbook/raw/master/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat'\n    birth_file = requests.get(birthdata_url)\n    birth_data = birth_file.text.split('\\r\\n')\n    birth_header = birth_data[0].split('\\t')\n    birth_data = [[float(x) for x in y.split('\\t') if len(x)>=1] for y in birth_data[1:] if len(y)>=1]\n    with open(birth_weight_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(birth_header)\n        writer.writerows(birth_data)\n        f.close()\n\n# Read birth weight data into memory\nbirth_data = []\nwith open(birth_weight_file, newline='') as csvfile:\n     csv_reader = csv.reader(csvfile)\n     birth_header = next(csv_reader)\n     for row in csv_reader:\n         birth_data.append(row)\n\nbirth_data = [[float(x) for x in row] for row in birth_data]\n\n# Pull out target variable\ny_vals = np.array([x[0] for x in birth_data])\n# Pull out predictor variables (not id, not target, and not birthweight)\nx_vals = np.array([x[1:8] for x in birth_data])\n\n# Set for reproducible results\nseed = 99\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Split data into train/test = 80%/20%\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n# Normalize by column (min-max norm)\ndef normalize_cols(m, col_min=np.array([None]), col_max=np.array([None])):\n    if not col_min[0]:\n        col_min = m.min(axis=0)\n    if not col_max[0]:\n        col_max = m.max(axis=0)\n    return (m-col_min) / (col_max - col_min), col_min, col_max\n    \nx_vals_train, train_min, train_max = np.nan_to_num(normalize_cols(x_vals_train))\nx_vals_test, _, _ = np.nan_to_num(normalize_cols(x_vals_test, train_min, train_max))\n\n###\n# Define Tensorflow computational graph\xc2\xb6\n###\n\n# Declare batch size\nbatch_size = 25\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 7], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[7,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare model operations\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n# Declare loss function (Cross Entropy loss)\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_opt.minimize(loss)\n\n###\n# Train model\n###\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Actual Prediction\nprediction = tf.round(tf.sigmoid(model_output))\npredictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\naccuracy = tf.reduce_mean(predictions_correct)\n\n# Training loop\nloss_vec = []\ntrain_acc = []\ntest_acc = []\nfor i in range(1500):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = x_vals_train[rand_index]\n    rand_y = np.transpose([y_vals_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n    temp_acc_train = sess.run(accuracy, feed_dict={x_data: x_vals_train, y_target: np.transpose([y_vals_train])})\n    train_acc.append(temp_acc_train)\n    temp_acc_test = sess.run(accuracy, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n    test_acc.append(temp_acc_test)\n    if (i+1)%300==0:\n        print('Loss = ' + str(temp_loss))\n        \n\n###\n# Display model performance\n###\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title('Cross Entropy Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Cross Entropy Loss')\nplt.show()\n\n# Plot train and test accuracy\nplt.plot(train_acc, 'k-', label='Train Set Accuracy')\nplt.plot(test_acc, 'r--', label='Test Set Accuracy')\nplt.title('Train and Test Accuracy')\nplt.xlabel('Generation')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n"""
04_Support_Vector_Machines/02_Working_with_Linear_SVMs/02_linear_svm.py,15,"b""# Linear Support Vector Machine: Soft Margin\n# ----------------------------------\n#\n# This function shows how to use TensorFlow to\n# create a soft margin SVM\n#\n# We will use the iris data, specifically:\n#  x1 = Sepal Length\n#  x2 = Petal Width\n# Class 1 : I. setosa\n# Class -1: not I. setosa\n#\n# We know here that x and y are linearly seperable\n# for I. setosa classification.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Set random seeds\nnp.random.seed(7)\ntf.set_random_seed(7)\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([[x[0], x[3]] for x in iris.data])\ny_vals = np.array([1 if y == 0 else -1 for y in iris.target])\n\n# Split data into train/test sets\ntrain_indices = np.random.choice(len(x_vals),\n                                 int(round(len(x_vals)*0.9)),\n                                 replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n# Declare batch size\nbatch_size = 135\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[2, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\n\n# Declare model operations\nmodel_output = tf.subtract(tf.matmul(x_data, A), b)\n\n# Declare vector L2 'norm' function squared\nl2_norm = tf.reduce_sum(tf.square(A))\n\n# Declare loss function\n# Loss = max(0, 1-pred*actual) + alpha * L2_norm(A)^2\n# L2 regularization parameter, alpha\nalpha = tf.constant([0.01])\n# Margin term in loss\nclassification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1., tf.multiply(model_output, y_target))))\n# Put terms together\nloss = tf.add(classification_term, tf.multiply(alpha, l2_norm))\n\n# Declare prediction function\nprediction = tf.sign(model_output)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), tf.float32))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\ntrain_accuracy = []\ntest_accuracy = []\nfor i in range(500):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = x_vals_train[rand_index]\n    rand_y = np.transpose([y_vals_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n\n    train_acc_temp = sess.run(accuracy, feed_dict={\n        x_data: x_vals_train,\n        y_target: np.transpose([y_vals_train])})\n    train_accuracy.append(train_acc_temp)\n\n    test_acc_temp = sess.run(accuracy, feed_dict={\n        x_data: x_vals_test,\n        y_target: np.transpose([y_vals_test])})\n    test_accuracy.append(test_acc_temp)\n\n    if (i + 1) % 100 == 0:\n        print('Step #{} A = {}, b = {}'.format(\n            str(i+1),\n            str(sess.run(A)),\n            str(sess.run(b))\n        ))\n        print('Loss = ' + str(temp_loss))\n\n# Extract coefficients\n[[a1], [a2]] = sess.run(A)\n[[b]] = sess.run(b)\nslope = -a2/a1\ny_intercept = b/a1\n\n# Extract x1 and x2 vals\nx1_vals = [d[1] for d in x_vals]\n\n# Get best fit line\nbest_fit = []\nfor i in x1_vals:\n    best_fit.append(slope*i+y_intercept)\n\n# Separate I. setosa\nsetosa_x = [d[1] for i, d in enumerate(x_vals) if y_vals[i] == 1]\nsetosa_y = [d[0] for i, d in enumerate(x_vals) if y_vals[i] == 1]\nnot_setosa_x = [d[1] for i, d in enumerate(x_vals) if y_vals[i] == -1]\nnot_setosa_y = [d[0] for i, d in enumerate(x_vals) if y_vals[i] == -1]\n\n# Plot data and line\nplt.plot(setosa_x, setosa_y, 'o', label='I. setosa')\nplt.plot(not_setosa_x, not_setosa_y, 'x', label='Non-setosa')\nplt.plot(x1_vals, best_fit, 'r-', label='Linear Separator', linewidth=3)\nplt.ylim([0, 10])\nplt.legend(loc='lower right')\nplt.title('Sepal Length vs Petal Width')\nplt.xlabel('Petal Width')\nplt.ylabel('Sepal Length')\nplt.show()\n\n# Plot train/test accuracies\nplt.plot(train_accuracy, 'k-', label='Training Accuracy')\nplt.plot(test_accuracy, 'r--', label='Test Accuracy')\nplt.title('Train and Test Set Accuracies')\nplt.xlabel('Generation')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title('Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Loss')\nplt.show()\n"""
04_Support_Vector_Machines/03_Reduction_to_Linear_Regression/03_support_vector_regression.py,10,"b""# SVM Regression\n#----------------------------------\n#\n# This function shows how to use TensorFlow to\n# solve support vector regression. We are going\n# to find the line that has the maximum margin\n# which INCLUDES as many points as possible\n#\n# We will use the iris data, specifically:\n#  y = Sepal Length\n#  x = Pedal Width\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n\n# Split data into train/test sets\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n# Declare batch size\nbatch_size = 50\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for linear regression\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare model operations\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n# Declare loss function\n# = max(0, abs(target - predicted) + epsilon)\n# 1/2 margin width parameter = epsilon\nepsilon = tf.constant([0.5])\n# Margin term in loss\nloss = tf.reduce_mean(tf.maximum(0., tf.subtract(tf.abs(tf.subtract(model_output, y_target)), epsilon)))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.075)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\ntrain_loss = []\ntest_loss = []\nfor i in range(200):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = np.transpose([x_vals_train[rand_index]])\n    rand_y = np.transpose([y_vals_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    \n    temp_train_loss = sess.run(loss, feed_dict={x_data: np.transpose([x_vals_train]), y_target: np.transpose([y_vals_train])})\n    train_loss.append(temp_train_loss)\n    \n    temp_test_loss = sess.run(loss, feed_dict={x_data: np.transpose([x_vals_test]), y_target: np.transpose([y_vals_test])})\n    test_loss.append(temp_test_loss)\n    if (i+1)%50==0:\n        print('-----------')\n        print('Generation: ' + str(i+1))\n        print('A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n        print('Train Loss = ' + str(temp_train_loss))\n        print('Test Loss = ' + str(temp_test_loss))\n\n# Extract Coefficients\n[[slope]] = sess.run(A)\n[[y_intercept]] = sess.run(b)\nwidth = sess.run(epsilon)\n\n# Get best fit line\nbest_fit = []\nbest_fit_upper = []\nbest_fit_lower = []\nfor i in x_vals:\n    best_fit.append(slope*i+y_intercept)\n    best_fit_upper.append(slope*i+y_intercept+width)\n    best_fit_lower.append(slope*i+y_intercept-width)\n\n# Plot fit with data\nplt.plot(x_vals, y_vals, 'o', label='Data Points')\nplt.plot(x_vals, best_fit, 'r-', label='SVM Regression Line', linewidth=3)\nplt.plot(x_vals, best_fit_upper, 'r--', linewidth=2)\nplt.plot(x_vals, best_fit_lower, 'r--', linewidth=2)\nplt.ylim([0, 10])\nplt.legend(loc='lower right')\nplt.title('Sepal Length vs Petal Width')\nplt.xlabel('Petal Width')\nplt.ylabel('Sepal Length')\nplt.show()\n\n# Plot loss over time\nplt.plot(train_loss, 'k-', label='Train Set Loss')\nplt.plot(test_loss, 'r--', label='Test Set Loss')\nplt.title('L2 Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('L2 Loss')\nplt.legend(loc='upper right')\nplt.show()\n"""
04_Support_Vector_Machines/04_Working_with_Kernels/04_svm_kernels.py,26,"b""# Illustration of Various Kernels\n#----------------------------------\n#\n# This function wll illustrate how to\n# implement various kernels in TensorFlow.\n#\n# Linear Kernel:\n# K(x1, x2) = t(x1) * x2\n#\n# Gaussian Kernel (RBF):\n# K(x1, x2) = exp(-gamma * abs(x1 - x2)^2)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Generate non-lnear data\n(x_vals, y_vals) = datasets.make_circles(n_samples=350, factor=.5, noise=.1)\ny_vals = np.array([1 if y==1 else -1 for y in y_vals])\nclass1_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==1]\nclass1_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==1]\nclass2_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==-1]\nclass2_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==-1]\n\n# Declare batch size\nbatch_size = 350\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nprediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n\n# Create variables for svm\nb = tf.Variable(tf.random_normal(shape=[1,batch_size]))\n\n# Apply kernel\n# Linear Kernel\n# my_kernel = tf.matmul(x_data, tf.transpose(x_data))\n\n# Gaussian (RBF) kernel\ngamma = tf.constant(-50.0)\ndist = tf.reduce_sum(tf.square(x_data), 1)\ndist = tf.reshape(dist, [-1,1])\nsq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(x_data, tf.transpose(x_data)))), tf.transpose(dist))\nmy_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))\n\n# Compute SVM Model\nfirst_term = tf.reduce_sum(b)\nb_vec_cross = tf.matmul(tf.transpose(b), b)\ny_target_cross = tf.matmul(y_target, tf.transpose(y_target))\nsecond_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)))\nloss = tf.negative(tf.subtract(first_term, second_term))\n\n# Create Prediction Kernel\n# Linear prediction kernel\n# my_kernel = tf.matmul(x_data, tf.transpose(prediction_grid))\n\n# Gaussian (RBF) prediction kernel\nrA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])\nrB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])\npred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))\npred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))\n\nprediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b), pred_kernel)\nprediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output))\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction), tf.squeeze(y_target)), tf.float32))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.002)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\nbatch_accuracy = []\nfor i in range(1000):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = x_vals[rand_index]\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    \n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n    \n    acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x,\n                                             y_target: rand_y,\n                                             prediction_grid:rand_x})\n    batch_accuracy.append(acc_temp)\n    \n    if (i+1)%250==0:\n        print('Step #' + str(i+1))\n        print('Loss = ' + str(temp_loss))\n\n# Create a mesh to plot points in\nx_min, x_max = x_vals[:, 0].min() - 1, x_vals[:, 0].max() + 1\ny_min, y_max = x_vals[:, 1].min() - 1, x_vals[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n[grid_predictions] = sess.run(prediction, feed_dict={x_data: rand_x,\n                                                   y_target: rand_y,\n                                                   prediction_grid: grid_points})\ngrid_predictions = grid_predictions.reshape(xx.shape)\n\n# Plot points and grid\nplt.contourf(xx, yy, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\nplt.plot(class1_x, class1_y, 'ro', label='Class 1')\nplt.plot(class2_x, class2_y, 'kx', label='Class -1')\nplt.title('Gaussian SVM Results')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(loc='lower right')\nplt.ylim([-1.5, 1.5])\nplt.xlim([-1.5, 1.5])\nplt.show()\n\n# Plot batch accuracy\nplt.plot(batch_accuracy, 'k-', label='Accuracy')\nplt.title('Batch Accuracy')\nplt.xlabel('Generation')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title('Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Loss')\nplt.show()\n\n# Evaluate on new/unseen data points\n# New data points:\nnew_points = np.array([(-0.75, -0.75),\n                       (-0.5, -0.5),\n                       (-0.25, -0.25),\n                       (0.25, 0.25),\n                       (0.5, 0.5),\n                       (0.75, 0.75)])\n\n[evaluations] = sess.run(prediction, feed_dict={x_data: x_vals,\n                                                y_target: np.transpose([y_vals]),\n                                                prediction_grid: new_points})\n\nfor ix, p in enumerate(new_points):\n    print('{} : class={}'.format(p, evaluations[ix]))\n"""
04_Support_Vector_Machines/05_Implementing_Nonlinear_SVMs/05_nonlinear_svm.py,22,"b""# Nonlinear SVM Example\n#\n# This function wll illustrate how to\n# implement the gaussian kernel on\n# the iris dataset.\n#\n# Gaussian Kernel:\n# K(x1, x2) = exp(-gamma * abs(x1 - x2)^2)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([[x[0], x[3]] for x in iris.data])\ny_vals = np.array([1 if y == 0 else -1 for y in iris.target])\nclass1_x = [x[0] for i, x in enumerate(x_vals) if y_vals[i] == 1]\nclass1_y = [x[1] for i, x in enumerate(x_vals) if y_vals[i] == 1]\nclass2_x = [x[0] for i, x in enumerate(x_vals) if y_vals[i] == -1]\nclass2_y = [x[1] for i, x in enumerate(x_vals) if y_vals[i] == -1]\n\n# Declare batch size\nbatch_size = 150\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nprediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n\n# Create variables for svm\nb = tf.Variable(tf.random_normal(shape=[1, batch_size]))\n\n# Gaussian (RBF) kernel\ngamma = tf.constant(-25.0)\nsq_dists = tf.multiply(2., tf.matmul(x_data, tf.transpose(x_data)))\nmy_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))\n\n# Compute SVM Model\nfirst_term = tf.reduce_sum(b)\nb_vec_cross = tf.matmul(tf.transpose(b), b)\ny_target_cross = tf.matmul(y_target, tf.transpose(y_target))\nsecond_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)))\nloss = tf.negative(tf.subtract(first_term, second_term))\n\n# Gaussian (RBF) prediction kernel\nrA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1), [-1, 1])\nrB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1), [-1, 1])\npred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))\npred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))\n\nprediction_output = tf.matmul(tf.multiply(tf.transpose(y_target), b), pred_kernel)\nprediction = tf.sign(prediction_output - tf.reduce_mean(prediction_output))\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction), tf.squeeze(y_target)), tf.float32))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\nbatch_accuracy = []\nfor i in range(300):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = x_vals[rand_index]\n    rand_y = np.transpose([y_vals[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    \n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n    \n    acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x,\n                                             y_target: rand_y,\n                                             prediction_grid: rand_x})\n    batch_accuracy.append(acc_temp)\n    \n    if (i + 1) % 75 == 0:\n        print('Step #' + str(i + 1))\n        print('Loss = ' + str(temp_loss))\n\n# Create a mesh to plot points in\nx_min, x_max = x_vals[:, 0].min() - 1, x_vals[:, 0].max() + 1\ny_min, y_max = x_vals[:, 1].min() - 1, x_vals[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n[grid_predictions] = sess.run(prediction, feed_dict={x_data: x_vals,\n                                                     y_target: np.transpose([y_vals]),\n                                                     prediction_grid: grid_points})\ngrid_predictions = grid_predictions.reshape(xx.shape)\n\n# Plot points and grid\nplt.contourf(xx, yy, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\nplt.plot(class1_x, class1_y, 'ro', label='I. setosa')\nplt.plot(class2_x, class2_y, 'kx', label='Non setosa')\nplt.title('Gaussian SVM Results on Iris Data')\nplt.xlabel('Petal Length')\nplt.ylabel('Sepal Width')\nplt.legend(loc='lower right')\nplt.ylim([-0.5, 3.0])\nplt.xlim([3.5, 8.5])\nplt.show()\n\n# Plot batch accuracy\nplt.plot(batch_accuracy, 'k-', label='Accuracy')\nplt.title('Batch Accuracy')\nplt.xlabel('Generation')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title('Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Loss')\nplt.show()\n"""
04_Support_Vector_Machines/06_Implementing_Multiclass_SVMs/06_multiclass_svm.py,26,"b""# Multi-class (Nonlinear) SVM Example\n#\n# This function wll illustrate how to\n# implement the gaussian kernel with\n# multiple classes on the iris dataset.\n#\n# Gaussian Kernel:\n# K(x1, x2) = exp(-gamma * abs(x1 - x2)^2)\n#\n# X : (Sepal Length, Petal Width)\n# Y: (I. setosa, I. virginica, I. versicolor) (3 classes)\n#\n# Basic idea: introduce an extra dimension to do\n# one vs all classification.\n#\n# The prediction of a point will be the category with\n# the largest margin or distance to boundary.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\n# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\niris = datasets.load_iris()\nx_vals = np.array([[x[0], x[3]] for x in iris.data])\ny_vals1 = np.array([1 if y == 0 else -1 for y in iris.target])\ny_vals2 = np.array([1 if y == 1 else -1 for y in iris.target])\ny_vals3 = np.array([1 if y == 2 else -1 for y in iris.target])\ny_vals = np.array([y_vals1, y_vals2, y_vals3])\nclass1_x = [x[0] for i, x in enumerate(x_vals) if iris.target[i] == 0]\nclass1_y = [x[1] for i, x in enumerate(x_vals) if iris.target[i] == 0]\nclass2_x = [x[0] for i, x in enumerate(x_vals) if iris.target[i] == 1]\nclass2_y = [x[1] for i, x in enumerate(x_vals) if iris.target[i] == 1]\nclass3_x = [x[0] for i, x in enumerate(x_vals) if iris.target[i] == 2]\nclass3_y = [x[1] for i, x in enumerate(x_vals) if iris.target[i] == 2]\n\n# Declare batch size\nbatch_size = 50\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[3, None], dtype=tf.float32)\nprediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n\n# Create variables for svm\nb = tf.Variable(tf.random_normal(shape=[3, batch_size]))\n\n# Gaussian (RBF) kernel\ngamma = tf.constant(-10.0)\ndist = tf.reduce_sum(tf.square(x_data), 1)\ndist = tf.reshape(dist, [-1, 1])\nsq_dists = tf.multiply(2., tf.matmul(x_data, tf.transpose(x_data)))\nmy_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))\n\n\n# Declare function to do reshape/batch multiplication\ndef reshape_matmul(mat, _size):\n    v1 = tf.expand_dims(mat, 1)\n    v2 = tf.reshape(v1, [3, _size, 1])\n    return tf.matmul(v2, v1)\n\n# Compute SVM Model\nfirst_term = tf.reduce_sum(b)\nb_vec_cross = tf.matmul(tf.transpose(b), b)\ny_target_cross = reshape_matmul(y_target, batch_size)\n\nsecond_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)), [1, 2])\nloss = tf.reduce_sum(tf.negative(tf.subtract(first_term, second_term)))\n\n# Gaussian (RBF) prediction kernel\nrA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1), [-1, 1])\nrB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1), [-1, 1])\npred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))\npred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))\n\nprediction_output = tf.matmul(tf.multiply(y_target, b), pred_kernel)\nprediction = tf.argmax(prediction_output - tf.expand_dims(tf.reduce_mean(prediction_output, 1), 1), 0)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(y_target, 0)), tf.float32))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\nbatch_accuracy = []\nfor i in range(100):\n    rand_index = np.random.choice(len(x_vals), size=batch_size)\n    rand_x = x_vals[rand_index]\n    rand_y = y_vals[:, rand_index]\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    \n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n    \n    acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x,\n                                             y_target: rand_y,\n                                             prediction_grid: rand_x})\n    batch_accuracy.append(acc_temp)\n    \n    if (i + 1) % 25 == 0:\n        print('Step #' + str(i+1))\n        print('Loss = ' + str(temp_loss))\n\n# Create a mesh to plot points in\nx_min, x_max = x_vals[:, 0].min() - 1, x_vals[:, 0].max() + 1\ny_min, y_max = x_vals[:, 1].min() - 1, x_vals[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\ngrid_predictions = sess.run(prediction, feed_dict={x_data: rand_x,\n                                                   y_target: rand_y,\n                                                   prediction_grid: grid_points})\ngrid_predictions = grid_predictions.reshape(xx.shape)\n\n# Plot points and grid\nplt.contourf(xx, yy, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\nplt.plot(class1_x, class1_y, 'ro', label='I. setosa')\nplt.plot(class2_x, class2_y, 'kx', label='I. versicolor')\nplt.plot(class3_x, class3_y, 'gv', label='I. virginica')\nplt.title('Gaussian SVM Results on Iris Data')\nplt.xlabel('Petal Length')\nplt.ylabel('Sepal Width')\nplt.legend(loc='lower right')\nplt.ylim([-0.5, 3.0])\nplt.xlim([3.5, 8.5])\nplt.show()\n\n# Plot batch accuracy\nplt.plot(batch_accuracy, 'k-', label='Accuracy')\nplt.title('Batch Accuracy')\nplt.xlabel('Generation')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n# Plot loss over time\nplt.plot(loss_vec, 'k-')\nplt.title('Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Loss')\nplt.show()\n\n# Evaluations on new/unseen data\n\n"""
05_Nearest_Neighbor_Methods/02_Working_with_Nearest_Neighbors/02_nearest_neighbor.py,16,"b""# k-Nearest Neighbor\n#----------------------------------\n#\n# This function illustrates how to use\n# k-nearest neighbors in tensorflow\n#\n# We will use the 1970s Boston housing dataset\n# which is available through the UCI\n# ML data repository.\n#\n# Data:\n#----------x-values-----------\n# CRIM   : per capita crime rate by town\n# ZN     : prop. of res. land zones\n# INDUS  : prop. of non-retail business acres\n# CHAS   : Charles river dummy variable\n# NOX    : nitrix oxides concentration / 10 M\n# RM     : Avg. # of rooms per building\n# AGE    : prop. of buildings built prior to 1940\n# DIS    : Weighted distances to employment centers\n# RAD    : Index of radian highway access\n# TAX    : Full tax rate value per $10k\n# PTRATIO: Pupil/Teacher ratio by town\n# B      : 1000*(Bk-0.63)^2, Bk=prop. of blacks\n# LSTAT  : % lower status of pop\n#------------y-value-----------\n# MEDV   : Median Value of homes in $1,000's\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport requests\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\nhousing_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\nhousing_header = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ncols_used = ['CRIM', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nnum_features = len(cols_used)\nhousing_file = requests.get(housing_url)\nhousing_data = [[float(x) for x in y.split(' ') if len(x)>=1] for y in housing_file.text.split('\\n') if len(y)>=1]\n\ny_vals = np.transpose([np.array([y[13] for y in housing_data])])\nx_vals = np.array([[x for i,x in enumerate(y) if housing_header[i] in cols_used] for y in housing_data])\n\n## Min-Max Scaling\nx_vals = (x_vals - x_vals.min(0)) / x_vals.ptp(0)\n\n# Split the data into train and test sets\nnp.random.seed(13)  #make results reproducible\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n# Declare k-value and batch size\nk = 4\nbatch_size=len(x_vals_test)\n\n# Placeholders\nx_data_train = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\nx_data_test = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\ny_target_train = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target_test = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Declare distance metric\n# L1\ndistance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), axis=2)\n\n# L2\n#distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), reduction_indices=1))\n\n# Predict: Get min distance index (Nearest neighbor)\n#prediction = tf.arg_min(distance, 0)\ntop_k_xvals, top_k_indices = tf.nn.top_k(tf.negative(distance), k=k)\ntop_k_xvals = tf.truediv(1.0, top_k_xvals)\nx_sums = tf.expand_dims(tf.reduce_sum(top_k_xvals, 1),1)\nx_sums_repeated = tf.matmul(x_sums,tf.ones([1, k], tf.float32))\nx_val_weights = tf.expand_dims(tf.div(top_k_xvals,x_sums_repeated), 1)\n\ntop_k_yvals = tf.gather(y_target_train, top_k_indices)\nprediction = tf.squeeze(tf.matmul(x_val_weights,top_k_yvals), axis=[1])\n\n# Calculate MSE\nmse = tf.div(tf.reduce_sum(tf.square(tf.subtract(prediction, y_target_test))), batch_size)\n\n# Calculate how many loops over training data\nnum_loops = int(np.ceil(len(x_vals_test)/batch_size))\n\nfor i in range(num_loops):\n    min_index = i*batch_size\n    max_index = min((i+1)*batch_size,len(x_vals_train))\n    x_batch = x_vals_test[min_index:max_index]\n    y_batch = y_vals_test[min_index:max_index]\n    predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n                                         y_target_train: y_vals_train, y_target_test: y_batch})\n    batch_mse = sess.run(mse, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n                                         y_target_train: y_vals_train, y_target_test: y_batch})\n\n    print('Batch #' + str(i+1) + ' MSE: ' + str(np.round(batch_mse,3)))\n\n# Plot prediction and actual distribution\nbins = np.linspace(5, 50, 45)\n\nplt.hist(predictions, bins, alpha=0.5, label='Prediction')\nplt.hist(y_batch, bins, alpha=0.5, label='Actual')\nplt.title('Histogram of Predicted and Actual Values')\nplt.xlabel('Med Home Value in $1,000s')\nplt.ylabel('Frequency')\nplt.legend(loc='upper right')\nplt.show()\n\n"""
05_Nearest_Neighbor_Methods/03_Working_with_Text_Distances/03_text_distances.py,10,"b""# Text Distances\n#----------------------------------\n#\n# This function illustrates how to use\n# the Levenstein distance (edit distance)\n# in TensorFlow.\n\nimport tensorflow as tf\n\nsess = tf.Session()\n\n#----------------------------------\n# First compute the edit distance between 'bear' and 'beers'\nhypothesis = list('bear')\ntruth = list('beers')\nh1 = tf.SparseTensor([[0,0,0], [0,0,1], [0,0,2], [0,0,3]],\n                     hypothesis,\n                     [1,1,1])\n\nt1 = tf.SparseTensor([[0,0,0], [0,0,1], [0,0,1], [0,0,3],[0,0,4]],\n                     truth,\n                     [1,1,1])\n\nprint(sess.run(tf.edit_distance(h1, t1, normalize=False)))\n\n#----------------------------------\n# Compute the edit distance between ('bear','beer') and 'beers':\nhypothesis2 = list('bearbeer')\ntruth2 = list('beersbeers')\nh2 = tf.SparseTensor([[0,0,0], [0,0,1], [0,0,2], [0,0,3], [0,1,0], [0,1,1], [0,1,2], [0,1,3]],\n                     hypothesis2,\n                     [1,2,4])\n\nt2 = tf.SparseTensor([[0,0,0], [0,0,1], [0,0,2], [0,0,3], [0,0,4], [0,1,0], [0,1,1], [0,1,2], [0,1,3], [0,1,4]],\n                     truth2,\n                     [1,2,5])\n\nprint(sess.run(tf.edit_distance(h2, t2, normalize=True)))\n\n#----------------------------------\n# Now compute distance between four words and 'beers' more efficiently with sparse tensors:\nhypothesis_words = ['bear','bar','tensor','flow']\ntruth_word = ['beers']\n\nnum_h_words = len(hypothesis_words)\nh_indices = [[xi, 0, yi] for xi,x in enumerate(hypothesis_words) for yi,y in enumerate(x)]\nh_chars = list(''.join(hypothesis_words))\n\nh3 = tf.SparseTensor(h_indices, h_chars, [num_h_words,1,1])\n\ntruth_word_vec = truth_word*num_h_words\nt_indices = [[xi, 0, yi] for xi,x in enumerate(truth_word_vec) for yi,y in enumerate(x)]\nt_chars = list(''.join(truth_word_vec))\n\nt3 = tf.SparseTensor(t_indices, t_chars, [num_h_words,1,1])\n\nprint(sess.run(tf.edit_distance(h3, t3, normalize=True)))\n"""
05_Nearest_Neighbor_Methods/04_Computing_with_Mixed_Distance_Functions/04_mixed_distance_functions_knn.py,17,"b""# Mixed Distance Functions for  k-Nearest Neighbor\n#----------------------------------\n#\n# This function shows how to use different distance\n# metrics on different features for kNN.\n#\n# Data:\n#----------x-values-----------\n# CRIM   : per capita crime rate by town\n# ZN     : prop. of res. land zones\n# INDUS  : prop. of non-retail business acres\n# CHAS   : Charles river dummy variable\n# NOX    : nitrix oxides concentration / 10 M\n# RM     : Avg. # of rooms per building\n# AGE    : prop. of buildings built prior to 1940\n# DIS    : Weighted distances to employment centers\n# RAD    : Index of radian highway access\n# TAX    : Full tax rate value per $10k\n# PTRATIO: Pupil/Teacher ratio by town\n# B      : 1000*(Bk-0.63)^2, Bk=prop. of blacks\n# LSTAT  : % lower status of pop\n#------------y-value-----------\n# MEDV   : Median Value of homes in $1,000's\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport requests\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\nhousing_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\nhousing_header = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ncols_used = ['CRIM', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nnum_features = len(cols_used)\nhousing_file = requests.get(housing_url)\nhousing_data = [[float(x) for x in y.split(' ') if len(x)>=1] for y in housing_file.text.split('\\n') if len(y)>=1]\n\ny_vals = np.transpose([np.array([y[13] for y in housing_data])])\nx_vals = np.array([[x for i,x in enumerate(y) if housing_header[i] in cols_used] for y in housing_data])\n\n## Min-Max Scaling\nx_vals = (x_vals - x_vals.min(0)) / x_vals.ptp(0)\n\n## Create distance metric weight matrix weighted by standard deviation\nweight_diagonal = x_vals.std(0)\nweight_matrix = tf.cast(tf.diag(weight_diagonal), dtype=tf.float32)\n\n# Split the data into train and test sets\nnp.random.seed(13)   # reproducible results\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n# Declare k-value and batch size\nk = 4\nbatch_size=len(x_vals_test)\n\n# Placeholders\nx_data_train = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\nx_data_test = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\ny_target_train = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target_test = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Declare weighted distance metric\n# Weighted L2 = sqrt((x-y)^T * A * (x-y))\nsubtraction_term =  tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))\nfirst_product = tf.matmul(subtraction_term, tf.tile(tf.expand_dims(weight_matrix,0), [batch_size,1,1]))\nsecond_product = tf.matmul(first_product, tf.transpose(subtraction_term, perm=[0,2,1]))\ndistance = tf.sqrt(tf.matrix_diag_part(second_product))\n\n# Predict: Get min distance index (Nearest neighbor)\ntop_k_xvals, top_k_indices = tf.nn.top_k(tf.negative(distance), k=k)\nx_sums = tf.expand_dims(tf.reduce_sum(top_k_xvals, 1),1)\nx_sums_repeated = tf.matmul(x_sums,tf.ones([1, k], tf.float32))\nx_val_weights = tf.expand_dims(tf.div(top_k_xvals,x_sums_repeated), 1)\n\ntop_k_yvals = tf.gather(y_target_train, top_k_indices)\nprediction = tf.squeeze(tf.matmul(x_val_weights,top_k_yvals), axis=[1])\n\n# Calculate MSE\nmse = tf.div(tf.reduce_sum(tf.square(tf.subtract(prediction, y_target_test))), batch_size)\n\n# Calculate how many loops over training data\nnum_loops = int(np.ceil(len(x_vals_test)/batch_size))\n\nfor i in range(num_loops):\n    min_index = i*batch_size\n    max_index = min((i+1)*batch_size,len(x_vals_train))\n    x_batch = x_vals_test[min_index:max_index]\n    y_batch = y_vals_test[min_index:max_index]\n    predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n                                         y_target_train: y_vals_train, y_target_test: y_batch})\n    batch_mse = sess.run(mse, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n                                         y_target_train: y_vals_train, y_target_test: y_batch})\n\n    print('Batch #' + str(i+1) + ' MSE: ' + str(np.round(batch_mse,3)))\n\n# Plot prediction and actual distribution\nbins = np.linspace(5, 50, 45)\n\nplt.hist(predictions, bins, alpha=0.5, label='Prediction')\nplt.hist(y_batch, bins, alpha=0.5, label='Actual')\nplt.title('Histogram of Predicted and Actual Values')\nplt.xlabel('Med Home Value in $1,000s')\nplt.ylabel('Frequency')\nplt.legend(loc='upper right')\nplt.show()"""
05_Nearest_Neighbor_Methods/05_An_Address_Matching_Example/05_address_matching.py,14,"b""# Address Matching with k-Nearest Neighbors\n#----------------------------------\n#\n# This function illustrates a way to perform\n# address matching between two data sets.\n#\n# For each test address, we will return the\n# closest reference address to it.\n#\n# We will consider two distance functions:\n# 1) Edit distance for street number/name and\n# 2) Euclidian distance (L2) for the zip codes\n\nimport random\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# First we generate the data sets we will need\n# n = Size of created data sets\nn = 10\nstreet_names = ['abbey', 'baker', 'canal', 'donner', 'elm']\nstreet_types = ['rd', 'st', 'ln', 'pass', 'ave']\n\nrandom.seed(31)  # make results reproducible\nrand_zips = [random.randint(65000, 65999) for i in range(5)]\n\n\n# Function to randomly create one typo in a string w/ a probability\ndef create_typo(s, prob=0.75):\n    if random.uniform(0, 1) < prob:\n        rand_ind = random.choice(range(len(s)))\n        s_list = list(s)\n        s_list[rand_ind] = random.choice(string.ascii_lowercase)\n        s = ''.join(s_list)\n    return s\n\n# Generate the reference dataset\nnumbers = [random.randint(1, 9999) for _ in range(n)]\nstreets = [random.choice(street_names) for _ in range(n)]\nstreet_suffs = [random.choice(street_types) for _ in range(n)]\nzips = [random.choice(rand_zips) for _ in range(n)]\nfull_streets = [str(x) + ' ' + y + ' ' + z for x, y, z in zip(numbers, streets, street_suffs)]\nreference_data = [list(x) for x in zip(full_streets,zips)]\n\n# Generate test dataset with some typos\ntypo_streets = [create_typo(x) for x in streets]\ntypo_full_streets = [str(x) + ' ' + y + ' ' + z for x, y, z in zip(numbers, typo_streets, street_suffs)]\ntest_data = [list(x) for x in zip(typo_full_streets, zips)]\n\n# Now we can perform address matching\n# Create graph\nsess = tf.Session()\n\n# Placeholders\ntest_address = tf.sparse_placeholder(dtype=tf.string)\ntest_zip = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nref_address = tf.sparse_placeholder(dtype=tf.string)\nref_zip = tf.placeholder(shape=[None, n], dtype=tf.float32)\n\n# Declare Zip code distance for a test zip and reference set\nzip_dist = tf.square(tf.subtract(ref_zip, test_zip))\n\n# Declare Edit distance for address\naddress_dist = tf.edit_distance(test_address, ref_address, normalize=True)\n\n# Create similarity scores\nzip_max = tf.gather(tf.squeeze(zip_dist), tf.argmax(zip_dist, 1))\nzip_min = tf.gather(tf.squeeze(zip_dist), tf.argmin(zip_dist, 1))\nzip_sim = tf.div(tf.subtract(zip_max, zip_dist), tf.subtract(zip_max, zip_min))\naddress_sim = tf.subtract(1., address_dist)\n\n# Combine distance functions\naddress_weight = 0.5\nzip_weight = 1. - address_weight\nweighted_sim = tf.add(tf.transpose(tf.multiply(address_weight, address_sim)), tf.multiply(zip_weight, zip_sim))\n\n# Predict: Get max similarity entry\ntop_match_index = tf.argmax(weighted_sim, 1)\n\n\n# Function to Create a character-sparse tensor from strings\ndef sparse_from_word_vec(word_vec):\n    num_words = len(word_vec)\n    indices = [[xi, 0, yi] for xi,x in enumerate(word_vec) for yi,y in enumerate(x)]\n    chars = list(''.join(word_vec))\n    return tf.SparseTensorValue(indices, chars, [num_words,1,1])\n\n# Loop through test indices\nreference_addresses = [x[0] for x in reference_data]\nreference_zips = np.array([[x[1] for x in reference_data]])\n\n# Create sparse address reference set\nsparse_ref_set = sparse_from_word_vec(reference_addresses)\n\nfor i in range(n):\n    test_address_entry = test_data[i][0]\n    test_zip_entry = [[test_data[i][1]]]\n    \n    # Create sparse address vectors\n    test_address_repeated = [test_address_entry] * n\n    sparse_test_set = sparse_from_word_vec(test_address_repeated)\n    \n    feeddict = {test_address: sparse_test_set,\n                test_zip: test_zip_entry,\n                ref_address: sparse_ref_set,\n                ref_zip: reference_zips}\n    best_match = sess.run(top_match_index, feed_dict=feeddict)\n    best_street = reference_addresses[best_match[0]]\n    [best_zip] = reference_zips[0][best_match]\n    [[test_zip_]] = test_zip_entry\n    print('Address: ' + str(test_address_entry) + ', ' + str(test_zip_))\n    print('Match  : ' + str(best_street) + ', ' + str(best_zip))\n"""
05_Nearest_Neighbor_Methods/06_Nearest_Neighbors_for_Image_Recognition/06_image_recognition.py,11,"b'# MNIST Digit Prediction with k-Nearest Neighbors\n#-----------------------------------------------\n#\n# This script will load the MNIST data, and split\n# it into test/train and perform prediction with\n# nearest neighbors\n#\n# For each test integer, we will return the\n# closest image/integer.\n#\n# Integer images are represented as 28x8 matrices\n# of floating point numbers\n\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Create graph\nsess = tf.Session()\n\n# Load the data\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\n# Random sample\nnp.random.seed(13)  # set seed for reproducibility\ntrain_size = 1000\ntest_size = 102\nrand_train_indices = np.random.choice(len(mnist.train.images), train_size, replace=False)\nrand_test_indices = np.random.choice(len(mnist.test.images), test_size, replace=False)\nx_vals_train = mnist.train.images[rand_train_indices]\nx_vals_test = mnist.test.images[rand_test_indices]\ny_vals_train = mnist.train.labels[rand_train_indices]\ny_vals_test = mnist.test.labels[rand_test_indices]\n\n# Declare k-value and batch size\nk = 4\nbatch_size=6\n\n# Placeholders\nx_data_train = tf.placeholder(shape=[None, 784], dtype=tf.float32)\nx_data_test = tf.placeholder(shape=[None, 784], dtype=tf.float32)\ny_target_train = tf.placeholder(shape=[None, 10], dtype=tf.float32)\ny_target_test = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n\n# Declare distance metric\n# L1\ndistance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), axis=2)\n\n# L2\n#distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), reduction_indices=1))\n\n# Predict: Get min distance index (Nearest neighbor)\ntop_k_xvals, top_k_indices = tf.nn.top_k(tf.negative(distance), k=k)\nprediction_indices = tf.gather(y_target_train, top_k_indices)\n# Predict the mode category\ncount_of_predictions = tf.reduce_sum(prediction_indices, axis=1)\nprediction = tf.argmax(count_of_predictions)\n\n# Calculate how many loops over training data\nnum_loops = int(np.ceil(len(x_vals_test)/batch_size))\n\ntest_output = []\nactual_vals = []\nfor i in range(num_loops):\n    min_index = i*batch_size\n    max_index = min((i+1)*batch_size,len(x_vals_train))\n    x_batch = x_vals_test[min_index:max_index]\n    y_batch = y_vals_test[min_index:max_index]\n    predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n                                         y_target_train: y_vals_train, y_target_test: y_batch})\n    test_output.extend(predictions)\n    actual_vals.extend(np.argmax(y_batch, axis=1))\n\naccuracy = sum([1./test_size for i in range(test_size) if test_output[i]==actual_vals[i]])\nprint(\'Accuracy on test set: \' + str(accuracy))\n\n# Plot the last batch results:\nactuals = np.argmax(y_batch, axis=1)\n\nNrows = 2\nNcols = 3\nfor i in range(len(actuals)):\n    plt.subplot(Nrows, Ncols, i+1)\n    plt.imshow(np.reshape(x_batch[i], [28,28]), cmap=\'Greys_r\')\n    plt.title(\'Actual: \' + str(actuals[i]) + \' Pred: \' + str(predictions[i]),\n                               fontsize=10)\n    frame = plt.gca()\n    frame.axes.get_xaxis().set_visible(False)\n    frame.axes.get_yaxis().set_visible(False)\n    \nplt.show()\n'"
06_Neural_Networks/02_Implementing_an_Operational_Gate/02_gates.py,15,"b""# Implementing Gates\n#----------------------------------\n#\n# This function shows how to implement\n# various gates in TensorFlow\n#\n# One gate will be one operation with\n# a variable and a placeholder.\n# We will ask TensorFlow to change the\n# variable based on our loss function\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start Graph Session\nsess = tf.Session()\n\n#----------------------------------\n# Create a multiplication gate:\n#   f(x) = a * x\n#\n#  a --\n#      |\n#      |---- (multiply) --> output\n#  x --|\n#\n\na = tf.Variable(tf.constant(4.))\nx_val = 5.\nx_data = tf.placeholder(dtype=tf.float32)\n\nmultiplication = tf.multiply(a, x_data)\n\n# Declare the loss function as the difference between\n# the output and a target value, 50.\nloss = tf.square(tf.subtract(multiplication, 50.))\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_opt.minimize(loss)\n\n# Run loop across gate\nprint('Optimizing a Multiplication Gate Output to 50.')\nfor _ in range(10):\n    sess.run(train_step, feed_dict={x_data: x_val})\n    a_val = sess.run(a)\n    mult_output = sess.run(multiplication, feed_dict={x_data: x_val})\n    print(str(a_val) + ' * ' + str(x_val) + ' = ' + str(mult_output))\n    \n'''\nCreate a nested gate:\n   f(x) = a * x + b\n\n  a --\n      |\n      |-- (multiply)--\n  x --|              |\n                     |-- (add) --> output\n                 b --|\n\n'''\n\n# Start a New Graph Session\nops.reset_default_graph()\nsess = tf.Session()\n\na = tf.Variable(tf.constant(1.))\nb = tf.Variable(tf.constant(1.))\nx_val = 5.\nx_data = tf.placeholder(dtype=tf.float32)\n\ntwo_gate = tf.add(tf.multiply(a, x_data), b)\n\n# Declare the loss function as the difference between\n# the output and a target value, 50.\nloss = tf.square(tf.subtract(two_gate, 50.))\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_opt.minimize(loss)\n\n# Run loop across gate\nprint('\\nOptimizing Two Gate Output to 50.')\nfor _ in range(10):\n    sess.run(train_step, feed_dict={x_data: x_val})\n    a_val, b_val = (sess.run(a), sess.run(b))\n    two_gate_output = sess.run(two_gate, feed_dict={x_data: x_val})\n    print(str(a_val) + ' * ' + str(x_val) + ' + ' + str(b_val) + ' = ' + str(two_gate_output))"""
06_Neural_Networks/03_Working_with_Activation_Functions/03_activation_functions.py,13,"b'""""""\nCombining Gates and Activation Functions\n\nThis function shows how to implement\nvarious gates with activation functions\nin TensorFlow\n\nThis function is an extension of the\nprior gates, but with various activation\nfunctions.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start Graph Session\nsess = tf.Session()\ntf.set_random_seed(5)\nnp.random.seed(42)\n\nbatch_size = 50\n\na1 = tf.Variable(tf.random_normal(shape=[1, 1]))\nb1 = tf.Variable(tf.random_uniform(shape=[1, 1]))\na2 = tf.Variable(tf.random_normal(shape=[1, 1]))\nb2 = tf.Variable(tf.random_uniform(shape=[1, 1]))\nx = np.random.normal(2, 0.1, 500)\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\nsigmoid_activation = tf.sigmoid(tf.add(tf.matmul(x_data, a1), b1))\n\nrelu_activation = tf.nn.relu(tf.add(tf.matmul(x_data, a2), b2))\n\n# Declare the loss function as the difference between\n# the output and a target value, 0.75.\nloss1 = tf.reduce_mean(tf.square(tf.subtract(sigmoid_activation, 0.75)))\nloss2 = tf.reduce_mean(tf.square(tf.subtract(relu_activation, 0.75)))\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.01)\ntrain_step_sigmoid = my_opt.minimize(loss1)\ntrain_step_relu = my_opt.minimize(loss2)\n\n# Run loop across gate\nprint(\'\\nOptimizing Sigmoid AND Relu Output to 0.75\')\nloss_vec_sigmoid = []\nloss_vec_relu = []\nfor i in range(500):\n    rand_indices = np.random.choice(len(x), size=batch_size)\n    x_vals = np.transpose([x[rand_indices]])\n    sess.run(train_step_sigmoid, feed_dict={x_data: x_vals})\n    sess.run(train_step_relu, feed_dict={x_data: x_vals})\n    \n    loss_vec_sigmoid.append(sess.run(loss1, feed_dict={x_data: x_vals}))\n    loss_vec_relu.append(sess.run(loss2, feed_dict={x_data: x_vals}))    \n    \n    sigmoid_output = np.mean(sess.run(sigmoid_activation, feed_dict={x_data: x_vals}))\n    relu_output = np.mean(sess.run(relu_activation, feed_dict={x_data: x_vals}))\n    \n    if i % 50 == 0:\n        print(\'sigmoid = \' + str(np.mean(sigmoid_output)) + \' relu = \' + str(np.mean(relu_output)))\n\n# Plot the loss\nplt.plot(loss_vec_sigmoid, \'k-\', label=\'Sigmoid Activation\')\nplt.plot(loss_vec_relu, \'r--\', label=\'Relu Activation\')\nplt.ylim([0, 1.0])\nplt.title(\'Loss per Generation\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Loss\')\nplt.legend(loc=\'upper right\')\nplt.show()\n'"
06_Neural_Networks/04_Single_Hidden_Layer_Network/04_single_hidden_layer_network.py,13,"b'""""""\nImplementing a one-layer Neural Network\n\nWe will illustrate how to create a one hidden layer NN\n\nWe will use the iris data for this exercise\n\nWe will build a one-hidden layer neural network\n to predict the fourth attribute, Petal Width from\n the other three (Sepal length, Sepal width, Petal length).\n""""""\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\niris = datasets.load_iris()\nx_vals = np.array([x[0:3] for x in iris.data])\ny_vals = np.array([x[3] for x in iris.data])\n\n# Create graph session \nsess = tf.Session()\n\n# make results reproducible\nseed = 2\ntf.set_random_seed(seed)\nnp.random.seed(seed)  \n\n# Split data into train/test = 80%/20%\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n\n# Normalize by column (min-max norm)\ndef normalize_cols(m):\n    col_max = m.max(axis=0)\n    col_min = m.min(axis=0)\n    return (m-col_min) / (col_max - col_min)\n    \nx_vals_train = np.nan_to_num(normalize_cols(x_vals_train))\nx_vals_test = np.nan_to_num(normalize_cols(x_vals_test))\n\n# Declare batch size\nbatch_size = 50\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 3], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Create variables for both NN layers\nhidden_layer_nodes = 10\nA1 = tf.Variable(tf.random_normal(shape=[3, hidden_layer_nodes]))  # inputs -> hidden nodes\nb1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))  # one biases for each hidden node\nA2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes, 1]))  # hidden inputs -> 1 output\nb2 = tf.Variable(tf.random_normal(shape=[1]))   # 1 bias for the output\n\n# Declare model operations\nhidden_output = tf.nn.relu(tf.add(tf.matmul(x_data, A1), b1))\nfinal_output = tf.nn.relu(tf.add(tf.matmul(hidden_output, A2), b2))\n\n# Declare loss function (MSE)\nloss = tf.reduce_mean(tf.square(y_target - final_output))\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.005)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\ntest_loss = []\nfor i in range(500):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = x_vals_train[rand_index]\n    rand_y = np.transpose([y_vals_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(np.sqrt(temp_loss))\n    \n    test_temp_loss = sess.run(loss, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n    test_loss.append(np.sqrt(test_temp_loss))\n    if (i + 1) % 50 == 0:\n        print(\'Generation: \' + str(i+1) + \'. Loss = \' + str(temp_loss))\n\n# Plot loss (MSE) over time\nplt.plot(loss_vec, \'k-\', label=\'Train Loss\')\nplt.plot(test_loss, \'r--\', label=\'Test Loss\')\nplt.title(\'Loss (MSE) per Generation\')\nplt.legend(loc=\'upper right\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Loss\')\nplt.show()\n'"
06_Neural_Networks/05_Implementing_Different_Layers/05_implementing_different_layers.py,43,"b'""""""\nImplementing Different Layers\n\nWe will illustrate how to use different types of layers in TensorFlow\n\nThe layers of interest are:\n (1) Convolutional Layer\n (2) Activation Layer\n (3) Max-Pool Layer\n (4) Fully Connected Layer\n\nWe will generate two different data sets for this\n script, a 1-D data set (row of data) and\n a 2-D data set (similar to picture)\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# ---------------------------------------------------|\n# -------------------1D-data-------------------------|\n# ---------------------------------------------------|\n\n# Create graph session\nsess = tf.Session()\n\n# parameters for the run\ndata_size = 25\nconv_size = 5\nmaxpool_size = 5\nstride_size = 1\n\n# ensure reproducibility\nseed = 13\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Generate 1D data\ndata_1d = np.random.normal(size=data_size)\n\n# Placeholder\nx_input_1d = tf.placeholder(dtype=tf.float32, shape=[data_size])\n\n\n# --------Convolution--------\ndef conv_layer_1d(input_1d, input_filter, stride):\n    """"""\n    TensorFlow\'s \'conv2d()\' function only works with 4D arrays:\n    [batch#, width, height, channels], we have 1 batch, and\n    width = 1, but height = the length of the input, and 1 channel.\n    So next we create the 4D array by inserting dimension 1\'s.\n    :param input_1d: 1D input array.\n    :param input_filter: Filter to convolve across the input_1d array.\n    :param stride: stride for filter.\n    :return: array.\n    """"""\n    input_2d = tf.expand_dims(input_1d, 0)\n    input_3d = tf.expand_dims(input_2d, 0)\n    input_4d = tf.expand_dims(input_3d, 3)\n    # Perform convolution with stride = 1, if we wanted to increase the stride,\n    # to say \'2\', then strides=[1,1,2,1]\n    convolution_output = tf.nn.conv2d(input_4d,\n                                      filter=input_filter,\n                                      strides=[1, 1, stride, 1],\n                                      padding=""VALID"")\n    # Get rid of extra dimensions\n    conv_output_1d = tf.squeeze(convolution_output)\n    return conv_output_1d\n\n# Create filter for convolution.\nmy_filter = tf.Variable(tf.random_normal(shape=[1, conv_size, 1, 1]))\n# Create convolution layer\nmy_convolution_output = conv_layer_1d(x_input_1d, my_filter, stride=stride_size)\n\n\n# --------Activation--------\ndef activation(input_1d):\n    return tf.nn.relu(input_1d)\n\n# Create activation layer\nmy_activation_output = activation(my_convolution_output)\n\n\n# --------Max Pool--------\ndef max_pool(input_1d, width, stride):\n    """"""\n    Just like \'conv2d()\' above, max_pool() works with 4D arrays.\n    [batch_size=1, width=1, height=num_input, channels=1]\n    :param input_1d: Input array to perform max-pool on.\n    :param width: Width of 1d-window for max-pool\n    :param stride: Stride of window across input array\n    :return: max-pooled array\n    """"""\n    input_2d = tf.expand_dims(input_1d, 0)\n    input_3d = tf.expand_dims(input_2d, 0)\n    input_4d = tf.expand_dims(input_3d, 3)\n    # Perform the max pooling with strides = [1,1,1,1]\n    # If we wanted to increase the stride on our data dimension, say by\n    # a factor of \'2\', we put strides = [1, 1, 2, 1]\n    # We will also need to specify the width of the max-window (\'width\')\n    pool_output = tf.nn.max_pool(input_4d, ksize=[1, 1, width, 1],\n                                 strides=[1, 1, stride, 1],\n                                 padding=\'VALID\')\n    # Get rid of extra dimensions\n    pool_output_1d = tf.squeeze(pool_output)\n    return pool_output_1d\n\nmy_maxpool_output = max_pool(my_activation_output, width=maxpool_size, stride=stride_size)\n\n\n# --------Fully Connected--------\ndef fully_connected(input_layer, num_outputs):\n    # First we find the needed shape of the multiplication weight matrix:\n    # The dimension will be (length of input) by (num_outputs)\n    weight_shape = tf.squeeze(tf.stack([tf.shape(input_layer), [num_outputs]]))\n    # Initialize such weight\n    weight = tf.random_normal(weight_shape, stddev=0.1)\n    # Initialize the bias\n    bias = tf.random_normal(shape=[num_outputs])\n    # Make the 1D input array into a 2D array for matrix multiplication\n    input_layer_2d = tf.expand_dims(input_layer, 0)\n    # Perform the matrix multiplication and add the bias\n    full_output = tf.add(tf.matmul(input_layer_2d, weight), bias)\n    # Get rid of extra dimensions\n    full_output_1d = tf.squeeze(full_output)\n    return full_output_1d\n\nmy_full_output = fully_connected(my_maxpool_output, 5)\n\n# Initialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfeed_dict = {x_input_1d: data_1d}\n\nprint(\'>>>> 1D Data <<<<\')\n\n# Convolution Output\nprint(\'Input = array of length {}\'.format(x_input_1d.shape.as_list()[0]))\nprint(\'Convolution w/ filter, length = {}, stride size = {},\'\n      \'results in an array of length {}:\'.format(conv_size,\n                                                 stride_size,\n                                                 my_convolution_output.shape.as_list()[0]))\nprint(sess.run(my_convolution_output, feed_dict=feed_dict))\n\n# Activation Output\nprint(\'\\nInput = above array of length {}\'.format(my_convolution_output.shape.as_list()[0]))\nprint(\'ReLU element wise returns \'\n      \'an array of length {}:\'.format(my_activation_output.shape.as_list()[0]))\nprint(sess.run(my_activation_output, feed_dict=feed_dict))\n\n# Max Pool Output\nprint(\'\\nInput = above array of length {}\'.format(my_activation_output.shape.as_list()[0]))\nprint(\'MaxPool, window length = {}, stride size = {},\'\n      \'results in the array of length {}\'.format(maxpool_size,\n                                                 stride_size,\n                                                 my_maxpool_output.shape.as_list()[0]))\nprint(sess.run(my_maxpool_output, feed_dict=feed_dict))\n\n# Fully Connected Output\nprint(\'\\nInput = above array of length {}\'.format(my_maxpool_output.shape.as_list()[0]))\nprint(\'Fully connected layer on all 4 rows \'\n      \'with {} outputs:\'.format(my_full_output.shape.as_list()[0]))\nprint(sess.run(my_full_output, feed_dict=feed_dict))\n\n# ---------------------------------------------------|\n# -------------------2D-data-------------------------|\n# ---------------------------------------------------|\n\n# Reset Graph\nops.reset_default_graph()\nsess = tf.Session()\n\n# Parameters for the run\nrow_size = 10\ncol_size = 10\nconv_size = 2\nconv_stride_size = 2\nmaxpool_size = 2\nmaxpool_stride_size = 1\n\n# Set seed to ensure reproducibility\nseed = 13\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Generate 2D data\ndata_size = [row_size, col_size]\ndata_2d = np.random.normal(size=data_size)\n\n# --------Placeholder--------\nx_input_2d = tf.placeholder(dtype=tf.float32, shape=data_size)\n\n\n# Convolution\ndef conv_layer_2d(input_2d, conv_filter, conv_stride):\n    """"""\n    TensorFlow\'s \'conv2d()\' function only works with 4D arrays:\n    [batch#, width, height, channels], we have 1 batch, and\n    1 channel, but we do have width AND height this time.\n    So next we create the 4D array by inserting dimension 1\'s.\n    :param input_2d: input array for 2D convolution.\n    :param conv_filter: 2D-filter.\n    :param conv_stride: 2D stride settings.\n    :return: Convoluted array.\n    """"""\n    input_3d = tf.expand_dims(input_2d, 0)\n    input_4d = tf.expand_dims(input_3d, 3)\n    # Note the stride difference below!\n    convolution_output = tf.nn.conv2d(input_4d,\n                                      filter=conv_filter,\n                                      strides=[1, conv_stride, conv_stride, 1],\n                                      padding=""VALID"")\n    # Get rid of unnecessary dimensions\n    conv_output_2d = tf.squeeze(convolution_output)\n    return conv_output_2d\n\n# Create Convolutional Filter\nmy_filter = tf.Variable(tf.random_normal(shape=[conv_size, conv_size, 1, 1]))\n# Create Convolutional Layer\nmy_convolution_output = conv_layer_2d(x_input_2d, my_filter, conv_stride=conv_stride_size)\n\n\n# --------Activation--------\ndef activation(input_1d):\n    return tf.nn.relu(input_1d)\n\n# Create Activation Layer\nmy_activation_output = activation(my_convolution_output)\n\n\n# --------Max Pool--------\ndef max_pool(input_2d, width, height, stride):\n    """"""\n    Just like \'conv2d()\' above, max_pool() works with 4D arrays.\n    [batch_size=1, width=given, height=given, channels=1]\n    :param input_2d: 2D input array\n    :param width: width of 2D max pool window\n    :param height: height of 2D max pool window\n    :param stride: 2d stride setting\n    :return: max-pool\'ed array\n    """"""\n    input_3d = tf.expand_dims(input_2d, 0)\n    input_4d = tf.expand_dims(input_3d, 3)\n    # Perform the max pooling with strides = [1,1,1,1]\n    # If we wanted to increase the stride on our data dimension, say by\n    # a factor of \'2\', we put strides = [1, 2, 2, 1]\n    pool_output = tf.nn.max_pool(input_4d, ksize=[1, height, width, 1],\n                                 strides=[1, stride, stride, 1],\n                                 padding=\'VALID\')\n    # Get rid of unnecessary dimensions\n    pool_output_2d = tf.squeeze(pool_output)\n    return pool_output_2d\n\n# Create Max-Pool Layer\nmy_maxpool_output = max_pool(my_activation_output, \n                             width=maxpool_size,\n                             height=maxpool_size,\n                             stride=maxpool_stride_size)\n\n\n# -------Fully Connected--------\ndef fully_connected(input_layer, num_outputs):\n    """"""\n    In order to connect our whole W byH 2d array, we first flatten it out to\n    a W times H 1D array.\n    :param input_layer: input array for fully connected layer.\n    :param num_outputs: how many outputs to give from layer.\n    :return: array of size num_outputs\n    """"""\n    flat_input = tf.reshape(input_layer, [-1])\n    # We then find out how long it is, and create an array for the shape of\n    # the multiplication weight = (WxH) by (num_outputs)\n    weight_shape = tf.squeeze(tf.stack([tf.shape(flat_input), [num_outputs]]))\n    # Initialize the weight\n    weight = tf.random_normal(weight_shape, stddev=0.1)\n    # Initialize the bias\n    bias = tf.random_normal(shape=[num_outputs])\n    # Now make the flat 1D array into a 2D array for multiplication\n    input_2d = tf.expand_dims(flat_input, 0)\n    # Multiply and add the bias\n    full_output = tf.add(tf.matmul(input_2d, weight), bias)\n    # Get rid of extra dimension\n    full_output_2d = tf.squeeze(full_output)\n    return full_output_2d\n\n# Create Fully Connected Layer\nmy_full_output = fully_connected(my_maxpool_output, 5)\n\n# Run graph\n# Initialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfeed_dict = {x_input_2d: data_2d}\n\nprint(\'\\n>>>> 2D Data <<<<\')\n\n# Convolution Output\nprint(\'Input = {} array\'.format(x_input_2d.shape.as_list()))\nprint(\'{} Convolution, stride size = [{}, {}], \'\n      \'results in the {} array\'.format(my_filter.get_shape().as_list()[:2],\n                                       conv_stride_size,\n                                       conv_stride_size,\n                                       my_convolution_output.shape.as_list()))\nprint(sess.run(my_convolution_output, feed_dict=feed_dict))\n\n# Activation Output\nprint(\'\\nInput = the above {} array\'.format(my_convolution_output.shape.as_list()))\nprint(\'ReLU element wise returns the {} array\'.format(my_activation_output.shape.as_list()))\nprint(sess.run(my_activation_output, feed_dict=feed_dict))\n\n# Max Pool Output\nprint(\'\\nInput = the above {} array\'.format(my_activation_output.shape.as_list()))\nprint(\'MaxPool, stride size = [{}, {}], \'\n      \'results in {} array\'.format(maxpool_stride_size,\n                                   maxpool_stride_size,\n                                   my_maxpool_output.shape.as_list()))\nprint(sess.run(my_maxpool_output, feed_dict=feed_dict))\n\n# Fully Connected Output\nprint(\'\\nInput = the above {} array\'.format(my_maxpool_output.shape.as_list()))\nprint(\'Fully connected layer on all {} rows \'\n      \'results in {} outputs:\'.format(my_maxpool_output.shape.as_list()[0],\n                                      my_full_output.shape.as_list()[0]))\nprint(sess.run(my_full_output, feed_dict=feed_dict))\n'"
06_Neural_Networks/06_Using_Multiple_Layers/06_using_a_multiple_layer_network.py,11,"b'""""""\nUsing a Multiple Layer Network\n------------------------------\nWe will illustrate how to use a Multiple\nLayer Network in TensorFlow\n\nLow Birthrate data:\n\nColumns    Variable                                Abbreviation\n----------------------------------------------------------------\nLow Birth Weight (0 = Birth Weight >= 2500g,            LOW\n                  1 = Birth Weight < 2500g)\nAge of the Mother in Years                              AGE\nWeight in Pounds at the Last Menstrual Period           LWT\nRace (1 = White, 2 = Black, 3 = Other)                  RACE\nSmoking Status During Pregnancy (1 = Yes, 0 = No)       SMOKE\nHistory of Premature Labor (0 = None  1 = One, etc.)    PTL\nHistory of Hypertension (1 = Yes, 0 = No)               HT\nPresence of Uterine Irritability (1 = Yes, 0 = No)      UI\nBirth Weight in Grams                                   BWT\n-----------------------------------------------------------------\n\nThe multiple neural network layer we will create will be composed of\nthree fully connected hidden layers, with node sizes 50, 25, and 5\n\n""""""\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport csv\nimport os\nimport numpy as np\nimport requests\nfrom tensorflow.python.framework import ops\n\n# name of data file\nbirth_weight_file = \'birth_weight.csv\'\nbirthdata_url = \'https://github.com/nfmcclure/tensorflow_cookbook/raw/master\' \\\n                \'/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat\'\n\n# Download data and create data file if file does not exist in current directory\nif not os.path.exists(birth_weight_file):\n    birth_file = requests.get(birthdata_url)\n    birth_data = birth_file.text.split(\'\\r\\n\')\n    birth_header = birth_data[0].split(\'\\t\')\n    birth_data = [[float(x) for x in y.split(\'\\t\') if len(x) >= 1]\n                  for y in birth_data[1:] if len(y) >= 1]\n    with open(birth_weight_file, ""w"") as f:\n        writer = csv.writer(f)\n        writer.writerows([birth_header])\n        writer.writerows(birth_data)\n\n# read birth weight data into memory\nbirth_data = []\nwith open(birth_weight_file, newline=\'\') as csvfile:\n    csv_reader = csv.reader(csvfile)\n    birth_header = next(csv_reader)\n    for row in csv_reader:\n        birth_data.append(row)\n\nbirth_data = [[float(x) for x in row] for row in birth_data]\n\n# Extract y-target (birth weight)\ny_vals = np.array([x[8] for x in birth_data])\n\n# Filter for features of interest\ncols_of_interest = [\'AGE\', \'LWT\', \'RACE\', \'SMOKE\', \'PTL\', \'HT\', \'UI\']\nx_vals = np.array([[x[ix] for ix, feature in enumerate(birth_header) if feature in cols_of_interest]\n                   for x in birth_data])\n\n# Reset the graph for new run\nops.reset_default_graph()\n\n# Create graph session \nsess = tf.Session()\n\n# set batch size for training\nbatch_size = 100\n\n# Set random seed to make results reproducible\nseed = 4\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Split data into train/test = 80%/20%\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n# Normalize by column (min-max norm to be between 0 and 1)\ndef normalize_cols(m, col_min=np.array([None]), col_max=np.array([None])):\n    if not col_min[0]:\n        col_min = m.min(axis=0)\n    if not col_max[0]:\n        col_max = m.max(axis=0)\n    return (m - col_min) / (col_max - col_min), col_min, col_max\n\n\nx_vals_train, train_min, train_max = np.nan_to_num(normalize_cols(x_vals_train))\nx_vals_test, _, _ = np.nan_to_num(normalize_cols(x_vals_test, train_min, train_max))\n\nx_vals_train = np.nan_to_num(normalize_cols(x_vals_train, train_max, train_min))\nx_vals_test = np.nan_to_num(normalize_cols(x_vals_test, train_max, train_min))\n\n\n# Define Variable Functions (weights and bias)\ndef init_weight(shape, st_dev):\n    weight = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n    return weight\n\n\ndef init_bias(shape, st_dev):\n    bias = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n    return bias\n\n# Create Placeholders\nx_data = tf.placeholder(shape=[None, 7], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n\n# Create a fully connected layer:\ndef fully_connected(input_layer, weights, biases):\n    layer = tf.add(tf.matmul(input_layer, weights), biases)\n    return tf.nn.relu(layer)\n\n# -------Create the first layer (50 hidden nodes)--------\nweight_1 = init_weight(shape=[7, 25], st_dev=10.0)\nbias_1 = init_bias(shape=[25], st_dev=10.0)\nlayer_1 = fully_connected(x_data, weight_1, bias_1)\n\n# -------Create second layer (25 hidden nodes)--------\nweight_2 = init_weight(shape=[25, 10], st_dev=10.0)\nbias_2 = init_bias(shape=[10], st_dev=10.0)\nlayer_2 = fully_connected(layer_1, weight_2, bias_2)\n\n\n# -------Create third layer (5 hidden nodes)--------\nweight_3 = init_weight(shape=[10, 3], st_dev=10.0)\nbias_3 = init_bias(shape=[3], st_dev=10.0)\nlayer_3 = fully_connected(layer_2, weight_3, bias_3)\n\n\n# -------Create output layer (1 output value)--------\nweight_4 = init_weight(shape=[3, 1], st_dev=10.0)\nbias_4 = init_bias(shape=[1], st_dev=10.0)\nfinal_output = fully_connected(layer_3, weight_4, bias_4)\n\n# Declare loss function (L1)\nloss = tf.reduce_mean(tf.abs(y_target - final_output))\n\n# Declare optimizer\nmy_opt = tf.train.AdamOptimizer(0.05)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\ntest_loss = []\nfor i in range(200):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = x_vals_train[rand_index]\n    rand_y = np.transpose([y_vals_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n    \n    test_temp_loss = sess.run(loss, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n    test_loss.append(test_temp_loss)\n    if (i+1) % 25 == 0:\n        print(\'Generation: \' + str(i+1) + \'. Loss = \' + str(temp_loss))\n\n# Plot loss (MSE) over time\nplt.plot(loss_vec, \'k-\', label=\'Train Loss\')\nplt.plot(test_loss, \'r--\', label=\'Test Loss\')\nplt.title(\'Loss (MSE) per Generation\')\nplt.legend(loc=\'upper right\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Loss\')\nplt.show()\n\n# Model Accuracy\nactuals = np.array([x[0] for x in birth_data])\ntest_actuals = actuals[test_indices]\ntrain_actuals = actuals[train_indices]\ntest_preds = [x[0] for x in sess.run(final_output, feed_dict={x_data: x_vals_test})]\ntrain_preds = [x[0] for x in sess.run(final_output, feed_dict={x_data: x_vals_train})]\ntest_preds = np.array([0.0 if x < 2500.0 else 1.0 for x in test_preds])\ntrain_preds = np.array([0.0 if x < 2500.0 else 1.0 for x in train_preds])\n# Print out accuracies\ntest_acc = np.mean([x == y for x, y in zip(test_preds, test_actuals)])\ntrain_acc = np.mean([x == y for x, y in zip(train_preds, train_actuals)])\nprint(\'On predicting the category of low birthweight from regression output (<2500g):\')\nprint(\'Test Accuracy: {}\'.format(test_acc))\nprint(\'Train Accuracy: {}\'.format(train_acc))\n\n# Evaluate new points on the model\n# Need vectors of \'AGE\', \'LWT\', \'RACE\', \'SMOKE\', \'PTL\', \'HT\', \'UI\'\nnew_data = np.array([[35, 185, 1., 0., 0., 0., 1.],\n                     [18, 160, 0., 1., 0., 0., 1.]])\nnew_data_scaled = np.nan_to_num(normalize_cols(new_data, train_max, train_min))\nnew_logits = [x[0] for x in sess.run(final_output, feed_dict={x_data: new_data_scaled})]\nnew_preds = np.array([1.0 if x < 2500.0 else 0.0 for x in new_logits])\n\nprint(\'New Data Predictions: {}\'.format(new_preds))\n'"
06_Neural_Networks/07_Improving_Linear_Regression/07_improving_linear_regression.py,13,"b'""""""\nImproving Linear Regression with Neural Networks (Logistic Regression)\n----------------------------------------------------------------------\n\nThis function shows how to use TensorFlow to\nsolve logistic regression with a multiple layer neural network\ny = sigmoid(A3 * sigmoid(A2* sigmoid(A1*x + b1) + b2) + b3)\n\nWe will use the low birth weight data, specifically:\n y = 0 or 1 = low birth weight\n x = demographic and medical history data\n""""""\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport requests\nimport os.path\nimport csv\nfrom tensorflow.python.framework import ops\n\n# Reset computational graph\nops.reset_default_graph()\n\n# Name of data file\nbirth_weight_file = \'birth_weight.csv\'\nbirthdata_url = \'https://github.com/nfmcclure/tensorflow_cookbook/raw/master\' \\\n                \'/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat\'\n\n# Download data and create data file if file does not exist in current directory\nif not os.path.exists(birth_weight_file):\n    birth_file = requests.get(birthdata_url)\n    birth_data = birth_file.text.split(\'\\r\\n\')\n    birth_header = birth_data[0].split(\'\\t\')\n    birth_data = [[float(x) for x in y.split(\'\\t\') if len(x) >= 1]\n                  for y in birth_data[1:] if len(y) >= 1]\n    with open(birth_weight_file, ""w"") as f:\n        writer = csv.writer(f)\n        writer.writerows([birth_header])\n        writer.writerows(birth_data)\n\n# read birth weight data into memory\nbirth_data = []\nwith open(birth_weight_file, newline=\'\') as csvfile:\n    csv_reader = csv.reader(csvfile)\n    birth_header = next(csv_reader)\n    for row in csv_reader:\n        birth_data.append(row)\n\nbirth_data = [[float(x) for x in row] for row in birth_data]\n\n# Pull out target variable\ny_vals = np.array([x[0] for x in birth_data])\n# Pull out predictor variables (not id, not target, and not birthweight)\nx_vals = np.array([x[1:8] for x in birth_data])\n\n# Set random seed for reproducible results\nseed = 99\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Declare batch size\nbatch_size = 90\n\n# Split data into train/test = 80%/20%\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_train = y_vals[train_indices]\ny_vals_test = y_vals[test_indices]\n\n\n# Normalize by column (min-max norm to be between 0 and 1)\ndef normalize_cols(m, col_min=np.array([None]), col_max=np.array([None])):\n    if not col_min[0]:\n        col_min = m.min(axis=0)\n    if not col_max[0]:\n        col_max = m.max(axis=0)\n    return (m - col_min) / (col_max - col_min), col_min, col_max\n\n\nx_vals_train, train_min, train_max = np.nan_to_num(normalize_cols(x_vals_train))\nx_vals_test, _, _ = np.nan_to_num(normalize_cols(x_vals_test, train_min, train_max))\n\n# Create graph\nsess = tf.Session()\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, 7], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n\n# Create variable definition\ndef init_variable(shape):\n    return tf.Variable(tf.random_normal(shape=shape))\n\n\n# Create a logistic layer definition\ndef logistic(input_layer, multiplication_weight, bias_weight, activation=True):\n    linear_layer = tf.add(tf.matmul(input_layer, multiplication_weight), bias_weight)\n    # We separate the activation at the end because the loss function will\n    # implement the last sigmoid necessary\n    if activation:\n        return tf.nn.sigmoid(linear_layer)\n    else:\n        return linear_layer\n\n# First logistic layer (7 inputs to 7 hidden nodes)\nA1 = init_variable(shape=[7, 14])\nb1 = init_variable(shape=[14])\nlogistic_layer1 = logistic(x_data, A1, b1)\n\n# Second logistic layer (7 hidden inputs to 5 hidden nodes)\nA2 = init_variable(shape=[14, 5])\nb2 = init_variable(shape=[5])\nlogistic_layer2 = logistic(logistic_layer1, A2, b2)\n\n# Final output layer (5 hidden nodes to 1 output)\nA3 = init_variable(shape=[5, 1])\nb3 = init_variable(shape=[1])\nfinal_output = logistic(logistic_layer2, A3, b3, activation=False)\n\n# Declare loss function (Cross Entropy loss)\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=final_output, labels=y_target))\n\n# Declare optimizer\nmy_opt = tf.train.AdamOptimizer(learning_rate=0.002)\ntrain_step = my_opt.minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Actual Prediction\nprediction = tf.round(tf.nn.sigmoid(final_output))\npredictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\naccuracy = tf.reduce_mean(predictions_correct)\n\n# Training loop\nloss_vec = []\ntrain_acc = []\ntest_acc = []\nfor i in range(1500):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = x_vals_train[rand_index]\n    rand_y = np.transpose([y_vals_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n\n    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n    loss_vec.append(temp_loss)\n    temp_acc_train = sess.run(accuracy, feed_dict={x_data: x_vals_train, y_target: np.transpose([y_vals_train])})\n    train_acc.append(temp_acc_train)\n    temp_acc_test = sess.run(accuracy, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n    test_acc.append(temp_acc_test)\n    if (i + 1) % 150 == 0:\n        print(\'Loss = {}\'.format(temp_loss))\n\n# Plot loss over time\nplt.plot(loss_vec, \'k-\')\nplt.title(\'Cross Entropy Loss per Generation\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Cross Entropy Loss\')\nplt.show()\n\n# Plot train and test accuracy\nplt.plot(train_acc, \'k-\', label=\'Train Set Accuracy\')\nplt.plot(test_acc, \'r--\', label=\'Test Set Accuracy\')\nplt.title(\'Train and Test Accuracy\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Accuracy\')\nplt.legend(loc=\'lower right\')\nplt.show()\n'"
06_Neural_Networks/08_Learning_Tic_Tac_Toe/tic_tac_toe_moves.py,10,"b'""""""\nLearning Optimal Tic-Tac-Toe Moves via a Neural Network\n-------------------------------------------------------\nWe will build a one-hidden layer neural network\n to predict the optimal response given a set\n of tic-tac-toe boards.\n\n""""""\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport csv\nimport numpy as np\nimport random\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Definition of X\'s, O\'s, and empty spots:\n# X = 1\n# O = -1\n# empty = 0\n# response on 1-9 grid for placement of next \'1\'\n\n# For example, the \'test_board\' is:\n#\n#   O  |  -  |  -\n# -----------------\n#   X  |  O  |  O\n# -----------------\n#   -  |  -  |  X\n#\n# board above = [-1, 0, 0, 1, -1, -1, 0, 0, 1]\n# Optimal response would be position 6, where\n# the position numbers are:\n#\n#   0  |  1  |  2\n# -----------------\n#   3  |  4  |  5\n# -----------------\n#   6  |  7  |  8\n\n# Test board optimal response:\nresponse = 6\n# Set batch size and five different symmetries of board positions\nbatch_size = 50\nsymmetry = [\'rotate180\', \'rotate90\', \'rotate270\', \'flip_v\', \'flip_h\']\n\n\n# Print a board\ndef print_board(board):\n    symbols = [\'O\', \' \', \'X\']\n    board_plus1 = [int(x) + 1 for x in board]\n    board_line1 = \' {} | {} | {}\'.format(symbols[board_plus1[0]],\n                                         symbols[board_plus1[1]],\n                                         symbols[board_plus1[2]])\n    board_line2 = \' {} | {} | {}\'.format(symbols[board_plus1[3]],\n                                         symbols[board_plus1[4]],\n                                         symbols[board_plus1[5]])\n    board_line3 = \' {} | {} | {}\'.format(symbols[board_plus1[6]],\n                                         symbols[board_plus1[7]],\n                                         symbols[board_plus1[8]])\n    print(board_line1)\n    print(\'___________\')\n    print(board_line2)\n    print(\'___________\')\n    print(board_line3)\n\n\n# Given a board, a response, and a transformation, get the new board+response\ndef get_symmetry(board, play_response, transformation):\n    """"""\n    :param board: list of integers 9 long:\n     opposing mark = -1\n     friendly mark = 1\n     empty space = 0\n    :param play_response: integer of where response is (0-8)\n    :param transformation: one of five transformations on a board:\n     \'rotate180\', \'rotate90\', \'rotate270\', \'flip_v\', \'flip_h\'\n    :return: tuple: (new_board, new_response)\n    """"""\n    if transformation == \'rotate180\':\n        new_response = 8 - play_response\n        return board[::-1], new_response\n    elif transformation == \'rotate90\':\n        new_response = [6, 3, 0, 7, 4, 1, 8, 5, 2].index(play_response)\n        tuple_board = list(zip(*[board[6:9], board[3:6], board[0:3]]))\n        return [value for item in tuple_board for value in item], new_response\n    elif transformation == \'rotate270\':\n        new_response = [2, 5, 8, 1, 4, 7, 0, 3, 6].index(play_response)\n        tuple_board = list(zip(*[board[0:3], board[3:6], board[6:9]]))[::-1]\n        return [value for item in tuple_board for value in item], new_response\n    elif transformation == \'flip_v\':\n        new_response = [6, 7, 8, 3, 4, 5, 0, 1, 2].index(play_response)\n        return board[6:9] + board[3:6] + board[0:3], new_response\n    elif transformation == \'flip_h\':  # flip_h = rotate180, then flip_v\n        new_response = [2, 1, 0, 5, 4, 3, 8, 7, 6].index(play_response)\n        new_board = board[::-1]\n        return new_board[6:9] + new_board[3:6] + new_board[0:3], new_response\n    else:\n        raise ValueError(\'Method not implemented.\')\n\n\n# Read in board move csv file\ndef get_moves_from_csv(csv_file):\n    """"""\n    :param csv_file: csv file location containing the boards w/ responses\n    :return: moves: list of moves with index of best response\n    """"""\n    play_moves = []\n    with open(csv_file, \'rt\') as csvfile:\n        reader = csv.reader(csvfile, delimiter=\',\')\n        for row in reader:\n            play_moves.append(([int(x) for x in row[0:9]], int(row[9])))\n    return play_moves\n\n\n# Get random board with optimal move\ndef get_rand_move(play_moves, rand_transforms=2):\n    """"""\n    :param play_moves: list of the boards w/responses\n    :param rand_transforms: how many random transforms performed on each\n    :return: (board, response), board is a list of 9 integers, response is 1 int\n    """"""\n    (board, play_response) = random.choice(play_moves)\n    possible_transforms = [\'rotate90\', \'rotate180\', \'rotate270\', \'flip_v\', \'flip_h\']\n    for _ in range(rand_transforms):\n        random_transform = random.choice(possible_transforms)\n        (board, play_response) = get_symmetry(board, play_response, random_transform)\n    return board, play_response\n\n# Get list of optimal moves w/ responses\nmoves = get_moves_from_csv(\'base_tic_tac_toe_moves.csv\')\n\n# Create a train set:\ntrain_length = 500\ntrain_set = []\nfor t in range(train_length):\n    train_set.append(get_rand_move(moves))\n\n# To see if the network learns anything new, we will remove\n# all instances of the board [-1, 0, 0, 1, -1, -1, 0, 0, 1],\n# which the optimal response will be the index \'6\'.  We will\n# Test this at the end.\ntest_board = [-1, 0, 0, 1, -1, -1, 0, 0, 1]\ntrain_set = [x for x in train_set if x[0] != test_board]\n\n\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape))\n\n\ndef model(X, A1, A2, bias1, bias2):\n    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(X, A1), bias1))\n    layer2 = tf.add(tf.matmul(layer1, A2), bias2)\n    # Note: we don\'t take the softmax at the end because our cost function does that for us\n    return layer2\n\nX = tf.placeholder(dtype=tf.float32, shape=[None, 9])\nY = tf.placeholder(dtype=tf.int32, shape=[None])\n\nA1 = init_weights([9, 81])\nbias1 = init_weights([81])\nA2 = init_weights([81, 9])\nbias2 = init_weights([9])\n\nmodel_output = model(X, A1, A2, bias1, bias2)\n\nloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=Y))\ntrain_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\nprediction = tf.argmax(model_output, 1)\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nloss_vec = []\nfor i in range(10000):\n    rand_indices = np.random.choice(range(len(train_set)), batch_size, replace=False)\n    batch_data = [train_set[i] for i in rand_indices]\n    x_input = [x[0] for x in batch_data]\n    y_target = np.array([y[1] for y in batch_data])\n    sess.run(train_step, feed_dict={X: x_input, Y: y_target})\n    \n    temp_loss = sess.run(loss, feed_dict={X: x_input, Y: y_target})\n    loss_vec.append(temp_loss)\n    if i % 500 == 0:\n        print(\'Iteration: {}, Loss: {}\'.format(i, temp_loss))\n\n\n# Print loss\nplt.plot(loss_vec, \'k-\', label=\'Loss\')\nplt.title(\'Loss (MSE) per Generation\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Loss\')\nplt.show()\n\n# Make Prediction:\ntest_boards = [test_board]\nfeed_dict = {X: test_boards}\nlogits = sess.run(model_output, feed_dict=feed_dict)\npredictions = sess.run(prediction, feed_dict=feed_dict)\nprint(predictions)\n\n\n# Declare function to check for win\ndef check(board):\n    wins = [[0, 1, 2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6]]\n    for ix in range(len(wins)):\n        if board[wins[ix][0]] == board[wins[ix][1]] == board[wins[ix][2]] == 1.:\n            return 1\n        elif board[wins[ix][0]] == board[wins[ix][1]] == board[wins[ix][2]] == -1.:\n            return 1\n    return 0\n\n# Let\'s play against our model\ngame_tracker = [0., 0., 0., 0., 0., 0., 0., 0., 0.]\nwin_logical = False\nnum_moves = 0\nwhile not win_logical:\n    player_index = input(\'Input index of your move (0-8): \')\n    num_moves += 1\n    # Add player move to game\n    game_tracker[int(player_index)] = 1.\n    \n    # Get model\'s move by first getting all the logits for each index\n    [potential_moves] = sess.run(model_output, feed_dict={X: [game_tracker]})\n    # Now find allowed moves (where game tracker values = 0.0)\n    allowed_moves = [ix for ix, x in enumerate(game_tracker) if x == 0.0]\n    # Find best move by taking argmax of logits if they are in allowed moves\n    model_move = np.argmax([x if ix in allowed_moves else -999.0 for ix, x in enumerate(potential_moves)])\n    \n    # Add model move to game\n    game_tracker[int(model_move)] = -1.\n    print(\'Model has moved\')\n    print_board(game_tracker)\n    # Now check for win or too many moves\n    if check(game_tracker) == 1 or num_moves >= 5:\n        print(\'Game Over!\')\n        win_logical = True\n'"
07_Natural_Language_Processing/02_Working_with_Bag_of_Words/02_bag_of_words.py,14,"b""# Working with Bag of Words\n#---------------------------------------\n#\n# In this example, we will download and preprocess the ham/spam\n#  text data.  We will then use a one-hot-encoding to make a\n#  bag of words set of features to use in logistic regression.\n#\n# We will use these one-hot-vectors for logistic regression to\n#  predict if a text is spam or ham.\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport csv\nimport string\nimport requests\nimport io\nfrom zipfile import ZipFile\nfrom tensorflow.contrib import learn\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a graph session\nsess = tf.Session()\n\n# Check if data was downloaded, otherwise download it and save for future use\nsave_file_name = os.path.join('temp','temp_spam_data.csv')\n\n# Create directory if it doesn't exist\nif not os.path.exists('temp'):\n    os.makedirs('temp')\n\nif os.path.isfile(save_file_name):\n    text_data = []\n    with open(save_file_name, 'r') as temp_output_file:\n        reader = csv.reader(temp_output_file)\n        for row in reader:\n            text_data.append(row)\nelse:\n    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n    r = requests.get(zip_url)\n    z = ZipFile(io.BytesIO(r.content))\n    file = z.read('SMSSpamCollection')\n    # Format Data\n    text_data = file.decode()\n    text_data = text_data.encode('ascii',errors='ignore')\n    text_data = text_data.decode().split('\\n')\n    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n    \n    # And write to csv\n    with open(save_file_name, 'w') as temp_output_file:\n        writer = csv.writer(temp_output_file)\n        writer.writerows(text_data)\n\ntexts = [x[1] for x in text_data]\ntarget = [x[0] for x in text_data]\n\n# Relabel 'spam' as 1, 'ham' as 0\ntarget = [1 if x == 'spam' else 0 for x in target]\n\n# Normalize text\n# Lower case\ntexts = [x.lower() for x in texts]\n\n# Remove punctuation\ntexts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n\n# Remove numbers\ntexts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n\n# Trim extra whitespace\ntexts = [' '.join(x.split()) for x in texts]\n\n# Plot histogram of text lengths\ntext_lengths = [len(x.split()) for x in texts]\ntext_lengths = [x for x in text_lengths if x < 50]\nplt.hist(text_lengths, bins=25)\nplt.title('Histogram of # of Words in Texts')\n\n# Choose max text word length at 25\nsentence_size = 25\nmin_word_freq = 3\n\n# Setup vocabulary processor\nvocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n\n# Have to fit transform to get length of unique words.\nvocab_processor.transform(texts)\ntransformed_texts = np.array([x for x in vocab_processor.transform(texts)])\nembedding_size = len(np.unique(transformed_texts))\n\n# Split up data set into train/test\ntrain_indices = np.random.choice(len(texts), round(len(texts)*0.8), replace=False)\ntest_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\ntexts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\ntexts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\ntarget_train = [x for ix, x in enumerate(target) if ix in train_indices]\ntarget_test = [x for ix, x in enumerate(target) if ix in test_indices]\n\n# Setup Index Matrix for one-hot-encoding\nidentity_mat = tf.diag(tf.ones(shape=[embedding_size]))\n\n# Create variables for logistic regression\nA = tf.Variable(tf.random_normal(shape=[embedding_size,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\ny_target = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n\n# Text-Vocab Embedding\nx_embed = tf.nn.embedding_lookup(identity_mat, x_data)\nx_col_sums = tf.reduce_sum(x_embed, 0)\n\n# Declare model operations\nx_col_sums_2D = tf.expand_dims(x_col_sums, 0)\nmodel_output = tf.add(tf.matmul(x_col_sums_2D, A), b)\n\n# Declare loss function (Cross Entropy loss)\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n\n# Prediction operation\nprediction = tf.sigmoid(model_output)\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.001)\ntrain_step = my_opt.minimize(loss)\n\n# Intitialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Start Logistic Regression\nprint('Starting Training Over {} Sentences.'.format(len(texts_train)))\nloss_vec = []\ntrain_acc_all = []\ntrain_acc_avg = []\nfor ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n    y_data = [[target_train[ix]]]\n    \n    # Run through each observation for training\n    sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n    temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n    loss_vec.append(temp_loss)\n    \n    if (ix + 1) % 10 == 0:\n        print('Training Observation #{}, Loss = {}'.format(ix+1, temp_loss))\n        \n    # Keep trailing average of past 50 observations accuracy\n    # Get prediction of single observation\n    [[temp_pred]] = sess.run(prediction, feed_dict={x_data: t, y_target: y_data})\n    # Get True/False if prediction is accurate\n    train_acc_temp = target_train[ix]==np.round(temp_pred)\n    train_acc_all.append(train_acc_temp)\n    if len(train_acc_all) >= 50:\n        train_acc_avg.append(np.mean(train_acc_all[-50:]))\n\n# Get test set accuracy\nprint('Getting Test Set Accuracy For {} Sentences.'.format(len(texts_test)))\ntest_acc_all = []\nfor ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n    y_data = [[target_test[ix]]]\n    \n    if (ix + 1) % 50 == 0:\n        print('Test Observation #{}'.format(str(ix+1)))\n    \n    # Keep trailing average of past 50 observations accuracy\n    # Get prediction of single observation\n    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n    # Get True/False if prediction is accurate\n    test_acc_temp = target_test[ix]==np.round(temp_pred)\n    test_acc_all.append(test_acc_temp)\n\nprint('\\nOverall Test Accuracy: {}'.format(np.mean(test_acc_all)))\n\n# Plot training accuracy over time\nplt.plot(range(len(train_acc_avg)), train_acc_avg, 'k-', label='Train Accuracy')\nplt.title('Avg Training Acc Over Past 50 Generations')\nplt.xlabel('Generation')\nplt.ylabel('Training Accuracy')\nplt.show()"""
07_Natural_Language_Processing/03_Implementing_tf_idf/03_implementing_tf_idf.py,12,"b""# Implementing TF-IDF\n#---------------------------------------\n#\n# Here we implement TF-IDF,\n#  (Text Frequency - Inverse Document Frequency)\n#  for the spam-ham text data.\n#\n# We will use a hybrid approach of encoding the texts\n#  with sci-kit learn's TFIDF vectorizer.  Then we will\n#  use the regular TensorFlow logistic algorithm outline.\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport csv\nimport numpy as np\nimport os\nimport string\nimport requests\nimport io\nimport nltk\nfrom zipfile import ZipFile\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a graph session\nsess = tf.Session()\n\nbatch_size = 200\nmax_features = 1000\n\n\n# Check if data was downloaded, otherwise download it and save for future use\nsave_file_name = 'temp_spam_data.csv'\nif os.path.isfile(save_file_name):\n    text_data = []\n    with open(save_file_name, 'r') as temp_output_file:\n        reader = csv.reader(temp_output_file)\n        for row in reader:\n            text_data.append(row)\nelse:\n    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n    r = requests.get(zip_url)\n    z = ZipFile(io.BytesIO(r.content))\n    file = z.read('SMSSpamCollection')\n    # Format Data\n    text_data = file.decode()\n    text_data = text_data.encode('ascii', errors='ignore')\n    text_data = text_data.decode().split('\\n')\n    text_data = [x.split('\\t') for x in text_data if len(x) >= 1]\n    \n    # And write to csv\n    with open(save_file_name, 'w') as temp_output_file:\n        writer = csv.writer(temp_output_file)\n        writer.writerows(text_data)\n\n\ntexts = [x[1] for x in text_data]\ntarget = [x[0] for x in text_data]\n\n# Relabel 'spam' as 1, 'ham' as 0\ntarget = [1. if x == 'spam' else 0. for x in target]\n\n# Normalize text\n# Lower case\ntexts = [x.lower() for x in texts]\n\n# Remove punctuation\ntexts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n\n# Remove numbers\ntexts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n\n# Trim extra whitespace\ntexts = [' '.join(x.split()) for x in texts]\n\n\n# Define tokenizer\ndef tokenizer(text):\n    words = nltk.word_tokenize(text)\n    return words\n\n# Create TF-IDF of texts\ntfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english', max_features=max_features)\nsparse_tfidf_texts = tfidf.fit_transform(texts)\n\n# Split up data set into train/test\ntrain_indices = np.random.choice(sparse_tfidf_texts.shape[0], round(0.8*sparse_tfidf_texts.shape[0]), replace=False)\ntest_indices = np.array(list(set(range(sparse_tfidf_texts.shape[0])) - set(train_indices)))\ntexts_train = sparse_tfidf_texts[train_indices]\ntexts_test = sparse_tfidf_texts[test_indices]\ntarget_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\ntarget_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])\n\n# Create variables for logistic regression\nA = tf.Variable(tf.random_normal(shape=[max_features, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, max_features], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Declare logistic model (sigmoid in loss function)\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n\n# Declare loss function (Cross Entropy loss)\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n\n# Actual Prediction\nprediction = tf.round(tf.sigmoid(model_output))\npredictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\naccuracy = tf.reduce_mean(predictions_correct)\n\n# Declare optimizer\nmy_opt = tf.train.GradientDescentOptimizer(0.0025)\ntrain_step = my_opt.minimize(loss)\n\n# Intitialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Start Logistic Regression\ntrain_loss = []\ntest_loss = []\ntrain_acc = []\ntest_acc = []\ni_data = []\nfor i in range(10000):\n    rand_index = np.random.choice(texts_train.shape[0], size=batch_size)\n    rand_x = texts_train[rand_index].todense()\n    rand_y = np.transpose([target_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    \n    # Only record loss and accuracy every 100 generations\n    if (i + 1) % 100 == 0:\n        i_data.append(i+1)\n        train_loss_temp = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n        train_loss.append(train_loss_temp)\n        \n        test_loss_temp = sess.run(loss, feed_dict={x_data: texts_test.todense(), y_target: np.transpose([target_test])})\n        test_loss.append(test_loss_temp)\n        \n        train_acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x, y_target: rand_y})\n        train_acc.append(train_acc_temp)\n    \n        test_acc_temp = sess.run(accuracy, feed_dict={x_data: texts_test.todense(), y_target: np.transpose([target_test])})\n        test_acc.append(test_acc_temp)\n    if (i + 1) % 500 == 0:\n        acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n        print('Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n\n\n# Plot loss over time\nplt.plot(i_data, train_loss, 'k-', label='Train Loss')\nplt.plot(i_data, test_loss, 'r--', label='Test Loss', linewidth=4)\nplt.title('Cross Entropy Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Cross Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()\n\n# Plot train and test accuracy\nplt.plot(i_data, train_acc, 'k-', label='Train Set Accuracy')\nplt.plot(i_data, test_acc, 'r--', label='Test Set Accuracy', linewidth=4)\nplt.title('Train and Test Accuracy')\nplt.xlabel('Generation')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()"""
07_Natural_Language_Processing/04_Working_With_Skip_Gram_Embeddings/04_working_with_skipgram.py,14,"b'# Word2Vec: Skipgram Model\n#---------------------------------------\n#\n# In this example, we will download and preprocess the movie\n#  review data.\n#\n# From this data set we will compute/fit the skipgram model of\n#  the Word2Vec Algorithm\n#\n# Skipgram: based on predicting the surrounding words from the\n#  Ex sentence ""the cat in the hat""\n#  context word:  [""hat""]\n#  target words: [""the"", ""cat"", ""in"", ""the""]\n#  context-target pairs:\n#    (""hat"", ""the""), (""hat"", ""cat""), (""hat"", ""in""), (""hat"", ""the"")\n\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport os\nimport string\nimport requests\nimport collections\nimport io\nimport gzip\nimport tarfile\nimport urllib.request\nfrom nltk.corpus import stopwords\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\nos.chdir(os.path.dirname(os.path.realpath(__file__)))\n\n# Start a graph session\nsess = tf.Session()\n\n# Declare model parameters\nbatch_size = 100\nembedding_size = 200\nvocabulary_size = 10000\ngenerations = 100000\nprint_loss_every = 2000\n\nnum_sampled = int(batch_size/2)    # Number of negative examples to sample.\nwindow_size = 2       # How many words to consider left and right.\n\n# Declare stop words\nstops = stopwords.words(\'english\')\n\n# We pick five test words. We are expecting synonyms to appear\nprint_valid_every = 5000\nvalid_words = [\'cliche\', \'love\', \'hate\', \'silly\', \'sad\']\n# Later we will have to transform these into indices\n\n\n# Load the movie review data\n# Check if data was downloaded, otherwise download it and save for future use\ndef load_movie_data():\n    save_folder_name = \'temp\'\n    pos_file = os.path.join(save_folder_name, \'rt-polaritydata\', \'rt-polarity.pos\')\n    neg_file = os.path.join(save_folder_name, \'rt-polaritydata\', \'rt-polarity.neg\')\n\n    # Check if files are already downloaded\n    if not os.path.exists(os.path.join(save_folder_name, \'rt-polaritydata\')):\n        movie_data_url = \'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\'\n\n        # Save tar.gz file\n        req = requests.get(movie_data_url, stream=True)\n        with open(\'temp_movie_review_temp.tar.gz\', \'wb\') as f:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        # Extract tar.gz file into temp folder\n        tar = tarfile.open(\'temp_movie_review_temp.tar.gz\', ""r:gz"")\n        tar.extractall(path=\'temp\')\n        tar.close()\n\n    pos_data = []\n    with open(pos_file, \'r\', encoding=\'latin-1\') as f:\n        for line in f:\n            pos_data.append(line.encode(\'ascii\', errors=\'ignore\').decode())\n    f.close()\n    pos_data = [x.rstrip() for x in pos_data]\n\n    neg_data = []\n    with open(neg_file, \'r\', encoding=\'latin-1\') as f:\n        for line in f:\n            neg_data.append(line.encode(\'ascii\', errors=\'ignore\').decode())\n    f.close()\n    neg_data = [x.rstrip() for x in neg_data]\n    \n    texts = pos_data + neg_data\n    target = [1] * len(pos_data) + [0] * len(neg_data)\n    \n    return texts, target\n\ntexts, target = load_movie_data()\n\n\n# Normalize text\ndef normalize_text(texts, stops):\n    # Lower case\n    texts = [x.lower() for x in texts]\n\n    # Remove punctuation\n    texts = [\'\'.join(c for c in x if c not in string.punctuation) for x in texts]\n\n    # Remove numbers\n    texts = [\'\'.join(c for c in x if c not in \'0123456789\') for x in texts]\n\n    # Remove stopwords\n    texts = [\' \'.join([word for word in x.split() if word not in stops]) for x in texts]\n\n    # Trim extra whitespace\n    texts = [\' \'.join(x.split()) for x in texts]\n    \n    return texts\n\n\ntexts = normalize_text(texts, stops)\n\n# Texts must contain at least 3 words\ntarget = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\ntexts = [x for x in texts if len(x.split()) > 2]\n\n\n# Build dictionary of words\ndef build_dictionary(sentences, vocabulary_size):\n    # Turn sentences (list of strings) into lists of words\n    split_sentences = [s.split() for s in sentences]\n    words = [x for sublist in split_sentences for x in sublist]\n    \n    # Initialize list of [word, word_count] for each word, starting with unknown\n    count = [[\'RARE\', -1]]\n    \n    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n    \n    # Now create the dictionary\n    word_dict = {}\n    # For each word, that we want in the dictionary, add it, then make it\n    # the value of the prior dictionary length\n    for word, word_count in count:\n        word_dict[word] = len(word_dict)\n    \n    return word_dict\n    \n\n# Turn text data into lists of integers from dictionary\ndef text_to_numbers(sentences, word_dict):\n    # Initialize the returned data\n    data = []\n    for sentence in sentences:\n        sentence_data = []\n        # For each word, either use selected index or rare word index\n        for word in sentence.split(\' \'):\n            if word in word_dict:\n                word_ix = word_dict[word]\n            else:\n                word_ix = 0\n            sentence_data.append(word_ix)\n        data.append(sentence_data)\n    return data\n\n\n# Build our data set and dictionaries\nword_dictionary = build_dictionary(texts, vocabulary_size)\nword_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\ntext_data = text_to_numbers(texts, word_dictionary)\n\n# Get validation word keys\nvalid_examples = [word_dictionary[x] for x in valid_words]\n\n\n# Generate data randomly (N words behind, target, N words ahead)\ndef generate_batch_data(sentences, batch_size, window_size, method=\'skip_gram\'):\n    # Fill up data batch\n    batch_data = []\n    label_data = []\n    while len(batch_data) < batch_size:\n        # select random sentence to start\n        rand_sentence = np.random.choice(sentences)\n        # Generate consecutive windows to look at\n        window_sequences = [rand_sentence[max((ix - window_size), 0):(ix + window_size + 1)] for ix, x in enumerate(rand_sentence)]\n        # Denote which element of each window is the center word of interest\n        label_indices = [ix if ix < window_size else window_size for ix, x in enumerate(window_sequences)]\n        \n        # Pull out center word of interest for each window and create a tuple for each window\n        if method == \'skip_gram\':\n            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x, y in zip(window_sequences, label_indices)]\n            # Make it in to a big list of tuples (target word, surrounding word)\n            tuple_data = [(x, y_) for x, y in batch_and_labels for y_ in y]\n        elif method == \'cbow\':\n            batch_and_labels = [(x[:y] + x[(y + 1):], x[y]) for x, y in zip(window_sequences, label_indices)]\n            # Make it in to a big list of tuples (target word, surrounding word)\n            tuple_data = [(x_, y) for x, y in batch_and_labels for x_ in x]\n        else:\n            raise ValueError(\'Method {} not implemented yet.\'.format(method))\n            \n        # extract batch and labels\n        batch, labels = [list(x) for x in zip(*tuple_data)]\n        batch_data.extend(batch[:batch_size])\n        label_data.extend(labels[:batch_size])\n    # Trim batch and label at the end\n    batch_data = batch_data[:batch_size]\n    label_data = label_data[:batch_size]\n    \n    # Convert to numpy array\n    batch_data = np.array(batch_data)\n    label_data = np.transpose(np.array([label_data]))\n    \n    return batch_data, label_data\n\n\n# Define Embeddings:\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n\n# NCE loss parameters\nnce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                                              stddev=1.0 / np.sqrt(embedding_size)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n# Create data/target placeholders\nx_inputs = tf.placeholder(tf.int32, shape=[batch_size])\ny_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n# Lookup the word embedding:\nembed = tf.nn.embedding_lookup(embeddings, x_inputs)\n\n# Get loss from prediction\nloss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n                                     biases=nce_biases,\n                                     labels=y_target,\n                                     inputs=embed,\n                                     num_sampled=num_sampled,\n                                     num_classes=vocabulary_size))\n                                     \n# Create optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n\n# Cosine similarity between words\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\nnormalized_embeddings = embeddings / norm\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\nsimilarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n\n# Add variable initializer.\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Run the skip gram model.\nloss_vec = []\nloss_x_vec = []\nfor i in range(generations):\n    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n    feed_dict = {x_inputs: batch_inputs, y_target: batch_labels}\n\n    # Run the train step\n    sess.run(optimizer, feed_dict=feed_dict)\n\n    # Return the loss\n    if (i + 1) % print_loss_every == 0:\n        loss_val = sess.run(loss, feed_dict=feed_dict)\n        loss_vec.append(loss_val)\n        loss_x_vec.append(i+1)\n        print(\'Loss at step {} : {}\'.format(i+1, loss_val))\n      \n    # Validation: Print some random words and top 5 related words\n    if (i+1) % print_valid_every == 0:\n        sim = sess.run(similarity, feed_dict=feed_dict)\n        for j in range(len(valid_words)):\n            valid_word = valid_words[j]\n            # top_k = number of nearest neighbors\n            top_k = 5\n            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n            log_str = ""Nearest to {}:"".format(valid_word)\n            for k in range(top_k):\n                close_word = word_dictionary_rev[nearest[k]]\n                log_str = \'{} {},\'.format(log_str, close_word)\n            print(log_str)\n'"
07_Natural_Language_Processing/05_Working_With_CBOW_Embeddings/05_Working_With_CBOW.py,17,"b'# Word2Vec: CBOW Model (Continuous Bag of Words)\n#---------------------------------------\n#\n# In this example, we will download and preprocess the movie\n#  review data.\n#\n# From this data set we will compute/fit the CBOW model of\n#  the Word2Vec Algorithm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport os\nimport pickle\nimport string\nimport requests\nimport collections\nimport io\nimport tarfile\nimport urllib.request\nimport text_helpers\nfrom nltk.corpus import stopwords\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Set Random Seeds\ntf.set_random_seed(42)\nnp.random.seed(42)\n\n# os.chdir(os.path.dirname(os.path.realpath(__file__)))\n\n# Make a saving directory if it doesn\'t exist\ndata_folder_name = \'temp\'\nif not os.path.exists(data_folder_name):\n    os.makedirs(data_folder_name)\n\n# Start a graph session\nsess = tf.Session()\n\n# Declare model parameters\nbatch_size = 500\nembedding_size = 200\nvocabulary_size = 2000\ngenerations = 50000\nmodel_learning_rate = 0.25\n\nnum_sampled = int(batch_size/2)    # Number of negative examples to sample.\nwindow_size = 3       # How many words to consider left and right.\n\n# Add checkpoints to training\nsave_embeddings_every = 5000\nprint_valid_every = 5000\nprint_loss_every = 100\n\n# Declare stop words\nstops = stopwords.words(\'english\')\n\n# We pick some test words. We are expecting synonyms to appear\nvalid_words = [\'love\', \'hate\', \'happy\', \'sad\', \'man\', \'woman\']\n# Later we will have to transform these into indices\n\n# Load the movie review data\nprint(\'Loading Data\')\ntexts, target = text_helpers.load_movie_data()\n\n# Normalize text\nprint(\'Normalizing Text Data\')\ntexts = text_helpers.normalize_text(texts, stops)\n\n# Texts must contain at least 3 words\ntarget = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\ntexts = [x for x in texts if len(x.split()) > 2]    \n\n# Build our data set and dictionaries\nprint(\'Creating Dictionary\')\nword_dictionary = text_helpers.build_dictionary(texts, vocabulary_size)\nword_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\ntext_data = text_helpers.text_to_numbers(texts, word_dictionary)\n\n# Get validation word keys\nvalid_examples = [word_dictionary[x] for x in valid_words]    \n\nprint(\'Creating Model\')\n# Define Embeddings:\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n\n# NCE loss parameters\nnce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                                               stddev=1.0 / np.sqrt(embedding_size)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n# Create data/target placeholders\nx_inputs = tf.placeholder(tf.int32, shape=[batch_size, 2*window_size])\ny_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n# Lookup the word embedding\n# Add together window embeddings:\nembed = tf.zeros([batch_size, embedding_size])\nfor element in range(2*window_size):\n    embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])\n\n# Get loss from prediction\nloss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n                                     biases=nce_biases,\n                                     labels=y_target,\n                                     inputs=embed,\n                                     num_sampled=num_sampled,\n                                     num_classes=vocabulary_size))\n                                     \n# Create optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate).minimize(loss)\n\n# Cosine similarity between words\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\nnormalized_embeddings = embeddings / norm\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\nsimilarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n\n# Create model saving operation\nsaver = tf.train.Saver({""embeddings"": embeddings})\n\n#Add variable initializer.\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Filter out sentences that aren\'t long enough:\ntext_data = [x for x in text_data if len(x)>=(2*window_size+1)]\n\n# Run the CBOW model.\nprint(\'Starting Training\')\nloss_vec = []\nloss_x_vec = []\nfor i in range(generations):\n    batch_inputs, batch_labels = text_helpers.generate_batch_data(text_data, batch_size,\n                                                                  window_size, method=\'cbow\')\n    feed_dict = {x_inputs: batch_inputs, y_target: batch_labels}\n\n    # Run the train step\n    sess.run(optimizer, feed_dict=feed_dict)\n\n    # Return the loss\n    if (i+1) % print_loss_every == 0:\n        loss_val = sess.run(loss, feed_dict=feed_dict)\n        loss_vec.append(loss_val)\n        loss_x_vec.append(i+1)\n        print(\'Loss at step {} : {}\'.format(i+1, loss_val))\n      \n    # Validation: Print some random words and top 5 related words\n    if (i+1) % print_valid_every == 0:\n        sim = sess.run(similarity, feed_dict=feed_dict)\n        for j in range(len(valid_words)):\n            valid_word = word_dictionary_rev[valid_examples[j]]\n            top_k = 5  # number of nearest neighbors\n            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n            log_str = ""Nearest to {}:"".format(valid_word)\n            for k in range(top_k):\n                close_word = word_dictionary_rev[nearest[k]]\n                log_str = \'{} {},\' .format(log_str, close_word)\n            print(log_str)\n            \n    # Save dictionary + embeddings\n    if (i + 1) % save_embeddings_every == 0:\n        # Save vocabulary dictionary\n        with open(os.path.join(data_folder_name, \'movie_vocab.pkl\'), \'wb\') as f:\n            pickle.dump(word_dictionary, f)\n        \n        # Save embeddings\n        model_checkpoint_path = os.path.join(os.getcwd(), data_folder_name, \'cbow_movie_embeddings.ckpt\')\n        save_path = saver.save(sess, model_checkpoint_path)\n        print(\'Model saved in file: {}\'.format(save_path))\n'"
07_Natural_Language_Processing/05_Working_With_CBOW_Embeddings/text_helpers.py,0,"b'# Text Helper Functions\n#---------------------------------------\n#\n# We pull out text helper functions to reduce redundant code\n\nimport string\nimport os\nimport urllib.request\nimport io\nimport tarfile\nimport collections\nimport numpy as np\nimport requests\nimport gzip\n\n# Normalize text\ndef normalize_text(texts, stops):\n    # Lower case\n    texts = [x.lower() for x in texts]\n\n    # Remove punctuation\n    texts = [\'\'.join(c for c in x if c not in string.punctuation) for x in texts]\n\n    # Remove numbers\n    texts = [\'\'.join(c for c in x if c not in \'0123456789\') for x in texts]\n\n    # Remove stopwords\n    texts = [\' \'.join([word for word in x.split() if word not in (stops)]) for x in texts]\n\n    # Trim extra whitespace\n    texts = [\' \'.join(x.split()) for x in texts]\n    \n    return(texts)\n\n\n# Build dictionary of words\ndef build_dictionary(sentences, vocabulary_size):\n    # Turn sentences (list of strings) into lists of words\n    split_sentences = [s.split() for s in sentences]\n    words = [x for sublist in split_sentences for x in sublist]\n    \n    # Initialize list of [word, word_count] for each word, starting with unknown\n    count = [[\'RARE\', -1]]\n    \n    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n    \n    # Now create the dictionary\n    word_dict = {}\n    # For each word, that we want in the dictionary, add it, then make it\n    # the value of the prior dictionary length\n    for word, word_count in count:\n        word_dict[word] = len(word_dict)\n    \n    return(word_dict)\n    \n\n# Turn text data into lists of integers from dictionary\ndef text_to_numbers(sentences, word_dict):\n    # Initialize the returned data\n    data = []\n    for sentence in sentences:\n        sentence_data = []\n        # For each word, either use selected index or rare word index\n        for word in sentence.split(\' \'):\n            if word in word_dict:\n                word_ix = word_dict[word]\n            else:\n                word_ix = 0\n            sentence_data.append(word_ix)\n        data.append(sentence_data)\n    return(data)\n    \n\n# Generate data randomly (N words behind, target, N words ahead)\ndef generate_batch_data(sentences, batch_size, window_size, method=\'skip_gram\'):\n    # Fill up data batch\n    batch_data = []\n    label_data = []\n    while len(batch_data) < batch_size:\n        # select random sentence to start\n        rand_sentence_ix = int(np.random.choice(len(sentences), size=1))\n        rand_sentence = sentences[rand_sentence_ix]\n        # Generate consecutive windows to look at\n        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n        # Denote which element of each window is the center word of interest\n        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n        \n        # Pull out center word of interest for each window and create a tuple for each window\n        batch, labels = [], []\n        if method==\'skip_gram\':\n            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n            # Make it in to a big list of tuples (target word, surrounding word)\n            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n            if len(tuple_data) > 0:\n                batch, labels = [list(x) for x in zip(*tuple_data)]\n        elif method==\'cbow\':\n            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n            # Only keep windows with consistent 2*window_size\n            batch_and_labels = [(x,y) for x,y in batch_and_labels if len(x)==2*window_size]\n            if len(batch_and_labels) > 0:\n                batch, labels = [list(x) for x in zip(*batch_and_labels)]\n        elif method==\'doc2vec\':\n            # For doc2vec we keep LHS window only to predict target word\n            batch_and_labels = [(rand_sentence[i:i+window_size], rand_sentence[i+window_size]) for i in range(0, len(rand_sentence)-window_size)]\n            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n            # Add document index to batch!! Remember that we must extract the last index in batch for the doc-index\n            batch = [x + [rand_sentence_ix] for x in batch]\n        else:\n            raise ValueError(\'Method {} not implemented yet.\'.format(method))\n            \n        # extract batch and labels\n        batch_data.extend(batch[:batch_size])\n        label_data.extend(labels[:batch_size])\n    # Trim batch and label at the end\n    batch_data = batch_data[:batch_size]\n    label_data = label_data[:batch_size]\n    \n    # Convert to numpy array\n    batch_data = np.array(batch_data)\n    label_data = np.transpose(np.array([label_data]))\n    \n    return batch_data, label_data\n\n\n# Load the movie review data\n# Check if data was downloaded, otherwise download it and save for future use\ndef load_movie_data():\n    save_folder_name = \'temp\'\n    pos_file = os.path.join(save_folder_name, \'rt-polaritydata\', \'rt-polarity.pos\')\n    neg_file = os.path.join(save_folder_name, \'rt-polaritydata\', \'rt-polarity.neg\')\n\n    # Check if files are already downloaded\n    if not os.path.exists(os.path.join(save_folder_name, \'rt-polaritydata\')):\n        movie_data_url = \'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\'\n\n        # Save tar.gz file\n        req = requests.get(movie_data_url, stream=True)\n        with open(\'temp_movie_review_temp.tar.gz\', \'wb\') as f:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        # Extract tar.gz file into temp folder\n        tar = tarfile.open(\'temp_movie_review_temp.tar.gz\', ""r:gz"")\n        tar.extractall(path=\'temp\')\n        tar.close()\n\n    pos_data = []\n    with open(pos_file, \'r\', encoding=\'latin-1\') as f:\n        for line in f:\n            pos_data.append(line.encode(\'ascii\',errors=\'ignore\').decode())\n    f.close()\n    pos_data = [x.rstrip() for x in pos_data]\n\n    neg_data = []\n    with open(neg_file, \'r\', encoding=\'latin-1\') as f:\n        for line in f:\n            neg_data.append(line.encode(\'ascii\',errors=\'ignore\').decode())\n    f.close()\n    neg_data = [x.rstrip() for x in neg_data]\n    \n    texts = pos_data + neg_data\n    target = [1]*len(pos_data) + [0]*len(neg_data)\n    \n    return(texts, target)'"
07_Natural_Language_Processing/06_Using_Word2Vec_Embeddings/06_using_word2vec.py,16,"b'# Using Word2Vec for prediction\n#---------------------------------------\n#\n# In this example, we will load our prior CBOW trained embeddings\n#   to perform logistic regression model for movie review predictions\n#\n# From this data set we will compute/fit the CBOW model of\n#  the Word2Vec Algorithm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport os\nimport pickle\nimport string\nimport requests\nimport collections\nimport io\nimport tarfile\nimport urllib.request\nimport text_helpers\nfrom nltk.corpus import stopwords\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\nos.chdir(os.path.dirname(os.path.realpath(__file__)))\n\n# Start a graph session\nsess = tf.Session()\n\n# Declare model parameters\nembedding_size = 200\nvocabulary_size = 2000\nbatch_size = 100\nmax_words = 100\n\n# Declare stop words\nstops = stopwords.words(\'english\')\n\n# Load Data\nprint(\'Loading Data\')\ntexts, target = text_helpers.load_movie_data()\n\n# Normalize text\nprint(\'Normalizing Text Data\')\ntexts = text_helpers.normalize_text(texts, stops)\n\n# Texts must contain at least 3 words\ntarget = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\ntexts = [x for x in texts if len(x.split()) > 2]\n\n# Split up data set into train/test\ntrain_indices = np.random.choice(len(target), round(0.8*len(target)), replace=False)\ntest_indices = np.array(list(set(range(len(target))) - set(train_indices)))\ntexts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\ntexts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\ntarget_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\ntarget_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])\n\n# Load dictionary and embedding matrix\ndict_file = os.path.join(\'..\', \'05_Working_With_CBOW_Embeddings\', \'temp\', \'movie_vocab.pkl\')\nword_dictionary = pickle.load(open(dict_file, \'rb\'))\n\n# Convert texts to lists of indices\ntext_data_train = np.array(text_helpers.text_to_numbers(texts_train, word_dictionary))\ntext_data_test = np.array(text_helpers.text_to_numbers(texts_test, word_dictionary))\n\n# Pad/crop movie reviews to specific length\ntext_data_train = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_train]])\ntext_data_test = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_test]])\n\nprint(\'Creating Model\')\n# Define Embeddings:\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n\n# Define model:\n# Create variables for logistic regression\nA = tf.Variable(tf.random_normal(shape=[embedding_size, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\n\n# Initialize placeholders\nx_data = tf.placeholder(shape=[None, max_words], dtype=tf.int32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n# Lookup embeddings vectors\nembed = tf.nn.embedding_lookup(embeddings, x_data)\n# Take average of all word embeddings in documents\nembed_avg = tf.reduce_mean(embed, 1)\n\n# Declare logistic model (sigmoid in loss function)\nmodel_output = tf.add(tf.matmul(embed_avg, A), b)\n\n# Declare loss function (Cross Entropy loss)\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n\n# Actual Prediction\nprediction = tf.round(tf.sigmoid(model_output))\npredictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\naccuracy = tf.reduce_mean(predictions_correct)\n\n# Declare optimizer\nmy_opt = tf.train.AdagradOptimizer(0.005)\ntrain_step = my_opt.minimize(loss)\n\n# Intitialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Load model embeddings\nmodel_checkpoint_path = os.path.join(\'..\', \'05_Working_With_CBOW_Embeddings\',\n                                     \'temp\', \'cbow_movie_embeddings.ckpt\')\nsaver = tf.train.Saver({""embeddings"": embeddings})\nsaver.restore(sess, model_checkpoint_path)\n\n# Start Logistic Regression\nprint(\'Starting Model Training\')\ntrain_loss = []\ntest_loss = []\ntrain_acc = []\ntest_acc = []\ni_data = []\nfor i in range(10000):\n    rand_index = np.random.choice(text_data_train.shape[0], size=batch_size)\n    rand_x = text_data_train[rand_index]\n    rand_y = np.transpose([target_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n    \n    # Only record loss and accuracy every 100 generations\n    if (i + 1) % 100 == 0:\n        i_data.append(i + 1)\n        train_loss_temp = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n        train_loss.append(train_loss_temp)\n        \n        test_loss_temp = sess.run(loss, feed_dict={x_data: text_data_test, y_target: np.transpose([target_test])})\n        test_loss.append(test_loss_temp)\n        \n        train_acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x, y_target: rand_y})\n        train_acc.append(train_acc_temp)\n    \n        test_acc_temp = sess.run(accuracy, feed_dict={x_data: text_data_test, y_target: np.transpose([target_test])})\n        test_acc.append(test_acc_temp)\n    if (i + 1) % 500 == 0:\n        acc_and_loss = [i + 1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n        print(\'Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})\'.format(*acc_and_loss))\n\n\n# Plot loss over time\nplt.plot(i_data, train_loss, \'k-\', label=\'Train Loss\')\nplt.plot(i_data, test_loss, \'r--\', label=\'Test Loss\', linewidth=4)\nplt.title(\'Cross Entropy Loss per Generation\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Cross Entropy Loss\')\nplt.legend(loc=\'upper right\')\nplt.show()\n\n# Plot train and test accuracy\nplt.plot(i_data, train_acc, \'k-\', label=\'Train Set Accuracy\')\nplt.plot(i_data, test_acc, \'r--\', label=\'Test Set Accuracy\', linewidth=4)\nplt.title(\'Train and Test Accuracy\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Accuracy\')\nplt.legend(loc=\'lower right\')\nplt.show()'"
07_Natural_Language_Processing/06_Using_Word2Vec_Embeddings/text_helpers.py,0,"b'# Text Helper Functions\n#---------------------------------------\n#\n# We pull out text helper functions to reduce redundant code\n\nimport string\nimport os\nimport urllib.request\nimport io\nimport tarfile\nimport collections\nimport numpy as np\nimport requests\nimport gzip\n\n# Normalize text\ndef normalize_text(texts, stops):\n    # Lower case\n    texts = [x.lower() for x in texts]\n\n    # Remove punctuation\n    texts = [\'\'.join(c for c in x if c not in string.punctuation) for x in texts]\n\n    # Remove numbers\n    texts = [\'\'.join(c for c in x if c not in \'0123456789\') for x in texts]\n\n    # Remove stopwords\n    texts = [\' \'.join([word for word in x.split() if word not in (stops)]) for x in texts]\n\n    # Trim extra whitespace\n    texts = [\' \'.join(x.split()) for x in texts]\n    \n    return(texts)\n\n\n# Build dictionary of words\ndef build_dictionary(sentences, vocabulary_size):\n    # Turn sentences (list of strings) into lists of words\n    split_sentences = [s.split() for s in sentences]\n    words = [x for sublist in split_sentences for x in sublist]\n    \n    # Initialize list of [word, word_count] for each word, starting with unknown\n    count = [[\'RARE\', -1]]\n    \n    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n    \n    # Now create the dictionary\n    word_dict = {}\n    # For each word, that we want in the dictionary, add it, then make it\n    # the value of the prior dictionary length\n    for word, word_count in count:\n        word_dict[word] = len(word_dict)\n    \n    return(word_dict)\n    \n\n# Turn text data into lists of integers from dictionary\ndef text_to_numbers(sentences, word_dict):\n    # Initialize the returned data\n    data = []\n    for sentence in sentences:\n        sentence_data = []\n        # For each word, either use selected index or rare word index\n        for word in sentence.split():\n            if word in word_dict:\n                word_ix = word_dict[word]\n            else:\n                word_ix = 0\n            sentence_data.append(word_ix)\n        data.append(sentence_data)\n    return(data)\n    \n\n# Generate data randomly (N words behind, target, N words ahead)\ndef generate_batch_data(sentences, batch_size, window_size, method=\'skip_gram\'):\n    # Fill up data batch\n    batch_data = []\n    label_data = []\n    while len(batch_data) < batch_size:\n        # select random sentence to start\n        rand_sentence_ix = int(np.random.choice(len(sentences), size=1))\n        rand_sentence = sentences[rand_sentence_ix]\n        # Generate consecutive windows to look at\n        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n        # Denote which element of each window is the center word of interest\n        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n\n        # Pull out center word of interest for each window and create a tuple for each window\n        batch, labels = [], []\n        if method == \'skip_gram\':\n            batch_and_labels = [(x[y], x[:y] + x[(y + 1):]) for x, y in zip(window_sequences, label_indices)]\n            # Make it in to a big list of tuples (target word, surrounding word)\n            tuple_data = [(x, y_) for x, y in batch_and_labels for y_ in y]\n            if len(tuple_data) > 0:\n                batch, labels = [list(x) for x in zip(*tuple_data)]\n        elif method == \'cbow\':\n            batch_and_labels = [(x[:y] + x[(y + 1):], x[y]) for x, y in zip(window_sequences, label_indices)]\n            # Only keep windows with consistent 2*window_size\n            batch_and_labels = [(x, y) for x, y in batch_and_labels if len(x) == 2 * window_size]\n            if len(batch_and_labels) > 0:\n                batch, labels = [list(x) for x in zip(*batch_and_labels)]\n        elif method == \'doc2vec\':\n            # For doc2vec we keep LHS window only to predict target word\n            batch_and_labels = [(rand_sentence[i:i + window_size], rand_sentence[i + window_size]) for i in\n                                range(0, len(rand_sentence) - window_size)]\n            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n            # Add document index to batch!! Remember that we must extract the last index in batch for the doc-index\n            batch = [x + [rand_sentence_ix] for x in batch]\n        else:\n            raise ValueError(\'Method {} not implemented yet.\'.format(method))\n            \n        # extract batch and labels\n        batch_data.extend(batch[:batch_size])\n        label_data.extend(labels[:batch_size])\n    # Trim batch and label at the end\n    batch_data = batch_data[:batch_size]\n    label_data = label_data[:batch_size]\n    \n    # Convert to numpy array\n    batch_data = np.array(batch_data)\n    label_data = np.transpose(np.array([label_data]))\n    \n    return(batch_data, label_data)\n    \n    \n# Load the movie review data\n# Check if data was downloaded, otherwise download it and save for future use\ndef load_movie_data():\n    save_folder_name = \'temp\'\n    pos_file = os.path.join(save_folder_name, \'rt-polaritydata\', \'rt-polarity.pos\')\n    neg_file = os.path.join(save_folder_name, \'rt-polaritydata\', \'rt-polarity.neg\')\n\n    # Check if files are already downloaded\n    if not os.path.exists(os.path.join(save_folder_name, \'rt-polaritydata\')):\n        movie_data_url = \'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\'\n\n        # Save tar.gz file\n        req = requests.get(movie_data_url, stream=True)\n        with open(\'temp_movie_review_temp.tar.gz\', \'wb\') as f:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        # Extract tar.gz file into temp folder\n        tar = tarfile.open(\'temp_movie_review_temp.tar.gz\', ""r:gz"")\n        tar.extractall(path=\'temp\')\n        tar.close()\n\n    pos_data = []\n    with open(pos_file, \'r\', encoding=\'latin-1\') as f:\n        for line in f:\n            pos_data.append(line.encode(\'ascii\',errors=\'ignore\').decode())\n    f.close()\n    pos_data = [x.rstrip() for x in pos_data]\n\n    neg_data = []\n    with open(neg_file, \'r\', encoding=\'latin-1\') as f:\n        for line in f:\n            neg_data.append(line.encode(\'ascii\',errors=\'ignore\').decode())\n    f.close()\n    neg_data = [x.rstrip() for x in neg_data]\n    \n    texts = pos_data + neg_data\n    target = [1]*len(pos_data) + [0]*len(neg_data)\n    \n    return(texts, target)\n'"
07_Natural_Language_Processing/07_Sentiment_Analysis_With_Doc2Vec/07_sentiment_with_doc2vec.py,36,"b'# Doc2Vec Model\n#---------------------------------------\n#\n# In this example, we will download and preprocess the movie\n#  review data.\n#\n# From this data set we will compute/fit a Doc2Vec model to get\n# Document vectors.  From these document vectors, we will split the\n# documents into train/test and use these doc vectors to do sentiment\n# analysis on the movie review dataset.\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport os\nimport pickle\nimport string\nimport requests\nimport collections\nimport io\nimport tarfile\nimport urllib.request\nimport text_helpers\nfrom nltk.corpus import stopwords\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\nos.chdir(os.path.dirname(os.path.realpath(__file__)))\n\n# Make a saving directory if it doesn\'t exist\ndata_folder_name = \'temp\'\nif not os.path.exists(data_folder_name):\n    os.makedirs(data_folder_name)\n\n# Start a graph session\nsess = tf.Session()\n\n# Declare model parameters\nbatch_size = 500\nvocabulary_size = 7500\ngenerations = 100000\nmodel_learning_rate = 0.001\n\nembedding_size = 200   # Word embedding size\ndoc_embedding_size = 100   # Document embedding size\nconcatenated_size = embedding_size + doc_embedding_size\n\nnum_sampled = int(batch_size/2)    # Number of negative examples to sample.\nwindow_size = 3       # How many words to consider to the left.\n\n# Add checkpoints to training\nsave_embeddings_every = 5000\nprint_valid_every = 5000\nprint_loss_every = 100\n\n# Declare stop words\n#stops = stopwords.words(\'english\')\nstops = []\n\n# We pick a few test words for validation.\nvalid_words = [\'love\', \'hate\', \'happy\', \'sad\', \'man\', \'woman\']\n# Later we will have to transform these into indices\n\n# Load the movie review data\nprint(\'Loading Data\')\ntexts, target = text_helpers.load_movie_data()\n\n# Normalize text\nprint(\'Normalizing Text Data\')\ntexts = text_helpers.normalize_text(texts, stops)\n\n# Texts must contain at least 3 words\ntarget = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > window_size]\ntexts = [x for x in texts if len(x.split()) > window_size]    \nassert(len(target)==len(texts))\n\n# Build our data set and dictionaries\nprint(\'Creating Dictionary\')\nword_dictionary = text_helpers.build_dictionary(texts, vocabulary_size)\nword_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\ntext_data = text_helpers.text_to_numbers(texts, word_dictionary)\n\n# Get validation word keys\nvalid_examples = [word_dictionary[x] for x in valid_words]\n\nprint(\'Creating Model\')\n# Define Embeddings:\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\ndoc_embeddings = tf.Variable(tf.random_uniform([len(texts), doc_embedding_size], -1.0, 1.0))\n\n# NCE loss parameters\nnce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, concatenated_size],\n                                               stddev=1.0 / np.sqrt(concatenated_size)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n# Create data/target placeholders\nx_inputs = tf.placeholder(tf.int32, shape=[None, window_size + 1]) # plus 1 for doc index\ny_target = tf.placeholder(tf.int32, shape=[None, 1])\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n# Lookup the word embedding\n# Add together element embeddings in window:\nembed = tf.zeros([batch_size, embedding_size])\nfor element in range(window_size):\n    embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])\n\ndoc_indices = tf.slice(x_inputs, [0,window_size],[batch_size,1])\ndoc_embed = tf.nn.embedding_lookup(doc_embeddings,doc_indices)\n\n# concatenate embeddings\nfinal_embed = tf.concat(axis=1, values=[embed, tf.squeeze(doc_embed)])\n\n# Get loss from prediction\nloss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n                                     biases=nce_biases,\n                                     labels=y_target,\n                                     inputs=final_embed,\n                                     num_sampled=num_sampled,\n                                     num_classes=vocabulary_size))\n                                     \n# Create optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate)\ntrain_step = optimizer.minimize(loss)\n\n# Cosine similarity between words\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\nnormalized_embeddings = embeddings / norm\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\nsimilarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n\n# Create model saving operation\nsaver = tf.train.Saver({""embeddings"": embeddings, ""doc_embeddings"": doc_embeddings})\n\n#Add variable initializer.\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Run the doc2vec model.\nprint(\'Starting Training\')\nloss_vec = []\nloss_x_vec = []\nfor i in range(generations):\n    batch_inputs, batch_labels = text_helpers.generate_batch_data(text_data, batch_size,\n                                                                  window_size, method=\'doc2vec\')\n    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n\n    # Run the train step\n    sess.run(train_step, feed_dict=feed_dict)\n\n    # Return the loss\n    if (i+1) % print_loss_every == 0:\n        loss_val = sess.run(loss, feed_dict=feed_dict)\n        loss_vec.append(loss_val)\n        loss_x_vec.append(i+1)\n        print(\'Loss at step {} : {}\'.format(i+1, loss_val))\n      \n    # Validation: Print some random words and top 5 related words\n    if (i+1) % print_valid_every == 0:\n        sim = sess.run(similarity, feed_dict=feed_dict)\n        for j in range(len(valid_words)):\n            valid_word = word_dictionary_rev[valid_examples[j]]\n            top_k = 5 # number of nearest neighbors\n            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n            log_str = ""Nearest to {}:"".format(valid_word)\n            for k in range(top_k):\n                close_word = word_dictionary_rev[nearest[k]]\n                log_str = \'{} {},\'.format(log_str, close_word)\n            print(log_str)\n            \n    # Save dictionary + embeddings\n    if (i+1) % save_embeddings_every == 0:\n        # Save vocabulary dictionary\n        with open(os.path.join(data_folder_name,\'movie_vocab.pkl\'), \'wb\') as f:\n            pickle.dump(word_dictionary, f)\n        \n        # Save embeddings\n        model_checkpoint_path = os.path.join(os.getcwd(),data_folder_name,\'doc2vec_movie_embeddings.ckpt\')\n        save_path = saver.save(sess, model_checkpoint_path)\n        print(\'Model saved in file: {}\'.format(save_path))\n\n# Start logistic model-------------------------\nmax_words = 20\nlogistic_batch_size = 500\n\n# Split dataset into train and test sets\n# Need to keep the indices sorted to keep track of document index\ntrain_indices = np.sort(np.random.choice(len(target), round(0.8*len(target)), replace=False))\ntest_indices = np.sort(np.array(list(set(range(len(target))) - set(train_indices))))\ntexts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\ntexts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\ntarget_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\ntarget_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])\n\n# Convert texts to lists of indices\ntext_data_train = np.array(text_helpers.text_to_numbers(texts_train, word_dictionary))\ntext_data_test = np.array(text_helpers.text_to_numbers(texts_test, word_dictionary))\n\n# Pad/crop movie reviews to specific length\ntext_data_train = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_train]])\ntext_data_test = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_test]])\n\n# Define Logistic placeholders\nlog_x_inputs = tf.placeholder(tf.int32, shape=[None, max_words + 1]) # plus 1 for doc index\nlog_y_target = tf.placeholder(tf.int32, shape=[None, 1])\n\n# Define logistic embedding lookup (needed if we have two different batch sizes)\n# Add together element embeddings in window:\nlog_embed = tf.zeros([logistic_batch_size, embedding_size])\nfor element in range(max_words):\n    log_embed += tf.nn.embedding_lookup(embeddings, log_x_inputs[:, element])\n\nlog_doc_indices = tf.slice(log_x_inputs, [0,max_words],[logistic_batch_size,1])\nlog_doc_embed = tf.nn.embedding_lookup(doc_embeddings,log_doc_indices)\n\n# concatenate embeddings\nlog_final_embed = tf.concat(axis=1, values=[log_embed, tf.squeeze(log_doc_embed)])\n\n# Define model:\n# Create variables for logistic regression\nA = tf.Variable(tf.random_normal(shape=[concatenated_size,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n\n# Declare logistic model (sigmoid in loss function)\nmodel_output = tf.add(tf.matmul(log_final_embed, A), b)\n\n# Declare loss function (Cross Entropy loss)\nlogistic_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=tf.cast(log_y_target, tf.float32)))\n\n# Actual Prediction\nprediction = tf.round(tf.sigmoid(model_output))\npredictions_correct = tf.cast(tf.equal(prediction, tf.cast(log_y_target, tf.float32)), tf.float32)\naccuracy = tf.reduce_mean(predictions_correct)\n\n# Declare optimizer\nlogistic_opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\nlogistic_train_step = logistic_opt.minimize(logistic_loss, var_list=[A, b])\n\n# Intitialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Start Logistic Regression\nprint(\'Starting Logistic Doc2Vec Model Training\')\ntrain_loss = []\ntest_loss = []\ntrain_acc = []\ntest_acc = []\ni_data = []\nfor i in range(10000):\n    rand_index = np.random.choice(text_data_train.shape[0], size=logistic_batch_size)\n    rand_x = text_data_train[rand_index]\n    # Append review index at the end of text data\n    rand_x_doc_indices = train_indices[rand_index]\n    rand_x = np.hstack((rand_x, np.transpose([rand_x_doc_indices])))\n    rand_y = np.transpose([target_train[rand_index]])\n    \n    feed_dict = {log_x_inputs : rand_x, log_y_target : rand_y}\n    sess.run(logistic_train_step, feed_dict=feed_dict)\n    \n    # Only record loss and accuracy every 100 generations\n    if (i + 1) % 100 == 0:\n        rand_index_test = np.random.choice(text_data_test.shape[0], size=logistic_batch_size)\n        rand_x_test = text_data_test[rand_index_test]\n        # Append review index at the end of text data\n        rand_x_doc_indices_test = test_indices[rand_index_test]\n        rand_x_test = np.hstack((rand_x_test, np.transpose([rand_x_doc_indices_test])))\n        rand_y_test = np.transpose([target_test[rand_index_test]])\n        \n        test_feed_dict = {log_x_inputs: rand_x_test, log_y_target: rand_y_test}\n        \n        i_data.append(i+1)\n\n        train_loss_temp = sess.run(logistic_loss, feed_dict=feed_dict)\n        train_loss.append(train_loss_temp)\n        \n        test_loss_temp = sess.run(logistic_loss, feed_dict=test_feed_dict)\n        test_loss.append(test_loss_temp)\n        \n        train_acc_temp = sess.run(accuracy, feed_dict=feed_dict)\n        train_acc.append(train_acc_temp)\n    \n        test_acc_temp = sess.run(accuracy, feed_dict=test_feed_dict)\n        test_acc.append(test_acc_temp)\n    if (i + 1) % 500 == 0:\n        acc_and_loss = [i + 1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n        print(\'Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})\'.format(*acc_and_loss))\n\n# Plot loss over time\nplt.plot(i_data, train_loss, \'k-\', label=\'Train Loss\')\nplt.plot(i_data, test_loss, \'r--\', label=\'Test Loss\', linewidth=4)\nplt.title(\'Cross Entropy Loss per Generation\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Cross Entropy Loss\')\nplt.legend(loc=\'upper right\')\nplt.show()\n\n# Plot train and test accuracy\nplt.plot(i_data, train_acc, \'k-\', label=\'Train Set Accuracy\')\nplt.plot(i_data, test_acc, \'r--\', label=\'Test Set Accuracy\', linewidth=4)\nplt.title(\'Train and Test Accuracy\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Accuracy\')\nplt.legend(loc=\'lower right\')\nplt.show()'"
07_Natural_Language_Processing/07_Sentiment_Analysis_With_Doc2Vec/text_helpers.py,0,"b'# Text Helper Functions\n#---------------------------------------\n#\n# We pull out text helper functions to reduce redundant code\n\nimport string\nimport os\nimport urllib.request\nimport io\nimport tarfile\nimport collections\nimport numpy as np\nimport requests\nimport gzip\n\n# Normalize text\ndef normalize_text(texts, stops):\n    # Lower case\n    texts = [x.lower() for x in texts]\n\n    # Remove punctuation\n    texts = [\'\'.join(c for c in x if c not in string.punctuation) for x in texts]\n\n    # Remove numbers\n    texts = [\'\'.join(c for c in x if c not in \'0123456789\') for x in texts]\n\n    # Remove stopwords\n    texts = [\' \'.join([word for word in x.split() if word not in (stops)]) for x in texts]\n\n    # Trim extra whitespace\n    texts = [\' \'.join(x.split()) for x in texts]\n    \n    return(texts)\n\n\n# Build dictionary of words\ndef build_dictionary(sentences, vocabulary_size):\n    # Turn sentences (list of strings) into lists of words\n    split_sentences = [s.split() for s in sentences]\n    words = [x for sublist in split_sentences for x in sublist]\n    \n    # Initialize list of [word, word_count] for each word, starting with unknown\n    count = [[\'RARE\', -1]]\n    \n    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n    \n    # Now create the dictionary\n    word_dict = {}\n    # For each word, that we want in the dictionary, add it, then make it\n    # the value of the prior dictionary length\n    for word, word_count in count:\n        word_dict[word] = len(word_dict)\n    \n    return(word_dict)\n    \n\n# Turn text data into lists of integers from dictionary\ndef text_to_numbers(sentences, word_dict):\n    # Initialize the returned data\n    data = []\n    for sentence in sentences:\n        sentence_data = []\n        # For each word, either use selected index or rare word index\n        for word in sentence.split():\n            if word in word_dict:\n                word_ix = word_dict[word]\n            else:\n                word_ix = 0\n            sentence_data.append(word_ix)\n        data.append(sentence_data)\n    return(data)\n    \n\n# Generate data randomly (N words behind, target, N words ahead)\ndef generate_batch_data(sentences, batch_size, window_size, method=\'skip_gram\'):\n    # Fill up data batch\n    batch_data = []\n    label_data = []\n    while len(batch_data) < batch_size:\n        # select random sentence to start\n        rand_sentence_ix = int(np.random.choice(len(sentences), size=1))\n        rand_sentence = sentences[rand_sentence_ix]\n        # Generate consecutive windows to look at\n        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n        # Denote which element of each window is the center word of interest\n        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n        \n        # Pull out center word of interest for each window and create a tuple for each window\n        if method==\'skip_gram\':\n            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n            # Make it in to a big list of tuples (target word, surrounding word)\n            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n            batch, labels = [list(x) for x in zip(*tuple_data)]\n        elif method==\'cbow\':\n            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n            # Only keep windows with consistent 2*window_size\n            batch_and_labels = [(x,y) for x,y in batch_and_labels if len(x)==2*window_size]\n            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n        elif method==\'doc2vec\':\n            # For doc2vec we keep LHS window only to predict target word\n            batch_and_labels = [(rand_sentence[i:i+window_size], rand_sentence[i+window_size]) for i in range(0, len(rand_sentence)-window_size)]\n            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n            # Add document index to batch!! Remember that we must extract the last index in batch for the doc-index\n            batch = [x + [rand_sentence_ix] for x in batch]\n        else:\n            raise ValueError(\'Method {} not implemented yet.\'.format(method))\n            \n        # extract batch and labels\n        batch_data.extend(batch[:batch_size])\n        label_data.extend(labels[:batch_size])\n    # Trim batch and label at the end\n    batch_data = batch_data[:batch_size]\n    label_data = label_data[:batch_size]\n    \n    # Convert to numpy array\n    batch_data = np.array(batch_data)\n    label_data = np.transpose(np.array([label_data]))\n    \n    return(batch_data, label_data)\n    \n    \n# Load the movie review data\n# Check if data was downloaded, otherwise download it and save for future use\ndef load_movie_data():\n    save_folder_name = \'temp\'\n    pos_file = os.path.join(save_folder_name, \'rt-polaritydata\', \'rt-polarity.pos\')\n    neg_file = os.path.join(save_folder_name, \'rt-polaritydata\', \'rt-polarity.neg\')\n\n    # Check if files are already downloaded\n    if not os.path.exists(os.path.join(save_folder_name, \'rt-polaritydata\')):\n        movie_data_url = \'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\'\n\n        # Save tar.gz file\n        req = requests.get(movie_data_url, stream=True)\n        with open(\'temp_movie_review_temp.tar.gz\', \'wb\') as f:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        # Extract tar.gz file into temp folder\n        tar = tarfile.open(\'temp_movie_review_temp.tar.gz\', ""r:gz"")\n        tar.extractall(path=\'temp\')\n        tar.close()\n\n    pos_data = []\n    with open(pos_file, \'r\', encoding=\'latin-1\') as f:\n        for line in f:\n            pos_data.append(line.encode(\'ascii\',errors=\'ignore\').decode())\n    f.close()\n    pos_data = [x.rstrip() for x in pos_data]\n\n    neg_data = []\n    with open(neg_file, \'r\', encoding=\'latin-1\') as f:\n        for line in f:\n            neg_data.append(line.encode(\'ascii\',errors=\'ignore\').decode())\n    f.close()\n    neg_data = [x.rstrip() for x in neg_data]\n    \n    texts = pos_data + neg_data\n    target = [1]*len(pos_data) + [0]*len(neg_data)\n    \n    return(texts, target)\n'"
08_Convolutional_Neural_Networks/02_Intro_to_CNN_MNIST/02_introductory_cnn.py,31,"b""# Introductory CNN Model: MNIST Digits\n# ---------------------------------------\n#\n# In this example, we will download the MNIST handwritten\n# digits and create a simple CNN network to predict the\n# digit category (0-9)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a graph session\nsess = tf.Session()\n\n# Load data\ndata_dir = 'temp'\nmnist = input_data.read_data_sets(data_dir, one_hot=False)\n\n# Convert images into 28x28 (they are downloaded as 1x784)\ntrain_xdata = np.array([np.reshape(x, (28, 28)) for x in mnist.train.images])\ntest_xdata = np.array([np.reshape(x, (28, 28)) for x in mnist.test.images])\n\n# Convert labels into one-hot encoded vectors\ntrain_labels = mnist.train.labels\ntest_labels = mnist.test.labels\n\n# Set model parameters\nbatch_size = 100\nlearning_rate = 0.005\nevaluation_size = 500\nimage_width = train_xdata[0].shape[0]\nimage_height = train_xdata[0].shape[1]\ntarget_size = np.max(train_labels) + 1\nnum_channels = 1  # greyscale = 1 channel\ngenerations = 500\neval_every = 5\nconv1_features = 25\nconv2_features = 50\nmax_pool_size1 = 2  # NxN window for 1st max pool layer\nmax_pool_size2 = 2  # NxN window for 2nd max pool layer\nfully_connected_size1 = 100\n\n# Declare model placeholders\nx_input_shape = (batch_size, image_width, image_height, num_channels)\nx_input = tf.placeholder(tf.float32, shape=x_input_shape)\ny_target = tf.placeholder(tf.int32, shape=(batch_size))\neval_input_shape = (evaluation_size, image_width, image_height, num_channels)\neval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\neval_target = tf.placeholder(tf.int32, shape=(evaluation_size))\n\n# Declare model parameters\nconv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features],\n                                               stddev=0.1, dtype=tf.float32))\nconv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n\nconv2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features],\n                                               stddev=0.1, dtype=tf.float32))\nconv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))\n\n# fully connected variables\nresulting_width = image_width // (max_pool_size1 * max_pool_size2)\nresulting_height = image_height // (max_pool_size1 * max_pool_size2)\nfull1_input_size = resulting_width * resulting_height * conv2_features\nfull1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1],\n                           stddev=0.1, dtype=tf.float32))\nfull1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\nfull2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size],\n                                               stddev=0.1, dtype=tf.float32))\nfull2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n\n\n# Initialize Model Operations\ndef my_conv_net(conv_input_data):\n    # First Conv-ReLU-MaxPool Layer\n    conv1 = tf.nn.conv2d(conv_input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')\n    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],\n                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n\n    # Second Conv-ReLU-MaxPool Layer\n    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')\n    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],\n                               strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')\n\n    # Transform Output into a 1xN layer for next fully connected layer\n    final_conv_shape = max_pool2.get_shape().as_list()\n    final_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]\n    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])\n\n    # First Fully Connected Layer\n    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n\n    # Second Fully Connected Layer\n    final_model_output = tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)\n    \n    return final_model_output\n\nmodel_output = my_conv_net(x_input)\ntest_model_output = my_conv_net(eval_input)\n\n# Declare Loss Function (softmax cross entropy)\nloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))\n\n# Create a prediction function\nprediction = tf.nn.softmax(model_output)\ntest_prediction = tf.nn.softmax(test_model_output)\n\n\n# Create accuracy function\ndef get_accuracy(logits, targets):\n    batch_predictions = np.argmax(logits, axis=1)\n    num_correct = np.sum(np.equal(batch_predictions, targets))\n    return 100. * num_correct/batch_predictions.shape[0]\n\n# Create an optimizer\nmy_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\ntrain_step = my_optimizer.minimize(loss)\n\n# Initialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Start training loop\ntrain_loss = []\ntrain_acc = []\ntest_acc = []\nfor i in range(generations):\n    rand_index = np.random.choice(len(train_xdata), size=batch_size)\n    rand_x = train_xdata[rand_index]\n    rand_x = np.expand_dims(rand_x, 3)\n    rand_y = train_labels[rand_index]\n    train_dict = {x_input: rand_x, y_target: rand_y}\n    \n    sess.run(train_step, feed_dict=train_dict)\n    temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n    temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n    \n    if (i+1) % eval_every == 0:\n        eval_index = np.random.choice(len(test_xdata), size=evaluation_size)\n        eval_x = test_xdata[eval_index]\n        eval_x = np.expand_dims(eval_x, 3)\n        eval_y = test_labels[eval_index]\n        test_dict = {eval_input: eval_x, eval_target: eval_y}\n        test_preds = sess.run(test_prediction, feed_dict=test_dict)\n        temp_test_acc = get_accuracy(test_preds, eval_y)\n        \n        # Record and print results\n        train_loss.append(temp_train_loss)\n        train_acc.append(temp_train_acc)\n        test_acc.append(temp_test_acc)\n        acc_and_loss = [(i+1), temp_train_loss, temp_train_acc, temp_test_acc]\n        acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n        print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n    \n    \n# Matlotlib code to plot the loss and accuracies\neval_indices = range(0, generations, eval_every)\n# Plot loss over time\nplt.plot(eval_indices, train_loss, 'k-')\nplt.title('Softmax Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Softmax Loss')\nplt.show()\n\n# Plot train and test accuracy\nplt.plot(eval_indices, train_acc, 'k-', label='Train Set Accuracy')\nplt.plot(eval_indices, test_acc, 'r--', label='Test Set Accuracy')\nplt.title('Train and Test Accuracy')\nplt.xlabel('Generation')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n# Plot some samples\n# Plot the 6 of the last batch results:\nactuals = rand_y[0:6]\npredictions = np.argmax(temp_train_preds, axis=1)[0:6]\nimages = np.squeeze(rand_x[0:6])\n\nNrows = 2\nNcols = 3\nfor i in range(6):\n    plt.subplot(Nrows, Ncols, i+1)\n    plt.imshow(np.reshape(images[i], [28, 28]), cmap='Greys_r')\n    plt.title('Actual: ' + str(actuals[i]) + ' Pred: ' + str(predictions[i]),\n              fontsize=10)\n    frame = plt.gca()\n    frame.axes.get_xaxis().set_visible(False)\n    frame.axes.get_yaxis().set_visible(False)\n"""
08_Convolutional_Neural_Networks/03_CNN_CIFAR10/03_cnn_cifar10.py,58,"b'# More Advanced CNN Model: CIFAR-10\n#---------------------------------------\n#\n# In this example, we will download the CIFAR-10 images\n# and build a CNN model with dropout and regularization\n#\n# CIFAR is composed ot 50k train and 10k test\n# images that are 32x32.\n\nimport os\nimport sys\nimport tarfile\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import urllib\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Change Directory\ntry:\n    abspath = os.path.abspath(__file__)\nexcept NameError:\n    abspath = os.getcwd()\ndname = os.path.dirname(abspath)\nos.chdir(dname)\n\n# Start a graph session\nsess = tf.Session()\n\n# Set model parameters\nbatch_size = 128\ndata_dir = \'temp\'\noutput_every = 50\ngenerations = 20000\neval_every = 500\nimage_height = 32\nimage_width = 32\ncrop_height = 24\ncrop_width = 24\nnum_channels = 3\nnum_targets = 10\nextract_folder = \'cifar-10-batches-bin\'\n\n# Exponential Learning Rate Decay Params\nlearning_rate = 0.1\nlr_decay = 0.1\nnum_gens_to_wait = 250.\n\n# Extract model parameters\nimage_vec_length = image_height * image_width * num_channels\nrecord_length = 1 + image_vec_length # ( + 1 for the 0-9 label)\n\n# Load data\ndata_dir = \'temp\'\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\ncifar10_url = \'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\'\n\n# Check if file exists, otherwise download it\ndata_file = os.path.join(data_dir, \'cifar-10-binary.tar.gz\')\nif os.path.isfile(data_file):\n    pass\nelse:\n    # Download file\n    def progress(block_num, block_size, total_size):\n        progress_info = [cifar10_url, float(block_num * block_size) / float(total_size) * 100.0]\n        print(\'\\r Downloading {} - {:.2f}%\'.format(*progress_info), end="""")\n    filepath, _ = urllib.request.urlretrieve(cifar10_url, data_file, progress)\n    # Extract file\n    tarfile.open(filepath, \'r:gz\').extractall(data_dir)\n    \n\n# Define CIFAR reader\ndef read_cifar_files(filename_queue, distort_images = True):\n    reader = tf.FixedLengthRecordReader(record_bytes=record_length)\n    key, record_string = reader.read(filename_queue)\n    record_bytes = tf.decode_raw(record_string, tf.uint8)\n    image_label = tf.cast(tf.slice(record_bytes, [0], [1]), tf.int32)\n  \n    # Extract image\n    image_extracted = tf.reshape(tf.slice(record_bytes, [1], [image_vec_length]),\n                                 [num_channels, image_height, image_width])\n    \n    # Reshape image\n    image_uint8image = tf.transpose(image_extracted, [1, 2, 0])\n    reshaped_image = tf.cast(image_uint8image, tf.float32)\n    # Randomly Crop image\n    final_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, crop_width, crop_height)\n    \n    if distort_images:\n        # Randomly flip the image horizontally, change the brightness and contrast\n        final_image = tf.image.random_flip_left_right(final_image)\n        final_image = tf.image.random_brightness(final_image,max_delta=63)\n        final_image = tf.image.random_contrast(final_image,lower=0.2, upper=1.8)\n\n    # Normalize whitening\n    final_image = tf.image.per_image_standardization(final_image)\n    return final_image, image_label\n\n\n# Create a CIFAR image pipeline from reader\ndef input_pipeline(batch_size, train_logical=True):\n    if train_logical:\n        files = [os.path.join(data_dir, extract_folder, \'data_batch_{}.bin\'.format(i)) for i in range(1,6)]\n    else:\n        files = [os.path.join(data_dir, extract_folder, \'test_batch.bin\')]\n    filename_queue = tf.train.string_input_producer(files)\n    image, label = read_cifar_files(filename_queue)\n    \n    # min_after_dequeue defines how big a buffer we will randomly sample\n    #   from -- bigger means better shuffling but slower start up and more\n    #   memory used.\n    # capacity must be larger than min_after_dequeue and the amount larger\n    #   determines the maximum we will prefetch.  Recommendation:\n    #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n    min_after_dequeue = 5000\n    capacity = min_after_dequeue + 3 * batch_size\n    example_batch, label_batch = tf.train.shuffle_batch([image, label],\n                                                        batch_size=batch_size,\n                                                        capacity=capacity,\n                                                        min_after_dequeue=min_after_dequeue)\n\n    return example_batch, label_batch\n\n    \n# Define the model architecture, this will return logits from images\ndef cifar_cnn_model(input_images, batch_size, train_logical=True):\n    def truncated_normal_var(name, shape, dtype):\n        return(tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.truncated_normal_initializer(stddev=0.05)))\n    def zero_var(name, shape, dtype):\n        return(tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.constant_initializer(0.0)))\n    \n    # First Convolutional Layer\n    with tf.variable_scope(\'conv1\') as scope:\n        # Conv_kernel is 5x5 for all 3 colors and we will create 64 features\n        conv1_kernel = truncated_normal_var(name=\'conv_kernel1\', shape=[5, 5, 3, 64], dtype=tf.float32)\n        # We convolve across the image with a stride size of 1\n        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1, 1, 1, 1], padding=\'SAME\')\n        # Initialize and add the bias term\n        conv1_bias = zero_var(name=\'conv_bias1\', shape=[64], dtype=tf.float32)\n        conv1_add_bias = tf.nn.bias_add(conv1, conv1_bias)\n        # ReLU element wise\n        relu_conv1 = tf.nn.relu(conv1_add_bias)\n    \n    # Max Pooling\n    pool1 = tf.nn.max_pool(relu_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],padding=\'SAME\', name=\'pool_layer1\')\n    \n    # Local Response Normalization (parameters from paper)\n    # paper: http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\'norm1\')\n\n    # Second Convolutional Layer\n    with tf.variable_scope(\'conv2\') as scope:\n        # Conv kernel is 5x5, across all prior 64 features and we create 64 more features\n        conv2_kernel = truncated_normal_var(name=\'conv_kernel2\', shape=[5, 5, 64, 64], dtype=tf.float32)\n        # Convolve filter across prior output with stride size of 1\n        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1, 1, 1, 1], padding=\'SAME\')\n        # Initialize and add the bias\n        conv2_bias = zero_var(name=\'conv_bias2\', shape=[64], dtype=tf.float32)\n        conv2_add_bias = tf.nn.bias_add(conv2, conv2_bias)\n        # ReLU element wise\n        relu_conv2 = tf.nn.relu(conv2_add_bias)\n    \n    # Max Pooling\n    pool2 = tf.nn.max_pool(relu_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool_layer2\')    \n    \n     # Local Response Normalization (parameters from paper)\n    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\'norm2\')\n    \n    # Reshape output into a single matrix for multiplication for the fully connected layers\n    reshaped_output = tf.reshape(norm2, [batch_size, -1])\n    reshaped_dim = reshaped_output.get_shape()[1].value\n    \n    # First Fully Connected Layer\n    with tf.variable_scope(\'full1\') as scope:\n        # Fully connected layer will have 384 outputs.\n        full_weight1 = truncated_normal_var(name=\'full_mult1\', shape=[reshaped_dim, 384], dtype=tf.float32)\n        full_bias1 = zero_var(name=\'full_bias1\', shape=[384], dtype=tf.float32)\n        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, full_weight1), full_bias1))\n\n    # Second Fully Connected Layer\n    with tf.variable_scope(\'full2\') as scope:\n        # Second fully connected layer has 192 outputs.\n        full_weight2 = truncated_normal_var(name=\'full_mult2\', shape=[384, 192], dtype=tf.float32)\n        full_bias2 = zero_var(name=\'full_bias2\', shape=[192], dtype=tf.float32)\n        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, full_weight2), full_bias2))\n\n    # Final Fully Connected Layer -> 10 categories for output (num_targets)\n    with tf.variable_scope(\'full3\') as scope:\n        # Final fully connected layer has 10 (num_targets) outputs.\n        full_weight3 = truncated_normal_var(name=\'full_mult3\', shape=[192, num_targets], dtype=tf.float32)\n        full_bias3 =  zero_var(name=\'full_bias3\', shape=[num_targets], dtype=tf.float32)\n        final_output = tf.add(tf.matmul(full_layer2, full_weight3), full_bias3)\n        \n    return final_output\n\n\n# Loss function\ndef cifar_loss(logits, targets):\n    # Get rid of extra dimensions and cast targets into integers\n    targets = tf.squeeze(tf.cast(targets, tf.int32))\n    # Calculate cross entropy from logits and targets\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n    # Take the average loss across batch size\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n    return cross_entropy_mean\n\n\n# Train step\ndef train_step(loss_value, generation_num):\n    # Our learning rate is an exponential decay after we wait a fair number of generations\n    model_learning_rate = tf.train.exponential_decay(learning_rate, generation_num,\n                                                     num_gens_to_wait, lr_decay, staircase=True)\n    # Create optimizer\n    my_optimizer = tf.train.GradientDescentOptimizer(model_learning_rate)\n    # Initialize train step\n    train_step = my_optimizer.minimize(loss_value)\n    return train_step\n\n\n# Accuracy function\ndef accuracy_of_batch(logits, targets):\n    # Make sure targets are integers and drop extra dimensions\n    targets = tf.squeeze(tf.cast(targets, tf.int32))\n    # Get predicted values by finding which logit is the greatest\n    batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n    # Check if they are equal across the batch\n    predicted_correctly = tf.equal(batch_predictions, targets)\n    # Average the 1\'s and 0\'s (True\'s and False\'s) across the batch size\n    accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32))\n    return accuracy\n\n# Get data\nprint(\'Getting/Transforming Data.\')\n# Initialize the data pipeline\nimages, targets = input_pipeline(batch_size, train_logical=True)\n# Get batch test images and targets from pipline\ntest_images, test_targets = input_pipeline(batch_size, train_logical=False)\n\n# Declare Model\nprint(\'Creating the CIFAR10 Model.\')\nwith tf.variable_scope(\'model_definition\') as scope:\n    # Declare the training network model\n    model_output = cifar_cnn_model(images, batch_size)\n    # This is very important!!!  We must set the scope to REUSE the variables,\n    #  otherwise, when we set the test network model, it will create new random\n    #  variables.  Otherwise we get random evaluations on the test batches.\n    scope.reuse_variables()\n    test_output = cifar_cnn_model(test_images, batch_size)\n\n# Declare loss function\nprint(\'Declare Loss Function.\')\nloss = cifar_loss(model_output, targets)\n\n# Create accuracy function\naccuracy = accuracy_of_batch(test_output, test_targets)\n\n# Create training operations\nprint(\'Creating the Training Operation.\')\ngeneration_num = tf.Variable(0, trainable=False)\ntrain_op = train_step(loss, generation_num)\n\n# Initialize Variables\nprint(\'Initializing the Variables.\')\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Initialize queue (This queue will feed into the model, so no placeholders necessary)\ntf.train.start_queue_runners(sess=sess)\n\n# Train CIFAR Model\nprint(\'Starting Training\')\ntrain_loss = []\ntest_accuracy = []\nfor i in range(generations):\n    _, loss_value = sess.run([train_op, loss])\n    \n    if (i+1) % output_every == 0:\n        train_loss.append(loss_value)\n        output = \'Generation {}: Loss = {:.5f}\'.format((i+1), loss_value)\n        print(output)\n    \n    if (i+1) % eval_every == 0:\n        [temp_accuracy] = sess.run([accuracy])\n        test_accuracy.append(temp_accuracy)\n        acc_output = \' --- Test Accuracy = {:.2f}%.\'.format(100.*temp_accuracy)\n        print(acc_output)\n\n# Print loss and accuracy\n# Matlotlib code to plot the loss and accuracies\neval_indices = range(0, generations, eval_every)\noutput_indices = range(0, generations, output_every)\n\n# Plot loss over time\nplt.plot(output_indices, train_loss, \'k-\')\nplt.title(\'Softmax Loss per Generation\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Softmax Loss\')\nplt.show()\n\n# Plot accuracy over time\nplt.plot(eval_indices, test_accuracy, \'k-\')\nplt.title(\'Test Accuracy\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Accuracy\')\nplt.show()'"
08_Convolutional_Neural_Networks/04_Retraining_Current_Architectures/04_download_cifar10.py,0,"b'# Download/Saving CIFAR-10 images in Inception format\n#---------------------------------------\n#\n# In this script, we download the CIFAR-10 images and\n# transform/save them in the Inception Retraining Format\n#\n# The end purpose of the files is for re-training the\n# Google Inception tensorflow model to work on the CIFAR-10.\n\nimport os\nimport tarfile\nimport _pickle as cPickle\nimport numpy as np\nimport urllib.request\nimport scipy.misc\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\ncifar_link = \'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\ndata_dir = \'temp\'\nif not os.path.isdir(data_dir):\n    os.makedirs(data_dir)\n\n# Download tar file\ntarget_file = os.path.join(data_dir, \'cifar-10-python.tar.gz\')\nif not os.path.isfile(target_file):\n    print(\'CIFAR-10 file not found. Downloading CIFAR data (Size = 163MB)\')\n    print(\'This may take a few minutes, please wait.\')\n    filename, headers = urllib.request.urlretrieve(cifar_link, target_file)\n\n# Extract into memory\ntar = tarfile.open(target_file)\ntar.extractall(path=data_dir)\ntar.close()\nobjects = [\'airplane\', \'automobile\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\']\n\n# Create train image folders\ntrain_folder = \'train_dir\'\nif not os.path.isdir(os.path.join(data_dir, train_folder)):\n    for i in range(10):\n        folder = os.path.join(data_dir, train_folder, objects[i])\n        os.makedirs(folder)\n# Create test image folders\ntest_folder = \'validation_dir\'\nif not os.path.isdir(os.path.join(data_dir, test_folder)):\n    for i in range(10):\n        folder = os.path.join(data_dir, test_folder, objects[i])\n        os.makedirs(folder)\n\n# Extract images accordingly\ndata_location = os.path.join(data_dir, \'cifar-10-batches-py\')\ntrain_names = [\'data_batch_\' + str(x) for x in range(1,6)]\ntest_names = [\'test_batch\']\n\n\ndef load_batch_from_file(file):\n    file_conn = open(file, \'rb\')\n    image_dictionary = cPickle.load(file_conn, encoding=\'latin1\')\n    file_conn.close()\n    return image_dictionary\n\n\ndef save_images_from_dict(image_dict, folder=\'data_dir\'):\n    # image_dict.keys() = \'labels\', \'filenames\', \'data\', \'batch_label\'\n    for ix, label in enumerate(image_dict[\'labels\']):\n        folder_path = os.path.join(data_dir, folder, objects[label])\n        filename = image_dict[\'filenames\'][ix]\n        #Transform image data\n        image_array = image_dict[\'data\'][ix]\n        image_array.resize([3, 32, 32])\n        # Save image\n        output_location = os.path.join(folder_path, filename)\n        scipy.misc.imsave(output_location,image_array.transpose())\n\n# Sort train images\nfor file in train_names:\n    print(\'Saving images from file: {}\'.format(file))\n    file_location = os.path.join(data_dir, \'cifar-10-batches-py\', file)\n    image_dict = load_batch_from_file(file_location)\n    save_images_from_dict(image_dict, folder=train_folder)\n\n# Sort test images\nfor file in test_names:\n    print(\'Saving images from file: {}\'.format(file))\n    file_location = os.path.join(data_dir, \'cifar-10-batches-py\', file)\n    image_dict = load_batch_from_file(file_location)\n    save_images_from_dict(image_dict, folder=test_folder)\n    \n# Create labels file\ncifar_labels_file = os.path.join(data_dir,\'cifar10_labels.txt\')\nprint(\'Writing labels file, {}\'.format(cifar_labels_file))\nwith open(cifar_labels_file, \'w\') as labels_file:\n    for item in objects:\n        labels_file.write(""{}\\n"".format(item))\n\n# After this is done, we proceed with the TensorFlow fine-tuning tutorial.\n\n# https://www.tensorflow.org/tutorials/image_retraining\n'"
08_Convolutional_Neural_Networks/05_Stylenet_NeuralStyle/05_stylenet.py,23,"b""# Using TensorFlow for Stylenet/NeuralStyle\n#---------------------------------------\n#\n# We use two images, an original image and a style image\n# and try to make the original image in the style of the style image.\n#\n# Reference paper:\n# https://arxiv.org/abs/1508.06576\n#\n# Need to download the model 'imagenet-vgg-verydee-19.mat' from:\n#   http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\n\nimport os\nimport scipy.io\nimport scipy.misc\nimport imageio\nfrom skimage.transform import resize\nfrom operator import mul\nfrom functools import reduce\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Image Files\noriginal_image_file = 'images/book_cover.jpg'\nstyle_image_file = 'images/starry_night.jpg'\n\n# Saved VGG Network path under the current project dir.\nvgg_path = 'imagenet-vgg-verydeep-19.mat'\n\n# Default Arguments\noriginal_image_weight = 5.0\nstyle_image_weight = 500.0\nregularization_weight = 100\nlearning_rate = 10\ngenerations = 100\noutput_generations = 25\nbeta1 = 0.9\nbeta2 = 0.999\n\n# Read in images\noriginal_image = imageio.imread(original_image_file)\nstyle_image = imageio.imread(style_image_file)\n\n# Get shape of target and make the style image the same\ntarget_shape = original_image.shape\nstyle_image = resize(style_image, target_shape)\n\n# VGG-19 Layer Setup\n# From paper\nvgg_layers = ['conv1_1', 'relu1_1',\n              'conv1_2', 'relu1_2', 'pool1',\n              'conv2_1', 'relu2_1',\n              'conv2_2', 'relu2_2', 'pool2',\n              'conv3_1', 'relu3_1',\n              'conv3_2', 'relu3_2',\n              'conv3_3', 'relu3_3',\n              'conv3_4', 'relu3_4', 'pool3',\n              'conv4_1', 'relu4_1',\n              'conv4_2', 'relu4_2',\n              'conv4_3', 'relu4_3',\n              'conv4_4', 'relu4_4', 'pool4',\n              'conv5_1', 'relu5_1',\n              'conv5_2', 'relu5_2',\n              'conv5_3', 'relu5_3',\n              'conv5_4', 'relu5_4']\n\n\n# Extract weights and matrix means\ndef extract_net_info(path_to_params):\n    vgg_data = scipy.io.loadmat(path_to_params)\n    normalization_matrix = vgg_data['normalization'][0][0][0]\n    mat_mean = np.mean(normalization_matrix, axis=(0,1))\n    network_weights = vgg_data['layers'][0]\n    return mat_mean, network_weights\n    \n\n# Create the VGG-19 Network\ndef vgg_network(network_weights, init_image):\n    network = {}\n    image = init_image\n\n    for i, layer in enumerate(vgg_layers):\n        if layer[0] == 'c':\n            weights, bias = network_weights[i][0][0][0][0]\n            weights = np.transpose(weights, (1, 0, 2, 3))\n            bias = bias.reshape(-1)\n            conv_layer = tf.nn.conv2d(image, tf.constant(weights), (1, 1, 1, 1), 'SAME')\n            image = tf.nn.bias_add(conv_layer, bias)\n        elif layer[0] == 'r':\n            image = tf.nn.relu(image)\n        else:  # pooling\n            image = tf.nn.max_pool(image, (1, 2, 2, 1), (1, 2, 2, 1), 'SAME')\n        network[layer] = image\n    return network\n\n# Here we define which layers apply to the original or style image\noriginal_layers = ['relu4_2', 'relu5_2']\nstyle_layers = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n\n# Get network parameters\nnormalization_mean, network_weights = extract_net_info(vgg_path)\n\nshape = (1,) + original_image.shape\nstyle_shape = (1,) + style_image.shape\noriginal_features = {}\nstyle_features = {}\n\n# Set style weights\nstyle_weights = {l: 1./(len(style_layers)) for l in style_layers}\n\n# Computer feature layers with original image\ng_original = tf.Graph()\nwith g_original.as_default(), tf.Session() as sess1:\n    image = tf.placeholder('float', shape=shape)\n    vgg_net = vgg_network(network_weights, image)\n    original_minus_mean = original_image - normalization_mean\n    original_norm = np.array([original_minus_mean])\n    for layer in original_layers:\n        original_features[layer] = vgg_net[layer].eval(feed_dict={image: original_norm})\n\n# Get style image network\ng_style = tf.Graph()\nwith g_style.as_default(), tf.Session() as sess2:\n    image = tf.placeholder('float', shape=style_shape)\n    vgg_net = vgg_network(network_weights, image)\n    style_minus_mean = style_image - normalization_mean\n    style_norm = np.array([style_minus_mean])\n    for layer in style_layers:\n        features = vgg_net[layer].eval(feed_dict={image: style_norm})\n        features = np.reshape(features, (-1, features.shape[3]))\n        gram = np.matmul(features.T, features) / features.size\n        style_features[layer] = gram\n\n# Make Combined Image via loss function\nwith tf.Graph().as_default():\n    # Get network parameters\n    initial = tf.random_normal(shape) * 0.256\n    init_image = tf.Variable(initial)\n    vgg_net = vgg_network(network_weights, init_image)\n\n    # Loss from Original Image\n    original_layers_w = {'relu4_2': 0.5, 'relu5_2': 0.5}\n    original_loss = 0\n    for o_layer in original_layers:\n        temp_original_loss = original_layers_w[o_layer] * original_image_weight *\\\n                             (2 * tf.nn.l2_loss(vgg_net[o_layer] - original_features[o_layer]))\n        original_loss += (temp_original_loss / original_features[o_layer].size)\n\n    # Loss from Style Image\n    style_loss = 0\n    style_losses = []\n    for style_layer in style_layers:\n        layer = vgg_net[style_layer]\n        feats, height, width, channels = [x.value for x in layer.get_shape()]\n        size = height * width * channels\n        features = tf.reshape(layer, (-1, channels))\n        style_gram_matrix = tf.matmul(tf.transpose(features), features) / size\n        style_expected = style_features[style_layer]\n        style_losses.append(style_weights[style_layer] * 2 *\n                            tf.nn.l2_loss(style_gram_matrix - style_expected) /\n                            style_expected.size)\n    style_loss += style_image_weight * tf.reduce_sum(style_losses)\n\n    # To Smooth the results, we add in total variation loss\n    total_var_x = reduce(mul, init_image[:, 1:, :, :].get_shape().as_list(), 1)\n    total_var_y = reduce(mul, init_image[:, :, 1:, :].get_shape().as_list(), 1)\n    first_term = regularization_weight * 2\n    second_term_numerator = tf.nn.l2_loss(init_image[:, 1:, :, :] - init_image[:, :shape[1]-1, :, :])\n    second_term = second_term_numerator / total_var_y\n    third_term = (tf.nn.l2_loss(init_image[:, :, 1:, :] - init_image[:, :, :shape[2]-1, :]) / total_var_x)\n    total_variation_loss = first_term * (second_term + third_term)\n\n    # Combined Loss\n    loss = original_loss + style_loss + total_variation_loss\n\n    # Declare Optimization Algorithm\n    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2)\n    train_step = optimizer.minimize(loss)\n\n    # Initialize variables and start training\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n        for i in range(generations):\n\n            train_step.run()\n\n            # Print update and save temporary output\n            if (i+1) % output_generations == 0:\n                print('Generation {} out of {}, loss: {}'.format(i + 1, generations,sess.run(loss)))\n                image_eval = init_image.eval()\n                best_image_add_mean = image_eval.reshape(shape[1:]) + normalization_mean\n                output_file = 'temp_output_{}.jpg'.format(i)\n                imageio.imwrite(output_file, best_image_add_mean)\n        \n        \n        # Save final image\n        image_eval = init_image.eval()\n        best_image_add_mean = image_eval.reshape(shape[1:]) + normalization_mean\n        output_file = 'final_output.jpg'\n        scipy.misc.imsave(output_file, best_image_add_mean)"""
08_Convolutional_Neural_Networks/06_Deepdream/06_deepdream.py,12,"b'# Using TensorFlow for Deep Dream\n#---------------------------------------\n# From: Alexander Mordvintsev\n#      --https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream\n#\n# Make sure to download the deep dream model here:\n#   https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\n#\n# Run:\n#  me@computer:~$ wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip \n#  me@computer:~$ unzip inception5h.zip\n#\n#  More comments added inline.\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nimport tensorflow as tf\nfrom io import BytesIO\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a graph session\ngraph = tf.Graph()\nsess = tf.InteractiveSession(graph=graph)\n\nos.chdir(\'~/Documents/tensorflow/inception-v1-model/\')\n\n# Model filename\nmodel_fn = \'tensorflow_inception_graph.pb\'\n\n# Load graph parameters\nwith tf.gfile.FastGFile(model_fn, \'rb\') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n\n# Create placeholder for input\nt_input = tf.placeholder(np.float32, name=\'input\')\n\n# Imagenet average bias to subtract off images\nimagenet_mean = 117.0\nt_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\ntf.import_graph_def(graph_def, {\'input\':t_preprocessed})\n\n# Create a list of layers that we can refer to later\nlayers = [op.name for op in graph.get_operations() if op.type==\'Conv2D\' and \'import/\' in op.name]\n\n# Count how many outputs for each layer\nfeature_nums = [int(graph.get_tensor_by_name(name+\':0\').get_shape()[-1]) for name in layers]\n\n# Print count of layers and outputs (features nodes)\nprint(\'Number of layers\', len(layers))\nprint(\'Total number of feature channels:\', sum(feature_nums))\n\n# Picking some internal layer. Note that we use outputs before applying the ReLU nonlinearity\n# to have non-zero gradients for features with negative initial activations.\nlayer = \'mixed4d_3x3_bottleneck_pre_relu\'\nchannel = 30 # picking some feature channel to visualize\n\n# start with a gray image with a little noise\nimg_noise = np.random.uniform(size=(224,224,3)) + 100.0\n\ndef showarray(a, fmt=\'jpeg\'):\n    # First make sure everything is between 0 and 255\n    a = np.uint8(np.clip(a, 0, 1)*255)\n    # Pick an in-memory format for image display\n    f = BytesIO()\n    # Create the in memory image\n    PIL.Image.fromarray(a).save(f, fmt)\n    # Show image\n    plt.imshow(a)\n\n\ndef T(layer):\n    \'\'\'Helper for getting layer output tensor\'\'\'\n    return graph.get_tensor_by_name(""import/%s:0""%layer)\n\n\n# The following function returns a function wrapper that will create the placeholder\n# inputs of a specified dtype\ndef tffunc(*argtypes):\n    \'\'\'Helper that transforms TF-graph generating function into a regular one.\n    See ""resize"" function below.\n    \'\'\'\n    placeholders = list(map(tf.placeholder, argtypes))\n    def wrap(f):\n        out = f(*placeholders)\n        def wrapper(*args, **kw):\n            return out.eval(dict(zip(placeholders, args)), session=kw.get(\'session\'))\n        return wrapper\n    return wrap\n\n\n# Helper function that uses TF to resize an image\ndef resize(img, size):\n    img = tf.expand_dims(img, 0)\n    # Change \'img\' size by linear interpolation\n    return tf.image.resize_bilinear(img, size)[0, :, :, :]\n\n\ndef calc_grad_tiled(img, t_grad, tile_size=512):\n    \'\'\'Compute the value of tensor t_grad over the image in a tiled way.\n    Random shifts are applied to the image to blur tile boundaries over \n    multiple iterations.\'\'\'\n    # Pick a subregion square size\n    sz = tile_size\n    # Get the image height and width\n    h, w = img.shape[:2]\n    # Get a random shift amount in the x and y direction\n    sx, sy = np.random.randint(sz, size=2)\n    # Randomly shift the image (roll image) in the x and y directions\n    img_shift = np.roll(np.roll(img, sx, 1), sy, 0)\n    # Initialize the while image gradient as zeros\n    grad = np.zeros_like(img)\n    # Now we loop through all the sub-tiles in the image\n    for y in range(0, max(h-sz//2, sz),sz):\n        for x in range(0, max(w-sz//2, sz),sz):\n            # Select the sub image tile\n            sub = img_shift[y:y+sz,x:x+sz]\n            # Calculate the gradient for the tile\n            g = sess.run(t_grad, {t_input:sub})\n            # Apply the gradient of the tile to the whole image gradient\n            grad[y:y+sz,x:x+sz] = g\n    # Return the gradient, undoing the roll operation\n    return np.roll(np.roll(grad, -sx, 1), -sy, 0)\n\ndef render_deepdream(t_obj, img0=img_noise,\n                     iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):\n    # defining the optimization objective, the objective is the mean of the feature\n    t_score = tf.reduce_mean(t_obj)\n    # Our gradients will be defined as changing the t_input to get closer to\n    # the values of t_score.  Here, t_score is the mean of the feature we select,\n    # and t_input will be the image octave (starting with the last)\n    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n\n    # Store the image\n    img = img0\n    # Initialize the octave list\n    octaves = []\n    # Since we stored the image, we need to only calculate n-1 octaves\n    for i in range(octave_n-1):\n        # Extract the image shape\n        hw = img.shape[:2]\n        # Resize the image, scale by the octave_scale (resize by linear interpolation)\n        lo = resize(img, np.int32(np.float32(hw)/octave_scale))\n        # Residual is hi.  Where residual = image - (Resize lo to be hw-shape)\n        hi = img-resize(lo, hw)\n        # Save the lo image for re-iterating\n        img = lo\n        # Save the extracted hi-image\n        octaves.append(hi)\n    \n    # generate details octave by octave\n    for octave in range(octave_n):\n        if octave>0:\n            # Start with the last octave\n            hi = octaves[-octave]\n            #\n            img = resize(img, hi.shape[:2])+hi\n        for i in range(iter_n):\n            # Calculate gradient of the image.\n            g = calc_grad_tiled(img, t_grad)\n            # Ideally, we would just add the gradient, g, but\n            # we want do a forward step size of it (\'step\'),\n            # and divide it by the avg. norm of the gradient, so\n            # we are adding a gradient of a certain size each step.\n            # Also, to make sure we aren\'t dividing by zero, we add 1e-7.\n            img += g*(step / (np.abs(g).mean()+1e-7))\n            print(\'.\',end = \' \')\n        showarray(img/255.0)\n\n# Run Deep Dream\nif __name__==""__main__"":\n    # Create resize function that has a wrapper that creates specified placeholder types\n    resize = tffunc(np.float32, np.int32)(resize)\n    \n    # Open image\n    img0 = PIL.Image.open(\'book_cover.jpg\')\n    img0 = np.float32(img0)\n    # Show Original Image\n    showarray(img0/255.0)\n\n    # Create deep dream\n    render_deepdream(T(layer)[:, :, :, channel], img0, iter_n=15)\n\n    sess.close()\n'"
09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction/02_implementing_rnn.py,22,"b'# Implementing an RNN in TensorFlow\n#----------------------------------\n#\n# We implement an RNN in TensorFlow to predict spam/ham from texts\n#\n\nimport os\nimport re\nimport io\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom zipfile import ZipFile\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a graph\nsess = tf.Session()\n\n# Set RNN parameters\nepochs = 20\nbatch_size = 250\nmax_sequence_length = 25\nrnn_size = 10\nembedding_size = 50\nmin_word_frequency = 10\nlearning_rate = 0.0005\ndropout_keep_prob = tf.placeholder(tf.float32)\n\n\n# Download or open data\ndata_dir = \'temp\'\ndata_file = \'text_data.txt\'\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\nif not os.path.isfile(os.path.join(data_dir, data_file)):\n    zip_url = \'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\'\n    r = requests.get(zip_url)\n    z = ZipFile(io.BytesIO(r.content))\n    file = z.read(\'SMSSpamCollection\')\n    # Format Data\n    text_data = file.decode()\n    text_data = text_data.encode(\'ascii\', errors=\'ignore\')\n    text_data = text_data.decode().split(\'\\n\')\n\n    # Save data to text file\n    with open(os.path.join(data_dir, data_file), \'w\') as file_conn:\n        for text in text_data:\n            file_conn.write(""{}\\n"".format(text))\nelse:\n    # Open data from text file\n    text_data = []\n    with open(os.path.join(data_dir, data_file), \'r\') as file_conn:\n        for row in file_conn:\n            text_data.append(row)\n    text_data = text_data[:-1]\n\ntext_data = [x.split(\'\\t\') for x in text_data if len(x) >= 1]\n[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]\n\n\n# Create a text cleaning function\ndef clean_text(text_string):\n    text_string = re.sub(r\'([^\\s\\w]|_|[0-9])+\', \'\', text_string)\n    text_string = "" "".join(text_string.split())\n    text_string = text_string.lower()\n    return text_string\n\n\n# Clean texts\ntext_data_train = [clean_text(x) for x in text_data_train]\n\n# Change texts into numeric vectors\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,\n                                                                     min_frequency=min_word_frequency)\ntext_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))\n\n# Shuffle and split data\ntext_processed = np.array(text_processed)\ntext_data_target = np.array([1 if x == \'ham\' else 0 for x in text_data_target])\nshuffled_ix = np.random.permutation(np.arange(len(text_data_target)))\nx_shuffled = text_processed[shuffled_ix]\ny_shuffled = text_data_target[shuffled_ix]\n\n# Split train/test set\nix_cutoff = int(len(y_shuffled)*0.80)\nx_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\ny_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\nvocab_size = len(vocab_processor.vocabulary_)\nprint(""Vocabulary Size: {:d}"".format(vocab_size))\nprint(""80-20 Train Test split: {:d} -- {:d}"".format(len(y_train), len(y_test)))\n\n# Create placeholders\nx_data = tf.placeholder(tf.int32, [None, max_sequence_length])\ny_output = tf.placeholder(tf.int32, [None])\n\n# Create embedding\nembedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\nembedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)\n\n# Define the RNN cell\n# tensorflow change >= 1.0, rnn is put into tensorflow.contrib directory. Prior version not test.\nif tf.__version__[0] >= \'1\':\n    cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)\nelse:\n    cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)\n\noutput, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\noutput = tf.nn.dropout(output, dropout_keep_prob)\n\n# Get output of RNN sequence\noutput = tf.transpose(output, [1, 0, 2])\nlast = tf.gather(output, int(output.get_shape()[0]) - 1)\n\nweight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))\nbias = tf.Variable(tf.constant(0.1, shape=[2]))\nlogits_out = tf.matmul(last, weight) + bias\n\n# Loss function\nlosses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)\nloss = tf.reduce_mean(losses)\n\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64)), tf.float32))\n\noptimizer = tf.train.RMSPropOptimizer(learning_rate)\ntrain_step = optimizer.minimize(loss)\n\ninit = tf.global_variables_initializer()\nsess.run(init)\n\ntrain_loss = []\ntest_loss = []\ntrain_accuracy = []\ntest_accuracy = []\n# Start training\nfor epoch in range(epochs):\n\n    # Shuffle training data\n    shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n    x_train = x_train[shuffled_ix]\n    y_train = y_train[shuffled_ix]\n    num_batches = int(len(x_train)/batch_size) + 1\n    # TO DO CALCULATE GENERATIONS ExACTLY\n    for i in range(num_batches):\n        # Select train data\n        min_ix = i * batch_size\n        max_ix = np.min([len(x_train), ((i+1) * batch_size)])\n        x_train_batch = x_train[min_ix:max_ix]\n        y_train_batch = y_train[min_ix:max_ix]\n        \n        # Run train step\n        train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:0.5}\n        sess.run(train_step, feed_dict=train_dict)\n        \n    # Run loss and accuracy for training\n    temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)\n    train_loss.append(temp_train_loss)\n    train_accuracy.append(temp_train_acc)\n    \n    # Run Eval Step\n    test_dict = {x_data: x_test, y_output: y_test, dropout_keep_prob:1.0}\n    temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)\n    test_loss.append(temp_test_loss)\n    test_accuracy.append(temp_test_acc)\n    print(\'Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}\'.format(epoch+1, temp_test_loss, temp_test_acc))\n    \n# Plot loss over time\nepoch_seq = np.arange(1, epochs+1)\nplt.plot(epoch_seq, train_loss, \'k--\', label=\'Train Set\')\nplt.plot(epoch_seq, test_loss, \'r-\', label=\'Test Set\')\nplt.title(\'Softmax Loss\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Softmax Loss\')\nplt.legend(loc=\'upper left\')\nplt.show()\n\n# Plot accuracy over time\nplt.plot(epoch_seq, train_accuracy, \'k--\', label=\'Train Set\')\nplt.plot(epoch_seq, test_accuracy, \'r-\', label=\'Test Set\')\nplt.title(\'Test Accuracy\')\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Accuracy\')\nplt.legend(loc=\'upper left\')\nplt.show()'"
09_Recurrent_Neural_Networks/03_Implementing_LSTM/03_implementing_lstm.py,31,"b'# -*- coding: utf-8 -*-\n#\n# Implementing an LSTM RNN Model\n#------------------------------\n#  Here we implement an LSTM model on all a data set of Shakespeare works.\n#\n#\n#\n\nimport os\nimport re\nimport string\nimport requests\nimport numpy as np\nimport collections\nimport random\nimport pickle\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a session\nsess = tf.Session()\n\n# Set RNN Parameters\nmin_word_freq = 5  # Trim the less frequent words off\nrnn_size = 128  # RNN Model size\nepochs = 10  # Number of epochs to cycle through data\nbatch_size = 100  # Train on this many examples at once\nlearning_rate = 0.001  # Learning rate\ntraining_seq_len = 50  # how long of a word group to consider\nembedding_size = rnn_size  # Word embedding size\nsave_every = 500  # How often to save model checkpoints\neval_every = 50  # How often to evaluate the test sentences\nprime_texts = [\'thou art more\', \'to be or not to\', \'wherefore art thou\']\n\n# Download/store Shakespeare data\ndata_dir = \'temp\'\ndata_file = \'shakespeare.txt\'\nmodel_path = \'shakespeare_model\'\nfull_model_dir = os.path.join(data_dir, model_path)\n\n# Declare punctuation to remove, everything except hyphens and apostrophes\npunctuation = string.punctuation\npunctuation = \'\'.join([x for x in punctuation if x not in [\'-\', ""\'""]])\n\n# Make Model Directory\nif not os.path.exists(full_model_dir):\n    os.makedirs(full_model_dir)\n\n# Make data directory\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\nprint(\'Loading Shakespeare Data\')\n# Check if file is downloaded.\nif not os.path.isfile(os.path.join(data_dir, data_file)):\n    print(\'Not found, downloading Shakespeare texts from www.gutenberg.org\')\n    shakespeare_url = \'http://www.gutenberg.org/cache/epub/100/pg100.txt\'\n    # Get Shakespeare text\n    response = requests.get(shakespeare_url)\n    shakespeare_file = response.content\n    # Decode binary into string\n    s_text = shakespeare_file.decode(\'utf-8\')\n    # Drop first few descriptive paragraphs.\n    s_text = s_text[7675:]\n    # Remove newlines\n    s_text = s_text.replace(\'\\r\\n\', \'\')\n    s_text = s_text.replace(\'\\n\', \'\')\n    \n    # Write to file\n    with open(os.path.join(data_dir, data_file), \'w\') as out_conn:\n        out_conn.write(s_text)\nelse:\n    # If file has been saved, load from that file\n    with open(os.path.join(data_dir, data_file), \'r\') as file_conn:\n        s_text = file_conn.read().replace(\'\\n\', \'\')\n\n# Clean text\nprint(\'Cleaning Text\')\ns_text = re.sub(r\'[{}]\'.format(punctuation), \' \', s_text)\ns_text = re.sub(\'\\s+\', \' \', s_text).strip().lower()\n\n\n# Build word vocabulary function\ndef build_vocab(text, min_freq):\n    word_counts = collections.Counter(text.split(\' \'))\n    # limit word counts to those more frequent than cutoff\n    word_counts = {key: val for key, val in word_counts.items() if val > min_freq}\n    # Create vocab --> index mapping\n    words = word_counts.keys()\n    vocab_to_ix_dict = {key: (i_x+1) for i_x, key in enumerate(words)}\n    # Add unknown key --> 0 index\n    vocab_to_ix_dict[\'unknown\'] = 0\n    # Create index --> vocab mapping\n    ix_to_vocab_dict = {val: key for key, val in vocab_to_ix_dict.items()}\n    \n    return ix_to_vocab_dict, vocab_to_ix_dict\n\n\n# Build Shakespeare vocabulary\nprint(\'Building Shakespeare Vocab\')\nix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\nvocab_size = len(ix2vocab) + 1\nprint(\'Vocabulary Length = {}\'.format(vocab_size))\n# Sanity Check\nassert(len(ix2vocab) == len(vocab2ix))\n\n# Convert text to word vectors\ns_text_words = s_text.split(\' \')\ns_text_ix = []\nfor ix, x in enumerate(s_text_words):\n    try:\n        s_text_ix.append(vocab2ix[x])\n    except KeyError:\n        s_text_ix.append(0)\ns_text_ix = np.array(s_text_ix)\n\n\n# Define LSTM RNN Model\nclass LSTM_Model():\n    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n                 training_seq_len, vocab_size, infer_sample=False):\n        self.embedding_size = embedding_size\n        self.rnn_size = rnn_size\n        self.vocab_size = vocab_size\n        self.infer_sample = infer_sample\n        self.learning_rate = learning_rate\n        \n        if infer_sample:\n            self.batch_size = 1\n            self.training_seq_len = 1\n        else:\n            self.batch_size = batch_size\n            self.training_seq_len = training_seq_len\n        \n        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n        \n        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n        \n        with tf.variable_scope(\'lstm_vars\'):\n            # Softmax Output Weights\n            W = tf.get_variable(\'W\', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n            b = tf.get_variable(\'b\', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n        \n            # Define Embedding\n            embedding_mat = tf.get_variable(\'embedding_mat\', [self.vocab_size, self.embedding_size],\n                                            tf.float32, tf.random_normal_initializer())\n                                            \n            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n        \n        # If we are inferring (generating text), we add a \'loop\' function\n        # Define how to get the i+1 th input from the i th output\n        def inferred_loop(prev):\n            # Apply hidden layer\n            prev_transformed = tf.matmul(prev, W) + b\n            # Get the index of the output (also don\'t run the gradient)\n            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n            # Get embedded vector\n            out = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n            return out\n        \n        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n        outputs, last_state = decoder(rnn_inputs_trimmed,\n                                      self.initial_state,\n                                      self.lstm_cell,\n                                      loop_function=inferred_loop if infer_sample else None)\n        # Non inferred outputs\n        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n        # Logits and output\n        self.logit_output = tf.matmul(output, W) + b\n        self.model_output = tf.nn.softmax(self.logit_output)\n        \n        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n        loss = loss_fun([self.logit_output], [tf.reshape(self.y_output, [-1])],\n                        [tf.ones([self.batch_size * self.training_seq_len])])\n        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n        self.final_state = last_state\n        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n        \n    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text=\'thou art\'):\n        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n        word_list = prime_text.split()\n        for word in word_list[:-1]:\n            x = np.zeros((1, 1))\n            x[0, 0] = vocab[word]\n            feed_dict = {self.x_data: x, self.initial_state: state}\n            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n\n        out_sentence = prime_text\n        word = word_list[-1]\n        for n in range(num):\n            x = np.zeros((1, 1))\n            x[0, 0] = vocab[word]\n            feed_dict = {self.x_data: x, self.initial_state: state}\n            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n            sample = np.argmax(model_output[0])\n            if sample == 0:\n                break\n            word = words[sample]\n            out_sentence = out_sentence + \' \' + word\n        return out_sentence\n\n\n# Define LSTM Model\nlstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n                        training_seq_len, vocab_size)\n\n# Tell TensorFlow we are reusing the scope for the testing\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\n    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n                                 training_seq_len, vocab_size, infer_sample=True)\n\n\n# Create model saver\nsaver = tf.train.Saver(tf.global_variables())\n\n# Create batches for each epoch\nnum_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n# Split up text indices into subarrays, of equal size\nbatches = np.array_split(s_text_ix, num_batches)\n# Reshape each split into [batch_size, training_seq_len]\nbatches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n\n# Initialize all variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Train model\ntrain_loss = []\niteration_count = 1\nfor epoch in range(epochs):\n    # Shuffle word indices\n    random.shuffle(batches)\n    # Create targets from shuffled batches\n    targets = [np.roll(x, -1, axis=1) for x in batches]\n    # Run a through one epoch\n    print(\'Starting Epoch #{} of {}.\'.format(epoch+1, epochs))\n    # Reset initial LSTM state every epoch\n    state = sess.run(lstm_model.initial_state)\n    for ix, batch in enumerate(batches):\n        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n        c, h = lstm_model.initial_state\n        training_dict[c] = state.c\n        training_dict[h] = state.h\n        \n        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n                                       feed_dict=training_dict)\n        train_loss.append(temp_loss)\n        \n        # Print status every 10 gens\n        if iteration_count % 10 == 0:\n            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n            print(\'Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}\'.format(*summary_nums))\n        \n        # Save the model and the vocab\n        if iteration_count % save_every == 0:\n            # Save model\n            model_file_name = os.path.join(full_model_dir, \'model\')\n            saver.save(sess, model_file_name, global_step=iteration_count)\n            print(\'Model Saved To: {}\'.format(model_file_name))\n            # Save vocabulary\n            dictionary_file = os.path.join(full_model_dir, \'vocab.pkl\')\n            with open(dictionary_file, \'wb\') as dict_file_conn:\n                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n        \n        if iteration_count % eval_every == 0:\n            for sample in prime_texts:\n                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n                \n        iteration_count += 1\n\n\n# Plot loss over time\nplt.plot(train_loss, \'k-\')\nplt.title(\'Sequence to Sequence Loss\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Loss\')\nplt.show()\n'"
09_Recurrent_Neural_Networks/04_Stacking_Multiple_LSTM_Layers/04_stacking_multiple_lstm.py,29,"b'# -*- coding: utf-8 -*-\n#\n# Stacking LSTM Layers\n#---------------------\n#  Here we implement an LSTM model on all a data set of Shakespeare works.\n#  We will stack multiple LSTM models for a more accurate representation\n#  of Shakespearean language.  We will also use characters instead of words.\n#\n\nimport os\nimport re\nimport string\nimport requests\nimport numpy as np\nimport collections\nimport random\nimport pickle\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a session\nsess = tf.Session()\n\n# Set RNN Parameters\nnum_layers = 3  # Number of RNN layers stacked\nmin_word_freq = 5  # Trim the less frequent words off\nrnn_size = 128  # RNN Model size, has to equal embedding size\nepochs = 10  # Number of epochs to cycle through data\nbatch_size = 100  # Train on this many examples at once\nlearning_rate = 0.0005  # Learning rate\ntraining_seq_len = 50  # how long of a word group to consider\nsave_every = 500  # How often to save model checkpoints\neval_every = 50  # How often to evaluate the test sentences\nprime_texts = [\'thou art more\', \'to be or not to\', \'wherefore art thou\']\n\n# Download/store Shakespeare data\ndata_dir = \'temp\'\ndata_file = \'shakespeare.txt\'\nmodel_path = \'shakespeare_model\'\nfull_model_dir = os.path.join(data_dir, model_path)\n\n# Declare punctuation to remove, everything except hyphens and apostrophes\npunctuation = string.punctuation\npunctuation = \'\'.join([x for x in punctuation if x not in [\'-\', ""\'""]])\n\n# Make Model Directory\nif not os.path.exists(full_model_dir):\n    os.makedirs(full_model_dir)\n\n# Make data directory\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\nprint(\'Loading Shakespeare Data\')\n# Check if file is downloaded.\nif not os.path.isfile(os.path.join(data_dir, data_file)):\n    print(\'Not found, downloading Shakespeare texts from www.gutenberg.org\')\n    shakespeare_url = \'http://www.gutenberg.org/cache/epub/100/pg100.txt\'\n    # Get Shakespeare text\n    response = requests.get(shakespeare_url)\n    shakespeare_file = response.content\n    # Decode binary into string\n    s_text = shakespeare_file.decode(\'utf-8\')\n    # Drop first few descriptive paragraphs.\n    s_text = s_text[7675:]\n    # Remove newlines\n    s_text = s_text.replace(\'\\r\\n\', \'\')\n    s_text = s_text.replace(\'\\n\', \'\')\n    \n    # Write to file\n    with open(os.path.join(data_dir, data_file), \'w\') as out_conn:\n        out_conn.write(s_text)\nelse:\n    # If file has been saved, load from that file\n    with open(os.path.join(data_dir, data_file), \'r\') as file_conn:\n        s_text = file_conn.read().replace(\'\\n\', \'\')\n\n# Clean text\nprint(\'Cleaning Text\')\ns_text = re.sub(r\'[{}]\'.format(punctuation), \' \', s_text)\ns_text = re.sub(\'\\s+\', \' \', s_text).strip().lower()\n\n# Split up by characters\nchar_list = list(s_text)\n\n\n# Build word vocabulary function\ndef build_vocab(characters):\n    character_counts = collections.Counter(characters)\n    # Create vocab --> index mapping\n    chars = character_counts.keys()\n    vocab_to_ix_dict = {key: (inx + 1) for inx, key in enumerate(chars)}\n    # Add unknown key --> 0 index\n    vocab_to_ix_dict[\'unknown\'] = 0\n    # Create index --> vocab mapping\n    ix_to_vocab_dict = {val: key for key, val in vocab_to_ix_dict.items()}\n    return ix_to_vocab_dict, vocab_to_ix_dict\n\n\n# Build Shakespeare vocabulary\nprint(\'Building Shakespeare Vocab by Characters\')\nix2vocab, vocab2ix = build_vocab(char_list)\nvocab_size = len(ix2vocab)\nprint(\'Vocabulary Length = {}\'.format(vocab_size))\n# Sanity Check\nassert(len(ix2vocab) == len(vocab2ix))\n\n# Convert text to word vectors\ns_text_ix = []\nfor x in char_list:\n    try:\n        s_text_ix.append(vocab2ix[x])\n    except KeyError:\n        s_text_ix.append(0)\ns_text_ix = np.array(s_text_ix)\n\n\n# Define LSTM RNN Model\nclass LSTM_Model():\n    def __init__(self, rnn_size, num_layers, batch_size, learning_rate,\n                 training_seq_len, vocab_size, infer_sample=False):\n        self.rnn_size = rnn_size\n        self.num_layers = num_layers\n        self.vocab_size = vocab_size\n        self.infer_sample = infer_sample\n        self.learning_rate = learning_rate\n        \n        if infer_sample:\n            self.batch_size = 1\n            self.training_seq_len = 1\n        else:\n            self.batch_size = batch_size\n            self.training_seq_len = training_seq_len\n        \n        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n        self.lstm_cell = tf.contrib.rnn.MultiRNNCell([self.lstm_cell for _ in range(self.num_layers)])\n        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n        \n        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n        \n        with tf.variable_scope(\'lstm_vars\'):\n            # Softmax Output Weights\n            W = tf.get_variable(\'W\', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n            b = tf.get_variable(\'b\', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n        \n            # Define Embedding\n            embedding_mat = tf.get_variable(\'embedding_mat\', [self.vocab_size, self.rnn_size],\n                                            tf.float32, tf.random_normal_initializer())\n                                            \n            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n        \n        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n        outputs, last_state = decoder(rnn_inputs_trimmed,\n                                      self.initial_state,\n                                      self.lstm_cell)\n        \n        # RNN outputs\n        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, rnn_size])\n        # Logits and output\n        self.logit_output = tf.matmul(output, W) + b\n        self.model_output = tf.nn.softmax(self.logit_output)\n        \n        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n                [tf.ones([self.batch_size * self.training_seq_len])],\n                self.vocab_size)\n        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n        self.final_state = last_state\n        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n        \n    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=20, prime_text=\'thou art\'):\n        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n        char_list = list(prime_text)\n        for char in char_list[:-1]:\n            x = np.zeros((1, 1))\n            x[0, 0] = vocab[char]\n            feed_dict = {self.x_data: x, self.initial_state:state}\n            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n\n        out_sentence = prime_text\n        char = char_list[-1]\n        for n in range(num):\n            x = np.zeros((1, 1))\n            x[0, 0] = vocab[char]\n            feed_dict = {self.x_data: x, self.initial_state:state}\n            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n            sample = np.argmax(model_output[0])\n            if sample == 0:\n                break\n            char = words[sample]\n            out_sentence = out_sentence + char\n        return out_sentence\n\n\n# Define LSTM Model\nlstm_model = LSTM_Model(rnn_size,\n                        num_layers,\n                        batch_size,\n                        learning_rate,\n                        training_seq_len,\n                        vocab_size)\n\n# Tell TensorFlow we are reusing the scope for the testing\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\n    test_lstm_model = LSTM_Model(rnn_size,\n                                 num_layers,\n                                 batch_size,\n                                 learning_rate,\n                                 training_seq_len,\n                                 vocab_size,\n                                 infer_sample=True)\n\n# Create model saver\nsaver = tf.train.Saver(tf.global_variables())\n\n# Create batches for each epoch\nnum_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n# Split up text indices into subarrays, of equal size\nbatches = np.array_split(s_text_ix, num_batches)\n# Reshape each split into [batch_size, training_seq_len]\nbatches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n\n# Initialize all variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Train model\ntrain_loss = []\niteration_count = 1\nfor epoch in range(epochs):\n    # Shuffle word indices\n    random.shuffle(batches)\n    # Create targets from shuffled batches\n    targets = [np.roll(x, -1, axis=1) for x in batches]\n    # Run a through one epoch\n    print(\'Starting Epoch #{} of {}.\'.format(epoch+1, epochs))\n    # Reset initial LSTM state every epoch\n    state = sess.run(lstm_model.initial_state)\n    for ix, batch in enumerate(batches):\n        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n        # We need to update initial state for each RNN cell:\n        for i, (c, h) in enumerate(lstm_model.initial_state):\n                    training_dict[c] = state[i].c\n                    training_dict[h] = state[i].h\n        \n        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n                                       feed_dict=training_dict)\n        train_loss.append(temp_loss)\n        \n        # Print status every 10 gens\n        if iteration_count % 10 == 0:\n            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n            print(\'Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}\'.format(*summary_nums))\n        \n        # Save the model and the vocab\n        if iteration_count % save_every == 0:\n            # Save model\n            model_file_name = os.path.join(full_model_dir, \'model\')\n            saver.save(sess, model_file_name, global_step=iteration_count)\n            print(\'Model Saved To: {}\'.format(model_file_name))\n            # Save vocabulary\n            dictionary_file = os.path.join(full_model_dir, \'vocab.pkl\')\n            with open(dictionary_file, \'wb\') as dict_file_conn:\n                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n        \n        if iteration_count % eval_every == 0:\n            for sample in prime_texts:\n                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n                \n        iteration_count += 1\n\n\n# Plot loss over time\nplt.plot(train_loss, \'k-\')\nplt.title(\'Sequence to Sequence Loss\')\nplt.xlabel(\'Generation\')\nplt.ylabel(\'Loss\')\nplt.show()\n'"
09_Recurrent_Neural_Networks/05_Creating_A_Sequence_To_Sequence_Model/05_seq2seq_translation.py,26,"b'# -*- coding: utf-8 -*-\n#\n# Creating Sequence to Sequence Models\n#-------------------------------------\n#  Here we show how to implement sequence to sequence models.\n#  Specifically, we will build an English to German translation model.\n#\n\nimport os\nimport re\nimport sys\nimport json\nimport math\nimport time\nimport string\nimport requests\nimport io\nimport numpy as np\nimport collections\nimport random\nimport pickle\nimport string\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom zipfile import ZipFile\nfrom collections import Counter\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\nlocal_repository = \'temp/seq2seq\'\n\n# models can be retrieved from github: https://github.com/tensorflow/models.git\n# put the models dir under python search lib path.\n\nif not os.path.exists(local_repository):\n    from git import Repo\n    tf_model_repository = \'https://github.com/tensorflow/nmt/\'\n    Repo.clone_from(tf_model_repository, local_repository)\n    sys.path.insert(0, \'temp/seq2seq/nmt/\')\n\n# May also try to use \'attention model\' by importing the attention model:\n# from temp.seq2seq.nmt import attention_model as attention_model\nfrom temp.seq2seq.nmt import model as model\nfrom temp.seq2seq.nmt.utils import vocab_utils as vocab_utils\nimport temp.seq2seq.nmt.model_helper as model_helper\nimport temp.seq2seq.nmt.utils.iterator_utils as iterator_utils\nimport temp.seq2seq.nmt.utils.misc_utils as utils\nimport temp.seq2seq.nmt.train as train\n\n# Start a session\nsess = tf.Session()\n\n# Model Parameters\nvocab_size = 10000\npunct = string.punctuation\n\n# Data Parameters\ndata_dir = \'temp\'\ndata_file = \'eng_ger.txt\'\nmodel_path = \'seq2seq_model\'\nfull_model_dir = os.path.join(data_dir, model_path)\n\n# Load hyper-parameters for translation model. (Good defaults are provided in Repository).\nhparams = tf.contrib.training.HParams()\nparam_file = \'temp/seq2seq/nmt/standard_hparams/wmt16.json\'\n# Can also try: (For different architectures)\n# \'temp/seq2seq/nmt/standard_hparams/iwslt15.json\'\n# \'temp/seq2seq/nmt/standard_hparams/wmt16_gnmt_4_layer.json\',\n# \'temp/seq2seq/nmt/standard_hparams/wmt16_gnmt_8_layer.json\',\n\nwith open(param_file, ""r"") as f:\n    params_json = json.loads(f.read())\n\nfor key, value in params_json.items():\n    hparams.add_hparam(key, value)\nhparams.add_hparam(\'num_gpus\', 0)\nhparams.add_hparam(\'num_encoder_layers\', hparams.num_layers)\nhparams.add_hparam(\'num_decoder_layers\', hparams.num_layers)\nhparams.add_hparam(\'num_encoder_residual_layers\', 0)\nhparams.add_hparam(\'num_decoder_residual_layers\', 0)\nhparams.add_hparam(\'init_op\', \'uniform\')\nhparams.add_hparam(\'random_seed\', None)\nhparams.add_hparam(\'num_embeddings_partitions\', 0)\nhparams.add_hparam(\'warmup_steps\', 0)\nhparams.add_hparam(\'length_penalty_weight\', 0)\nhparams.add_hparam(\'sampling_temperature\', 0.0)\nhparams.add_hparam(\'num_translations_per_input\', 1)\nhparams.add_hparam(\'warmup_scheme\', \'t2t\')\nhparams.add_hparam(\'epoch_step\', 0)\nhparams.num_train_steps = 5000\n\n# Not use any pretrained embeddings\nhparams.add_hparam(\'src_embed_file\', \'\')\nhparams.add_hparam(\'tgt_embed_file\', \'\')\nhparams.add_hparam(\'num_keep_ckpts\', 5)\nhparams.add_hparam(\'avg_ckpts\', False)\n\n# Remove attention\nhparams.attention = None\n\n# Make Model Directory\nif not os.path.exists(full_model_dir):\n    os.makedirs(full_model_dir)\n\n# Make data directory\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\nprint(\'Loading English-German Data\')\n# Check for data, if it doesn\'t exist, download it and save it\nif not os.path.isfile(os.path.join(data_dir, data_file)):\n    print(\'Data not found, downloading Eng-Ger sentences from www.manythings.org\')\n    sentence_url = \'http://www.manythings.org/anki/deu-eng.zip\'\n    r = requests.get(sentence_url)\n    z = ZipFile(io.BytesIO(r.content))\n    file = z.read(\'deu.txt\')\n    # Format Data\n    eng_ger_data = file.decode()\n    eng_ger_data = eng_ger_data.encode(\'ascii\', errors=\'ignore\')\n    eng_ger_data = eng_ger_data.decode().split(\'\\n\')\n    # Write to file\n    with open(os.path.join(data_dir, data_file), \'w\') as out_conn:\n        for sentence in eng_ger_data:\n            out_conn.write(sentence + \'\\n\')\nelse:\n    eng_ger_data = []\n    with open(os.path.join(data_dir, data_file), \'r\') as in_conn:\n        for row in in_conn:\n            eng_ger_data.append(row[:-1])\nprint(\'Done!\')\n\n# Remove punctuation\neng_ger_data = [\'\'.join(char for char in sent if char not in punct) for sent in eng_ger_data]\n# Split each sentence by tabs    \neng_ger_data = [x.split(\'\\t\') for x in eng_ger_data if len(x) >= 1]\n[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)]\nenglish_sentence = [x.lower().split() for x in english_sentence]\ngerman_sentence = [x.lower().split() for x in german_sentence]\n\n# We need to write them to separate text files for the text-line-dataset operations.\ntrain_prefix = \'train\'\nsrc_suffix = \'en\'  # English\ntgt_suffix = \'de\'  # Deutsch (German)\nsource_txt_file = train_prefix + \'.\' + src_suffix\nhparams.add_hparam(\'src_file\', source_txt_file)\ntarget_txt_file = train_prefix + \'.\' + tgt_suffix\nhparams.add_hparam(\'tgt_file\', target_txt_file)\nwith open(source_txt_file, \'w\') as f:\n    for sent in english_sentence:\n        f.write(\' \'.join(sent) + \'\\n\')\n\nwith open(target_txt_file, \'w\') as f:\n    for sent in german_sentence:\n        f.write(\' \'.join(sent) + \'\\n\')\n\n\n# Partition some sentences off for testing files\ntest_prefix = \'test_sent\'\nhparams.add_hparam(\'dev_prefix\', test_prefix)\nhparams.add_hparam(\'train_prefix\', train_prefix)\nhparams.add_hparam(\'test_prefix\', test_prefix)\nhparams.add_hparam(\'src\', src_suffix)\nhparams.add_hparam(\'tgt\', tgt_suffix)\n\nnum_sample = 100\ntotal_samples = len(english_sentence)\n# Get around \'num_sample\'s every so often in the src/tgt sentences\nix_sample = [x for x in range(total_samples) if x % (total_samples // num_sample) == 0]\ntest_src = [\' \'.join(english_sentence[x]) for x in ix_sample]\ntest_tgt = [\' \'.join(german_sentence[x]) for x in ix_sample]\n\n# Write test sentences to file\nwith open(test_prefix + \'.\' + src_suffix, \'w\') as f:\n    for eng_test in test_src:\n        f.write(eng_test + \'\\n\')\n\nwith open(test_prefix + \'.\' + tgt_suffix, \'w\') as f:\n    for ger_test in test_src:\n        f.write(ger_test + \'\\n\')\n\nprint(\'Processing the vocabularies.\')\n# Process the English Vocabulary\nall_english_words = [word for sentence in english_sentence for word in sentence]\nall_english_counts = Counter(all_english_words)\neng_word_keys = [x[0] for x in all_english_counts.most_common(vocab_size-3)]  # -3 because UNK, S, /S is also in there\neng_vocab2ix = dict(zip(eng_word_keys, range(1, vocab_size)))\neng_ix2vocab = {val: key for key, val in eng_vocab2ix.items()}\nenglish_processed = []\nfor sent in english_sentence:\n    temp_sentence = []\n    for word in sent:\n        try:\n            temp_sentence.append(eng_vocab2ix[word])\n        except KeyError:\n            temp_sentence.append(0)\n    english_processed.append(temp_sentence)\n\n\n# Process the German Vocabulary\nall_german_words = [word for sentence in german_sentence for word in sentence]\nall_german_counts = Counter(all_german_words)\nger_word_keys = [x[0] for x in all_german_counts.most_common(vocab_size-3)]  # -3 because UNK, S, /S is also in there\nger_vocab2ix = dict(zip(ger_word_keys, range(1, vocab_size)))\nger_ix2vocab = {val: key for key, val in ger_vocab2ix.items()}\ngerman_processed = []\nfor sent in german_sentence:\n    temp_sentence = []\n    for word in sent:\n        try:\n            temp_sentence.append(ger_vocab2ix[word])\n        except KeyError:\n            temp_sentence.append(0)\n    german_processed.append(temp_sentence)\n\n\n# Save vocab files for data processing\nsource_vocab_file = \'vocab\' + \'.\' + src_suffix\nhparams.add_hparam(\'src_vocab_file\', source_vocab_file)\neng_word_keys = [\'<unk>\', \'<s>\', \'</s>\'] + eng_word_keys\n\ntarget_vocab_file = \'vocab\' + \'.\' + tgt_suffix\nhparams.add_hparam(\'tgt_vocab_file\', target_vocab_file)\nger_word_keys = [\'<unk>\', \'<s>\', \'</s>\'] + ger_word_keys\n\n# Write out all unique english words\nwith open(source_vocab_file, \'w\') as f:\n    for eng_word in eng_word_keys:\n        f.write(eng_word + \'\\n\')\n\n# Write out all unique german words\nwith open(target_vocab_file, \'w\') as f:\n    for ger_word in ger_word_keys:\n        f.write(ger_word + \'\\n\')\n\n# Add vocab size to hyper parameters\nhparams.add_hparam(\'src_vocab_size\', vocab_size)\nhparams.add_hparam(\'tgt_vocab_size\', vocab_size)\n\n# Add out-directory\nout_dir = \'temp/seq2seq/nmt_out\'\nhparams.add_hparam(\'out_dir\', out_dir)\nif not tf.gfile.Exists(out_dir):\n    tf.gfile.MakeDirs(out_dir)\n\n\nclass TrainGraph(collections.namedtuple(""TrainGraph"", (""graph"", ""model"", ""iterator"", ""skip_count_placeholder""))):\n    pass\n\n\ndef create_train_graph(scope=None):\n    graph = tf.Graph()\n    with graph.as_default():\n        src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(hparams.src_vocab_file,\n                                                                           hparams.tgt_vocab_file,\n                                                                           share_vocab=False)\n\n        src_dataset = tf.data.TextLineDataset(hparams.src_file)\n        tgt_dataset = tf.data.TextLineDataset(hparams.tgt_file)\n        skip_count_placeholder = tf.placeholder(shape=(), dtype=tf.int64)\n\n        iterator = iterator_utils.get_iterator(src_dataset, tgt_dataset, src_vocab_table, tgt_vocab_table,\n                                               batch_size=hparams.batch_size,\n                                               sos=hparams.sos,\n                                               eos=hparams.eos,\n                                               random_seed=None,\n                                               num_buckets=hparams.num_buckets,\n                                               src_max_len=hparams.src_max_len,\n                                               tgt_max_len=hparams.tgt_max_len,\n                                               skip_count=skip_count_placeholder)\n        final_model = model.Model(hparams,\n                                  iterator=iterator,\n                                  mode=tf.contrib.learn.ModeKeys.TRAIN,\n                                  source_vocab_table=src_vocab_table,\n                                  target_vocab_table=tgt_vocab_table,\n                                  scope=scope)\n\n    return TrainGraph(graph=graph, model=final_model, iterator=iterator, skip_count_placeholder=skip_count_placeholder)\n\n\ntrain_graph = create_train_graph()\n\n\n# Create the evaluation graph\nclass EvalGraph(collections.namedtuple(""EvalGraph"", (""graph"", ""model"", ""src_file_placeholder"", ""tgt_file_placeholder"",\n                                                     ""iterator""))):\n    pass\n\n\ndef create_eval_graph(scope=None):\n    graph = tf.Graph()\n\n    with graph.as_default():\n        src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(\n            hparams.src_vocab_file, hparams.tgt_vocab_file, hparams.share_vocab)\n        src_file_placeholder = tf.placeholder(shape=(), dtype=tf.string)\n        tgt_file_placeholder = tf.placeholder(shape=(), dtype=tf.string)\n        src_dataset = tf.data.TextLineDataset(src_file_placeholder)\n        tgt_dataset = tf.data.TextLineDataset(tgt_file_placeholder)\n        iterator = iterator_utils.get_iterator(\n            src_dataset,\n            tgt_dataset,\n            src_vocab_table,\n            tgt_vocab_table,\n            hparams.batch_size,\n            sos=hparams.sos,\n            eos=hparams.eos,\n            random_seed=hparams.random_seed,\n            num_buckets=hparams.num_buckets,\n            src_max_len=hparams.src_max_len_infer,\n            tgt_max_len=hparams.tgt_max_len_infer)\n        final_model = model.Model(hparams,\n                                  iterator=iterator,\n                                  mode=tf.contrib.learn.ModeKeys.EVAL,\n                                  source_vocab_table=src_vocab_table,\n                                  target_vocab_table=tgt_vocab_table,\n                                  scope=scope)\n    return EvalGraph(graph=graph,\n                     model=final_model,\n                     src_file_placeholder=src_file_placeholder,\n                     tgt_file_placeholder=tgt_file_placeholder,\n                     iterator=iterator)\n\n\neval_graph = create_eval_graph()\n\n\n# Inference graph\nclass InferGraph(\n    collections.namedtuple(""InferGraph"", (""graph"", ""model"", ""src_placeholder"", ""batch_size_placeholder"", ""iterator""))):\n    pass\n\n\ndef create_infer_graph(scope=None):\n    graph = tf.Graph()\n    with graph.as_default():\n        src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(hparams.src_vocab_file,\n                                                                           hparams.tgt_vocab_file,\n                                                                           hparams.share_vocab)\n        reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_file(hparams.tgt_vocab_file,\n                                                                             default_value=vocab_utils.UNK)\n\n        src_placeholder = tf.placeholder(shape=[None], dtype=tf.string)\n        batch_size_placeholder = tf.placeholder(shape=[], dtype=tf.int64)\n        src_dataset = tf.data.Dataset.from_tensor_slices(src_placeholder)\n        iterator = iterator_utils.get_infer_iterator(src_dataset,\n                                                     src_vocab_table,\n                                                     batch_size=batch_size_placeholder,\n                                                     eos=hparams.eos,\n                                                     src_max_len=hparams.src_max_len_infer)\n        final_model = model.Model(hparams,\n                                  iterator=iterator,\n                                  mode=tf.contrib.learn.ModeKeys.INFER,\n                                  source_vocab_table=src_vocab_table,\n                                  target_vocab_table=tgt_vocab_table,\n                                  reverse_target_vocab_table=reverse_tgt_vocab_table,\n                                  scope=scope)\n    return InferGraph(graph=graph,\n                      model=final_model,\n                      src_placeholder=src_placeholder,\n                      batch_size_placeholder=batch_size_placeholder,\n                      iterator=iterator)\n\n\ninfer_graph = create_infer_graph()\n\n\n# Create sample data for evaluation\nsample_ix = [25, 125, 240, 450]\nsample_src_data = [\' \'.join(english_sentence[x]) for x in sample_ix]\nsample_tgt_data = [\' \'.join(german_sentence[x]) for x in sample_ix]\n\nconfig_proto = utils.get_config_proto()\n\ntrain_sess = tf.Session(config=config_proto, graph=train_graph.graph)\neval_sess = tf.Session(config=config_proto, graph=eval_graph.graph)\ninfer_sess = tf.Session(config=config_proto, graph=infer_graph.graph)\n\n# Load the training graph\nwith train_graph.graph.as_default():\n    loaded_train_model, global_step = model_helper.create_or_load_model(train_graph.model,\n                                                                        hparams.out_dir,\n                                                                        train_sess,\n                                                                        ""train"")\n\n\nsummary_writer = tf.summary.FileWriter(os.path.join(hparams.out_dir, \'Training\'), train_graph.graph)\n\nfor metric in hparams.metrics:\n    hparams.add_hparam(""best_"" + metric, 0)\n    best_metric_dir = os.path.join(hparams.out_dir, ""best_"" + metric)\n    hparams.add_hparam(""best_"" + metric + ""_dir"", best_metric_dir)\n    tf.gfile.MakeDirs(best_metric_dir)\n\n\neval_output = train.run_full_eval(hparams.out_dir, infer_graph, infer_sess, eval_graph, eval_sess,\n                                  hparams, summary_writer, sample_src_data, sample_tgt_data)\n\neval_results, _, acc_blue_scores = eval_output\n\n# Training Initialization\nlast_stats_step = global_step\nlast_eval_step = global_step\nlast_external_eval_step = global_step\n\nsteps_per_eval = 10 * hparams.steps_per_stats\nsteps_per_external_eval = 5 * steps_per_eval\n\navg_step_time = 0.0\nstep_time, checkpoint_loss, checkpoint_predict_count = 0.0, 0.0, 0.0\ncheckpoint_total_count = 0.0\nspeed, train_ppl = 0.0, 0.0\n\nutils.print_out(""# Start step %d, lr %g, %s"" %\n                (global_step, loaded_train_model.learning_rate.eval(session=train_sess),\n                 time.ctime()))\nskip_count = hparams.batch_size * hparams.epoch_step\nutils.print_out(""# Init train iterator, skipping %d elements"" % skip_count)\n\ntrain_sess.run(train_graph.iterator.initializer,\n              feed_dict={train_graph.skip_count_placeholder: skip_count})\n\n\n# Run training\nwhile global_step < hparams.num_train_steps:\n    start_time = time.time()\n    try:\n        step_result = loaded_train_model.train(train_sess)\n        (_, step_loss, step_predict_count, step_summary, global_step, step_word_count,\n         batch_size, __, ___) = step_result\n        hparams.epoch_step += 1\n    except tf.errors.OutOfRangeError:\n        # Next Epoch\n        hparams.epoch_step = 0\n        utils.print_out(""# Finished an epoch, step %d. Perform external evaluation"" % global_step)\n        train.run_sample_decode(infer_graph,\n                                infer_sess,\n                                hparams.out_dir,\n                                hparams,\n                                summary_writer,\n                                sample_src_data,\n                                sample_tgt_data)\n        dev_scores, test_scores, _ = train.run_external_eval(infer_graph,\n                                                             infer_sess,\n                                                             hparams.out_dir,\n                                                             hparams,\n                                                             summary_writer)\n        train_sess.run(train_graph.iterator.initializer, feed_dict={train_graph.skip_count_placeholder: 0})\n        continue\n\n    summary_writer.add_summary(step_summary, global_step)\n\n    # Statistics\n    step_time += (time.time() - start_time)\n    checkpoint_loss += (step_loss * batch_size)\n    checkpoint_predict_count += step_predict_count\n    checkpoint_total_count += float(step_word_count)\n\n    # print statistics\n    if global_step - last_stats_step >= hparams.steps_per_stats:\n        last_stats_step = global_step\n        avg_step_time = step_time / hparams.steps_per_stats\n        train_ppl = utils.safe_exp(checkpoint_loss / checkpoint_predict_count)\n        speed = checkpoint_total_count / (1000 * step_time)\n\n        utils.print_out(""  global step %d lr %g ""\n                        ""step-time %.2fs wps %.2fK ppl %.2f %s"" %\n                        (global_step,\n                         loaded_train_model.learning_rate.eval(session=train_sess),\n                         avg_step_time, speed, train_ppl, train._get_best_results(hparams)))\n\n        if math.isnan(train_ppl):\n            break\n\n        # Reset timer and loss.\n        step_time, checkpoint_loss, checkpoint_predict_count = 0.0, 0.0, 0.0\n        checkpoint_total_count = 0.0\n\n    if global_step - last_eval_step >= steps_per_eval:\n        last_eval_step = global_step\n        utils.print_out(""# Save eval, global step %d"" % global_step)\n        utils.add_summary(summary_writer, global_step, ""train_ppl"", train_ppl)\n\n        # Save checkpoint\n        loaded_train_model.saver.save(train_sess, os.path.join(hparams.out_dir, ""translate.ckpt""),\n                                      global_step=global_step)\n\n        # Evaluate on dev/test\n        train.run_sample_decode(infer_graph,\n                                infer_sess,\n                                out_dir,\n                                hparams,\n                                summary_writer,\n                                sample_src_data,\n                                sample_tgt_data)\n        dev_ppl, test_ppl = train.run_internal_eval(eval_graph,\n                                                    eval_sess,\n                                                    out_dir,\n                                                    hparams,\n                                                    summary_writer)\n\n    if global_step - last_external_eval_step >= steps_per_external_eval:\n        last_external_eval_step = global_step\n\n        # Save checkpoint\n        loaded_train_model.saver.save(train_sess, os.path.join(hparams.out_dir, ""translate.ckpt""),\n                                      global_step=global_step)\n\n        train.run_sample_decode(infer_graph,\n                                infer_sess,\n                                out_dir,\n                                hparams,\n                                summary_writer,\n                                sample_src_data,\n                                sample_tgt_data)\n        dev_scores, test_scores, _ = train.run_external_eval(infer_graph,\n                                                             infer_sess,\n                                                             out_dir,\n                                                             hparams,\n                                                             summary_writer)'"
09_Recurrent_Neural_Networks/05_Creating_A_Sequence_To_Sequence_Model/05_translation_model_sample.py,17,"b'# -*- coding: utf-8 -*-\n#\n# Creating Sequence to Sequence Model Class\n#-------------------------------------\n#  Here we implement the Seq2Seq class for modeling language translation\n#\n\nimport numpy as np\nimport tensorflow as tf\n\n\n# Declare Seq2seq translation model\nclass Seq2Seq(object):\n    def __init__(self, vocab_size, x_buckets, y_buckets, rnn_size,\n                 num_layers, max_gradient, batch_size, learning_rate,\n                 lr_decay_rate, forward_only=False):\n        self.vocab_size = vocab_size\n        self.x_buckets = x_buckets\n        self.y_buckets = y_buckets\n        self.batch_size = batch_size\n        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=tf.float32)\n        self.lr_decay = self.learning_rate.assign(self.learning_rate * lr_decay_rate)\n        self.global_step = tf.Variable(0, trainable=False)\n        \n        cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n        \n        # Decoding function\n        def decode_wrapper(x, y, forward_only):\n            decode_fun = tf.nn.seq2seq.embedding_attention_seq2seq(x,\n                y,\n                cell,\n                num_encoder_symbols=vocab_size,\n                num_decoder_symbols=vocab_size,\n                embedding_size=rnn_size,\n                feed_previous=forward_only,\n                dtype=tf.float32)\n            return(decode_fun)\n        \n        # Loss function\n        loss_fun = tf.nn.softmax_cross_entropy_with_logits\n        \n        self.encoder_inputs = []\n        self.decoder_inputs = []\n        self.target_weights = []\n        for i in range(x_buckets[-1]):  # Last bucket is the biggest one.\n            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                      name=""encoder{}"".format(i)))\n                                                      \n        for i in range(x_buckets[-1] + 1):\n            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                      name=""decoder{}"".format(i)))\n            self.target_weights.append(tf.placeholder(tf.float32, shape=[None],\n                                                      name=""weight{}"".format(i)))\n        \n        \n        targets = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]\n        xy_buckets = [x for x in zip(x_buckets, y_buckets)]\n        \n        if forward_only:\n            self.outputs, self.loss =  tf.nn.seq2seq.model_with_buckets(\n                self.encoder_inputs, self.decoder_inputs, targets,\n                self.target_weights, xy_buckets, lambda x, y: decode_wrapper(x, y, True),\n                softmax_loss_function=loss_fun)\n        else:\n            self.outputs, self.loss =  tf.nn.seq2seq.model_with_buckets(\n                self.encoder_inputs, self.decoder_inputs, targets,\n                self.target_weights, xy_buckets, lambda x, y: decode_wrapper(x, y, False),\n                softmax_loss_function=loss_fun)\n        \n        # Gradients and SGD update operation for training the model.\n        if not forward_only:\n            # Initialize gradient and update functions\n            self.gradient_norms = []\n            self.update_funs = []\n            # Create a optimizer to use for each data bucket\n            my_optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n            for b in range(len(xy_buckets)):\n                # For each bucket, get the gradients\n                gradients = tf.gradients(self.losses[b], tf.trainable_variables())\n                # Clip the gradients\n                clipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient)\n                self.gradient_norms.append(norm)\n                # Get the gradient update step for each variable\n                temp_optimizer = my_optimizer.apply_gradients(zip(clipped_gradients, tf.trainable_variables()),\n                                                              global_step=self.global_step)\n                self.update_funs.append(temp_optimizer)\n\n        self.saver = tf.train.Saver(tf.global_variables())\n        \n    # Define how to step forward (or backward) in the model\n    def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n             bucket_id, forward_only):\n        # Check if the sizes match.\n        encoder_size, decoder_size = self.buckets[bucket_id]\n        if len(encoder_inputs) != encoder_size:\n            raise ValueError(""Encoder length must be equal to the one in bucket,""\n                           "" %d != %d."" % (len(encoder_inputs), encoder_size))\n        if len(decoder_inputs) != decoder_size:\n            raise ValueError(""Decoder length must be equal to the one in bucket,""\n                           "" %d != %d."" % (len(decoder_inputs), decoder_size))\n        if len(target_weights) != decoder_size:\n            raise ValueError(""Weights length must be equal to the one in bucket,""\n                           "" %d != %d."" % (len(target_weights), decoder_size))\n\n        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n        input_feed = {}\n        for l in range(encoder_size):\n            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n        for l in range(decoder_size):\n            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n            input_feed[self.target_weights[l].name] = target_weights[l]\n\n        # Since our targets are decoder inputs shifted by one, we need one more.\n        last_target = self.decoder_inputs[decoder_size].name\n        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n        # Output feed: depends on whether we do a backward step or not.\n        if not forward_only:\n            output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                           self.gradient_norms[bucket_id],  # Gradient norm.\n                           self.losses[bucket_id]]  # Loss for this batch.\n        else:\n            output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n        for l in range(decoder_size):  # Output logits.\n            output_feed.append(self.outputs[bucket_id][l])\n        outputs = session.run(output_feed, input_feed)\n        if not forward_only:\n            return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n        else:\n            return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n    \n    # Define batch iteration function\n    def batch_iter(self, data, bucket_num):\n        decoder_len = self.y_buckets[bucket_num]\n        encoder_len = self.x_buckets[bucket_num]\n        batch_ix = np.random.choice(range(len(data[bucket_num])),\n                                    size = self.batch_size)\n        batch_data = [data[bucket_num][ix] for ix in batch_ix]\n        encoder_inputs = [x[0] for x in batch_data]\n        decoder_inputs = [x[1] for x in batch_data]\n        \n        # Pad encoder inputs with zeros\n        encoder_inputs = [(x + [0]*encoder_len)[:encoder_len] for x in encoder_inputs]\n        \n        # Put a \'1\' at the start of decoder, and pad end with zeros\n        decoder_inputs = [([1] + x + [0]*decoder_len)[:(decoder_len)] for x in decoder_inputs]\n        \n        # Transpose the inputs/outputs into list of arrays, each array is the i-th element\n        encoder_inputs_t = [np.array(x) for x in zip(*encoder_inputs)]\n        decoder_inputs_t = [np.array(x) for x in zip(*decoder_inputs)]\n        \n        # Create batch weights (0 for padding, 1 otherwise)\n        target_weights = np.ones(shape=np.array(decoder_inputs_t).shape)\n        zero_ix = [[(row, col) for col, c_val in enumerate(rows) if c_val==0] for row, rows in enumerate(decoder_inputs_t)]\n        zero_ix = [val for sublist in zero_ix for val in sublist if sublist]\n        # Set batch weights to zero\n        for row, col in zero_ix:\n            target_weights[row,col]=0\n        \n        # Need to roll the target weights (weights point to the next case)\n        batch_weights = np.roll(batch_weights, -1, axis=0)\n        # Last row should be all zeros\n        target_weights[-1,:] = [0]*self.batch_size\n        # Make weights a list of arrays\n        target_weights = [np.array(r, dtype=np.float32) for r in target_weights]\n        \n        return(encoder_inputs_t, decoder_inputs_t, target_weights)'"
09_Recurrent_Neural_Networks/06_Training_A_Siamese_Similarity_Measure/06_siamese_similarity_driver.py,10,"b'# -*- coding: utf-8 -*-\n# Siamese Address Similarity with TensorFlow (Driver File)\n#------------------------------------------\n#\n# Here, we show how to perform address matching\n#   with a Siamese RNN model\n\nimport random\nimport string\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\nimport siamese_similarity_model as model\n\n# Start a graph session\nsess = tf.Session()\n\n# Model parameters\nbatch_size = 200\nn_batches = 300\nmax_address_len = 20\nmargin = 0.25\nnum_features = 50\ndropout_keep_prob = 0.8\n\n\n# Function to randomly create one typo in a string w/ a probability\ndef create_typo(s):\n    rand_ind = random.choice(range(len(s)))\n    s_list = list(s)\n    s_list[rand_ind]=random.choice(string.ascii_lowercase + \'0123456789\')\n    s = \'\'.join(s_list)\n    return s\n\n# Generate data\nstreet_names = [\'abbey\', \'baker\', \'canal\', \'donner\', \'elm\', \'fifth\',\n                \'grandvia\', \'hollywood\', \'interstate\', \'jay\', \'kings\']\nstreet_types = [\'rd\', \'st\', \'ln\', \'pass\', \'ave\', \'hwy\', \'cir\', \'dr\', \'jct\']\n\n# Define test addresses\ntest_queries = [\'111 abbey ln\', \'271 doner cicle\',\n                \'314 king avenue\', \'tensorflow is fun\']\ntest_references = [\'123 abbey ln\', \'217 donner cir\', \'314 kings ave\',\n                   \'404 hollywood st\', \'tensorflow is so fun\']\n\n# Get a batch of size n, half of which is similar addresses, half are not\ndef get_batch(n):\n    # Generate a list of reference addresses with similar addresses that have\n    # a typo.\n    numbers = [random.randint(1, 9999) for i in range(n)]\n    streets = [random.choice(street_names) for i in range(n)]\n    street_suffs = [random.choice(street_types) for i in range(n)]\n    full_streets = [str(w) + \' \' + x + \' \' + y for w,x,y in zip(numbers, streets, street_suffs)]\n    typo_streets = [create_typo(x) for x in full_streets]\n    reference = [list(x) for x in zip(full_streets, typo_streets)]\n    \n    # Shuffle last half of them for training on dissimilar addresses\n    half_ix = int(n/2)\n    bottom_half = reference[half_ix:]\n    true_address = [x[0] for x in bottom_half]\n    typo_address = [x[1] for x in bottom_half]\n    typo_address = list(np.roll(typo_address, 1))\n    bottom_half = [[x,y] for x,y in zip(true_address, typo_address)]\n    reference[half_ix:] = bottom_half\n    \n    # Get target similarities (1\'s for similar, -1\'s for non-similar)\n    target = [1]*(n-half_ix) + [-1]*half_ix\n    reference = [[x,y] for x,y in zip(reference, target)]\n    return reference\n    \n\n# Define vocabulary dictionary (remember to save \'0\' for padding)\nvocab_chars = string.ascii_lowercase + \'0123456789 \'\nvocab2ix_dict = {char:(ix+1) for ix, char in enumerate(vocab_chars)}\nvocab_length = len(vocab_chars) + 1\n\n# Define vocab one-hot encoding\ndef address2onehot(address,\n                   vocab2ix_dict = vocab2ix_dict,\n                   max_address_len = max_address_len):\n    # translate address string into indices\n    address_ix = [vocab2ix_dict[x] for x in list(address)]\n\n    # Pad or crop to max_address_len\n    address_ix = (address_ix + [0]*max_address_len)[0:max_address_len]\n    return address_ix\n\n\n# Define placeholders\naddress1_ph = tf.placeholder(tf.int32, [None, max_address_len], name=""address1_ph"")\naddress2_ph = tf.placeholder(tf.int32, [None, max_address_len], name=""address2_ph"")\n    \ny_target_ph = tf.placeholder(tf.int32, [None], name=""y_target_ph"")\ndropout_keep_prob_ph = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n\n# Create embedding lookup\nidentity_mat = tf.diag(tf.ones(shape=[vocab_length]))\naddress1_embed = tf.nn.embedding_lookup(identity_mat, address1_ph)\naddress2_embed = tf.nn.embedding_lookup(identity_mat, address2_ph)\n\n# Define Model\ntext_snn = model.snn(address1_embed, address2_embed, dropout_keep_prob_ph,\n                     vocab_length, num_features, max_address_len)\n\n# Define Accuracy\nbatch_accuracy = model.accuracy(text_snn, y_target_ph)\n# Define Loss\nbatch_loss = model.loss(text_snn, y_target_ph, margin)\n# Define Predictions\npredictions = model.get_predictions(text_snn)\n\n# Declare optimizer\noptimizer = tf.train.AdamOptimizer(0.01)\n# Apply gradients\ntrain_op = optimizer.minimize(batch_loss)\n\n# Initialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Train loop\ntrain_loss_vec = []\ntrain_acc_vec = []\nfor b in range(n_batches):\n    # Get a batch of data\n    batch_data = get_batch(batch_size)\n    # Shuffle data\n    np.random.shuffle(batch_data)\n    # Parse addresses and targets\n    input_addresses = [x[0] for x in batch_data]\n    target_similarity = np.array([x[1] for x in batch_data])\n    address1 = np.array([address2onehot(x[0]) for x in input_addresses])\n    address2 = np.array([address2onehot(x[1]) for x in input_addresses])\n    \n    train_feed_dict = {address1_ph: address1,\n                       address2_ph: address2,\n                       y_target_ph: target_similarity,\n                       dropout_keep_prob_ph: dropout_keep_prob}\n\n    _, train_loss, train_acc = sess.run([train_op, batch_loss, batch_accuracy],\n                                        feed_dict=train_feed_dict)\n    # Save train loss and accuracy\n    train_loss_vec.append(train_loss)\n    train_acc_vec.append(train_acc)\n    # Print out statistics\n    if b%10==0:\n        print(\'Training Metrics, Batch {0}: Loss={1:.3f}, Accuracy={2:.3f}.\'.format(b, train_loss, train_acc))\n\n\n# Calculate the nearest addresses for test inputs\n# First process the test_queries and test_references\ntest_queries_ix = np.array([address2onehot(x) for x in test_queries])\ntest_references_ix = np.array([address2onehot(x) for x in test_references])\nnum_refs = test_references_ix.shape[0]\nbest_fit_refs = []\nfor query in test_queries_ix:\n    test_query = np.repeat(np.array([query]), num_refs, axis=0)\n    test_feed_dict = {address1_ph: test_query,\n                      address2_ph: test_references_ix,\n                      y_target_ph: target_similarity,\n                      dropout_keep_prob_ph: 1.0}\n    test_out = sess.run(text_snn, feed_dict=test_feed_dict)\n    best_fit = test_references[np.argmax(test_out)]\n    best_fit_refs.append(best_fit)\n\nprint(\'Query Addresses: {}\'.format(test_queries))\nprint(\'Model Found Matches: {}\'.format(best_fit_refs))\n\n# Plot the loss and accuracy\nplt.plot(train_loss_vec, \'k-\', lw=2, label=\'Batch Loss\')\nplt.plot(train_acc_vec, \'r:\', label=\'Batch Accuracy\')\nplt.xlabel(\'Iterations\')\nplt.ylabel(\'Accuracy and Loss\')\nplt.title(\'Accuracy and Loss of Siamese RNN\')\nplt.grid()\nplt.legend(loc=\'lower right\')\nplt.show()\n'"
09_Recurrent_Neural_Networks/06_Training_A_Siamese_Similarity_Measure/siamese_similarity_model.py,42,"b'# -*- coding: utf-8 -*-\n# Siamese Address Similarity with TensorFlow (Model File)\n#------------------------------------------\n#\n# Here, we show how to perform address matching\n#   with a Siamese RNN model\n\nimport tensorflow as tf\n\n\ndef snn(address1, address2, dropout_keep_prob,\n        vocab_size, num_features, input_length):\n    \n    # Define the siamese double RNN with a fully connected layer at the end\n    def siamese_nn(input_vector, num_hidden):\n        cell_unit = tf.contrib.rnn.BasicLSTMCell#tf.nn.rnn_cell.BasicLSTMCell\n        \n        # Forward direction cell\n        lstm_forward_cell = cell_unit(num_hidden, forget_bias=1.0)\n        lstm_forward_cell = tf.contrib.rnn.DropoutWrapper(lstm_forward_cell, output_keep_prob=dropout_keep_prob)\n        \n        # Backward direction cell\n        lstm_backward_cell = cell_unit(num_hidden, forget_bias=1.0)\n        lstm_backward_cell = tf.contrib.rnn.DropoutWrapper(lstm_backward_cell, output_keep_prob=dropout_keep_prob)\n    \n        # Split title into a character sequence\n        input_embed_split = tf.split(axis=1, num_or_size_splits=input_length, value=input_vector)\n        input_embed_split = [tf.squeeze(x, axis=[1]) for x in input_embed_split]\n        \n        # Create bidirectional layer\n        try:\n            outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_forward_cell,\n                                                                    lstm_backward_cell,\n                                                                    input_embed_split,\n                                                                    dtype=tf.float32)\n        except Exception:\n            outputs = tf.contrib.rnn.static_bidirectional_rnn(lstm_forward_cell,\n                                                              lstm_backward_cell,\n                                                              input_embed_split,\n                                                              dtype=tf.float32)\n        # Average The output over the sequence\n        temporal_mean = tf.add_n(outputs) / input_length\n        \n        # Fully connected layer\n        output_size = 10\n        A = tf.get_variable(name=""A"", shape=[2*num_hidden, output_size],\n                            dtype=tf.float32,\n                            initializer=tf.random_normal_initializer(stddev=0.1))\n        b = tf.get_variable(name=""b"", shape=[output_size], dtype=tf.float32,\n                            initializer=tf.random_normal_initializer(stddev=0.1))\n        \n        final_output = tf.matmul(temporal_mean, A) + b\n        final_output = tf.nn.dropout(final_output, dropout_keep_prob)\n        \n        return(final_output)\n        \n    output1 = siamese_nn(address1, num_features)\n    # Declare that we will use the same variables on the second string\n    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n        output2 = siamese_nn(address2, num_features)\n    \n    # Unit normalize the outputs\n    output1 = tf.nn.l2_normalize(output1, 1)\n    output2 = tf.nn.l2_normalize(output2, 1)\n    # Return cosine distance\n    #   in this case, the dot product of the norms is the same.\n    dot_prod = tf.reduce_sum(tf.multiply(output1, output2), 1)\n    \n    return dot_prod\n\n\ndef get_predictions(scores):\n    predictions = tf.sign(scores, name=""predictions"")\n    return predictions\n\n\ndef loss(scores, y_target, margin):\n    # Calculate the positive losses\n    pos_loss_term = 0.25 * tf.square(tf.subtract(1., scores))\n    \n    # If y-target is -1 to 1, then do the following\n    pos_mult = tf.add(tf.multiply(0.5, tf.cast(y_target, tf.float32)), 0.5)\n    # Else if y-target is 0 to 1, then do the following\n    pos_mult = tf.cast(y_target, tf.float32)\n    \n    # Make sure positive losses are on similar strings\n    positive_loss = tf.multiply(pos_mult, pos_loss_term)\n    \n    # Calculate negative losses, then make sure on dissimilar strings\n    \n    # If y-target is -1 to 1, then do the following:\n    neg_mult = tf.add(tf.multiply(-0.5, tf.cast(y_target, tf.float32)), 0.5)\n    # Else if y-target is 0 to 1, then do the following\n    neg_mult = tf.subtract(1., tf.cast(y_target, tf.float32))\n    \n    negative_loss = neg_mult*tf.square(scores)\n    \n    # Combine similar and dissimilar losses\n    loss = tf.add(positive_loss, negative_loss)\n    \n    # Create the margin term.  This is when the targets are 0.,\n    #  and the scores are less than m, return 0.\n    \n    # Check if target is zero (dissimilar strings)\n    target_zero = tf.equal(tf.cast(y_target, tf.float32), 0.)\n    # Check if cosine outputs is smaller than margin\n    less_than_margin = tf.less(scores, margin)\n    # Check if both are true\n    both_logical = tf.logical_and(target_zero, less_than_margin)\n    both_logical = tf.cast(both_logical, tf.float32)\n    # If both are true, then multiply by (1-1)=0.\n    multiplicative_factor = tf.cast(1. - both_logical, tf.float32)\n    total_loss = tf.multiply(loss, multiplicative_factor)\n    \n    # Average loss over batch\n    avg_loss = tf.reduce_mean(total_loss)\n    return avg_loss\n\n\ndef accuracy(scores, y_target):\n    predictions = get_predictions(scores)\n    # Cast into integers (outputs can only be -1 or +1)\n    y_target_int = tf.cast(y_target, tf.int32)\n    # Change targets from (0,1) --> (-1, 1)\n    #    via (2 * x - 1)\n    #y_target_int = tf.sub(tf.mul(y_target_int, 2), 1)\n    predictions_int = tf.cast(tf.sign(predictions), tf.int32)\n    correct_predictions = tf.equal(predictions_int, y_target_int)\n    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n    return accuracy'"
10_Taking_TensorFlow_to_Production/01_Implementing_Unit_Tests/01_implementing_unit_tests.py,38,"b""# -*- coding: utf-8 -*-\n# Implementing Unit Tests\n#----------------------------------\n#\n# Here, we will show how to implement different unit tests\n#  on the MNIST example\n\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Start a graph session\nsess = tf.Session()\n\n# Load data\ndata_dir = 'temp'\nmnist = tf.keras.datasets.mnist\n(train_xdata, train_labels), (test_xdata, test_labels) = mnist.load_data()\ntrain_xdata = train_xdata / 255.0\ntest_xdata = test_xdata / 255.0\n\n# Set model parameters\nbatch_size = 100\nlearning_rate = 0.005\nevaluation_size = 100\nimage_width = train_xdata[0].shape[0]\nimage_height = train_xdata[0].shape[1]\ntarget_size = max(train_labels) + 1\nnum_channels = 1 # greyscale = 1 channel\ngenerations = 100\neval_every = 5\nconv1_features = 25\nconv2_features = 50\nmax_pool_size1 = 2 # NxN window for 1st max pool layer\nmax_pool_size2 = 2 # NxN window for 2nd max pool layer\nfully_connected_size1 = 100\ndropout_prob = 0.75\n\n# Declare model placeholders\nx_input_shape = (batch_size, image_width, image_height, num_channels)\nx_input = tf.placeholder(tf.float32, shape=x_input_shape)\ny_target = tf.placeholder(tf.int32, shape=(batch_size))\neval_input_shape = (evaluation_size, image_width, image_height, num_channels)\neval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\neval_target = tf.placeholder(tf.int32, shape=(evaluation_size))\n\n# Dropout placeholder\ndropout = tf.placeholder(tf.float32, shape=())\n\n# Declare model parameters\nconv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features],\n                                               stddev=0.1, dtype=tf.float32))\nconv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n\nconv2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features],\n                                               stddev=0.1, dtype=tf.float32))\nconv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))\n\n# fully connected variables\nresulting_width = image_width // (max_pool_size1 * max_pool_size2)\nresulting_height = image_height // (max_pool_size1 * max_pool_size2)\nfull1_input_size = resulting_width * resulting_height * conv2_features\nfull1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1], stddev=0.1, dtype=tf.float32))\nfull1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\nfull2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size],\n                                               stddev=0.1, dtype=tf.float32))\nfull2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n\n\n# Initialize Model Operations\ndef my_conv_net(input_data):\n    # First Conv-ReLU-MaxPool Layer\n    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')\n    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],\n                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n\n    # Second Conv-ReLU-MaxPool Layer\n    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')\n    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],\n                               strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')\n\n    # Transform Output into a 1xN layer for next fully connected layer\n    final_conv_shape = max_pool2.get_shape().as_list()\n    final_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]\n    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])\n\n    # First Fully Connected Layer\n    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n\n    # Second Fully Connected Layer\n    final_model_output = tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)\n    \n    # Add dropout\n    final_model_output = tf.nn.dropout(final_model_output, dropout)\n    \n    return final_model_output\n\n\nmodel_output = my_conv_net(x_input)\ntest_model_output = my_conv_net(eval_input)\n\n# Declare Loss Function (softmax cross entropy)\nloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))\n\n# Create a prediction function\nprediction = tf.nn.softmax(model_output)\ntest_prediction = tf.nn.softmax(test_model_output)\n\n\n# Create accuracy function\ndef get_accuracy(logits, targets):\n    batch_predictions = np.argmax(logits, axis=1)\n    num_correct = np.sum(np.equal(batch_predictions, targets))\n    return 100. * num_correct/batch_predictions.shape[0]\n\n\n# Create an optimizer\nmy_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\ntrain_step = my_optimizer.minimize(loss)\n\n# Initialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n\n# Check values of tensors!\nclass drop_out_test(tf.test.TestCase):\n    # Make sure that we don't drop too much\n    def dropout_greaterthan(self):\n        with self.test_session():\n            self.assertGreater(dropout.eval(), 0.25)\n\n\n# Test accuracy function\nclass accuracy_test(tf.test.TestCase):\n    # Make sure accuracy function behaves correctly\n    def accuracy_exact_test(self):\n        with self.test_session():\n            test_preds = [[0.9, 0.1], [0.01, 0.99]]\n            test_targets = [0, 1]\n            test_acc = get_accuracy(test_preds, test_targets)\n            self.assertEqual(test_acc.eval(), 100.)\n\n\n# Test tensorshape\nclass shape_test(tf.test.TestCase):\n    # Make sure our model output is size [batch_size, num_classes]\n    def output_shape_test(self):\n        with self.test_session():\n            numpy_array = np.ones([batch_size, target_size])\n            self.assertShapeEqual(numpy_array, model_output)\n\n\ndef main(argv):\n    # Start training loop\n    train_loss = []\n    train_acc = []\n    test_acc = []\n    for i in range(generations):\n        rand_index = np.random.choice(len(train_xdata), size=batch_size)\n        rand_x = train_xdata[rand_index]\n        rand_x = np.expand_dims(rand_x, 3)\n        rand_y = train_labels[rand_index]\n        train_dict = {x_input: rand_x, y_target: rand_y, dropout: dropout_prob}\n\n        sess.run(train_step, feed_dict=train_dict)\n        temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n        temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n\n        if (i + 1) % eval_every == 0:\n            eval_index = np.random.choice(len(test_xdata), size=evaluation_size)\n            eval_x = test_xdata[eval_index]\n            eval_x = np.expand_dims(eval_x, 3)\n            eval_y = test_labels[eval_index]\n            test_dict = {eval_input: eval_x, eval_target: eval_y, dropout: 1.0}\n            test_preds = sess.run(test_prediction, feed_dict=test_dict)\n            temp_test_acc = get_accuracy(test_preds, eval_y)\n\n            # Record and print results\n            train_loss.append(temp_train_loss)\n            train_acc.append(temp_train_acc)\n            test_acc.append(temp_test_acc)\n            acc_and_loss = [(i + 1), temp_train_loss, temp_train_acc, temp_test_acc]\n            acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n            print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n\n\nif __name__ == '__main__':\n    cmd_args = sys.argv\n    if len(cmd_args) > 1 and cmd_args[1] == 'test':\n        # Perform unit tests\n        tf.test.main(argv=cmd_args[1:])\n    else:\n        # Run TF App\n        tf.app.run(main=None, argv=cmd_args)\n"""
10_Taking_TensorFlow_to_Production/02_Using_Multiple_Devices/02_using_multiple_devices.py,19,"b""# -*- coding: utf-8 -*-\n# Using Multiple Devices\n#----------------------------------\n#\n# This function gives us the ways to use\n#  multiple devices (executors) in TensorFlow.\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# To find out where placement occurs, set 'log_device_placement'\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\nc = tf.matmul(a, b)\n\n# Runs the op.\nprint(sess.run(c))\n\n\n# If we load a graph and want device placement to be forgotten,\n#  we set a parameter in our session:\nconfig = tf.ConfigProto()\nconfig.allow_soft_placement = True\nsess_soft = tf.Session(config=config)\n\n# GPUs\n#---------------------------------\n# Note that the GPU must have a compute capability > 3.5 for TF to use.\n# http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability\n\n\n# Careful with GPU memory allocation, TF never releases it.  TF starts with almost\n# all of the GPU memory allocated.  We can slowly grow to that limit with an\n# option setting:\n\nconfig.gpu_options.allow_growth = True\nsess_grow = tf.Session(config=config)\n\n# Also, we can limit the size of GPU memory used, with the following option\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nsess_limited = tf.Session(config=config)\n\n\n# How to set placements on multiple devices.\n# Here, assume we have three devies CPU:0, GPU:0, and GPU:1\nif tf.test.is_built_with_cuda():\n    with tf.device('/cpu:0'):\n        a = tf.constant([1.0, 3.0, 5.0], shape=[1, 3])\n        b = tf.constant([2.0, 4.0, 6.0], shape=[3, 1])\n        \n        with tf.device('/gpu:1'):\n            c = tf.matmul(a,b)\n            c = tf.reshape(c, [-1])\n        \n        with tf.device('/gpu:2'):\n            d = tf.matmul(b,a)\n            flat_d = tf.reshape(d, [-1])\n        \n        combined = tf.multiply(c, flat_d)\n    print(sess.run(combined))\n"""
10_Taking_TensorFlow_to_Production/03_Parallelizing_TensorFlow/03_parallelizing_tensorflow.py,9,"b'# -*- coding: utf-8 -*-\n# Parallelizing TensorFlow\n#----------------------------------\n#\n# We will show how to use TensorFlow distributed\n\nimport tensorflow as tf\n\n# We will setup a local cluster (on localhost)\n\n# Cluster for 2 local workers (tasks 0 and 1):\ncluster = tf.train.ClusterSpec({\'local\': [\'localhost:2222\', \'localhost:2223\']})\n# Server definition:\nserver = tf.train.Server(cluster, job_name=""local"", task_index=0)\nserver = tf.train.Server(cluster, job_name=""local"", task_index=1)\n# Finish and add\n# server.join()\n\n# Have each worker do a task\n# Worker 0 : create matrices\n# Worker 1 : calculate sum of all elements\nmat_dim = 25\nmatrix_list = {}\n\nwith tf.device(\'/job:local/task:0\'):\n    for i in range(0, 2):\n        m_label = \'m_{}\'.format(i)\n        matrix_list[m_label] = tf.random_normal([mat_dim, mat_dim])\n\n# Have each worker calculate the Cholesky Decomposition\nsum_outs = {}\nwith tf.device(\'/job:local/task:1\'):\n    for i in range(0, 2):\n        A = matrix_list[\'m_{}\'.format(i)]\n        sum_outs[\'m_{}\'.format(i)] = tf.reduce_sum(A)\n\n    # Sum all the cholesky decompositions\n    summed_out = tf.add_n(list(sum_outs.values()))\n\nwith tf.Session(server.target) as sess:\n    result = sess.run(summed_out)\n    print(\'Summed Values:{}\'.format(result))\n'"
10_Taking_TensorFlow_to_Production/04_Production_Tips/04_production_tips_for_tf.py,13,"b'# -*- coding: utf-8 -*-\n# Tips for TensorFlow to Production\n#----------------------------------\n#\n# Various Tips for Taking TensorFlow to Production\n\n############################################\n#\n# THIS SCRIPT IS NOT RUNNABLE.\n#  -it only contains tips for production code\n#\n############################################\n\n# Also you can clear the default graph from memory\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Saving Models\n# File types created from saving:    \n# checkpoint file:  Holds info on where the most recent models are\n# events file:      Strictly for viewing graph in Tensorboard\n# pbtxt file:       Textual protobufs file (uncompressed), used for debugging\n# chkp file:        Holds data and model weights (large file)\n# meta chkp files:  Model Graph and Meta-data (learning rate and operations)\n\n\n# Saving data pipeline structures (vocabulary, )\nword_list = [\'to\', \'be\', \'or\', \'not\', \'to\', \'be\']\nvocab_list = list(set(word_list))\nvocab2ix_dict = dict(zip(vocab_list, range(len(vocab_list))))\nix2vocab_dict = {val:key for key,val in vocab2ix_dict.items()}\n\n# Save vocabulary\nimport json\nwith open(\'vocab2ix_dict.json\', \'w\') as file_conn:\n    json.dump(vocab2ix_dict, file_conn)\n\n# Load vocabulary\nwith open(\'vocab2ix_dict.json\', \'r\') as file_conn:\n    vocab2ix_dict = json.load(file_conn)\n\n# After model declaration, add a saving operations\nsaver = tf.train.Saver()\n# Then during training, save every so often, referencing the training generation\nfor i in range(generations):\n    ...\n    if i%save_every == 0:\n        saver.save(sess, \'my_model\', global_step=step)\n\n# Can also save only specific variables:\nsaver = tf.train.Saver({""my_var"": my_variable})\n\n\n# other options for saver are \'keep checkpoint_every_n_hours\'\n#      also \'max_to_keep\'= default 5.\n        \n# Be sure to name operations, and variables for easy loading for referencing later\nconv_weights = tf.Variable(tf.random_normal(), name=\'conv_weights\')\nloss = tf.reduce_mean(... , name=\'loss\')\n\n# Instead of tyring argparse and main(), TensorFlow provides an \'app\' function\n#  to handle running and loading of arguments\n\n# At the beginning of the file, define the flags.\ntf.flags.DEFINE_string(""worker_locations"", """", ""List of worker addresses."")\ntf.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\ntf.flags.DEFINE_integer(\'generations\', 1000, \'Number of training generations.\')\ntf.flags.DEFINE_boolean(\'run_unit_tests\', False, \'If true, run tests.\')\nFLAGS = tf.flags.FLAGS\n\n# Need to define a \'main\' function for the app to run\ndef main(_):\n    worker_ips = FLAGS.worker_locations.split("","")\n    learning_rate = FLAGS.learning_rate\n    generations = FLAGS.generations\n    run_unit_tests = FLAGS.run_unit_tests\n\n# Run the TensorFlow app\nif __name__ == ""__main__"":\n    # The following is looking for a ""main()"" function to run and will pass.\n    tf.app.run()\n    # Can modify this to be more custom:\n    tf.app.run(main=my_main_function(), argv=my_arguments)\n\n\n# Use of TensorFlow\'s built in logging:\n# Five levels: DEBUG, INFO, WARN, ERROR, and FATAL\ntf.logging.set_verbosity(tf.logging.WARN)\n# WARN is the default value, but to see more information, you can set it to\n#    INFO or DEBUG\ntf.logging.set_verbosity(tf.logging.DEBUG)\n# Note: \'DEBUG\' is quite verbose.\n\n'"
10_Taking_TensorFlow_to_Production/05_Production_Example/05_production_ex_eval.py,12,"b'# -*- coding: utf-8 -*-\n# TensorFlow Production Example (Evaluating)\n#----------------------------------\n#\n# We pull together everything and create an example\n#    of best tensorflow production tips\n#\n# The example we will productionalize is the spam/ham RNN\n#    from the RNN Chapter.\n\nimport os\nimport re\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\ntf.flags.DEFINE_string(""storage_folder"", ""temp"", ""Where to store model and data."")\ntf.flags.DEFINE_bool(\'model_file\', False, \'Model file location.\')\ntf.flags.DEFINE_bool(\'run_unit_tests\', False, \'If true, run tests.\')\nFLAGS = tf.flags.FLAGS\n\n\n# Create a text cleaning function\ndef clean_text(text_string):\n    text_string = re.sub(r\'([^\\s\\w]|_|[0-9])+\', \'\', text_string)\n    text_string = "" "".join(text_string.split())\n    text_string = text_string.lower()\n    return text_string\n\n\n# Load vocab processor\ndef load_vocab():\n    vocab_path = os.path.join(FLAGS.storage_folder, ""vocab"")\n    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n    return vocab_processor\n\n\n# Process input data:\ndef process_data(input_data, vocab_processor):\n    input_data = clean_text(input_data)\n    input_data = input_data.split()\n    processed_input = np.array(list(vocab_processor.transform(input_data)))\n    return processed_input\n\n\n# Get input function\ndef get_input_data():\n    """"""\n    For this function, we just prompt the user for a text message to evaluate\n        But this function could also potentially read a file in as well.\n    """"""\n    input_text = input(""Please enter a text message to evaluate: "")\n    vocab_processor = load_vocab()\n    return process_data(input_text, vocab_processor)\n\n\n# Test clean_text function\nclass clean_test(tf.test.TestCase):\n    # Make sure cleaning function behaves correctly\n    def clean_string_test(self):\n        with self.test_session():\n            test_input = \'--TensorFlow\\\'s so Great! Don\\t you think so?   \'\n            test_expected = \'tensorflows so great don you think so\'\n            test_out = clean_text(test_input)\n            self.assertEqual(test_expected, test_out)\n\n\n# Main function\ndef main(args):\n    # Get flags\n    storage_folder = FLAGS.storage_folder\n    \n    # Get user input text\n    x_data = get_input_data()\n    \n    # Load model\n    graph = tf.Graph()\n    with graph.as_default():\n        sess = tf.Session()\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{}.meta"".format(os.path.join(storage_folder, ""model.ckpt"")))\n            saver.restore(sess, os.path.join(storage_folder, ""model.ckpt""))\n\n            # Get the placeholders from the graph by name\n            x_data_ph = graph.get_operation_by_name(""x_data_ph"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            probability_outputs = graph.get_operation_by_name(""probability_outputs"").outputs[0]\n\n            # Make the prediction\n            eval_feed_dict = {x_data_ph: x_data, dropout_keep_prob: 1.0}\n            probability_prediction = sess.run(tf.reduce_mean(probability_outputs, 0), eval_feed_dict)\n            \n            # Print output (Or save to file or DB connection?)\n            print(\'Probability of Spam: {:.4}\'.format(probability_prediction[1]))\n\n\n# Run main module/tf App\nif __name__ == ""__main__"":\n    if FLAGS.run_unit_tests:\n        # Perform unit tests\n        tf.test.main()\n    else:\n        # Run evaluation\n        tf.app.run()'"
10_Taking_TensorFlow_to_Production/05_Production_Example/05_production_ex_train.py,45,"b'# -*- coding: utf-8 -*-\n# TensorFlow Production Example (Training)\n#----------------------------------\n#\n# We pull together everything and create an example\n#    of best tensorflow production tips\n#\n# The example we will productionalize is the spam/ham RNN\n#    from Chapter 9 (RNNs), section 2\n\nimport os\nimport re\nimport io\nimport sys\nimport requests\nimport numpy as np\nimport tensorflow as tf\nfrom zipfile import ZipFile\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Define App Flags\ntf.flags.DEFINE_string(""storage_folder"", ""temp"", ""Where to store model and data."")\ntf.flags.DEFINE_float(\'learning_rate\', 0.0005, \'Initial learning rate.\')\ntf.flags.DEFINE_float(\'dropout_prob\', 0.5, \'Per to keep probability for dropout.\')\ntf.flags.DEFINE_integer(\'epochs\', 20, \'Number of epochs for training.\')\ntf.flags.DEFINE_integer(\'batch_size\', 250, \'Batch Size for training.\')\ntf.flags.DEFINE_integer(\'rnn_size\', 15, \'RNN feature size.\')\ntf.flags.DEFINE_integer(\'embedding_size\', 25, \'Word embedding size.\')\ntf.flags.DEFINE_integer(\'min_word_frequency\', 20, \'Word frequency cutoff.\')\ntf.flags.DEFINE_boolean(\'run_unit_tests\', False, \'If true, run tests.\')\n\nFLAGS = tf.flags.FLAGS\n\n\n# Define how to get data\ndef get_data(storage_folder=FLAGS.storage_folder, data_file=""text_data.txt""):\n    """"""\n    This function gets the spam/ham data.  It will download it if it doesn\'t\n    already exist on disk (at specified folder/file location).\n    """"""\n    # Make a storage folder for models and data\n    if not os.path.exists(storage_folder):\n        os.makedirs(storage_folder)\n    \n    if not os.path.isfile(os.path.join(storage_folder, data_file)):\n        zip_url = \'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\'\n        r = requests.get(zip_url)\n        z = ZipFile(io.BytesIO(r.content))\n        file = z.read(\'SMSSpamCollection\')\n        # Format Data\n        text_data = file.decode()\n        text_data = text_data.encode(\'ascii\', errors=\'ignore\')\n        text_data = text_data.decode().split(\'\\n\')\n\n        # Save data to text file\n        with open(os.path.join(storage_folder, data_file), \'w\') as file_conn:\n            for text in text_data:\n                file_conn.write(""{}\\n"".format(text))\n    else:\n        # Open data from text file\n        text_data = []\n        with open(os.path.join(storage_folder, data_file), \'r\') as file_conn:\n            for row in file_conn:\n                text_data.append(row)\n        text_data = text_data[:-1]\n    text_data = [x.split(\'\\t\') for x in text_data if len(x) >= 1]\n    [y_data, x_data] = [list(x) for x in zip(*text_data)]\n    \n    return x_data, y_data\n\n\n# Create a text cleaning function\ndef clean_text(text_string):\n    text_string = re.sub(r\'([^\\s\\w]|_|[0-9])+\', \'\', text_string)\n    text_string = "" "".join(text_string.split())\n    text_string = text_string.lower()\n    return text_string\n\n\n# Test clean_text function\nclass clean_test(tf.test.TestCase):\n    # Make sure cleaning function behaves correctly\n    def clean_string_test(self):\n        with self.test_session():\n            test_input = \'--TensorFlow\\\'s so Great! Don\\t you think so?   \'\n            test_expected = \'tensorflows so great don you think so\'\n            test_out = clean_text(test_input)\n            self.assertEqual(test_expected, test_out)\n\n\n# Define RNN Model\ndef rnn_model(x_data_ph, vocab_size, embedding_size, rnn_size, dropout_keep_prob):\n    # Create embedding\n    embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n    embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data_ph)\n\n    # Define the RNN cell\n    cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)\n    output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\n    output = tf.nn.dropout(output, dropout_keep_prob)\n\n    # Get output of RNN sequence\n    output = tf.transpose(output, [1, 0, 2])\n    last = tf.gather(output, int(output.get_shape()[0]) - 1)\n\n    weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))\n    bias = tf.Variable(tf.constant(0.1, shape=[2]))\n    logits_out = tf.matmul(last, weight) + bias\n    \n    return logits_out\n\n\n# Define accuracy function\ndef get_accuracy(logits, actuals):\n    # Calulate if each output is correct\n    batch_acc = tf.equal(tf.argmax(logits, 1), tf.cast(actuals, tf.int64))\n    # Convert logical to float\n    batch_acc = tf.cast(batch_acc, tf.float32)\n    return batch_acc\n\n\n# Define main program\ndef main(args):\n    # Set verbosity to get more information from TensorFlow\n    tf.logging.set_verbosity(tf.logging.INFO)\n    \n    # Create a visualizer object for Tensorboard viewing\n    summary_writer = tf.summary.FileWriter(\'tensorboard\', tf.get_default_graph())\n    # Create tensorboard folder if not exists\n    if not os.path.exists(\'tensorboard\'):\n        os.makedirs(\'tensorboard\')\n    \n    # Set model parameters\n    storage_folder = FLAGS.storage_folder\n    learning_rate = FLAGS.learning_rate\n    run_unit_tests = FLAGS.run_unit_tests\n    epochs = FLAGS.epochs\n    batch_size = FLAGS.batch_size\n    rnn_size = FLAGS.rnn_size\n    embedding_size = FLAGS.embedding_size\n    min_word_frequency = FLAGS.min_word_frequency\n    \n    # Get text->spam/ham data\n    x_data, y_data = get_data()\n    \n    # Clean texts\n    x_data = [clean_text(x) for x in x_data]\n\n    # Change texts into numeric vectors\n    # Set a max sequence length for speeding up the computations.\n    # But we can easily set ""max_sequence_length = max([len(x) for x in x_data])"" as well.\n    max_sequence_length = 20\n    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,\n                                                                         min_frequency=min_word_frequency)\n    text_processed = np.array(list(vocab_processor.fit_transform(x_data)))\n    \n    # Save vocab processor (for loading and future evaluation)\n    vocab_processor.save(os.path.join(storage_folder, ""vocab""))\n    \n    # Shuffle and split data\n    text_processed = np.array(text_processed)\n    y_data = np.array([1 if x == \'ham\' else 0 for x in y_data])\n    shuffled_ix = np.random.permutation(np.arange(len(y_data)))\n    x_shuffled = text_processed[shuffled_ix]\n    y_shuffled = y_data[shuffled_ix]\n\n    # Split train/test set\n    ix_cutoff = int(len(y_shuffled)*0.80)\n    x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n    y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n    vocab_size = len(vocab_processor.vocabulary_)\n\n    with tf.Graph().as_default():\n        sess = tf.Session()\n        # Define placeholders\n        x_data_ph = tf.placeholder(tf.int32, [None, max_sequence_length], name=\'x_data_ph\')\n        y_output_ph = tf.placeholder(tf.int32, [None], name=\'y_output_ph\')\n        dropout_keep_prob = tf.placeholder(tf.float32, name=\'dropout_keep_prob\')\n\n        # Define Model\n        rnn_model_outputs = rnn_model(x_data_ph, vocab_size, embedding_size, rnn_size, dropout_keep_prob)\n\n        # Prediction\n        # Although we won\'t use the following operation, we declare and name\n        #   the probability outputs so that we can recall them later for evaluation\n        rnn_prediction = tf.nn.softmax(rnn_model_outputs, name=""probability_outputs"")\n        \n        # Loss function\n        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rnn_model_outputs, labels=y_output_ph)\n        # Remember that for this loss function, logits=float32, labels=int32\n        loss = tf.reduce_mean(losses, name=""loss"")\n\n        # Model Accuracy Operation\n        accuracy = tf.reduce_mean(get_accuracy(rnn_model_outputs, y_output_ph), name=""accuracy"")\n    \n        # Add scalar summaries for Tensorboard\n        with tf.name_scope(\'Scalar_Summaries\'):\n                tf.summary.scalar(\'Loss\', loss)\n                tf.summary.scalar(\'Accuracy\', accuracy)\n    \n        # Declare Optimizer/train step\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n        train_step = optimizer.minimize(loss)\n        \n        # Declare summary merging operation\n        summary_op = tf.summary.merge_all()\n    \n        # Create a graph/Variable saving/loading operations\n        saver = tf.train.Saver()    \n    \n        init = tf.global_variables_initializer()\n        sess.run(init)\n    \n        # Start training\n        for epoch in range(epochs):\n\n            # Shuffle training data\n            shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n            x_train = x_train[shuffled_ix]\n            y_train = y_train[shuffled_ix]\n            num_batches = int(len(x_train)/batch_size) + 1\n            #\n            for i in range(num_batches):\n                # Select train data\n                min_ix = i * batch_size\n                max_ix = np.min([len(x_train), ((i+1) * batch_size)])\n                x_train_batch = x_train[min_ix:max_ix]\n                y_train_batch = y_train[min_ix:max_ix]\n        \n                # Run train step\n                train_dict = {x_data_ph: x_train_batch,\n                              y_output_ph: y_train_batch,\n                              dropout_keep_prob: 0.5}\n                _, summary = sess.run([train_step, summary_op], feed_dict=train_dict)\n                \n                summary_writer = tf.summary.FileWriter(\'tensorboard\')\n                summary_writer.add_summary(summary, i)\n        \n            # Run loss and accuracy for training\n            temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)\n\n            test_dict = {x_data_ph: x_test, y_output_ph: y_test, dropout_keep_prob: 1.0}\n            temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)\n            \n            # Print Epoch Summary\n            print(\'Epoch: {}, Train Loss:{:.2}, Train Acc: {:.2}\'.format(epoch+1, temp_train_loss, temp_train_acc))\n            print(\'Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}\'.format(epoch+1, temp_test_loss, temp_test_acc))\n            \n            # Save model every epoch\n            saver.save(sess, os.path.join(storage_folder, ""model.ckpt""))\n\n\n# Run main module/tf App\nif __name__ == ""__main__"":\n    cmd_args = sys.argv\n    if len(cmd_args) > 1 and cmd_args[1] == \'test\':\n        # Perform unit tests\n        tf.test.main(argv=cmd_args[1:])\n    else:\n        # Run TF App\n        tf.app.run(main=None, argv=cmd_args)\n'"
10_Taking_TensorFlow_to_Production/06_Using_TensorFlow_Serving/06_Using_TensorFlow_Serving_Client.py,7,"b'# -*- coding: utf-8 -*-\n# Using TensorFlow Serving (CLIENT)\n#----------------------------------\n#\n# We show how to use ""TensorFlow Serving"", a model serving api from TensorFlow to serve a model.\n#\n# Pre-requisites:\n#  - Visit https://www.tensorflow.org/serving/setup\n#    and follow all the instructions on setting up TensorFlow Serving (including installing Bazel).\n#\n# The example we will query the TensorFlow-Serving-API we have running on port 9000\n\nimport os\nimport re\nimport grpc\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\ntf.flags.DEFINE_string(\'server\', \'9000\', \'PredictionService host\')\ntf.flags.DEFINE_string(\'port\', \'0.0.0.0\', \'PredictionService port\')\ntf.flags.DEFINE_string(\'data_dir\', \'temp\', \'Folder where vocabulary is.\')\nFLAGS = tf.flags.FLAGS\n\n\n# Def a functions to process texts into arrays of indices\n# Create a text cleaning function\ndef clean_text(text_string):\n    text_string = re.sub(r\'([^\\s\\w]|_|[0-9])+\', \'\', text_string)\n    text_string = "" "".join(text_string.split())\n    text_string = text_string.lower()\n    return text_string\n\n\n# Load vocab processor\ndef load_vocab():\n    vocab_path = os.path.join(FLAGS.data_dir, \'vocab\')\n    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n    return vocab_processor\n\n\n# Process input data:\ndef process_data(input_data):\n    vocab_processor = load_vocab()\n    input_data = [clean_text(x) for x in input_data]\n    processed_input = np.array(list(vocab_processor.transform(input_data)))\n    return processed_input\n\n\ndef get_results(data, server, port):\n    channel = grpc.insecure_channel(\':\'.join([server, port]))\n    stub = prediction_service_pb2.PredictionServiceStub(channel)\n    processed_data = process_data(data)\n\n    results = []\n    for input_x in processed_data:\n        request = predict_pb2.PredictRequest()\n        request.model_spec.name = \'spam_ham\'\n        request.model_spec.signature_name = \'predict_spam\'  # Change to predict spam\n        request.inputs[\'texts\'].CopyFrom(tf.contrib.util.make_tensor_proto(input_x, shape=[4, 20]))  # \'texts\'\n        prediction_future = stub.Predict(request)\n        prediction = prediction_future.result().outputs[\'scores\']\n        # prediction = np.array(prediction_future.result().outputs[\'scores\'].float_val)\n        results.append(prediction)\n    return results\n\n\ndef main(data):\n    if not FLAGS.server:\n        print(\'please specify server host:port\')\n        return\n    results = get_results(data, FLAGS.server, FLAGS.port)\n\n    for input_text, output_pred in zip(data, results):\n        print(\'Input text: {}, Prediction: {}\'.format(input_text, output_pred))\n\n\nif __name__ == \'__main__\':\n    # Get sample data, here you may feel free to change this to a file, cloud-address, user input, etc...\n    test_data = [\'Please respond ASAP to claim your prize !\',\n                 \'Hey, are you coming over for dinner tonight?\',\n                 \'Text 444 now to see the top users in your area\',\n                 \'drive safe, and thanks for visiting again!\']\n\n    tf.app.run(argv=test_data)\n'"
10_Taking_TensorFlow_to_Production/06_Using_TensorFlow_Serving/06_Using_TensorFlow_Serving_Train.py,60,"b'# -*- coding: utf-8 -*-\n# Using TensorFlow Serving\n#----------------------------------\n#\n# We show how to use ""TensorFlow Serving"", a model serving api from TensorFlow to serve a model.\n#\n# Pre-requisites:\n#  - Visit https://www.tensorflow.org/serving/setup\n#    and follow all the instructions on setting up TensorFlow Serving (including installing Bazel).\n#\n# The example we will productionalize is the spam/ham RNN\n#    from Chapter 9 (RNNs), section 2\n\nimport os\nimport re\nimport io\nimport sys\nimport requests\nimport numpy as np\nimport tensorflow as tf\nfrom zipfile import ZipFile\nfrom tensorflow.python.framework import ops\n\nops.reset_default_graph()\n\n# Define App Flags\ntf.flags.DEFINE_string(""storage_folder"", ""temp"", ""Where to store model and data."")\ntf.flags.DEFINE_float(\'learning_rate\', 0.0005, \'Initial learning rate.\')\ntf.flags.DEFINE_float(\'dropout_prob\', 0.5, \'Per to keep probability for dropout.\')\ntf.flags.DEFINE_integer(\'epochs\', 20, \'Number of epochs for training.\')\ntf.flags.DEFINE_integer(\'batch_size\', 250, \'Batch Size for training.\')\ntf.flags.DEFINE_integer(\'rnn_size\', 15, \'RNN feature size.\')\ntf.flags.DEFINE_integer(\'embedding_size\', 25, \'Word embedding size.\')\ntf.flags.DEFINE_integer(\'min_word_frequency\', 20, \'Word frequency cutoff.\')\ntf.flags.DEFINE_boolean(\'run_unit_tests\', False, \'If true, run tests.\')\n\nFLAGS = tf.flags.FLAGS\n\n\n# Define how to get data\ndef get_data(storage_folder=FLAGS.storage_folder, data_file=""text_data.txt""):\n    """"""\n    This function gets the spam/ham data.  It will download it if it doesn\'t\n    already exist on disk (at specified folder/file location).\n    """"""\n    # Make a storage folder for models and data\n    if not os.path.exists(storage_folder):\n        os.makedirs(storage_folder)\n\n    if not os.path.isfile(os.path.join(storage_folder, data_file)):\n        zip_url = \'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\'\n        r = requests.get(zip_url)\n        z = ZipFile(io.BytesIO(r.content))\n        file = z.read(\'SMSSpamCollection\')\n        # Format Data\n        text_data = file.decode()\n        text_data = text_data.encode(\'ascii\', errors=\'ignore\')\n        text_data = text_data.decode().split(\'\\n\')\n\n        # Save data to text file\n        with open(os.path.join(storage_folder, data_file), \'w\') as file_conn:\n            for text in text_data:\n                file_conn.write(""{}\\n"".format(text))\n    else:\n        # Open data from text file\n        text_data = []\n        with open(os.path.join(storage_folder, data_file), \'r\') as file_conn:\n            for row in file_conn:\n                text_data.append(row)\n        text_data = text_data[:-1]\n    text_data = [x.split(\'\\t\') for x in text_data if len(x) >= 1]\n    [y_data, x_data] = [list(x) for x in zip(*text_data)]\n\n    return x_data, y_data\n\n\n# Create a text cleaning function\ndef clean_text(text_string):\n    text_string = re.sub(r\'([^\\s\\w]|_|[0-9])+\', \'\', text_string)\n    text_string = "" "".join(text_string.split())\n    text_string = text_string.lower()\n    return text_string\n\n\n# Test clean_text function\nclass clean_test(tf.test.TestCase):\n    # Make sure cleaning function behaves correctly\n    def clean_string_test(self):\n        with self.test_session():\n            test_input = \'--TensorFlow\\\'s so Great! Don\\t you think so?   \'\n            test_expected = \'tensorflows so great don you think so\'\n            test_out = clean_text(test_input)\n            self.assertEqual(test_expected, test_out)\n\n\n# Define RNN Model\ndef rnn_model(x_data_ph, vocab_size, embedding_size, rnn_size, dropout_keep_prob):\n    # Create embedding\n    embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n    embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data_ph)\n\n    # Define the RNN cell\n    cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)\n    output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\n    output = tf.nn.dropout(output, dropout_keep_prob)\n\n    # Get output of RNN sequence\n    output = tf.transpose(output, [1, 0, 2])\n    last = tf.gather(output, int(output.get_shape()[0]) - 1)\n\n    weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))\n    bias = tf.Variable(tf.constant(0.1, shape=[2]))\n    logits_out = tf.matmul(last, weight) + bias\n\n    return logits_out\n\n\n# Define accuracy function\ndef get_accuracy(logits, actuals):\n    # Calulate if each output is correct\n    batch_acc = tf.equal(tf.argmax(logits, 1), tf.cast(actuals, tf.int64))\n    # Convert logical to float\n    batch_acc = tf.cast(batch_acc, tf.float32)\n    return batch_acc\n\n\n# Define main program\ndef main(args):\n    # Set verbosity to get more information from TensorFlow\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Create a visualizer object for Tensorboard viewing\n    summary_writer = tf.summary.FileWriter(\'tensorboard\', tf.get_default_graph())\n    # Create tensorboard folder if not exists\n    if not os.path.exists(\'tensorboard\'):\n        os.makedirs(\'tensorboard\')\n\n    # Set model parameters\n    storage_folder = FLAGS.storage_folder\n    learning_rate = FLAGS.learning_rate\n    run_unit_tests = FLAGS.run_unit_tests\n    epochs = FLAGS.epochs\n    batch_size = FLAGS.batch_size\n    rnn_size = FLAGS.rnn_size\n    embedding_size = FLAGS.embedding_size\n    min_word_frequency = FLAGS.min_word_frequency\n\n    # Get text->spam/ham data\n    x_data, y_data = get_data()\n\n    # Clean texts\n    x_data = [clean_text(x) for x in x_data]\n\n    # Change texts into numeric vectors\n    # Set a max sequence length for speeding up the computations.\n    # But we can easily set ""max_sequence_length = max([len(x) for x in x_data])"" as well.\n    max_sequence_length = 20\n    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,\n                                                                         min_frequency=min_word_frequency)\n    text_processed = np.array(list(vocab_processor.fit_transform(x_data)))\n\n    # Save vocab processor (for loading and future evaluation)\n    vocab_processor.save(os.path.join(storage_folder, ""vocab""))\n\n    # Shuffle and split data\n    text_processed = np.array(text_processed)\n    y_data = np.array([1 if x == \'ham\' else 0 for x in y_data])\n    shuffled_ix = np.random.permutation(np.arange(len(y_data)))\n    x_shuffled = text_processed[shuffled_ix]\n    y_shuffled = y_data[shuffled_ix]\n\n    # Split train/test set\n    ix_cutoff = int(len(y_shuffled) * 0.80)\n    x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n    y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n    vocab_size = len(vocab_processor.vocabulary_)\n\n    with tf.Graph().as_default():\n        sess = tf.Session()\n        # Define placeholders\n        x_data_ph = tf.placeholder(tf.int32, [None, max_sequence_length], name=\'x_data_ph\')\n        y_output_ph = tf.placeholder(tf.int32, [None], name=\'y_output_ph\')\n        dropout_keep_prob = tf.placeholder(tf.float32, name=\'dropout_keep_prob\')\n\n        # Define Model\n        rnn_model_outputs = rnn_model(x_data_ph, vocab_size, embedding_size, rnn_size, dropout_keep_prob)\n\n        # Prediction\n        # Although we won\'t use the following operation, we declare and name\n        #   the probability outputs so that we can recall them later for evaluation\n        rnn_prediction = tf.nn.softmax(rnn_model_outputs, name=""probability_outputs"")\n\n        # Loss function\n        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rnn_model_outputs, labels=y_output_ph)\n        # Remember that for this loss function, logits=float32, labels=int32\n        loss = tf.reduce_mean(losses, name=""loss"")\n\n        # Model Accuracy Operation\n        accuracy = tf.reduce_mean(get_accuracy(rnn_model_outputs, y_output_ph), name=""accuracy"")\n\n        # Add scalar summaries for Tensorboard\n        with tf.name_scope(\'Scalar_Summaries\'):\n            tf.summary.scalar(\'Loss\', loss)\n            tf.summary.scalar(\'Accuracy\', accuracy)\n\n        # Declare Optimizer/train step\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n        train_step = optimizer.minimize(loss)\n\n        # Declare summary merging operation\n        summary_op = tf.summary.merge_all()\n\n        # Create a graph/Variable saving/loading operations\n        saver = tf.train.Saver()\n\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        # Start training\n        for epoch in range(epochs):\n\n            # Shuffle training data\n            shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n            x_train = x_train[shuffled_ix]\n            y_train = y_train[shuffled_ix]\n            num_batches = int(len(x_train) / batch_size) + 1\n            #\n            for i in range(num_batches):\n                # Select train data\n                min_ix = i * batch_size\n                max_ix = np.min([len(x_train), ((i + 1) * batch_size)])\n                x_train_batch = x_train[min_ix:max_ix]\n                y_train_batch = y_train[min_ix:max_ix]\n\n                # Run train step\n                train_dict = {x_data_ph: x_train_batch,\n                              y_output_ph: y_train_batch,\n                              dropout_keep_prob: 0.5}\n                _, summary = sess.run([train_step, summary_op], feed_dict=train_dict)\n\n                summary_writer = tf.summary.FileWriter(\'tensorboard\')\n                summary_writer.add_summary(summary, i)\n\n            # Run loss and accuracy for training\n            temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)\n\n            test_dict = {x_data_ph: x_test, y_output_ph: y_test, dropout_keep_prob: 1.0}\n            temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)\n\n            # Print Epoch Summary\n            print(\'Epoch: {}, Train Loss:{:.2}, Train Acc: {:.2}\'.format(epoch + 1, temp_train_loss, temp_train_acc))\n            print(\'Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}\'.format(epoch + 1, temp_test_loss, temp_test_acc))\n\n            # Save model every epoch\n            saver.save(sess, os.path.join(storage_folder, ""model.ckpt""))\n\n        # Save the finished model for TensorFlow Serving (pb file)\n        # Here, it\'s our storage folder / version number\n        out_path = os.path.join(tf.compat.as_bytes(os.path.join(storage_folder, \'1\')))\n        print(\'Exporting finished model to : {}\'.format(out_path))\n        builder = tf.saved_model.builder.SavedModelBuilder(out_path)\n\n        # Build the signature_def_map.\n        classification_inputs = tf.saved_model.utils.build_tensor_info(x_data_ph)\n        classification_outputs_classes = tf.saved_model.utils.build_tensor_info(rnn_model_outputs)\n\n        classification_signature = (tf.saved_model.signature_def_utils.build_signature_def(\n                inputs={tf.saved_model.signature_constants.CLASSIFY_INPUTS: classification_inputs},\n                outputs={tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES: classification_outputs_classes},\n                method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME)\n        )\n\n        tensor_info_x = tf.saved_model.utils.build_tensor_info(x_data_ph)\n        tensor_info_y = tf.saved_model.utils.build_tensor_info(y_output_ph)\n\n        prediction_signature = (\n            tf.saved_model.signature_def_utils.build_signature_def(\n                inputs={\'texts\': tensor_info_x},\n                outputs={\'scores\': tensor_info_y},\n                method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n\n        legacy_init_op = tf.group(tf.tables_initializer(), name=\'legacy_init_op\')\n        builder.add_meta_graph_and_variables(\n            sess, [tf.saved_model.tag_constants.SERVING],\n            signature_def_map={\n                \'predict_spam\':\n                    prediction_signature,\n                tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n                    classification_signature,\n            },\n            legacy_init_op=legacy_init_op)\n\n        builder.save()\n\n        print(\'Done exporting!\')\n\n\n# Run main module/tf App\nif __name__ == ""__main__"":\n    cmd_args = sys.argv\n    if len(cmd_args) > 1 and cmd_args[1] == \'test\':\n        # Perform unit tests\n        tf.test.main(argv=cmd_args[1:])\n    else:\n        # Run TF App\n        tf.app.run(main=None, argv=cmd_args)\n'"
11_More_with_TensorFlow/01_Visualizing_Computational_Graphs/01_using_tensorboard.py,19,"b'# -*- coding: utf-8 -*-\n# Using Tensorboard\n#----------------------------------\n#\n# We illustrate the various ways to use\n#  Tensorboard\n\nimport os\nimport io\nimport time\nimport pathlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Initialize a graph session\nsess = tf.Session()\n\n# Create tensorboard folder if not exists\nif not os.path.exists(\'tensorboard\'):\n    os.makedirs(\'tensorboard\')\nprint(\'Running a slowed down linear regression. \'\n      \'Run the command: $tensorboard --logdir=""tensorboard""  \'\n      \' Then navigate to http://127.0.0.1:6006\')\n\n# You can also specify a port option with --port 6006\n\n# Create a visualizer object\nsummary_writer = tf.summary.FileWriter(\'tensorboard\', sess.graph)\n\n# Wait a few seconds for user to run tensorboard commands\ntime.sleep(3)\n\n# Some parameters\nbatch_size = 50\ngenerations = 100\n\n# Create sample input data\nx_data = np.arange(1000)/10.\ntrue_slope = 2.\ny_data = x_data * true_slope + np.random.normal(loc=0.0, scale=25, size=1000)\n\n# Split into train/test\ntrain_ix = np.random.choice(len(x_data), size=int(len(x_data)*0.9), replace=False)\ntest_ix = np.setdiff1d(np.arange(1000), train_ix)\nx_data_train, y_data_train = x_data[train_ix], y_data[train_ix]\nx_data_test, y_data_test = x_data[test_ix], y_data[test_ix]\n\n# Declare placeholders\nx_graph_input = tf.placeholder(tf.float32, [None])\ny_graph_input = tf.placeholder(tf.float32, [None])\n\n# Declare model variables\nm = tf.Variable(tf.random_normal([1], dtype=tf.float32), name=\'Slope\')\n\n# Declare model\noutput = tf.multiply(m, x_graph_input, name=\'Batch_Multiplication\')\n\n# Declare loss function (L1)\nresiduals = output - y_graph_input\nl1_loss = tf.reduce_mean(tf.abs(residuals), name=""L1_Loss"")\n\n# Declare optimization function\nmy_optim = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_optim.minimize(l1_loss)\n\n# Visualize a scalar\nwith tf.name_scope(\'Slope_Estimate\'):\n    tf.summary.scalar(\'Slope_Estimate\', tf.squeeze(m))\n    \n# Visualize a histogram (errors)\nwith tf.name_scope(\'Loss_and_Residuals\'):\n    tf.summary.histogram(\'Histogram_Errors\', tf.squeeze(l1_loss))\n    tf.summary.histogram(\'Histogram_Residuals\', tf.squeeze(residuals))\n\n\n# Declare summary merging operation\nsummary_op = tf.summary.merge_all()\n\n# Initialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor i in range(generations):\n    batch_indices = np.random.choice(len(x_data_train), size=batch_size)\n    x_batch = x_data_train[batch_indices]\n    y_batch = y_data_train[batch_indices]\n    _, train_loss, summary = sess.run([train_step, l1_loss, summary_op],\n                             feed_dict={x_graph_input: x_batch,\n                                        y_graph_input: y_batch})\n    \n    test_loss, test_resids = sess.run([l1_loss, residuals], feed_dict={x_graph_input: x_data_test,\n                                                                       y_graph_input: y_data_test})\n    \n    if (i + 1) % 10 == 0:\n        print(\'Generation {} of {}. Train Loss: {:.3}, Test Loss: {:.3}.\'.format(i+1, generations, train_loss, test_loss))\n\n    log_writer = tf.summary.FileWriter(\'tensorboard\')\n    log_writer.add_summary(summary, i)\n    time.sleep(0.5)\n\n#Create a function to save a protobuf bytes version of the graph\ndef gen_linear_plot(slope):\n    linear_prediction = x_data * slope\n    plt.plot(x_data, y_data, \'b.\', label=\'data\')\n    plt.plot(x_data, linear_prediction, \'r-\', linewidth=3, label=\'predicted line\')\n    plt.legend(loc=\'upper left\')\n    buf = io.BytesIO()\n    plt.savefig(buf, format=\'png\')\n    buf.seek(0)\n    return(buf)\n\n# Add image to tensorboard (plot the linear fit!)\nslope = sess.run(m)\nplot_buf = gen_linear_plot(slope[0])\n# Convert PNG buffer to TF image\nimage = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n# Add the batch dimension\nimage = tf.expand_dims(image, 0)\n# Add image summary\nimage_summary_op = tf.summary.image(""Linear_Plot"", image)\nimage_summary = sess.run(image_summary_op)\nlog_writer.add_summary(image_summary, i)\nlog_writer.close()\n'"
11_More_with_TensorFlow/02_Working_with_a_Genetic_Algorithm/02_genetic_algorithm.py,21,"b'# -*- coding: utf-8 -*-\n# Implementing a Genetic Algorithm\n# -------------------------------\n#\n# Genetic Algorithm Optimization in TensorFlow\n#\n# We are going to implement a genetic algorithm\n#   to optimize to a ground truth array.  The ground\n#   truth will be an array of 50 floating point\n#   numbers which are generated by:\n#   f(x)=sin(2*pi*x/50) where 0<x<50\n#\n# Each individual will be an array of 50 floating\n#   point numbers and the fitness will be the average\n#   mean squared error from the ground truth.\n#\n# We will use TensorFlow\'s update function to run the\n#   different parts of the genetic algorithm.\n#\n# While TensorFlow isn\'t really the best for GA\'s,\n#   this example shows that we can implement different\n#   procedural algorithms with TensorFlow operations.\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n\n# Genetic Algorithm Parameters\npop_size = 100\nfeatures = 50\nselection = 0.2\nmutation = 1./pop_size\ngenerations = 200\nnum_parents = int(pop_size*selection)\nnum_children = pop_size - num_parents\n\n# Start a graph session\nsess = tf.Session()\n\n# Create ground truth\ntruth = np.sin(2*np.pi*(np.arange(features, dtype=np.float32))/features)\n\n# Initialize population array\npopulation = tf.Variable(np.random.randn(pop_size, features), dtype=tf.float32)\n\n# Initialize placeholders\ntruth_ph = tf.placeholder(tf.float32, [1, features])\ncrossover_mat_ph = tf.placeholder(tf.float32, [num_children, features])\nmutation_val_ph = tf.placeholder(tf.float32, [num_children, features])\n\n# Calculate fitness (MSE)\nfitness = -tf.reduce_mean(tf.square(tf.subtract(population, truth_ph)), 1)\ntop_vals, top_ind = tf.nn.top_k(fitness, k=pop_size)\n\n# Get best fit individual\nbest_val = tf.reduce_min(top_vals)\nbest_ind = tf.argmin(top_vals, 0)\nbest_individual = tf.gather(population, best_ind)\n\n# Get parents\npopulation_sorted = tf.gather(population, top_ind)\nparents = tf.slice(population_sorted, [0, 0], [num_parents, features])\n\n\n# Get offspring\n# Indices to shuffle-gather parents\nrand_parent1_ix = np.random.choice(num_parents, num_children)\nrand_parent2_ix = np.random.choice(num_parents, num_children)\n# Gather parents by shuffled indices, expand back out to pop_size too\nrand_parent1 = tf.gather(parents, rand_parent1_ix)\nrand_parent2 = tf.gather(parents, rand_parent2_ix)\nrand_parent1_sel = tf.multiply(rand_parent1, crossover_mat_ph)\nrand_parent2_sel = tf.multiply(rand_parent2, tf.subtract(1., crossover_mat_ph))\nchildren_after_sel = tf.add(rand_parent1_sel, rand_parent2_sel)\n\n# Mutate Children\nmutated_children = tf.add(children_after_sel, mutation_val_ph)\n\n# Combine children and parents into new population\nnew_population = tf.concat(axis=0, values=[parents, mutated_children])\n\nstep = tf.group(population.assign(new_population))\n\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Run through generations\nfor i in range(generations):\n    # Create cross-over matrices for plugging in.\n    crossover_mat = np.ones(shape=[num_children, features])\n    crossover_point = np.random.choice(np.arange(1, features-1, step=1), num_children)\n    for pop_ix in range(num_children):\n        crossover_mat[pop_ix,0:crossover_point[pop_ix]]=0.\n    # Generate mutation probability matrices\n    mutation_prob_mat = np.random.uniform(size=[num_children, features])\n    mutation_values = np.random.normal(size=[num_children, features])\n    mutation_values[mutation_prob_mat >= mutation] = 0\n    \n    # Run GA step\n    feed_dict = {truth_ph: truth.reshape([1, features]),\n                 crossover_mat_ph: crossover_mat,\n                 mutation_val_ph: mutation_values}\n    step.run(feed_dict, session=sess)\n    best_individual_val = sess.run(best_individual, feed_dict=feed_dict)\n    \n    if i % 5 == 0:\n         best_fit = sess.run(best_val, feed_dict = feed_dict)\n         print(\'Generation: {}, Best Fitness (lowest MSE): {:.2}\'.format(i, -best_fit))\n\nplt.plot(truth, label=""True Values"")\nplt.plot(np.squeeze(best_individual_val), label=""Best Individual"")\nplt.axis((0, features, -1.25, 1.25))\nplt.legend(loc=\'upper right\')\nplt.show()\n'"
11_More_with_TensorFlow/03_Clustering_Using_KMeans/03_k_means.py,13,"b""# -*- coding: utf-8 -*-\n# K-means with TensorFlow\n#----------------------------------\n#\n# This script shows how to do k-means with TensorFlow\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom scipy.spatial import cKDTree\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\nsess = tf.Session()\n\niris = datasets.load_iris()\n\nnum_pts = len(iris.data)\nnum_feats = len(iris.data[0])\n\n# Set k-means parameters\n# There are 3 types of iris flowers, see if we can predict them\nk = 3\ngenerations = 25\n\ndata_points = tf.Variable(iris.data)\ncluster_labels = tf.Variable(tf.zeros([num_pts], dtype=tf.int64))\n\n# Randomly choose starting points\nrand_starts = np.array([iris.data[np.random.choice(len(iris.data))] for _ in range(k)])\n\ncentroids = tf.Variable(rand_starts)\n\n# In order to calculate the distance between every data point and every centroid, we\n#  repeat the centroids into a (num_points) by k matrix.\ncentroid_matrix = tf.reshape(tf.tile(centroids, [num_pts, 1]), [num_pts, k, num_feats])\n# Then we reshape the data points into k (3) repeats\npoint_matrix = tf.reshape(tf.tile(data_points, [1, k]), [num_pts, k, num_feats])\ndistances = tf.reduce_sum(tf.square(point_matrix - centroid_matrix), axis=2)\n\n# Find the group it belongs to with tf.argmin()\ncentroid_group = tf.argmin(distances, 1)\n\n\n# Find the group average\ndef data_group_avg(group_ids, data):\n    # Sum each group\n    sum_total = tf.unsorted_segment_sum(data, group_ids, 3)\n    # Count each group\n    num_total = tf.unsorted_segment_sum(tf.ones_like(data), group_ids, 3)\n    # Calculate average\n    avg_by_group = sum_total/num_total\n    return avg_by_group\n\n\nmeans = data_group_avg(centroid_group, data_points)\n\nupdate = tf.group(centroids.assign(means), cluster_labels.assign(centroid_group))\n\ninit = tf.global_variables_initializer()\n\nsess.run(init)\n\nfor i in range(generations):\n    print('Calculating gen {}, out of {}.'.format(i, generations))\n    _, centroid_group_count = sess.run([update, centroid_group])\n    group_count = []\n    for ix in range(k):\n        group_count.append(np.sum(centroid_group_count==ix))\n    print('Group counts: {}'.format(group_count))\n    \n\n[centers, assignments] = sess.run([centroids, cluster_labels])\n\n\n# Find which group assignments correspond to which group labels\n# First, need a most common element function\ndef most_common(my_list):\n    return max(set(my_list), key=my_list.count)\n\n\nlabel0 = most_common(list(assignments[0:50]))\nlabel1 = most_common(list(assignments[50:100]))\nlabel2 = most_common(list(assignments[100:150]))\n\ngroup0_count = np.sum(assignments[0:50] == label0)\ngroup1_count = np.sum(assignments[50:100] == label1)\ngroup2_count = np.sum(assignments[100:150] == label2)\n\naccuracy = (group0_count + group1_count + group2_count)/150.\n\nprint('Accuracy: {:.2}'.format(accuracy))\n\n# Also plot the output\n# First use PCA to transform the 4-dimensional data into 2-dimensions\npca_model = PCA(n_components=2)\nreduced_data = pca_model.fit_transform(iris.data)\n# Transform centers\nreduced_centers = pca_model.transform(centers)\n\n# Step size of mesh for plotting\nh = .02\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Get k-means classifications for the grid points\nxx_pt = list(xx.ravel())\nyy_pt = list(yy.ravel())\nxy_pts = np.array([[x, y] for x, y in zip(xx_pt, yy_pt)])\nmytree = cKDTree(reduced_centers)\ndist, indexes = mytree.query(xy_pts)\n\n# Put the result into a color plot\nindexes = indexes.reshape(xx.shape)\nplt.figure(1)\nplt.clf()\nplt.imshow(indexes, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\n\n# Plot each of the true iris data groups\nsymbols = ['o', '^', 'D']\nlabel_name = ['Setosa', 'Versicolour', 'Virginica']\nfor i in range(3):\n    temp_group = reduced_data[(i*50):(50)*(i+1)]\n    plt.plot(temp_group[:, 0], temp_group[:, 1], symbols[i], markersize=10, label=label_name[i])\n# Plot the centroids as a white X\nplt.scatter(reduced_centers[:, 0], reduced_centers[:, 1], marker='x', s=169, linewidths=3, color='w', zorder=10)\nplt.title('K-means clustering on Iris Dataset Centroids are marked with white cross')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.legend(loc='lower right')\nplt.show()\n"""
11_More_with_TensorFlow/04_Solving_A_System_of_ODEs/04_solving_ode_system.py,12,"b""# -*- coding: utf-8 -*-\n# Solving a Sytem of ODEs\n#----------------------------------\n#\n# In this script, we use TensorFlow to solve a sytem\n#   of ODEs.\n#\n# The system of ODEs we will solve is the Lotka-Volterra\n#   predator-prey system.\n\n\n# Declaring Operations\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Open interactive graph session\nsess = tf.Session()\n\n# Discrete Lotka-Volterra predator/prey equations\n#\n# X(t+1) = X(t) + (aX(t) + bX(t)Y(t)) * t_delta # Prey\n#\n# Y(t+1) = Y(t) + (cY(t) + dX(t)Y(t)) * t_delta # Predator\n\n# Declare constants and variables\nx_initial = tf.constant(1.0)\ny_initial = tf.constant(1.0)\nX_t1 = tf.Variable(x_initial)\nY_t1 = tf.Variable(y_initial)\n\n# Make the placeholders\nt_delta = tf.placeholder(tf.float32, shape=())\na = tf.placeholder(tf.float32, shape=())\nb = tf.placeholder(tf.float32, shape=())\nc = tf.placeholder(tf.float32, shape=())\nd = tf.placeholder(tf.float32, shape=())\n\n# Discretized ODE update\nX_t2 = X_t1 + (a * X_t1 + b * X_t1 * Y_t1) * t_delta\nY_t2 = Y_t1 + (c * Y_t1 + d * X_t1 * Y_t1) * t_delta\n\n# Update to New Population\nstep = tf.group(\n  X_t1.assign(X_t2),\n  Y_t1.assign(Y_t2))\n  \ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Run the ODE\nprey_values = []\npredator_values = []\nfor i in range(1000):\n    # Step simulation (using constants for a known cyclic solution)\n    step.run({a: (2./3.), b: (-4./3.), c: -1.0, d: 1.0, t_delta: 0.01}, session=sess)\n    # Store each outcome\n    temp_prey, temp_pred = sess.run([X_t1, Y_t1])\n    prey_values.append(temp_prey)\n    predator_values.append(temp_pred)\n\n# Visualize the output\nplt.plot(prey_values)\nplt.plot(predator_values)\nplt.legend(['Prey', 'Predator'], loc='upper right')\nplt.show()\n"""
11_More_with_TensorFlow/05_Using_a_Random_Forest/05_Using_a_Random_Forest.py,8,"b'""""""\nUsing a Random Forest\n---------------------\n\nThis script will illustrate how to use TensorFlow\'s Boosted Random Forest algorithm.\n\n\nFor illustrative purposes we will show how to do this with the boston housing data.\n\nAttribute Information:\n\n    1. CRIM      per capita crime rate by town\n    2. ZN        proportion of residential land zoned for lots over\n                 25,000 sq.ft.\n    3. INDUS     proportion of non-retail business acres per town\n    4. CHAS      Charles River dummy variable (= 1 if tract bounds\n                 river; 0 otherwise)\n    5. NOX       nitric oxides concentration (parts per 10 million)\n    6. RM        average number of rooms per dwelling\n    7. AGE       proportion of owner-occupied units built prior to 1940\n    8. DIS       weighted distances to five Boston employment centres\n    9. RAD       index of accessibility to radial highways\n    10. TAX      full-value property-tax rate per $10,000\n    11. PTRATIO  pupil-teacher ratio by town\n    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks\n                 by town\n    13. LSTAT    % lower status of the population\n    14. y_target Median value of owner-occupied homes in $1000\'s.\n""""""\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom keras.datasets import boston_housing\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# For using the boosted trees classifier (binary classification) in TF:\n# Note: target labels have to be 0 and 1.\nboosted_classifier = tf.estimator.BoostedTreesClassifier\n\n# For using a boosted trees regression classifier (binary classification) in TF:\nregression_classifier = tf.estimator.BoostedTreesRegressor\n\n# Load data\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n\n# Set model parameters\n# Batch size\nbatch_size = 32\n# Number of training steps\ntrain_steps = 500\n# Number of trees in our \'forest\'\nn_trees = 100\n# Maximum depth of any tree in forest\nmax_depth = 6\n\n# Data ETL\nbinary_split_cols = [\'CHAS\', \'RAD\']\ncol_names = [\'CRIM\', \'ZN\', \'INDUS\', \'CHAS\', \'NOX\', \'RM\', \'AGE\', \'DIS\', \'RAD\', \'TAX\', \'PTRATIO\', \'B\', \'LSTAT\']\nX_dtrain = {col: x_train[:, ix] for ix, col in enumerate(col_names)}\nX_dtest = {col: x_test[:, ix] for ix, col in enumerate(col_names)}\n\n# Create feature columns!\nfeature_cols = []\nfor ix, column in enumerate(x_train.T):\n    col_name = col_names[ix]\n\n    # Create binary split feature\n    if col_name in binary_split_cols:\n        # To create 2 buckets, need 1 boundary - the mean\n        bucket_boundaries = [column.mean()]\n        numeric_feature = tf.feature_column.numeric_column(col_name)\n        final_feature = tf.feature_column.bucketized_column(source_column=numeric_feature, boundaries=bucket_boundaries)\n    # Create bucketed feature\n    else:\n        # To create 5 buckets, need 4 boundaries\n        bucket_boundaries = list(np.linspace(column.min() * 1.1, column.max() * 0.9, 4))\n        numeric_feature = tf.feature_column.numeric_column(col_name)\n        final_feature = tf.feature_column.bucketized_column(source_column=numeric_feature, boundaries=bucket_boundaries)\n\n    # Add feature to feature_col list\n    feature_cols.append(final_feature)\n\n\n# Create an input function\ninput_fun = tf.estimator.inputs.numpy_input_fn(X_dtrain, y=y_train, batch_size=batch_size, num_epochs=10, shuffle=True)\n\n# Training\nmodel = regression_classifier(feature_columns=feature_cols,\n                              n_trees=n_trees,\n                              max_depth=max_depth,\n                              learning_rate=0.25,\n                              n_batches_per_layer=batch_size)\nmodel.train(input_fn=input_fun, steps=train_steps)\n\n# Evaluation on test set\n# Do not shuffle when predicting\np_input_fun = tf.estimator.inputs.numpy_input_fn(X_dtest, y=y_test, batch_size=batch_size, num_epochs=1, shuffle=False)\n# Get predictions\npredictions = list(model.predict(input_fn=p_input_fun))\nfinal_preds = [pred[\'predictions\'][0] for pred in predictions]\n\n# Get accuracy (mean absolute error, MAE)\nmae = np.mean([np.abs((actual - predicted) / predicted) for actual, predicted in zip(y_test, final_preds)])\nprint(\'Mean Abs Err on test set: {}\'.format(acc))\n'"
11_More_with_TensorFlow/06_Using_TensorFlow_with_Keras/06_Using_TensorFlow_with_Keras.py,4,"b'# Using TensorFlow with Keras\n#----------------------------------\n#\n# This script will show you how to create model layers with Keras\n#\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom keras.utils import to_categorical\nfrom tensorflow import keras\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n\n# Load MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n# The following loads the MNIST dataset into\n#\n# mnist.[train/test].[images/labels]\n#\n# where images are a 1x784 flatt array and labels are an integer between 0 and 9.\n#\n\nmnist = input_data.read_data_sets(""MNIST_data/"")\nx_train = mnist.train.images\nx_test = mnist.test.images\ny_train = mnist.train.labels\ny_train = [[i] for i in y_train]\ny_test = mnist.test.labels\ny_test = [[i] for i in y_test]\n\n# One-hot encode labels\none_hot = MultiLabelBinarizer()\ny_train = one_hot.fit_transform(y_train)\ny_test = one_hot.transform(y_test)\n\n# Example 1: Fully connected neural network model\n# We start with a \'sequential\' model type (connecting layers together)\nmodel = keras.Sequential()\n# Adds a densely-connected layer with 32 units to the model, followed by an ReLU activation.\nmodel.add(keras.layers.Dense(32, activation=\'relu\'))\n# Adds a densely-connected layer with 16 units to the model, followed by an ReLU activation.\nmodel.add(keras.layers.Dense(16, activation=\'relu\'))\n# Add a softmax layer with 10 output units:\nmodel.add(keras.layers.Dense(10, activation=\'softmax\'))\n\n# Train the model:\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.001),\n              loss=\'categorical_crossentropy\',\n              metrics=[\'accuracy\'])\n\n# Configure a model for mean-squared error regression.\n# model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n#              loss=\'mse\',       # mean squared error\n#              metrics=[\'mae\'])  # mean absolute error\n\n# Configure a model for categorical classification.\n#model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n#              loss=keras.losses.categorical_crossentropy,\n#              metrics=[keras.metrics.categorical_accuracy])\n\n# Fit the model:\nmodel.fit(x_train,\n          y_train,\n          epochs=5,\n          batch_size=64,\n          validation_data=(x_test, y_test))\n\n\n# ---------------------\n# Simple CNN in Keras:\n# ---------------------\n# First we transform the input images from 1D arrays to 2D matrices. (28 x 28)\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\ninput_shape = (28, 28, 1)\nnum_classes = 10\n\n# Categorize y targets\ny_test = to_categorical(mnist.test.labels)\ny_train = to_categorical(mnist.train.labels)\n\n# Decrease test size for memory usage\nx_test = x_test[:64]\ny_test = y_test[:64]\n\n# Start our sequential model\ncnn_model = keras.Sequential()\ncnn_model.add(keras.layers.Conv2D(25,\n                                  kernel_size=(4, 4),\n                                  strides=(1, 1),\n                                  activation=\'relu\',\n                                  input_shape=input_shape))\ncnn_model.add(keras.layers.MaxPooling2D(pool_size=(2, 2),\n                                        strides=(2, 2)))\n\ncnn_model.add(keras.layers.Conv2D(50,\n                                  kernel_size=(5, 5),\n                                  strides=(1, 1),\n                                  activation=\'relu\'))\n\ncnn_model.add(keras.layers.MaxPooling2D(pool_size=(2, 2),\n                                        strides=(2, 2)))\n\ncnn_model.add(keras.layers.Flatten())\n\ncnn_model.add(keras.layers.Dense(num_classes, activation=\'softmax\'))\n\ncnn_model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n                  loss=\'categorical_crossentropy\',\n                  metrics=[\'accuracy\'])\n\n\nclass AccuracyHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.acc = []\n\n    def on_epoch_end(self, batch, logs={}):\n        self.acc.append(logs.get(\'acc\'))\n\n\nhistory = AccuracyHistory()\n\ncnn_model.fit(x_train,\n              y_train,\n              batch_size=64,\n              epochs=3,\n              verbose=1,\n              validation_data=(x_test, y_test),\n              callbacks=[history])\n\nprint(history.acc)\n'"
