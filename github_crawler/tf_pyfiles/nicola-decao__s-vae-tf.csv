file_path,api_count,code
setup.py,0,"b""\nimport os\nfrom setuptools import setup\nfrom setuptools import find_packages\n\nsetup(\n    name='hyperspherical_vae',\n    version='0.1.1',\n    author='Nicola De Cao, Tim R. Davidson, Luca Falorsi',\n    author_email='nicola.decao@gmail.com',\n    description='Tensorflow implementation of Hyperspherical Variational Auto-Encoders',\n    license='MIT',\n    keywords='tensorflow vae variational-auto-encoder von-mises-fisher  machine-learning deep-learning manifold-learning',\n    url='https://nicola-decao.github.io/s-vae-tf/',\n    download_url='https://github.com/nicola-decao/SVAE',\n    long_description=open(os.path.join(os.path.dirname(__file__), 'README.md')).read(),\n    install_requires=['numpy', 'tensorflow>=1.7.0', 'scipy'],\n    packages=find_packages()\n)\n"""
examples/mnist.py,25,"b'\nimport numpy as np\nimport tensorflow as tf\n\nfrom hyperspherical_vae.distributions import VonMisesFisher\nfrom hyperspherical_vae.distributions import HypersphericalUniform\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\'data/\', one_hot=False)\n\n\nclass ModelVAE(object):\n\n    def __init__(self, x, h_dim, z_dim, activation=tf.nn.relu, distribution=\'normal\'):\n        """"""\n        ModelVAE initializer\n\n        :param x: placeholder for input\n        :param h_dim: dimension of the hidden layers\n        :param z_dim: dimension of the latent representation\n        :param activation: callable activation function\n        :param distribution: string either `normal` or `vmf`, indicates which distribution to use\n        """"""\n        self.x, self.h_dim, self.z_dim, self.activation, self.distribution = x, h_dim, z_dim, activation, distribution\n\n        self.z_mean, self.z_var = self._encoder(self.x)\n\n        if distribution == \'normal\':\n            self.q_z = tf.distributions.Normal(self.z_mean, self.z_var)\n        elif distribution == \'vmf\':\n            self.q_z = VonMisesFisher(self.z_mean, self.z_var)\n        else:\n            raise NotImplemented\n\n        self.z = self.q_z.sample()\n\n        self.logits = self._decoder(self.z)\n\n    def _encoder(self, x):\n        """"""\n        Encoder network\n\n        :param x: placeholder for input\n        :return: tuple `(z_mean, z_var)` with mean and concentration around the mean\n        """"""\n        \n        # 2 hidden layers encoder\n        h0 = tf.layers.dense(x, units=self.h_dim * 2, activation=self.activation)\n        h1 = tf.layers.dense(h0, units=self.h_dim, activation=self.activation)\n\n        if self.distribution == \'normal\':\n            # compute mean and std of the normal distribution\n            z_mean = tf.layers.dense(h1, units=self.z_dim, activation=None)\n            z_var = tf.layers.dense(h1, units=self.z_dim, activation=tf.nn.softplus)\n        elif self.distribution == \'vmf\':\n            # compute mean and concentration of the von Mises-Fisher\n            z_mean = tf.layers.dense(h1, units=self.z_dim, activation=lambda x: tf.nn.l2_normalize(x, axis=-1))\n            # the `+ 1` prevent collapsing behaviors\n            z_var = tf.layers.dense(h1, units=1, activation=tf.nn.softplus) + 1\n        else:\n            raise NotImplemented\n\n        return z_mean, z_var\n\n    def _decoder(self, z):\n        """"""\n        Decoder network\n\n        :param z: tensor, latent representation of input (x)\n        :return: logits, `reconstruction = sigmoid(logits)`\n        """"""\n        # 2 hidden layers decoder\n        h2 = tf.layers.dense(z, units=self.h_dim, activation=self.activation)\n        h2 = tf.layers.dense(h2, units=self.h_dim * 2, activation=self.activation)\n        logits = tf.layers.dense(h2, units=self.x.shape[-1], activation=None)\n\n        return logits\n\n\nclass OptimizerVAE(object):\n\n    def __init__(self, model, learning_rate=1e-3):\n        """"""\n        OptimizerVAE initializer\n\n        :param model: a model object\n        :param learning_rate: float, learning rate of the optimizer\n        """"""\n\n        # binary cross entropy error\n        self.bce = tf.nn.sigmoid_cross_entropy_with_logits(labels=model.x, logits=model.logits)\n        self.reconstruction_loss = tf.reduce_mean(tf.reduce_sum(self.bce, axis=-1))\n\n        if model.distribution == \'normal\':\n            # KL divergence between normal approximate posterior and standard normal prior\n            self.p_z = tf.distributions.Normal(tf.zeros_like(model.z), tf.ones_like(model.z))\n            kl = model.q_z.kl_divergence(self.p_z)\n            self.kl = tf.reduce_mean(tf.reduce_sum(kl, axis=-1))\n        elif model.distribution == \'vmf\':\n            # KL divergence between vMF approximate posterior and uniform hyper-spherical prior\n            self.p_z = HypersphericalUniform(model.z_dim - 1, dtype=model.x.dtype)\n            kl = model.q_z.kl_divergence(self.p_z)\n            self.kl = tf.reduce_mean(kl)\n        else:\n            raise NotImplemented\n\n        self.ELBO = - self.reconstruction_loss - self.kl\n\n        self.train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-self.ELBO)\n\n        self.print = {\'recon loss\': self.reconstruction_loss, \'ELBO\': self.ELBO, \'KL\': self.kl}\n\n\ndef log_likelihood(model, optimizer, n=10):\n    """"""\n\n    :param model: model object\n    :param optimizer: optimizer object\n    :param n: number of MC samples\n    :return: MC estimate of log-likelihood\n    """"""\n\n    z = model.q_z.sample(n)\n\n    log_p_z = optimizer.p_z.log_prob(z)\n\n    if model.distribution == \'normal\':\n        log_p_z = tf.reduce_sum(log_p_z, axis=-1)\n\n    log_p_x_z = -tf.reduce_sum(optimizer.bce, axis=-1)\n\n    log_q_z_x = model.q_z.log_prob(z)\n\n    if model.distribution == \'normal\':\n        log_q_z_x = tf.reduce_sum(log_q_z_x, axis=-1)\n\n    return tf.reduce_mean(tf.reduce_logsumexp(\n        tf.transpose(log_p_x_z + log_p_z - log_q_z_x) - np.log(n), axis=-1))\n\n# hidden dimension and dimension of latent space\nH_DIM = 128\nZ_DIM = 2\n\n# digit placeholder\nx = tf.placeholder(tf.float32, shape=(None, 784))\n\n# normal VAE\nmodelN = ModelVAE(x=x, h_dim=H_DIM, z_dim=Z_DIM, distribution=\'normal\')\noptimizerN = OptimizerVAE(modelN)\n\n# hyper-spherical VAE\nmodelS = ModelVAE(x=x, h_dim=H_DIM, z_dim=Z_DIM + 1, distribution=\'vmf\')\noptimizerS = OptimizerVAE(modelS)\n\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\n\nprint(\'##### Normal VAE #####\')\nfor i in range(1000):\n    # training\n    x_mb, _ = mnist.train.next_batch(64)\n    # dynamic binarization\n    x_mb = (x_mb > np.random.random(size=x_mb.shape)).astype(np.float32)\n    \n    session.run(optimizerN.train_step, {modelN.x: x_mb})\n\n    # every 100 iteration plot validation\n    if i % 100 == 0:\n        x_mb = mnist.validation.images\n        # dynamic binarization\n        x_mb = (x_mb > np.random.random(size=x_mb.shape)).astype(np.float32)\n        \n        print(i, session.run({**optimizerN.print}, {modelN.x: x_mb}))\n\nprint(\'Test set:\')\nx_mb = mnist.test.images\n# dynamic binarization\nx_mb = (x_mb > np.random.random(size=x_mb.shape)).astype(np.float32)\n    \nprint_ = {**optimizerN.print}\nprint_[\'LL\'] = log_likelihood(modelN, optimizerN, n=100)\nprint(session.run(print_, {modelN.x: x_mb}))\n\nprint()\nprint(\'##### Hyper-spherical VAE #####\')\nfor i in range(1000):\n    # training\n    x_mb, _ = mnist.train.next_batch(64)\n    # dynamic binarization\n    x_mb = (x_mb > np.random.random(size=x_mb.shape)).astype(np.float32)\n    \n    session.run(optimizerS.train_step, {modelS.x: x_mb})\n\n    # every 100 iteration plot validation\n    if i % 100 == 0:\n        x_mb = mnist.validation.images\n        # dynamic binarization\n        x_mb = (x_mb > np.random.random(size=x_mb.shape)).astype(np.float32)\n        \n        print(i, session.run({**optimizerS.print}, {modelS.x: x_mb}))\n\nprint(\'Test set:\')\nx_mb = mnist.test.images\n# dynamic binarization\nx_mb = (x_mb > np.random.random(size=x_mb.shape)).astype(np.float32)\n    \nprint_ = {**optimizerS.print}\nprint_[\'LL\'] = log_likelihood(modelS, optimizerS, n=100)\nprint(session.run(print_, {modelS.x: x_mb}))\n'"
hyperspherical_vae/__init__.py,0,b'import hyperspherical_vae.ops\nimport hyperspherical_vae.distributions\n'
hyperspherical_vae/distributions/__init__.py,0,b'from hyperspherical_vae.distributions.von_mises_fisher import VonMisesFisher\nfrom hyperspherical_vae.distributions.hyperspherical_uniform import HypersphericalUniform\n'
hyperspherical_vae/distributions/hyperspherical_uniform.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The Hyperspherical Uniform distribution class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops.distributions import distribution\n\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.framework.tensor_shape import TensorShape\nfrom tensorflow.python.ops import gen_math_ops\n\n\nclass HypersphericalUniform(distribution.Distribution):\n    """"""Hyperspherical Uniform distribution with `dim` parameter.\n\n    #### Mathematical Details\n\n    """"""\n\n    def __init__(self, dim, dtype=dtypes.float32, validate_args=False, allow_nan_stats=True,\n                 name=""HypersphericalUniform""):\n        """"""Initialize a batch of Hyperspherical Uniform distributions.\n\n        Args:\n          dim: Integer tensor, dimensionality of the distribution(s). Must\n            be `dim > 0`.\n          validate_args: Python `bool`, default `False`. When `True` distribution\n            parameters are checked for validity despite possibly degrading runtime\n            performance. When `False` invalid inputs may silently render incorrect\n            outputs.\n          allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n            (e.g., mean, mode, variance) use the value ""`NaN`"" to indicate the\n            result is undefined. When `False`, an exception is raised if one or\n            more of the statistic\'s batch members are undefined.\n          name: Python `str` name prefixed to Ops created by this class.\n\n        Raises:\n          InvalidArgumentError: if `dim > 0` and `validate_args=False`.\n        """"""\n        parameters = locals()\n        with ops.name_scope(name, values=[dim]):\n            with ops.control_dependencies([check_ops.assert_positive(dim),\n                                           check_ops.assert_integer(dim),\n                                           check_ops.assert_scalar(dim)] if validate_args else []):\n                self._dim = dim\n\n            super(HypersphericalUniform, self).__init__(\n                dtype=dtype,\n                reparameterization_type=distribution.FULLY_REPARAMETERIZED,\n                validate_args=validate_args,\n                allow_nan_stats=allow_nan_stats,\n                parameters=parameters,\n                graph_parents=[],\n                name=name)\n\n    @staticmethod\n    def _param_shapes(sample_shape):\n        return {}\n\n    @property\n    def dim(self):\n        """"""Dimensionality of the distribution(s).""""""\n        return self._dim\n\n    def _batch_shape_tensor(self):\n        return constant_op.constant([self._dim + 1], dtype=dtypes.int32)\n\n    def _batch_shape(self):\n        return TensorShape(self._dim + 1)\n\n    def _event_shape_tensor(self):\n        return constant_op.constant([], dtype=dtypes.int32)\n\n    def _event_shape(self):\n        return tensor_shape.scalar()\n\n    def _sample_n(self, n, seed=None):\n        return nn_impl.l2_normalize(random_ops.random_normal(shape=array_ops.concat(([n], [self._dim + 1]), 0),\n                                                             dtype=self.dtype, seed=seed), axis=-1)\n\n    def _log_prob(self, x):\n        return - array_ops.ones(shape=array_ops.shape(x)[:-1], dtype=self.dtype) * self.__log_surface_area()\n\n    def _prob(self, x):\n        return math_ops.exp(self._log_prob(x))\n\n    def _entropy(self):\n        return self.__log_surface_area()\n\n    def __log_surface_area(self):\n        return math.log(2) + ((self._dim + 1) / 2) * math.log(math.pi) - gen_math_ops.lgamma(\n            math_ops.cast((self._dim + 1) / 2, dtype=self.dtype))\n'"
hyperspherical_vae/distributions/von_mises_fisher.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The von-Mises-Fisher distribution class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops.distributions import distribution\nfrom tensorflow.python.ops.distributions import kullback_leibler\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_math_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops.distributions.beta import Beta\n\nfrom hyperspherical_vae.ops.ive import ive\nfrom hyperspherical_vae.distributions.hyperspherical_uniform import HypersphericalUniform\n\n__all__ = [\n    ""VonMisesFisher"",\n]\n\n\nclass VonMisesFisher(distribution.Distribution):\n    """"""The von-Mises-Fisher distribution with location `loc` and `scale` parameters.\n    #### Mathematical details\n    \n    The probability density function (pdf) is,\n    \n    ```none\n    pdf(x; mu, k) = exp(k mu^T x) / Z\n    Z = (k ** (m / 2 - 1)) / ((2pi ** m / 2) * besseli(m / 2 - 1, k))\n    ```\n    where `loc = mu` is the mean, `scale = k` is the concentration, `m` is the dimensionality, and, `Z`\n    is the normalization constant.\n    \n    See https://en.wikipedia.org/wiki/Von_Mises-Fisher distribution for more details on the \n    Von Mises-Fiser distribution.\n    \n    """"""\n\n    def __init__(self, loc, scale, validate_args=False, allow_nan_stats=True, name=""von-Mises-Fisher""):\n        """"""Construct von-Mises-Fisher distributions with mean and concentration `loc` and `scale`.\n\n        Args:\n          loc: Floating point tensor; the mean of the distribution(s).\n          scale: Floating point tensor; the concentration of the distribution(s).\n            Must contain only non-negative values.\n          validate_args: Python `bool`, default `False`. When `True` distribution\n            parameters are checked for validity despite possibly degrading runtime\n            performance. When `False` invalid inputs may silently render incorrect\n            outputs.\n          allow_nan_stats: Python `bool`, default `True`. When `True`,\n            statistics (e.g., mean, mode, variance) use the value ""`NaN`"" to\n            indicate the result is undefined. When `False`, an exception is raised\n            if one or more of the statistic\'s batch members are undefined.\n          name: Python `str` name prefixed to Ops created by this class.\n\n        Raises:\n          TypeError: if `loc` and `scale` have different `dtype`.\n        """"""\n        parameters = locals()\n        with ops.name_scope(name, values=[loc, scale]):\n            with ops.control_dependencies([check_ops.assert_positive(scale),\n                                           check_ops.assert_near(linalg_ops.norm(loc, axis=-1), 1, atol=1e-7)]\n                                          if validate_args else []):\n                self._loc = array_ops.identity(loc, name=""loc"")\n                self._scale = array_ops.identity(scale, name=""scale"")\n                check_ops.assert_same_float_dtype([self._loc, self._scale])\n\n        super(VonMisesFisher, self).__init__(\n            dtype=self._scale.dtype,\n            reparameterization_type=distribution.FULLY_REPARAMETERIZED,\n            validate_args=validate_args,\n            allow_nan_stats=allow_nan_stats,\n            parameters=parameters,\n            graph_parents=[self._loc, self._scale],\n            name=name)\n\n        self.__m = math_ops.cast(self._loc.shape[-1], dtypes.int32)\n        self.__mf = math_ops.cast(self.__m, dtype=self.dtype)\n        self.__e1 = array_ops.one_hot([0], self.__m, dtype=self.dtype)\n\n    @staticmethod\n    def _param_shapes(sample_shape):\n        return dict(zip((""loc"", ""scale""), ([ops.convert_to_tensor(sample_shape, dtype=dtypes.int32),\n                                            ops.convert_to_tensor(sample_shape[:-1].concatenate([1]),\n                                                                  dtype=dtypes.int32)])))\n\n    @property\n    def loc(self):\n        """"""Distribution parameter for the mean.""""""\n        return self._loc\n\n    @property\n    def scale(self):\n        """"""Distribution parameter for concentration.""""""\n        return self._scale\n\n    def _batch_shape_tensor(self):\n        return array_ops.broadcast_dynamic_shape(\n            array_ops.shape(self._loc),\n            array_ops.shape(self._scale))\n\n    def _batch_shape(self):\n        return array_ops.broadcast_static_shape(\n            self._loc.get_shape(),\n            self._scale.get_shape())\n\n    def _event_shape_tensor(self):\n        return constant_op.constant([], dtype=dtypes.int32)\n\n    def _event_shape(self):\n        return tensor_shape.scalar()\n\n    def _sample_n(self, n, seed=None):\n        shape = array_ops.concat([[n], self.batch_shape_tensor()], 0)\n        w = control_flow_ops.cond(gen_math_ops.equal(self.__m, 3),\n                                  lambda: self.__sample_w3(n, seed),\n                                  lambda: self.__sample_w_rej(n, seed))\n\n        v = nn_impl.l2_normalize(array_ops.transpose(\n            array_ops.transpose(random_ops.random_normal(shape, dtype=self.dtype, seed=seed))[1:]), axis=-1)\n\n        x = array_ops.concat((w, math_ops.sqrt(1 - w ** 2) * v), axis=-1)\n        z = self.__householder_rotation(x)\n\n        return z\n\n    def __sample_w3(self, n, seed):\n        shape = array_ops.concat(([n], self.batch_shape_tensor()[:-1], [1]), 0)\n        u = random_ops.random_uniform(shape, dtype=self.dtype, seed=seed)\n        self.__w = 1 + math_ops.reduce_logsumexp([math_ops.log(u), math_ops.log(1 - u) - 2 * self.scale], axis=0) / self.scale\n        return self.__w\n\n    def __sample_w_rej(self, n, seed):\n        c = math_ops.sqrt((4 * (self.scale ** 2)) + (self.__mf - 1) ** 2)\n        b_true = (-2 * self.scale + c) / (self.__mf - 1)\n        \n        # using Taylor approximation with a smooth swift from 10 < scale < 11\n        # to avoid numerical errors for large scale\n        b_app = (self.__mf - 1) / (4 * self.scale)\n        s = gen_math_ops.minimum(gen_math_ops.maximum(0., self.scale - 10), 1.)\n        b = b_app * s + b_true * (1 - s)\n        \n        a = (self.__mf - 1 + 2 * self.scale + c) / 4\n        d = (4 * a * b) / (1 + b) - (self.__mf - 1) * math_ops.log(self.__mf - 1)\n\n        self.__b, (self.__e, self.__w) = b, self.__while_loop(b, a, d, n, seed)\n        return self.__w\n\n    def __while_loop(self, b, a, d, n, seed):\n        def __cond(w, e, bool_mask, b, a, d):\n            return math_ops.reduce_any(bool_mask)\n\n        def __body(w_, e_, bool_mask, b, a, d):\n            e = math_ops.cast(Beta((self.__mf - 1) / 2, (self.__mf - 1) / 2).sample(\n                shape, seed=seed), dtype=self.dtype)\n\n            u = random_ops.random_uniform(shape, dtype=self.dtype, seed=seed)\n\n            w = (1 - (1 + b) * e) / (1 - (1 - b) * e)\n            t = (2 * a * b) / (1 - (1 - b) * e)\n\n            accept = gen_math_ops.greater(((self.__mf - 1) * math_ops.log(t) - t + d), math_ops.log(u))\n            reject = gen_math_ops.logical_not(accept)\n\n            w_ = array_ops.where(gen_math_ops.logical_and(bool_mask, accept), w, w_)\n            e_ = array_ops.where(gen_math_ops.logical_and(bool_mask, accept), e, e_)\n            bool_mask = array_ops.where(gen_math_ops.logical_and(bool_mask, accept), reject, bool_mask)\n\n            return w_, e_, bool_mask, b, a, d\n\n        shape = array_ops.concat([[n], self.batch_shape_tensor()[:-1], [1]], 0)\n        b, a, d = [gen_array_ops.tile(array_ops.expand_dims(e, axis=0), [n] + [1] * len(e.shape)) for e in (b, a, d)]\n\n        w, e, bool_mask, b, a, d = control_flow_ops.while_loop(__cond, __body,\n                                                               [array_ops.zeros_like(b, dtype=self.dtype),\n                                                                array_ops.zeros_like(b, dtype=self.dtype),\n                                                                array_ops.ones_like(b, dtypes.bool),\n                                                                b, a, d])\n\n        return e, w\n\n    def __householder_rotation(self, x):\n        u = nn_impl.l2_normalize(self.__e1 - self._loc, axis=-1)\n        z = x - 2 * math_ops.reduce_sum(x * u, axis=-1, keepdims=True) * u\n        return z\n\n    def _log_prob(self, x):\n        return self._log_unnormalized_prob(x) - self._log_normalization()\n\n    def _prob(self, x):\n        return math_ops.exp(self._log_prob(x))\n\n    def _log_unnormalized_prob(self, x):\n        with ops.control_dependencies(\n                [check_ops.assert_near(linalg_ops.norm(x, axis=-1), 1, atol=1e-3)] if self.validate_args else []):\n            output = self.scale * math_ops.reduce_sum(self._loc * x, axis=-1, keepdims=True)\n\n        return array_ops.reshape(output, ops.convert_to_tensor(array_ops.shape(output)[:-1]))\n\n    def _log_normalization(self):\n        output = -((self.__mf / 2 - 1) * math_ops.log(self.scale) - (self.__mf / 2) * math.log(2 * math.pi) - (\n                    self.scale + math_ops.log(ive(self.__mf / 2 - 1, self.scale))))\n\n        return array_ops.reshape(output, ops.convert_to_tensor(array_ops.shape(output)[:-1]))\n\n    def _entropy(self):\n        return - array_ops.reshape(self.scale * ive(self.__mf / 2, self.scale) / ive((self.__mf / 2) - 1, self.scale),\n                                   ops.convert_to_tensor(array_ops.shape(self.scale)[:-1])) + self._log_normalization()\n\n    def _mean(self):\n        return self._loc * (ive(self.__mf / 2, self.scale) / ive(self.__mf / 2 - 1, self.scale))\n\n    def _mode(self):\n        return self._mean()\n\n\n@kullback_leibler.RegisterKL(VonMisesFisher, HypersphericalUniform)\ndef _kl_vmf_uniform(vmf, hyu, name=None):\n    with ops.control_dependencies([check_ops.assert_equal(vmf.loc.shape[-1] - 1, hyu.dim)]):\n        with ops.name_scope(name, ""_kl_vmf_uniform"", [vmf.scale]):\n            return - vmf.entropy() + hyu.entropy()\n'"
hyperspherical_vae/ops/__init__.py,0,b'from hyperspherical_vae.ops.ive import ive\n'
hyperspherical_vae/ops/ive.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The exponentially scaled modified Bessel function of the first kind.""""""\n\nimport numpy as np\nimport scipy.special\n\nfrom tensorflow.python.ops import script_ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops.custom_gradient import custom_gradient\n\n\n@custom_gradient\ndef ive(v, z):\n    """"""Exponentially scaled modified Bessel function of the first kind.""""""\n    output = array_ops.reshape(script_ops.py_func(\n        lambda v, z: np.select(condlist=[v == 0, v == 1],\n                               choicelist=[scipy.special.i0e(z, dtype=z.dtype),\n                                           scipy.special.i1e(z, dtype=z.dtype)],\n                               default=scipy.special.ive(v, z, dtype=z.dtype)), [v, z], z.dtype),\n        ops.convert_to_tensor(array_ops.shape(z), dtype=dtypes.int32))\n\n    def grad(dy):\n        return None, dy * (ive(v - 1, z) - ive(v, z) * (v + z) / z)\n\n    return output, grad\n'"
