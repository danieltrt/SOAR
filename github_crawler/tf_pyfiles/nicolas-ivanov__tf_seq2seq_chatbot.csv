file_path,api_count,code
chat.py,1,"b'import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tensorflow as tf\n\nfrom tf_seq2seq_chatbot.lib.chat import chat\n\n\ndef main(_):\n    chat()\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
test.py,1,"b'import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tensorflow as tf\n\nfrom tf_seq2seq_chatbot.lib.predict import predict\n\n\ndef main(_):\n    predict()\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
train.py,1,"b'import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tensorflow as tf\n\nfrom tf_seq2seq_chatbot.lib.train import train\n\n\ndef main(_):\n    train()\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
tf_seq2seq_chatbot/__init__.py,0,b''
tf_seq2seq_chatbot/configs/__init__.py,0,"b""__author__ = 'nicolas'\n"""
tf_seq2seq_chatbot/configs/config.py,13,"b""import tensorflow as tf\n\nTEST_DATASET_PATH = 'tf_seq2seq_chatbot/data/test/test_set.txt'\nSAVE_DATA_DIR = '/var/lib/tf_seq2seq_chatbot/'\n\ntf.app.flags.DEFINE_string('data_dir', SAVE_DATA_DIR + 'data', 'Data directory')\ntf.app.flags.DEFINE_string('model_dir', SAVE_DATA_DIR + 'nn_models', 'Train directory')\ntf.app.flags.DEFINE_string('results_dir', SAVE_DATA_DIR + 'results', 'Train directory')\n\ntf.app.flags.DEFINE_float('learning_rate', 0.5, 'Learning rate.')\ntf.app.flags.DEFINE_float('learning_rate_decay_factor', 0.99, 'Learning rate decays by this much.')\ntf.app.flags.DEFINE_float('max_gradient_norm', 5.0, 'Clip gradients to this norm.')\ntf.app.flags.DEFINE_integer('batch_size', 128, 'Batch size to use during training.')\n\ntf.app.flags.DEFINE_integer('vocab_size', 20000, 'Dialog vocabulary size.')\ntf.app.flags.DEFINE_integer('size', 128, 'Size of each model layer.')\ntf.app.flags.DEFINE_integer('num_layers', 1, 'Number of layers in the model.')\n\ntf.app.flags.DEFINE_integer('max_train_data_size', 0, 'Limit on the size of training data (0: no limit).')\ntf.app.flags.DEFINE_integer('steps_per_checkpoint', 100, 'How many training steps to do per checkpoint.')\n\nFLAGS = tf.app.flags.FLAGS\n\n# We use a number of buckets and pad to the closest one for efficiency.\n# See seq2seq_model.Seq2SeqModel for details of how they work.\nBUCKETS = [(5, 10), (10, 15), (20, 25), (40, 50)]\n\n\n\n"""
tf_seq2seq_chatbot/lib/__init__.py,0,b''
tf_seq2seq_chatbot/lib/chat.py,1,"b'import os\nimport sys\n\nimport tensorflow as tf\n\nfrom tf_seq2seq_chatbot.configs.config import FLAGS\nfrom tf_seq2seq_chatbot.lib import data_utils\nfrom tf_seq2seq_chatbot.lib.seq2seq_model_utils import create_model, get_predicted_sentence\n\n\ndef chat():\n  with tf.Session() as sess:\n    # Create model and load parameters.\n    model = create_model(sess, forward_only=True)\n    model.batch_size = 1  # We decode one sentence at a time.\n\n    # Load vocabularies.\n    vocab_path = os.path.join(FLAGS.data_dir, ""vocab%d.in"" % FLAGS.vocab_size)\n    vocab, rev_vocab = data_utils.initialize_vocabulary(vocab_path)\n\n    # Decode from standard input.\n    sys.stdout.write(""> "")\n    sys.stdout.flush()\n    sentence = sys.stdin.readline()\n\n    while sentence:\n        predicted_sentence = get_predicted_sentence(sentence, vocab, rev_vocab, model, sess)\n        print(predicted_sentence)\n        print(""> "")\n        sys.stdout.flush()\n        sentence = sys.stdin.readline()'"
tf_seq2seq_chatbot/lib/data_utils.py,0,"b'""""""Utilities for downloading data from WMT, tokenizing, vocabularies.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\n\nfrom tensorflow.python.platform import gfile\n\n# Special vocabulary symbols - we always put them at the start.\nfrom tf_seq2seq_chatbot.configs.config import BUCKETS\n\n_PAD = ""_PAD""\n_GO = ""_GO""\n_EOS = ""_EOS""\n_UNK = ""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\n\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(r""\\d{3,}"")\n\n\ndef get_dialog_train_set_path(path):\n  return os.path.join(path, \'chat\')\n\n\ndef get_dialog_dev_set_path(path):\n  return os.path.join(path, \'chat_test\')\n\n\ndef basic_tokenizer(sentence):\n  """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n  words = []\n  for space_separated_fragment in sentence.strip().split():\n    words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n  return [w.lower() for w in words if w]\n\n\ndef create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n                      tokenizer=None, normalize_digits=True):\n  """"""Create vocabulary file (if it does not exist yet) from data file.\n\n  Data file is assumed to contain one sentence per line. Each sentence is\n  tokenized and digits are normalized (if normalize_digits is set).\n  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n  We write it to vocabulary_path in a one-token-per-line format, so that later\n  token in the first line gets id=0, second line gets id=1, and so on.\n\n  Args:\n    vocabulary_path: path where the vocabulary will be created.\n    data_path: data file that will be used to create vocabulary.\n    max_vocabulary_size: limit on the size of the created vocabulary.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n    vocab = {}\n    with gfile.GFile(data_path, mode=""r"") as f:\n      counter = 0\n      for line in f:\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  processing line %d"" % counter)\n        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n        for w in tokens:\n          word = re.sub(_DIGIT_RE, ""0"", w) if normalize_digits else w\n          if word in vocab:\n            vocab[word] += 1\n          else:\n            vocab[word] = 1\n      vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n      if len(vocab_list) > max_vocabulary_size:\n        vocab_list = vocab_list[:max_vocabulary_size]\n      with gfile.GFile(vocabulary_path, mode=""w"") as vocab_file:\n        for w in vocab_list:\n          vocab_file.write(w + ""\\n"")\n\n\ndef initialize_vocabulary(vocabulary_path):\n  """"""Initialize vocabulary from file.\n\n  We assume the vocabulary is stored one-item-per-line, so a file:\n    dog\n    cat\n  will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n  also return the reversed-vocabulary [""dog"", ""cat""].\n\n  Args:\n    vocabulary_path: path to the file containing the vocabulary.\n\n  Returns:\n    a pair: the vocabulary (a dictionary mapping string to integers), and\n    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n  Raises:\n    ValueError: if the provided vocabulary_path does not exist.\n  """"""\n  if gfile.Exists(vocabulary_path):\n    rev_vocab = []\n\n    with gfile.GFile(vocabulary_path, mode=""r"") as f:\n      rev_vocab.extend(f.readlines())\n\n    rev_vocab = [line.strip() for line in rev_vocab]\n    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n    return vocab, rev_vocab\n\n  else:\n    raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary,\n                          tokenizer=None, normalize_digits=True):\n  """"""Convert a string to list of integers representing token-ids.\n\n  For example, a sentence ""I have a dog"" may become tokenized into\n  [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n  ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n  Args:\n    sentence: a string, the sentence to convert to token-ids.\n    vocabulary: a dictionary mapping tokens to integers.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n  Returns:\n    a list of integers, the token-ids for the sentence.\n  """"""\n  if tokenizer:\n    words = tokenizer(sentence)\n  else:\n    words = basic_tokenizer(sentence)\n  if not normalize_digits:\n    return [vocabulary.get(w, UNK_ID) for w in words]\n  # Normalize digits by 0 before looking words up in the vocabulary.\n  return [vocabulary.get(re.sub(_DIGIT_RE, ""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary_path,\n                      tokenizer=None, normalize_digits=True):\n  """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n  This function loads data line-by-line from data_path, calls the above\n  sentence_to_token_ids, and saves the result to target_path. See comment\n  for sentence_to_token_ids on the details of token-ids format.\n\n  Args:\n    data_path: path to the data file in one-sentence-per-line format.\n    target_path: path where the file with token-ids will be created.\n    vocabulary_path: path to the vocabulary file.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(target_path):\n    print(""Tokenizing data in %s"" % data_path)\n    vocab, _ = initialize_vocabulary(vocabulary_path)\n    with gfile.GFile(data_path, mode=""r"") as data_file:\n      with gfile.GFile(target_path, mode=""w"") as tokens_file:\n        counter = 0\n        for line in data_file:\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  tokenizing line %d"" % counter)\n          token_ids = sentence_to_token_ids(line, vocab, tokenizer,\n                                            normalize_digits)\n          tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\ndef prepare_dialog_data(data_dir, vocabulary_size):\n  """"""Get dialog data into data_dir, create vocabularies and tokenize data.\n\n  Args:\n    data_dir: directory in which the data sets will be stored.\n    vocabulary_size: size of the English vocabulary to create and use.\n\n  Returns:\n    A tuple of 3 elements:\n      (1) path to the token-ids for chat training data-set,\n      (2) path to the token-ids for chat development data-set,\n      (3) path to the chat vocabulary file\n  """"""\n  # Get dialog data to the specified directory.\n  train_path = get_dialog_train_set_path(data_dir)\n  dev_path = get_dialog_dev_set_path(data_dir)\n\n  # Create vocabularies of the appropriate sizes.\n  vocab_path = os.path.join(data_dir, ""vocab%d.in"" % vocabulary_size)\n  create_vocabulary(vocab_path, train_path + "".in"", vocabulary_size)\n\n  # Create token ids for the training data.\n  train_ids_path = train_path + ("".ids%d.in"" % vocabulary_size)\n  data_to_token_ids(train_path + "".in"", train_ids_path, vocab_path)\n\n  # Create token ids for the development data.\n  dev_ids_path = dev_path + ("".ids%d.in"" % vocabulary_size)\n  data_to_token_ids(dev_path + "".in"", dev_ids_path, vocab_path)\n\n  return (train_ids_path, dev_ids_path, vocab_path)\n\n\ndef read_data(tokenized_dialog_path, max_size=None):\n  """"""Read data from source file and put into buckets.\n\n  Args:\n    source_path: path to the files with token-ids.\n    max_size: maximum number of lines to read, all other will be ignored;\n      if 0 or None, data files will be read completely (no limit).\n\n  Returns:\n    data_set: a list of length len(_buckets); data_set[n] contains a list of\n      (source, target) pairs read from the provided data files that fit\n      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n      len(target) < _buckets[n][1]; source and target are lists of token-ids.\n  """"""\n  data_set = [[] for _ in BUCKETS]\n\n  with gfile.GFile(tokenized_dialog_path, mode=""r"") as fh:\n      source, target = fh.readline(), fh.readline()\n      counter = 0\n      while source and target and (not max_size or counter < max_size):\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  reading data line %d"" % counter)\n          sys.stdout.flush()\n\n        source_ids = [int(x) for x in source.split()]\n        target_ids = [int(x) for x in target.split()]\n        target_ids.append(EOS_ID)\n\n        for bucket_id, (source_size, target_size) in enumerate(BUCKETS):\n          if len(source_ids) < source_size and len(target_ids) < target_size:\n            data_set[bucket_id].append([source_ids, target_ids])\n            break\n        source, target = fh.readline(), fh.readline()\n  return data_set'"
tf_seq2seq_chatbot/lib/predict.py,1,"b'import os\n\nimport tensorflow as tf\n\nfrom tf_seq2seq_chatbot.configs.config import TEST_DATASET_PATH, FLAGS\nfrom tf_seq2seq_chatbot.lib import data_utils\nfrom tf_seq2seq_chatbot.lib.seq2seq_model_utils import create_model, get_predicted_sentence\n\n\ndef predict():\n    def _get_test_dataset():\n        with open(TEST_DATASET_PATH) as test_fh:\n            test_sentences = [s.strip() for s in test_fh.readlines()]\n        return test_sentences\n\n    results_filename = \'_\'.join([\'results\', str(FLAGS.num_layers), str(FLAGS.size), str(FLAGS.vocab_size)])\n    results_path = os.path.join(FLAGS.results_dir, results_filename)\n\n    with tf.Session() as sess, open(results_path, \'w\') as results_fh:\n        # Create model and load parameters.\n        model = create_model(sess, forward_only=True)\n        model.batch_size = 1  # We decode one sentence at a time.\n\n        # Load vocabularies.\n        vocab_path = os.path.join(FLAGS.data_dir, ""vocab%d.in"" % FLAGS.vocab_size)\n        vocab, rev_vocab = data_utils.initialize_vocabulary(vocab_path)\n\n        test_dataset = _get_test_dataset()\n\n        for sentence in test_dataset:\n            # Get token-ids for the input sentence.\n            predicted_sentence = get_predicted_sentence(sentence, vocab, rev_vocab, model, sess)\n            print(sentence, \' -> \', predicted_sentence)\n\n            results_fh.write(predicted_sentence + \'\\n\')\n'"
tf_seq2seq_chatbot/lib/seq2seq_model.py,22,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Sequence-to-sequence model with an attention mechanism.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.models.rnn.translate import data_utils\n\n\nclass Seq2SeqModel(object):\n  """"""Sequence-to-sequence model with attention and for multiple buckets.\n\n  This class implements a multi-layer recurrent neural network as encoder,\n  and an attention-based decoder. This is the same as the model described in\n  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n  or into the seq2seq library for complete model implementation.\n  This class also allows to use GRU cells in addition to LSTM cells, and\n  sampled softmax to handle large output vocabulary size. A single-layer\n  version of this model, but with bi-directional encoder, was presented in\n    http://arxiv.org/abs/1409.0473\n  and sampled softmax is described in Section 3 of the following paper.\n    http://arxiv.org/abs/1412.2007\n  """"""\n\n  def __init__(self, source_vocab_size, target_vocab_size, buckets, size,\n               num_layers, max_gradient_norm, batch_size, learning_rate,\n               learning_rate_decay_factor, use_lstm=False,\n               num_samples=512, forward_only=False):\n    """"""Create the model.\n\n    Args:\n      source_vocab_size: size of the source vocabulary.\n      target_vocab_size: size of the target vocabulary.\n      buckets: a list of pairs (I, O), where I specifies maximum input length\n        that will be processed in that bucket, and O specifies maximum output\n        length. Training instances that have inputs longer than I or outputs\n        longer than O will be pushed to the next bucket and padded accordingly.\n        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n      size: number of units in each layer of the model.\n      num_layers: number of layers in the model.\n      max_gradient_norm: gradients will be clipped to maximally this norm.\n      batch_size: the size of the batches used during training;\n        the model construction is independent of batch_size, so it can be\n        changed after initialization if this is convenient, e.g., for decoding.\n      learning_rate: learning rate to start with.\n      learning_rate_decay_factor: decay learning rate by this much when needed.\n      use_lstm: if true, we use LSTM cells instead of GRU cells.\n      num_samples: number of samples for sampled softmax.\n      forward_only: if set, we do not construct the backward pass in the model.\n    """"""\n    self.source_vocab_size = source_vocab_size\n    self.target_vocab_size = target_vocab_size\n    self.buckets = buckets\n    self.batch_size = batch_size\n    self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n    self.learning_rate_decay_op = self.learning_rate.assign(\n        self.learning_rate * learning_rate_decay_factor)\n    self.global_step = tf.Variable(0, trainable=False)\n\n    # If we use sampled softmax, we need an output projection.\n    output_projection = None\n    softmax_loss_function = None\n    # Sampled softmax only makes sense if we sample less than vocabulary size.\n    if num_samples > 0 and num_samples < self.target_vocab_size:\n      w = tf.get_variable(""proj_w"", [size, self.target_vocab_size])\n      w_t = tf.transpose(w)\n      b = tf.get_variable(""proj_b"", [self.target_vocab_size])\n      output_projection = (w, b)\n\n      def sampled_loss(inputs, labels):\n        labels = tf.reshape(labels, [-1, 1])\n        return tf.nn.sampled_softmax_loss(w_t, b, inputs, labels, num_samples,\n                self.target_vocab_size)\n      softmax_loss_function = sampled_loss\n\n    # Create the internal multi-layer cell for our RNN.\n    single_cell = tf.nn.rnn_cell.GRUCell(size)\n    if use_lstm:\n      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n    cell = single_cell\n    if num_layers > 1:\n      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n    # The seq2seq function: we use embedding for the input and attention.\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return tf.nn.seq2seq.embedding_attention_seq2seq(\n          encoder_inputs, decoder_inputs, cell,\n          num_encoder_symbols=source_vocab_size,\n          num_decoder_symbols=target_vocab_size,\n          embedding_size=size,\n          output_projection=output_projection,\n          feed_previous=do_decode)\n\n    # Feeds for inputs.\n    self.encoder_inputs = []\n    self.decoder_inputs = []\n    self.target_weights = []\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""encoder{0}"".format(i)))\n    for i in xrange(buckets[-1][1] + 1):\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""decoder{0}"".format(i)))\n      self.target_weights.append(tf.placeholder(tf.float32, shape=[None],\n                                                name=""weight{0}"".format(i)))\n\n    # Our targets are decoder inputs shifted by one.\n    targets = [self.decoder_inputs[i + 1]\n               for i in xrange(len(self.decoder_inputs) - 1)]\n\n    # Training outputs and losses.\n    if forward_only:\n      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          self.outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in self.outputs[b]\n          ]\n    else:\n      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    if not forward_only:\n      self.gradient_norms = []\n      self.updates = []\n      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(self.losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                         max_gradient_norm)\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n            zip(clipped_gradients, params), global_step=self.global_step))\n\n    self.saver = tf.train.Saver(tf.all_variables())\n\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n           bucket_id, forward_only):\n    """"""Run a step of the model feeding the given inputs.\n\n    Args:\n      session: tensorflow session to use.\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n      target_weights: list of numpy float vectors to feed as target weights.\n      bucket_id: which bucket of the model to use.\n      forward_only: whether to do the backward step or only forward.\n\n    Returns:\n      A triple consisting of gradient norm (or None if we did not do backward),\n      average perplexity, and the outputs.\n\n    Raises:\n      ValueError: if length of encoder_inputs, decoder_inputs, or\n        target_weights disagrees with bucket size for the specified bucket_id.\n    """"""\n    # Check if the sizes match.\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    if len(encoder_inputs) != encoder_size:\n      raise ValueError(""Encoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(encoder_inputs), encoder_size))\n    if len(decoder_inputs) != decoder_size:\n      raise ValueError(""Decoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(decoder_inputs), decoder_size))\n    if len(target_weights) != decoder_size:\n      raise ValueError(""Weights length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(target_weights), decoder_size))\n\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n    input_feed = {}\n    for l in xrange(encoder_size):\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n    for l in xrange(decoder_size):\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n      input_feed[self.target_weights[l].name] = target_weights[l]\n\n    # Since our targets are decoder inputs shifted by one, we need one more.\n    last_target = self.decoder_inputs[decoder_size].name\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n    # Output feed: depends on whether we do a backward step or not.\n    if not forward_only:\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                     self.gradient_norms[bucket_id],  # Gradient norm.\n                     self.losses[bucket_id]]  # Loss for this batch.\n    else:\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n      for l in xrange(decoder_size):  # Output logits.\n        output_feed.append(self.outputs[bucket_id][l])\n\n    outputs = session.run(output_feed, input_feed)\n    if not forward_only:\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n    else:\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\n  def get_batch(self, data, bucket_id):\n    """"""Get a random batch of data from the specified bucket, prepare for step.\n\n    To feed data in step(..) it must be a list of batch-major vectors, while\n    data here contains single length-major cases. So the main logic of this\n    function is to re-index data cases to be in the proper format for feeding.\n\n    Args:\n      data: a tuple of size len(self.buckets) in which each element contains\n        lists of pairs of input and output data that we use to create a batch.\n      bucket_id: integer, which bucket to get the batch for.\n\n    Returns:\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\n      the constructed batch that has the proper format to call step(...) later.\n    """"""\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    encoder_inputs, decoder_inputs = [], []\n\n    # Get a random batch of encoder and decoder inputs from data,\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\n    for _ in xrange(self.batch_size):\n      encoder_input, decoder_input = random.choice(data[bucket_id])\n\n      # Encoder inputs are padded and then reversed.\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n      # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\n      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n                            [data_utils.PAD_ID] * decoder_pad_size)\n\n    # Now we create batch-major vectors from the data selected above.\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n    # Batch encoder inputs are just re-indexed encoder_inputs.\n    for length_idx in xrange(encoder_size):\n      batch_encoder_inputs.append(\n          np.array([encoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n    for length_idx in xrange(decoder_size):\n      batch_decoder_inputs.append(\n          np.array([decoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n      # Create target_weights to be 0 for targets that are padding.\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n      for batch_idx in xrange(self.batch_size):\n        # We set weight to 0 if the corresponding target is a PAD symbol.\n        # The corresponding target is decoder_input shifted by 1 forward.\n        if length_idx < decoder_size - 1:\n          target = decoder_inputs[batch_idx][length_idx + 1]\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n          batch_weight[batch_idx] = 0.0\n      batch_weights.append(batch_weight)\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n'"
tf_seq2seq_chatbot/lib/seq2seq_model_utils.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\n\nfrom tf_seq2seq_chatbot.configs.config import FLAGS, BUCKETS\nfrom tf_seq2seq_chatbot.lib import data_utils\nfrom tf_seq2seq_chatbot.lib import seq2seq_model\n\n\ndef create_model(session, forward_only):\n  """"""Create translation model and initialize or load parameters in session.""""""\n  model = seq2seq_model.Seq2SeqModel(\n      source_vocab_size=FLAGS.vocab_size,\n      target_vocab_size=FLAGS.vocab_size,\n      buckets=BUCKETS,\n      size=FLAGS.size,\n      num_layers=FLAGS.num_layers,\n      max_gradient_norm=FLAGS.max_gradient_norm,\n      batch_size=FLAGS.batch_size,\n      learning_rate=FLAGS.learning_rate,\n      learning_rate_decay_factor=FLAGS.learning_rate_decay_factor,\n      use_lstm=False,\n      forward_only=forward_only)\n\n  ckpt = tf.train.get_checkpoint_state(FLAGS.model_dir)\n  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n    print(""Reading model parameters from %s"" % ckpt.model_checkpoint_path)\n    model.saver.restore(session, ckpt.model_checkpoint_path)\n  else:\n    print(""Created model with fresh parameters."")\n    session.run(tf.initialize_all_variables())\n  return model\n\n\ndef get_predicted_sentence(input_sentence, vocab, rev_vocab, model, sess):\n    input_token_ids = data_utils.sentence_to_token_ids(input_sentence, vocab)\n\n    # Which bucket does it belong to?\n    bucket_id = min([b for b in xrange(len(BUCKETS)) if BUCKETS[b][0] > len(input_token_ids)])\n    outputs = []\n\n    feed_data = {bucket_id: [(input_token_ids, outputs)]}\n    # Get a 1-element batch to feed the sentence to the model.\n    encoder_inputs, decoder_inputs, target_weights = model.get_batch(feed_data, bucket_id)\n\n    # Get output logits for the sentence.\n    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=True)\n\n    outputs = []\n    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n    for logit in output_logits:\n        selected_token_id = int(np.argmax(logit, axis=1))\n\n        if selected_token_id == data_utils.EOS_ID:\n            break\n        else:\n            outputs.append(selected_token_id)\n\n    # Forming output sentence on natural language\n    output_sentence = \' \'.join([rev_vocab[output] for output in outputs])\n\n    return output_sentence'"
tf_seq2seq_chatbot/lib/train.py,1,"b'import sys\nimport os\nimport math\nimport time\n\nimport numpy as np\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport tensorflow as tf\n\nfrom tf_seq2seq_chatbot.lib.seq2seq_model_utils import create_model\nfrom tf_seq2seq_chatbot.configs.config import FLAGS, BUCKETS\nfrom tf_seq2seq_chatbot.lib.data_utils import read_data\nfrom tf_seq2seq_chatbot.lib import data_utils\n\n\ndef train():\n    print(""Preparing dialog data in %s"" % FLAGS.data_dir)\n    train_data, dev_data, _ = data_utils.prepare_dialog_data(FLAGS.data_dir, FLAGS.vocab_size)\n\n    with tf.Session() as sess:\n\n        # Create model.\n        print(""Creating %d layers of %d units."" % (FLAGS.num_layers, FLAGS.size))\n        model = create_model(sess, forward_only=False)\n\n        # Read data into buckets and compute their sizes.\n        print (""Reading development and training data (limit: %d)."" % FLAGS.max_train_data_size)\n        dev_set = read_data(dev_data)\n        train_set = read_data(train_data, FLAGS.max_train_data_size)\n        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(BUCKETS))]\n        train_total_size = float(sum(train_bucket_sizes))\n\n        # A bucket scale is a list of increasing numbers from 0 to 1 that we\'ll use\n        # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n        # the size if i-th training bucket, as used later.\n        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                               for i in xrange(len(train_bucket_sizes))]\n\n        # This is the training loop.\n        step_time, loss = 0.0, 0.0\n        current_step = 0\n        previous_losses = []\n\n        while True:\n          # Choose a bucket according to data distribution. We pick a random number\n          # in [0, 1] and use the corresponding interval in train_buckets_scale.\n          random_number_01 = np.random.random_sample()\n          bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                           if train_buckets_scale[i] > random_number_01])\n\n          # Get a batch and make a step.\n          start_time = time.time()\n          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n              train_set, bucket_id)\n\n          _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                       target_weights, bucket_id, forward_only=False)\n\n          step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n          loss += step_loss / FLAGS.steps_per_checkpoint\n          current_step += 1\n\n          # Once in a while, we save checkpoint, print statistics, and run evals.\n          if current_step % FLAGS.steps_per_checkpoint == 0:\n            # Print statistics for the previous epoch.\n            perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\n            print (""global step %d learning rate %.4f step-time %.2f perplexity %.2f"" %\n                   (model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))\n\n            # Decrease learning rate if no improvement was seen over last 3 times.\n            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n              sess.run(model.learning_rate_decay_op)\n\n            previous_losses.append(loss)\n\n            # Save checkpoint and zero timer and loss.\n            checkpoint_path = os.path.join(FLAGS.model_dir, ""model.ckpt"")\n            model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n            step_time, loss = 0.0, 0.0\n\n            # Run evals on development set and print their perplexity.\n            for bucket_id in xrange(len(BUCKETS)):\n              encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\n              _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n\n              eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float(\'inf\')\n              print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\n\n            sys.stdout.flush()\n'"
