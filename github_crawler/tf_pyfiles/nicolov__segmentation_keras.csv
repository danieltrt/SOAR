file_path,api_count,code
convert_masks.py,0,"b""#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n'''\nConverts the .mat segmentation labels in the Augmented Pascal VOC\ndataset to color-coded .png images.\n\nDownload the dataset from:\n\nhttp://home.bharathh.info/pubs/codes/SBD/download.html\n'''\n\nimport argparse\nimport glob\nimport os\nfrom os import path\nimport scipy.io\nimport PIL\nimport numpy as np\n\nPASCAL_PALETTE = {\n    0: (0, 0, 0),\n    1: (128, 0, 0),\n    2: (0, 128, 0),\n    3: (128, 128, 0),\n    4: (0, 0, 128),\n    5: (128, 0, 128),\n    6: (0, 128, 128),\n    7: (128, 128, 128),\n    8: (64, 0, 0),\n    9: (192, 0, 0),\n    10: (64, 128, 0),\n    11: (192, 128, 0),\n    12: (64, 0, 128),\n    13: (192, 0, 128),\n    14: (64, 128, 128),\n    15: (192, 128, 128),\n    16: (0, 64, 0),\n    17: (128, 64, 0),\n    18: (0, 192, 0),\n    19: (128, 192, 0),\n    20: (0, 64, 128),\n}\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--in-dir', type=str, help='Input folder',\n                        required=True)\n    parser.add_argument('--out-dir', type=str, help='Output folder',\n                        required=True)\n    args = parser.parse_args()\n\n    files = sorted(glob.glob(path.join(args.in_dir, '*.mat')))\n\n    assert len(files), 'no matlab files found in the input folder'\n\n    try:\n        os.makedirs(args.out_dir)\n    except OSError:\n        pass\n\n    # BOUNDARIES_IDX = 0\n    SEGMENTATION_IDX = 1\n    # CATEGORIES_PRESENT_IDX  = 2\n\n    for f_cnt, fname in enumerate(files):\n        mat = scipy.io.loadmat(fname, mat_dtype=True)\n        seg_data = mat['GTcls'][0][0][SEGMENTATION_IDX]\n        img_data = np.zeros(seg_data.shape, dtype=np.uint8)\n\n        for i in range(img_data.shape[0]):\n            for j in range(img_data.shape[1]):\n                img_data[i, j] = seg_data[i, j]\n\n        img = PIL.Image.fromarray(img_data)\n        img_name = str.replace(path.basename(fname), '.mat', '.png')\n        img.save(path.join(args.out_dir, img_name), 'png')\n\n        print(f'{f_cnt:05}/{len(files):05}')\n\nif __name__ == '__main__':\n    main()"""
model.py,0,"b'from keras.layers import Activation, Reshape, Dropout\nfrom keras.layers import AtrousConvolution2D, Convolution2D, MaxPooling2D, ZeroPadding2D\nfrom keras.models import Sequential\n\n\n#\n# The VGG16 keras model is taken from here:\n# https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069\n# The (caffe) structure of DilatedNet is here:\n# https://github.com/fyu/dilation/blob/master/models/dilation8_pascal_voc_deploy.prototxt\n\ndef get_frontend(input_width, input_height) -> Sequential:\n    model = Sequential()\n    # model.add(ZeroPadding2D((1, 1), input_shape=(input_width, input_height, 3)))\n    model.add(Convolution2D(64, 3, 3, activation=\'relu\', name=\'conv1_1\', input_shape=(input_width, input_height, 3)))\n    model.add(Convolution2D(64, 3, 3, activation=\'relu\', name=\'conv1_2\'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    model.add(Convolution2D(128, 3, 3, activation=\'relu\', name=\'conv2_1\'))\n    model.add(Convolution2D(128, 3, 3, activation=\'relu\', name=\'conv2_2\'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\', name=\'conv3_1\'))\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\', name=\'conv3_2\'))\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\', name=\'conv3_3\'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\', name=\'conv4_1\'))\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\', name=\'conv4_2\'))\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\', name=\'conv4_3\'))\n\n    # Compared to the original VGG16, we skip the next 2 MaxPool layers,\n    # and go ahead with dilated convolutional layers instead\n\n    model.add(AtrousConvolution2D(512, 3, 3, atrous_rate=(2, 2), activation=\'relu\', name=\'conv5_1\'))\n    model.add(AtrousConvolution2D(512, 3, 3, atrous_rate=(2, 2), activation=\'relu\', name=\'conv5_2\'))\n    model.add(AtrousConvolution2D(512, 3, 3, atrous_rate=(2, 2), activation=\'relu\', name=\'conv5_3\'))\n\n    # Compared to the VGG16, we replace the FC layer with a convolution\n\n    model.add(AtrousConvolution2D(4096, 7, 7, atrous_rate=(4, 4), activation=\'relu\', name=\'fc6\'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(4096, 1, 1, activation=\'relu\', name=\'fc7\'))\n    model.add(Dropout(0.5))\n    # Note: this layer has linear activations, not ReLU\n    model.add(Convolution2D(21, 1, 1, activation=\'linear\', name=\'fc-final\'))\n\n    # model.layers[-1].output_shape == (None, 16, 16, 21)\n    return model\n\n\ndef add_softmax(model: Sequential) -> Sequential:\n    """""" Append the softmax layers to the frontend or frontend + context net. """"""\n    # The softmax layer doesn\'t work on the (width, height, channel)\n    # shape, so we reshape to (width*height, channel) first.\n    # https://github.com/fchollet/keras/issues/1169\n    _, curr_width, curr_height, curr_channels = model.layers[-1].output_shape\n\n    model.add(Reshape((curr_width * curr_height, curr_channels)))\n    model.add(Activation(\'softmax\'))\n    # Technically, we need another Reshape here to reshape to 2d, but TF\n    # the complains when batch_size > 1. We\'re just going to reshape in numpy.\n    # model.add(Reshape((curr_width, curr_height, curr_channels)))\n\n    return model\n\n\ndef add_context(model: Sequential) -> Sequential:\n    """""" Append the context layers to the frontend. """"""\n    model.add(ZeroPadding2D(padding=(33, 33)))\n    model.add(Convolution2D(42, 3, 3, activation=\'relu\', name=\'ct_conv1_1\'))\n    model.add(Convolution2D(42, 3, 3, activation=\'relu\', name=\'ct_conv1_2\'))\n    model.add(AtrousConvolution2D(84, 3, 3, atrous_rate=(2, 2), activation=\'relu\', name=\'ct_conv2_1\'))\n    model.add(AtrousConvolution2D(168, 3, 3, atrous_rate=(4, 4), activation=\'relu\', name=\'ct_conv3_1\'))\n    model.add(AtrousConvolution2D(336, 3, 3, atrous_rate=(8, 8), activation=\'relu\', name=\'ct_conv4_1\'))\n    model.add(AtrousConvolution2D(672, 3, 3, atrous_rate=(16, 16), activation=\'relu\', name=\'ct_conv5_1\'))\n    model.add(Convolution2D(672, 3, 3, activation=\'relu\', name=\'ct_fc1\'))\n    model.add(Convolution2D(21, 1, 1, name=\'ct_final\'))\n\n    return model\n'"
predict.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\'\'\'\nSegment images using weights from Fisher Yu (2016). Defaults to\nsettings for the Pascal VOC dataset.\n\'\'\'\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport os\n\nimport numpy as np\nfrom PIL import Image\nfrom IPython import embed\n\nfrom model import get_frontend, add_softmax, add_context\nfrom utils import interp_map, pascal_palette\n\n# Settings for the Pascal dataset\ninput_width, input_height = 900, 900\nlabel_margin = 186\n\nhas_context_module = False\n\ndef get_trained_model(args):\n    """""" Returns a model with loaded weights. """"""\n\n    model = get_frontend(input_width, input_height)\n\n    if has_context_module:\n        model = add_context(model)\n\n    model = add_softmax(model)\n\n    def load_tf_weights():\n        """""" Load pretrained weights converted from Caffe to TF. """"""\n\n        # \'latin1\' enables loading .npy files created with python2\n        weights_data = np.load(args.weights_path, encoding=\'latin1\').item()\n\n        for layer in model.layers:\n            if layer.name in weights_data.keys():\n                layer_weights = weights_data[layer.name]\n                layer.set_weights((layer_weights[\'weights\'],\n                                   layer_weights[\'biases\']))\n\n    def load_keras_weights():\n        """""" Load a Keras checkpoint. """"""\n        model.load_weights(args.weights_path)\n\n    if args.weights_path.endswith(\'.npy\'):\n        load_tf_weights()\n    elif args.weights_path.endswith(\'.hdf5\'):\n        load_keras_weights()\n    else:\n        raise Exception(""Unknown weights format."")\n\n    return model\n\n\ndef forward_pass(args):\n    \'\'\' Runs a forward pass to segment the image. \'\'\'\n\n    model = get_trained_model(args)\n\n    # Load image and swap RGB -> BGR to match the trained weights\n    image_rgb = np.array(Image.open(args.input_path)).astype(np.float32)\n    image = image_rgb[:, :, ::-1] - args.mean\n    image_size = image.shape\n\n    # Network input shape (batch_size=1)\n    net_in = np.zeros((1, input_height, input_width, 3), dtype=np.float32)\n\n    output_height = input_height - 2 * label_margin\n    output_width = input_width - 2 * label_margin\n\n    # This simplified prediction code is correct only if the output\n    # size is large enough to cover the input without tiling\n    assert image_size[0] < output_height\n    assert image_size[1] < output_width\n\n    # Center pad the original image by label_margin.\n    # This initial pad adds the context required for the prediction\n    # according to the preprocessing during training.\n    image = np.pad(image,\n                   ((label_margin, label_margin),\n                    (label_margin, label_margin),\n                    (0, 0)), \'reflect\')\n\n    # Add the remaining margin to fill the network input width. This\n    # time the image is aligned to the upper left corner though.\n    margins_h = (0, input_height - image.shape[0])\n    margins_w = (0, input_width - image.shape[1])\n    image = np.pad(image,\n                   (margins_h,\n                    margins_w,\n                    (0, 0)), \'reflect\')\n\n    # Run inference\n    net_in[0] = image\n    prob = model.predict(net_in)[0]\n\n    # Reshape to 2d here since the networks outputs a flat array per channel\n    prob_edge = np.sqrt(prob.shape[0]).astype(np.int)\n    prob = prob.reshape((prob_edge, prob_edge, 21))\n\n    # Upsample\n    if args.zoom > 1:\n        prob = interp_map(prob, args.zoom, image_size[1], image_size[0])\n\n    # Recover the most likely prediction (actual segment class)\n    prediction = np.argmax(prob, axis=2)\n\n    # Apply the color palette to the segmented image\n    color_image = np.array(pascal_palette)[prediction.ravel()].reshape(\n        prediction.shape + (3,))\n\n    print(\'Saving results to: \', args.output_path)\n    with open(args.output_path, \'wb\') as out_file:\n        Image.fromarray(color_image).save(out_file)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'input_path\', nargs=\'?\', default=\'images/cat.jpg\',\n                        help=\'Required path to input image\')\n    parser.add_argument(\'--output_path\', default=None,\n                        help=\'Path to segmented image\')\n    parser.add_argument(\'--mean\', nargs=\'*\', default=[102.93, 111.36, 116.52],\n                        help=\'Mean pixel value (BGR) for the dataset.\\n\'\n                             \'Default is the mean pixel of PASCAL dataset.\')\n    parser.add_argument(\'--zoom\', default=8, type=int,\n                        help=\'Upscaling factor\')\n    parser.add_argument(\'--weights_path\', default=\'./dilation_pascal16.npy\',\n                        help=\'Weights file\')\n\n    args = parser.parse_args()\n\n    if not args.output_path:\n        dir_name, file_name = os.path.split(args.input_path)\n        args.output_path = os.path.join(\n            dir_name,\n            \'{}_seg.png\'.format(\n                os.path.splitext(file_name)[0]))\n\n    forward_pass(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
train.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport time\nimport shutil\n\nimport click\nimport numpy as np\nfrom keras import callbacks, optimizers\nfrom IPython import embed\n\nfrom model import get_frontend, add_softmax\nfrom utils.image_reader import (\n    RandomTransformer,\n    SegmentationDataGenerator)\n\n\ndef load_weights(model, weights_path):\n    weights_data = np.load(weights_path, encoding=\'latin1\').item()\n\n    for layer in model.layers:\n        if layer.name in weights_data.keys():\n            layer_weights = weights_data[layer.name]\n            layer.set_weights((layer_weights[\'weights\'],\n                               layer_weights[\'biases\']))\n\n\n@click.command()\n@click.option(\'--train-list-fname\', type=click.Path(exists=True),\n              default=\'benchmark_RELEASE/dataset/train.txt\')\n@click.option(\'--val-list-fname\', type=click.Path(exists=True),\n              default=\'benchmark_RELEASE/dataset/val.txt\')\n@click.option(\'--img-root\', type=click.Path(exists=True),\n              default=\'benchmark_RELEASE/dataset/img\')\n@click.option(\'--mask-root\', type=click.Path(exists=True),\n              default=\'benchmark_RELEASE/dataset/pngs\')\n@click.option(\'--weights-path\', type=click.Path(exists=True),\n              default=\'conversion/converted/vgg_conv.npy\')\n@click.option(\'--batch-size\', type=int, default=1)\n@click.option(\'--learning-rate\', type=float, default=1e-4)\ndef train(train_list_fname,\n          val_list_fname,\n          img_root,\n          mask_root,\n          weights_path,\n          batch_size,\n          learning_rate):\n\n    # Create image generators for the training and validation sets. Validation has\n    # no data augmentation.\n    transformer_train = RandomTransformer(horizontal_flip=True, vertical_flip=True)\n    datagen_train = SegmentationDataGenerator(transformer_train)\n\n    transformer_val = RandomTransformer(horizontal_flip=False, vertical_flip=False)\n    datagen_val = SegmentationDataGenerator(transformer_val)\n\n    train_desc = \'{}-lr{:.0e}-bs{:03d}\'.format(\n        time.strftime(""%Y-%m-%d %H:%M""),\n        learning_rate,\n        batch_size)\n    checkpoints_folder = \'trained/\' + train_desc\n    try:\n        os.makedirs(checkpoints_folder)\n    except OSError:\n        shutil.rmtree(checkpoints_folder, ignore_errors=True)\n        os.makedirs(checkpoints_folder)\n\n    model_checkpoint = callbacks.ModelCheckpoint(\n        checkpoints_folder + \'/ep{epoch:02d}-vl{val_loss:.4f}.hdf5\',\n        monitor=\'loss\')\n    tensorboard_cback = callbacks.TensorBoard(\n        log_dir=\'{}/tboard\'.format(checkpoints_folder),\n        histogram_freq=0,\n        write_graph=False,\n        write_images=False)\n    csv_log_cback = callbacks.CSVLogger(\n        \'{}/history.log\'.format(checkpoints_folder))\n    reduce_lr_cback = callbacks.ReduceLROnPlateau(\n        monitor=\'val_loss\',\n        factor=0.2,\n        patience=5,\n        verbose=1,\n        min_lr=0.05 * learning_rate)\n\n    model = add_softmax(\n        get_frontend(500, 500))\n\n    load_weights(model, weights_path)\n\n    model.compile(loss=\'sparse_categorical_crossentropy\',\n                  optimizer=optimizers.SGD(lr=learning_rate, momentum=0.9),\n                  metrics=[\'accuracy\'])\n\n    # Build absolute image paths\n    def build_abs_paths(basenames):\n        img_fnames = [os.path.join(img_root, f) + \'.jpg\' for f in basenames]\n        mask_fnames = [os.path.join(mask_root, f) + \'.png\' for f in basenames]\n        return img_fnames, mask_fnames\n\n    train_basenames = [l.strip() for l in open(train_list_fname).readlines()]\n    val_basenames = [l.strip() for l in open(val_list_fname).readlines()][:500]\n\n    train_img_fnames, train_mask_fnames = build_abs_paths(train_basenames)\n    val_img_fnames, val_mask_fnames = build_abs_paths(val_basenames)\n\n    skipped_report_cback = callbacks.LambdaCallback(\n        on_epoch_end=lambda a, b: open(\n            \'{}/skipped.txt\'.format(checkpoints_folder), \'a\').write(\n            \'{}\\n\'.format(datagen_train.skipped_count)))\n\n    model.fit_generator(\n        datagen_train.flow_from_list(\n            train_img_fnames,\n            train_mask_fnames,\n            shuffle=True,\n            batch_size=batch_size,\n            img_target_size=(500, 500),\n            mask_target_size=(16, 16)),\n        samples_per_epoch=len(train_basenames),\n        nb_epoch=20,\n        validation_data=datagen_val.flow_from_list(\n            val_img_fnames,\n            val_mask_fnames,\n            batch_size=8,\n            img_target_size=(500, 500),\n            mask_target_size=(16, 16)),\n        nb_val_samples=len(val_basenames),\n        callbacks=[\n            model_checkpoint,\n            tensorboard_cback,\n            csv_log_cback,\n            reduce_lr_cback,\n            skipped_report_cback,\n        ])\n\n\nif __name__ == \'__main__\':\n    train()\n'"
utils/__init__.py,0,"b'import numpy as np\n\npascal_nclasses = 21\npascal_palette = np.array([(0, 0, 0)\n    , (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128)\n    , (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0)\n    , (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128)\n    , (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)], dtype=np.uint8)\n\n\n# 0=background\n# 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n# 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n# 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person\n# 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n\n\ndef mask_to_label(mask_rgb):\n    """"""From color-coded RGB mask to classes [0-21]""""""\n    mask_labels = np.zeros(mask_rgb.shape[:2])\n\n    for i in range(mask_rgb.shape[0]):\n        for j in range(mask_rgb.shape[1]):\n            mask_labels[i, j] = pascal_palette.index(tuple(mask_rgb[i, j, :].astype(np.uint8)))\n\n    return mask_labels\n\n\ndef interp_map(prob, zoom, width, height):\n    zoom_prob = np.zeros((height, width, prob.shape[2]), dtype=np.float32)\n    for c in range(prob.shape[2]):\n        for h in range(height):\n            for w in range(width):\n                r0 = h // zoom\n                r1 = r0 + 1\n                c0 = w // zoom\n                c1 = c0 + 1\n                rt = float(h) / zoom - r0\n                ct = float(w) / zoom - c0\n                v0 = rt * prob[r1, c0, c] + (1 - rt) * prob[r0, c0, c]\n                v1 = rt * prob[r1, c1, c] + (1 - rt) * prob[r0, c1, c]\n                zoom_prob[h, w, c] = (1 - ct) * v0 + ct * v1\n    return zoom_prob\n'"
utils/image_reader.py,0,"b'import os\nimport random\nfrom collections import namedtuple\n\nimport click\nimport numpy as np\nfrom IPython import embed\nfrom keras.preprocessing.image import (\n    load_img, img_to_array,\n    flip_axis)\n\n# The set of parameters that describes an instance of\n# (random) augmentation\nTransformParams = namedtuple(\n    \'TransformParameters\',\n    (\'do_hor_flip\', \'do_vert_flip\'))\n\npascal_mean = np.array([102.93, 111.36, 116.52])\n\nlabel_margin = 186\n\n\ndef load_img_array(fname, grayscale=False, target_size=None, dim_ordering=\'default\'):\n    """"""Loads and image file and returns an array.""""""\n    img = load_img(fname,\n                   grayscale=grayscale,\n                   target_size=target_size)\n    x = img_to_array(img, dim_ordering=dim_ordering)\n    return x\n\n\nclass RandomTransformer:\n    """"""To consistently add data augmentation to image pairs, we split the process in\n    two steps. First, we generate a stream of random augmentation parameters, that\n    can be zipped together with the images. Second, we do the actual transformation,\n    that has no randomness since the parameters are passed in.""""""\n\n    def __init__(self,\n                 horizontal_flip=False,\n                 vertical_flip=False):\n        self.horizontal_flip = horizontal_flip\n        self.vertical_flip = vertical_flip\n\n    def random_params_gen(self) -> TransformParams:\n        """"""Returns a generator of random transformation parameters.""""""\n        while True:\n            do_hor_flip = self.horizontal_flip and (np.random.random() < 0.5)\n            do_vert_flip = self.vertical_flip and (np.random.random() < 0.5)\n\n            yield TransformParams(do_hor_flip=do_hor_flip,\n                                  do_vert_flip=do_vert_flip)\n\n    @staticmethod\n    def transform(x: np.array, params: TransformParams) -> np.array:\n        """"""Transforms a single image according to the parameters given.""""""\n        if params.do_hor_flip:\n            x = flip_axis(x, 1)\n\n        if params.do_vert_flip:\n            x = flip_axis(x, 0)\n\n        return x\n\n\nclass SegmentationDataGenerator:\n    """"""A data generator for segmentation tasks, similar to ImageDataGenerator\n    in Keras, but with distinct pipelines for images and masks.\n\n    The idea is that this object holds no data, and only knows how to run\n    the pipeline to load, augment, and batch samples. The actual data (csv,\n    numpy, etc..) must be passed in to the fit/flow functions directly.""""""\n\n    skipped_count = 0\n\n    def __init__(self,\n                 random_transformer: RandomTransformer):\n        self.random_transformer = random_transformer\n\n    def get_processed_pairs(self,\n                            img_fnames,\n                            mask_fnames):\n        # Generators for image data\n        img_arrs = (load_img_array(f) for f in img_fnames)\n        mask_arrs = (load_img_array(f, grayscale=True) for f in mask_fnames)\n\n        def add_context_margin(image, margin_size, **pad_kwargs):\n            """""" Adds a margin-size border around the image, used for\n            providing context. """"""\n            return np.pad(image,\n                          ((margin_size, margin_size),\n                           (margin_size, margin_size),\n                           (0, 0)), **pad_kwargs)\n\n        def pad_to_square(image, min_size, **pad_kwargs):\n            """""" Add padding to make sure that the image is larger than (min_size * min_size).\n            This time, the image is aligned to the top left corner. """"""\n\n            h, w = image.shape[:2]\n\n            if h >= min_size and w >= min_size:\n                return image\n\n            top = bottom = left = right = 0\n\n            if h < min_size:\n                top = (min_size - h) // 2\n                bottom = min_size - h - top\n            if w < min_size:\n                left = (min_size - w) // 2\n                right = min_size - w - left\n\n            return np.pad(image,\n                          ((top, bottom),\n                           (left, right),\n                           (0, 0)), **pad_kwargs)\n\n        def pad_image(image):\n            image_pad_kwargs = dict(mode=\'reflect\')\n            image = add_context_margin(image, label_margin, **image_pad_kwargs)\n            return pad_to_square(image, 500, **image_pad_kwargs)\n\n        def pad_label(image):\n            # Same steps as the image, but the borders are constant white\n            label_pad_kwargs = dict(mode=\'constant\', constant_values=255)\n            image = add_context_margin(image, label_margin, **label_pad_kwargs)\n            return pad_to_square(image, 500, **label_pad_kwargs)\n\n        pairs = ((pad_image(image), pad_label(label)) for\n                 image, label in zip(img_arrs, mask_arrs))\n\n        # random/center crop\n        def crop_to(image, target_h=500, target_w=500):\n            # TODO: random cropping\n            h_off = (image.shape[0] - target_h) // 2\n            w_off = (image.shape[1] - target_w) // 2\n            return image[h_off:h_off + target_h,\n                   w_off:w_off + target_w, :]\n\n        pairs = ((crop_to(image), crop_to(label)) for\n                 image, label in pairs)\n\n        # random augmentation\n        augmentation_params = self.random_transformer.random_params_gen()\n        transf_fn = self.random_transformer.transform\n        pairs = ((transf_fn(image, params), transf_fn(label, params)) for\n                 ((image, label), params) in zip(pairs, augmentation_params))\n\n        def rgb_to_bgr(image):\n            # Swap color channels to use pretrained VGG weights\n            return image[:, :, ::-1]\n\n        pairs = ((rgb_to_bgr(image), rgb_to_bgr(label)) for\n                 image, label in pairs)\n\n        def remove_mean(image):\n            # Note that there\'s no 0..1 normalization in VGG\n            return image - pascal_mean\n\n        pairs = ((remove_mean(image), label) for\n                 image, label in pairs)\n\n        def slice_label(image, offset, label_size, stride):\n            # Builds label_size * label_size pixels labels, starting from\n            # offset from the original image, and stride stride\n            return image[offset:offset + label_size * stride:stride,\n                   offset:offset + label_size * stride:stride]\n\n        pairs = ((image, slice_label(label, label_margin, 16, 8)) for\n                 image, label in pairs)\n\n        return pairs\n\n    def flow_from_list(self,\n                       img_fnames,\n                       mask_fnames,\n                       batch_size,\n                       img_target_size,\n                       mask_target_size,\n                       shuffle=False):\n        assert batch_size > 0\n\n        paired_fnames = list(zip(img_fnames, mask_fnames))\n\n        while True:\n            # Starting a new epoch..\n            if shuffle:\n                random.shuffle(paired_fnames)  # Shuffles in place\n            img_fnames, mask_fnames = zip(*paired_fnames)\n\n            pairs = self.get_processed_pairs(img_fnames, mask_fnames)\n\n            i = 0\n            img_batch = np.zeros((batch_size, img_target_size[0], img_target_size[1], 3))\n            mask_batch = np.zeros((batch_size, mask_target_size[0] * mask_target_size[1], 1))\n            for img, mask in pairs:\n                # Fill up the batch one pair at a time\n                img_batch[i] = img\n                # Pass the label image as 1D array to avoid the problematic Reshape\n                # layer after Softmax (see model.py)\n                mask_batch[i] = np.reshape(mask, (-1, 1))\n\n                # TODO: remove this ugly workaround to skip pairs whose mask\n                # has non-labeled pixels.\n                if 255. in mask:\n                    self.skipped_count += 1\n                    continue\n\n                i += 1\n                if i == batch_size:\n                    i = 0\n                    yield img_batch, mask_batch\n\n\n@click.command()\n@click.option(\'--list-fname\', type=click.Path(exists=True),\n              default=\'/mnt/pascal_voc/benchmark_RELEASE/dataset/train.txt\')\n@click.option(\'--img-root\', type=click.Path(exists=True),\n              default=\'/mnt/pascal_voc/benchmark_RELEASE/dataset/img\')\n@click.option(\'--mask-root\', type=click.Path(exists=True),\n              default=\'/mnt/pascal_voc/benchmark_RELEASE/dataset/pngs\')\ndef test_datagen(list_fname, img_root, mask_root):\n    datagen = SegmentationDataGenerator()\n\n    basenames = [l.strip() for l in open(list_fname).readlines()]\n    img_fnames = [os.path.join(img_root, f) + \'.jpg\' for f in basenames]\n    mask_fnames = [os.path.join(mask_root, f) + \'.png\' for f in basenames]\n\n    datagen.flow_from_list(img_fnames, mask_fnames)\n\n\nif __name__ == \'__main__\':\n    test_datagen()\n'"
