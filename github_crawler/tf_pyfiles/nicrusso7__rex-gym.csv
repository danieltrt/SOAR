file_path,api_count,code
setup.py,0,"b""import os\nimport platform\nfrom distutils.core import setup\n\nfrom setuptools import find_packages\n\nthis_directory = os.path.abspath(os.path.dirname(__file__))\nos_name = platform.system()\n\nwith open(os.path.join(this_directory, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\nwith open(os.path.join(this_directory, 'requirements.txt'), encoding='utf-8') as f:\n    requirements = f.read()\n\n\ndef copy_assets(dir_path):\n    base_dir = os.path.join('rex_gym', dir_path)\n    if os_name == 'Windows':\n        sep = '\\\\'\n    else:\n        sep = '/'\n    for (dirpath, dirnames, files) in os.walk(base_dir):\n        for f in files:\n            yield os.path.join(dirpath.split(sep, 1)[1], f)\n\n\nsetup(\n    name='rex_gym',\n    version='0.1.9',\n    license='Apache 2.0',\n    packages=find_packages(),\n    author='Nicola Russo',\n    author_email='dott.nicolarusso@gmail.com',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://github.com/nicrusso7/rex-gym',\n    download_url='https://github.com/nicrusso7/rex-gym/archive/master.zip',\n    install_requires=requirements,\n    entry_points='''\n        [console_scripts]\n        rex-gym=rex_gym.cli.entry_point:cli\n    ''',\n    package_data={\n        '': [f for f in copy_assets('policies')] + [a for a in copy_assets('util')]\n    },\n    keywords=['openai', 'gym', 'robot', 'quadruped', 'pybullet', 'ai', 'reinforcement learning', 'machine learning',\n              'RL', 'ML', 'tensorflow', 'spotmicro', 'rex'],\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Framework :: Robot Framework :: Library',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python :: 3.7']\n)\n"""
rex_gym/__init__.py,0,"b'from rex_gym.agents import ppo, tools, scripts\nfrom rex_gym.envs import gym, rex_gym_env\nfrom rex_gym.model import motor, rex\nfrom rex_gym.util import pybullet_data, bullet_client\n'"
rex_gym/agents/__init__.py,0,b'\n'
rex_gym/cli/__init__.py,0,b''
rex_gym/cli/entry_point.py,0,"b'import click\n\nfrom rex_gym.util import action_mapper\n\n\n@click.group()\ndef cli():\n    pass\n\n\n@cli.command()\n@click.option(\'--env\', \'-e\', required=True, help=""The Environment name."",\n              type=click.Choice([e for e in action_mapper.ENV_ID_TO_ENV_NAMES.keys()], case_sensitive=True))\n@click.option(\'--arg\', \'-a\', type=(str, float), help=""The Environment\'s arg(s)."", multiple=True)\n@click.option(\'--flag\', \'-f\', type=(str, bool), help=""The Environment\'s flag(s)."", multiple=True)\ndef policy(env, arg, flag):\n    from rex_gym.playground.policy_player import PolicyPlayer\n    args = _parse_args(arg + flag)\n    PolicyPlayer(env, args).play()\n\n\n@cli.command()\n@click.option(\'--env\', \'-e\', required=True, help=""The Environment name."",\n              type=click.Choice([e for e in action_mapper.ENV_ID_TO_ENV_NAMES.keys()], case_sensitive=True))\n@click.option(\'--arg\', \'-a\', type=(str, float), help=""The Environment\'s arg(s)."", multiple=True)\n@click.option(\'--flag\', \'-f\', type=(str, bool), help=""The Environment\'s flag(s)."", multiple=True)\n@click.option(\'--log-dir\', \'-l\', required=True, help=""The path where the log directory will be created."")\n@click.option(\'--playground\', \'-p\', type=bool, default=False, help=""Playground training: 1 agent, render enabled."")\n@click.option(\'--agents-number\', \'-n\', type=int, default=None, help="" Set the number of parallel agents."")\ndef train(env, arg, flag, log_dir, playground, agents_number):\n    from rex_gym.playground.trainer import Trainer\n    args = _parse_args(arg + flag)\n    Trainer(env, args, playground, log_dir, agents_number).start_training()\n\n\ndef _parse_args(params):\n    args = {}\n    for k, v in params:\n        args[k] = v\n    return args\n'"
rex_gym/envs/__init__.py,0,b'\n'
rex_gym/envs/rex_gym_env.py,0,"b'""""""This file implements the gym environment of Rex.\n\n""""""\nimport math\nimport time\nimport gym\nimport numpy as np\nimport pybullet\nimport pybullet_data\n\nfrom gym import spaces\nfrom gym.utils import seeding\n\nfrom ..model import rex, motor\nfrom ..util import bullet_client\n\nNUM_MOTORS = 12\nMOTOR_ANGLE_OBSERVATION_INDEX = 0\nMOTOR_VELOCITY_OBSERVATION_INDEX = MOTOR_ANGLE_OBSERVATION_INDEX + NUM_MOTORS\nMOTOR_TORQUE_OBSERVATION_INDEX = MOTOR_VELOCITY_OBSERVATION_INDEX + NUM_MOTORS\nBASE_ORIENTATION_OBSERVATION_INDEX = MOTOR_TORQUE_OBSERVATION_INDEX + NUM_MOTORS\nOBSERVATION_EPS = 0.01\nRENDER_HEIGHT = 360\nRENDER_WIDTH = 480\nSENSOR_NOISE_STDDEV = rex.SENSOR_NOISE_STDDEV\nDEFAULT_URDF_VERSION = ""default""\nNUM_SIMULATION_ITERATION_STEPS = 300\n\nREX_URDF_VERSION_MAP = {\n    DEFAULT_URDF_VERSION: rex.Rex\n}\n\n\ndef convert_to_list(obj):\n    try:\n        iter(obj)\n        return obj\n    except TypeError:\n        return [obj]\n\n\nclass RexGymEnv(gym.Env):\n    """"""The gym environment for Rex.\n\n      It simulates the locomotion of Rex, a quadruped robot. The state space\n      include the angles, velocities and torques for all the motors and the action\n      space is the desired motor angle for each motor. The reward function is based\n      on how far Rex walks in 1000 steps and penalizes the energy\n      expenditure.\n\n      """"""\n    metadata = {""render.modes"": [""human"", ""rgb_array""], ""video.frames_per_second"": 100}\n\n    def __init__(self,\n                 urdf_root=pybullet_data.getDataPath(),\n                 urdf_version=None,\n                 distance_weight=1.0,\n                 energy_weight=0.005,\n                 shake_weight=0.0,\n                 drift_weight=2.0,\n                 distance_limit=float(""inf""),\n                 observation_noise_stdev=SENSOR_NOISE_STDDEV,\n                 self_collision_enabled=True,\n                 motor_velocity_limit=np.inf,\n                 pd_control_enabled=False,\n                 leg_model_enabled=True,\n                 accurate_motor_model_enabled=False,\n                 remove_default_joint_damping=False,\n                 motor_kp=1.0,\n                 motor_kd=0.02,\n                 control_latency=0.0,\n                 pd_latency=0.0,\n                 torque_control_enabled=False,\n                 motor_overheat_protection=False,\n                 hard_reset=True,\n                 on_rack=False,\n                 render=True,\n                 num_steps_to_log=1000,\n                 action_repeat=1,\n                 control_time_step=None,\n                 env_randomizer=None,\n                 forward_reward_cap=float(""inf""),\n                 reflection=True,\n                 log_path=None,\n                 target_orient=None,\n                 init_orient=None):\n        """""" Initialize the rex gym environment.\n\n            Args:\n              urdf_root: The path to the urdf data folder.\n              urdf_version: [DEFAULT_URDF_VERSION] are allowable\n                versions. If None, DEFAULT_URDF_VERSION is used.\n              distance_weight: The weight of the distance term in the reward.\n              energy_weight: The weight of the energy term in the reward.\n              shake_weight: The weight of the vertical shakiness term in the reward.\n              drift_weight: The weight of the sideways drift term in the reward.\n              distance_limit: The maximum distance to terminate the episode.\n              observation_noise_stdev: The standard deviation of observation noise.\n              self_collision_enabled: Whether to enable self collision in the sim.\n              motor_velocity_limit: The velocity limit of each motor.\n              pd_control_enabled: Whether to use PD controller for each motor.\n              leg_model_enabled: Whether to use a leg motor to reparameterize the action\n                space.\n              accurate_motor_model_enabled: Whether to use the accurate DC motor model.\n              remove_default_joint_damping: Whether to remove the default joint damping.\n              motor_kp: proportional gain for the accurate motor model.\n              motor_kd: derivative gain for the accurate motor model.\n              control_latency: It is the delay in the controller between when an\n                observation is made at some point, and when that reading is reported\n                back to the Neural Network.\n              pd_latency: latency of the PD controller loop. PD calculates PWM based on\n                the motor angle and velocity. The latency measures the time between when\n                the motor angle and velocity are observed on the microcontroller and\n                when the true state happens on the motor. It is typically (0.001-\n                0.002s).\n              torque_control_enabled: Whether to use the torque control, if set to\n                False, pose control will be used.\n              motor_overheat_protection: Whether to shutdown the motor that has exerted\n                large torque (OVERHEAT_SHUTDOWN_TORQUE) for an extended amount of time\n                (OVERHEAT_SHUTDOWN_TIME). See ApplyAction() in rex.py for more\n                details.\n              hard_reset: Whether to wipe the simulation and load everything when reset\n                is called. If set to false, reset just place Rex back to start\n                position and set its pose to initial configuration.\n              on_rack: Whether to place Rex on rack. This is only used to debug\n                the walking gait. In this mode, Rex\'s base is hanged midair so\n                that its walking gait is clearer to visualize.\n              render: Whether to render the simulation.\n              num_steps_to_log: The max number of control steps in one episode that will\n                be logged. If the number of steps is more than num_steps_to_log, the\n                environment will still be running, but only first num_steps_to_log will\n                be recorded in logging.\n              action_repeat: The number of simulation steps before actions are applied.\n              control_time_step: The time step between two successive control signals.\n              env_randomizer: An instance (or a list) of EnvRandomizer(s). An\n                EnvRandomizer may randomize the physical property of rex, change\n                  the terrrain during reset(), or add perturbation forces during step().\n              forward_reward_cap: The maximum value that forward reward is capped at.\n                Disabled (Inf) by default.\n              log_path: The path to write out logs. For the details of logging, refer to\n                rex_logging.proto.\n            Raises:\n              ValueError: If the urdf_version is not supported.\n        """"""\n        # Set up logging.\n        self._log_path = log_path\n        # @TODO fix logging\n        self.logging = None\n        # PD control needs smaller time step for stability.\n        if control_time_step is not None:\n            self.control_time_step = control_time_step\n            self._action_repeat = action_repeat\n            self._time_step = control_time_step / action_repeat\n        else:\n            # Default values for time step and action repeat\n            if accurate_motor_model_enabled or pd_control_enabled:\n                self._time_step = 0.002\n                self._action_repeat = 5\n            else:\n                self._time_step = 0.01\n                self._action_repeat = 1\n            self.control_time_step = self._time_step * self._action_repeat\n        # TODO: Fix the value of self._num_bullet_solver_iterations.\n        self._num_bullet_solver_iterations = int(NUM_SIMULATION_ITERATION_STEPS / self._action_repeat)\n        self._urdf_root = urdf_root\n        self._self_collision_enabled = self_collision_enabled\n        self._motor_velocity_limit = motor_velocity_limit\n        self._observation = []\n        self._true_observation = []\n        self._objectives = []\n        self._objective_weights = [distance_weight, energy_weight, drift_weight, shake_weight]\n        self._env_step_counter = 0\n        self._num_steps_to_log = num_steps_to_log\n        self._is_render = render\n        self._last_base_position = [0, 0, 0]\n        self._last_base_orientation = [0, 0, 0, 1]\n        self._distance_weight = distance_weight\n        self._energy_weight = energy_weight\n        self._drift_weight = drift_weight\n        self._shake_weight = shake_weight\n        self._distance_limit = distance_limit\n        self._observation_noise_stdev = observation_noise_stdev\n        self._action_bound = 1\n        self._pd_control_enabled = pd_control_enabled\n        self._leg_model_enabled = leg_model_enabled\n        self._accurate_motor_model_enabled = accurate_motor_model_enabled\n        self._remove_default_joint_damping = remove_default_joint_damping\n        self._motor_kp = motor_kp\n        self._motor_kd = motor_kd\n        self._torque_control_enabled = torque_control_enabled\n        self._motor_overheat_protection = motor_overheat_protection\n        self._on_rack = on_rack\n        self._cam_dist = 1.0\n        self._cam_yaw = 0\n        self._cam_pitch = -30\n        self._forward_reward_cap = forward_reward_cap\n        self._hard_reset = True\n        self._last_frame_time = 0.0\n        self._control_latency = control_latency\n        self._pd_latency = pd_latency\n        self._urdf_version = urdf_version\n        self._ground_id = None\n        self._reflection = reflection\n        self._env_randomizers = convert_to_list(env_randomizer) if env_randomizer else []\n        # @TODO fix logging\n        self._episode_proto = None\n        if self._is_render:\n            self._pybullet_client = bullet_client.BulletClient(connection_mode=pybullet.GUI)\n        else:\n            self._pybullet_client = bullet_client.BulletClient()\n        if self._urdf_version is None:\n            self._urdf_version = DEFAULT_URDF_VERSION\n        self._pybullet_client.setPhysicsEngineParameter(enableConeFriction=0)\n        self._target_orient = target_orient\n        self._init_orient = init_orient\n        self._random_pos_target = False\n        self._random_pos_start = False\n        self._random_orient_target = False\n        self._random_orient_start = False\n        self.seed()\n        self.reset()\n        observation_high = (self._get_observation_upper_bound() + OBSERVATION_EPS)\n        observation_low = (self._get_observation_lower_bound() - OBSERVATION_EPS)\n        action_dim = NUM_MOTORS\n        action_high = np.array([self._action_bound] * action_dim)\n        self.action_space = spaces.Box(-action_high, action_high)\n        self.observation_space = spaces.Box(observation_low, observation_high)\n        self.viewer = None\n        self._hard_reset = hard_reset  # This assignment need to be after reset()\n        self.goal_reached = False\n\n    def close(self):\n        # @TODO fix logger\n        # if self._env_step_counter > 0:\n        #     self.logging.save_episode(self._episode_proto)\n        self.rex.Terminate()\n\n    def add_env_randomizer(self, env_randomizer):\n        self._env_randomizers.append(env_randomizer)\n\n    def reset(self, initial_motor_angles=None, reset_duration=1.0):\n        self._pybullet_client.configureDebugVisualizer(self._pybullet_client.COV_ENABLE_RENDERING, 0)\n        # @TODO fix logger\n        # if self._env_step_counter > 0:\n        #     self.logging.save_episode(self._episode_proto)\n        # self._episode_proto = rex_logging_pb2.RexEpisode()\n        # rex_logging.preallocate_episode_proto(self._episode_proto, self._num_steps_to_log)\n        if self._hard_reset:\n            self._pybullet_client.resetSimulation()\n            self._pybullet_client.setPhysicsEngineParameter(\n                numSolverIterations=int(self._num_bullet_solver_iterations))\n            self._pybullet_client.setTimeStep(self._time_step)\n            self._ground_id = self._pybullet_client.loadURDF(""%s/plane.urdf"" % self._urdf_root)\n            if self._reflection:\n                self._pybullet_client.changeVisualShape(self._ground_id, -1, rgbaColor=[1, 1, 1, 0.8])\n                self._pybullet_client.configureDebugVisualizer(\n                    self._pybullet_client.COV_ENABLE_PLANAR_REFLECTION, self._ground_id)\n            self._pybullet_client.setGravity(0, 0, -10)\n            acc_motor = self._accurate_motor_model_enabled\n            motor_protect = self._motor_overheat_protection\n            if self._urdf_version not in REX_URDF_VERSION_MAP:\n                raise ValueError(""%s is not a supported urdf_version."" % self._urdf_version)\n            else:\n                self.rex = (REX_URDF_VERSION_MAP[self._urdf_version](\n                    pybullet_client=self._pybullet_client,\n                    action_repeat=self._action_repeat,\n                    urdf_root=self._urdf_root,\n                    time_step=self._time_step,\n                    self_collision_enabled=self._self_collision_enabled,\n                    motor_velocity_limit=self._motor_velocity_limit,\n                    pd_control_enabled=self._pd_control_enabled,\n                    accurate_motor_model_enabled=acc_motor,\n                    remove_default_joint_damping=self._remove_default_joint_damping,\n                    motor_kp=self._motor_kp,\n                    motor_kd=self._motor_kd,\n                    control_latency=self._control_latency,\n                    pd_latency=self._pd_latency,\n                    observation_noise_stdev=self._observation_noise_stdev,\n                    torque_control_enabled=self._torque_control_enabled,\n                    motor_overheat_protection=motor_protect,\n                    on_rack=self._on_rack))\n        self.rex.Reset(reload_urdf=False,\n                       default_motor_angles=initial_motor_angles,\n                       reset_time=reset_duration)\n\n        # Loop over all env randomizers.\n        for env_randomizer in self._env_randomizers:\n            env_randomizer.randomize_env(self)\n\n        self._pybullet_client.setPhysicsEngineParameter(enableConeFriction=0)\n        self._env_step_counter = 0\n        self._last_base_position = [0, 0, 0]\n        self._last_base_orientation = [0, 0, 0, 1]\n        self._objectives = []\n        self._pybullet_client.resetDebugVisualizerCamera(self._cam_dist, self._cam_yaw,\n                                                         self._cam_pitch, [0, 0, 0])\n        self._pybullet_client.configureDebugVisualizer(self._pybullet_client.COV_ENABLE_RENDERING, 1)\n        return self._get_observation()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _transform_action_to_motor_command(self, action):\n        pass\n\n    def step(self, action):\n        """"""Step forward the simulation, given the action.\n\n        Args:\n          action: A list of desired motor angles for eight motors.\n\n        Returns:\n          observations: The angles, velocities and torques of all motors.\n          reward: The reward for the current state-action pair.\n          done: Whether the episode has ended.\n          info: A dictionary that stores diagnostic information.\n\n        Raises:\n          ValueError: The action dimension is not the same as the number of motors.\n          ValueError: The magnitude of actions is out of bounds.\n        """"""\n        self._last_base_position = self.rex.GetBasePosition()\n        self._last_base_orientation = self.rex.GetBaseOrientation()\n        if self._is_render:\n            # Sleep, otherwise the computation takes less time than real time,\n            # which will make the visualization like a fast-forward video.\n            time_spent = time.time() - self._last_frame_time\n            self._last_frame_time = time.time()\n            time_to_sleep = self.control_time_step - time_spent\n            if time_to_sleep > 0:\n                time.sleep(time_to_sleep)\n            base_pos = self.rex.GetBasePosition()\n            # Keep the previous orientation of the camera set by the user.\n            [yaw, pitch, dist] = self._pybullet_client.getDebugVisualizerCamera()[8:11]\n            self._pybullet_client.resetDebugVisualizerCamera(dist, yaw, pitch, base_pos)\n\n        for env_randomizer in self._env_randomizers:\n            env_randomizer.randomize_step(self)\n\n        action = self._transform_action_to_motor_command(action)\n        self.rex.Step(action)\n        reward = self._reward()\n        done = self._termination()\n        # @TODO fix logging\n        # if self._log_path is not None:\n        #     rex_logging.update_episode_proto(self._episode_proto, self.rex, action,\n        #                                      self._env_step_counter)\n        self._env_step_counter += 1\n        if done:\n            self.rex.Terminate()\n        return np.array(self._get_observation()), reward, done, {\'action\': action}\n\n    def render(self, mode=""rgb_array"", close=False):\n        if mode != ""rgb_array"":\n            return np.array([])\n        base_pos = self.rex.GetBasePosition()\n        view_matrix = self._pybullet_client.computeViewMatrixFromYawPitchRoll(\n            cameraTargetPosition=base_pos,\n            distance=self._cam_dist,\n            yaw=self._cam_yaw,\n            pitch=self._cam_pitch,\n            roll=0,\n            upAxisIndex=2)\n        proj_matrix = self._pybullet_client.computeProjectionMatrixFOV(fov=60,\n                                                                       aspect=float(RENDER_WIDTH) / RENDER_HEIGHT,\n                                                                       nearVal=0.1,\n                                                                       farVal=100.0)\n        (_, _, px, _, _) = self._pybullet_client.getCameraImage(\n            width=RENDER_WIDTH,\n            height=RENDER_HEIGHT,\n            renderer=self._pybullet_client.ER_BULLET_HARDWARE_OPENGL,\n            viewMatrix=view_matrix,\n            projectionMatrix=proj_matrix)\n        rgb_array = np.array(px)\n        rgb_array = rgb_array[:, :, :3]\n        return rgb_array\n\n    def get_rex_motor_angles(self):\n        """"""Get the rex\'s motor angles.\n\n        Returns:\n          A numpy array of motor angles.\n        """"""\n        return np.array(self._observation[MOTOR_ANGLE_OBSERVATION_INDEX:MOTOR_ANGLE_OBSERVATION_INDEX + NUM_MOTORS])\n\n    def get_rex_motor_velocities(self):\n        """"""Get the rex\'s motor velocities.\n\n        Returns:\n          A numpy array of motor velocities.\n        """"""\n        return np.array(\n            self._observation[MOTOR_VELOCITY_OBSERVATION_INDEX:MOTOR_VELOCITY_OBSERVATION_INDEX + NUM_MOTORS])\n\n    def get_rex_motor_torques(self):\n        """"""Get the rex\'s motor torques.\n\n        Returns:\n          A numpy array of motor torques.\n        """"""\n        return np.array(\n            self._observation[MOTOR_TORQUE_OBSERVATION_INDEX:MOTOR_TORQUE_OBSERVATION_INDEX + NUM_MOTORS])\n\n    def get_rex_base_orientation(self):\n        """"""Get the rex\'s base orientation, represented by a quaternion.\n\n        Returns:\n          A numpy array of rex\'s orientation.\n        """"""\n        return np.array(self._observation[BASE_ORIENTATION_OBSERVATION_INDEX:])\n\n    def is_fallen(self):\n        """"""Decide whether Rex has fallen.\n\n        If the up directions between the base and the world is larger (the dot\n        product is smaller than 0.85) or the base is very low on the ground\n        (the height is smaller than 0.13 meter), rex is considered fallen.\n\n        Returns:\n          Boolean value that indicates whether rex has fallen.\n        """"""\n        orientation = self.rex.GetBaseOrientation()\n        rot_mat = self._pybullet_client.getMatrixFromQuaternion(orientation)\n        local_up = rot_mat[6:]\n        return np.dot(np.asarray([0, 0, 1]), np.asarray(local_up)) < 0.85\n\n    def _termination(self):\n        if self.is_fallen():\n            print(""IS FALLING DOWN!"")\n        o = self.rex.GetBaseOrientation()\n        if o[1] < -0.13:\n            print(""IS ROTATING!"")\n        return self.is_fallen() or o[1] < -0.13\n\n    def _reward(self):\n        current_base_position = self.rex.GetBasePosition()\n        # forward direction\n        forward_reward = -current_base_position[0] + self._last_base_position[0]\n        # Cap the forward reward if a cap is set.\n        forward_reward = min(forward_reward, self._forward_reward_cap)\n        # Penalty for sideways translation.\n        drift_reward = -abs(current_base_position[1] - self._last_base_position[1])\n        # Penalty for sideways rotation of the body.\n        orientation = self.rex.GetBaseOrientation()\n        rot_matrix = pybullet.getMatrixFromQuaternion(orientation)\n        local_up_vec = rot_matrix[6:]\n        shake_reward = -abs(np.dot(np.asarray([1, 1, 0]), np.asarray(local_up_vec)))\n        energy_reward = -np.abs(\n            np.dot(self.rex.GetMotorTorques(),\n                   self.rex.GetMotorVelocities())) * self._time_step\n        objectives = [forward_reward, energy_reward, drift_reward, shake_reward]\n        weighted_objectives = [o * w for o, w in zip(objectives, self._objective_weights)]\n        reward = sum(weighted_objectives)\n        self._objectives.append(objectives)\n        return reward\n\n    def get_objectives(self):\n        return self._objectives\n\n    @property\n    def objective_weights(self):\n        """"""Accessor for the weights for all the objectives.\n\n        Returns:\n          List of floating points that corresponds to weights for the objectives in\n          the order that objectives are stored.\n        """"""\n        return self._objective_weights\n\n    def _get_observation(self):\n        """"""Get observation of this environment, including noise and latency.\n\n        rex class maintains a history of true observations. Based on the\n        latency, this function will find the observation at the right time,\n        interpolate if necessary. Then Gaussian noise is added to this observation\n        based on self.observation_noise_stdev.\n\n        Returns:\n          The noisy observation with latency.\n        """"""\n\n        observation = []\n        observation.extend(self.rex.GetMotorAngles().tolist())\n        observation.extend(self.rex.GetMotorVelocities().tolist())\n        observation.extend(self.rex.GetMotorTorques().tolist())\n        observation.extend(list(self.rex.GetBaseOrientation()))\n        self._observation = observation\n        return self._observation\n\n    def _get_true_observation(self):\n        """"""Get the observations of this environment.\n\n        It includes the angles, velocities, torques and the orientation of the base.\n\n        Returns:\n          The observation list. observation[0:8] are motor angles. observation[8:16]\n          are motor velocities, observation[16:24] are motor torques.\n          observation[24:28] is the orientation of the base, in quaternion form.\n        """"""\n        observation = []\n        observation.extend(self.rex.GetTrueMotorAngles().tolist())\n        observation.extend(self.rex.GetTrueMotorVelocities().tolist())\n        observation.extend(self.rex.GetTrueMotorTorques().tolist())\n        observation.extend(list(self.rex.GetTrueBaseOrientation()))\n\n        self._true_observation = observation\n        return self._true_observation\n\n    def _get_observation_upper_bound(self):\n        """"""Get the upper bound of the observation.\n\n        Returns:\n          The upper bound of an observation. See GetObservation() for the details\n            of each element of an observation.\n        """"""\n        upper_bound = np.zeros(self._get_observation_dimension())\n        num_motors = self.rex.num_motors\n        upper_bound[0:num_motors] = math.pi  # Joint angle.\n        upper_bound[num_motors:2 * num_motors] = (motor.MOTOR_SPEED_LIMIT)  # Joint velocity.\n        upper_bound[2 * num_motors:3 * num_motors] = (motor.OBSERVED_TORQUE_LIMIT)  # Joint torque.\n        upper_bound[3 * num_motors:] = 1.0  # Quaternion of base orientation.\n        return upper_bound\n\n    def _get_observation_lower_bound(self):\n        """"""Get the lower bound of the observation.""""""\n        return -self._get_observation_upper_bound()\n\n    def _get_observation_dimension(self):\n        """"""Get the length of the observation list.\n\n        Returns:\n          The length of the observation list.\n        """"""\n        return len(self._get_observation())\n\n    def set_time_step(self, control_step, simulation_step=0.001):\n        """"""Sets the time step of the environment.\n\n        Args:\n          control_step: The time period (in seconds) between two adjacent control\n            actions are applied.\n          simulation_step: The simulation time step in PyBullet. By default, the\n            simulation step is 0.001s, which is a good trade-off between simulation\n            speed and accuracy.\n        Raises:\n          ValueError: If the control step is smaller than the simulation step.\n        """"""\n        if control_step < simulation_step:\n            raise ValueError(""Control step should be larger than or equal to simulation step."")\n        self.control_time_step = control_step\n        self._time_step = simulation_step\n        self._action_repeat = int(round(control_step / simulation_step))\n        self._num_bullet_solver_iterations = (NUM_SIMULATION_ITERATION_STEPS / self._action_repeat)\n        self._pybullet_client.setPhysicsEngineParameter(\n            numSolverIterations=self._num_bullet_solver_iterations)\n        self._pybullet_client.setTimeStep(self._time_step)\n        self.rex.SetTimeSteps(action_repeat=self._action_repeat, simulation_step=self._time_step)\n\n    @property\n    def pybullet_client(self):\n        return self._pybullet_client\n\n    @property\n    def ground_id(self):\n        return self._ground_id\n\n    @ground_id.setter\n    def ground_id(self, new_ground_id):\n        self._ground_id = new_ground_id\n\n    @property\n    def env_step_counter(self):\n        return self._env_step_counter\n'"
rex_gym/model/__init__.py,0,b''
rex_gym/model/motor.py,0,"b'""""""This file implements an accurate motor model.""""""\n\nimport numpy as np\n\n# TODO: set params to match mg996r servo\nVOLTAGE_CLIPPING = 50\n# TODO: Clamp the pwm signal instead of the OBSERVED_TORQUE_LIMIT.\nOBSERVED_TORQUE_LIMIT = 5.7\nMOTOR_VOLTAGE = 16.0\nMOTOR_RESISTANCE = 0.186\nMOTOR_TORQUE_CONSTANT = 0.0954\nMOTOR_VISCOUS_DAMPING = 0\nMOTOR_SPEED_LIMIT = MOTOR_VOLTAGE / (MOTOR_VISCOUS_DAMPING + MOTOR_TORQUE_CONSTANT)\nNUM_MOTORS = 12\n\n\nclass MotorModel(object):\n    """"""The accurate motor model, which is based on the physics of DC motors.\n\n        The motor model support two types of control: position control and torque\n        control. In position control mode, a desired motor angle is specified, and a\n        torque is computed based on the internal motor model. When the torque control\n        is specified, a pwm signal in the range of [-1.0, 1.0] is converted to the\n        torque.\n\n        The internal motor model takes the following factors into consideration:\n        pd gains, viscous friction, back-EMF voltage and current-torque profile.\n    """"""\n\n    def __init__(self, torque_control_enabled=False, kp=1.2, kd=0):\n        self._torque_control_enabled = torque_control_enabled\n        self._kp = kp\n        self._kd = kd\n        self._resistance = MOTOR_RESISTANCE\n        self._voltage = MOTOR_VOLTAGE\n        self._torque_constant = MOTOR_TORQUE_CONSTANT\n        self._viscous_damping = MOTOR_VISCOUS_DAMPING\n        self._current_table = [0, 10, 20, 30, 40, 50, 60]\n        self._torque_table = [0, 1, 1.9, 2.45, 3.0, 3.25, 3.5]\n        self._strength_ratios = [1.0] * NUM_MOTORS\n\n    def set_strength_ratios(self, ratios):\n        """"""Set the strength of each motors relative to the default value.\n\n        Args:\n          ratios: The relative strength of motor output. A numpy array ranging from\n            0.0 to 1.0.\n        """"""\n        self._strength_ratios = np.array(ratios)\n\n    def set_motor_gains(self, kp, kd):\n        """"""Set the gains of all motors.\n\n        These gains are PD gains for motor positional control. kp is the\n        proportional gain and kd is the derivative gain.\n\n        Args:\n          kp: proportional gain of the motors.\n          kd: derivative gain of the motors.\n        """"""\n        self._kp = kp\n        self._kd = kd\n\n    def set_voltage(self, voltage):\n        self._voltage = voltage\n\n    def get_voltage(self):\n        return self._voltage\n\n    def set_viscous_damping(self, viscous_damping):\n        self._viscous_damping = viscous_damping\n\n    def get_viscous_dampling(self):\n        return self._viscous_damping\n\n    def convert_to_torque(self,\n                          motor_commands,\n                          motor_angle,\n                          motor_velocity,\n                          true_motor_velocity,\n                          kp=None,\n                          kd=None):\n        """"""Convert the commands (position control or torque control) to torque.\n\n        Args:\n          motor_commands: The desired motor angle if the motor is in position\n            control mode. The pwm signal if the motor is in torque control mode.\n          motor_angle: The motor angle observed at the current time step. It is\n            actually the true motor angle observed a few milliseconds ago (pd\n            latency).\n          motor_velocity: The motor velocity observed at the current time step, it\n            is actually the true motor velocity a few milliseconds ago (pd latency).\n          true_motor_velocity: The true motor velocity. The true velocity is used\n            to compute back EMF voltage and viscous damping.\n          kp: Proportional gains for the motors\' PD controllers. If not provided, it\n            uses the default kp of Rex for all the motors.\n          kd: Derivative gains for the motors\' PD controllers. If not provided, it\n            uses the default kp of Rex for all the motors.\n\n        Returns:\n          actual_torque: The torque that needs to be applied to the motor.\n          observed_torque: The torque observed by the sensor.\n        """"""\n        if self._torque_control_enabled:\n            pwm = motor_commands\n        else:\n            if kp is None:\n                kp = np.full(NUM_MOTORS, self._kp)\n            if kd is None:\n                kd = np.full(NUM_MOTORS, self._kd)\n            pwm = -1 * kp * (motor_angle - motor_commands) - kd * motor_velocity\n\n        pwm = np.clip(pwm, -1.0, 1.0)\n        return self._convert_to_torque_from_pwm(pwm, true_motor_velocity)\n\n    def _convert_to_torque_from_pwm(self, pwm, true_motor_velocity):\n        """"""Convert the pwm signal to torque.\n\n        Args:\n          pwm: The pulse width modulation.\n          true_motor_velocity: The true motor velocity at the current moment. It is\n            used to compute the back EMF voltage and the viscous damping.\n        Returns:\n          actual_torque: The torque that needs to be applied to the motor.\n          observed_torque: The torque observed by the sensor.\n        """"""\n        observed_torque = np.clip(\n            self._torque_constant * (np.asarray(pwm) * self._voltage / self._resistance),\n            -OBSERVED_TORQUE_LIMIT, OBSERVED_TORQUE_LIMIT)\n\n        # Net voltage is clipped at 50V by diodes on the motor controller.\n        voltage_net = np.clip(\n            np.asarray(pwm) * self._voltage -\n            (self._torque_constant + self._viscous_damping) * np.asarray(true_motor_velocity),\n            -VOLTAGE_CLIPPING, VOLTAGE_CLIPPING)\n        current = voltage_net / self._resistance\n        current_sign = np.sign(current)\n        current_magnitude = np.absolute(current)\n        # Saturate torque based on empirical current relation.\n        actual_torque = np.interp(current_magnitude, self._current_table, self._torque_table)\n        actual_torque = np.multiply(current_sign, actual_torque)\n        actual_torque = np.multiply(self._strength_ratios, actual_torque)\n        return actual_torque, observed_torque\n'"
rex_gym/model/rex.py,0,"b'""""""This file models a rex using pybullet.""""""\n\nimport collections\nimport copy\nimport math\nimport re\nimport numpy as np\nfrom . import motor\nfrom ..util import pybullet_data\n\nINIT_POSITION = [0, 0, 0.21]\nINIT_RACK_POSITION = [0, 0, 1]\nINIT_ORIENTATION = [0, 0, 0, 1]\nOVERHEAT_SHUTDOWN_TORQUE = 2.45\nOVERHEAT_SHUTDOWN_TIME = 1.0\n\nLEG_POSITION = [""front_left"", ""front_right"", ""rear_left"", ""rear_right""]\nMOTOR_NAMES = [\n    ""motor_front_left_shoulder"", ""motor_front_left_leg"", ""foot_motor_front_left"",\n    ""motor_front_right_shoulder"", ""motor_front_right_leg"", ""foot_motor_front_right"",\n    ""motor_rear_left_shoulder"", ""motor_rear_left_leg"", ""foot_motor_rear_left"",\n    ""motor_rear_right_shoulder"", ""motor_rear_right_leg"", ""foot_motor_rear_right""\n]\n_CHASSIS_NAME_PATTERN = re.compile(r""chassis\\D*"")\n_MOTOR_NAME_PATTERN = re.compile(r""motor\\D*"")\n_FOOT_NAME_PATTERN = re.compile(r""foot_motor\\D*"")\nSENSOR_NOISE_STDDEV = (0.0, 0.0, 0.0, 0.0, 0.0)\nTWO_PI = 2 * math.pi\n\n\ndef MapToMinusPiToPi(angles):\n    """"""Maps a list of angles to [-pi, pi].\n\n      Args:\n        angles: A list of angles in rad.\n      Returns:\n        A list of angle mapped to [-pi, pi].\n    """"""\n    mapped_angles = copy.deepcopy(angles)\n    for i in range(len(angles)):\n        mapped_angles[i] = math.fmod(angles[i], TWO_PI)\n        if mapped_angles[i] >= math.pi:\n            mapped_angles[i] -= TWO_PI\n        elif mapped_angles[i] < -math.pi:\n            mapped_angles[i] += TWO_PI\n    return mapped_angles\n\n\nclass Rex(object):\n    """"""The Rex class that simulates a quadruped robot.""""""\n    INIT_POSES = {\n        \'stand_low\': np.array([\n            0.1, -0.82, 1.35,\n            -0.1, -0.82, 1.35,\n            0.1, -0.87, 1.35,\n            -0.1, -0.87, 1.35\n        ]),\n        \'stand_high\': np.array([\n            0, -0.658319, 1.0472,\n            0, -0.658319, 1.0472,\n            0, -0.658319, 1.0472,\n            0, -0.658319, 1.0472\n        ]),\n        \'rest_position\': np.array([\n            -0.4, -1.5, 6,\n            0.4, -1.5, 6,\n            -0.4, -1.5, 6,\n            0.4, -1.5, 6\n        ])\n    }\n\n    def __init__(self,\n                 pybullet_client,\n                 urdf_root=pybullet_data.getDataPath(),\n                 time_step=0.01,\n                 action_repeat=1,\n                 self_collision_enabled=False,\n                 motor_velocity_limit=np.inf,\n                 pd_control_enabled=False,\n                 accurate_motor_model_enabled=False,\n                 remove_default_joint_damping=False,\n                 motor_kp=1.0,\n                 motor_kd=0.02,\n                 pd_latency=0.0,\n                 control_latency=0.0,\n                 observation_noise_stdev=SENSOR_NOISE_STDDEV,\n                 torque_control_enabled=False,\n                 motor_overheat_protection=False,\n                 on_rack=False,\n                 pose_id=\'stand_low\'):\n        """"""Constructs a Rex and reset it to the initial states.\n\n        Args:\n          pybullet_client: The instance of BulletClient to manage different\n            simulations.\n          urdf_root: The path to the urdf folder.\n          time_step: The time step of the simulation.\n          action_repeat: The number of ApplyAction() for each control step.\n          self_collision_enabled: Whether to enable self collision.\n          motor_velocity_limit: The upper limit of the motor velocity.\n          pd_control_enabled: Whether to use PD control for the motors.\n          accurate_motor_model_enabled: Whether to use the accurate DC motor model.\n          remove_default_joint_damping: Whether to remove the default joint damping.\n          motor_kp: proportional gain for the accurate motor model.\n          motor_kd: derivative gain for the accurate motor model.\n          pd_latency: The latency of the observations (in seconds) used to calculate\n            PD control. On the real hardware, it is the latency between the\n            microcontroller and the motor controller.\n          control_latency: The latency of the observations (in second) used to\n            calculate action. On the real hardware, it is the latency from the motor\n            controller, the microcontroller to the host (Nvidia TX2).\n          observation_noise_stdev: The standard deviation of a Gaussian noise model\n            for the sensor. It should be an array for separate sensors in the\n            following order [motor_angle, motor_velocity, motor_torque,\n            base_roll_pitch_yaw, base_angular_velocity]\n          torque_control_enabled: Whether to use the torque control, if set to\n            False, pose control will be used.\n          motor_overheat_protection: Whether to shutdown the motor that has exerted\n            large torque (OVERHEAT_SHUTDOWN_TORQUE) for an extended amount of time\n            (OVERHEAT_SHUTDOWN_TIME). See ApplyAction() in rex.py for more\n            details.\n          on_rack: Whether to place the Rex on rack. This is only used to debug\n            the walking gait. In this mode, the Rex\'s base is hanged midair so\n            that its walking gait is clearer to visualize.\n        """"""\n        self.num_motors = 12\n        self.num_legs = 4\n        self._pybullet_client = pybullet_client\n        self._action_repeat = action_repeat\n        self._urdf_root = urdf_root\n        self._self_collision_enabled = self_collision_enabled\n        self._motor_velocity_limit = motor_velocity_limit\n        self._pd_control_enabled = pd_control_enabled\n        self._motor_direction = [1 for _ in range(12)]\n        self._observed_motor_torques = np.zeros(self.num_motors)\n        self._applied_motor_torques = np.zeros(self.num_motors)\n        self._max_force = 3.5\n        self._pd_latency = pd_latency\n        self._control_latency = control_latency\n        self._observation_noise_stdev = observation_noise_stdev\n        self._accurate_motor_model_enabled = accurate_motor_model_enabled\n        self._remove_default_joint_damping = remove_default_joint_damping\n        self._observation_history = collections.deque(maxlen=100)\n        self._control_observation = []\n        self._chassis_link_ids = [-1]\n        self._leg_link_ids = []\n        self._motor_link_ids = []\n        self._foot_link_ids = []\n        self._torque_control_enabled = torque_control_enabled\n        self._motor_overheat_protection = motor_overheat_protection\n        self._on_rack = on_rack\n        self._pose_id = pose_id\n        # @TODO fix MotorModel\n        if self._accurate_motor_model_enabled:\n            self._kp = motor_kp\n            self._kd = motor_kd\n            self._motor_model = motor.MotorModel(torque_control_enabled=self._torque_control_enabled,\n                                                 kp=self._kp,\n                                                 kd=self._kd)\n        elif self._pd_control_enabled:\n            self._kp = 8\n            self._kd = 0.3\n        else:\n            self._kp = 1\n            self._kd = 1\n        self.time_step = time_step\n        self._step_counter = 0\n        # reset_time=-1.0 means skipping the reset motion.\n        # See Reset for more details.\n        self.Reset(reset_time=-1)\n        self.init_on_rack_position = INIT_RACK_POSITION\n        self.init_position = INIT_POSITION\n        self.initial_pose = self.INIT_POSES[pose_id]\n\n    def GetTimeSinceReset(self):\n        return self._step_counter * self.time_step\n\n    def Step(self, action):\n        for _ in range(self._action_repeat):\n            self.ApplyAction(action)\n            self._pybullet_client.stepSimulation()\n            self.ReceiveObservation()\n            self._step_counter += 1\n\n    def Terminate(self):\n        pass\n\n    def _RecordMassInfoFromURDF(self):\n        self._base_mass_urdf = []\n        for chassis_id in self._chassis_link_ids:\n            self._base_mass_urdf.append(\n                self._pybullet_client.getDynamicsInfo(self.quadruped, chassis_id)[0])\n        self._leg_masses_urdf = []\n        for leg_id in self._leg_link_ids:\n            self._leg_masses_urdf.append(\n                self._pybullet_client.getDynamicsInfo(self.quadruped, leg_id)[0])\n        for motor_id in self._motor_link_ids:\n            self._leg_masses_urdf.append(\n                self._pybullet_client.getDynamicsInfo(self.quadruped, motor_id)[0])\n\n    def _RecordInertiaInfoFromURDF(self):\n        """"""Record the inertia of each body from URDF file.""""""\n        self._link_urdf = []\n        num_bodies = self._pybullet_client.getNumJoints(self.quadruped)\n        for body_id in range(-1, num_bodies):  # -1 is for the base link.\n            inertia = self._pybullet_client.getDynamicsInfo(self.quadruped, body_id)[2]\n            self._link_urdf.append(inertia)\n        # We need to use id+1 to index self._link_urdf because it has the base\n        # (index = -1) at the first element.\n        self._base_inertia_urdf = [\n            self._link_urdf[chassis_id + 1] for chassis_id in self._chassis_link_ids\n        ]\n        self._leg_inertia_urdf = [self._link_urdf[leg_id + 1] for leg_id in self._leg_link_ids]\n        self._leg_inertia_urdf.extend(\n            [self._link_urdf[motor_id + 1] for motor_id in self._motor_link_ids])\n\n    def _BuildJointNameToIdDict(self):\n        num_joints = self._pybullet_client.getNumJoints(self.quadruped)\n        self._joint_name_to_id = {}\n        for i in range(num_joints):\n            joint_info = self._pybullet_client.getJointInfo(self.quadruped, i)\n            self._joint_name_to_id[joint_info[1].decode(""UTF-8"")] = joint_info[0]\n\n    def _BuildUrdfIds(self):\n        """"""Build the link Ids from its name in the URDF file.""""""\n        num_joints = self._pybullet_client.getNumJoints(self.quadruped)\n        self._chassis_link_ids = [-1]\n        # the self._leg_link_ids include both the upper and lower links of the leg.\n        self._leg_link_ids = []\n        self._motor_link_ids = []\n        self._foot_link_ids = []\n        for i in range(num_joints):\n            joint_info = self._pybullet_client.getJointInfo(self.quadruped, i)\n            joint_name = joint_info[1].decode(""UTF-8"")\n            joint_id = self._joint_name_to_id[joint_name]\n            if _CHASSIS_NAME_PATTERN.match(joint_name):\n                self._chassis_link_ids.append(joint_id)\n            elif _MOTOR_NAME_PATTERN.match(joint_name):\n                self._motor_link_ids.append(joint_id)\n            elif _FOOT_NAME_PATTERN.match(joint_name):\n                self._foot_link_ids.append(joint_id)\n            else:\n                self._leg_link_ids.append(joint_id)\n        self._leg_link_ids.extend(self._foot_link_ids)\n        self._chassis_link_ids.sort()\n        self._motor_link_ids.sort()\n        self._foot_link_ids.sort()\n        self._leg_link_ids.sort()\n\n    def _RemoveDefaultJointDamping(self):\n        num_joints = self._pybullet_client.getNumJoints(self.quadruped)\n        for i in range(num_joints):\n            joint_info = self._pybullet_client.getJointInfo(self.quadruped, i)\n            self._pybullet_client.changeDynamics(joint_info[0], -1, linearDamping=0, angularDamping=0)\n\n    def _BuildMotorIdList(self):\n        self._motor_id_list = [self._joint_name_to_id[motor_name] for motor_name in MOTOR_NAMES]\n\n    @staticmethod\n    def IsObservationValid():\n        """"""Whether the observation is valid for the current time step.\n\n        In simulation, observations are always valid. In real hardware, it may not\n        be valid from time to time when communication error happens.\n\n        Returns:\n          Whether the observation is valid for the current time step.\n        """"""\n        return True\n\n    def Reset(self, reload_urdf=True, default_motor_angles=None, reset_time=3.0):\n        """"""Reset the Rex to its initial states.\n\n        Args:\n          reload_urdf: Whether to reload the urdf file. If not, Reset() just place\n            the Rex back to its starting position.\n          default_motor_angles: The default motor angles. If it is None, Rex\n            will hold a default pose for 100 steps. In\n            torque control mode, the phase of holding the default pose is skipped.\n          reset_time: The duration (in seconds) to hold the default motor angles. If\n            reset_time <= 0 or in torque control mode, the phase of holding the\n            default pose is skipped.\n        """"""\n        print(""reset"")\n        if self._on_rack:\n            init_position = INIT_RACK_POSITION\n        else:\n            init_position = INIT_POSITION\n\n        if reload_urdf:\n            if self._self_collision_enabled:\n                self.quadruped = self._pybullet_client.loadURDF(\n                    pybullet_data.getDataPath() + ""/assets/urdf/rex.urdf"",\n                    init_position,\n                    INIT_ORIENTATION,\n                    useFixedBase=self._on_rack,\n                    flags=self._pybullet_client.URDF_USE_SELF_COLLISION)\n            else:\n                self.quadruped = self._pybullet_client.loadURDF(pybullet_data.getDataPath() + ""/assets/urdf/rex.urdf"",\n                                                                init_position,\n                                                                INIT_ORIENTATION,\n                                                                useFixedBase=self._on_rack)\n            self._BuildJointNameToIdDict()\n            self._BuildUrdfIds()\n            if self._remove_default_joint_damping:\n                self._RemoveDefaultJointDamping()\n            self._BuildMotorIdList()\n            self._RecordMassInfoFromURDF()\n            self._RecordInertiaInfoFromURDF()\n            self.ResetPose()\n        else:\n            self._pybullet_client.resetBasePositionAndOrientation(self.quadruped, init_position,\n                                                                  INIT_ORIENTATION)\n            self._pybullet_client.resetBaseVelocity(self.quadruped, [0, 0, 0], [0, 0, 0])\n            self.ResetPose()\n        self._overheat_counter = np.zeros(self.num_motors)\n        self._motor_enabled_list = [True] * self.num_motors\n        self._step_counter = 0\n\n        # Perform reset motion within reset_duration if in position control mode.\n        # Nothing is performed if in torque control mode for now.\n        self._observation_history.clear()\n        if reset_time > 0.0 and default_motor_angles is not None:\n            self.ReceiveObservation()\n            for _ in range(100):\n                self.ApplyAction(self.initial_pose)\n                self._pybullet_client.stepSimulation()\n                self.ReceiveObservation()\n            num_steps_to_reset = int(reset_time / self.time_step)\n            for _ in range(num_steps_to_reset):\n                self.ApplyAction(default_motor_angles)\n                self._pybullet_client.stepSimulation()\n                self.ReceiveObservation()\n        self.ReceiveObservation()\n\n    def _SetMotorTorqueById(self, motor_id, torque):\n        self._pybullet_client.setJointMotorControl2(bodyIndex=self.quadruped,\n                                                    jointIndex=motor_id,\n                                                    controlMode=self._pybullet_client.TORQUE_CONTROL,\n                                                    force=torque)\n\n    def _SetDesiredMotorAngleById(self, motor_id, desired_angle):\n        self._pybullet_client.setJointMotorControl2(bodyIndex=self.quadruped,\n                                                    jointIndex=motor_id,\n                                                    controlMode=self._pybullet_client.POSITION_CONTROL,\n                                                    targetPosition=desired_angle,\n                                                    positionGain=self._kp,\n                                                    velocityGain=self._kd,\n                                                    force=self._max_force)\n\n    def _SetDesiredMotorAngleByName(self, motor_name, desired_angle):\n        self._SetDesiredMotorAngleById(self._joint_name_to_id[motor_name], desired_angle)\n\n    def ResetPose(self):\n        """"""Reset the pose of the Rex.\n\n        Args:\n          add_constraint: Whether to add a constraint at the joints of two feet.\n        """"""\n        for i in range(self.num_legs):\n            self._ResetPoseForLeg(i)\n\n    def _ResetPoseForLeg(self, leg_id):\n        """"""Reset the initial pose for the leg.\n\n        Args:\n          leg_id: It should be 0, 1, 2, or 3, which represents the leg at\n            front_left, back_left, front_right and back_right.\n          add_constraint: Whether to add a constraint at the joints of two feet.\n        """"""\n        knee_friction_force = 0\n        leg_position = LEG_POSITION[leg_id]\n        self._pybullet_client.resetJointState(self.quadruped,\n                                              self._joint_name_to_id[""motor_"" + leg_position +\n                                                                     ""_shoulder""],\n                                              self.INIT_POSES[self._pose_id][3 * leg_id],\n                                              targetVelocity=0)\n\n        self._pybullet_client.resetJointState(self.quadruped,\n                                              self._joint_name_to_id[""motor_"" + leg_position +\n                                                                     ""_leg""],\n                                              self.INIT_POSES[self._pose_id][3 * leg_id + 1],\n                                              targetVelocity=0)\n        self._pybullet_client.resetJointState(self.quadruped,\n                                              self._joint_name_to_id[""foot_motor_"" + leg_position],\n                                              self.INIT_POSES[self._pose_id][3 * leg_id + 2],\n                                              targetVelocity=0)\n\n        if self._accurate_motor_model_enabled or self._pd_control_enabled:\n            # Disable the default motor in pybullet.\n            self._pybullet_client.setJointMotorControl2(\n                bodyIndex=self.quadruped,\n                jointIndex=(self._joint_name_to_id[""motor_"" + leg_position + ""_shoulder""]),\n                controlMode=self._pybullet_client.VELOCITY_CONTROL,\n                targetVelocity=0,\n                force=knee_friction_force)\n            self._pybullet_client.setJointMotorControl2(\n                bodyIndex=self.quadruped,\n                jointIndex=(self._joint_name_to_id[""motor_"" + leg_position + ""_leg""]),\n                controlMode=self._pybullet_client.VELOCITY_CONTROL,\n                targetVelocity=0,\n                force=knee_friction_force)\n            self._pybullet_client.setJointMotorControl2(\n                bodyIndex=self.quadruped,\n                jointIndex=(self._joint_name_to_id[""foot_motor_"" + leg_position]),\n                controlMode=self._pybullet_client.VELOCITY_CONTROL,\n                targetVelocity=0,\n                force=knee_friction_force)\n\n    def GetBasePosition(self):\n        """"""Get the position of Rex\'s base.\n\n        Returns:\n          The position of Rex\'s base.\n        """"""\n        position, _ = (self._pybullet_client.getBasePositionAndOrientation(self.quadruped))\n        return position\n\n    def GetTrueBaseRollPitchYaw(self):\n        """"""Get Rex\'s base orientation in euler angle in the world frame.\n\n        Returns:\n          A tuple (roll, pitch, yaw) of the base in world frame.\n        """"""\n        orientation = self.GetTrueBaseOrientation()\n        roll_pitch_yaw = self._pybullet_client.getEulerFromQuaternion(orientation)\n        return np.asarray(roll_pitch_yaw)\n\n    def GetBaseRollPitchYaw(self):\n        """"""Get Rex\'s base orientation in euler angle in the world frame.\n\n        This function mimicks the noisy sensor reading and adds latency.\n        Returns:\n          A tuple (roll, pitch, yaw) of the base in world frame polluted by noise\n          and latency.\n        """"""\n        delayed_orientation = np.array(\n            self._control_observation[3 * self.num_motors:3 * self.num_motors + 4])\n        delayed_roll_pitch_yaw = self._pybullet_client.getEulerFromQuaternion(delayed_orientation)\n        roll_pitch_yaw = self._AddSensorNoise(np.array(delayed_roll_pitch_yaw),\n                                              self._observation_noise_stdev[3])\n        return roll_pitch_yaw\n\n    def GetTrueMotorAngles(self):\n        """"""Gets the eight motor angles at the current moment, mapped to [-pi, pi].\n\n        Returns:\n          Motor angles, mapped to [-pi, pi].\n        """"""\n        motor_angles = [\n            self._pybullet_client.getJointState(self.quadruped, motor_id)[0]\n            for motor_id in self._motor_id_list\n        ]\n        motor_angles = np.multiply(motor_angles, self._motor_direction)\n        return motor_angles\n\n    def GetMotorAngles(self):\n        """"""Gets the eight motor angles.\n\n        This function mimicks the noisy sensor reading and adds latency. The motor\n        angles that are delayed, noise polluted, and mapped to [-pi, pi].\n\n        Returns:\n          Motor angles polluted by noise and latency, mapped to [-pi, pi].\n        """"""\n        motor_angles = self._AddSensorNoise(np.array(self._control_observation[0:self.num_motors]),\n                                            self._observation_noise_stdev[0])\n        return MapToMinusPiToPi(motor_angles)\n\n    def GetTrueMotorVelocities(self):\n        """"""Get the velocity of all eight motors.\n\n        Returns:\n          Velocities of all eight motors.\n        """"""\n        motor_velocities = [\n            self._pybullet_client.getJointState(self.quadruped, motor_id)[1]\n            for motor_id in self._motor_id_list\n        ]\n        motor_velocities = np.multiply(motor_velocities, self._motor_direction)\n        return motor_velocities\n\n    def GetMotorVelocities(self):\n        """"""Get the velocity of all eight motors.\n\n        This function mimicks the noisy sensor reading and adds latency.\n        Returns:\n          Velocities of all eight motors polluted by noise and latency.\n        """"""\n        return self._AddSensorNoise(\n            np.array(self._control_observation[self.num_motors:2 * self.num_motors]),\n            self._observation_noise_stdev[1])\n\n    def GetTrueMotorTorques(self):\n        """"""Get the amount of torque the motors are exerting.\n\n        Returns:\n          Motor torques of all eight motors.\n        """"""\n        if self._accurate_motor_model_enabled or self._pd_control_enabled:\n            return self._observed_motor_torques\n        else:\n            motor_torques = [\n                self._pybullet_client.getJointState(self.quadruped, motor_id)[3]\n                for motor_id in self._motor_id_list\n            ]\n            motor_torques = np.multiply(motor_torques, self._motor_direction)\n        return motor_torques\n\n    def GetMotorTorques(self):\n        """"""Get the amount of torque the motors are exerting.\n\n        This function mimicks the noisy sensor reading and adds latency.\n        Returns:\n          Motor torques of all eight motors polluted by noise and latency.\n        """"""\n        return self._AddSensorNoise(\n            np.array(self._control_observation[2 * self.num_motors:3 * self.num_motors]),\n            self._observation_noise_stdev[2])\n\n    def GetTrueBaseOrientation(self):\n        """"""Get the orientation of Rex\'s base, represented as quaternion.\n\n        Returns:\n          The orientation of Rex\'s base.\n        """"""\n        _, orientation = (self._pybullet_client.getBasePositionAndOrientation(self.quadruped))\n        return orientation\n\n    def GetBaseOrientation(self):\n        """"""Get the orientation of Rex\'s base, represented as quaternion.\n\n        This function mimicks the noisy sensor reading and adds latency.\n        Returns:\n          The orientation of Rex\'s base polluted by noise and latency.\n        """"""\n        return self._pybullet_client.getQuaternionFromEuler(self.GetBaseRollPitchYaw())\n\n    def GetTrueBaseRollPitchYawRate(self):\n        """"""Get the rate of orientation change of the Rex\'s base in euler angle.\n\n        Returns:\n          rate of (roll, pitch, yaw) change of the Rex\'s base.\n        """"""\n        vel = self._pybullet_client.getBaseVelocity(self.quadruped)\n        return np.asarray([vel[1][0], vel[1][1], vel[1][2]])\n\n    def GetBaseRollPitchYawRate(self):\n        """"""Get the rate of orientation change of the Rex\'s base in euler angle.\n\n        This function mimicks the noisy sensor reading and adds latency.\n        Returns:\n          rate of (roll, pitch, yaw) change of the Rex\'s base polluted by noise\n          and latency.\n        """"""\n        return self._AddSensorNoise(\n            np.array(self._control_observation[3 * self.num_motors + 4:3 * self.num_motors + 7]),\n            self._observation_noise_stdev[4])\n\n    def GetActionDimension(self):\n        """"""Get the length of the action list.\n\n        Returns:\n          The length of the action list.\n        """"""\n        return self.num_motors\n\n    def ApplyAction(self, motor_commands, motor_kps=None, motor_kds=None):\n        """"""Set the desired motor angles to the motors of the Rex.\n\n        The desired motor angles are clipped based on the maximum allowed velocity.\n        If the pd_control_enabled is True, a torque is calculated according to\n        the difference between current and desired joint angle, as well as the joint\n        velocity. This torque is exerted to the motor. For more information about\n        PD control, please refer to: https://en.wikipedia.org/wiki/PID_controller.\n\n        Args:\n          motor_commands: The eight desired motor angles.\n          motor_kps: Proportional gains for the motor model. If not provided, it\n            uses the default kp of the Rex for all the motors.\n          motor_kds: Derivative gains for the motor model. If not provided, it\n            uses the default kd of the Rex for all the motors.\n        """"""\n        if self._motor_velocity_limit < np.inf:\n            current_motor_angle = self.GetTrueMotorAngles()\n            motor_commands_max = (current_motor_angle + self.time_step * self._motor_velocity_limit)\n            motor_commands_min = (current_motor_angle - self.time_step * self._motor_velocity_limit)\n            motor_commands = np.clip(motor_commands, motor_commands_min, motor_commands_max)\n        # Set the kp and kd for all the motors if not provided as an argument.\n        if motor_kps is None:\n            motor_kps = np.full(12, self._kp)\n        if motor_kds is None:\n            motor_kds = np.full(12, self._kd)\n\n        if self._accurate_motor_model_enabled or self._pd_control_enabled:\n            q, qdot = self._GetPDObservation()\n            qdot_true = self.GetTrueMotorVelocities()\n            if self._accurate_motor_model_enabled:\n                actual_torque, observed_torque = self._motor_model.convert_to_torque(\n                    motor_commands, q, qdot, qdot_true, motor_kps, motor_kds)\n                if self._motor_overheat_protection:\n                    for i in range(self.num_motors):\n                        if abs(actual_torque[i]) > OVERHEAT_SHUTDOWN_TORQUE:\n                            self._overheat_counter[i] += 1\n                        else:\n                            self._overheat_counter[i] = 0\n                        if self._overheat_counter[i] > OVERHEAT_SHUTDOWN_TIME / self.time_step:\n                            self._motor_enabled_list[i] = False\n\n                # The torque is already in the observation space because we use\n                # GetMotorAngles and GetMotorVelocities.\n                self._observed_motor_torques = observed_torque\n\n                # Transform into the motor space when applying the torque.\n                self._applied_motor_torque = np.multiply(actual_torque, self._motor_direction)\n\n                for motor_id, motor_torque, motor_enabled in zip(self._motor_id_list,\n                                                                 self._applied_motor_torque,\n                                                                 self._motor_enabled_list):\n                    if motor_enabled:\n                        self._SetMotorTorqueById(motor_id, motor_torque)\n                    else:\n                        self._SetMotorTorqueById(motor_id, 0)\n            else:\n                torque_commands = -1 * motor_kps * (q - motor_commands) - motor_kds * qdot\n\n                # The torque is already in the observation space because we use\n                # GetMotorAngles and GetMotorVelocities.\n                self._observed_motor_torques = torque_commands\n\n                # Transform into the motor space when applying the torque.\n                self._applied_motor_torques = np.multiply(self._observed_motor_torques,\n                                                          self._motor_direction)\n\n                for motor_id, motor_torque in zip(self._motor_id_list, self._applied_motor_torques):\n                    self._SetMotorTorqueById(motor_id, motor_torque)\n        else:\n            motor_commands_with_direction = np.multiply(motor_commands, self._motor_direction)\n            for motor_id, motor_command_with_direction in zip(self._motor_id_list,\n                                                              motor_commands_with_direction):\n                self._SetDesiredMotorAngleById(motor_id, motor_command_with_direction)\n\n    def GetBaseMassesFromURDF(self):\n        """"""Get the mass of the base from the URDF file.""""""\n        return self._base_mass_urdf\n\n    def GetBaseInertiasFromURDF(self):\n        """"""Get the inertia of the base from the URDF file.""""""\n        return self._base_inertia_urdf\n\n    def GetLegMassesFromURDF(self):\n        """"""Get the mass of the legs from the URDF file.""""""\n        return self._leg_masses_urdf\n\n    def GetLegInertiasFromURDF(self):\n        """"""Get the inertia of the legs from the URDF file.""""""\n        return self._leg_inertia_urdf\n\n    def SetBaseMasses(self, base_mass):\n        """"""Set the mass of Rex\'s base.\n\n        Args:\n          base_mass: A list of masses of each body link in CHASIS_LINK_IDS. The\n            length of this list should be the same as the length of CHASIS_LINK_IDS.\n        Raises:\n          ValueError: It is raised when the length of base_mass is not the same as\n            the length of self._chassis_link_ids.\n        """"""\n        if len(base_mass) != len(self._chassis_link_ids):\n            raise ValueError(""The length of base_mass {} and self._chassis_link_ids {} are not ""\n                             ""the same."".format(len(base_mass), len(self._chassis_link_ids)))\n        for chassis_id, chassis_mass in zip(self._chassis_link_ids, base_mass):\n            self._pybullet_client.changeDynamics(self.quadruped, chassis_id, mass=chassis_mass)\n\n    def SetLegMasses(self, leg_masses):\n        """"""Set the mass of the legs.\n\n        Args:\n          leg_masses: The leg and motor masses for all the leg links and motors.\n\n        Raises:\n          ValueError: It is raised when the length of masses is not equal to number\n            of links + motors.\n        """"""\n        if len(leg_masses) != len(self._leg_link_ids) + len(self._motor_link_ids):\n            raise ValueError(""The number of values passed to SetLegMasses are ""\n                             ""different than number of leg links and motors."")\n        for leg_id, leg_mass in zip(self._leg_link_ids, leg_masses):\n            self._pybullet_client.changeDynamics(self.quadruped, leg_id, mass=leg_mass)\n        motor_masses = leg_masses[len(self._leg_link_ids):]\n        for link_id, motor_mass in zip(self._motor_link_ids, motor_masses):\n            self._pybullet_client.changeDynamics(self.quadruped, link_id, mass=motor_mass)\n\n    def SetBaseInertias(self, base_inertias):\n        """"""Set the inertias of Rex\'s base.\n\n        Args:\n          base_inertias: A list of inertias of each body link in CHASIS_LINK_IDS.\n            The length of this list should be the same as the length of\n            CHASIS_LINK_IDS.\n        Raises:\n          ValueError: It is raised when the length of base_inertias is not the same\n            as the length of self._chassis_link_ids and base_inertias contains\n            negative values.\n        """"""\n        if len(base_inertias) != len(self._chassis_link_ids):\n            raise ValueError(""The length of base_inertias {} and self._chassis_link_ids {} are ""\n                             ""not the same."".format(len(base_inertias), len(self._chassis_link_ids)))\n        for chassis_id, chassis_inertia in zip(self._chassis_link_ids, base_inertias):\n            for inertia_value in chassis_inertia:\n                if (np.asarray(inertia_value) < 0).any():\n                    raise ValueError(""Values in inertia matrix should be non-negative."")\n            self._pybullet_client.changeDynamics(self.quadruped,\n                                                 chassis_id,\n                                                 localInertiaDiagonal=chassis_inertia)\n\n    def GetTrueObservation(self):\n        observation = []\n        observation.extend(self.GetTrueMotorAngles())\n        observation.extend(self.GetTrueMotorVelocities())\n        observation.extend(self.GetTrueMotorTorques())\n        observation.extend(self.GetTrueBaseOrientation())\n        observation.extend(self.GetTrueBaseRollPitchYawRate())\n        return observation\n\n    def ReceiveObservation(self):\n        """"""Receive the observation from sensors.\n\n        This function is called once per step. The observations are only updated\n        when this function is called.\n        """"""\n        self._observation_history.appendleft(self.GetTrueObservation())\n        self._control_observation = self._GetControlObservation()\n\n    def _GetDelayedObservation(self, latency):\n        """"""Get observation that is delayed by the amount specified in latency.\n\n        Args:\n          latency: The latency (in seconds) of the delayed observation.\n        Returns:\n          observation: The observation which was actually latency seconds ago.\n        """"""\n        if latency <= 0 or len(self._observation_history) == 1:\n            observation = self._observation_history[0]\n        else:\n            n_steps_ago = int(latency / self.time_step)\n            if n_steps_ago + 1 >= len(self._observation_history):\n                return self._observation_history[-1]\n            remaining_latency = latency - n_steps_ago * self.time_step\n            blend_alpha = remaining_latency / self.time_step\n            observation = ((1.0 - blend_alpha) * np.array(self._observation_history[n_steps_ago]) +\n                           blend_alpha * np.array(self._observation_history[n_steps_ago + 1]))\n        return observation\n\n    def _GetPDObservation(self):\n        pd_delayed_observation = self._GetDelayedObservation(self._pd_latency)\n        q = pd_delayed_observation[0:self.num_motors]\n        qdot = pd_delayed_observation[self.num_motors:2 * self.num_motors]\n        return (np.array(q), np.array(qdot))\n\n    def _GetControlObservation(self):\n        control_delayed_observation = self._GetDelayedObservation(self._control_latency)\n        return control_delayed_observation\n\n    def _AddSensorNoise(self, sensor_values, noise_stdev):\n        if noise_stdev <= 0:\n            return sensor_values\n        observation = sensor_values + np.random.normal(scale=noise_stdev, size=sensor_values.shape)\n        return observation\n\n    def SetTimeSteps(self, action_repeat, simulation_step):\n        """"""Set the time steps of the control and simulation.\n\n        Args:\n          action_repeat: The number of simulation steps that the same action is\n            repeated.\n          simulation_step: The simulation time step.\n        """"""\n        self.time_step = simulation_step\n        self._action_repeat = action_repeat\n\n    @property\n    def chassis_link_ids(self):\n        return self._chassis_link_ids\n'"
rex_gym/playground/__init__.py,0,"b""import gym\nfrom gym.envs.registration import registry\n\n\ndef register(id, *args, **kvargs):\n    if id in registry.env_specs:\n        return\n    else:\n        return gym.envs.registration.register(id, *args, **kvargs)\n\n\ndef getList():\n    btenvs = ['- ' + spec.id for spec in gym.envs.registry.all() if spec.id.find('Bullet') >= 0]\n    return btenvs\n\n\nregister(\n    id='RexGalloping-v0',\n    entry_point='rex_gym.envs.gym.galloping_env:RexReactiveEnv',\n    max_episode_steps=1000,\n    reward_threshold=5.0,\n)\n\nregister(\n    id='RexWalk-v0',\n    entry_point='rex_gym.envs.gym.walk_env:RexWalkEnv',\n    max_episode_steps=1000,\n    reward_threshold=5.0,\n)\n\nregister(\n    id='RexTurn-v0',\n    entry_point='rex_gym.envs.gym.turn_env:RexTurnEnv',\n    max_episode_steps=1000,\n    reward_threshold=5.0,\n)\n\nregister(\n    id='RexStandup-v0',\n    entry_point='rex_gym.envs.gym.standup_env:RexStandupEnv',\n    max_episode_steps=400,\n    reward_threshold=5.0,\n)\n"""
rex_gym/playground/policy_player.py,1,"b'r""""""Running a pre-trained ppo agent on rex environments""""""\nimport logging\nimport os\nimport site\nimport time\n\nimport tensorflow as tf\nfrom rex_gym.agents.scripts import utility\nfrom rex_gym.agents.ppo import simple_ppo_agent\nfrom rex_gym.util import action_mapper\n\n\nclass PolicyPlayer:\n    def __init__(self, env_id: str, args: dict):\n        self.gym_dir_path = str(site.getsitepackages()[0])\n        self.env_id = env_id\n        self.args = args\n\n    def play(self):\n        policy_dir = os.path.join(self.gym_dir_path, action_mapper.ENV_ID_TO_POLICY[self.env_id][0])\n        config = utility.load_config(policy_dir)\n        policy_layers = config.policy_layers\n        value_layers = config.value_layers\n        env = config.env(render=True, **self.args)\n        network = config.network\n        checkpoint = os.path.join(policy_dir, action_mapper.ENV_ID_TO_POLICY[self.env_id][1])\n        with tf.Session() as sess:\n            agent = simple_ppo_agent.SimplePPOPolicy(sess,\n                                                     env,\n                                                     network,\n                                                     policy_layers=policy_layers,\n                                                     value_layers=value_layers,\n                                                     checkpoint=checkpoint)\n            sum_reward = 0\n            observation = env.reset()\n            while True:\n                action = agent.get_action([observation])\n                observation, reward, done, _ = env.step(action[0])\n                time.sleep(0.002)\n                sum_reward += reward\n                logging.info(f""Reward={sum_reward}"")\n                if done:\n                    break\n'"
rex_gym/playground/trainer.py,7,"b'r""""""Script to render a training session.""""""\nimport datetime\nimport functools\nimport logging\nimport os\n\nimport gym\nimport tensorflow as tf\n\nfrom rex_gym.agents.tools import wrappers\nfrom rex_gym.agents.scripts import configs, utility\nfrom rex_gym.agents.tools.attr_dict import AttrDict\nfrom rex_gym.agents.tools.loop import Loop\n\n\nclass Trainer:\n    def __init__(self, env_id: str, args: dict, playground: bool, log_dir: str, agents_number):\n        self.env_id = env_id\n        self.args = args\n        self.playground = playground\n        self.log_dir = log_dir\n        self.agents = agents_number\n\n    def _create_environment(self, config):\n        """"""Constructor for an instance of the environment.\n\n          Args:\n            config: Object providing configurations via attributes.\n\n          Returns:\n            Wrapped OpenAI Gym environment.\n        """"""\n        if self.playground:\n            self.args[\'render\'] = True\n        env = gym.make(config.env, **self.args)\n        if config.max_length:\n            env = wrappers.LimitDuration(env, config.max_length)\n        env = wrappers.RangeNormalize(env)\n        env = wrappers.ClipAction(env)\n        env = wrappers.ConvertTo32Bit(env)\n        return env\n\n    @staticmethod\n    def _define_loop(graph, logdir, train_steps, eval_steps):\n        """"""Create and configure a training loop with training and evaluation phases.\n\n          Args:\n            graph: Object providing graph elements via attributes.\n            logdir: Log directory for storing checkpoints and summaries.\n            train_steps: Number of training steps per epoch.\n            eval_steps: Number of evaluation steps per epoch.\n\n          Returns:\n            Loop object.\n        """"""\n        loop = Loop(logdir, graph.step, graph.should_log, graph.do_report, graph.force_reset)\n        loop.add_phase(\'train\',\n                       graph.done,\n                       graph.score,\n                       graph.summary,\n                       train_steps,\n                       report_every=None,\n                       log_every=train_steps // 2,\n                       checkpoint_every=None,\n                       feed={graph.is_training: True})\n        loop.add_phase(\'eval\',\n                       graph.done,\n                       graph.score,\n                       graph.summary,\n                       eval_steps,\n                       report_every=eval_steps,\n                       log_every=eval_steps // 2,\n                       checkpoint_every=10 * eval_steps,\n                       feed={graph.is_training: False})\n        return loop\n\n    def _train(self, config, env_processes):\n        """"""Training and evaluation entry point yielding scores.\n\n          Resolves some configuration attributes, creates environments, graph, and\n          training loop. By default, assigns all operations to the CPU.\n\n          Args:\n            config: Object providing configurations via attributes.\n            env_processes: Whether to step environments in separate processes.\n\n          Yields:\n            Evaluation scores.\n        """"""\n        tf.reset_default_graph()\n        with config.unlocked:\n            config.network = functools.partial(utility.define_network, config.network, config)\n            config.policy_optimizer = getattr(tf.train, config.policy_optimizer)\n            config.value_optimizer = getattr(tf.train, config.value_optimizer)\n        if config.update_every % config.num_agents:\n            logging.warning(\'Number of agents should divide episodes per update.\')\n        with tf.device(\'/cpu:0\'):\n            agents = self.agents if self.agents is not None else config.num_agents\n            num_agents = 1 if self.playground else agents\n            batch_env = utility.define_batch_env(lambda: self._create_environment(config), num_agents, env_processes)\n            graph = utility.define_simulation_graph(batch_env, config.algorithm, config)\n            loop = self._define_loop(graph, config.logdir, config.update_every * config.max_length,\n                                     config.eval_episodes * config.max_length)\n            total_steps = int(config.steps / config.update_every * (config.update_every + config.eval_episodes))\n        # Exclude episode related variables since the Python state of environments is\n        # not checkpointed and thus new episodes start after resuming.\n        saver = utility.define_saver(exclude=(r\'.*_temporary/.*\',))\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n        with tf.Session(config=sess_config) as sess:\n            utility.initialize_variables(sess, saver, config.logdir)\n            for score in loop.run(sess, saver, total_steps):\n                yield score\n        batch_env.close()\n\n    def start_training(self):\n        """"""Create configuration and launch the trainer.""""""\n        utility.set_up_logging()\n        timestamp = datetime.datetime.now().strftime(\'%Y%m%dT%H%M%S\')\n        full_logdir = os.path.expanduser(os.path.join(self.log_dir, \'{}-{}\'.format(timestamp, self.env_id)))\n        config = AttrDict(getattr(configs, self.env_id)())\n        config = utility.save_config(config, full_logdir)\n        for score in self._train(config, True):\n            tf.logging.info(\'Score {}.\'.format(score))\n'"
rex_gym/util/__init__.py,0,b'\n'
rex_gym/util/action_mapper.py,0,"b""ENV_ID_TO_POLICY = {\n    'gallop': ('rex_gym/policies/galloping/balanced', 'model.ckpt-20000000'),\n    'walk': ('rex_gym/policies/walking/alternating_legs', 'model.ckpt-16000000'),\n    'standup': ('rex_gym/policies/standup', 'model.ckpt-10000000'),\n    'turn': ('rex_gym/policies/turn', 'model.ckpt-16000000')\n}\n\nENV_ID_TO_ENV_NAMES = {\n    'gallop': 'RexReactiveEnv',\n    'walk': 'RexWalkEnv',\n    'turn': 'RexTurnEnv',\n    'standup': 'RexStandupEnv'\n}\n"""
rex_gym/util/bullet_client.py,0,"b'""""""A wrapper for pybullet to manage different clients.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nimport functools\nimport inspect\nimport pybullet\n\n\nclass BulletClient(object):\n    """"""A wrapper for pybullet to manage different clients.""""""\n\n    def __init__(self, connection_mode=None):\n        """"""Creates a Bullet client and connects to a simulation.\n\n    Args:\n      connection_mode:\n        `None` connects to an existing simulation or, if fails, creates a\n          new headless simulation,\n        `pybullet.GUI` creates a new simulation with a GUI,\n        `pybullet.DIRECT` creates a headless simulation,\n        `pybullet.SHARED_MEMORY` connects to an existing simulation.\n    """"""\n        self._shapes = {}\n\n        if connection_mode is None:\n            self._client = pybullet.connect(pybullet.SHARED_MEMORY)\n            if self._client >= 0:\n                return\n            else:\n                connection_mode = pybullet.DIRECT\n        self._client = pybullet.connect(connection_mode)\n\n    def __del__(self):\n        """"""Clean up connection if not already done.""""""\n        try:\n            pybullet.disconnect(physicsClientId=self._client)\n        except pybullet.error:\n            pass\n\n    def __getattr__(self, name):\n        """"""Inject the client id into Bullet functions.""""""\n        attribute = getattr(pybullet, name)\n        if inspect.isbuiltin(attribute):\n            if name not in [\n                ""invertTransform"",\n                ""multiplyTransforms"",\n                ""getMatrixFromQuaternion"",\n                ""getEulerFromQuaternion"",\n                ""computeViewMatrixFromYawPitchRoll"",\n                ""computeProjectionMatrixFOV"",\n                ""getQuaternionFromEuler"",\n            ]:  # A temporary hack for now.\n                attribute = functools.partial(attribute, physicsClientId=self._client)\n        return attribute\n'"
rex_gym/agents/ppo/__init__.py,0,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Proximal Policy Optimization algorithm.""""""\n\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n#\n# from .algorithm import PPOAlgorithm\n'"
rex_gym/agents/ppo/algorithm.py,137,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Proximal Policy Optimization algorithm.\n\nBased on John Schulman\'s implementation in Python and Theano:\nhttps://github.com/joschu/modular_rl/blob/master/modular_rl/ppo.py\n""""""\nimport collections\n\nimport tensorflow as tf\n\nfrom . import memory\nfrom . import normalize\nfrom . import utility\n\n_NetworkOutput = collections.namedtuple(\'NetworkOutput\', \'policy, mean, logstd, value, state\')\n\n\nclass PPOAlgorithm(object):\n    """"""A vectorized implementation of the PPO algorithm by John Schulman.""""""\n\n    def __init__(self, batch_env, step, is_training, should_log, config):\n        """"""Create an instance of the PPO algorithm.\n\n        Args:\n          batch_env: In-graph batch environment.\n          step: Integer tensor holding the current training step.\n          is_training: Boolean tensor for whether the algorithm should train.\n          should_log: Boolean tensor for whether summaries should be returned.\n          config: Object containing the agent configuration as attributes.\n        """"""\n        self._batch_env = batch_env\n        self._step = step\n        self._is_training = is_training\n        self._should_log = should_log\n        self._config = config\n        self._observ_filter = normalize.StreamingNormalize(self._batch_env.observ[0],\n                                                           center=True,\n                                                           scale=True,\n                                                           clip=5,\n                                                           name=\'normalize_observ\')\n        self._reward_filter = normalize.StreamingNormalize(self._batch_env.reward[0],\n                                                           center=False,\n                                                           scale=True,\n                                                           clip=10,\n                                                           name=\'normalize_reward\')\n        # Memory stores tuple of observ, action, mean, logstd, reward.\n        template = (self._batch_env.observ[0], self._batch_env.action[0], self._batch_env.action[0],\n                    self._batch_env.action[0], self._batch_env.reward[0])\n        self._memory = memory.EpisodeMemory(template, config.update_every, config.max_length, \'memory\')\n        self._memory_index = tf.Variable(0, False)\n        use_gpu = self._config.use_gpu and utility.available_gpus()\n        with tf.device(\'/gpu:0\' if use_gpu else \'/cpu:0\'):\n            # Create network variables for later calls to reuse.\n            self._network(tf.zeros_like(self._batch_env.observ)[:, None],\n                          tf.ones(len(self._batch_env)),\n                          reuse=None)\n            cell = self._config.network(self._batch_env.action.shape[1].value)\n            with tf.variable_scope(\'ppo_temporary\'):\n                self._episodes = memory.EpisodeMemory(template, len(batch_env), config.max_length,\n                                                      \'episodes\')\n                self._last_state = utility.create_nested_vars(cell.zero_state(len(batch_env), tf.float32))\n                self._last_action = tf.Variable(tf.zeros_like(self._batch_env.action),\n                                                False,\n                                                name=\'last_action\')\n                self._last_mean = tf.Variable(tf.zeros_like(self._batch_env.action),\n                                              False,\n                                              name=\'last_mean\')\n                self._last_logstd = tf.Variable(tf.zeros_like(self._batch_env.action),\n                                                False,\n                                                name=\'last_logstd\')\n        self._penalty = tf.Variable(self._config.kl_init_penalty, False, dtype=tf.float32)\n        self._policy_optimizer = self._config.policy_optimizer(self._config.policy_lr,\n                                                               name=\'policy_optimizer\')\n        self._value_optimizer = self._config.value_optimizer(self._config.value_lr,\n                                                             name=\'value_optimizer\')\n\n    def begin_episode(self, agent_indices):\n        """"""Reset the recurrent states and stored episode.\n\n        Args:\n          agent_indices: 1D tensor of batch indices for agents starting an episode.\n\n        Returns:\n          Summary tensor.\n        """"""\n        with tf.name_scope(\'begin_episode/\'):\n            reset_state = utility.reinit_nested_vars(self._last_state, agent_indices)\n            reset_buffer = self._episodes.clear(agent_indices)\n            with tf.control_dependencies([reset_state, reset_buffer]):\n                return tf.constant(\'\')\n\n    def perform(self, observ):\n        """"""Compute batch of actions and a summary for a batch of observation.\n\n        Args:\n          observ: Tensor of a batch of observations for all agents.\n\n        Returns:\n          Tuple of action batch tensor and summary tensor.\n        """"""\n        with tf.name_scope(\'perform/\'):\n            observ = self._observ_filter.transform(observ)\n            network = self._network(observ[:, None], tf.ones(observ.shape[0]), self._last_state)\n            action = tf.cond(self._is_training, network.policy.sample, lambda: network.mean)\n            logprob = network.policy.log_prob(action)[:, 0]\n            # pylint: disable=g-long-lambda\n            summary = tf.cond(\n                self._should_log, lambda: tf.summary.merge([\n                    tf.summary.histogram(\'mean\', network.mean[:, 0]),\n                    tf.summary.histogram(\'std\', tf.exp(network.logstd[:, 0])),\n                    tf.summary.histogram(\'action\', action[:, 0]),\n                    tf.summary.histogram(\'logprob\', logprob)\n                ]), str)\n            # Remember current policy to append to memory in the experience callback.\n            with tf.control_dependencies([\n                utility.assign_nested_vars(self._last_state, network.state),\n                self._last_action.assign(action[:, 0]),\n                self._last_mean.assign(network.mean[:, 0]),\n                self._last_logstd.assign(network.logstd[:, 0])\n            ]):\n                return tf.check_numerics(action[:, 0], \'action\'), tf.identity(summary)\n\n    def experience(self, observ, action, reward, unused_done, unused_nextob):\n        """"""Process the transition tuple of the current step.\n\n        When training, add the current transition tuple to the memory and update\n        the streaming statistics for observations and rewards. A summary string is\n        returned if requested at this step.\n\n        Args:\n          observ: Batch tensor of observations.\n          action: Batch tensor of actions.\n          reward: Batch tensor of rewards.\n          unused_done: Batch tensor of done flags.\n          unused_nextob: Batch tensor of successor observations.\n\n        Returns:\n          Summary tensor.\n        """"""\n        with tf.name_scope(\'experience/\'):\n            return tf.cond(self._is_training, lambda: self._define_experience(observ, action, reward),\n                           str)\n\n    def _define_experience(self, observ, action, reward):\n        """"""Implement the branch of experience() entered during training.""""""\n        update_filters = tf.summary.merge(\n            [self._observ_filter.update(observ),\n             self._reward_filter.update(reward)])\n        with tf.control_dependencies([update_filters]):\n            if self._config.train_on_agent_action:\n                # NOTE: Doesn\'t seem to change much.\n                action = self._last_action\n            batch = observ, action, self._last_mean, self._last_logstd, reward\n            append = self._episodes.append(batch, tf.range(len(self._batch_env)))\n        with tf.control_dependencies([append]):\n            norm_observ = self._observ_filter.transform(observ)\n            norm_reward = tf.reduce_mean(self._reward_filter.transform(reward))\n            # pylint: disable=g-long-lambda\n            summary = tf.cond(\n                self._should_log, lambda: tf.summary.merge([\n                    update_filters,\n                    self._observ_filter.summary(),\n                    self._reward_filter.summary(),\n                    tf.summary.scalar(\'memory_size\', self._memory_index),\n                    tf.summary.histogram(\'normalized_observ\', norm_observ),\n                    tf.summary.histogram(\'action\', self._last_action),\n                    tf.summary.scalar(\'normalized_reward\', norm_reward)\n                ]), str)\n            return summary\n\n    def end_episode(self, agent_indices):\n        """"""Add episodes to the memory and perform update steps if memory is full.\n\n        During training, add the collected episodes of the batch indices that\n        finished their episode to the memory. If the memory is full, train on it,\n        and then clear the memory. A summary string is returned if requested at\n        this step.\n\n        Args:\n          agent_indices: 1D tensor of batch indices for agents starting an episode.\n\n        Returns:\n           Summary tensor.\n        """"""\n        with tf.name_scope(\'end_episode/\'):\n            return tf.cond(self._is_training, lambda: self._define_end_episode(agent_indices), str)\n\n    def _define_end_episode(self, agent_indices):\n        """"""Implement the branch of end_episode() entered during training.""""""\n        episodes, length = self._episodes.data(agent_indices)\n        space_left = self._config.update_every - self._memory_index\n        use_episodes = tf.range(tf.minimum(tf.shape(agent_indices)[0], space_left))\n        episodes = [tf.gather(elem, use_episodes) for elem in episodes]\n        append = self._memory.replace(episodes, tf.gather(length, use_episodes),\n                                      use_episodes + self._memory_index)\n        with tf.control_dependencies([append]):\n            inc_index = self._memory_index.assign_add(tf.shape(use_episodes)[0])\n        with tf.control_dependencies([inc_index]):\n            memory_full = self._memory_index >= self._config.update_every\n            return tf.cond(memory_full, self._training, str)\n\n    def _training(self):\n        """"""Perform multiple training iterations of both policy and value baseline.\n\n        Training on the episodes collected in the memory. Reset the memory\n        afterwards. Always returns a summary string.\n\n        Returns:\n          Summary tensor.\n        """"""\n        with tf.name_scope(\'training\'):\n            assert_full = tf.assert_equal(self._memory_index, self._config.update_every)\n            with tf.control_dependencies([assert_full]):\n                data = self._memory.data()\n            (observ, action, old_mean, old_logstd, reward), length = data\n            with tf.control_dependencies([tf.assert_greater(length, 0)]):\n                length = tf.identity(length)\n            observ = self._observ_filter.transform(observ)\n            reward = self._reward_filter.transform(reward)\n            policy_summary = self._update_policy(observ, action, old_mean, old_logstd, reward, length)\n            with tf.control_dependencies([policy_summary]):\n                value_summary = self._update_value(observ, reward, length)\n            with tf.control_dependencies([value_summary]):\n                penalty_summary = self._adjust_penalty(observ, old_mean, old_logstd, length)\n            with tf.control_dependencies([penalty_summary]):\n                clear_memory = tf.group(self._memory.clear(), self._memory_index.assign(0))\n            with tf.control_dependencies([clear_memory]):\n                weight_summary = utility.variable_summaries(tf.trainable_variables(),\n                                                            self._config.weight_summaries)\n                return tf.summary.merge([policy_summary, value_summary, penalty_summary, weight_summary])\n\n    def _update_value(self, observ, reward, length):\n        """"""Perform multiple update steps of the value baseline.\n\n        We need to decide for the summary of one iteration, and thus choose the one\n        after half of the iterations.\n\n        Args:\n          observ: Sequences of observations.\n          reward: Sequences of reward.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Summary tensor.\n        """"""\n        with tf.name_scope(\'update_value\'):\n            loss, summary = tf.scan(lambda _1, _2: self._update_value_step(observ, reward, length),\n                                    tf.range(self._config.update_epochs_value), [0., \'\'],\n                                    parallel_iterations=1)\n            print_loss = tf.Print(0, [tf.reduce_mean(loss)], \'value loss: \')\n            with tf.control_dependencies([loss, print_loss]):\n                return summary[self._config.update_epochs_value // 2]\n\n    def _update_value_step(self, observ, reward, length):\n        """"""Compute the current value loss and perform a gradient update step.\n\n        Args:\n          observ: Sequences of observations.\n          reward: Sequences of reward.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        """"""\n        loss, summary = self._value_loss(observ, reward, length)\n        gradients, variables = (zip(*self._value_optimizer.compute_gradients(loss)))\n        optimize = self._value_optimizer.apply_gradients(zip(gradients, variables))\n        summary = tf.summary.merge([\n            summary,\n            tf.summary.scalar(\'gradient_norm\', tf.global_norm(gradients)),\n            utility.gradient_summaries(zip(gradients, variables), dict(value=r\'.*\'))\n        ])\n        with tf.control_dependencies([optimize]):\n            return [tf.identity(loss), tf.identity(summary)]\n\n    def _value_loss(self, observ, reward, length):\n        """"""Compute the loss function for the value baseline.\n\n        The value loss is the difference between empirical and approximated returns\n        over the collected episodes. Returns the loss tensor and a summary strin.\n\n        Args:\n          observ: Sequences of observations.\n          reward: Sequences of reward.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        """"""\n        with tf.name_scope(\'value_loss\'):\n            value = self._network(observ, length).value\n            return_ = utility.discounted_return(reward, length, self._config.discount)\n            advantage = return_ - value\n            value_loss = 0.5 * self._mask(advantage**2, length)\n            summary = tf.summary.merge([\n                tf.summary.histogram(\'value_loss\', value_loss),\n                tf.summary.scalar(\'avg_value_loss\', tf.reduce_mean(value_loss))\n            ])\n            value_loss = tf.reduce_mean(value_loss)\n            return tf.check_numerics(value_loss, \'value_loss\'), summary\n\n    def _update_policy(self, observ, action, old_mean, old_logstd, reward, length):\n        """"""Perform multiple update steps of the policy.\n\n        The advantage is computed once at the beginning and shared across\n        iterations. We need to decide for the summary of one iteration, and thus\n        choose the one after half of the iterations.\n\n        Args:\n          observ: Sequences of observations.\n          action: Sequences of actions.\n          old_mean: Sequences of action means of the behavioral policy.\n          old_logstd: Sequences of action log stddevs of the behavioral policy.\n          reward: Sequences of rewards.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Summary tensor.\n        """"""\n        with tf.name_scope(\'update_policy\'):\n            return_ = utility.discounted_return(reward, length, self._config.discount)\n            value = self._network(observ, length).value\n            if self._config.gae_lambda:\n                advantage = utility.lambda_return(reward, value, length, self._config.discount,\n                                                  self._config.gae_lambda)\n            else:\n                advantage = return_ - value\n            mean, variance = tf.nn.moments(advantage, axes=[0, 1], keep_dims=True)\n            advantage = (advantage - mean) / (tf.sqrt(variance) + 1e-8)\n            advantage = tf.Print(\n                advantage, [tf.reduce_mean(return_), tf.reduce_mean(value)], \'return and value: \')\n            advantage = tf.Print(advantage, [tf.reduce_mean(advantage)], \'normalized advantage: \')\n            # pylint: disable=g-long-lambda\n            loss, summary = tf.scan(lambda _1, _2: self._update_policy_step(\n                observ, action, old_mean, old_logstd, advantage, length),\n                                    tf.range(self._config.update_epochs_policy), [0., \'\'],\n                                    parallel_iterations=1)\n            print_loss = tf.Print(0, [tf.reduce_mean(loss)], \'policy loss: \')\n            with tf.control_dependencies([loss, print_loss]):\n                return summary[self._config.update_epochs_policy // 2]\n\n    def _update_policy_step(self, observ, action, old_mean, old_logstd, advantage, length):\n        """"""Compute the current policy loss and perform a gradient update step.\n\n        Args:\n          observ: Sequences of observations.\n          action: Sequences of actions.\n          old_mean: Sequences of action means of the behavioral policy.\n          old_logstd: Sequences of action log stddevs of the behavioral policy.\n          advantage: Sequences of advantages.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        """"""\n        network = self._network(observ, length)\n        loss, summary = self._policy_loss(network.mean, network.logstd, old_mean, old_logstd, action,\n                                          advantage, length)\n        gradients, variables = (zip(*self._policy_optimizer.compute_gradients(loss)))\n        optimize = self._policy_optimizer.apply_gradients(zip(gradients, variables))\n        summary = tf.summary.merge([\n            summary,\n            tf.summary.scalar(\'gradient_norm\', tf.global_norm(gradients)),\n            utility.gradient_summaries(zip(gradients, variables), dict(policy=r\'.*\'))\n        ])\n        with tf.control_dependencies([optimize]):\n            return [tf.identity(loss), tf.identity(summary)]\n\n    def _policy_loss(self, mean, logstd, old_mean, old_logstd, action, advantage, length):\n        """"""Compute the policy loss composed of multiple components.\n\n        1. The policy gradient loss is importance sampled from the data-collecting\n           policy at the beginning of training.\n        2. The second term is a KL penalty between the policy at the beginning of\n           training and the current policy.\n        3. Additionally, if this KL already changed more than twice the target\n           amount, we activate a strong penalty discouraging further divergence.\n\n        Args:\n          mean: Sequences of action means of the current policy.\n          logstd: Sequences of action log stddevs of the current policy.\n          old_mean: Sequences of action means of the behavioral policy.\n          old_logstd: Sequences of action log stddevs of the behavioral policy.\n          action: Sequences of actions.\n          advantage: Sequences of advantages.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        """"""\n        with tf.name_scope(\'policy_loss\'):\n            entropy = utility.diag_normal_entropy(mean, logstd)\n            kl = tf.reduce_mean(\n                self._mask(utility.diag_normal_kl(old_mean, old_logstd, mean, logstd), length), 1)\n            policy_gradient = tf.exp(\n                utility.diag_normal_logpdf(mean, logstd, action) -\n                utility.diag_normal_logpdf(old_mean, old_logstd, action))\n            surrogate_loss = -tf.reduce_mean(\n                self._mask(policy_gradient * tf.stop_gradient(advantage), length), 1)\n            kl_penalty = self._penalty * kl\n            cutoff_threshold = self._config.kl_target * self._config.kl_cutoff_factor\n            cutoff_count = tf.reduce_sum(tf.cast(kl > cutoff_threshold, tf.int32))\n            with tf.control_dependencies(\n                    [tf.cond(cutoff_count > 0, lambda: tf.Print(0, [cutoff_count], \'kl cutoff! \'), int)]):\n                kl_cutoff = (self._config.kl_cutoff_coef * tf.cast(kl > cutoff_threshold, tf.float32) *\n                             (kl - cutoff_threshold)**2)\n            policy_loss = surrogate_loss + kl_penalty + kl_cutoff\n            summary = tf.summary.merge([\n                tf.summary.histogram(\'entropy\', entropy),\n                tf.summary.histogram(\'kl\', kl),\n                tf.summary.histogram(\'surrogate_loss\', surrogate_loss),\n                tf.summary.histogram(\'kl_penalty\', kl_penalty),\n                tf.summary.histogram(\'kl_cutoff\', kl_cutoff),\n                tf.summary.histogram(\'kl_penalty_combined\', kl_penalty + kl_cutoff),\n                tf.summary.histogram(\'policy_loss\', policy_loss),\n                tf.summary.scalar(\'avg_surr_loss\', tf.reduce_mean(surrogate_loss)),\n                tf.summary.scalar(\'avg_kl_penalty\', tf.reduce_mean(kl_penalty)),\n                tf.summary.scalar(\'avg_policy_loss\', tf.reduce_mean(policy_loss))\n            ])\n            policy_loss = tf.reduce_mean(policy_loss, 0)\n            return tf.check_numerics(policy_loss, \'policy_loss\'), summary\n\n    def _adjust_penalty(self, observ, old_mean, old_logstd, length):\n        """"""Adjust the KL policy between the behavioral and current policy.\n\n        Compute how much the policy actually changed during the multiple\n        update steps. Adjust the penalty strength for the next training phase if we\n        overshot or undershot the target divergence too much.\n\n        Args:\n          observ: Sequences of observations.\n          old_mean: Sequences of action means of the behavioral policy.\n          old_logstd: Sequences of action log stddevs of the behavioral policy.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Summary tensor.\n        """"""\n        with tf.name_scope(\'adjust_penalty\'):\n            network = self._network(observ, length)\n            assert_change = tf.assert_equal(tf.reduce_all(tf.equal(network.mean, old_mean)),\n                                            False,\n                                            message=\'policy should change\')\n            print_penalty = tf.Print(0, [self._penalty], \'current penalty: \')\n            with tf.control_dependencies([assert_change, print_penalty]):\n                kl_change = tf.reduce_mean(\n                    self._mask(utility.diag_normal_kl(old_mean, old_logstd, network.mean, network.logstd),\n                               length))\n                kl_change = tf.Print(kl_change, [kl_change], \'kl change: \')\n                maybe_increase = tf.cond(\n                    kl_change > 1.3 * self._config.kl_target,\n                    # pylint: disable=g-long-lambda\n                    lambda: tf.Print(self._penalty.assign(self._penalty * 1.5), [0], \'increase penalty \'),\n                    float)\n                maybe_decrease = tf.cond(\n                    kl_change < 0.7 * self._config.kl_target,\n                    # pylint: disable=g-long-lambda\n                    lambda: tf.Print(self._penalty.assign(self._penalty / 1.5), [0], \'decrease penalty \'),\n                    float)\n            with tf.control_dependencies([maybe_increase, maybe_decrease]):\n                return tf.summary.merge([\n                    tf.summary.scalar(\'kl_change\', kl_change),\n                    tf.summary.scalar(\'penalty\', self._penalty)\n                ])\n\n    def _mask(self, tensor, length):\n        """"""Set padding elements of a batch of sequences to zero.\n\n        Useful to then safely sum along the time dimension.\n\n        Args:\n          tensor: Tensor of sequences.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Masked sequences.\n        """"""\n        with tf.name_scope(\'mask\'):\n            range_ = tf.range(tensor.shape[1].value)\n            mask = tf.cast(range_[None, :] < length[:, None], tf.float32)\n            masked = tensor * mask\n            return tf.check_numerics(masked, \'masked\')\n\n    def _network(self, observ, length=None, state=None, reuse=True):\n        """"""Compute the network output for a batched sequence of observations.\n\n        Optionally, the initial state can be specified. The weights should be\n        reused for all calls, except for the first one. Output is a named tuple\n        containing the policy as a TensorFlow distribution, the policy mean and log\n        standard deviation, the approximated state value, and the new recurrent\n        state.\n\n        Args:\n          observ: Sequences of observations.\n          length: Batch of sequence lengths.\n          state: Batch of initial recurrent states.\n          reuse: Python boolean whether to reuse previous variables.\n\n        Returns:\n          NetworkOutput tuple.\n        """"""\n        with tf.variable_scope(\'network\', reuse=reuse):\n            observ = tf.convert_to_tensor(observ)\n            use_gpu = self._config.use_gpu and utility.available_gpus()\n            with tf.device(\'/gpu:0\' if use_gpu else \'/cpu:0\'):\n                observ = tf.check_numerics(observ, \'observ\')\n                cell = self._config.network(self._batch_env.action.shape[1].value)\n                (mean, logstd, value), state = tf.nn.dynamic_rnn(cell,\n                                                                 observ,\n                                                                 length,\n                                                                 state,\n                                                                 tf.float32,\n                                                                 swap_memory=True)\n            mean = tf.check_numerics(mean, \'mean\')\n            logstd = tf.check_numerics(logstd, \'logstd\')\n            value = tf.check_numerics(value, \'value\')\n            policy = tf.contrib.distributions.MultivariateNormalDiag(mean, tf.exp(logstd))\n            return _NetworkOutput(policy, mean, logstd, value, state)'"
rex_gym/agents/ppo/memory.py,28,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Memory that stores episodes.""""""\nimport tensorflow as tf\n\n\nclass EpisodeMemory(object):\n\n    def __init__(self, template, capacity, max_length, scope):\n        """"""\n        Create a memory that stores episodes.\n\n        Each transition tuple consists of quantities specified by the template.\n        These quantities would typically be be observartions, actions, rewards, and\n        done indicators.\n\n        Args:\n          template: List of tensors to derive shapes and dtypes of each transition.\n          capacity: Number of episodes, or rows, hold by the memory.\n          max_length: Allocated sequence length for the episodes.\n          scope: Variable scope to use for internal variables.\n        """"""\n        self._capacity = capacity\n        self._max_length = max_length\n        with tf.variable_scope(scope) as scope:\n            self._scope = scope\n            self._length = tf.Variable(tf.zeros(capacity, tf.int32), False)\n            self._buffers = [\n                tf.Variable(tf.zeros([capacity, max_length] + elem.shape.as_list(), elem.dtype), False)\n                for elem in template\n            ]\n\n    def length(self, rows=None):\n        """"""Tensor holding the current length of episodes.\n\n        Args:\n          rows: Episodes to select length from, defaults to all.\n\n        Returns:\n          Batch tensor of sequence lengths.\n        """"""\n        rows = tf.range(self._capacity) if rows is None else rows\n        return tf.gather(self._length, rows)\n\n    def append(self, transitions, rows=None):\n        """"""Append a batch of transitions to rows of the memory.\n\n        Args:\n          transitions: Tuple of transition quantities with batch dimension.\n          rows: Episodes to append to, defaults to all.\n\n        Returns:\n          Operation.\n        """"""\n        rows = tf.range(self._capacity) if rows is None else rows\n        assert rows.shape.ndims == 1\n        assert_capacity = tf.assert_less(rows, self._capacity, message=\'capacity exceeded\')\n        with tf.control_dependencies([assert_capacity]):\n            assert_max_length = tf.assert_less(tf.gather(self._length, rows),\n                                               self._max_length,\n                                               message=\'max length exceeded\')\n        append_ops = []\n        with tf.control_dependencies([assert_max_length]):\n            for buffer_, elements in zip(self._buffers, transitions):\n                timestep = tf.gather(self._length, rows)\n                indices = tf.stack([rows, timestep], 1)\n                append_ops.append(tf.scatter_nd_update(buffer_, indices, elements))\n        with tf.control_dependencies(append_ops):\n            episode_mask = tf.reduce_sum(tf.one_hot(rows, self._capacity, dtype=tf.int32), 0)\n            return self._length.assign_add(episode_mask)\n\n    def replace(self, episodes, length, rows=None):\n        """"""Replace full episodes.\n\n        Args:\n          episodes: Tuple of transition quantities with batch and time dimensions.\n          length: Batch of sequence lengths.\n          rows: Episodes to replace, defaults to all.\n\n        Returns:\n          Operation.\n        """"""\n        rows = tf.range(self._capacity) if rows is None else rows\n        assert rows.shape.ndims == 1\n        assert_capacity = tf.assert_less(rows, self._capacity, message=\'capacity exceeded\')\n        with tf.control_dependencies([assert_capacity]):\n            assert_max_length = tf.assert_less_equal(length,\n                                                     self._max_length,\n                                                     message=\'max length exceeded\')\n        replace_ops = []\n        with tf.control_dependencies([assert_max_length]):\n            for buffer_, elements in zip(self._buffers, episodes):\n                replace_op = tf.scatter_update(buffer_, rows, elements)\n                replace_ops.append(replace_op)\n        with tf.control_dependencies(replace_ops):\n            return tf.scatter_update(self._length, rows, length)\n\n    def data(self, rows=None):\n        """"""Access a batch of episodes from the memory.\n\n        Padding elements after the length of each episode are unspecified and might\n        contain old data.\n\n        Args:\n          rows: Episodes to select, defaults to all.\n\n        Returns:\n          Tuple containing a tuple of transition quantiries with batch and time\n          dimensions, and a batch of sequence lengths.\n        """"""\n        rows = tf.range(self._capacity) if rows is None else rows\n        assert rows.shape.ndims == 1\n        episode = [tf.gather(buffer_, rows) for buffer_ in self._buffers]\n        length = tf.gather(self._length, rows)\n        return episode, length\n\n    def clear(self, rows=None):\n        """"""Reset episodes in the memory.\n\n        Internally, this only sets their lengths to zero. The memory entries will\n        be overridden by future calls to append() or replace().\n\n        Args:\n          rows: Episodes to clear, defaults to all.\n\n        Returns:\n          Operation.\n        """"""\n        rows = tf.range(self._capacity) if rows is None else rows\n        assert rows.shape.ndims == 1\n        return tf.scatter_update(self._length, rows, tf.zeros_like(rows))\n'"
rex_gym/agents/ppo/normalize.py,32,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Normalize tensors based on streaming estimates of mean and variance.""""""\nimport tensorflow as tf\n\n\nclass StreamingNormalize(object):\n    """"""Normalize tensors based on streaming estimates of mean and variance.""""""\n\n    def __init__(self, template, center=True, scale=True, clip=10, name=\'normalize\'):\n        """"""Normalize tensors based on streaming estimates of mean and variance.\n\n        Centering the value, scaling it by the standard deviation, and clipping\n        outlier values are optional.\n\n        Args:\n          template: Example tensor providing shape and dtype of the vaule to track.\n          center: Python boolean indicating whether to subtract mean from values.\n          scale: Python boolean indicating whether to scale values by stddev.\n          clip: If and when to clip normalized values.\n          name: Parent scope of operations provided by this class.\n        """"""\n        self._center = center\n        self._scale = scale\n        self._clip = clip\n        self._name = name\n        with tf.name_scope(name):\n            self._count = tf.Variable(0, False)\n            self._mean = tf.Variable(tf.zeros_like(template), False)\n            self._var_sum = tf.Variable(tf.zeros_like(template), False)\n\n    def transform(self, value):\n        """"""Normalize a single or batch tensor.\n\n        Applies the activated transformations in the constructor using current\n        estimates of mean and variance.\n\n        Args:\n          value: Batch or single value tensor.\n\n        Returns:\n          Normalized batch or single value tensor.\n        """"""\n        with tf.name_scope(self._name + \'/transform\'):\n            no_batch_dim = value.shape.ndims == self._mean.shape.ndims\n            if no_batch_dim:\n                # Add a batch dimension if necessary.\n                value = value[None, ...]\n            if self._center:\n                value -= self._mean[None, ...]\n            if self._scale:\n                # We cannot scale before seeing at least two samples.\n                value /= tf.cond(\n                    self._count > 1, lambda: self._std() + 1e-8, lambda: tf.ones_like(self._var_sum))[None]\n            if self._clip:\n                value = tf.clip_by_value(value, -self._clip, self._clip)\n            # Remove batch dimension if necessary.\n            if no_batch_dim:\n                value = value[0]\n            return tf.check_numerics(value, \'value\')\n\n    def update(self, value):\n        """"""Update the mean and variance estimates.\n\n        Args:\n          value: Batch or single value tensor.\n\n        Returns:\n          Summary tensor.\n        """"""\n        with tf.name_scope(self._name + \'/update\'):\n            if value.shape.ndims == self._mean.shape.ndims:\n                # Add a batch dimension if necessary.\n                value = value[None, ...]\n            count = tf.shape(value)[0]\n            with tf.control_dependencies([self._count.assign_add(count)]):\n                step = tf.cast(self._count, tf.float32)\n                mean_delta = tf.reduce_sum(value - self._mean[None, ...], 0)\n                new_mean = self._mean + mean_delta / step\n                new_mean = tf.cond(self._count > 1, lambda: new_mean, lambda: value[0])\n                var_delta = (value - self._mean[None, ...]) * (value - new_mean[None, ...])\n                new_var_sum = self._var_sum + tf.reduce_sum(var_delta, 0)\n            with tf.control_dependencies([new_mean, new_var_sum]):\n                update = self._mean.assign(new_mean), self._var_sum.assign(new_var_sum)\n            with tf.control_dependencies(update):\n                if value.shape.ndims == 1:\n                    value = tf.reduce_mean(value)\n                return self._summary(\'value\', tf.reduce_mean(value))\n\n    def reset(self):\n        """"""Reset the estimates of mean and variance.\n\n        Resets the full state of this class.\n\n        Returns:\n          Operation.\n        """"""\n        with tf.name_scope(self._name + \'/reset\'):\n            return tf.group(self._count.assign(0), self._mean.assign(tf.zeros_like(self._mean)),\n                            self._var_sum.assign(tf.zeros_like(self._var_sum)))\n\n    def summary(self):\n        """"""Summary string of mean and standard deviation.\n\n        Returns:\n          Summary tensor.\n        """"""\n        with tf.name_scope(self._name + \'/summary\'):\n            mean_summary = tf.cond(self._count > 0, lambda: self._summary(\'mean\', self._mean), str)\n            std_summary = tf.cond(self._count > 1, lambda: self._summary(\'stddev\', self._std()), str)\n            return tf.summary.merge([mean_summary, std_summary])\n\n    def _std(self):\n        """"""Computes the current estimate of the standard deviation.\n\n        Note that the standard deviation is not defined until at least two samples\n        were seen.\n\n        Returns:\n          Tensor of current variance.\n        """"""\n        variance = tf.cond(self._count > 1, lambda: self._var_sum / tf.cast(\n            self._count - 1, tf.float32), lambda: tf.ones_like(self._var_sum) * float(\'nan\'))\n        # The epsilon corrects for small negative variance values caused by\n        # the algorithm. It was empirically chosen to work with all environments\n        # tested.\n        return tf.sqrt(variance + 1e-4)\n\n    def _summary(self, name, tensor):\n        """"""Create a scalar or histogram summary matching the rank of the tensor.\n\n        Args:\n          name: Name for the summary.\n          tensor: Tensor to summarize.\n\n        Returns:\n          Summary tensor.\n        """"""\n        if tensor.shape.ndims == 0:\n            return tf.summary.scalar(name, tensor)\n        else:\n            return tf.summary.histogram(name, tensor)\n'"
rex_gym/agents/ppo/simple_ppo_agent.py,8,"b'""""""An agent that can restore and run a policy learned by PPO.""""""\nimport tensorflow as tf\nfrom . import normalize\nfrom ..scripts import utility\n\n\nclass SimplePPOPolicy(object):\n    """"""A simple PPO policy that is independent to the PPO infrastructure.\n\n  This class restores the policy network from a tensorflow checkpoint that was\n  learned from PPO training. The purpose of this class is to conveniently\n  visualize a learned policy or deploy the learned policy on real robots without\n  need to change the PPO evaluation infrastructure:\n  https://cs.corp.google.com/piper///depot/google3/robotics/reinforcement_learning/agents/scripts/visualize.py.\n  """"""\n\n    def __init__(self, sess, env, network, policy_layers, value_layers, checkpoint):\n        self.env = env\n        self.sess = sess\n        observation_size = len(env.observation_space.low)\n        action_size = len(env.action_space.low)\n        self.observation_placeholder = tf.placeholder(tf.float32, [None, observation_size],\n                                                      name=""Input"")\n        self._observ_filter = normalize.StreamingNormalize(self.observation_placeholder[0],\n                                                           center=True,\n                                                           scale=True,\n                                                           clip=5,\n                                                           name=""normalize_observ"")\n        self._restore_policy(network,\n                             policy_layers=policy_layers,\n                             value_layers=value_layers,\n                             action_size=action_size,\n                             checkpoint=checkpoint)\n\n    def _restore_policy(self, network, policy_layers, value_layers, action_size, checkpoint):\n        """"""Restore the PPO policy from a TensorFlow checkpoint.\n\n    Args:\n      network: The neural network definition.\n      policy_layers: A tuple specify the number of layers and number of neurons\n        of each layer for the policy network.\n      value_layers: A tuple specify the number of layers and number of neurons\n        of each layer for the value network.\n      action_size: The dimension of the action space.\n      checkpoint: The checkpoint path.\n    """"""\n        observ = self._observ_filter.transform(self.observation_placeholder)\n        with tf.variable_scope(""network/rnn""):\n            self.network = network(policy_layers=policy_layers,\n                                   value_layers=value_layers,\n                                   action_size=action_size)\n\n        with tf.variable_scope(""temporary""):\n            self.last_state = tf.Variable(self.network.zero_state(1, tf.float32), False)\n            self.sess.run(self.last_state.initializer)\n\n        with tf.variable_scope(""network""):\n            (mean_action, _, _), new_state = tf.nn.dynamic_rnn(self.network,\n                                                               observ[:, None],\n                                                               tf.ones(1),\n                                                               self.last_state,\n                                                               tf.float32,\n                                                               swap_memory=True)\n            self.mean_action = mean_action\n            self.update_state = self.last_state.assign(new_state)\n\n        saver = utility.define_saver(exclude=(r""temporary/.*"",))\n        saver.restore(self.sess, checkpoint)\n\n    def get_action(self, observation):\n        normalized_observation = self._normalize_observ(observation)\n        normalized_action, _ = self.sess.run(\n            [self.mean_action, self.update_state],\n            feed_dict={self.observation_placeholder: normalized_observation})\n        action = self._denormalize_action(normalized_action)\n        return action[:, 0]\n\n    def _denormalize_action(self, action):\n        min_ = self.env.action_space.low\n        max_ = self.env.action_space.high\n        action = (action + 1) / 2 * (max_ - min_) + min_\n        return action\n\n    def _normalize_observ(self, observ):\n        min_ = self.env.observation_space.low\n        max_ = self.env.observation_space.high\n        observ = 2 * (observ - min_) / (max_ - min_) - 1\n        return observ\n'"
rex_gym/agents/ppo/utility.py,53,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for the PPO algorithm.""""""\nimport collections\nimport math\nimport re\n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\n\ndef create_nested_vars(tensors):\n    """"""Create variables matching a nested tuple of tensors.\n\n  Args:\n    tensors: Nested tuple of list of tensors.\n\n  Returns:\n    Nested tuple or list of variables.\n  """"""\n    if isinstance(tensors, (tuple, list)):\n        return type(tensors)(create_nested_vars(tensor) for tensor in tensors)\n    return tf.Variable(tensors, False)\n\n\ndef reinit_nested_vars(variables, indices=None):\n    """"""Reset all variables in a nested tuple to zeros.\n\n  Args:\n    variables: Nested tuple or list of variaables.\n    indices: Indices along the first dimension to reset, defaults to all.\n\n  Returns:\n    Operation.\n  """"""\n    if isinstance(variables, (tuple, list)):\n        return tf.group(*[reinit_nested_vars(variable, indices) for variable in variables])\n    if indices is None:\n        return variables.assign(tf.zeros_like(variables))\n    else:\n        zeros = tf.zeros([tf.shape(indices)[0]] + variables.shape[1:].as_list())\n        return tf.scatter_update(variables, indices, zeros)\n\n\ndef assign_nested_vars(variables, tensors):\n    """"""Assign tensors to matching nested tuple of variables.\n\n  Args:\n    variables: Nested tuple or list of variables to update.\n    tensors: Nested tuple or list of tensors to assign.\n\n  Returns:\n    Operation.\n  """"""\n    if isinstance(variables, (tuple, list)):\n        return tf.group(\n            *[assign_nested_vars(variable, tensor) for variable, tensor in zip(variables, tensors)])\n    return variables.assign(tensors)\n\n\ndef discounted_return(reward, length, discount):\n    """"""Discounted Monte-Carlo returns.""""""\n    timestep = tf.range(reward.shape[1].value)\n    mask = tf.cast(timestep[None, :] < length[:, None], tf.float32)\n    return_ = tf.reverse(\n        tf.transpose(\n            tf.scan(lambda agg, cur: cur + discount * agg,\n                    tf.transpose(tf.reverse(mask * reward, [1]), [1, 0]),\n                    tf.zeros_like(reward[:, -1]), 1, False), [1, 0]), [1])\n    return tf.check_numerics(tf.stop_gradient(return_), \'return\')\n\n\ndef fixed_step_return(reward, value, length, discount, window):\n    """"""N-step discounted return.""""""\n    timestep = tf.range(reward.shape[1].value)\n    mask = tf.cast(timestep[None, :] < length[:, None], tf.float32)\n    return_ = tf.zeros_like(reward)\n    for _ in range(window):\n        return_ += reward\n        reward = discount * tf.concat([reward[:, 1:], tf.zeros_like(reward[:, -1:])], 1)\n    return_ += discount ** window * tf.concat(\n        [value[:, window:], tf.zeros_like(value[:, -window:]), 1])\n    return tf.check_numerics(tf.stop_gradient(mask * return_), \'return\')\n\n\ndef lambda_return(reward, value, length, discount, lambda_):\n    """"""TD-lambda returns.""""""\n    timestep = tf.range(reward.shape[1].value)\n    mask = tf.cast(timestep[None, :] < length[:, None], tf.float32)\n    sequence = mask * reward + discount * value * (1 - lambda_)\n    discount = mask * discount * lambda_\n    sequence = tf.stack([sequence, discount], 2)\n    return_ = tf.reverse(\n        tf.transpose(\n            tf.scan(lambda agg, cur: cur[0] + cur[1] * agg,\n                    tf.transpose(tf.reverse(sequence, [1]), [1, 2, 0]), tf.zeros_like(value[:, -1]),\n                    1, False), [1, 0]), [1])\n    return tf.check_numerics(tf.stop_gradient(return_), \'return\')\n\n\ndef lambda_advantage(reward, value, length, discount):\n    """"""Generalized Advantage Estimation.""""""\n    timestep = tf.range(reward.shape[1].value)\n    mask = tf.cast(timestep[None, :] < length[:, None], tf.float32)\n    next_value = tf.concat([value[:, 1:], tf.zeros_like(value[:, -1:])], 1)\n    delta = reward + discount * next_value - value\n    advantage = tf.reverse(\n        tf.transpose(\n            tf.scan(lambda agg, cur: cur + discount * agg,\n                    tf.transpose(tf.reverse(mask * delta, [1]), [1, 0]), tf.zeros_like(delta[:, -1]),\n                    1, False), [1, 0]), [1])\n    return tf.check_numerics(tf.stop_gradient(advantage), \'advantage\')\n\n\ndef diag_normal_kl(mean0, logstd0, mean1, logstd1):\n    """"""Epirical KL divergence of two normals with diagonal covariance.""""""\n    logstd0_2, logstd1_2 = 2 * logstd0, 2 * logstd1\n    return 0.5 * (tf.reduce_sum(tf.exp(logstd0_2 - logstd1_2), -1) + tf.reduce_sum(\n        (mean1 - mean0) ** 2 / tf.exp(logstd1_2), -1) + tf.reduce_sum(logstd1_2, -1) -\n                  tf.reduce_sum(logstd0_2, -1) - mean0.shape[-1].value)\n\n\ndef diag_normal_logpdf(mean, logstd, loc):\n    """"""Log density of a normal with diagonal covariance.""""""\n    constant = -0.5 * (math.log(2 * math.pi) + logstd)\n    value = -0.5 * ((loc - mean) / tf.exp(logstd)) ** 2\n    return tf.reduce_sum(constant + value, -1)\n\n\ndef diag_normal_entropy(mean, logstd):\n    """"""Empirical entropy of a normal with diagonal covariance.""""""\n    constant = mean.shape[-1].value * math.log(2 * math.pi * math.e)\n    return (constant + tf.reduce_sum(2 * logstd, 1)) / 2\n\n\ndef available_gpus():\n    """"""List of GPU device names detected by TensorFlow.""""""\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == \'GPU\']\n\n\ndef gradient_summaries(grad_vars, groups=None, scope=\'gradients\'):\n    """"""Create histogram summaries of the gradient.\n\n  Summaries can be grouped via regexes matching variables names.\n\n  Args:\n    grad_vars: List of (gradient, variable) tuples as returned by optimizers.\n    groups: Mapping of name to regex for grouping summaries.\n    scope: Name scope for this operation.\n\n  Returns:\n    Summary tensor.\n  """"""\n    groups = groups or {r\'all\': r\'.*\'}\n    grouped = collections.defaultdict(list)\n    for grad, var in grad_vars:\n        if grad is None:\n            continue\n        for name, pattern in groups.items():\n            if re.match(pattern, var.name):\n                name = re.sub(pattern, name, var.name)\n                grouped[name].append(grad)\n    for name in groups:\n        if name not in grouped:\n            tf.logging.warn(""No variables matching \'{}\' group."".format(name))\n    summaries = []\n    for name, grads in grouped.items():\n        grads = [tf.reshape(grad, [-1]) for grad in grads]\n        grads = tf.concat(grads, 0)\n        summaries.append(tf.summary.histogram(scope + \'/\' + name, grads))\n    return tf.summary.merge(summaries)\n\n\ndef variable_summaries(vars_, groups=None, scope=\'weights\'):\n    """"""Create histogram summaries for the provided variables.\n\n  Summaries can be grouped via regexes matching variables names.\n\n  Args:\n    vars_: List of variables to summarize.\n    groups: Mapping of name to regex for grouping summaries.\n    scope: Name scope for this operation.\n\n  Returns:\n    Summary tensor.\n  """"""\n    groups = groups or {r\'all\': r\'.*\'}\n    grouped = collections.defaultdict(list)\n    for var in vars_:\n        for name, pattern in groups.items():\n            if re.match(pattern, var.name):\n                name = re.sub(pattern, name, var.name)\n                grouped[name].append(var)\n    for name in groups:\n        if name not in grouped:\n            tf.logging.warn(""No variables matching \'{}\' group."".format(name))\n    summaries = []\n    for name, vars_ in grouped.items():\n        vars_ = [tf.reshape(var, [-1]) for var in vars_]\n        vars_ = tf.concat(vars_, 0)\n        summaries.append(tf.summary.histogram(scope + \'/\' + name, vars_))\n    return tf.summary.merge(summaries)\n'"
rex_gym/agents/scripts/__init__.py,0,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Executable scripts for reinforcement learning.""""""\n\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n#\n# from . import train\n# from . import utility\n# from . import visualize\n'"
rex_gym/agents/scripts/configs.py,0,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example configurations using the PPO algorithm.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom ..ppo.algorithm import PPOAlgorithm\n\nfrom ..scripts import networks\n\n\ndef default():\n    """"""Default configuration for PPO.""""""\n    # General\n    algorithm = PPOAlgorithm\n    num_agents = 25\n    eval_episodes = 25\n    use_gpu = False\n    # Network\n    network = networks.ForwardGaussianPolicy\n    weight_summaries = dict(all=r\'.*\', policy=r\'.*/policy/.*\', value=r\'.*/value/.*\')\n    policy_layers = 200, 100\n    value_layers = 200, 100\n    init_mean_factor = 0.05\n    init_logstd = -1\n    # Optimization\n    update_every = 25\n    policy_optimizer = \'AdamOptimizer\'\n    value_optimizer = \'AdamOptimizer\'\n    update_epochs_policy = 50\n    update_epochs_value = 50\n    policy_lr = 1e-4\n    value_lr = 3e-4\n    # Losses\n    discount = 0.985\n    kl_target = 1e-2\n    kl_cutoff_factor = 2\n    kl_cutoff_coef = 1000\n    kl_init_penalty = 1\n    return locals()\n\n\ndef gallop():\n    """"""Configuration for Rex galloping task.""""""\n    locals().update(default())\n    # Environment\n    env = \'RexGalloping-v0\'\n    max_length = 1000\n    steps = 8e6  # 8M\n    return locals()\n\n\ndef walk():\n    """"""Configuration for Rex walking task.""""""\n    locals().update(default())\n    # Environment\n    env = \'RexWalk-v0\'\n    max_length = 1000\n    steps = 8e6  # 8M\n    return locals()\n\n\ndef turn():\n    """"""Configuration for Rex turn task.""""""\n    locals().update(default())\n    # Environment\n    env = \'RexTurn-v0\'\n    max_length = 1000\n    steps = 8e6  # 8M\n    return locals()\n\n\ndef standup():\n    """"""Configuration for Rex stand up task.""""""\n    locals().update(default())\n    # Environment\n    env = \'RexStandup-v0\'\n    max_length = 1000\n    steps = 5e6  # 5M\n    return locals()\n'"
rex_gym/agents/scripts/networks.py,41,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Networks for the PPO algorithm defined as recurrent cells.""""""\nimport tensorflow as tf\n\n_MEAN_WEIGHTS_INITIALIZER = tf.contrib.layers.variance_scaling_initializer(factor=0.1)\n_LOGSTD_INITIALIZER = tf.random_normal_initializer(-1, 1e-10)\n\n\nclass LinearGaussianPolicy(tf.contrib.rnn.RNNCell):\n    """"""Indepent linear network with a tanh at the end for policy and feedforward network for the value.\n\n  The policy network outputs the mean action and the log standard deviation\n  is learned as indepent parameter vector.\n  """"""\n\n    def __init__(self,\n                 policy_layers,\n                 value_layers,\n                 action_size,\n                 mean_weights_initializer=_MEAN_WEIGHTS_INITIALIZER,\n                 logstd_initializer=_LOGSTD_INITIALIZER):\n        self._policy_layers = policy_layers\n        self._value_layers = value_layers\n        self._action_size = action_size\n        self._mean_weights_initializer = mean_weights_initializer\n        self._logstd_initializer = logstd_initializer\n\n    @property\n    def state_size(self):\n        unused_state_size = 1\n        return unused_state_size\n\n    @property\n    def output_size(self):\n        return self._action_size, self._action_size, tf.TensorShape([])\n\n    def __call__(self, observation, state):\n        with tf.variable_scope(\'policy\'):\n            x = tf.contrib.layers.flatten(observation)\n            mean = tf.contrib.layers.fully_connected(x,\n                                                     self._action_size,\n                                                     tf.tanh,\n                                                     weights_initializer=self._mean_weights_initializer)\n            logstd = tf.get_variable(\'logstd\', mean.shape[1:], tf.float32, self._logstd_initializer)\n            logstd = tf.tile(logstd[None, ...], [tf.shape(mean)[0]] + [1] * logstd.shape.ndims)\n        with tf.variable_scope(\'value\'):\n            x = tf.contrib.layers.flatten(observation)\n            for size in self._value_layers:\n                x = tf.contrib.layers.fully_connected(x, size, tf.nn.relu)\n            value = tf.contrib.layers.fully_connected(x, 1, None)[:, 0]\n        return (mean, logstd, value), state\n\n\nclass ForwardGaussianPolicy(tf.contrib.rnn.RNNCell):\n    """"""Independent feed forward networks for policy and value.\n\n  The policy network outputs the mean action and the log standard deviation\n  is learned as independent parameter vector.\n  """"""\n\n    def __init__(self,\n                 policy_layers,\n                 value_layers,\n                 action_size,\n                 mean_weights_initializer=_MEAN_WEIGHTS_INITIALIZER,\n                 logstd_initializer=_LOGSTD_INITIALIZER):\n        self._policy_layers = policy_layers\n        self._value_layers = value_layers\n        self._action_size = action_size\n        self._mean_weights_initializer = mean_weights_initializer\n        self._logstd_initializer = logstd_initializer\n\n    @property\n    def state_size(self):\n        unused_state_size = 1\n        return unused_state_size\n\n    @property\n    def output_size(self):\n        return self._action_size, self._action_size, tf.TensorShape([])\n\n    def __call__(self, observation, state):\n        with tf.variable_scope(\'policy\'):\n            x = tf.contrib.layers.flatten(observation)\n            for size in self._policy_layers:\n                x = tf.contrib.layers.fully_connected(x, size, tf.nn.relu)\n            mean = tf.contrib.layers.fully_connected(x,\n                                                     self._action_size,\n                                                     tf.tanh,\n                                                     weights_initializer=self._mean_weights_initializer)\n            logstd = tf.get_variable(\'logstd\', mean.shape[1:], tf.float32, self._logstd_initializer)\n            logstd = tf.tile(logstd[None, ...], [tf.shape(mean)[0]] + [1] * logstd.shape.ndims)\n        with tf.variable_scope(\'value\'):\n            x = tf.contrib.layers.flatten(observation)\n            for size in self._value_layers:\n                x = tf.contrib.layers.fully_connected(x, size, tf.nn.relu)\n            value = tf.contrib.layers.fully_connected(x, 1, None)[:, 0]\n        return (mean, logstd, value), state\n\n\nclass RecurrentGaussianPolicy(tf.contrib.rnn.RNNCell):\n    """"""Independent recurrent policy and feed forward value networks.\n\n  The policy network outputs the mean action and the log standard deviation\n  is learned as independent parameter vector. The last policy layer is recurrent\n  and uses a GRU cell.\n  """"""\n\n    def __init__(self,\n                 policy_layers,\n                 value_layers,\n                 action_size,\n                 mean_weights_initializer=_MEAN_WEIGHTS_INITIALIZER,\n                 logstd_initializer=_LOGSTD_INITIALIZER):\n        self._policy_layers = policy_layers\n        self._value_layers = value_layers\n        self._action_size = action_size\n        self._mean_weights_initializer = mean_weights_initializer\n        self._logstd_initializer = logstd_initializer\n        self._cell = tf.contrib.rnn.GRUBlockCell(100)\n\n    @property\n    def state_size(self):\n        return self._cell.state_size\n\n    @property\n    def output_size(self):\n        return self._action_size, self._action_size, tf.TensorShape([])\n\n    def __call__(self, observation, state):\n        with tf.variable_scope(\'policy\'):\n            x = tf.contrib.layers.flatten(observation)\n            for size in self._policy_layers[:-1]:\n                x = tf.contrib.layers.fully_connected(x, size, tf.nn.relu)\n            x, state = self._cell(x, state)\n            mean = tf.contrib.layers.fully_connected(x,\n                                                     self._action_size,\n                                                     tf.tanh,\n                                                     weights_initializer=self._mean_weights_initializer)\n            logstd = tf.get_variable(\'logstd\', mean.shape[1:], tf.float32, self._logstd_initializer)\n            logstd = tf.tile(logstd[None, ...], [tf.shape(mean)[0]] + [1] * logstd.shape.ndims)\n        with tf.variable_scope(\'value\'):\n            x = tf.contrib.layers.flatten(observation)\n            for size in self._value_layers:\n                x = tf.contrib.layers.fully_connected(x, size, tf.nn.relu)\n            value = tf.contrib.layers.fully_connected(x, 1, None)[:, 0]\n        return (mean, logstd, value), state\n'"
rex_gym/agents/scripts/utility.py,21,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for using reinforcement learning algorithms.""""""\nimport logging\nimport os\nimport re\n\nimport ruamel.yaml as yaml\nimport tensorflow as tf\n\nfrom rex_gym.agents.tools import wrappers\nfrom rex_gym.agents.tools.attr_dict import AttrDict\nfrom rex_gym.agents.tools.batch_env import BatchEnv\nfrom rex_gym.agents.tools.count_weights import count_weights\nfrom rex_gym.agents.tools.in_graph_batch_env import InGraphBatchEnv\nfrom rex_gym.agents.tools.simulate import simulate\n\n\ndef define_simulation_graph(batch_env, algo_cls, config):\n    """"""Define the algortihm and environment interaction.\n\n  Args:\n    batch_env: In-graph environments object.\n    algo_cls: Constructor of a batch algorithm.\n    config: Configuration object for the algorithm.\n\n  Returns:\n    Object providing graph elements via attributes.\n  """"""\n    step = tf.Variable(0, False, dtype=tf.int32, name=\'global_step\')\n    is_training = tf.placeholder(tf.bool, name=\'is_training\')\n    should_log = tf.placeholder(tf.bool, name=\'should_log\')\n    do_report = tf.placeholder(tf.bool, name=\'do_report\')\n    force_reset = tf.placeholder(tf.bool, name=\'force_reset\')\n    algo = algo_cls(batch_env, step, is_training, should_log, config)\n    done, score, summary = simulate(batch_env, algo, should_log, force_reset)\n    message = \'Graph contains {} trainable variables.\'\n    tf.logging.info(message.format(count_weights()))\n    return AttrDict(locals())\n\n\ndef define_batch_env(constructor, num_agents, env_processes):\n    """"""Create environments and apply all desired wrappers.\n\n  Args:\n    constructor: Constructor of an OpenAI gym environment.\n    num_agents: Number of environments to combine in the batch.\n    env_processes: Whether to step environment in external processes.\n\n  Returns:\n    In-graph environments object.\n  """"""\n    with tf.variable_scope(\'environments\'):\n        if env_processes:\n            envs = [wrappers.ExternalProcess(constructor) for _ in range(num_agents)]\n        else:\n            envs = [constructor() for _ in range(num_agents)]\n        batch_env = BatchEnv(envs, blocking=not env_processes)\n        batch_env = InGraphBatchEnv(batch_env)\n    return batch_env\n\n\ndef define_saver(exclude=None):\n    """"""Create a saver for the variables we want to checkpoint.\n\n  Args:\n    exclude: List of regexes to match variable names to exclude.\n\n  Returns:\n    Saver object.\n  """"""\n    variables = []\n    exclude = exclude or []\n    exclude = [re.compile(regex) for regex in exclude]\n    for variable in tf.global_variables():\n        if any(regex.match(variable.name) for regex in exclude):\n            continue\n        variables.append(variable)\n    saver = tf.train.Saver(variables, keep_checkpoint_every_n_hours=5)\n    return saver\n\n\ndef define_network(constructor, config, action_size):\n    """"""Constructor for the recurrent cell for the algorithm.\n\n  Args:\n    constructor: Callable returning the network as RNNCell.\n    config: Object providing configurations via attributes.\n    action_size: Integer indicating the amount of action dimensions.\n\n  Returns:\n    Created recurrent cell object.\n  """"""\n    mean_weights_initializer = (tf.contrib.layers.variance_scaling_initializer(\n        factor=config.init_mean_factor))\n    logstd_initializer = tf.random_normal_initializer(config.init_logstd, 1e-10)\n    network = constructor(config.policy_layers,\n                          config.value_layers,\n                          action_size,\n                          mean_weights_initializer=mean_weights_initializer,\n                          logstd_initializer=logstd_initializer)\n    return network\n\n\ndef initialize_variables(sess, saver, logdir, checkpoint=None, resume=None):\n    """"""Initialize or restore variables from a checkpoint if available.\n\n  Args:\n    sess: Session to initialize variables in.\n    saver: Saver to restore variables.\n    logdir: Directory to search for checkpoints.\n    checkpoint: Specify what checkpoint name to use; defaults to most recent.\n    resume: Whether to expect recovering a checkpoint or starting a new run.\n\n  Raises:\n    ValueError: If resume expected but no log directory specified.\n    RuntimeError: If no resume expected but a checkpoint was found.\n  """"""\n    sess.run(tf.group(tf.local_variables_initializer(), tf.global_variables_initializer()))\n    if resume and not (logdir or checkpoint):\n        raise ValueError(\'Need to specify logdir to resume a checkpoint.\')\n    if logdir:\n        state = tf.train.get_checkpoint_state(logdir)\n        if checkpoint:\n            checkpoint = os.path.join(logdir, checkpoint)\n        if not checkpoint and state and state.model_checkpoint_path:\n            checkpoint = state.model_checkpoint_path\n        if checkpoint and resume is False:\n            message = \'Found unexpected checkpoint when starting a new run.\'\n            raise RuntimeError(message)\n        if checkpoint:\n            saver.restore(sess, checkpoint)\n\n\ndef save_config(config, logdir=None):\n    """"""Save a new configuration by name.\n\n  If a logging directory is specified, is will be created and the configuration\n  will be stored there. Otherwise, a log message will be printed.\n\n  Args:\n    config: Configuration object.\n    logdir: Location for writing summaries and checkpoints if specified.\n\n  Returns:\n    Configuration object.\n  """"""\n    if logdir:\n        with config.unlocked:\n            config.logdir = logdir\n        message = \'Start a new run and write summaries and checkpoints to {}.\'\n        tf.logging.info(message.format(config.logdir))\n        tf.gfile.MakeDirs(config.logdir)\n        config_path = os.path.join(config.logdir, \'config.yaml\')\n        with tf.gfile.GFile(config_path, \'w\') as file_:\n            yaml.dump(config, file_, default_flow_style=False)\n    else:\n        message = (\'Start a new run without storing summaries and checkpoints since no \'\n                   \'logging directory was specified.\')\n        tf.logging.info(message)\n    return config\n\n\ndef load_config(logdir):\n    """"""Load a configuration from the log directory.\n\n  Args:\n    logdir: The logging directory containing the configuration file.\n\n  Raises:\n    IOError: The logging directory does not contain a configuration file.\n\n  Returns:\n    Configuration object.\n  """"""\n    config_path = logdir and os.path.join(logdir, \'config.yaml\')\n    if not config_path or not tf.gfile.Exists(config_path):\n        message = (\'Cannot resume an existing run since the logging directory does not \'\n                   \'contain a configuration file.\')\n        raise IOError(message)\n    with tf.gfile.FastGFile(config_path, \'r\') as file_:\n        config = yaml.load(file_)\n    message = \'Resume run and write summaries and checkpoints to {}.\'\n    tf.logging.info(message.format(config.logdir))\n    return config\n\n\ndef set_up_logging():\n    """"""Configure the TensorFlow logger.""""""\n    tf.logging.set_verbosity(tf.logging.INFO)\n    logging.getLogger(\'tensorflow\').propagate = False\n'"
rex_gym/agents/tools/__init__.py,0,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tools for reinforcement learning.""""""\n\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n#\n# from .attr_dict import AttrDict\n# from .batch_env import BatchEnv\n# from .count_weights import count_weights\n# from .in_graph_batch_env import InGraphBatchEnv\n# from .in_graph_env import InGraphEnv\n# from .loop import Loop\n# from .mock_algorithm import MockAlgorithm\n# from .mock_environment import MockEnvironment\n# from .simulate import simulate\n# from .streaming_mean import StreamingMean\n# from . import wrappers\n'"
rex_gym/agents/tools/attr_dict.py,0,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Wrap a dictionary to access keys as attributes.""""""\nimport contextlib\n\n\nclass AttrDict(dict):\n    """"""Wrap a dictionary to access keys as attributes.""""""\n\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        super(AttrDict, self).__setattr__(\'_mutable\', False)\n\n    def __getattr__(self, key):\n        # Do not provide None for unimplemented magic attributes.\n        if key.startswith(\'__\'):\n            raise AttributeError\n        return self.get(key, None)\n\n    def __setattr__(self, key, value):\n        if not self._mutable:\n            message = ""Cannot set attribute \'{}\'."".format(key)\n            message += "" Use \'with obj.unlocked:\' scope to set attributes.""\n            raise RuntimeError(message)\n        if key.startswith(\'__\'):\n            raise AttributeError(""Cannot set magic attribute \'{}\'"".format(key))\n        self[key] = value\n\n    @property\n    @contextlib.contextmanager\n    def unlocked(self):\n        super(AttrDict, self).__setattr__(\'_mutable\', True)\n        yield\n        super(AttrDict, self).__setattr__(\'_mutable\', False)\n\n    def copy(self):\n        return type(self)(super(AttrDict, self).copy())\n'"
rex_gym/agents/tools/attr_dict_test.py,2,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for the attribute dictionary.""""""\nimport tensorflow as tf\n\nfrom . import attr_dict\n\n\nclass AttrDictTest(tf.test.TestCase):\n\n    def test_construct_from_dict(self):\n        initial = dict(foo=13, bar=42)\n        obj = attr_dict.AttrDict(initial)\n        self.assertEqual(13, obj.foo)\n        self.assertEqual(42, obj.bar)\n\n    def test_construct_from_kwargs(self):\n        obj = attr_dict.AttrDict(foo=13, bar=42)\n        self.assertEqual(13, obj.foo)\n        self.assertEqual(42, obj.bar)\n\n    def test_has_attribute(self):\n        obj = attr_dict.AttrDict(foo=13)\n        self.assertTrue(\'foo\' in obj)\n        self.assertFalse(\'bar\' in obj)\n\n    def test_access_default(self):\n        obj = attr_dict.AttrDict()\n        self.assertEqual(None, obj.foo)\n\n    def test_access_magic(self):\n        obj = attr_dict.AttrDict()\n        with self.assertRaises(AttributeError):\n            obj.__getstate__  # pylint: disable=pointless-statement\n\n    def test_immutable_create(self):\n        obj = attr_dict.AttrDict()\n        with self.assertRaises(RuntimeError):\n            obj.foo = 42\n\n    def test_immutable_modify(self):\n        obj = attr_dict.AttrDict(foo=13)\n        with self.assertRaises(RuntimeError):\n            obj.foo = 42\n\n    def test_immutable_unlocked(self):\n        obj = attr_dict.AttrDict()\n        with obj.unlocked:\n            obj.foo = 42\n        self.assertEqual(42, obj.foo)\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
rex_gym/agents/tools/batch_env.py,0,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Combine multiple environments to step them in batch.""""""\nimport numpy as np\n\n\nclass BatchEnv(object):\n    """"""Combine multiple environments to step them in batch.""""""\n\n    def __init__(self, envs, blocking):\n        """"""Combine multiple environments to step them in batch.\n\n    To step environments in parallel, environments must support a\n    `blocking=False` argument to their step and reset functions that makes them\n    return callables instead to receive the result at a later time.\n\n    Args:\n      envs: List of environments.\n      blocking: Step environments after another rather than in parallel.\n\n    Raises:\n      ValueError: Environments have different observation or action spaces.\n    """"""\n        self._envs = envs\n        self._blocking = blocking\n        observ_space = self._envs[0].observation_space\n        if not all(env.observation_space == observ_space for env in self._envs):\n            raise ValueError(\'All environments must use the same observation space.\')\n        action_space = self._envs[0].action_space\n        if not all(env.action_space == action_space for env in self._envs):\n            raise ValueError(\'All environments must use the same observation space.\')\n\n    def __len__(self):\n        """"""Number of combined environments.""""""\n        return len(self._envs)\n\n    def __getitem__(self, index):\n        """"""Access an underlying environment by index.""""""\n        return self._envs[index]\n\n    def __getattr__(self, name):\n        """"""Forward unimplemented attributes to one of the original environments.\n\n    Args:\n      name: Attribute that was accessed.\n\n    Returns:\n      Value behind the attribute name one of the wrapped environments.\n    """"""\n        return getattr(self._envs[0], name)\n\n    def step(self, action):\n        """"""Forward a batch of actions to the wrapped environments.\n\n    Args:\n      action: Batched action to apply to the environment.\n\n    Raises:\n      ValueError: Invalid actions.\n\n    Returns:\n      Batch of observations, rewards, and done flags.\n    """"""\n        actions = action\n        for index, (env, action) in enumerate(zip(self._envs, actions)):\n            if not env.action_space.contains(action):\n                message = \'Invalid action at index {}: {}\'\n                raise ValueError(message.format(index, action))\n        if self._blocking:\n            transitions = [env.step(action) for env, action in zip(self._envs, actions)]\n        else:\n            transitions = [env.step(action, blocking=False) for env, action in zip(self._envs, actions)]\n            transitions = [transition() for transition in transitions]\n        observs, rewards, dones, infos = zip(*transitions)\n        observ = np.stack(observs)\n        reward = np.stack(rewards)\n        done = np.stack(dones)\n        info = tuple(infos)\n        return observ, reward, done, info\n\n    def reset(self, indices=None):\n        """"""Reset the environment and convert the resulting observation.\n\n    Args:\n      indices: The batch indices of environments to reset; defaults to all.\n\n    Returns:\n      Batch of observations.\n    """"""\n        if indices is None:\n            indices = np.arange(len(self._envs))\n        if self._blocking:\n            observs = [self._envs[index].reset() for index in indices]\n        else:\n            observs = [self._envs[index].reset(blocking=False) for index in indices]\n            observs = [observ() for observ in observs]\n        observ = np.stack(observs)\n        return observ\n\n    def close(self):\n        """"""Send close messages to the external process and join them.""""""\n        for env in self._envs:\n            if hasattr(env, \'close\'):\n                env.close()\n'"
rex_gym/agents/tools/count_weights.py,2,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Count learnable parameters.""""""\nimport re\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef count_weights(scope=None, exclude=None, graph=None):\n    """"""Count learnable parameters.\n\n  Args:\n    scope: Resrict the count to a variable scope.\n    exclude: Regex to match variable names to exclude.\n    graph: Operate on a graph other than the current default graph.\n\n  Returns:\n    Number of learnable parameters as integer.\n  """"""\n    if scope:\n        scope = scope if scope.endswith(\'/\') else scope + \'/\'\n    graph = graph or tf.get_default_graph()\n    vars_ = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    if scope:\n        vars_ = [var for var in vars_ if var.name.startswith(scope)]\n    if exclude:\n        exclude = re.compile(exclude)\n        vars_ = [var for var in vars_ if not exclude.match(var.name)]\n    shapes = [var.get_shape().as_list() for var in vars_]\n    return int(sum(np.prod(shape) for shape in shapes))\n'"
rex_gym/agents/tools/count_weights_test.py,40,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for the weight counting utility.""""""\nimport tensorflow as tf\n\nfrom . import count_weights\n\n\nclass CountWeightsTest(tf.test.TestCase):\n\n    def test_count_trainable(self):\n        tf.Variable(tf.zeros((5, 3)), trainable=True)\n        tf.Variable(tf.zeros((1, 1)), trainable=True)\n        tf.Variable(tf.zeros((5,)), trainable=True)\n        self.assertEqual(15 + 1 + 5, count_weights())\n\n    def test_ignore_non_trainable(self):\n        tf.Variable(tf.zeros((5, 3)), trainable=False)\n        tf.Variable(tf.zeros((1, 1)), trainable=False)\n        tf.Variable(tf.zeros((5,)), trainable=False)\n        self.assertEqual(0, count_weights())\n\n    def test_trainable_and_non_trainable(self):\n        tf.Variable(tf.zeros((5, 3)), trainable=True)\n        tf.Variable(tf.zeros((8, 2)), trainable=False)\n        tf.Variable(tf.zeros((1, 1)), trainable=True)\n        tf.Variable(tf.zeros((5,)), trainable=True)\n        tf.Variable(tf.zeros((3, 1)), trainable=False)\n        self.assertEqual(15 + 1 + 5, count_weights())\n\n    def test_include_scopes(self):\n        tf.Variable(tf.zeros((3, 2)), trainable=True)\n        with tf.variable_scope(\'foo\'):\n            tf.Variable(tf.zeros((5, 2)), trainable=True)\n        self.assertEqual(6 + 10, count_weights())\n\n    def test_restrict_scope(self):\n        tf.Variable(tf.zeros((3, 2)), trainable=True)\n        with tf.variable_scope(\'foo\'):\n            tf.Variable(tf.zeros((5, 2)), trainable=True)\n            with tf.variable_scope(\'bar\'):\n                tf.Variable(tf.zeros((1, 2)), trainable=True)\n        self.assertEqual(10 + 2, count_weights(\'foo\'))\n\n    def test_restrict_nested_scope(self):\n        tf.Variable(tf.zeros((3, 2)), trainable=True)\n        with tf.variable_scope(\'foo\'):\n            tf.Variable(tf.zeros((5, 2)), trainable=True)\n            with tf.variable_scope(\'bar\'):\n                tf.Variable(tf.zeros((1, 2)), trainable=True)\n        self.assertEqual(2, count_weights(\'foo/bar\'))\n\n    def test_restrict_invalid_scope(self):\n        tf.Variable(tf.zeros((3, 2)), trainable=True)\n        with tf.variable_scope(\'foo\'):\n            tf.Variable(tf.zeros((5, 2)), trainable=True)\n            with tf.variable_scope(\'bar\'):\n                tf.Variable(tf.zeros((1, 2)), trainable=True)\n        self.assertEqual(0, count_weights(\'bar\'))\n\n    def test_exclude_by_regex(self):\n        tf.Variable(tf.zeros((3, 2)), trainable=True)\n        with tf.variable_scope(\'foo\'):\n            tf.Variable(tf.zeros((5, 2)), trainable=True)\n            with tf.variable_scope(\'bar\'):\n                tf.Variable(tf.zeros((1, 2)), trainable=True)\n        self.assertEqual(0, count_weights(exclude=r\'.*\'))\n        self.assertEqual(6, count_weights(exclude=r\'(^|/)foo/.*\'))\n        self.assertEqual(16, count_weights(exclude=r\'.*/bar/.*\'))\n\n    def test_non_default_graph(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            tf.Variable(tf.zeros((5, 3)), trainable=True)\n            tf.Variable(tf.zeros((8, 2)), trainable=False)\n        self.assertNotEqual(graph, tf.get_default_graph)\n        self.assertEqual(15, count_weights(graph=graph))\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
rex_gym/agents/tools/in_graph_batch_env.py,26,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Batch of environments inside the TensorFlow graph.""""""\nimport gym\nimport tensorflow as tf\n\n\nclass InGraphBatchEnv(object):\n    """"""Batch of environments inside the TensorFlow graph.\n\n  The batch of environments will be stepped and reset inside of the graph using\n  a tf.py_func(). The current batch of observations, actions, rewards, and done\n  flags are held in according variables.\n  """"""\n\n    def __init__(self, batch_env):\n        """"""Batch of environments inside the TensorFlow graph.\n\n    Args:\n      batch_env: Batch environment.\n    """"""\n        self._batch_env = batch_env\n        observ_shape = self._parse_shape(self._batch_env.observation_space)\n        observ_dtype = self._parse_dtype(self._batch_env.observation_space)\n        action_shape = self._parse_shape(self._batch_env.action_space)\n        action_dtype = self._parse_dtype(self._batch_env.action_space)\n        with tf.variable_scope(\'env_temporary\'):\n            self._observ = tf.Variable(tf.zeros((len(self._batch_env),) + observ_shape, observ_dtype),\n                                       name=\'observ\',\n                                       trainable=False)\n            self._action = tf.Variable(tf.zeros((len(self._batch_env),) + action_shape, action_dtype),\n                                       name=\'action\',\n                                       trainable=False)\n            self._reward = tf.Variable(tf.zeros((len(self._batch_env),), tf.float32),\n                                       name=\'reward\',\n                                       trainable=False)\n            self._done = tf.Variable(tf.cast(tf.ones((len(self._batch_env),)), tf.bool),\n                                     name=\'done\',\n                                     trainable=False)\n\n    def __getattr__(self, name):\n        """"""Forward unimplemented attributes to one of the original environments.\n\n    Args:\n      name: Attribute that was accessed.\n\n    Returns:\n      Value behind the attribute name in one of the original environments.\n    """"""\n        return getattr(self._batch_env, name)\n\n    def __len__(self):\n        """"""Number of combined environments.""""""\n        return len(self._batch_env)\n\n    def __getitem__(self, index):\n        """"""Access an underlying environment by index.""""""\n        return self._batch_env[index]\n\n    def simulate(self, action):\n        """"""Step the batch of environments.\n\n    The results of the step can be accessed from the variables defined below.\n\n    Args:\n      action: Tensor holding the batch of actions to apply.\n\n    Returns:\n      Operation.\n    """"""\n        with tf.name_scope(\'environment/simulate\'):\n            if action.dtype in (tf.float16, tf.float32, tf.float64):\n                action = tf.check_numerics(action, \'action\')\n            observ_dtype = self._parse_dtype(self._batch_env.observation_space)\n            observ, reward, done = tf.py_func(lambda a: self._batch_env.step(a)[:3], [action],\n                                              [observ_dtype, tf.float32, tf.bool],\n                                              name=\'step\')\n            observ = tf.check_numerics(observ, \'observ\')\n            reward = tf.check_numerics(reward, \'reward\')\n            return tf.group(self._observ.assign(observ), self._action.assign(action),\n                            self._reward.assign(reward), self._done.assign(done))\n\n    def reset(self, indices=None):\n        """"""Reset the batch of environments.\n\n    Args:\n      indices: The batch indices of the environments to reset; defaults to all.\n\n    Returns:\n      Batch tensor of the new observations.\n    """"""\n        if indices is None:\n            indices = tf.range(len(self._batch_env))\n        observ_dtype = self._parse_dtype(self._batch_env.observation_space)\n        observ = tf.py_func(self._batch_env.reset, [indices], observ_dtype, name=\'reset\')\n        observ = tf.check_numerics(observ, \'observ\')\n        reward = tf.zeros_like(indices, tf.float32)\n        done = tf.zeros_like(indices, tf.bool)\n        with tf.control_dependencies([\n            tf.scatter_update(self._observ, indices, observ),\n            tf.scatter_update(self._reward, indices, reward),\n            tf.scatter_update(self._done, indices, done)\n        ]):\n            return tf.identity(observ)\n\n    @property\n    def observ(self):\n        """"""Access the variable holding the current observation.""""""\n        return self._observ\n\n    @property\n    def action(self):\n        """"""Access the variable holding the last recieved action.""""""\n        return self._action\n\n    @property\n    def reward(self):\n        """"""Access the variable holding the current reward.""""""\n        return self._reward\n\n    @property\n    def done(self):\n        """"""Access the variable indicating whether the episode is done.""""""\n        return self._done\n\n    def close(self):\n        """"""Send close messages to the external process and join them.""""""\n        self._batch_env.close()\n\n    def _parse_shape(self, space):\n        """"""Get a tensor shape from a OpenAI Gym space.\n\n    Args:\n      space: Gym space.\n\n    Returns:\n      Shape tuple.\n    """"""\n        if isinstance(space, gym.spaces.Discrete):\n            return ()\n        if isinstance(space, gym.spaces.Box):\n            return space.shape\n        raise NotImplementedError()\n\n    def _parse_dtype(self, space):\n        """"""Get a tensor dtype from a OpenAI Gym space.\n\n    Args:\n      space: Gym space.\n\n    Returns:\n      TensorFlow data type.\n    """"""\n        if isinstance(space, gym.spaces.Discrete):\n            return tf.int32\n        if isinstance(space, gym.spaces.Box):\n            return tf.float32\n        raise NotImplementedError()\n'"
rex_gym/agents/tools/in_graph_env.py,21,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Put an OpenAI Gym environment into the TensorFlow graph.""""""\nimport gym\nimport tensorflow as tf\n\n\nclass InGraphEnv(object):\n    """"""Put an OpenAI Gym environment into the TensorFlow graph.\n\n  The environment will be stepped and reset inside of the graph using\n  tf.py_func(). The current observation, action, reward, and done flag are held\n  in according variables.\n  """"""\n\n    def __init__(self, env):\n        """"""Put an OpenAI Gym environment into the TensorFlow graph.\n\n    Args:\n      env: OpenAI Gym environment.\n    """"""\n        self._env = env\n        observ_shape = self._parse_shape(self._env.observation_space)\n        observ_dtype = self._parse_dtype(self._env.observation_space)\n        action_shape = self._parse_shape(self._env.action_space)\n        action_dtype = self._parse_dtype(self._env.action_space)\n        with tf.name_scope(\'environment\'):\n            self._observ = tf.Variable(tf.zeros(observ_shape, observ_dtype),\n                                       name=\'observ\',\n                                       trainable=False)\n            self._action = tf.Variable(tf.zeros(action_shape, action_dtype),\n                                       name=\'action\',\n                                       trainable=False)\n            self._reward = tf.Variable(0.0, dtype=tf.float32, name=\'reward\', trainable=False)\n            self._done = tf.Variable(True, dtype=tf.bool, name=\'done\', trainable=False)\n            self._step = tf.Variable(0, dtype=tf.int32, name=\'step\', trainable=False)\n\n    def __getattr__(self, name):\n        """"""Forward unimplemented attributes to the original environment.\n\n    Args:\n      name: Attribute that was accessed.\n\n    Returns:\n      Value behind the attribute name in the wrapped environment.\n    """"""\n        return getattr(self._env, name)\n\n    def simulate(self, action):\n        """"""Step the environment.\n\n    The result of the step can be accessed from the variables defined below.\n\n    Args:\n      action: Tensor holding the action to apply.\n\n    Returns:\n      Operation.\n    """"""\n        with tf.name_scope(\'environment/simulate\'):\n            if action.dtype in (tf.float16, tf.float32, tf.float64):\n                action = tf.check_numerics(action, \'action\')\n            observ_dtype = self._parse_dtype(self._env.observation_space)\n            observ, reward, done = tf.py_func(lambda a: self._env.step(a)[:3], [action],\n                                              [observ_dtype, tf.float32, tf.bool],\n                                              name=\'step\')\n            observ = tf.check_numerics(observ, \'observ\')\n            reward = tf.check_numerics(reward, \'reward\')\n            return tf.group(self._observ.assign(observ), self._action.assign(action),\n                            self._reward.assign(reward), self._done.assign(done),\n                            self._step.assign_add(1))\n\n    def reset(self):\n        """"""Reset the environment.\n\n    Returns:\n      Tensor of the current observation.\n    """"""\n        observ_dtype = self._parse_dtype(self._env.observation_space)\n        observ = tf.py_func(self._env.reset, [], observ_dtype, name=\'reset\')\n        observ = tf.check_numerics(observ, \'observ\')\n        with tf.control_dependencies(\n                [self._observ.assign(observ),\n                 self._reward.assign(0),\n                 self._done.assign(False)]):\n            return tf.identity(observ)\n\n    @property\n    def observ(self):\n        """"""Access the variable holding the current observation.""""""\n        return self._observ\n\n    @property\n    def action(self):\n        """"""Access the variable holding the last recieved action.""""""\n        return self._action\n\n    @property\n    def reward(self):\n        """"""Access the variable holding the current reward.""""""\n        return self._reward\n\n    @property\n    def done(self):\n        """"""Access the variable indicating whether the episode is done.""""""\n        return self._done\n\n    @property\n    def step(self):\n        """"""Access the variable containg total steps of this environment.""""""\n        return self._step\n\n    def _parse_shape(self, space):\n        """"""Get a tensor shape from a OpenAI Gym space.\n\n    Args:\n      space: Gym space.\n\n    Returns:\n      Shape tuple.\n    """"""\n        if isinstance(space, gym.spaces.Discrete):\n            return ()\n        if isinstance(space, gym.spaces.Box):\n            return space.shape\n        raise NotImplementedError()\n\n    def _parse_dtype(self, space):\n        """"""Get a tensor dtype from a OpenAI Gym space.\n\n    Args:\n      space: Gym space.\n\n    Returns:\n      TensorFlow data type.\n    """"""\n        if isinstance(space, gym.spaces.Discrete):\n            return tf.int32\n        if isinstance(space, gym.spaces.Box):\n            return tf.float32\n        raise NotImplementedError()\n'"
rex_gym/agents/tools/loop.py,20,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Execute operations in a loop and coordinate logging and checkpoints.""""""\nimport collections\nimport os\n\nimport tensorflow as tf\n\nfrom . import streaming_mean\n\n_Phase = collections.namedtuple(\n    \'Phase\', \'name, writer, op, batch, steps, feed, report_every, log_every,\'\n             \'checkpoint_every\')\n\n\nclass Loop(object):\n    """"""Execute operations in a loop and coordinate logging and checkpoints.\n\n  Supports multiple phases, that define their own operations to run, and\n  intervals for reporting scores, logging summaries, and storing checkpoints.\n  All class state is stored in-graph to properly recover from checkpoints.\n  """"""\n\n    def __init__(self, logdir, step=None, log=None, report=None, reset=None):\n        """"""Execute operations in a loop and coordinate logging and checkpoints.\n\n    The step, log, report, and report arguments will get created if not\n    provided. Reset is used to indicate switching to a new phase, so that the\n    model can start a new computation in case its computation is split over\n    multiple training steps.\n\n    Args:\n      logdir: Will contain checkpoints and summaries for each phase.\n      step: Variable of the global step (optional).\n      log: Tensor indicating to the model to compute summary tensors.\n      report: Tensor indicating to the loop to report the current mean score.\n      reset: Tensor indicating to the model to start a new computation.\n    """"""\n        self._logdir = logdir\n        self._step = (tf.Variable(0, False, name=\'global_step\') if step is None else step)\n        self._log = tf.placeholder(tf.bool) if log is None else log\n        self._report = tf.placeholder(tf.bool) if report is None else report\n        self._reset = tf.placeholder(tf.bool) if reset is None else reset\n        self._phases = []\n\n    def add_phase(self,\n                  name,\n                  done,\n                  score,\n                  summary,\n                  steps,\n                  report_every=None,\n                  log_every=None,\n                  checkpoint_every=None,\n                  feed=None):\n        """"""Add a phase to the loop protocol.\n\n    If the model breaks long computation into multiple steps, the done tensor\n    indicates whether the current score should be added to the mean counter.\n    For example, in reinforcement learning we only have a valid score at the\n    end of the episode.\n\n    Score and done tensors can either be scalars or vectors, to support\n    single and batched computations.\n\n    Args:\n      name: Name for the phase, used for the summary writer.\n      done: Tensor indicating whether current score can be used.\n      score: Tensor holding the current, possibly intermediate, score.\n      summary: Tensor holding summary string to write if not an empty string.\n      steps: Duration of the phase in steps.\n      report_every: Yield mean score every this number of steps.\n      log_every: Request summaries via `log` tensor every this number of steps.\n      checkpoint_every: Write checkpoint every this number of steps.\n      feed: Additional feed dictionary for the session run call.\n\n    Raises:\n      ValueError: Unknown rank for done or score tensors.\n    """"""\n        done = tf.convert_to_tensor(done, tf.bool)\n        score = tf.convert_to_tensor(score, tf.float32)\n        summary = tf.convert_to_tensor(summary, tf.string)\n        feed = feed or {}\n        if done.shape.ndims is None or score.shape.ndims is None:\n            raise ValueError(""Rank of \'done\' and \'score\' tensors must be known."")\n        writer = self._logdir and tf.summary.FileWriter(\n            os.path.join(self._logdir, name), tf.get_default_graph(), flush_secs=60)\n        op = self._define_step(done, score, summary)\n        batch = 1 if score.shape.ndims == 0 else score.shape[0].value\n        self._phases.append(\n            _Phase(name, writer, op, batch, int(steps), feed, report_every, log_every,\n                   checkpoint_every))\n\n    def run(self, sess, saver, max_step=None):\n        """"""Run the loop schedule for a specified number of steps.\n\n    Call the operation of the current phase until the global step reaches the\n    specified maximum step. Phases are repeated over and over in the order they\n    were added.\n\n    Args:\n      sess: Session to use to run the phase operation.\n      saver: Saver used for checkpointing.\n      max_step: Run the operations until the step reaches this limit.\n\n    Yields:\n      Reported mean scores.\n    """"""\n        global_step = sess.run(self._step)\n        steps_made = 1\n        while True:\n            if max_step and global_step >= max_step:\n                break\n            phase, epoch, steps_in = self._find_current_phase(global_step)\n            phase_step = epoch * phase.steps + steps_in\n            if steps_in % phase.steps < steps_made:\n                message = \'\\n\' + (\'-\' * 50) + \'\\n\'\n                message += \'Phase {} (phase step {}, global step {}).\'\n                tf.logging.info(message.format(phase.name, phase_step, global_step))\n            # Populate book keeping tensors.\n            phase.feed[self._reset] = (steps_in < steps_made)\n            phase.feed[self._log] = (phase.writer and\n                                     self._is_every_steps(phase_step, phase.batch, phase.log_every))\n            phase.feed[self._report] = (self._is_every_steps(phase_step, phase.batch,\n                                                             phase.report_every))\n            summary, mean_score, global_step, steps_made = sess.run(phase.op, phase.feed)\n            if self._is_every_steps(phase_step, phase.batch, phase.checkpoint_every):\n                self._store_checkpoint(sess, saver, global_step)\n            if self._is_every_steps(phase_step, phase.batch, phase.report_every):\n                yield mean_score\n            if summary and phase.writer:\n                # We want smaller phases to catch up at the beginnig of each epoch so\n                # that their graphs are aligned.\n                longest_phase = max(phase.steps for phase in self._phases)\n                summary_step = epoch * longest_phase + steps_in\n                phase.writer.add_summary(summary, summary_step)\n\n    @staticmethod\n    def _is_every_steps(phase_step, batch, every):\n        """"""Determine whether a periodic event should happen at this step.\n\n    Args:\n      phase_step: The incrementing step.\n      batch: The number of steps progressed at once.\n      every: The interval of the periode.\n\n    Returns:\n      Boolean of whether the event should happen.\n    """"""\n        if not every:\n            return False\n        covered_steps = range(phase_step, phase_step + batch)\n        return any((step + 1) % every == 0 for step in covered_steps)\n\n    def _find_current_phase(self, global_step):\n        """"""Determine the current phase based on the global step.\n\n    This ensures continuing the correct phase after restoring checkoints.\n\n    Args:\n      global_step: The global number of steps performed across all phases.\n\n    Returns:\n      Tuple of phase object, epoch number, and phase steps within the epoch.\n    """"""\n        epoch_size = sum(phase.steps for phase in self._phases)\n        epoch = int(global_step // epoch_size)\n        steps_in = global_step % epoch_size\n        for phase in self._phases:\n            if steps_in < phase.steps:\n                return phase, epoch, steps_in\n            steps_in -= phase.steps\n\n    def _define_step(self, done, score, summary):\n        """"""Combine operations of a phase.\n\n    Keeps track of the mean score and when to report it.\n\n    Args:\n      done: Tensor indicating whether current score can be used.\n      score: Tensor holding the current, possibly intermediate, score.\n      summary: Tensor holding summary string to write if not an empty string.\n\n    Returns:\n      Tuple of summary tensor, mean score, and new global step. The mean score\n      is zero for non reporting steps.\n    """"""\n        if done.shape.ndims == 0:\n            done = done[None]\n        if score.shape.ndims == 0:\n            score = score[None]\n        score_mean = streaming_mean.StreamingMean((), tf.float32)\n        with tf.control_dependencies([done, score, summary]):\n            done_score = tf.gather(score, tf.where(done)[:, 0])\n            submit_score = tf.cond(tf.reduce_any(done), lambda: score_mean.submit(done_score), tf.no_op)\n        with tf.control_dependencies([submit_score]):\n            mean_score = tf.cond(self._report, score_mean.clear, float)\n            steps_made = tf.shape(score)[0]\n            next_step = self._step.assign_add(steps_made)\n        with tf.control_dependencies([mean_score, next_step]):\n            return tf.identity(summary), mean_score, next_step, steps_made\n\n    def _store_checkpoint(self, sess, saver, global_step):\n        """"""Store a checkpoint if a log directory was provided to the constructor.\n\n    The directory will be created if needed.\n\n    Args:\n      sess: Session containing variables to store.\n      saver: Saver used for checkpointing.\n      global_step: Step number of the checkpoint name.\n    """"""\n        if not self._logdir or not saver:\n            return\n        tf.gfile.MakeDirs(self._logdir)\n        filename = os.path.join(self._logdir, \'model.ckpt\')\n        saver.save(sess, filename, global_step)\n'"
rex_gym/agents/tools/mock_algorithm.py,5,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Mock algorithm for testing reinforcement learning code.""""""\nimport tensorflow as tf\n\n\nclass MockAlgorithm(object):\n    """"""Produce random actions and empty summaries.""""""\n\n    def __init__(self, envs):\n        """"""Produce random actions and empty summaries.\n\n    Args:\n      envs: List of in-graph environments.\n    """"""\n        self._envs = envs\n\n    def begin_episode(self, unused_agent_indices):\n        return tf.constant(\'\')\n\n    def perform(self, unused_observ):\n        shape = (len(self._envs),) + self._envs[0].action_space.shape\n        low = self._envs[0].action_space.low\n        high = self._envs[0].action_space.high\n        action = tf.random_uniform(shape) * (high - low) + low\n        return action, tf.constant(\'\')\n\n    def experience(self, *unused_transition):\n        return tf.constant(\'\')\n\n    def end_episode(self, unused_agent_indices):\n        return tf.constant(\'\')\n'"
rex_gym/agents/tools/mock_environment.py,0,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Mock environment for testing reinforcement learning code.""""""\nimport gym\nimport gym.spaces\nimport numpy as np\n\n\nclass MockEnvironment(object):\n    """"""Generate random agent input and keep track of statistics.""""""\n\n    def __init__(self, observ_shape, action_shape, min_duration, max_duration):\n        """"""Generate random agent input and keep track of statistics.\n\n    Args:\n      observ_shape: Shape for the random observations.\n      action_shape: Shape for the action space.\n      min_duration: Minimum number of steps per episode.\n      max_duration: Maximum number of steps per episode.\n\n    Attributes:\n      steps: List of actual simulated lengths for all episodes.\n      durations: List of decided lengths for all episodes.\n    """"""\n        self._observ_shape = observ_shape\n        self._action_shape = action_shape\n        self._min_duration = min_duration\n        self._max_duration = max_duration\n        self._random = np.random.RandomState(0)\n        self.steps = []\n        self.durations = []\n\n    @property\n    def observation_space(self):\n        low = np.zeros(self._observ_shape)\n        high = np.ones(self._observ_shape)\n        return gym.spaces.Box(low, high)\n\n    @property\n    def action_space(self):\n        low = np.zeros(self._action_shape)\n        high = np.ones(self._action_shape)\n        return gym.spaces.Box(low, high)\n\n    @property\n    def unwrapped(self):\n        return self\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n        assert self.steps[-1] < self.durations[-1]\n        self.steps[-1] += 1\n        observ = self._current_observation()\n        reward = self._current_reward()\n        done = self.steps[-1] >= self.durations[-1]\n        info = {}\n        return observ, reward, done, info\n\n    def reset(self):\n        duration = self._random.randint(self._min_duration, self._max_duration + 1)\n        self.steps.append(0)\n        self.durations.append(duration)\n        return self._current_observation()\n\n    def _current_observation(self):\n        return self._random.uniform(0, 1, self._observ_shape)\n\n    def _current_reward(self):\n        return self._random.uniform(-1, 1)\n'"
rex_gym/agents/tools/simulate.py,39,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""In-graph simulation step of a vecrotized algorithm with environments.""""""\nimport tensorflow as tf\n\nfrom . import streaming_mean\n\n\ndef simulate(batch_env, algo, log=True, reset=False):\n    """"""Simulation step of a vecrotized algorithm with in-graph environments.\n\n  Integrates the operations implemented by the algorithm and the environments\n  into a combined operation.\n\n  Args:\n    batch_env: In-graph batch environment.\n    algo: Algorithm instance implementing required operations.\n    log: Tensor indicating whether to compute and return summaries.\n    reset: Tensor causing all environments to reset.\n\n  Returns:\n    Tuple of tensors containing done flags for the current episodes, possibly\n    intermediate scores for the episodes, and a summary tensor.\n  """"""\n\n    def _define_begin_episode(agent_indices):\n        """"""Reset environments, intermediate scores and durations for new episodes.\n\n    Args:\n      agent_indices: Tensor containing batch indices starting an episode.\n\n    Returns:\n      Summary tensor.\n    """"""\n        assert agent_indices.shape.ndims == 1\n        zero_scores = tf.zeros_like(agent_indices, tf.float32)\n        zero_durations = tf.zeros_like(agent_indices)\n        reset_ops = [\n            batch_env.reset(agent_indices),\n            tf.scatter_update(score, agent_indices, zero_scores),\n            tf.scatter_update(length, agent_indices, zero_durations)\n        ]\n        with tf.control_dependencies(reset_ops):\n            return algo.begin_episode(agent_indices)\n\n    def _define_step():\n        """"""Request actions from the algorithm and apply them to the environments.\n\n    Increments the lengths of all episodes and increases their scores by the\n    current reward. After stepping the environments, provides the full\n    transition tuple to the algorithm.\n\n    Returns:\n      Summary tensor.\n    """"""\n        prevob = batch_env.observ + 0  # Ensure a copy of the variable value.\n        action, step_summary = algo.perform(prevob)\n        action.set_shape(batch_env.action.shape)\n        with tf.control_dependencies([batch_env.simulate(action)]):\n            add_score = score.assign_add(batch_env.reward)\n            inc_length = length.assign_add(tf.ones(len(batch_env), tf.int32))\n        with tf.control_dependencies([add_score, inc_length]):\n            experience_summary = algo.experience(prevob, batch_env.action, batch_env.reward,\n                                                 batch_env.done, batch_env.observ)\n        return tf.summary.merge([step_summary, experience_summary])\n\n    def _define_end_episode(agent_indices):\n        """"""Notify the algorithm of ending episodes.\n\n    Also updates the mean score and length counters used for summaries.\n\n    Args:\n      agent_indices: Tensor holding batch indices that end their episodes.\n\n    Returns:\n      Summary tensor.\n    """"""\n        assert agent_indices.shape.ndims == 1\n        submit_score = mean_score.submit(tf.gather(score, agent_indices))\n        submit_length = mean_length.submit(tf.cast(tf.gather(length, agent_indices), tf.float32))\n        with tf.control_dependencies([submit_score, submit_length]):\n            return algo.end_episode(agent_indices)\n\n    def _define_summaries():\n        """"""Reset the average score and duration, and return them as summary.\n\n    Returns:\n      Summary string.\n    """"""\n        score_summary = tf.cond(tf.logical_and(log, tf.cast(\n            mean_score.count, tf.bool)), lambda: tf.summary.scalar(\'mean_score\', mean_score.clear()),\n                                str)\n        length_summary = tf.cond(tf.logical_and(\n            log, tf.cast(mean_length.count,\n                         tf.bool)), lambda: tf.summary.scalar(\'mean_length\', mean_length.clear()), str)\n        return tf.summary.merge([score_summary, length_summary])\n\n    with tf.name_scope(\'simulate\'):\n        log = tf.convert_to_tensor(log)\n        reset = tf.convert_to_tensor(reset)\n        with tf.variable_scope(\'simulate_temporary\'):\n            score = tf.Variable(tf.zeros(len(batch_env), dtype=tf.float32), False, name=\'score\')\n            length = tf.Variable(tf.zeros(len(batch_env), dtype=tf.int32), False, name=\'length\')\n        mean_score = streaming_mean.StreamingMean((), tf.float32)\n        mean_length = streaming_mean.StreamingMean((), tf.float32)\n        agent_indices = tf.cond(reset, lambda: tf.range(len(batch_env)), lambda: tf.cast(\n            tf.where(batch_env.done)[:, 0], tf.int32))\n        begin_episode = tf.cond(tf.cast(tf.shape(agent_indices)[0],\n                                        tf.bool), lambda: _define_begin_episode(agent_indices), str)\n        with tf.control_dependencies([begin_episode]):\n            step = _define_step()\n        with tf.control_dependencies([step]):\n            agent_indices = tf.cast(tf.where(batch_env.done)[:, 0], tf.int32)\n            end_episode = tf.cond(tf.cast(tf.shape(agent_indices)[0],\n                                          tf.bool), lambda: _define_end_episode(agent_indices), str)\n        with tf.control_dependencies([end_episode]):\n            summary = tf.summary.merge([_define_summaries(), begin_episode, step, end_episode])\n        with tf.control_dependencies([summary]):\n            done, score = tf.identity(batch_env.done), tf.identity(score)\n        return done, score, summary\n'"
rex_gym/agents/tools/streaming_mean.py,10,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Compute a streaming estimation of the mean of submitted tensors.""""""\nimport tensorflow as tf\n\n\nclass StreamingMean(object):\n    """"""Compute a streaming estimation of the mean of submitted tensors.""""""\n\n    def __init__(self, shape, dtype):\n        """"""Specify the shape and dtype of the mean to be estimated.\n\n    Note that a float mean to zero submitted elements is NaN, while computing\n    the integer mean of zero elements raises a division by zero error.\n\n    Args:\n      shape: Shape of the mean to compute.\n      dtype: Data type of the mean to compute.\n    """"""\n        self._dtype = dtype\n        self._sum = tf.Variable(lambda: tf.zeros(shape, dtype), False)\n        self._count = tf.Variable(lambda: 0, trainable=False)\n\n    @property\n    def value(self):\n        """"""The current value of the mean.""""""\n        return self._sum / tf.cast(self._count, self._dtype)\n\n    @property\n    def count(self):\n        """"""The number of submitted samples.""""""\n        return self._count\n\n    def submit(self, value):\n        """"""Submit a single or batch tensor to refine the streaming mean.""""""\n        # Add a batch dimension if necessary.\n        if value.shape.ndims == self._sum.shape.ndims:\n            value = value[None, ...]\n        return tf.group(self._sum.assign_add(tf.reduce_sum(value, 0)),\n                        self._count.assign_add(tf.shape(value)[0]))\n\n    def clear(self):\n        """"""Return the mean estimate and reset the streaming statistics.""""""\n        value = self._sum / tf.cast(self._count, self._dtype)\n        with tf.control_dependencies([value]):\n            reset_value = self._sum.assign(tf.zeros_like(self._sum))\n            reset_count = self._count.assign(0)\n        with tf.control_dependencies([reset_value, reset_count]):\n            return tf.identity(value)\n'"
rex_gym/agents/tools/wrappers.py,4,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Wrappers for OpenAI Gym environments.""""""\nimport atexit\nimport functools\nimport multiprocessing\nimport sys\nimport traceback\n\nimport gym\nimport gym.spaces\nimport numpy as np\nimport tensorflow as tf\n\n\nclass AutoReset(object):\n    """"""Automatically reset environment when the episode is done.""""""\n\n    def __init__(self, env):\n        self._env = env\n        self._done = True\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    def step(self, action):\n        if self._done:\n            observ, reward, done, info = self._env.reset(), 0.0, False, {}\n        else:\n            observ, reward, done, info = self._env.step(action)\n        self._done = done\n        return observ, reward, done, info\n\n    def reset(self):\n        self._done = False\n        return self._env.reset()\n\n\nclass ActionRepeat(object):\n    """"""Repeat the agent action multiple steps.""""""\n\n    def __init__(self, env, amount):\n        self._env = env\n        self._amount = amount\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    def step(self, action):\n        done = False\n        total_reward = 0\n        current_step = 0\n        while current_step < self._amount and not done:\n            observ, reward, done, info = self._env.step(action)\n            total_reward += reward\n            current_step += 1\n        return observ, total_reward, done, info\n\n\nclass RandomStart(object):\n    """"""Perform random number of random actions at the start of the episode.""""""\n\n    def __init__(self, env, max_steps):\n        self._env = env\n        self._max_steps = max_steps\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    def reset(self):\n        observ = self._env.reset()\n        random_steps = np.random.randint(0, self._max_steps)\n        for _ in range(random_steps):\n            action = self._env.action_space.sample()\n            observ, unused_reward, done, unused_info = self._env.step(action)\n            if done:\n                tf.logging.warning(\'Episode ended during random start.\')\n                return self.reset()\n        return observ\n\n\nclass FrameHistory(object):\n    """"""Augment the observation with past observations.""""""\n\n    def __init__(self, env, past_indices, flatten):\n        """"""Augment the observation with past observations.\n\n    Implemented as a Numpy ring buffer holding the necessary past observations.\n\n    Args:\n      env: OpenAI Gym environment to wrap.\n      past_indices: List of non-negative integers indicating the time offsets\n        from the current time step of observations to include.\n      flatten: Concatenate the past observations rather than stacking them.\n\n    Raises:\n      KeyError: The current observation is not included in the indices.\n    """"""\n        if 0 not in past_indices:\n            raise KeyError(\'Past indices should include 0 for the current frame.\')\n        self._env = env\n        self._past_indices = past_indices\n        self._step = 0\n        self._buffer = None\n        self._capacity = max(past_indices)\n        self._flatten = flatten\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    @property\n    def observation_space(self):\n        low = self._env.observation_space.low\n        high = self._env.observation_space.high\n        low = np.repeat(low[None, ...], len(self._past_indices), 0)\n        high = np.repeat(high[None, ...], len(self._past_indices), 0)\n        if self._flatten:\n            low = np.reshape(low, (-1,) + low.shape[2:])\n            high = np.reshape(high, (-1,) + high.shape[2:])\n        return gym.spaces.Box(low, high)\n\n    def step(self, action):\n        observ, reward, done, info = self._env.step(action)\n        self._step += 1\n        self._buffer[self._step % self._capacity] = observ\n        observ = self._select_frames()\n        return observ, reward, done, info\n\n    def reset(self):\n        observ = self._env.reset()\n        self._buffer = np.repeat(observ[None, ...], self._capacity, 0)\n        self._step = 0\n        return self._select_frames()\n\n    def _select_frames(self):\n        indices = [(self._step - index) % self._capacity for index in self._past_indices]\n        observ = self._buffer[indices]\n        if self._flatten:\n            observ = np.reshape(observ, (-1,) + observ.shape[2:])\n        return observ\n\n\nclass FrameDelta(object):\n    """"""Convert the observation to a difference from the previous observation.""""""\n\n    def __init__(self, env):\n        self._env = env\n        self._last = None\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    @property\n    def observation_space(self):\n        low = self._env.observation_space.low\n        high = self._env.observation_space.high\n        low, high = low - high, high - low\n        return gym.spaces.Box(low, high)\n\n    def step(self, action):\n        observ, reward, done, info = self._env.step(action)\n        delta = observ - self._last\n        self._last = observ\n        return delta, reward, done, info\n\n    def reset(self):\n        observ = self._env.reset()\n        self._last = observ\n        return observ\n\n\nclass RangeNormalize(object):\n    """"""Normalize the specialized observation and action ranges to [-1, 1].""""""\n\n    def __init__(self, env, observ=None, action=None):\n        self._env = env\n        self._should_normalize_observ = (observ is not False and\n                                         self._is_finite(self._env.observation_space))\n        if observ is True and not self._should_normalize_observ:\n            raise ValueError(\'Cannot normalize infinite observation range.\')\n        if observ is None and not self._should_normalize_observ:\n            tf.logging.info(\'Not normalizing infinite observation range.\')\n        self._should_normalize_action = (action is not False and\n                                         self._is_finite(self._env.action_space))\n        if action is True and not self._should_normalize_action:\n            raise ValueError(\'Cannot normalize infinite action range.\')\n        if action is None and not self._should_normalize_action:\n            tf.logging.info(\'Not normalizing infinite action range.\')\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    @property\n    def observation_space(self):\n        space = self._env.observation_space\n        if not self._should_normalize_observ:\n            return space\n        return gym.spaces.Box(-np.ones(space.shape), np.ones(space.shape))\n\n    @property\n    def action_space(self):\n        space = self._env.action_space\n        if not self._should_normalize_action:\n            return space\n        return gym.spaces.Box(-np.ones(space.shape), np.ones(space.shape))\n\n    def step(self, action):\n        if self._should_normalize_action:\n            action = self._denormalize_action(action)\n        observ, reward, done, info = self._env.step(action)\n        if self._should_normalize_observ:\n            observ = self._normalize_observ(observ)\n        return observ, reward, done, info\n\n    def reset(self):\n        observ = self._env.reset()\n        if self._should_normalize_observ:\n            observ = self._normalize_observ(observ)\n        return observ\n\n    def _denormalize_action(self, action):\n        min_ = self._env.action_space.low\n        max_ = self._env.action_space.high\n        action = (action + 1) / 2 * (max_ - min_) + min_\n        return action\n\n    def _normalize_observ(self, observ):\n        min_ = self._env.observation_space.low\n        max_ = self._env.observation_space.high\n        observ = 2 * (observ - min_) / (max_ - min_) - 1\n        return observ\n\n    def _is_finite(self, space):\n        return np.isfinite(space.low).all() and np.isfinite(space.high).all()\n\n\nclass ClipAction(object):\n    """"""Clip out of range actions to the action space of the environment.""""""\n\n    def __init__(self, env):\n        self._env = env\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    @property\n    def action_space(self):\n        shape = self._env.action_space.shape\n        return gym.spaces.Box(-np.inf * np.ones(shape), np.inf * np.ones(shape))\n\n    def step(self, action):\n        action_space = self._env.action_space\n        action = np.clip(action, action_space.low, action_space.high)\n        return self._env.step(action)\n\n\nclass LimitDuration(object):\n    """"""End episodes after specified number of steps.""""""\n\n    def __init__(self, env, duration):\n        self._env = env\n        self._duration = duration\n        self._step = None\n\n    def __getattr__(self, name):\n        return getattr(self._env, name)\n\n    def step(self, action):\n        if self._step is None:\n            raise RuntimeError(\'Must reset environment.\')\n        observ, reward, done, info = self._env.step(action)\n        self._step += 1\n        if self._step >= self._duration:\n            done = True\n            self._step = None\n        return observ, reward, done, info\n\n    def reset(self):\n        self._step = 0\n        return self._env.reset()\n\n\nclass ExternalProcess(object):\n    """"""Step environment in a separate process for lock free paralellism.""""""\n\n    # Message types for communication via the pipe.\n    _ACTION = 1\n    _RESET = 2\n    _CLOSE = 3\n    _ATTRIBUTE = 4\n    _TRANSITION = 5\n    _OBSERV = 6\n    _EXCEPTION = 7\n    _VALUE = 8\n\n    def __init__(self, constructor):\n        """"""Step environment in a separate process for lock free paralellism.\n\n    The environment will be created in the external process by calling the\n    specified callable. This can be an environment class, or a function\n    creating the environment and potentially wrapping it. The returned\n    environment should not access global variables.\n\n    Args:\n      constructor: Callable that creates and returns an OpenAI gym environment.\n\n    Attributes:\n      observation_space: The cached observation space of the environment.\n      action_space: The cached action space of the environment.\n    """"""\n        self._conn, conn = multiprocessing.Pipe()\n        self._process = multiprocessing.Process(target=self._worker, args=(constructor, conn))\n        atexit.register(self.close)\n        self._process.start()\n        self._observ_space = None\n        self._action_space = None\n\n    @property\n    def observation_space(self):\n        if not self._observ_space:\n            self._observ_space = self.__getattr__(\'observation_space\')\n        return self._observ_space\n\n    @property\n    def action_space(self):\n        if not self._action_space:\n            self._action_space = self.__getattr__(\'action_space\')\n        return self._action_space\n\n    def __getattr__(self, name):\n        """"""Request an attribute from the environment.\n\n    Note that this involves communication with the external process, so it can\n    be slow.\n\n    Args:\n      name: Attribute to access.\n\n    Returns:\n      Value of the attribute.\n    """"""\n        self._conn.send((self._ATTRIBUTE, name))\n        return self._receive(self._VALUE)\n\n    def step(self, action, blocking=True):\n        """"""Step the environment.\n\n    Args:\n      action: The action to apply to the environment.\n      blocking: Whether to wait for the result.\n\n    Returns:\n      Transition tuple when blocking, otherwise callable that returns the\n      transition tuple.\n    """"""\n        self._conn.send((self._ACTION, action))\n        if blocking:\n            return self._receive(self._TRANSITION)\n        else:\n            return functools.partial(self._receive, self._TRANSITION)\n\n    def reset(self, blocking=True):\n        """"""Reset the environment.\n\n    Args:\n      blocking: Whether to wait for the result.\n\n    Returns:\n      New observation when blocking, otherwise callable that returns the new\n      observation.\n    """"""\n        self._conn.send((self._RESET, None))\n        if blocking:\n            return self._receive(self._OBSERV)\n        else:\n            return functools.partial(self._receive, self._OBSERV)\n\n    def close(self):\n        """"""Send a close message to the external process and join it.""""""\n        try:\n            self._conn.send((self._CLOSE, None))\n            self._conn.close()\n        except IOError:\n            # The connection was already closed.\n            pass\n        self._process.join()\n\n    def _receive(self, expected_message):\n        """"""Wait for a message from the worker process and return its payload.\n\n    Args:\n      expected_message: Type of the expected message.\n\n    Raises:\n      Exception: An exception was raised inside the worker process.\n      KeyError: The reveived message is not of the expected type.\n\n    Returns:\n      Payload object of the message.\n    """"""\n        message, payload = self._conn.recv()\n        # Re-raise exceptions in the main process.\n        if message == self._EXCEPTION:\n            stacktrace = payload\n            raise Exception(stacktrace)\n        if message == expected_message:\n            return payload\n        raise KeyError(\'Received message of unexpected type {}\'.format(message))\n\n    def _worker(self, constructor, conn):\n        """"""The process waits for actions and sends back environment results.\n\n    Args:\n      constructor: Constructor for the OpenAI Gym environment.\n      conn: Connection for communication to the main process.\n    """"""\n        try:\n            env = constructor()\n            while True:\n                try:\n                    # Only block for short times to have keyboard exceptions be raised.\n                    if not conn.poll(0.1):\n                        continue\n                    message, payload = conn.recv()\n                except (EOFError, KeyboardInterrupt):\n                    break\n                if message == self._ACTION:\n                    action = payload\n                    conn.send((self._TRANSITION, env.step(action)))\n                    continue\n                if message == self._RESET:\n                    assert payload is None\n                    conn.send((self._OBSERV, env.reset()))\n                    continue\n                if message == self._ATTRIBUTE:\n                    name = payload\n                    conn.send((self._VALUE, getattr(env, name)))\n                    continue\n                if message == self._CLOSE:\n                    assert payload is None\n                    break\n                raise KeyError(\'Received message of unknown type {}\'.format(message))\n        except Exception:  # pylint: disable=broad-except\n            stacktrace = \'\'.join(traceback.format_exception(*sys.exc_info()))\n            conn.send((self._EXCEPTION, stacktrace))\n            tf.logging.error(\'Error in environment process: {}\'.format(stacktrace))\n        conn.close()\n\n\nclass ConvertTo32Bit(object):\n    """"""Convert data types of an OpenAI Gym environment to 32 bit.""""""\n\n    def __init__(self, env):\n        """"""Convert data types of an OpenAI Gym environment to 32 bit.\n\n    Args:\n      env: OpenAI Gym environment.\n    """"""\n        self._env = env\n\n    def __getattr__(self, name):\n        """"""Forward unimplemented attributes to the original environment.\n\n    Args:\n      name: Attribute that was accessed.\n\n    Returns:\n      Value behind the attribute name in the wrapped environment.\n    """"""\n        return getattr(self._env, name)\n\n    def step(self, action):\n        """"""Forward action to the wrapped environment.\n\n    Args:\n      action: Action to apply to the environment.\n\n    Raises:\n      ValueError: Invalid action.\n\n    Returns:\n      Converted observation, converted reward, done flag, and info object.\n    """"""\n        observ, reward, done, info = self._env.step(action)\n        observ = self._convert_observ(observ)\n        reward = self._convert_reward(reward)\n        return observ, reward, done, info\n\n    def reset(self):\n        """"""Reset the environment and convert the resulting observation.\n\n    Returns:\n      Converted observation.\n    """"""\n        observ = self._env.reset()\n        observ = self._convert_observ(observ)\n        return observ\n\n    def _convert_observ(self, observ):\n        """"""Convert the observation to 32 bits.\n\n    Args:\n      observ: Numpy observation.\n\n    Raises:\n      ValueError: Observation contains infinite values.\n\n    Returns:\n      Numpy observation with 32-bit data type.\n    """"""\n        if not np.isfinite(observ).all():\n            raise ValueError(\'Infinite observation encountered.\')\n        if observ.dtype == np.float64:\n            return observ.astype(np.float32)\n        if observ.dtype == np.int64:\n            return observ.astype(np.int32)\n        return observ\n\n    def _convert_reward(self, reward):\n        """"""Convert the reward to 32 bits.\n\n    Args:\n      reward: Numpy reward.\n\n    Raises:\n      ValueError: Rewards contain infinite values.\n\n    Returns:\n      Numpy reward with 32-bit data type.\n    """"""\n        if not np.isfinite(reward).all():\n            raise ValueError(\'Infinite reward encountered.\')\n        return np.array(reward, dtype=np.float32)\n'"
rex_gym/agents/tools/wrappers_test.py,2,"b'# Copyright 2017 The TensorFlow Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for environment wrappers.""""""\nimport functools\n\nimport tensorflow as tf\n\nfrom . import wrappers\nfrom .mock_environment import MockEnvironment\n\n\nclass ExternalProcessTest(tf.test.TestCase):\n\n    def test_close_no_hang_after_init(self):\n        constructor = functools.partial(MockEnvironment,\n                                        observ_shape=(2, 3),\n                                        action_shape=(2,),\n                                        min_duration=2,\n                                        max_duration=2)\n        env = wrappers.ExternalProcess(constructor)\n        env.close()\n\n    def test_close_no_hang_after_step(self):\n        constructor = functools.partial(MockEnvironment,\n                                        observ_shape=(2, 3),\n                                        action_shape=(2,),\n                                        min_duration=5,\n                                        max_duration=5)\n        env = wrappers.ExternalProcess(constructor)\n        env.reset()\n        env.step(env.action_space.sample())\n        env.step(env.action_space.sample())\n        env.close()\n\n    def test_reraise_exception_in_init(self):\n        constructor = MockEnvironmentCrashInInit\n        env = wrappers.ExternalProcess(constructor)\n        with self.assertRaises(Exception):\n            env.step(env.action_space.sample())\n\n    def test_reraise_exception_in_step(self):\n        constructor = functools.partial(MockEnvironmentCrashInStep, crash_at_step=3)\n        env = wrappers.ExternalProcess(constructor)\n        env.reset()\n        env.step(env.action_space.sample())\n        env.step(env.action_space.sample())\n        with self.assertRaises(Exception):\n            env.step(env.action_space.sample())\n\n\nclass MockEnvironmentCrashInInit(object):\n    """"""Raise an error when instantiated.""""""\n\n    def __init__(self, *unused_args, **unused_kwargs):\n        raise RuntimeError()\n\n\nclass MockEnvironmentCrashInStep(MockEnvironment):\n    """"""Raise an error after specified number of steps in an episode.""""""\n\n    def __init__(self, crash_at_step):\n        super(MockEnvironmentCrashInStep, self).__init__(observ_shape=(2, 3),\n                                                         action_shape=(2,),\n                                                         min_duration=crash_at_step + 1,\n                                                         max_duration=crash_at_step + 1)\n        self._crash_at_step = crash_at_step\n\n    def step(self, *args, **kwargs):\n        transition = super(MockEnvironmentCrashInStep, self).step(*args, **kwargs)\n        if self.steps[-1] == self._crash_at_step:\n            raise RuntimeError()\n        return transition\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
rex_gym/envs/gym/__init__.py,0,b'\n'
rex_gym/envs/gym/galloping_env.py,0,"b'""""""This file implements the gym environment of Rex.\n\n""""""\nimport collections\nimport math\nfrom gym import spaces\nimport numpy as np\n\nfrom .. import rex_gym_env\n\n# Radiant\nfrom ...model.rex import Rex\n\nINIT_SHOULDER_POS = 0.0\n# -math.pi / 5\nINIT_LEG_POS = -0.658319\n# math.pi / 3\nINIT_FOOT_POS = 1.0472\nNUM_LEGS = 4\nNUM_MOTORS = 3 * NUM_LEGS\n\nRexPose = collections.namedtuple(\n    ""RexPose"", ""shoulder_angle_1, leg_angle_1, foot_angle_1, ""\n               ""shoulder_angle_2, leg_angle_2, foot_angle_2, shoulder_angle_3, leg_angle_3, foot_angle_3,""\n               ""shoulder_angle_4, leg_angle_4, foot_angle_4"")\n\n\nclass RexReactiveEnv(rex_gym_env.RexGymEnv):\n    """"""The gym environment for Rex.\n\n  It simulates the locomotion of Rex, a quadruped robot. The state space\n  include the angles, velocities and torques for all the motors and the action\n  space is the desired motor angle for each motor. The reward function is based\n  on how far Rex walks in 1000 steps and penalizes the energy\n  expenditure.\n\n  """"""\n    metadata = {""render.modes"": [""human"", ""rgb_array""], ""video.frames_per_second"": 166}\n\n    def __init__(self,\n                 urdf_version=None,\n                 energy_weight=0.005,\n                 control_time_step=0.006,\n                 action_repeat=6,\n                 control_latency=0.02,\n                 pd_latency=0.003,\n                 on_rack=False,\n                 motor_kp=1.0,\n                 motor_kd=0.015,\n                 remove_default_joint_damping=True,\n                 render=False,\n                 num_steps_to_log=1000,\n                 accurate_motor_model_enabled=True,\n                 use_angle_in_observation=True,\n                 hard_reset=False,\n                 env_randomizer=None,\n                 log_path=None):\n        """"""Initialize Rex trotting gym environment.\n\n    Args:\n      urdf_version: [DEFAULT_URDF_VERSION] are allowable\n        versions. If None, DEFAULT_URDF_VERSION is used. Refer to\n        rex_gym_env for more details.\n      energy_weight: The weight of the energy term in the reward function. Refer\n        to rex_gym_env for more details.\n      control_time_step: The time step between two successive control signals.\n      action_repeat: The number of simulation steps that an action is repeated.\n      control_latency: The latency between get_observation() and the actual\n        observation. See rex.py for more details.\n      pd_latency: The latency used to get motor angles/velocities used to\n        compute PD controllers. See rex.py for more details.\n      on_rack: Whether to place Rex on rack. This is only used to debug\n        the walking gait. In this mode, Rex\'s base is hung midair so\n        that its walking gait is clearer to visualize.\n      motor_kp: The P gain of the motor.\n      motor_kd: The D gain of the motor.\n      remove_default_joint_damping: Whether to remove the default joint damping.\n      render: Whether to render the simulation.\n      num_steps_to_log: The max number of control steps in one episode. If the\n        number of steps is over num_steps_to_log, the environment will still\n        be running, but only first num_steps_to_log will be recorded in logging.\n      accurate_motor_model_enabled: Whether to use the accurate motor model from\n        system identification. Refer to rex_gym_env for more details.\n      use_angle_in_observation: Whether to include motor angles in observation.\n      hard_reset: Whether hard reset (swipe out everything and reload) the\n        simulation. If it is false, Rex is set to the default pose\n        and moved to the origin.\n      env_randomizer: An instance (or a list) of EnvRanzomier(s) that can\n        randomize the environment during when env.reset() is called and add\n        perturbations when env.step() is called.\n      log_path: The path to write out logs. For the details of logging, refer to\n        rex_logging.proto.\n    """"""\n        self._use_angle_in_observation = use_angle_in_observation\n\n        super(RexReactiveEnv,\n              self).__init__(urdf_version=urdf_version,\n                             energy_weight=energy_weight,\n                             accurate_motor_model_enabled=accurate_motor_model_enabled,\n                             motor_overheat_protection=True,\n                             motor_kp=motor_kp,\n                             motor_kd=motor_kd,\n                             remove_default_joint_damping=remove_default_joint_damping,\n                             control_latency=control_latency,\n                             pd_latency=pd_latency,\n                             on_rack=on_rack,\n                             render=render,\n                             hard_reset=hard_reset,\n                             num_steps_to_log=num_steps_to_log,\n                             env_randomizer=env_randomizer,\n                             log_path=log_path,\n                             control_time_step=control_time_step,\n                             action_repeat=action_repeat)\n\n        action_dim = 4\n        action_low = np.array([-0.6] * action_dim)\n        action_high = -action_low\n        self.action_space = spaces.Box(action_low, action_high)\n        self._cam_dist = 1.0\n        self._cam_yaw = 30\n        self._cam_pitch = -30\n        self.init_leg = INIT_LEG_POS\n        self.init_foot = INIT_FOOT_POS\n\n    def reset(self):\n        super(RexReactiveEnv, self).reset(initial_motor_angles=Rex.INIT_POSES[\'stand_high\'],\n                                          reset_duration=0.5)\n        return self._get_observation()\n\n    def _convert_from_leg_model(self, leg_pose):\n        motor_pose = np.zeros(NUM_MOTORS)\n        for i in range(NUM_LEGS):\n            motor_pose[int(3 * i)] = 0\n            if i == 0 or i == 1:\n                leg_action = self.init_leg + leg_pose[0]\n                motor_pose[int(3 * i + 1)] = max(min(leg_action, self.init_leg + 0.60), self.init_leg - 0.60)\n                foot_pose = self.init_foot + leg_pose[1]\n                motor_pose[int(3 * i + 2)] = max(min(foot_pose, self.init_foot + 0.60), self.init_foot - 0.60)\n            else:\n                leg_action = self.init_leg + leg_pose[2]\n                motor_pose[int(3 * i + 1)] = max(min(leg_action, self.init_leg + 0.60), self.init_leg - 0.60)\n                foot_pose = self.init_foot + leg_pose[3]\n                motor_pose[int(3 * i + 2)] = max(min(foot_pose, self.init_foot + 0.60), self.init_foot - 0.60)\n\n        return motor_pose\n\n    # REX_LEG_MODELS = [\n    #   JUMP  ------>\n    #   motor_pose[int(3 * i)] = 0\n    #   leg_action = self.init_leg + leg_pose[int(3 * i)]\n    #   motor_pose[int(3 * i + 1)] = max(min(leg_action, self.init_leg + 0.45), self.init_leg - 0.78)\n    #   foot_pose = self.init_foot + leg_pose[int(3 * i)]\n    #   motor_pose[int(3 * i + 2)] = max(min(foot_pose, self.init_foot + 0.63), self.init_foot - 0.63)\n    #   -----------------------------------------------------------------------------------------------\n    # ]\n\n    def _transform_action_to_motor_command(self, action):\n        # Add the reference trajectory.\n        return self._convert_from_leg_model(action)\n\n    def is_fallen(self):\n        """"""Decides whether Rex is in a fallen state.\n\n    If the roll or the pitch of the base is greater than 0.3 radians, the\n    rex is considered fallen.\n\n    Returns:\n      Boolean value that indicates whether Rex has fallen.\n    """"""\n        roll, pitch, _ = self.rex.GetTrueBaseRollPitchYaw()\n        return math.fabs(roll) > 0.3 or math.fabs(pitch) > 0.5\n\n    def _get_true_observation(self):\n        """"""Get the true observations of this environment.\n\n    It includes the roll, the pitch, the roll dot and the pitch dot of the base.\n    If _use_angle_in_observation is true, eight motor angles are added into the\n    observation.\n\n    Returns:\n      The observation list, which is a numpy array of floating-point values.\n    """"""\n        roll, pitch, _ = self.rex.GetTrueBaseRollPitchYaw()\n        roll_rate, pitch_rate, _ = self.rex.GetTrueBaseRollPitchYawRate()\n        observation = [roll, pitch, roll_rate, pitch_rate]\n        if self._use_angle_in_observation:\n            observation.extend(self.rex.GetMotorAngles().tolist())\n        self._true_observation = np.array(observation)\n        return self._true_observation\n\n    def _get_observation(self):\n        roll, pitch, _ = self.rex.GetBaseRollPitchYaw()\n        roll_rate, pitch_rate, _ = self.rex.GetBaseRollPitchYawRate()\n        observation = [roll, pitch, roll_rate, pitch_rate]\n        if self._use_angle_in_observation:\n            observation.extend(self.rex.GetMotorAngles().tolist())\n        self._observation = np.array(observation)\n        return self._observation\n\n    def _get_observation_upper_bound(self):\n        """"""Get the upper bound of the observation.\n\n    Returns:\n      The upper bound of an observation. See _get_true_observation() for the\n      details of each element of an observation.\n    """"""\n        upper_bound_roll = 2 * math.pi\n        upper_bound_pitch = 2 * math.pi\n        upper_bound_roll_dot = 2 * math.pi / self._time_step\n        upper_bound_pitch_dot = 2 * math.pi / self._time_step\n        upper_bound_motor_angle = 2 * math.pi\n        upper_bound = [\n            upper_bound_roll, upper_bound_pitch, upper_bound_roll_dot, upper_bound_pitch_dot\n        ]\n\n        if self._use_angle_in_observation:\n            upper_bound.extend([upper_bound_motor_angle] * NUM_MOTORS)\n        return np.array(upper_bound)\n\n    def _get_observation_lower_bound(self):\n        lower_bound = -self._get_observation_upper_bound()\n        return lower_bound\n'"
rex_gym/envs/gym/standup_env.py,0,"b'""""""This file implements the gym environment of rex alternating legs.\n\n""""""\nimport math\nimport random\n\nfrom gym import spaces\nimport numpy as np\nfrom .. import rex_gym_env\nfrom ...model.rex import Rex\n\nSTEP_PERIOD = 1.0 / 15.0  # 15 steps per second.\nSTEP_AMPLITUDE = 0.25\n\nNUM_LEGS = 4\nNUM_MOTORS = 3 * NUM_LEGS\n\n\nclass RexStandupEnv(rex_gym_env.RexGymEnv):\n    """"""The gym environment for the rex.\n\n  It simulates the locomotion of a rex, a quadruped robot. The state space\n  include the angles, velocities and torques for all the motors and the action\n  space is the desired motor angle for each motor. The reward function is based\n  on how far the rex walks in 1000 steps and penalizes the energy\n  expenditure.\n\n  """"""\n    metadata = {""render.modes"": [""human"", ""rgb_array""], ""video.frames_per_second"": 66}\n\n    def __init__(self,\n                 urdf_version=None,\n                 control_time_step=0.006,\n                 action_repeat=6,\n                 control_latency=0,\n                 pd_latency=0,\n                 on_rack=False,\n                 motor_kp=1.0,\n                 motor_kd=0.02,\n                 remove_default_joint_damping=False,\n                 render=False,\n                 num_steps_to_log=1000,\n                 env_randomizer=None,\n                 log_path=None):\n        """"""Initialize the rex alternating legs gym environment.\n\n    Args:\n      urdf_version: [DEFAULT_URDF_VERSION, DERPY_V0_URDF_VERSION] are allowable\n        versions. If None, DEFAULT_URDF_VERSION is used. Refer to\n        rex_gym_env for more details.\n      control_time_step: The time step between two successive control signals.\n      action_repeat: The number of simulation steps that an action is repeated.\n      control_latency: The latency between get_observation() and the actual\n        observation. See minituar.py for more details.\n      pd_latency: The latency used to get motor angles/velocities used to\n        compute PD controllers. See rex.py for more details.\n      on_rack: Whether to place the rex on rack. This is only used to debug\n        the walking gait. In this mode, the rex\'s base is hung midair so\n        that its walking gait is clearer to visualize.\n      motor_kp: The P gain of the motor.\n      motor_kd: The D gain of the motor.\n      remove_default_joint_damping: Whether to remove the default joint damping.\n      render: Whether to render the simulation.\n      num_steps_to_log: The max number of control steps in one episode. If the\n        number of steps is over num_steps_to_log, the environment will still\n        be running, but only first num_steps_to_log will be recorded in logging.\n      env_randomizer: An instance (or a list) of EnvRanzomier(s) that can\n        randomize the environment during when env.reset() is called and add\n        perturbations when env.step() is called.\n      log_path: The path to write out logs. For the details of logging, refer to\n        rex_logging.proto.\n    """"""\n        super(RexStandupEnv,\n              self).__init__(urdf_version=urdf_version,\n                             accurate_motor_model_enabled=True,\n                             motor_overheat_protection=True,\n                             hard_reset=False,\n                             motor_kp=motor_kp,\n                             motor_kd=motor_kd,\n                             remove_default_joint_damping=remove_default_joint_damping,\n                             control_latency=control_latency,\n                             pd_latency=pd_latency,\n                             on_rack=on_rack,\n                             render=render,\n                             num_steps_to_log=num_steps_to_log,\n                             env_randomizer=env_randomizer,\n                             log_path=log_path,\n                             control_time_step=control_time_step,\n                             action_repeat=action_repeat)\n\n        action_dim = 12\n        action_high = np.array([0.1] * action_dim)\n        self.action_space = spaces.Box(-action_high, action_high)\n        self._cam_dist = 1.0\n        self._cam_yaw = 30\n        self._cam_pitch = -30\n        self.stand = False\n        if self._on_rack:\n            self._cam_pitch = 0\n\n    def reset(self):\n        self.desired_pitch = 0\n        super(RexStandupEnv, self).reset(initial_motor_angles=Rex.INIT_POSES[\'rest_position\'],\n                                         reset_duration=0.5)\n        return self._get_observation()\n\n    def _signal(self, t):\n        if t > 0.2:\n            self.stand = True\n            return self.rex.INIT_POSES[\'stand_low\']\n        t += 1\n        # apply a \'brake\' function\n        signal = self.rex.INIT_POSES[\'stand_low\'] * (0.1 / t + 1.5)\n        return signal\n\n    def _convert_from_leg_model(self, leg_pose):\n        if self.stand:\n            return self.rex.INIT_POSES[\'stand_low\']\n        motor_pose = np.zeros(NUM_MOTORS)\n        for i in range(NUM_LEGS):\n            motor_pose[3 * i] = leg_pose[3 * i]\n            motor_pose[3 * i + 1] = leg_pose[3 * i + 1]\n            motor_pose[3 * i + 2] = leg_pose[3 * i + 2]\n        return motor_pose\n\n    def _transform_action_to_motor_command(self, action):\n        action += self._signal(self.rex.GetTimeSinceReset())\n        action = self._convert_from_leg_model(action)\n        return action\n\n    def _termination(self):\n        if self.is_fallen():\n            print(""IS FALLEN!"")\n        return self.is_fallen()\n\n    def is_fallen(self):\n        """"""Decide whether the rex has fallen.\n\n    If the up directions between the base and the world is large (the dot\n    product is smaller than 0.85), the rex is considered fallen.\n\n    Returns:\n      Boolean value that indicates whether the rex has fallen.\n    """"""\n        roll, pitch, _ = self.rex.GetTrueBaseRollPitchYaw()\n        return math.fabs(roll) > 0.3 or math.fabs(pitch) > 0.5\n\n    def _reward(self):\n        # target position\n        t_pos = [0.0, 0.0, 0.225]\n\n        current_base_position = self.rex.GetBasePosition()\n\n        position_reward = abs(t_pos[0] - current_base_position[0]) + \\\n                          abs(t_pos[1] - current_base_position[1]) + \\\n                          abs(t_pos[2] - current_base_position[2])\n\n        is_pos = False\n\n        if abs(position_reward) < 0.1:\n            position_reward = 1.0 - position_reward\n            is_pos = True\n        else:\n            position_reward = -position_reward\n\n        if current_base_position[2] > t_pos[2]:\n            position_reward = -1000 - position_reward\n            print(""jump!"")\n\n        if is_pos:\n            self.goal_reached = True\n\n        reward = position_reward\n        return reward\n\n    def _get_true_observation(self):\n        """"""Get the true observations of this environment.\n\n    It includes the roll, the error between current pitch and desired pitch,\n    roll dot and pitch dot of the base.\n\n    Returns:\n      The observation list.\n    """"""\n        observation = []\n        roll, pitch, _ = self.rex.GetTrueBaseRollPitchYaw()\n        roll_rate, pitch_rate, _ = self.rex.GetTrueBaseRollPitchYawRate()\n        observation.extend([roll, pitch, roll_rate, pitch_rate])\n        observation[1] -= self.desired_pitch  # observation[1] is the pitch\n        self._true_observation = np.array(observation)\n        return self._true_observation\n\n    def _get_observation(self):\n        observation = []\n        roll, pitch, _ = self.rex.GetBaseRollPitchYaw()\n        roll_rate, pitch_rate, _ = self.rex.GetBaseRollPitchYawRate()\n        observation.extend([roll, pitch, roll_rate, pitch_rate])\n        observation[1] -= self.desired_pitch  # observation[1] is the pitch\n        self._observation = np.array(observation)\n        return self._observation\n\n    def _get_observation_upper_bound(self):\n        """"""Get the upper bound of the observation.\n\n    Returns:\n      The upper bound of an observation. See GetObservation() for the details\n        of each element of an observation.\n    """"""\n        upper_bound = np.zeros(self._get_observation_dimension())\n        upper_bound[0:2] = 2 * math.pi  # Roll, pitch, yaw of the base.\n        upper_bound[2:4] = 2 * math.pi / self._time_step  # Roll, pitch, yaw rate.\n        return upper_bound\n\n    def _get_observation_lower_bound(self):\n        lower_bound = -self._get_observation_upper_bound()\n        return lower_bound\n\n    def set_swing_offset(self, value):\n        """"""Set the swing offset of each leg.\n\n    It is to mimic the bent leg.\n\n    Args:\n      value: A list of four values.\n    """"""\n        self._swing_offset = value\n\n    def set_extension_offset(self, value):\n        """"""Set the extension offset of each leg.\n\n    It is to mimic the bent leg.\n\n    Args:\n      value: A list of four values.\n    """"""\n        self._extension_offset = value\n\n    def set_desired_pitch(self, value):\n        """"""Set the desired pitch of the base, which is a user input.\n\n    Args:\n      value: A scalar.\n    """"""\n        self.desired_pitch = value\n'"
rex_gym/envs/gym/turn_env.py,0,"b'""""""This file implements the gym environment of rex alternating legs.\n\n""""""\nimport math\nimport random\n\nfrom gym import spaces\nimport numpy as np\nfrom rex_gym.util import pybullet_data\n\nfrom .. import rex_gym_env\n\nNUM_LEGS = 4\nNUM_MOTORS = 3 * NUM_LEGS\nSTEP_PERIOD = 1.0 / 10.0  # 10 steps per second.\nTARGET_POSITION = [0.0, 0.0, 0.21]\nCOMPANION_OBJECTS = {}\n\n\nclass RexTurnEnv(rex_gym_env.RexGymEnv):\n    """"""The gym environment for the rex.\n    It simulates the locomotion of a rex, a quadruped robot. The state space\n    include the angles, velocities and torques for all the motors and the action\n    space is the desired motor angle for each motor. The reward function is based\n    on how far the rex walks in 1000 steps and penalizes the energy expenditure.""""""\n    metadata = {""render.modes"": [""human"", ""rgb_array""], ""video.frames_per_second"": 66}\n\n    def __init__(self,\n                 urdf_version=None,\n                 control_time_step=0.006,\n                 action_repeat=6,\n                 control_latency=0,\n                 pd_latency=0,\n                 on_rack=False,\n                 motor_kp=1.0,\n                 motor_kd=0.02,\n                 remove_default_joint_damping=False,\n                 render=False,\n                 num_steps_to_log=1000,\n                 env_randomizer=None,\n                 log_path=None,\n                 target_orient=None,\n                 init_orient=None):\n        """"""Initialize the rex alternating legs gym environment.\n\n        Args:\n          urdf_version: [DEFAULT_URDF_VERSION, DERPY_V0_URDF_VERSION] are allowable\n            versions. If None, DEFAULT_URDF_VERSION is used. Refer to\n            rex_gym_env for more details.\n          control_time_step: The time step between two successive control signals.\n          action_repeat: The number of simulation steps that an action is repeated.\n          control_latency: The latency between get_observation() and the actual\n            observation. See minituar.py for more details.\n          pd_latency: The latency used to get motor angles/velocities used to\n            compute PD controllers. See rex.py for more details.\n          on_rack: Whether to place the rex on rack. This is only used to debug\n            the walking gait. In this mode, the rex\'s base is hung midair so\n            that its walking gait is clearer to visualize.\n          motor_kp: The P gain of the motor.\n          motor_kd: The D gain of the motor.\n          remove_default_joint_damping: Whether to remove the default joint damping.\n          render: Whether to render the simulation.\n          num_steps_to_log: The max number of control steps in one episode. If the\n            number of steps is over num_steps_to_log, the environment will still\n            be running, but only first num_steps_to_log will be recorded in logging.\n          env_randomizer: An instance (or a list) of EnvRanzomier(s) that can\n            randomize the environment during when env.reset() is called and add\n            perturbations when env.step() is called.\n          log_path: The path to write out logs. For the details of logging, refer to\n            rex_logging.proto.\n        """"""\n        super(RexTurnEnv, self).__init__(\n            urdf_version=urdf_version,\n            accurate_motor_model_enabled=True,\n            motor_overheat_protection=True,\n            hard_reset=False,\n            motor_kp=motor_kp,\n            motor_kd=motor_kd,\n            remove_default_joint_damping=remove_default_joint_damping,\n            control_latency=control_latency,\n            pd_latency=pd_latency,\n            on_rack=on_rack,\n            render=render,\n            num_steps_to_log=num_steps_to_log,\n            env_randomizer=env_randomizer,\n            log_path=log_path,\n            control_time_step=control_time_step,\n            action_repeat=action_repeat,\n            target_orient=target_orient,\n            init_orient=init_orient)\n\n        action_dim = 12\n        action_high = np.array([0.1] * action_dim)\n        self.action_space = spaces.Box(-action_high, action_high)\n        self._cam_dist = 1.8\n        self._cam_yaw = 30\n        self._cam_pitch = -30\n        self.last_step = 0\n        self._target_orient = target_orient\n        self._init_orient = init_orient\n        self._random_orient_target = False\n        self._random_orient_start = False\n        self._cube = None\n        if self._on_rack:\n            self._cam_pitch = 0\n\n    def reset(self):\n        self.goal_reached = False\n        super(RexTurnEnv, self).reset()\n        if self._target_orient is None or self._random_orient_target:\n            self._target_orient = random.uniform(0.2, 6)\n            self._random_orient_target = True\n\n        if self._on_rack:\n            # on rack debug simulation\n            self._init_orient = 2.1\n            position = self.rex.init_on_rack_position\n        else:\n            position = self.rex.init_position\n            if self._init_orient is None or self._random_orient_start:\n                self._init_orient = random.uniform(0.2, 6)\n                self._random_orient_start = True\n        print(f""Start Orientation: {self._init_orient}, Target Orientation: {self._target_orient}"")\n        clockwise = self._solve_direction()\n        print(""Turning right"") if clockwise else print(""Turning left"")\n        self._load_cube(self._target_orient)\n        q = self.pybullet_client.getQuaternionFromEuler([0, 0, self._init_orient])\n        self.pybullet_client.resetBasePositionAndOrientation(self.rex.quadruped, position, q)\n        return self._get_observation()\n\n    def _signal(self, t):\n        initial_pose = self.rex.INIT_POSES[\'stand_high\']\n        period = STEP_PERIOD\n        extension = 0.1\n        swing = 0.2\n        swipe = 0.1\n        ith_leg = int(t / period) % 2\n\n        pose = {\n            \'left_0\': np.array([swipe, extension, -swing,\n                                -swipe, extension, swing,\n                                swipe, -extension, swing,\n                                -swipe, -extension, -swing]),\n            \'left_1\': np.array([-swipe, 0, swing,\n                                swipe, 0, -swing,\n                                -swipe, 0, -swing,\n                                swipe, 0, swing]),\n            \'right_0\': np.array([swipe, extension, swing,\n                                 -swipe, extension, -swing,\n                                 swipe, -extension, -swing,\n                                 -swipe, -extension, swing]),\n            \'right_1\': np.array([-swipe, 0, -swing,\n                                 swipe, 0, swing,\n                                 -swipe, 0, swing,\n                                 swipe, 0, -swing])\n        }\n        clockwise = self._solve_direction()\n        if clockwise:\n            # turn right\n            first_leg = pose[\'right_0\']\n            second_leg = pose[\'right_1\']\n        else:\n            # turn left\n            first_leg = pose[\'left_0\']\n            second_leg = pose[\'left_1\']\n\n        if ith_leg:\n            signal = initial_pose + second_leg\n        else:\n            signal = initial_pose + first_leg\n        return signal\n\n    def _solve_direction(self):\n        diff = abs(self._init_orient - self._target_orient)\n        clockwise = False\n        if self._init_orient < self._target_orient:\n            if diff > 3.14:\n                clockwise = True\n        else:\n            if diff < 3.14:\n                clockwise = True\n        return clockwise\n\n    def _convert_from_leg_model(self, leg_pose, t):\n        if t < .4:\n            # set init position\n            return self.rex.INIT_POSES[\'stand_high\']\n        motor_pose = np.zeros(NUM_MOTORS)\n        for i in range(NUM_LEGS):\n            if self.goal_reached:\n                return self.rex.initial_pose\n            else:\n                motor_pose[3 * i] = leg_pose[3 * i]\n                motor_pose[3 * i + 1] = leg_pose[3 * i + 1]\n                motor_pose[3 * i + 2] = leg_pose[3 * i + 2]\n        return motor_pose\n\n    def _transform_action_to_motor_command(self, action):\n        t = self.rex.GetTimeSinceReset()\n        action += self._signal(t)\n        action = self._convert_from_leg_model(action, t)\n        return action\n\n    def is_fallen(self):\n        """"""Decide whether the rex has fallen.\n\n        If the up directions between the base and the world is large (the dot\n        product is smaller than 0.85), the rex is considered fallen.\n\n        Returns:\n          Boolean value that indicates whether the rex has fallen.\n        """"""\n        orientation = self.rex.GetBaseOrientation()\n        rot_mat = self._pybullet_client.getMatrixFromQuaternion(orientation)\n        local_up = rot_mat[6:]\n        return np.dot(np.asarray([0, 0, 1]), np.asarray(local_up)) < 0.85\n\n    def _reward(self):\n        current_base_position = self.rex.GetBasePosition()\n        current_base_orientation = self.pybullet_client.getEulerFromQuaternion(self.rex.GetBaseOrientation())\n        target_orient = (0, 0, self._target_orient)\n        yaw = current_base_orientation[2]\n        if yaw < 0:\n            yaw += 6.28\n\n        proximity_reward = \\\n            abs(target_orient[0] - current_base_orientation[0]) + \\\n            abs(target_orient[1] - current_base_orientation[1]) + \\\n            abs(target_orient[2] - yaw)\n\n        position_reward = \\\n            abs(TARGET_POSITION[0] - current_base_position[0]) + \\\n            abs(TARGET_POSITION[1] - current_base_position[1]) + \\\n            abs(TARGET_POSITION[2] - current_base_position[2])\n\n        is_oriented = False\n        is_pos = False\n        if abs(proximity_reward) <= 0.15:\n            proximity_reward = 100 - proximity_reward\n            is_oriented = True\n        else:\n            proximity_reward = -proximity_reward\n\n        if abs(position_reward) <= 0.3:\n            position_reward = 100 - position_reward\n            is_pos = True\n        else:\n            position_reward = -position_reward\n\n        if is_pos and is_oriented:\n            self.goal_reached = True\n            self.goal_t = self.rex.GetTimeSinceReset()\n\n        reward = position_reward + proximity_reward\n        return reward\n\n    def _load_cube(self, angle):\n        if len(COMPANION_OBJECTS) > 0:\n            self.pybullet_client.removeBody(COMPANION_OBJECTS[\'cube\'])\n        urdf_root = pybullet_data.getDataPath()\n        self._cube = self._pybullet_client.loadURDF(f""{urdf_root}/cube_small.urdf"")\n        COMPANION_OBJECTS[\'cube\'] = self._cube\n        orientation = [0, 0, 0, 1]\n        x, y = math.cos(angle + 3.14), math.sin(angle + 3.14)\n        position = [x, y, 1]\n        self.pybullet_client.resetBasePositionAndOrientation(self._cube, position, orientation)\n\n    def _get_true_observation(self):\n        """"""Get the true observations of this environment.\n\n        It includes the roll, the error between current pitch and desired pitch,\n        roll dot and pitch dot of the base.\n\n        Returns:\n          The observation list.\n        """"""\n        observation = []\n        roll, pitch, _ = self.rex.GetTrueBaseRollPitchYaw()\n        roll_rate, pitch_rate, _ = self.rex.GetTrueBaseRollPitchYawRate()\n        observation.extend([roll, pitch, roll_rate, pitch_rate])\n        self._true_observation = np.array(observation)\n        return self._true_observation\n\n    def _get_observation(self):\n        observation = []\n        roll, pitch, _ = self.rex.GetBaseRollPitchYaw()\n        roll_rate, pitch_rate, _ = self.rex.GetBaseRollPitchYawRate()\n        observation.extend([roll, pitch, roll_rate, pitch_rate])\n        self._observation = np.array(observation)\n        return self._observation\n\n    def _get_observation_upper_bound(self):\n        """"""Get the upper bound of the observation.\n\n        Returns:\n          The upper bound of an observation. See GetObservation() for the details\n            of each element of an observation.\n        """"""\n        upper_bound = np.zeros(self._get_observation_dimension())\n        upper_bound[0:2] = 2 * math.pi  # Roll, pitch, yaw of the base.\n        upper_bound[2:4] = 2 * math.pi / self._time_step  # Roll, pitch, yaw rate.\n        return upper_bound\n\n    def _get_observation_lower_bound(self):\n        lower_bound = -self._get_observation_upper_bound()\n        return lower_bound\n'"
rex_gym/envs/gym/walk_env.py,0,"b'""""""This file implements the gym environment of rex alternating legs.\n\n""""""\nimport math\n\nfrom gym import spaces\nimport numpy as np\nfrom .. import rex_gym_env\n\nDESIRED_PITCH = 0\nNUM_LEGS = 4\nNUM_MOTORS = 3 * NUM_LEGS\nSTEP_PERIOD = 1.0 / 4.5\n\n\nclass RexWalkEnv(rex_gym_env.RexGymEnv):\n    """"""The gym environment for the rex.\n\n  It simulates the locomotion of a rex, a quadruped robot. The state space\n  include the angles, velocities and torques for all the motors and the action\n  space is the desired motor angle for each motor. The reward function is based\n  on how far the rex walks in 1000 steps and penalizes the energy\n  expenditure.\n\n  """"""\n    metadata = {""render.modes"": [""human"", ""rgb_array""], ""video.frames_per_second"": 66}\n\n    def __init__(self,\n                 urdf_version=None,\n                 control_time_step=0.006,\n                 action_repeat=6,\n                 control_latency=0,\n                 pd_latency=0,\n                 on_rack=False,\n                 motor_kp=1.0,\n                 motor_kd=0.02,\n                 remove_default_joint_damping=False,\n                 render=False,\n                 num_steps_to_log=1000,\n                 env_randomizer=None,\n                 log_path=None):\n        """"""Initialize the rex alternating legs gym environment.\n\n    Args:\n      urdf_version: [DEFAULT_URDF_VERSION, DERPY_V0_URDF_VERSION] are allowable\n        versions. If None, DEFAULT_URDF_VERSION is used. Refer to\n        rex_gym_env for more details.\n      control_time_step: The time step between two successive control signals.\n      action_repeat: The number of simulation steps that an action is repeated.\n      control_latency: The latency between get_observation() and the actual\n        observation. See minituar.py for more details.\n      pd_latency: The latency used to get motor angles/velocities used to\n        compute PD controllers. See rex.py for more details.\n      on_rack: Whether to place the rex on rack. This is only used to debug\n        the walking gait. In this mode, the rex\'s base is hung midair so\n        that its walking gait is clearer to visualize.\n      motor_kp: The P gain of the motor.\n      motor_kd: The D gain of the motor.\n      remove_default_joint_damping: Whether to remove the default joint damping.\n      render: Whether to render the simulation.\n      num_steps_to_log: The max number of control steps in one episode. If the\n        number of steps is over num_steps_to_log, the environment will still\n        be running, but only first num_steps_to_log will be recorded in logging.\n      env_randomizer: An instance (or a list) of EnvRanzomier(s) that can\n        randomize the environment during when env.reset() is called and add\n        perturbations when env.step() is called.\n      log_path: The path to write out logs. For the details of logging, refer to\n        rex_logging.proto.\n    """"""\n        super(RexWalkEnv,\n              self).__init__(urdf_version=urdf_version,\n                             accurate_motor_model_enabled=True,\n                             motor_overheat_protection=True,\n                             hard_reset=False,\n                             motor_kp=motor_kp,\n                             motor_kd=motor_kd,\n                             remove_default_joint_damping=remove_default_joint_damping,\n                             control_latency=control_latency,\n                             pd_latency=pd_latency,\n                             on_rack=on_rack,\n                             render=render,\n                             num_steps_to_log=num_steps_to_log,\n                             env_randomizer=env_randomizer,\n                             log_path=log_path,\n                             control_time_step=control_time_step,\n                             action_repeat=action_repeat)\n\n        action_dim = 12\n        action_high = np.array([0.1] * action_dim)\n        self.action_space = spaces.Box(-action_high, action_high)\n        self._cam_dist = 1.0\n        self._cam_yaw = 30\n        self._cam_pitch = -30\n\n    def reset(self):\n        self.desired_pitch = DESIRED_PITCH\n        super(RexWalkEnv, self).reset()\n        return self._get_observation()\n\n    @staticmethod\n    def _convert_from_leg_model(leg_pose):\n        motor_pose = np.zeros(NUM_MOTORS)\n        for i in range(NUM_LEGS):\n            if i % 2 == 0:\n                motor_pose[3 * i] = 0.1\n            else:\n                motor_pose[3 * i] = -0.1\n            motor_pose[3 * i + 1] = leg_pose[3 * i + 1]\n            motor_pose[3 * i + 2] = leg_pose[3 * i + 2]\n        return motor_pose\n\n    def _signal(self, t):\n        initial_pose = self.rex.initial_pose\n        period = STEP_PERIOD\n        l_extension = 0.2 * math.cos(2 * math.pi / period * t)\n        l_swing = -l_extension\n        extension = 0.3 * math.cos(2 * math.pi / period * t)\n        swing = -extension\n        pose = np.array([0, l_extension, extension,\n                         0, l_swing, swing,\n                         0, l_swing, swing,\n                         0, l_extension, extension])\n\n        signal = initial_pose + pose\n        return signal\n\n    def _transform_action_to_motor_command(self, action):\n        action += self._signal(self.rex.GetTimeSinceReset())\n        action = self._convert_from_leg_model(action)\n        return action\n\n    def is_fallen(self):\n        """"""Decide whether the rex has fallen.\n\n    If the up directions between the base and the world is large (the dot\n    product is smaller than 0.85), the rex is considered fallen.\n\n    Returns:\n      Boolean value that indicates whether the rex has fallen.\n    """"""\n        orientation = self.rex.GetBaseOrientation()\n        rot_mat = self._pybullet_client.getMatrixFromQuaternion(orientation)\n        local_up = rot_mat[6:]\n        return np.dot(np.asarray([0, 0, 1]), np.asarray(local_up)) < 0.85\n\n    def _get_true_observation(self):\n        """"""Get the true observations of this environment.\n\n    It includes the roll, the error between current pitch and desired pitch,\n    roll dot and pitch dot of the base.\n\n    Returns:\n      The observation list.\n    """"""\n        observation = []\n        roll, pitch, _ = self.rex.GetTrueBaseRollPitchYaw()\n        roll_rate, pitch_rate, _ = self.rex.GetTrueBaseRollPitchYawRate()\n        observation.extend([roll, pitch, roll_rate, pitch_rate])\n        observation[1] -= self.desired_pitch  # observation[1] is the pitch\n        self._true_observation = np.array(observation)\n        return self._true_observation\n\n    def _get_observation(self):\n        observation = []\n        roll, pitch, _ = self.rex.GetBaseRollPitchYaw()\n        roll_rate, pitch_rate, _ = self.rex.GetBaseRollPitchYawRate()\n        observation.extend([roll, pitch, roll_rate, pitch_rate])\n        observation[1] -= self.desired_pitch  # observation[1] is the pitch\n        self._observation = np.array(observation)\n        return self._observation\n\n    def _get_observation_upper_bound(self):\n        """"""Get the upper bound of the observation.\n\n    Returns:\n      The upper bound of an observation. See GetObservation() for the details\n        of each element of an observation.\n    """"""\n        upper_bound = np.zeros(self._get_observation_dimension())\n        upper_bound[0:2] = 2 * math.pi  # Roll, pitch, yaw of the base.\n        upper_bound[2:4] = 2 * math.pi / self._time_step  # Roll, pitch, yaw rate.\n        return upper_bound\n\n    def _get_observation_lower_bound(self):\n        lower_bound = -self._get_observation_upper_bound()\n        return lower_bound\n\n    def set_swing_offset(self, value):\n        """"""Set the swing offset of each leg.\n\n    It is to mimic the bent leg.\n\n    Args:\n      value: A list of four values.\n    """"""\n        self._swing_offset = value\n\n    def set_extension_offset(self, value):\n        """"""Set the extension offset of each leg.\n\n    It is to mimic the bent leg.\n\n    Args:\n      value: A list of four values.\n    """"""\n        self._extension_offset = value\n\n    def set_desired_pitch(self, value):\n        """"""Set the desired pitch of the base, which is a user input.\n\n    Args:\n      value: A scalar.\n    """"""\n        self.desired_pitch = value\n'"
rex_gym/util/pybullet_data/__init__.py,0,b'import os\n\n\ndef getDataPath():\n  resdir = os.path.join(os.path.dirname(__file__))\n  return resdir\n'
