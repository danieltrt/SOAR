file_path,api_count,code
data.py,8,"b""from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nclass DataSet(object):\r\n  def __init__(self, images_list_path, num_epoch, batch_size):\r\n    # filling the record_list\r\n    input_file = open(images_list_path, 'r')\r\n    self.record_list = []\r\n    for line in input_file:\r\n      line = line.strip()\r\n      self.record_list.append(line)\r\n    filename_queue = tf.train.string_input_producer(self.record_list, num_epochs=num_epoch)\r\n    image_reader = tf.WholeFileReader()\r\n    _, image_file = image_reader.read(filename_queue)\r\n    image = tf.image.decode_jpeg(image_file, 3)\r\n    #preprocess\r\n    hr_image = tf.image.resize_images(image, [32, 32])\r\n    lr_image = tf.image.resize_images(image, [8, 8])\r\n    hr_image = tf.cast(hr_image, tf.float32)\r\n    lr_image = tf.cast(lr_image, tf.float32)\r\n    #\r\n    min_after_dequeue = 1000\r\n    capacity = min_after_dequeue + 400 * batch_size\r\n    self.hr_images, self.lr_images = tf.train.shuffle_batch([hr_image, lr_image], batch_size=batch_size, capacity=capacity,\r\n      min_after_dequeue=min_after_dequeue)\r\n"""
net.py,13,"b'from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom ops import *\r\n\r\nclass Net(object):\r\n  def __init__(self, hr_images, lr_images, scope):\r\n    """"""\r\n    Args:[0, 255]\r\n      hr_images: [batch_size, hr_height, hr_width, in_channels] float32\r\n      lr_images: [batch_size, lr_height, lr_width, in_channels] float32\r\n    """"""\r\n    with tf.variable_scope(scope) as scope:\r\n      self.train = tf.placeholder(tf.bool)\r\n      self.construct_net(hr_images, lr_images)\r\n  def prior_network(self, hr_images):\r\n    """"""\r\n    Args:[-0.5, 0.5]\r\n      hr_images: [batch_size, hr_height, hr_width, in_channels]\r\n    Returns:\r\n      prior_logits: [batch_size, hr_height, hr_width, 3*256]\r\n    """"""\r\n    with tf.variable_scope(\'prior\') as scope:\r\n      conv1 = conv2d(hr_images, 64, [7, 7], strides=[1, 1], mask_type=\'A\', scope=""conv1"")\r\n      inputs = conv1\r\n      state = conv1\r\n      for i in range(20):\r\n        inputs, state = gated_conv2d(inputs, state, [5, 5], scope=\'gated\' + str(i))\r\n      conv2 = conv2d(inputs, 1024, [1, 1], strides=[1, 1], mask_type=\'B\', scope=""conv2"")\r\n      conv2 = tf.nn.relu(conv2)\r\n      prior_logits = conv2d(conv2, 3 * 256, [1, 1], strides=[1, 1], mask_type=\'B\', scope=""conv3"")\r\n\r\n      prior_logits = tf.concat([prior_logits[:, :, :, 0::3], prior_logits[:, :, :, 1::3], prior_logits[:, :, :, 2::3]], 3)\r\n\r\n      return prior_logits\r\n\r\n\r\n  def conditioning_network(self, lr_images):\r\n    """"""\r\n    Args:[-0.5, 0.5]\r\n      lr_images: [batch_size, lr_height, lr_width, in_channels]\r\n    Returns:\r\n      conditioning_logits: [batch_size, hr_height, hr_width, 3*256]\r\n    """"""\r\n    res_num = 6\r\n    with tf.variable_scope(\'conditioning\') as scope:\r\n      inputs = lr_images\r\n      inputs = conv2d(inputs, 32, [1, 1], strides=[1, 1], mask_type=None, scope=""conv_init"")\r\n      for i in range(2):\r\n        for j in range(res_num):\r\n          inputs = resnet_block(inputs, 32, [3, 3], strides=[1, 1], scope=\'res\' + str(i) + str(j), train=self.train)\r\n        inputs = deconv2d(inputs, 32, [3, 3], strides=[2, 2], scope=""deconv"" + str(i))\r\n        inputs = tf.nn.relu(inputs)\r\n      for i in range(res_num):\r\n        inputs = resnet_block(inputs, 32, [3, 3], strides=[1, 1], scope=\'res3\' + str(i), train=self.train)\r\n      conditioning_logits = conv2d(inputs, 3*256, [1, 1], strides=[1, 1], mask_type=None, scope=""conv"")\r\n\r\n      return conditioning_logits\r\n\r\n  def softmax_loss(self, logits, labels):\r\n    logits = tf.reshape(logits, [-1, 256])\r\n    labels = tf.cast(labels, tf.int32)\r\n    labels = tf.reshape(labels, [-1])\r\n    return tf.losses.sparse_softmax_cross_entropy(\r\n           labels, logits)\r\n  def construct_net(self, hr_images, lr_images):\r\n    """"""\r\n    Args: [0, 255]\r\n    """"""\r\n    #labels\r\n    labels = hr_images\r\n    #normalization images [-0.5, 0.5]\r\n    hr_images = hr_images / 255.0 - 0.5\r\n    lr_images = lr_images / 255.0 - 0.5\r\n    self.prior_logits = self.prior_network(hr_images)\r\n    self.conditioning_logits = self.conditioning_network(lr_images)\r\n\r\n    loss1 = self.softmax_loss(self.prior_logits + self.conditioning_logits, labels)\r\n    loss2 = self.softmax_loss(self.conditioning_logits, labels)\r\n    loss3 = self.softmax_loss(self.prior_logits, labels)\r\n\r\n    self.loss = loss1 + loss2\r\n    tf.summary.scalar(\'loss\', self.loss)\r\n    tf.summary.scalar(\'loss_prior\', loss3)'"
ops.py,19,"b'from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef conv2d(inputs, num_outputs, kernel_shape, strides=[1, 1], mask_type=None, scope=""conv2d""):\r\n  """"""\r\n  Args:\r\n    inputs: nhwc\r\n    kernel_shape: [height, width]\r\n    mask_type: None or \'A\' or \'B\' or \'C\'\r\n  Returns:\r\n    outputs: nhwc\r\n  """"""\r\n  with tf.variable_scope(scope) as scope:\r\n    kernel_h, kernel_w = kernel_shape\r\n    stride_h, stride_w = strides\r\n    batch_size, height, width, in_channel = inputs.get_shape().as_list()\r\n\r\n    center_h = kernel_h // 2\r\n    center_w = kernel_w // 2\r\n\r\n    assert kernel_h % 2 == 1 and kernel_w % 2 == 1, ""kernel height and width must be odd number""\r\n    mask = np.zeros((kernel_h, kernel_w, in_channel, num_outputs), dtype=np.float32)\r\n    if mask_type is not None:\r\n      #C\r\n      mask[:center_h, :, :, :] = 1\r\n      if mask_type == \'A\':\r\n        mask[center_h, :center_w, :, :] = 1\r\n        """"""\r\n        mask[center_h, :center_w, :, :] = 1\r\n        #G channel\r\n        mask[center_h, center_w, 0:in_channel:3, 1:num_outputs:3] = 1\r\n        #B Channel\r\n        mask[center_h, center_w, 0:in_channel:3, 2:num_outputs:3] = 1\r\n        mask[center_h, center_w, 1:in_channel:3, 2:num_outputs:3] = 1\r\n        """"""\r\n      if mask_type == \'B\':\r\n        mask[center_h, :center_w+1, :, :] = 1\r\n        """"""\r\n        mask[center_h, :center_w, :, :] = 1\r\n        #R Channel\r\n        mask[center_h, center_w, 0:in_channel:3, 0:num_outputs:3] = 1\r\n        #G channel\r\n        mask[center_h, center_w, 0:in_channel:3, 1:num_outputs:3] = 1\r\n        mask[center_h, center_w, 1:in_channel:3, 1:num_outputs:3] = 1\r\n        #B Channel\r\n        mask[center_h, center_w, :, 2:num_outputs:3] = 1\r\n        """"""\r\n    else:\r\n      mask[:, :, :, :] = 1\r\n\r\n    weights_shape = [kernel_h, kernel_w, in_channel, num_outputs]\r\n    weights = tf.get_variable(""weights"", weights_shape,\r\n      tf.float32, tf.truncated_normal_initializer(stddev=0.1))\r\n    weights = weights * mask\r\n    biases = tf.get_variable(""biases"", [num_outputs],\r\n          tf.float32, tf.constant_initializer(0.0))\r\n\r\n    outputs = tf.nn.conv2d(inputs, weights, [1, stride_h, stride_w, 1], padding=""SAME"")\r\n    outputs = tf.nn.bias_add(outputs, biases)\r\n\r\n    return outputs\r\n\r\ndef gated_conv2d(inputs, state, kernel_shape, scope):\r\n  """"""\r\n  Args:\r\n    inputs: nhwc\r\n    state:  nhwc\r\n    kernel_shape: [height, width]\r\n  Returns:\r\n    outputs: nhwc\r\n    new_state: nhwc\r\n  """"""\r\n  with tf.variable_scope(scope) as scope:\r\n    batch_size, height, width, in_channel = inputs.get_shape().as_list()\r\n    kernel_h, kernel_w = kernel_shape\r\n    #state route\r\n    left = conv2d(state, 2 * in_channel, kernel_shape, strides=[1, 1], mask_type=\'C\', scope=""conv_s1"")\r\n    left1 = left[:, :, :, 0:in_channel]\r\n    left2 = left[:, :, :, in_channel:]\r\n    left1 = tf.nn.tanh(left1)\r\n    left2 = tf.nn.sigmoid(left2)\r\n    new_state = left1 * left2\r\n    left2right = conv2d(left, 2 * in_channel, [1, 1], strides=[1, 1], scope=""conv_s2"")\r\n    #input route\r\n    right = conv2d(inputs, 2 * in_channel, [1, kernel_w], strides=[1, 1], mask_type=\'B\', scope=""conv_r1"")\r\n    right = right + left2right\r\n    right1 = right[:, :, :, 0:in_channel]\r\n    right2 = right[:, :, :, in_channel:]\r\n    right1 = tf.nn.tanh(right1)\r\n    right2 = tf.nn.sigmoid(right2)\r\n    up_right = right1 * right2\r\n    up_right = conv2d(up_right, in_channel, [1, 1], strides=[1, 1], mask_type=\'B\', scope=""conv_r2"")\r\n    outputs = inputs + up_right\r\n\r\n    return outputs, new_state\r\n\r\ndef batch_norm(x, train=True, scope=None):\r\n  return tf.contrib.layers.batch_norm(x, center=True, scale=True, updates_collections=None, is_training=train, trainable=True, scope=scope)\r\n\r\ndef resnet_block(inputs, num_outputs, kernel_shape, strides=[1, 1], scope=None, train=True):\r\n  """"""\r\n  Args:\r\n    inputs: nhwc\r\n    num_outputs: int\r\n    kernel_shape: [kernel_h, kernel_w]\r\n  Returns:\r\n    outputs: nhw(num_outputs)\r\n  """"""\r\n  with tf.variable_scope(scope) as scope:\r\n    conv1 = conv2d(inputs, num_outputs, kernel_shape, strides=[1, 1], mask_type=None, scope=""conv1"")\r\n    bn1 = batch_norm(conv1, train=train, scope=\'bn1\')\r\n    relu1 = tf.nn.relu(bn1)\r\n    conv2 = conv2d(relu1, num_outputs, kernel_shape, strides=[1, 1], mask_type=None, scope=""conv2"")\r\n    bn2 = batch_norm(conv2, train=train, scope=\'bn2\')\r\n    output = inputs + bn2\r\n\r\n    return output \r\n\r\ndef deconv2d(inputs, num_outputs, kernel_shape, strides=[1, 1], scope=""deconv2d""):\r\n  """"""\r\n  Args:\r\n    inputs: nhwc\r\n    num_outputs: int\r\n    kernel_shape: [kernel_h, kernel_w]\r\n    strides: [stride_h, stride_w]\r\n  Returns:\r\n    outputs: nhwc\r\n  """"""\r\n  with tf.variable_scope(scope) as scope:\r\n    return tf.contrib.layers.convolution2d_transpose(inputs, num_outputs, kernel_shape, strides, \\\r\n          padding=\'SAME\', weights_initializer=tf.truncated_normal_initializer(stddev=0.1), \\\r\n          biases_initializer=tf.constant_initializer(0.0))'"
solver.py,14,"b""from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom ops import *\r\nfrom data import *\r\nfrom net import *\r\nfrom utils import *\r\nimport os\r\nimport time\r\n\r\nflags = tf.app.flags\r\nconf = flags.FLAGS\r\nclass Solver(object):\r\n  def __init__(self):\r\n    self.device_id = conf.device_id\r\n    self.train_dir = conf.train_dir\r\n    self.samples_dir = conf.samples_dir\r\n    if not os.path.exists(self.train_dir):\r\n      os.makedirs(self.train_dir)\r\n    if not os.path.exists(self.samples_dir):\r\n      os.makedirs(self.samples_dir)    \r\n    #datasets params\r\n    self.num_epoch = conf.num_epoch\r\n    self.batch_size = conf.batch_size\r\n    #optimizer parameter\r\n    self.learning_rate = conf.learning_rate\r\n    if conf.use_gpu:\r\n      device_str = '/gpu:' +  str(self.device_id)\r\n    else:\r\n      device_str = '/cpu:0'\r\n    with tf.device(device_str):\r\n      #dataset\r\n      self.dataset = DataSet(conf.imgs_list_path, self.num_epoch, self.batch_size)\r\n      self.net = Net(self.dataset.hr_images, self.dataset.lr_images, 'prsr')\r\n      #optimizer\r\n      self.global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\r\n      learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\r\n                                           500000, 0.5, staircase=True)\r\n      optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9, epsilon=1e-8)\r\n      self.train_op = optimizer.minimize(self.net.loss, global_step=self.global_step)\r\n  def train(self):\r\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n    summary_op = tf.summary.merge_all()\r\n    saver = tf.train.Saver()\r\n    # Create a session for running operations in the Graph.\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n\r\n    # Initialize the variables (like the epoch counter).\r\n    sess.run(init_op)\r\n    #saver.restore(sess, './models/model.ckpt-30000')\r\n    summary_writer = tf.summary.FileWriter(self.train_dir, sess.graph)\r\n    # Start input enqueue threads.\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    iters = 0\r\n    try:\r\n      while not coord.should_stop():\r\n        # Run training steps or whatever\r\n        t1 = time.time()\r\n        _, loss = sess.run([self.train_op, self.net.loss], feed_dict={self.net.train: True})\r\n        t2 = time.time()\r\n        print('step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)' % ((iters, loss, self.batch_size/(t2-t1), (t2-t1))))\r\n        iters += 1\r\n        if iters % 10 == 0:\r\n          summary_str = sess.run(summary_op, feed_dict={self.net.train: True})\r\n          summary_writer.add_summary(summary_str, iters)\r\n        if iters % 1000 == 0:\r\n          #self.sample(sess, mu=1.0, step=iters)\r\n          self.sample(sess, mu=1.1, step=iters)\r\n          #self.sample(sess, mu=100, step=iters)\r\n        if iters % 10000 == 0:\r\n          checkpoint_path = os.path.join(self.train_dir, 'model.ckpt')\r\n          saver.save(sess, checkpoint_path, global_step=iters)\r\n    except tf.errors.OutOfRangeError:\r\n      checkpoint_path = os.path.join(self.train_dir, 'model.ckpt')\r\n      saver.save(sess, checkpoint_path)\r\n      print('Done training -- epoch limit reached')\r\n    finally:\r\n      # When done, ask the threads to stop.\r\n      coord.request_stop()\r\n\r\n    # Wait for threads to finish.\r\n    coord.join(threads)\r\n    sess.close()\r\n  def sample(self, sess, mu=1.1, step=None):\r\n    c_logits = self.net.conditioning_logits\r\n    p_logits = self.net.prior_logits\r\n    lr_imgs = self.dataset.lr_images\r\n    hr_imgs = self.dataset.hr_images\r\n    np_hr_imgs, np_lr_imgs = sess.run([hr_imgs, lr_imgs])\r\n    gen_hr_imgs = np.zeros((self.batch_size, 32, 32, 3), dtype=np.float32)\r\n    #gen_hr_imgs = np_hr_imgs\r\n    #gen_hr_imgs[:,16:,16:,:] = 0.0\r\n    np_c_logits = sess.run(c_logits, feed_dict={lr_imgs: np_lr_imgs, self.net.train:False})\r\n    print('iters %d: ' % step)\r\n    \r\n    for i in range(32):\r\n      for j in range(32):\r\n        for c in range(3):\r\n          np_p_logits = sess.run(p_logits, feed_dict={hr_imgs: gen_hr_imgs})\r\n          new_pixel = logits_2_pixel_value(np_c_logits[:, i, j, c*256:(c+1)*256] + np_p_logits[:, i, j, c*256:(c+1)*256], mu=mu)\r\n          gen_hr_imgs[:, i, j, c] = new_pixel\r\n    #\r\n    save_samples(np_lr_imgs, self.samples_dir + '/lr_' + str(mu*10) + '_' + str(step) + '.jpg')\r\n    save_samples(np_hr_imgs, self.samples_dir + '/hr_' + str(mu*10) + '_' + str(step) + '.jpg')\r\n    save_samples(gen_hr_imgs, self.samples_dir + '/generate_' + str(mu*10) + '_' + str(step) + '.jpg')"""
utils.py,0,"b'import numpy as np\r\nfrom skimage.io import imsave\r\n\r\ndef softmax(x):\r\n    """"""Compute softmax values for each sets of scores in x.""""""\r\n    e_x = np.exp(x - np.expand_dims(np.max(x, axis=-1), axis=-1))\r\n    return e_x / np.expand_dims(e_x.sum(axis=-1), axis=-1) # only difference\r\ndef save_samples(np_imgs, img_path):\r\n  """"""\r\n  Args:\r\n    np_imgs: [N, H, W, 3] float32\r\n    img_path: str\r\n  """"""\r\n  np_imgs = np_imgs.astype(np.uint8)\r\n  N, H, W, _ = np_imgs.shape\r\n  num = int(N ** (0.5))\r\n  merge_img = np.zeros((num * H, num * W, 3), dtype=np.uint8)\r\n  for i in range(num):\r\n    for j in range(num):\r\n      merge_img[i*H:(i+1)*H, j*W:(j+1)*W, :] = np_imgs[i*num+j,:,:,:]\r\n\r\n  imsave(img_path, merge_img)\r\ndef logits_2_pixel_value(logits, mu=1.1):\r\n  """"""\r\n  Args:\r\n    logits: [n, 256] float32\r\n    mu    : float32\r\n  Returns:\r\n    pixels: [n] float32\r\n  """"""\r\n  rebalance_logits = logits * mu\r\n  probs = softmax(rebalance_logits)\r\n  pixel_dict = np.arange(0, 256, dtype=np.float32)\r\n  pixels = np.sum(probs * pixel_dict, axis=1)\r\n  return np.floor(pixels)\r\n\r\n'"
tools/create_img_lists.py,0,"b'""""""Create image-list file\r\nExample:\r\npython tools/create_img_lists.py --dataset=data/celebA --outfile=data/train.txt\r\n""""""\r\nimport os\r\nfrom optparse import OptionParser\r\n\r\nparser = OptionParser()\r\nparser.add_option(""--dataset"", dest=""dataset"",  \r\n                  help=""dataset path"")\r\n\r\nparser.add_option(""--outfile"", dest=""outfile"",  \r\n                  help=""outfile path"")\r\n(options, args) = parser.parse_args()\r\n\r\nf = open(options.outfile, \'w\')\r\ndataset_basepath = options.dataset\r\nfor p1 in os.listdir(dataset_basepath):\r\n  image = os.path.abspath(dataset_basepath + \'/\' + p1)\r\n  f.write(image + \'\\n\')\r\nf.close()\r\n'"
tools/train.py,2,"b'import tensorflow as tf\r\nimport sys\r\nsys.path.insert(0, \'./\')\r\nfrom solver import *\r\n\r\nflags = tf.app.flags\r\n\r\n#solver\r\nflags.DEFINE_string(""train_dir"", ""models"", ""trained model save path"")\r\nflags.DEFINE_string(""samples_dir"", ""samples"", ""sampled images save path"")\r\nflags.DEFINE_string(""imgs_list_path"", ""data/train.txt"", ""images list file path"")\r\n\r\nflags.DEFINE_boolean(""use_gpu"", True, ""whether to use gpu for training"")\r\nflags.DEFINE_integer(""device_id"", 0, ""gpu device id"")\r\n\r\nflags.DEFINE_integer(""num_epoch"", 30, ""train epoch num"")\r\nflags.DEFINE_integer(""batch_size"", 32, ""batch_size"")\r\n\r\nflags.DEFINE_float(""learning_rate"", 4e-4, ""learning rate"")\r\n\r\nconf = flags.FLAGS\r\n\r\ndef main(_):\r\n  solver = Solver()\r\n  solver.train()\r\n\r\nif __name__ == \'__main__\':\r\n  tf.app.run()\r\n'"
