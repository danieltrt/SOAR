file_path,api_count,code
src/__init__.py,0,b''
src/constants.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of src.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines constants used in src.\n""""""\n\n\nclass VIEWS:\n    L_CC = ""L-CC""\n    R_CC = ""R-CC""\n    L_MLO = ""L-MLO""\n    R_MLO = ""R-MLO""\n\n    LIST = [L_CC, R_CC, L_MLO, R_MLO]\n\n    @classmethod\n    def is_cc(cls, view):\n        return view in (cls.L_CC, cls.R_CC)\n\n    @classmethod\n    def is_mlo(cls, view):\n        return view in (cls.L_MLO, cls.R_MLO)\n\n    @classmethod\n    def is_left(cls, view):\n        return view in (cls.L_CC, cls.L_MLO)\n\n    @classmethod\n    def is_right(cls, view):\n        return view in (cls.R_CC, cls.R_MLO)\n\n\nclass VIEWANGLES:\n    CC = ""CC""\n    MLO = ""MLO""\n\n    LIST = [CC, MLO]\n\n\nclass LABELS:\n    LEFT_BENIGN = ""left_benign""\n    RIGHT_BENIGN = ""right_benign""\n    LEFT_MALIGNANT = ""left_malignant""\n    RIGHT_MALIGNANT = ""right_malignant""\n\n    LIST = [LEFT_BENIGN, RIGHT_BENIGN, LEFT_MALIGNANT, RIGHT_MALIGNANT]\n\n\nclass MODELMODES:\n    VIEW_SPLIT = ""view_split""\n    IMAGE = ""image""\n\n\nINPUT_SIZE_DICT = {\n    VIEWS.L_CC: (2677, 1942),\n    VIEWS.R_CC: (2677, 1942),\n    VIEWS.L_MLO: (2974, 1748),\n    VIEWS.R_MLO: (2974, 1748),\n}\n'"
src/cropping/__init__.py,0,b''
src/cropping/crop_mammogram.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n\nimport os\nfrom multiprocessing import Pool\nimport argparse\nfrom functools import partial\nimport scipy.ndimage\nimport numpy as np\nimport pandas as pd\n\nimport src.utilities.pickling as pickling\nimport src.utilities.reading_images as reading_images\nimport src.utilities.saving_images as saving_images\nimport src.utilities.data_handling as data_handling\n\n\ndef get_masks_and_sizes_of_connected_components(img_mask):\n    """"""\n    Finds the connected components from the mask of the image\n    """"""\n    mask, num_labels = scipy.ndimage.label(img_mask)\n\n    mask_pixels_dict = {}\n    for i in range(num_labels+1):\n        this_mask = (mask == i)\n        if img_mask[this_mask][0] != 0:\n            # Exclude the 0-valued mask\n            mask_pixels_dict[i] = np.sum(this_mask)\n        \n    return mask, mask_pixels_dict\n\n\ndef get_mask_of_largest_connected_component(img_mask):\n    """"""\n    Finds the largest connected component from the mask of the image\n    """"""\n    mask, mask_pixels_dict = get_masks_and_sizes_of_connected_components(img_mask)\n    largest_mask_index = pd.Series(mask_pixels_dict).idxmax()\n    largest_mask = mask == largest_mask_index\n    return largest_mask\n\n\ndef get_edge_values(img, largest_mask, axis):\n    """"""\n    Finds the bounding box for the largest connected component\n    """"""\n    assert axis in [""x"", ""y""]\n    has_value = np.any(largest_mask, axis=int(axis == ""y""))\n    edge_start = np.arange(img.shape[int(axis == ""x"")])[has_value][0]\n    edge_end = np.arange(img.shape[int(axis == ""x"")])[has_value][-1] + 1\n    return edge_start, edge_end\n\n\ndef get_bottommost_pixels(img, largest_mask, y_edge_bottom):\n    """"""\n    Gets the bottommost nonzero pixels of dilated mask before cropping. \n    """"""\n    bottommost_nonzero_y = y_edge_bottom - 1\n    bottommost_nonzero_x = np.arange(img.shape[1])[largest_mask[bottommost_nonzero_y, :] > 0]\n    return bottommost_nonzero_y, bottommost_nonzero_x\n\n\ndef get_distance_from_starting_side(img, mode, x_edge_left, x_edge_right):\n    """"""\n    If we fail to recover the original shape as a result of erosion-dilation \n    on the side where the breast starts to appear in the image, \n    we record this information.\n    """"""\n    if mode == ""left"":\n        return img.shape[1] - x_edge_right\n    else:\n        return x_edge_left\n\n\ndef include_buffer_y_axis(img, y_edge_top, y_edge_bottom, buffer_size):\n    """"""\n    Includes buffer in all sides of the image in y-direction\n    """"""\n    if y_edge_top > 0:\n        y_edge_top -= min(y_edge_top, buffer_size)\n    if y_edge_bottom < img.shape[0]:\n        y_edge_bottom += min(img.shape[0] - y_edge_bottom, buffer_size)\n    return y_edge_top, y_edge_bottom     \n\n\ndef include_buffer_x_axis(img, mode, x_edge_left, x_edge_right, buffer_size):\n    """"""\n    Includes buffer in only one side of the image in x-direction\n    """"""\n    if mode == ""left"":\n        if x_edge_left > 0:\n            x_edge_left -= min(x_edge_left, buffer_size)\n    else:\n        if x_edge_right < img.shape[1]:\n            x_edge_right += min(img.shape[1] - x_edge_right, buffer_size)\n    return x_edge_left, x_edge_right\n\n\ndef convert_bottommost_pixels_wrt_cropped_image(mode, bottommost_nonzero_y, bottommost_nonzero_x,\n                                                y_edge_top, x_edge_right, x_edge_left):\n    """"""\n    Once the image is cropped, adjusts the bottommost pixel values which was originally w.r.t. the original image\n    """"""\n    bottommost_nonzero_y -= y_edge_top\n    if mode == ""left"":\n        bottommost_nonzero_x = x_edge_right - bottommost_nonzero_x  # in this case, not in sorted order anymore.\n        bottommost_nonzero_x = np.flip(bottommost_nonzero_x, 0)\n    else:\n        bottommost_nonzero_x -= x_edge_left\n    return bottommost_nonzero_y, bottommost_nonzero_x\n\n\ndef get_rightmost_pixels_wrt_cropped_image(mode, largest_mask_cropped, find_rightmost_from_ratio):\n    """"""\n    Ignores top find_rightmost_from_ratio of the image and searches the rightmost nonzero pixels\n    of the dilated mask from the bottom portion of the image.\n    """"""\n    ignore_height = int(largest_mask_cropped.shape[0] * find_rightmost_from_ratio)\n    rightmost_pixel_search_area = largest_mask_cropped[ignore_height:, :]\n    rightmost_pixel_search_area_has_value = np.any(rightmost_pixel_search_area, axis=0)\n    rightmost_nonzero_x = np.arange(rightmost_pixel_search_area.shape[1])[\n        rightmost_pixel_search_area_has_value][-1 if mode == \'right\' else 0]\n    rightmost_nonzero_y = np.arange(rightmost_pixel_search_area.shape[0])[\n        rightmost_pixel_search_area[:, rightmost_nonzero_x] > 0] + ignore_height\n\n    # rightmost pixels are already found w.r.t. newly cropped image, except that we still need to\n    #   reflect horizontal_flip\n    if mode == ""left"":\n        rightmost_nonzero_x = largest_mask_cropped.shape[1] - rightmost_nonzero_x\n        \n    return rightmost_nonzero_y, rightmost_nonzero_x\n\n\ndef crop_img_from_largest_connected(img, mode, erode_dialate=True, iterations=100, \n                                    buffer_size=50, find_rightmost_from_ratio=1/3):\n    """"""\n    Performs erosion on the mask of the image, selects largest connected component,\n    dialates the largest connected component, and draws a bounding box for the result\n    with buffers\n\n    input:\n        - img:   2D numpy array\n        - mode:  breast pointing left or right\n\n    output: a tuple of (window_location, rightmost_points, \n                        bottommost_points, distance_from_starting_side)\n        - window_location: location of cropping window w.r.t. original dicom image so that segmentation\n           map can be cropped in the same way for training.\n        - rightmost_points: rightmost nonzero pixels after correctly being flipped in the format of \n                            ((y_start, y_end), x)\n        - bottommost_points: bottommost nonzero pixels after correctly being flipped in the format of\n                             (y, (x_start, x_end))\n        - distance_from_starting_side: number of zero columns between the start of the image and start of\n           the largest connected component w.r.t. original dicom image.\n    """"""\n    assert mode in (""left"", ""right"")\n\n    img_mask = img > 0\n\n    # Erosion in order to remove thin lines in the background\n    if erode_dialate:\n        img_mask = scipy.ndimage.morphology.binary_erosion(img_mask, iterations=iterations)\n\n    # Select mask for largest connected component\n    largest_mask = get_mask_of_largest_connected_component(img_mask)\n\n    # Dilation to recover the original mask, excluding the thin lines\n    if erode_dialate:\n        largest_mask = scipy.ndimage.morphology.binary_dilation(largest_mask, iterations=iterations)\n    \n    # figure out where to crop\n    y_edge_top, y_edge_bottom = get_edge_values(img, largest_mask, ""y"")\n    x_edge_left, x_edge_right = get_edge_values(img, largest_mask, ""x"")\n\n    # extract bottommost pixel info\n    bottommost_nonzero_y, bottommost_nonzero_x = get_bottommost_pixels(img, largest_mask, y_edge_bottom)\n\n    # include maximum \'buffer_size\' more pixels on both sides just to make sure we don\'t miss anything\n    y_edge_top, y_edge_bottom = include_buffer_y_axis(img, y_edge_top, y_edge_bottom, buffer_size)\n    \n    # If cropped image not starting from corresponding edge, they are wrong. Record the distance, will reject if not 0.\n    distance_from_starting_side = get_distance_from_starting_side(img, mode, x_edge_left, x_edge_right)\n\n    # include more pixels on either side just to make sure we don\'t miss anything, if the next column\n    #   contains non-zero value and isn\'t noise\n    x_edge_left, x_edge_right = include_buffer_x_axis(img, mode, x_edge_left, x_edge_right, buffer_size)\n\n    # convert bottommost pixel locations w.r.t. newly cropped image. Flip if necessary.\n    bottommost_nonzero_y, bottommost_nonzero_x = convert_bottommost_pixels_wrt_cropped_image(\n        mode,\n        bottommost_nonzero_y,\n        bottommost_nonzero_x,\n        y_edge_top,\n        x_edge_right,\n        x_edge_left\n    )\n\n    # calculate rightmost point from bottom portion of the image w.r.t. cropped image. Flip if necessary.\n    rightmost_nonzero_y, rightmost_nonzero_x = get_rightmost_pixels_wrt_cropped_image(\n        mode,\n        largest_mask[y_edge_top: y_edge_bottom, x_edge_left: x_edge_right],\n        find_rightmost_from_ratio\n    )\n\n    # save window location in medical mode, but everything else in training mode\n    return (y_edge_top, y_edge_bottom, x_edge_left, x_edge_right), \\\n        ((rightmost_nonzero_y[0], rightmost_nonzero_y[-1]), rightmost_nonzero_x), \\\n        (bottommost_nonzero_y, (bottommost_nonzero_x[0], bottommost_nonzero_x[-1])), \\\n        distance_from_starting_side\n\n\ndef image_orientation(horizontal_flip, side):\n    """"""\n    Returns the direction where the breast should be facing in the original image\n    This information is used in cropping.crop_img_horizontally_from_largest_connected\n    """"""\n    assert horizontal_flip in [\'YES\', \'NO\'], ""Wrong horizontal flip""\n    assert side in [\'L\', \'R\'], ""Wrong side""\n    if horizontal_flip == \'YES\':\n        if side == \'R\':\n            return \'right\'\n        else:\n            return \'left\'\n    else:\n        if side == \'R\':\n            return \'left\'\n        else:\n            return \'right\'\n\n\ndef crop_mammogram(input_data_folder, exam_list_path, cropped_exam_list_path, output_data_folder,\n                   num_processes, num_iterations, buffer_size):\n    """"""\n    In parallel, crops mammograms in DICOM format found in input_data_folder and save as png format in\n    output_data_folder and saves new image list in cropped_image_list_path\n    """"""\n    exam_list = pickling.unpickle_from_file(exam_list_path)\n    \n    image_list = data_handling.unpack_exam_into_images(exam_list)\n    \n    if os.path.exists(output_data_folder):\n        # Prevent overwriting to an existing directory\n        print(""Error: the directory to save cropped images already exists."")\n        return\n    else:\n        os.makedirs(output_data_folder)\n\n    crop_mammogram_one_image_func = partial(\n        crop_mammogram_one_image_short_path,\n        input_data_folder=input_data_folder, \n        output_data_folder=output_data_folder,\n        num_iterations=num_iterations,\n        buffer_size=buffer_size,\n    )\n    with Pool(num_processes) as pool:\n        cropped_image_info = pool.map(crop_mammogram_one_image_func, image_list)\n    \n    window_location_dict = dict([x[0] for x in cropped_image_info])\n    rightmost_points_dict = dict([x[1] for x in cropped_image_info])\n    bottommost_points_dict = dict([x[2] for x in cropped_image_info])\n    distance_from_starting_side_dict = dict([x[3] for x in cropped_image_info])\n\n    data_handling.add_metadata(exam_list, ""window_location"", window_location_dict)\n    data_handling.add_metadata(exam_list, ""rightmost_points"", rightmost_points_dict)\n    data_handling.add_metadata(exam_list, ""bottommost_points"", bottommost_points_dict)\n    data_handling.add_metadata(exam_list, ""distance_from_starting_side"", distance_from_starting_side_dict)\n    \n    pickling.pickle_to_file(cropped_exam_list_path, exam_list)\n    \n\ndef crop_mammogram_one_image(scan, input_file_path, output_file_path, num_iterations, buffer_size):\n    """"""\n    Crops a mammogram, saves as png file, includes the following additional information:\n        - window_location: location of cropping window w.r.t. original dicom image so that segmentation\n           map can be cropped in the same way for training.\n        - rightmost_points: rightmost nonzero pixels after correctly being flipped\n        - bottommost_points: bottommost nonzero pixels after correctly being flipped\n        - distance_from_starting_side: number of zero columns between the start of the image and start of\n           the largest connected component w.r.t. original dicom image.\n    """"""\n\n    image = reading_images.read_image_png(input_file_path)\n    try:\n        # error detection using erosion. Also get cropping information for this image.\n        cropping_info = crop_img_from_largest_connected(\n            image, \n            image_orientation(scan[\'horizontal_flip\'], scan[\'side\']), \n            True, \n            num_iterations, \n            buffer_size, \n            1/3\n        )\n    except Exception as error:\n        print(input_file_path, ""\\n\\tFailed to crop image because image is invalid."", str(error))\n    else:\n        \n        top, bottom, left, right = cropping_info[0]\n\n        target_parent_dir = os.path.split(output_file_path)[0]\n        if not os.path.exists(target_parent_dir):\n            os.makedirs(target_parent_dir)\n        \n        try:\n            saving_images.save_image_as_png(image[top:bottom, left:right], output_file_path)\n        except Exception as error:\n            print(input_file_path, ""\\n\\tError while saving image."", str(error))\n\n        return cropping_info\n\n\ndef crop_mammogram_one_image_short_path(scan, input_data_folder, output_data_folder,\n                                        num_iterations, buffer_size):\n    """"""\n    Crops a mammogram from a short_file_path\n\n    See: crop_mammogram_one_image\n    """"""\n    full_input_file_path = os.path.join(input_data_folder, scan[\'short_file_path\']+\'.png\')\n    full_output_file_path = os.path.join(output_data_folder, scan[\'short_file_path\'] + \'.png\')\n    cropping_info = crop_mammogram_one_image(\n        scan=scan,\n        input_file_path=full_input_file_path,\n        output_file_path=full_output_file_path,\n        num_iterations=num_iterations,\n        buffer_size=buffer_size,\n    )\n    return list(zip([scan[\'short_file_path\']] * 4, cropping_info))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Remove background of image and save cropped files\')\n    parser.add_argument(\'--input-data-folder\', required=True)\n    parser.add_argument(\'--output-data-folder\', required=True)\n    parser.add_argument(\'--exam-list-path\', required=True)\n    parser.add_argument(\'--cropped-exam-list-path\', required=True)\n    parser.add_argument(\'--num-processes\', default=10, type=int)\n    parser.add_argument(\'--num-iterations\', default=100, type=int)\n    parser.add_argument(\'--buffer-size\', default=50, type=int)\n    args = parser.parse_args()\n    \n    crop_mammogram(\n        input_data_folder=args.input_data_folder, \n        exam_list_path=args.exam_list_path, \n        cropped_exam_list_path=args.cropped_exam_list_path, \n        output_data_folder=args.output_data_folder, \n        num_processes=args.num_processes,\n        num_iterations=args.num_iterations,\n        buffer_size=args.buffer_size,\n    )\n'"
src/cropping/crop_single.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n\nimport argparse\n\nimport src.cropping.crop_mammogram as crop_mammogram\nimport src.utilities.pickling as pickling\n\n\ndef crop_single_mammogram(mammogram_path, horizontal_flip, view,\n                          cropped_mammogram_path, metadata_path,\n                          num_iterations, buffer_size):\n    """"""\n    Crop a single mammogram image\n    """"""\n    metadata_dict = dict(\n        short_file_path=None,\n        horizontal_flip=horizontal_flip,\n        full_view=view,\n        side=view[0],\n        view=view[2:],\n    )\n    cropped_image_info = crop_mammogram.crop_mammogram_one_image(\n        scan=metadata_dict,\n        input_file_path=mammogram_path,\n        output_file_path=cropped_mammogram_path,\n        num_iterations=num_iterations,\n        buffer_size=buffer_size,\n    )\n    metadata_dict[""window_location""] = cropped_image_info[0]\n    metadata_dict[""rightmost_points""] = cropped_image_info[1]\n    metadata_dict[""bottommost_points""] = cropped_image_info[2]\n    metadata_dict[""distance_from_starting_side""] = cropped_image_info[3]\n    pickling.pickle_to_file(metadata_path, metadata_dict)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Remove background of image and save cropped files\')\n    parser.add_argument(\'--mammogram-path\', required=True)\n    parser.add_argument(\'--view\', required=True)\n    parser.add_argument(\'--horizontal-flip\', default=""NO"", type=str)\n    parser.add_argument(\'--cropped-mammogram-path\', required=True)\n    parser.add_argument(\'--metadata-path\', required=True)\n    parser.add_argument(\'--num-iterations\', default=100, type=int)\n    parser.add_argument(\'--buffer-size\', default=50, type=int)\n    args = parser.parse_args()\n\n    crop_single_mammogram(\n        mammogram_path=args.mammogram_path,\n        view=args.view,\n        horizontal_flip=args.horizontal_flip,\n        cropped_mammogram_path=args.cropped_mammogram_path,\n        metadata_path=args.metadata_path,\n        num_iterations=args.num_iterations,\n        buffer_size=args.buffer_size,\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/data_loading/__init__.py,0,b''
src/data_loading/augmentations.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\nimport cv2\nimport numpy as np\n\nfrom src.constants import VIEWS\n\n\ndef shift_window_inside_image(start, end, image_axis_size, input_axis_size):\n    """"""\n    If the window goes outside the bound of the image, then shifts it to fit inside the image.\n    """"""\n    if start < 0:\n        start = 0\n        end = start + input_axis_size\n    elif end > image_axis_size:\n        end = image_axis_size\n        start = end - input_axis_size\n\n    return start, end\n\n\ndef zero_pad_and_align_window(image_axis_size, input_axis_size, max_crop_and_size_noise, bidirectional):\n    """"""\n    Adds Zero padding to the image if cropped image is smaller than required window size. \n    """"""\n    pad_width = input_axis_size - image_axis_size + max_crop_and_size_noise * (2 if bidirectional else 1)\n    assert (pad_width >= 0)\n\n    if bidirectional:\n        pad_front = int(pad_width / 2)\n        start = max_crop_and_size_noise\n    else:\n        start, pad_front = 0, 0\n\n    pad_back = pad_width - pad_front\n    end = start + input_axis_size\n    \n    return start, end, pad_front, pad_back\n\n\ndef simple_resize(image_to_resize, size):\n    """"""\n    Resizes image to the required size \n    """"""\n    image_resized = cv2.resize(image_to_resize, (size[1], size[0]), interpolation=cv2.INTER_CUBIC)\n    if len(image_to_resize.shape) == 3 and len(image_resized.shape) == 2 and image_to_resize.shape[2] == 1:\n        image_resized = np.expand_dims(image_resized, 2)\n\n    return image_resized\n\n\ndef crop_image(image, input_size, borders):\n    """"""\n    Crops image to the required size using window location\n    """"""\n    cropped_image = image[borders[0]: borders[1], borders[2]: borders[3]]\n\n    if ((borders[1] - borders[0]) != input_size[0]) or ((borders[3] - borders[2]) != input_size[1]):\n        cropped_image = simple_resize(cropped_image, input_size)\n\n    return cropped_image\n\n\ndef window_location_at_center_point(input_size, center_y, center_x):\n    """"""\n    Calculates window location (top, bottom, left, right) \n    given center point and size of augmentation window\n    """"""\n    half_height = input_size[0] // 2\n    half_width = input_size[1] // 2\n    top = center_y - half_height\n    bottom = center_y + input_size[0] - half_height\n    left = center_x - half_width\n    right = center_x + input_size[1] - half_width\n    return top, bottom, left, right\n\n\ndef sample_crop_best_center(image, input_size, random_number_generator, max_crop_noise, max_crop_size_noise,\n                            best_center, view):\n    """"""\n    Crops using the best center point and ideal window size.\n    Pads small images to have enough room for crop noise and size noise.\n    Applies crop noise in location of the window borders.\n    """"""\n\n    max_crop_noise = np.array(max_crop_noise)\n    crop_noise_multiplier = np.zeros(2, dtype=np.float32)\n\n    if max_crop_noise.any():\n        # there is no point in sampling crop_noise_multiplier if it\'s going to be multiplied by (0, 0)\n        crop_noise_multiplier = random_number_generator.uniform(low=-1.0, high=1.0, size=2)\n\n    center_y, center_x = best_center\n\n    # get the window around the center point. The window might be outside of the image.\n    top, bottom, left, right = window_location_at_center_point(input_size, center_y, center_x)\n\n    pad_y_top, pad_y_bottom, pad_x_right = 0, 0, 0\n\n    if VIEWS.is_cc(view):\n        if image.shape[0] < input_size[0] + (max_crop_noise[0] + max_crop_size_noise) * 2:\n            # Image is smaller than window size + noise margin in y direction.\n            # CC view: pad at both top and bottom\n            top, bottom, pad_y_top, pad_y_bottom = zero_pad_and_align_window(image.shape[0], input_size[0],\n                                                                             max_crop_noise[0] + max_crop_size_noise,\n                                                                             True)\n    elif VIEWS.is_mlo(view):\n        if image.shape[0] < input_size[0] + max_crop_noise[0] + max_crop_size_noise:\n            # Image is smaller than window size + noise margin in y direction.\n            # MLO view: only pad at the bottom\n            top, bottom, _, pad_y_bottom = zero_pad_and_align_window(image.shape[0], input_size[0],\n                                                                     max_crop_noise[0] + max_crop_size_noise, False)\n    else:\n        raise KeyError(""Unknown view"", view)\n\n    if image.shape[1] < input_size[1] + max_crop_noise[1] + max_crop_size_noise:\n        # Image is smaller than window size + noise margin in x direction.\n        left, right, _, pad_x_right = zero_pad_and_align_window(image.shape[1], input_size[1],\n                                                                max_crop_noise[1] + max_crop_size_noise, False)\n\n    # Pad image if necessary by allocating new memory and copying contents over\n    if pad_y_top > 0 or pad_y_bottom > 0 or pad_x_right > 0:\n        new_zero_array = np.zeros((\n            image.shape[0] + pad_y_top + pad_y_bottom,\n            image.shape[1] + pad_x_right, image.shape[2]), dtype=image.dtype)\n        new_zero_array[pad_y_top: image.shape[0] + pad_y_top, 0: image.shape[1]] = image\n        image = new_zero_array\n\n    # if window is drawn outside of image, shift it to be inside the image.\n    top, bottom = shift_window_inside_image(top, bottom, image.shape[0], input_size[0])\n    left, right = shift_window_inside_image(left, right, image.shape[1], input_size[1])\n\n    if top == 0:\n        # there is nowhere to shift upwards, we only apply noise downwards\n        crop_noise_multiplier[0] = np.abs(crop_noise_multiplier[0])\n    elif bottom == image.shape[0]:\n        # there is nowhere to shift down, we only apply noise upwards\n        crop_noise_multiplier[0] = -np.abs(crop_noise_multiplier[0])\n    # else: we do nothing to the noise multiplier\n\n    if left == 0:\n        # there is nowhere to shift left, we only apply noise to move right\n        crop_noise_multiplier[1] = np.abs(crop_noise_multiplier[1])\n    elif right == image.shape[1]:\n        # there is nowhere to shift right, we only apply noise to move left\n        crop_noise_multiplier[1] = -np.abs(crop_noise_multiplier[1])\n    # else: we do nothing to the noise multiplier\n\n    borders = np.array((top, bottom, left, right), dtype=np.int32)\n\n    # Calculate maximum amount of how much the window can move for cropping noise\n    top_margin = top\n    bottom_margin = image.shape[0] - bottom\n    left_margin = left\n    right_margin = image.shape[1] - right\n\n    if crop_noise_multiplier[0] >= 0:\n        vertical_margin = bottom_margin\n    else:\n        vertical_margin = top_margin\n\n    if crop_noise_multiplier[1] >= 0:\n        horizontal_margin = right_margin\n    else:\n        horizontal_margin = left_margin\n\n    if vertical_margin < max_crop_noise[0]:\n        max_crop_noise[0] = vertical_margin\n\n    if horizontal_margin < max_crop_noise[1]:\n        max_crop_noise[1] = horizontal_margin\n\n    crop_noise = np.round(max_crop_noise * crop_noise_multiplier)\n    crop_noise = np.array((crop_noise[0], crop_noise[0], crop_noise[1], crop_noise[1]), dtype=np.int32)\n    borders = borders + crop_noise\n\n    # this is to make sure that the cropping window isn\'t outside of the image\n    assert (borders[0] >= 0) and (borders[1] <= image.shape[0]) and (borders[2] >= 0) and (borders[3] <= image.shape[\n        1]), ""Centre of the crop area is sampled such that the borders are outside of the image. Borders: "" + str(\n        borders) + \', image shape: \' + str(image.shape)\n\n    # return the padded image and cropping window information\n    return image, borders\n\n\ndef sample_crop(image, input_size, borders, random_number_generator, max_crop_size_noise):\n    """"""\n    Applies size noise of the window borders.\n    """"""\n    size_noise_multiplier = random_number_generator.uniform(low=-1.0, high=1.0, size=4)\n\n    top_margin = borders[0]\n    bottom_margin = image.shape[0] - borders[1]\n    left_margin = borders[2]\n    right_margin = image.shape[1] - borders[3]\n\n    max_crop_size_noise = min(max_crop_size_noise, top_margin, bottom_margin, left_margin, right_margin)\n\n    if input_size[0] >= input_size[1]:\n        max_crop_size_vertical_noise = max_crop_size_noise\n        max_crop_size_horizontal_noise = np.round(max_crop_size_noise * (input_size[1] / input_size[0]))\n    elif input_size[0] < input_size[1]:\n        max_crop_size_vertical_noise = np.round(max_crop_size_noise * (input_size[0] / input_size[1]))\n        max_crop_size_horizontal_noise = max_crop_size_noise\n    else:\n        raise RuntimeError()\n\n    max_crop_size_noise = np.array((max_crop_size_vertical_noise, max_crop_size_vertical_noise,\n                                    max_crop_size_horizontal_noise, max_crop_size_horizontal_noise),\n                                   dtype=np.int32)\n    size_noise = np.round(max_crop_size_noise * size_noise_multiplier)\n    size_noise = np.array(size_noise, dtype=np.int32)\n    borders = borders + size_noise\n\n    # this is to make sure that the cropping window isn\'t outside of the image\n    assert (borders[0] >= 0) and (borders[1] <= image.shape[0]) and (borders[2] >= 0) and (borders[3] <= image.shape[\n        1]), ""Center of the crop area is sampled such that the borders are outside of the image. Borders: "" + str(\n        borders) + \', image shape: \' + str(image.shape)\n\n    # Sanity check. make sure that the top is above the bottom\n    assert borders[1] > borders[0], ""Bottom above the top. Top: "" + str(borders[0]) + \', bottom: \' + str(borders[1])\n\n    # Sanity check. make sure that the left is left to the right\n    assert borders[3] > borders[2], ""Left on the right. Left: "" + str(borders[2]) + \', right: \' + str(borders[3])\n\n    return borders\n\n\ndef random_augmentation_best_center(image, input_size, random_number_generator, max_crop_noise=(0, 0),\n                                    max_crop_size_noise=0, auxiliary_image=None,\n                                    best_center=None, view=""""):\n    """"""\n    Crops augmentation window from a given image \n    by applying noise in location and size of the window.\n    """"""\n    joint_image = np.expand_dims(image, 2)\n    if auxiliary_image is not None:\n        joint_image = np.concatenate([joint_image, auxiliary_image], axis=2)\n\n    joint_image, borders = sample_crop_best_center(joint_image, input_size, random_number_generator, max_crop_noise,\n                                                   max_crop_size_noise, best_center, view)\n    borders = sample_crop(joint_image, input_size, borders, random_number_generator, max_crop_size_noise)\n    sampled_joint_image = crop_image(joint_image, input_size, borders)\n\n    if auxiliary_image is None:\n        return sampled_joint_image[:, :, 0], None\n    else:\n        return sampled_joint_image[:, :, 0], sampled_joint_image[:, :, 1:]\n'"
src/data_loading/loading.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\nimport numpy as np\nfrom src.constants import VIEWS, INPUT_SIZE_DICT\nimport src.data_loading.augmentations as augmentations\nfrom src.utilities.reading_images import read_image_mat, read_image_png\n\n\ndef flip_image(image, view, horizontal_flip):\n    """"""\n    If training mode, makes all images face right direction.\n    In medical, keeps the original directions unless horizontal_flip is set.\n    """"""\n    if horizontal_flip == \'NO\':\n        if VIEWS.is_right(view):\n            image = np.fliplr(image)\n    elif horizontal_flip == \'YES\':\n        if VIEWS.is_left(view):\n            image = np.fliplr(image)\n\n    return image\n\n\ndef standard_normalize_single_image(image):\n    """"""\n    Standardizes an image in-place \n    """"""\n    image -= np.mean(image)\n    image /= np.maximum(np.std(image), 10**(-5))\n\n\ndef load_image(image_path, view, horizontal_flip):\n    """"""\n    Loads a png or hdf5 image as floats and flips according to its view.\n    """"""\n    if image_path.endswith(""png""):\n        image = read_image_png(image_path)\n    elif image_path.endswith(""hdf5""):\n        image = read_image_mat(image_path)\n    else:\n        raise RuntimeError()\n    image = image.astype(np.float32)\n    image = flip_image(image, view, horizontal_flip)\n    return image\n\n\ndef load_heatmaps(benign_heatmap_path, malignant_heatmap_path, view, horizontal_flip):\n    """"""\n    Loads two heatmaps as one numpy array\n    """"""\n    assert bool(benign_heatmap_path) == bool(malignant_heatmap_path)\n    benign_heatmap = load_image(benign_heatmap_path, view, horizontal_flip)\n    malignant_heatmap = load_image(malignant_heatmap_path, view, horizontal_flip)\n    heatmaps = np.stack([benign_heatmap, malignant_heatmap], axis=2)\n    return heatmaps\n\n\ndef load_image_and_heatmaps(image_path, benign_heatmap_path, malignant_heatmap_path, view, horizontal_flip):\n    """"""\n    Loads an image and its corresponding heatmaps if required\n    """"""\n    image = load_image(image_path, view, horizontal_flip)\n    assert bool(benign_heatmap_path) == bool(malignant_heatmap_path)\n    if benign_heatmap_path:\n        heatmaps = load_heatmaps(benign_heatmap_path, malignant_heatmap_path, view, horizontal_flip)\n    else:\n        heatmaps = None\n    return image, heatmaps\n\n\ndef augment_and_normalize_image(image, auxiliary_image, view, best_center, random_number_generator,\n                                augmentation, max_crop_noise, max_crop_size_noise):\n    """"""\n    Applies augmentation window with random noise in location and size\n    and return normalized cropped image. \n    """"""\n    view_input_size = INPUT_SIZE_DICT[view]\n    if augmentation:\n        cropped_image, cropped_auxiliary_image = augmentations.random_augmentation_best_center(\n            image=image,\n            input_size=view_input_size,\n            random_number_generator=random_number_generator,\n            max_crop_noise=max_crop_noise,\n            max_crop_size_noise=max_crop_size_noise,\n            auxiliary_image=auxiliary_image,\n            best_center=best_center,\n            view=view,\n        )\n    else:\n        cropped_image, cropped_auxiliary_image = augmentations.random_augmentation_best_center(\n            image=image,\n            input_size=view_input_size,\n            random_number_generator=random_number_generator,\n            max_crop_noise=(0, 0),\n            max_crop_size_noise=0,\n            auxiliary_image=auxiliary_image,\n            best_center=best_center,\n            view=view,\n        )\n    \n    # For test time only, normalize a copy of the cropped image\n    # in order to avoid changing the value of original image which gets augmented multiple times\n    cropped_image = cropped_image.copy()\n    standard_normalize_single_image(cropped_image)\n    \n    return cropped_image, cropped_auxiliary_image\n'"
src/heatmaps/__init__.py,0,b'\n\n\n'
src/heatmaps/models.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines the heatmap generation model used in run_producer.py\n""""""\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.functional import pad\nimport torchvision.models.densenet as densenet\n\n\nclass ModifiedDenseNet121(nn.Module):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.densenet = densenet.densenet121(*args, **kwargs)\n        self._is_modified = False\n\n    def _modify_densenet(self):\n        """"""\n        Replace Conv2d and MaxPool2d to resolve the differences in padding \n        between TensorFlow and PyTorch\n        """"""\n        assert not self._is_modified\n        for full_name, nn_module in self.densenet.named_modules():\n            if isinstance(nn_module, (nn.Conv2d, nn.MaxPool2d)):\n                module_name_parts = full_name.split(""."")\n                parent = self._get_module(self.densenet, module_name_parts[:-1])\n                actual_module_name = module_name_parts[-1]\n                assert ""conv"" in module_name_parts[-1] or ""pool"" in module_name_parts[-1]\n                setattr(parent, actual_module_name, TFSamePadWrapper(nn_module))\n        self._is_modified = True\n\n    def load_from_path(self, model_path):\n        self.densenet.load_state_dict(torch.load(model_path))\n        self._modify_densenet()\n\n    def forward(self, x):\n        if not self._is_modified:\n            self._modify_densenet()\n        features = self.densenet.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n        out = self.densenet.classifier(out)\n        return out\n\n    @classmethod\n    def _get_module(cls, model, module_name_parts):\n        obj = model\n        for module_name_part in module_name_parts:\n            obj = getattr(obj, module_name_part)\n        return obj\n\n\nclass TFSamePadWrapper(nn.Module):\n    """"""\n    Outputs a new convolutional or pooling layer which uses TensorFlow-style ""SAME"" padding\n    """"""\n    def __init__(self, sub_module):\n        super(TFSamePadWrapper, self).__init__()\n        self.sub_module = copy.deepcopy(sub_module)\n        self.sub_module.padding = 0\n        if isinstance(self.sub_module.kernel_size, int):\n            self.kernel_size = (self.sub_module.kernel_size, self.sub_module.kernel_size)\n            self.stride = (self.sub_module.stride, self.sub_module.stride)\n        else:\n            self.kernel_size = self.sub_module.kernel_size\n            self.stride = self.sub_module.stride\n\n    def forward(self, x):\n        return self.sub_module(self.apply_pad(x))\n\n    def apply_pad(self, x):\n        pad_height = self.calculate_padding(x.shape[2], self.kernel_size[0], self.stride[0])\n        pad_width = self.calculate_padding(x.shape[3], self.kernel_size[1], self.stride[1])\n\n        pad_top, pad_left = pad_height // 2, pad_width // 2\n        pad_bottom, pad_right = pad_height - pad_top, pad_width - pad_left\n        return pad(x, [pad_top, pad_bottom, pad_left, pad_right])\n\n    @classmethod\n    def calculate_padding(cls, in_dim, kernel_dim, stride_dim):\n        if in_dim % stride_dim == 0:\n            return max(0, kernel_dim - stride_dim)\n        return max(0, kernel_dim - (in_dim % stride_dim))\n'"
src/heatmaps/run_producer.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nGenerates benign and malignant heatmaps for cropped images using patch classifier. \n""""""\nimport numpy as np\nimport random\nimport os\nimport argparse\nimport tqdm\n\nimport torch\nimport torch.nn.functional as F\n\nimport src.heatmaps.models as models\nimport src.data_loading.loading as loading\nimport src.utilities.pickling as pickling\nimport src.utilities.saving_images as saving_images\nimport src.utilities.tools as tools\nfrom src.constants import VIEWS\n\n\ndef stride_list_generator(img_width, patch_size, more_patches=0, stride_fixed=-1):\n    """"""\n    Determines how an image should be split up into patches \n    """"""\n    if stride_fixed != -1:\n        patch_num_lower_bound = (img_width - patch_size) // stride_fixed + 1\n        pixel_left = (img_width - patch_size) % stride_fixed\n        more_patches = 0\n    else:\n        patch_num_lower_bound = img_width // patch_size\n        pixel_left = img_width % patch_size\n        stride_fixed = patch_size\n        \n    if pixel_left == 0 and more_patches == 0:\n        stride = stride_fixed\n        patch_num = patch_num_lower_bound\n        sliding_steps = patch_num - 1\n        stride_list = [stride] * sliding_steps\n    else:\n        pixel_overlap = stride_fixed - pixel_left + more_patches * stride_fixed\n        patch_num = patch_num_lower_bound + 1 + more_patches\n        sliding_steps = patch_num - 1\n        \n        stride_avg = stride_fixed - pixel_overlap // sliding_steps\n        \n        sliding_steps_smaller = pixel_overlap % sliding_steps\n        stride_smaller = stride_avg - 1\n        \n        stride_list = [stride_avg] * sliding_steps\n\n        for step in random.sample(range(sliding_steps), sliding_steps_smaller):\n            stride_list[step] = stride_smaller\n            \n    return stride_list\n\n\ndef prediction_by_batch(minibatch_patches, model, device, parameters):\n    """"""\n    Puts patches into a batch and gets predictions of patch classifier.\n    """"""\n    minibatch_x = np.stack((minibatch_patches,) * parameters[\'input_channels\'], axis=-1).reshape(\n        -1, parameters[\'patch_size\'], parameters[\'patch_size\'], parameters[\'input_channels\']\n    ).transpose(0, 3, 1, 2)\n\n    with torch.no_grad():\n        output = F.softmax(model(torch.FloatTensor(minibatch_x).to(device)), dim=1).cpu().detach().numpy()\n    return output\n\n\ndef ori_image_prepare(image_path, view, horizontal_flip, parameters):\n    """"""\n    Loads an image and creates stride_lists\n    """"""\n    patch_size = parameters[\'patch_size\']\n    more_patches = parameters[\'more_patches\']\n    stride_fixed = parameters[\'stride_fixed\']\n\n    image = loading.load_image(image_path, view, horizontal_flip)\n    image = image.astype(float)\n    loading.standard_normalize_single_image(image)\n    \n    img_width, img_length = image.shape\n    width_stride_list = stride_list_generator(img_width, patch_size, more_patches, stride_fixed)\n    length_stride_list = stride_list_generator(img_length, patch_size, more_patches, stride_fixed)\n\n    return image, width_stride_list, length_stride_list\n\n\ndef patch_batch_prepare(image, length_stride_list, width_stride_list, patch_size):\n    """"""\n    Samples patches from an image according to stride_lists\n    """"""\n    min_x, min_y = 0, 0\n    minibatch_patches = []\n    img_width, img_length = image.shape\n\n    for stride_y in length_stride_list + [0]:\n        for stride_x in width_stride_list + [-(img_width - patch_size)]:\n            patch = image[min_x:min_x + patch_size, min_y:min_y + patch_size]\n            minibatch_patches.append(np.expand_dims(patch, axis=2))\n            min_x += stride_x\n        min_y += stride_y\n    \n    return minibatch_patches\n\n\ndef probabilities_to_heatmap(patch_counter, all_prob, image_shape, length_stride_list, width_stride_list,\n                             patch_size, heatmap_type):\n    """"""\n    Generates heatmaps using output of patch classifier\n    """"""\n    min_x, min_y = 0, 0\n    \n    prob_map = np.zeros(image_shape, dtype=np.float32)\n    count_map = np.zeros(image_shape, dtype=np.float32)\n    \n    img_width, img_length = image_shape\n\n    for stride_y in length_stride_list + [0]:\n        for stride_x in width_stride_list + [-(img_width - patch_size)]:\n            prob_map[min_x:min_x + patch_size, min_y:min_y + patch_size] += all_prob[\n                patch_counter, heatmap_type\n            ]\n            count_map[min_x:min_x + patch_size, min_y:min_y + patch_size] += 1\n            min_x += stride_x\n            patch_counter += 1\n        min_y += stride_y\n    \n    heatmap = prob_map / count_map\n    \n    return heatmap, patch_counter\n\n\ndef get_all_prob(all_patches, minibatch_size, model, device, parameters):   \n    """"""\n    Gets predictions for all sampled patches\n    """"""\n    all_prob = np.zeros((len(all_patches), parameters[\'number_of_classes\']))\n\n    for i, minibatch in enumerate(tools.partition_batch(all_patches, minibatch_size)):\n        minibatch_prob = prediction_by_batch(minibatch, model, device, parameters)\n        all_prob[i * minibatch_size: i * minibatch_size + minibatch_prob.shape[0]] = minibatch_prob\n                \n    return all_prob.astype(np.float32)\n\n\ndef save_heatmaps(heatmap_malignant, heatmap_benign, short_file_path, view, horizontal_flip, parameters):\n    """"""\n    Saves the heatmaps after flipping back to the original direction\n    """"""\n    heatmap_malignant = loading.flip_image(heatmap_malignant, view, horizontal_flip)\n    heatmap_benign = loading.flip_image(heatmap_benign, view, horizontal_flip)\n    heatmap_save_path_malignant = os.path.join(\n        parameters[\'save_heatmap_path\'][0], \n        short_file_path + \'.hdf5\'\n    )\n    saving_images.save_image_as_hdf5(heatmap_malignant, heatmap_save_path_malignant)\n\n    heatmap_save_path_benign = os.path.join(\n        parameters[\'save_heatmap_path\'][1],\n        short_file_path + \'.hdf5\'\n    )\n    saving_images.save_image_as_hdf5(heatmap_benign, heatmap_save_path_benign)\n\n\ndef get_image_path(short_file_path, parameters):\n    """"""\n    Convert short_file_path to full file path\n    """"""\n    image_extension = \'.hdf5\' if parameters[\'use_hdf5\'] else \'.png\'\n    return os.path.join(parameters[\'original_image_path\'], short_file_path + image_extension)\n\n\ndef sample_patches(exam, parameters):\n    """"""\n    Samples patches for one exam\n    """"""\n    all_patches = []\n    all_cases = []\n    for view in VIEWS.LIST:\n        for short_file_path in exam[view]:\n            image_path = get_image_path(short_file_path, parameters)\n            patches, case = sample_patches_single(\n                image_path=image_path,\n                view=view,\n                horizontal_flip=exam[\'horizontal_flip\'],\n                parameters=parameters,\n            )\n\n            all_patches += patches\n            all_cases.append([short_file_path] + case)\n\n    return all_patches, all_cases\n\n\ndef sample_patches_single(image_path, view, horizontal_flip, parameters):\n    """"""\n    Sample patches for a single mammogram image\n    """"""\n    image, width_stride_list, length_stride_list = ori_image_prepare(\n        image_path,\n        view,\n        horizontal_flip,\n        parameters,\n    )\n    patches = patch_batch_prepare(\n        image,\n        length_stride_list,\n        width_stride_list,\n        parameters[\'patch_size\'],\n    )\n    case = [\n        image.shape,\n        view,\n        horizontal_flip,\n        width_stride_list,\n        length_stride_list,\n    ]\n    return patches, case\n\n\ndef making_heatmap_with_large_minibatch_potential(parameters, model, exam_list, device):\n    """"""\n    Samples patches for each exam, gets batch prediction, creates and saves heatmaps\n    """"""\n    minibatch_size = parameters[\'minibatch_size\']\n    \n    os.makedirs(parameters[\'save_heatmap_path\'][0], exist_ok=True)\n    os.makedirs(parameters[\'save_heatmap_path\'][1], exist_ok=True)\n    \n    for exam in tqdm.tqdm(exam_list):\n        \n        # create patches and other information with the images\n        all_patches, all_cases = sample_patches(exam, parameters)\n\n        if len(all_patches) != 0:\n            all_prob = get_all_prob(\n                all_patches, \n                minibatch_size, \n                model,\n                device,\n                parameters\n            )\n        \n            del all_patches\n            \n            patch_counter = 0\n        \n            for (short_file_path, image_shape, view, horizontal_flip, width_stride_list, length_stride_list) \\\n                    in all_cases:\n\n                heatmap_malignant, _ = probabilities_to_heatmap(\n                    patch_counter, \n                    all_prob, \n                    image_shape, \n                    length_stride_list, \n                    width_stride_list, \n                    parameters[\'patch_size\'], \n                    parameters[\'heatmap_type\'][0]\n                )\n                heatmap_benign, patch_counter = probabilities_to_heatmap(\n                    patch_counter, \n                    all_prob, \n                    image_shape, \n                    length_stride_list, \n                    width_stride_list, \n                    parameters[\'patch_size\'], \n                    parameters[\'heatmap_type\'][1]\n                )\n                save_heatmaps(\n                    heatmap_malignant, \n                    heatmap_benign, \n                    short_file_path, \n                    view, \n                    horizontal_flip, \n                    parameters\n                )\n\n                del heatmap_malignant, heatmap_benign\n                \n            del all_prob, all_cases\n\n\ndef load_model(parameters):\n    """"""\n    Load trained patch classifier\n    """"""\n    if (parameters[""device_type""] == ""gpu"") and torch.has_cudnn:\n        device = torch.device(""cuda:{}"".format(parameters[""gpu_number""]))\n    else:\n        device = torch.device(""cpu"")\n\n    model = models.ModifiedDenseNet121(num_classes=parameters[\'number_of_classes\'])\n    model.load_from_path(parameters[""initial_parameters""])\n    model = model.to(device)\n    model.eval()\n    return model, device\n\n\ndef produce_heatmaps(model, device, parameters):\n    """"""\n    Generates heatmaps for all exams\n    """"""\n    # Load exam info\n    exam_list = pickling.unpickle_from_file(parameters[\'data_file\'])    \n\n    # Create heatmaps\n    making_heatmap_with_large_minibatch_potential(parameters, model, exam_list, device)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Produce Heatmaps\')\n    parser.add_argument(\'--model-path\', required=True)\n    parser.add_argument(\'--data-path\', required=True)\n    parser.add_argument(\'--image-path\', required=True)\n    parser.add_argument(\'--batch-size\', default=100, type=int)\n    parser.add_argument(\'--output-heatmap-path\', required=True)\n    parser.add_argument(\'--seed\', default=0, type=int)\n    parser.add_argument(\'--device-type\', default=""cpu"", choices=[\'gpu\', \'cpu\'])\n    parser.add_argument(""--gpu-number"", type=int, default=0)\n    parser.add_argument(""--use-hdf5"", action=""store_true"")\n    args = parser.parse_args()\n\n    parameters = dict(\n        device_type=args.device_type,\n        gpu_number=args.gpu_number,\n        \n        patch_size=256,\n\n        stride_fixed=70,\n        more_patches=5,\n        minibatch_size=args.batch_size,\n        seed=args.seed,\n        \n        initial_parameters=args.model_path,\n        input_channels=3,\n        number_of_classes=4,\n        \n        data_file=args.data_path,\n        original_image_path=args.image_path,\n        save_heatmap_path=[os.path.join(args.output_heatmap_path, \'heatmap_malignant\'),\n                           os.path.join(args.output_heatmap_path, \'heatmap_benign\')],\n        \n        heatmap_type=[0, 1],  # 0: malignant 1: benign 0: nothing\n\n        use_hdf5=args.use_hdf5\n    )\n    random.seed(parameters[\'seed\'])\n    model, device = load_model(parameters)\n    produce_heatmaps(model, device, parameters)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/heatmaps/run_producer_single.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nGenerates benign and malignant heatmaps for cropped images using patch classifier.\n""""""\nimport argparse\nimport random\n\nimport src.data_loading.loading as loading\nimport src.utilities.pickling as pickling\nimport src.utilities.saving_images as saving_images\n\nimport src.heatmaps.run_producer as run_producer\n\n\ndef produce_heatmaps(parameters):\n    """"""\n    Generate heatmaps for single example\n    """"""\n    random.seed(parameters[\'seed\'])\n    image_path = parameters[""cropped_mammogram_path""]\n    model, device = run_producer.load_model(parameters)\n    metadata = pickling.unpickle_from_file(parameters[\'metadata_path\'])\n    patches, case = run_producer.sample_patches_single(\n        image_path=image_path,\n        view=metadata[""view""],\n        horizontal_flip=metadata[\'horizontal_flip\'],\n        parameters=parameters,\n    )\n    all_prob = run_producer.get_all_prob(\n        all_patches=patches,\n        minibatch_size=parameters[""minibatch_size""],\n        model=model,\n        device=device,\n        parameters=parameters\n    )\n    heatmap_malignant, _ = run_producer.probabilities_to_heatmap(\n        patch_counter=0,\n        all_prob=all_prob,\n        image_shape=case[0],\n        length_stride_list=case[4],\n        width_stride_list=case[3],\n        patch_size=parameters[\'patch_size\'],\n        heatmap_type=parameters[\'heatmap_type\'][0],\n    )\n    heatmap_benign, patch_counter = run_producer.probabilities_to_heatmap(\n        patch_counter=0,\n        all_prob=all_prob,\n        image_shape=case[0],\n        length_stride_list=case[4],\n        width_stride_list=case[3],\n        patch_size=parameters[\'patch_size\'],\n        heatmap_type=parameters[\'heatmap_type\'][1],\n    )\n    heatmap_malignant = loading.flip_image(\n        image=heatmap_malignant,\n        view=metadata[""view""],\n        horizontal_flip=metadata[""horizontal_flip""],\n    )\n    heatmap_benign = loading.flip_image(\n        image=heatmap_benign,\n        view=metadata[""view""],\n        horizontal_flip=metadata[""horizontal_flip""],\n    )\n    saving_images.save_image_as_hdf5(\n        image=heatmap_malignant,\n        filename=parameters[""heatmap_path_malignant""],\n    )\n    saving_images.save_image_as_hdf5(\n        image=heatmap_benign,\n        filename=parameters[""heatmap_path_benign""],\n    )\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Produce Heatmaps\')\n    parser.add_argument(\'--model-path\', required=True)\n    parser.add_argument(\'--cropped-mammogram-path\', required=True)\n    parser.add_argument(\'--metadata-path\', required=True)\n    parser.add_argument(\'--batch-size\', default=100, type=int)\n    parser.add_argument(\'--heatmap-path-malignant\', required=True)\n    parser.add_argument(\'--heatmap-path-benign\', required=True)\n    parser.add_argument(\'--seed\', default=0, type=int)\n    parser.add_argument(\'--device-type\', default=""cpu"", choices=[\'gpu\', \'cpu\'])\n    parser.add_argument(""--gpu-number"", type=int, default=0)\n    parser.add_argument(""--use-hdf5"", action=""store_true"")\n    args = parser.parse_args()\n\n    parameters = dict(\n        device_type=args.device_type,\n        gpu_number=args.gpu_number,\n\n        patch_size=256,\n\n        stride_fixed=70,\n        more_patches=5,\n        minibatch_size=args.batch_size,\n        seed=args.seed,\n\n        initial_parameters=args.model_path,\n        input_channels=3,\n        number_of_classes=4,\n\n        cropped_mammogram_path=args.cropped_mammogram_path,\n        metadata_path=args.metadata_path,\n        heatmap_path_malignant=args.heatmap_path_malignant,\n        heatmap_path_benign=args.heatmap_path_benign,\n\n        heatmap_type=[0, 1],  # 0: malignant 1: benign 0: nothing\n\n        use_hdf5=args.use_hdf5\n    )\n    produce_heatmaps(parameters)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/heatmaps/run_producer_single_tf.py,7,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nGenerates benign and malignant heatmaps for cropped images using patch classifier.\n""""""\nimport argparse\nimport json\nimport numpy as np\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom nets.densenet import densenet121, densenet_arg_scope\nimport torch\n\nimport src.data_loading.loading as loading\nimport src.utilities.pickling as pickling\nimport src.utilities.saving_images as saving_images\nimport src.utilities.tf_utils as tf_utils\nimport src.utilities.tools as tools\n\nimport src.heatmaps.run_producer as run_producer\n\n\ndef construct_densenet_match_dict(tf_variables, torch_weights, tf_torch_weights_map):\n    tf_var_dict = {var.name: var for var in tf_variables}\n    match_dict = {}\n    for tf_key, v in tf_var_dict.items():\n        torch_w = torch_weights[tf_torch_weights_map[tf_key]].cpu().numpy()\n        if tf_key == ""densenet121/logits/weights:0"":\n            torch_w = tf_utils.convert_fc_weight_torch2tf(torch_w)[[np.newaxis, np.newaxis]]\n        elif len(v.shape) == 4:\n            torch_w = tf_utils.convert_conv_torch2tf(torch_w)\n        elif len(v.shape) == 2:\n            torch_w = tf_utils.convert_fc_weight_torch2tf(torch_w)\n        match_dict[v] = torch_w\n    return match_dict\n\n\ndef load_model_tf(parameters):\n    # Setup model params\n    if (parameters[""device_type""] == ""gpu"") and tf.test.is_gpu_available():\n        device_str = ""/device:GPU:{}"".format(parameters[""gpu_number""])\n    else:\n        device_str = ""/cpu:0""\n\n    # Setup Graph\n    graph = tf.Graph()\n    with graph.as_default():\n        with tf.device(device_str):\n            x = tf.placeholder(tf.float32, [None, 256, 256, 3])\n            with slim.arg_scope(densenet_arg_scope(weight_decay=0.0, data_format=\'NHWC\')):\n                densenet121_net, end_points = densenet121(\n                    x,\n                    num_classes=parameters[""number_of_classes""],\n                    data_format=\'NHWC\',\n                    is_training=False,\n                )\n            y_logits = densenet121_net[:, 0, 0, :]\n            y = tf.nn.softmax(y_logits)\n\n    # Load weights\n    sess = tf.Session(graph=graph, config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n    ))\n    with open(parameters[""tf_torch_weights_map_path""]) as f:\n        tf_torch_weights_map = json.loads(f.read())\n\n    with sess.as_default():\n        torch_weights = torch.load(parameters[""initial_parameters""])\n        match_dict = construct_densenet_match_dict(\n            tf_variables=tf_utils.get_tf_variables(graph, batch_norm_key=""BatchNorm""),\n            torch_weights=torch_weights,\n            tf_torch_weights_map=tf_torch_weights_map\n        )\n        sess.run(tf_utils.construct_weight_assign_ops(match_dict))\n\n    return sess, x, y\n\n\ndef prediction_by_batch_tf(minibatch_patches, sess, x, y, parameters):\n    """"""\n    Puts patches into a batch and gets predictions of patch classifier.\n    """"""\n    minibatch_x = np.stack((minibatch_patches,) * parameters[\'input_channels\'], axis=-1).reshape(\n        -1, parameters[\'patch_size\'], parameters[\'patch_size\'], parameters[\'input_channels\']\n    )\n\n    output = sess.run(y, feed_dict={x: minibatch_x})\n    return output\n\n\ndef get_all_prob_tf(all_patches, minibatch_size, sess, x, y, parameters):\n    """"""\n    Gets predictions for all sampled patches\n    """"""\n    all_prob = np.zeros((len(all_patches), parameters[\'number_of_classes\']))\n\n    for i, minibatch in enumerate(tools.partition_batch(all_patches, minibatch_size)):\n        minibatch_prob = prediction_by_batch_tf(minibatch, sess, x, y, parameters)\n        all_prob[i * minibatch_size: i * minibatch_size + minibatch_prob.shape[0]] = minibatch_prob\n\n    return all_prob.astype(np.float32)\n\n\ndef produce_heatmaps(parameters):\n    """"""\n    Generate heatmaps for single example\n    """"""\n    random.seed(parameters[\'seed\'])\n    image_path = parameters[""cropped_mammogram_path""]\n    sess, x, y = load_model_tf(parameters)\n    metadata = pickling.unpickle_from_file(parameters[\'metadata_path\'])\n    patches, case = run_producer.sample_patches_single(\n        image_path=image_path,\n        view=metadata[""view""],\n        horizontal_flip=metadata[\'horizontal_flip\'],\n        parameters=parameters,\n    )\n    all_prob = get_all_prob_tf(\n        all_patches=patches,\n        minibatch_size=parameters[""minibatch_size""],\n        sess=sess,\n        x=x,\n        y=y,\n        parameters=parameters\n    )\n    heatmap_malignant, _ = run_producer.probabilities_to_heatmap(\n        patch_counter=0,\n        all_prob=all_prob,\n        image_shape=case[0],\n        length_stride_list=case[4],\n        width_stride_list=case[3],\n        patch_size=parameters[\'patch_size\'],\n        heatmap_type=parameters[\'heatmap_type\'][0],\n    )\n    heatmap_benign, patch_counter = run_producer.probabilities_to_heatmap(\n        patch_counter=0,\n        all_prob=all_prob,\n        image_shape=case[0],\n        length_stride_list=case[4],\n        width_stride_list=case[3],\n        patch_size=parameters[\'patch_size\'],\n        heatmap_type=parameters[\'heatmap_type\'][1],\n    )\n    heatmap_malignant = loading.flip_image(\n        image=heatmap_malignant,\n        view=metadata[""view""],\n        horizontal_flip=metadata[""horizontal_flip""],\n    )\n    heatmap_benign = loading.flip_image(\n        image=heatmap_benign,\n        view=metadata[""view""],\n        horizontal_flip=metadata[""horizontal_flip""],\n    )\n    saving_images.save_image_as_hdf5(\n        image=heatmap_malignant,\n        filename=parameters[""heatmap_path_malignant""],\n    )\n    saving_images.save_image_as_hdf5(\n        image=heatmap_benign,\n        filename=parameters[""heatmap_path_benign""],\n    )\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Produce Heatmaps\')\n    parser.add_argument(\'--model-path\', required=True)\n    parser.add_argument(\'--tf-torch-weights-map-path\', required=True)\n    parser.add_argument(\'--cropped-mammogram-path\', required=True)\n    parser.add_argument(\'--metadata-path\', required=True)\n    parser.add_argument(\'--batch-size\', default=100, type=int)\n    parser.add_argument(\'--heatmap-path-malignant\', required=True)\n    parser.add_argument(\'--heatmap-path-benign\', required=True)\n    parser.add_argument(\'--seed\', default=0, type=int)\n    parser.add_argument(\'--device-type\', default=""cpu"", choices=[\'gpu\', \'cpu\'])\n    parser.add_argument(""--gpu-number"", type=int, default=0)\n    parser.add_argument(""--use-hdf5"", action=""store_true"")\n    args = parser.parse_args()\n\n    parameters = dict(\n        device_type=args.device_type,\n        gpu_number=args.gpu_number,\n\n        patch_size=256,\n\n        stride_fixed=70,\n        more_patches=5,\n        minibatch_size=args.batch_size,\n        seed=args.seed,\n\n        initial_parameters=args.model_path,\n        input_channels=3,\n        number_of_classes=4,\n        tf_torch_weights_map_path=args.tf_torch_weights_map_path,\n\n        cropped_mammogram_path=args.cropped_mammogram_path,\n        metadata_path=args.metadata_path,\n        heatmap_path_malignant=args.heatmap_path_malignant,\n        heatmap_path_benign=args.heatmap_path_benign,\n\n        heatmap_type=[0, 1],  # 0: malignant 1: benign\n\n        use_hdf5=args.use_hdf5\n    )\n    produce_heatmaps(parameters)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/modeling/__init__.py,0,b''
src/modeling/layers.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines layers used in models.py.\n""""""\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models.resnet import conv3x3\n\nfrom src.constants import VIEWS\n\n\nclass OutputLayer(nn.Module):\n    def __init__(self, in_features, output_shape):\n        super(OutputLayer, self).__init__()\n        if not isinstance(output_shape, (list, tuple)):\n            output_shape = [output_shape]\n        self.output_shape = output_shape\n        self.flattened_output_shape = int(np.prod(output_shape))\n        self.fc_layer = nn.Linear(in_features, self.flattened_output_shape)\n\n    def forward(self, x):\n        h = self.fc_layer(x)\n        if len(self.output_shape) > 1:\n            h = h.view(h.shape[0], *self.output_shape)\n        h = F.log_softmax(h, dim=-1)\n        return h\n\n\nclass BasicBlockV2(nn.Module):\n    """"""\n    Adapted fom torchvision ResNet, converted to v2\n    """"""\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlockV2, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = conv3x3(inplanes, planes, stride=stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride=1)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        # Phase 1\n        out = self.bn1(x)\n        out = self.relu(out)\n        if self.downsample is not None:\n            residual = self.downsample(out)\n        out = self.conv1(out)\n\n        # Phase 2\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out += residual\n\n        return out\n\n\nclass AllViewsGaussianNoise(nn.Module):\n    """"""Add gaussian noise across all 4 views""""""\n\n    def __init__(self, gaussian_noise_std):\n        super(AllViewsGaussianNoise, self).__init__()\n        self.gaussian_noise_std = gaussian_noise_std\n\n    def forward(self, x):\n        return {\n            VIEWS.L_CC: self.single_add_gaussian_noise(x[VIEWS.L_CC]),\n            VIEWS.L_MLO: self.single_add_gaussian_noise(x[VIEWS.L_MLO]),\n            VIEWS.R_CC: self.single_add_gaussian_noise(x[VIEWS.R_CC]),\n            VIEWS.R_MLO: self.single_add_gaussian_noise(x[VIEWS.R_MLO]),\n        }\n\n    def single_add_gaussian_noise(self, single_view):\n        if not self.gaussian_noise_std or not self.training:\n            return single_view\n        return single_view + single_view.new(single_view.shape).normal_(std=self.gaussian_noise_std)\n\n\nclass AllViewsAvgPool(nn.Module):\n    """"""Average-pool across all 4 views""""""\n\n    def __init__(self):\n        super(AllViewsAvgPool, self).__init__()\n\n    def forward(self, x):\n        return {\n            view_name: self.single_avg_pool(view_tensor)\n            for view_name, view_tensor in x.items()\n        }\n\n    @staticmethod\n    def single_avg_pool(single_view):\n        n, c, _, _ = single_view.size()\n        return single_view.view(n, c, -1).mean(-1)\n'"
src/modeling/layers_tf.py,18,"b'import numpy as np\n\nimport tensorflow as tf\n\n# Todo: remove data_format\n\n_BATCH_NORM_DECAY = 0.997\n_BATCH_NORM_EPSILON = 1e-5\n_BLOCK_EXPANSION = 1\n\n\ndef output_layer(inputs, output_shape):\n    with tf.variable_scope(""output_layer""):\n        flattened_output_shape = int(np.prod(output_shape))\n        h = tf.layers.dense(inputs=inputs, units=flattened_output_shape)\n        if len(output_shape) > 1:\n            h = tf.reshape(h, [-1] + list(output_shape))\n        h = tf.nn.log_softmax(h, axis=-1)\n        return h\n\n\ndef batch_norm(inputs, training, data_format, name=None):\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n        scale=True, training=training, fused=True, name=name,\n    )\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format,\n                         padding=""SAME"", name=None):\n    return tf.layers.conv2d(\n        inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n        padding=padding, use_bias=False,\n        kernel_initializer=tf.variance_scaling_initializer(),\n        data_format=data_format,\n        name=name,\n    )\n\n\ndef conv3x3(inputs, planes, data_format, strides, name=None):\n    inputs_shape = inputs.shape.as_list()\n\n    # Basically: Pad a dimension if it\'s even, and the slice off the extra output\n    do_pad = False\n    if inputs_shape[2] % 2 == 0:\n        h_in_pad = [0, 1]\n        h_out_slice = slice(None, -1)\n        do_pad = True\n    else:\n        h_in_pad = [0, 0]\n        h_out_slice = slice(None)\n    if inputs_shape[3] % 2 == 0:\n        w_in_pad = [0, 1]\n        w_out_slice = slice(None, -1)\n        do_pad = True\n    else:\n        w_in_pad = [0, 0]\n        w_out_slice = slice(None)\n\n    if do_pad:\n        inputs = tf.pad(inputs, [[0, 0], [0, 0], h_in_pad, w_in_pad], ""CONSTANT"")\n\n    conv_out = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=planes,\n        kernel_size=3,\n        strides=strides,\n        data_format=data_format,\n        name=name,\n    )\n\n    if do_pad:\n        conv_out = conv_out[:, :, h_out_slice, w_out_slice]\n    return conv_out\n\n\ndef conv1x1(inputs, planes, data_format, strides, name=None):\n    return conv2d_fixed_padding(\n        inputs=inputs,\n        filters=planes,\n        kernel_size=1,\n        strides=strides,\n        data_format=data_format,\n        name=name,\n    )\n\n\ndef basic_block_v2(inputs, planes, training, data_format, strides, downsample=None):\n    with tf.variable_scope(""basic_block""):\n        residual = inputs\n\n        # Phase 1\n        out = batch_norm(inputs, training, data_format, name=""bn1"")\n        out = tf.nn.relu(out)\n        if downsample:\n            """"""\n            residual = conv1x1(x, planes * BLOCK_EXPANSION, data_format, \n                               strides=strides,\n                               name=""downsample"")\n            """"""\n            residual = tf.layers.conv2d(\n                inputs=out,\n                filters=planes * _BLOCK_EXPANSION,\n                kernel_size=1,\n                strides=strides,\n                padding=\'VALID\',\n                use_bias=False,\n                kernel_initializer=tf.variance_scaling_initializer(),\n                data_format=data_format,\n                name=""downsample"",\n            )\n        out = conv3x3(out, planes, data_format, strides=strides, name=""conv1"")\n\n        # Phase 2\n        out = batch_norm(out, training, data_format, name=""bn2"")\n        out = tf.nn.relu(out)\n        out = conv3x3(out, planes, data_format, strides=1, name=""conv2"")\n\n        out = out + residual\n        return out\n\n\ndef gaussian_noise_layer(inputs, std, training):\n    with tf.variable_scope(""gaussian_noise_layer""):\n        if training:\n            noise = tf.random_normal(tf.shape(inputs), mean=0.0, stddev=std, dtype=tf.float32)\n            output = tf.add_n([inputs, noise])\n            return output\n        else:\n            return tf.identity(inputs)\n\n\ndef avg_pool_layer(inputs):\n    return tf.reduce_mean(inputs, axis=(2, 3), name=""avg_pool"")\n'"
src/modeling/models.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines architectures for breast cancer classification models. \n""""""\nimport collections as col\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport src.modeling.layers as layers\nfrom src.constants import VIEWS, VIEWANGLES\n\n\nclass SplitBreastModel(nn.Module):\n    def __init__(self, input_channels):\n        super(SplitBreastModel, self).__init__()\n\n        self.four_view_resnet = FourViewResNet(input_channels)\n\n        self.fc1_cc = nn.Linear(256 * 2, 256 * 2)\n        self.fc1_mlo = nn.Linear(256 * 2, 256 * 2)\n        self.output_layer_cc = layers.OutputLayer(256 * 2, (4, 2))\n        self.output_layer_mlo = layers.OutputLayer(256 * 2, (4, 2))\n\n        self.all_views_avg_pool = layers.AllViewsAvgPool()\n        self.all_views_gaussian_noise_layer = layers.AllViewsGaussianNoise(0.01)\n\n    def forward(self, x):\n        h = self.all_views_gaussian_noise_layer(x)\n        result = self.four_view_resnet(h)\n        h = self.all_views_avg_pool(result)\n\n        # Pool, flatten, and fully connected layers\n        h_cc = torch.cat([h[VIEWS.L_CC], h[VIEWS.R_CC]], dim=1)\n        h_mlo = torch.cat([h[VIEWS.L_MLO], h[VIEWS.R_MLO]], dim=1)\n\n        h_cc = F.relu(self.fc1_cc(h_cc))\n        h_mlo = F.relu(self.fc1_mlo(h_mlo))\n\n        h_cc = self.output_layer_cc(h_cc)\n        h_mlo = self.output_layer_mlo(h_mlo)\n\n        h = {\n            VIEWANGLES.CC: h_cc,\n            VIEWANGLES.MLO: h_mlo,\n        }\n\n        return h\n\n\nclass ImageBreastModel(nn.Module):\n    def __init__(self, input_channels):\n        super(ImageBreastModel, self).__init__()\n\n        self.four_view_resnet = FourViewResNet(input_channels)\n\n        self.fc1_lcc = nn.Linear(256, 256)\n        self.fc1_rcc = nn.Linear(256, 256)\n        self.fc1_lmlo = nn.Linear(256, 256)\n        self.fc1_rmlo = nn.Linear(256, 256)\n        self.output_layer_lcc = layers.OutputLayer(256, (4, 2))\n        self.output_layer_rcc = layers.OutputLayer(256, (4, 2))\n        self.output_layer_lmlo = layers.OutputLayer(256, (4, 2))\n        self.output_layer_rmlo = layers.OutputLayer(256, (4, 2))\n\n        self.all_views_avg_pool = layers.AllViewsAvgPool()\n        self.all_views_gaussian_noise_layer = layers.AllViewsGaussianNoise(0.01)\n\n    def forward(self, x):\n        h = self.all_views_gaussian_noise_layer(x)\n        result = self.four_view_resnet(h)\n        h = self.all_views_avg_pool(result)\n\n        h_lcc = F.relu(self.fc1_lcc(h[VIEWS.L_CC]))\n        h_rcc = F.relu(self.fc1_rcc(h[VIEWS.R_CC]))\n        h_lmlo = F.relu(self.fc1_lmlo(h[VIEWS.L_MLO]))\n        h_rmlo = F.relu(self.fc1_rmlo(h[VIEWS.R_MLO]))\n\n        h_lcc = self.output_layer_lcc(h_lcc)\n        h_rcc = self.output_layer_rcc(h_rcc)\n        h_lmlo = self.output_layer_lmlo(h_lmlo)\n        h_rmlo = self.output_layer_rmlo(h_rmlo)\n\n        h = {\n            VIEWS.L_CC: h_lcc,\n            VIEWS.R_CC: h_rcc,\n            VIEWS.L_MLO: h_lmlo,\n            VIEWS.R_MLO: h_rmlo,\n        }\n\n        return h\n\n\nclass SingleImageBreastModel(nn.Module):\n    def __init__(self, input_channels):\n        super(SingleImageBreastModel, self).__init__()\n\n        self.view_resnet = resnet22(input_channels)\n\n        self.fc1 = nn.Linear(256, 256)\n        self.output_layer = layers.OutputLayer(256, (2, 2))\n\n        self.all_views_avg_pool = layers.AllViewsAvgPool()\n        self.all_views_gaussian_noise_layer = layers.AllViewsGaussianNoise(0.01)\n\n    def forward(self, x):\n        h = self.all_views_gaussian_noise_layer.single_add_gaussian_noise(x)\n        result = self.view_resnet(h)\n        h = self.all_views_avg_pool.single_avg_pool(result)\n        h = F.relu(self.fc1(h))\n        h = self.output_layer(h)[:2]\n        return h\n\n    def load_state_from_shared_weights(self, state_dict, view):\n        view_angle = view.lower().split(""-"")[-1]\n        view_key = view.lower().replace(""-"", """")\n        self.view_resnet.load_state_dict(\n            filter_strip_prefix(state_dict, ""four_view_resnet.{}."".format(view_angle))\n        )\n        self.fc1.load_state_dict(\n            filter_strip_prefix(state_dict, ""fc1_{}."".format(view_key))\n        )\n        self.output_layer.load_state_dict({\n            ""fc_layer.weight"": state_dict[""output_layer_{}.fc_layer.weight"".format(view_key)][:4],\n            ""fc_layer.bias"": state_dict[""output_layer_{}.fc_layer.bias"".format(view_key)][:4],\n        })\n\n\nclass FourViewResNet(nn.Module):\n    def __init__(self, input_channels):\n        super(FourViewResNet, self).__init__()\n\n        self.cc = resnet22(input_channels)\n        self.mlo = resnet22(input_channels)\n        self.model_dict = {}\n        self.model_dict[VIEWS.L_CC] = self.l_cc = self.cc\n        self.model_dict[VIEWS.L_MLO] = self.l_mlo = self.mlo\n        self.model_dict[VIEWS.R_CC] = self.r_cc = self.cc\n        self.model_dict[VIEWS.R_MLO] = self.r_mlo = self.mlo\n\n    def forward(self, x):\n        h_dict = {\n            view: self.single_forward(x[view], view)\n            for view in VIEWS.LIST\n        }\n        return h_dict\n\n    def single_forward(self, single_x, view):\n        return self.model_dict[view](single_x)\n\n\nclass ViewResNetV2(nn.Module):\n    """"""\n    Adapted fom torchvision ResNet, converted to v2\n    """"""\n    def __init__(self,\n                 input_channels, num_filters,\n                 first_layer_kernel_size, first_layer_conv_stride,\n                 blocks_per_layer_list, block_strides_list, block_fn,\n                 first_layer_padding=0,\n                 first_pool_size=None, first_pool_stride=None, first_pool_padding=0,\n                 growth_factor=2):\n        super(ViewResNetV2, self).__init__()\n        self.first_conv = nn.Conv2d(\n            in_channels=input_channels, out_channels=num_filters,\n            kernel_size=first_layer_kernel_size,\n            stride=first_layer_conv_stride,\n            padding=first_layer_padding,\n            bias=False,\n        )\n        self.first_pool = nn.MaxPool2d(\n            kernel_size=first_pool_size,\n            stride=first_pool_stride,\n            padding=first_pool_padding,\n        )\n\n        self.layer_list = nn.ModuleList()\n        current_num_filters = num_filters\n        self.inplanes = num_filters\n        for i, (num_blocks, stride) in enumerate(zip(\n                blocks_per_layer_list, block_strides_list)):\n            self.layer_list.append(self._make_layer(\n                block=block_fn,\n                planes=current_num_filters,\n                blocks=num_blocks,\n                stride=stride,\n            ))\n            current_num_filters *= growth_factor\n        self.final_bn = nn.BatchNorm2d(\n            current_num_filters // growth_factor * block_fn.expansion\n        )\n        self.relu = nn.ReLU()\n\n        # Expose attributes for downstream dimension computation\n        self.num_filters = num_filters\n        self.growth_factor = growth_factor\n\n    def forward(self, x):\n        h = self.first_conv(x)\n        h = self.first_pool(h)\n        for i, layer in enumerate(self.layer_list):\n            h = layer(h)\n        h = self.final_bn(h)\n        h = self.relu(h)\n        return h\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n        )\n\n        layers_ = [\n            block(self.inplanes, planes, stride, downsample)\n        ]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers_.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers_)\n\n\ndef resnet22(input_channels):\n    return ViewResNetV2(\n        input_channels=input_channels,\n        num_filters=16,\n        first_layer_kernel_size=7,\n        first_layer_conv_stride=2,\n        blocks_per_layer_list=[2, 2, 2, 2, 2],\n        block_strides_list=[1, 2, 2, 2, 2],\n        block_fn=layers.BasicBlockV2,\n        first_layer_padding=0,\n        first_pool_size=3,\n        first_pool_stride=2,\n        first_pool_padding=0,\n        growth_factor=2\n    )\n\n\ndef filter_strip_prefix(weights_dict, prefix):\n    return {\n        k.replace(prefix, """"): v\n        for k, v in weights_dict.items()\n        if k.startswith(prefix)\n    }\n'"
src/modeling/models_tf.py,12,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nTensorFlow model definition and utils\n""""""\n\nimport tensorflow as tf\n\nimport src.modeling.layers_tf as layers\nimport src.utilities.tf_utils as tf_utils\nfrom src.constants import VIEWS\n\nDATA_FORMAT = ""channels_first""\n\n\ndef single_image_breast_model(inputs, training):\n    with tf.variable_scope(""model""):\n        h = layers.gaussian_noise_layer(\n            inputs=inputs,\n            std=0.01,\n            training=training,\n        )\n        h = resnet22(\n            inputs=h,\n            training=training,\n        )\n        h = layers.avg_pool_layer(h)\n        with tf.variable_scope(""fc1""):\n            h = tf.layers.dense(h, activation=""relu"", units=256)\n        h = layers.output_layer(h, output_shape=(2, 2))\n        return h\n\n\ndef construct_single_image_breast_model_match_dict(\n        view_str, tf_variables, torch_weights, tf_torch_weights_map):\n    """"""\n    view_str: e.g. ""r_mlo""\n    """"""\n    if isinstance(torch_weights, str):\n        import torch\n        torch_weights = torch.load(torch_weights)[""model""]\n    torch_weights = {k: w.numpy() for k, w in torch_weights.items()}\n    match_dict = {}\n    torch_resnet_prefix = ""four_view_resnet.{}."".format(view_str)\n    tf_var_dict = {var.name: var for var in tf_variables}\n    for tf_var_name, tf_var in tf_var_dict.items():\n        if ""resnet"" not in tf_var.name:\n            continue\n        lookup_key = tf_var_name.replace(""model/"", """")\n        weight = torch_weights[torch_resnet_prefix + tf_torch_weights_map[lookup_key]]\n        if len(weight.shape) == 4:\n            weight = tf_utils.convert_conv_torch2tf(weight)\n        assert tf_var.shape == weight.shape\n        match_dict[tf_var] = weight\n    short_view_str = view_str.replace(""_"", """")\n    match_dict[tf_var_dict[""model/fc1/dense/kernel:0""]] = \\\n        tf_utils.convert_fc_weight_torch2tf(torch_weights[""fc1_{}.weight"".format(short_view_str)])\n    match_dict[tf_var_dict[""model/fc1/dense/bias:0""]] = \\\n        torch_weights[""fc1_{}.bias"".format(short_view_str)]\n    match_dict[tf_var_dict[""model/output_layer/dense/kernel:0""]] = \\\n        tf_utils.convert_fc_weight_torch2tf(\n            torch_weights[""output_layer_{}.fc_layer.weight"".format(short_view_str)])[:, :4]\n    match_dict[tf_var_dict[""model/output_layer/dense/bias:0""]] = \\\n        torch_weights[""output_layer_{}.fc_layer.bias"".format(short_view_str)][:4]\n    assert len(match_dict) == len(tf_variables)\n    return match_dict\n\n\ndef four_view_resnet(inputs, training):\n    result_dict = {}\n    for view in VIEWS.LIST:\n        with tf.variable_scope(""view_{}"".format(view)):\n            result_dict[view] = resnet22(inputs, training)\n    return result_dict\n\n\ndef resnet22(inputs, training):\n    return view_resnet_v2(\n        inputs=inputs,\n        training=training,\n        num_filters=16,\n        first_layer_kernel_size=7,\n        first_layer_conv_stride=2,\n        blocks_per_layer_list=[2, 2, 2, 2, 2],\n        block_strides_list=[1, 2, 2, 2, 2],\n        first_pool_size=3,\n        first_pool_stride=2,\n        growth_factor=2\n    )\n\n\ndef view_resnet_v2(inputs, training, num_filters,\n                   first_layer_kernel_size, first_layer_conv_stride,\n                   blocks_per_layer_list, block_strides_list,\n                   first_pool_size=None, first_pool_stride=None,\n                   growth_factor=2):\n    with tf.variable_scope(""resnet""):\n        with tf.variable_scope(""first""):\n            h = layers.conv2d_fixed_padding(\n                inputs=inputs,\n                filters=num_filters,\n                kernel_size=first_layer_kernel_size,\n                strides=first_layer_conv_stride,\n                data_format=DATA_FORMAT,\n                padding=""valid"",\n                name=""first_conv"",\n            )\n            h = tf.layers.max_pooling2d(\n                inputs=h,\n                pool_size=first_pool_size,\n                strides=first_pool_stride,\n                padding=\'valid\',\n                data_format=DATA_FORMAT,\n            )\n        current_num_filters = num_filters\n        for i, (num_blocks, stride) in enumerate(zip(\n                blocks_per_layer_list, block_strides_list)):\n            with tf.variable_scope(""major_{}"".format(i + 1)):\n                h = make_layer(\n                    inputs=h,\n                    planes=current_num_filters,\n                    training=training,\n                    blocks=num_blocks,\n                    stride=stride,\n                )\n                current_num_filters *= growth_factor\n        with tf.variable_scope(""final""):\n            h = layers.batch_norm(\n                inputs=h,\n                training=training,\n                data_format=DATA_FORMAT,\n                name=""bn"",\n            )\n            h = tf.nn.relu(h)\n    return h\n\n\ndef make_layer(inputs, planes, training, blocks, stride=1):\n    with tf.variable_scope(""block_0""):\n        h = layers.basic_block_v2(\n            inputs=inputs,\n            planes=planes,\n            training=training,\n            data_format=DATA_FORMAT,\n            strides=stride,\n            downsample=True,\n        )\n    for i in range(1, blocks):\n        with tf.variable_scope(""block_{}"".format(i)):\n            h = layers.basic_block_v2(\n                inputs=h,\n                planes=planes,\n                training=training,\n                data_format=DATA_FORMAT,\n                strides=1,\n                downsample=False,\n            )\n    return h\n'"
src/modeling/run_model.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nRuns the image only model and image+heatmaps model for breast cancer prediction.\n""""""\nimport argparse\nimport collections as col\nimport numpy as np\nimport os\nimport pandas as pd\nimport torch\nimport tqdm\n\nimport src.utilities.pickling as pickling\nimport src.utilities.tools as tools\nimport src.modeling.models as models\nimport src.data_loading.loading as loading\nfrom src.constants import VIEWS, VIEWANGLES, LABELS, MODELMODES\n\n\ndef load_model(parameters):\n    """"""\n    Loads trained cancer classifier\n    """"""\n    input_channels = 3 if parameters[""use_heatmaps""] else 1\n    model_class = {\n        MODELMODES.VIEW_SPLIT: models.SplitBreastModel,\n        MODELMODES.IMAGE: models.ImageBreastModel,\n    }[parameters[""model_mode""]]\n    model = model_class(input_channels)\n    model.load_state_dict(torch.load(parameters[""model_path""])[""model""])\n\n    if (parameters[""device_type""] == ""gpu"") and torch.has_cudnn:\n        device = torch.device(""cuda:{}"".format(parameters[""gpu_number""]))\n    else:\n        device = torch.device(""cpu"")\n    model = model.to(device)\n    model.eval()\n    return model, device\n\n\ndef run_model(model, device, exam_list, parameters):\n    """"""\n    Returns predictions of image only model or image+heatmaps model.\n    Prediction for each exam is averaged for a given number of epochs.\n    """"""\n    random_number_generator = np.random.RandomState(parameters[""seed""])\n\n    image_extension = "".hdf5"" if parameters[""use_hdf5""] else "".png""\n\n    with torch.no_grad():\n        predictions_ls = []\n        for datum in tqdm.tqdm(exam_list):\n            predictions_for_datum = []\n            loaded_image_dict = {view: [] for view in VIEWS.LIST}\n            loaded_heatmaps_dict = {view: [] for view in VIEWS.LIST}\n            for view in VIEWS.LIST:\n                for short_file_path in datum[view]:\n                    loaded_image = loading.load_image(\n                        image_path=os.path.join(parameters[""image_path""], short_file_path + image_extension),\n                        view=view,\n                        horizontal_flip=datum[""horizontal_flip""],\n                    )\n                    if parameters[""use_heatmaps""]:\n                        loaded_heatmaps = loading.load_heatmaps(\n                            benign_heatmap_path=os.path.join(parameters[""heatmaps_path""], ""heatmap_benign"",\n                                                             short_file_path + "".hdf5""),\n                            malignant_heatmap_path=os.path.join(parameters[""heatmaps_path""], ""heatmap_malignant"",\n                                                                short_file_path + "".hdf5""),\n                            view=view,\n                            horizontal_flip=datum[""horizontal_flip""],\n                        )\n                    else:\n                        loaded_heatmaps = None\n\n                    loaded_image_dict[view].append(loaded_image)\n                    loaded_heatmaps_dict[view].append(loaded_heatmaps)\n            for data_batch in tools.partition_batch(range(parameters[""num_epochs""]), parameters[""batch_size""]):\n                batch_dict = {view: [] for view in VIEWS.LIST}\n                for _ in data_batch:\n                    for view in VIEWS.LIST:\n                        image_index = 0\n                        if parameters[""augmentation""]:\n                            image_index = random_number_generator.randint(low=0, high=len(datum[view]))\n                        cropped_image, cropped_heatmaps = loading.augment_and_normalize_image(\n                            image=loaded_image_dict[view][image_index],\n                            auxiliary_image=loaded_heatmaps_dict[view][image_index],\n                            view=view,\n                            best_center=datum[""best_center""][view][image_index],\n                            random_number_generator=random_number_generator,\n                            augmentation=parameters[""augmentation""],\n                            max_crop_noise=parameters[""max_crop_noise""],\n                            max_crop_size_noise=parameters[""max_crop_size_noise""],\n                        )\n                        if loaded_heatmaps_dict[view][image_index] is None:\n                            batch_dict[view].append(cropped_image[:, :, np.newaxis])\n                        else:\n                            batch_dict[view].append(np.concatenate([\n                                cropped_image[:, :, np.newaxis],\n                                cropped_heatmaps,\n                            ], axis=2))\n\n                tensor_batch = {\n                    view: torch.tensor(np.stack(batch_dict[view])).permute(0, 3, 1, 2).to(device)\n                    for view in VIEWS.LIST\n                }\n                output = model(tensor_batch)\n                batch_predictions = compute_batch_predictions(output, mode=parameters[""model_mode""])\n                pred_df = pd.DataFrame({k: v[:, 1] for k, v in batch_predictions.items()})\n                pred_df.columns.names = [""label"", ""view_angle""]\n                predictions = pred_df.T.reset_index().groupby(""label"").mean().T[LABELS.LIST].values\n                predictions_for_datum.append(predictions)\n            predictions_ls.append(np.mean(np.concatenate(predictions_for_datum, axis=0), axis=0))\n\n    return np.array(predictions_ls)\n\n\ndef compute_batch_predictions(y_hat, mode):\n    """"""\n    Format predictions from different heads\n    """"""\n\n    if mode == MODELMODES.VIEW_SPLIT:\n        assert y_hat[VIEWANGLES.CC].shape[1:] == (4, 2)\n        assert y_hat[VIEWANGLES.MLO].shape[1:] == (4, 2)\n        batch_prediction_tensor_dict = col.OrderedDict()\n        batch_prediction_tensor_dict[LABELS.LEFT_BENIGN, VIEWANGLES.CC] = y_hat[VIEWANGLES.CC][:, 0]\n        batch_prediction_tensor_dict[LABELS.LEFT_BENIGN, VIEWANGLES.MLO] = y_hat[VIEWANGLES.MLO][:, 0]\n        batch_prediction_tensor_dict[LABELS.RIGHT_BENIGN, VIEWANGLES.CC] = y_hat[VIEWANGLES.CC][:, 1]\n        batch_prediction_tensor_dict[LABELS.RIGHT_BENIGN, VIEWANGLES.MLO] = y_hat[VIEWANGLES.MLO][:, 1]\n        batch_prediction_tensor_dict[LABELS.LEFT_MALIGNANT, VIEWANGLES.CC] = y_hat[VIEWANGLES.CC][:, 2]\n        batch_prediction_tensor_dict[LABELS.LEFT_MALIGNANT, VIEWANGLES.MLO] = y_hat[VIEWANGLES.MLO][:, 2]\n        batch_prediction_tensor_dict[LABELS.RIGHT_MALIGNANT, VIEWANGLES.CC] = y_hat[VIEWANGLES.CC][:, 3]\n        batch_prediction_tensor_dict[LABELS.RIGHT_MALIGNANT, VIEWANGLES.MLO] = y_hat[VIEWANGLES.MLO][:, 3]\n        batch_prediction_dict = col.OrderedDict([\n            (k, np.exp(v.cpu().detach().numpy()))\n            for k, v in batch_prediction_tensor_dict.items()\n        ])\n    elif mode == MODELMODES.IMAGE:\n        assert y_hat[VIEWS.L_CC].shape[1:] == (2, 2)\n        assert y_hat[VIEWS.R_CC].shape[1:] == (2, 2)\n        assert y_hat[VIEWS.L_MLO].shape[1:] == (2, 2)\n        assert y_hat[VIEWS.R_MLO].shape[1:] == (2, 2)\n        batch_prediction_tensor_dict = col.OrderedDict()\n        batch_prediction_tensor_dict[LABELS.LEFT_BENIGN, VIEWS.L_CC] = y_hat[VIEWS.L_CC][:, 0]\n        batch_prediction_tensor_dict[LABELS.LEFT_BENIGN, VIEWS.L_MLO] = y_hat[VIEWS.L_MLO][:, 0]\n        batch_prediction_tensor_dict[LABELS.RIGHT_BENIGN, VIEWS.R_CC] = y_hat[VIEWS.R_CC][:, 0]\n        batch_prediction_tensor_dict[LABELS.RIGHT_BENIGN, VIEWS.R_MLO] = y_hat[VIEWS.R_MLO][:, 0]\n        batch_prediction_tensor_dict[LABELS.LEFT_MALIGNANT, VIEWS.L_CC] = y_hat[VIEWS.L_CC][:, 1]\n        batch_prediction_tensor_dict[LABELS.LEFT_MALIGNANT, VIEWS.L_MLO] = y_hat[VIEWS.L_MLO][:, 1]\n        batch_prediction_tensor_dict[LABELS.RIGHT_MALIGNANT, VIEWS.R_CC] = y_hat[VIEWS.R_CC][:, 1]\n        batch_prediction_tensor_dict[LABELS.RIGHT_MALIGNANT, VIEWS.R_MLO] = y_hat[VIEWS.R_MLO][:, 1]\n\n        batch_prediction_dict = col.OrderedDict([\n            (k, np.exp(v.cpu().detach().numpy()))\n            for k, v in batch_prediction_tensor_dict.items()\n        ])\n    else:\n        raise KeyError(mode)\n    return batch_prediction_dict\n\n\ndef load_run_save(data_path, output_path, parameters):\n    """"""\n    Outputs the predictions as csv file\n    """"""\n    exam_list = pickling.unpickle_from_file(data_path)\n    model, device = load_model(parameters)\n    predictions = run_model(model, device, exam_list, parameters)\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    # Take the positive prediction\n    df = pd.DataFrame(predictions, columns=LABELS.LIST)\n    df.to_csv(output_path, index=False, float_format=\'%.4f\')\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Run image-only model or image+heatmap model\')\n    parser.add_argument(\'--model-mode\', default=MODELMODES.VIEW_SPLIT, type=str)\n    parser.add_argument(\'--model-path\', required=True)\n    parser.add_argument(\'--data-path\', required=True)\n    parser.add_argument(\'--image-path\', required=True)\n    parser.add_argument(\'--output-path\', required=True)\n    parser.add_argument(\'--batch-size\', default=1, type=int)\n    parser.add_argument(\'--seed\', default=0, type=int)\n    parser.add_argument(\'--use-heatmaps\', action=""store_true"")\n    parser.add_argument(\'--heatmaps-path\')\n    parser.add_argument(\'--use-augmentation\', action=""store_true"")\n    parser.add_argument(\'--use-hdf5\', action=""store_true"")\n    parser.add_argument(\'--num-epochs\', default=1, type=int)\n    parser.add_argument(\'--device-type\', default=""cpu"", choices=[\'gpu\', \'cpu\'])\n    parser.add_argument(""--gpu-number"", type=int, default=0)\n    args = parser.parse_args()\n\n    parameters = {\n        ""device_type"": args.device_type,\n        ""gpu_number"": args.gpu_number,\n        ""max_crop_noise"": (100, 100),\n        ""max_crop_size_noise"": 100,\n        ""image_path"": args.image_path,\n        ""batch_size"": args.batch_size,\n        ""seed"": args.seed,\n        ""augmentation"": args.use_augmentation,\n        ""num_epochs"": args.num_epochs,\n        ""use_heatmaps"": args.use_heatmaps,\n        ""heatmaps_path"": args.heatmaps_path,\n        ""use_hdf5"": args.use_hdf5,\n        ""model_mode"": args.model_mode,\n        ""model_path"": args.model_path,\n    }\n    load_run_save(\n        data_path=args.data_path,\n        output_path=args.output_path,\n        parameters=parameters,\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/modeling/run_model_single.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nRuns the image only model and image+heatmaps model for breast cancer prediction.\n""""""\nimport argparse\nimport numpy as np\nimport torch\nimport json\n\nimport src.utilities.pickling as pickling\nimport src.utilities.tools as tools\nimport src.modeling.models as models\nimport src.data_loading.loading as loading\n\n\nclass ModelInput:\n    def __init__(self, image, heatmaps, metadata):\n        self.image = image\n        self.heatmaps = heatmaps\n        self.metadata = metadata\n\n\ndef load_model(parameters):\n    """"""\n    Loads trained cancer classifier\n    """"""\n    input_channels = 3 if parameters[""use_heatmaps""] else 1\n    model = models.SingleImageBreastModel(input_channels)\n    model.load_state_from_shared_weights(\n        state_dict=torch.load(parameters[""model_path""])[""model""],\n        view=parameters[""view""],\n    )\n    if (parameters[""device_type""] == ""gpu"") and torch.has_cudnn:\n        device = torch.device(""cuda:{}"".format(parameters[""gpu_number""]))\n    else:\n        device = torch.device(""cpu"")\n    model = model.to(device)\n    model.eval()\n    return model, device\n\n\ndef load_inputs(image_path, metadata_path,\n                use_heatmaps, benign_heatmap_path=None, malignant_heatmap_path=None):\n    """"""\n    Load a single input example, optionally with heatmaps\n    """"""\n    if use_heatmaps:\n        assert benign_heatmap_path is not None\n        assert malignant_heatmap_path is not None\n    else:\n        assert benign_heatmap_path is None\n        assert malignant_heatmap_path is None\n    metadata = pickling.unpickle_from_file(metadata_path)\n    image = loading.load_image(\n        image_path=image_path,\n        view=metadata[""full_view""],\n        horizontal_flip=metadata[""horizontal_flip""],\n    )\n    if use_heatmaps:\n        heatmaps = loading.load_heatmaps(\n            benign_heatmap_path=benign_heatmap_path,\n            malignant_heatmap_path=malignant_heatmap_path,\n            view=metadata[""full_view""],\n            horizontal_flip=metadata[""horizontal_flip""],\n        )\n    else:\n        heatmaps = None\n    return ModelInput(image=image, heatmaps=heatmaps, metadata=metadata)\n\n\ndef process_augment_inputs(model_input, random_number_generator, parameters):\n    """"""\n    Augment, normalize and convert inputs to np.ndarray\n    """"""\n    cropped_image, cropped_heatmaps = loading.augment_and_normalize_image(\n        image=model_input.image,\n        auxiliary_image=model_input.heatmaps,\n        view=model_input.metadata[""full_view""],\n        best_center=model_input.metadata[""best_center""],\n        random_number_generator=random_number_generator,\n        augmentation=parameters[""augmentation""],\n        max_crop_noise=parameters[""max_crop_noise""],\n        max_crop_size_noise=parameters[""max_crop_size_noise""],\n    )\n    if parameters[""use_heatmaps""]:\n        return np.concatenate([\n            cropped_image[:, :, np.newaxis],\n            cropped_heatmaps,\n        ], axis=2)\n    else:\n        return cropped_image[:, :, np.newaxis]\n\n\ndef batch_to_tensor(batch, device):\n    """"""\n    Convert list of input ndarrays to tensor on device\n    """"""\n    return torch.tensor(np.stack(batch)).permute(0, 3, 1, 2).to(device)\n\n\ndef run(parameters):\n    """"""\n    Outputs the predictions as csv file\n    """"""\n    random_number_generator = np.random.RandomState(parameters[""seed""])\n    model, device = load_model(parameters)\n\n    model_input = load_inputs(\n        image_path=parameters[""cropped_mammogram_path""],\n        metadata_path=parameters[""metadata_path""],\n        use_heatmaps=parameters[""use_heatmaps""],\n        benign_heatmap_path=parameters[""heatmap_path_benign""],\n        malignant_heatmap_path=parameters[""heatmap_path_malignant""],\n    )\n    assert model_input.metadata[""full_view""] == parameters[""view""]\n\n    all_predictions = []\n    for data_batch in tools.partition_batch(range(parameters[""num_epochs""]), parameters[""batch_size""]):\n        batch = []\n        for _ in data_batch:\n            batch.append(process_augment_inputs(\n                model_input=model_input,\n                random_number_generator=random_number_generator,\n                parameters=parameters,\n            ))\n        tensor_batch = batch_to_tensor(batch, device)\n        with torch.no_grad():\n            y_hat = model(tensor_batch)\n        predictions = np.exp(y_hat.cpu().detach().numpy())[:, :2, 1]\n        all_predictions.append(predictions)\n    agg_predictions = np.concatenate(all_predictions, axis=0).mean(0)\n    predictions_dict = {\n        ""benign"": float(agg_predictions[0]),\n        ""malignant"": float(agg_predictions[1]),\n    }\n    print(json.dumps(predictions_dict))\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Run image-only model or image+heatmap model\')\n    parser.add_argument(\'--view\', required=True)\n    parser.add_argument(\'--model-path\', required=True)\n    parser.add_argument(\'--cropped-mammogram-path\', required=True)\n    parser.add_argument(\'--metadata-path\', required=True)\n    parser.add_argument(\'--batch-size\', default=1, type=int)\n    parser.add_argument(\'--seed\', default=0, type=int)\n    parser.add_argument(\'--use-heatmaps\', action=""store_true"")\n    parser.add_argument(\'--heatmap-path-malignant\')\n    parser.add_argument(\'--heatmap-path-benign\')\n    parser.add_argument(\'--use-augmentation\', action=""store_true"")\n    parser.add_argument(\'--use-hdf5\', action=""store_true"")\n    parser.add_argument(\'--num-epochs\', default=1, type=int)\n    parser.add_argument(\'--device-type\', default=""cpu"", choices=[\'gpu\', \'cpu\'])\n    parser.add_argument(""--gpu-number"", type=int, default=0)\n    args = parser.parse_args()\n\n    parameters = {\n        ""view"": args.view,\n        ""model_path"": args.model_path,\n        ""cropped_mammogram_path"": args.cropped_mammogram_path,\n        ""metadata_path"": args.metadata_path,\n        ""device_type"": args.device_type,\n        ""gpu_number"": args.gpu_number,\n        ""max_crop_noise"": (100, 100),\n        ""max_crop_size_noise"": 100,\n        ""batch_size"": args.batch_size,\n        ""seed"": args.seed,\n        ""augmentation"": args.use_augmentation,\n        ""num_epochs"": args.num_epochs,\n        ""use_heatmaps"": args.use_heatmaps,\n        ""heatmap_path_benign"": args.heatmap_path_benign,\n        ""heatmap_path_malignant"": args.heatmap_path_malignant,\n        ""use_hdf5"": args.use_hdf5,\n    }\n    run(parameters)\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
src/modeling/run_model_single_tf.py,6,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nRuns the image only model and image+heatmaps model for breast cancer prediction.\n""""""\nimport argparse\nimport numpy as np\nimport json\nimport tensorflow as tf\n\nimport src.utilities.pickling as pickling\nimport src.utilities.tf_utils as tf_utils\nimport src.utilities.tools as tools\nimport src.modeling.models_tf as models\nimport src.data_loading.loading as loading\nfrom src.constants import INPUT_SIZE_DICT\n\n\nclass ModelInput:\n    def __init__(self, image, heatmaps, metadata):\n        self.image = image\n        self.heatmaps = heatmaps\n        self.metadata = metadata\n\n\ndef load_model(parameters):\n    """"""\n    Loads trained cancer classifier\n    """"""\n    # Setup model params\n    input_channels = 3 if parameters[""use_heatmaps""] else 1\n    if (parameters[""device_type""] == ""gpu"") and tf.test.is_gpu_available():\n        device_str = ""/device:GPU:{}"".format(parameters[""gpu_number""])\n    else:\n        device_str = ""/cpu:0""\n    view_str = parameters[""view""].replace(""-"", ""_"").lower()\n    h, w = INPUT_SIZE_DICT[parameters[""view""]]\n\n    # Setup Graph\n    graph = tf.Graph()\n    with graph.as_default():\n        with tf.device(device_str):\n            x = tf.placeholder(tf.float32, shape=[None, input_channels, h, w], name=""inputs"")\n            y = models.single_image_breast_model(x, training=False)\n\n    # Load weights\n    sess = tf.Session(graph=graph, config=tf.ConfigProto(\n        gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n    ))\n    with open(parameters[""tf_torch_weights_map_path""]) as f:\n        tf_torch_weights_map = json.loads(f.read())\n\n    with sess.as_default():\n        match_dict = models.construct_single_image_breast_model_match_dict(\n            view_str=view_str,\n            tf_variables=tf_utils.get_tf_variables(graph, batch_norm_key=""bn""),\n            torch_weights=parameters[""model_path""],\n            tf_torch_weights_map=tf_torch_weights_map,\n        )\n        sess.run(tf_utils.construct_weight_assign_ops(match_dict))\n\n    return sess, x, y\n\n\ndef load_inputs(image_path, metadata_path,\n                use_heatmaps, benign_heatmap_path=None, malignant_heatmap_path=None):\n    """"""\n    Load a single input example, optionally with heatmaps\n    """"""\n    if use_heatmaps:\n        assert benign_heatmap_path is not None\n        assert malignant_heatmap_path is not None\n    else:\n        assert benign_heatmap_path is None\n        assert malignant_heatmap_path is None\n    metadata = pickling.unpickle_from_file(metadata_path)\n    image = loading.load_image(\n        image_path=image_path,\n        view=metadata[""full_view""],\n        horizontal_flip=metadata[""horizontal_flip""],\n    )\n    if use_heatmaps:\n        heatmaps = loading.load_heatmaps(\n            benign_heatmap_path=benign_heatmap_path,\n            malignant_heatmap_path=malignant_heatmap_path,\n            view=metadata[""full_view""],\n            horizontal_flip=metadata[""horizontal_flip""],\n        )\n    else:\n        heatmaps = None\n    return ModelInput(image=image, heatmaps=heatmaps, metadata=metadata)\n\n\ndef process_augment_inputs(model_input, random_number_generator, parameters):\n    """"""\n    Augment, normalize and convert inputs to np.ndarray\n    """"""\n    cropped_image, cropped_heatmaps = loading.augment_and_normalize_image(\n        image=model_input.image,\n        auxiliary_image=model_input.heatmaps,\n        view=model_input.metadata[""full_view""],\n        best_center=model_input.metadata[""best_center""],\n        random_number_generator=random_number_generator,\n        augmentation=parameters[""augmentation""],\n        max_crop_noise=parameters[""max_crop_noise""],\n        max_crop_size_noise=parameters[""max_crop_size_noise""],\n    )\n    if parameters[""use_heatmaps""]:\n        return np.concatenate([\n            cropped_image[:, :, np.newaxis],\n            cropped_heatmaps,\n        ], axis=2)\n    else:\n        return cropped_image[:, :, np.newaxis]\n\n\ndef batch_to_inputs(batch):\n    """"""\n    Convert list of input ndarrays to prepped inputs\n    """"""\n    return np.transpose(np.stack(batch), [0, 3, 1, 2])\n\n\ndef run(parameters):\n    """"""\n    Outputs the predictions as csv file\n    """"""\n    random_number_generator = np.random.RandomState(parameters[""seed""])\n    sess, x, y = load_model(parameters)\n\n    model_input = load_inputs(\n        image_path=parameters[""cropped_mammogram_path""],\n        metadata_path=parameters[""metadata_path""],\n        use_heatmaps=parameters[""use_heatmaps""],\n        benign_heatmap_path=parameters[""heatmap_path_benign""],\n        malignant_heatmap_path=parameters[""heatmap_path_malignant""],\n    )\n    assert model_input.metadata[""full_view""] == parameters[""view""]\n\n    all_predictions = []\n    for data_batch in tools.partition_batch(range(parameters[""num_epochs""]), parameters[""batch_size""]):\n        batch = []\n        for _ in data_batch:\n            batch.append(process_augment_inputs(\n                model_input=model_input,\n                random_number_generator=random_number_generator,\n                parameters=parameters,\n            ))\n        x_data = batch_to_inputs(batch)\n        with sess.as_default():\n            y_hat = sess.run(y, feed_dict={x: x_data})\n        predictions = np.exp(y_hat)[:, :, 1]\n        all_predictions.append(predictions)\n    agg_predictions = np.concatenate(all_predictions, axis=0).mean(0)\n    predictions_dict = {\n        ""benign"": float(agg_predictions[0]),\n        ""malignant"": float(agg_predictions[1]),\n    }\n    print(json.dumps(predictions_dict))\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Run image-only model or image+heatmap model\')\n    parser.add_argument(\'--view\', required=True)\n    parser.add_argument(\'--model-path\', required=True)\n    parser.add_argument(\'--tf-torch-weights-map-path\', required=True)\n    parser.add_argument(\'--cropped-mammogram-path\', required=True)\n    parser.add_argument(\'--metadata-path\', required=True)\n    parser.add_argument(\'--batch-size\', default=1, type=int)\n    parser.add_argument(\'--seed\', default=0, type=int)\n    parser.add_argument(\'--use-heatmaps\', action=""store_true"")\n    parser.add_argument(\'--heatmap-path-malignant\')\n    parser.add_argument(\'--heatmap-path-benign\')\n    parser.add_argument(\'--use-augmentation\', action=""store_true"")\n    parser.add_argument(\'--use-hdf5\', action=""store_true"")\n    parser.add_argument(\'--num-epochs\', default=1, type=int)\n    parser.add_argument(\'--device-type\', default=""cpu"", choices=[\'gpu\', \'cpu\'])\n    parser.add_argument(""--gpu-number"", type=int, default=0)\n    args = parser.parse_args()\n\n    parameters = {\n        ""view"": args.view,\n        ""model_path"": args.model_path,\n        ""tf_torch_weights_map_path"": args.tf_torch_weights_map_path,\n        ""cropped_mammogram_path"": args.cropped_mammogram_path,\n        ""metadata_path"": args.metadata_path,\n        ""device_type"": args.device_type,\n        ""gpu_number"": args.gpu_number,\n        ""max_crop_noise"": (100, 100),\n        ""max_crop_size_noise"": 100,\n        ""batch_size"": args.batch_size,\n        ""seed"": args.seed,\n        ""augmentation"": args.use_augmentation,\n        ""num_epochs"": args.num_epochs,\n        ""use_heatmaps"": args.use_heatmaps,\n        ""heatmap_path_benign"": args.heatmap_path_benign,\n        ""heatmap_path_malignant"": args.heatmap_path_malignant,\n        ""use_hdf5"": args.use_hdf5,\n    }\n    run(parameters)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/optimal_centers/__init__.py,0,b''
src/optimal_centers/calc_optimal_centers.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines functions used in search_windows_and_centers.py\n""""""\nimport math\nimport numpy as np\nimport pandas as pd\n\n\ndef get_images_optimal_window_info(image, com, window_dim_ls, step=1,\n                                   tl_br_constraint=None):\n    """"""\n    Given an image, window_dim_ls to search over, and constraints on tl_br\n    (e.g. rightmost-pixel constraint), return data about the potential\n    optimal windows\n    """"""\n\n    image_result_ls = []\n    cumsum = get_image_cumsum(image)\n    for window_dim in window_dim_ls:\n        image_result_ls.append(get_image_optimal_window_info(\n            image, com, window_dim,\n            step=step, tl_br_constraint=tl_br_constraint, cumsum=cumsum,\n        ))\n\n    return pd.DataFrame(image_result_ls)\n\n\ndef get_image_optimal_window_info(image, com, window_dim,\n                                  step=1, tl_br_constraint=None, cumsum=None):\n    image_dim = image.shape\n    if cumsum is None:\n        cumsum = get_image_cumsum(image)\n    window_area = np.prod(window_dim)\n    tl, br = get_candidate_center_topleft_bottomright(\n        com=com, image_dim=image_dim, window_dim=window_dim, step=step)\n    if tl_br_constraint:\n        tl, br = tl_br_constraint(tl=tl, br=br, image=image, window_dim=window_dim)\n    y_grid_axis = np.arange(tl[0], br[0], step)\n    x_grid_axis = np.arange(tl[1], br[1], step)\n    window_center_ls = get_joint_axes(y_grid_axis, x_grid_axis)\n\n    tl_ls, br_ls = get_candidate_topleft_bottomright(\n        image_dim=image_dim,\n        window_center=window_center_ls,\n        window_dim=window_dim,\n    )\n    partial_sum = v_get_topleft_bottomright_partialsum(\n        cumsum=cumsum,\n        topleft=tl_ls,\n        bottomright=br_ls,\n    )\n    if len(partial_sum) == 1:\n        best_center = tl\n        fraction_of_non_zero_pixels = partial_sum[0] / window_area\n    else:\n        best_sum = partial_sum.max()\n        best_center_ls = window_center_ls[partial_sum == best_sum]\n        if len(best_center_ls) == 1:\n            best_center = best_center_ls[0]\n        else:\n            best_indices = best_center_ls - com\n            best_idx = np.argmin((best_indices ** 2).sum(1))\n            best_offset = best_indices[best_idx]\n            best_center = com + best_offset\n        fraction_of_non_zero_pixels = best_sum / window_area\n    return {\n        ""window_dim_y"": window_dim[0],\n        ""window_dim_x"": window_dim[1],\n        ""best_center_y"": best_center[0],\n        ""best_center_x"": best_center[1],\n        ""fraction"": fraction_of_non_zero_pixels,\n    }\n\n\ndef get_image_cumsum(image):\n    binary = image > 0\n    return get_topleft_bottomright_cumsum(binary)\n\n\ndef get_joint_axes(y_grid_axis, x_grid_axis):\n    return np.array(np.meshgrid(\n        y_grid_axis, x_grid_axis,\n    )).T.reshape(-1, 2)\n\n\ndef get_candidate_center_topleft_bottomright(com, image_dim, window_dim, step):\n    """"""\n    br returned is exclusive\n    """"""\n    half_dim = window_dim // 2\n    rem_window_dim = window_dim - half_dim\n\n    # Find extremes of possible centers, based on window size\n    center_tl = half_dim\n    center_br = image_dim - rem_window_dim\n\n    #\n    tl = com - step * ((com - center_tl) // step)\n    br = com + step * ((center_br - com) // step)\n\n    # Default to Center\n    if tl[0] >= br[0]:\n        tl[0] = br[0] = com[0]\n    if tl[1] >= br[1]:\n        tl[1] = br[1] = com[1]\n    return tl, br + 1\n\n\ndef get_candidate_topleft_bottomright(image_dim, window_center, window_dim):\n    """"""\n    Given a desired window_center, compute the feasible (array-indexable)\n    topleft and bottomright\n\n    implicitly ""zero-pads"" if window cannot fit in image\n    """"""\n    half_dim = window_dim // 2\n    rem_half_dim = window_dim - half_dim\n    tl = window_center - half_dim\n    offset = (-tl).clip(min=0)\n    tl = tl.clip(min=0)\n    br = window_center + rem_half_dim + offset\n    br = br.clip(max=image_dim)\n    return tl, br\n\n\ndef get_topleft_bottomright_cumsum(x):\n    return np.cumsum(np.cumsum(x, axis=0), axis=1)\n\n\ndef v_get_topleft_bottomright_partialsum(cumsum, topleft, bottomright):\n    """"""\n    bottomright is exclusive, to match slicing indices\n    """"""\n    tl_x, tl_y = topleft[:, 0], topleft[:, 1]\n    br_x, br_y = bottomright[:, 0], bottomright[:, 1]\n    assert np.all(br_x >= tl_x)\n    assert np.all(br_y >= tl_y)\n    assert np.all(br_x <= cumsum.shape[0])\n    assert np.all(br_y <= cumsum.shape[1])\n\n    length = len(topleft)\n    topslice = np.zeros(length)\n    leftslice = np.zeros(length)\n    topleft_slice = np.zeros(length)\n    bottomright_slice = np.zeros(length)\n\n    selector = (tl_y > 0) & (br_x > 0)\n    topslice[selector] = cumsum[br_x - 1, tl_y - 1][selector]\n\n    selector = (tl_x > 0) & (br_y > 0)\n    leftslice[selector] = cumsum[tl_x - 1, br_y - 1][selector]\n\n    selector = (tl_x > 0) & (tl_y > 0)\n    topleft_slice[selector] = cumsum[tl_x - 1, tl_y - 1][selector]\n\n    selector = (br_x > 0) & (br_y > 0)\n    bottomright_slice[selector] = cumsum[br_x - 1, br_y - 1][selector]\n\n    return bottomright_slice - topslice - leftslice + topleft_slice\n\n\ndef get_rightmost_pixel_constraint(rightmost_x):\n    """"""\n    Given a rightmost_x (x-coord of rightmost nonzero pixel),\n    return a constraint function that remaps candidate tl/brs\n    such that the right-edge = rightmost_x\n\n    (Should reduce 2D search to 1D)\n    """"""\n\n    def _f(tl, br, image, window_dim, rightmost_x_=rightmost_x):\n        if tl[1] == br[1]:\n            # We have no room to shift the center-X anyway\n            return tl, br\n        half_dim_x = window_dim[1] // 2\n        tl = tl.copy()\n        br = br.copy()\n        new_x = rightmost_x_ - half_dim_x\n        tl[1] = new_x - 1\n        br[1] = new_x\n        return tl, br\n\n    return _f\n\n\ndef get_bottomrightmost_pixel_constraint(rightmost_x, bottommost_y):\n    """"""\n    Given a rightmost_x (x-coord of rightmost nonzero pixel),\n    return a constraint function that remaps candidate tl/brs\n    such that the right-edge = rightmost_x\n\n    (Should reduce 2D search to 1D)\n    """"""\n\n    def _f(tl, br, image, window_dim,\n           bottommost_y_=bottommost_y, rightmost_x_=rightmost_x):\n\n        # Check for empty rows at bottom\n        relevant_image_from_right = image[:, -window_dim[1]:]\n        non_zero_rows = np.any((relevant_image_from_right != 0), axis=1)\n        if non_zero_rows.any():\n            last_nonzero_row = np.arange(non_zero_rows.shape[0])[non_zero_rows][-1]\n            bottommost_y_ = min(last_nonzero_row, bottommost_y_)\n\n        half_dim = window_dim // 2\n        br = np.array([bottommost_y_, rightmost_x_]) - half_dim\n        tl = br - 1\n\n        return tl, br\n\n    return _f\n'"
src/optimal_centers/get_optimal_center_single.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nRuns search_windows_and_centers.py and extract_centers.py in the same directory\n""""""\nimport argparse\n\nimport src.utilities.pickling as pickling\nimport src.utilities.reading_images as reading_images\nimport src.optimal_centers.get_optimal_centers as get_optimal_centers\n\n\ndef get_optimal_center_single(cropped_mammogram_path, metadata_path):\n    """"""\n    Get optimal center for single example\n    """"""\n    metadata = pickling.unpickle_from_file(metadata_path)\n    image = reading_images.read_image_png(cropped_mammogram_path)\n    optimal_center = get_optimal_centers.extract_center(metadata, image)\n    metadata[""best_center""] = optimal_center\n    pickling.pickle_to_file(metadata_path, metadata)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Compute and Extract Optimal Centers\')\n    parser.add_argument(\'--cropped-mammogram-path\', required=True)\n    parser.add_argument(\'--metadata-path\', required=True)\n    parser.add_argument(\'--num-processes\', default=20)\n    args = parser.parse_args()\n    get_optimal_center_single(\n        cropped_mammogram_path=args.cropped_mammogram_path,\n        metadata_path=args.metadata_path,\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/optimal_centers/get_optimal_centers.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nRuns search_windows_and_centers.py and extract_centers.py in the same directory\n""""""\nimport argparse\nimport numpy as np\nimport os\nfrom itertools import repeat\nfrom multiprocessing import Pool\n\nfrom src.constants import INPUT_SIZE_DICT\nimport src.utilities.pickling as pickling\nimport src.utilities.data_handling as data_handling\nimport src.utilities.reading_images as reading_images\nimport src.data_loading.loading as loading\nimport src.optimal_centers.calc_optimal_centers as calc_optimal_centers\n\n\ndef extract_center(datum, image):\n    """"""\n    Compute the optimal center for an image\n    """"""\n    image = loading.flip_image(image, datum[""full_view""], datum[\'horizontal_flip\'])\n    if datum[""view""] == ""MLO"":\n        tl_br_constraint = calc_optimal_centers.get_bottomrightmost_pixel_constraint(\n            rightmost_x=datum[""rightmost_points""][1],\n            bottommost_y=datum[""bottommost_points""][0],\n        )\n    elif datum[""view""] == ""CC"":\n        tl_br_constraint = calc_optimal_centers.get_rightmost_pixel_constraint(\n            rightmost_x=datum[""rightmost_points""][1]\n        )\n    else:\n        raise RuntimeError(datum[""view""])\n    optimal_center = calc_optimal_centers.get_image_optimal_window_info(\n        image,\n        com=np.array(image.shape) // 2,\n        window_dim=np.array(INPUT_SIZE_DICT[datum[""full_view""]]),\n        tl_br_constraint=tl_br_constraint,\n    )\n    return optimal_center[""best_center_y""], optimal_center[""best_center_x""]\n\n\ndef load_and_extract_center(datum, data_prefix):\n    """"""\n    Load image and computer optimal center\n    """"""\n    full_image_path = os.path.join(data_prefix, datum[""short_file_path""] + \'.png\')\n    image = reading_images.read_image_png(full_image_path)\n    return datum[""short_file_path""], extract_center(datum, image)\n\n\ndef get_optimal_centers(data_list, data_prefix, num_processes=1):\n    """"""\n    Compute optimal centers for each image in data list\n    """"""\n    pool = Pool(num_processes)\n    result = pool.starmap(load_and_extract_center, zip(data_list, repeat(data_prefix)))\n    return dict(result)\n\n\ndef main(cropped_exam_list_path, data_prefix, output_exam_list_path, num_processes=1):\n    exam_list = pickling.unpickle_from_file(cropped_exam_list_path)\n    data_list = data_handling.unpack_exam_into_images(exam_list, cropped=True)\n    optimal_centers = get_optimal_centers(\n        data_list=data_list,\n        data_prefix=data_prefix,\n        num_processes=num_processes\n    )\n    data_handling.add_metadata(exam_list, ""best_center"", optimal_centers)\n    os.makedirs(os.path.dirname(output_exam_list_path), exist_ok=True)\n    pickling.pickle_to_file(output_exam_list_path, exam_list)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Compute and Extract Optimal Centers\')\n    parser.add_argument(\'--cropped-exam-list-path\')\n    parser.add_argument(\'--data-prefix\')\n    parser.add_argument(\'--output-exam-list-path\', required=True)\n    parser.add_argument(\'--num-processes\', default=20)\n    args = parser.parse_args()\n\n    main(\n        cropped_exam_list_path=args.cropped_exam_list_path,\n        data_prefix=args.data_prefix,\n        output_exam_list_path=args.output_exam_list_path,\n        num_processes=int(args.num_processes),\n    )\n'"
src/utilities/__init__.py,0,b''
src/utilities/data_handling.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines utility functions for managing the dataset.\n""""""\nfrom src.constants import VIEWS\n\n\ndef unpack_exam_into_images(exam_list, cropped=False):\n    """"""\n    Turn exam_list into image_list for parallel functions which process each image separately.\n    """"""\n    image_list = []\n    for i, exam in enumerate(exam_list):\n        for view in VIEWS.LIST:\n            for j, image in enumerate(exam[view]):\n                image_dict = dict(\n                    short_file_path=image,\n                    horizontal_flip=exam[\'horizontal_flip\'],\n                    full_view=view,\n                    side=view[0],\n                    view=view[2:],\n                )\n                if cropped:\n                    image_dict[""window_location""] = exam[\'window_location\'][view][j]\n                    image_dict[""rightmost_points""] = exam[\'rightmost_points\'][view][j]\n                    image_dict[""bottommost_points""] = exam[\'bottommost_points\'][view][j]\n                    image_dict[""distance_from_starting_side""] = exam[\'distance_from_starting_side\'][view][j]\n                image_list.append(image_dict)\n    return image_list\n\n\ndef add_metadata(exam_list, additional_metadata_name, additional_metadata_dict):\n    """"""\n    Includes new information about images into exam_list\n    """"""\n    for exam in exam_list:\n        assert additional_metadata_name not in exam, ""this metadata is already included""\n        exam[additional_metadata_name] = dict()\n        for view in VIEWS.LIST:\n            exam[additional_metadata_name][view] = []\n            for j, image in enumerate(exam[view]):\n                exam[additional_metadata_name][view].append(additional_metadata_dict[image])\n'"
src/utilities/pickling.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines utility functions for saving and loading pickles and tensorflow networks.\n""""""\nimport pickle\nimport pandas as pd\n\n\ndef pickle_to_file(file_name, data, protocol = pickle.HIGHEST_PROTOCOL):\n    with open(file_name, \'wb\') as handle:\n        pickle.dump(data, handle, protocol)\n\ndef unpickle_from_file(file_name):\n    with open(file_name, \'rb\') as handle:\n        return pickle.load(handle)'"
src/utilities/reading_images.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines utility functions for reading png and hdf5 images.\n""""""\nimport numpy as np\nimport imageio\nimport h5py\n\n\ndef read_image_png(file_name):\n    image = np.array(imageio.imread(file_name))\n    return image\n\n\ndef read_image_mat(file_name):\n    data = h5py.File(file_name, \'r\')\n    image = np.array(data[\'image\']).T\n    data.close()\n    return image\n'"
src/utilities/saving_images.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines utility functions for saving png and hdf5 images.\n""""""\nimport imageio\nimport h5py\n\n\ndef save_image_as_png(image, filename):\n    """"""\n    Saves image as png files while preserving bit depth of the image\n    """"""\n    imageio.imwrite(filename, image)\n\n\ndef save_image_as_hdf5(image, filename):\n    """"""\n    Saves image as hdf5 files to preserve the floating point values.\n    """"""\n    h5f = h5py.File(filename, \'w\')\n    h5f.create_dataset(\'image\', data=image.transpose(), compression=""lzf"")\n    h5f.close()\n'"
src/utilities/tf_utils.py,2,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh,\n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao,\n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema,\n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\nimport numpy as np\n\nimport tensorflow as tf\n\n\ndef get_tf_variables(graph, batch_norm_key=""bn""):\n    param_variables = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    bn_running_variables = []\n    for variable in graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n        if batch_norm_key in variable.name \\\n                and ""moving"" in variable.name:\n            bn_running_variables.append(variable)\n    return param_variables + bn_running_variables\n\n\ndef construct_weight_assign_ops(match_dict):\n    assign_list = []\n    for tf_var, np_weights in match_dict.items():\n        assign_list.append(tf_var.assign(np_weights))\n    return assign_list\n\n\ndef convert_conv_torch2tf(w):\n    # [C_out, C_in, H, W] => [H, W, C_in, C_out]\n    return np.transpose(w, [2, 3, 1, 0])\n\n\ndef convert_fc_weight_torch2tf(w):\n    return w.swapaxes(0, 1)\n'"
src/utilities/tools.py,0,"b'# Copyright (C) 2019 Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, \n#   Stanis\xc5\x82aw Jastrz\xc4\x99bski, Thibault F\xc3\xa9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, \n#   Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, \n#   Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, \n#   Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, \n#   Kyunghyun Cho, Krzysztof J. Geras\n#\n# This file is part of breast_cancer_classifier.\n#\n# breast_cancer_classifier is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# breast_cancer_classifier is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with breast_cancer_classifier.  If not, see <http://www.gnu.org/licenses/>.\n# ==============================================================================\n""""""\nDefines utility functions for various tasks in breast_cancer_classifier.\n""""""\n\n\ndef partition_batch(ls, size):\n    """"""\n    Partitions a list into buckets of given maximum length.\n    """"""\n    i = 0\n    partitioned_lists = []\n    while i < len(ls):\n        partitioned_lists.append(ls[i: i+size])\n        i += size\n    return partitioned_lists\n'"
