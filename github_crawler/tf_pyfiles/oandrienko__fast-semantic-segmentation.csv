file_path,api_count,code
compress.py,7,"b'r"""""" Prune weights from checkpoint file.\n\nWhen training a model with, your training directory will\ncontaining a GraphDef file (usually ending with the .pb or .pbtxt extension)\nand a set of checkpoint files. We load both here and output a pruned\nversion of the GraphDef and checkpoint file.\n\nAs described in https://arxiv.org/abs/1608.08710.\n  Pruning Filters for Efficient ConvNets\n  Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf\n\nUsage:\n\n    python compress.py \\\n        --input_graph /tmp/models/prediction_graph.pbtxt \\\n        --input_checkpoint /tmp/models/model.ckpt-XYZ \\\n        --output_dir /tmp/pruned_model \\\n        --config_path configs/compression/icnet_resnet_v1_prune_all.config \\\n        --skippable_nodes ""Predictions/postrain/biases""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport functools\nfrom google.protobuf import text_format\nimport tensorflow as tf\n\nfrom protos import compressor_pb2\nfrom builders import compressor_builder\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'input_checkpoint\', None,\n                    \'TensorFlow variables file to load.\')\nflags.mark_flag_as_required(\'input_checkpoint\')\n\nflags.DEFINE_string(\'prune_config\', None,\n                    \'The compression config to use to compression.\')\nflags.mark_flag_as_required(\'prune_config\')\n\nflags.DEFINE_float(\'compression_factor\', 0.5,\n                   \'The compression factor to apply when prunin filters.\')\n\nflags.DEFINE_boolean(\'input_binary\', False,\n                     \'Whether the input files are in binary format.\')\n\nflags.DEFINE_string(\'output_dir\', \'\',\n                    \'Location to save prunned output checkpoints\')\n\nflags.DEFINE_string(\'skippable_nodes\', \'\',\n                    \'Nodes to not validate when pruning.\')\n\nflags.DEFINE_boolean(\'interactive\', False,\n                     \'Whether the input files are in binary format.\')\n\nflags.DEFINE_boolean(\'soft_apply\', False,\n                     \'Simulate compression by setting weights to zero but \'\n                     \'keeping the original shape of each variable.\')\n\n\ndef main(unused_args):\n    if (not tf.train.checkpoint_exists(FLAGS.input_checkpoint)\n        or not tf.gfile.Exists(FLAGS.input_checkpoint + \'.meta\')):\n        print(\'The input checkpoint prefix specified from \'\n              \'`FLAGS.input_checkpoint` must point to a location with \'\n              \'valid meta and data cehckpoint files.\')\n        return -1\n\n    output_path_name = ""pruned_model.ckpt""\n    compression_config = compressor_pb2.CommpressionConfig()\n    with tf.gfile.GFile(FLAGS.prune_config, ""r"") as f:\n        proto_str = f.read()\n        text_format.Merge(proto_str, compression_config)\n    compression_strategy_config = compression_config.compression_strategy\n\n    skippable_nodes = FLAGS.skippable_nodes.replace("" "", """").split("","")\n    compression_fn = functools.partial(\n        compressor_builder.build,\n        compression_factor=FLAGS.compression_factor,\n        skippable_nodes=skippable_nodes,\n        compression_config=compression_strategy_config,\n        interactive_mode=FLAGS.interactive,\n        soft_apply=FLAGS.soft_apply)\n\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n    compressor = compression_fn()\n    compressor.compress(FLAGS.input_checkpoint)\n    compressor.save(\n        output_checkpoint_dir=FLAGS.output_dir,\n        output_checkpoint_name=output_path_name)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
eval.py,10,"b'r""""""Main Evaluation script for ICNet""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport functools\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom builders import dataset_builder\nfrom builders import model_builder\nfrom protos import pipeline_pb2\nfrom libs.evaluator import eval_segmentation_model\nfrom libs.evaluator import eval_segmentation_model_once\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'evaluate_all_from_checkpoint\', None,\n                    \'FIlename to a checkpoint from which to begin running \'\n                    \'evaluation from. All proceeding checkpoints will also \'\n                    \' be evaluated. Should be the similar to model.ckpt.XXX\')\n\nflags.DEFINE_string(\'train_dir\', \'\',\n                    \'Directory containing checkpoints to evaluate, typically \'\n                    \'set to `train_dir` used in the training job.\')\nflags.mark_flag_as_required(\'train_dir\')\n\nflags.DEFINE_string(\'eval_dir\', \'\',\n                    \'Directory to write eval summaries to.\')\nflags.mark_flag_as_required(\'eval_dir\')\n\nflags.DEFINE_string(\'config_path\', \'\',\n                    \'Path to a pipeline_pb2.TrainEvalConfig config \'\n                    \'file. If provided, other configs are ignored\')\nflags.mark_flag_as_required(\'config_path\')\n\nflags.DEFINE_boolean(\'image_summaries\', False,\n                     \'Show summaries of eval predictions and save to \'\n                     \'Tensorboard for viewing.\')\n\nflags.DEFINE_boolean(\'verbose\', False,\n                     \'Show streamed mIoU updates during evaluation of \'\n                     \'various checkpoint evaluation runs.\')\n\nflags.DEFINE_boolean(\'limit_gpu_mem\', False,\n                     \'Set `allow_growth` in GPU options in Session Config.\')\n\n\ndef get_checkpoints_from_path(initial_checkpoint_path, checkpoint_dir):\n    checkpoints = tf.train.get_checkpoint_state(checkpoint_dir)\n    if checkpoints is None:\n        raise ValueError(\'No checkpoints found in `train_dir`.\')\n    all_checkpoints = checkpoints.all_model_checkpoint_paths\n    # Loop through all checkpoints in the specified dir\n    checkpoints_to_evaluate = None\n    tf.logging.info(\'Searching checkpoints in %s\', checkpoint_dir)\n    for idx, ckpt in enumerate(all_checkpoints):\n        print(idx, \' \', str(ckpt))\n        dirname = os.path.dirname(ckpt)\n        full_init_ckpt_path = os.path.join(dirname, initial_checkpoint_path)\n        if str(ckpt) == full_init_ckpt_path:\n            checkpoints_to_evaluate = all_checkpoints[idx:]\n            break\n    # We must be able to find the checkpoint specified\n    if checkpoints_to_evaluate is None:\n        raise ValueError(\'Checkpoint not found. Exiting.\')\n    return checkpoints_to_evaluate\n\n\ndef main(_):\n    tf.gfile.MakeDirs(FLAGS.eval_dir)\n    if not tf.gfile.IsDirectory(FLAGS.train_dir):\n        raise ValueError(\'`train_dir` must be a valid directory \'\n                         \'containing model checkpoints from training.\')\n    pipeline_config = pipeline_pb2.PipelineConfig()\n    with tf.gfile.GFile(FLAGS.config_path, ""r"") as f:\n        proto_str = f.read()\n        text_format.Merge(proto_str, pipeline_config)\n    eval_config = pipeline_config.eval_config\n    input_config = pipeline_config.eval_input_reader\n    model_config = pipeline_config.model\n\n    create_input_fn = functools.partial(\n        dataset_builder.build,\n        input_reader_config=input_config)\n    create_model_fn = functools.partial(\n        model_builder.build,\n        model_config=model_config,\n        is_training=False)\n\n    eval_input_type = eval_config.eval_input_type\n    input_type = eval_input_type.WhichOneof(\'eval_input_type_oneof\')\n    if input_type == \'cropped_eval_input\':\n        cropped_eval_input = eval_input_type.cropped_eval_input\n        input_dims = (cropped_eval_input.height,\n                      cropped_eval_input.width)\n        cropped_evaluation = True\n    elif input_type == \'padded_eval_input\':\n        padded_eval_input = eval_input_type.padded_eval_input\n        input_dims = (padded_eval_input.height,\n                      padded_eval_input.width)\n        cropped_evaluation = False\n    else:\n        raise ValueError(\'Must specify an `eval_input_type` for evaluation.\')\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = FLAGS.limit_gpu_mem\n    if FLAGS.evaluate_all_from_checkpoint is not None:\n        checkpoints_to_evaluate = get_checkpoints_from_path(\n            FLAGS.evaluate_all_from_checkpoint, FLAGS.train_dir)\n        # Run eval on each checkpoint only once. Exit when done.\n        for curr_checkpoint in checkpoints_to_evaluate:\n            tf.reset_default_graph()\n            eval_segmentation_model_once(curr_checkpoint,\n                                         create_model_fn,\n                                         create_input_fn,\n                                         input_dims,\n                                         eval_config,\n                                         eval_dir=FLAGS.eval_dir,\n                                         cropped_evaluation=cropped_evaluation,\n                                         image_summaries=FLAGS.image_summaries,\n                                         verbose=FLAGS.verbose,\n                                         sess_config=config)\n    else:\n        eval_segmentation_model(\n            create_model_fn,\n            create_input_fn,\n            input_dims,\n            eval_config,\n            train_dir=FLAGS.train_dir,\n            eval_dir=FLAGS.eval_dir,\n            cropped_evaluation=cropped_evaluation,\n            evaluate_single_checkpoint=FLAGS.evaluate_all_from_checkpoint,\n            image_summaries=FLAGS.image_summaries,\n            verbose=FLAGS.verbose,\n            sess_config=config)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
export.py,19,"b'r""""""Export segmentation model to a serialized frozen graph file.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\nfrom google.protobuf import text_format\n\nfrom tensorflow.python.tools.freeze_graph import freeze_graph_with_def_protos\n\nfrom protos import pipeline_pb2\nfrom builders import model_builder\nfrom libs.constants import CITYSCAPES_LABEL_COLORS\nfrom libs.exporter import deploy_segmentation_inference_graph\n\n\nslim = tf.contrib.slim\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'input_shape\', None,\n                    \'The shape to use for the placeholder tensor. This should \'\n                    \'be in the form of [batch, height, width, channels] or \'\n                    \'[height, width, channels].\')\n\nflags.DEFINE_string(\'pad_to_shape\', None,\n                     \'Pad the input image to the specified shape. Must have \'\n                     \'the shape specified as [height, width].\')\n\nflags.DEFINE_string(\'config_path\', None,\n                    \'Path to a pipeline_pb2.TrainEvalPipelineConfig config \'\n                    \'file.\')\n\nflags.DEFINE_string(\'trained_checkpoint\', None,\n                    \'Path to trained checkpoint, typically of the form \'\n                    \'path/to/model.ckpt\')\n\nflags.DEFINE_string(\'output_dir\', None, \'Path to write outputs.\')\n\nflags.DEFINE_boolean(\'output_colours\', False,\n                     \'Whether the output should be RGB image.\')\n\n\ndef write_graph_and_checkpoint(inference_graph_def,\n                               model_path,\n                               input_saver_def,\n                               trained_checkpoint_prefix):\n    for node in inference_graph_def.node:\n        node.device = \'\'\n    with tf.Graph().as_default():\n        tf.import_graph_def(inference_graph_def, name=\'\')\n        with tf.Session() as sess:\n            saver = tf.train.Saver(saver_def=input_saver_def,\n                              save_relative_paths=True)\n            saver.restore(sess, trained_checkpoint_prefix)\n            saver.save(sess, model_path)\n\n\ndef profile_inference_graph(graph):\n    tfprof_vars_option = (\n        tf.contrib.tfprof.model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)\n    tfprof_flops_option = tf.contrib.tfprof.model_analyzer.FLOAT_OPS_OPTIONS\n\n    tfprof_vars_option[\'trim_name_regexes\'] = [\'.*BatchNorm.*\']\n    tfprof_flops_option[\'trim_name_regexes\'] = [\n        \'.*BatchNorm.*\', \'.*Initializer.*\', \'.*Regularizer.*\', \'.*BiasAdd.*\'\n    ]\n\n    tf.contrib.tfprof.model_analyzer.print_model_analysis(\n        graph,\n        tfprof_options=tfprof_vars_option)\n\n    tf.contrib.tfprof.model_analyzer.print_model_analysis(\n        graph,\n        tfprof_options=tfprof_flops_option)\n\n\ndef export_inference_graph(pipeline_config,\n                           trained_checkpoint_prefix,\n                           output_directory,\n                           input_shape=None,\n                           pad_to_shape=None,\n                           output_colours=False,\n                           output_collection_name=\'predictions\'):\n\n    _, segmentation_model = model_builder.build(\n        pipeline_config.model, is_training=False)\n\n    tf.gfile.MakeDirs(output_directory)\n    frozen_graph_path = os.path.join(output_directory,\n                                   \'frozen_inference_graph.pb\')\n    eval_graphdef_path = os.path.join(output_directory,\n                                    \'export_graph.pbtxt\')\n    saved_model_path = os.path.join(output_directory, \'saved_model\')\n    model_path = os.path.join(output_directory, \'model.ckpt\')\n\n    outputs, placeholder_tensor = deploy_segmentation_inference_graph(\n        model=segmentation_model,\n        input_shape=input_shape,\n        pad_to_shape=pad_to_shape,\n        label_color_map=(CITYSCAPES_LABEL_COLORS\n            if output_colours else None),\n        output_collection_name=output_collection_name)\n\n    profile_inference_graph(tf.get_default_graph())\n\n    saver = tf.train.Saver()\n    input_saver_def = saver.as_saver_def()\n\n    graph_def = tf.get_default_graph().as_graph_def()\n    f = tf.gfile.FastGFile(eval_graphdef_path, ""w"")\n    f.write(str(graph_def))\n\n    write_graph_and_checkpoint(\n        inference_graph_def=tf.get_default_graph().as_graph_def(),\n        model_path=model_path,\n        input_saver_def=input_saver_def,\n        trained_checkpoint_prefix=trained_checkpoint_prefix)\n\n    output_node_names = outputs.name.split("":"")[0]\n\n    freeze_graph_with_def_protos(\n        input_graph_def=tf.get_default_graph().as_graph_def(),\n        input_saver_def=input_saver_def,\n        input_checkpoint=trained_checkpoint_prefix,\n        output_graph=frozen_graph_path,\n        output_node_names=output_node_names,\n        restore_op_name=\'save/restore_all\',\n        filename_tensor_name=\'save/Const:0\',\n        clear_devices=True,\n        initializer_nodes=\'\')\n\n    print(""Done!"")\n\n\ndef main(_):\n    pipeline_config = pipeline_pb2.PipelineConfig()\n    with tf.gfile.GFile(FLAGS.config_path, \'r\') as f:\n        text_format.Merge(f.read(), pipeline_config)\n    if FLAGS.input_shape:\n        input_shape = [\n            int(dim) if dim != \'-1\' else None\n            for dim in FLAGS.input_shape.split(\',\')]\n    else:\n        input_shape = None\n\n    pad_to_shape = None\n    if FLAGS.pad_to_shape:\n        pad_to_shape = [\n            int(dim) if dim != \'-1\' else None\n            for dim in FLAGS.pad_to_shape.split(\',\')]\n\n    export_inference_graph(pipeline_config,\n                           FLAGS.trained_checkpoint,\n                           FLAGS.output_dir, input_shape,\n                           pad_to_shape,FLAGS.output_colours)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
inference.py,8,"b'r""""""Run inference on an image or group of images.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport timeit\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\nfrom google.protobuf import text_format\n\nfrom protos import pipeline_pb2\nfrom builders import model_builder\nfrom libs.exporter import deploy_segmentation_inference_graph\nfrom libs.constants import CITYSCAPES_LABEL_COLORS, CITYSCAPES_LABEL_IDS\n\n\nslim = tf.contrib.slim\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'input_path\', None,\n                    \'Path to an image or a directory of images.\')\nflags.mark_flag_as_required(\'input_path\')\n\nflags.DEFINE_string(\'input_shape\', \'1024,2048,3\', # default Cityscapes values\n                    \'The shape to use for inference. This should \'\n                    \'be in the form [height, width, channels]. A batch \'\n                    \'dimension is not supported for this test script.\')\nflags.mark_flag_as_required(\'input_shape\')\n\nflags.DEFINE_string(\'pad_to_shape\', \'1025,2049\', # default Cityscapes values\n                     \'Pad the input image to the specified shape. Must have \'\n                     \'the shape specified as [height, width].\')\n\nflags.DEFINE_string(\'config_path\', None,\n                    \'Path to a pipeline_pb2.TrainEvalPipelineConfig config \'\n                    \'file.\')\nflags.mark_flag_as_required(\'config_path\')\n\nflags.DEFINE_string(\'trained_checkpoint\', None,\n                    \'Path to trained checkpoint, typically of the form \'\n                    \'path/to/model.ckpt\')\nflags.mark_flag_as_required(\'trained_checkpoint\')\n\nflags.DEFINE_string(\'output_dir\', \'./\', \'Path to write outputs images.\')\n\nflags.DEFINE_boolean(\'label_ids\', False,\n                     \'Whether the output should be label ids.\')\n\n\ndef _valid_file_ext(input_path):\n    ext = os.path.splitext(input_path)[-1].upper()\n    return ext in [\'.JPG\', \'.JPEG\', \'.PNG\']\n\n\ndef _get_images_from_path(input_path):\n    image_file_paths = []\n    if os.path.isdir(input_path):\n        for dirpath,_,filenames in os.walk(input_path):\n            for f in filenames:\n                file_path = os.path.abspath(os.path.join(dirpath, f))\n                if _valid_file_ext(file_path):\n                    image_file_paths.append(file_path)\n        if len(image_file_paths) == 0:\n            raise ValueError(\'No images in directory. \'\n                             \'Files must be JPG or PNG\')\n    else:\n        if not _valid_file_ext(input_path):\n            raise ValueError(\'File must be JPG or PNG.\')\n        image_file_paths.append(input_path)\n    return image_file_paths\n\n\ndef run_inference_graph(model, trained_checkpoint_prefix,\n                        input_images, input_shape, pad_to_shape,\n                        label_color_map, output_directory):\n\n    outputs, placeholder_tensor = deploy_segmentation_inference_graph(\n        model=model,\n        input_shape=input_shape,\n        pad_to_shape=pad_to_shape,\n        label_color_map=label_color_map)\n\n    with tf.Session() as sess:\n        input_graph_def = tf.get_default_graph().as_graph_def()\n        saver = tf.train.Saver()\n        saver.restore(sess, trained_checkpoint_prefix)\n\n        for idx, image_path in enumerate(input_images):\n            image_raw = np.array(Image.open(image_path))\n\n            start_time = timeit.default_timer()\n            predictions = sess.run(outputs,\n                feed_dict={placeholder_tensor: image_raw})\n            elapsed = timeit.default_timer() - start_time\n\n            print(\'{}) wall time: {}\'.format(elapsed, idx+1))\n            filename = os.path.basename(image_path)\n            save_location = os.path.join(output_directory, filename)\n\n            predictions = predictions.astype(np.uint8)\n            if len(label_color_map[0]) == 1:\n               predictions = np.squeeze(predictions,-1)\n            im = Image.fromarray(predictions[0])\n            im.save(save_location, ""PNG"")\n\n\ndef main(_):\n    output_directory = FLAGS.output_dir\n    tf.gfile.MakeDirs(output_directory)\n    pipeline_config = pipeline_pb2.PipelineConfig()\n    with tf.gfile.GFile(FLAGS.config_path, \'r\') as f:\n        text_format.Merge(f.read(), pipeline_config)\n\n    pad_to_shape = None\n    if FLAGS.input_shape:\n        input_shape = [\n            int(dim) if dim != \'-1\' else None\n                for dim in FLAGS.input_shape.split(\',\')]\n    else:\n        raise ValueError(\'Must supply `input_shape`\')\n\n    if FLAGS.pad_to_shape:\n        pad_to_shape = [\n            int(dim) if dim != \'-1\' else None\n                for dim in FLAGS.pad_to_shape.split(\',\')]\n\n    input_images = _get_images_from_path(FLAGS.input_path)\n    label_map = (CITYSCAPES_LABEL_IDS\n        if FLAGS.label_ids else CITYSCAPES_LABEL_COLORS)\n\n    num_classes, segmentation_model = model_builder.build(\n        pipeline_config.model, is_training=False)\n\n    run_inference_graph(segmentation_model, FLAGS.trained_checkpoint,\n                        input_images, input_shape, pad_to_shape,\n                        label_map, output_directory)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
train.py,5,"b'r""""""Main Training script for ICNet""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom builders import model_builder\nfrom builders import dataset_builder\nfrom protos import pipeline_pb2\nfrom libs.trainer import train_segmentation_model\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\n# Distributed training settings\n\nflags.DEFINE_integer(\'num_clones\', 1,\n                     \'Number of model clones to deploy to each worker replica.\'\n                     \'This should be greater than one if you want to use \'\n                     \'multiple GPUs located on a single machine.\')\n\nflags.DEFINE_boolean(\'clone_on_cpu\', False, \'Use CPUs to deploy clones.\')\n\nflags.DEFINE_integer(\'num_replicas\', 1,\n                     \'Number of worker replicas. This typically corresponds \'\n                     \'to the number of machines you are training on. Note \'\n                     \'that the training will be done asynchronously.\')\n\nflags.DEFINE_integer(\'startup_delay_steps\', 15,\n                     \'Number of training steps between replicas startup.\')\n\nflags.DEFINE_integer(\'num_ps_tasks\', 0,\n                     \'The number of parameter servers. If the value is 0, then \'\n                     \'the parameters are handled locally by the worker. It is \'\n                     \'reccomended to use num_ps_tasks=num_replicas/2.\')\n\nflags.DEFINE_string(\'master\', \'\', \'BNS name of the tensorflow server\')\n\nflags.DEFINE_integer(\'task\', 0, \'The task ID. Should increment per worker \'\n                     \'replica added to achieve between graph replication.\')\n\n# Training configuration settings\n\nflags.DEFINE_string(\'config_path\', \'\',\n                    \'Path to a pipeline_pb2.TrainEvalConfig config \'\n                    \'file. If provided, other configs are ignored\')\nflags.mark_flag_as_required(\'config_path\')\n\nflags.DEFINE_string(\'logdir\', \'\',\n                    \'Directory to save the checkpoints and training summaries.\')\nflags.mark_flag_as_required(\'logdir\')\n\nflags.DEFINE_integer(\'save_interval_secs\', 600, # default to 5 min\n                     \'Time between successive saves of a checkpoint in secs.\')\n\nflags.DEFINE_integer(\'max_checkpoints_to_keep\', 15, # might want to cut this down\n                     \'Number of checkpoints to keep in the `logdir`.\')\n\n# Debug flag\n\nflags.DEFINE_boolean(\'image_summaries\', False, \'\')\n\n\ndef main(_):\n    tf.gfile.MakeDirs(FLAGS.logdir)\n    pipeline_config = pipeline_pb2.PipelineConfig()\n    with tf.gfile.GFile(FLAGS.config_path, ""r"") as f:\n        proto_str = f.read()\n        text_format.Merge(proto_str, pipeline_config)\n\n    model_config = pipeline_config.model\n    train_config = pipeline_config.train_config\n    input_config = pipeline_config.train_input_reader\n\n    create_model_fn = functools.partial(\n        model_builder.build,\n        model_config=model_config,\n        is_training=True)\n\n    create_input_fn = functools.partial(\n        dataset_builder.build,\n        input_reader_config=input_config)\n\n    is_chief = (FLAGS.task == 0)\n\n    train_segmentation_model(\n        create_model_fn,\n        create_input_fn,\n        train_config,\n        master=FLAGS.master,\n        task=FLAGS.task,\n        is_chief=is_chief,\n        startup_delay_steps=FLAGS.startup_delay_steps,\n        train_dir=FLAGS.logdir,\n        num_clones=FLAGS.num_clones,\n        num_worker_replicas=FLAGS.num_replicas,\n        clone_on_cpu=FLAGS.clone_on_cpu,\n        replica_id=FLAGS.task,\n        num_replicas=FLAGS.num_replicas,\n        num_ps_tasks=FLAGS.num_ps_tasks,\n        max_checkpoints_to_keep=FLAGS.max_checkpoints_to_keep,\n        save_interval_secs=FLAGS.save_interval_secs,\n        image_summaries=FLAGS.image_summaries)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
train_mem_saving.py,6,"b'r""""""Train with gradient checkpointing\n\nAllows for training with larger batch size than would normally be\npermitted. Used to train PSPNet and ICNet to facilitate the required\nbatch size of 16. Also facilities measuring memory usage. Thanks to Yaroslav\nBulatov and Tim Salimans for their gradient checkpointing implementation.\nTheir implementation can be found here:\n    https://github.com/openai/gradient-checkpointing\n\nFor ICNet, the suggested checkpoint nodes are:\n\n    SharedFeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/Relu\n    SharedFeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/Relu\n    SharedFeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/Relu\n    SharedFeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/Relu\n    FastPSPModule/Conv/Relu\n    CascadeFeatureFusion/Relu\n    CascadeFeatureFusion_1/Relu\n\nFor PSPNet 50, the suggested checkpoint nodes are:\n\n    SharedFeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/Relu\n    SharedFeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/Relu\n    SharedFeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/Relu\n    SharedFeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/Relu\n    PSPModule/Conv/Relu\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom third_party.memory_saving_gradients_patch import gradients\n\nfrom builders import model_builder\nfrom builders import dataset_builder\nfrom protos import pipeline_pb2\nfrom libs.trainer import train_segmentation_model\n\n\n# CEHCKPOINT NODES FOR MEM SAVING GRADIENTS\nICNET_GRADIENT_CHECKPOINTS = [\n    \'SharedFeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/Relu\',\n    \'SharedFeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/Relu\',\n    \'SharedFeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/Relu\',\n    \'SharedFeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/Relu\',\n    \'FastPSPModule/Conv/Relu6\',\n    \'CascadeFeatureFusion/Relu\',\n    \'CascadeFeatureFusion_1/Relu\'\n]\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nslim = tf.contrib.slim\n\nprefetch_queue = slim.prefetch_queue\n\nflags = tf.app.flags\n\nFLAGS = flags.FLAGS\n\n# Distributed training settings\n\nflags.DEFINE_integer(\'num_clones\', 1,\n                     \'Number of model clones to deploy to each worker replica.\'\n                     \'This should be greater than one if you want to use \'\n                     \'multiple GPUs located on a single machine.\')\n\nflags.DEFINE_boolean(\'clone_on_cpu\', False, \'Use CPUs to deploy clones.\')\n\nflags.DEFINE_integer(\'num_replicas\', 1,\n                     \'Number of worker replicas. This typically corresponds \'\n                     \'to the number of machines you are training on. Note \'\n                     \'that the training will be done asynchronously.\')\n\nflags.DEFINE_integer(\'startup_delay_steps\', 15,\n                     \'Number of training steps between replicas startup.\')\n\nflags.DEFINE_integer(\'num_ps_tasks\', 0,\n                     \'The number of parameter servers. If the value is 0, then \'\n                     \'the parameters are handled locally by the worker. It is \'\n                     \'reccomended to use num_ps_tasks=num_replicas/2.\')\n\nflags.DEFINE_string(\'master\', \'\', \'BNS name of the tensorflow server\')\n\nflags.DEFINE_integer(\'task\', 0, \'The task ID. Should increment per worker \'\n                     \'replica added to achieve between graph replication.\')\n\n# Training configuration settings\n\nflags.DEFINE_string(\'config_path\', \'\',\n                    \'Path to a pipeline_pb2.TrainEvalConfig config \'\n                    \'file. If provided, other configs are ignored\')\nflags.mark_flag_as_required(\'config_path\')\n\nflags.DEFINE_string(\'logdir\', \'\',\n                    \'Directory to save the checkpoints and training summaries.\')\nflags.mark_flag_as_required(\'logdir\')\n\nflags.DEFINE_integer(\'save_interval_secs\', 600, # default to 5 min\n                     \'Time between successive saves of a checkpoint in secs.\')\n\nflags.DEFINE_integer(\'max_checkpoints_to_keep\', 50, # might want to cut this down\n                     \'Number of checkpoints to keep in the `logdir`.\')\n\nflags.DEFINE_string(\'checkpoint_nodes\', None,\n                    \'Names of nodes to use as checkpoint nodes for training.\')\n\n# Debug flag\n\nflags.DEFINE_boolean(\'log_memory\', False, \'\')\n\nflags.DEFINE_boolean(\'image_summaries\', False, \'\')\n\n\ndef main(_):\n    tf.gfile.MakeDirs(FLAGS.logdir)\n    pipeline_config = pipeline_pb2.PipelineConfig()\n    with tf.gfile.GFile(FLAGS.config_path, ""r"") as f:\n        proto_str = f.read()\n        text_format.Merge(proto_str, pipeline_config)\n\n    model_config = pipeline_config.model\n    train_config = pipeline_config.train_config\n    input_config = pipeline_config.train_input_reader\n\n    create_model_fn = functools.partial(\n        model_builder.build,\n        model_config=model_config,\n        is_training=True)\n\n    create_input_fn = functools.partial(\n        dataset_builder.build,\n        input_reader_config=input_config)\n\n    is_chief = (FLAGS.task == 0)\n\n    checkpoint_nodes = FLAGS.checkpoint_nodes\n    if checkpoint_nodes is None:\n        checkpoint_nodes = ICNET_GRADIENT_CHECKPOINTS\n    else:\n        checkpoint_nodes = checkpoint_nodes.replace("" "", """").split("","")\n\n    train_segmentation_model(\n        create_model_fn,\n        create_input_fn,\n        train_config,\n        master=FLAGS.master,\n        task=FLAGS.task,\n        is_chief=is_chief,\n        startup_delay_steps=FLAGS.startup_delay_steps,\n        train_dir=FLAGS.logdir,\n        num_clones=FLAGS.num_clones,\n        num_worker_replicas=FLAGS.num_replicas,\n        clone_on_cpu=FLAGS.clone_on_cpu,\n        replica_id=FLAGS.task,\n        num_replicas=FLAGS.num_replicas,\n        num_ps_tasks=FLAGS.num_ps_tasks,\n        max_checkpoints_to_keep=FLAGS.max_checkpoints_to_keep,\n        save_interval_secs=FLAGS.save_interval_secs,\n        image_summaries=FLAGS.image_summaries,\n        log_memory=FLAGS.log_memory,\n        gradient_checkpoints=checkpoint_nodes)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
architectures/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'
architectures/icnet_architecture.py,22,"b'r""""""ICNet Semantic Segmentation architecture.\n\nAs described in http://arxiv.org/abs/1704.08545.\n\n  ICNet for Real-Time Semantic Segmentation\n    on High-Resolution Images\n  Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs import base_model as model\nfrom libs import compressible_ops as ops\n\nslim = tf.contrib.slim  # pylint: disable=C0103,E1101\n\n\nclass ICNetArchitecture(model.FastSegmentationModel):\n    """"""ICNet Architecture definition.""""""\n\n    def __init__(self,\n                 is_training,\n                 model_arg_scope,\n                 num_classes,\n                 feature_extractor,\n                 classification_loss,\n                 filter_scale,\n                 pooling_factors,\n                 pretrain_single_branch_mode=False,\n                 use_aux_loss=True,\n                 main_loss_weight=1.0,\n                 first_branch_loss_weight=0.0,\n                 second_branch_loss_weight=0.0,\n                 upsample_train_logits=False,\n                 add_summaries=True,\n                 no_add_n_op=False,\n                 scope=None):\n        super(ICNetArchitecture, self).__init__(num_classes=num_classes)\n        self._is_training = is_training\n        self._model_arg_scope = model_arg_scope\n        self._num_classes = num_classes\n        self._feature_extractor = feature_extractor\n        self._filter_scale = filter_scale\n        self._pooling_factors = pooling_factors\n        self._pretrain_single_branch_mode = pretrain_single_branch_mode\n        self._classification_loss = classification_loss\n        self._use_aux_loss = use_aux_loss\n        self._default_aux_weight = 0.4  # TODO: put this in protos\n        self._main_loss_weight = main_loss_weight\n        self._first_branch_loss_weight = first_branch_loss_weight\n        self._second_branch_loss_weight = second_branch_loss_weight\n        self._add_summaries = add_summaries\n        self._no_add_n_op = no_add_n_op\n        self._upsample_train_logits = upsample_train_logits\n        self._output_zoom_factor = (\n            8 if self._pretrain_single_branch_mode else 4)\n        self._scope = scope\n\n    @property\n    def main_class_predictions_key(self):\n        return \'class_predictions\'\n\n    @property\n    def first_aux_predictions_key(self):\n        return \'first_aux_predictions\'\n\n    @property\n    def second_aux_predictions_key(self):\n        return \'second_aux_predictions\'\n\n    @property\n    def single_branch_mode_predictions_key(self):\n        return \'single_branch_mode_predictions\'\n\n    @property\n    def main_loss_key(self):\n        return \'loss\'\n\n    @property\n    def first_aux_loss_key(self):\n        return \'first_aux_loss\'\n\n    @property\n    def second_aux_loss_key(self):\n        return \'second_aux_loss\'\n\n    @property\n    def pretrain_single_branch_mode_loss_key(self):\n        return \'pretrain_single_branch_mode_loss\'\n\n    def preprocess(self, inputs):\n        if inputs.dtype is not tf.float32:\n            raise ValueError(\'`preprocess` expects a tf.float32 tensor\')\n        with tf.name_scope(\'Preprocessor\'):\n            return self._feature_extractor.preprocess(inputs)\n\n    def _extract_shared_features(self, preprocessed_inputs, scope):\n        if not self._pretrain_single_branch_mode:\n            extractor_inputs = self._dynamic_interpolation(preprocessed_inputs,\n                                                           s_factor=0.5)\n        else:\n            extractor_inputs = preprocessed_inputs\n        outputs = self._feature_extractor.extract_features(extractor_inputs,\n                                                           scope=scope)\n        return outputs\n\n    def predict(self, preprocessed_inputs):\n        """"""Build main inference pass""""""\n        with slim.arg_scope(self._model_arg_scope):\n            # Feature extraction from arbitrary extractor\n            half_res, quarter_res, psp_aux_out = self._extract_shared_features(\n                preprocessed_inputs,\n                scope=self.shared_feature_extractor_scope)\n            # Branch specific layers\n            pooled_quarter_res = self._icnet_pspmodule(quarter_res)\n\n            # We enable the option to train using only the main resolution\n            # branch which includes the PSPNet based module\n            if not self._pretrain_single_branch_mode:\n                # Full resolution branch\n                full_res = self._third_feature_branch(preprocessed_inputs)\n                # Fusions of all branches using CFF module\n                first_fusion, first_aux_logits = self._cascade_feature_fusion(\n                    pooled_quarter_res, half_res)\n                (second_fusion,\n                 second_aux_logits) = self._cascade_feature_fusion(\n                     first_fusion, full_res)\n                final_logits = self._dynamic_interpolation(\n                    second_fusion,\n                    z_factor=2.0)\n            else:\n                final_logits = pooled_quarter_res\n\n            # Class class_predictions\n            with tf.variable_scope(\'Predictions\'):\n                predictions = ops.conv2d(final_logits,\n                                         self._num_classes, 1, 1,\n                                         prediction_output=True)\n                if not self._is_training:  # evaluation output\n                    predictions = self._dynamic_interpolation(\n                        predictions, z_factor=self._output_zoom_factor)\n\n            # Main output used in both pretrain and regular train mode\n            prediction_dict = {}\n            prediction_dict[self.main_class_predictions_key] = predictions\n\n            # Auxilarary loss for training all three ICNet branches\n            if self._is_training and self._use_aux_loss:\n                if self._pretrain_single_branch_mode:\n                    with tf.variable_scope(\'AuxPredictions\'):\n                        psp_aux_out = ops.conv2d(psp_aux_out,\n                                                 self._num_classes, 1, 1,\n                                                 prediction_output=True)\n                        prediction_dict[\n                            self.single_branch_mode_predictions_key\n                        ] = psp_aux_out\n                else:\n                    prediction_dict[\n                        self.first_aux_predictions_key] = ops.conv2d(\n                            first_aux_logits,\n                            self._num_classes, 1, 1,\n                            prediction_output=True,\n                            scope=\'AuxOutput\')\n                    prediction_dict[\n                        self.second_aux_predictions_key] = ops.conv2d(\n                            second_aux_logits,\n                            self._num_classes, 1, 1,\n                            prediction_output=True,\n                            scope=\'AuxOutput_1\')\n\n            return prediction_dict\n\n    def _icnet_pspmodule(self, input_features, scope=None):\n        """"""Modified PSPModule for fast inference.\n\n        Modifications are primarily the removal of convs within each branch\n        and the replacement concatenation with addition for the aggregation\n        operation at the end of the module.\n\n        A suggestion here is to first train the first resolution\n        branche without considering other branches. After some M number of\n        steps, begin training all branches as normal.\n        """"""\n        pooled_features = input_features\n        added_features = input_features\n        input_shape = input_features.shape.as_list()\n        input_h, input_w = input_shape[1], input_shape[2]\n\n        output_pooling_shape = (input_h, input_w)\n        with tf.variable_scope(scope, \'FastPSPModule\'):\n            for pooling_factor in self._pooling_factors:\n                input_pooling_shape = (int(input_h / pooling_factor),\n                                       int(input_w / pooling_factor))\n                pooled_features = slim.avg_pool2d(\n                    input_features,\n                    input_pooling_shape,\n                    stride=input_pooling_shape)\n                pooled_features = tf.image.resize_bilinear(\n                    pooled_features,\n                    size=output_pooling_shape,\n                    align_corners=True)\n                added_features = tf.add(added_features, pooled_features)\n\n            # Final Conv\n            final_output = ops.conv2d(added_features, 512, 1,\n                                      stride=1,\n                                      compression_ratio=self._filter_scale)\n        return final_output\n\n    def _third_feature_branch(self, preprocessed_inputs):\n        conv_0 = ops.conv2d(preprocessed_inputs,\n                            64, (3, 3), stride=2,\n                            compression_ratio=self._filter_scale)\n        conv_1 = ops.conv2d(conv_0,\n                            64, (3, 3), stride=2,\n                            compression_ratio=self._filter_scale)\n        conv_2 = ops.conv2d(conv_1,\n                            128, (3, 3), stride=2,\n                            compression_ratio=self._filter_scale)\n        output = ops.conv2d(conv_2,\n                            256, (3, 3), stride=1,\n                            compression_ratio=self._filter_scale)\n        return output\n\n    def _cascade_feature_fusion(self,\n                                first_feature_map,\n                                second_feature_map,\n                                scope=None):\n        """"""Cascade Feature Fusion Branch.\n\n        Note how the two convs have no acitvations. The acitvation is applied\n        at the end of the operation.\n        """"""\n        with tf.variable_scope(scope, \'CascadeFeatureFusion\'):\n            upsampled_inputs = self._dynamic_interpolation(first_feature_map,\n                                                           z_factor=2.0)\n            dilated_conv = ops.conv2d(\n                upsampled_inputs, 256, (3, 3), stride=1,\n                compression_ratio=self._filter_scale, rate=2,\n                activation_fn=None, scope=\'DilatedConv\')\n            conv = ops.conv2d(\n                second_feature_map, 256, (1, 1),\n                compression_ratio=self._filter_scale,\n                activation_fn=None, scope=\'Conv\')\n            conv_shape = tf.shape(conv)[1:3]\n            dilated_conv = tf.image.resize_bilinear(dilated_conv, conv_shape)\n            # Merge both convs to output\n            branch_merge = tf.add(conv, dilated_conv)\n            output = tf.nn.relu(branch_merge)\n        return output, upsampled_inputs\n\n    def _dynamic_interpolation(self, features_to_upsample,\n                               s_factor=1.0, z_factor=1.0):\n        with tf.name_scope(\'Interp\'):\n            feature_shape = features_to_upsample.shape.as_list()\n            input_h, input_w = feature_shape[1], feature_shape[2]\n            shrink_h = (input_h - 1) * s_factor + 1\n            shrink_w = (input_w - 1) * s_factor + 1\n            zoom_h = shrink_h + (shrink_h - 1) * (z_factor - 1)\n            zoom_w = shrink_w + (shrink_w - 1) * (z_factor - 1)\n            return tf.image.resize_bilinear(features_to_upsample,\n                                            size=[int(zoom_h), int(zoom_w)],\n                                            align_corners=True)\n\n    def loss(self, prediction_dict):\n        losses_dict = {}\n\n        # TODO: Make this an optional choice. For now only scale\n        # down labels like in original paper\n        def _resize_labels_to_logits(labels, logits):\n            logits_shape = logits.get_shape().as_list()\n            scaled_labels = tf.image.resize_nearest_neighbor(\n                labels, logits_shape[1:3], align_corners=True)\n            return scaled_labels\n\n        main_preds = prediction_dict[self.main_class_predictions_key]\n        with tf.name_scope(\'SegmentationLoss\'):  # 1/4 labels\n            if self._upsample_train_logits:\n                main_preds = self._dynamic_interpolation(\n                    main_preds, z_factor=self._output_zoom_factor)\n            main_scaled_labels = _resize_labels_to_logits(\n                self._groundtruth_labels, main_preds)\n            main_loss = self._classification_loss(main_preds,\n                                                  main_scaled_labels)\n            losses_dict[\n                self.main_loss_key] = (main_loss * self._main_loss_weight)\n\n        if self._is_training and self._use_aux_loss:\n            if not self._pretrain_single_branch_mode:\n                first_aux_preds = prediction_dict[\n                    self.first_aux_predictions_key]\n                second_aux_preds = prediction_dict[\n                    self.second_aux_predictions_key]\n\n                with tf.name_scope(\'FirstBranchAuxLoss\'):  # 1/16 labels\n                    if self._upsample_train_logits:\n                        first_aux_preds = self._dynamic_interpolation(\n                            first_aux_preds,\n                            z_factor=self._output_zoom_factor)\n                    first_scaled_labels = _resize_labels_to_logits(\n                        self._groundtruth_labels, first_aux_preds)\n                    first_aux_loss = self._classification_loss(\n                        first_aux_preds, first_scaled_labels)\n                    losses_dict[self.first_aux_loss_key] = (\n                        self._first_branch_loss_weight * first_aux_loss)\n\n                with tf.name_scope(\'SecondBranchAuxLoss\'):  # 1/8 labels\n                    if self._upsample_train_logits:\n                        second_aux_preds = self._dynamic_interpolation(\n                            second_aux_preds,\n                            z_factor=self._output_zoom_factor)\n                    second_scaled_labels = _resize_labels_to_logits(\n                        self._groundtruth_labels, second_aux_preds)\n                    second_aux_loss = self._classification_loss(\n                        second_aux_preds, second_scaled_labels)\n                    losses_dict[self.second_aux_loss_key] = (\n                        self._second_branch_loss_weight * second_aux_loss)\n            else:\n                with tf.name_scope(\'PretrainMainAuxLoss\'):  # 1/8 labels\n                    psp_pretrain_preds = prediction_dict[\n                        self.single_branch_mode_predictions_key]\n                    psp_aux_scaled_labels = _resize_labels_to_logits(\n                        self._groundtruth_labels, psp_pretrain_preds)\n                    psp_pretrain_loss = self._classification_loss(\n                        psp_pretrain_preds, psp_aux_scaled_labels)\n                    losses_dict[self.pretrain_single_branch_mode_loss_key] = (\n                        self._default_aux_weight * psp_pretrain_loss)\n        return losses_dict\n\n    def restore_map(self, fine_tune_checkpoint_type=\'segmentation\'):\n        """"""Restore variables for checkpoints correctly.""""""\n        if fine_tune_checkpoint_type not in [\n                \'segmentation\', \'classification\', \'segmentation-finetune\']:\n            raise ValueError(\'Not supported fine_tune_checkpoint_type: \'\n                             \'{}\'.format(fine_tune_checkpoint_type))\n        if fine_tune_checkpoint_type == \'classification\':\n            tf.logging.info(\'Fine-tuning from classification checkpoints.\')\n            return self._feature_extractor.restore_from_classif_checkpoint_fn(\n                self.shared_feature_extractor_scope)\n        exclude_list = [\'global_step\', \'Predictions\']\n        variables_to_restore = slim.get_variables_to_restore(\n            exclude=exclude_list)\n        return variables_to_restore\n'"
architectures/pspnet_architecture.py,18,"b'r""""""PSPNet Semantic Segmentation architecture.\n\nAs described in http://arxiv.org/abs/1612.01105.\n\n  Pyramid Scene Parsing Network\n  Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia\n\nThis is a baseline architecture that was implemented for the purposes\nof validating the other segmentation models which are more efficient.\n\nPlease note that although this network is accurate, it is VERY\nslow and memory intensive. It should not be used under the assumption that\nit will result in similar performance as the other models in this project.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\nfrom libs import base_model as model\nfrom libs import compressible_ops as ops\n\nslim = tf.contrib.slim\n\n\nclass PSPNetArchitecture(model.FastSegmentationModel):\n    """"""PSPNet Architecture definition.""""""\n\n    def __init__(self,\n                 is_training,\n                 model_arg_scope,\n                 num_classes,\n                 feature_extractor,\n                 classification_loss,\n                 filter_scale,\n                 pooling_factors,\n                 use_aux_loss=True,\n                 main_loss_weight=1,\n                 aux_loss_weight=0,\n                 upsample_train_logits=False,\n                 add_summaries=True,\n                 scope=None):\n        super(PSPNetArchitecture, self).__init__(num_classes=num_classes)\n        self._is_training = is_training\n        self._model_arg_scope = model_arg_scope\n        self._num_classes = num_classes\n        self._feature_extractor = feature_extractor\n        self._classification_loss = classification_loss\n        self._filter_scale = filter_scale\n        self._pooling_factors = pooling_factors\n        self._use_aux_loss = use_aux_loss\n        self._main_loss_weight = main_loss_weight\n        self._aux_loss_weight = aux_loss_weight\n        self._add_summaries = add_summaries\n        self._upsample_train_logits = upsample_train_logits\n        self._output_zoom_factor = 8.0\n        self._scope = scope\n\n    @property\n    def main_class_predictions_key(self):\n        return \'class_predictions\'\n\n    @property\n    def aux_predictions_key(self):\n        return \'aux_predictions\'\n\n    @property\n    def main_loss_key(self):\n        return \'loss\'\n\n    @property\n    def aux_loss_key(self):\n        return \'aux_loss\'\n\n    def preprocess(self, inputs):\n        if inputs.dtype is not tf.float32:\n            raise ValueError(\'`preprocess` expects a tf.float32 tensor\')\n        with tf.name_scope(\'Preprocessor\'):\n            return self._feature_extractor.preprocess(inputs)\n\n    def _extract_shared_features(self, preprocessed_inputs, scope):\n        return self._feature_extractor.extract_features(\n            preprocessed_inputs, scope=scope)\n\n    def predict(self, preprocessed_inputs):\n        """"""Build main inference pass""""""\n        with slim.arg_scope(self._model_arg_scope):\n            # Feature extraction from arbitrary extractor\n            _, backbone_logits, psp_aux_out = self._extract_shared_features(\n                preprocessed_inputs,\n                scope=self.shared_feature_extractor_scope)\n            # Branch specific layers\n            final_logits = self._pspnet_pspmodule(backbone_logits)\n            # Class class_predictions\n            with tf.variable_scope(\'Predictions\'):\n                predictions = ops.conv2d(final_logits,\n                                         self._num_classes, 1, 1,\n                                         prediction_output=True)\n                if not self._is_training:  # evaluation\n                    predictions = self._dynamic_interpolation(\n                        predictions, z_factor=self._output_zoom_factor)\n            # Outputs with auxilarary loss for training\n            prediction_dict = {\n                self.main_class_predictions_key: predictions}\n            # Aux loss as described in PSPNet paper\n            if self._is_training and self._use_aux_loss:\n                with tf.variable_scope(\'AuxPredictions\'):\n                    aux_preds = ops.conv2d(psp_aux_out,\n                                           self._num_classes, 1, 1,\n                                           prediction_output=True,\n                                           scope=\'AuxOutput\')\n                prediction_dict[self.aux_predictions_key] = aux_preds\n            return prediction_dict\n\n    def _pspnet_pspmodule(self, input_features, scope=None):\n        """"""PSP Module present in the original paper.""""""\n        pooled_features = input_features\n        branch_outputs = [input_features]\n        input_shape = input_features.shape.as_list()\n        input_h, input_w = input_shape[1], input_shape[2]\n\n        output_pooling_shape = (input_h, input_w)\n        with tf.variable_scope(scope, \'PSPModule\'):\n            for pooling_factor in self._pooling_factors:\n                input_pooling_shape = (int(input_h / pooling_factor),\n                                       int(input_w / pooling_factor))\n                pooled_features = slim.avg_pool2d(\n                    input_features,\n                    input_pooling_shape,\n                    stride=input_pooling_shape)\n                # No filter are removed from these convs for simplicity.\n                pooled_features = ops.conv2d(pooled_features, 512, 1, stride=1)\n                pooled_features = tf.image.resize_bilinear(\n                    pooled_features,\n                    size=output_pooling_shape,\n                    align_corners=True)\n                branch_outputs.append(pooled_features)\n\n            # Concat all branches\n            branch_merge = tf.concat(branch_outputs, axis=-1)\n            final_output = ops.conv2d(branch_merge, 512, 3,\n                                      stride=1,\n                                      compression_ratio=self._filter_scale)\n        return final_output\n\n    def _dynamic_interpolation(self, features_to_upsample,\n                               s_factor=1.0, z_factor=1.0):\n        with tf.name_scope(\'Interp\'):\n            features_shape = features_to_upsample.shape.as_list()\n            input_h, input_w = features_shape[1], features_shape[2]\n            shrink_h = (input_h - 1) * s_factor + 1\n            shrink_w = (input_w - 1) * s_factor + 1\n            zoom_h = shrink_h + (shrink_h - 1) * (z_factor - 1)\n            zoom_w = shrink_w + (shrink_w - 1) * (z_factor - 1)\n            return tf.image.resize_bilinear(features_to_upsample,\n                                            size=[int(zoom_h), int(zoom_w)],\n                                            align_corners=True)\n\n    def loss(self, prediction_dict):\n        losses_dict = {}\n\n        # TODO: Make this an optional choice. For now only scale\n        # down labels like in original paper\n        def _resize_labels_to_logits(labels, logits):\n            logits_shape = logits.get_shape().as_list()\n            scaled_labels = tf.image.resize_nearest_neighbor(\n                labels, logits_shape[1:3], align_corners=True)\n            return scaled_labels\n\n        main_preds = prediction_dict[self.main_class_predictions_key]\n        with tf.name_scope(\'SegmentationLoss\'):  # 1/8th labels\n            if self._upsample_train_logits:\n                main_preds = self._dynamic_interpolation(\n                    main_preds, z_factor=self._output_zoom_factor)\n            main_scaled_labels = _resize_labels_to_logits(\n                self._groundtruth_labels, main_preds)\n            main_loss = self._classification_loss(main_preds,\n                                                  main_scaled_labels)\n            losses_dict[self.main_loss_key] = (\n                self._main_loss_weight * main_loss)\n\n        if self._use_aux_loss and self._is_training:\n            aux_preds = prediction_dict[self.aux_predictions_key]\n            with tf.name_scope(\'AuxLoss\'):  # 1/8th labels\n                if self._upsample_train_logits:\n                    aux_preds = self._dynamic_interpolation(\n                        aux_preds, z_factor=self._output_zoom_factor)\n                aux_scaled_labels = _resize_labels_to_logits(\n                    self._groundtruth_labels, aux_preds)\n                first_aux_loss = self._classification_loss(aux_preds,\n                                                           aux_scaled_labels)\n                losses_dict[self.aux_loss_key] = (\n                    self._aux_loss_weight * first_aux_loss)\n        return losses_dict\n\n    def restore_map(self, fine_tune_checkpoint_type=\'classification\'):\n        """"""Restore variables for checkpoints correctly""""""\n        if fine_tune_checkpoint_type not in [\'segmentation\', \'classification\']:\n            raise ValueError(\'Not supported fine_tune_checkpoint_type: \'\n                             \' {}\'.format(fine_tune_checkpoint_type))\n        if fine_tune_checkpoint_type == \'classification\':\n            tf.logging.info(\'Fine-tuning from classification checkpoints.\')\n            return self._feature_extractor.restore_from_classif_checkpoint_fn(\n                self.shared_feature_extractor_scope)\n        exclude_list = [\'global_step\']\n        variables_to_restore = slim.get_variables_to_restore(\n            exclude=exclude_list)\n        return variables_to_restore\n\n\nclass PSPNetFeatureExtractor(object):\n    """"""PSPNet ICNet Based Feature Extractor definition.""""""\n\n    def __init__(self,\n                 is_training,\n                 features_stride,\n                 batch_norm_trainable=False,\n                 reuse_weights=None,\n                 weight_decay=0.0):\n        self._is_training = is_training\n        self._features_stride = features_stride\n        self._train_batch_norm = (batch_norm_trainable and is_training)\n        self._reuse_weights = reuse_weights\n        self._weight_decay = weight_decay\n\n    @abstractmethod\n    def preprocess(self, raw_inputs):\n        pass\n\n    def extract_features(self, preprocessed_inputs, scope=None):\n        """"""Extracts half resolution features.""""""\n        with tf.variable_scope(scope,\n                               values=[preprocessed_inputs],\n                               reuse=tf.AUTO_REUSE):\n            return self._extract_features(preprocessed_inputs, scope)\n\n    @abstractmethod\n    def _extract_features(self, preprocessed_inputs, scope):\n        pass\n\n    @staticmethod\n    def restore_from_classif_checkpoint_fn(scope_name):\n        variables_to_restore = {}\n        for variable in tf.global_variables():\n            if variable.op.name.startswith(scope_name):\n                var_name = variable.op.name.replace(scope_name + \'/\', \'\')\n                variables_to_restore[var_name] = variable\n        return variables_to_restore\n'"
builders/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'
builders/compressor_builder.py,0,"b'r""""""Builder for compressor.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nfrom libs.filter_pruner import FilterPruner, FilterPrunerNodeSpec\nfrom protos import compressor_pb2\n\n\ndef _complete_node_scope(name, parent_scope, overide_scope=None):\n    if not name:\n        return \'\'\n    if name[:3] == ""..."":\n        return name[3:]\n    main_scope = overide_scope if overide_scope is not None else parent_scope\n    return os.path.join(main_scope, name)\n\n\ndef _build_filter_pruning_compressor(filter_pruning_config, skippable_nodes,\n                                     compression_factor, interactive_mode,\n                                     soft_apply):\n    input_node_name = filter_pruning_config.input.name\n    output_node_name = filter_pruning_config.output.name\n    # skippable_nodes\n    config_skip_nodes = []\n    for skip_node in filter_pruning_config.skip_node:\n        config_skip_nodes.append(skip_node.name)\n    config_skip_nodes += skippable_nodes\n    # Get prespecified pruner specs for complex nodes pruning schemes\n    pruner_specs = {}\n    nonoveride_complete_scope = functools.partial(\n        _complete_node_scope,\n        parent_scope=filter_pruning_config.node_scope)\n    for node in filter_pruning_config.node:\n        overide_scope = (\n            node.node_scope if node.node_scope != ""null"" else None)\n        complete_scope = functools.partial(\n            nonoveride_complete_scope,\n            overide_scope=overide_scope)\n        pruner_spec_key = complete_scope(node.target.name)\n        following = []\n        for follow_node in node.following:\n            following.append(complete_scope(follow_node.name))\n        pruner_spec = FilterPrunerNodeSpec(\n            source=complete_scope(node.source.name),\n            target=complete_scope(node.target.name),\n            following=following)\n        pruner_specs[pruner_spec_key] = pruner_spec\n    # Pruner class to use in script\n    return FilterPruner(input_node=input_node_name,\n                        output_node=output_node_name,\n                        compression_factor=compression_factor,\n                        init_pruner_specs=pruner_specs,\n                        skippable_nodes=config_skip_nodes,\n                        interactive_mode=interactive_mode,\n                        soft_apply=soft_apply)\n\n\ndef build(compression_config, skippable_nodes, compression_factor,\n          interactive_mode=False, soft_apply=False):\n    if not isinstance(compression_config, compressor_pb2.CompressionStrategy):\n        raise ValueError(\'pruner_config not of type \'\n                         \'compressor_pb2.CompressionStrategy.\')\n    compression_strategy = compression_config.WhichOneof(\'compression_strategy\')\n    if compression_strategy == \'filter_pruner\':\n        return _build_filter_pruning_compressor(\n            compression_config.filter_pruner,\n            skippable_nodes,\n            compression_factor,\n            interactive_mode,\n            soft_apply)\n\n    raise ValueError(\'Unknown compression strategy: {}\'.format(\n        compression_strategy))\n'"
builders/dataset_builder.py,14,"b'r""""""Builder for semantic segmentation dataset.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport tensorflow as tf\n\nfrom libs import standard_fields as fields\nfrom protos import input_reader_pb2\n\n\nslim = tf.contrib.slim\ntfexample_decoder = slim.tfexample_decoder\n\n\ndef _create_tf_example_decoder():\n\n    keys_to_features = {\n        fields.TFRecordFields.image_encoded:\n            tf.FixedLenFeature((), tf.string, \'\'),\n        fields.TFRecordFields.image_format:\n            tf.FixedLenFeature((), tf.string, \'\'),\n        fields.TFRecordFields.image_filename:\n            tf.FixedLenFeature((), tf.string, \'\'),\n        fields.TFRecordFields.image_height:\n            tf.FixedLenFeature((), tf.int64, 0),\n        fields.TFRecordFields.image_width:\n            tf.FixedLenFeature((), tf.int64, 0),\n        fields.TFRecordFields.segmentation_class_encoded:\n            tf.FixedLenFeature((), tf.string, \'\'),\n        fields.TFRecordFields.segmentation_class_format:\n            tf.FixedLenFeature((), tf.string, \'\'),\n    }\n\n    # Main GT Input and output tensors for full image segmentation task\n    input_image = tfexample_decoder.Image(\n        image_key=fields.TFRecordFields.image_encoded,\n        format_key=fields.TFRecordFields.image_format,\n        # shape=(1024, 2048, 3),  # TODO: Move this, it\'s CITYSCAPES SPECIFIC\n        channels=3)\n    output_mask = tfexample_decoder.Image(\n        image_key=fields.TFRecordFields.segmentation_class_encoded,\n        format_key=fields.TFRecordFields.segmentation_class_format,\n        # shape=(1024, 2048, 1),  # TODO: Move this, it\'s CITYSCAPES SPECIFIC\n        channels=1)\n\n    items_to_handlers = {\n        fields.GroundtruthFields.input_image_path:\n            tfexample_decoder.Tensor(\'image/filename\'),\n        fields.GroundtruthFields.input_image_height:\n            tfexample_decoder.Tensor(\'image/height\'),\n        fields.GroundtruthFields.input_image_width:\n            tfexample_decoder.Tensor(\'image/width\'),\n        # Masks\n        fields.GroundtruthFields.input_image: input_image,\n        fields.GroundtruthFields.output_mask: output_mask\n    }\n\n    return tfexample_decoder.TFExampleDecoder(\n        keys_to_features, items_to_handlers)\n\n\ndef _process_fn(tf_serialized_example, decoder):\n    serialized_example = tf.reshape(tf_serialized_example, [])\n    # Return Tensordict\n    keys = decoder.list_items()\n    tensors = decoder.decode(serialized_example, items=keys)\n    tensor_dict = dict(zip(keys, tensors))\n    return tensor_dict\n\n\ndef build(input_reader_config):\n    if not isinstance(input_reader_config, input_reader_pb2.InputReader):\n        raise ValueError(\'input_reader_config not of type \'\n                         \'input_reader_pb2.InputReader.\')\n\n    reader_config = input_reader_config.tf_record_input_reader\n    if reader_config is None:\n        raise ValueError(\'input_reader_config must have \'\n                         \'`tf_record_input_reader`.\')\n\n    input_record_patterns = reader_config.input_path\n    input_record_paths = tf.gfile.Glob(input_record_patterns)\n    num_records = len(input_record_paths)\n    if num_records == 0:\n        raise ValueError(\'At least one input path must be specified in \'\n                         \'`input_reader_config`.\')\n\n    files_dataset = tf.data.Dataset.from_tensor_slices(input_record_paths)\n    files_dataset = files_dataset.repeat()\n    dataset = files_dataset.apply(tf.contrib.data.parallel_interleave(\n        tf.data.TFRecordDataset, cycle_length=input_reader_config.num_readers))\n    if input_reader_config.shuffle:\n        dataset = dataset.shuffle(input_reader_config.shuffle_buffer)\n\n    decoder = _create_tf_example_decoder()\n    dataset = dataset.map(\n        functools.partial(_process_fn, decoder=decoder),\n        num_parallel_calls=input_reader_config.num_parallel_calls)\n\n    iterator = dataset.make_initializable_iterator()\n    tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)\n\n    return iterator.get_next()\n'"
builders/hyperparams_builder.py,3,"b'r""""""Build segmentation model hyper parameters.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs import compressible_ops as ops\nfrom protos import hyperparams_pb2\n\nslim = tf.contrib.slim\n\n\ndef _build_regularizer(regularizer):\n    regularizer_oneof = regularizer.WhichOneof(\'regularizer_oneof\')\n    if regularizer_oneof == \'l1_regularizer\':\n        return slim.l1_regularizer(scale=float(\n            regularizer.l1_regularizer.weight))\n    if regularizer_oneof == \'l2_regularizer\':\n        return slim.l2_regularizer(scale=float(\n            regularizer.l2_regularizer.weight))\n    raise ValueError(\'Unknown regularizer function: {}\'.format(\n        regularizer_oneof))\n\n\ndef _build_initializer(initializer):\n    initializer_oneof = initializer.WhichOneof(\'initializer_oneof\')\n    if initializer_oneof == \'truncated_normal_initializer\':\n        return tf.truncated_normal_initializer(\n            mean=initializer.truncated_normal_initializer.mean,\n            stddev=initializer.truncated_normal_initializer.stddev)\n    if initializer_oneof == \'variance_scaling_initializer\':\n        enum_descriptor = (hyperparams_pb2.VarianceScalingInitializer.\n                           DESCRIPTOR.enum_types_by_name[\'Mode\'])\n        mode = enum_descriptor.values_by_number[\n            initializer. variance_scaling_initializer.mode].name\n        return slim.variance_scaling_initializer(\n            factor=initializer.variance_scaling_initializer.factor,\n            mode=mode,\n            uniform=initializer.variance_scaling_initializer.uniform)\n    raise ValueError(\'Unknown initializer function: {}\'.format(\n        initializer_oneof))\n\n\ndef build(hyperparams_config, is_training):\n    batch_norm = None\n    batch_norm_params = None\n    if hyperparams_config.HasField(\'batch_norm\'):\n        batch_norm = hyperparams_config.batch_norm\n        batch_norm_params = {\n            \'decay\': batch_norm.decay,\n            \'center\': batch_norm.center,\n            \'scale\': batch_norm.scale,\n            \'epsilon\': batch_norm.epsilon,\n            \'is_training\': is_training and batch_norm.train}\n    affected_ops = [\n        slim.conv2d,\n        slim.separable_conv2d, slim.conv2d_transpose]\n    with slim.arg_scope(affected_ops,\n                        weights_regularizer=_build_regularizer(\n                            hyperparams_config.regularizer),\n                        weights_initializer=_build_initializer(\n                            hyperparams_config.initializer),\n                        activation_fn=tf.nn.relu6,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n            return sc\n'"
builders/losses_builder.py,8,"b'r""""""Losses builder for supprted training losses for segmentation models.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom functools import partial\n\nimport tensorflow as tf\n\nfrom protos import losses_pb2\n\n\ndef _softmax_classification_loss(predictions, labels, ignore_label):\n    flattened_labels = tf.reshape(labels, shape=[-1])\n    num_classes = predictions.get_shape().as_list()[-1]\n    predictions = tf.reshape(predictions, [-1, num_classes])\n\n    one_hot_target = tf.contrib.slim.one_hot_encoding(\n        tf.cast(flattened_labels, tf.int32),\n        num_classes, on_value=1.0, off_value=0.0)\n    not_ignore_mask = tf.to_float(\n        tf.not_equal(flattened_labels, ignore_label))\n\n    return tf.losses.softmax_cross_entropy(\n        one_hot_target,\n        logits=tf.to_float(predictions),\n        weights=not_ignore_mask)\n\n\ndef build(loss_config):\n    if not isinstance(loss_config, losses_pb2.Loss):\n        raise ValueError(\'loss_config not of type \'\n                         \'losses_pb2.ClassificationLoss.\')\n\n    loss_type = loss_config.classification_loss.WhichOneof(\'loss_type\')\n    if loss_type == \'softmax\':\n        return partial(_softmax_classification_loss,\n            ignore_label=loss_config.ignore_label)\n\n    raise ValueError(\'Empty loss config.\')\n'"
builders/model_builder.py,0,"b'r""""""Builder for segmentation models.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom architectures import icnet_architecture\nfrom architectures import pspnet_architecture\nfrom builders import hyperparams_builder\nfrom builders import losses_builder\nfrom extractors import pspnet_icnet_mobilenet_v2\nfrom extractors import pspnet_icnet_resnet_v1\nfrom protos import model_pb2\n\n\nPSPNET_ICNET_FEATURE_EXTRACTER = {\n    \'dilated_resnet50\':\n        pspnet_icnet_resnet_v1.PSPNetICNetDilatedResnet50FeatureExtractor,\n    \'dilated_mobilenet\':\n        pspnet_icnet_mobilenet_v2.PSPNetICNetMobilenetFeatureExtractor\n}\n\n\ndef _build_pspnet_icnet_extractor(\n        feature_extractor_config, filter_scale, is_training,\n        mid_downsample=False, reuse_weights=None):\n    feature_type = feature_extractor_config.type\n\n    if feature_type not in PSPNET_ICNET_FEATURE_EXTRACTER:\n        raise ValueError(\'Unknown ICNet feature_extractor: {}\'.format(\n            feature_type))\n\n    feature_extractor_class = PSPNET_ICNET_FEATURE_EXTRACTER[\n        feature_type]\n    return feature_extractor_class(is_training,\n                                   batch_norm_trainable=is_training,\n                                   filter_scale=filter_scale,\n                                   mid_downsample=mid_downsample,\n                                   reuse_weights=reuse_weights)\n\n\ndef _build_pspnet_icnet_model(model_config, is_training, add_summaries,\n                              build_baseline_psp=False):\n    num_classes = model_config.num_classes\n    if not num_classes:\n        raise ValueError(\'""num_classes"" must be greater than 0.\')\n\n    pooling_factors = model_config.pooling_factors\n    in_filter_scale = model_config.filter_scale\n    if in_filter_scale > 1 or in_filter_scale < 0:\n        raise ValueError(\'""filter_scale"" must be in the range (0,1].\')\n    filter_scale = 1.0 / in_filter_scale\n\n    should_downsample_extractor = False\n    if not build_baseline_psp:\n        pretrain_single_branch_mode = model_config.pretrain_single_branch_mode\n        should_downsample_extractor = not pretrain_single_branch_mode\n\n    feature_extractor = _build_pspnet_icnet_extractor(\n        model_config.feature_extractor, filter_scale, is_training,\n        mid_downsample=should_downsample_extractor)\n\n    model_arg_scope = hyperparams_builder.build(model_config.hyperparams,\n                                                is_training)\n\n    loss_config = model_config.loss\n    classification_loss = (\n        losses_builder.build(loss_config))\n    use_aux_loss = loss_config.use_auxiliary_loss\n\n    common_kwargs = {\n        \'is_training\': is_training,\n        \'num_classes\': num_classes,\n        \'model_arg_scope\': model_arg_scope,\n        \'feature_extractor\': feature_extractor,\n        \'classification_loss\': classification_loss,\n        \'use_aux_loss\': use_aux_loss,\n        \'upsample_train_logits\': loss_config.upsample_logits,\n        \'add_summaries\': add_summaries\n    }\n\n    if not build_baseline_psp:\n        if use_aux_loss:\n            common_kwargs[\'main_loss_weight\'] = (\n                model_config.main_branch_loss_weight)\n            common_kwargs[\'second_branch_loss_weight\'] = (\n                model_config.second_branch_loss_weight)\n            common_kwargs[\'first_branch_loss_weight\'] = (\n                model_config.first_branch_loss_weight)\n        common_kwargs[\'no_add_n_op\'] = (\n            model_config.mobile_ops_only)\n        model = (num_classes, icnet_architecture.ICNetArchitecture(\n            filter_scale=filter_scale,\n            pooling_factors=pooling_factors,\n            pretrain_single_branch_mode=pretrain_single_branch_mode,\n            **common_kwargs))\n    else:\n        if use_aux_loss:\n            # TODO: remove hardcoded values here\n            common_kwargs[\'main_loss_weight\'] = 1.0\n            common_kwargs[\'aux_loss_weight\'] = 0.4\n        model = (num_classes, pspnet_architecture.PSPNetArchitecture(\n            filter_scale=filter_scale,\n            pooling_factors=pooling_factors,\n            **common_kwargs))\n\n    return model\n\n\ndef build(model_config, is_training, add_summaries=True):\n    if not isinstance(model_config, model_pb2.FastSegmentationModel):\n        raise ValueError(\'model_config not of type \'\n                         \'model_pb2.FastSegmentationModel.\')\n\n    model = model_config.WhichOneof(\'model\')\n    if model == \'pspnet\':\n        return _build_pspnet_icnet_model(\n            model_config.pspnet, is_training, add_summaries,\n            build_baseline_psp=True)\n    elif model == \'icnet\':\n        return _build_pspnet_icnet_model(\n            model_config.icnet, is_training, add_summaries)\n\n    raise ValueError(\'Unknown model: {}\'.format(model))\n'"
builders/optimizer_builder.py,7,"b'r""""""Builder optimizer for training segmentation model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef _create_learning_rate(learning_rate_config):\n    learning_rate = None\n    learning_rate_type = learning_rate_config.WhichOneof(\'learning_rate\')\n\n    # Authors of ENet claim to use constant constant learning rate policy\n    # with a constant value of of 0.0002\n    if learning_rate_type == \'constant_learning_rate\':\n        config = learning_rate_config.constant_learning_rate\n        learning_rate = tf.constant(config.learning_rate, dtype=tf.float32)\n\n    # For ICNet, authors claim to use ""poly"" learning rate policy with a\n    # base learning rate is 0.01 and with power of 0.9 for growth\n    if learning_rate_type == \'polynomial_decay_learning_rate\':\n        config = config = learning_rate_config.polynomial_decay_learning_rate\n        learning_rate = tf.train.polynomial_decay(\n            config.initial_learning_rate,\n            tf.train.get_or_create_global_step(),\n            config.decay_steps,\n            power=config.power,\n            end_learning_rate=0)\n\n    if learning_rate_type == \'exponential_decay_learning_rate\':\n        config = learning_rate_config.exponential_decay_learning_rate\n        learning_rate = tf.train.exponential_decay(\n            config.initial_learning_rate,\n            tf.train.get_or_create_global_step(),\n            config.decay_steps,\n            config.decay_factor,\n            staircase=config.staircase)\n\n    if learning_rate is None:\n        raise ValueError(\'Learning_rate %s not supported.\' % learning_rate_type)\n\n    return learning_rate\n\n\ndef build(optimizer_config):\n    optimizer_type = optimizer_config.WhichOneof(\'optimizer\')\n    optimizer = None\n\n    summary_vars = []\n\n    # Used by authors of ICNet for training. Momentum is set to 0.9.\n    if optimizer_type == \'momentum_optimizer\':\n        config = optimizer_config.momentum_optimizer\n        learning_rate = _create_learning_rate(config.learning_rate)\n        summary_vars.append(learning_rate)\n        optimizer = tf.train.MomentumOptimizer(\n            learning_rate,\n            momentum=config.momentum_optimizer_value)\n\n    # The ADAM optimizer is used by ENet authors\n    if optimizer_type == \'adam_optimizer\':\n        config = optimizer_config.adam_optimizer\n        learning_rate = _create_learning_rate(config.learning_rate)\n        summary_vars.append(learning_rate)\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n\n    if optimizer is None:\n        raise ValueError(\'Optimizer %s not supported.\' % optimizer_type)\n\n    return optimizer, summary_vars\n'"
builders/preprocessor_builder.py,51,"b'r""""""Preprocessing step for inptut images.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport tensorflow as tf\n\nfrom libs import standard_fields as fields\nfrom builders import dataset_builder\nfrom protos import preprocessor_pb2\n\n\nRESIZE_METHOD_MAP = {\n    preprocessor_pb2.BICUBIC: tf.image.ResizeMethod.BICUBIC,\n    preprocessor_pb2.BILINEAR: tf.image.ResizeMethod.BILINEAR,\n    preprocessor_pb2.NEAREST_NEIGHBOR: (\n        tf.image.ResizeMethod.NEAREST_NEIGHBOR),\n}\n\n_RANDOM_SCALE_STEP_KEY = \'RANDOM_SCALE_STEP\'\n\n_IMAGE_CROP_KEY = \'IMAGE_CROP_STEP\'\n\n_IMAGE_SCALE_KEY = \'IMAGE_SCALE_KEY\'\n\n_IMAGE_HORIZONTAL_FLIP_KEY = \'IMAGE_HORIZONTAL_FLIP_STEP\'\n\n_RANDOM_PREPROCESSOR_SEED = 7\n\n\ndef _get_or_create_preprocess_rand_vars(generator_func,\n                                        function_id,\n                                        preprocess_vars_cache):\n    if preprocess_vars_cache is not None:\n        var = preprocess_vars_cache.get(function_id)\n        if var is None:\n            var = generator_func()\n        preprocess_vars_cache.update({function_id: var})\n    else:\n        var = generator_func()\n    return var\n\n\ndef set_fixed_image_size(images,\n                         labels,\n                         height_to_set,\n                         width_to_set,\n                         images_channel_dim=3,\n                         labels_channel_dim=1,\n                         preprocess_vars_cache=None):\n    with tf.name_scope(\'DimensionInput\', values=[images, labels]):\n        fixed_input_tensor_shape = (\n            height_to_set, width_to_set, images_channel_dim)\n        images.set_shape(fixed_input_tensor_shape)\n        fixed_label_tensor_shape = (\n            height_to_set, width_to_set, labels_channel_dim)\n        labels.set_shape(fixed_label_tensor_shape)\n        return images, labels\n\n\ndef pad_to_specific_size(images,\n                         labels,\n                         height_to_set,\n                         width_to_set,\n                         images_channel_dim=3,\n                         labels_channel_dim=1,\n                         preprocess_vars_cache=None):\n    with tf.name_scope(\'PadInput\', values=[images, labels]):\n        fixed_input_tensor_shape = (\n            height_to_set, width_to_set, images_channel_dim)\n        padded_images = tf.image.pad_to_bounding_box(\n            images, 0, 0, height_to_set, width_to_set)\n        padded_images.set_shape(fixed_input_tensor_shape)\n        fixed_label_tensor_shape = (\n            height_to_set, width_to_set, labels_channel_dim)\n        padded_labels = None\n        if labels is not None:\n            padded_labels = tf.image.pad_to_bounding_box(\n                labels, 0, 0, height_to_set, width_to_set)\n            padded_labels.set_shape(fixed_label_tensor_shape)\n        return padded_images, padded_labels\n\n\ndef _compute_new_static_size(image, min_dimension, max_dimension):\n    """"""Compute new static shape for resize_to_range method.""""""\n    image_shape = image.get_shape().as_list()\n    orig_height = image_shape[0]\n    orig_width = image_shape[1]\n    num_channels = image_shape[2]\n    orig_min_dim = min(orig_height, orig_width)\n    # Calculates the larger of the possible sizes\n    large_scale_factor = min_dimension / float(orig_min_dim)\n    # Scaling orig_(height|width) by large_scale_factor will make the smaller\n    # dimension equal to min_dimension, save for floating point rounding errors.\n    # For reasonably-sized images, taking the nearest integer will reliably\n    # eliminate this error.\n    large_height = int(round(orig_height * large_scale_factor))\n    large_width = int(round(orig_width * large_scale_factor))\n    large_size = [large_height, large_width]\n    if max_dimension:\n        # Calculates the smaller of the possible sizes, use that if the larger\n        # is too big.\n        orig_max_dim = max(orig_height, orig_width)\n        small_scale_factor = max_dimension / float(orig_max_dim)\n        # Scaling orig_(height|width) by small_scale_factor will make the larger\n        # dimension equal to max_dimension, save for floating point rounding\n        # errors. For reasonably-sized images, taking the nearest integer will\n        # reliably eliminate this error.\n        small_height = int(round(orig_height * small_scale_factor))\n        small_width = int(round(orig_width * small_scale_factor))\n        small_size = [small_height, small_width]\n        new_size = large_size\n        if max(large_size) > max_dimension:\n            new_size = small_size\n    else:\n        new_size = large_size\n    return tf.constant(new_size + [num_channels])\n\n\ndef resize_to_range(image,\n                    label=None,\n                    min_dimension=None,\n                    max_dimension=None,\n                    align_corners=True):\n    with tf.name_scope(\'ResizeToRange\', values=[image, min_dimension]):\n        if image.get_shape().is_fully_defined():\n            new_size = _compute_new_static_size(\n                image, min_dimension, max_dimension)\n        else:\n            raise ValueError(\'Dynamic shapes and sizes not supported.\'\n                             \'Please specify input height, width.\')\n        new_image = tf.image.resize_bilinear(\n            image, new_size[:-1], align_corners=align_corners)\n        new_label = tf.image.resize_nearest_neighbor(\n            label, new_size[:-1], align_corners=align_corners)\n    return (new_image, new_label)\n\n\ndef random_scale(images,\n                 labels,\n                 min_scale_ratio=0.5,\n                 max_scale_ratio=2.0,\n                 pad_to_dims=None,\n                 seed=_RANDOM_PREPROCESSOR_SEED,\n                 preprocess_vars_cache=None):\n    with tf.name_scope(\'RandomScale\', values=[images, labels]):\n        image_shape = tf.shape(images)\n        image_height = image_shape[0]\n        image_width = image_shape[1]\n\n        generator_func = functools.partial(\n            tf.random_uniform, [],\n            minval=min_scale_ratio, maxval=max_scale_ratio,\n            dtype=tf.float32, seed=seed)\n        size_coef = _get_or_create_preprocess_rand_vars(\n            generator_func, _IMAGE_SCALE_KEY,\n            preprocess_vars_cache)\n\n        image_newysize = tf.to_int32(\n            tf.multiply(tf.to_float(image_height), size_coef))\n        image_newxsize = tf.to_int32(\n            tf.multiply(tf.to_float(image_width), size_coef))\n        new_shape = (image_newysize, image_newxsize)\n\n        # Must be 4D tensor for resize ops\n        images = tf.expand_dims(images, 0)\n        labels = tf.expand_dims(labels, 0)\n        scaled_images = tf.image.resize_bilinear(\n            images, new_shape, align_corners=True)\n        scaled_labels = tf.image.resize_nearest_neighbor(\n            labels, new_shape, align_corners=True)\n        if pad_to_dims is not None:\n            crop_height, crop_width = pad_to_dims\n            target_height = (image_newysize +\n                tf.maximum(crop_height - image_newysize, 0))\n            target_width = (image_newxsize +\n                tf.maximum(crop_width - image_newxsize, 0))\n            scaled_images = tf.image.pad_to_bounding_box(\n                scaled_images, 0, 0, target_height, target_width)\n            scaled_labels = tf.image.pad_to_bounding_box(\n                scaled_labels, 0, 0, target_height, target_width)\n        output_images = tf.squeeze(scaled_images, [0])\n        output_labels = tf.squeeze(scaled_labels, [0])\n        return output_images, output_labels\n\n\ndef random_crop(images, labels,\n                crop_height, crop_width,\n                images_channel_dim=3,\n                labels_channel_dim=1,\n                preprocess_vars_cache=None):\n\n    def _apply_random_crop(inputs, offsets, crop_shape):\n        sliced_inputs = tf.slice(inputs, offsets, crop_shape)\n        out_inputs = tf.reshape(sliced_inputs, crop_shape)\n        return out_inputs\n\n    with tf.name_scope(\'RandomCropImage\', values=[images, labels]):\n        images_shape = tf.shape(images)\n        images_height = images_shape[0]\n        images_width = images_shape[1]\n\n        max_offset_height = tf.reshape(images_height - crop_height + 1, [])\n        max_offset_width = tf.reshape(images_width - crop_width + 1, [])\n\n        generator_func_height = functools.partial(\n            tf.random_uniform,\n            shape=[], maxval=max_offset_height, dtype=tf.int32)\n        generator_func_width = functools.partial(\n            tf.random_uniform,\n            shape=[], maxval=max_offset_width, dtype=tf.int32)\n\n        offset_height = _get_or_create_preprocess_rand_vars(\n            generator_func_height,\n            _IMAGE_CROP_KEY + \'_0\',\n            preprocess_vars_cache)\n        offset_width = _get_or_create_preprocess_rand_vars(\n            generator_func_width,\n            _IMAGE_CROP_KEY + \'_1\',\n            preprocess_vars_cache)\n\n        offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n        crop_shape_images = tf.stack(\n            [crop_height, crop_width, images_channel_dim])\n        crop_shape_labels = tf.stack(\n            [crop_height, crop_width, labels_channel_dim])\n\n        cropped_images = _apply_random_crop(images, offsets, crop_shape_images)\n        cropped_labels = _apply_random_crop(labels, offsets, crop_shape_labels)\n\n        # Must set shape here or in the set shape preprocessor step\n        # when dealing with ICNet\n        if images_channel_dim and labels_channel_dim:\n            cropped_images.set_shape((\n                crop_height, crop_width, images_channel_dim))\n            cropped_labels.set_shape((\n                crop_height, crop_width, labels_channel_dim))\n\n        return cropped_images, cropped_labels\n\n\ndef random_horizontal_flip(images,\n                           labels,\n                           seed=_RANDOM_PREPROCESSOR_SEED,\n                           preprocess_vars_cache=None):\n\n    def _flip_image(item):\n        flipped_item = tf.image.flip_left_right(item)\n        return flipped_item\n\n    with tf.name_scope(\'RandomHorizontalFlip\', values=[images, labels]):\n        generator_func = functools.partial(\n            tf.random_uniform, [], seed=seed)\n        do_a_flip_random = _get_or_create_preprocess_rand_vars(\n            generator_func, _IMAGE_HORIZONTAL_FLIP_KEY,\n            preprocess_vars_cache)\n        do_a_flip_random = tf.greater(do_a_flip_random, 0.5)\n\n        flipped_images = tf.cond(do_a_flip_random,\n                                 lambda: _flip_image(images),\n                                 lambda: images)\n        flipped_labels = tf.cond(do_a_flip_random,\n                                 lambda: _flip_image(labels),\n                                 lambda: labels)\n        return flipped_images, flipped_labels\n\n\ndef preprocess_runner(tensor_dict, func_list,\n                      skip_labels=False,\n                      preprocess_vars_cache=None):\n    if fields.GroundtruthFields.input_image not in tensor_dict \\\n       or fields.GroundtruthFields.output_mask not in tensor_dict:\n        raise ValueError(\'""tensor_dict"" must have both image\'\n                         \'and label fields\')\n    for item_key in [fields.GroundtruthFields.input_image,\n                     fields.GroundtruthFields.output_mask]:\n        items = tensor_dict[item_key]\n        if len(items.get_shape()) != 3:\n            raise ValueError(\'Images or Labels in tensor_dict should be rank 4\')\n        tensor_dict[item_key] = items\n\n    if preprocess_vars_cache is None:\n        preprocess_vars_cache = {}\n\n    images = tf.to_float(tensor_dict[fields.GroundtruthFields.input_image])\n    images_shape = tf.shape(images)\n    # For now, we skip labels preprocessing for eval only, since we\n    # do whole image evaluation\n    # TODO: Fix this so it doesn\'t break for training\n    labels = None\n    if not skip_labels:\n        labels = tf.to_float(tensor_dict[fields.GroundtruthFields.output_mask])\n\n    # Apple proprocessor functions\n    for preprocessor_step_func in func_list:\n        images, labels = preprocessor_step_func(images=images, labels=labels,\n                        preprocess_vars_cache=preprocess_vars_cache)\n\n    output_dict = {}\n    output_dict[fields.GroundtruthFields.input_image] = images\n    output_dict[fields.GroundtruthFields.output_mask] = labels\n    return output_dict\n\n\ndef build(preprocessor_config_list):\n    proprocessor_func_list = []\n\n    for preprocessor_step_config in preprocessor_config_list:\n        step_type = preprocessor_step_config.WhichOneof(\'preprocessing_step\')\n\n        # Fixed image width and height for PSP module\n        if step_type == \'set_fixed_image_size\':\n            config = preprocessor_step_config.set_fixed_image_size\n            dimension_image_fn = functools.partial(\n                set_fixed_image_size,\n                height_to_set=config.fixed_height,\n                width_to_set=confi.fixed_width,\n                images_channel_dim=config.images_channel_dim,\n                labels_channel_dim=config.labels_channel_dim)\n            proprocessor_func_list.append(dimension_image_fn)\n\n        # Resize the image and keep the aspect_ratio\n        if step_type == \'aspect_ratio_image_resize\':\n            config = preprocessor_step_config.aspect_ratio_image_resize\n            if not (config.min_dimension <= config.max_dimension):\n                raise ValueError(\'min_dimension > max_dimension\')\n            method = RESIZE_METHOD_MAP[config.resize_method]\n            image_resizer_fn = functools.partial(\n                resize_to_range,\n                min_dimension=config.min_dimension,\n                max_dimension=config.max_dimension,\n                pad_to_max_dimension=config.pad_to_max_dimension)\n            proprocessor_func_list.append(image_resizer_fn)\n\n        # Randomly Scale the image\n        if step_type == \'random_image_scale\':\n            config = preprocessor_step_config.random_image_scale\n            if not (config.max_scale_ratio >= config.min_scale_ratio):\n                raise ValueError(\'min_scale_ratio > max_scale_ratio\')\n\n            pad_to_dims = None\n            for cfg in preprocessor_config_list:\n                step_t = cfg.WhichOneof(\'preprocessing_step\')\n                if step_t == \'random_image_crop\':\n                    dim = cfg.random_image_crop\n                    pad_to_dims = (dim.crop_height, dim.crop_width)\n\n            image_scale_fn = functools.partial(\n                random_scale,\n                pad_to_dims=pad_to_dims,\n                min_scale_ratio=config.min_scale_ratio,\n                max_scale_ratio=config.max_scale_ratio)\n            proprocessor_func_list.append(image_scale_fn)\n\n        # Randomly crop the image\n        if step_type == \'random_image_crop\':\n            config = preprocessor_step_config.random_image_crop\n            image_crop_fn = functools.partial(\n                random_crop,\n                crop_height=config.crop_height,\n                crop_width=config.crop_width,\n                images_channel_dim=config.images_channel_dim,\n                labels_channel_dim=config.labels_channel_dim)\n            proprocessor_func_list.append(image_crop_fn)\n\n        # Random Flips and Rotations\n        if step_type == \'random_horizontal_flip\':\n            config = preprocessor_step_config.random_horizontal_flip\n            image_horizontal_flip_fn = functools.partial(\n                random_horizontal_flip)\n            proprocessor_func_list.append(image_horizontal_flip_fn)\n\n        if len(proprocessor_func_list) <= 0 and \\\n                len(preprocessor_config_list) > 0:\n            raise ValueError(\'Unknown preprocessing step.\')\n\n    preprocessor = functools.partial(\n        preprocess_runner,\n        func_list=proprocessor_func_list)\n\n    return preprocessor\n'"
dataset_tools/create_cityscapes_tfrecord.py,19,"b'r""""""Build a TF Record for Cityscapes Semantic Segmentation dataset.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\nimport glob\nimport io\nimport json\nimport os\nimport numpy as np\nimport PIL.Image\n\nimport tensorflow as tf\n\nflags = tf.app.flags\ntf.flags.DEFINE_string(\'input_pattern\', \'\',\n                       \'Cityscapes dataset root folder.\')\ntf.flags.DEFINE_string(\'annot_pattern\', \'\',\n                       \'Pattern matching input images for Cityscapes.\')\ntf.flags.DEFINE_string(\'cityscapes_dir\', \'\',\n                       \'Pattern matching ground truth images for Cityscapes.\')\ntf.flags.DEFINE_string(\'split_type\', \'\',\n                       \'Type of split: `train`, `test` or `val`.\')\ntf.flags.DEFINE_string(\'output_dir\', \'\', \'Output data directory.\')\n\nFLAGS = flags.FLAGS\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\n_DEFAULT_PATTEN = {\n    \'input\': \'*_leftImg8bit.png\',\n    \'annot\': \'*_gtFine_labelTrainIds.png\'\n}\n\n_DEFAULT_DIR = {\n    \'image\': \'leftImg8bit\',\n    \'label\': \'gtFine\'\n}\n\n\ndef _bytes_feature(values):\n    return tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef _int64_feature(values):\n    if not isinstance(values, (tuple, list)):\n        values = [values]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef _open_file(full_path):\n    with tf.gfile.GFile(full_path, \'rb\') as fid:\n        encoded_file = fid.read()\n    encoded_file_io = io.BytesIO(encoded_file)\n    image = PIL.Image.open(encoded_file_io)\n    return image, encoded_file\n\n\ndef create_tf_example(image_path, label_path, image_dir=\'\', is_jpeg=False):\n    file_format = \'jpeg\' if is_jpeg else \'png\'\n    full_image_path = os.path.join(image_dir, image_path)\n    full_label_path = os.path.join(image_dir, label_path)\n    image, encoded_image = _open_file(full_image_path)\n    label, encoded_label = _open_file(full_label_path)\n\n    height = image.height\n    width = image.width\n    if height != label.height or width != label.width:\n        raise ValueError(\'Input and annotated images must have same dims.\'\n                        \'verify the matching pair for {}\'.format(full_image_path))\n\n    feature_dict = {\n        \'image/encoded\': _bytes_feature(encoded_image),\n        \'image/filename\': _bytes_feature(\n                full_image_path.encode(\'utf8\')),\n        \'image/format\': _bytes_feature(\n                file_format.encode(\'utf8\')),\n        \'image/height\': _int64_feature(height),\n        \'image/width\': _int64_feature(width),\n        \'image/channels\': _int64_feature(3),\n        \'image/segmentation/class/encoded\': _bytes_feature(encoded_label),\n        \'image/segmentation/class/format\':_bytes_feature(\n                \'png\'.encode(\'utf8\')),\n    }\n\n    example = tf.train.Example(\n        features=tf.train.Features(feature=feature_dict))\n    return example\n\n\ndef _create_tf_record(images, labels, output_path):\n    writer = tf.python_io.TFRecordWriter(output_path)\n    for idx, image in enumerate(images):\n        if idx % 100 == 0:\n            tf.logging.info(\'On image %d of %d\', idx, len(images))\n        tf_example = create_tf_example(\n            image, labels[idx], is_jpeg=False)\n        writer.write(tf_example.SerializeToString())\n    writer.close()\n    tf.logging.info(\'Finished writing!\')\n\n\ndef main(_):\n    assert FLAGS.output_dir, \'`output_dir` missing.\'\n    assert FLAGS.split_type, \'`split_type` missing.\'\n    assert (FLAGS.cityscapes_dir) or \\\n           (FLAGS.input_pattern and FLAGS.annot_pattern), \\\n           \'Must specify either `cityscapes_dir` or \' \\\n           \'`input_pattern` and `annot_pattern`.\'\n\n    if not tf.gfile.IsDirectory(FLAGS.output_dir):\n        tf.gfile.MakeDirs(FLAGS.output_dir)\n    train_output_path = os.path.join(FLAGS.output_dir,\n        \'cityscapes_{}.record\'.format(FLAGS.split_type))\n\n    if FLAGS.cityscapes_dir:\n        search_image_files = os.path.join(FLAGS.cityscapes_dir,\n            _DEFAULT_DIR[\'image\'], FLAGS.split_type, \'*\', _DEFAULT_PATTEN[\'input\'])\n        search_annot_files = os.path.join(FLAGS.cityscapes_dir,\n            _DEFAULT_DIR[\'label\'], FLAGS.split_type, \'*\', _DEFAULT_PATTEN[\'annot\'])\n        image_filenames = glob.glob(search_image_files)\n        annot_filenames = glob.glob(search_annot_files)\n    else:\n        image_filenames = glob.glob(FLAGS.input_pattern)\n        annot_filenames = glob.glob(FLAGS.annot_pattern)\n        if len(image_filenames) != len(annot_filenames):\n            raise ValueError(\'Supplied patterns do not have image counts.\')\n\n    _create_tf_record(\n            sorted(image_filenames),\n            sorted(annot_filenames),\n            output_path=train_output_path)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
extractors/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'
extractors/pspnet_icnet_mobilenet_v2.py,1,"b'r""""""MobileNet V2 feature extracter interface implementation.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom third_party import mobilenet_v2\n\nfrom architectures import pspnet_architecture\n\n\nslim = tf.contrib.slim\n\n\nclass PSPNetICNetMobilenetV2FeatureExtractor(\n        pspnet_architecture.PSPNetFeatureExtractor):\n    """"""PSPNet/ICNet feature extractor implementation.""""""\n\n    _channel_means = [127.5, 127.5, 127.5]\n\n    def __init__(self,\n                 architecture,\n                 mobilenet_model,\n                 is_training,\n                 filter_scale,\n                 features_stride,\n                 mid_downsample=False,\n                 batch_norm_trainable=False,\n                 reuse_weights=None,\n                 weight_decay=0.0):\n        if features_stride != 8:\n            raise ValueError(\'`features_stride` must be 8 \'\n                             \'for ICNet and PSPNet.\')\n        self._filter_scale = filter_scale\n        self._architecture = architecture\n        self._mobilenet_model = mobilenet_model\n        self._mid_downsample = mid_downsample\n        # Mobilenet specific options\n        self._depth_multiplier = 1.0\n        super(PSPNetICNetMobilenetV2FeatureExtractor, self).__init__(\n            is_training, features_stride, batch_norm_trainable,\n            reuse_weights, weight_decay)\n\n    def preprocess(self, raw_inputs):\n        channel_means = self._channel_means  # We normalize between [-1, 1]\n        return (raw_inputs - [[channel_means]]) / [[channel_means]]\n\n    def _extract_features(self, preprocessed_inputs, scope):\n        half_res_scope = \'layer_5\'  # expanded_conv_3\n        quarter_res_scope = \'layer_18\'  # expanded_conv_16\n        psp_aux_scope = \'layer_8\'  # expanded_conv_6\n\n        conv_defs = mobilenet_v2.make_conv_defs(\n            filter_scale=self._filter_scale,\n            mid_downsample=self._mid_downsample)\n        with slim.arg_scope(\n            mobilenet_v2.training_scope(\n                is_training=self._is_training,\n                weight_decay=self._weight_decay)):\n            _, activations = mobilenet_v2.mobilenet_base(\n                preprocessed_inputs,\n                conv_defs=conv_defs,\n                depth_multiplier=self._depth_multiplier,\n                min_depth=(8 if self._depth_multiplier == 1.0 else 1),\n                divisible_by=(8 if self._depth_multiplier == 1.0 else 1),\n                output_stride=self._features_stride,\n                final_endpoint=\'layer_18\')\n\n            half_res_features = activations[half_res_scope]\n            quarter_res_features = activations[quarter_res_scope]\n            psp_aux_features = activations[psp_aux_scope]\n            return half_res_features, quarter_res_features, psp_aux_features\n\n\nclass PSPNetICNetMobilenetFeatureExtractor(\n        PSPNetICNetMobilenetV2FeatureExtractor):\n    """"""ICNet Dilated Resnet 50 feature extractor implementation.\n\n    The implementation with dilations contains dilated convolutions in the last\n    two blocks of the network. This is how the resnet backbone is\n    implemented in the original ICNet paper.\n    """"""\n\n    def __init__(self,\n                 is_training,\n                 filter_scale=1.0,\n                 mid_downsample=False,\n                 features_stride=8,\n                 batch_norm_trainable=False,\n                 reuse_weights=None,\n                 weight_decay=0.0):\n        super(PSPNetICNetMobilenetFeatureExtractor, self).__init__(\n            \'MobilenetV2\', mobilenet_v2.mobilenet_base, is_training,\n            filter_scale, features_stride, mid_downsample, batch_norm_trainable,\n            reuse_weights, weight_decay)\n'"
extractors/pspnet_icnet_resnet_v1.py,1,"b'r""""""ResNet V1 feature extracter interface implementation.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.nets import resnet_utils\nfrom third_party import dilated_resnet_v1\n\nfrom architectures import pspnet_architecture\n\n\nslim = tf.contrib.slim\n\n\nclass PSPNetICNetResnetV1FeatureExtractor(\n        pspnet_architecture.PSPNetFeatureExtractor):\n    """"""ICNet feature extractor implementation.""""""\n\n    _channel_means = [123.68, 116.779, 103.939]\n\n    def __init__(self,\n                 architecture,\n                 resnet_model,\n                 is_training,\n                 filter_scale,\n                 features_stride,\n                 mid_downsample=False,\n                 batch_norm_trainable=False,\n                 reuse_weights=None,\n                 weight_decay=0.0):\n        if features_stride != 8:\n            raise ValueError(\'`features_stride` must be 8 \'\n                             \'for ICNet and PSPNet.\')\n        self._filter_scale = filter_scale\n        self._architecture = architecture\n        self._resnet_model = resnet_model\n        self._mid_downsample = mid_downsample\n        super(PSPNetICNetResnetV1FeatureExtractor, self).__init__(\n            is_training, features_stride, batch_norm_trainable,\n            reuse_weights, weight_decay)\n\n    def preprocess(self, raw_inputs):\n        channel_means = self._channel_means  # RGB VGG ImageNet mean\n        return raw_inputs - [[channel_means]]\n\n    def _extract_features(self, preprocessed_inputs, scope):\n        half_res_scope = scope + \'/%s/block1\' % self._architecture\n        quarter_res_scope = scope + \'/%s/block4\' % self._architecture\n        psp_aux_scope = scope + \'/%s/block2\' % self._architecture\n\n        with slim.arg_scope(\n            resnet_utils.resnet_arg_scope(\n                batch_norm_epsilon=1e-5,\n                batch_norm_scale=True,\n                weight_decay=self._weight_decay)):\n            _, activations = self._resnet_model(\n                preprocessed_inputs,\n                filter_scale=self._filter_scale,\n                mid_downsample=self._mid_downsample,\n                num_classes=None,\n                is_training=(self._is_training and self._train_batch_norm),\n                global_pool=False,\n                output_stride=self._features_stride)\n            half_res_features = activations[half_res_scope]\n            quarter_res_features = activations[quarter_res_scope]\n            psp_aux_features = activations[psp_aux_scope]\n            return half_res_features, quarter_res_features, psp_aux_features\n\n\nclass PSPNetICNetDilatedResnet50FeatureExtractor(\n        PSPNetICNetResnetV1FeatureExtractor):\n    """"""ICNet Dilated Resnet 50 feature extractor implementation.\n\n    The implementation with dilations contains dilated convolutions in the last\n    two blocks of the network. This is how the resnet backbone is\n    implemented in the original ICNet paper.\n    """"""\n\n    def __init__(self,\n                 is_training,\n                 filter_scale=1.0,\n                 mid_downsample=False,\n                 features_stride=8,\n                 batch_norm_trainable=False,\n                 reuse_weights=None,\n                 weight_decay=0.0):\n        super(PSPNetICNetDilatedResnet50FeatureExtractor, self).__init__(\n            \'resnet_v1_50\', dilated_resnet_v1.dilated_resnet_v1_50, is_training,\n            filter_scale, features_stride, mid_downsample, batch_norm_trainable,\n            reuse_weights, weight_decay)\n'"
libs/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'
libs/base_model.py,2,"b'r""""""Abstract semantic segmentation model.\n\nDefines a base class to be used by groups of semantic segmentation models in\nthe project. Any supporting scripts such as trainers, evaluators and\nexporters should only call the methods defined in the abstract class.\n\nThe general order of using the models implementing this abstract class can be\nas followed for both training and evaluation\n\nTraining flow:\ninputs -> preprocess -> predict -> loss -> outputs\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim  # pylint: disable=C0103,E1101\n\n\nclass FastSegmentationModel(object):\n    """"""Abstract base class for semantic segmentation models.""""""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, num_classes):\n        self._num_classes = num_classes\n        self._groundtruth_labels = {}\n\n    @property\n    def shared_feature_extractor_scope(self):\n        """"""Scope for feature extracter.""""""\n        return \'SharedFeatureExtractor\'\n\n    @property\n    def num_classes(self):\n        """"""Training classes for the model.""""""\n        return self._num_classes\n\n    @abstractmethod\n    def preprocess(self, inputs):\n        """"""Proprocessing call for input images.\n\n        This method should be used for any preprocessing to be done before\n        running a prediction. Primarily would be used for image resizing\n        and zero centering if a model requires it.\n\n        Args:\n          inputs: a [batch, height_in, width_in, channels] float32 tensor\n            representing a batch of images with values between 0 and 255.0.\n\n        Returns:\n          preprocessed_inputs: a [batch, height_out, width_out, channels]\n            float32 tensor representing a batch of images.\n        """"""\n        pass\n\n    @abstractmethod\n    def predict(self, preprocessed_inputs):\n        """"""Run model inference on a set of input images.\n\n        When training, the output should be passed to the loss method\n\n        Args:\n          preprocessed_inputs: a [batch, height, width, channels] float32\n            tensor representing a batch of images.\n\n        Returns:\n          prediction_dict: a dictionary holding prediction tensors\n        """"""\n        pass\n\n    @abstractmethod\n    def loss(self, prediction_dict):\n        """"""Computes a loss tensor to be used for training.\n\n        Note that the class must have been supplied the groundtruth tensors\n        first with the provide groundtruth method.\n\n        Args:\n          prediction_dict: a dictionary holding predicted tensors.\n\n        Returns:\n          a dictionary mapping loss names to scalar tensors representing\n            loss values.\n        """"""\n        pass\n\n    def provide_groundtruth(self, groundtruth_labels):\n        """"""Provide groundtruth tensors.\n\n        Args:\n          groundtruth_masks: a list of 3-D tf.float32 tensors of\n            shape [num_classes, height_in, width_in] containing a overall\n            segmentation mask.  If None, no masks are provided.\n        """"""\n        self._groundtruth_labels = groundtruth_labels\n'"
libs/compressible_ops.py,1,"b'r""""""Ops compatable with filter pruner.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef convolution2d_compressible(inputs,\n                               num_outputs,\n                               kernel_size,\n                               stride=1,\n                               compression_ratio=1.0,  # The additional arg\n                               prediction_output=False,\n                               **kwargs):\n    if prediction_output:\n        kwargs[\'activation_fn\'] = None\n        kwargs[\'normalizer_fn\'] = None\n    num_filter_with_compression = num_outputs // compression_ratio\n    return tf.contrib.slim.conv2d(inputs,\n                       num_filter_with_compression,\n                       kernel_size,\n                       stride,\n                       **kwargs)\n\n\n# Export aliases.\ncompressible_conv2d = convolution2d_compressible  # pylint: disable=C0103\nconv2d_compressible = convolution2d_compressible  # pylint: disable=C0103,E0303\nconv2d = convolution2d_compressible  # pylint: disable=C0103,E0303\n'"
libs/constants.py,0,"b'r""""""Cityscapes train IDs to label IDs or label colors.""""""\n\n# pylint: skip-file\n# flake8: noqa\n\nCITYSCAPES_LABEL_COLORS = [\n    (128,  64, 128),        # road\n    (244,  35, 231),        # sidewalk\n    ( 69,  69,  69),        # building\n    (102, 102, 156),        # wall\n    (190, 153, 153),        # fence\n    (153, 153, 153),        # pole\n    (250, 170,  29),        # traffic light\n    (219, 219,   0),        # traffic sign\n    (106, 142,   3),        # vegetation\n    (152, 250, 152),        # terrain\n    ( 69, 129, 180),        # sky\n    (219,  19,  60),        # person\n    (255,   0,   0),        # rider\n    (  0,   0, 142),        # car\n    (  0,   0,  69),        # truck\n    (  0,  60, 100),        # bus\n    (  0,  79, 100),        # train\n    (  0,   0, 230),        # motocycle\n    (119,  10,   3)]        # bicycle\n\nCITYSCAPES_LABEL_IDS = [\n    (7,     ),              # 0\n    (8,     ),              # 1\n    (11,    ),              # 2\n    (12,    ),              # 3\n    (13,    ),              # 4\n    (17,    ),              # 5\n    (19,    ),              # 6\n    (20,    ),              # 7\n    (21,    ),              # 8\n    (22,    ),              # 9\n    (23,    ),              # 10\n    (24,    ),              # 11\n    (25,    ),              # 12\n    (26,    ),              # 13\n    (27,    ),              # 14\n    (28,    ),              # 15\n    (31,    ),              # 16\n    (32,    ),              # 17\n    (33,    ),              # 18\n]\n'"
libs/evaluator.py,34,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nimport six\n\nimport tensorflow as tf\n\nfrom libs import standard_fields as fields\nfrom builders import dataset_builder\nfrom builders import preprocessor_builder as preprocessor\n\n\nslim = tf.contrib.slim\n\nprefetch_queue = slim.prefetch_queue\n\n\ndef create_evaluation_input(create_input_dict_fn,\n                            input_height,\n                            input_width,\n                            cropped_eval=False):\n    input_dict = create_input_dict_fn()\n    if cropped_eval:\n        # We evaluate on a random cropped of the validation set.\n        cropper_fn = functools.partial(\n            preprocessor.random_crop,\n            crop_height=input_height,\n            crop_width=input_width)\n        output_dict = preprocessor.preprocess_runner(\n            input_dict, func_list=[cropper_fn])\n    else:\n        # Here we only pad input image.\n        padding_fn = functools.partial(\n            preprocessor.pad_to_specific_size,\n            height_to_set=input_height,\n            width_to_set=input_width)\n        output_dict = preprocessor.preprocess_runner(\n            input_dict, func_list=[padding_fn])\n    processed_labels = tf.to_float(\n        output_dict[fields.GroundtruthFields.output_mask])\n    processed_images = tf.to_float(\n        output_dict[fields.GroundtruthFields.input_image])\n    return processed_images, processed_labels\n\n\ndef create_predictions_and_labels(model, create_input_dict_fn,\n                                  input_height, input_width, cropped_eval,\n                                  eval_dir=None):\n    eval_input_pair = create_evaluation_input(\n        create_input_dict_fn, input_height, input_width, cropped_eval)\n    # Setup a queue for feeding to slim evaluation helpers\n    input_queue = prefetch_queue.prefetch_queue(eval_input_pair)\n    eval_images, eval_labels = input_queue.dequeue()\n    eval_labels = tf.expand_dims(eval_labels, 0)\n    eval_images = tf.expand_dims(eval_images, 0)\n    # Main predictions\n    mean_subtracted_inputs = model.preprocess(eval_images)\n    model.provide_groundtruth(eval_labels)\n    output_dict = model.predict(mean_subtracted_inputs)\n\n    # Awkward fix from preprocessing step - we resize back down to label shape\n    if not cropped_eval:\n        eval_labels_shape = eval_labels.get_shape().as_list()\n        padded_predictions = output_dict[model.main_class_predictions_key]\n        padded_predictions = tf.image.resize_bilinear(\n            padded_predictions,\n            size=eval_labels_shape[1:3],\n            align_corners=True)\n        output_dict[model.main_class_predictions_key] = padded_predictions\n\n    # Output graph def for pruning\n    if eval_dir is not None:\n        graph_def = tf.get_default_graph().as_graph_def()\n        pred_graph_def_path = os.path.join(eval_dir, ""eval_graph.pbtxt"")\n        f = tf.gfile.FastGFile(pred_graph_def_path, ""w"")\n        f.write(str(graph_def))\n    # Validation loss to fight overfitting\n    validation_losses = model.loss(output_dict)\n    eval_total_loss = sum(validation_losses.values())\n    # Argmax final outputs to feed to a metric function\n    model_scores = output_dict[model.main_class_predictions_key]\n    eval_predictions = tf.argmax(model_scores, 3)\n    eval_predictions = tf.expand_dims(eval_predictions, -1)\n\n    return eval_predictions, eval_labels, eval_images, eval_total_loss\n\n\ndef eval_segmentation_model_once(checkpoint_path,\n                                 create_model_fn,\n                                 create_input_fn,\n                                 input_dimensions,\n                                 eval_config,\n                                 eval_dir,\n                                 cropped_evaluation=False,\n                                 image_summaries=False,\n                                 verbose=False,\n                                 sess_config=None):\n    return eval_segmentation_model(\n        create_model_fn,\n        create_input_fn,\n        input_dimensions,\n        eval_config,\n        train_dir=None,\n        eval_dir=eval_dir,\n        cropped_evaluation=cropped_evaluation,\n        evaluate_single_checkpoint=checkpoint_path,\n        image_summaries=image_summaries,\n        verbose=verbose,\n        sess_config=sess_config)\n\n\ndef eval_segmentation_model(create_model_fn,\n                            create_input_fn,\n                            input_dimensions,\n                            eval_config,\n                            train_dir,\n                            eval_dir,\n                            cropped_evaluation=False,\n                            evaluate_single_checkpoint=None,\n                            image_summaries=False,\n                            verbose=False,\n                            sess_config=None):\n    ignore_label = eval_config.ignore_label\n    num_classes, segmentation_model = create_model_fn()\n\n    input_height, input_width = input_dimensions\n    (predictions_for_eval, labels_for_eval,\n     inputs_summary, validation_loss_summary) = create_predictions_and_labels(\n         model=segmentation_model,\n         create_input_dict_fn=create_input_fn,\n         input_height=input_height,\n         input_width=input_width,\n         cropped_eval=cropped_evaluation,\n         eval_dir=eval_dir)\n    variables_to_restore = tf.global_variables()\n    global_step = tf.train.get_or_create_global_step()\n    variables_to_restore.append(global_step)\n\n    # Prepare inputs to metric calculation steps\n    flattened_predictions = tf.reshape(predictions_for_eval, shape=[-1])\n    flattened_labels = tf.reshape(labels_for_eval, shape=[-1])\n    validity_mask = tf.equal(flattened_labels, ignore_label)\n    neg_validity_mask = tf.not_equal(flattened_labels, ignore_label)\n    eval_labels = tf.where(validity_mask, tf.zeros_like(\n        flattened_labels), flattened_labels)\n    # Calculate metrics from predictions\n    metric_map = {}\n    predictions_tag = \'EvalMetrics/mIoU - \'\n    value_op, update_op = tf.contrib.metrics.streaming_mean_iou(\n        flattened_predictions, eval_labels, num_classes,\n        weights=tf.to_float(neg_validity_mask))\n    # Print updates if verbosity is requested\n    if verbose:\n        update_op = tf.Print(update_op, [value_op], predictions_tag)\n    # TODO: Extend the metrics tuple if needed in the future\n    metric_map[predictions_tag] = (value_op, update_op)\n    metrics_to_values, metrics_to_updates = (\n        tf.contrib.metrics.aggregate_metric_map(metric_map))\n    for metric_name, metric_value in six.iteritems(metrics_to_values):\n        tf.summary.scalar(metric_name, metric_value)\n    eval_op = list(metrics_to_updates.values())\n\n    # Summaries for Tensorboard\n    if validation_loss_summary is not None:\n        tf.summary.scalar(""Losses/EvalValidationLoss"", validation_loss_summary)\n    # Image summaries if requested\n    if image_summaries:\n        pixel_scaling = max(1, 255 // num_classes)\n        tf.summary.image(\n            \'InputImage\', inputs_summary, family=\'EvalImages\')\n        groundtruth_viz = tf.cast(\n            labels_for_eval * pixel_scaling, tf.uint8)\n        tf.summary.image(\n            \'GroundtruthImage\', groundtruth_viz, family=\'EvalImages\')\n        predictions_viz = tf.cast(\n            predictions_for_eval * pixel_scaling, tf.uint8)\n        tf.summary.image(\n            \'PredictionImage\', predictions_viz, family=\'EvalImages\')\n    summary_op = tf.summary.merge_all()\n\n    tf.logging.info(\'Evaluating over %d samples...\',\n                    eval_config.num_examples)\n\n    total_eval_examples = eval_config.num_examples\n    if evaluate_single_checkpoint:\n        curr_checkpoint = evaluate_single_checkpoint\n        metric_results = slim.evaluation.evaluate_once(\n            master=\'\',\n            checkpoint_path=curr_checkpoint,\n            logdir=eval_dir,\n            num_evals=total_eval_examples,\n            eval_op=eval_op,\n            final_op=value_op,\n            summary_op=summary_op,\n            variables_to_restore=variables_to_restore,\n            session_config=sess_config)\n        tf.logging.info(\'Evaluation of `{}` over. Eval values: {}\'.format(\n            curr_checkpoint, metric_results))\n    else:\n        metric_results = slim.evaluation.evaluation_loop(\n            master=\'\',\n            checkpoint_dir=train_dir,\n            logdir=eval_dir,\n            num_evals=total_eval_examples,\n            eval_op=eval_op,\n            final_op=value_op,\n            summary_op=summary_op,\n            variables_to_restore=variables_to_restore,\n            session_config=sess_config)\n        tf.logging.info(\'Evaluation over. Eval values: {}\'.format(\n            metric_results))\n\n    return metric_results\n'"
libs/exporter.py,17,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom builders import preprocessor_builder as preprocessor\nfrom builders import dataset_builder\n\n\nslim = tf.contrib.slim\n\n\ndef _map_to_colored_labels(segmentation_map, shape_list, color_map):\n    # resolve shapes\n    num_classes = len(color_map)\n    output_channels = len(color_map[0])\n    # convert label map format\n    color_map_constant_mat = []\n    for color in color_map:\n        color_map_constant_mat.append(list(color))\n    color_table = tf.constant(color_map_constant_mat, dtype=tf.float32)\n    segmentation_map = tf.cast(segmentation_map, dtype=tf.int32)\n    onehot_labels = tf.one_hot(segmentation_map, depth=num_classes)\n    onehot_labels = tf.reshape(onehot_labels, (-1, num_classes))\n    colored_label = tf.matmul(onehot_labels, color_table)\n    colored_label = tf.reshape(colored_label,\n        (shape_list[0], shape_list[1], shape_list[2], output_channels))\n    return colored_label\n\n\ndef _get_outputs_from_inputs(model, input_tensors):\n    # models expect a batch dimension\n    if len(input_tensors.get_shape()) < 4:\n        input_tensors = tf.expand_dims(input_tensors, axis=0)\n    # build model, which expects a floating point input\n    inputs = tf.to_float(input_tensors)\n    preprocessed_inputs = model.preprocess(inputs)\n    outputs_dict = model.predict(preprocessed_inputs)\n    output_tensors = outputs_dict[model.main_class_predictions_key]\n    prediction_tensor = tf.argmax(output_tensors, 3)\n    prediction_tensor = tf.expand_dims(prediction_tensor, -1)\n    return prediction_tensor\n\n\ndef _image_tensor_input_placeholder(input_shape=None, pad_to_shape=None):\n    if input_shape is None:\n        input_shape = (None, None, None, 3)\n    placeholder_tensor = tf.placeholder(\n        dtype=tf.uint8, shape=input_shape, name=\'inputs\')\n    if pad_to_shape is not None:\n        input_tensor = tf.image.pad_to_bounding_box(placeholder_tensor,\n            0, 0, pad_to_shape[0], pad_to_shape[1])\n    else:\n        input_tensor = placeholder_tensor\n    return placeholder_tensor, input_tensor\n\n\ndef deploy_segmentation_inference_graph(model, input_shape,\n                                        pad_to_shape=None,\n                                        label_color_map=None,\n                                        output_collection_name=""predictions""):\n    (placeholder_tensor,\n      input_tensor) = _image_tensor_input_placeholder(input_shape, pad_to_shape)\n    outputs = _get_outputs_from_inputs(model, input_tensor)\n\n    if label_color_map is not None:\n        output_shape = outputs.get_shape().as_list()\n        outputs = _map_to_colored_labels(outputs, output_shape, label_color_map)\n\n    if pad_to_shape is not None:\n        outputs = tf.image.crop_to_bounding_box(\n            outputs, 0, 0, input_shape[0], input_shape[1])\n\n    # name tensor to make inference with frozen weights easier\n    final_op = tf.identity(outputs,\n        name=output_collection_name)\n\n    tf.train.get_or_create_global_step()\n    return final_op, placeholder_tensor\n'"
libs/filter_pruner.py,8,"b'r"""""" Filter Pruner for model compression\n\nSee the paper ""Pruning Filters for Efficient ConvNets"" for more details\n    https://arxiv.org/abs/1608.08710\n\nUsage:\n\n    compressor = FilterPruner(input_node=input_node_name,\n        output_node=output_node_name, compression_factor=compression_factor)\n\n    compressor.compress(input_graph_def, FLAGS.input_checkpoint)\n\n    compressor.save(\n        output_checkpoint_dir=FLAGS.output_dir,\n        output_checkpoint_name=output_path_name)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom . import graph_utils\n\n\nFilterPrunerNodeSpec = collections.namedtuple(\n    ""FilterPrunerNodeSpec"", [""source"", ""target"", ""following""])\n\n\ndef plot_magnitude_of_weights(plot_name, weights, compression):\n    weight_magnitudes = np.sort(np.abs(weights).sum((0, 1, 2)))\n    weight_magnitudes_list = weight_magnitudes.tolist()\n    _, _, _, channel_dim = weights.shape\n    cut_off_x = channel_dim - int(channel_dim * compression)\n    cut_off_y = max(weight_magnitudes_list)\n    plt.figure(figsize=(10, 5))\n    plt.plot(weight_magnitudes_list)\n    plt.xlabel(""Output channel"")\n    plt.xlim([0, channel_dim])\n    plt.ylabel(""L1 norm"")\n    plt.title(plot_name)\n    plt.axvline(\n        x=cut_off_x, ymin=0, ymax=cut_off_y,\n        color=\'red\', zorder=2)\n    plt.xticks(list(plt.xticks()[0]) + [cut_off_x])\n    plt.show()\n\n\nclass FilterPruner(object):\n\n    def __init__(self,\n                 input_node,\n                 output_node,\n                 compression_factor,\n                 init_pruner_specs=None,\n                 skippable_nodes=[],\n                 checkpoint_version=tf.train.SaverDef.V2,\n                 clear_devices=True,\n                 interactive_mode=False,\n                 pruner_mode=""ICNetPruner"",\n                 soft_apply=False):\n        self.input_node = input_node\n        self.output_node = output_node\n        self.compression_factor = compression_factor\n        self.checkpoint_version = checkpoint_version\n        self.clear_devices = clear_devices\n        self.skippable_nodes = skippable_nodes\n        self.init_pruner_specs = (\n            init_pruner_specs\n            if init_pruner_specs is not None else {})\n        # Empty members filled on call to prune\n        self.pruner_specs = []  # Must be updated in order\n        self.nodes_map = {}\n        self.output_graph = None\n        self.output_graph_def = None\n        if pruner_mode != ""ICNetPruner"":\n            raise ValueError(""Currently only the pruner mode `ICNetPruner`""\n                             "" is implemented for pruning ICNet filters."")\n        self.mode = pruner_mode\n        self.interactive_mode = interactive_mode\n        self.soft_apply = soft_apply\n\n    def _init_pruning_graph(self, input_checkpoint):\n        # Import graph def to use in the session\n        session = tf.Session()\n        vars_dict = graph_utils.get_vars_from_checkpoint(\n            session, input_checkpoint, self.checkpoint_version)\n        self.input_graph_def = tf.get_default_graph().as_graph_def()\n        # Grab all the variables found in the checkpoint\n        self.trainable_vars = vars_dict.keys()\n        self.nodes_map = graph_utils.create_nodes_map(self.input_graph_def)\n        self.values_map = graph_utils.create_var_const_map(\n            session, self.trainable_vars)\n        # Make sure to close the session, we will make a new one later\n        # to save a new checkpoint\n        session.close()\n\n    def _make_pruner_spec(self, target, following, source=u\'\'):\n        kwargs = {\n            ""target"": target,\n            ""source"": source,\n            ""following"": following}\n        pruner_node_spec = FilterPrunerNodeSpec(**kwargs)\n        return pruner_node_spec\n\n    def _create_adjacency_list(self, output_node_name):\n        adj_list = {}\n        already_visited = []\n        output_node = self.nodes_map[output_node_name]\n        traversal_queue = [output_node]\n        while traversal_queue:\n            curr_node = traversal_queue.pop(0)\n            curr_node_name = graph_utils.node_name_from_input(curr_node.name)\n            if curr_node_name not in already_visited:\n                already_visited.append(curr_node_name)\n                for i, input_node_name in enumerate(curr_node.input):\n                    name = graph_utils.node_name_from_input(\n                        input_node_name)\n                    input_node = self.nodes_map[name]\n                    if name not in adj_list:\n                        adj_list[name] = []\n                    adj_list[name].append(curr_node_name)\n                    traversal_queue.append(input_node)\n        return adj_list\n\n    def _get_node(self, node_name):\n        return self.nodes_map[node_name]\n\n    def _get_neighbours(self, node_name):\n        if node_name not in self.neighbors:\n            return []\n        return self.neighbors[node_name]\n\n    def _get_next_op_instance_name(self, curr_node_name, op_name):\n        results = []\n        curr_node = self.nodes_map[curr_node_name]\n        next_node_names = self._get_neighbours(curr_node_name)\n        # Found our node\n        if curr_node.op == op_name:\n            return [curr_node_name]\n        # Need to keep looking\n        for next_node_name in next_node_names:\n            new_result = self._get_next_op_instance_name(\n                next_node_name, op_name)\n            if new_result is not None:\n                for nr in new_result:\n                    if nr not in results:\n                        results.append(nr)\n        return results\n\n    def _get_conv_weights_node_name(self, conv_node_name):\n        curr_node = self.nodes_map[conv_node_name]\n        weights_node_name = graph_utils.remove_ref_from_node_name(\n            curr_node.input[1])\n        return weights_node_name\n\n    def _get_prune_idxs(self, weights_node_name):\n        weights = self.values_map[weights_node_name]\n        # plot weights for debug\n        if self.interactive_mode:\n            plot_magnitude_of_weights(weights_node_name, weights,\n                                      self.compression_factor)\n        # Find filters to keep\n        num_filters = weights.shape[-1]\n        num_filters_to_keep = int(num_filters * self.compression_factor)\n        weight_magnitudes = np.abs(weights).sum((0, 1, 2))\n        smallest_idxs = np.argsort(-weight_magnitudes)\n        top_smallest_idxs = np.sort(smallest_idxs[:num_filters_to_keep])\n        prune_idxs = np.zeros(num_filters).astype(bool)\n        prune_idxs[top_smallest_idxs] = True\n        return prune_idxs\n\n    def _prune_conv_node(self, conv_node_name, idxs=None):\n        weights_node_name = self._get_conv_weights_node_name(conv_node_name)\n        # If this node has had input channels removed before,\n        # we need to reuse that instance of weight values\n        if weights_node_name in self.output_values_map:\n            weights = self.output_values_map[weights_node_name]\n        else:\n            weights = self.values_map[weights_node_name]\n        # Grab the indices of the weights to prune\n        prune_idxs = self._get_prune_idxs(weights_node_name)\n        if idxs is not None:\n            prune_idxs = idxs\n        # Apply by actually changing shape, or simulate pruning\n        pruned_weights = np.copy(weights)\n        if not self.soft_apply:\n            pruned_weights = pruned_weights[:, :, :, prune_idxs]\n        else:\n            pruned_weights[:, :, :, ~prune_idxs] = 0\n        # Create new Conv layer with pruned weights\n        self.output_values_map[weights_node_name] = pruned_weights\n        return weights_node_name, prune_idxs\n\n    def _remove_bn_param_channels(self, next_bn_node_name, prune_idxs):\n        curr_node = self.nodes_map[next_bn_node_name]\n        # BN param names\n        scale_node_name = graph_utils.remove_ref_from_node_name(\n            curr_node.input[1])\n        shift_node_name = graph_utils.remove_ref_from_node_name(\n            curr_node.input[2])\n        # Moving average node names\n        base_moment_name = next_bn_node_name[:next_bn_node_name.rfind(""/"")]\n        mean_node_name = base_moment_name + ""/moving_mean""\n        variance_node_name = base_moment_name + ""/moving_variance""\n        # Adjust values to account for removed idxs\n        for var_name in [scale_node_name, shift_node_name,\n                         mean_node_name, variance_node_name]:\n            value = self.values_map[var_name]\n            pruned_value = np.copy(value)\n            if not self.soft_apply:\n                pruned_value = pruned_value[prune_idxs]\n            else:\n                pruned_value[~prune_idxs] = 0\n            self.output_values_map[var_name] = pruned_value\n        return next_bn_node_name\n\n    def _remove_conv_param_channels(self, next_conv_node_name, src_prune_idxs):\n        curr_node = self.nodes_map[next_conv_node_name]\n        weights_node_name = graph_utils.remove_ref_from_node_name(\n            curr_node.input[1])\n        # If this node has had FILTERS removed, we can still remove\n        # input channels. So this case should be fine\n        if weights_node_name in self.output_values_map:\n            weights = self.output_values_map[weights_node_name]\n        else:\n            weights = self.values_map[weights_node_name]\n        kernel_h, kernel_w, batch, num_filters = weights.shape\n        # In order to prune the last Conv Op in the PSPModule, we need\n        # to pad the channels of src_prune_idxs to match that output of the\n        # Concat op. See the PSP prune config for more information.\n        prune_idxs = np.copy(src_prune_idxs)\n        if batch != len(src_prune_idxs):\n            prune_idxs.resize(batch)\n            prune_idxs[len(src_prune_idxs):] = True  # keep extra channels\n        # Soft apply if we need to retrain without changing variable shape\n        updated_kernels = np.copy(weights)\n        if not self.soft_apply:\n            updated_kernels = updated_kernels[:, :, prune_idxs, :]\n        else:\n            updated_kernels[:, :, ~prune_idxs, :] = 0\n        self.output_values_map[weights_node_name] = updated_kernels\n        return weights_node_name\n\n    def _apply_pruner_specs(self, pruner_specs):\n        self.output_values_map = {}\n        pruned_node_idxs = {}\n        for pruner_spec in pruner_specs:\n            curr_node_name = pruner_spec.target\n            source_node_name = pruner_spec.source\n            following_node_names = pruner_spec.following\n            curr_node = self.nodes_map[curr_node_name]\n\n            print(\'\\x1b[6;30;44m\' + \\\n                \'Applying Pruning Spec to `%s`!\\x1b[0m\' % curr_node_name)\n\n            if curr_node.op != ""Conv2D"":\n                raise ValueError(""Only Conv nodes can be prunned with the ""\n                                 ""FilterPruner compressor."")\n            # Prune the current conv we are dealing with\n            source_node_idxs = None\n            if source_node_name:\n                # TODO(oandrien): This is redundant, should fix traversal\n                # instead. Look ahead if we havent encountered the node yet.\n                if source_node_name not in pruned_node_idxs:\n                    weights_node_name = self._get_conv_weights_node_name(\n                        source_node_name)\n                    source_node_idxs = self._get_prune_idxs(weights_node_name)\n                else:\n                    source_node_idxs = pruned_node_idxs[source_node_name]\n            # Actually prune the variable now\n            new_weights_node_name, prune_idxs = self._prune_conv_node(\n                curr_node_name, source_node_idxs)\n            pruned_node_idxs[curr_node_name] = prune_idxs\n            # Prune following BN\'s and Convs\n            for following_node_name in following_node_names:\n                following_node = self.nodes_map[following_node_name]\n                # BATCH NORM\n                if following_node.op == ""FusedBatchNorm"":\n                    new_following_node_name = self._remove_bn_param_channels(\n                        following_node_name, prune_idxs)\n                # CONVS\n                elif following_node.op == ""Conv2D"":\n                    new_following_node_name = self._remove_conv_param_channels(\n                        following_node_name, prune_idxs)\n                else:\n                    raise ValueError(\'Following nodes should only be\'\n                                     \' BatchNorms or Convs...\')\n                print(\'Removed channels from %s...\' % new_following_node_name)\n        print(\'\\x1b[6;30;42m DONE! \\x1b[0m\')\n\n    def _get_following_bn_and_conv_names(self, curr_node_name):\n        next_conv_node_names = []\n        next_bn_node_names = []\n        next_node_names = self._get_neighbours(curr_node_name)\n        for next_node_name in next_node_names:\n            next_node = self.nodes_map[next_node_name]\n            if next_node.op == ""BatchToSpaceND"":\n                next_node_name = self._get_next_op_instance_name(\n                    next_node_name, ""FusedBatchNorm"")\n                if len(next_node_name) > 1:\n                    raise ValueError(\'Something went wrong with BatchNorms...\')\n                next_node_name = next_node_name[0]\n                next_node = self.nodes_map[next_node_name]\n            if next_node.op == ""FusedBatchNorm"":\n                next_bn_node_names = [next_node_name]\n                next_conv_node_names = self._get_next_op_instance_name(\n                    next_node_name, ""Conv2D"")\n            elif next_node.op == ""Relu"" or next_node.op == ""Relu6"":\n                next_conv_node_names = self._get_next_op_instance_name(\n                    next_node_name, ""Conv2D"")\n            elif next_node.op == ""Conv2D"":\n                next_conv_node_names = [next_node_name]\n            elif next_node_name == self.output_node:\n                return None\n            else:\n                raise ValueError(\'Incompatable model file.\')\n        return next_bn_node_names + next_conv_node_names\n\n    def _create_pruner_specs_recursively(self, curr_node_name):\n        # Update traversal state\n        if curr_node_name in self.state.already_visited:\n            return\n        self.state.already_visited[curr_node_name] = True\n        # Get node info\n        curr_node = self.nodes_map[curr_node_name]\n        next_node_names = self._get_neighbours(curr_node_name)\n\n        if curr_node_name not in self.skippable_nodes:\n            # If not conv, we skip since we only deal with\n            # convs and djecent convs and proceding batch norms\n            if curr_node_name in self.init_pruner_specs.keys():\n                pruner_spec = self.init_pruner_specs[curr_node_name]\n                self.pruner_specs.append(pruner_spec)\n\n                print(\'\\x1b[6;30;42m\' + \\\n                      \'Currently on `%s`\\x1b[0m\' % curr_node_name)\n                print("" - Added from INIT_PRUNER_SPEC"")\n                for name in pruner_spec.following:\n                    print("" - Following: "" + name)\n\n            elif curr_node.op == ""Conv2D"":\n                # Create filter spec from traversal\n                dependant_nodes = self._get_following_bn_and_conv_names(\n                    curr_node_name)\n                if dependant_nodes is not None and len(dependant_nodes) > 0:\n                    pruner_spec = self._make_pruner_spec(\n                        curr_node_name, following=dependant_nodes)\n                    self.pruner_specs.append(pruner_spec)\n\n                    print(\'\\x1b[6;30;42m\' + \\\n                          \'Currently on `%s`\\x1b[0m\' % curr_node_name)\n                    print("" - Added from CREATED_PRUNER_SPEC"")\n                    for name in pruner_spec.following:\n                        print("" - Following: "" + name)\n                else:\n                    print(\'\\x1b[6;30;43m\' + \\\n                         \'Skipping last Conv `%s`\\x1b[0m\' % curr_node_name)\n            else:\n                print(""Currently on {}, node is Non-Conv, skipping..."".format(\n                    curr_node_name))\n        else:\n            print(""Currently on {}, node in skip list, skipping..."".format(\n                curr_node_name))\n\n        # Traverse adjacent nodes\n        for next_node in next_node_names:\n            self._create_pruner_specs_recursively(next_node)\n\n    def compress(self, input_checkpoint, skip_apply=False):\n        # Create a session and graph all variable values\n        self._init_pruning_graph(input_checkpoint)\n        if self.clear_devices:\n            graph_utils.clear_node_devices(self.input_graph_def.node)\n        self.neighbors = self._create_adjacency_list(self.output_node)\n        # Traverse graph and collect all nodes and dependencies\n        self.state = graph_utils.GraphTraversalState(\n            already_visited={}, output_node_stack=[])\n        self._create_pruner_specs_recursively(self.input_node)\n        if not skip_apply:\n            self._apply_pruner_specs(self.pruner_specs)\n\n    def save(self, output_checkpoint_dir, output_checkpoint_name):\n        output_checkpoint_path = os.path.join(\n            output_checkpoint_dir, output_checkpoint_name)\n        output_graph = tf.Graph()\n        with output_graph.as_default():\n            session = tf.Session(graph=output_graph)\n            var_list = []\n            for trainable_var in self.trainable_vars:\n                if (trainable_var in self.skippable_nodes or\n                    trainable_var not in self.output_values_map):\n                    print(\'WARNING: Copying original node %s...\' %\n                        trainable_var)\n                    init_value = self.values_map[trainable_var]\n                else:\n                    init_value = self.output_values_map[trainable_var]\n                new_var = graph_utils.add_variable_to_graph(\n                    session.graph, trainable_var, init_value)\n                var_list.append(new_var)\n            # To avoid error with searching for global step\n            global_step = tf.train.get_or_create_global_step()\n            # Need to run init to assign all our new variable vals\n            session.run(tf.variables_initializer(var_list=var_list))\n            write_saver = tf.train.Saver(\n                var_list=var_list, write_version=self.checkpoint_version)\n            write_saver.save(session, output_checkpoint_path)\n            print(\'Saving pruned model checkpoint to {}\'.format(\n                output_checkpoint_path))\n        session.close()\n'"
libs/graph_utils.py,0,"b'r""""""Utils for working with variables and proto defs.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport collections\n\nfrom tensorflow.python.ops.variables import Variable\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.training import saver as saver_lib\nfrom tensorflow.python.framework import ops\n\n\nGraphTraversalState = collections.namedtuple(\n    ""GraphTraversalState"", [""already_visited"", ""output_node_stack""])\n\n\ndef remove_ref_from_node_name(node_name):\n    if node_name.endswith(""/read""):\n        node_name = node_name[:-5]\n    return node_name\n\n\ndef node_name_matches(node_name, search_str):\n    if node_name.startswith(""^""):\n        node_name = node_name[1:]\n    m = re.search(r""(.*)%s:\\d+$""%search_str, node_name)\n    if m:\n        return m.group(1)\n    return None\n\n\ndef node_name_from_input(node_name):\n    if node_name.startswith(""^""):\n        node_name = node_name[1:]\n    m = re.search(r""(.*)?:\\d+$"", node_name)\n    if m:\n        node_name = m.group(1)\n    return node_name\n\n\ndef clear_node_devices(input_graph_def_nodes):\n    for node in input_graph_def_nodes:\n        node.device = """"\n\n\ndef create_var_const_map(session, var_names):\n    values_dict = {}\n    for key in var_names:\n        tensor_name = key + "":0""\n        values_dict[key] = session.run(tensor_name)\n    return values_dict\n\n\ndef create_nodes_map(graph):\n    nodes_map = {}\n    for node in graph.node:\n        if node.name not in nodes_map.keys():\n            nodes_map[node.name] = node\n        else:\n            raise ValueError(""Duplicate node names detected."")\n    return nodes_map\n\n\ndef create_constant_node(name, value, dtype, shape=None):\n    node = create_node(""Const"", name, [])\n    set_attr_dtype(node, ""dtype"", dtype)\n    set_attr_tensor(node, ""value"", value, dtype, shape)\n    return node\n\n\ndef create_node(op, name, inputs):\n    new_node = node_def_pb2.NodeDef()\n    new_node.op = op\n    new_node.name = name\n    for input_name in inputs:\n        new_node.input.extend([input_name])\n    return new_node\n\n\ndef copy_variable_ref_to_graph(input_graph, output_graph,\n                               var_ref, init_value, scope=\'\'):\n    if scope != \'\':\n        new_name = (\n            scope + \'/\' + var_ref.name[:var_ref.name.index(\':\')])\n    else:\n        new_name = var_ref.name[:var_ref.name.index(\':\')]\n    collections = []\n    for name, collection in input_graph._collections.items():\n        if var_ref in collection:\n            if (name == ops.GraphKeys.GLOBAL_VARIABLES or\n                name == ops.GraphKeys.TRAINABLE_VARIABLES or\n                scope == \'\'):\n                collections.append(name)\n            else:\n                collections.append(scope + \'/\' + name)\n    trainable = (var_ref in input_graph.get_collection(\n            ops.GraphKeys.TRAINABLE_VARIABLES))\n    with output_graph.as_default():\n        new_var = Variable(\n            init_value,\n            trainable,\n            name=new_name,\n            collections=collections,\n            validate_shape=False)\n        new_var.set_shape(init_value.shape)\n    return new_var\n\n\ndef add_variable_to_graph(output_graph, var_name, init_value,\n                          trainable=True, collections=[], scope=\'\'):\n    if scope != \'\':\n        new_name = scope + \'/\' + var_name\n    else:\n        new_name = var_name\n\n    with output_graph.as_default():\n        new_var = Variable(\n            init_value,\n            trainable,\n            name=new_name,\n            collections=collections,\n            validate_shape=False)\n        new_var.set_shape(init_value.shape)\n    return new_var\n\n\ndef get_vars_from_checkpoint(session, checkpoint, checkpoint_version):\n    var_list = {}\n    meta_graph_file = checkpoint + \'.meta\'\n    saver = saver_lib.import_meta_graph(meta_graph_file)\n    reader = pywrap_tensorflow.NewCheckpointReader(checkpoint)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    for key in sorted(var_to_shape_map):\n        try:\n            tensor_name = key + "":0""\n            tensor = session.graph.get_tensor_by_name(\n                tensor_name)\n        except KeyError:\n            continue\n        var_list[key] = tensor\n    saver.restore(session, checkpoint)\n    return var_list\n'"
libs/standard_fields.py,0,"b'r""""""Commonly used keys.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass TFRecordFields(object):\n    """"""Keys for parsing TFRecords.""""""\n    image_encoded = \'image/encoded\'\n    image_filename = \'image/filename\'\n    image_format = \'image/format\'\n    image_height = \'image/height\'\n    image_width = \'image/width\'\n    image_channels = \'image/channels\'\n    segmentation_class_encoded = \'image/segmentation/class/encoded\'\n    segmentation_class_format = \'image/segmentation/class/format\'\n\n\nclass GroundtruthFields(object):\n    """"""Keys for groudtruth dicts.""""""\n    input_image = \'input_image\'\n    input_image_path = \'input_image_path\'\n    input_image_height = \'input_image_height\'\n    input_image_width = \'input_image_width\'\n    output_mask = \'label_mask\'\n'"
libs/trainer.py,52,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport time\n\nimport tensorflow as tf\n\nfrom libs import standard_fields as fields\nfrom third_party import model_deploy\nfrom third_party import mem_util\nfrom builders import dataset_builder\nfrom builders import preprocessor_builder\nfrom builders import optimizer_builder\n\n\nslim = tf.contrib.slim\n\nprefetch_queue = slim.prefetch_queue\n\n\ndef create_training_input(create_input_fn,\n                          preprocess_fn,\n                          batch_size,\n                          batch_queue_capacity,\n                          batch_queue_threads,\n                          prefetch_queue_capacity):\n\n    tensor_dict = create_input_fn()\n\n    def cast_and_reshape(tensor_dict, dicy_key):\n        items = tensor_dict[dicy_key]\n        float_images = tf.to_float(items)\n        tensor_dict[dicy_key] = float_images\n        return tensor_dict\n\n    tensor_dict = cast_and_reshape(tensor_dict,\n                    fields.GroundtruthFields.input_image)\n\n    if preprocess_fn is not None:\n        preprocessor = preprocess_fn()\n        tensor_dict = preprocessor(tensor_dict)\n\n    batched_tensors = tf.train.batch(tensor_dict,\n        batch_size=batch_size, num_threads=batch_queue_threads,\n        capacity=batch_queue_capacity, dynamic_pad=True)\n\n    return prefetch_queue.prefetch_queue(batched_tensors,\n        capacity=prefetch_queue_capacity,\n        dynamic_pad=False)\n\n\ndef create_training_model_losses(input_queue, create_model_fn, train_config,\n                                 train_dir=None, gradient_checkpoints=None):\n\n    _, segmentation_model = create_model_fn()\n    read_data_list = input_queue.dequeue()\n\n    def extract_images_and_targets(read_data):\n        images = read_data[fields.GroundtruthFields.input_image]\n        labels = read_data[fields.GroundtruthFields.output_mask]\n        return (images, labels)\n    (images, labels) = zip(*map(extract_images_and_targets, [read_data_list]))\n\n    # Incase we need to do zero centering, we do it here\n    preprocessed_images = []\n    for image in images:\n        resized_image = segmentation_model.preprocess(image)\n        preprocessed_images.append(resized_image)\n    images = tf.concat(preprocessed_images, 0, name=""Inputs"")\n\n    segmentation_model.provide_groundtruth(labels[0])\n    prediction_dict = segmentation_model.predict(images)\n\n    # Optional quantization - this is experimental and might not work.\n    if train_config.quantize_with_delay:\n        tf.logging.info(\'Adding quantization nodes to training graph...\')\n        tf.contrib.quantize.create_training_graph(\n            quant_delay=train_config.quantize_with_delay)\n\n    # Add checkpointing nodes to correct collection\n    if gradient_checkpoints is not None:\n        tf.logging.info(\n            \'Adding gradient checkpoints to `checkpoints` collection\')\n        graph = tf.get_default_graph()\n        clone_scope = graph.get_name_scope()\n        checkpoint_list = gradient_checkpoints\n        for checkpoint_node_name in checkpoint_list:\n            curr_tensor_name = checkpoint_node_name + "":0""\n            if clone_scope is not None:\n                curr_tensor_name = clone_scope + \'/\' + curr_tensor_name\n            node = graph.get_tensor_by_name(curr_tensor_name)\n            tf.add_to_collection(\'checkpoints\', node)\n\n    # Gather main and aux losses here to single collection\n    losses_dict = segmentation_model.loss(prediction_dict)\n    for loss_tensor in losses_dict.values():\n        tf.losses.add_loss(loss_tensor)\n\n\ndef train_segmentation_model(create_model_fn,\n                             create_input_fn,\n                             train_config,\n                             master,\n                             task,\n                             is_chief,\n                             startup_delay_steps,\n                             train_dir,\n                             num_clones,\n                             num_worker_replicas,\n                             num_ps_tasks,\n                             clone_on_cpu,\n                             replica_id,\n                             num_replicas,\n                             max_checkpoints_to_keep,\n                             save_interval_secs,\n                             image_summaries,\n                             log_memory=False,\n                             gradient_checkpoints=None,\n                             sync_bn_accross_gpu=False):\n    """"""Create an instance of the FastSegmentationModel""""""\n    _, segmentation_model = create_model_fn()\n    deploy_config = model_deploy.DeploymentConfig(\n        num_clones=num_clones,\n        clone_on_cpu=clone_on_cpu,\n        replica_id=task,\n        num_replicas=num_worker_replicas,\n        num_ps_tasks=num_ps_tasks)\n    startup_delay_steps = task * startup_delay_steps\n\n    per_clone_batch_size = train_config.batch_size // num_clones\n\n    preprocess_fn = None\n    if train_config.preprocessor_step:\n        preprocess_fn = functools.partial(\n            preprocessor_builder.build,\n            preprocessor_config_list=train_config.preprocessor_step)\n\n    with tf.Graph().as_default():\n        # Create the global step on the device storing the variables.\n        with tf.device(deploy_config.variables_device()):\n            global_step = tf.train.get_or_create_global_step()\n\n        with tf.device(deploy_config.inputs_device()): # CPU of each worker\n            input_queue = create_training_input(\n                create_input_fn,\n                preprocess_fn,\n                per_clone_batch_size,\n                batch_queue_capacity=train_config.batch_queue_capacity,\n                batch_queue_threads=train_config.num_batch_queue_threads,\n                prefetch_queue_capacity=train_config.prefetch_queue_capacity)\n\n        # CPU of common ps server\n        with tf.device(deploy_config.variables_device()):\n            global_step = tf.train.get_or_create_global_step()\n            # Note: it is assumed that any loss created by `model_fn`\n            # is collected at the tf.GraphKeys.LOSSES collection.\n            model_fn = functools.partial(create_training_model_losses,\n                create_model_fn=create_model_fn,\n                train_config=train_config,\n                train_dir=train_dir,\n                gradient_checkpoints=gradient_checkpoints)\n            clones = model_deploy.create_clones(deploy_config,\n                model_fn, [input_queue])\n            first_clone_scope = deploy_config.clone_scope(0)\n\n            if sync_bn_accross_gpu:\n                # Attempt to sync BN updates across all GPU\'s in a tower.\n                # Caution since this is very slow. Might not be needed\n                update_ops = []\n                for clone_idx in range(num_clones):\n                    nth_clone_sope = deploy_config.clone_scope(clone_idx)\n                    update_ops.extend(tf.get_collection(\n                        tf.GraphKeys.UPDATE_OPS, nth_clone_sope))\n            else:\n                # Gather updates from first GPU only\n                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n                                               first_clone_scope)\n\n        # Init variable to collect summeries\n        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        # Add summaries for losses.\n        for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n            summaries.add(tf.summary.scalar(\'Losses/%s\' % loss.op.name, loss))\n\n        with tf.device(deploy_config.optimizer_device()):  # CPU of each worker\n            (training_optimizer,\n              optimizer_summary_vars) = optimizer_builder.build(\n                train_config.optimizer)\n            for var in optimizer_summary_vars:\n                summaries.add(\n                    tf.summary.scalar(var.op.name, var, family=\'LearningRate\'))\n\n        # Add summaries for model variables.\n        for model_var in slim.get_model_variables():\n            summaries.add(tf.summary.histogram(model_var.op.name, model_var))\n\n        # Fine tune from classification or segmentation checkpoints\n        trainable_vars = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES)\n        if train_config.fine_tune_checkpoint:\n            if not train_config.fine_tune_checkpoint_type:\n                raise ValueError(\'Must specify `fine_tune_checkpoint_type`.\')\n\n            tf.logging.info(\'Initializing %s model from checkpoint %s\',\n                train_config.fine_tune_checkpoint_type,\n                train_config.fine_tune_checkpoint)\n\n            variables_to_restore = segmentation_model.restore_map(\n                fine_tune_checkpoint_type=(\n                    train_config.fine_tune_checkpoint_type))\n\n            init_fn = slim.assign_from_checkpoint_fn(\n                train_config.fine_tune_checkpoint,\n                variables_to_restore,\n                ignore_missing_vars=True)\n\n            if train_config.freeze_fine_tune_backbone:\n                tf.logging.info(\'Freezing %s scope from checkpoint.\')\n                non_frozen_vars = []\n                for var in trainable_vars:\n                    if not var.op.name.startswith(\n                        segmentation_model.shared_feature_extractor_scope):\n                        non_frozen_vars.append(var)\n                        tf.logging.info(\'Training variable: %s\', var.op.name)\n                trainable_vars = non_frozen_vars\n        else:\n            tf.logging.info(\'Not initializing the model from a checkpoint. \'\n                            \'Initializing from scratch!\')\n\n        # TODO(@oandrien): we might want to add gradient multiplier here\n        # for the last layer if we have trouble with training\n        # CPU of common ps server\n        with tf.device(deploy_config.optimizer_device()):\n            reg_losses = (\n                None if train_config.add_regularization_loss else [])\n            total_loss, grads_and_vars = model_deploy.optimize_clones(\n                clones, training_optimizer,\n                regularization_losses=reg_losses,\n                var_list=trainable_vars)\n            total_loss = tf.check_numerics(\n                total_loss, \'LossTensor is inf or nan.\')\n            summaries.add(\n                tf.summary.scalar(\'Losses/TotalLoss\', total_loss))\n\n            grad_updates = training_optimizer.apply_gradients(\n                grads_and_vars, global_step=global_step)\n            update_ops.append(grad_updates)\n            update_op = tf.group(*update_ops, name=\'update_barrier\')\n            with tf.control_dependencies([update_op]):\n                train_op = tf.identity(total_loss, name=\'train_op\')\n\n        # TODO: this ideally should not be hardcoded like this.\n        #   should have a way to access the prediction and GT tensor\n        if image_summaries:\n            graph = tf.get_default_graph()\n            pixel_scaling = max(1, 255 // 19)\n            summ_first_clone_scope = (first_clone_scope + \'/\'\n                if first_clone_scope else \'\')\n            main_labels = graph.get_tensor_by_name(\n                \'%sSegmentationLoss/Reshape:0\' % summ_first_clone_scope)\n            main_preds = graph.get_tensor_by_name(\n                \'%sSegmentationLoss/Reshape_1:0\' % summ_first_clone_scope)\n            main_preds = tf.cast(main_preds * pixel_scaling, tf.uint8)\n            summaries.add(\n              tf.summary.image(\'VerifyTrainImages/Predictions\', main_preds))\n            main_labels = tf.cast(main_labels * pixel_scaling, tf.uint8)\n            summaries.add(\n              tf.summary.image(\'VerifyTrainImages/Groundtruths\', main_labels))\n\n        # Add the summaries from the first clone. These contain the summaries\n        # created by model_fn and either optimize_clones()\n        # or _gather_clone_loss().\n        summaries |= set(\n            tf.get_collection(tf.GraphKeys.SUMMARIES, first_clone_scope))\n\n        # Merge all summaries together.\n        summary_op = tf.summary.merge(list(summaries))\n\n        session_config = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n\n        # Save checkpoints regularly.\n        saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep)\n\n        # HACK to see memory usage.\n        # TODO: Clean up, pretty messy.\n        def train_step_mem(sess, train_op, global_step, train_step_kwargs):\n            start_time = time.time()\n            run_metadata = tf.RunMetadata()\n            options = tf.RunOptions(trace_level=(tf.RunOptions.FULL_TRACE\n                if log_memory else tf.RunOptions.NO_TRACE))\n            total_loss, np_global_step = sess.run([train_op, global_step],\n                                        options=options,\n                                        run_metadata=run_metadata)\n            time_elapsed = time.time() - start_time\n\n            if \'should_log\' in train_step_kwargs:\n                if sess.run(train_step_kwargs[\'should_log\']):\n                    tf.logging.info(\n                        \'global step %d: loss = %.4f (%.3f sec/step)\',\n                        np_global_step, total_loss, time_elapsed)\n\n            if log_memory:\n                mem_use = mem_util.peak_memory(run_metadata)\n                master_device_mem = list(mem_use.values())[0] / 1e6 # \'/gpu:0\'\n                tf.logging.info(\'Memory used: %.2f MB\', (master_device_mem))\n\n            if \'should_stop\' in train_step_kwargs:\n                should_stop = sess.run(train_step_kwargs[\'should_stop\'])\n            else:\n                should_stop = False\n\n            return total_loss, should_stop\n\n        # Main training loop\n        slim.learning.train(\n            train_op,\n            train_step_fn=train_step_mem,\n            logdir=train_dir,\n            master=master,\n            is_chief=is_chief,\n            session_config=session_config,\n            number_of_steps=train_config.num_steps,\n            startup_delay_steps=startup_delay_steps,\n            init_fn=init_fn,\n            summary_op=summary_op,\n            save_summaries_secs=120,\n            save_interval_secs=save_interval_secs,\n            saver=saver)\n'"
protos/__init__.py,0,b''
third_party/__init__.py,0,b''
third_party/conv_blocks.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolution blocks for mobilenet.""""""\nimport contextlib\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\ndef _split_divisible(num, num_ways, divisible_by=8):\n  """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result\n\n\n@contextlib.contextmanager\ndef _v1_compatible_scope_naming(scope):\n  if scope is None:  # Create uniqified separable blocks.\n    with tf.variable_scope(None, default_name=\'separable\') as s, \\\n         tf.name_scope(s.original_name_scope):\n      yield \'\'\n  else:\n    # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.\n    # which provide numbered scopes.\n    scope += \'_\'\n    yield scope\n\n\n@slim.add_arg_scope\ndef split_separable_conv2d(input_tensor,\n                           num_outputs,\n                           scope=None,\n                           normalizer_fn=None,\n                           stride=1,\n                           rate=1,\n                           endpoints=None,\n                           use_explicit_padding=False):\n  """"""Separable mobilenet V1 style convolution.\n\n  Depthwise convolution, with default non-linearity,\n  followed by 1x1 depthwise convolution.  This is similar to\n  slim.separable_conv2d, but differs in tha it applies batch\n  normalization and non-linearity to depthwise. This  matches\n  the basic building of Mobilenet Paper\n  (https://arxiv.org/abs/1704.04861)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs\n    scope: optional name of the scope. Note if provided it will use\n    scope_depthwise for deptwhise, and scope_pointwise for pointwise.\n    normalizer_fn: which normalizer function to use for depthwise/pointwise\n    stride: stride\n    rate: output rate (also known as dilation rate)\n    endpoints: optional, if provided, will export additional tensors to it.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n\n  Returns:\n    output tesnor\n  """"""\n\n  with _v1_compatible_scope_naming(scope) as scope:\n    dw_scope = scope + \'depthwise\'\n    endpoints = endpoints if endpoints is not None else {}\n    kernel_size = [3, 3]\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n      input_tensor = _fixed_padding(input_tensor, kernel_size, rate)\n    net = slim.separable_conv2d(\n        input_tensor,\n        None,\n        kernel_size,\n        depth_multiplier=1,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=dw_scope)\n\n    endpoints[dw_scope] = net\n\n    pw_scope = scope + \'pointwise\'\n    net = slim.conv2d(\n        net,\n        num_outputs, [1, 1],\n        stride=1,\n        normalizer_fn=normalizer_fn,\n        scope=pw_scope)\n    endpoints[pw_scope] = net\n  return net\n\n\ndef expand_input_by_factor(n, divisible_by=8):\n  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n\n\n@slim.add_arg_scope\ndef expanded_conv(input_tensor,\n                  num_outputs,\n                  expansion_size=expand_input_by_factor(6),\n                  stride=1,\n                  rate=1,\n                  kernel_size=(3, 3),\n                  residual=True,\n                  normalizer_fn=None,\n                  project_activation_fn=tf.identity,\n                  split_projection=1,\n                  split_expansion=1,\n                  expansion_transform=None,\n                  depthwise_location=\'expansion\',\n                  depthwise_channel_multiplier=1,\n                  endpoints=None,\n                  use_explicit_padding=False,\n                  padding=\'SAME\',\n                  scope=None):\n  """"""Depthwise Convolution Block with expansion.\n\n  Builds a composite convolution that has the following structure\n  expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs in the final layer.\n    expansion_size: the size of expansion, could be a constant or a callable.\n      If latter it will be provided \'num_inputs\' as an input. For forward\n      compatibility it should accept arbitrary keyword arguments.\n      Default will expand the input by factor of 6.\n    stride: depthwise stride\n    rate: depthwise rate\n    kernel_size: depthwise kernel\n    residual: whether to include residual connection between input\n      and output.\n    normalizer_fn: batchnorm or otherwise\n    project_activation_fn: activation function for the project layer\n    split_projection: how many ways to split projection operator\n      (that is conv expansion->bottleneck)\n    split_expansion: how many ways to split expansion op\n      (that is conv bottleneck->expansion) ops will keep depth divisible\n      by this value.\n    expansion_transform: Optional function that takes expansion\n      as a single input and returns output.\n    depthwise_location: where to put depthwise covnvolutions supported\n      values None, \'input\', \'output\', \'expansion\'\n    depthwise_channel_multiplier: depthwise channel multiplier:\n    each input will replicated (with different filters)\n    that many times. So if input had c channels,\n    output will have c x depthwise_channel_multpilier.\n    endpoints: An optional dictionary into which intermediate endpoints are\n      placed. The keys ""expansion_output"", ""depthwise_output"",\n      ""projection_output"" and ""expansion_transform"" are always populated, even\n      if the corresponding functions are not invoked.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    padding: Padding type to use if `use_explicit_padding` is not set.\n    scope: optional scope.\n\n  Returns:\n    Tensor of depth num_outputs\n\n  Raises:\n    TypeError: on inval\n  """"""\n  with tf.variable_scope(scope, default_name=\'expanded_conv\') as s, \\\n       tf.name_scope(s.original_name_scope):\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    if  depthwise_location not in [None, \'input\', \'output\', \'expansion\']:\n      raise TypeError(\'%r is unknown value for depthwise_location\' %\n                      depthwise_location)\n    if use_explicit_padding:\n      if padding != \'SAME\':\n        raise TypeError(\'`use_explicit_padding` should only be used with \'\n                        \'""SAME"" padding.\')\n      padding = \'VALID\'\n    depthwise_func = functools.partial(\n        slim.separable_conv2d,\n        num_outputs=None,\n        kernel_size=kernel_size,\n        depth_multiplier=depthwise_channel_multiplier,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=\'depthwise\')\n    # b1 -> b2 * r -> b2\n    #   i -> (o * r) (bottleneck) -> o\n    input_tensor = tf.identity(input_tensor, \'input\')\n    net = input_tensor\n\n    if depthwise_location == \'input\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(expansion_size):\n      inner_size = expansion_size(num_inputs=prev_depth)\n    else:\n      inner_size = expansion_size\n\n    if inner_size > net.shape[3]:\n      net = split_conv(\n          net,\n          inner_size,\n          num_ways=split_expansion,\n          scope=\'expand\',\n          stride=1,\n          normalizer_fn=normalizer_fn)\n      net = tf.identity(net, \'expansion_output\')\n    if endpoints is not None:\n      endpoints[\'expansion_output\'] = net\n\n    if depthwise_location == \'expansion\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net)\n\n    net = tf.identity(net, name=\'depthwise_output\')\n    if endpoints is not None:\n      endpoints[\'depthwise_output\'] = net\n    if expansion_transform:\n      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n    # Note in contrast with expansion, we always have\n    # projection to produce the desired output size.\n    net = split_conv(\n        net,\n        num_outputs,\n        num_ways=split_projection,\n        stride=1,\n        scope=\'project\',\n        normalizer_fn=normalizer_fn,\n        activation_fn=project_activation_fn)\n    if endpoints is not None:\n      endpoints[\'projection_output\'] = net\n    if depthwise_location == \'output\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(residual):  # custom residual\n      net = residual(input_tensor=input_tensor, output_tensor=net)\n    elif (residual and\n          # stride check enforces that we don\'t add residuals when spatial\n          # dimensions are None\n          stride == 1 and\n          # Depth matches\n          net.get_shape().as_list()[3] ==\n          input_tensor.get_shape().as_list()[3]):\n      net += input_tensor\n    return tf.identity(net, name=\'output\')\n\n\ndef split_conv(input_tensor,\n               num_outputs,\n               num_ways,\n               scope,\n               divisible_by=8,\n               **kwargs):\n  """"""Creates a split convolution.\n\n  Split convolution splits the input and output into\n  \'num_blocks\' blocks of approximately the same size each,\n  and only connects $i$-th input to $i$ output.\n\n  Args:\n    input_tensor: input tensor\n    num_outputs: number of output filters\n    num_ways: num blocks to split by.\n    scope: scope for all the operators.\n    divisible_by: make sure that every part is divisiable by this.\n    **kwargs: will be passed directly into conv2d operator\n  Returns:\n    tensor\n  """"""\n  b = input_tensor.get_shape().as_list()[3]\n\n  if num_ways == 1 or min(b // num_ways,\n                          num_outputs // num_ways) < divisible_by:\n    # Don\'t do any splitting if we end up with less than 8 filters\n    # on either side.\n    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n\n  outs = []\n  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n  output_splits = _split_divisible(\n      num_outputs, num_ways, divisible_by=divisible_by)\n  inputs = tf.split(input_tensor, input_splits, axis=3, name=\'split_\' + scope)\n  base = scope\n  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n    scope = base + \'_part_%d\' % (i,)\n    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n    n = tf.identity(n, scope + \'_output\')\n    outs.append(n)\n  return tf.concat(outs, 3, name=scope + \'_concat\')\n'"
third_party/dilated_resnet_v1.py,9,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Resnet v1 model variants.\n\nHACK(oandrien):  Code branched out from slim/nets/resnet_v1.py, and please refer to it for more details. A filter_scale option and interp layer have been added\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nfrom third_party import resnet_utils\nfrom third_party import resnet_v1\n\n\nslim = tf.contrib.slim\n\n\nclass DownSampleBlock(\n  collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """""" """"""\n\n\n@slim.add_arg_scope\ndef downsample(inputs, s_factor, stride=1, rate=1, scope=None):\n  """"""""""""\n  with tf.variable_scope(scope, \'Interp\', [inputs]) as sc:\n    _, input_h, input_w, _ = inputs.get_shape().as_list()\n    shrink_h = (input_h-1)*s_factor+1\n    shrink_w = (input_w-1)*s_factor+1\n    return tf.image.resize_bilinear(inputs,\n                                    [int(shrink_h), int(shrink_w)],\n                                    align_corners=True)\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs,\n               depth,\n               depth_bottleneck,\n               stride,\n               overide_rate=1,\n               rate=1,\n               outputs_collections=None,\n               scope=None,\n               use_bounded_activations=False):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    unit_rate: An integer, overiding rate for atrous convolution.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n    use_bounded_activations: Whether or not to use bounded activations. Bounded\n      activations better lend themselves to quantized inference.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(\n          inputs,\n          depth, [1, 1],\n          stride=stride,\n          activation_fn=tf.nn.relu6 if use_bounded_activations else None,\n          scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate*overide_rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    if use_bounded_activations:\n      # Use clip_by_value to simulate bandpass activation.\n      residual = tf.clip_by_value(residual, -6.0, 6.0)\n      output = tf.nn.relu6(shortcut + residual)\n    else:\n      output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.name,\n                                            output)\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride, rate=1):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  minor_depth = int(base_depth)\n  major_depth = int(base_depth * 4)\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': major_depth,\n      \'depth_bottleneck\': minor_depth,\n      \'stride\': 1,\n      \'overide_rate\': rate\n  }] * (num_units - 1) + [{\n      \'depth\': major_depth,\n      \'depth_bottleneck\': minor_depth,\n      \'stride\': stride,\n      \'overide_rate\': rate\n  }])\n\n\ndef resnet_v1_downsample_block(scope, factor):\n  """""" """"""\n  return DownSampleBlock(scope, downsample, [{\n    \'s_factor\': factor\n  }])\n\n\ndef resnet_v1(inputs,\n              blocks,\n              filter_scale=1.0,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              reuse=None,\n              scope=None):\n  with tf.variable_scope(\n      scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    with slim.arg_scope(\n        [slim.conv2d, bottleneck, resnet_utils.stack_blocks_dense],\n        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64//filter_scale, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = math_ops.reduce_mean(net, [1, 2], name=\'pool5\', keepdims=True)\n        if num_classes is not None:\n          net = slim.conv2d(\n              net,\n              num_classes, [1, 1],\n              activation_fn=None,\n              normalizer_fn=None,\n              scope=\'logits\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n          end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(\n              net, scope=\'predictions\')\n        return net, end_points\nresnet_v1.default_image_size = 224\n\n\ndef dilated_resnet_v1_50(inputs,\n                         filter_scale=1.0,\n                         mid_downsample=False,\n                         num_classes=None,\n                         is_training=True,\n                         global_pool=True,\n                         output_stride=None,\n                         reuse=None,\n                         scope=\'resnet_v1_50\'):\n  """"""Resnet v1 50 variant with dilations.\n\n  This variant modifies the first convolution layer of ResNet-v1-50. In\n  particular, it changes the original several of the last ResNet blocks\n  to include dilated convolutions as used in ICNet.\n  """"""\n\n  blocks = [\n    resnet_v1_block(\'block1\', base_depth=64/filter_scale,\n                    num_units=3, stride=2)]\n\n  if mid_downsample:\n    blocks.append(\n      resnet_v1_downsample_block(\'downsample_block\', factor=0.5))\n\n  blocks += [\n    resnet_v1_block(\'block2\', base_depth=128/filter_scale,\n                    num_units=4, stride=2),\n    resnet_v1_block(\'block3\', base_depth=256/filter_scale,\n                    num_units=6, stride=2, rate=2),\n    resnet_v1_block(\'block4\', base_depth=512/filter_scale,\n                    num_units=3, stride=1, rate=4)]\n\n  return resnet_v1(inputs, blocks, filter_scale, num_classes, is_training,\n                   global_pool=global_pool,output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n'"
third_party/mem_util.py,2,"b'# Utilities to figure out memory usage of run call\n#\n# Usage:\n#   import mem_util\n#   run_metadata = tf.RunMetadata()\n#   options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n#   sess.run(tensor, options=options, run_metadata=run_metadata)\n#   print(mem_util.peak_memory(run_metadata))\n#\n#  To print memory usage for particular device:\n#   print(mem_util.peak_memory(run_metadata)[""/gpu:0""])\n\n\n# Developer notes:\n# RunMetadata\n# https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/protobuf/config.proto#L349\n#\n# StepStats (run_metadata.step_stats)\n# https://github.com/tensorflow/tensorflow/blob/a2d9b3bf5f9e96bf459074d079b01e1c74b25afa/tensorflow/core/framework/step_stats.proto\n#\n# NodeExecStats (run_metadata.step_stats.dev_stats[0].step_stats[0])\n# https://github.com/tensorflow/tensorflow/blob/a2d9b3bf5f9e96bf459074d079b01e1c74b25afa/tensorflow/core/framework/step_stats.proto#L52\n\n\n# Note, there are several methods of tracking memory allocation. There\'s\n# requested bytes, and allocated bytes. Allocator may choose to give more bytes\n# than is requested. Currently allocated_bytes is used to give more realistic\n# results\n#\n# allocation_description {\n#   requested_bytes: 1000000\n#   allocated_bytes: 1000192\n#   allocator_name: ""GPU_0_bfc""\n#\n#\n# There\'s also additional field in NodeExecStats which tracks allocator state\n# node_stats {\n#   node_name: ""Bn_1/value""\n#   all_start_micros: 1512081861177033\n#   op_start_rel_micros: 1\n#   op_end_rel_micros: 3\n#   all_end_rel_micros: 5\n#   memory {\n#     allocator_name: ""GPU_0_bfc""\n#     allocator_bytes_in_use: 3072\n#   }\n#\n# Additionally one could use LOG_MEMORY messages to get memory allocation info\n# See multiple_memory_obtain_test.py for details on using these additional\n# methods\n\ndef peak_memory(run_metadata):\n  """"""Return dictionary of peak memory usage (bytes) for each device.\n\n  {""cpu:0"": 20441, ...\n  """"""\n  \n  assert run_metadata != None\n  assert hasattr(run_metadata, ""step_stats"")\n  assert hasattr(run_metadata.step_stats, ""dev_stats"")\n  \n  dev_stats = run_metadata.step_stats.dev_stats\n  result = {}\n  for dev_stat in dev_stats:\n    device_name = _simplify_device_name(dev_stat.device)\n    result[device_name] = _peak_from_nodestats(dev_stat.node_stats)\n  return result\n\n\ndef _timeline_from_nodestats(nodestats):\n  """"""Return sorted memory allocation records from list of nodestats\n  [NodeExecStats, NodeExecStats...], it\'s the\n  run_metadata.step_stats.dev_stats[0].step_stats object.\n\n  Timeline looks like this:\n\n timestamp         nodename  mem delta, allocator name\n [1509481813012895, \'concat\', 1000496, \'cpu\'],\n [1509481813012961, \'a04\', -1000000, \'cpu\'],\n [1509481813012968, \'TanhGrad\', 0, \'cpu\'], \n\n  0 memory allocation is reported for nodes without allocation_records\n  """"""\n  \n  lines = []\n  if not nodestats:\n    return []\n  for node in nodestats:\n    for mem in node.memory:  # can have both cpu and gpu allocator for op\n      try:\n        records = mem.allocation_records\n      except:\n        records = []\n      allocator = mem.allocator_name\n      if len(records)>0:\n        for record in records:\n          line = [record.alloc_micros, node.node_name, record.alloc_bytes,\n                  allocator]\n          lines.append(line)\n      else:\n        output_bytes = -1\n        try:\n          output_bytes = node.output[0].tensor_description.allocation_description.requested_bytes\n        except:\n          pass\n        line = [node.all_start_micros, node.node_name, 0, ""unknown""]\n        lines.append(line)\n  def first_key(x): return x[0]\n  return sorted(lines, key=first_key)\n\n# todo: get rid of ""timeline_from_nodestats""\n\ndef _position_of_largest(my_list):\n  """"""Return index of largest entry """"""\n  import operator\n  index, value = max(enumerate(my_list), key=operator.itemgetter(1))\n  return index\n\ndef _peak_from_nodestats(nodestats):\n  """"""Given a list of NodeExecStats messages, construct memory timeline.""""""\n  \n  timeline = _timeline_from_nodestats(nodestats)\n  timestamps = []\n  data = []\n\n  total_memory = 0\n  peak_memory = total_memory\n  for record in timeline:\n    timestamp, kernel_name, allocated_bytes, allocator_type = record\n    allocated_bytes = int(allocated_bytes)\n    total_memory += allocated_bytes\n    peak_memory = max(total_memory, peak_memory)\n  return peak_memory\n\n\ndef _print_parsed_timeline(timeline, gpu_only=False, ignore_less_than_bytes=0):\n  """"""pretty print parsed memory timeline.""""""\n\n  total_memory = 0\n  timestamps = []\n  data = []\n  first_timestamp = timeline[0][0]\n  for record in timeline:\n    timestamp, kernel_name, allocated_bytes, allocator_type = record\n    allocated_bytes = int(allocated_bytes)\n        \n    if abs(allocated_bytes)<ignore_less_than_bytes:\n      continue  # ignore small allocations\n        \n    total_memory += allocated_bytes\n    print(""%6d %10d %10d %s""%(timestamp-first_timestamp, total_memory,\n                              allocated_bytes, kernel_name))\n\n\ndef _simplify_device_name(device):\n  """"""/job:localhost/replica:0/task:0/device:CPU:0 -> /cpu:0""""""\n\n  prefix = \'/job:localhost/replica:0/task:0/device:\'\n  if device.startswith(prefix):\n    device = \'/\'+device[len(prefix):]\n  return device.lower()\n  \n\ndef _device_stats_dict(run_metadata):\n  """"""Returns dictionary of device_name->[NodeExecStats, NodeExecStats...]""""""\n  result = {}\n  for dev_stat in run_metadata.step_stats.dev_stats:\n    device_name = _simplify_device_name(dev_stat.device)\n    result[device_name] = dev_stat.node_stats\n\n  return result\n\n\ndef print_memory_timeline(run_metadata, device=None):\n  """"""Human readable timeline of memory allocation/deallocation for given\n  device. If device is None, prints timeline of the device with highest memory\n  usage""""""\n  \n\n  if device is None:\n    peak_dict = peak_memory(run_metadata)\n    peak_pairs = list(peak_dict.items())\n    chosen_peak = _position_of_largest([peak for (dev, peak) in peak_pairs])\n    device = peak_pairs[chosen_peak][0]\n\n\n  device_metadata = _device_stats_dict(run_metadata)\n  print(""Printing timeline for ""+device)\n  parsed_timeline = _timeline_from_nodestats(device_metadata[device])\n  _print_parsed_timeline(parsed_timeline)\n'"
third_party/memory_saving_gradients.py,13,"b'from toposort import toposort\nimport contextlib\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.graph_editor as ge\nimport time\nimport sys\nsys.setrecursionlimit(10000)\n# refers back to current module if we decide to split helpers out\nutil = sys.modules[__name__]\n\n# getting rid of ""WARNING:tensorflow:VARIABLES collection name is deprecated""\nsetattr(tf.GraphKeys, ""VARIABLES"", ""variables"")\n\n# save original gradients since tf.gradient could be monkey-patched to point\n# to our version\nfrom tensorflow.python.ops import gradients as tf_gradients_lib\ntf_gradients = tf_gradients_lib.gradients\n\nMIN_CHECKPOINT_NODE_SIZE=1024    # use lower value during testing\n\n# specific versions we can use to do process-wide replacement of tf.gradients\ndef gradients_speed(ys, xs, grad_ys=None, **kwargs):\n    return gradients(ys, xs, grad_ys, checkpoints=\'speed\', **kwargs)\n\ndef gradients_memory(ys, xs, grad_ys=None, **kwargs):\n    return gradients(ys, xs, grad_ys, checkpoints=\'memory\', **kwargs)\n        \ndef gradients_collection(ys, xs, grad_ys=None, **kwargs):\n    return gradients(ys, xs, grad_ys, checkpoints=\'collection\', **kwargs)\n\ndef gradients(ys, xs, grad_ys=None, checkpoints=\'collection\', **kwargs):\n    \'\'\'\n    Authors: Tim Salimans & Yaroslav Bulatov\n\n    memory efficient gradient implementation inspired by ""Training Deep Nets with Sublinear Memory Cost""\n    by Chen et al. 2016 (https://arxiv.org/abs/1604.06174)\n\n    ys,xs,grad_ys,kwargs are the arguments to standard tensorflow tf.gradients\n    (https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#gradients)\n\n    \'checkpoints\' can either be\n        - a list consisting of tensors from the forward pass of the neural net\n          that we should re-use when calculating the gradients in the backward pass\n          all other tensors that do not appear in this list will be re-computed\n        - a string specifying how this list should be determined. currently we support\n            - \'speed\':  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,\n                        so checkpointing them maximizes the running speed\n                        (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)\n            - \'memory\': try to minimize the memory usage\n                        (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)\n            - \'collection\': look for a tensorflow collection named \'checkpoints\', which holds the tensors to checkpoint\n    \'\'\'\n\n    #    print(""Calling memsaving gradients with"", checkpoints)\n    if not isinstance(ys,list):\n        ys = [ys]\n    if not isinstance(xs,list):\n        xs = [xs]\n\n    bwd_ops = ge.get_backward_walk_ops([y.op for y in ys],\n                                       inclusive=True)\n\n    debug_print(""bwd_ops: %s"", bwd_ops)\n    \n    # forward ops are all ops that are candidates for recomputation\n    fwd_ops = ge.get_forward_walk_ops([x.op for x in xs],\n                                      inclusive=True,\n                                      within_ops=bwd_ops)\n    debug_print(""fwd_ops: %s"", fwd_ops)\n    \n    # exclude ops with no inputs\n    fwd_ops = [op for op in fwd_ops if op.inputs]\n\n    # don\'t recompute xs, remove variables\n    xs_ops = _to_ops(xs)\n    fwd_ops = [op for op in fwd_ops if not op in xs_ops]\n    fwd_ops = [op for op in fwd_ops if not \'/assign\' in op.name]\n    fwd_ops = [op for op in fwd_ops if not \'/Assign\' in op.name]\n    fwd_ops = [op for op in fwd_ops if not \'/read\' in op.name]\n    ts_all = ge.filter_ts(fwd_ops, True) # get the tensors\n    ts_all = [t for t in ts_all if \'/read\' not in t.name]\n    ts_all = set(ts_all) - set(xs) - set(ys)\n\n    # construct list of tensors to checkpoint during forward pass, if not\n    # given as input\n    if type(checkpoints) is not list:\n        if checkpoints == \'collection\':\n            checkpoints = tf.get_collection(\'checkpoints\')\n            \n        elif checkpoints == \'speed\':\n            # checkpoint all expensive ops to maximize running speed\n            checkpoints = ge.filter_ts_from_regex(fwd_ops, \'conv2d|Conv|MatMul\')\n            \n        elif checkpoints == \'memory\':\n\n            # remove very small tensors and some weird ops\n            def fixdims(t): # tf.Dimension values are not compatible with int, convert manually\n                try:\n                    return [int(e if e.value is not None else 64) for e in t]\n                except:\n                    return [0]  # unknown shape\n            ts_all = [t for t in ts_all if np.prod(fixdims(t.shape)) > MIN_CHECKPOINT_NODE_SIZE]\n            ts_all = [t for t in ts_all if \'L2Loss\' not in t.name]\n            ts_all = [t for t in ts_all if \'entropy\' not in t.name]\n            ts_all = [t for t in ts_all if \'FusedBatchNorm\' not in t.name]\n            ts_all = [t for t in ts_all if \'Switch\' not in t.name]\n            ts_all = [t for t in ts_all if \'dropout\' not in t.name]\n            # DV: FP16_FIX - need to add \'Cast\' layer here to make it work for FP16\n            ts_all = [t for t in ts_all if \'Cast\' not in t.name]\n\n            # filter out all tensors that are inputs of the backward graph\n            with util.capture_ops() as bwd_ops:\n                tf_gradients(ys, xs, grad_ys, **kwargs)\n\n            bwd_inputs = [t for op in bwd_ops for t in op.inputs]\n            # list of tensors in forward graph that is in input to bwd graph\n            ts_filtered = list(set(bwd_inputs).intersection(ts_all))\n            debug_print(""Using tensors %s"", ts_filtered)\n\n            # try two slightly different ways of getting bottlenecks tensors\n            # to checkpoint\n            for ts in [ts_filtered, ts_all]:\n\n                # get all bottlenecks in the graph\n                bottleneck_ts = []\n                for t in ts:\n                    b = set(ge.get_backward_walk_ops(t.op, inclusive=True, within_ops=fwd_ops))\n                    f = set(ge.get_forward_walk_ops(t.op, inclusive=False, within_ops=fwd_ops))\n                    # check that there are not shortcuts\n                    b_inp = set([inp for op in b for inp in op.inputs]).intersection(ts_all)\n                    f_inp = set([inp for op in f for inp in op.inputs]).intersection(ts_all)\n                    if not set(b_inp).intersection(f_inp) and len(b_inp)+len(f_inp) >= len(ts_all):\n                        bottleneck_ts.append(t)  # we have a bottleneck!\n                    else:\n                        debug_print(""Rejected bottleneck candidate and ops %s"", [t] + list(set(ts_all) - set(b_inp) - set(f_inp)))\n\n                # success? or try again without filtering?\n                if len(bottleneck_ts) >= np.sqrt(len(ts_filtered)): # yes, enough bottlenecks found!\n                    break\n\n            if not bottleneck_ts:\n                raise Exception(\'unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=""speed"".\')\n\n            # sort the bottlenecks\n            bottlenecks_sorted_lists = tf_toposort(bottleneck_ts, within_ops=fwd_ops)\n            sorted_bottlenecks = [t for ts in bottlenecks_sorted_lists for t in ts]\n\n            # save an approximately optimal number ~ sqrt(N)\n            N = len(ts_filtered)\n            if len(bottleneck_ts) <= np.ceil(np.sqrt(N)):\n                checkpoints = sorted_bottlenecks\n            else:\n                step = int(np.ceil(len(bottleneck_ts) / np.sqrt(N)))\n                checkpoints = sorted_bottlenecks[step::step]\n            \n        else:\n            raise Exception(\'%s is unsupported input for ""checkpoints""\' % (checkpoints,))\n\n    checkpoints = list(set(checkpoints).intersection(ts_all))\n\n    # at this point automatic selection happened and checkpoints is list of nodes\n    assert isinstance(checkpoints, list)\n\n    debug_print(""Checkpoint nodes used: %s"", checkpoints)\n    # better error handling of special cases\n    # xs are already handled as checkpoint nodes, so no need to include them\n    xs_intersect_checkpoints = set(xs).intersection(set(checkpoints))\n    if xs_intersect_checkpoints:\n        debug_print(""Warning, some input nodes are also checkpoint nodes: %s"",\n                    xs_intersect_checkpoints)\n    ys_intersect_checkpoints = set(ys).intersection(set(checkpoints))\n    debug_print(""ys: %s, checkpoints: %s, intersect: %s"", ys, checkpoints,\n                ys_intersect_checkpoints)\n    # saving an output node (ys) gives no benefit in memory while creating\n    # new edge cases, exclude them\n    if ys_intersect_checkpoints:\n        debug_print(""Warning, some output nodes are also checkpoints nodes: %s"",\n              format_ops(ys_intersect_checkpoints))\n\n    # remove initial and terminal nodes from checkpoints list if present\n    checkpoints = list(set(checkpoints) - set(ys) - set(xs))\n    \n    # check that we have some nodes to checkpoint\n    if not checkpoints:\n        raise Exception(\'no checkpoints nodes found or given as input! \')\n\n    # disconnect dependencies between checkpointed tensors\n    checkpoints_disconnected = {}\n    for x in checkpoints:\n        if x.op and x.op.name is not None:\n            grad_node = tf.stop_gradient(x, name=x.op.name+""_sg"")\n        else:\n            grad_node = tf.stop_gradient(x)\n        checkpoints_disconnected[x] = grad_node\n\n    # partial derivatives to the checkpointed tensors and xs\n    ops_to_copy = fast_backward_ops(seed_ops=[y.op for y in ys],\n                                    stop_at_ts=checkpoints, within_ops=fwd_ops)\n    debug_print(""Found %s ops to copy within fwd_ops %s, seed %s, stop_at %s"",\n                    len(ops_to_copy), fwd_ops, [r.op for r in ys], checkpoints)\n    debug_print(""ops_to_copy = %s"", ops_to_copy)\n    debug_print(""Processing list %s"", ys)\n    copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops_to_copy), {})\n    for origin_op, op in info._transformed_ops.items():\n        op._set_device(origin_op.node_def.device)\n    copied_ops = info._transformed_ops.values()\n    debug_print(""Copied %s to %s"", ops_to_copy, copied_ops)\n    ge.reroute_ts(checkpoints_disconnected.values(), checkpoints_disconnected.keys(), can_modify=copied_ops)\n    debug_print(""Rewired %s in place of %s restricted to %s"",\n                checkpoints_disconnected.values(), checkpoints_disconnected.keys(), copied_ops)\n\n    # get gradients with respect to current boundary + original x\'s\n    copied_ys = [info._transformed_ops[y.op]._outputs[0] for y in ys]\n    boundary = list(checkpoints_disconnected.values())\n    dv = tf_gradients(ys=copied_ys, xs=boundary+xs, grad_ys=grad_ys, **kwargs)\n    debug_print(""Got gradients %s"", dv)\n    debug_print(""for %s"", copied_ys)\n    debug_print(""with respect to %s"", boundary+xs)\n\n    inputs_to_do_before = [y.op for y in ys]\n    if grad_ys is not None:\n        inputs_to_do_before += grad_ys\n    wait_to_do_ops = list(copied_ops) + [g.op for g in dv if g is not None]\n    my_add_control_inputs(wait_to_do_ops, inputs_to_do_before)\n\n    # partial derivatives to the checkpointed nodes\n    # dictionary of ""node: backprop"" for nodes in the boundary\n    d_checkpoints = {r: dr for r,dr in zip(checkpoints_disconnected.keys(),\n                                        dv[:len(checkpoints_disconnected)])}\n    # partial derivatives to xs (usually the params of the neural net)\n    d_xs = dv[len(checkpoints_disconnected):]\n\n    # incorporate derivatives flowing through the checkpointed nodes\n    checkpoints_sorted_lists = tf_toposort(checkpoints, within_ops=fwd_ops)\n    for ts in checkpoints_sorted_lists[::-1]:\n        debug_print(""Processing list %s"", ts)\n        checkpoints_other = [r for r in checkpoints if r not in ts]\n        checkpoints_disconnected_other = [checkpoints_disconnected[r] for r in checkpoints_other]\n\n        # copy part of the graph below current checkpoint node, stopping at\n        # other checkpoints nodes\n        ops_to_copy = fast_backward_ops(within_ops=fwd_ops, seed_ops=[r.op for r in ts], stop_at_ts=checkpoints_other)\n        debug_print(""Found %s ops to copy within %s, seed %s, stop_at %s"",\n                    len(ops_to_copy), fwd_ops, [r.op for r in ts],\n                    checkpoints_other)\n        debug_print(""ops_to_copy = %s"", ops_to_copy)\n        if not ops_to_copy: # we\'re done!\n            break\n        copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops_to_copy), {})\n        for origin_op, op in info._transformed_ops.items():\n            op._set_device(origin_op.node_def.device)\n        copied_ops = info._transformed_ops.values()\n        debug_print(""Copied %s to %s"", ops_to_copy, copied_ops)\n        ge.reroute_ts(checkpoints_disconnected_other, checkpoints_other, can_modify=copied_ops)\n        debug_print(""Rewired %s in place of %s restricted to %s"",\n                    checkpoints_disconnected_other, checkpoints_other, copied_ops)\n\n        # gradient flowing through the checkpointed node\n        boundary = [info._transformed_ops[r.op]._outputs[0] for r in ts]\n        substitute_backprops = [d_checkpoints[r] for r in ts]\n        dv = tf_gradients(boundary,\n                          checkpoints_disconnected_other+xs,\n                          grad_ys=substitute_backprops, **kwargs)\n        debug_print(""Got gradients %s"", dv)\n        debug_print(""for %s"", boundary)\n        debug_print(""with respect to %s"", checkpoints_disconnected_other+xs)\n        debug_print(""with boundary backprop substitutions %s"", substitute_backprops)\n\n        inputs_to_do_before = [d_checkpoints[r].op for r in ts]\n        wait_to_do_ops = list(copied_ops) + [g.op for g in dv if g is not None]\n        my_add_control_inputs(wait_to_do_ops, inputs_to_do_before)\n\n        # partial derivatives to the checkpointed nodes\n        for r, dr in zip(checkpoints_other, dv[:len(checkpoints_other)]):\n            if dr is not None:\n                if d_checkpoints[r] is None:\n                    d_checkpoints[r] = dr\n                else:\n                    d_checkpoints[r] += dr\n        def _unsparsify(x):\n            if not isinstance(x, tf.IndexedSlices):\n                return x\n            assert x.dense_shape is not None, ""memory_saving_gradients encountered sparse gradients of unknown shape""\n            indices = x.indices\n            while indices.shape.ndims < x.values.shape.ndims:\n                indices = tf.expand_dims(indices, -1)\n            return tf.scatter_nd(indices, x.values, x.dense_shape)\n\n        # partial derivatives to xs (usually the params of the neural net)\n        d_xs_new = dv[len(checkpoints_other):]\n        for j in range(len(xs)):\n            if d_xs_new[j] is not None:\n                if d_xs[j] is None:\n                    d_xs[j] = _unsparsify(d_xs_new[j])\n                else:\n                    d_xs[j] += _unsparsify(d_xs_new[j])\n\n\n    return d_xs\n\ndef tf_toposort(ts, within_ops=None):\n    all_ops = ge.get_forward_walk_ops([x.op for x in ts], within_ops=within_ops)\n\n    deps = {}\n    for op in all_ops:\n        for o in op.outputs:\n            deps[o] = set(op.inputs)\n    sorted_ts = toposort(deps)\n\n    # only keep the tensors from our original list\n    ts_sorted_lists = []\n    for l in sorted_ts:\n        keep = list(set(l).intersection(ts))\n        if keep:\n            ts_sorted_lists.append(keep)\n\n    return ts_sorted_lists\n\ndef fast_backward_ops(within_ops, seed_ops, stop_at_ts):\n    bwd_ops = set(ge.get_backward_walk_ops(seed_ops, stop_at_ts=stop_at_ts))\n    ops = bwd_ops.intersection(within_ops).difference([t.op for t in stop_at_ts])\n    return list(ops)\n\n@contextlib.contextmanager\ndef capture_ops():\n  """"""Decorator to capture ops created in the block.\n  with capture_ops() as ops:\n    # create some ops\n  print(ops) # => prints ops created.\n  """"""\n\n  micros = int(time.time()*10**6)\n  scope_name = str(micros)\n  op_list = []\n  with tf.name_scope(scope_name):\n    yield op_list\n\n  g = tf.get_default_graph()\n  op_list.extend(ge.select_ops(scope_name+""/.*"", graph=g))\n\ndef _to_op(tensor_or_op):\n  if hasattr(tensor_or_op, ""op""):\n    return tensor_or_op.op\n  return tensor_or_op\n\ndef _to_ops(iterable):\n  if not _is_iterable(iterable):\n    return iterable\n  return [_to_op(i) for i in iterable]\n\ndef _is_iterable(o):\n  try:\n    _ = iter(o)\n  except Exception:\n    return False\n  return True\n\nDEBUG_LOGGING=False\ndef debug_print(s, *args):\n  """"""Like logger.log, but also replaces all TensorFlow ops/tensors with their\n  names. Sensitive to value of DEBUG_LOGGING, see enable_debug/disable_debug\n\n  Usage:\n    debug_print(""see tensors %s for %s"", tensorlist, [1,2,3])\n  """"""\n\n  if DEBUG_LOGGING:\n    formatted_args = [format_ops(arg) for arg in args]\n    print(""DEBUG ""+s % tuple(formatted_args))\n\ndef format_ops(ops, sort_outputs=True):\n  """"""Helper method for printing ops. Converts Tensor/Operation op to op.name,\n  rest to str(op).""""""\n    \n  if hasattr(ops, \'__iter__\') and not isinstance(ops, str):\n    l = [(op.name if hasattr(op, ""name"") else str(op)) for op in ops]\n    if sort_outputs:\n      return sorted(l)\n    return l\n  else:\n    return ops.name if hasattr(ops, ""name"") else str(ops)\n\ndef my_add_control_inputs(wait_to_do_ops, inputs_to_do_before):\n    for op in wait_to_do_ops:\n        ci = [i for i in inputs_to_do_before if op.control_inputs is None or i not in op.control_inputs]\n        ge.add_control_inputs(op, ci)\n'"
third_party/memory_saving_gradients_patch.py,0,"b'r""""""Monkey patch for using memory saving gradients package from OpenAI""""""\nfrom tensorflow.python.ops import gradients\nfrom third_party import memory_saving_gradients\n\nCHECKPOINT_TYPES = ""collection""\n\nTF_GRADIENTS_KEY = ""gradients""\n\ndef gradients_memory(ys, xs, grad_ys=None, **kwargs):\n    return memory_saving_gradients.gradients(\n        ys, xs, grad_ys, checkpoints=CHECKPOINT_TYPES, **kwargs)\ngradients.__dict__[TF_GRADIENTS_KEY] = gradients_memory\n'"
third_party/mobilenet.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Mobilenet Base Class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport collections\nimport contextlib\nimport copy\nimport os\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef apply_activation(x, name=None, activation_fn=None):\n  return activation_fn(x, name=name) if activation_fn else x\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\n@contextlib.contextmanager\ndef _set_arg_scope_defaults(defaults):\n  """"""Sets arg scope defaults for all items present in defaults.\n\n  Args:\n    defaults: dictionary/list of pairs, containing a mapping from\n    function to a dictionary of default args.\n\n  Yields:\n    context manager where all defaults are set.\n  """"""\n  if hasattr(defaults, \'items\'):\n    items = list(defaults.items())\n  else:\n    items = defaults\n  if not items:\n    yield\n  else:\n    func, default_arg = items[0]\n    with slim.arg_scope(func, **default_arg):\n      with _set_arg_scope_defaults(items[1:]):\n        yield\n\n\n@slim.add_arg_scope\ndef depth_multiplier(output_params,\n                     multiplier,\n                     divisible_by=8,\n                     min_depth=8,\n                     **unused_kwargs):\n  if \'num_outputs\' not in output_params:\n    return\n  d = output_params[\'num_outputs\']\n  output_params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\n                                                 min_depth)\n\n\n_Op = collections.namedtuple(\'Op\', [\'op\', \'params\', \'multiplier_func\'])\n\n\ndef op(opfunc, **params):\n  multiplier = params.pop(\'multiplier_transorm\', depth_multiplier)\n  return _Op(opfunc, params=params, multiplier_func=multiplier)\n\n\nclass NoOpScope(object):\n  """"""No-op context manager.""""""\n\n  def __enter__(self):\n    return None\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    return False\n\n\ndef safe_arg_scope(funcs, **kwargs):\n  """"""Returns `slim.arg_scope` with all None arguments removed.\n\n  Arguments:\n    funcs: Functions to pass to `arg_scope`.\n    **kwargs: Arguments to pass to `arg_scope`.\n\n  Returns:\n    arg_scope or No-op context manager.\n\n  Note: can be useful if None value should be interpreted as ""do not overwrite\n    this parameter value"".\n  """"""\n  filtered_args = {name: value for name, value in kwargs.items()\n                   if value is not None}\n  if filtered_args:\n    return slim.arg_scope(funcs, **filtered_args)\n  else:\n    return NoOpScope()\n\n\n@slim.add_arg_scope\ndef mobilenet_base(  # pylint: disable=invalid-name\n    inputs,\n    conv_defs,\n    multiplier=1.0,\n    final_endpoint=None,\n    output_stride=None,\n    use_explicit_padding=False,\n    scope=None,\n    is_training=False):\n  """"""Mobilenet base network.\n\n  Constructs a network from inputs to the given final endpoint. By default\n  the network is constructed in inference mode. To create network\n  in training mode use:\n\n  with slim.arg_scope(mobilenet.training_scope()):\n     logits, endpoints = mobilenet_base(...)\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    conv_defs: A list of op(...) layers specifying the net architecture.\n    multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    final_endpoint: The name of last layer, for early termination for\n    for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 1 or any even number, excluding\n      zero. Typical values are 8 (accurate fully convolutional mode), 16\n      (fast fully convolutional mode), and 32 (classification mode).\n\n      NOTE- output_stride relies on all consequent operators to support dilated\n      operators via ""rate"" parameter. This might require wrapping non-conv\n      operators to operate properly.\n\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional variable scope.\n    is_training: How to setup batch_norm and other ops. Note: most of the time\n      this does not need be set directly. Use mobilenet.training_scope() to set\n      up training instead. This parameter is here for backward compatibility\n      only. It is safe to set it to the value matching\n      training_scope(is_training=...). It is also safe to explicitly set\n      it to False, even if there is outer training_scope set to to training.\n      (The network will be built in inference mode). If this is set to None,\n      no arg_scope is added for slim.batch_norm\'s is_training parameter.\n\n  Returns:\n    tensor_out: output tensor.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  if multiplier <= 0:\n    raise ValueError(\'multiplier is not greater than zero.\')\n\n  # Set conv defs defaults and overrides.\n  conv_defs_defaults = conv_defs.get(\'defaults\', {})\n  conv_defs_overrides = conv_defs.get(\'overrides\', {})\n  if use_explicit_padding:\n    conv_defs_overrides = copy.deepcopy(conv_defs_overrides)\n    conv_defs_overrides[\n        (slim.conv2d, slim.separable_conv2d)] = {\'padding\': \'VALID\'}\n\n  if output_stride is not None:\n    if output_stride == 0 or (output_stride > 1 and output_stride % 2):\n      raise ValueError(\'Output stride must be None, 1 or a multiple of 2.\')\n\n  # a) Set the tensorflow scope\n  # b) set padding to default: note we might consider removing this\n  # since it is also set by mobilenet_scope\n  # c) set all defaults\n  # d) set all extra overrides.\n  with _scope_all(scope, default_scope=\'Mobilenet\'), \\\n      safe_arg_scope([slim.batch_norm], is_training=is_training), \\\n      _set_arg_scope_defaults(conv_defs_defaults), \\\n      _set_arg_scope_defaults(conv_defs_overrides):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    # Insert default parameters before the base scope which includes\n    # any custom overrides set in mobilenet.\n    end_points = {}\n    scopes = {}\n    for i, opdef in enumerate(conv_defs[\'spec\']):\n      params = dict(opdef.params)\n      opdef.multiplier_func(params, multiplier)\n      stride = params.get(\'stride\', 1)\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= stride\n      else:\n        layer_stride = stride\n        layer_rate = 1\n        current_stride *= stride\n      # Update params.\n      params[\'stride\'] = layer_stride\n      # Only insert rate to params if rate > 1.\n      if layer_rate > 1:\n        params[\'rate\'] = layer_rate\n      # Set padding\n      if use_explicit_padding:\n        if \'kernel_size\' in params:\n          net = _fixed_padding(net, params[\'kernel_size\'], layer_rate)\n        else:\n          params[\'use_explicit_padding\'] = True\n\n      end_point = \'layer_%d\' % (i + 1)\n      try:\n        net = opdef.op(net, **params)\n      except Exception:\n        print(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\n        raise\n      end_points[end_point] = net\n      scope = os.path.dirname(net.name)\n      scopes[scope] = end_point\n      if final_endpoint is not None and end_point == final_endpoint:\n        break\n\n    # Add all tensors that end with \'output\' to\n    # endpoints\n    for t in net.graph.get_operations():\n      scope = os.path.dirname(t.name)\n      bn = os.path.basename(t.name)\n      if scope in scopes and t.name.endswith(\'output\'):\n        end_points[scopes[scope] + \'/\' + bn] = t.outputs[0]\n    return net, end_points\n\n\n@contextlib.contextmanager\ndef _scope_all(scope, default_scope=None):\n  with tf.variable_scope(scope, default_name=default_scope) as s,\\\n       tf.name_scope(s.original_name_scope):\n    yield s\n\n\n@slim.add_arg_scope\ndef mobilenet(inputs,\n              num_classes=1001,\n              prediction_fn=slim.softmax,\n              reuse=None,\n              scope=\'Mobilenet\',\n              base_only=False,\n              **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_classes:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_classes\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n\ndef global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n  """"""Applies avg pool to produce 1x1 output.\n\n  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\n  baked in average pool which has better support across hardware.\n\n  Args:\n    input_tensor: input tensor\n    pool_op: pooling op (avg pool is default)\n  Returns:\n    a tensor batch_size x 1 x 1 x depth.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size = tf.convert_to_tensor(\n        [1, tf.shape(input_tensor)[1],\n         tf.shape(input_tensor)[2], 1])\n  else:\n    kernel_size = [1, shape[1], shape[2], 1]\n  output = pool_op(\n      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n  # Recover output shape, for unknown shape.\n  output.set_shape([None, 1, 1, None])\n  return output\n\n\ndef training_scope(is_training=True,\n                   weight_decay=0.00004,\n                   stddev=0.09,\n                   dropout_keep_prob=0.8,\n                   bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n     # the network created will be trainble with dropout/batch norm\n     # initialized appropriately.\n  Args:\n    is_training: if set to False this will ensure that all customizations are\n      set to non-training mode. This might be helpful for code that is reused\n      across both training/evaluation, but most of the time training_scope with\n      value False is not needed. If this is set to None, the parameters is not\n      added to the batch_norm arg_scope.\n\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: Standard deviation for initialization, if negative uses xavier.\n    dropout_keep_prob: dropout keep probability (not set if equals to None).\n    bn_decay: decay for the batch norm moving averages (not set if equals to\n      None).\n\n  Returns:\n    An argument scope to use via arg_scope.\n  """"""\n  # Note: do not introduce parameters that would change the inference\n  # model here (for example whether to use bias), modify conv_def instead.\n  batch_norm_params = {\n      \'decay\': bn_decay,\n      \'is_training\': is_training\n  }\n  if stddev < 0:\n    weight_intitializer = slim.initializers.xavier_initializer()\n  else:\n    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n      weights_initializer=weight_intitializer,\n      normalizer_fn=slim.batch_norm), \\\n      slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training),\\\n      safe_arg_scope([slim.batch_norm], **batch_norm_params), \\\n      safe_arg_scope([slim.dropout], is_training=is_training,\n                     keep_prob=dropout_keep_prob), \\\n      slim.arg_scope([slim.conv2d], \\\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\n      slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\n    return s\n'"
third_party/mobilenet_v2.py,7,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of Mobilenet V2.\n\nArchitecture: https://arxiv.org/abs/1801.04381\n\nThe base model gives 72.2% accuracy on ImageNet, with 300MMadds,\n3.4 M parameters.\n\nHACK(oandrien): Code branched out from slim/nets/mobilenet/mobilenet_v2.py, and please refer to it for more details. A filter_scale option and interp layer have been added\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport functools\n\nimport tensorflow as tf\n\nfrom . import conv_blocks as ops\nfrom . import mobilenet as lib\n\nslim = tf.contrib.slim\nop = lib.op\n\nexpand_input = ops.expand_input_by_factor\n\n@slim.add_arg_scope\ndef downsample(inputs, s_factor, stride=1, rate=1, scope=None):\n  """"""""""""\n  with tf.variable_scope(scope, \'Interp\', [inputs]) as sc:\n    _, input_h, input_w, _ = inputs.get_shape().as_list()\n    shrink_h = (input_h-1)*s_factor+1\n    shrink_w = (input_w-1)*s_factor+1\n    return tf.image.resize_bilinear(inputs,\n                                    [int(shrink_h), int(shrink_w)],\n                                    align_corners=True)\n\n# pyformat: disable\n# Architecture: https://arxiv.org/abs/1801.04381\ndef make_conv_defs(filter_scale=1.0, mid_downsample=True):\n  spec = [\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16),\n        op(ops.expanded_conv, stride=2, num_outputs=24/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=24/filter_scale),\n        op(ops.expanded_conv, stride=2, num_outputs=32/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=32/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=32/filter_scale),\n        op(ops.expanded_conv, stride=2, num_outputs=64/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=64/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=64/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=64/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=96/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=96/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=96/filter_scale),\n        op(ops.expanded_conv, stride=2, num_outputs=160/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=160/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=160/filter_scale),\n        op(ops.expanded_conv, stride=1, num_outputs=320/filter_scale),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)]\n  if mid_downsample:\n    spec.insert(5, op(downsample, s_factor=0.5))\n  conv_defs = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=spec,\n  )\n  return conv_defs\n\nV2_DEF = make_conv_defs()\n# pyformat: enable\n\n\n@slim.add_arg_scope\ndef mobilenet(input_tensor,\n              num_classes=1001,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              activation_fn=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n\n  Inference mode is created by default. To create training use training_scope\n  below.\n\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    activation_fn: Activation function to use, defaults to tf.nn.relu6 if not\n      specified.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediction_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  if \'multiplier\' in kwargs:\n    raise ValueError(\'mobilenetv2 doesn\\\'t support generic \'\n                     \'multiplier parameter use ""depth_multiplier"" instead.\')\n  if finegrain_classification_mode:\n    conv_defs = copy.deepcopy(conv_defs)\n    if depth_multiplier < 1:\n      conv_defs[\'spec\'][-1].params[\'num_outputs\'] /= depth_multiplier\n  if activation_fn:\n    conv_defs = copy.deepcopy(conv_defs)\n    defaults = conv_defs[\'defaults\']\n    conv_defaults = (\n        defaults[(slim.conv2d, slim.fully_connected, slim.separable_conv2d)])\n    conv_defaults[\'activation_fn\'] = activation_fn\n\n  depth_args = {}\n  # NB: do not set depth_args unless they are provided to avoid overriding\n  # whatever default depth_multiplier might have thanks to arg_scope.\n  if min_depth is not None:\n    depth_args[\'min_depth\'] = min_depth\n  if divisible_by is not None:\n    depth_args[\'divisible_by\'] = divisible_by\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet(\n        input_tensor,\n        num_classes=num_classes,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\nmobilenet.default_image_size = 224\n\n\ndef wrapped_partial(func, *args, **kwargs):\n  partial_func = functools.partial(func, *args, **kwargs)\n  functools.update_wrapper(partial_func, func)\n  return partial_func\n\n\n# Wrappers for mobilenet v2 with depth-multipliers. Be noticed that\n# \'finegrain_classification_mode\' is set to True, which means the embedding\n# layer will not be shrinked when given a depth-multiplier < 1.0.\nmobilenet_v2_140 = wrapped_partial(mobilenet, depth_multiplier=1.4)\nmobilenet_v2_050 = wrapped_partial(mobilenet, depth_multiplier=0.50,\n                                   finegrain_classification_mode=True)\nmobilenet_v2_035 = wrapped_partial(mobilenet, depth_multiplier=0.35,\n                                   finegrain_classification_mode=True)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):\n  """"""Creates base of the mobilenet (no pooling and no logits) .""""""\n  return mobilenet(input_tensor,\n                   depth_multiplier=depth_multiplier,\n                   base_only=True, **kwargs)\n\n\ndef training_scope(**kwargs):\n  """"""Defines MobilenetV2 training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  with slim.\n\n  Args:\n    **kwargs: Passed to mobilenet.training_scope. The following parameters\n    are supported:\n      weight_decay- The weight decay to use for regularizing the model.\n      stddev-  Standard deviation for initialization, if negative uses xavier.\n      dropout_keep_prob- dropout keep probability\n      bn_decay- decay for the batch norm moving averages.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  return lib.training_scope(**kwargs)\n\n\n__all__ = [\'training_scope\', \'mobilenet_base\', \'mobilenet\', \'V2_DEF\']\n'"
third_party/model_deploy.py,54,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\n\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\n\nUsage:\n\n  g = tf.Graph()\n\n  # Set up DeploymentConfig\n  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n\n  # Create the global step on the device storing the variables.\n  with tf.device(config.variables_device()):\n    global_step = slim.create_global_step()\n\n  # Define the inputs\n  with tf.device(config.inputs_device()):\n    images, labels = LoadData(...)\n    inputs_queue = slim.data.prefetch_queue((images, labels))\n\n  # Define the optimizer.\n  with tf.device(config.optimizer_device()):\n    optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n  # Define the model including the loss.\n  def model_fn(inputs_queue):\n    images, labels = inputs_queue.dequeue()\n    predictions = CreateNetwork(images)\n    slim.losses.log_loss(predictions, labels)\n\n  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                 optimizer=optimizer)\n\n  # Run training.\n  slim.learning.train(model_dp.train_op, my_log_dir,\n                      summary_op=model_dp.summary_op)\n\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n  * outputs: The return values of the calls to `model_fn()`.\n  * scope: The scope used to create the clone.\n  * device: The device used to create the clone.\n\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n  * train_op: An operation that run the optimizer training op and include\n    all the update ops created by `model_fn`. Present only if an optimizer\n    was specified.\n  * summary_op: An operation that run the summaries created by `model_fn`\n    and process_gradients.\n  * total_loss: A `Tensor` that contains the sum of all losses created by\n    `model_fn` plus the regularization losses.\n  * clones: List of `Clone` tuples returned by `create_clones()`.\n\nDeploymentConfig parameters:\n  * num_clones: Number of model clones to deploy in each replica.\n  * clone_on_cpu: True if clones should be placed on CPU.\n  * replica_id: Integer.  Index of the replica for which the model is\n      deployed.  Usually 0 for the chief replica.\n  * num_replicas: Number of replicas to use.\n  * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n  * worker_job_name: A name for the worker job.\n  * ps_job_name: A name for the parameter server job.\n\nTODO(sguada):\n  - describe side effect to the graph.\n  - what happens to summaries and update_ops.\n  - which graph collections are altered.\n  - write a tutorial on how to use this.\n  - analyze the possibility of calling deploy more than once.\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\n__all__ = [\'create_clones\',\n           \'deploy\',\n           \'optimize_clones\',\n           \'DeployedModel\',\n           \'DeploymentConfig\',\n           \'Clone\',\n          ]\n\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\'Clone\',\n                               [\'outputs\',  # Whatever model_fn() returned.\n                                \'scope\',  # The scope used to create it.\n                                \'device\',  # The device used to create.\n                               ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\'DeployedModel\',\n                                       [\'train_op\',  # The `train_op`\n                                        \'summary_op\',  # The `summary_op`\n                                        \'total_loss\',  # The loss `Tensor`\n                                        \'clones\',  # A list of `Clones` tuples.\n                                       ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\'num_clones\': 1,\n                      \'clone_on_cpu\': False,\n                      \'replica_id\': 0,\n                      \'num_replicas\': 1,\n                      \'num_ps_tasks\': 0,\n                      \'worker_job_name\': \'worker\',\n                      \'ps_job_name\': \'ps\'}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n  """"""Creates multiple clones according to config using a `model_fn`.\n\n  The returned values of `model_fn(*args, **kwargs)` are collected along with\n  the scope and device used to created it in a namedtuple\n  `Clone(outputs, scope, device)`\n\n  Note: it is assumed that any loss created by `model_fn` is collected at\n  the tf.GraphKeys.LOSSES collection.\n\n  To recover the losses, summaries or update_ops created by the clone use:\n  ```python\n    losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n  ```\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A DeploymentConfig object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n\n  Returns:\n    A list of namedtuples `Clone`.\n  """"""\n  clones = []\n  args = args or []\n  kwargs = kwargs or {}\n  with slim.arg_scope([slim.model_variable, slim.variable],\n                      device=config.variables_device()):\n    # Create clones.\n    for i in range(0, config.num_clones):\n      with tf.name_scope(config.clone_scope(i)) as clone_scope:\n        clone_device = config.clone_device(i)\n        with tf.device(clone_device):\n          with tf.variable_scope(tf.get_variable_scope(),\n                                 reuse=True if i > 0 else None):\n            outputs = model_fn(*args, **kwargs)\n          clones.append(Clone(outputs, clone_scope, clone_device))\n  return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n  """"""Gather the loss for a single clone.\n\n  Args:\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n\n  Returns:\n    A tensor for the total loss for the clone.  Can be None.\n  """"""\n  # The return value.\n  sum_loss = None\n  # Individual components of the loss that will need summaries.\n  clone_loss = None\n  regularization_loss = None\n  # Compute and aggregate losses on the clone device.\n  with tf.device(clone.device):\n    all_losses = []\n    clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    if clone_losses:\n      clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n      if num_clones > 1:\n        clone_loss = tf.div(clone_loss, 1.0 * num_clones,\n                            name=\'scaled_clone_loss\')\n      all_losses.append(clone_loss)\n    if regularization_losses:\n      regularization_loss = tf.add_n(regularization_losses,\n                                     name=\'regularization_loss\')\n      all_losses.append(regularization_loss)\n    if all_losses:\n      sum_loss = tf.add_n(all_losses)\n  # Add the summaries out of the clone device block.\n  if clone_loss is not None:\n    tf.summary.scalar(\'/\'.join(filter(None,\n                                      [\'Losses\', clone.scope, \'clone_loss\'])),\n                      clone_loss)\n  if regularization_loss is not None:\n    tf.summary.scalar(\'Losses/regularization_loss\', regularization_loss)\n  return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                    **kwargs):\n  """"""Compute losses and gradients for a single clone.\n\n  Args:\n    optimizer: A tf.Optimizer  object.\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n    **kwargs: Dict of kwarg to pass to compute_gradients().\n\n  Returns:\n    A tuple (clone_loss, clone_grads_and_vars).\n      - clone_loss: A tensor for the total loss for the clone.  Can be None.\n      - clone_grads_and_vars: List of (gradient, variable) for the clone.\n        Can be empty.\n  """"""\n  sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n  clone_grad = None\n  if sum_loss is not None:\n    with tf.device(clone.device):\n      clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\n  return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer,\n                    regularization_losses=None,\n                    **kwargs):\n  """"""Compute clone losses and gradients for the given list of `Clones`.\n\n  Note: The regularization_losses are added to the first clone losses.\n\n  Args:\n   clones: List of `Clones` created by `create_clones()`.\n   optimizer: An `Optimizer` object.\n   regularization_losses: Optional list of regularization losses. If None it\n     will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n     exclude them.\n   **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n\n  Returns:\n   A tuple (total_loss, grads_and_vars).\n     - total_loss: A Tensor containing the average of the clone losses including\n       the regularization loss.\n     - grads_and_vars: A List of tuples (gradient, variable) containing the sum\n       of the gradients for each variable.\n\n  """"""\n  grads_and_vars = []\n  clones_losses = []\n  num_clones = len(clones)\n  if regularization_losses is None:\n    regularization_losses = tf.get_collection(\n        tf.GraphKeys.REGULARIZATION_LOSSES)\n  for clone in clones:\n    with tf.name_scope(clone.scope):\n      clone_loss, clone_grad = _optimize_clone(\n          optimizer, clone, num_clones, regularization_losses, **kwargs)\n      if clone_loss is not None:\n        clones_losses.append(clone_loss)\n        grads_and_vars.append(clone_grad)\n      # Only use regularization_losses for the first clone\n      regularization_losses = None\n  # Compute the total_loss summing all the clones_losses.\n  total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n  # Sum the gradients across clones.\n  grads_and_vars = _sum_clones_gradients(grads_and_vars)\n  return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n  """"""Deploys a Slim-constructed model across multiple clones.\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n  the deployed model is configured for training with that optimizer.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A `DeploymentConfig` object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n    optimizer: Optional `Optimizer` object.  If passed the model is deployed\n      for training with that optimizer.\n    summarize_gradients: Whether or not add summaries to the gradients.\n\n  Returns:\n    A `DeployedModel` namedtuple.\n\n  """"""\n  # Gather initial summaries.\n  summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n  # Create Clones.\n  clones = create_clones(config, model_fn, args, kwargs)\n  first_clone = clones[0]\n\n  # Gather update_ops from the first clone. These contain, for example,\n  # the updates for the batch_norm variables created by model_fn.\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n  train_op = None\n  total_loss = None\n  with tf.device(config.optimizer_device()):\n    if optimizer:\n      # Place the global step on the device storing the variables.\n      with tf.device(config.variables_device()):\n        global_step = slim.get_or_create_global_step()\n\n      # Compute the gradients for the clones.\n      total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n      if clones_gradients:\n        if summarize_gradients:\n          # Add summaries to the gradients.\n          summaries |= set(_add_gradients_summaries(clones_gradients))\n\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                 global_step=global_step)\n        update_ops.append(grad_updates)\n\n        update_op = tf.group(*update_ops)\n        with tf.control_dependencies([update_op]):\n          train_op = tf.identity(total_loss, name=\'train_op\')\n    else:\n      clones_losses = []\n      regularization_losses = tf.get_collection(\n          tf.GraphKeys.REGULARIZATION_LOSSES)\n      for clone in clones:\n        with tf.name_scope(clone.scope):\n          clone_loss = _gather_clone_loss(clone, len(clones),\n                                          regularization_losses)\n          if clone_loss is not None:\n            clones_losses.append(clone_loss)\n          # Only use regularization_losses for the first clone\n          regularization_losses = None\n      if clones_losses:\n        total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone.scope))\n\n    if total_loss is not None:\n      # Add total_loss to summary.\n      summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n    if summaries:\n      # Merge all summaries together.\n      summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n    else:\n      summary_op = None\n\n  return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n  """"""Calculate the sum gradient for each shared variable across all clones.\n\n  This function assumes that the clone_grads has been scaled appropriately by\n  1 / num_clones.\n\n  Args:\n    clone_grads: A List of List of tuples (gradient, variable), one list per\n    `Clone`.\n\n  Returns:\n     List of tuples of (gradient, variable) where the gradient has been summed\n     across all clones.\n  """"""\n  sum_grads = []\n  for grad_and_vars in zip(*clone_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n    grads = []\n    var = grad_and_vars[0][1]\n    for g, v in grad_and_vars:\n      assert v == var\n      if g is not None:\n        grads.append(g)\n    if grads:\n      if len(grads) > 1:\n        sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n      else:\n        sum_grad = grads[0]\n      sum_grads.append((sum_grad, var))\n  return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n  """"""Add histogram summaries to gradients.\n\n  Note: The summaries are also added to the SUMMARIES collection.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n\n  Returns:\n    The _list_ of the added summaries for grads_and_vars.\n  """"""\n  summaries = []\n  for grad, var in grads_and_vars:\n    if grad is not None:\n      if isinstance(grad, tf.IndexedSlices):\n        grad_values = grad.values\n      else:\n        grad_values = grad\n      summaries.append(tf.summary.histogram(var.op.name + \':gradient\',\n                                            grad_values))\n      summaries.append(tf.summary.histogram(var.op.name + \':gradient_norm\',\n                                            tf.global_norm([grad_values])))\n    else:\n      tf.logging.info(\'Var %s has no gradient\', var.op.name)\n  return summaries\n\n\nclass DeploymentConfig(object):\n  """"""Configuration for deploying a model with `deploy()`.\n\n  You can pass an instance of this class to `deploy()` to specify exactly\n  how to deploy the model to build.  If you do not pass one, an instance built\n  from the default deployment_hparams will be used.\n  """"""\n\n  def __init__(self,\n               num_clones=1,\n               clone_on_cpu=False,\n               replica_id=0,\n               num_replicas=1,\n               num_ps_tasks=0,\n               worker_job_name=\'worker\',\n               ps_job_name=\'ps\'):\n    """"""Create a DeploymentConfig.\n\n    The config describes how to deploy a model across multiple clones and\n    replicas.  The model will be replicated `num_clones` times in each replica.\n    If `clone_on_cpu` is True, each clone will placed on CPU.\n\n    If `num_replicas` is 1, the model is deployed via a single process.  In that\n    case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n\n    If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n    must specify TensorFlow devices for the `worker` and `ps` jobs and\n    `num_ps_tasks` must be positive.\n\n    Args:\n      num_clones: Number of model clones to deploy in each replica.\n      clone_on_cpu: If True clones would be placed on CPU.\n      replica_id: Integer.  Index of the replica for which the model is\n        deployed.  Usually 0 for the chief replica.\n      num_replicas: Number of replicas to use.\n      num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n      worker_job_name: A name for the worker job.\n      ps_job_name: A name for the parameter server job.\n\n    Raises:\n      ValueError: If the arguments are invalid.\n    """"""\n    if num_replicas > 1:\n      if num_ps_tasks < 1:\n        raise ValueError(\'When using replicas num_ps_tasks must be positive\')\n    if num_replicas > 1 or num_ps_tasks > 0:\n      if not worker_job_name:\n        raise ValueError(\'Must specify worker_job_name when using replicas\')\n      if not ps_job_name:\n        raise ValueError(\'Must specify ps_job_name when using parameter server\')\n    if replica_id >= num_replicas:\n      raise ValueError(\'replica_id must be less than num_replicas\')\n    self._num_clones = num_clones\n    self._clone_on_cpu = clone_on_cpu\n    self._replica_id = replica_id\n    self._num_replicas = num_replicas\n    self._num_ps_tasks = num_ps_tasks\n    self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n    self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n  @property\n  def num_clones(self):\n    return self._num_clones\n\n  @property\n  def clone_on_cpu(self):\n    return self._clone_on_cpu\n\n  @property\n  def replica_id(self):\n    return self._replica_id\n\n  @property\n  def num_replicas(self):\n    return self._num_replicas\n\n  @property\n  def num_ps_tasks(self):\n    return self._num_ps_tasks\n\n  @property\n  def ps_device(self):\n    return self._ps_device\n\n  @property\n  def worker_device(self):\n    return self._worker_device\n\n  def caching_device(self):\n    """"""Returns the device to use for caching variables.\n\n    Variables are cached on the worker CPU when using replicas.\n\n    Returns:\n      A device string or None if the variables do not need to be cached.\n    """"""\n    if self._num_ps_tasks > 0:\n      return lambda op: op.device\n    else:\n      return None\n\n  def clone_device(self, clone_index):\n    """"""Device used to create the clone and all the ops inside the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A value suitable for `tf.device()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    if self._clone_on_cpu:\n      device += \'/device:CPU:0\'\n    else:\n      device += \'/device:GPU:%d\' % clone_index\n    return device\n\n  def clone_scope(self, clone_index):\n    """"""Name scope to create the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A name_scope suitable for `tf.name_scope()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    scope = \'\'\n    if self._num_clones > 1:\n      scope = \'clone_%d\' % clone_index\n    return scope\n\n  def optimizer_device(self):\n    """"""Device to use with the optimizer.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    if self._num_ps_tasks > 0 or self._num_clones > 0:\n      return self._worker_device + \'/device:CPU:0\'\n    else:\n      return \'\'\n\n  def inputs_device(self):\n    """"""Device to use to build the inputs.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    device += \'/device:CPU:0\'\n    return device\n\n  def variables_device(self):\n    """"""Returns the device to use for variables created inside the clone.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._ps_device\n    device += \'/device:CPU:0\'\n\n    class _PSDeviceChooser(object):\n      """"""Slim device chooser for variables when using PS.""""""\n\n      def __init__(self, device, tasks):\n        self._device = device\n        self._tasks = tasks\n        self._task = 0\n\n      def choose(self, op):\n        if op.device:\n          return op.device\n        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n        if node_def.op.startswith(\'Variable\'):\n          t = self._task\n          self._task = (self._task + 1) % self._tasks\n          d = \'%s/task:%d\' % (self._device, t)\n          return d\n        else:\n          return op.device\n\n    if not self._num_ps_tasks:\n      return device\n    else:\n      chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n      return chooser.choose\n'"
third_party/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       store_non_strided_activations=False,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    store_non_strided_activations: If True, we compute non-strided (undecimated)\n      activations at the last unit of each block and store them in the\n      `outputs_collections` before subsampling them. This gives us access to\n      higher resolution intermediate activations which are useful in some\n      dense prediction problems but increases 4x the computation and memory cost\n      at the last unit of each block.\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      block_stride = 1\n      for i, unit in enumerate(block.args):\n        if store_non_strided_activations and i == len(block.args) - 1:\n          # Move stride from the block\'s last unit to the end of the block.\n          block_stride = unit.get(\'stride\', 1)\n          unit = dict(unit, stride=1)\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n            if output_stride is not None and current_stride > output_stride:\n              raise ValueError(\'The target output_stride cannot be reached.\')\n\n      # Collect activations at the block\'s end before performing subsampling.\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n      # Subsampling of the block\'s output activations.\n      if output_stride is not None and current_stride == output_stride:\n        rate *= block_stride\n      else:\n        net = subsample(net, block_stride)\n        current_stride *= block_stride\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True,\n                     activation_fn=tf.nn.relu,\n                     use_batch_norm=True,\n                     batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n    activation_fn: The activation function which is used in ResNet.\n    use_batch_norm: Whether or not to use batch normalization.\n    batch_norm_updates_collections: Collection for the update ops for\n      batch norm.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': batch_norm_updates_collections,\n      \'fused\': None,  # Use fused batch norm if possible.\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=activation_fn,\n      normalizer_fn=slim.batch_norm if use_batch_norm else None,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
third_party/resnet_v1.py,9,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom . import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\nclass NoOpScope(object):\n  """"""No-op context manager.""""""\n\n  def __enter__(self):\n    return None\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    return False\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs,\n               depth,\n               depth_bottleneck,\n               stride,\n               rate=1,\n               outputs_collections=None,\n               scope=None,\n               use_bounded_activations=False):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n    use_bounded_activations: Whether or not to use bounded activations. Bounded\n      activations better lend themselves to quantized inference.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(\n          inputs,\n          depth, [1, 1],\n          stride=stride,\n          activation_fn=tf.nn.relu6 if use_bounded_activations else None,\n          scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    if use_bounded_activations:\n      # Use clip_by_value to simulate bandpass activation.\n      residual = tf.clip_by_value(residual, -6.0, 6.0)\n      output = tf.nn.relu6(shortcut + residual)\n    else:\n      output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.name,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              store_non_strided_activations=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks.\n      If 0 or None, we return the features before the logit layer.\n    is_training: whether batch_norm layers are in training mode. If this is set\n      to None, the callers can specify slim.batch_norm\'s is_training parameter\n      from an outer slim.arg_scope.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n        To use this parameter, the input images must be smaller than 300x300\n        pixels, in which case the output logit layer does not contain spatial\n        information and can be removed.\n    store_non_strided_activations: If True, we compute non-strided (undecimated)\n      activations at the last unit of each block and store them in the\n      `outputs_collections` before subsampling them. This gives us access to\n      higher resolution intermediate activations which are useful in some\n      dense prediction problems but increases 4x the computation and memory cost\n      at the last unit of each block.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is 0 or None,\n      then net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes a non-zero integer, net contains the\n      pre-softmax activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with (slim.arg_scope([slim.batch_norm], is_training=is_training)\n            if is_training is not None else NoOpScope()):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride,\n                                              store_non_strided_activations)\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n          end_points[\'global_pool\'] = net\n        if num_classes:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n          end_points[sc.name + \'/logits\'] = net\n          if spatial_squeeze:\n            net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n            end_points[sc.name + \'/spatial_squeeze\'] = net\n          end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n        return net, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 store_non_strided_activations=False,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  store_non_strided_activations=False,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  store_non_strided_activations=False,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  store_non_strided_activations=False,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
