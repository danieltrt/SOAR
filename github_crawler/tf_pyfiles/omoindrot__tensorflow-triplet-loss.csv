file_path,api_count,code
evaluate.py,5,"b'""""""Evaluate the model""""""\n\nimport argparse\nimport os\n\nimport tensorflow as tf\n\nfrom model.input_fn import test_input_fn\nfrom model.model_fn import model_fn\nfrom model.utils import Params\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\',\n                    help=""Experiment directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/mnist\',\n                    help=""Directory containing the dataset"")\n\n\nif __name__ == \'__main__\':\n    tf.reset_default_graph()\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Load the parameters\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Define the model\n    tf.logging.info(""Creating the model..."")\n    estimator = tf.estimator.Estimator(model_fn, params=params, model_dir=args.model_dir)\n\n    # Evaluate the model on the test set\n    tf.logging.info(""Evaluation on the test set."")\n    res = estimator.evaluate(lambda: test_input_fn(args.data_dir, params))\n    for key in res:\n        print(""{}: {}"".format(key, res[key]))\n'"
search_hyperparams.py,0,"b'""""""Peform hyperparameter search""""""\n\nimport argparse\nimport os\nfrom subprocess import check_call\nimport sys\n\nfrom model.utils import Params\n\n\nPYTHON = sys.executable\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments/learning_rate\',\n                    help=""Directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/mnist\',\n                    help=""Directory containing the dataset"")\n\n\ndef launch_training_job(parent_dir, data_dir, job_name, params):\n    """"""Launch training of the model with a set of hyperparameters in parent_dir/job_name\n\n    Args:\n        parent_dir: (string) directory containing config, weights and log\n        data_dir: (string) directory containing the dataset\n        params: (dict) containing hyperparameters\n    """"""\n    # Create a new folder in parent_dir with unique_name ""job_name""\n    model_dir = os.path.join(parent_dir, job_name)\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    # Write parameters in json file\n    json_path = os.path.join(model_dir, \'params.json\')\n    params.save(json_path)\n\n    # Launch training with this config\n    cmd = ""{python} train.py --model_dir {model_dir} --data_dir {data_dir}""\n    cmd = cmd.format(python=PYTHON, model_dir=model_dir, data_dir=data_dir)\n    print(cmd)\n    check_call(cmd, shell=True)\n\n\nif __name__ == ""__main__"":\n    # Load the ""reference"" parameters from parent_dir json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.parent_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Perform hypersearch over one parameter\n    learning_rates = [1e-4, 3e-4, 1e-3, 3e-3]\n\n    for learning_rate in learning_rates:\n        # Modify the relevant parameter in params\n        params.learning_rate = learning_rate\n\n        # Launch job (name has to be unique)\n        job_name = ""learning_rate_{}"".format(learning_rate)\n        launch_training_job(args.parent_dir, args.data_dir, job_name, params)\n'"
train.py,7,"b'""""""Train the model""""""\n\nimport argparse\nimport os\n\nimport tensorflow as tf\n\nfrom model.input_fn import train_input_fn\nfrom model.input_fn import test_input_fn\nfrom model.model_fn import model_fn\nfrom model.utils import Params\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\',\n                    help=""Experiment directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/mnist\',\n                    help=""Directory containing the dataset"")\n\n\nif __name__ == \'__main__\':\n    tf.reset_default_graph()\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Load the parameters from json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Define the model\n    tf.logging.info(""Creating the model..."")\n    config = tf.estimator.RunConfig(tf_random_seed=230,\n                                    model_dir=args.model_dir,\n                                    save_summary_steps=params.save_summary_steps)\n    estimator = tf.estimator.Estimator(model_fn, params=params, config=config)\n\n    # Train the model\n    tf.logging.info(""Starting training for {} epoch(s)."".format(params.num_epochs))\n    estimator.train(lambda: train_input_fn(args.data_dir, params))\n\n    # Evaluate the model on the test set\n    tf.logging.info(""Evaluation on test set."")\n    res = estimator.evaluate(lambda: test_input_fn(args.data_dir, params))\n    for key in res:\n        print(""{}: {}"".format(key, res[key]))\n'"
visualize_embeddings.py,12,"b'""""""Train the model""""""\n\nimport argparse\nimport os\nimport pathlib\nimport shutil\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\nimport model.mnist_dataset as mnist_dataset\nfrom model.utils import Params\nfrom model.input_fn import test_input_fn\nfrom model.model_fn import model_fn\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\',\n                    help=""Experiment directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/mnist\',\n                    help=""Directory containing the dataset"")\nparser.add_argument(\'--sprite_filename\', default=\'experiments/mnist_10k_sprite.png\',\n                    help=""Sprite image for the projector"")\n\n\nif __name__ == \'__main__\':\n    tf.reset_default_graph()\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Load the parameters from json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Define the model\n    tf.logging.info(""Creating the model..."")\n    config = tf.estimator.RunConfig(tf_random_seed=230,\n                                    model_dir=args.model_dir,\n                                    save_summary_steps=params.save_summary_steps)\n    estimator = tf.estimator.Estimator(model_fn, params=params, config=config)\n\n\n    # EMBEDDINGS VISUALIZATION\n\n    # Compute embeddings on the test set\n    tf.logging.info(""Predicting"")\n    predictions = estimator.predict(lambda: test_input_fn(args.data_dir, params))\n\n    # TODO (@omoindrot): remove the hard-coded 10000\n    embeddings = np.zeros((10000, params.embedding_size))\n    for i, p in enumerate(predictions):\n        embeddings[i] = p[\'embeddings\']\n\n    tf.logging.info(""Embeddings shape: {}"".format(embeddings.shape))\n\n    # Visualize test embeddings\n    embedding_var = tf.Variable(embeddings, name=\'mnist_embedding\')\n\n    eval_dir = os.path.join(args.model_dir, ""eval"")\n    summary_writer = tf.summary.FileWriter(eval_dir)\n\n    config = projector.ProjectorConfig()\n    embedding = config.embeddings.add()\n    embedding.tensor_name = embedding_var.name\n\n    # Specify where you find the sprite (we will create this later)\n    # Copy the embedding sprite image to the eval directory\n    shutil.copy2(args.sprite_filename, eval_dir)\n    embedding.sprite.image_path = pathlib.Path(args.sprite_filename).name\n    embedding.sprite.single_image_dim.extend([28, 28])\n\n    with tf.Session() as sess:\n        # TODO (@omoindrot): remove the hard-coded 10000\n        # Obtain the test labels\n        dataset = mnist_dataset.test(args.data_dir)\n        dataset = dataset.map(lambda img, lab: lab)\n        dataset = dataset.batch(10000)\n        labels_tensor = dataset.make_one_shot_iterator().get_next()\n        labels = sess.run(labels_tensor)\n\n    # Specify where you find the metadata\n    # Save the metadata file needed for Tensorboard projector\n    metadata_filename = ""mnist_metadata.tsv""\n    with open(os.path.join(eval_dir, metadata_filename), \'w\') as f:\n        for i in range(params.eval_size):\n            c = labels[i]\n            f.write(\'{}\\n\'.format(c))\n    embedding.metadata_path = metadata_filename\n\n    # Say that you want to visualise the embeddings\n    projector.visualize_embeddings(summary_writer, config)\n\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        sess.run(embedding_var.initializer)\n        saver.save(sess, os.path.join(eval_dir, ""embeddings.ckpt""))\n'"
model/__init__.py,0,b''
model/input_fn.py,1,"b'""""""Create the input data pipeline using `tf.data`""""""\n\nimport tensorflow as tf\n\nimport model.mnist_dataset as mnist_dataset\n\n\ndef train_input_fn(data_dir, params):\n    """"""Train input function for the MNIST dataset.\n\n    Args:\n        data_dir: (string) path to the data directory\n        params: (Params) contains hyperparameters of the model (ex: `params.num_epochs`)\n    """"""\n    dataset = mnist_dataset.train(data_dir)\n    dataset = dataset.shuffle(params.train_size)  # whole dataset into the buffer\n    dataset = dataset.repeat(params.num_epochs)  # repeat for multiple epochs\n    dataset = dataset.batch(params.batch_size)\n    dataset = dataset.prefetch(1)  # make sure you always have one batch ready to serve\n    return dataset\n\n\ndef test_input_fn(data_dir, params):\n    """"""Test input function for the MNIST dataset.\n\n    Args:\n        data_dir: (string) path to the data directory\n        params: (Params) contains hyperparameters of the model (ex: `params.num_epochs`)\n    """"""\n    dataset = mnist_dataset.test(data_dir)\n    dataset = dataset.batch(params.batch_size)\n    dataset = dataset.prefetch(1)  # make sure you always have one batch ready to serve\n    return dataset\n'"
model/mnist_dataset.py,17,"b'#  Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n""""""tf.data.Dataset interface to the MNIST dataset.""""""\n\nimport gzip\nimport os\nimport shutil\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\n\ndef read32(bytestream):\n    """"""Read 4 bytes from bytestream as an unsigned 32-bit integer.""""""\n    dt = np.dtype(np.uint32).newbyteorder(\'>\')\n    return np.frombuffer(bytestream.read(4), dtype=dt)[0]\n\n\ndef check_image_file_header(filename):\n    """"""Validate that filename corresponds to images for the MNIST dataset.""""""\n    with tf.gfile.Open(filename, \'rb\') as f:\n        magic = read32(f)\n        read32(f)  # num_images, unused\n        rows = read32(f)\n        cols = read32(f)\n        if magic != 2051:\n            raise ValueError(\'Invalid magic number %d in MNIST file %s\' % (magic, f.name))\n        if rows != 28 or cols != 28:\n            raise ValueError(\n                    \'Invalid MNIST file %s: Expected 28x28 images, found %dx%d\' %\n                    (f.name, rows, cols))\n\n\ndef check_labels_file_header(filename):\n    """"""Validate that filename corresponds to labels for the MNIST dataset.""""""\n    with tf.gfile.Open(filename, \'rb\') as f:\n        magic = read32(f)\n        read32(f)  # num_items, unused\n        if magic != 2049:\n            raise ValueError(\'Invalid magic number %d in MNIST file %s\' % (magic, f.name))\n\n\ndef download(directory, filename):\n    """"""Download (and unzip) a file from the MNIST dataset if not already done.""""""\n    filepath = os.path.join(directory, filename)\n    if tf.gfile.Exists(filepath):\n        return filepath\n    if not tf.gfile.Exists(directory):\n        tf.gfile.MakeDirs(directory)\n    # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n    url = \'https://storage.googleapis.com/cvdf-datasets/mnist/\' + filename + \'.gz\'\n    zipped_filepath = filepath + \'.gz\'\n    print(\'Downloading %s to %s\' % (url, zipped_filepath))\n    urllib.request.urlretrieve(url, zipped_filepath)\n    with gzip.open(zipped_filepath, \'rb\') as f_in, open(filepath, \'wb\') as f_out:\n        shutil.copyfileobj(f_in, f_out)\n    os.remove(zipped_filepath)\n    return filepath\n\n\ndef dataset(directory, images_file, labels_file):\n    """"""Download and parse MNIST dataset.""""""\n\n    images_file = download(directory, images_file)\n    labels_file = download(directory, labels_file)\n\n    check_image_file_header(images_file)\n    check_labels_file_header(labels_file)\n\n    def decode_image(image):\n        # Normalize from [0, 255] to [0.0, 1.0]\n        image = tf.decode_raw(image, tf.uint8)\n        image = tf.cast(image, tf.float32)\n        image = tf.reshape(image, [784])\n        return image / 255.0\n\n    def decode_label(label):\n        label = tf.decode_raw(label, tf.uint8)  # tf.string -> [tf.uint8]\n        label = tf.reshape(label, [])  # label is a scalar\n        return tf.to_int32(label)\n\n    images = tf.data.FixedLengthRecordDataset(images_file, 28 * 28, header_bytes=16)\n    images = images.map(decode_image)\n    labels = tf.data.FixedLengthRecordDataset(labels_file, 1, header_bytes=8).map(decode_label)\n    return tf.data.Dataset.zip((images, labels))\n\n\ndef train(directory):\n    """"""tf.data.Dataset object for MNIST training data.""""""\n    return dataset(directory, \'train-images-idx3-ubyte\',\n                   \'train-labels-idx1-ubyte\')\n\n\ndef test(directory):\n    """"""tf.data.Dataset object for MNIST test data.""""""\n    return dataset(directory, \'t10k-images-idx3-ubyte\', \'t10k-labels-idx1-ubyte\')\n'"
model/model_fn.py,34,"b'""""""Define the model.""""""\n\nimport tensorflow as tf\n\nfrom model.triplet_loss import batch_all_triplet_loss\nfrom model.triplet_loss import batch_hard_triplet_loss\n\n\ndef build_model(is_training, images, params):\n    """"""Compute outputs of the model (embeddings for triplet loss).\n\n    Args:\n        is_training: (bool) whether we are training or not\n        images: (dict) contains the inputs of the graph (features)\n                this can be `tf.placeholder` or outputs of `tf.data`\n        params: (Params) hyperparameters\n\n    Returns:\n        output: (tf.Tensor) output of the model\n    """"""\n    out = images\n    # Define the number of channels of each convolution\n    # For each block, we do: 3x3 conv -> batch norm -> relu -> 2x2 maxpool\n    num_channels = params.num_channels\n    bn_momentum = params.bn_momentum\n    channels = [num_channels, num_channels * 2]\n    for i, c in enumerate(channels):\n        with tf.variable_scope(\'block_{}\'.format(i+1)):\n            out = tf.layers.conv2d(out, c, 3, padding=\'same\')\n            if params.use_batch_norm:\n                out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)\n            out = tf.nn.relu(out)\n            out = tf.layers.max_pooling2d(out, 2, 2)\n\n    assert out.shape[1:] == [7, 7, num_channels * 2]\n\n    out = tf.reshape(out, [-1, 7 * 7 * num_channels * 2])\n    with tf.variable_scope(\'fc_1\'):\n        out = tf.layers.dense(out, params.embedding_size)\n\n    return out\n\n\ndef model_fn(features, labels, mode, params):\n    """"""Model function for tf.estimator\n\n    Args:\n        features: input batch of images\n        labels: labels of the images\n        mode: can be one of tf.estimator.ModeKeys.{TRAIN, EVAL, PREDICT}\n        params: contains hyperparameters of the model (ex: `params.learning_rate`)\n\n    Returns:\n        model_spec: tf.estimator.EstimatorSpec object\n    """"""\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n    images = features\n    images = tf.reshape(images, [-1, params.image_size, params.image_size, 1])\n    assert images.shape[1:] == [params.image_size, params.image_size, 1], ""{}"".format(images.shape)\n\n    # -----------------------------------------------------------\n    # MODEL: define the layers of the model\n    with tf.variable_scope(\'model\'):\n        # Compute the embeddings with the model\n        embeddings = build_model(is_training, images, params)\n    embedding_mean_norm = tf.reduce_mean(tf.norm(embeddings, axis=1))\n    tf.summary.scalar(""embedding_mean_norm"", embedding_mean_norm)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        predictions = {\'embeddings\': embeddings}\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n    labels = tf.cast(labels, tf.int64)\n\n    # Define triplet loss\n    if params.triplet_strategy == ""batch_all"":\n        loss, fraction = batch_all_triplet_loss(labels, embeddings, margin=params.margin,\n                                                squared=params.squared)\n    elif params.triplet_strategy == ""batch_hard"":\n        loss = batch_hard_triplet_loss(labels, embeddings, margin=params.margin,\n                                       squared=params.squared)\n    else:\n        raise ValueError(""Triplet strategy not recognized: {}"".format(params.triplet_strategy))\n\n    # -----------------------------------------------------------\n    # METRICS AND SUMMARIES\n    # Metrics for evaluation using tf.metrics (average over whole dataset)\n    # TODO: some other metrics like rank-1 accuracy?\n    with tf.variable_scope(""metrics""):\n        eval_metric_ops = {""embedding_mean_norm"": tf.metrics.mean(embedding_mean_norm)}\n\n        if params.triplet_strategy == ""batch_all"":\n            eval_metric_ops[\'fraction_positive_triplets\'] = tf.metrics.mean(fraction)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\n    # Summaries for training\n    tf.summary.scalar(\'loss\', loss)\n    if params.triplet_strategy == ""batch_all"":\n        tf.summary.scalar(\'fraction_positive_triplets\', fraction)\n\n    tf.summary.image(\'train_image\', images, max_outputs=1)\n\n    # Define training step that minimizes the loss with the Adam optimizer\n    optimizer = tf.train.AdamOptimizer(params.learning_rate)\n    global_step = tf.train.get_global_step()\n    if params.use_batch_norm:\n        # Add a dependency to update the moving mean and variance for batch normalization\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            train_op = optimizer.minimize(loss, global_step=global_step)\n    else:\n        train_op = optimizer.minimize(loss, global_step=global_step)\n\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n'"
model/triplet_loss.py,47,"b'""""""Define functions to create the triplet loss with online triplet mining.""""""\n\nimport tensorflow as tf\n\n\ndef _pairwise_distances(embeddings, squared=False):\n    """"""Compute the 2D matrix of distances between all the embeddings.\n\n    Args:\n        embeddings: tensor of shape (batch_size, embed_dim)\n        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n                 If false, output is the pairwise euclidean distance matrix.\n\n    Returns:\n        pairwise_distances: tensor of shape (batch_size, batch_size)\n    """"""\n    # Get the dot product between all embeddings\n    # shape (batch_size, batch_size)\n    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n\n    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n    # shape (batch_size,)\n    square_norm = tf.diag_part(dot_product)\n\n    # Compute the pairwise distance matrix as we have:\n    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n    # shape (batch_size, batch_size)\n    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n\n    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n    distances = tf.maximum(distances, 0.0)\n\n    if not squared:\n        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n        # we need to add a small epsilon where distances == 0.0\n        mask = tf.to_float(tf.equal(distances, 0.0))\n        distances = distances + mask * 1e-16\n\n        distances = tf.sqrt(distances)\n\n        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n        distances = distances * (1.0 - mask)\n\n    return distances\n\n\ndef _get_anchor_positive_triplet_mask(labels):\n    """"""Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n\n    Args:\n        labels: tf.int32 `Tensor` with shape [batch_size]\n\n    Returns:\n        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n    """"""\n    # Check that i and j are distinct\n    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n    indices_not_equal = tf.logical_not(indices_equal)\n\n    # Check if labels[i] == labels[j]\n    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n\n    # Combine the two masks\n    mask = tf.logical_and(indices_not_equal, labels_equal)\n\n    return mask\n\n\ndef _get_anchor_negative_triplet_mask(labels):\n    """"""Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n\n    Args:\n        labels: tf.int32 `Tensor` with shape [batch_size]\n\n    Returns:\n        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n    """"""\n    # Check if labels[i] != labels[k]\n    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n\n    mask = tf.logical_not(labels_equal)\n\n    return mask\n\n\ndef _get_triplet_mask(labels):\n    """"""Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n\n    A triplet (i, j, k) is valid if:\n        - i, j, k are distinct\n        - labels[i] == labels[j] and labels[i] != labels[k]\n\n    Args:\n        labels: tf.int32 `Tensor` with shape [batch_size]\n    """"""\n    # Check that i, j and k are distinct\n    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n    indices_not_equal = tf.logical_not(indices_equal)\n    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n\n    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n\n\n    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n    i_equal_j = tf.expand_dims(label_equal, 2)\n    i_equal_k = tf.expand_dims(label_equal, 1)\n\n    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n\n    # Combine the two masks\n    mask = tf.logical_and(distinct_indices, valid_labels)\n\n    return mask\n\n\ndef batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n    """"""Build the triplet loss over a batch of embeddings.\n\n    We generate all the valid triplets and average the loss over the positive ones.\n\n    Args:\n        labels: labels of the batch, of size (batch_size,)\n        embeddings: tensor of shape (batch_size, embed_dim)\n        margin: margin for triplet loss\n        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n                 If false, output is the pairwise euclidean distance matrix.\n\n    Returns:\n        triplet_loss: scalar tensor containing the triplet loss\n    """"""\n    # Get the pairwise distance matrix\n    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n\n    # shape (batch_size, batch_size, 1)\n    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n    assert anchor_positive_dist.shape[2] == 1, ""{}"".format(anchor_positive_dist.shape)\n    # shape (batch_size, 1, batch_size)\n    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n    assert anchor_negative_dist.shape[1] == 1, ""{}"".format(anchor_negative_dist.shape)\n\n    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n    # and the 2nd (batch_size, 1, batch_size)\n    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n\n    # Put to zero the invalid triplets\n    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n    mask = _get_triplet_mask(labels)\n    mask = tf.to_float(mask)\n    triplet_loss = tf.multiply(mask, triplet_loss)\n\n    # Remove negative losses (i.e. the easy triplets)\n    triplet_loss = tf.maximum(triplet_loss, 0.0)\n\n    # Count number of positive triplets (where triplet_loss > 0)\n    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n    num_positive_triplets = tf.reduce_sum(valid_triplets)\n    num_valid_triplets = tf.reduce_sum(mask)\n    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n\n    # Get final mean triplet loss over the positive valid triplets\n    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n\n    return triplet_loss, fraction_positive_triplets\n\n\ndef batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n    """"""Build the triplet loss over a batch of embeddings.\n\n    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n\n    Args:\n        labels: labels of the batch, of size (batch_size,)\n        embeddings: tensor of shape (batch_size, embed_dim)\n        margin: margin for triplet loss\n        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n                 If false, output is the pairwise euclidean distance matrix.\n\n    Returns:\n        triplet_loss: scalar tensor containing the triplet loss\n    """"""\n    # Get the pairwise distance matrix\n    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n\n    # For each anchor, get the hardest positive\n    # First, we need to get a mask for every valid positive (they should have same label)\n    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n\n    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n\n    # shape (batch_size, 1)\n    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n    tf.summary.scalar(""hardest_positive_dist"", tf.reduce_mean(hardest_positive_dist))\n\n    # For each anchor, get the hardest negative\n    # First, we need to get a mask for every valid negative (they should have different labels)\n    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n\n    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n\n    # shape (batch_size,)\n    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n    tf.summary.scalar(""hardest_negative_dist"", tf.reduce_mean(hardest_negative_dist))\n\n    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n\n    # Get final mean triplet loss\n    triplet_loss = tf.reduce_mean(triplet_loss)\n\n    return triplet_loss\n'"
model/utils.py,0,"b'""""""General utility functions""""""\n\nimport json\nimport logging\n\n\nclass Params():\n    """"""Class that loads hyperparameters from a json file.\n\n    Example:\n    ```\n    params = Params(json_path)\n    print(params.learning_rate)\n    params.learning_rate = 0.5  # change the value of learning_rate in params\n    ```\n    """"""\n\n    def __init__(self, json_path):\n        self.update(json_path)\n\n    def save(self, json_path):\n        """"""Saves parameters to json file""""""\n        with open(json_path, \'w\') as f:\n            json.dump(self.__dict__, f, indent=4)\n\n    def update(self, json_path):\n        """"""Loads parameters from json file""""""\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    @property\n    def dict(self):\n        """"""Gives dict-like access to Params instance by `params.dict[\'learning_rate\']`""""""\n        return self.__dict__\n\n\ndef set_logger(log_path):\n    """"""Sets the logger to log info in terminal and file `log_path`.\n\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n\n    Example:\n    ```\n    logging.info(""Starting training..."")\n    ```\n\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        logger.addHandler(stream_handler)\n\n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict of floats in json file\n\n    Args:\n        d: (dict) of float-castable values (np.float, int, float, etc.)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        # We need to convert the values to float for json (it doesn\'t accept np.array, np.float, )\n        d = {k: float(v) for k, v in d.items()}\n        json.dump(d, f, indent=4)\n'"
model/tests/__init__.py,0,b''
model/tests/test_triplet_loss.py,11,"b'""""""Test for the triplet loss computation.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom model.triplet_loss import batch_all_triplet_loss\nfrom model.triplet_loss import batch_hard_triplet_loss\nfrom model.triplet_loss import _pairwise_distances\nfrom model.triplet_loss import _get_triplet_mask\nfrom model.triplet_loss import _get_anchor_positive_triplet_mask\nfrom model.triplet_loss import _get_anchor_negative_triplet_mask\n\n\ndef pairwise_distance_np(feature, squared=False):\n    """"""Computes the pairwise distance matrix in numpy.\n    Args:\n        feature: 2-D numpy array of size [number of data, feature dimension]\n        squared: Boolean. If true, output is the pairwise squared euclidean\n                 distance matrix; else, output is the pairwise euclidean distance matrix.\n    Returns:\n        pairwise_distances: 2-D numpy array of size\n                            [number of data, number of data].\n    """"""\n    triu = np.triu_indices(feature.shape[0], 1)\n    upper_tri_pdists = np.linalg.norm(feature[triu[1]] - feature[triu[0]], axis=1)\n    if squared:\n        upper_tri_pdists **= 2.\n    num_data = feature.shape[0]\n    pairwise_distances = np.zeros((num_data, num_data))\n    pairwise_distances[np.triu_indices(num_data, 1)] = upper_tri_pdists\n    # Make symmetrical.\n    pairwise_distances = pairwise_distances + pairwise_distances.T - np.diag(\n            pairwise_distances.diagonal())\n    return pairwise_distances\n\n\ndef test_pairwise_distances():\n    """"""Test the pairwise distances function.""""""\n    num_data = 64\n    feat_dim = 6\n\n    embeddings = np.random.randn(num_data, feat_dim).astype(np.float32)\n    embeddings[1] = embeddings[0]  # to get distance 0\n\n    with tf.Session() as sess:\n        for squared in [True, False]:\n            res_np = pairwise_distance_np(embeddings, squared=squared)\n            res_tf = sess.run(_pairwise_distances(embeddings, squared=squared))\n            assert np.allclose(res_np, res_tf)\n\n\ndef test_pairwise_distances_are_positive():\n    """"""Test that the pairwise distances are always positive.\n\n    Use a tricky case where numerical errors are common.\n    """"""\n    num_data = 64\n    feat_dim = 6\n\n    # Create embeddings very close to each other in [1.0 - 2e-7, 1.0 + 2e-7]\n    # This will encourage errors in the computation\n    embeddings = 1.0 + 2e-7 * np.random.randn(num_data, feat_dim).astype(np.float32)\n    embeddings[1] = embeddings[0]  # to get distance 0\n\n    with tf.Session() as sess:\n        for squared in [True, False]:\n            res_tf = sess.run(_pairwise_distances(embeddings, squared=squared))\n            assert np.all(res_tf >= 0.0)\n\n\ndef test_gradients_pairwise_distances():\n    """"""Check that the gradients of the pairwise distances are not nan.\n\n    This happens if one of the distance is exactly 0.0 (or negative), as the gradient of the\n    square root will be infinite.\n    """"""\n\n    num_data = 64\n    feat_dim = 6\n\n    embeddings = np.random.randn(num_data, feat_dim).astype(np.float32)\n    # Make the first two embeddings equal to get d(0, 1) = 0.0\n    embeddings[1] = embeddings[0]\n    # Make the last 10 embeddings very close to each other\n    embeddings[num_data - 10: num_data] = 1.0 + 2e-7 * np.random.randn(10, feat_dim)\n    embeddings = tf.constant(embeddings)\n\n    with tf.Session() as sess:\n        for squared in [True, False]:\n            dists = _pairwise_distances(embeddings, squared=squared)\n            grads = tf.gradients(dists, embeddings)\n\n            g = sess.run(grads)\n            assert not np.any(np.isnan(g)), ""Gradient shouldn\'t be nan, squared={}"".format(squared)\n\n\ndef test_triplet_mask():\n    """"""Test function _get_triplet_mask.""""""\n    num_data = 64\n    num_classes = 10\n\n    labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n\n    mask_np = np.zeros((num_data, num_data, num_data))\n    for i in range(num_data):\n        for j in range(num_data):\n            for k in range(num_data):\n                distinct = (i != j and i != k and j != k)\n                valid = (labels[i] == labels[j]) and (labels[i] != labels[k])\n                mask_np[i, j, k] = (distinct and valid)\n\n    mask_tf = _get_triplet_mask(labels)\n    with tf.Session() as sess:\n        mask_tf_val = sess.run(mask_tf)\n\n    assert np.allclose(mask_np, mask_tf_val)\n\n\ndef test_anchor_positive_triplet_mask():\n    """"""Test function _get_anchor_positive_triplet_mask.""""""\n    num_data = 64\n    num_classes = 10\n\n    labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n\n    mask_np = np.zeros((num_data, num_data))\n    for i in range(num_data):\n        for j in range(num_data):\n            distinct = (i != j)\n            valid = labels[i] == labels[j]\n            mask_np[i, j] = (distinct and valid)\n\n    mask_tf = _get_anchor_positive_triplet_mask(labels)\n    with tf.Session() as sess:\n        mask_tf_val = sess.run(mask_tf)\n\n    assert np.allclose(mask_np, mask_tf_val)\n\n\ndef test_anchor_negative_triplet_mask():\n    """"""Test function _get_anchor_negative_triplet_mask.""""""\n    num_data = 64\n    num_classes = 10\n\n    labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n\n    mask_np = np.zeros((num_data, num_data))\n    for i in range(num_data):\n        for k in range(num_data):\n            distinct = (i != k)\n            valid = (labels[i] != labels[k])\n            mask_np[i, k] = (distinct and valid)\n\n    mask_tf = _get_anchor_negative_triplet_mask(labels)\n    with tf.Session() as sess:\n        mask_tf_val = sess.run(mask_tf)\n\n    assert np.allclose(mask_np, mask_tf_val)\n\n\ndef test_simple_batch_all_triplet_loss():\n    """"""Test the triplet loss with batch all triplet mining in a simple case.\n\n    There is just one class in this super simple edge case, and we want to make sure that\n    the loss is 0.\n    """"""\n    num_data = 10\n    feat_dim = 6\n    margin = 0.2\n    num_classes = 1\n\n    embeddings = np.random.rand(num_data, feat_dim).astype(np.float32)\n    labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n\n    for squared in [True, False]:\n        loss_np = 0.0\n\n        # Compute the loss in TF.\n        loss_tf, fraction = batch_all_triplet_loss(labels, embeddings, margin, squared=squared)\n        with tf.Session() as sess:\n            loss_tf_val, fraction_val = sess.run([loss_tf, fraction])\n        assert np.allclose(loss_np, loss_tf_val)\n        assert np.allclose(fraction_val, 0.0)\n\n\ndef test_batch_all_triplet_loss():\n    """"""Test the triplet loss with batch all triplet mining""""""\n    num_data = 10\n    feat_dim = 6\n    margin = 0.2\n    num_classes = 5\n\n    embeddings = np.random.rand(num_data, feat_dim).astype(np.float32)\n    labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n\n    for squared in [True, False]:\n        pdist_matrix = pairwise_distance_np(embeddings, squared=squared)\n\n        loss_np = 0.0\n        num_positives = 0.0\n        num_valid = 0.0\n        for i in range(num_data):\n            for j in range(num_data):\n                for k in range(num_data):\n                    distinct = (i != j and i != k and j != k)\n                    valid = (labels[i] == labels[j]) and (labels[i] != labels[k])\n                    if distinct and valid:\n                        num_valid += 1.0\n\n                        pos_distance = pdist_matrix[i][j]\n                        neg_distance = pdist_matrix[i][k]\n\n                        loss = np.maximum(0.0, pos_distance - neg_distance + margin)\n                        loss_np += loss\n\n                        num_positives += (loss > 0)\n\n        loss_np /= num_positives\n\n        # Compute the loss in TF.\n        loss_tf, fraction = batch_all_triplet_loss(labels, embeddings, margin, squared=squared)\n        with tf.Session() as sess:\n            loss_tf_val, fraction_val = sess.run([loss_tf, fraction])\n        assert np.allclose(loss_np, loss_tf_val)\n        assert np.allclose(num_positives / num_valid, fraction_val)\n\n\ndef test_batch_hard_triplet_loss():\n    """"""Test the triplet loss with batch hard triplet mining""""""\n    num_data = 50\n    feat_dim = 6\n    margin = 0.2\n    num_classes = 5\n\n    embeddings = np.random.rand(num_data, feat_dim).astype(np.float32)\n    labels = np.random.randint(0, num_classes, size=(num_data)).astype(np.float32)\n\n    for squared in [True, False]:\n        pdist_matrix = pairwise_distance_np(embeddings, squared=squared)\n\n        loss_np = 0.0\n        for i in range(num_data):\n            # Select the hardest positive\n            max_pos_dist = np.max(pdist_matrix[i][labels == labels[i]])\n\n            # Select the hardest negative\n            min_neg_dist = np.min(pdist_matrix[i][labels != labels[i]])\n\n            loss = np.maximum(0.0, max_pos_dist - min_neg_dist + margin)\n            loss_np += loss\n\n        loss_np /= num_data\n\n        # Compute the loss in TF.\n        loss_tf = batch_hard_triplet_loss(labels, embeddings, margin, squared=squared)\n        with tf.Session() as sess:\n            loss_tf_val = sess.run(loss_tf)\n        assert np.allclose(loss_np, loss_tf_val)\n'"
