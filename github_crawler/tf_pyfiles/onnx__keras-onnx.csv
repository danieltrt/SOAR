file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom distutils.core import setup\nfrom setuptools import find_packages\nimport os\nfrom os import path\n\n\ndef find_embedded_package(folder):\n    packages = find_packages(where=folder)\n    merged = [p_ if p_.startswith(folder) else folder + \'.\' + p_ for p_ in packages]\n    return [p_[2:].replace(\'/\', \'.\') for p_ in merged]\n\n\nthis = os.path.dirname(__file__)\nwith open(os.path.join(this, ""requirements.txt""), ""r"") as f:\n    requirements = [_ for _ in [_.strip(""\\r\\n "")\n                                for _ in f.readlines()] if _ is not None]\n\npackages = find_packages()\nassert packages\nroot_package = packages[0]\n\n# read version from the package file.\nversion_str = \'0.1.0.0000\'\nwith (open(os.path.join(this, \'{}/__init__.py\'.format(root_package)), ""r"")) as f:\n    line = [_ for _ in [_.strip(""\\r\\n "")\n                        for _ in f.readlines()] if _.startswith(""__version__"")]\n    if len(line) > 0:\n        version_str = line[0].split(\'=\')[1].strip(\'"" \')\n\nthis_directory = path.abspath(path.dirname(__file__))\nwith open(path.join(this_directory, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n    start_pos = long_description.find(\'# Introduction\')\n    if start_pos >= 0:\n        long_description = long_description[start_pos:]\n\nsetup(\n    name=root_package,\n    version=version_str,\n    description=""Converts Machine Learning models to ONNX for use in Windows ML"",\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    license=\'MIT License\',\n    author=\'Microsoft Corporation\',\n    author_email=\'winmlcvt@microsoft.com\',\n    url=\'https://github.com/onnx/keras-onnx\',\n    packages=packages,\n    include_package_data=True,\n    install_requires=requirements,\n    tests_require=[\'pytest\', \'pytest-cov\'],\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Environment :: Console\',\n        \'Intended Audience :: Developers\',\n        \'Operating System :: POSIX\',\n        \'Operating System :: Microsoft :: Windows\',\n        \'Programming Language :: Python :: 3 :: Only\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'License :: OSI Approved :: MIT License\']\n)\n'"
docs/conf.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License.\n\n# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n\nimport os\nimport sys\nimport shutil\nimport sphinx_gallery.gen_gallery\nimport keras2onnx\nimport onnxruntime\nimport sphinx_keras2onnx_extension\nimport sphinx_modern_theme_modified\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'keras-onnx\'\ncopyright = \'2018-2019, Microsoft\'\nauthor = \'Microsoft\'\nversion = keras2onnx.__version__\nrelease = version\n\n# -- General configuration ---------------------------------------------------\n\nextensions = [\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.imgmath\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    ""sphinx.ext.autodoc"",\n    \'sphinx.ext.githubpages\',\n    ""sphinx_gallery.gen_gallery"",\n    \'sphinx.ext.autodoc\',\n    \'sphinx_keras2onnx_extension\',\n    ""sphinxcontrib.blockdiag"",\n]\n\ntemplates_path = [\'_templates\']\nsource_suffix = [\'.rst\']\n\nmaster_doc = \'index\'\nlanguage = ""en""\nexclude_patterns = []\npygments_style = \'default\'\n\n# -- Options for HTML output -------------------------------------------------\n\nhtml_theme = ""sphinx_mo""\nhtml_static_path = [\'_static\']\nhtml_theme = ""sphinx_modern_theme_modified""\nhtml_theme_path = [sphinx_modern_theme_modified.get_html_theme_path()]\nhtml_logo = ""logo_main.png""\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None}\n\n# -- Options for Sphinx Gallery ----------------------------------------------\n\nsphinx_gallery_conf = {\n     \'examples_dirs\': \'examples\',\n     \'gallery_dirs\': \'auto_examples\',\n}\n\n# -- Setup actions -----------------------------------------------------------\n\ndef setup(app):\n    # Placeholder to initialize the folder before\n    # generating the documentation.\n    return app\n\n'"
docs/sphinx_keras2onnx_extension.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License.\n""""""\nExtension for sphinx.\n""""""\nimport sphinx\nfrom docutils import nodes\nfrom docutils.parsers.rst import Directive\nimport keras2onnx\nimport onnxruntime\n\n\ndef kerasonnx_version_role(role, rawtext, text, lineno, inliner, options=None, content=None):\n    """"""\n    Defines custom role *keras2onnx-version* which returns\n    *keras2onnx* version.\n    """"""\n    if options is None:\n        options = {}\n    if content is None:\n        content = []\n    if text == \'v\':\n        version = \'v\' + keras2onnx.__version__\n    elif text == \'rt\':\n        version = \'v\' + onnxruntime.__version__\n    else:\n        raise RuntimeError(""keras2onnx_version_role cannot interpret content \'{0}\'."".format(text))\n    node = nodes.Text(version)\n    return [node], []\n\n\ndef setup(app):\n    # Placeholder to initialize the folder before\n    # generating the documentation.\n    app.add_role(\'keras2onnxversion\', kerasonnx_version_role)\n    return {\'version\': sphinx.__display_version__, \'parallel_read_safe\': True}\n\n'"
keras2onnx/__init__.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\n""""""\nkeras2onnx\nThis package converts keras models into ONNX for use with any inference engine supporting ONNX\n""""""\n__version__ = ""1.7.0""\n__author__ = ""Microsoft Corporation""\n__producer__ = ""keras2onnx""\n\n__producer_version__ = __version__\n__domain__ = ""onnxmltools""\n__model_version__ = 0\n\ntry:\n    import sys\n    import os.path\n    from os.path import dirname, abspath\n    import tensorflow\n    from distutils.version import StrictVersion\n\n    if StrictVersion(tensorflow.__version__.split(\'-\')[0]) >= StrictVersion(\'2.0.0\'):\n        tensorflow.compat.v1.disable_tensor_equality()\nexcept ImportError:\n    raise AssertionError(\'Please conda install / pip install tensorflow or tensorflow-gpu before the model conversion.\')\n\nfrom .proto import save_model\nfrom .common import Variable, cvtfunc, set_logger_level\nfrom .funcbook import set_converter, set_converters\n\nfrom .main import convert_keras\nfrom .main import export_tf_frozen_graph\nfrom .main import build_io_names_tf2onnx\n\n\ndef tfname_to_onnx(name): return Variable.tfname_to_onnx(name)\n'"
keras2onnx/_builtin.py,1,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport sys\nimport numbers\nimport tensorflow\nimport numpy as np\n\nfrom keras2onnx._consts import TYPES, NCHW_TO_NHWC, NHWC_TO_NCHW, HWCN_TO_NCHW\nfrom onnx import numpy_helper\nfrom onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\nfrom .common.utils import count_dynamic_dim\nfrom .common.onnx_ops import apply_identity, apply_reshape, OnnxOperatorBuilder\nfrom .funcbook import converter_func, set_converters\nfrom .proto import keras\nfrom .proto.tfcompat import is_tf2\nfrom ._tf_utils import (is_nhwc as _is_nhwc,\n                        tf_attrs_to_onnx as _to_onnx_attrs,\n                        cal_tensor_value as _cal_tensor_value,\n                        cal_tensor_shape as _cal_tensor_shape,\n                        to_onnx_type as _to_onnx_type)\n\n\ndef default_convert(scope, operator, container):\n    apply_identity(scope, operator.inputs[0].full_name, operator.outputs[0].full_name, container)\n\n\n@converter_func(TYPES.Identity)\ndef convert_tf_identity(scope, operator, container):\n    default_convert(scope, operator, container)\n\n\n@converter_func(TYPES.AddN)\ndef convert_tf_addn(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.apply_op_with_output(""apply_add"",\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name + \'_add\')\n\n\ndef _convert_tf_argmax_argmin_helper(scope, operator, container, arg_str):\n    node = operator.raw_operator\n    axis = _cal_tensor_value(node.inputs[1]).item(0)\n    dtype = _to_onnx_type(node.outputs[0].dtype)\n    oopb = OnnxOperatorBuilder(container, scope)\n    arg_func = oopb.apply_argmax if arg_str == \'argmax\' else oopb.apply_argmin\n    if dtype == oopb.int64:\n        oopb.apply_op_with_output(""apply_"" + arg_str,\n                                  operator.input_full_names[0],\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_\' + arg_str,\n                                  axis=axis,\n                                  keepdims=0)\n    else:\n        arg_output = arg_func(operator.input_full_names[0],\n                              name=operator.full_name + \'_\' + arg_str,\n                              axis=axis,\n                              keepdims=0)\n        oopb.apply_op_with_output(""apply_cast"",\n                                  arg_output,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_cast\',\n                                  to=dtype)\n\n\n@converter_func(TYPES.ArgMax)\ndef convert_tf_argmax(scope, operator, container):\n    _convert_tf_argmax_argmin_helper(scope, operator, container, \'argmax\')\n\n\n@converter_func(TYPES.ArgMin)\ndef convert_tf_argmin(scope, operator, container):\n    _convert_tf_argmax_argmin_helper(scope, operator, container, \'argmin\')\n\n\n@converter_func(TYPES.BatchToSpaceND)\ndef convert_tf_batch_to_space(scope, operator, container):\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n    blocksize = _cal_tensor_value(node.inputs[1])\n    crops = _cal_tensor_value(node.inputs[2])\n    if operator.target_opset <= 10 or (blocksize is not None and crops is not None):\n        input_shape = _cal_tensor_shape(node.outputs[0])\n        assert len(input_shape) in (4, 3)\n        assert len(blocksize) == 2 and blocksize[0] == blocksize[1]\n\n        if len(input_shape) == 3:\n            unsqueeze_node_1 = oopb.apply_unsqueeze(operator.inputs[0].full_name,\n                                                    name=operator.full_name + \'_unsqueeze_0\',\n                                                    axes=[3])\n            transpose_node_1 = oopb.apply_transpose(unsqueeze_node_1,\n                                                    name=operator.full_name + \'_transpose_1\',\n                                                    perm=[3, 0, 1, 2])\n        else:\n            transpose_node_1 = oopb.apply_transpose(operator.inputs[0].full_name,\n                                                    name=operator.full_name + \'_transpose_1\',\n                                                    perm=[3, 0, 1, 2])\n        depth_to_space_node = oopb.add_node(\'DepthToSpace\',\n                                            transpose_node_1,\n                                            operator.inputs[0].full_name + \'_depth_to_space\',\n                                            blocksize=blocksize[0])\n        transpose_node_2 = oopb.apply_transpose(depth_to_space_node,\n                                                name=operator.full_name + \'_transpose_2\',\n                                                perm=[1, 2, 3, 0])\n\n        if np.count_nonzero(crops) == 0:\n            oopb.apply_op_with_output(""apply_identity"",\n                                      transpose_node_2,\n                                      operator.output_full_names,\n                                      name=operator.full_name + \'_slice\')\n            return\n\n        slice_axis = [1, 2]\n        top, bottom = crops[0]\n        left, right = crops[1]\n        starts = [top, left]\n        ends = []\n        for end in [bottom, right]:\n            if end != 0:\n                ends.append(-end)\n            else:\n                ends.append(np.iinfo(np.int32).max)\n\n        if len(input_shape) == 3:\n            slice_node = oopb.apply_slice(transpose_node_2,\n                                          name=operator.full_name + \'_slice\',\n                                          starts=starts, ends=ends, axes=slice_axis)\n            oopb.apply_op_with_output(""apply_squeeze"",\n                                      slice_node,\n                                      operator.output_full_names,\n                                      name=operator.full_name + \'_squeeze_output\',\n                                      axes=[3])\n        else:\n            oopb.apply_op_with_output(""apply_slice"",\n                                      transpose_node_2,\n                                      operator.output_full_names,\n                                      name=operator.full_name + \'_slice\',\n                                      starts=starts, ends=ends, axes=slice_axis)\n\n    else:\n        shape_x = oopb.add_node(\'Shape\', [operator.inputs[0].full_name],\n                                operator.full_name + \'_input_0_shape\')\n        block_shape = oopb.apply_cast(operator.inputs[1].full_name,\n                                      to=oopb.int64,\n                                      name=operator.full_name + \'_input_1_cast\')\n        crop = oopb.apply_cast(operator.inputs[2].full_name,\n                               to=oopb.int64,\n                               name=operator.full_name + \'_input_2_cast\')\n        block_size = oopb.apply_slice(block_shape,\n                                      name=operator.full_name + \'_slice_0\',\n                                      starts=[0], ends=[1])\n        block_prod = oopb.apply_mul(block_size + block_size,\n                                    name=operator.full_name + \'_mul_0\')\n        padded_block_prod = oopb.apply_pad(block_prod,\n                                           name=operator.full_name + \'_pad_0\',\n                                           pads=[0, 3],\n                                           value=1)\n        new_shape_x = oopb.apply_div([shape_x] + padded_block_prod,\n                                     name=operator.full_name + \'_div\')\n        concat_new_shape_x = oopb.apply_concat(block_shape + new_shape_x,\n                                               name=operator.full_name + \'_concat\',\n                                               axis=0)\n        reshaped_x = oopb.apply_reshape([operator.inputs[0].full_name],\n                                        name=operator.full_name + \'_reshape_0\',\n                                        desired_shape=concat_new_shape_x[0])\n        transposed_x = oopb.apply_transpose(reshaped_x,\n                                            name=operator.full_name + \'_transpose_0\',\n                                            perm=[2, 3, 0, 4, 1, 5])\n        padded_block_shape = oopb.apply_pad(block_shape,\n                                            name=operator.full_name + \'_pad_1\',\n                                            pads=[1, 1],\n                                            value=1)\n        new_shape_x_v2 = oopb.apply_mul(new_shape_x + padded_block_shape,\n                                        name=operator.full_name + \'_mul_1\')\n        reshaped_x_v2 = oopb.apply_reshape(transposed_x,\n                                           name=operator.full_name + \'_reshape_1\',\n                                           desired_shape=new_shape_x_v2[0])\n        transposed_crop = oopb.apply_transpose(crop,\n                                               name=operator.full_name + \'_transpose_1\',\n                                               perm=[1, 0])\n        slice_crop_starts = oopb.apply_slice(transposed_crop,\n                                             name=operator.full_name + \'_slice_starts\',\n                                             starts=[0, 0], ends=[1, 2])\n        reshaped_slice_crop_starts = oopb.apply_reshape(slice_crop_starts,\n                                                        name=operator.full_name + \'_reshape_starts\',\n                                                        desired_shape=[2])\n        slice_crop_ends = oopb.apply_slice(transposed_crop,\n                                           name=operator.full_name + \'_slice_ends\',\n                                           starts=[1, 0], ends=[2, 2])\n        reshaped_slice_crop_ends = oopb.apply_reshape(slice_crop_ends,\n                                                      name=operator.full_name + \'_reshape_ends\',\n                                                      desired_shape=[2])\n        sliced_new_shape_x_v2 = oopb.apply_slice(new_shape_x_v2,\n                                                 name=operator.full_name + \'_slice_3\',\n                                                 starts=[1], ends=[3])\n        neged_reshaped_slice_crop_ends = oopb.apply_sub(sliced_new_shape_x_v2 + reshaped_slice_crop_ends,\n                                                        name=operator.full_name + \'_sub\')\n        oopb.apply_op_with_output(""apply_slice"",\n                                  reshaped_x_v2,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_slice_final\',\n                                  starts=reshaped_slice_crop_starts[0],\n                                  ends=neged_reshaped_slice_crop_ends[0],\n                                  axes=[1, 2])\n\n\n@converter_func(TYPES.SpaceToBatchND)\ndef convert_tf_space_to_batch(scope, operator, container):\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n    blocksize = _cal_tensor_value(node.inputs[1])\n    paddings = _cal_tensor_value(node.inputs[2])\n    if operator.target_opset <= 10 or (blocksize is not None and paddings is not None):\n        input_shape = _cal_tensor_shape(node.outputs[0])\n        assert len(input_shape) == 4\n        assert len(blocksize) == 2 and blocksize[0] == blocksize[1]\n\n        top, bottom = paddings[0]\n        left, right = paddings[1]\n        pads = [0, top, left, 0,\n                0, bottom, right, 0]\n\n        if np.count_nonzero(pads) > 0:\n            pad_op = oopb.apply_pad(operator.inputs[0].full_name,\n                                    name=operator.full_name + \'_pad_1\',\n                                    pads=pads)\n        else:\n            pad_op = operator.inputs[0].full_name\n\n        transpose_node_1 = oopb.apply_transpose(pad_op,\n                                                name=operator.full_name + \'_transpose_1\',\n                                                perm=[3, 0, 1, 2])\n        space_to_depth_node = oopb.add_node(\'SpaceToDepth\',\n                                            transpose_node_1,\n                                            operator.inputs[0].full_name + \'_space_to_depth\',\n                                            blocksize=blocksize[0])\n        oopb.apply_op_with_output(""apply_transpose"",\n                                  space_to_depth_node,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_transpose_2\',\n                                  perm=[1, 2, 3, 0])\n    else:\n        shape_x = oopb.add_node(\'Shape\', [operator.inputs[0].full_name],\n                                operator.full_name + \'_input_0_shape\')\n        block_shape = oopb.apply_cast(operator.inputs[1].full_name,\n                                      to=oopb.int64,\n                                      name=operator.full_name + \'_input_1_cast\')\n        pad_x = oopb.apply_cast(operator.inputs[2].full_name,\n                                to=oopb.int64,\n                                name=operator.full_name + \'_input_2_cast\')\n        concated_pad_x = oopb.apply_concat(\n            [(\'_const_zero_zero\', oopb.int64, np.array([[0, 0]], dtype=\'int64\'))] + pad_x,\n            name=operator.full_name + \'_concat_1\',\n            axis=0)\n        concated_pad_x_v2 = oopb.apply_concat(\n            concated_pad_x + [(\'_const_zero_zero\', oopb.int64, np.array([[0, 0]], dtype=\'int64\'))],\n            name=operator.full_name + \'_concat_2\',\n            axis=0)\n        transposed_concated_pad_x_v2 = oopb.apply_transpose(concated_pad_x_v2,\n                                                            name=operator.full_name + \'_transpose_0\',\n                                                            perm=[1, 0])\n        reshaped_transposed_pad_x = oopb.apply_reshape(transposed_concated_pad_x_v2,\n                                                       name=operator.full_name + \'_reshape_0\',\n                                                       desired_shape=[8])\n        padded_input_x = oopb.apply_pad(operator.inputs[0].full_name,\n                                        name=operator.full_name + \'_pad_1\',\n                                        pads=reshaped_transposed_pad_x)\n        padded_block_shape = oopb.apply_pad(block_shape,\n                                            name=operator.full_name + \'_pad_2\',\n                                            pads=[1, 1], value=1)\n        new_shape_x = oopb.apply_div([shape_x] + padded_block_shape,\n                                     name=operator.full_name + \'_div\')\n        first_row_new_shape_x = oopb.apply_slice(new_shape_x,\n                                                 name=operator.full_name + \'_slice_0\',\n                                                 starts=[0], ends=[2])\n        block_size = oopb.apply_slice(block_shape,\n                                      name=operator.full_name + \'_slice_1\',\n                                      starts=[0], ends=[1])\n        new_first_row_new_shape_x = oopb.apply_concat(first_row_new_shape_x + block_size,\n                                                      name=operator.full_name + \'_concat_3\',\n                                                      axis=0)\n        second_row_new_shape_x_first_half = oopb.apply_slice(new_shape_x,\n                                                             name=operator.full_name + \'_slice_second_first\',\n                                                             starts=[2], ends=[3])\n        second_row_new_shape_x_second_half = oopb.apply_slice(new_shape_x,\n                                                              name=operator.full_name + \'_slice_second_second\',\n                                                              starts=[3], ends=[4])\n        new_second_row_new_shape_x_first_half = oopb.apply_concat(second_row_new_shape_x_first_half + block_size,\n                                                                  name=operator.full_name + \'_concat_second_first\',\n                                                                  axis=0)\n        new_second_row_new_shape_x = oopb.apply_concat(\n            new_second_row_new_shape_x_first_half + second_row_new_shape_x_second_half,\n            name=operator.full_name + \'_concat_second_shape\',\n            axis=0)\n        new_shape_x_v2 = oopb.apply_concat(new_first_row_new_shape_x + new_second_row_new_shape_x,\n                                           name=operator.full_name + \'_concat_shape\',\n                                           axis=0)\n        new_x = oopb.apply_reshape(padded_input_x[0],\n                                   name=operator.full_name + \'_reshape_new_x\',\n                                   desired_shape=new_shape_x_v2[0])\n        transposed_new_x = oopb.apply_transpose(new_x,\n                                                name=operator.full_name + \'_transpose_new\',\n                                                perm=[2, 4, 0, 1, 3, 5])\n        block_size_prod = oopb.apply_mul(block_size + block_size,\n                                         name=operator.full_name + \'_mul_0\')\n        padded_block_size_prod = oopb.apply_pad(block_size_prod,\n                                                name=operator.full_name + \'_pad_block_size\',\n                                                pads=[0, 3], value=1)\n        new_shape_x_v3 = oopb.apply_mul(new_shape_x + padded_block_size_prod,\n                                        name=operator.full_name + \'_mul_shape_v3\')\n        oopb.apply_op_with_output(""apply_reshape"",\n                                  transposed_new_x,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_transpose_2\',\n                                  desired_shape=new_shape_x_v3)\n\n\n@converter_func(TYPES.BiasAdd, TYPES.BiasAddV1)\ndef convert_tf_bias_add(scope, operator, container):\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n    if not _is_nhwc(node):\n        shape0 = _cal_tensor_shape(node.inputs[0])\n        shape1 = _cal_tensor_shape(node.inputs[1])\n        if node.inputs[1].op.type == \'Const\':\n            new_broadcast_shape = [shape1[0]] + [1] * (len(shape0) - 2)\n            reshape_node = oopb.apply_reshape(operator.inputs[1].full_name,\n                                              name=operator.full_name + \'_reshape\',\n                                              desired_shape=new_broadcast_shape)\n            oopb.apply_op_with_output(""apply_add"",\n                                      [node.inputs[0].name, reshape_node[0]],\n                                      operator.output_full_names,\n                                      name=operator.full_name + \'_add\')\n            return\n\n    oopb.apply_op_with_output(""apply_add"",\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name + \'_add\')\n\n\n@converter_func(TYPES.Cumsum)\ndef convert_tf_cum_sum(scope, operator, container):\n    if operator.target_opset < 11:\n        raise ValueError(""CumSum op is not supported for opset < 11"")\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n    attrs = {\'exclusive\': node.get_attr(\'exclusive\'), \'reverse\': node.get_attr(\'reverse\')}\n    oopb.add_node_with_output(\'CumSum\',\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              **attrs)\n\n\ndef _calc_explicit_padding(input_size, output_shape, output_padding, kernel_shape, stride, dilation,\n                           perm):\n    def to_nchw(x, perm):\n        return [x[perm[n_]] for n_ in range(len(x))]\n    input_size = to_nchw(input_size, perm)[2:]\n\n    spatial = len(kernel_shape)\n    total_padding = []\n    pads = [None] * 2 * spatial\n    for i in range(spatial):\n        total_padding[i:] = [stride[i] * ((input_size[i] - 1) // stride[i]) + 1 +\n                             output_padding[i] + (kernel_shape[i] - 1) * dilation[i] - input_size[i]]\n        total_padding[i] = max(total_padding[i], 0)\n        pads[i] = total_padding[i] // 2\n        pads[i + spatial] = total_padding[i] - (total_padding[i] // 2)\n\n    return pads\n\n\n@converter_func(TYPES.DepthToSpace)\ndef convert_tf_depth_to_space(scope, operator, container):\n    node = operator.raw_operator\n    block_size = node.get_attr(\'block_size\')\n    oopb = OnnxOperatorBuilder(container, scope)\n    if _is_nhwc(node):\n        adjusted_input_name = oopb.apply_transpose(operator.input_full_names,\n                                                   name=operator.full_name + \'_pre_transpose\',\n                                                   perm=[0, 3, 1, 2])\n        depth_to_space_result = oopb.add_node(""DepthToSpace"",\n                                              adjusted_input_name,\n                                              name=operator.full_name,\n                                              blocksize=node.get_attr(\'block_size\'),\n                                              mode=""DCR"",\n                                              op_version=11)\n        oopb.apply_op_with_output(""apply_transpose"",\n                                  depth_to_space_result,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_post_transpose\',\n                                  perm=[0, 2, 3, 1])\n    else:\n        oopb.add_node_with_output(""DepthToSpace"",\n                                  operator.input_full_names,\n                                  operator.output_full_names,\n                                  name=operator.full_name,\n                                  blocksize=block_size,\n                                  mode=""DCR"",\n                                  op_version=11)\n\n\n@converter_func(TYPES.DepthwiseConv2dNative)\ndef convert_tf_depthwise_conv2d(scope, operator, container):\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n\n    channels_first = node.get_attr(\'data_format\') == b\'NCHW\'\n\n    if channels_first:\n        adjusted_input_name = [operator.inputs[0].full_name]\n    else:\n        adjusted_input_name = oopb.apply_transpose(operator.inputs[0].full_name,\n                                                   name=operator.full_name + \'_transpose_0\',\n                                                   perm=[0, 3, 1, 2])\n\n    weight_perm_axes = [3, 2, 0, 1]\n    weight_shape = _cal_tensor_shape(node.inputs[1])\n    new_shape = weight_shape[:2] + [1, weight_shape[2] * weight_shape[3]]\n    weight_reshape = oopb.apply_reshape(operator.inputs[1].full_name,\n                                        name=operator.full_name + \'_reshape_ends\',\n                                        desired_shape=new_shape)\n    transposed_weight = oopb.apply_transpose(weight_reshape,\n                                             name=operator.full_name + \'_transpose_new\',\n                                             perm=weight_perm_axes)\n\n    attrs = {}\n    dilation_rate = node.get_attr(\'dilations\')\n    dilation_rate = dilation_rate[2:] if channels_first else dilation_rate[1:3]\n    attrs[\'dilations\'] = dilation_rate\n    strides = node.get_attr(\'strides\')\n    strides = strides[2:] if channels_first else strides[1:3]\n    attrs[\'strides\'] = strides\n    kernel_size = weight_shape[:2]\n    input_channels, output_channels = weight_shape[-2:]\n    group = input_channels\n    attrs[\'group\'] = group\n\n    input_shape = _cal_tensor_shape(node.inputs[0])\n    output_shape = _cal_tensor_shape(node.outputs[0])\n\n    if node.get_attr(\'padding\') == b\'VALID\':\n        attrs[\'auto_pad\'] = \'VALID\'\n    elif node.get_attr(\'padding\') == b\'SAME\':\n        if count_dynamic_dim(input_shape) > 1:\n            attrs[\'auto_pad\'] = \'SAME_UPPER\'\n        else:\n            attrs[\'auto_pad\'] = \'NOTSET\'\n            output_padding = [0] * len(kernel_size)\n            attrs[\'pads\'] = _calc_explicit_padding(input_shape,\n                                                   output_shape,\n                                                   output_padding,\n                                                   kernel_size,\n                                                   strides,\n                                                   dilation_rate,\n                                                   list(range(\n                                                       len(input_shape))) if channels_first else [0, 2, 3, 1])\n\n    intermediate_output_name = oopb.apply_conv(adjusted_input_name + transposed_weight,\n                                               name=operator.full_name + \'_conv\',\n                                               **attrs)\n\n    if not channels_first:\n        oopb.apply_op_with_output(""apply_transpose"",\n                                  intermediate_output_name,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_transpose_final\',\n                                  perm=[0, 2, 3, 1])\n    else:\n        oopb.apply_op_with_output(""apply_identity"",\n                                  intermediate_output_name,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_identity_final\')\n\n\n@converter_func(TYPES.MatMul, TYPES.BatchMatMul, TYPES.BatchMatMulV2)\ndef convert_tf_batchmatmul(scope, operator, container):\n    node = operator.raw_operator  # type: tensorflow.Operation\n    oopb = OnnxOperatorBuilder(container, scope)\n\n    tranpose_a = node.get_attr(\'transpose_a\') if node.type == TYPES.MatMul else node.get_attr(\'adj_x\')\n    tranpose_b = node.get_attr(\'transpose_b\') if node.type == TYPES.MatMul else node.get_attr(\'adj_y\')\n\n    input_names = operator.input_full_names\n    for idx_, flag in enumerate([tranpose_a, tranpose_b]):\n        if flag:\n            shape_len = len(node.inputs[idx_].shape)\n            perm = list(range(0, shape_len))[:-2] + [shape_len - 1, shape_len - 2]\n            input_names[idx_] = oopb.apply_transpose(input_names[idx_],\n                                                     name=operator.full_name + \'_transpose_%d\' % idx_,\n                                                     perm=perm)[0]\n\n    oopb.apply_op_with_output(""apply_matmul"",\n                              input_names,\n                              operator.output_full_names,\n                              name=operator.full_name + \'_add\')\n\n\n@converter_func(TYPES.SquaredDifference)\ndef convert_tf_squared_difference(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    sub_node = oopb.apply_sub(operator.input_full_names, name=operator.full_name + \'_sub\')\n    oopb.apply_op_with_output(\'apply_mul\', sub_node + sub_node, operator.output_full_names, name=operator.full_name)\n\n\n@converter_func(TYPES.ConcatV2)\ndef convert_tf_concat_v2(scope, operator, container):\n    node = operator.raw_operator\n    input_name_idx = []\n    original_input_number = len(operator.input_full_names) - 1\n    for idx in range(original_input_number):\n        val = _cal_tensor_value(node.inputs[idx])\n        if not (val is not None and len(val) == 0):\n            input_name_idx.append(idx)\n\n    input_full_names = [operator.input_full_names[idx] for idx in input_name_idx]\n\n    axis_val = _cal_tensor_value(node.inputs[-1]).item(0)\n    if axis_val < 0 and operator.target_opset < 11:\n        input_shape = _cal_tensor_shape(node.inputs[0])\n        axis_val = len(input_shape) + axis_val\n\n    oopb = OnnxOperatorBuilder(container, scope)\n    need_casting = False\n    if operator.target_opset < 8:\n        supported_types = [oopb.float, oopb.float16]\n        dtype = _to_onnx_type(node.outputs[0].dtype)\n        need_casting = dtype not in supported_types\n\n    if need_casting:\n        concat_node = oopb.apply_concat(input_full_names,\n                                        name=operator.full_name + \'_concat\',\n                                        axis=axis_val)\n        oopb.apply_op_with_output(""apply_cast"",\n                                  concat_node,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_cast\',\n                                  to=oopb.float)\n    else:\n        oopb.apply_op_with_output(""apply_concat"",\n                                  input_full_names,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_concat\',\n                                  axis=axis_val)\n\n\n@converter_func(TYPES.Const)\ndef convert_tf_const(scope, operator, container):\n    node = operator.raw_operator\n    np_arr = _cal_tensor_value(node.outputs[0])\n    onnx_tensor = numpy_helper.from_array(np_arr, operator.outputs[0].onnx_name)\n    container.add_initializer_from_tensor(onnx_tensor)\n\n\ndef _spatial_map(shape, perm):\n    new_shape = shape[:]\n    for i in perm:\n        new_shape[i] = shape[perm[i]]\n    return new_shape\n\n\ndef _conv_convert_inputs(oopb, operator, node, attrs, with_kernel=False, new_kernel_shape=None,\n                         output_indices=None):\n    if output_indices is None:\n        output_indices = [0]\n\n    if _is_nhwc(node):\n        # transpose input if needed, no need to record shapes on input\n        transpose_node_1 = oopb.apply_transpose(node.inputs[0].name,\n                                                name=operator.full_name + \'_transpose_1\',\n                                                perm=NHWC_TO_NCHW)\n    else:\n        transpose_node_1 = [node.inputs[0].name]\n\n    # kernel must to be transposed\n    if with_kernel:\n        val = _cal_tensor_value(node.inputs[1])\n        if val is not None:\n            val = val.transpose(HWCN_TO_NCHW)\n            onnx_type = _to_onnx_type(node.inputs[1].dtype)\n            transpose_node_kernel = oopb.apply_identity([(\'_start\', onnx_type, val)],\n                                                        name=operator.full_name + \'_transpose_kernel\')\n        else:\n            transpose_node_kernel = oopb.apply_transpose(node.inputs[1].name,\n                                                         name=operator.full_name + \'_transpose_kernel\',\n                                                         perm=HWCN_TO_NCHW)\n        # TODO, some onnx conv ops require the reshape the kernel (ie. depthwise_conv2d)\n    else:\n        transpose_node_kernel = [node.inputs[1].name]\n\n    conv_node = oopb.apply_conv(transpose_node_1 + transpose_node_kernel,\n                                name=operator.full_name + \'_conv\',\n                                **attrs)\n\n    # transpose outputs if needed\n    if _is_nhwc(node):\n        for idx in output_indices:\n            oopb.add_node_with_output(""Transpose"",\n                                      conv_node,\n                                      operator.outputs[idx].full_name,\n                                      name=operator.full_name + \'_transpose_2_\' + str(idx),\n                                      perm=NCHW_TO_NHWC)\n    else:\n        for idx in output_indices:\n            oopb.apply_op_with_output(""apply_identity"",\n                                      conv_node,\n                                      operator.outputs[idx].full_name,\n                                      name=operator.full_name + \'_identity_\' + str(idx))\n\n\ndef _conv_dims_attr(node, dims):\n    if _is_nhwc(node):\n        if len(dims) == 2:\n            h, w = dims\n        else:\n            n, h, w, c = dims\n    else:\n        n, c, h, w = dims\n    dims = [h, w]\n    return dims\n\n\ndef _convert_tf_conv2d(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    kernel_shape = _cal_tensor_shape(node.inputs[1])[0:2]\n    strides = _conv_dims_attr(node, node.get_attr(\'strides\'))\n    dilations = _conv_dims_attr(node, node.get_attr(\'dilations\'))\n    padding = node.get_attr(\'padding\')\n    spatial = 2\n    attrs = {\'strides\': strides, \'dilations\': dilations, \'kernel_shape\': kernel_shape}\n    if padding:\n        if dilations is None:\n            dilations = [1] * spatial * 2\n        if padding == b\'SAME\':\n            pads = [0] * spatial * 2\n            input_shape = _cal_tensor_shape(node.inputs[0])\n            output_shape = _cal_tensor_shape(node.outputs[0])\n            # transpose shape to nchw\n            if _is_nhwc(node):\n                input_shape = _spatial_map(input_shape, NHWC_TO_NCHW)\n                output_shape = _spatial_map(output_shape, NHWC_TO_NCHW)\n            # calculate pads\n            if any(input_shape[i + 2] is None or output_shape[i + 2] is None for i in range(spatial)):\n                attrs[""auto_pad""] = ""SAME_UPPER""\n            else:\n                for i in range(spatial):\n                    pad = (output_shape[i + 2] - 1) * strides[i] + dilations[i] * kernel_shape[i] - input_shape[i + 2]\n                    pad = max(pad, 0)\n                    pads[i] = pad // 2\n                    pads[i + spatial] = pad - pad // 2\n                attrs[""pads""] = pads\n\n    _conv_convert_inputs(oopb, operator, node, attrs, with_kernel=True)\n\n\n@converter_func(TYPES.Conv1D)\ndef convert_tf_conv1d(scope, operator, container):\n    _convert_tf_conv2d(scope, operator, container)\n\n\n@converter_func(TYPES.Conv2D)\ndef convert_tf_conv2d(scope, operator, container):\n    _convert_tf_conv2d(scope, operator, container)\n\n\n@converter_func(TYPES.Einsum)\ndef convert_tf_einsum(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    equation_str = node.get_attr(\'equation\').decode(""utf-8"")\n    oopb.add_node_with_output(""Einsum"",\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              equation=equation_str,\n                              op_version=12)\n\n\n@converter_func(TYPES.ExpandDims)\ndef convert_tf_expand_dims(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    axis = _cal_tensor_value(node.inputs[1]).tolist()\n    rank = len(_cal_tensor_shape(node.inputs[0]))\n    oopb.apply_op_with_output(""apply_unsqueeze"",\n                              [operator.inputs[0].full_name],\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              axes=[axis],\n                              rank=rank)\n\n\ndef _convert_tf_fused_batch_norm_core(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    input_dim = len(_cal_tensor_shape(node.inputs[0]))\n    epsilon = node.get_attr(\'epsilon\')\n    attrs = {\'epsilon\': epsilon, \'momentum\': 0.9, \'spatial\': 1}\n    outputs_num = min(5, len(node.outputs))\n\n    if _is_nhwc(node):\n        input_perm = [0, input_dim - 1] + list(range(1, input_dim - 1))\n        transpose_node_1 = oopb.apply_transpose(operator.inputs[0].full_name, name=operator.full_name + \'_transpose_1\',\n                                                perm=input_perm)\n        for idx in range(1, 5):\n            transpose_node_1.append(operator.inputs[idx].full_name)\n        batch_norm = oopb.apply_batch_norm(transpose_node_1, name=operator.full_name + \'_batch_norm\',\n                                           outputs_num=outputs_num, **attrs)\n        output_perm = [0] + list(range(2, input_dim)) + [1]\n        final_node = oopb.apply_transpose(batch_norm[0], name=operator.full_name + \'_transpose_2\',\n                                          perm=output_perm)\n    else:\n        transpose_node_1 = []\n        for idx in range(5):\n            transpose_node_1.append(operator.inputs[idx].full_name)\n        batch_norm = oopb.apply_batch_norm(transpose_node_1, name=operator.full_name + \'_batch_norm\',\n                                           outputs_num=outputs_num, **attrs)\n        final_node = batch_norm[0]\n\n    oopb.apply_op_with_output(""apply_identity"",\n                              final_node,\n                              operator.outputs[0].full_name,\n                              name=operator.full_name)\n\n\n@converter_func(TYPES.Fill)\ndef convert_tf_fill(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    if operator.target_opset < 9:\n        fill_shape = _cal_tensor_shape(node.inputs[0])\n        fill_shape_dims = fill_shape[0]\n        val_dtype = _to_onnx_type(node.inputs[1].dtype)\n        need_cast = val_dtype != oopb.float and operator.target_opset < 9\n        if need_cast:\n            cast_input_val = oopb.apply_cast(operator.inputs[1].full_name,\n                                             to=oopb.float,\n                                             name=operator.full_name + \'_input_value_cast\')\n        else:\n            cast_input_val = [operator.inputs[1].full_name]\n        idx = 0\n        for _ in range(fill_shape_dims):\n            cast_input_val = oopb.apply_unsqueeze(cast_input_val,\n                                                  name=operator.full_name + \'_unsqueeze_\' + str(idx),\n                                                  axes=[0])\n            idx += 1\n        cast_input_dim = oopb.apply_cast(operator.inputs[0].full_name,\n                                         to=oopb.int64,\n                                         name=operator.full_name + \'_input_dim_cast\')\n        if need_cast:\n            tile_node = oopb.apply_tile(cast_input_val + cast_input_dim,\n                                        name=operator.full_name + \'_tile\')\n            oopb.apply_op_with_output(""apply_cast"",\n                                      tile_node,\n                                      operator.output_full_names,\n                                      name=operator.full_name)\n        else:\n            oopb.apply_op_with_output(""apply_tile"",\n                                      cast_input_val,\n                                      operator.output_full_names,\n                                      name=operator.full_name,\n                                      repeats=cast_input_dim[0])\n    else:\n        val_dtype = _to_onnx_type(node.inputs[0].dtype)\n        if val_dtype != oopb.int64:\n            cast_input_dim = oopb.apply_cast(operator.inputs[0].full_name,\n                                             to=oopb.int64,\n                                             name=operator.full_name + \'_input_dim_cast\')\n        else:\n            cast_input_dim = [operator.inputs[0].full_name]\n\n        val = _cal_tensor_value(node.inputs[1])\n        value = np.array([val])\n        attrs = {""value"": numpy_helper.from_array(value)}\n        oopb.add_node_with_output(\'ConstantOfShape\',\n                                  cast_input_dim,\n                                  operator.outputs[0].full_name,\n                                  name=operator.full_name,\n                                  **attrs)\n\n\n@converter_func(TYPES.FloorDiv)\ndef convert_tf_floor_div(scope, operator, container):\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n    dtype = _to_onnx_type(node.outputs[0].dtype)\n    if dtype in [oopb.float16, oopb.float, oopb.double]:\n        div_node = oopb.apply_div(operator.input_full_names,\n                                  name=operator.full_name + \'_div\')[0]\n        oopb.apply_op_with_output(\'apply_floor\', div_node,\n                                  operator.outputs[0].full_name,\n                                  name=operator.full_name)\n    else:\n        oopb.apply_op_with_output(\'apply_div\', operator.input_full_names,\n                                  operator.outputs[0].full_name,\n                                  name=operator.full_name)\n\n\n@converter_func(TYPES.FusedBatchNorm)\ndef convert_tf_fused_batch_norm(scope, operator, container):\n    _convert_tf_fused_batch_norm_core(scope, operator, container)\n\n\n@converter_func(TYPES.FusedBatchNormV2)\ndef convert_tf_fused_batch_norm_v2(scope, operator, container):\n    _convert_tf_fused_batch_norm_core(scope, operator, container)\n\n\n@converter_func(TYPES.FusedBatchNormV3)\ndef convert_tf_fused_batch_norm_v3(scope, operator, container):\n    _convert_tf_fused_batch_norm_core(scope, operator, container)\n\n\n@converter_func(TYPES.GatherV2)\ndef convert_tf_gather_v2(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    axis = _cal_tensor_value(node.inputs[2]).tolist()\n    oopb.apply_op_with_output(""apply_gather"",\n                              [operator.inputs[0].full_name, operator.inputs[1].full_name],\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              axis=axis)\n\n\n@converter_func(TYPES.GatherNd)\ndef convert_tf_gather_nd(scope, operator, container):\n    if operator.target_opset < 11:\n        raise ValueError(""GatherND op is not supported for opset < 11"")\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n    indices_dtype = _to_onnx_type(node.inputs[1].dtype)\n    if indices_dtype != oopb.int64:\n        cast_node = oopb.apply_cast(operator.inputs[1].full_name,\n                                    to=oopb.int64,\n                                    name=operator.full_name + \'_cast\')[0]\n    else:\n        cast_node = operator.inputs[1].full_name\n    oopb.add_node_with_output(\'GatherND\',\n                              [operator.inputs[0].full_name, cast_node],\n                              operator.outputs[0].full_name,\n                              name=operator.full_name)\n\n\n@converter_func(TYPES.GreaterEqual)\ndef convert_tf_greater_equal(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.apply_op_with_output(\'apply_greater_or_equal\', operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name)\n\n\n@converter_func(TYPES.LessEqual)\ndef convert_tf_less_equal(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.apply_op_with_output(\'apply_less_or_equal\', operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name)\n\n\n@converter_func(TYPES.LogicalAnd)\ndef convert_tf_logical_and(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.add_node_with_output(\'And\',\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name)\n\n\n@converter_func(TYPES.LogicalNot)\ndef convert_tf_logical_not(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.add_node_with_output(\'Not\',\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name)\n\n\n@converter_func(TYPES.LogSoftmax)\ndef convert_tf_logsoftmax(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    logits_rank = len(_cal_tensor_shape(node.inputs[0]))\n    attrs = _to_onnx_attrs(node)\n    axis = attrs[\'axis\'] if hasattr(attrs, \'axis\') else -1\n    if operator.target_opset < 11 and axis < 0:\n        axis += logits_rank\n\n    oopb.add_node_with_output(\'LogSoftmax\',\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              axis=axis)\n\n\ndef _convert_tf_maximum_minimum(scope, operator, container, oopb, apply_func):\n    node = operator.raw_operator\n    supported_types = [oopb.double, oopb.float, oopb.float16]\n    if container.target_opset >= 12:\n        supported_types.extend([oopb.int32, oopb.int64])\n    output_type = _to_onnx_type(node.outputs[0].dtype)\n    need_cast = False\n    cast_inputs = []\n\n    for idx, inp in enumerate(node.inputs):\n        inp_type = _to_onnx_type(inp.dtype)\n        if inp_type not in supported_types:\n            diff_output = oopb.apply_cast(inp.name,\n                                          to=oopb.float,\n                                          name=operator.full_name + \'_input_\' + str(idx))\n            cast_inputs.extend(diff_output)\n            need_cast = True\n        else:\n            cast_inputs.append(inp.name)\n\n    # tensorflow minimum/maximum does support broadcast, onnx < opset 8 does not.\n    # handle this by doing something like:\n    # y = min(x1, add(x2, sub(x1, x1))), where x1, x2 are the inputs and x2 is a scalar\n    # this will create a tensor of zeros of the shape of x1, adds x2 to it (which broadcasts) and use that for min.\n    broadcast_inputs = []\n    needs_broadcast_op = []\n    if operator.target_opset < 8:\n        output_shape = _cal_tensor_shape(node.outputs[0])\n        has_correct_shape = []\n        for i, input_name in enumerate(node.inputs):\n            input_shape = _cal_tensor_shape(node.inputs[i])\n            if input_shape != output_shape:\n                needs_broadcast_op.append(i)\n            else:\n                has_correct_shape.append(cast_inputs[i])\n\n    if needs_broadcast_op:\n        has_correct_shape = has_correct_shape[0]\n        for i in range(len(cast_inputs)):\n            if i in needs_broadcast_op:\n                # get a tensor with zeros (since there is no Fill op as of opset8)\n                sub_node = oopb.apply_sub([has_correct_shape, has_correct_shape],\n                                          name=operator.full_name + \'_diff_\' + str(i))\n                # use add as \'broadcast\' op\n                add_node = oopb.apply_add([cast_inputs[i]] + sub_node,\n                                          name=operator.full_name + \'_add_\' + str(i))\n                broadcast_inputs.extend(add_node)\n            else:\n                broadcast_inputs.append(cast_inputs[i])\n    else:\n        broadcast_inputs = cast_inputs\n\n    op_postfix = \'_max\' if apply_func == oopb.apply_max else \'_min\'\n    max_node = apply_func(broadcast_inputs,\n                          name=operator.full_name + op_postfix)\n\n    if need_cast:\n        oopb.apply_op_with_output(""apply_cast"",\n                                  max_node,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_castback\',\n                                  to=output_type)\n    else:\n        oopb.apply_op_with_output(""apply_identity"",\n                                  max_node,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_identity\')\n\n\n@converter_func(TYPES.Maximum)\ndef convert_tf_maximum(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    _convert_tf_maximum_minimum(scope, operator, container, oopb, oopb.apply_max)\n\n\n@converter_func(TYPES.Minimum)\ndef convert_tf_minimum(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    _convert_tf_maximum_minimum(scope, operator, container, oopb, oopb.apply_min)\n\n\n@converter_func(TYPES.NonMaxSuppressionV2, TYPES.NonMaxSuppressionV3)\ndef convert_tf_nonmaxsuppression(scope, operator, container):\n    if operator.target_opset < 10:\n        raise ValueError(""NonMaxSuppression op is not supported for opset < 10"")\n    else:\n        oopb = OnnxOperatorBuilder(container, scope)\n        input_0 = oopb.apply_unsqueeze(operator.inputs[0].full_name,\n                                       name=operator.full_name + \'_unsqueeze_0\',\n                                       axes=[0])\n        input_1 = oopb.apply_unsqueeze(operator.inputs[1].full_name,\n                                       name=operator.full_name + \'_unsqueeze_1\',\n                                       axes=[0, 1])\n        input_2 = oopb.apply_cast(operator.inputs[2].full_name,\n                                  to=oopb.int64,\n                                  name=operator.full_name + \'_cast_0\')\n        non_max_v = 10 if operator.target_opset < 11 else 11\n        nonmaxsuppress = oopb.add_node(\'NonMaxSuppression\',\n                                       input_0 + input_1 + input_2 + operator.input_full_names[3:],\n                                       operator.full_name + \'_nonmax\',\n                                       op_version=non_max_v)\n        slice_node = oopb.apply_slice(nonmaxsuppress,\n                                      name=operator.full_name + \'_slice\',\n                                      starts=[2], ends=[3], axes=[1])\n        squeeze_node = oopb.apply_squeeze(slice_node,\n                                          name=operator.full_name + \'_squeeze\',\n                                          axes=[1])\n        oopb.apply_op_with_output(""apply_cast"",\n                                  squeeze_node,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_castback\',\n                                  to=oopb.int32)\n\n\ndef _make_range_const(scope, operator, container, start, limit, delta, onnx_type):\n    start = _cal_tensor_value(start).tolist()\n    limit = _cal_tensor_value(limit).tolist()\n    delta = _cal_tensor_value(delta).tolist()\n    val = np.arange(start, limit, delta)\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.add_node_with_output(\'Identity\',\n                              [(\'_start\', onnx_type, val)],\n                              operator.outputs[0].full_name,\n                              name=operator.full_name + \'_range\')\n\n\ndef _make_range_non_const(scope, operator, container, start, limit, delta, onnx_type):\n    oopb = OnnxOperatorBuilder(container, scope)\n    diff_node = oopb.apply_sub([limit.name, start.name],\n                               name=operator.full_name + \'_diff\')\n    delta_cast = delta.name\n    if onnx_type in [oopb.int32, oopb.int64]:\n        diff_output = oopb.apply_cast(diff_node,\n                                      to=oopb.float,\n                                      name=operator.full_name + \'_cast_diff\')\n        delta_cast = oopb.apply_cast(delta.name,\n                                     to=oopb.float,\n                                     name=operator.full_name + \'_cast_delta\')\n\n    div_node = oopb.apply_div(diff_output + delta_cast,\n                              name=operator.full_name + \'_div\')\n    ceil_node = oopb.add_node(""Ceil"",\n                              div_node,\n                              name=operator.full_name + \'_ceil\')\n    trip_count_node = oopb.apply_cast(ceil_node,\n                                      to=oopb.int64,\n                                      name=operator.full_name + \'_trip_cnt\')\n    loop_inputs = [trip_count_node[0],\n                   # TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE maps BOOL to INT32\n                   # so we need change np.array(True, dtype=\'bool\') to int32 here\n                   (\'_cond\', oopb.bool, np.array(1, dtype=\'int32\')),\n                   start.name]\n    from onnx import helper\n    n1 = helper.make_node(""Identity"", [""cond""], [""cond_out""], name=""n1"")\n    n2 = helper.make_node(""Add"", [""prev"", delta.name], [""current""], name=""n2"")\n    n3 = helper.make_node(""Identity"", [""prev""], [""range""], name=""n3"")\n\n    graph_proto = helper.make_graph(\n        nodes=[n1, n2, n3],\n        name=""test"",\n        inputs=[helper.make_tensor_value_info(""i"", oopb.int64, []),\n                helper.make_tensor_value_info(""cond"", oopb.bool, []),\n                helper.make_tensor_value_info(""prev"", onnx_type, [])],\n        outputs=[helper.make_tensor_value_info(""cond_out"", oopb.bool, []),\n                 helper.make_tensor_value_info(""current"", onnx_type, []),\n                 helper.make_tensor_value_info(""range"", onnx_type, [])],\n        initializer=[]\n    )\n    loop_node = oopb.add_node_all(""Loop"",\n                                  loop_inputs,\n                                  name=operator.full_name + \'_loop\',\n                                  outputs_num=2,\n                                  body=graph_proto)\n    oopb.apply_op_with_output(""apply_identity"",\n                              loop_node[1],\n                              operator.output_full_names,\n                              name=operator.full_name + \'_identity\')\n\n\ndef _make_range(scope, operator, container, start, limit, delta, onnx_type):\n    if all(_cal_tensor_value(n) is not None for n in [start, limit, delta]) is True:\n        _make_range_const(scope, operator, container, start, limit, delta, onnx_type)\n    else:\n        _make_range_non_const(scope, operator, container, start, limit, delta, onnx_type)\n\n\n@converter_func(TYPES.Range)\ndef convert_tf_range(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    if operator.target_opset < 11:\n        onnx_type = _to_onnx_type(node.outputs[0].dtype)\n        _make_range(scope, operator, container, node.inputs[0], node.inputs[1], node.inputs[2], onnx_type)\n    else:\n        oopb.add_node_with_output(""Range"",\n                                  operator.input_full_names,\n                                  operator.outputs[0].full_name,\n                                  name=operator.full_name + \'_range\',\n                                  op_version=11)\n\n\n@converter_func(TYPES.TD_Reshape)\ndef convert_reshape_timedistributed(scope, operator, container):\n    target_shape = operator.get_attr(\'target_shape\')\n    apply_reshape(scope, operator.inputs[0].full_name, operator.outputs[0].full_name, container,\n                  operator_name=operator.full_name, desired_shape=target_shape)\n\n\n@converter_func(TYPES.All, TYPES.Any)\ndef convert_tf_any_all(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    axis = _cal_tensor_value(node.inputs[1]).tolist()\n    axis = [axis] if np.isscalar(axis) else axis\n\n    # It is fine to have nagative reduce_dim.\n    cast_op = oopb.apply_cast(operator.input_full_names[0],\n                              to=oopb.float,\n                              name=operator.full_name + \'_cast\')\n    keepdims = node.get_attr(""keep_dims"")\n    op_type = ""ReduceMin"" if node.type == ""All"" else ""ReduceSum""\n    reduce_op = oopb.add_node(op_type, cast_op,\n                              axes=axis,\n                              keepdims=keepdims,\n                              name=operator.full_name + \'_reduce\')\n    oopb.apply_op_with_output(\'apply_greater\',\n                              [reduce_op, np.array(0, dtype=np.float32)],\n                              operator.output_full_names,\n                              name=operator.full_name)\n\n\n@converter_func(TYPES.Pack)\ndef convert_tf_pack(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    axis = node.get_attr(\'axis\')\n    if axis < 0 and operator.target_opset < 11:\n        axis += len(_cal_tensor_shape(node.inputs[0])) + 1\n\n    inputs = []\n    for i in range(len(node.inputs)):\n        unsqueeze = oopb.add_node(\'Unsqueeze\',\n                                  operator.inputs[i].full_name,\n                                  operator.full_name + \'_unsqueeze\' + str(i), axes=[axis])\n        inputs.append(unsqueeze)\n\n    oopb.apply_op_with_output(""apply_concat"",\n                              inputs,\n                              operator.outputs[0].full_name,\n                              name=operator.full_name + \'_concat\',\n                              axis=axis)\n\n\ndef _convert_tf_pad(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    paddings_value = _cal_tensor_value(node.inputs[1])\n    if paddings_value is None:\n        padding_dtype = _to_onnx_type(node.inputs[1].dtype)\n        if padding_dtype != oopb.int64:\n            cast_node = oopb.apply_cast(operator.input_full_names[1],\n                                        to=oopb.int64,\n                                        name=operator.full_name + \'_paddings_cast\')\n        else:\n            cast_node = operator.input_full_names[1]\n        transpose_node_1 = oopb.apply_transpose(cast_node,\n                                                name=operator.full_name + \'_transpose_1\',\n                                                perm=[1, 0])\n        paddings = oopb.apply_reshape(transpose_node_1,\n                                      name=operator.full_name + \'_reshape\',\n                                      desired_shape=[-1])[0]\n    else:\n        paddings = np.array(_cal_tensor_value(node.inputs[1])).transpose().flatten()\n    attrs = _to_onnx_attrs(node)\n    mode = attrs[""mode""] if hasattr(attrs, \'mode\') else None\n\n    if mode:\n        mode = mode.s.decode(""utf-8"").lower()\n    if mode not in [None, ""constant""]:\n        raise ValueError(mode + "" pad mode is not supported"")\n\n    origin_dtype = _to_onnx_type(node.outputs[0].dtype)\n    if origin_dtype not in [oopb.float16, oopb.float,\n                            oopb.double]:\n        cast_op = oopb.apply_cast(operator.input_full_names[0],\n                                  to=oopb.float,\n                                  name=operator.full_name + \'_cast\')\n    else:\n        cast_op = operator.input_full_names[0]\n\n    if mode in [None, ""constant""] and len(node.inputs) == 3:\n        const_val = _cal_tensor_value(node.inputs[2]).tolist()\n    else:\n        const_val = None\n\n    if operator.target_opset < 11:\n        pad_node = oopb.apply_pad(cast_op,\n                                  name=operator.full_name + \'_pad\',\n                                  mode=mode,\n                                  pads=paddings,\n                                  value=const_val)\n    else:\n        pad_node = oopb.apply_pad(cast_op,\n                                  name=operator.full_name + \'_pad\',\n                                  mode=mode,\n                                  pads=paddings,\n                                  value=const_val,\n                                  onnx_type=_to_onnx_type(node.inputs[0].dtype))\n\n    if origin_dtype not in [oopb.float16, oopb.float,\n                            oopb.double]:\n        oopb.apply_op_with_output(""apply_cast"",\n                                  pad_node,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_castback\',\n                                  to=origin_dtype)\n    else:\n        oopb.apply_op_with_output(""apply_identity"",\n                                  pad_node,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_identity\')\n\n\n@converter_func(TYPES.Pad)\ndef convert_tf_pad(scope, operator, container):\n    _convert_tf_pad(scope, operator, container)\n\n\n@converter_func(TYPES.PadV2)\ndef convert_tf_pad_v2(scope, operator, container):\n    _convert_tf_pad(scope, operator, container)\n\n\ndef _convert_tf_reduce_op(scope, operator, container, onnx_op):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    axes = _cal_tensor_value(node.inputs[1]).tolist()\n    axes = [axes] if np.isscalar(axes) else axes\n\n    if operator.target_opset < 11:\n        input_shape = _cal_tensor_shape(node.inputs[0])\n        if input_shape is None:\n            if any([val < 0 for val in axes]):\n                raise ValueError(""reduce_op: cannot have negative axis because we don\'t know input rank"")\n        else:\n            input_rank = len(input_shape)\n            axes = [val + input_rank if val < 0 else val for val in axes]\n\n    keepdims = node.get_attr(""keep_dims"")\n    oopb.add_node_with_output(onnx_op,\n                              operator.inputs[0].full_name,\n                              operator.outputs[0].full_name,\n                              name=operator.full_name + \'_reduce_min\',\n                              axes=axes, keepdims=keepdims)\n\n\n@converter_func(TYPES.Max)\ndef convert_tf_max(scope, operator, container):\n    _convert_tf_reduce_op(scope, operator, container, \'ReduceMax\')\n\n\n@converter_func(TYPES.Min)\ndef convert_tf_min(scope, operator, container):\n    _convert_tf_reduce_op(scope, operator, container, \'ReduceMin\')\n\n\n@converter_func(TYPES.Mean)\ndef convert_tf_mean(scope, operator, container):\n    _convert_tf_reduce_op(scope, operator, container, \'ReduceMean\')\n\n\n@converter_func(TYPES.Sum)\ndef convert_tf_sum(scope, operator, container):\n    _convert_tf_reduce_op(scope, operator, container, \'ReduceSum\')\n\n\n@converter_func(TYPES.Prod)\ndef convert_tf_prod(scope, operator, container):\n    _convert_tf_reduce_op(scope, operator, container, \'ReduceProd\')\n\n\n@converter_func(TYPES.Reshape)\ndef convert_tf_reshape(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    if _cal_tensor_value(node.inputs[1]) is None:\n        temp_shape_value = node.inputs[1].name\n        shape_value = temp_shape_value\n        shape_dtype = _to_onnx_type(node.inputs[0].dtype)\n        if shape_dtype != oopb.int64:\n            shape_value = oopb.apply_cast(temp_shape_value,\n                                          to=oopb.int64,\n                                          name=operator.full_name + \'_cast\')[0]\n    else:\n        shape_value = _cal_tensor_value(node.inputs[1]).tolist()\n\n    oopb.apply_op_with_output(""apply_reshape"",\n                              operator.inputs[0].full_name,\n                              operator.outputs[0].full_name,\n                              name=operator.full_name + \'_reshape\',\n                              desired_shape=shape_value)\n\n\n@converter_func(TYPES.ScatterNd)\ndef convert_tf_scatter_nd(scope, operator, container):\n    if operator.target_opset < 11:\n        raise ValueError(""ScatterNd op is not supported for opset = "" + str(operator.target_opset))\n    else:\n        oopb = OnnxOperatorBuilder(container, scope)\n        node = operator.raw_operator\n\n        const_shape_dtype = _to_onnx_type(node.inputs[2].dtype)\n        if const_shape_dtype != oopb.int64:\n            const_of_shape_input = oopb.apply_cast(operator.inputs[2].full_name,\n                                                   to=oopb.int64,\n                                                   name=operator.full_name + \'_const_of_shape_input\')\n        else:\n            const_of_shape_input = [operator.inputs[2].full_name]\n\n        np_val = np.array([0], dtype=np.int64)\n        onnx_tensor = numpy_helper.from_array(np_val, operator.inputs[2].full_name + \'_value\')\n        const_of_shape = oopb.add_node(\'ConstantOfShape\',\n                                       const_of_shape_input,\n                                       operator.inputs[2].full_name + \'_const_of_shape\',\n                                       value=onnx_tensor)\n\n        node_input_0_dtype = _to_onnx_type(node.inputs[0].dtype)\n        if node_input_0_dtype != oopb.int64:\n            node_input_0_cast = oopb.apply_cast(operator.inputs[0].full_name,\n                                                to=oopb.int64,\n                                                name=operator.full_name + \'_input_0\')\n        else:\n            node_input_0_cast = [operator.inputs[0].full_name]\n\n        oopb.add_node_with_output(\'ScatterND\',\n                                  [const_of_shape] + node_input_0_cast + [operator.inputs[1].full_name],\n                                  operator.outputs[0].full_name,\n                                  name=operator.full_name + \'_scatter_nd\')\n\n\n@converter_func(TYPES.Select)\ndef convert_tf_select(scope, operator, container):\n    if operator.target_opset < 9:\n        raise ValueError(""Select op is not supported for opset = "" + str(operator.target_opset))\n    else:\n        oopb = OnnxOperatorBuilder(container, scope)\n        node = operator.raw_operator\n        cond_shape = _cal_tensor_shape(node.inputs[0])\n        input_shape = _cal_tensor_shape(node.inputs[1])\n        if input_shape is None:\n            input_shape = _cal_tensor_shape(node.inputs[2])\n        input_rank = len(input_shape)\n        if len(cond_shape) == 1 and input_rank > 1:\n            broadcast_shape = [cond_shape[0]] + [1] * (input_rank - 1)\n            reshape_node = oopb.apply_reshape(operator.inputs[0].full_name,\n                                              name=operator.full_name + \'_reshape\',\n                                              desired_shape=broadcast_shape)\n            input_nodes = reshape_node + operator.input_full_names[1:]\n        else:\n            input_nodes = operator.input_full_names\n\n        oopb.add_node_with_output(\'Where\',\n                                  input_nodes,\n                                  operator.outputs[0].full_name,\n                                  name=operator.full_name + \'_where\',\n                                  op_version=9)\n\n\n@converter_func(TYPES.Size)\ndef convert_tf_size(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    dtype = _to_onnx_type(node.outputs[0].dtype)\n    if dtype != oopb.int64:\n        size_node = oopb.add_node(\'Size\',\n                                  operator.inputs[0].full_name,\n                                  operator.inputs[0].full_name + \'_size\')\n        oopb.apply_op_with_output(""apply_cast"",\n                                  size_node,\n                                  operator.outputs[0].full_name,\n                                  name=operator.full_name + \'_size_cast\',\n                                  to=dtype)\n    else:\n        oopb.add_node_with_output(\'Size\',\n                                  operator.inputs[0].full_name,\n                                  operator.output_full_names,\n                                  name=operator.inputs[0].full_name + \'_size\')\n\n\ndef _convert_tf_resize(scope, operator, container, mode):\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n    shape = _cal_tensor_shape(node.inputs[0])\n    target_shape = _cal_tensor_value(node.inputs[1])\n\n    if shape and shape[1] is not None and shape[2] is not None and target_shape is not None:\n        n, h, w, c = shape\n        nh, nw = target_shape\n        scale_val = np.array([1.0, 1.0, float(nh) / h, float(nw) / w]).astype(np.float32)\n        scales = (\'_scale\', oopb.float, scale_val)\n    else:\n        if operator.target_opset < 10:\n            raise ValueError(""dynamic shape is not supported for Upsample when opset = "" + str(operator.target_opset))\n        input_shape = oopb.add_node(\'Shape\',\n                                    operator.inputs[0].full_name,\n                                    operator.inputs[0].full_name + \'_input_shape\')\n        sliced_score = oopb.add_node(\'Slice\',\n                                     [input_shape,\n                                      (\'_start\', oopb.int64, np.array([1], dtype=\'int64\')),\n                                      (\'_end\', oopb.int64, np.array([3], dtype=\'int64\')),\n                                      (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                      ],\n                                     operator.inputs[0].full_name + \'_sliced\')\n        ori_cast = oopb.add_node(\'Cast\',\n                                 sliced_score,\n                                 operator.inputs[0].full_name + \'_ori_cast\', to=oopb.float)\n        target_cast = oopb.add_node(\'Cast\',\n                                    operator.inputs[1].full_name,\n                                    operator.inputs[1].full_name + \'_target_cast\', to=oopb.float)\n        scales_hw = oopb.add_node(\'Div\',\n                                  [target_cast, ori_cast],\n                                  operator.inputs[1].full_name + \'_scales_hw\')\n        scales = oopb.add_node(\'Concat\',\n                               [(\'_concat\', oopb.float, np.array([1.0, 1.0], dtype=\'float32\')),\n                                scales_hw\n                                ],\n                               operator.inputs[0].full_name + \'_concat\',\n                               axis=0)\n\n    input_nchw = oopb.add_node(\'Transpose\',\n                               operator.inputs[0].full_name,\n                               operator.inputs[0].full_name + \'_transpose\',\n                               perm=[0, 3, 1, 2])\n    attrs = {""mode"": mode}\n    attrs[\'coordinate_transformation_mode\'] = \'asymmetric\'\n    if attrs[\'mode\'] == \'nearest\':\n        attrs[\'nearest_mode\'] = \'floor\'\n    if operator.target_opset < 10:\n        op_type = \'Upsample\'\n    else:\n        op_type = \'Resize\'\n\n    if operator.target_opset < 8:\n        attrs = {""mode"": mode, ""scales"": [1.0, 1.0, float(nh) / h, float(nw) / w]}\n        upsample = oopb.add_node(op_type,\n                                 input_nchw,\n                                 operator.inputs[0].full_name + \'_upsample\',\n                                 **attrs)\n    elif operator.target_opset < 11:\n        upsample = oopb.add_node(op_type,\n                                 [input_nchw,\n                                  scales],\n                                 operator.inputs[0].full_name + \'_upsample\',\n                                 mode=mode)\n    else:\n        upsample = oopb.add_node(op_type,\n                                 [input_nchw,\n                                  (\'_rois\', oopb.float, np.array([0.0, 0.0, 1.0, 1.0], dtype=\'float32\')),\n                                  scales],\n                                 operator.inputs[0].full_name + \'_upsample\',\n                                 **attrs)\n    oopb.add_node_with_output(\'Transpose\',\n                              upsample,\n                              operator.output_full_names,\n                              name=operator.inputs[0].full_name + \'_transpose_2\',\n                              perm=[0, 2, 3, 1])\n\n\n@converter_func(TYPES.ResizeBilinear)\ndef convert_tf_resize_bilinear(scope, operator, container):\n    _convert_tf_resize(scope, operator, container, ""linear"")\n\n\n@converter_func(TYPES.ResizeNearestNeighbor)\ndef convert_tf_resize_nearest_neighbor(scope, operator, container):\n    _convert_tf_resize(scope, operator, container, ""nearest"")\n\n\n@converter_func(TYPES.Round)\ndef convert_tf_round(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    if operator.target_opset < 11:\n        add_output_name = oopb.add_node(\'Add\',\n                                        [operator.inputs[0].full_name,\n                                         (\'_add\', oopb.float, np.array(-0.5, dtype=np.float32))\n                                         ],\n                                        operator.inputs[0].full_name + \'_add\')\n        cast_0 = oopb.add_node(\'Cast\',\n                               add_output_name,\n                               operator.inputs[0].full_name + \'_0_cast\', to=oopb.float)\n        oopb.add_node_with_output(""Ceil"",\n                                  cast_0,\n                                  operator.output_full_names,\n                                  name=operator.full_name)\n    else:\n        oopb.add_node_with_output(""Round"",\n                                  operator.input_full_names,\n                                  operator.output_full_names,\n                                  name=operator.full_name)\n\n\n@converter_func(TYPES.Rsqrt)\ndef convert_tf_rsqrt(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    sqrt_node = oopb.add_node(\'Sqrt\',\n                              operator.inputs[0].full_name,\n                              operator.inputs[0].full_name + \'_sqrt\')\n    oopb.apply_op_with_output(""apply_reciprocal"",\n                              sqrt_node,\n                              operator.output_full_names,\n                              name=operator.full_name + \'_cast\')\n\n\n@converter_func(TYPES.Shape)\ndef convert_tf_shape(scope, operator, container):\n    node = operator.raw_operator\n    dtype = _to_onnx_type(node.outputs[0].dtype)\n    oopb = OnnxOperatorBuilder(container, scope)\n    shape_node = oopb.add_node(\'Shape\',\n                               operator.input_full_names[0],\n                               operator.input_full_names[0] + \'_shape\')\n    if dtype == oopb.int64:\n        oopb.add_node_with_output(\'Identity\',\n                                  shape_node,\n                                  operator.output_full_names,\n                                  operator.inputs[0].full_name + \'_identity\')\n    else:\n        oopb.apply_op_with_output(""apply_cast"",\n                                  shape_node,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_cast\',\n                                  to=dtype)\n\n\n@converter_func(TYPES.Split)\ndef convert_tf_split(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    split_dims = _cal_tensor_value(node.inputs[0]).tolist()\n    oopb.apply_op_with_output(\'apply_split\',\n                              operator.input_full_names[1:],\n                              operator.output_full_names,\n                              operator.inputs[0].full_name + \'_split\',\n                              axis=split_dims)\n\n\n@converter_func(TYPES.SplitV)\ndef convert_tf_splitv(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    split = _cal_tensor_value(node.inputs[1]).tolist()\n    split_dims = _cal_tensor_value(node.inputs[2]).tolist()\n    oopb.apply_op_with_output(\'apply_split\',\n                              operator.input_full_names[0],\n                              operator.output_full_names,\n                              operator.inputs[0].full_name + \'_split\',\n                              split=split,\n                              axis=split_dims)\n\n\n@converter_func(TYPES.Squeeze)\ndef convert_tf_squeeze(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    shape = _cal_tensor_shape(node.inputs[0])\n    axis = node.get_attr(\'squeeze_dims\')\n\n    if axis:\n        neg_axis = any([val < 0 for val in axis])\n        if neg_axis and operator.target_opset < 11:\n            shape_len = len(shape)\n            axis = [a + shape_len if a < 0 else a for a in axis]\n    else:\n        axis = [i for i, j in enumerate(shape) if j == 1]\n\n    if shape is None:\n        raise ValueError(""Squeeze input shape cannot be None for node {}"".format(node.name))\n\n    oopb.add_node_with_output(\'Squeeze\',\n                              operator.input_full_names[0],\n                              operator.output_full_names,\n                              operator.inputs[0].full_name + \'_squeeze\',\n                              axes=axis)\n\n\n@converter_func(TYPES.Tile)\ndef convert_tf_tile(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    cast_1 = oopb.add_node(\'Cast\',\n                           operator.inputs[1].full_name,\n                           operator.inputs[1].full_name + \'_1_cast\', to=oopb.int64)\n    oopb.add_node_with_output(\'Tile\',\n                              [operator.input_full_names[0], cast_1],\n                              operator.output_full_names,\n                              operator.inputs[0].full_name + \'_tile\')\n\n\n@converter_func(TYPES.TopKV2)\ndef convert_tf_topkv2(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    cast_0 = oopb.add_node(\'Cast\',\n                           operator.inputs[0].full_name,\n                           operator.inputs[0].full_name + \'_0_cast\', to=oopb.float)\n    k = _cal_tensor_value(node.inputs[1])\n    if k is None:\n        if operator.target_opset < 10:\n            raise ValueError(""TopK op k need be static until opset 10"")\n        cast_1 = oopb.add_node(\'Cast\',\n                               operator.inputs[1].full_name,\n                               operator.inputs[1].full_name + \'_1_cast\', to=oopb.int64)\n        unsqueeze = oopb.add_node(\'Unsqueeze\',\n                                  cast_1,\n                                  operator.inputs[1].full_name + \'_unsqueeze\', axes=[0])\n        k_value = unsqueeze\n    else:\n        k_value = k.item(0)\n    oopb.apply_op_with_output(\'apply_topk\',\n                              cast_0,\n                              operator.output_full_names,\n                              operator.inputs[0].full_name + \'_topk\',\n                              k=k_value)\n\n\n@converter_func(TYPES.Transpose)\ndef convert_tf_transpose(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    perm = _cal_tensor_value(node.inputs[1])\n    oopb.apply_op_with_output(""apply_transpose"",\n                              operator.inputs[0].full_name,\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              perm=perm)\n\n\n@converter_func(TYPES.Cast)\ndef convert_tf_cast(scope, operator, container):\n    node = operator.raw_operator\n    to = _to_onnx_type(node.get_attr(""DstT""))\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.apply_op_with_output(""apply_cast"",\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              to=to)\n\n\n@converter_func(TYPES.NotEqual)\ndef convert_tf_not_equal(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    if operator.target_opset >= 11:\n        equal_out = oopb.add_node(\'Equal\', [operator.inputs[0].full_name, operator.inputs[1].full_name],\n                                  operator.full_name + \'_mask\')\n        oopb.add_node_with_output(\'Not\', equal_out,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_not\')\n    else:\n        equal_input_0 = oopb.add_node(\'Cast\', [operator.inputs[0].full_name],\n                                      operator.full_name + \'_input_0_cast\', to=6)\n        equal_input_1 = oopb.add_node(\'Cast\', [operator.inputs[1].full_name],\n                                      operator.full_name + \'_input_1_cast\', to=6)\n        equal_out = oopb.add_node(\'Equal\', [equal_input_0, equal_input_1],\n                                  operator.full_name + \'_mask\')\n        oopb.add_node_with_output(\'Not\', equal_out,\n                                  operator.output_full_names,\n                                  name=operator.full_name + \'_not\')\n\n\n@converter_func(TYPES.OneHot)\ndef convert_tf_one_hot(scope, operator, container):\n    if operator.target_opset < 9:\n        raise ValueError(""OneHot op is not supported until opset 9"")\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    axis = node.get_attr(\'axis\')\n\n    depth = oopb.apply_unsqueeze(operator.inputs[1].full_name,\n                                 name=operator.full_name + \'_unsqueeze_1\',\n                                 axes=[0])\n    on_value = oopb.apply_unsqueeze(operator.inputs[2].full_name,\n                                    name=operator.full_name + \'_unsqueeze_2\',\n                                    axes=[0])\n    off_value = oopb.apply_unsqueeze(operator.inputs[3].full_name,\n                                     name=operator.full_name + \'_unsqueeze_3\',\n                                     axes=[0])\n    off_on_value = oopb.apply_concat(off_value + on_value,\n                                     name=operator.full_name + \'_concat\',\n                                     axis=0)\n    oopb.add_node_with_output(\'OneHot\', [operator.inputs[0].full_name] + depth + off_on_value,\n                              operator.output_full_names,\n                              name=operator.full_name + \'_one_hot\', axis=axis)\n\n\n@converter_func(TYPES.Pow)\ndef convert_tf_pow(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    if container.target_opset < 12:\n        supported_types = [oopb.float16, oopb.float, oopb.double]\n        for input_idx_ in range(2):\n            dtype = _to_onnx_type(node.inputs[input_idx_].dtype)\n            if dtype not in supported_types:\n                raise ValueError(""The input type of Pow is not supported for opset < 12."")\n        dtype = _to_onnx_type(node.outputs[0].dtype)\n        if dtype not in supported_types:\n            raise ValueError(""The output type of Pow is not supported for opset < 12."")\n\n    oopb.apply_op_with_output(""apply_pow"",\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name)\n\n\n@converter_func(TYPES.ReadVariableOp)\ndef convert_tf_read_variable_op(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    if len(node.inputs) == 1 and len(node.outputs) == 1:\n        oopb.apply_op_with_output(""apply_identity"",\n                                  operator.input_full_names,\n                                  operator.output_full_names,\n                                  name=operator.full_name)\n\n\n@converter_func(TYPES.Relu6)\ndef convert_tf_relu6(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    np_type = TENSOR_TYPE_TO_NP_TYPE[operator.inputs[0].type.to_onnx_type().tensor_type.elem_type]\n    zero_value = np.zeros(shape=(1,), dtype=np_type)\n    oopb.apply_op_with_output(""apply_relu_6"",\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name + \'_clip\',\n                              zero_value=zero_value)\n\n\n@converter_func(TYPES.Slice)\ndef convert_tf_slice(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    begin = _cal_tensor_value(node.inputs[1])\n    size = _cal_tensor_value(node.inputs[2])\n\n    if begin is not None and size is not None:\n        begin_value = begin.tolist()\n        size_value = size.tolist()\n        end_value = []\n        for begin_, size_ in zip(begin_value, size_value):\n            if size_ == -1 or (begin_ < 0 and (begin_ + size_) >= 0):\n                end_value.append(np.iinfo(np.int64).max)\n            else:\n                end_value.append(begin_ + size_)\n    else:\n        if operator.target_opset < 10:\n            raise ValueError(""Dynamic inputs for tf.slice is not supported until opset 10"")\n\n        dtype = _to_onnx_type(node.inputs[1].dtype)\n        if dtype != oopb.int64:\n            cast_begin = oopb.apply_cast(operator.inputs[1].full_name,\n                                         to=oopb.int64,\n                                         name=operator.full_name + \'_begin_cast\')\n        else:\n            cast_begin = [operator.inputs[1].full_name]\n\n        dtype = _to_onnx_type(node.inputs[2].dtype)\n        if dtype != oopb.int64:\n            cast_size = oopb.apply_cast(operator.inputs[2].full_name,\n                                        to=oopb.int64,\n                                        name=operator.full_name + \'_size_cast\')\n        else:\n            cast_size = [operator.inputs[2].full_name]\n\n        neg_one_size = oopb.add_node(\'Equal\',\n                                     cast_size + [(\'_neg_one\', oopb.int64, np.array(-1, dtype=np.int64))],\n                                     operator.full_name + \'_equal_neg_one\')\n        cast_equal = oopb.apply_cast(neg_one_size,\n                                     to=oopb.int64,\n                                     name=operator.full_name + \'_equal_cast\')\n        value_offset = oopb.apply_mul(\n            cast_equal + [(\'_max_int\', oopb.int64, np.array(np.iinfo(np.int64).max, dtype=np.int64))],\n            name=operator.full_name + \'_mul_max\')\n        size_adjust = oopb.apply_add(cast_size + value_offset,\n                                     name=operator.full_name + \'_size_adjust\')\n        begin_value = cast_begin[0]\n        end_value = oopb.apply_add(cast_begin + size_adjust,\n                                   name=operator.full_name + \'_ends\')[0]\n\n    oopb.apply_op_with_output(""apply_slice"",\n                              operator.inputs[0].full_name,\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              starts=begin_value,\n                              ends=end_value)\n\n\n@converter_func(TYPES.Softmax)\ndef convert_tf_softmax(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    logits_rank = len(_cal_tensor_shape(node.inputs[0]))\n    attrs = _to_onnx_attrs(node)\n    axis = attrs[\'axis\'] if hasattr(attrs, \'axis\') else -1\n    if operator.target_opset < 11 and axis < 0:\n        axis += logits_rank\n\n    oopb.apply_op_with_output(""apply_softmax"",\n                              operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name,\n                              axis=axis)\n\n\n@converter_func(TYPES.Square)\ndef convert_tf_square(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.apply_op_with_output(\'apply_mul\',\n                              operator.input_full_names + operator.input_full_names,\n                              operator.output_full_names,\n                              name=operator.full_name)\n\n\ndef _process_begin_end(new_begin, new_end, stride):\n    if stride >= 0:\n        new_begin.append(0)\n        new_end.append(sys.maxsize)\n    else:\n        new_begin.append(-1)\n        new_end.append(-sys.maxsize)\n\n\ndef _prepare_StridedSlice(node, target_opset):\n    max_size = sys.maxsize\n    begin = _cal_tensor_value(node.inputs[1])\n    if begin is None:\n        begin = [0] * node.inputs[1].shape[0]\n    end = _cal_tensor_value(node.inputs[2])\n    if end is None:\n        dynamic_end = True\n        end = [max_size] * node.inputs[2].shape[0]  # this is dummy and not really used.\n    else:\n        dynamic_end = False\n    strides = _cal_tensor_value(node.inputs[3])\n    if strides is None:\n        strides = [1] * node.inputs[3].shape[0]\n    begin_mask = node.get_attr(""begin_mask"")\n    begin_mask = begin_mask if begin_mask is not None else 0\n    end_mask = node.get_attr(""end_mask"")\n    end_mask = end_mask if end_mask is not None else 0\n    end_mask_array = [0] * node.inputs[2].shape[0]\n    end_mask_temp = end_mask\n    end_mask_array_idx = 0\n    while end_mask_temp > 0:\n        if end_mask_temp & 1:\n            end_mask_array[end_mask_array_idx] = 1\n        end_mask_temp = end_mask_temp >> 1\n        end_mask_array_idx += 1\n\n    new_axis_mask = node.get_attr(""new_axis_mask"")\n    new_axis_mask = new_axis_mask if new_axis_mask is not None else 0\n    shrink_axis_mask = node.get_attr(""shrink_axis_mask"")\n    shrink_axis_mask = shrink_axis_mask if shrink_axis_mask is not None else 0\n    ellipsis_mask = node.get_attr(""ellipsis_mask"")\n    ellipsis_mask = ellipsis_mask if ellipsis_mask is not None else 0\n    extra_mask = new_axis_mask or shrink_axis_mask or ellipsis_mask\n    new_begin = []\n    new_end = []\n    axes = []\n    steps = []\n    # onnx slice op can\'t remove a axis, track axis and add a squeeze op if needed\n    needs_squeeze = []\n    ellipsis_gap = 0\n\n    new_axis_len = 0\n    cur_new_axis_mask = new_axis_mask\n    while cur_new_axis_mask > 0:\n        if cur_new_axis_mask & 1:\n            new_axis_len += 1\n        cur_new_axis_mask = cur_new_axis_mask >> 1\n    new_axis_axes = []\n\n    for idx, begin_item in enumerate(begin):\n        if target_opset < 10 and strides[idx] != 1:\n            raise ValueError(""StridedSlice: only strides=1 are supported, current stride ="" + str(strides[idx]))\n\n        if (ellipsis_mask >> idx) & 1:\n            input_shape = node.inputs[0].shape  # ctx.get_shape(node.input[0])\n            if input_shape is None:\n                raise ValueError(""StridedSlice op {} requires the shape of input"".format(node.name))\n            ellipsis_gap = len(input_shape) + new_axis_len - len(begin)\n            for ellipsis_start_idx in range(idx, idx + ellipsis_gap + 1):\n                new_begin.append(0)\n                new_end.append(max_size)\n                axes.append(ellipsis_start_idx)\n                steps.append(1)\n            continue\n\n        if (new_axis_mask >> idx) & 1:\n            new_axis_axes.append(idx + ellipsis_gap)\n            continue\n\n        end_item = end[idx]\n        axes.append(idx + ellipsis_gap)\n        steps.append(strides[idx])\n\n        if (begin_mask >> idx) & 1 != 0 and (end_mask >> idx) & 1 != 0:\n            _process_begin_end(new_begin, new_end, strides[idx])\n            continue\n\n        if begin_item == 0 and end_item == 0:\n            _process_begin_end(new_begin, new_end, strides[idx])\n            continue\n\n        shrink_mask = (shrink_axis_mask >> idx) & 1\n        if shrink_mask != 0:\n            new_begin.append(begin_item)\n            if begin_item == -1:\n                new_end.append(max_size)\n            else:\n                new_end.append(begin_item + 1)\n            needs_squeeze.append(idx + ellipsis_gap)\n            continue\n\n        if (begin_mask >> idx) & 1 != 0:\n            new_begin.append(0) if strides[idx] >= 0 else new_begin.append(-1)\n            new_end.append(end_item)\n            continue\n\n        if (end_mask >> idx) & 1 != 0:\n            new_begin.append(begin_item)\n            new_end.append(max_size) if strides[idx] >= 0 else new_begin.append(-max_size)\n            continue\n\n        new_begin.append(begin_item)\n        new_end.append(end_item)\n\n    return new_begin, new_end, axes, steps, needs_squeeze, \\\n        begin_mask, end_mask, extra_mask, new_axis_axes, end_mask_array, dynamic_end\n\n\n@converter_func(TYPES.StridedSlice)\ndef convert_tf_strided_slice(scope, operator, container):\n    node = operator.raw_operator\n    new_begin, new_end, axes, steps, needs_squeeze, \\\n        begin_mask, end_mask, extra_mask, new_axis_axes, end_mask_array, dynamic_end = _prepare_StridedSlice(\n            node, operator.target_opset)\n    oopb = OnnxOperatorBuilder(container, scope)\n\n    if len(new_axis_axes) > 0:\n        new_axis_unsqueeze = oopb.add_node(\'Unsqueeze\',\n                                           operator.inputs[0].full_name,\n                                           operator.inputs[0].full_name + \'_unsqueeze\',\n                                           axes=new_axis_axes)\n    else:\n        new_axis_unsqueeze = operator.inputs[0].full_name\n\n    if operator.target_opset < 10:\n        # for now we implement common cases. Things like strides!=1 are not mappable to onnx.\n        if dynamic_end:\n            raise ValueError(""Slice op does not support dynamic input for opset < 10."")\n        cropped_tensor_name = oopb.add_node(\'Slice\',\n                                            new_axis_unsqueeze,\n                                            operator.inputs[0].full_name + \'_cropping\',\n                                            starts=new_begin, ends=new_end, axes=axes)\n    else:\n        if extra_mask or begin_mask:\n            cast_node_begin = True\n        else:\n            start_cast = oopb.add_node(\'Cast\',\n                                       operator.inputs[1].full_name,\n                                       operator.inputs[1].full_name + \'_start_cast\', to=7)\n            cast_node_begin = False\n\n        if extra_mask or end_mask:\n            cast_node_end = True\n        else:\n            end_cast = oopb.add_node(\'Cast\',\n                                     operator.inputs[2].full_name,\n                                     operator.inputs[2].full_name + \'_end_cast\', to=7)\n            cast_node_end = False\n\n        data_shape = oopb.add_node(\'Shape\',\n                                   operator.inputs[0].full_name,\n                                   operator.inputs[0].full_name + \'_shape\',\n                                   op_version=9)\n        data_shape_mul = oopb.apply_mul([data_shape,\n                                         (\'_start\', oopb.int64, np.array(end_mask_array, dtype=np.int64))],\n                                        name=operator.inputs[0].full_name + \'_shape_mul\')\n        end_mask_array_neg = 1 - np.array(end_mask_array, dtype=np.int64)\n        end_cast_0 = oopb.apply_cast(node.inputs[2].name,\n                                     name=node.inputs[2].name + \'_end_cast_0\',\n                                     to=7)\n        end_cast_0_mul = oopb.apply_mul(end_cast_0 +\n                                        [(\'_start\', oopb.int64, np.array(end_mask_array_neg, dtype=np.int64))],\n                                        name=operator.inputs[0].full_name + \'_end_cast_0_mul\')\n        end_combine = oopb.apply_add(data_shape_mul + end_cast_0_mul,\n                                     name=operator.inputs[0].full_name + \'_end_combine\')\n\n        if cast_node_end:\n            if dynamic_end:\n                end_point = end_combine[0]\n            else:\n                end_point = (\'_end\', oopb.int64, np.array(new_end, dtype=np.int64))\n        else:\n            end_point = end_cast\n\n        cropped_tensor_name = oopb.add_node(\'Slice\',\n                                            [new_axis_unsqueeze,\n                                             (\'_start\', oopb.int64,\n                                              np.array(new_begin, dtype=np.int64)) if cast_node_begin else start_cast,\n                                             end_point,\n                                             (\'_axes\', oopb.int64, np.array(axes, dtype=np.int64)),\n                                             (\'_steps\', oopb.int64, np.array(steps, dtype=np.int64))\n                                             ],\n                                            operator.inputs[0].full_name + \'_cropping\')\n\n    if needs_squeeze:\n        oopb.add_node_with_output(\'Squeeze\',\n                                  cropped_tensor_name,\n                                  operator.output_full_names,\n                                  operator.inputs[0].full_name + \'_squeeze\',\n                                  axes=needs_squeeze)\n    else:\n        oopb.add_node_with_output(\'Identity\',\n                                  cropped_tensor_name,\n                                  operator.output_full_names,\n                                  operator.inputs[0].full_name + \'_identity\')\n\n\n@converter_func(TYPES.Unpack)\ndef convert_tf_unpack(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    axis_val = node.get_attr(\'axis\')\n    input_shape = _cal_tensor_shape(node.inputs[0])\n    if axis_val < 0 and operator.target_opset < 11:\n        axis_val = len(input_shape) + axis_val\n\n    split_node = oopb.add_node_all(\'Split\',\n                                   operator.inputs[0].full_name,\n                                   operator.full_name + \'_split\',\n                                   outputs_num=input_shape[axis_val],\n                                   axis=axis_val)\n\n    for i in range(len(split_node)):\n        oopb.apply_op_with_output(""apply_squeeze"",\n                                  split_node[i],\n                                  operator.outputs[i].full_name,\n                                  name=operator.full_name + \'_squeeze_\' + str(i),\n                                  axes=[axis_val])\n\n\ndef _convert_tf_var_handle_helper(scope, operator, container, var_handle_name, graph_op_type):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n\n    if is_tf2:\n        v_output = node.outputs[0].name\n        get_assign_value = False\n        for graph_node_name in node.graph._nodes_by_name:\n            graph_op = node.graph._nodes_by_name[graph_node_name]\n            if graph_op.type == graph_op_type and len(graph_op.inputs) > 1 and v_output == graph_op.inputs[0].name:\n                cur_i = graph_op.inputs[1].op\n                if cur_i.type == \'Const\':\n                    val_type = cur_i.get_attr(\'dtype\')\n                    val_shape = [dim.size for dim in cur_i.get_attr(\'value\').tensor_shape.dim]\n                    if cur_i.get_attr(\'value\').tensor_content != b\'\':\n                        val_arr = np.frombuffer(cur_i.get_attr(\'value\').tensor_content,\n                                                val_type.as_numpy_dtype).reshape(*val_shape)\n                    else:\n                        val = cur_i.get_attr(\'value\').float_val[0]\n                        val_arr = np.full(tuple(val_shape), val)\n                    node_input = [(\'_identity\', _to_onnx_type(val_type), val_arr)]\n                    get_assign_value = True\n                    break\n    else:\n        sess = keras.backend.get_session()\n        if node.type == \'VarHandleOp\':\n            val_arr = sess.run([node.name + ""/Read/ReadVariableOp:0""])[0]\n            graph_op = node.graph._nodes_by_name[node.name + ""/Read/ReadVariableOp""]\n        else:\n            val_arr = sess.run([node.name + "":0""])[0]\n            graph_op = node.graph._nodes_by_name[node.name]\n        val_type = graph_op.get_attr(\'dtype\')\n        node_input = [(\'_identity\', _to_onnx_type(val_type), val_arr)]\n        get_assign_value = True\n\n    if get_assign_value:\n        oopb.add_node_with_output(\'Identity\',\n                                  node_input,\n                                  operator.output_full_names,\n                                  operator.outputs[0].full_name + \'_identity\')\n    else:\n        raise ValueError(var_handle_name + "" op "" + node.name + "" is not properly processed"")\n\n\n@converter_func(TYPES.VarHandleOp)\ndef convert_tf_var_handle_op(scope, operator, container):\n    _convert_tf_var_handle_helper(scope, operator, container, ""VarHandleOp"", ""AssignVariableOp"")\n\n\n@converter_func(TYPES.VariableV2)\ndef convert_tf_variable_v2(scope, operator, container):\n    _convert_tf_var_handle_helper(scope, operator, container, ""VariableV2"", ""Assign"")\n\n\n@converter_func(TYPES.Where)\ndef convert_tf_where(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    where_node = oopb.add_node(\'NonZero\',\n                               operator.inputs[0].full_name,\n                               operator.inputs[0].full_name + \'_non_zero\',\n                               op_version=9)\n    oopb.apply_op_with_output(""apply_transpose"",\n                              where_node,\n                              operator.output_full_names,\n                              name=operator.full_name + \'_transpose\',\n                              perm=list(reversed(range(len(node.outputs[0].shape)))))\n\n\n@converter_func(TYPES.ZerosLike)\ndef convert_tf_zeros_like(scope, operator, container):\n    node = operator.raw_operator\n    oopb = OnnxOperatorBuilder(container, scope)\n    dtype = _to_onnx_type(node.outputs[0].dtype)\n    oopb.apply_op_with_output(\'apply_mul\',\n                              [operator.inputs[0].full_name,\n                               (\'_zero\', dtype, np.zeros((), dtype=np.int64))],\n                              operator.outputs[0].full_name,\n                              name=operator.full_name)\n\n\ndirect_ops = {\n    ""Abs"": (""apply_abs"",),\n    ""Acos"": 7,\n    ""Acosh"": 9,\n    ""Add"": (""apply_add"",),\n    ""AddV2"": (""apply_add"",),\n    ""Asin"": 7,\n    ""Asinh"": 9,\n    ""Atan"": 7,\n    ""Atanh"": 9,\n    ""Ceil"": (""apply_ceil"",),\n    ""Cos"": 7,\n    ""Cosh"": 9,\n    ""Div"": (""apply_div"",),\n    ""Elu"": (""apply_elu"",),\n    ""Equal"": 7,\n    ""Erf"": 9,\n    ""Exp"": (""apply_exp"",),\n    ""Floor"": (""apply_floor"",),\n    ""Greater"": (""apply_greater"",),\n    ""Less"": (""apply_less"",),\n    ""Log"": (""apply_log"",),\n    ""Mul"": (""apply_mul"",),\n    ""Neg"": (""apply_neg"",),\n    ""RealDiv"": (""apply_div"",),\n    ""Reciprocal"": (""apply_reciprocal"",),\n    ""Relu"": (""apply_relu"",),\n    ""Sigmoid"": (""apply_sigmoid"",),\n    ""Sin"": 7,\n    ""Sinh"": 9,\n    ""Softplus"": 1,\n    ""Softsign"": 1,\n    ""Sqrt"": (""apply_sqrt"",),\n    ""StopGradient"": (""apply_identity"",),\n    ""Sub"": (""apply_sub"",),\n    ""Tan"": 7,\n    ""Tanh"": (""apply_tanh"",)\n}\n\n\ndef direct_tf_op_convert(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    type = operator.raw_operator.type\n    item = direct_ops[type]\n    assert item is not None, ""Can\'t find the tf op item.""\n    if isinstance(item, numbers.Integral):\n        oopb.add_node_with_output(type,\n                                  [var_.full_name for var_ in operator.inputs],\n                                  [var_.full_name for var_ in operator.outputs],\n                                  name=operator.raw_operator.name,\n                                  op_version=item\n                                  )\n    else:\n        apply_func_name = item[0]\n        oopb.apply_op_with_output(apply_func_name,\n                                  [var_.full_name for var_ in operator.inputs],\n                                  [var_.full_name for var_ in operator.outputs],\n                                  name=operator.raw_operator.name,\n                                  )\n\n\ndef register_direct_tf_ops():\n    set_converters({k: direct_tf_op_convert for k in direct_ops.keys()})\n'"
keras2onnx/_consts.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\n\n""""""\n    Define the common constant data and type for the converter.\n""""""\n\n\nclass TYPES:\n    # tf-node types:\n    Identity = \'Identity\'\n    Const = \'Const\'\n    AddN = \'AddN\'\n    Any = \'Any\'\n    All = \'All\'\n    ArgMax = \'ArgMax\'\n    ArgMin = \'ArgMin\'\n    BatchMatMul = \'BatchMatMul\'\n    BatchMatMulV2 = \'BatchMatMulV2\'\n    BatchToSpaceND = \'BatchToSpaceND\'\n    BiasAdd = \'BiasAdd\'\n    BiasAddV1 = \'BiasAddV1\'\n    Cast = \'Cast\'\n    ConcatV2 = \'ConcatV2\'\n    Conv1D = \'Conv1D\'\n    Conv2D = \'Conv2D\'\n    Cumsum = \'Cumsum\'\n    DepthToSpace = \'DepthToSpace\'\n    DepthwiseConv2dNative = \'DepthwiseConv2dNative\'\n    Div = \'Div\'\n    Einsum = \'Einsum\'\n    ExpandDims = \'ExpandDims\'\n    Fill = \'Fill\'\n    FloorDiv = \'FloorDiv\'\n    FusedBatchNorm = \'FusedBatchNorm\'\n    FusedBatchNormV2 = \'FusedBatchNormV2\'\n    FusedBatchNormV3 = \'FusedBatchNormV3\'\n    GatherNd = \'GatherNd\'\n    GatherV2 = \'GatherV2\'\n    GreaterEqual = \'GreaterEqual\'\n    LessEqual = \'LessEqual\'\n    LogicalAnd = \'LogicalAnd\'\n    LogicalNot = \'LogicalNot\'\n    LogSoftmax = \'LogSoftmax\'\n    MatMul = \'MatMul\'\n    Max = \'Max\'\n    Maximum = \'Maximum\'\n    Mean = \'Mean\'\n    Min = \'Min\'\n    Minimum = \'Minimum\'\n    NonMaxSuppressionV2 = \'NonMaxSuppressionV2\'\n    NonMaxSuppressionV3 = \'NonMaxSuppressionV3\'\n    NotEqual = \'NotEqual\'\n    OneHot = \'OneHot\'\n    Pack = \'Pack\'\n    Pad = \'Pad\'\n    PadV2 = \'PadV2\'\n    Pow = \'Pow\'\n    Prod = \'Prod\'\n    Range = \'Range\'\n    ReadVariableOp = \'ReadVariableOp\'\n    RealDiv = \'RealDiv\'\n    Relu6 = \'Relu6\'\n    Reshape = \'Reshape\'\n    ResizeBilinear = \'ResizeBilinear\'\n    ResizeNearestNeighbor = \'ResizeNearestNeighbor\'\n    Round = \'Round\'\n    Rsqrt = \'Rsqrt\'\n    ScatterNd = \'ScatterNd\'\n    Select = \'Select\'\n    Shape = \'Shape\'\n    Size = \'Size\'\n    Slice = \'Slice\'\n    Softmax = \'Softmax\'\n    SpaceToBatchND = \'SpaceToBatchND\'\n    Split = \'Split\'\n    SplitV = \'SplitV\'\n    Square = \'Square\'\n    SquaredDifference = \'SquaredDifference\'\n    Squeeze = \'Squeeze\'\n    StridedSlice = \'StridedSlice\'\n    Sum = \'Sum\'\n    Tile = \'Tile\'\n    TopKV2 = \'TopKV2\'\n    Transpose = \'Transpose\'\n    Unpack = \'Unpack\'\n    VarHandleOp = \'VarHandleOp\'\n    VariableV2 = \'VariableV2\'\n    Where = \'Where\'\n    ZerosLike = \'ZerosLike\'\n\n    # converter internal types:\n    TD_Reshape = \'_reshape_timedistributed\'\n\n\nNCHW_TO_NHWC = [0, 2, 3, 1]\nNHWC_TO_NCHW = [0, 3, 1, 2]\nHWCN_TO_NCHW = [3, 2, 0, 1]\nNCHW_TO_HWCN = [2, 3, 1, 0]\n'"
keras2onnx/_graph_cvt.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helpers to convert variables to constants in TensorFlow 2.0.""""""\n# flake8: noqa\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import tensor_shape_pb2\nfrom tensorflow.core.framework import variable_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import meta_graph_pb2\nfrom tensorflow.python.eager import wrap_function\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.grappler import tf_optimizer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.util import object_identity\nfrom tensorflow.python.training.saver import export_meta_graph\n\n\n_CONDITIONAL_OPS = set([""If"", ""StatelessIf""])\n_LOOP_OPS = set([""While"", ""StatelessWhile""])\n_CONTROL_FLOW_OPS = _CONDITIONAL_OPS.union(_LOOP_OPS)\n\n\ndef disable_lower_using_switch_merge(graph_def):\n  """"""Set \'_lower_using_switch_merge\' attributes to False.\n\n  Sets the attribute to False in the NodeDefs in the main graph and the NodeDefs\n  in each function\'s graph.\n\n  Args:\n    graph_def: GraphDef proto.\n\n  Returns:\n    GraphDef\n  """"""\n  output_graph_def = graph_pb2.GraphDef()\n  output_graph_def.CopyFrom(graph_def)\n\n  def disable_control_flow_lowering(node):\n    if node.op in _CONTROL_FLOW_OPS:\n      node.attr[""_lower_using_switch_merge""].b = False\n\n  for node in output_graph_def.node:\n    disable_control_flow_lowering(node)\n\n  if output_graph_def.library:\n    for func in output_graph_def.library.function:\n      for node in func.node_def:\n        disable_control_flow_lowering(node)\n  return output_graph_def\n\n\ndef _run_inline_graph_optimization(func, lower_control_flow):\n  """"""Apply function inline optimization to the graph.\n\n  Returns the GraphDef after Grappler\'s function inlining optimization is\n  applied. This optimization does not work on models with control flow.\n\n  Args:\n    func: ConcreteFunction.\n    lower_control_flow: Boolean indicating whether or not to lower control flow\n      ops such as If and While. (default True)\n\n  Returns:\n    GraphDef\n  """"""\n  graph_def = func.graph.as_graph_def()\n  if not lower_control_flow:\n    graph_def = disable_lower_using_switch_merge(graph_def)\n\n  # In some cases, a secondary implementation of the function (e.g. for GPU) is\n  # written to the ""api_implements"" attribute. (e.g. `tf.keras.layers.LSTM` in\n  # TF2 produces a CuDNN-based RNN for GPU).\n  # This function suppose to inline all functions calls, but ""api_implements""\n  # prevents this from happening. Removing the attribute solves the problem.\n  # To learn more about ""api_implements"", see:\n  #   tensorflow/core/grappler/optimizers/implementation_selector.h\n  for function in graph_def.library.function:\n    if ""api_implements"" in function.attr:\n      del function.attr[""api_implements""]\n\n  meta_graph = export_meta_graph(graph_def=graph_def, graph=func.graph)\n\n  # Clear the initializer_name for the variables collections, since they are not\n  # needed after saved to saved_model.\n  for name in [\n      ""variables"", ""model_variables"", ""trainable_variables"", ""local_variables""\n  ]:\n    raw_list = []\n    for raw in meta_graph.collection_def[""variables""].bytes_list.value:\n      variable = variable_pb2.VariableDef()\n      variable.ParseFromString(raw)\n      variable.ClearField(""initializer_name"")\n      raw_list.append(variable.SerializeToString())\n    meta_graph.collection_def[name].bytes_list.value[:] = raw_list\n\n  # Add a collection \'train_op\' so that Grappler knows the outputs.\n  fetch_collection = meta_graph_pb2.CollectionDef()\n  for array in func.inputs + func.outputs:\n    fetch_collection.node_list.value.append(array.name)\n  meta_graph.collection_def[""train_op""].CopyFrom(fetch_collection)\n\n  # Initialize RewriterConfig with everything disabled except function inlining.\n  config = config_pb2.ConfigProto()\n  rewrite_options = config.graph_options.rewrite_options\n  rewrite_options.min_graph_nodes = -1  # do not skip small graphs\n  rewrite_options.optimizers.append(""function"")\n  return tf_optimizer.OptimizeGraph(config, meta_graph)\n\n\ndef _get_tensor_name(name):\n  """"""Returns the name of the input tensor.\n\n  Args:\n    name: str\n\n  Returns:\n    str\n  """"""\n  return name.split("":"")[0]\n\n\ndef _get_new_function_name(name):\n  """"""Returns the function name with \'_frozen\' appended.\n\n  Args:\n    name: str\n\n  Returns:\n    str\n  """"""\n  return name + ""_frozen""\n\n\ndef _get_node_defs_list(graph_def):\n  """"""Returns a list of NodeDefs in the GraphDef.\n\n  This list consists of all NodeDefs in the main graph as well as all control\n  flow NodeDefs in the functions.\n\n  The remaining NodeDefs in the functions are not included because the op names\n  are not unique and the variables are handled differently than the main graph.\n  The control flow ops need to be extracted because they are need their\n  attributes to be updated similar to the control flow ops in the main graph.\n\n  Args:\n    graph_def: GraphDef proto.\n\n  Returns:\n    [NodeDef]\n  """"""\n  node_defs = list(graph_def.node)\n\n  if graph_def.library:\n    for func in graph_def.library.function:\n      node_defs.extend(\n          [node for node in func.node_def if node.op in _CONTROL_FLOW_OPS])\n  return node_defs\n\n\ndef _get_tensor_data(func):\n  """"""Gets the tensor data for all Placeholders in the model.\n\n  Returns a dictionary that maps the tensor name to a dictionary containing:\n    data: numpy data\n    index: int index in func.graph.captures\n    is_variable: bool indicating whether the tensor is a variable or not\n\n  Args:\n    func: ConcreteFunction.\n\n  Returns:\n    Dict\n  """"""\n  tensor_data = {}\n  map_index_to_variable = {}\n  for var in func.graph.variables:\n    for idx, captured_input in enumerate(func.captured_inputs):\n      if var.handle is captured_input:  # pylint: disable=protected-access\n        map_index_to_variable[idx] = var\n        break\n\n  # Iterates through all captures which are represented as Placeholders.\n  for idx, (val_tensor, name_tensor) in enumerate(func.graph.captures):\n    tensor_name = _get_tensor_name(name_tensor.name)\n    is_variable = idx in map_index_to_variable\n    if is_variable:\n      data = map_index_to_variable[idx].numpy()\n    else:\n      data = val_tensor.numpy()\n    tensor_data[tensor_name] = {\n        ""data"": data,\n        ""index"": idx,\n        ""is_variable"": is_variable,\n    }\n  return tensor_data\n\n\ndef _get_control_flow_function_data(node_defs, tensor_data, name_to_node):\n  """"""Gets the types and shapes for the parameters to the function.\n\n  Creates a map from function name to a list of types and a list of shapes that\n  correspond with the function arguments. The data is primarily determined from\n  the corresponding ""If"" or ""While"" op. If the argument is a resource variable,\n  then the type is determined from the type of the data contained within the\n  Tensor. The shape data is only determined in the case of the ""While"" op.\n\n  `is_also_output_type` is used to identify the ""While"" bodies that require the\n  output types to be updated at the same time the input types are updated.\n\n  Args:\n    node_defs: List of NodeDefs.\n    tensor_data: {str name : Tensor}.\n    name_to_node: Dictionary mapping node name to node object.\n\n  Returns:\n    {str function name : {""types"" : [int representing DataType],\n                          ""shapes"" : [[int] representing TensorShape]],\n                          ""is_also_output_type"" : bool}\n  """"""\n  func_data = {}\n\n  def get_source_node_name_through_identities(node_name):\n    # Trace the source node along with a chain of Identity nodes.\n    # For example, given Plaecholder -> Identity -> Identity -> node_name\n    # The function will return the name of the Placeholder.\n    while name_to_node[node_name].op == ""Identity"":\n      node_name = _get_tensor_name(name_to_node[node_name].input[0])\n    return node_name\n\n  def get_resource_type(node_name):\n    node_name = get_source_node_name_through_identities(node_name)\n\n    numpy_type = tensor_data[node_name][""data""].dtype\n    return dtypes.as_dtype(numpy_type).as_datatype_enum\n\n  def get_resource_shape(node_name):\n    node_name = get_source_node_name_through_identities(node_name)\n\n    return tensor_shape_pb2.TensorShapeProto(dim=[\n        tensor_shape_pb2.TensorShapeProto.Dim(size=dim)\n        for dim in tensor_data[node_name][""data""].shape\n    ])\n\n  def add_value(func_name, arg_types, output_shapes, is_also_output_type):\n    func_data[func_name] = {\n        ""types"": arg_types,\n        ""shapes"": output_shapes,\n        ""is_also_output_type"": is_also_output_type\n    }\n\n  for node in node_defs:\n    if node.op in _CONDITIONAL_OPS:\n      arg_types = [dtype for dtype in node.attr[""Tin""].list.type]\n\n      for idx in range(len(arg_types)):\n        if arg_types[idx] == dtypes.resource:\n          # Skip first index which represents the condition.\n          arg_types[idx] = get_resource_type(node.input[idx + 1])\n\n      add_value(node.attr[""then_branch""].func.name, arg_types, None, False)\n      add_value(node.attr[""else_branch""].func.name, arg_types, None, False)\n    elif node.op in _LOOP_OPS:\n      arg_types = [dtype for dtype in node.attr[""T""].list.type]\n      output_shapes = [shape for shape in node.attr[""output_shapes""].list.shape]\n\n      for idx in range(len(arg_types)):\n        if arg_types[idx] == dtypes.resource:\n          input_name = node.input[idx]\n          arg_types[idx] = get_resource_type(input_name)\n          output_shapes[idx] = get_resource_shape(input_name)\n\n      add_value(node.attr[""body""].func.name, arg_types, output_shapes, True)\n      add_value(node.attr[""cond""].func.name, arg_types, output_shapes, False)\n  return func_data\n\n\ndef _populate_const_op(output_node, node_name, dtype, data, data_shape):\n  """"""Creates a Const op.\n\n  Args:\n    output_node: TensorFlow NodeDef.\n    node_name: str node name.\n    dtype: AttrValue with a populated .type field.\n    data: numpy data value.\n    data_shape: Tuple of integers containing data shape.\n  """"""\n  output_node.op = ""Const""\n  output_node.name = node_name\n  output_node.attr[""dtype""].CopyFrom(dtype)\n  tensor = tensor_util.make_tensor_proto(\n      data, dtype=dtype.type, shape=data_shape)\n  output_node.attr[""value""].tensor.CopyFrom(tensor)\n\n\ndef _populate_identity_op(output_node, input_node):\n  """"""Creates an Identity op from a ReadVariable op.\n\n  Args:\n    output_node: TensorFlow NodeDef.\n    input_node: TensorFlow NodeDef.\n  """"""\n  output_node.op = ""Identity""\n  output_node.name = input_node.name\n  output_node.input.append(input_node.input[0])\n  output_node.attr[""T""].CopyFrom(input_node.attr[""dtype""])\n  if ""_class"" in input_node.attr:\n    output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n\n\ndef _populate_if_op(output_node, input_node, function_data):\n  """"""Updates the type attributes and function names of If or StatelessIf.\n\n  Args:\n    output_node: TensorFlow NodeDef.\n    input_node: TensorFlow NodeDef.\n    function_data: Map of function names to the list of types and shapes that\n      correspond with the function arguments.\n  """"""\n  output_node.CopyFrom(input_node)\n  then_func = input_node.attr[""then_branch""].func.name\n  output_node.attr[""then_branch""].func.name = _get_new_function_name(then_func)\n  output_node.attr[""else_branch""].func.name = _get_new_function_name(\n      input_node.attr[""else_branch""].func.name)\n  output_node.attr[""Tin""].list.CopyFrom(\n      attr_value_pb2.AttrValue.ListValue(\n          type=function_data[then_func][""types""]))\n\n\ndef _populate_while_op(output_node, input_node, function_data):\n  """"""Updates the type attributes and function names of While or StatelessWhile.\n\n  Args:\n    output_node: TensorFlow NodeDef.\n    input_node: TensorFlow NodeDef.\n    function_data: Map of function names to the list of types and shapes that\n      correspond with the function arguments.\n  """"""\n  output_node.CopyFrom(input_node)\n  cond_func = input_node.attr[""cond""].func.name\n  output_node.attr[""cond""].func.name = _get_new_function_name(cond_func)\n  output_node.attr[""body""].func.name = _get_new_function_name(\n      input_node.attr[""body""].func.name)\n  output_node.attr[""T""].list.CopyFrom(\n      attr_value_pb2.AttrValue.ListValue(\n          type=function_data[cond_func][""types""]))\n  output_node.attr[""output_shapes""].list.CopyFrom(\n      attr_value_pb2.AttrValue.ListValue(\n          shape=function_data[cond_func][""shapes""]))\n\n\ndef _construct_concrete_function(func, output_graph_def,\n                                 converted_input_indices):\n  """"""Constructs a concrete function from the `output_graph_def`.\n\n  Args:\n    func: ConcreteFunction\n    output_graph_def: GraphDef proto.\n    converted_input_indices: Set of integers of input indices that were\n      converted to constants.\n\n  Returns:\n    ConcreteFunction.\n  """"""\n  # Create a ConcreteFunction from the new GraphDef.\n  input_tensors = func.graph.internal_captures\n  converted_inputs = object_identity.ObjectIdentitySet(\n      [input_tensors[index] for index in converted_input_indices])\n  not_converted_inputs = [\n      tensor for tensor in func.inputs if tensor not in converted_inputs]\n  not_converted_inputs_map = {\n      tensor.name: tensor for tensor in not_converted_inputs\n  }\n\n  new_input_names = [tensor.name for tensor in not_converted_inputs]\n  new_output_names = [tensor.name for tensor in func.outputs]\n  new_func = wrap_function.function_from_graph_def(output_graph_def,\n                                                   new_input_names,\n                                                   new_output_names)\n\n  # Manually propagate shape for input tensors where the shape is not correctly\n  # propagated. Scalars shapes are lost when wrapping the function.\n  for input_tensor in new_func.inputs:\n    input_tensor.set_shape(not_converted_inputs_map[input_tensor.name].shape)\n  return new_func\n\n\ndef convert_variables_to_constants_v2(func, lower_control_flow=True):\n  """"""Replaces all the variables in a graph with constants of the same values.\n\n  TensorFlow 2.0 function for converting all Variable ops into Const ops holding\n  the same values. This makes it possible to describe the network fully with a\n  single GraphDef file, and allows the removal of a lot of ops related to\n  loading and saving the variables. This function runs Grappler\'s function\n  inlining optimization in order to return a single subgraph.\n\n  The current implementation only works for graphs that do not contain any\n  control flow or embedding related ops.\n\n  Args:\n    func: ConcreteFunction.\n    lower_control_flow: Boolean indicating whether or not to lower control flow\n      ops such as If and While. (default True)\n\n  Returns:\n    ConcreteFunction containing a simplified version of the original.\n  """"""\n  # Inline the graph in order to remove functions when possible.\n  graph_def = _run_inline_graph_optimization(func, lower_control_flow)\n\n  # Gets list of all node defs include those in the library.\n  node_defs = _get_node_defs_list(graph_def)\n\n  # Get mapping from node name to node.\n  name_to_node = {_get_tensor_name(node.name): node for node in node_defs}\n\n  # Get mapping from node name to variable value.\n  tensor_data = _get_tensor_data(func)\n\n  # Get mapping from function name to argument types.\n  function_data = _get_control_flow_function_data(\n      node_defs, tensor_data, name_to_node)\n\n  # Get variable data for all nodes in `node_defs`.\n  reference_variables = {}\n  resource_identities = {}\n  placeholders = {}\n  converted_input_indices = set()\n\n  def _save_placeholder(node_name, dtype):\n    placeholders[node_name] = {\n        ""dtype"": dtype,\n        ""data"": tensor_data[node_name][""data""],\n    }\n    converted_input_indices.add(tensor_data[node_name][""index""])\n\n  for node in node_defs:\n    if node.op in _CONDITIONAL_OPS:\n      # Get dtype and data for resource Placeholders.\n      then_func = node.attr[""then_branch""].func.name\n      arg_types = function_data[then_func][""types""]\n      for idx, input_tensor in enumerate(node.input[1:]):\n        input_name = _get_tensor_name(input_tensor)\n        if input_name in tensor_data:\n          dtype = attr_value_pb2.AttrValue(type=arg_types[idx])\n          _save_placeholder(_get_tensor_name(input_tensor), dtype)\n    elif node.op in _LOOP_OPS:\n      # Get dtype and data for resource Placeholders.\n      cond_func = node.attr[""cond""].func.name\n      arg_types = function_data[cond_func][""types""]\n      for idx, input_tensor in enumerate(node.input):\n        input_name = _get_tensor_name(input_tensor)\n        if input_name in tensor_data:\n          dtype = attr_value_pb2.AttrValue(type=arg_types[idx])\n          _save_placeholder(_get_tensor_name(input_tensor), dtype)\n    elif (node.op == ""Identity"" and node.attr[""T""].type == dtypes.resource and\n          name_to_node[_get_tensor_name(node.input[0])].op in _LOOP_OPS):\n      # Store the dtype for Identity resource ops that are outputs of While ops.\n      while_node = name_to_node[_get_tensor_name(node.input[0])]\n      body_func = while_node.attr[""body""].func.name\n      input_data = node.input[0].split("":"")\n      idx = 0 if len(input_data) == 1 else int(input_data[1])\n\n      dtype = attr_value_pb2.AttrValue(\n          type=function_data[body_func][""types""][idx])\n      resource_identities[node.name] = dtype\n    elif node.op == ""VariableV2"":\n      # Get data for VariableV2 ops (reference variables) that cannot be lifted.\n      with func.graph.as_default():\n        identity_node = array_ops.identity(\n            func.graph.as_graph_element(node.name + "":0""))\n      reference_variables[node.name] = (\n          func.prune([], [identity_node.name])()[0])\n    elif node.name in tensor_data and not tensor_data[node.name][""is_variable""]:\n      # Get dtype and data for non-variable Placeholders (ex. values for 1.X\n      # Const ops that are loaded as Placeholders in 2.0)\n      _save_placeholder(node.name, node.attr[""dtype""])\n    elif node.op in [""ReadVariableOp"", ""ResourceGather"", ""ResourceGatherNd"", ""AssignSubVariableOp""]:\n      # Get dtype and data for Placeholder ops associated with ReadVariableOp\n      # and ResourceGather ops. There can be an Identity in between the\n      # resource op and Placeholder. Store the dtype for the Identity ops.\n      input_name = _get_tensor_name(node.input[0])\n      while name_to_node[input_name].op == ""Identity"":\n        resource_identities[input_name] = node.attr[""dtype""]\n        input_name = _get_tensor_name(name_to_node[input_name].input[0])\n      if name_to_node[input_name].op != ""Placeholder"":\n        raise ValueError(""Cannot find the Placeholder op that is an input ""\n                         ""to the ReadVariableOp."")\n      _save_placeholder(input_name, node.attr[""dtype""])\n\n  # Reconstruct the graph with constants in place of variables.\n  output_graph_def = graph_pb2.GraphDef()\n\n  for input_node in graph_def.node:\n    output_node = output_graph_def.node.add()\n    # Convert VariableV2 ops to Const ops.\n    if input_node.name in reference_variables:\n      data = reference_variables[input_node.name]\n      dtype = attr_value_pb2.AttrValue(type=data.dtype.as_datatype_enum)\n      _populate_const_op(output_node, input_node.name, dtype, data.numpy(),\n                         data.shape)\n    # Convert Placeholder ops to Const ops.\n    elif input_node.name in placeholders:\n      data = placeholders[input_node.name][""data""]\n      dtype = placeholders[input_node.name][""dtype""]\n      _populate_const_op(output_node, input_node.name, dtype, data, data.shape)\n    # Update the dtype for Identity ops that are inputs to ReadVariableOps.\n    elif input_node.name in resource_identities:\n      output_node.CopyFrom(input_node)\n      output_node.attr[""T""].CopyFrom(resource_identities[input_node.name])\n    # Convert ReadVariableOps to Identity ops.\n    elif input_node.op == ""ReadVariableOp"":\n      _populate_identity_op(output_node, input_node)\n    # Convert ResourceGather to Gather ops with a Const axis feeding into it.\n    elif input_node.op == ""AssignSubVariableOp"":\n      output_node.op = ""Sub""\n      output_node.name = input_node.name\n      output_node.input.extend(input_node.input)\n      output_node.attr[""T""].CopyFrom(input_node.attr[""dtype""])\n      if ""_class"" in input_node.attr:\n        output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n    elif input_node.op == ""ResourceGather"":\n      if input_node.attr[""batch_dims""].i != 0:\n        raise ValueError(""batch_dims != 0 is not supported by freeze_graph."")\n      output_axis_node = output_graph_def.node.add()\n      axis_node_name = input_node.name + ""/axis""\n      axis_dtype = input_node.attr[""Tindices""]\n      axis_data = np.array(input_node.attr[""batch_dims""].i)\n      _populate_const_op(output_axis_node, axis_node_name, axis_dtype,\n                         axis_data, axis_data.shape)\n\n      output_node.op = ""GatherV2""\n      output_node.name = input_node.name\n      output_node.input.extend(\n          [input_node.input[0], input_node.input[1], axis_node_name])\n      output_node.attr[""Tparams""].CopyFrom(input_node.attr[""dtype""])\n      output_node.attr[""Tindices""].CopyFrom(input_node.attr[""Tindices""])\n      output_node.attr[""Taxis""].CopyFrom(axis_dtype)\n      if ""_class"" in input_node.attr:\n        output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n    elif input_node.op == ""ResourceGatherNd"":\n      output_node.op = ""GatherNd""\n      output_node.name = input_node.name\n      output_node.input.extend(\n          [input_node.input[0], input_node.input[1]])\n      output_node.attr[""Tparams""].CopyFrom(input_node.attr[""dtype""])\n      output_node.attr[""Tindices""].CopyFrom(input_node.attr[""Tindices""])\n      if ""_class"" in input_node.attr:\n        output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n    # Update the function names and argument types for the conditional ops.\n    elif input_node.op in _CONDITIONAL_OPS:\n      _populate_if_op(output_node, input_node, function_data)\n    elif input_node.op in _LOOP_OPS:\n      _populate_while_op(output_node, input_node, function_data)\n    else:\n      output_node.CopyFrom(input_node)\n\n  # Add functions to reconstructed graph.\n  if graph_def.library:\n    library = output_graph_def.library\n\n    for input_library_func in graph_def.library.function:\n      orig_func_name = input_library_func.signature.name\n      new_func_name = _get_new_function_name(orig_func_name)\n\n      # Do not copy any functions that aren\'t being used in the graph. Any\n      # functions that are not used by control flow should have been inlined.\n      if orig_func_name not in function_data:\n        continue\n\n      output_library_func = library.function.add()\n      for key, value in input_library_func.ret.items():\n        output_library_func.ret[key] = value\n      for key, value in input_library_func.control_ret.items():\n        output_library_func.control_ret[key] = value\n\n      # Update the input types in the function signature. Update the output\n      # types for functions that are while loop bodies.\n      output_library_func.signature.CopyFrom(input_library_func.signature)\n      output_library_func.signature.name = new_func_name\n      for dtype, arg in zip(function_data[orig_func_name][""types""],\n                            output_library_func.signature.input_arg):\n        arg.type = dtype\n      if function_data[orig_func_name][""is_also_output_type""]:\n        for dtype, arg in zip(function_data[orig_func_name][""types""],\n                              output_library_func.signature.output_arg):\n          arg.type = dtype\n\n      # Update the NodeDefs.\n      func_variables = {\n          node.name: node.input[0]\n          for node in input_library_func.node_def\n          if node.op in [""ReadVariableOp"", ""AssignSubVariableOp""]\n      }\n\n      for input_node in input_library_func.node_def:\n        output_node = output_library_func.node_def.add()\n        # Convert ReadVariableOps to Identity ops.\n        if input_node.op == ""ReadVariableOp"":\n          _populate_identity_op(output_node, input_node)\n        # Update the function names and argument types for the conditional ops.\n        elif input_node.op in _CONDITIONAL_OPS:\n          _populate_if_op(output_node, input_node, function_data)\n        elif input_node.op in _LOOP_OPS:\n          _populate_while_op(output_node, input_node, function_data)\n        else:\n          output_node.CopyFrom(input_node)\n          # Convert :value to :output for ops that use the ReadVariableOp.\n          for idx, full_name in enumerate(input_node.input):\n            input_name = _get_tensor_name(full_name)\n            if input_name in func_variables:\n              full_name_parts = full_name.split("":"")\n              full_name_parts[1] = ""output""\n              input_name = "":"".join(full_name_parts)\n              output_node.input[idx] = input_name\n\n  output_graph_def.versions.CopyFrom(graph_def.versions)\n  return output_graph_def, converted_input_indices\n'"
keras2onnx/_parser_1x.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom typing import Iterable\n\nfrom .common import k2o_logger\nfrom .funcbook import get_converter\nfrom ._parser_tf import infer_variable_type, tsname_to_node, adjust_input_batch_size\n\n\ndef extract_inbound_nodes(layer):\n    if hasattr(layer, \'inbound_nodes\'):\n        return layer.inbound_nodes\n    elif hasattr(layer, \'_inbound_nodes\'):\n        return layer._inbound_nodes\n    else:\n        raise ValueError(""Failed to find inbound_nodes and _inbound_nodes when parsing %s"" % layer.name)\n\n\ndef list_input_tensors(node):\n    """"""\n    Since Tensorflow 1.14, sometimes the node.input_tensors may not be a list, though the word is plural.\n    """"""\n    return [node.input_tensors] if hasattr(node.input_tensors, \'dtype\') else node.input_tensors\n\n\ndef list_output_tensors(node):\n    """"""\n    Since Tensorflow 1.14, sometimes the node.output_tensors may not be a list, though the output_tensors is plural.\n    """"""\n    return [node.output_tensors] if hasattr(node.output_tensors, \'dtype\') else node.output_tensors\n\n\ndef list_input_shapes(node):\n    """"""\n    Since Tensorflow 1.14, sometimes the node.input_shapes may not be a list, though the input_shapes is plural.\n    """"""\n    return node.input_shapes if isinstance(node.input_shapes[0], Iterable) else [node.input_shapes]\n\n\ndef list_output_shapes(node):\n    """"""\n    Since Tensorflow 1.14, sometimes the node.output_shapes may not be a list, though the output_shapes is plural.\n    """"""\n    return node.output_shapes if isinstance(node.output_shapes[0], Iterable) else [node.output_shapes]\n\n\ndef list_input_mask(layer):\n    if hasattr(layer, \'input_mask\'):\n        if hasattr(layer.input_mask, \'dtype\'):\n            return [layer.input_mask]\n        if layer.input_mask is not None:\n            return [ts_ for ts_ in layer.input_mask if ts_ is not None]\n\n    return []\n\n\ndef list_output_mask(layer):\n    if hasattr(layer, \'output_mask\'):\n        if hasattr(layer.output_mask, \'dtype\'):\n            return [layer.output_mask]\n        if layer.output_mask is not None:\n            return [ts_ for ts_ in layer.output_mask if ts_ is not None]\n\n    return []\n\n\ndef on_parsing_keras_layer(graph, node_list, layer, kenode, model, varset, prefix=None):\n    operator = varset.declare_local_operator(type(layer), raw_model=layer, op_name=layer.name)\n    operator.nodelist = node_list\n\n    inputs = list_input_tensors(kenode)\n    outputs = list_output_tensors(kenode)\n\n    # This layer will be visited because its output is other layer\'s input\n    assert len(node_list) == 0 or node_list[0] in [ts_.op for ts_ in outputs]\n\n    if prefix is None:  # prefix is designed for the distinguish among the shared model instances.\n        prefix = \'\'\n\n    kenode_input_shapes = kenode.input_shapes if isinstance(kenode.input_shapes, list) else [kenode.input_shapes]\n    for n_, i_ in enumerate(inputs):\n        iname = prefix + i_.name\n        k2o_logger().debug(\'\\tinput : \' + iname)\n        var_type = adjust_input_batch_size(infer_variable_type(i_, varset.target_opset, kenode_input_shapes[n_]))\n        i0 = varset.get_local_variable_or_declare_one(iname, var_type)\n        operator.add_input(i0)\n\n    if hasattr(layer, \'input_mask\') and layer.input_mask is not None:\n        in_mask = layer.input_mask if isinstance(layer.input_mask, (list, tuple)) else [layer.input_mask]\n        for im_ in [m_ for m_ in in_mask if m_ is not None]:\n            mts_name = im_.name  # input mask in a shared model is not supported yet, why is it needed?\n            k2o_logger().debug(\'\\tinput mask: \' + mts_name)\n            mts_var = varset.get_local_variable_or_declare_one(mts_name, infer_variable_type(im_, varset.target_opset))\n            operator.add_input_mask(mts_var)\n\n    kenode_output_shapes = kenode.output_shapes if isinstance(kenode.output_shapes, list) else [kenode.output_shapes]\n    for n_, o_ in enumerate(outputs):\n        oname = prefix + o_.name\n        k2o_logger().debug(\'\\toutput: \' + oname)\n        o1 = varset.get_local_variable_or_declare_one(oname,\n                                                      infer_variable_type(o_, varset.target_opset,\n                                                                          kenode_output_shapes[n_]))\n        operator.add_output(o1)\n\n    if hasattr(layer, \'output_mask\') and layer.output_mask is not None:\n        out_mask = layer.output_mask if isinstance(layer.output_mask, (list, tuple)) else [layer.output_mask]\n        for om_ in [m_ for m_ in out_mask if m_ is not None]:\n            mts_name = prefix + om_.name\n            k2o_logger().debug(\'\\toutput mask: \' + mts_name)\n            mts_var = varset.get_local_variable_or_declare_one(mts_name, infer_variable_type(om_, varset.target_opset))\n            operator.add_output_mask(mts_var)\n\n    if hasattr(layer, \'mask_value\') and layer.mask_value is not None:\n        operator.mask_value = layer.mask_value\n\n    cvt = get_converter(operator.type)\n    if cvt is not None and hasattr(cvt, \'shape_infer\'):\n        operator.shape_infer = cvt.shape_infer\n\n    return operator\n\n\ndef build_opdict_from_keras(model):\n    output_dict = {}\n    for l_ in model.layers:\n        if hasattr(l_, \'layers\'):\n            submodel_dict = build_opdict_from_keras(l_)\n            shared_layer = False\n            for node_ in extract_inbound_nodes(l_):\n                shared_layer |= any(\n                    ts_.name not in submodel_dict for ts_ in list_output_tensors(node_))\n                if shared_layer:\n                    break\n            if not shared_layer:  # shared layer(model) will be processed as a whole.\n                output_dict.update(submodel_dict)\n                continue\n\n        for node_ in extract_inbound_nodes(l_):\n            for ts_ in list_output_tensors(node_):\n                output_dict[ts_.name] = (l_, model)\n\n        for ts_ in list_output_mask(l_):\n            output_dict[ts_.name] = (l_, model)\n\n    return {tsname_to_node(n_): v_ for n_, v_ in output_dict.items()}\n'"
keras2onnx/_parser_tf.py,10,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport re\nimport tensorflow as tf\nfrom onnxconverter_common import Int32TensorType, Int64TensorType, FloatTensorType, DoubleTensorType, BooleanTensorType\nfrom .common import k2o_logger, get_default_batch_size\nfrom .funcbook import get_converter\n\nfrom .proto import keras\nfrom .proto.tfcompat import normalize_tensor_shape, is_subclassed\nfrom .ke2onnx import keras_layer_spec\nfrom ._consts import TYPES\nfrom ._tf_utils import is_placeholder_node, tsname_to_node\n\n\ndef infer_variable_type(tensor, opset, inbound_node_shape=None):\n    tensor_shape = []\n    if inbound_node_shape is None:\n        if tensor.shape not in (tf.TensorShape(None), tf.TensorShape([])):\n            if opset > 8:\n                tensor_shape = normalize_tensor_shape(tensor.shape)\n            else:  # most inference engine has problem with unset dim param if they released around opset 8 publish\n                tensor_shape = [\'None\' if d is None else d for d in normalize_tensor_shape(tensor.shape)]\n    else:\n        tensor_shape = list(inbound_node_shape)\n\n    # Determine the tensor\'s element type\n    tensor_type = tensor.dtype.base_dtype\n    if tensor.dtype == \'resource\':\n        node_attr = tensor.op.node_def.attr\n        tensor_type = node_attr[\'dtype\'].type\n        tensor_shape = [\'None\' if d.size is None else d.size for d in node_attr[\'shape\'].shape.dim]\n    if tensor_type in [tf.int8, tf.int16, tf.int32]:\n        return Int32TensorType(shape=tensor_shape)\n    elif tensor_type == tf.int64:\n        return Int64TensorType(shape=tensor_shape)\n    elif tensor_type in [tf.float16, tf.float32]:\n        return FloatTensorType(shape=tensor_shape)\n    elif tensor_type == tf.float64:\n        return DoubleTensorType(shape=tensor_shape)\n    elif tensor_type == tf.bool:\n        return BooleanTensorType(shape=tensor_shape)\n    else:\n        raise ValueError(\n            ""Unable to find out a correct type for tensor type = {} of {}"".format(tensor_type, tensor.name))\n\n\ndef adjust_input_batch_size(var_type):\n    if len(var_type.shape) > 0 and var_type.shape[0] is None:\n        var_type.shape = [get_default_batch_size()] + var_type.shape[1:]\n    return var_type\n\n\ndef adjust_input_output_size(var_type, dim_variable_counter):\n    if len(var_type.shape) > 0:\n        for dim in range(1, len(var_type.shape)):\n            if var_type.shape[dim] is None:\n                dim_variable_counter += 1\n                var_type.shape[dim] = \'M\' + str(dim_variable_counter)\n    return dim_variable_counter\n\n\ndef _get_layer_name(reserved, ts_or_op):\n    return ts_or_op.rsplit(\'/\', 1)[0]\n\n\ndef _get_input_mask(layer):\n    # type: (keras.models.Layer) -> []\n    if hasattr(layer, \'input_mask\') and layer.input_mask is not None:\n        return layer.input_mask if isinstance(layer.input_mask, (list, tuple)) else [layer.input_mask]\n    return []\n\n\ndef _get_output_mask(layer):\n    # type: (keras.models.Layer) -> []\n    if hasattr(layer, \'output_mask\') and layer.output_mask is not None:\n        return layer.output_mask if isinstance(layer.output_mask, (list, tuple)) else [layer.output_mask]\n    return []\n\n\nclass LayerInfo(object):\n    def __init__(self, _ly):\n        self.layer = _ly\n        self.inputs = []\n        self.outputs = []\n        self.nodelist = []\n\n    @staticmethod\n    def create_single_node(node, visited):\n        info = LayerInfo(None)\n        info.inputs = list(node.inputs)\n        info.outputs = list(node.outputs)\n        info.nodelist = [node]\n\n        # const_nodes = [ts_.op for ts_ in info.inputs if ts_.op.type == ""Const"" and ts_.op not in visited]\n        # info.nodelist.extend(const_nodes)\n        # info.inputs = [ts_ for ts_ in info.inputs if ts_.op not in info.nodelist]\n        return info\n\n    @staticmethod\n    def create(node, layer, outputs_map, inference_nodeset):\n        graph = node.graph\n        layer_info = LayerInfo(layer)\n        # find the output\n        next_itr = set()\n        if node.type == TYPES.Identity:  # the case on not subclassing model\n            fstr_list, fx_list = (None, None)\n        else:\n            fstr_list, fx_list = keras_layer_spec(type(layer))\n        fx_layer_name = _get_layer_name\n        if fstr_list is not None:\n            fx_layer_name = fx_list[0]\n        layer_name = fx_layer_name(fstr_list, node.name)\n        for nn_, layer_info_ in outputs_map.items():\n            if layer_info_[0] == layer and fx_layer_name(fstr_list, nn_) == layer_name:\n                op_node = graph.get_operation_by_name(tsname_to_node(nn_))\n                next_itr.add(op_node)\n                layer_info.outputs.extend(op_node.outputs)\n\n        visited = set()\n        while next_itr:\n            visited |= next_itr\n            next_itr.clear()\n            for n_ in visited:\n                for i_ in n_.inputs:\n                    # in layer_spec model, the layer name will be checked\n                    if fstr_list is not None and i_.op.name.find(layer_name) == -1:\n                        continue\n                    if i_.op in visited or i_.op not in inference_nodeset:\n                        continue\n                    if (not is_placeholder_node(i_.op)) and i_.op.name in outputs_map:\n                        continue\n                    next_itr.add(i_.op)\n\n        layer_info.nodelist = list(visited)\n        return layer_info\n\n\ndef is_subclassing(model):\n    subclassed = not (model._is_graph_network or  # pylint:disable=protected-access\n                      isinstance(model, keras.engine.sequential.Sequential))\n    if subclassed:\n        return True\n\n    def subclassed_layer(layer):\n        if hasattr(layer, \'layers\'):\n            if any(is_subclassed(l_) for l_ in layer.layers):\n                return True\n            for l_ in layer.layers:\n                if subclassed_layer(l_):\n                    return True\n        return False\n\n    return subclassed_layer(model)\n\n\ndef _get_layers(tf_utils, layer):\n    if hasattr(layer, \'layers\'):\n        return layer.layers\n    if hasattr(layer, \'submodules\'):\n        sub_layers = layer.submodules\n        if len(sub_layers) == 0:\n            return None\n        return sub_layers[0].layers if isinstance(sub_layers[0], tf_utils.ListWrapper) else sub_layers\n    return None\n\n\ndef _layer_name_dict(tf_utils, layer, prefix, parent=None):\n    output_dict = {}\n    sub_layers = layer if isinstance(layer, list) else _get_layers(tf_utils, layer)\n\n    if sub_layers is not None:\n        for l_ in sub_layers:\n            if isinstance(l_, list):\n                submodel_dict = _layer_name_dict(tf_utils, l_, prefix, layer)\n            else:\n                prefix_l = ""{}/{}"".format(prefix, l_.name)\n                submodel_dict = _layer_name_dict(tf_utils, l_, prefix_l, layer)\n            output_dict.update(submodel_dict)\n\n    output_dict[prefix] = (layer, parent)\n    return output_dict\n\n\ndef _to_tf_ops(layer_name, fstr, ops_table):\n    ops = []\n    op_name = fstr.format(layer_name) if fstr is not None else None\n    if op_name is None:\n        return ops\n\n    if re.match(r"".+_\\d+$"", layer_name):  # if layer name already numbered, skipped then.\n        return ops\n\n    idx = 0\n    while True:\n        op_name = fstr.format(""%s_%d"" % (layer_name, idx + 1))\n        if op_name in ops_table:\n            ops.append(ops_table[op_name])\n        else:\n            break\n        idx += 1\n\n    return ops\n\n\ndef build_layer_outputs(model, graph, outputs):\n    # type: (keras.Model, tf.Graph, []) -> {}\n\n    from tensorflow.python.training.tracking import data_structures as tf_utils\n    output_dict = {}\n    layer_dict = _layer_name_dict(tf_utils, model, model.name)\n\n    ops_table = {op_.name: op_ for op_ in graph.get_operations()}\n\n    def add_output_node(graph, op, fx_list, layer_name):\n        output_node_name = op.name\n        if len(fx_list) > 1:  # if there is no output node function.\n            # fx_[1] is output node redirect function.\n            output_node_name = fx_list[1](lobj, op)\n        assert graph.get_operation_by_name(output_node_name) is not None, ""Parsing layer({}) failed."".format(lobj)\n        if output_node_name not in output_dict:  # if there is already a same kind of layer, not overwrite it.\n            output_dict[output_node_name] = layer_dict[layer_name]\n\n    for ln_, layer_info_ in layer_dict.items():\n        lobj = layer_info_[0]\n        fstr_list, fx_list = keras_layer_spec(type(lobj))\n        if fstr_list is None:\n            continue\n\n        for fstr_ in fstr_list:\n            op_name = fstr_.format(ln_)\n            if op_name not in ops_table:\n                continue\n            add_output_node(graph, ops_table[op_name], fx_list, ln_)\n\n    # now process the case when a layer was re-used several times in one model.\n    for ln_, layer_info_ in layer_dict.items():\n        lobj = layer_info_[0]\n        fstr_list, fx_list = keras_layer_spec(type(lobj))\n        if fstr_list is None:\n            continue\n\n        for fstr_ in fstr_list:\n            for op_ in _to_tf_ops(ln_, fstr_, ops_table):\n                add_output_node(graph, op_, fx_list, ln_)\n\n    return output_dict\n\n\ndef extract_outputs_from_subclassing_model(model, output_dict, input_names, output_names):\n    from tensorflow.python.keras.saving import saving_utils as _saving_utils\n    from tensorflow.python.util import object_identity\n    from ._graph_cvt import convert_variables_to_constants_v2 as _convert_to_constants\n\n    function = _saving_utils.trace_model_call(model)\n    concrete_func = function.get_concrete_function()\n    for k_, v_ in concrete_func.structured_outputs.items():\n        output_names.extend([ts_.name for ts_ in v_.op.outputs])\n    output_dict.update(build_layer_outputs(model, concrete_func.graph, concrete_func.outputs))\n    graph_def, converted_input_indices = _convert_to_constants(\n        concrete_func, lower_control_flow=True)\n    input_tensors = concrete_func.graph.internal_captures\n    converted_inputs = object_identity.ObjectIdentitySet(\n        [input_tensors[index] for index in converted_input_indices])\n    input_names.extend([\n        tensor.name for tensor in concrete_func.inputs if tensor not in converted_inputs])\n\n    with tf.Graph().as_default() as tf_graph:\n        tf.import_graph_def(graph_def, name=\'\')\n\n    return tf_graph\n\n\ndef extract_outputs_from_inbound_nodes(model):\n    output_dict = {}\n    if hasattr(model, \'layers\'):\n        for l_ in model.layers:\n            output_dict.update(extract_outputs_from_inbound_nodes(l_))\n\n    if hasattr(model, \'inbound_nodes\'):\n        for nd_ in model.inbound_nodes:\n            output_tensors = [nd_.output_tensors] if hasattr(nd_.output_tensors, \'dtype\') else \\\n                nd_.output_tensors\n            for ts_ in output_tensors:\n                op_name = tsname_to_node(ts_.name)\n                if op_name not in output_dict:\n                    output_dict[op_name] = (model, None)\n\n        for ts_ in _get_output_mask(model):\n            if ts_ is not None:\n                output_dict[ts_.op.name] = (model, model)\n\n    return output_dict\n\n\ndef build_layer_output_from_model(model, output_dict, input_names, output_names):\n    if is_subclassing(model):\n        tf.compat.v1.enable_tensor_equality()  # re-enable tensor tensor equality for subclassing model.\n        return extract_outputs_from_subclassing_model(model, output_dict, input_names, output_names)\n    else:\n        graph = model.outputs[0].graph\n        output_names.extend([n.name for n in model.outputs])\n        output_dict.update(extract_outputs_from_inbound_nodes(model))\n        return graph\n\n\ndef on_parsing_keras_layer_v2(graph, layer_info, varset, prefix=None):\n    layer = layer_info.layer\n    node_list = layer_info.nodelist\n    operator = varset.declare_local_operator(type(layer), raw_model=layer, op_name=layer.name)\n    operator.nodelist = node_list\n\n    if prefix is None:  # prefix is designed for the distinguish among the shared model instances.\n        prefix = \'\'\n\n    input_masks = _get_input_mask(layer)\n    output_masks = _get_output_mask(layer)\n    for o_ in layer_info.outputs:\n        if o_ not in output_masks:  # the layer converter will handle output_mask by itself.\n            oname = prefix + o_.name\n            k2o_logger().debug(\'\\toutput: \' + oname)\n            o1 = varset.get_local_variable_or_declare_one(oname, infer_variable_type(o_, varset.target_opset))\n            operator.add_output(o1)\n\n    for i_ in layer_info.inputs:\n        if i_ not in input_masks:  # the layer converter will handle input_mask by itself.\n            iname = prefix + i_.name\n            k2o_logger().debug(\'\\tinput : \' + iname)\n            var_type = adjust_input_batch_size(infer_variable_type(i_, varset.target_opset))\n            i0 = varset.get_local_variable_or_declare_one(iname, var_type)\n            operator.add_input(i0)\n\n    for om_ in [m_ for m_ in output_masks if m_ is not None]:\n        mts_name = prefix + om_.name\n        k2o_logger().debug(\'\\toutput mask: \' + mts_name)\n        mts_var = varset.get_local_variable_or_declare_one(mts_name, infer_variable_type(om_, varset.target_opset))\n        operator.add_output_mask(mts_var)\n\n    for im_ in [m_ for m_ in input_masks if m_ is not None]:\n        mts_name = im_.name  # input mask in a shared model is not supported yet, why is it needed?\n        k2o_logger().debug(\'\\tinput mask: \' + mts_name)\n        mts_var = varset.get_local_variable_or_declare_one(mts_name, infer_variable_type(im_, varset.target_opset))\n        operator.add_input_mask(mts_var)\n\n    if hasattr(layer, \'mask_value\') and layer.mask_value is not None:\n        operator.mask_value = layer.mask_value\n\n    cvt = get_converter(operator.type)\n    if cvt is not None and hasattr(cvt, \'shape_infer\'):\n        operator.shape_infer = cvt.shape_infer\n\n    # in some cases, some constants will be used by an operator outside of this layer.\n    for nd_ in layer_info.nodelist:\n        if nd_.type == \'Const\' and nd_.name not in varset.variable_name_mapping:\n            operator = varset.declare_local_operator(nd_.type, raw_model=nd_, op_name=nd_.name)\n            o1 = varset.get_local_variable_or_declare_one(nd_.name,\n                                                          infer_variable_type(nd_.outputs[0], varset.target_opset))\n            operator.add_output(o1)\n\n    return operator\n'"
keras2onnx/_tf_ops.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom onnxconverter_common.oopb import OnnxOperatorBuilder\nfrom .funcbook import converter_func\nfrom ._tf_utils import tf_attrs_to_onnx as _to_onnx_attrs\nfrom ._tf_utils import cal_tensor_shape as _cal_tensor_shape\n\n\ndef _random_converter(scope, operator, container):\n    tf_op = operator.raw_operator\n    op_type = tf_op.type\n    if op_type == \'RandomStandardNormal\':\n        op_type = \'RandomNormalLike\'\n    else:\n        op_type = op_type + \'Like\'\n    inputs = [var_.full_name for var_ in operator.inputs]\n\n    attrs = {}\n    if \'seed2\' in _to_onnx_attrs(tf_op):\n        attrs[\'seed\'] = float(tf_op.get_attr(\'seed2\'))\n    with OnnxOperatorBuilder(container, scope).as_default(tf_op.name) as oopb:\n        cast_n = oopb.cast(inputs, to=oopb.int64)\n        const_op = oopb.add_node(\'ConstantOfShape\', cast_n, op_version=9)\n        oopb.add_node(op_type, const_op,\n                      outputs=[var_.full_name for var_ in operator.outputs],\n                      op_version=1, **attrs)\n\n\n@converter_func(\n    \'RandomNormal\',\n    \'RandomStandardNormal\',\n    \'RandomUniform\')\ndef convert_tf_random_standard_normal(scope, operator, container):\n    _random_converter(scope, operator, container)\n\n\n@converter_func(\'Select\', \'SelectV2\')\ndef convert_tf_select(scope, operator, container):\n    tf_op = operator.raw_operator\n    shape_i0 = _cal_tensor_shape(tf_op.inputs[0])\n    target_shape = _cal_tensor_shape(tf_op.inputs[1])\n    if len(target_shape) == 0:\n        target_shape = _cal_tensor_shape(tf_op.inputs[2])\n    input0 = operator.input_full_names[0]\n    with OnnxOperatorBuilder(container, scope).as_default(operator.full_name) as oopb:  # type: OnnxOperatorBuilder\n        if len(shape_i0) == 1 and len(target_shape) > 1:\n            input0 = oopb.unsqueeze(input0, axes=list(range(len(target_shape)))[1:])\n        oopb.add_node(""Where"", [input0] + operator.input_full_names[1:],\n                      outputs=operator.output_full_names,\n                      op_version=9)\n\n\n@converter_func(\'LogicalNot\', \'LogicalAnd\', \'LogicalOr\')\ndef convert_tf_logical_ops(scope, operator, container):\n    onnx_type = operator.type[len(\'Logical\'):]\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.add_node(onnx_type,\n                  operator.input_full_names,\n                  name=operator.full_name,\n                  outputs=operator.output_full_names,\n                  op_version=1)\n\n\ndef pass_thru_converter(scope, operator, container):\n    """"""\n    This converter is to copy the original graph node with its def into a ONNX node format.\n    """"""\n    tf_op = operator.raw_operator\n    attrs = _to_onnx_attrs(tf_op)\n\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.add_node(operator.type,\n                  operator.input_full_names,\n                  name=operator.full_name,\n                  outputs=operator.output_full_names,\n                  op_domain=\'ai.onnx.contrib\',\n                  op_version=1,\n                  **attrs)\n'"
keras2onnx/_tf_utils.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport tensorflow\nimport numpy as np\nfrom typing import Union\nfrom onnx import mapping\n\n\ndef is_placeholder_node(node):\n    return len(node.inputs) == 0 and node.type in [\'Placeholder\', ""PlaceholderV2"", \'PlaceholderWithDefault\'] and \\\n           node.outputs[0].dtype.name != \'resource\'\n\n\ndef tsname_to_node(name):\n    return name.split(\':\')[0]\n\n\ndef is_nhwc(node):\n    return node.get_attr(\'data_format\') == b\'NHWC\'\n\n\n_MAX_FOLDING_NODE_NUMBER = 15\n\n\ndef _count_input_nodes(tensor):  # type: (tensorflow.Tensor)->int\n    nodes_to_keep = set()\n    node_inputs = [tensor.op]\n    while node_inputs:\n        nd_ = node_inputs[0]\n        del node_inputs[0]\n        if nd_ in nodes_to_keep:\n            continue\n\n        if is_placeholder_node(nd_):\n            return -1\n        nodes_to_keep.add(nd_)\n        if len(nodes_to_keep) >= _MAX_FOLDING_NODE_NUMBER:\n            return -1\n\n        node_inputs.extend(in_.op for in_ in nd_.inputs)\n\n    return len(nodes_to_keep)\n\n\ndef cal_tensor_value(tensor):  # type: (tensorflow.Tensor)->Union[np.ndarray, None]\n    if _count_input_nodes(tensor) < 0:\n        return None\n\n    node = tensor.op\n    if node.type in [""Const"", ""ConstV2""]:\n        make_ndarray = tensorflow.make_ndarray\n        np_arr = make_ndarray(node.get_attr(""value""))\n        return np_arr\n    else:\n        try:\n            cls_sess = tensorflow.Session if hasattr(tensorflow, \'Session\') else tensorflow.compat.v1.Session\n            with cls_sess(graph=node.graph) as sess:\n                np_arr = sess.run(tensor)\n                return np_arr\n        except (ValueError, tensorflow.errors.InvalidArgumentError, tensorflow.errors.OpError):\n            return None\n\n\ndef cal_tensor_shape(tensor):\n    if len(tensor.shape) > 0 and hasattr(tensor.shape[0], \'value\'):\n        return [x.value for x in tensor.shape]\n    else:\n        return list(tensor.shape)\n\n\ndef to_onnx_type(dt_type):\n    # TensorFlow data types integrate seamlessly with numpy\n    return mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype(dt_type.as_numpy_dtype)]\n\n\ndef tf_attrs_to_onnx(node):\n    attrs = {}\n    for s_ in node.node_def.attr:\n        if s_.startswith(\'T\'):  # all T starts attr is TF internal.\n            continue\n        v = node.get_attr(s_)\n        if isinstance(v, tensorflow.dtypes.DType):\n            v = to_onnx_type(v)\n        attrs[s_] = v\n    return attrs\n'"
keras2onnx/cli.py,1,"b'import os\nimport fire\nimport onnx\nimport tensorflow as tf\nfrom .main import convert_keras\n\n\ndef main(input_file, output_file=None, opset=None, channel_first=None):\n    """"""\n    A command line interface for Keras model to ONNX converter.\n    :param input_file: the original model file path, could be a folder name of TF saved model\n    :param output_file: the converted ONNX model file path (optional)\n    :param opset: the target opset for the ONNX model.\n    :param channel_first: the input name needs to be transposed as NCHW\n    :return:\n    """"""\n\n    if not os.path.exists(input_file):\n        print(""File or directory name \'{}\' is invalid!"".format(input_file))\n        return\n\n    file_ext = os.path.splitext(input_file)\n    if output_file is None:\n        output_file = file_ext[0] + \'.onnx\'\n\n    assert file_ext[-1] == \'.h5\', ""Unknown file extension.""\n    kml = tf.keras.models.load_model(input_file)\n    oxml = convert_keras(kml, kml.model, \'\', opset, channel_first)\n    onnx.save_model(oxml, output_file)\n\n\nif __name__ == \'__main__\':\n    # the color output doesn\'t work on some Windows cmdline tools\n    if os.environ.get(\'OS\', \'\') == \'Windows_NT\':\n        os.environ.update(ANSI_COLORS_DISABLED=\'1\')\n    fire.Fire(main)\n'"
keras2onnx/funcbook.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom uuid import uuid4\nimport re\nimport six\n\n_converters = {}\nfb_key, fb_id, fb_additional = range(3)\n\n\ndef set_converter(op_type, functor):\n    _converters[op_type] = functor\n    return functor\n\n\ndef get_converter(op_type):\n    return _converters.get(op_type)\n\n\ndef create_pattern_dict():\n    dict_p = {}\n    for k_, v_ in six.iteritems(_converters):\n        if hasattr(v_, 'patterns') and len(v_.patterns) > 0:\n            dict_p[re.compile(v_.patterns[0])] = (k_, uuid4(), [re.compile(p) for p in v_.patterns[1:]])\n\n    return dict_p\n\n\ndef set_converters(op_conv_dict):\n    _converters.update(op_conv_dict)\n\n\ndef converter_func(*types):\n    def my_func(func):\n        for type_ in types:\n            set_converter(type_, func)\n        return func\n\n    return my_func\n"""
keras2onnx/main.py,6,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport logging\nfrom onnxconverter_common.onnx_ex import get_maximum_opset_supported\nfrom .proto import keras, is_tf_keras\nfrom .proto.tfcompat import tensorflow as tf\nfrom .proto.tfcompat import is_tf2, dump_graph_into_tensorboard\nfrom .proto import onnx\nfrom .topology import convert_topology\nfrom .ke2onnx import static_set_ke2onnx_converters\nfrom .parser import parse_graph, parse_graph_modeless\nfrom .topology import Topology\nfrom .common.utils import set_logger_level, k2o_logger\nfrom .funcbook import set_converter\nfrom ._tf_utils import tsname_to_node\nfrom ._builtin import register_direct_tf_ops\nfrom ._parser_1x import build_opdict_from_keras\nfrom ._parser_tf import build_layer_output_from_model\n\n\ndef convert_keras(model, name=None, doc_string=\'\', target_opset=None,\n                  channel_first_inputs=None, debug_mode=False, custom_op_conversions=None):\n    # type: (keras.Model, str, str, int, [], bool, {}) -> onnx.ModelProto\n    """"""\n    :param model: keras model\n    :param name: the converted onnx model internal name\n    :param doc_string: doc string\n    :param target_opset: the targeted onnx model opset\n    :param channel_first_inputs: A list of channel first input\n    :param debug_mode: will enable the log and try to convert as much as possible on conversion\n    :param custom_op_conversions: the handler for custom operator conversion\n    :return an ONNX ModelProto\n    """"""\n    if isinstance(model, tf.keras.Model) and not is_tf_keras:\n        raise Exception(""This is a tensorflow keras model, but keras standalone converter is used."" +\n                        "" Please set environment variable TF_KERAS = 1 before importing keras2onnx."")\n\n    set_logger_level(logging.DEBUG if debug_mode else logging.INFO)\n    if is_tf2:\n        from tensorflow.python.eager import context\n        k2o_logger().info(""tf executing eager_mode: {}"".format(context.executing_eagerly()))\n        if hasattr(model, \'run_eagerly\'):\n            k2o_logger().info(""tf.keras model eager_mode: {}"".format(model.run_eagerly))\n    if debug_mode:\n        print(model.summary())\n\n    name = name or model.name\n    cvt_default_opset = get_maximum_opset_supported()\n    if target_opset is None:\n        target_opset = cvt_default_opset\n    elif target_opset > cvt_default_opset:\n        raise RuntimeError(\n            ""The opset {} conversion not support yet, the current maximum opset version supported is {}."".format(\n                target_opset, cvt_default_opset))\n    input_names = []\n    output_names = []\n    output_dict = {}\n    if is_tf2 and is_tf_keras:\n        tf_graph = build_layer_output_from_model(model, output_dict, input_names, output_names)\n    else:\n        tf_graph = model.outputs[0].graph if is_tf2 else keras.backend.get_session().graph\n        output_dict = build_opdict_from_keras(model)\n        output_names = [n.name for n in model.outputs]\n\n    static_set_ke2onnx_converters(set_converter)\n    register_direct_tf_ops()\n    dump_graph_into_tensorboard(tf_graph)\n    topology = Topology(model, tf_graph,\n                        target_opset=target_opset,\n                        custom_op_dict=custom_op_conversions)\n    topology.debug_mode = debug_mode\n    if (not model.inputs) or (not model.outputs):\n        # Since Tensorflow 2.2, For the subclassed tf.keras model, there is no inputs/outputs info ...\n        # ... stored in model object any more.\n        parse_graph_modeless(topology, tf_graph, target_opset, input_names, output_names, output_dict)\n    else:\n        parse_graph(topology, tf_graph, target_opset, output_names, output_dict)\n    topology.compile()\n\n    return convert_topology(topology, name, doc_string, target_opset, channel_first_inputs)\n\n\ndef build_io_names_tf2onnx(model):\n    return {\n        \'input_names\': [n_.name for n_ in model.inputs],\n        \'output_names\': [n_.name for n_ in model.outputs]\n    }\n\n\ndef _freeze_graph(session, keep_var_names=None, output_names=None):\n    graph = tf.get_default_graph()\n    freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n    input_graph_def = graph.as_graph_def()\n    for node in input_graph_def.node:\n        node.device = """"\n    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n        session, input_graph_def, output_names, freeze_var_names)\n    return frozen_graph_def\n\n\ndef export_tf_frozen_graph(model, keep_var_names=None, output_names=None):\n    """"""\n    Freezes internal tensorflow graph for the specified keras model.\n    :return The frozen graph object.\n    """"""\n    if is_tf2:\n        raise RuntimeError(""Only Tensorflow 1.x supported."")\n    session = keras.backend.get_session()\n    graph = model.outputs[0].graph if is_tf2 else session.graph\n    with graph.as_default():\n        output_names = output_names or \\\n                       [tsname_to_node(n_) for n_ in build_io_names_tf2onnx(model)[\'output_names\']]\n        return _freeze_graph(session, keep_var_names, output_names)\n'"
keras2onnx/parser.py,5,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport re\nimport queue\n\nfrom .proto import keras, is_tf_keras\nfrom .proto.tfcompat import tensorflow as tf\nfrom .proto.tfcompat import is_tf2\nfrom .common import k2o_logger\nfrom .topology import Topology\nfrom .funcbook import get_converter, set_converter\nfrom ._consts import TYPES\nfrom ._tf_ops import pass_thru_converter\nfrom ._parser_tf import (infer_variable_type, LayerInfo, is_placeholder_node,\n                         tsname_to_node, on_parsing_keras_layer_v2, adjust_input_batch_size as _adjust_input_batch_size,\n                         adjust_input_output_size as _adjust_input_output_size)\nfrom ._parser_1x import (extract_inbound_nodes,\n                         list_input_tensors, list_input_mask, list_output_mask,\n                         list_output_tensors, list_input_shapes, list_output_shapes, on_parsing_keras_layer)\n\n\ndef _find_node(nodes, name):\n    try:\n        opname = tsname_to_node(name)\n        return next(n_ for n_ in nodes if n_.name == opname)\n    except StopIteration:\n        return None\n\n\ndef _locate_inputs_by_node(node_list, varset):\n    inputs = {}\n    for n_ in node_list:\n        assert n_ in node_list\n\n        for i_ in n_.inputs:\n            op = i_.op\n            if op.name[0] == \'^\':\n                continue\n            if (not is_placeholder_node(op)) and op in node_list:\n                continue\n\n            if i_ not in inputs:\n                v0 = varset.get_local_variable_or_declare_one(i_.name, infer_variable_type(i_, varset.target_opset))\n                inputs[i_] = v0\n\n    return list(inputs.values()), list(inputs.keys())\n\n\ndef _locate_outputs(node_list, varset):\n    var_output = []\n    nodes = []\n    for n_ in varset.variable_name_mapping.keys():\n        node = _find_node(node_list, n_)\n        if node is not None and (not is_placeholder_node(node)):\n            nodes.append(node)\n\n    assert nodes\n    for n0_ in nodes:\n        for n_ in n0_.outputs:\n            var_output.append(\n                varset.get_local_variable_or_declare_one(n_.name, infer_variable_type(n_, varset.target_opset)))\n\n    return var_output\n\n\ndef _is_relevant_keras_node(model, node):\n    # type: (keras.Model, object) -> bool\n    if not hasattr(model, \'_nodes_by_depth\'):\n        return True  # \'Sequential\' object has no attribute \'_nodes_by_depth\' in the legacy keras version.\n\n    for v in model._nodes_by_depth.values():\n        if node in v:\n            return True\n    return False\n\n\ndef _on_parsing_time_distributed_layer(graph, node_list, layer, model, varset, prefix=None):\n    """"""\n        This conversion supports timedistributed wrapper partially where the layer itself can be converted by onnx.\n    """"""\n    inputs = []\n    ishapes = []\n    outputs = []\n    oshapes = []\n    num_relevant_keras_node = 0\n    for nb_ in extract_inbound_nodes(layer):\n        if _is_relevant_keras_node(model, nb_):\n            inputs += list_input_tensors(nb_)\n            ishapes += list_input_shapes(nb_)\n            outputs += list_output_tensors(nb_)\n            oshapes += list_output_shapes(nb_)\n            num_relevant_keras_node = num_relevant_keras_node + 1\n\n    assert num_relevant_keras_node == 1\n\n    prefix = prefix or \'\'\n    i_ = inputs[0]\n    iname = prefix + i_.name\n    k2o_logger().debug(\'td_layer input: \' + iname)\n    i0 = varset.get_local_variable_or_declare_one(iname, infer_variable_type(i_, varset.target_opset))\n    o1_reshape_shape = (-1,) + oshapes[0][2:]\n    i0_reshape_name = i_.op.name + \'_reshape_0:0\'\n    i0_reshape = varset.declare_local_variable(i0_reshape_name, infer_variable_type(i_, varset.target_opset))\n    i0_reshape_shape = (-1,) + ishapes[0][2:]\n    i0_reshape.type.shape = i0_reshape_shape\n    operator_reshape_0 = varset.declare_local_operator(TYPES.TD_Reshape,\n                                                       op_name=layer.name + \'_reshape_0\', target_shape=i0_reshape_shape)\n    operator_reshape_0.add_input(i0)\n    operator_reshape_0.add_output(i0_reshape)\n\n    o_ = outputs[0]\n    oname = prefix + o_.name\n    k2o_logger().debug(\'td_layer output: \' + oname)\n    o1 = varset.get_local_variable_or_declare_one(oname, infer_variable_type(o_, varset.target_opset))\n    oshapes1 = [-1 if s_ is None else s_ for s_ in oshapes[0]]\n    operator_reshape_1 = varset.declare_local_operator(TYPES.TD_Reshape,\n                                                       op_name=layer.name + \'_reshape_1\', target_shape=oshapes1)\n    operator_reshape_1.add_output(o1)\n    o1_reshape_name = o_.op.name + \'_reshape_1:0\'\n    o1_reshape = varset.declare_local_variable(o1_reshape_name, infer_variable_type(o_, varset.target_opset))\n    o1_reshape.type.shape = o1_reshape_shape\n    operator_reshape_1.add_input(o1_reshape)\n\n    if isinstance(layer.layer, keras.Model):\n        kenode = extract_inbound_nodes(layer.layer)[0]\n        intop = varset.declare_local_operator(TYPES.Identity)\n        intop.add_input(i0_reshape)\n        intop.add_output(varset.get_local_variable_or_declare_one(list_input_tensors(kenode)[0].name))\n        _on_parsing_model_layer(layer.layer, graph, kenode, varset)\n        intop = varset.declare_local_operator(TYPES.Identity)\n        intop.add_input(varset.get_local_variable_or_declare_one(list_output_tensors(kenode)[0].name))\n        intop.add_output(o1_reshape)\n    else:\n        operator = varset.declare_local_operator(type(layer.layer), raw_model=layer.layer, op_name=layer.name)\n        operator.nodelist = node_list\n        operator.add_input(i0_reshape)\n        operator.add_output(o1_reshape)\n        cvt = get_converter(type(layer.layer))\n        if cvt is not None and hasattr(cvt, \'shape_infer\'):\n            operator.shape_infer = cvt.shape_infer\n\n\ndef _check_layer_converter_availability(sub_model):\n    for l_ in sub_model.layers:\n        if isinstance(l_, keras.Model):\n            exist = _check_layer_converter_availability(l_)\n        else:\n            layer_type = type(l_)\n            exist = get_converter(layer_type) or \\\n                layer_type in [keras.layers.InputLayer, keras.layers.wrappers.TimeDistributed]\n\n        if not exist:\n            k2o_logger().info(""The layer {} doesn\'t have a specific converter, fall back."".format(str(l_)))\n            break\n    else:\n        return True\n\n    return False\n\n\ndef _create_identity(ts_from, ts_to, varset):\n    ty_ = infer_variable_type(ts_from, varset.target_opset)\n    var0 = varset.get_local_variable_or_declare_one(ts_from.name, ty_)\n    var1 = varset.get_local_variable_or_declare_one(ts_to.name, ty_)\n    op = varset.declare_local_operator(TYPES.Identity, op_name=ts_to.name)\n    op.add_input(var0)\n    op.add_output(var1)\n    return op\n\n\ndef _create_model_input_mapping_operators(ts_from, ts_to, prefix, subprefix, varset):\n    ty_ = infer_variable_type(ts_from, varset.target_opset)\n    # type(_infer_variable_type(ts_to, varset.target_opset) and ...\n    # ... type(ty_) can be different which is resolved by implicit cast.\n    var0 = varset.get_local_variable_or_declare_one(subprefix + ts_from.name, ty_)\n    var1 = varset.get_local_variable_or_declare_one(prefix + ts_to.name, ty_)\n    op = varset.declare_local_operator(TYPES.Identity, op_name=prefix + ts_to.name)\n    op.add_input(var0)\n    op.add_output(var1)\n    k2o_logger().debug(\n        ""mapping:  %s -> %s (%s -> %s)"" % (ts_from.name, ts_to.name, subprefix + ts_from.name, prefix + ts_to.name))\n    return op\n\n\ndef _find_kenode_by_output_tensor(inbound_nodes, output_name):\n    def find_ts_name(tensors, name):\n        return next((ts_ for ts_ in tensors if ts_.name.find(name) == 0), None)\n\n    return next((n_ for n_ in inbound_nodes if find_ts_name(list_output_tensors(n_), output_name) is not None), None)\n\n\ndef _is_template_tensors(tensors, templ_tensors):\n    for t_, tt_ in zip(tensors, templ_tensors):\n        # t_.shape and tt_.shape can be different if the input shape is different.\n        if t_.name.find(tt_.name) < 0:\n            return False\n\n    return True\n\n\ndef _on_parsing_model_layer(sub_model, graph, target_kenode, varset, top_kenode=None, upper_prefix=None):\n    ts_inputs = []\n    ts_outputs = []\n    upper_prefix = upper_prefix if upper_prefix else \'\'\n    prefix = \'\'\n    # mapping input/output nodes for the sub_model.\n    inbound_nodes = extract_inbound_nodes(sub_model)\n\n    sub_model_node_idx = 0\n    if len(inbound_nodes) > 1 and inbound_nodes[0] is not target_kenode:\n        # Assumption: the first node in the inbound node list is always the one used in the keras layers.\n        curr_node = target_kenode\n        assert curr_node is not None\n        found = False\n        base_node = inbound_nodes[0]\n        for nodes_ in sub_model._nodes_by_depth.values():\n            for nd_ in nodes_:\n                if _is_template_tensors(list_output_tensors(curr_node), list_output_tensors(nd_)):\n                    found = True\n                    base_node = nd_\n                    break\n            else:\n                sub_model_node_idx += 1\n            if found:\n                break\n        else:\n            assert False, ""Cannot find the node for the model layer {}"".format(sub_model.name)\n\n        bn_name_list = [bn_.name for bn_ in list_output_tensors(base_node)]\n        prefix_found = False\n        for idx_, out_ in enumerate(list_output_tensors(curr_node)):\n            if not prefix_found:\n                name_match_len = -1\n                for bn_name_ in bn_name_list:\n                    cur_match_len = out_.name.find(bn_name_)\n                    if cur_match_len > -1:\n                        name_match_len = cur_match_len\n                        break\n                assert name_match_len > 0\n                prefix = out_.name[0:name_match_len]\n                prefix_found = True\n            ts_outputs.append(out_)\n\n        if top_kenode is None:\n            top_kenode = curr_node\n\n        # the input node needs to be mapped to the outmost inbound keras node.\n        for idx_, in_ in enumerate(list_input_tensors(top_kenode)):\n            _create_model_input_mapping_operators(in_, list_input_tensors(inbound_nodes[0])[idx_],\n                                                  upper_prefix + prefix, upper_prefix,\n                                                  varset)\n            ts_inputs.append(in_)\n\n    k2o_logger().debug(""prefix_beg: %s"" % prefix)\n    for i_ in range(sub_model_node_idx, len(sub_model._nodes_by_depth)):\n        nodes_ = sub_model._nodes_by_depth[i_]\n        for n_ in nodes_:\n            layer = n_.outbound_layer\n            if isinstance(layer, keras.layers.InputLayer):\n                continue\n            elif isinstance(layer, keras.layers.wrappers.TimeDistributed):\n                _on_parsing_time_distributed_layer(graph, [], layer, sub_model, varset, upper_prefix + prefix)\n            elif isinstance(layer, keras.Model):\n                k2o_logger().debug(""Processing a keras sub model - %s"" % layer.name)\n                cur_kenode = _find_kenode_by_output_tensor(extract_inbound_nodes(layer), sub_model.outputs[0].name)\n                _on_parsing_model_layer(layer, graph, n_, varset, cur_kenode, upper_prefix + prefix)\n            else:\n                on_parsing_keras_layer(graph, [], layer, n_, sub_model, varset, upper_prefix + prefix)\n\n    k2o_logger().debug(""prefix_end: - %s"" % prefix)\n    return ts_inputs, ts_outputs\n\n\ndef _check_tfnode_converter_availability(graph, node):\n    var_assign_map = {\'VarHandleOp\': \'AssignVariableOp\', \'VariableV2\': \'Assign\'}\n    if node.type in var_assign_map:\n        if is_tf2:\n            v_output = node.outputs[0].name\n            for graph_node_name in graph._nodes_by_name:\n                graph_op = graph._nodes_by_name[graph_node_name]\n                if graph_op.type == var_assign_map[node.type] and len(graph_op.inputs) > 1 and v_output == \\\n                        graph_op.inputs[0].name:\n                    cur_i = graph_op.inputs[1].op\n                    if cur_i.type == \'Const\' and cur_i.get_attr(\'value\').tensor_content != b\'\':\n                        return True\n            return False\n        else:\n            return True\n    else:\n        cvt = get_converter(node.type)\n        return cvt is not None\n\n\ndef _check_tfnodes_converter_availability(graph, nodelist, debug_mode):\n    status = True\n    for n_ in nodelist:\n        if not _check_tfnode_converter_availability(graph, n_):\n            k2o_logger().warning(\n                ""WARN: No corresponding ONNX op matches the tf.op node {} of type {}"".format(n_.name, n_.type) +\n                ""\\n      The generated ONNX model needs run with the custom op supports."")\n            status = False\n\n    return status\n\n\ndef _on_parsing_tf_nodes(graph, nodelist, varset, debug_mode):\n    _check_tfnodes_converter_availability(graph, nodelist, debug_mode)\n    for node_ in nodelist:\n        k2o_logger().debug(""Processing a tf node - %s"" % node_.name)\n        operator = varset.declare_local_operator(node_.type, raw_model=node_, op_name=node_.name)\n\n        for o_ in node_.outputs:\n            oname = o_.name\n            k2o_logger().debug(\'\\toutput: \' + oname)\n            out0 = varset.get_local_variable_or_declare_one(oname, infer_variable_type(o_, varset.target_opset))\n            operator.add_output(out0)\n\n        for i_ in node_.inputs:\n            k2o_logger().debug(\'\\tinput : \' + i_.name)\n            var_type = infer_variable_type(i_, varset.target_opset)\n            i0 = varset.get_local_variable_or_declare_one(i_.name, var_type)\n            operator.add_input(i0)\n\n        cvt = get_converter(operator.type)\n        if cvt is None:\n            assert isinstance(operator.type, str), \\\n                ""Only tf-op can be pass_thru conversion, type: {}"".format(type(operator.type))\n            set_converter(operator.type, pass_thru_converter)\n        elif hasattr(cvt, \'shape_infer\'):\n            operator.shape_infer = cvt.shape_infer\n\n\ndef _infer_graph_shape(topology, top_level, varset):\n    raw_model_container = topology.raw_model\n    var_queue = queue.Queue()\n    for i_ in raw_model_container.input_names:\n        var_queue.put_nowait(top_level.get_local_variable_or_declare_one(i_))\n\n    visited = set()\n    while not var_queue.empty():\n        var = var_queue.get_nowait()\n        k2o_logger().debug(""var: "" + var.full_name)\n        for oop in var.op_to:\n            if oop in visited:\n                continue\n\n            visited.add(oop)\n            if isinstance(oop.raw_operator, (keras.layers.Layer, tf.Operation)):\n                assert oop.outputs\n            elif oop.raw_operator:\n                oop.outputs = _locate_outputs(oop.raw_operator, varset)\n            else:\n                assert oop.outputs\n            for o_ in oop.outputs:\n                o_.op_from = oop\n\n            si = oop.shape_infer\n            if si is not None:\n                # let operator to build its own shape if it can\'t be deduced from the tf.graph.\n                si(oop)\n\n            for o_ in oop.outputs:\n                var_queue.put_nowait(o_)\n\n\ndef _create_link_node(var_ts, top_level, varset, reversed_io=False, adjust_batch_size=False):\n    if adjust_batch_size:\n        ty_ = _adjust_input_batch_size(infer_variable_type(var_ts, varset.target_opset))\n    else:\n        ty_ = infer_variable_type(var_ts, varset.target_opset)\n    var0 = top_level.get_local_variable_or_declare_one(var_ts.name, ty_)\n    var1 = varset.get_local_variable_or_declare_one(var_ts.name, ty_)\n    op = varset.declare_local_operator(TYPES.Identity)\n    if reversed_io:\n        var0, var1 = var1, var0\n    op.add_input(var1)\n    op.add_output(var0)\n    return op\n\n\ndef _build_inference_nodeset(graph, outputs):\n    nodes_to_keep = set()\n    node_inputs = outputs[:]\n    while node_inputs:\n        nd_ = node_inputs[0]\n        del node_inputs[0]\n        if nd_ in nodes_to_keep:\n            continue\n\n        nodes_to_keep.add(nd_)\n        node_inputs.extend(in_.op for in_ in nd_.inputs)\n\n    return nodes_to_keep\n\n\ndef _create_keras_nodelist(layer, inference_nodeset, out_node=None):\n    newly = list()\n    ts_end = set()  # the input tensor set of the whole layer/model.\n    for node_ in extract_inbound_nodes(layer):\n        if out_node is not None and out_node.name not in \\\n                [tsname_to_node(ts_.name) for ts_ in list_output_tensors(node_)]:\n            continue  # this layer could be reused several times in the whole graph.\n        for ts_ in list_output_tensors(node_):\n            if ts_.op in inference_nodeset:\n                newly.extend([ts_.op for ts_ in list_output_tensors(node_)])\n        ts_end |= set(list_input_tensors(node_))\n\n    for ts_ in list_input_mask(layer):\n        ts_end.add(ts_)\n\n    for ts_ in list_output_mask(layer):\n        newly.append(ts_.op)\n\n    visited = set()\n    nodelist = list()  # keep the node list order.\n    while newly:\n        visited.update(newly)\n        nodelist.extend(newly)\n        newly.clear()\n        for n_ in visited:\n            for i_ in n_.inputs:\n                if i_ in ts_end or i_.op in visited or i_.op not in inference_nodeset:\n                    continue\n                if isinstance(layer, keras.Model):  # ugly fixing for the shared layer.\n                    if i_.name.startswith(layer.name):\n                        pass\n                    elif i_.name.startswith(\'^\' + layer.name):\n                        pass\n                    else:\n                        continue\n\n                newly.append(i_.op)\n\n    return nodelist\n\n\ndef _general_nodelist_closure(node, nodeset, keras_nodeset):\n    nodes = set()\n    visited = set()\n\n    def is_stop_node(nd):\n        return is_placeholder_node(nd) or nd in keras_nodeset\n\n    node_added = [node]\n    updated = True\n    while updated:\n        updated = False\n        while node_added:\n            nd_ = node_added[0]\n            del node_added[0]\n            if nd_ not in visited:\n                visited.add(nd_)\n                if not is_stop_node(nd_) and nd_ not in nodes:\n                    nodes.add(nd_)\n                    updated = True\n                node_added.extend(in_.op for in_ in nd_.inputs if not is_stop_node(in_.op))\n\n        node_added = []\n        for nd_ in nodeset:\n            if any(in_.op in nodes for in_ in nd_.inputs):\n                node_added.append(nd_)\n\n    return nodes\n\n\ndef _build_keras_nodeset(inference_nodeset, keras_node_dict):\n    nodes = set()\n    for layer_, _ in keras_node_dict.values():\n        nodes.update(_create_keras_nodelist(layer_, inference_nodeset))\n    return nodes\n\n\ndef _get_output_nodes(node_list, node):\n    nodes_has_children = set()\n    for node in node_list:\n        if node:\n            for input_tensor in node.inputs:\n                nodes_has_children.add(input_tensor.op)\n    return [n_ for n_ in node_list if n_ not in nodes_has_children]  # need to keep the order.\n\n\ndef _filter_out_input(node_name):\n    # tf.keras BN layer sometimes create a placeholder node \'scale\' in tf 2.x.\n    # It creates \'cond/input\' since tf 2.2.\n    # Given bn layer will be converted in a whole layer, it\'s fine to just filter this node out.\n    filter_patterns = [r""batch_normalization_\\d+\\/scale$"", r""batch_normalization_\\d+\\/cond/input""]\n    filter_out = False\n    for pattern_ in filter_patterns:\n        filter_out = filter_out or re.match(pattern_, node_name)\n    return filter_out\n\n\ndef _advance_by_input(cur_node, layer_nodes, subgraph, inputs, graph_inputs, q_overall):\n    for input_ in cur_node.inputs:\n        predecessor = input_.op\n        if is_placeholder_node(predecessor) and not _filter_out_input(predecessor.name):\n            inputs.add(predecessor)\n            graph_inputs.add(predecessor)\n            continue\n        if predecessor in layer_nodes or len(layer_nodes) == 0:\n            subgraph.append(predecessor)\n        else:\n            inputs.add(predecessor)\n            q_overall.put_nowait(predecessor)\n\n\ndef _visit_nodelist(activated_keras_nodes, input_nodes, layer_key,\n                    keras_node_dict, node, nodelist, q_overall, visited):\n    subgraph = list()\n    i_subgraph = set()\n    for ot_ in (_get_output_nodes(activated_keras_nodes, node) if activated_keras_nodes else [node]):\n        if ot_ not in nodelist:\n            visited.add(ot_)\n            nodelist.append(ot_)\n            _advance_by_input(ot_, activated_keras_nodes, subgraph, i_subgraph, input_nodes, q_overall)\n    while subgraph:\n        int_node = subgraph.pop(0)\n        if int_node in input_nodes or int_node in visited or int_node in keras_node_dict:\n            continue\n\n        visited.add(int_node)\n        nodelist.append(int_node)\n        _advance_by_input(int_node, activated_keras_nodes, subgraph, i_subgraph, input_nodes, q_overall)\n\n    return i_subgraph\n\n\ndef _parse_nodes(graph, inference_nodeset, graph_inputs, keras_node_dict, keras_nodeset, node, nodelist, varset,\n                 visited, q_overall):\n    layer_key_, model_ = (None, None)\n    if node.name in keras_node_dict:\n        layer_key_, model_ = keras_node_dict[node.name]\n        if isinstance(layer_key_, keras.Model) and \\\n                _check_layer_converter_availability(layer_key_):\n            k2o_logger().debug(""Processing a keras sub model - %s"" % layer_key_.name)\n            kenode = _find_kenode_by_output_tensor(extract_inbound_nodes(layer_key_), node.name)\n            ts_in, ts_out = _on_parsing_model_layer(layer_key_, graph, kenode, varset)\n            for ts_ in ts_in:\n                if is_placeholder_node(ts_.op):\n                    graph_inputs.add(ts_.op)\n                else:\n                    q_overall.put_nowait(ts_.op)\n\n            visited.update(ts_.op for ts_ in ts_out)\n            return layer_key_, model_\n\n        activated_keras_nodes = _create_keras_nodelist(layer_key_, inference_nodeset, node)\n    else:\n        activated_keras_nodes = _general_nodelist_closure(node, inference_nodeset, keras_nodeset)\n\n    _visit_nodelist(activated_keras_nodes, graph_inputs, layer_key_,\n                    keras_node_dict, node, nodelist, q_overall, visited)\n\n    return layer_key_, model_\n\n\ndef _parse_graph_core(graph, keras_node_dict, topology, top_scope, output_names):\n    """"""\n    travel the tensor Graph and build the corresponding intermediate operation objects.\n    :param graph: the tensorflow session graph of the Keras mode.\n    :param keras_node_dict: the mapping of operation node to keras layer output.\n    :param topology: The whole topology of the intermediate objects.\n    :param top_scope: The top varset\n    :param output_names: the output names of the TF graph\n    :return: The whole topology of the intermediate objects.\n    """"""\n    input_nodes = set()\n\n    # build the node in the working scope.\n    varset = topology.declare_scope(\'curr_\', top_scope)\n\n    model_outputs = []\n    for name in output_names:\n        var_ts = graph.get_operation_by_name(tsname_to_node(name)).outputs[0]\n        _create_link_node(var_ts, top_scope, varset, adjust_batch_size=True)\n        model_outputs.append(var_ts.op)\n\n    # starting from the output node.\n    q_overall = queue.Queue()\n    for n_ in model_outputs:\n        q_overall.put_nowait(n_)\n\n    visited = set()  # since the output could be shared among the successor nodes.\n    inference_nodeset = _build_inference_nodeset(graph, model_outputs)\n    keras_nodeset = _build_keras_nodeset(inference_nodeset, keras_node_dict)\n    while not q_overall.empty():\n        node = q_overall.get_nowait()\n        if node in input_nodes or node in visited or node not in inference_nodeset:\n            continue\n\n        nodes = []\n        layer_key_, model_ = _parse_nodes(graph, inference_nodeset, input_nodes, keras_node_dict, keras_nodeset,\n                                          node, nodes, varset, visited, q_overall)\n\n        if not nodes:  # already processed by the _parse_nodes\n            continue\n\n        k2o_logger().debug(\'Processing a keras layer - (%s: %s)\' % (layer_key_.name, type(layer_key_)) if\n                           layer_key_ else (nodes[0].name, ""Custom_Layer""))\n        if isinstance(layer_key_, keras.layers.TimeDistributed):\n            _on_parsing_time_distributed_layer(graph, nodes, layer_key_, model_, varset)\n        elif layer_key_ is None or get_converter(type(layer_key_)) is None:\n            _on_parsing_tf_nodes(graph, nodes, varset, topology.debug_mode)\n        else:\n            kenode = _find_kenode_by_output_tensor(extract_inbound_nodes(layer_key_), nodes[0].name)\n            on_parsing_keras_layer(graph, nodes, layer_key_, kenode, model_, varset)\n\n    for nd_ in input_nodes:\n        var_ts = nd_.outputs[0]  # since it\'s placeholder node, safely claim there is only one output.\n        _create_link_node(var_ts, top_scope, varset, True)\n\n    _infer_graph_shape(topology, top_scope, varset)\n    topology.root_names = [variable.onnx_name for variable in top_scope.variables.values()]\n    return topology\n\n\ndef _sorted_inputs(nodelist, outputs, inputs_set):\n    inputs = []\n    node_set = frozenset(nodelist)\n    visited = set()\n\n    def travel(node):\n        for in_ts_ in node.inputs:\n            op_node = in_ts_.op\n            if op_node in visited:\n                continue\n            visited.add(op_node)\n            if (op_node in inputs_set) and (op_node not in inputs):\n                inputs.append(op_node)\n            elif op_node in node_set:\n                travel(op_node)\n\n    for ts_ in outputs:\n        travel(ts_.op)\n\n    return inputs\n\n\ndef _parse_nodes_v2(graph, inference_nodeset, graph_inputs, keras_node_dict, node, varset, visited, q_overall):\n    layer_key, model_ = (None, None)\n    current_layer_outputs = {}\n    if node.name in keras_node_dict:\n        layer_key, model_ = keras_node_dict[node.name]\n    else:\n        ts_out = node.outputs[0]\n        kh_ = getattr(ts_out, \'_keras_history\', None)\n        if kh_ is not None:\n            layer_key = kh_.layer\n            kenode = extract_inbound_nodes(layer_key)[kh_.node_index]\n            current_layer_outputs.update({ts_.op.name: (layer_key, None) for ts_ in list_output_tensors(kenode)})\n\n    if layer_key is None:\n        layer_info = LayerInfo.create_single_node(node, visited)\n    else:\n        if isinstance(layer_key, keras.Model):\n            k2o_logger().debug(""Processing a keras model layer - %s"" % layer_key.name)\n            kenode = _find_kenode_by_output_tensor(extract_inbound_nodes(layer_key), node.outputs[0].name)\n            for ts_ in list_output_tensors(kenode):\n                _create_identity(ts_.op.inputs[0], ts_, varset)\n                visited.add(ts_.op)\n                _advance_by_input(ts_.op, [ts_.op], list(), set(), graph_inputs, q_overall)\n            return None, model_\n        else:\n            layer_info = LayerInfo.create(node, layer_key,\n                                          {**keras_node_dict, **current_layer_outputs}, inference_nodeset)\n\n    nodelist = []\n    layer_inputs = _visit_nodelist(layer_info.nodelist, graph_inputs, None, keras_node_dict, node, nodelist,\n                                   q_overall, visited)\n    sorted_inputs = _sorted_inputs(layer_info.nodelist, layer_info.outputs, layer_inputs)\n    for input_ in sorted_inputs:\n        layer_info.inputs.extend(input_.outputs)\n\n    layer_info.nodelist = [n_ for n_ in layer_info.nodelist if not is_placeholder_node(n_)]\n    return layer_info, model_\n\n\ndef _parse_graph_core_v2(graph, keras_node_dict, topology, top_scope, output_names):\n    """"""\n    travel the tensor Graph and build the corresponding intermediate operation objects.\n    :param graph: the tensorflow session graph of the Keras mode.\n    :param keras_node_dict: the mapping of operation node to keras layer output.\n    :param topology: The whole topology of the intermediate objects.\n    :param top_scope: The top varset\n    :param output_names: the output names of the TF graph\n    :return: The whole topology of the intermediate objects.\n    """"""\n    input_nodes = set()\n\n    # build the node in the working scope.\n    varset = topology.declare_scope(\'curr_\', top_scope)\n\n    model_outputs = []\n    for name in output_names:\n        var_ts = graph.get_operation_by_name(tsname_to_node(name)).outputs[0]\n        _create_link_node(var_ts, top_scope, varset, adjust_batch_size=True)\n        model_outputs.append(var_ts.op)\n\n    # starting from the output node.\n    q_overall = queue.Queue()\n    for n_ in model_outputs:\n        q_overall.put_nowait(n_)\n\n    visited = set()  # since the output could be shared among the successor nodes.\n    # Some complicated layer may have some nodes which cannot be visited from the graph output...\n    # ..., so the layer outputs are added into visit graph to avoid missing nodes.\n    layer_outputs = [graph.get_operation_by_name(nm_) for nm_ in keras_node_dict]\n    inference_nodeset = _build_inference_nodeset(graph, model_outputs + layer_outputs)\n    while not q_overall.empty():\n        node = q_overall.get_nowait()\n        if node in input_nodes or node in visited or node not in inference_nodeset:\n            continue\n\n        layer_info, model_ = _parse_nodes_v2(graph, inference_nodeset, input_nodes, keras_node_dict, node,\n                                             varset, visited, q_overall)\n        if not layer_info:  # already processed by the _parse_nodes_v2\n            continue\n\n        k2o_logger().debug(\'Processing a keras layer - (%s: %s)\' % (layer_info.layer.name, type(layer_info.layer)) if\n                           layer_info.layer else (layer_info.nodelist[0].name, ""Custom_Layer""))\n        if layer_info.layer and isinstance(layer_info.layer, keras.layers.TimeDistributed):\n            _on_parsing_time_distributed_layer(graph, layer_info.nodelist, layer_info.layer, model_, varset)\n        elif layer_info.layer and get_converter(type(layer_info.layer)):\n            on_parsing_keras_layer_v2(graph, layer_info, varset)\n        else:\n            _on_parsing_tf_nodes(graph, layer_info.nodelist, varset, topology.debug_mode)\n\n    for nd_ in input_nodes:\n        var_ts = nd_.outputs[0]  # since it\'s placeholder node, safely claim there is only one output.\n        _create_link_node(var_ts, top_scope, varset, True)\n\n    _infer_graph_shape(topology, top_scope, varset)\n    topology.root_names = [variable.onnx_name for variable in top_scope.variables.values()]\n    return topology\n\n\ndef parse_graph_modeless(topo, graph, target_opset, input_names, output_names, keras_node_dict):\n    top_level = topo.declare_scope(\'__root\')\n    input_tensors = [graph.get_tensor_by_name(n_) for n_ in input_names]\n    output_tensors = [graph.get_tensor_by_name(n_) for n_ in output_names]\n\n    for ts_i_ in input_tensors:\n        var_type = _adjust_input_batch_size(infer_variable_type(ts_i_, target_opset))\n        if ts_i_.name.endswith(\':0\'):\n            str_value = ts_i_.name[:-2]\n            op = top_level.declare_local_operator(TYPES.Identity)\n            var0 = top_level.get_local_variable_or_declare_one(str_value, var_type)\n            var1 = top_level.get_local_variable_or_declare_one(ts_i_.name, var_type)\n            op.add_input(var0)\n            op.add_output(var1)\n        else:\n            str_value = ts_i_.name\n        top_level.get_local_variable_or_declare_one(str_value, var_type)\n        topo.raw_model.add_input_name(str_value)\n\n    for ts_o_ in output_tensors:\n        var_type = _adjust_input_batch_size(infer_variable_type(ts_o_, target_opset))\n        str_value = ts_o_.name\n        top_level.get_local_variable_or_declare_one(str_value, var_type)\n        topo.raw_model.add_output_name(str_value)\n\n    return _parse_graph_core_v2(\n        graph, keras_node_dict, topo, top_level, output_names\n    )\n\n\ndef parse_graph(topo, graph, target_opset, output_names, keras_node_dict):\n    # type: (Topology, tf.Graph, int, [], []) -> Topology\n    """"""\n    Build the node-layer mapper and parse the whole TF graph of Keras Model.\n    """"""\n    top_level = topo.declare_scope(\'__root\')\n\n    dim_variable_counter = 0\n    # Create the onnx model input name before parsing to keep ...\n    # ... the model input names are identical to the original Keras model.\n    for idx_ in range(len(topo.raw_model.model.inputs)):\n        op = top_level.declare_local_operator(TYPES.Identity)\n        idx_key = idx_\n        if isinstance(topo.raw_model.model.inputs, dict):\n            idx_key = list(topo.raw_model.model.inputs.keys())[idx_]\n        input_ts = topo.raw_model.model.inputs[idx_key]\n        var_type = _adjust_input_batch_size(infer_variable_type(input_ts, target_opset))\n        dim_variable_counter = _adjust_input_output_size(var_type, dim_variable_counter)\n        str_value = input_ts.name\n        var0 = None\n        if hasattr(topo.raw_model.model, \'input_names\'):\n            str_value = topo.raw_model.model.input_names[idx_]\n        elif input_ts.name.endswith(\':0\'):\n            str_value = input_ts.name[:-2]\n        else:\n            # if there is no difference between input tensor name and model input name,\n            # skip it.\n            var0 = top_level.get_local_variable_or_declare_one(str_value, var_type)\n        if not var0:\n            var0 = top_level.get_local_variable_or_declare_one(str_value, var_type)\n            var1 = top_level.get_local_variable_or_declare_one(input_ts.name, var_type)\n            op.add_input(var0)\n            op.add_output(var1)\n        topo.raw_model.add_input_name(str_value)\n\n    output_name_dict = {}\n    output_tensors = topo.raw_model.model.outputs\n    if output_names:\n        output_tensors = [graph.get_tensor_by_name(n_) for n_ in output_names]\n    for idx_, ts_ in enumerate(output_tensors):\n        op = top_level.declare_local_operator(TYPES.Identity)\n        var_type = _adjust_input_batch_size(infer_variable_type(ts_, target_opset))\n        dim_variable_counter = _adjust_input_output_size(var_type, dim_variable_counter)\n        str_value = ts_.name\n        use_ts_name = False\n        if hasattr(topo.raw_model.model, \'output_names\'):\n            str_value = topo.raw_model.model.output_names[idx_]\n        elif ts_.name.endswith(\':0\'):\n            str_value = tsname_to_node(ts_.name)\n        else:\n            # if there is no difference between output tensor name and model output name\n            # skip it.\n            use_ts_name = True\n\n        if str_value in output_name_dict:\n            cur_count = output_name_dict[str_value]\n            output_name_dict[str_value] = cur_count + 1\n            str_value = str_value + \':\' + str(cur_count)\n        else:\n            output_name_dict[str_value] = 1\n\n        if not use_ts_name:\n            var0 = top_level.get_local_variable_or_declare_one(str_value, var_type)\n            var1 = top_level.get_local_variable_or_declare_one(ts_.name, var_type)\n            op.add_input(var1)\n            op.add_output(var0)\n\n        topo.raw_model.add_output_name(str_value)\n\n    return _parse_graph_core_v2(\n        graph, keras_node_dict, topo, top_level, output_names\n    ) if is_tf2 and is_tf_keras else _parse_graph_core(\n        graph, keras_node_dict, topo, top_level, output_names)\n'"
keras2onnx/topology.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom onnxconverter_common.onnx_ex import make_model_ex\nfrom .common import utils, k2o_logger\nfrom .common import OnnxObjectContainer, Variable, InterimContext\nfrom .common.data_types import TensorType, Int64Type, FloatType, StringType\nfrom .funcbook import get_converter\nfrom .proto import helper\n\n\nclass KerasTfModelContainer(object):\n    def __init__(self, model, graph):\n        self._input_raw_names = list()\n        self._output_raw_names = list()\n        self.tf_graph = graph\n        self.model = model\n\n    @property\n    def raw_model(self):\n        return self.tf_graph\n\n    def add_input_name(self, name):\n        # The order of adding strings matters. The final model\'s input names are sequentially added as this list\n        if name not in self._input_raw_names:\n            self._input_raw_names.append(name)\n\n    def add_output_name(self, name):\n        # The order of adding strings matters. The final model\'s output names are sequentially added as this list\n        if name not in self._output_raw_names:\n            self._output_raw_names.append(name)\n\n    @property\n    def input_names(self):\n        return [name for name in self._input_raw_names]\n\n    @property\n    def output_names(self):\n        return [name for name in self._output_raw_names]\n\n\nclass Topology:\n\n    def __init__(self, model, graph, target_opset=None, custom_op_dict=None,\n                 reserved_variable_names=None, reserved_operator_names=None):\n        """"""\n        Initialize a Topology object, which is an intermediate representation of a computational graph.\n\n        :param model: RawModelContainer object or one of its derived classes. It contains the original model.\n        :param default_batch_size: batch_size prepend to scalar and array types\n        :param initial_types: A list providing some types for some root variables. Each element is a tuple of a variable\n        name and a type defined in data_types.py.\n        :param target_opset: the onnx model targeted opset number.\n        :param reserved_variable_names: A set of strings which are not allowed to be used as a variable name\n        :param reserved_operator_names: A set of strings which are not allowed to be used as a operator name\n        """"""\n        self.scopes = []\n        self.raw_model = KerasTfModelContainer(model, graph)\n        self.scope_names = set()\n        self.variable_name_set = reserved_variable_names if reserved_variable_names is not None else set()\n        self.operator_name_set = reserved_operator_names if reserved_operator_names is not None else set()\n        self.target_opset = target_opset\n        self.debug_mode = False\n        self.custom_op_dict = {} if custom_op_dict is None else custom_op_dict\n\n        # This attribute is used in optimizing the graph structure. If root_names is not empty, only the variables\n        # specified will be treated as the roots (i.e., set is_fed to True in the beginning of a graph evaluation) of\n        # the graph. Specifying all root variables in this list and leaving it empty are equivalent. This attribute\n        # directly affects initialize_graph_status_for_traversing function and indirectly affects _infer_all_shapes and\n        # _prune functions.\n        self.root_names = list()\n\n    def get_unique_scope_name(self, seed):\n        return Variable.generate_unique_name(seed, self.scope_names)\n\n    def declare_scope(self, seed, parent_scopes=None):\n        scope = InterimContext(self.get_unique_scope_name(seed), parent_scopes, self.variable_name_set,\n                               self.operator_name_set, self.target_opset)\n        self.scopes.append(scope)\n        return scope\n\n    def unordered_operator_iterator(self):\n        for scope in self.scopes:\n            for operator in scope.operators.values():\n                yield operator\n\n    def unordered_variable_iterator(self):\n        for scope in self.scopes:\n            for variable in scope.variables.values():\n                yield variable\n\n    def topological_operator_iterator(self):\n        """"""\n        This is an iterator of all operators in Topology object. Operators may be produced in a topological order.\n        If you want to simply go though all operators without considering their topological structure, please use\n        another function, unordered_operator_iterator.\n        """"""\n        self.initialize_graph_status_for_traversing()\n        while not all(operator.is_evaluated for scope in self.scopes for operator in scope.operators.values()):\n            is_evaluation_happened = False\n            for operator in self.unordered_operator_iterator():\n                if not operator.is_evaluated:\n                    # Make this operator as handled\n                    operator.is_evaluated = True\n                    is_evaluation_happened = True\n                    # Send out an operator\n                    yield operator\n            # After scanning through the whole computational graph, at least one operator should be evaluated. If not,\n            # we need to terminate this procedure to avoid dead lock.\n            if not is_evaluation_happened:\n                break\n\n    def _check_structure(self):\n        """"""\n        This function applies some rules to check if the parsed model is proper. Currently, it only checks if isolated\n        variable and isolated operator exists.\n        """"""\n        # Collect all variable names and operator names\n        unused_variables = set()\n        unused_operators = set()\n        for variable in self.unordered_variable_iterator():\n            unused_variables.add(variable.full_name)\n        for operator in self.unordered_operator_iterator():\n            unused_operators.add(operator.full_name)\n\n        for operator in self.unordered_operator_iterator():\n            for variable in operator.inputs:\n                # A variable is used by an operator, so we remove the variable from the unused-variable list.\n                unused_variables.discard(variable.full_name)\n                # A operator has an input, so we remove the operator from the unused-operator list.\n                unused_operators.discard(operator.full_name)\n            for variable in operator.outputs:\n                # A variable is used by an operator, so we remove the variable from the unused-variable list.\n                unused_variables.discard(variable.full_name)\n                # A operator has an output, so we remove the operator from the unused-operator list.\n                unused_operators.discard(operator.full_name)\n            for variable in operator.input_masks + operator.output_masks:\n                if variable is None:\n                    continue\n                # A variable is used by an operator, so we remove the variable from the unused-variable list.\n                unused_variables.discard(variable.full_name)\n                # A operator has an output, so we remove the operator from the unused-operator list.\n                unused_operators.discard(operator.full_name)\n\n        if len(unused_variables) > 0:\n            raise RuntimeError(\'Isolated variables exist: %s\' % unused_variables)\n\n        # if len(unused_operators) > 0:\n        #     raise RuntimeError(\'Isolated operators exist: %s\' % unused_operators)\n\n    def initialize_graph_status_for_traversing(self):\n        """"""\n        Initialize the status of all variables and operators for traversing the underline graph\n        """"""\n        # In the beginning, we set is_root and is_leaf true. For is_fed, we have two different behaviors depending on\n        # whether root_names is empty.\n        for variable in self.unordered_variable_iterator():\n            # If root_names is set, we only set those variable to be fed. Otherwise, all roots would be fed.\n            if self.root_names:\n                if variable.onnx_name in self.root_names:\n                    variable.is_fed = True\n                else:\n                    variable.is_fed = False\n            else:\n                variable.is_fed = True\n            variable.is_root = True\n            variable.is_leaf = True\n\n        # Then, we flip some flags by applying some simple rules so that only\n        #   1. all roots get is_root=True and is_fed=True\n        #   2. all leaves get is_leaf=True\n        for operator in self.unordered_operator_iterator():\n            operator.is_evaluated = False  # All operators are not processed in the beginning\n            for variable in operator.outputs:\n                # Output cannot be fed before graph traversing\n                variable.is_fed = False\n                # If the variable is an output of one operator, it must not be a root\n                variable.is_root = False\n            for variable in operator.inputs:\n                # If the variable is an input of one operator, it must not be a leaf\n                variable.is_leaf = False\n\n    def compile(self):\n        """"""\n        This function aims at giving every operator enough information so that all operator conversions can happen\n        independently. We also want to check, fix, and simplify the network structure here.\n        """"""\n        self._check_structure()\n\n\ndef _remove_unused_initializers(nodes, initializers):\n    adjusted_initializers = []\n    nodes_input_set = set()\n    for n_ in nodes:\n        for input_name_ in n_.input:\n            nodes_input_set.add(input_name_)\n\n    for initializers_ in initializers:\n        if initializers_.name in nodes_input_set:\n            adjusted_initializers.append(initializers_)\n\n    return adjusted_initializers\n\n\ndef _remove_unused_nodes(nodes, inputs, outputs):\n    nodes_input_set = set()\n    for n_ in nodes:\n        for input_name_ in n_.input:\n            nodes_input_set.add(input_name_)\n\n    input_dict = set([in_.name for in_ in inputs])\n    output_dict = {}\n    for nd_ in nodes:\n        output_dict.update({o_: nd_ for o_ in nd_.output})\n\n    nodes_to_keep = set()\n    node_inputs = [output_dict[ts_.name] for ts_ in outputs]\n    while node_inputs:\n        nd_ = node_inputs[0]\n        del node_inputs[0]\n        if id(nd_) in nodes_to_keep:\n            continue\n\n        nodes_to_keep.add(id(nd_))\n        for in_ in nd_.input:\n            if in_ in output_dict:\n                node_inputs.append(output_dict[in_])\n            else:\n                assert in_ == \'\' or in_ in input_dict, \\\n                    ""{} is disconnected, check the parsing log for more details."".format(in_)\n\n    return [nd_ for nd_ in nodes if id(nd_) in nodes_to_keep]\n\n\ndef _build_extra_inputs(container):\n    # When calling ModelComponentContainer\'s add_initializer(...), nothing is added into the input list.\n    # However, In ONNX, for target opset < 9, initializers should also be model\'s (GraphProto) inputs.\n    # Thus, we create ValueInfoProto objects from initializers (type: TensorProto) ...\n    # ... directly and then add them into model\'s input list.\n    extra_inputs = []  # ValueInfoProto list of the initializers\n    for tensor in container.initializers:\n        # Sometimes (especially when creating optional input values such as RNN\'s initial hidden state), an initializer\n        # is also one of the original model\'s input, so it has been added into the container\'s input list. If this is\n        # the case, we need to skip one iteration to avoid duplicated inputs.\n        if tensor.name in [value_info.name for value_info in container.inputs]:\n            continue\n\n        # Initializers are always tensors so we can just call make_tensor_value_info(...)\n        value_info = helper.make_tensor_value_info(tensor.name, tensor.data_type, tensor.dims)\n        extra_inputs.append(value_info)\n    return extra_inputs\n\n\ndef convert_topology(topology, model_name, doc_string, target_opset, channel_first_inputs=None):\n    """"""\n    This function is used to convert our Topology object defined in _parser.py into a ONNX model (type: ModelProto).\n    :param topology: The Topology object we are going to convert\n    :param model_name: GraphProto\'s name. Let ""model"" denote the returned model. The string ""model_name"" would be\n    assigned to ""model.graph.name.""\n    :param doc_string: A string attached to the produced model\n    :param target_opset: The maximun opset number in the model.\n    :param channel_first_inputs: A list of channel first input.\n    :return: a ONNX ModelProto\n    """"""\n    topology.initialize_graph_status_for_traversing()\n\n    container = OnnxObjectContainer(target_opset)\n\n    # Put roots and leaves as ONNX\'s model into buffers. They will be added into ModelComponentContainer later.\n    tensor_inputs = {}\n    other_inputs = {}\n    tensor_outputs = {}\n    other_outputs = {}\n    for scope in topology.scopes:\n        for variable in scope.variables.values():\n            if variable.is_root:\n                if isinstance(variable.type, (TensorType, Int64Type, FloatType, StringType)):\n                    tensor_inputs[variable.raw_name] = variable\n                else:\n                    other_inputs[variable.raw_name] = variable\n            if variable.is_leaf:\n                if isinstance(variable.type, (TensorType, Int64Type, FloatType, StringType)):\n                    tensor_outputs[variable.raw_name] = variable\n                else:\n                    other_outputs[variable.raw_name] = variable\n\n    # Add roots the graph according to their order in the original model\n    nchw_inputs = []\n    if channel_first_inputs is None:\n        channel_first_inputs = []\n    for name in topology.raw_model.input_names:\n        if name in tensor_inputs:\n            onnx_input = tensor_inputs[name]  # type: Variable\n            if name in channel_first_inputs or \\\n                    (name.endswith(\':0\') and name[:-2] in channel_first_inputs):\n                nchw_inputs.append(onnx_input.full_name)\n                s = onnx_input.type.shape\n                onnx_input.type.shape = [s[0], s[3], s[1], s[2]]\n            container.add_input(onnx_input)\n\n    for name in topology.raw_model.input_names:\n        if name in other_inputs:\n            container.add_input(other_inputs[name])\n\n    # Add leaves the graph according to their order in the original model\n    for name in topology.raw_model.output_names:\n        if name in tensor_outputs:\n            container.add_output(tensor_outputs[name])\n    for name in topology.raw_model.output_names:\n        if name in other_outputs:\n            container.add_output(other_outputs[name])\n\n    # Traverse the graph from roots to leaves\n    for operator in topology.topological_operator_iterator():\n        scope = next(scope for scope in topology.scopes if scope.name == operator.scope)\n        k2o_logger().debug(""Converting the operator (%s): %s"" % (operator.full_name, operator.type))\n        cvt = get_converter(operator.type)\n        if cvt is None:\n            raise RuntimeError(""Unexpected error on find the converter for op {}"".format(operator.type))\n        cvt(scope, operator, container)\n\n    # enable the ONNX optimizations\n    graph = None\n    extra_inputs = _build_extra_inputs(container)\n    nodes = _remove_unused_nodes(container.nodes, container.inputs + extra_inputs, container.outputs)\n\n    if not topology.debug_mode:\n        try:\n            import onnxconverter_common\n            origin_node_number = len(nodes)\n            if target_opset < 9:\n                nodes = onnxconverter_common.optimizer.optimize_onnx(nodes, nchw_inputs=nchw_inputs,\n                                                                     inputs=container.inputs + extra_inputs,\n                                                                     outputs=container.outputs)\n                node_number = len(nodes)\n            else:\n                graph = onnxconverter_common.optimizer.optimize_onnx_graph(nodes, nchw_inputs=nchw_inputs,\n                                                                           inputs=container.inputs,\n                                                                           outputs=container.outputs,\n                                                                           initializers=container.initializers,\n                                                                           model_value_info=container.value_info,\n                                                                           model_name=model_name,\n                                                                           target_opset=container.target_opset)\n                node_number = len(graph.node)\n            k2o_logger().info(\n                ""The ONNX operator number change on the optimization: {} -> {}"".format(origin_node_number, node_number))\n        except ImportError:\n            onnx_not_imported = \'onnxconverter_common is not imported,\'\n            if nchw_inputs:\n                raise Exception(\n                    \'{} nchw_inputs does not make effect. Please set nchw_inputs to empty.\'.format(onnx_not_imported))\n            k2o_logger().warning(\'{} so the convertor optimizer is not enabled.\'.format(onnx_not_imported))\n        except Exception as e:  # noqa\n            # either optimizer issue or converter issue, we just let it go...\n            # ... so that we can diagnose the issue from the converted model.\n            k2o_logger().warning(\n                \'There is an error({}) happened during optimizing on the converted model!\'.format(type(e)))\n            k2o_logger().warning(str(e))\n            import traceback\n            tb = traceback.format_exc()\n            k2o_logger().warning(tb)\n\n    if graph is None:\n        # Create a graph from its main components\n        adjusted_initializers = _remove_unused_initializers(nodes, container.initializers)\n        if target_opset < 9:\n            adjusted_extra_inputs = _remove_unused_initializers(nodes, extra_inputs)\n            graph = helper.make_graph(nodes, model_name, container.inputs + adjusted_extra_inputs,\n                                      container.outputs, adjusted_initializers)\n        else:\n            graph = helper.make_graph(nodes, model_name, container.inputs,\n                                      container.outputs, adjusted_initializers)\n\n        # Add extra information related to the graph\n        graph.value_info.extend(container.value_info)\n\n    # Create model\n    onnx_model = make_model_ex(graph,\n                               container.node_domain_version_pair_sets,\n                               target_opset, doc_string=doc_string,\n                               producer_name=utils.get_producer(),\n                               domain=utils.get_domain())\n    return onnx_model\n'"
tests/conftest.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport pytest\n\nfrom keras2onnx.proto import keras\nfrom test_utils import run_onnx_runtime\n\nK = keras.backend\n\n\n@pytest.fixture(scope='function')\ndef runner():\n    model_files = []\n\n    def runner_func(*args, **kwargs):\n        return run_onnx_runtime(*args, model_files, **kwargs)\n\n    # Ensure Keras layer naming is reset for each function\n    K.reset_uids()\n    # Reset the TensorFlow session to avoid resource leaking between tests\n    K.clear_session()\n\n    # Provide wrapped run_onnx_runtime function\n    yield runner_func\n\n    # Remove model files\n    for fl in model_files:\n        os.remove(fl)\n"""
tests/test_cgan.py,1,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport pytest\nimport tensorflow as tf\nimport keras2onnx\nimport numpy as np\nfrom keras2onnx.proto import keras, is_tf_keras\nfrom distutils.version import StrictVersion\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py\nclass CGAN():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.num_classes = 10\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise and the target label as input\n        # and generates the corresponding digit of that label\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,))\n        img = self.generator([noise, label])\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The discriminator takes generated image as input and determines validity\n        # and the label of that image\n        valid = self.discriminator([img, label])\n\n        # The combined model  (stacked generator and discriminator)\n        # Trains generator to fool discriminator\n        self.combined = Model([noise, label], valid)\n\n    def get_model(self):\n        return self.combined\n\n    def build_generator(self):\n        model = Sequential()\n\n        model.add(Dense(256, input_dim=self.latent_dim))\n\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(np.prod(self.img_shape), activation=\'tanh\'))\n        model.add(Reshape(self.img_shape))\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,), dtype=\'int32\')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n\n        model_input = multiply([noise, label_embedding])\n        img = model(model_input)\n\n        return Model([noise, label], img)\n\n    def build_discriminator(self):\n        model = Sequential()\n\n        model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.4))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.4))\n        model.add(Dense(1, activation=\'sigmoid\'))\n\n        model.add(Dense(1, activation=\'sigmoid\'))\n\n        img = Input(shape=self.img_shape)\n        label = Input(shape=(1,), dtype=\'int32\')\n\n        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n        flat_img = Flatten()(img)\n\n        model_input = multiply([flat_img, label_embedding])\n\n        validity = model(model_input)\n\n        return Model([img, label], validity)\n\n\n@pytest.mark.skipif(keras2onnx.proto.tfcompat.is_tf2 and is_tf_keras, reason=""Tensorflow 1.x only tests."")\n@pytest.mark.skipif(is_tf_keras and StrictVersion(tf.__version__.split(\'-\')[0]) < StrictVersion(""1.14.0""),\n                    reason=""Not supported before tensorflow 1.14.0 for tf_keras"")\ndef test_CGAN(runner):\n    keras_model = CGAN().combined\n    batch = 5\n    x = np.random.rand(batch, 100).astype(np.float32)\n    y = np.random.rand(batch, 1).astype(np.float32)\n    expected = keras_model.predict([x, y])\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n    assert runner(onnx_model.graph.name, onnx_model,\n                  {keras_model.input_names[0]: x, keras_model.input_names[1]: y}, expected)\n'"
tests/test_layers.py,113,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport pytest\nimport keras2onnx\nimport numpy as np\nfrom onnxconverter_common.onnx_ex import get_maximum_opset_supported\nfrom keras2onnx.proto.tfcompat import is_tf2, tensorflow as tf\nfrom keras2onnx.proto import (keras, is_tf_keras,\n                              is_tensorflow_older_than, is_tensorflow_later_than,\n                              is_keras_older_than, is_keras_later_than)\n\nK = keras.backend\nActivation = keras.layers.Activation\nAdd = keras.layers.Add\nadvanced_activations = keras.layers.advanced_activations\nAlphaDropout = keras.layers.AlphaDropout\nAverage = keras.layers.Average\nAveragePooling1D = keras.layers.AveragePooling1D\nAveragePooling2D = keras.layers.AveragePooling2D\nAveragePooling3D = keras.layers.AveragePooling3D\nBatchNormalization = keras.layers.BatchNormalization\nBidirectional = keras.layers.Bidirectional\nConcatenate = keras.layers.Concatenate\nConv1D = keras.layers.Conv1D\nConv2D = keras.layers.Conv2D\nConv2DTranspose = keras.layers.Conv2DTranspose\nConv3D = keras.layers.Conv3D\nConv3DTranspose = keras.layers.Conv3DTranspose\nCropping1D = keras.layers.Cropping1D\nCropping2D = keras.layers.Cropping2D\nCropping3D = keras.layers.Cropping3D\nDense = keras.layers.Dense\nDot = keras.layers.Dot\ndot = keras.layers.dot\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nGaussianDropout = keras.layers.GaussianDropout\nGaussianNoise = keras.layers.GaussianNoise\nGlobalAveragePooling2D = keras.layers.GlobalAveragePooling2D\nGRU = keras.layers.GRU\nInput = keras.layers.Input\nInputLayer = keras.layers.InputLayer\nLambda = keras.layers.Lambda\nLayer = keras.layers.Layer\nLeakyReLU = keras.layers.LeakyReLU\nLSTM = keras.layers.LSTM\nMaximum = keras.layers.Maximum\nMaxPool1D = keras.layers.MaxPool1D\nMaxPool3D = keras.layers.MaxPool3D\nMaxPooling2D = keras.layers.MaxPooling2D\nModel = keras.models.Model\nMultiply = keras.layers.Multiply\nReshape = keras.layers.Reshape\nSeparableConv1D = keras.layers.SeparableConv1D\nSeparableConv2D = keras.layers.SeparableConv2D\nSequential = keras.models.Sequential\nSimpleRNN = keras.layers.SimpleRNN\nSpatialDropout2D = keras.layers.SpatialDropout2D\nSubtract = keras.layers.Subtract\nTimeDistributed = keras.layers.TimeDistributed\nUpSampling1D = keras.layers.UpSampling1D\nUpSampling2D = keras.layers.UpSampling2D\nUpSampling3D = keras.layers.UpSampling3D\nZeroPadding2D = keras.layers.ZeroPadding2D\nif not (is_keras_older_than(""2.2.4"") or is_tf_keras):\n    ReLU = keras.layers.ReLU\n\nRNN_CLASSES = [SimpleRNN, GRU, LSTM]\n\n\ndef _asarray(*a):\n    return np.array([a], dtype=\'f\')\n\n\ndef test_keras_lambda(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: x ** 2, input_shape=[3, 5]))\n    if get_maximum_opset_supported() >= 11:\n        model.add(Lambda(lambda x: tf.round(x), input_shape=[3, 5]))\n    model.add(Flatten(data_format=\'channels_last\'))\n    model.compile(optimizer=\'sgd\', loss=\'mse\')\n\n    onnx_model = keras2onnx.convert_keras(model, \'test_keras_lambda\')\n    data = np.random.rand(3 * 5).astype(np.float32).reshape(1, 3, 5)\n    expected = model.predict(data)\n    assert runner(\'onnx_lambda\', onnx_model, data, expected)\n\n\n@pytest.mark.skipif(is_tensorflow_older_than(\'1.12.0\'),\n                    reason=""tf.nn.depth_to_space not supported."")\n@pytest.mark.skipif(get_maximum_opset_supported() < 11,\n                    reason=""DepthToSpace is not supported before opset 11."")\n@pytest.mark.parametrize(""data_format"", [""NCHW"", ""NHWC""])\n@pytest.mark.parametrize(""input_shape"", [(4, 6, 8), (None, None, 8)])\ndef test_keras_lambda_depth_to_space(runner, data_format, input_shape):\n    if data_format == ""NCHW"" and is_tensorflow_older_than(""2.1.0""):\n        pytest.skip(""tf.nn.depth_to_space with NCHW not supported for Tensorflow older than 2.1.0"")\n    model = Sequential()\n    model.add(Lambda(\n        lambda x: tf.nn.depth_to_space(x, block_size=2, data_format=data_format),\n        input_shape=input_shape\n    ))\n\n    onnx_model = keras2onnx.convert_keras(model, \'test_keras_lambda_depth_to_space\')\n    data = np.random.rand(3, 4, 6, 8).astype(np.float32)  # batch dimension + \'input_shape\'\n    expected = model.predict(data)\n    assert runner(\'tf_depth_to_space\', onnx_model, data, expected)\n\n\ndef test_tf_addn(runner):\n    input1 = Input(shape=(5, 3, 4), dtype=tf.float32)\n    input2 = Input(shape=(5, 3, 4), dtype=tf.float32)\n    sum = Lambda(tf.add_n)([input1, input2])\n    model = keras.models.Model(inputs=[input1, input2], outputs=sum)\n\n    onnx_model = keras2onnx.convert_keras(model, \'tf_add_n\')\n    batch_data1_shape = (2, 5, 3, 4)\n    batch_data2_shape = (2, 5, 3, 4)\n    data1 = np.random.rand(*batch_data1_shape).astype(np.float32)\n    data2 = np.random.rand(*batch_data2_shape).astype(np.float32)\n    expected = model.predict([data1, data2])\n    assert runner(\'tf_add_n\', onnx_model, [data1, data2], expected)\n\n\n@pytest.mark.parametrize(""arg_func"", [tf.argmax, tf.argmin])\ndef test_tf_argmax_argmin(runner, arg_func):\n    model = Sequential()\n    model.add(Lambda(lambda x: arg_func(x, axis=2), input_shape=[3, 4, 2]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_arg\')\n    data = np.random.rand(5, 3, 4, 2).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_arg\', onnx_model, data, expected)\n\n    model = Sequential()\n    model.add(Lambda(lambda x: arg_func(x, axis=2, output_type=tf.int32), input_shape=[3, 4, 2]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_arg\')\n    data = np.random.rand(5, 3, 4, 2).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_arg\', onnx_model, data, expected)\n\n\ndef test_tf_conv(runner):\n    model = Sequential()\n    k = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(1, 2, 3, 5)).astype(np.float32))\n    model.add(Lambda(lambda x: tf.nn.conv2d(x, k, strides=[1, 1, 2, 1], padding=\'SAME\', data_format=\'NHWC\'),\n                     input_shape=[10, 14, 3]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_conv\')\n    data = np.random.rand(1, 10, 14, 3).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_conv\', onnx_model, data, expected)\n\n    model = Sequential()\n    k = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(1, 2, 3, 5)).astype(np.float32))\n    model.add(Lambda(lambda x: tf.nn.conv2d(x, k, strides=[1, 1, 2, 1], padding=\'VALID\', data_format=\'NHWC\'),\n                     input_shape=[10, 14, 3]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_conv\')\n    data = np.random.rand(1, 10, 14, 3).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_conv\', onnx_model, data, expected)\n\n    model = Sequential()\n    k = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(1, 3, 5)).astype(np.float32))\n    model.add(Lambda(lambda x: tf.nn.conv1d(x, k, stride=2, padding=\'SAME\', data_format=\'NWC\'),\n                     input_shape=[10, 3]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_conv\')\n    data = np.random.rand(1, 10, 3).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_conv\', onnx_model, data, expected)\n\n\ndef test_tf_rsqrt(runner):\n    def my_func_1(x):\n        beta = tf.constant([0.0, 0.0, 0.0, 0.0])\n        gamma = tf.constant([0.0, 0.0, 0.0, 0.0])\n        mean = tf.constant([0.0, 0.0, 0.0, 0.0])\n        variance = tf.constant([1.0, 1.0, 1.0, 1.0])\n        return tf.nn.batch_normalization(x, mean, variance, beta, gamma, 0.001)\n\n    model = Sequential()\n    model.add(Lambda(lambda x: my_func_1(x), input_shape=[2, 3, 4]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_rsqrt\')\n    data = np.random.rand(1, 2, 3, 4).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_rsqrt\', onnx_model, data, expected)\n\n\ndef test_tf_bias_add(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: tf.nn.bias_add(x, tf.constant([100., -100.])), input_shape=[3, 4, 2]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_bias_add\')\n    data = np.random.rand(5, 3, 4, 2).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_bias_add\', onnx_model, data, expected)\n\n    model = Sequential()\n    model.add(\n        Lambda(lambda x: tf.nn.bias_add(x, tf.constant([100., -100.]), data_format=\'NCHW\'), input_shape=[2, 3, 4]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_bias_add\')\n    data = np.random.rand(5, 2, 3, 4).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_bias_add\', onnx_model, data, expected)\n\n\ndef test_tf_clip(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: K.clip(x, 0, 10), input_shape=[5, 5]))\n    data = np.random.randint(-5, 15, size=(1, 5, 5)).astype(np.float32)\n    expected = model.predict(data)\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_clip\')\n    assert runner(\'onnx_tf_clip\', onnx_model, data, expected)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 12,\n                    reason=""Result mismatch on ORT, skip conversion for unsupported types."")\ndef test_tf_pow(runner):\n    model = Sequential()\n    y = tf.constant([[2.0, 2.0], [2.0, 2.0]])\n    model.add(Lambda(lambda x: tf.math.pow(tf.cast(x, tf.int32), tf.cast(y, tf.int32)), input_shape=[2, 2]))\n    data = (100 * np.random.rand(3, 2, 2)).astype(np.float32)\n    expected = model.predict(data)\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_pow\')\n    assert runner(\'onnx_tf_pow\', onnx_model, data, expected)\n\n\ndef test_tf_concat(runner):\n    def my_func_1(x):\n        return tf.concat([x[0], x[1]], 1)\n\n    def my_func_2(x):\n        return tf.concat([x[0], x[1]], -1)\n\n    input1_shape = [(2, 3), (3, 2)]\n    input2_shape = [(4, 3), (3, 4)]\n    myFunc = [my_func_1, my_func_2]\n    for idx_ in range(2):\n        input1 = Input(shape=input1_shape[idx_])\n        input2 = Input(shape=input2_shape[idx_])\n        added = Lambda(myFunc[idx_])([input1, input2])\n        model = keras.models.Model(inputs=[input1, input2], outputs=added)\n\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_concat\')\n        batch_data1_shape = (2,) + input1_shape[idx_]\n        batch_data2_shape = (2,) + input2_shape[idx_]\n        data1 = np.random.rand(*batch_data1_shape).astype(np.float32)\n        data2 = np.random.rand(*batch_data2_shape).astype(np.float32)\n        expected = model.predict([data1, data2])\n        assert runner(\'onnx_concat\', onnx_model, [data1, data2], expected)\n\n\n@pytest.mark.parametrize(""use_bias"", [True, False])\ndef test_depthwise_conv2d(runner, use_bias):\n    model = Sequential()\n    model.add(InputLayer(input_shape=(8, 8, 2)))\n    model.add(keras.layers.DepthwiseConv2D(\n        kernel_size=(3, 3), strides=(1, 1), padding=""VALID"",\n        data_format=\'channels_last\', use_bias=use_bias))\n    onnx_model = keras2onnx.convert_keras(model, \'test_depthwise_conv2d\')\n    data = np.random.rand(3, 8, 8, 2).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_depthwise_conv2d\', onnx_model, data, expected)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 12,\n                    reason=""Einsum is not supported until opset 12."")\ndef test_tf_einsum(runner):\n    def my_func_1(x):\n        return tf.einsum(\'i,d->id\', x[0][:, 0], x[1][:, 1])\n\n    def my_func_2(x):\n        return tf.einsum(\'ibh,hnd->ibnd\', x[0][:, 0], x[1][:, 1])\n\n    def my_func_3(x):\n        return tf.einsum(\'ibnd,hnd->ibh\', x[0][:, 0], x[1][:, 1])\n\n    def my_func_4(x):\n        return tf.einsum(\'ibnd,jbnd->ijbn\', x[0][:, 0], x[1][:, 1])\n\n    def my_func_5(x):\n        return tf.einsum(\'ijbn,jbnd->ibnd\', x[0][:, 0], x[1][:, 1])\n\n    input1_shape = [(3,), (2, 3, 2), (2, 3, 4, 2), (2, 3, 4, 2), (2, 2, 4, 3)]\n    input2_shape = [(3,), (2, 4, 5), (2, 4, 2), (2, 3, 4, 2), (2, 4, 3, 5)]\n    myFunc = [my_func_1, my_func_2, my_func_3, my_func_4, my_func_5]\n\n    for idx_ in range(len(myFunc)):\n        K.clear_session()\n        input1 = Input(shape=input1_shape[idx_])\n        input2 = Input(shape=input2_shape[idx_])\n        added = Lambda(myFunc[idx_])([input1, input2])\n        model = keras.models.Model(inputs=[input1, input2], outputs=added)\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_einsum\')\n        batch_data1_shape = (2,) + input1_shape[idx_]\n        batch_data2_shape = (2,) + input2_shape[idx_]\n        data1 = np.random.rand(*batch_data1_shape).astype(np.float32)\n        data2 = np.random.rand(*batch_data2_shape).astype(np.float32)\n        expected = model.predict([data1, data2])\n        assert runner(\'onnx_einsum\', onnx_model, [data1, data2], expected)\n\n\ndef test_tf_expand_dims(runner):\n    for dim in [0, 1, -1]:\n        model = Sequential()\n        model.add(Lambda(lambda x: tf.expand_dims(x, dim), input_shape=[2, 3, 4]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_expand_dims\')\n        data = np.random.rand(1, 2, 3, 4).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_tf_expand_dims\', onnx_model, data, expected)\n\n\ndef test_tf_fill(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: x + tf.fill([2, 3], 2.3), input_shape=[2, 3]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_fill\')\n    data = np.random.rand(3, 2, 3).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_fill\', onnx_model, data, expected)\n\n\ndef test_tf_fused_batch_norm(runner):\n    def my_func_1(x):\n        beta = tf.constant([0.2, 0.3, 0.4, 0.5])\n        gamma = tf.constant([0.5, 0.4, 0.3, 0.2])\n        mean = tf.constant([0.1, 0.2, 0.3, 0.4])\n        variance = tf.constant([0.9, 1.0, 1.0, 1.1])\n        return tf.nn.fused_batch_norm(x, mean, variance, beta, gamma, 0.001, data_format=\'NHWC\', is_training=False)[\n            0]\n\n    def my_func_2(x):\n        beta = tf.constant([0.2, 0.3])\n        gamma = tf.constant([0.5, 0.4])\n        mean = tf.constant([0.1, 0.2])\n        variance = tf.constant([0.9, 1.0])\n        return tf.nn.fused_batch_norm(x, mean, variance, beta, gamma, 0.001, data_format=\'NCHW\', is_training=False)[\n            0]\n\n    for my_func in [my_func_1, my_func_2]:\n        model = Sequential()\n        model.add(Lambda(lambda x: my_func(x), input_shape=[2, 3, 4]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_fused_batch_norm\')\n        data = np.random.rand(1, 2, 3, 4).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_tf_fused_batch_norm\', onnx_model, data, expected)\n\n\ndef test_tf_gather(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: tf.gather(x, [1, 1], axis=1), input_shape=[5, 5]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_gather\')\n    data = np.random.rand(3, 5, 5).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_gather\', onnx_model, data, expected)\n\n\ndef test_tf_maximum_minimum(runner):\n    input1_shape_list = [(2, 3), (2, 3)]\n    input2_shape_list = [(2, 3), (2, 1)]\n\n    def my_func_1(x):\n        return tf.minimum(tf.maximum(x[0], x[1]), 0.5)\n\n    def my_func_2(x):\n        return tf.minimum(tf.maximum(x[0], 0.5), x[1])\n\n    for idx_ in range(len(input1_shape_list)):\n        for myFunc in [my_func_1, my_func_2]:\n            input1 = Input(shape=input1_shape_list[idx_], dtype=tf.float32)\n            input2 = Input(shape=input2_shape_list[idx_], dtype=tf.float32)\n            added = Lambda(myFunc)([input1, input2])\n            model = keras.models.Model(inputs=[input1, input2], outputs=added)\n\n            onnx_model = keras2onnx.convert_keras(model, \'tf_maximum_minimum\')\n            batch_data1_shape = (2,) + input1_shape_list[idx_]\n            batch_data2_shape = (2,) + input2_shape_list[idx_]\n            data1 = np.random.rand(*batch_data1_shape).astype(np.float32)\n            data2 = np.random.rand(*batch_data2_shape).astype(np.float32)\n            expected = model.predict([data1, data2])\n            assert runner(\'tf_maximum_minimum\', onnx_model, [data1, data2], expected)\n\n    def my_func_3(x):\n        return tf.minimum(tf.maximum(x[0], x[1]), 50)\n\n    def my_func_4(x):\n        return tf.minimum(tf.maximum(x[0], 50), x[1])\n\n    for idx_ in range(len(input1_shape_list)):\n        for myFunc in [my_func_3, my_func_4]:\n            input1 = Input(shape=input1_shape_list[idx_], dtype=tf.int32)\n            input2 = Input(shape=input2_shape_list[idx_], dtype=tf.int32)\n            added = Lambda(myFunc)([input1, input2])\n            model = keras.models.Model(inputs=[input1, input2], outputs=added)\n\n            onnx_model = keras2onnx.convert_keras(model, \'tf_maximum_minimum\')\n            batch_data1_shape = (2,) + input1_shape_list[idx_]\n            batch_data2_shape = (2,) + input2_shape_list[idx_]\n            data1 = (100 * np.random.rand(*batch_data1_shape)).astype(np.int32)\n            data2 = (100 * np.random.rand(*batch_data2_shape)).astype(np.int32)\n            expected = model.predict([data1, data2])\n            assert runner(\'tf_maximum_minimum\', onnx_model, [data1, data2], expected)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 9,\n                    reason=""opset < 9 is not supported."")\ndef test_tf_one_hot(runner):\n    def my_func(x):\n        return tf.one_hot(tf.cast(x, tf.int32), 3, 5.0, -1.0, 1)\n\n    model = Sequential()\n    model.add(Lambda(lambda x: my_func(x), input_shape=[3]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_one_hot\')\n    data = np.array([[0, 1, 2]]).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'tf_one_hot\', onnx_model, data, expected)\n\n\ndef test_tf_pad(runner):\n    def my_func_1(x):\n        paddings = tf.constant([[0, 0], [1, 3], [2, 4]])\n        return tf.pad(x, paddings, mode=\'CONSTANT\')\n\n    def my_func_2(x):\n        paddings = tf.constant([[0, 0], [1, 3], [2, 4]])\n        return tf.pad(x, paddings, mode=\'CONSTANT\', constant_values=1)\n\n    for my_func in [my_func_1, my_func_2]:\n        model = Sequential()\n        model.add(Lambda(lambda x: my_func(x), input_shape=[2, 2]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_pad\')\n        data = np.random.rand(3, 2, 2).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_pad\', onnx_model, data, expected)\n\n\ndef test_tf_range(runner):\n    def my_func_1(x):\n        return x + tf.cast(tf.range(3, 18, 3), tf.float32)\n\n    def my_func_2(x):\n        return x + tf.range(2.3, 4.6, 0.8, dtype=tf.float32)\n\n    for my_func_ in [my_func_1, my_func_2]:\n        K.clear_session()\n        model = Sequential()\n        model.add(Lambda(lambda x: my_func_(x), input_shape=[1]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_range\')\n        data = np.random.rand(3, 1).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_range_1\', onnx_model, data, expected)\n\n    def my_func_3(x):\n        return x[0] + tf.cast(tf.range(3, 18, tf.cast(x[1][0, 0], tf.int32)), tf.float32)\n\n    K.clear_session()\n    input1 = Input(shape=(5,))\n    input2 = Input(shape=(1,))\n    added = Lambda(my_func_3)([input1, input2])\n    model = keras.models.Model(inputs=[input1, input2], outputs=added)\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_range\')\n    data_1 = np.random.randint(1, 3, size=(1, 5)).astype(np.float32)\n    data_2 = np.array([3]).astype(np.float32).reshape(1, 1)\n    expected = model.predict([data_1, data_2])\n    assert runner(\'onnx_range_2\', onnx_model, [data_1, data_2], expected)\n\n\ndef test_tf_compare_equal(runner):\n    for tf_op_ in [tf.not_equal, tf.less_equal, tf.greater_equal]:\n        input1_shape = [[3], [3]]\n        input1 = Input(shape=input1_shape[0], dtype=\'int32\')\n        input2 = Input(shape=input1_shape[1], dtype=\'int32\')\n        comp = Lambda(lambda x: tf_op_(x[0], x[1]))([input1, input2])\n        model = keras.models.Model(inputs=[input1, input2], outputs=comp)\n\n        onnx_model = keras2onnx.convert_keras(model, \'tf_compare_equal\')\n        data1 = np.array([[1, 2, 3], [1, 2, 3]]).astype(np.int32)\n        data2 = np.array([[1, 2, 3], [2, 1, 4]]).astype(np.int32)\n        expected = model.predict([data1, data2])\n        assert runner(\'tf_compare_equal\', onnx_model, [data1, data2], expected)\n\n\ndef test_tf_realdiv(runner):\n    input1_shape = [(2, 3), (2, 3)]\n    input2_shape = [(2, 3), (3,)]\n    for idx_ in range(2):\n        input1 = Input(shape=input1_shape[idx_])\n        input2 = Input(shape=input2_shape[idx_])\n        added = Lambda(lambda x: tf.realdiv(x[0], x[1]))([input1, input2])\n        model = keras.models.Model(inputs=[input1, input2], outputs=added, name=\'realdiv\')\n\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        batch_data1_shape = (2,) + input1_shape[idx_]\n        batch_data2_shape = (2,) + input2_shape[idx_]\n        data1 = np.random.rand(*batch_data1_shape).astype(np.float32)\n        data2 = np.random.rand(*batch_data2_shape).astype(np.float32)\n        expected = model.predict([data1, data2])\n        assert runner(model.name, onnx_model, [data1, data2], expected)\n\n\ndef test_tf_reduce_op(runner):\n    reduce_name = [\'tf_min\', \'tf_max\', \'tf_mean\', \'tf_sum\', \'tf_prod\']\n    reduce_ops = [K.min, K.max, K.mean, K.sum, K.prod]\n    axis_list = [1] if is_tf_keras else [1, None]\n    keepdims_val = [True] if is_tf_keras else [True, False]\n    for idx, reduce_op in enumerate(reduce_ops):\n        for axis in axis_list:\n            for keepdims in keepdims_val:\n                model = Sequential()\n                model.add(Lambda(lambda x: reduce_op(x, axis=axis, keepdims=keepdims), input_shape=[2, 2]))\n                onnx_model = keras2onnx.convert_keras(model, \'test_\' + reduce_name[idx])\n                data = np.random.rand(3, 2, 2).astype(np.float32)\n                expected = model.predict(data)\n                assert runner(\'onnx_\' + reduce_name[idx], onnx_model, data, expected)\n\n    axis_list = [1] if is_tf2 and is_tf_keras else [1, None]\n    for idx, reduce_op in enumerate(reduce_ops):\n        for axis in axis_list:\n            for keepdims in keepdims_val:\n                model = Sequential()\n                model.add(Lambda(lambda x: reduce_op(x, axis=axis, keepdims=keepdims), input_shape=[2, 2]))\n                onnx_model = keras2onnx.convert_keras(model, \'test_\' + reduce_name[idx])\n                data = np.random.rand(1, 2, 2).astype(np.float32)\n                expected = model.predict(data)\n                assert runner(\'onnx_\' + reduce_name[idx], onnx_model, data, expected)\n\n\ndef test_tf_reshape(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: tf.reshape(x, [-1, 2, 4]), input_shape=[2, 2, 2]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_reshape_float\')\n    data = np.random.rand(3, 2, 2, 2).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_reshape_float\', onnx_model, data, expected)\n\n    model = Sequential()\n    model.add(Lambda(lambda x: tf.reshape(x, [-1, 2, 4]), input_shape=[2, 2, 2], dtype=tf.int32))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_reshape_int\')\n    data = np.random.randint(5, size=(3, 2, 2, 2)).astype(np.int32)\n    expected = model.predict(data)\n    assert runner(\'onnx_reshape_int\', onnx_model, data, expected)\n\n    def my_func(x):\n        return tf.reshape(x[0][0], tf.cast(x[1][0], tf.int32))\n\n    input1 = Input(shape=(6,))\n    input2 = Input(shape=(3,))\n    added = Lambda(my_func)([input1, input2])\n    model = keras.models.Model(inputs=[input1, input2], outputs=added)\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_reshape_dynamic\')\n    data_1 = np.random.rand(1, 6).astype(np.float32).reshape(1, 6)\n    data_2 = np.array([1, 2, 3]).astype(np.float32).reshape(1, 3)\n    expected = model.predict([data_1, data_2])\n    assert runner(\'onnx_reshape_dynamic\', onnx_model, [data_1, data_2], expected)\n\n\ndef test_tf_resize(runner):\n    target_opset = get_maximum_opset_supported()\n    shape_list = [10, None] if target_opset >= 10 else [10]\n    size_list = [[5, 10], [20, 30]] if target_opset >= 10 else [[20, 30]]\n    for g in [tf.image.resize_bilinear, tf.image.resize_nearest_neighbor]:\n        for shape_1_dim in shape_list:\n            for size in size_list:\n                model = Sequential()\n                model.add(Lambda(lambda x: g(x, size=size), input_shape=[shape_1_dim, 20, 3]))\n\n                onnx_model = keras2onnx.convert_keras(model, \'test_tf_resize\', target_opset=target_opset)\n                data = np.random.rand(2, 10, 20, 3).astype(np.float32)\n                expected = model.predict(data)\n                assert runner(\'onnx_resize\', onnx_model, data, expected)\n\n\ndef test_tf_size(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: x + tf.cast(tf.size(x), tf.float32), input_shape=[2, 3, 5]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_size\')\n    data = np.random.rand(3, 2, 3, 5).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_size\', onnx_model, data, expected)\n\n\ndef test_tf_slice(runner):\n    model = Sequential()\n    # Need 0th: start=0 size=batch_dim\n    model.add(Lambda(lambda x: tf.slice(x, [0, 1, 0, 2], [3, 1, 2, 2]), input_shape=[2, 3, 5]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_slice\')\n    data = np.random.rand(3, 2, 3, 5).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_slice\', onnx_model, data, expected)\n\n    if get_maximum_opset_supported() < 10:\n        return\n\n    def my_func_1(x):\n        return tf.slice(x[0], tf.cast(x[1][0], tf.int32), [3, 1, 1, 2])\n\n    input1 = Input(shape=(2, 3, 5), name=\'inputs\')\n    input2 = Input(shape=(4,), dtype=tf.int32, name=\'begin\')\n    added = Lambda(my_func_1)([input1, input2])\n    model = keras.models.Model(inputs=[input1, input2], outputs=added)\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_slice\')\n    data1 = np.random.rand(3, 2, 3, 5).astype(np.float32)\n    data2 = np.array([[0, 1, 0, 2], [0, 1, 0, 2], [0, 1, 0, 2]]).astype(np.int32)\n    expected = model.predict([data1, data2])\n    assert runner(\'onnx_tf_slice\', onnx_model, {""inputs"": data1, \'begin\': data2}, expected)\n\n    def my_func_2(x):\n        return tf.slice(x[0], [0, 1, 0, 2], tf.cast(x[1][0], tf.int32))\n\n    input1 = Input(shape=(2, 3, 5), name=\'inputs\')\n    input2 = Input(shape=(4,), dtype=tf.int32, name=\'size\')\n    added = Lambda(my_func_2)([input1, input2])\n    model = keras.models.Model(inputs=[input1, input2], outputs=added)\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_slice\')\n    data1 = np.random.rand(3, 2, 3, 5).astype(np.float32)\n    data2 = np.array([[3, 1, 1, 2], [3, 1, 1, 2], [3, 1, 1, 2]]).astype(np.int32)\n    expected = model.predict([data1, data2])\n    assert runner(\'onnx_tf_slice\', onnx_model, {""inputs"": data1, \'size\': data2}, expected)\n\n\ndef test_tf_softmax(runner):\n    for func_ in [lambda x: tf.nn.softmax(x), lambda x: tf.nn.softmax(x, axis=-1), lambda x: tf.nn.softmax(x, axis=1)]:\n        model = Sequential()\n        model.add(Lambda(func_, input_shape=[2, 3, 5]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_softmax\')\n        data = np.random.rand(3, 2, 3, 5).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_tf_softmax\', onnx_model, data, expected)\n\n\n@pytest.mark.skipif(is_tensorflow_older_than(\'1.14.0\'),\n                    reason=""dilations in tf.nn.depthwise_conv2d not supported."")\ndef test_tf_space_to_batch_nd(runner):\n    model = Sequential()\n    filter_value = np.random.rand(3, 3, 2, 2).astype(np.float32)\n    filter_constant = tf.constant(filter_value.tolist(), dtype=tf.float32)\n    model.add(Lambda(lambda x: tf.nn.depthwise_conv2d(\n        x, filter=filter_constant, strides=(1, 1, 1, 1), padding=""VALID"",\n        data_format=\'NHWC\', dilations=(2, 2)), input_shape=(8, 8, 2)))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_space_to_batch_nd\')\n    data = np.random.rand(3, 8, 8, 2).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_space_to_batch_nd\', onnx_model, data, expected)\n\n\ndef test_tf_splitv(runner):\n    def my_func_1(x):\n        return tf.split(x, [4, 15, 11], 2)[0]\n\n    model = Sequential()\n    model.add(Lambda(lambda x: my_func_1(x), input_shape=[5, 30]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_splitv\')\n    data = np.random.rand(2, 5, 30).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_splitv\', onnx_model, data, expected)\n\n\ndef test_tf_square(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: x + tf.square(x), input_shape=[2, 3, 5]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_square\')\n    data = np.random.rand(3, 2, 3, 5).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tf_square\', onnx_model, data, expected)\n\n\ndef test_tf_squeeze(runner):\n    for func_ in [lambda x: tf.squeeze(x, [1]), lambda x: tf.squeeze(x), lambda x: tf.squeeze(x, [-2])]:\n        model = Sequential()\n        model.add(Lambda(func_, input_shape=[1, 2, 1, 2]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_squeeze\')\n        data = np.random.rand(3, 1, 2, 1, 2).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_squeeze\', onnx_model, data, expected)\n\n\ndef test_tf_stack(runner):\n    def my_func_1(x):\n        return tf.stack([x[0], x[1], x[2]], axis=1)\n\n    def my_func_2(x):\n        return tf.stack([x[0], x[1], x[2]], axis=-1)\n\n    for myFunc in [my_func_1, my_func_2]:\n        K.clear_session()\n        input_shape = (2, 3)\n        input1 = Input(shape=input_shape)\n        input2 = Input(shape=input_shape)\n        input3 = Input(shape=input_shape)\n        added = Lambda(myFunc)([input1, input2, input3])\n        model = keras.models.Model(inputs=[input1, input2, input3], outputs=added)\n\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_stack\')\n        batch_data_shape = (1,) + input_shape\n        data1 = np.random.rand(*batch_data_shape).astype(np.float32)\n        data2 = np.random.rand(*batch_data_shape).astype(np.float32)\n        data3 = np.random.rand(*batch_data_shape).astype(np.float32)\n        expected = model.predict([data1, data2, data3])\n        assert runner(\'onnx_stack\', onnx_model, [data1, data2, data3], expected)\n\n\ndef test_stridedslice_with_version(runner):\n    target_opset = get_maximum_opset_supported()\n    for v1 in [-1, 1]:\n        for v2 in [-1, 2]:\n            model = Sequential()\n            model.add(\n                Lambda(lambda x: x[:, tf.newaxis, v1:, tf.newaxis, :v2, tf.newaxis, 3], input_shape=[2, 3, 4, 5]))\n            onnx_model = keras2onnx.convert_keras(model, \'test\', target_opset=target_opset)\n\n            data = np.random.rand(6 * 2 * 3 * 4 * 5).astype(np.float32).reshape(6, 2, 3, 4, 5)\n            expected = model.predict(data)\n            assert runner(\'onnx_stridedslice\', onnx_model, data, expected)\n\n\ndef test_stridedslice_ellipse_newaxis(runner):\n    target_opset = get_maximum_opset_supported()\n    model = Sequential()\n    model.add(\n        Lambda(lambda x: x[:, 1:, tf.newaxis, ..., :, 1:, tf.newaxis], input_shape=[2, 3, 4, 3, 2, 2]))\n    onnx_model = keras2onnx.convert_keras(model, \'test\', target_opset=target_opset)\n    data = np.random.rand(6 * 2 * 3 * 4 * 3 * 2 * 2).astype(np.float32).reshape(6, 2, 3, 4, 3, 2, 2)\n    expected = model.predict(data)\n    assert runner(\'onnx_stridedslice\', onnx_model, data, expected)\n\n    model = Sequential()\n    model.add(\n        Lambda(lambda x: x[...], input_shape=[2, 3, 4, 5]))\n    onnx_model = keras2onnx.convert_keras(model, \'test\', target_opset=target_opset)\n    data = np.random.rand(6 * 2 * 3 * 4 * 5).astype(np.float32).reshape(6, 2, 3, 4, 5)\n    expected = model.predict(data)\n    assert runner(\'onnx_stridedslice\', onnx_model, data, expected)\n\n\ndef test_stridedslice_ellipsis_mask_with_version(runner):\n    target_opset = get_maximum_opset_supported()\n    model = Sequential()\n    model.add(Lambda(lambda x: x[:, :2, ..., 1:], input_shape=[3, 4, 5, 6, 3]))\n    onnx_model = keras2onnx.convert_keras(model, \'test\', target_opset=target_opset)\n\n    data = np.random.rand(5 * 3 * 4 * 5 * 6 * 3).astype(np.float32).reshape(5, 3, 4, 5, 6, 3)\n    expected = model.predict(data)\n    assert runner(\'onnx_stridedslice_ellipsis_mask\', onnx_model, data, expected)\n\n\ndef test_stridedslice_shrink_mask_with_version(runner):\n    target_opset = get_maximum_opset_supported()\n    for shrink_value in [-1, 2]:\n        model = Sequential()\n        model.add(Lambda(lambda x: x[:, shrink_value, :], input_shape=[3, 4, 5]))\n        onnx_model = keras2onnx.convert_keras(model, \'test\', target_opset=target_opset)\n        data = np.random.rand(2 * 3 * 4 * 5).astype(np.float32).reshape(2, 3, 4, 5)\n        expected = model.predict(data)\n        assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 10,\n                    reason=""dynamic end is not supported for Slice op, opset < 10."")\ndef test_stridedslice_dynamic_end(runner):\n    def my_func(x):\n        frame_dim = tf.shape(x)[2]\n        return x[:, :-1, 1:frame_dim - 1, :]\n\n    model = Sequential()\n    filters = 8\n    kernel_size = (2, 5)\n    strides = (1, 2)\n    model.add(Conv2DTranspose(filters, kernel_size, strides=strides, use_bias=False,\n                              padding=""valid"", name=\'conv2d_transpose\', input_shape=[3, 4, 5]))\n    model.add(Lambda(my_func))\n    data1 = np.random.rand(2 * 3 * 4 * 5).astype(np.float32).reshape(2, 3, 4, 5)\n    expected = model.predict(data1)\n    onnx_model = keras2onnx.convert_keras(model, \'test_strided_slice_dynamic_input\')\n    assert runner(onnx_model.graph.name, onnx_model, data1, expected)\n\n\ndef test_tf_tile(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: tf.tile(x, [1, 1, 3]), input_shape=[2, 2]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_tile\')\n    data = np.random.rand(3, 2, 2).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_tile\', onnx_model, data, expected)\n\n\ndef test_tf_topk(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: tf.nn.top_k(x, k=2)[0], input_shape=[5, 5]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_topk\')\n    data = np.random.rand(3, 5, 5).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_topk\', onnx_model, data, expected)\n\n\ndef test_tf_transpose(runner):\n    model = Sequential()\n    model.add(Lambda(lambda x: tf.transpose(x, perm=[0, 2, 3, 1]), input_shape=[2, 3, 4]))\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_transpose\')\n    data = np.random.rand(2, 2, 3, 4).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'onnx_transpose_1\', onnx_model, data, expected)\n\n    if is_tensorflow_later_than(\'1.13.0\'):\n        model = Sequential()\n        model.add(Lambda(lambda x: tf.transpose(x), input_shape=[2, 3, 4]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_transpose\')\n        data = np.random.rand(4, 2, 3, 4).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_transpose_2\', onnx_model, data, expected)\n\n        def my_func_1(x):\n            a = tf.constant([[1, 2, 3], [4, 5, 6]], tf.float32)\n            return x + tf.transpose(a)\n\n        model = Sequential()\n        model.add(Lambda(lambda x: my_func_1(x), input_shape=[3, 2]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_transpose\')\n        data = np.random.rand(2, 3, 2).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_transpose_3\', onnx_model, data, expected)\n\n\ndef test_tf_unpack(runner):\n    for axis in [1, -1]:\n        model = Sequential()\n        model.add(Lambda(lambda x: tf.unstack(x, axis=axis)[0], input_shape=[2, 3, 4]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_unpack\')\n        data = np.random.rand(3, 2, 3, 4).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_unpack\', onnx_model, data, expected)\n\n\n@pytest.mark.skipif(is_tf2,\n                    reason=""tf 2.0 is not supported."")\ndef test_tf_variable(runner):\n    val = np.random.random((2, 3, 4))\n    for var_ in [K.variable(value=val), K.zeros(shape=(2, 3, 4)), K.ones(shape=(2, 3, 4))]:\n        model = Sequential()\n        model.add(Lambda(lambda x: x + var_, input_shape=[2, 3, 4]))\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_variable\')\n        data = np.random.rand(3, 2, 3, 4).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'onnx_variable\', onnx_model, data, expected)\n\n\n@pytest.mark.skipif(is_tf2 or get_maximum_opset_supported() < 9,\n                    reason=""tf 2.0 or opset < 9 is not supported."")\ndef test_tf_where(runner):\n    model = Sequential()\n    a = tf.constant([[[1, 1], [3, 6]], [[7, 8], [9, 9]]])\n    b = tf.where(tf.equal(a, 3))\n    model.add(Lambda(lambda x: b, input_shape=(2,)))\n    data = np.random.rand(1, 2).astype(np.float32)\n    expected = model.predict(data)\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_where\')\n    assert runner(\'onnx_where\', onnx_model, data, expected)\n\n    model = Sequential()\n    a = tf.constant([[[1, 1], [3, 6]], [[7, 8], [3, 3]]])\n    b = tf.where(tf.equal(a, 3))\n    model.add(Lambda(lambda x: b, input_shape=(2,)))\n    data = np.random.rand(3, 2).astype(np.float32)\n    expected = model.predict(data)\n    onnx_model = keras2onnx.convert_keras(model, \'test_tf_where\')\n    assert runner(\'onnx_where\', onnx_model, data, expected)\n\n    target_opset = get_maximum_opset_supported()\n    if target_opset >= 9:\n        model = Sequential()\n        x = tf.constant([[1, 2, 3], [4, 5, 6]])\n        y = tf.constant([[7, 8, 9], [10, 11, 12]])\n        condition = tf.constant([[True, False, False], [False, True, True]])\n        b = tf.where(condition, x, y)\n        model.add(Lambda(lambda x: b, input_shape=(2,)))\n        data = np.random.rand(2, 2).astype(np.float32)\n        expected = model.predict(data)\n        onnx_model = keras2onnx.convert_keras(model, \'test_tf_where\')\n        assert runner(\'onnx_where\', onnx_model, data, expected)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 9, reason=""conversion needs opset 9."")\ndef test_any_all(runner):\n    for l_ in [keras.backend.any, keras.backend.all]:\n        for axis in [1, -1]:\n            keras_model = Sequential()\n            keras_model.add(Lambda(lambda x: l_(x, axis=axis), input_shape=[3, 5]))\n            onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n            x = np.random.rand(2, 3, 5).astype(np.float32)\n            expected = keras_model.predict(x)\n            assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\ndef test_dense(runner):\n    for bias_value in [True, False]:\n        model = keras.Sequential()\n        model.add(Dense(5, input_shape=(4,), activation=\'sigmoid\'))\n        model.add(Dense(3, input_shape=(5,), use_bias=bias_value))\n        model.compile(\'sgd\', \'mse\')\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n\n        data = _asarray(1, 0, 0, 1)\n        expected = model.predict(data)\n        assert runner(\'dense\', onnx_model, data, expected)\n\n\ndef test_dense_add(runner):\n    input1 = Input(shape=(4,))\n    x1 = Dense(3, activation=\'relu\')(input1)\n    input2 = Input(shape=(5,))\n    x2 = Dense(3, activation=\'sigmoid\')(input2)\n    input3 = Input(shape=(3,))\n    x3 = Dense(3)(input3)\n    added = Add()([x1, x2, x3])  # equivalent to added = add([x1, x2])\n    model = keras.models.Model(inputs=[input1, input2, input3], outputs=added)\n    model.compile(\'sgd\', \'mse\')\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    data = [_asarray(1.2, 2.4, -2, 1), _asarray(-1, -2, 0, 1, 2), _asarray(0.5, 1.5, -3.14159)]\n    expected = model.predict(data)\n    assert runner(\'onnx_dense_add\', onnx_model, data, expected)\n\n\n@pytest.mark.skipif(is_tf2, reason=""const is not initialized this way for tf2"")\ndef test_conv_add(runner):\n    input1 = Input(shape=(10, 10, 1))\n    x1 = Conv2D(32, strides=(2, 2), kernel_size=3,\n                bias_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None))(input1)\n    input2 = Input(tensor=tf.constant(np.random.rand(1, 32).astype(np.float32)))\n    added = Add()([x1, input2])\n    model = keras.models.Model(inputs=[input1, input2], outputs=added)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    data = [np.random.rand(1, 10, 10, 1).astype(np.float32)]\n    expected = model.predict(data)\n    data += [np.random.rand(1, 32).astype(np.float32)]\n    assert runner(\'onnx_conv_add\', onnx_model, data, expected)\n\n\ndef test_dense_softmax(runner):\n    data = _asarray(1, 2, 3, 4)\n    model = Sequential()\n    model.add(Dense(5, input_shape=(4,), activation=\'softmax\'))\n    model.add(Dense(3, input_shape=(5,), use_bias=True))\n    model.compile(\'sgd\', \'mse\')\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    expected = model.predict(data)\n    assert runner(\'dense_softmax_1\', onnx_model, data, expected)\n\n    model = Sequential()\n    model.add(Dense(5, input_shape=(4,)))\n    model.add(Activation(\'softmax\'))\n    model.add(Dense(3, input_shape=(5,), use_bias=True))\n    model.compile(\'sgd\', \'mse\')\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    expected = model.predict(data)\n    assert runner(\'dense_softmax_2\', onnx_model, data, expected)\n\n\n@pytest.mark.parametrize(""layer_type, data"", [\n    (Add, ([1, 2, 3], [4, 5, 6])),\n    (Add, ([1, 2, 3], [4, 5, 6], [-3, -1, 1.5])),\n    (Subtract, ([1, 2, 3], [4, 5, 6])),\n    (Multiply, ([1, 2, 3], [4, 5, 6])),\n    (Average, ([1, -2, 3], [3, 1, 1])),\n    (Maximum, ([1, -2, 3], [3, 1, 1])),\n    (lambda: Concatenate(), ([1, 2, 3], [4, 5, 6, 7])),\n    (lambda: Concatenate(), ([1, 2, 3], [4, 5, 6, 7])),\n    (lambda: Concatenate(-1), ([[1, 2], [3, 4]], [[4, 5], [6, 7]])),\n    (lambda: Concatenate(1), ([[1, 2], [3, 4]], [[4, 5], [6, 7]])),\n    (lambda: Concatenate(2), ([[1, 2], [3, 4]], [[4, 5], [6, 7]])),\n])\ndef test_merge_layer(runner, layer_type, data):\n    data2 = [_asarray(*d) for d in data]\n    inputs = [Input(shape=d.shape[1:]) for d in data2]\n    layer = layer_type()(inputs)\n    model = keras.models.Model(inputs=inputs, outputs=layer)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    expected = model.predict(data2)\n    assert runner(onnx_model.graph.name, onnx_model, data2, expected)\n\n\n@pytest.fixture(scope=\'function\')\ndef conv_runner(runner):\n    def func(layer_type, input_channels, output_channels, kernel_size, strides, input_size, activation,\n             rtol, atol, bias, channels_first=False, padding=\'valid\'):\n        model = keras.Sequential()\n        input_size_seq = (input_size,) if isinstance(input_size, int) else input_size\n        kwargs = {}\n        if channels_first:\n            input_shape = (input_channels,) + input_size_seq\n            if not isinstance(layer_type, Conv1D):\n                kwargs[\'data_format\'] = \'channels_first\'\n        else:\n            input_shape = input_size_seq + (input_channels,)\n\n        model.add(layer_type(output_channels, kernel_size, input_shape=input_shape, strides=strides, padding=padding,\n                             dilation_rate=1, activation=activation, use_bias=bias, **kwargs))\n        data = np.random.uniform(-0.5, 0.5, size=(1,) + input_shape).astype(np.float32)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n\n        expected = model.predict(data)\n        assert runner(onnx_model.graph.name, onnx_model, data, expected, rtol=rtol, atol=atol)\n\n    return func\n\n\n@pytest.fixture(scope=\'function\')\ndef conv1_runner(conv_runner):\n    def func(*args, activation=None, rtol=1e-4, atol=1e-6, bias=False, padding=\'valid\'):\n        return conv_runner(Conv1D, *args, activation, rtol, atol, bias, padding=padding)\n\n    return func\n\n\ndef test_conv1d(conv1_runner):\n    conv1_runner(4, 5, 3, 1, 15)\n    conv1_runner(4, 5, 3, 2, 15)\n\n\ndef test_conv1d_padding(conv1_runner):\n    conv1_runner(4, 5, 3, 1, 15, padding=\'same\')\n\n    test_causal = False\n    if is_tf_keras:\n        import tensorflow\n        from distutils.version import StrictVersion\n        if StrictVersion(tensorflow.__version__.split(\'-\')[0]) >= StrictVersion(\'1.12.0\'):\n            test_causal = True\n    else:\n        test_causal = True\n\n    if test_causal:\n        conv1_runner(4, 5, 3, 1, 15, padding=\'causal\')\n\n\ndef test_conv1d_activation(conv1_runner):\n    conv1_runner(4, 5, 3, 1, 15, activation=\'sigmoid\')\n\n\ndef test_conv1d_bias(conv1_runner):\n    conv1_runner(4, 5, 3, 1, 15, bias=True)\n\n\n@pytest.fixture(scope=\'function\')\ndef conv2_runner(conv_runner):\n    def func(*args, activation=None, rtol=1e-3, atol=1e-5, bias=False, channels_first=False, padding=\'valid\'):\n        input_dims = args[-1]\n        assert len(input_dims) == 2\n        conv_runner(Conv2D, *args, activation, rtol, atol, bias, channels_first, padding)\n\n    return func\n\n\n@pytest.fixture(scope=\'function\')\ndef conv2trans_runner(conv_runner):\n    def func(*args, activation=None, rtol=1e-3, atol=1e-5, bias=False, channels_first=False, padding=\'valid\'):\n        input_dims = args[-1]\n        assert len(input_dims) == 2\n        conv_runner(Conv2DTranspose, *args, activation, rtol, atol, bias, channels_first, padding)\n\n    return func\n\n\ndef test_conv2d(conv2_runner):\n    conv2_runner(3, 5, (2, 2), (1, 1), (5, 5))\n\n\ndef test_conv2d_transpose(conv2trans_runner):\n    conv2trans_runner(3, 5, (2, 2), (1, 1), (5, 5))\n\n\ndef test_conv2d_padding_same(conv2_runner):\n    conv2_runner(3, 5, (2, 2), (1, 1), (5, 5), padding=\'same\')\n    conv2_runner(8, 16, (1, 1), (2, 2), (60, 60), padding=\'same\')\n    conv2_runner(1, 1, (3, 3), (2, 2), (6, 6), padding=\'same\')\n    conv2_runner(1, 1, (7, 7), (2, 2), (25, 25), padding=\'same\')\n    conv2_runner(1, 1, (5, 7), (3, 5), (25, 25), padding=\'same\')\n\n\n@pytest.mark.skipif(is_tf_keras, reason=""Generic conv implementation only supports NHWC tensor format in tf_keras"")\ndef test_conv2d_format(conv2_runner):\n    conv2_runner(3, 5, (2, 2), (1, 1), (5, 5), channels_first=True)\n\n\ndef test_conv2d_activation(conv2_runner):\n    conv2_runner(3, 5, (2, 2), (1, 1), (5, 5), activation=\'relu\')\n    conv2_runner(3, 5, (2, 2), (1, 1), (5, 5), activation=\'softmax\')\n\n\ndef test_conv2d_bias(conv2_runner):\n    conv2_runner(3, 5, (2, 2), (1, 1), (5, 5), bias=True)\n\n\ndef test_conv2d_larger(conv2_runner):\n    conv2_runner(3, 5, (7, 9), 1, (30, 20))\n\n\ndef test_conv2d_uneven_stride(conv2_runner):\n    conv2_runner(3, 5, (4, 4), (3, 2), (20, 10))\n\n\n@pytest.fixture(scope=\'function\')\ndef conv3_runner(conv_runner):\n    def func(*args, activation=None, rtol=1e-3, atol=1e-5, bias=False, channels_first=False, padding=\'valid\'):\n        input_dims = args[-1]\n        assert len(input_dims) == 3\n        conv_runner(Conv3D, *args, activation, rtol, atol, bias, channels_first, padding)\n\n    return func\n\n\ndef test_conv3d(conv3_runner):\n    conv3_runner(3, 5, (2, 2, 2), (1, 1, 1), (5, 5, 8))\n\n\n@pytest.fixture(scope=\'function\')\ndef conv3trans_runner(conv_runner):\n    def func(*args, activation=None, rtol=1e-3, atol=1e-5, bias=False, channels_first=False, padding=\'valid\'):\n        input_dims = args[-1]\n        assert len(input_dims) == 3\n        conv_runner(Conv3DTranspose, *args, activation, rtol, atol, bias, channels_first, padding)\n\n    return func\n\n\n@pytest.mark.skip(""ONNXRuntime doesn\'t support 3D ConvTranspose."")\ndef test_conv3d_transpose(conv3trans_runner):\n    conv3trans_runner(3, 5, (2, 2, 2), (1, 1, 1), (5, 5, 8))\n\n\ndef test_flatten(runner):\n    model = keras.Sequential()\n    model.add(keras.layers.core.Flatten(input_shape=(3, 2)))\n    model.add(Dense(3))\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    data = np.array([[[1, 2], [3, 4], [5, 6]]]).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(\'flatten\', onnx_model, data, expected)\n\n\ndef test_opt_push_transpose_unsqueeze(runner):\n    for input_shape_ in [(1, 5, 7), (1, 5)]:\n        model = keras.Sequential()\n        if len(input_shape_) == 3:\n            model.add(Conv2D(64, (3, 3),\n                             input_shape=input_shape_, padding=\'same\', ))\n        else:\n            model.add(Conv1D(64, 3,\n                             input_shape=input_shape_, padding=\'same\', ))\n        model.add(Lambda(lambda x: tf.squeeze(x, [1])))\n        model.add(Lambda(lambda x: tf.expand_dims(x, 1)))\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        batch_input_shape = (4,) + input_shape_\n        x = np.random.rand(*batch_input_shape).astype(np.float32)\n        expected = model.predict(x)\n        assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\ndef test_flatten2(runner):\n    C = 3\n    H = 5\n    W = 7\n    for data_format in [\'channels_first\', \'channels_last\']:\n        model = keras.Sequential()\n        model.add(Conv2D(64, (3, 3),\n                         input_shape=(C, H, W), padding=\'same\', ))\n        model.add(Flatten(data_format=data_format))\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.rand(4, C, H, W).astype(np.float32)\n        expected = model.predict(x)\n        assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\ndef test_reshape(runner):\n    model = keras.Sequential()\n    model.add(keras.layers.core.Reshape((2, 3), input_shape=(3, 2)))\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    data = np.array([[[1, 2], [3, 4], [5, 6]]]).astype(np.float32)\n\n    expected = model.predict(data)\n    assert runner(\'reshape\', onnx_model, data, expected)\n\n\ndef test_permute(runner):\n    model = keras.Sequential()\n    model.add(keras.layers.core.Permute((2, 1), input_shape=(3, 2)))\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    data = np.array([[[1, 2], [3, 4], [5, 6]]]).astype(np.float32)\n\n    expected = model.predict(data)\n    assert runner(\'permute\', onnx_model, data, expected)\n\n\ndef test_repeat_vector(runner):\n    model = keras.Sequential()\n    model.add(keras.layers.core.RepeatVector(3, input_shape=(4,)))\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    data = _asarray(1, 2, 3, 4)\n\n    expected = model.predict(data)\n    assert runner(\'repeat_vector\', onnx_model, data, expected)\n\n\n@pytest.fixture(scope=\'function\')\ndef pooling_runner(runner):\n    def func(layer, ishape, data_format=\'channels_last\'):\n        model = keras.Sequential()\n        if is_keras_later_than(\'2.1.6\'):\n            nlayer = layer(data_format=data_format, input_shape=ishape) if \\\n                (layer.__name__.startswith(""Global"")) else layer(2, data_format=data_format, input_shape=ishape)\n        else:\n            nlayer = layer(input_shape=ishape) if \\\n                (layer.__name__.startswith(""Global"")) else layer(2, input_shape=ishape)\n\n        model.add(nlayer)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n\n        data = np.random.uniform(-0.5, 0.5, size=(1,) + ishape).astype(np.float32)\n\n        expected = model.predict(data)\n        assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n    return func\n\n\ndef test_pooling_1d(pooling_runner):\n    pooling_runner(AveragePooling1D, (4, 6))\n    pooling_runner(MaxPool1D, (4, 6))\n    if is_keras_later_than(\'2.1.6\'):\n        pooling_runner(AveragePooling1D, (4, 6), \'channels_first\')\n        pooling_runner(MaxPool1D, (4, 6), \'channels_first\')\n\n\ndef test_pooling_2d(pooling_runner, runner):\n    pooling_runner(AveragePooling2D, (4, 4, 3))\n\n    N, C, H, W = 2, 3, 5, 5\n    x = np.random.rand(N, H, W, C).astype(np.float32, copy=False)\n\n    model = Sequential()\n    model.add(MaxPooling2D((2, 2), strides=(2, 2), input_shape=(H, W, C), data_format=\'channels_last\'))\n    model.compile(optimizer=\'sgd\', loss=\'mse\')\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    expected = model.predict(x)\n    assert runner(\'max_pooling_2d\', onnx_model, x, expected)\n\n    # test padding=\'same\'\n    model = Sequential()\n    model.add(\n        MaxPooling2D((2, 2), strides=(2, 2), padding=\'same\', input_shape=(H, W, C), data_format=\'channels_last\'))\n    model.compile(optimizer=\'sgd\', loss=\'mse\')\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    expected = model.predict(x)\n    assert runner(\'max_pooling_2d\', onnx_model, x, expected)\n\n\ndef test_pooling_3d(pooling_runner):\n    pooling_runner(AveragePooling3D, (4, 4, 4, 3))\n    pooling_runner(MaxPool3D, (4, 4, 4, 3))\n\n\ndef test_pooling_global(pooling_runner):\n    pooling_runner(GlobalAveragePooling2D, (4, 6, 2))\n\n\n@pytest.mark.parametrize(""layer"", [\n    \'tanh\',\n    keras.activations.tanh,\n    \'sigmoid\',\n    keras.activations.sigmoid,\n    \'hard_sigmoid\',\n    keras.activations.hard_sigmoid,\n    \'relu\',\n    keras.activations.relu,\n    \'elu\',\n    keras.activations.elu,\n    \'selu\',\n    keras.activations.selu,\n    \'softsign\',\n    keras.activations.softsign,\n    \'softplus\',\n    keras.activations.softplus,\n    \'softmax\',\n    keras.activations.softmax,\n    \'linear\',\n    keras.activations.linear,\n])\ndef test_activation_layer(runner, layer):\n    data = _asarray(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n    layer = Activation(layer, input_shape=(data.size,))\n\n    model = keras.Sequential()\n    model.add(layer)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    expected = model.predict(data)\n    assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\n@pytest.fixture(scope=\'function\')\ndef advanced_activation_runner(runner):\n    def runner_func(layer, data, op_version=None):\n        if op_version is None:\n            op_version = get_maximum_opset_supported()\n\n        model = keras.Sequential()\n        model.add(layer)\n        onnx_model = keras2onnx.convert_keras(model, model.name, target_opset=op_version)\n\n        expected = model.predict(data)\n        assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n    return runner_func\n\n\ndef test_selu(runner):\n    SIZE = 10\n    NB_CLASS = 5\n    model = Sequential()\n    model.add(Conv2D(32, strides=(2, 2), kernel_size=3, input_shape=(SIZE, SIZE, 1)))\n    model.add(Flatten())\n    model.add(Dense(32, activation=\'selu\'))\n    model.add(Dense(NB_CLASS, activation=\'softmax\'))\n    model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n    data = np.random.rand(5, SIZE, SIZE, 1).astype(np.float32)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    expected = model.predict(data)\n    assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\ndef test_LeakyReLU(advanced_activation_runner):\n    data = _asarray(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n    layer = advanced_activations.LeakyReLU(alpha=0.1, input_shape=(data.size,))\n    advanced_activation_runner(layer, data)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 8,\n                    reason=""ThresoldRelu needs ONNX opset 8"")\ndef test_ThresholdedReLU(advanced_activation_runner):\n    data = _asarray(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n    layer = advanced_activations.ThresholdedReLU(theta=1.0, input_shape=(data.size,))\n    advanced_activation_runner(layer, data, op_version=8)\n\n    layer = advanced_activations.ThresholdedReLU(theta=1.0, input_shape=(data.size,))\n    advanced_activation_runner(layer, data)\n\n\ndef test_ELU(advanced_activation_runner):\n    data = _asarray(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n    layer = advanced_activations.ELU(alpha=1.0, input_shape=(data.size,))\n    advanced_activation_runner(layer, data)\n\n\ndef test_PReLU(advanced_activation_runner):\n    data = _asarray(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n    layer = advanced_activations.PReLU(alpha_initializer=\'zeros\', input_shape=(data.size,))\n    advanced_activation_runner(layer, data)\n    layer = advanced_activations.PReLU(alpha_initializer=\'ones\', input_shape=(data.size,))\n    advanced_activation_runner(layer, data)\n    layer = advanced_activations.PReLU(alpha_initializer=\'RandomNormal\', input_shape=(data.size,))\n    advanced_activation_runner(layer, data)\n\n\ndef test_Softmax(advanced_activation_runner):\n    data = _asarray(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n    layer = advanced_activations.Softmax(axis=-1, input_shape=(data.size,))\n    advanced_activation_runner(layer, data)\n\n\n@pytest.mark.skipif(is_tensorflow_older_than(\'1.14.0\') and is_tf_keras, reason=\'old tf version\')\ndef test_tf_nn_activation(runner):\n    for activation in [\'relu\', tf.nn.relu, tf.nn.relu6, tf.nn.softmax, tf.nn.leaky_relu]:\n        model = keras.Sequential([\n            Dense(64, activation=activation, input_shape=[10]),\n            Dense(64, activation=activation),\n            Dense(1)\n        ])\n        if is_tf_keras:\n            model.add(Activation(tf.keras.layers.LeakyReLU(alpha=0.2)))\n            model.add(Activation(tf.keras.layers.ReLU()))\n            model.add(tf.keras.layers.PReLU())\n            model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n        x = np.random.rand(5, 10).astype(np.float32)\n        expected = model.predict(x)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\n@pytest.fixture(scope=\'function\')\ndef misc_conv_runner(runner):\n    def func(layer, ishape, target_opset=None):\n        if target_opset is None:\n            target_opset = get_maximum_opset_supported()\n        input = keras.Input(ishape)\n        out = layer(input)\n        model = keras.models.Model(input, out)\n        onnx_model = keras2onnx.convert_keras(model, model.name, target_opset=target_opset)\n\n        data = np.random.uniform(0, 1, size=(1,) + ishape).astype(np.float32)\n\n        expected = model.predict(data)\n        assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n    return func\n\n\ndef test_crop(misc_conv_runner):\n    # It also passes the test for opset 9, we skip here because it uses a legacy experimental op DynamicSlice.\n    opset_ = get_maximum_opset_supported()\n    if opset_ >= 10:\n        ishape = (10, 20)\n        for crop_v in [2, (1, 2)]:\n            layer = Cropping1D(cropping=crop_v)\n            misc_conv_runner(layer, ishape, opset_)\n\n        for data_format_ in [\'channels_last\', \'channels_first\']:\n            ishape = (20, 20, 1)\n            for crop_v in [2, (2, 2), ((1, 2), (2, 3))]:\n                layer = Cropping2D(cropping=crop_v, data_format=data_format_)\n                misc_conv_runner(layer, ishape, opset_)\n            ishape = (20, 20, 20, 1)\n            for crop_v in [2, (2, 3, 4), ((1, 2), (2, 3), (3, 5))]:\n                layer = Cropping3D(cropping=crop_v, data_format=data_format_)\n                misc_conv_runner(layer, ishape, opset_)\n\n    # TODO handle other cases for opset 8\n    ishape = (20, 20, 1)\n    layer = Cropping2D(cropping=((1, 2), (2, 3)), data_format=\'channels_last\')\n    misc_conv_runner(layer, ishape, opset_)\n\n\ndef test_upsample(misc_conv_runner):\n    if is_keras_later_than(\'2.1.6\'):\n        ishape = (20, 5)\n        layer = UpSampling1D(size=2)\n        misc_conv_runner(layer, ishape)\n        if not is_tf_keras:\n            ishape = (20,)\n            layer = UpSampling1D(size=2)\n            misc_conv_runner(layer, ishape)\n    ishape = (20, 20, 1)\n    for size in [2, (2, 3)]:\n        layer = UpSampling2D(size=size, data_format=\'channels_last\')\n        misc_conv_runner(layer, ishape)\n        if not is_keras_older_than(""2.2.3""):\n            opset_ = get_maximum_opset_supported()\n            if opset_ >= 11 or not is_tf_keras:\n                layer = UpSampling2D(size=size, data_format=\'channels_last\', interpolation=\'bilinear\')\n                misc_conv_runner(layer, ishape)\n    ishape = (20, 20, 20, 1)\n    layer = UpSampling3D(size=(2, 3, 4), data_format=\'channels_last\')\n    misc_conv_runner(layer, ishape)\n\n\ndef test_padding(misc_conv_runner):\n    ishape = (20, 20, 1)\n    layer = ZeroPadding2D(padding=((1, 2), (2, 3)), data_format=\'channels_last\')\n    misc_conv_runner(layer, ishape)\n\n\ndef test_embedding(runner):\n    model = keras.Sequential()\n    model.add(Embedding(1000, 64, input_length=10))\n    input_array = np.random.randint(1000, size=(1, 10)).astype(np.float32)\n\n    model.compile(\'rmsprop\', \'mse\')\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    expected = model.predict(input_array)\n    assert runner(onnx_model.graph.name, onnx_model, input_array, expected)\n\n\n@pytest.fixture(scope=\'function\')\ndef dot_runner(runner):\n    def func(l2Normalize, input1, input2):\n        data = [input1, input2]\n        inputs = [Input(shape=d.shape[1:]) for d in data]\n\n        layer = Dot(axes=-1, normalize=l2Normalize)(inputs)\n        model = keras.models.Model(inputs=inputs, outputs=layer)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n\n        expected = model.predict(data)\n        assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n    return func\n\n\ndef test_dot(dot_runner):\n    dot_runner(False, _asarray(1, 2, 3), _asarray(4, 5, 6))\n    dot_runner(True, _asarray(1, 2, 3), _asarray(4, 5, 6))\n\n\ndef test_dot2(runner):\n    input_1_shapes = [[32, 20, 1], [2, 3, 5], [2, 3, 5], [4, 3, 5], [2, 7], [2, 3, 4, 12, 3], [1, 3]]\n    input_2_shapes = [[32, 30, 20], [2, 3, 5], [2, 3, 5], [4, 5], [2, 7, 5], [2, 3, 4, 15, 3], [1, 3]]\n    axes_list = [[1, 2], 1, 2, [2, 1], [1, 1], 4, 1]\n    for i_ in range(len(input_1_shapes)):\n        for normalize in [True, False]:\n            drop2_embed_title = Input(batch_shape=tuple(input_1_shapes[i_]), name=\'input1\')\n            att_weight = Input(batch_shape=tuple(input_2_shapes[i_]), name=\'input2\')\n            doc_vec1 = dot([drop2_embed_title, att_weight], axes=axes_list[i_], normalize=normalize)\n            model = keras.models.Model(inputs=[drop2_embed_title, att_weight], outputs=doc_vec1)\n            data1 = np.random.rand(*input_1_shapes[i_]).astype(np.float32)\n            data2 = np.random.rand(*input_2_shapes[i_]).astype(np.float32)\n            expected = model.predict([data1, data2])\n            onnx_model = keras2onnx.convert_keras(model, model.name)\n            assert runner(onnx_model.graph.name, onnx_model, [data1, data2], expected)\n\n    drop2_embed_title = Input(batch_shape=(None, 7), name=\'input1\')\n    att_weight = Input(batch_shape=(None, 7, 5), name=\'input2\')\n    doc_vec1 = dot([drop2_embed_title, att_weight], axes=[1, 1])\n    model = keras.models.Model(inputs=[drop2_embed_title, att_weight], outputs=doc_vec1)\n    data1 = np.random.rand(2, 7).astype(np.float32)\n    data2 = np.random.rand(2, 7, 5).astype(np.float32)\n    expected = model.predict([data1, data2])\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, [data1, data2], expected)\n\n\ndef test_training_layer(runner):\n    model = keras.Sequential()\n    model.add(Dense(32, input_shape=(2, 3, 4)))\n    model.add(GaussianNoise(0.1))\n    model.add(Activation(\'relu\'))\n    model.add(GaussianDropout(0.1))\n    model.add(AlphaDropout(0.1))\n    model.add(SpatialDropout2D(0.2))\n    model.add(Dense(1))\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    data = np.random.rand(2, 2, 3, 4).astype(np.float32)\n    expected = model.predict(data)\n    assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\n@pytest.fixture(scope=\'function\')\ndef batch_norm_runner(runner):\n    def func(data, gamma, beta, scale, center, axis):\n        model = keras.Sequential()\n        layer = BatchNormalization(\n            axis=axis,\n            input_shape=data.shape[1:],\n            moving_mean_initializer=keras.initializers.constant(np.mean(data)),\n            moving_variance_initializer=keras.initializers.constant(np.var(data)),\n            gamma_initializer=gamma,\n            beta_initializer=beta,\n            center=center,\n            scale=scale,\n        )\n        model.add(layer)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n\n        expected = model.predict(data)\n        assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n    return func\n\n\ndef test_batch_normalization(batch_norm_runner):\n    data = _asarray([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n    batch_norm_runner(data, \'ones\', \'zeros\', True, True, 3)\n    batch_norm_runner(data, \'ones\', \'ones\', True, True, 3)\n    # The CPU implementation of FusedBatchNorm only supports NHWC tensor format in tf keras\n    if not is_tf_keras:\n        batch_norm_runner(data, \'ones\', \'zeros\', True, True, 1)\n        batch_norm_runner(data, \'ones\', \'ones\', True, True, 1)\n        batch_norm_runner(data, \'ones\', \'ones\', True, False, 1)\n        batch_norm_runner(data, \'zeros\', \'zeros\', False, True, 1)\n\n\ndef test_batch_normalization_2(runner):\n    # The CPU implementation of FusedBatchNorm only supports NHWC tensor format in tf keras\n    axis_list = [-1] if is_tf_keras else [1, -1]\n    for axis in axis_list:\n        batch_size = 4\n        input_dim_1 = 10\n        input_dim_2 = 20\n        input_dim_3 = 30\n\n        model = Sequential()\n        model.add(InputLayer(input_shape=(input_dim_1,)))\n        model.add(BatchNormalization(axis=axis))\n        model.add(Dense(5))\n        data = np.random.randn(batch_size, input_dim_1).astype(np.float32)\n        onnx_model = keras2onnx.convert_keras(model)\n        expected = model.predict(data)\n        assert runner(\'test_batch_normalization_2_2d\', onnx_model, [data], expected)\n\n        model = Sequential()\n        model.add(InputLayer(input_shape=(input_dim_1, input_dim_2)))\n        if axis == -1:\n            model.add(Conv1D(32, strides=(2,), kernel_size=3))\n        model.add(BatchNormalization(axis=axis))\n        model.add(Dense(5))\n        data = np.random.randn(batch_size, input_dim_1, input_dim_2).astype(np.float32)\n        onnx_model = keras2onnx.convert_keras(model)\n        expected = model.predict(data)\n        assert runner(\'test_batch_normalization_2_3d\', onnx_model, [data], expected)\n\n        model = Sequential()\n        model.add(InputLayer(input_shape=(input_dim_1, input_dim_2, input_dim_3)))\n        if axis == -1:\n            model.add(Conv2D(32, strides=(2, 2), kernel_size=3))\n        model.add(BatchNormalization(axis=axis))\n        model.add(Dense(5))\n        data = np.random.randn(batch_size, input_dim_1, input_dim_2, input_dim_3).astype(np.float32)\n        onnx_model = keras2onnx.convert_keras(model)\n        expected = model.predict(data)\n        assert runner(\'test_batch_normalization_2_4d\', onnx_model, [data], expected)\n\n\ndef test_simpleRNN(runner):\n    K.clear_session()\n    inputs1 = keras.Input(shape=(3, 1))\n    cls = SimpleRNN(2, return_state=False, return_sequences=True)\n    oname = cls(inputs1)  # , initial_state=t0)\n    model = keras.Model(inputs=inputs1, outputs=[oname])\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    data = np.array([0.1, 0.2, 0.3]).astype(np.float32).reshape((1, 3, 1))\n    expected = model.predict(data)\n    assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n    # with initial state\n    inputs2 = keras.Input(shape=(1, 2))\n    state = keras.Input(shape=(5,))\n    hidden_1 = SimpleRNN(5, activation=\'relu\', return_sequences=True)(inputs2, initial_state=[state])\n    output = Dense(2, activation=\'sigmoid\')(hidden_1)\n    keras_model = keras.Model(inputs=[inputs2, state], outputs=output)\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n\n    N, H, W, C = 3, 1, 2, 5\n    x = np.random.rand(N, H, W).astype(np.float32, copy=False)\n    s = np.random.rand(N, C).astype(np.float32, copy=False)\n    expected = keras_model.predict([x, s])\n    assert runner(onnx_model.graph.name, onnx_model, [x, s], expected)\n\n    # with initial state and output state\n    input = keras.Input(shape=(1, 2))\n    state_in = keras.Input(shape=(10,))\n    hidden_1, state_out = SimpleRNN(10, activation=\'relu\', return_sequences=True,\n                                    return_state=True)(input, initial_state=[state_in])\n    output = Dense(2, activation=\'linear\')(hidden_1)\n    keras_model = keras.Model(inputs=[input, state_in], outputs=[output, state_out])\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n\n    N, H, W, C = 3, 1, 2, 10\n    x = np.random.rand(N, H, W).astype(np.float32, copy=False)\n    s = np.random.rand(N, C).astype(np.float32, copy=False)\n    expected = keras_model.predict([x, s])\n    assert runner(onnx_model.graph.name, onnx_model, [x, s], expected)\n\n\ndef test_GRU(runner):\n    inputs1 = keras.Input(shape=(3, 1))\n\n    cls = GRU(2, return_state=False, return_sequences=False)\n    oname = cls(inputs1)\n    model = keras.Model(inputs=inputs1, outputs=[oname])\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    data = np.array([0.1, 0.2, 0.3]).astype(np.float32).reshape((1, 3, 1))\n    expected = model.predict(data)\n    assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n    # GRU with initial state\n    for return_sequences in [True, False]:\n        cls = GRU(2, return_state=False, return_sequences=return_sequences)\n        initial_state_input = keras.Input(shape=(2,))\n        oname = cls(inputs1, initial_state=initial_state_input)\n        model = keras.Model(inputs=[inputs1, initial_state_input], outputs=[oname])\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n\n        data = np.array([0.1, 0.2, 0.3]).astype(np.float32).reshape((1, 3, 1))\n        init_state = np.array([0.4, 0.5]).astype(np.float32).reshape((1, 2))\n        init_state_onnx = np.array([0.4, 0.5]).astype(np.float32).reshape((1, 2))\n        expected = model.predict([data, init_state])\n        assert runner(onnx_model.graph.name, onnx_model, [data, init_state_onnx], expected)\n\n\ndef test_LSTM(runner):\n    inputs1 = keras.Input(shape=(3, 5))\n    data = np.random.rand(3, 5).astype(np.float32).reshape((1, 3, 5))\n    for use_bias in [True, False]:\n        for return_sequences in [True, False]:\n            cls = LSTM(units=2, return_state=True, return_sequences=return_sequences, use_bias=use_bias)\n            lstm1, state_h, state_c = cls(inputs1)\n            model = keras.Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n            onnx_model = keras2onnx.convert_keras(model, model.name)\n            expected = model.predict(data)\n            assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\n@pytest.mark.skipif((is_tensorflow_older_than(\'1.14.0\') or (not is_tf_keras)),\n                    reason=""keras LSTM does not have time_major attribute"")\ndef test_LSTM_time_major_return_seq_true(runner):\n    inputs1 = keras.Input(shape=(3, 5))\n    data = np.random.rand(1, 3, 5).astype(np.float32)\n    # Transpose input to be time major\n    input_transposed = tf.transpose(inputs1, perm=[1, 0, 2])\n    lstm1, state_h, state_c = LSTM(units=2, time_major=True, return_state=True,\n                                   return_sequences=True)(input_transposed)\n    lstm1_trans = tf.transpose(lstm1, perm=[1, 0, 2])\n    model = keras.Model(inputs=inputs1, outputs=[lstm1_trans, state_h, state_c])\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    expected = model.predict(data)\n    assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\n@pytest.mark.skipif((is_tensorflow_older_than(\'1.14.0\') or (not is_tf_keras)),\n                    reason=""keras LSTM does not have time_major attribute"")\ndef test_LSTM_time_major_return_seq_false(runner):\n    inputs1 = keras.Input(shape=(3, 5))\n    data = np.random.rand(1, 3, 5).astype(np.float32)\n    # Transpose input to be time major\n    input_transposed = tf.transpose(inputs1, perm=[1, 0, 2])\n    lstm1, state_h, state_c = LSTM(units=2, time_major=True, return_state=True,\n                                   return_sequences=False)(input_transposed)\n    model = keras.Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    expected = model.predict(data)\n    assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\ndef test_LSTM_with_bias(runner):\n    inputs1 = keras.Input(shape=(1, 1))\n    cls = LSTM(units=1, return_state=True, return_sequences=True)\n    lstm1, state_h, state_c = cls(inputs1)\n    model = keras.Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n    # Set weights: kernel, recurrent_kernel and bias\n    model.set_weights((np.array([[1, 2, 3, 4]]), np.array([[5, 6, 7, 8]]), np.array([1, 2, 3, 4])))\n    data = np.random.rand(1, 1).astype(np.float32).reshape((1, 1, 1))\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n\n    expected = model.predict(data)\n    assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\ndef test_LSTM_reshape(runner):\n    input_dim = 7\n    sequence_len = 3\n    inputs1 = keras.Input(shape=(sequence_len, input_dim))\n    cls = LSTM(units=5, return_state=False, return_sequences=True)\n    lstm1 = cls(inputs1)\n    output = Reshape((sequence_len, 5))(lstm1)\n    model = keras.Model(inputs=inputs1, outputs=output)\n    model.compile(optimizer=\'sgd\', loss=\'mse\')\n\n    onnx_model = keras2onnx.convert_keras(model, \'test\')\n    data = np.random.rand(input_dim, sequence_len).astype(np.float32).reshape((1, sequence_len, input_dim))\n    expected = model.predict(data)\n    assert runner(\'tf_lstm\', onnx_model, data, expected)\n\n\ndef test_LSTM_with_initializer(runner):\n    # batch_size = N\n    # seq_length = H\n    # input_size = W\n    # hidden_size = C\n    N, H, W, C = 3, 1, 2, 5\n\n    # inputs shape: (batch_size, seq_length)\n    inputs = keras.Input(shape=(H, W), name=\'inputs\')\n\n    # initial state shape: (hidden_size, 1)\n    state_h = keras.Input(shape=(C,), name=\'state_h\')\n    state_c = keras.Input(shape=(C,), name=\'state_c\')\n\n    # create keras model\n    lstm_layer = LSTM(units=C, activation=\'relu\', return_sequences=True)(inputs,\n                                                                         initial_state=[state_h,\n                                                                                        state_c])\n    outputs = Dense(W, activation=\'sigmoid\')(lstm_layer)\n    keras_model = keras.Model(inputs=[inputs, state_h, state_c], outputs=outputs)\n\n    x = np.random.rand(1, H, W).astype(np.float32)\n    sh = np.random.rand(1, C).astype(np.float32)\n    sc = np.random.rand(1, C).astype(np.float32)\n    expected = keras_model.predict([x, sh, sc])\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n    assert runner(onnx_model.graph.name, onnx_model, {""inputs"": x, \'state_h\': sh, \'state_c\': sc}, expected)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 5,\n                    reason=""None seq_length LSTM is not supported before opset 5."")\ndef test_LSTM_seqlen_none(runner):\n    lstm_dim = 2\n    data = np.random.rand(1, 5, 1).astype(np.float32)\n    for return_sequences in [True, False]:\n        inp = Input(batch_shape=(1, None, 1))\n        out = LSTM(lstm_dim, return_sequences=return_sequences, stateful=True)(inp)\n        keras_model = keras.Model(inputs=inp, outputs=out)\n\n        onnx_model = keras2onnx.convert_keras(keras_model)\n        expected = keras_model.predict(data)\n        assert runner(onnx_model.graph.name, onnx_model, data, expected)\n\n\n@pytest.mark.parametrize(""return_sequences"", [True, False])\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_bidirectional(runner, rnn_class, return_sequences):\n    input_dim = 10\n    sequence_len = 5\n    op_version = get_maximum_opset_supported()\n    batch_list = [1, 4] if op_version >= 9 else [1]\n\n    model = keras.Sequential()\n    model.add(Bidirectional(rnn_class(7, return_sequences=return_sequences),\n                            input_shape=(5, 10)))\n    model.add(Dense(5))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\')\n    onnx_model = keras2onnx.convert_keras(model, \'test\', target_opset=op_version)\n    for batch in batch_list:\n        data = np.random.rand(batch, sequence_len, input_dim).astype(np.float32)\n        expected = model.predict(data)\n        assert runner(\'bidirectional\', onnx_model, data, expected)\n\n    for merge_mode in [\'concat\', None]:\n        sub_input1 = Input(shape=(sequence_len, input_dim))\n        sub_mapped1 = Bidirectional(rnn_class(7, return_sequences=return_sequences),\n                                    input_shape=(5, 10), merge_mode=merge_mode)(sub_input1)\n        keras_model = keras.Model(inputs=sub_input1, outputs=sub_mapped1)\n        onnx_model = keras2onnx.convert_keras(keras_model, \'test_2\', target_opset=op_version)\n        for batch in batch_list:\n            data = np.random.rand(batch, sequence_len, input_dim).astype(np.float32)\n            expected = keras_model.predict(data)\n            assert runner(\'bidirectional\', onnx_model, data, expected)\n\n\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_bidirectional_with_bias(runner, rnn_class):\n    K.clear_session()\n    model = keras.Sequential()\n    model.add(Bidirectional(rnn_class(4, return_sequences=False),\n                            input_shape=(3, 5), name=\'bi\'))\n\n    x = np.random.uniform(100, 999, size=(2, 3, 5)).astype(np.float32)\n\n    # Test with the default bias\n    expected = model.predict(x)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n    # Set bias values to random floats\n    rnn_layer = model.get_layer(\'bi\')\n    weights = rnn_layer.get_weights()\n    weights[2] = np.random.uniform(size=weights[2].shape)\n    weights[5] = weights[2]\n    rnn_layer.set_weights(weights)\n\n    # Test with random bias\n    expected = model.predict(x)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n@pytest.mark.skipif((is_tensorflow_older_than(\'2.3.0\') or (not is_tf_keras)),\n                     reason=(""keras LSTM does not have time_major attribute. There was a bug in tf.keras bidirectional lstm with time_major true which will be fixed in tf-2.3, See - https://github.com/tensorflow/tensorflow/issues/39635""))\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_bidirectional_time_major_true(runner, rnn_class):\n    feature_dim = 1\n    seq_len = 3\n    x = np.ones((1, seq_len, feature_dim), dtype=np.float32)\n\n    for ret_seq in [True, False]:\n        for merge_mode in [\'concat\', None]:\n            K.clear_session()\n            input = keras.Input(shape=(seq_len, feature_dim))\n            # Transpose input to be time major\n            input_transposed = tf.transpose(input, perm=[1,0,2])\n            output = Bidirectional(rnn_class(1, return_sequences=ret_seq,\n                                             time_major=True),\n                                   name=\'bi\', merge_mode=merge_mode)(input_transposed)\n            if ret_seq and merge_mode == \'concat\':\n                output = tf.transpose(output, perm=[1,0,2])\n            model = keras.Model(inputs=input, outputs=output)\n\n            expected = model.predict(x)\n            onnx_model = keras2onnx.convert_keras(model, model.name)\n            assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_bidirectional_with_initial_states(runner, rnn_class):\n    input1 = Input(shape=(None, 5))\n    states = Bidirectional(rnn_class(2, return_state=True))(input1)\n    model = Model(input1, states)\n\n    x = np.random.uniform(0.1, 1.0, size=(4, 3, 5)).astype(np.float32)\n    inputs = [x]\n\n    expected = model.predict(inputs)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, inputs, expected)\n\n    input2 = Input(shape=(None, 5))\n    states = Bidirectional(rnn_class(2, return_state=True))(input1)[1:]\n    out = Bidirectional(rnn_class(2, return_sequences=True))(input2, initial_state=states)\n    model = Model([input1, input2], out)\n    inputs = [x, x]\n\n    expected = model.predict(inputs)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, inputs, expected, atol=1e-5)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 5,\n                    reason=""None seq_length Bidirectional LSTM is not supported before opset 5."")\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_bidirectional_seqlen_none(runner, rnn_class):\n    model = Sequential()\n    model.add(Embedding(39, 128))\n    model.add(Bidirectional(rnn_class(256, input_shape=(None, 32), return_sequences=True)))\n    model.add(Dense(44))\n\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    for batch in [1, 4]:\n        x = np.random.rand(batch, 50).astype(np.float32)\n        expected = model.predict(x)\n        assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\n@pytest.mark.skipif(is_tf2, reason=\'TODO\')\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_rnn_state_passing(runner, rnn_class):\n    input1 = Input(shape=(None, 5))\n    input2 = Input(shape=(None, 5))\n\n    states = rnn_class(2, return_state=True)(input1)[1:]\n    out = rnn_class(2, return_sequences=True)(input2, initial_state=states)\n    model = Model([input1, input2], out)\n\n    x = np.random.uniform(0.1, 1.0, size=(4, 3, 5)).astype(np.float32)\n    inputs = [x, x]\n\n    expected = model.predict(inputs)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, inputs, expected, atol=1e-5)\n\n\ndef test_seq_dynamic_batch_size(runner):\n    K.clear_session()\n    data_dim = 4  # input_size\n    timesteps = 3  # seq_length\n\n    # expected input data shape: (batch_size, timesteps, data_dim)\n    test_input = np.random.random_sample((100, timesteps, data_dim))\n    test_output = np.random.random_sample((100, 128))\n\n    # Number of layer and number of neurons in each layer\n    num_neur = [128, 256, 128]\n    epochs = 200\n    batch_size = 50\n    nodeFuncList = [SimpleRNN, GRU, LSTM]\n\n    for nodeFunc in nodeFuncList:\n        model = Sequential()\n        for i in range(len(num_neur)):  # multi-layer\n            if len(num_neur) == 1:\n                model.add(nodeFunc(num_neur[i], input_shape=(timesteps, data_dim), unroll=True))\n            else:\n                if i < len(num_neur) - 1:\n                    model.add(\n                        nodeFunc(num_neur[i], input_shape=(timesteps, data_dim), return_sequences=True,\n                                 unroll=True))\n                else:\n                    model.add(nodeFunc(num_neur[i], input_shape=(timesteps, data_dim), unroll=True))\n\n        # Compile the neural network\n        model.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n        model.fit(test_input, test_output, epochs=epochs, batch_size=batch_size, verbose=0)\n        test_input = np.random.random_sample((5, timesteps, data_dim)).astype(np.float32)\n        test_output = model.predict(test_input)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        assert runner(onnx_model.graph.name, onnx_model, test_input, test_output)\n\n\ndef test_separable_convolution(runner):\n    N, C, H, W = 2, 3, 5, 5\n    x = np.random.rand(N, H, W, C).astype(np.float32, copy=False)\n    model = Sequential()\n    model.add(\n        SeparableConv2D(filters=10, kernel_size=(1, 2), strides=(1, 1), padding=\'valid\', input_shape=(H, W, C),\n                        data_format=\'channels_last\', depth_multiplier=4))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2), data_format=\'channels_last\'))\n    model.compile(optimizer=\'sgd\', loss=\'mse\')\n    onnx_model = keras2onnx.convert_keras(model, \'test\')\n    expected = model.predict(x)\n    assert runner(\'separable_convolution_1\', onnx_model, x, expected)\n\n    x = np.random.rand(N, H, C).astype(np.float32, copy=False)\n    model = Sequential()\n    model.add(SeparableConv1D(filters=10, kernel_size=2, strides=1, padding=\'valid\', input_shape=(H, C),\n                              data_format=\'channels_last\'))\n    model.compile(optimizer=\'sgd\', loss=\'mse\')\n    onnx_model = keras2onnx.convert_keras(model, \'test\')\n    expected = model.predict(x)\n    assert runner(\'separable_convolution_2\', onnx_model, x, expected)\n\n\ndef test_shared_embed(runner):\n    max_cont_length = 5\n    max_ques_length = 7\n    word_dict_len = 10\n    word_dim = 6\n    h_word_mat = \'aa\'\n    # Input Embedding Layer\n    contw_input_ = Input((max_cont_length,))  # [bs, c_len]\n    quesw_input_ = Input((max_ques_length,))  # [bs, q_len]\n\n    # embedding word\n    WordEmbedding = Embedding(word_dict_len, word_dim, trainable=False,\n                              name=""word_embedding_"" + h_word_mat)\n    xw_cont = Dropout(0.)(WordEmbedding(contw_input_))  # [bs, c_len, word_dim]\n    xw_ques = Dropout(0.)(WordEmbedding(quesw_input_))  # [bs, c_len, word_dim]\n\n    keras_model = keras.models.Model(inputs=[contw_input_, quesw_input_],\n                                     outputs=[xw_cont, xw_ques])\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n    batch_size = 3\n    x = np.random.rand(batch_size, max_cont_length).astype(np.float32)\n    y = np.random.rand(batch_size, max_ques_length).astype(np.float32)\n    expected = keras_model.predict([x, y])\n    assert runner(onnx_model.graph.name, onnx_model, [x, y], expected)\n\n\ndef test_recursive_model(runner):\n    keras.backend.set_learning_phase(0)\n\n    N, C, D = 2, 3, 3\n    x = np.random.rand(N, C).astype(np.float32, copy=False)\n\n    sub_input1 = Input(shape=(C,))\n    sub_mapped1 = Dense(D)(sub_input1)\n    sub_model1 = keras.Model(inputs=sub_input1, outputs=sub_mapped1)\n\n    sub_input2 = Input(shape=(C,))\n    sub_mapped2 = Dense(D)(sub_input2)\n    sub_model2 = keras.Model(inputs=sub_input2, outputs=sub_mapped2)\n\n    input1 = Input(shape=(D,))\n    input2 = Input(shape=(D,))\n    mapped1_2 = sub_model1(input1)\n    mapped2_2 = sub_model2(input2)\n    sub_sum = Add()([mapped1_2, mapped2_2])\n    keras_model = keras.Model(inputs=[input1, input2], outputs=sub_sum)\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n\n    x = [x, 2 * x]\n    expected = keras_model.predict(x)\n    assert runner(\'recursive\', onnx_model, x, expected)\n\n\ndef test_recursive_and_shared_model(runner):\n    keras.backend.set_learning_phase(0)\n    N, C, D = 2, 3, 3\n    x = np.random.rand(N, C).astype(np.float32, copy=False)\n\n    sub_input1 = Input(shape=(C,))\n    sub_mapped1 = Dense(D)(sub_input1)\n    sub_output1 = Activation(\'sigmoid\')(sub_mapped1)\n    sub_model1 = keras.Model(inputs=sub_input1, outputs=sub_output1)\n\n    sub_input2 = Input(shape=(C,))\n    sub_mapped2 = sub_model1(sub_input2)\n    sub_output2 = Activation(\'tanh\')(sub_mapped2)\n    sub_model2 = keras.Model(inputs=sub_input2, outputs=sub_output2)\n\n    input1 = Input(shape=(D,))\n    input2 = Input(shape=(D,))\n    mapped1_1 = Activation(\'tanh\')(input1)\n    mapped2_1 = Activation(\'sigmoid\')(input2)\n    mapped1_2 = sub_model1(mapped1_1)\n    mapped1_3 = sub_model1(mapped1_2)\n    mapped2_2 = sub_model2(mapped2_1)\n    sub_sum = Add()([mapped1_3, mapped2_2])\n    keras_model = keras.Model(inputs=[input1, input2], outputs=sub_sum)\n    keras_model.compile(\'sgd\', loss=\'mse\')\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n\n    x = [x, 2 * x]\n    expected = keras_model.predict(x)\n    assert runner(\'recursive_and_shared\', onnx_model, x, expected)\n\n\n@pytest.mark.skipif(is_keras_older_than(""2.2.4""),\n                    reason=""Low keras version is not supported."")\ndef test_shared_model_2(runner):\n    K.set_learning_phase(0)\n\n    def _conv_layer(input, filters, kernel_size, relu_flag=False, strides=1, dilation_rate=1):\n        padding = \'same\' if strides == 1 else \'valid\'\n        if strides > 1:\n            input = ZeroPadding2D(((0, 1), (0, 1)), data_format=K.image_data_format())(input)\n        x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n                   padding=padding, use_bias=False, dilation_rate=dilation_rate)(input)\n        ch_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n        x = BatchNormalization(axis=ch_axis)(x)\n        if relu_flag:\n            return ReLU()(x)\n        else:\n            return x\n\n    def _model(relu_flag=False):\n        input = Input(shape=(3, 320, 320), name=\'input_1\')\n        x = _conv_layer(input, 16, 3, relu_flag)\n        return Model(inputs=input, outputs=x, name=\'backbone\')\n\n    relu_flags = [False] if is_tf2 or is_tf_keras else [True, False]\n    for relu_flag_ in relu_flags:\n        input = Input(shape=(3, 320, 320), name=\'input\')\n        backbone = _model(relu_flag_)\n        x = backbone(input)\n        x = _conv_layer(x, 16, 3)\n        model = Model(inputs=[input], outputs=[x])\n\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.rand(2, 3, 320, 320).astype(np.float32)\n        expected = model.predict(x)\n        assert runner(onnx_model.graph.name, onnx_model, x, expected, atol=1e-5)\n\n\n@pytest.mark.skipif(is_keras_older_than(""2.2.4""),\n                    reason=""ReLU support requires keras 2.2.4 or later."")\ndef test_shared_model_3(runner):\n    def _bottleneck(x, filters, activation, strides, block_id):\n        padding = \'same\' if strides == 1 else \'valid\'\n        ch_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n        if strides > 1:\n            x = ZeroPadding2D(((0, 1), (0, 1)), data_format=K.image_data_format())(x)\n\n        x = Conv2D(filters // 2, (1, 1), padding=\'same\', name=\'bottleneck_\' + str(block_id) + \'_conv_0\',\n                   use_bias=False, data_format=K.image_data_format())(x)\n\n        x = BatchNormalization(axis=ch_axis, name=\'bottleneck_\' + str(block_id) + \'_bnorm_0\')(x)\n\n        if activation == \'relu\':\n            x = ReLU(name=\'bottleneck_\' + str(block_id) + \'_relu_0\')(x)\n        elif activation == \'leaky\':\n            x = LeakyReLU(name=\'bottleneck_\' + str(block_id) + \'_leaky_0\')(x)\n        else:\n            assert False\n\n        x = Conv2D(filters // 2, (3, 3), padding=padding, name=\'bottleneck_\' + str(block_id) + \'_conv_1\',\n                   strides=strides, use_bias=False, data_format=K.image_data_format())(x)\n        x = BatchNormalization(axis=ch_axis, name=\'bottleneck_\' + str(block_id) + \'_bnorm_1\')(x)\n        if activation == \'relu\':\n            x = ReLU(name=\'bottleneck_\' + str(block_id) + \'_relu_1\')(x)\n        elif activation == \'leaky\':\n            x = LeakyReLU(name=\'bottleneck_\' + str(block_id) + \'_leaky_1\')(x)\n        else:\n            assert False\n\n        x = Conv2D(filters, (1, 1), padding=\'same\', name=\'bottleneck_\' + str(block_id) + \'_conv_2\',\n                   use_bias=False, data_format=K.image_data_format())(x)\n        x = BatchNormalization(axis=ch_axis, name=\'bottleneck_\' + str(block_id) + \'_bnorm_2\')(x)\n        if activation == \'relu\':\n            x = ReLU(name=\'bottleneck_\' + str(block_id) + \'_relu_2\')(x)\n        elif activation == \'leaky\':\n            x = LeakyReLU(name=\'bottleneck_\' + str(block_id) + \'_leaky_2\')(x)\n        else:\n            assert False\n\n        return x\n\n    def convnet_7(input_shape, activation):\n        input = Input(shape=input_shape, name=\'input_1\')\n        x = _bottleneck(input, filters=16, strides=1, activation=activation, block_id=1)\n        x = _bottleneck(x, filters=32, strides=2, activation=activation, block_id=2)\n        return Model(inputs=input, outputs=x, name=\'convnet_7\')\n\n    activation_list = [\'leaky\'] if is_tf2 or is_tf_keras else [\'relu\', \'leaky\']\n    for activation in activation_list:\n        model = convnet_7(input_shape=(3, 96, 128), activation=activation)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.rand(1, 3, 96, 128).astype(np.float32)\n        expected = model.predict(x)\n        assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_masking(runner, rnn_class):\n    timesteps, features = (3, 5)\n    model = Sequential([\n        keras.layers.Masking(mask_value=0., input_shape=(timesteps, features)),\n        rnn_class(8, return_state=False, return_sequences=False)\n    ])\n\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    x = np.random.uniform(100, 999, size=(2, 3, 5)).astype(np.float32)\n    expected = model.predict(x)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_masking_bias(runner, rnn_class):\n    timesteps, features = (3, 5)\n    model = Sequential([\n        keras.layers.Masking(mask_value=0., input_shape=(timesteps, features)),\n        rnn_class(8, return_state=False, return_sequences=False, use_bias=True, name=\'rnn\')\n    ])\n\n    x = np.random.uniform(100, 999, size=(2, 3, 5)).astype(np.float32)\n    # Fill one of the entries with all zeros except the first timestep\n    x[1, 1:, :] = 0\n\n    # Test with the default bias\n    expected = model.predict(x)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n    # Set bias values to random floats\n    rnn_layer = model.get_layer(\'rnn\')\n    weights = rnn_layer.get_weights()\n    weights[2] = np.random.uniform(size=weights[2].shape)\n    rnn_layer.set_weights(weights)\n\n    # Test with random bias\n    expected = model.predict(x)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\n@pytest.mark.skipif(get_maximum_opset_supported() < 9, reason=\'bidirectional is not supported for opset < 9\')\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_masking_bias_bidirectional(runner, rnn_class):\n    timesteps, features = (3, 5)\n    model = Sequential([\n        keras.layers.Masking(mask_value=0., input_shape=(timesteps, features)),\n        Bidirectional(rnn_class(8, return_state=False, return_sequences=False, use_bias=True), name=\'bi\')\n    ])\n\n    x = np.random.uniform(100, 999, size=(2, 3, 5)).astype(np.float32)\n    # Fill one of the entries with all zeros except the first timestep\n    x[1, 1:, :] = 0\n\n    # Test with the default bias\n    expected = model.predict(x)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n    # Set bias values to random floats\n    rnn_layer = model.get_layer(\'bi\')\n    weights = rnn_layer.get_weights()\n    weights[2] = np.random.uniform(size=weights[2].shape)\n    weights[5] = weights[2]\n    rnn_layer.set_weights(weights)\n\n    # Test with random bias\n    expected = model.predict(x)\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\n@pytest.mark.parametrize(""rnn_class"", RNN_CLASSES)\ndef test_masking_value(runner, rnn_class):\n    if rnn_class is SimpleRNN:\n        pytest.skip(\'SimpleRNN intermittently fails this test\')\n\n    timesteps, features = (3, 5)\n    mask_value = 5.\n    model = Sequential([\n        keras.layers.Masking(mask_value=mask_value, input_shape=(timesteps, features)),\n        rnn_class(8, return_state=False, return_sequences=False)\n    ])\n\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    x = np.random.uniform(100, 999, size=(2, 3, 5)).astype(np.float32)\n    x[1, :, :] = mask_value\n    expected = model.predict(x)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\ndef test_masking_custom(runner):\n    class MyPoolingMask(keras.layers.Layer):\n        def __init__(self, **kwargs):\n            self.supports_masking = True\n            super(MyPoolingMask, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            super(MyPoolingMask, self).build(input_shape)\n\n        def compute_mask(self, inputs, input_mask=None):\n            return None\n\n        def call(self, inputs, mask=None, **kwargs):\n            if mask is not None:\n                return K.sum(inputs, axis=-2) / (\n                        K.sum(K.cast(mask, K.dtype(inputs)), axis=-1, keepdims=True) + K.epsilon())\n            else:\n                output = K.mean(inputs, axis=-2)\n                return output\n\n        def compute_output_shape(self, input_shape):\n            return input_shape[:-2] + input_shape[-1:]\n\n    timesteps, features = (3, 5)\n    model = Sequential([\n        keras.layers.Masking(mask_value=0., input_shape=(timesteps, features)),\n        MyPoolingMask()\n    ])\n\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    x = np.random.uniform(100, 999, size=(2, 3, 5)).astype(np.float32)\n    expected = model.predict(x)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\ndef test_timedistributed(runner):\n    keras_model = keras.Sequential()\n    keras_model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n    # keras_model.output_shape == (None, 10, 8)\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n    x = np.random.rand(32, 10, 16).astype(np.float32)\n    expected = keras_model.predict(x)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n    keras_model = keras.Sequential()\n    N, D, W, H, C = 5, 10, 15, 15, 3\n    keras_model.add(TimeDistributed(Conv2D(64, (3, 3)),\n                                    input_shape=(D, W, H, C)))\n    onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n    x = np.random.rand(N, D, W, H, C).astype(np.float32)\n    expected = keras_model.predict(x)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n\n\ndef test_channel_first_input(runner):\n    N, W, H, C = 2, 5, 6, 3\n    inp1 = Input(batch_shape=(N, W, H, C), name=\'input1\')\n    inp2 = Input(batch_shape=(N, W, H, C), name=\'input2\')\n    output = Add()([inp1, inp2])\n    model = keras.models.Model(inputs=[inp1, inp2], outputs=output)\n    onnx_model = keras2onnx.convert_keras(model, model.name, channel_first_inputs=[\'input1\'])\n    assert onnx_model is not None\n\n    data1 = np.random.rand(N, W, H, C).astype(np.float32).reshape((N, W, H, C))\n    data2 = np.random.rand(N, W, H, C).astype(np.float32).reshape((N, W, H, C))\n    data_transpose = np.transpose(data1, (0, 3, 1, 2))\n    assert data_transpose.shape == (N, C, W, H)\n\n    expected = model.predict([data1, data2])\n    assert runner(\'channel_first_input\', onnx_model, [data_transpose, data2], expected)\n\n\ndef test_channel_last(runner):\n    N, C, H, W = 2, 3, 5, 5\n    x = np.random.rand(N, H, W, C).astype(np.float32, copy=False)\n\n    model = Sequential()\n    model.add(Conv2D(2, kernel_size=(1, 2), strides=(1, 1), padding=\'valid\', input_shape=(H, W, C),\n                     data_format=\'channels_last\'))\n    model.add(MaxPooling2D((2, 2), strides=(2, 2), data_format=\'channels_last\'))\n\n    model.compile(optimizer=\'sgd\', loss=\'mse\')\n    onnx_model = keras2onnx.convert_keras(model, channel_first_inputs=[model.input_names[0]])\n\n    expected = model.predict(x)\n    assert expected is not None\n    assert onnx_model is not None\n\n    x = np.transpose(x.astype(np.float32), [0, 3, 1, 2])\n    assert runner(\'channel_last_input\', onnx_model, x, expected)\n\n\ndef test_sub_model(runner):\n    class IdentityLayer(Layer):\n        def __init__(self, **kwargs):\n            super(IdentityLayer, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            super(IdentityLayer, self).build(input_shape)\n\n        def call(self, inputs, training=None):\n            return inputs\n\n        def compute_output_shape(self, input_shape):\n            return input_shape\n\n    input_shape = [700, 420, 1]\n    num_classes = 10\n\n    image_input = Input(shape=input_shape, name=\'image_input\')\n\n    model = Sequential()  # 28, 28, 1\n    model.add(Conv2D(32, kernel_size=(3, 3), activation=\'relu\',\n                     input_shape=input_shape, padding=\'valid\'))  # 28, 28, 1\n    model.add(Conv2D(64, (3, 3), activation=\'relu\', padding=\'valid\'))  # 28, 28, 1\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=""valid""))  # 14, 14, 1\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, kernel_size=(12, 12), strides=(14, 14), padding=""valid"", activation=\'relu\'))\n    model.add(Dropout(0.5))\n\n    features = model(image_input)\n\n    outputs = []\n    for _ in range(3):\n        output1 = Dense(num_classes, activation=""softmax"")(\n            Dense(64, activation=""relu"")(Dense(128, activation=""relu"")(features)))\n        output2 = Dense(1, activation=""sigmoid"")(\n            Dense(64, activation=""relu"")(Dense(128, activation=""relu"")(features)))\n        output3 = Dense(2, activation=""tanh"")(\n            Dense(64, activation=""relu"")(Dense(128, activation=""relu"")(features)))\n        output4 = Dense(2, activation=""tanh"")(\n            Dense(64, activation=""relu"")(Dense(128, activation=""relu"")(features)))\n        outputs += [output1, output2, output3, output4]\n\n    output = Concatenate(name=""output"")(outputs)\n    output = IdentityLayer()(output)\n    model1 = Model(image_input, output)\n    onnx_model = keras2onnx.convert_keras(model1, model1.name)\n    x = np.random.rand(2, 700, 420, 1).astype(np.float32)\n    expected = model1.predict(x)\n    assert runner(onnx_model.graph.name, onnx_model, x, expected)\n'"
tests/test_misc.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport onnx\nimport pytest\nimport numpy as np\n\nimport keras2onnx\nfrom keras2onnx import common as _cmn\nfrom keras2onnx.proto import keras\nfrom onnxconverter_common.onnx_ex import get_maximum_opset_supported\n\nfrom distutils.version import StrictVersion\n\n""""""\nTests for ONNX Operator Builder\n""""""\n\n\ndef test_apply():\n    oopb = _cmn.onnx_ops.OnnxOperatorBuilder(_cmn.OnnxObjectContainer(get_maximum_opset_supported()),\n                                             _cmn.InterimContext(\'_curr\'))\n    value = oopb.apply_add((np.array([[1.0], [0.5]], dtype=\'float32\'),\n                            (\'_i1\', oopb.float, np.array([2.0], dtype=\'float32\'))), \'add\')\n    assert value[0].startswith(\'add\')\n\n\n@pytest.mark.skipif(StrictVersion(onnx.__version__) < StrictVersion(""1.2""),\n                    reason=""Not supported in ONNX version less than 1.2, since this test requires opset 7."")\ndef test_model_creation():\n    N, C, H, W = 2, 3, 5, 5\n    input1 = keras.layers.Input(shape=(H, W, C))\n    x1 = keras.layers.Dense(8, activation=\'relu\')(input1)\n    input2 = keras.layers.Input(shape=(H, W, C))\n    x2 = keras.layers.Dense(8, activation=\'relu\')(input2)\n    maximum_layer = keras.layers.Maximum()([x1, x2])\n\n    out = keras.layers.Dense(8)(maximum_layer)\n    model = keras.models.Model(inputs=[input1, input2], outputs=out)\n\n    trial1 = np.random.rand(N, H, W, C).astype(np.float32, copy=False)\n    trial2 = np.random.rand(N, H, W, C).astype(np.float32, copy=False)\n\n    predicted = model.predict([trial1, trial2])\n    assert predicted is not None\n\n    converted_model_7 = keras2onnx.convert_keras(model, target_opset=7)\n    converted_model_5 = keras2onnx.convert_keras(model, target_opset=5)\n\n    assert converted_model_7 is not None\n    assert converted_model_5 is not None\n\n    assert converted_model_7.opset_import[0].version > converted_model_5.opset_import[0].version\n'"
tests/test_tf2_keras.py,37,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport pytest\nimport keras2onnx\nimport numpy as np\nimport tensorflow as tf\nfrom keras2onnx.proto import is_tensorflow_older_than\n\nif (not keras2onnx.proto.is_tf_keras) or (not keras2onnx.proto.tfcompat.is_tf2):\n    pytest.skip(""Tensorflow 2.0 only tests."", allow_module_level=True)\n\n\nclass LeNet(tf.keras.Model):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv2d_1 = tf.keras.layers.Conv2D(filters=6,\n                                               kernel_size=(3, 3), activation=\'relu\',\n                                               input_shape=(32, 32, 1))\n        self.average_pool = tf.keras.layers.AveragePooling2D()\n        self.conv2d_2 = tf.keras.layers.Conv2D(filters=16,\n                                               kernel_size=(3, 3), activation=\'relu\')\n        self.flatten = tf.keras.layers.Flatten()\n        self.fc_1 = tf.keras.layers.Dense(120, activation=\'relu\')\n        self.fc_2 = tf.keras.layers.Dense(84, activation=\'relu\')\n        self.out = tf.keras.layers.Dense(10, activation=\'softmax\')\n\n    def call(self, inputs, **kwargs):\n        x = self.conv2d_1(inputs)\n        x = self.average_pool(x)\n        x = self.conv2d_2(x)\n        x = self.average_pool(x)\n        x = self.flatten(x)\n        x = self.fc_2(self.fc_1(x))\n        return self.out(x)\n\n\nclass MLP(tf.keras.Model):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.flatten = tf.keras.layers.Flatten()\n        self.dense1 = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n        self.dense2 = tf.keras.layers.Dense(units=10)\n\n    def call(self, inputs, **kwargs):\n        x = self.flatten(inputs)\n        x = self.dense1(x)\n        output = self.dense2(x)\n        return output\n\n\nclass SimpleWrapperModel(tf.keras.Model):\n    def __init__(self, func):\n        super(SimpleWrapperModel, self).__init__()\n        self.func = func\n\n    def call(self, inputs, **kwargs):\n        return self.func(inputs)\n\n\ndef test_lenet(runner):\n    tf.keras.backend.clear_session()\n    lenet = LeNet()\n    data = np.random.rand(2 * 416 * 416 * 3).astype(np.float32).reshape(2, 416, 416, 3)\n    expected = lenet(data)\n    lenet._set_inputs(data)\n    oxml = keras2onnx.convert_keras(lenet)\n    assert runner(\'lenet\', oxml, data, expected)\n\n\ndef test_mlf(runner):\n    tf.keras.backend.clear_session()\n    mlf = MLP()\n    np_input = tf.random.normal((2, 20))\n    expected = mlf.predict(np_input)\n    oxml = keras2onnx.convert_keras(mlf)\n    assert runner(\'lenet\', oxml, np_input.numpy(), expected)\n\n\ndef test_tf_ops(runner):\n    tf.keras.backend.clear_session()\n\n    def op_func(arg_inputs):\n        x = tf.math.squared_difference(arg_inputs[0], arg_inputs[1])\n        x = tf.matmul(x, x, adjoint_b=True)\n        r = tf.rank(x)\n        x = x - tf.cast(tf.expand_dims(r, axis=0), tf.float32)\n        return x\n\n    dm = SimpleWrapperModel(op_func)\n    inputs = [tf.random.normal((3, 2, 20)), tf.random.normal((3, 2, 20))]\n    expected = dm.predict(inputs)\n    oxml = keras2onnx.convert_keras(dm)\n    assert runner(\'op_model\', oxml, [i_.numpy() for i_ in inputs], expected)\n\n\nlayers = tf.keras.layers\n\n\nclass Sampling(layers.Layer):\n    """"""Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.""""""\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        # epsilon = tf.fill(dims=(batch, dim), value=.9)\n        # epsilon = tf.compat.v1.random_normal(shape=(batch, dim), seed=1234)\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), seed=12340)\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n\nclass Encoder(layers.Layer):\n    """"""Maps MNIST digits to a triplet (z_mean, z_log_var, z).""""""\n\n    def __init__(self,\n                 latent_dim=32,\n                 intermediate_dim=64,\n                 name=\'encoder\',\n                 **kwargs):\n        super(Encoder, self).__init__(name=name, **kwargs)\n        self.dense_proj = layers.Dense(intermediate_dim, activation=\'relu\')\n        self.dense_mean = layers.Dense(latent_dim)\n        self.dense_log_var = layers.Dense(latent_dim)\n        self.sampling = Sampling()\n\n    def call(self, inputs):\n        x = self.dense_proj(inputs)\n        z_mean = self.dense_mean(x)\n        z_log_var = self.dense_log_var(x)\n        z = self.sampling((z_mean, z_log_var))\n        return z_mean, z_log_var, z\n\n\nclass Decoder(layers.Layer):\n    """"""Converts z, the encoded digit vector, back into a readable digit.""""""\n\n    def __init__(self,\n                 original_dim,\n                 intermediate_dim=64,\n                 name=\'decoder\',\n                 **kwargs):\n        super(Decoder, self).__init__(name=name, **kwargs)\n        self.dense_proj = layers.Dense(intermediate_dim, activation=\'relu\')\n        self.dense_output = layers.Dense(original_dim, activation=\'sigmoid\')\n\n    def call(self, inputs):\n        x = self.dense_proj(inputs)\n        return self.dense_output(x)\n\n\nclass VariationalAutoEncoder(tf.keras.Model):\n    """"""Combines the encoder and decoder into an end-to-end model for training.""""""\n\n    def __init__(self,\n                 original_dim,\n                 intermediate_dim=64,\n                 latent_dim=32,\n                 name=\'autoencoder\',\n                 **kwargs):\n        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n        self.original_dim = original_dim\n        self.encoder = Encoder(latent_dim=latent_dim,\n                               intermediate_dim=intermediate_dim)\n        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n\n    def call(self, inputs):\n        z_mean, z_log_var, z = self.encoder(inputs)\n        reconstructed = self.decoder(z)\n        # Add KL divergence regularization loss.\n        kl_loss = - 0.5 * tf.reduce_mean(\n            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n        self.add_loss(kl_loss)\n        return reconstructed\n\n\ndef test_auto_encoder(runner):\n    tf.keras.backend.clear_session()\n    original_dim = 20\n    vae = VariationalAutoEncoder(original_dim, 64, 32)\n    x = tf.random.normal((7, original_dim))\n    expected = vae.predict(x)\n    oxml = keras2onnx.convert_keras(vae)\n    # assert runner(\'variational_auto_encoder\', oxml, [x.numpy()], expected)\n    # The random generator is not same between different engiens.\n    import onnx\n    onnx.checker.check_model(oxml)\n\n\ndef test_tf_where(runner):\n    def _tf_where(input_0):\n        a = tf.where(True, input_0, [0, 1, 2, 5, 7])\n        b = tf.where([True], tf.expand_dims(input_0, axis=0), tf.expand_dims([0, 1, 2, 5, 7], axis=0))\n        c = tf.logical_or(tf.cast(a, tf.bool), tf.cast(b, tf.bool))\n        return c\n\n    swm = SimpleWrapperModel(_tf_where)\n    const_in = [np.array([2, 4, 6, 8, 10]).astype(np.int32)]\n    expected = swm(const_in)\n    swm._set_inputs(const_in)\n    oxml = keras2onnx.convert_keras(swm)\n    assert runner(\'where_test\', oxml, const_in, expected)\n'"
tests/test_utils.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport onnx\nimport numpy as np\nimport keras2onnx\nfrom keras2onnx.proto import keras, is_keras_older_than\nfrom keras2onnx.common.onnx_ops import apply_identity, OnnxOperatorBuilder\n\nworking_path = os.path.abspath(os.path.dirname(__file__))\ntmp_path = os.path.join(working_path, \'temp\')\n\n\ndef convert_tf_crop_and_resize(scope, operator, container):\n    if operator.target_opset < 11:\n        raise ValueError(""CropAndResize op is not supported for opset < 11"")\n    oopb = OnnxOperatorBuilder(container, scope)\n    node = operator.raw_operator\n    mode_value = node.get_attr(\'method\')\n    transpose_node = oopb.apply_transpose(operator.inputs[0].full_name,\n                                          name=operator.full_name + \'_transpose_1\',\n                                          perm=[0, 3, 1, 2])\n    cropandresize = oopb.add_node(\'CropAndResize\',\n                                  transpose_node + operator.input_full_names[1:],\n                                  operator.full_name + \'_crop_and_resize\',\n                                  op_domain=\'com.microsoft\',\n                                  op_version=1,\n                                  mode=mode_value)\n    oopb.apply_op_with_output(""apply_transpose"",\n                              cropandresize,\n                              operator.output_full_names,\n                              name=operator.full_name + \'_transpose_final\',\n                              perm=[0, 2, 3, 1])\n\n\n# convert keras_contrib.layers.InstanceNormalization\ndef convert_InstanceNormalizationLayer(scope, operator, container):\n    from keras2onnx.common.onnx_ops import OnnxOperatorBuilder\n    op = operator.raw_operator\n    params = op.get_weights()\n    assert len(op.input_shape) == 4\n    beta = params[0].reshape(1, 1, 1, 1).astype(\'float32\')\n    gamma = params[1].reshape(1, 1, 1, 1).astype(\'float32\')\n    oopb = OnnxOperatorBuilder(container, scope)\n\n    reducemean_1 = oopb.add_node(\'ReduceMean\',\n                                 [operator.inputs[0].full_name],\n                                 operator.inputs[0].full_name + \'_reduce_mean_1\',\n                                 axes=[1, 2, 3], keepdims=1)\n\n    sub_1 = oopb.add_node(\'Sub\',\n                          [operator.inputs[0].full_name, reducemean_1],\n                          operator.inputs[0].full_name + \'_sub_1\')\n\n    mul = oopb.add_node(\'Mul\',\n                        [sub_1, sub_1],\n                        operator.inputs[0].full_name + \'_mul\')\n\n    reducemean_2 = oopb.add_node(\'ReduceMean\',\n                                 [mul],\n                                 operator.inputs[0].full_name + \'_reduce_mean_2\',\n                                 axes=[1, 2, 3], keepdims=1)\n\n    sqrt = oopb.add_node(\'Sqrt\',\n                         [reducemean_2],\n                         operator.inputs[0].full_name + \'_sqrt\')\n\n    add = oopb.add_node(\'Add\',\n                        [sqrt,\n                         (\'_start\', oopb.float, np.array([op.epsilon], dtype=\'float32\'))],\n                        operator.inputs[0].full_name + \'_add\')\n\n    div = oopb.add_node(\'Div\',\n                        [sub_1, add],\n                        operator.inputs[0].full_name + \'_div\')\n\n    mul_scale = oopb.add_node(\'Mul\',\n                              [div,\n                               (\'_start\', oopb.float, beta)],\n                              operator.inputs[0].full_name + \'_mul_scale\')\n\n    add_bias = oopb.add_node(\'Add\',\n                             [mul_scale,\n                              (\'_start\', oopb.float, gamma)],\n                             operator.inputs[0].full_name + \'_add_bias\')\n\n    apply_identity(scope, add_bias, operator.outputs[0].full_name, container)\n\n\ndef print_mismatches(case_name, list_idx, expected_list, actual_list, rtol=1.e-3, atol=1.e-6):\n    diff_list = abs(expected_list - actual_list)\n    count_total = len(expected_list)\n    count_error = 0\n    count_current = 0\n\n    for e_, a_, d_ in zip(expected_list, actual_list, diff_list):\n        if d_ > atol + rtol * abs(a_):\n            if count_error < 10:  # print the first 10 mismatches\n                print(\n                    ""case = "" + case_name + "", result mismatch for expected = "" + str(e_) +\n                    "", actual = "" + str(a_) + "" at location "" + str(count_current), file=sys.stderr)\n            count_error = count_error + 1\n        count_current += 1\n\n    print(""case = "" + case_name + "", "" +\n          str(count_error) + "" mismatches out of "" + str(count_total) + "" for list "" + str(list_idx),\n          file=sys.stderr)\n\n\ndef run_onnx_runtime(case_name, onnx_model, data, expected, model_files, rtol=1.e-3, atol=1.e-6):\n    if not os.path.exists(tmp_path):\n        os.mkdir(tmp_path)\n    temp_model_file = os.path.join(tmp_path, \'temp_\' + case_name + \'.onnx\')\n    onnx.save_model(onnx_model, temp_model_file)\n    try:\n        import onnxruntime\n        sess = onnxruntime.InferenceSession(temp_model_file)\n    except ImportError:\n        keras2onnx.common.k2o_logger().warning(""Cannot import ONNXRuntime!"")\n        return True\n\n    if isinstance(data, dict):\n        feed_input = data\n    else:\n        data = data if isinstance(data, list) else [data]\n        input_names = sess.get_inputs()\n        # to avoid too complicated test code, we restrict the input name in Keras test cases must be\n        # in alphabetical order. It\'s always true unless there is any trick preventing that.\n        feed = zip(sorted(i_.name for i_ in input_names), data)\n        feed_input = dict(feed)\n    actual = sess.run(None, feed_input)\n\n    if expected is None:\n        return\n\n    if isinstance(expected, tuple):\n        expected = list(expected)\n    elif not isinstance(expected, list):\n        expected = [expected]\n\n    res = all(np.allclose(expected[n_], actual[n_], rtol=rtol, atol=atol) for n_ in range(len(expected)))\n\n    if res and temp_model_file not in model_files:  # still keep the failed case files for the diagnosis.\n        model_files.append(temp_model_file)\n\n    if not res:\n        for n_ in range(len(expected)):\n            expected_list = expected[n_].flatten()\n            actual_list = actual[n_].flatten()\n            print_mismatches(case_name, n_, expected_list, actual_list, rtol, atol)\n\n    return res\n\n\ndef run_image(model, model_files, img_path, model_name=\'onnx_conversion\', rtol=1.e-3, atol=1.e-5, color_mode=""rgb"",\n              target_size=224, tf_v2=False):\n    if tf_v2:\n        preprocess_input = keras.applications.imagenet_utils.preprocess_input\n    else:\n        preprocess_input = keras.applications.resnet50.preprocess_input\n    image = keras.preprocessing.image\n\n    try:\n        if not isinstance(target_size, tuple):\n            target_size = (target_size, target_size)\n        if is_keras_older_than(""2.2.3""):\n            # color_mode is not supported in old keras version\n            img = image.load_img(img_path, target_size=target_size)\n        else:\n            img = image.load_img(img_path, color_mode=color_mode, target_size=target_size)\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        if color_mode == ""rgb"":\n            x = preprocess_input(x)\n    except FileNotFoundError:\n        return False, \'The image data does not exist.\'\n\n    msg = \'\'\n    preds = None\n    try:\n        preds = model.predict(x)\n    except RuntimeError:\n        msg = \'keras prediction throws an exception for model \' + model.name + \', skip comparison.\'\n\n    onnx_model = keras2onnx.convert_keras(model, model.name)\n    res = run_onnx_runtime(model_name, onnx_model, x, preds, model_files, rtol=rtol, atol=atol)\n    return res, msg\n'"
applications/lpcnet/convert_lpcnet_to_onnx.py,0,"b'# -------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n# --------------------------------------------------------------------------\n\nimport lpcnet\nimport sys\n\nmodel, enc, dec = lpcnet.new_lpcnet_model(use_gpu=False)\nmodel.compile(optimizer=\'adam\', loss=\'sparse_categorical_crossentropy\', metrics=[\'sparse_categorical_accuracy\'])\nmodel_file = sys.argv[1]\nmodel.load_weights(model_file)\n\nimport keras2onnx\noxml_enc = keras2onnx.convert_keras(enc, \'lpcnet_enc\')\noxml_dec = keras2onnx.convert_keras(dec, \'lpcnet_dec\')\n\nimport onnx\nonnx.save(oxml_enc, ""lpcnet_enc.onnx"")\nonnx.save(oxml_dec, ""lpcnet_dec.onnx"")\n'"
applications/mask_rcnn/mask_rcnn.py,0,"b'import os\nimport sys\nimport numpy as np\nimport skimage\nimport onnx\nimport keras2onnx\n\nfrom mrcnn.config import Config\nfrom mrcnn.model import BatchNorm, DetectionLayer\nfrom mrcnn import model as modellib\nfrom mrcnn import visualize\n\nfrom keras2onnx import set_converter\nfrom keras2onnx.ke2onnx.batch_norm import convert_keras_batch_normalization\nfrom keras2onnx.proto import onnx_proto\nfrom keras2onnx.common.onnx_ops import apply_transpose, apply_identity\nfrom keras2onnx.common.onnx_ops import OnnxOperatorBuilder\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import convert_tf_crop_and_resize\n\n\nROOT_DIR = os.path.abspath(""./"")\n\n# Directory to save logs and trained model\nMODEL_DIR = os.path.join(ROOT_DIR, ""logs"")\n\n# Path to trained weights file\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")\n\n\nclass CocoConfig(Config):\n    """"""Configuration for training on MS COCO.\n    Derives from the base Config class and overrides values specific\n    to the COCO dataset.\n    """"""\n    # Give the configuration a recognizable name\n    NAME = ""coco""\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Uncomment to train on 8 GPUs (default is 1)\n    # GPU_COUNT = 8\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 80  # COCO has 80 classes\n\n\nclass InferenceConfig(CocoConfig):\n    # Set batch size to 1 since we\'ll be running inference on\n    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\n\nconfig = InferenceConfig()\nconfig.display()\n\nmodel = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)\n\n# Load weights trained on MS-COCO\nmodel.load_weights(COCO_MODEL_PATH, by_name=True)\n\n\ndef convert_BatchNorm(scope, operator, container):\n    convert_keras_batch_normalization(scope, operator, container)\n\n\ndef convert_apply_box_deltas_graph(scope, operator, container, oopb, box_transpose, score_identity, deltas_transpose, windows_transpose):\n    box_squeeze = scope.get_unique_variable_name(\'box_squeeze\')\n    attrs = {\'axes\': [0]}\n    container.add_node(\'Squeeze\', box_transpose, box_squeeze, op_version=operator.target_opset,\n                       **attrs)\n    # output shape: [spatial_dimension, 4]\n\n    deltas_squeeze = scope.get_unique_variable_name(\'deltas_squeeze\')\n    attrs = {\'axes\': [0]}\n    container.add_node(\'Squeeze\', deltas_transpose, deltas_squeeze, op_version=operator.target_opset,\n                       **attrs)\n    # output shape: [spatial_dimension, num_classes, 4]\n\n    score_squeeze = scope.get_unique_variable_name(\'score_squeeze\')\n    attrs = {\'axes\': [0]}\n    container.add_node(\'Squeeze\', score_identity, score_squeeze, op_version=operator.target_opset,\n                       **attrs)\n    # output shape: [spatial_dimension, num_classes]\n\n    class_ids = scope.get_unique_variable_name(\'class_ids\')\n    attrs = {\'axis\': 1}\n    container.add_node(\'ArgMax\', score_squeeze, class_ids, op_version=operator.target_opset,\n                       **attrs)\n    # output shape: [spatial_dimension, 1]\n\n    prob_shape = oopb.add_node(\'Shape\',\n                                 [score_squeeze],\n                                 operator.inputs[1].full_name + \'_prob_shape\')\n    prob_shape_0 = oopb.add_node(\'Slice\',\n                         [prob_shape,\n                          (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                          (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                          (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                          ],\n                         operator.inputs[1].full_name + \'_prob_shape_0\')\n    prob_range = oopb.add_node(\'Range\',\n                         [(\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                          prob_shape_0,\n                          # (\'_limit\', oopb.int64, np.array([1000], dtype=\'int64\')),\n                          (\'_delta\', oopb.int64, np.array([1], dtype=\'int64\'))\n                          ],\n                         operator.inputs[1].full_name + \'_prob_range\',\n                         op_domain=\'com.microsoft\',\n                         op_version=1)\n\n    attrs = {\'axes\': [1]}\n    prob_range_unsqueeze = oopb.add_node(\'Unsqueeze\',\n                         [prob_range],\n                         operator.inputs[1].full_name + \'_prob_range_unsqueeze\',\n                         **attrs)\n    # output shape: [spatial_dimension, 1]\n\n    attrs = {\'axis\': 1}\n    indices = oopb.add_node(\'Concat\',\n                         [prob_range_unsqueeze,\n                          class_ids\n                          ],\n                         operator.inputs[1].full_name + \'_indices\', **attrs)\n    # output shape: [spatial_dimension, 2]\n\n    deltas_specific = oopb.add_node(\'GatherND\',\n                         [deltas_squeeze, indices],\n                         operator.inputs[2].full_name + \'_deltas_specific\')\n    # output shape: [spatial_dimension, 4]\n\n    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2], dtype=\'float32\')\n    delta_mul_output = oopb.add_node(\'Mul\',\n                                     [deltas_specific,\n                                      (\'_mul_constant\', oopb.float, BBOX_STD_DEV)\n                                     ],\n                                     operator.inputs[2].full_name + \'_mul\')\n    # output shape: [spatial_dimension, 4]\n\n    box_0 = oopb.add_node(\'Slice\',\n                         [box_squeeze,\n                          (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                          (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                          (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                          ],\n                         operator.inputs[0].full_name + \'_sliced_0\')\n    box_1 = oopb.add_node(\'Slice\',\n                          [box_squeeze,\n                           (\'_start\', oopb.int64, np.array([1], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([2], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                           ],\n                          operator.inputs[0].full_name + \'_sliced_1\')\n    box_2 = oopb.add_node(\'Slice\',\n                          [box_squeeze,\n                           (\'_start\', oopb.int64, np.array([2], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([3], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                           ],\n                          operator.inputs[0].full_name + \'_sliced_2\')\n    box_3 = oopb.add_node(\'Slice\',\n                          [box_squeeze,\n                           (\'_start\', oopb.int64, np.array([3], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([4], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                           ],\n                          operator.inputs[0].full_name + \'_sliced_3\')\n\n    delta_0 = oopb.add_node(\'Slice\',\n                         [delta_mul_output,\n                          (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                          (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                          (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                          ],\n                         operator.inputs[3].full_name + \'_sliced_0\')\n    delta_1 = oopb.add_node(\'Slice\',\n                          [delta_mul_output,\n                           (\'_start\', oopb.int64, np.array([1], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([2], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                           ],\n                          operator.inputs[3].full_name + \'_sliced_1\')\n    delta_2 = oopb.add_node(\'Slice\',\n                          [delta_mul_output,\n                           (\'_start\', oopb.int64, np.array([2], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([3], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                           ],\n                          operator.inputs[3].full_name + \'_sliced_2\')\n    delta_3 = oopb.add_node(\'Slice\',\n                          [delta_mul_output,\n                           (\'_start\', oopb.int64, np.array([3], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([4], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                           ],\n                          operator.inputs[3].full_name + \'_sliced_3\')\n\n    height = oopb.add_node(\'Sub\',\n                          [box_2, box_0],\n                          operator.inputs[0].full_name + \'_height\')\n    width = oopb.add_node(\'Sub\',\n                          [box_3, box_1],\n                          operator.inputs[0].full_name + \'_width\')\n\n    half_height_0 = oopb.add_node(\'Mul\',\n                                  [height,\n                                   (\'_mul_constant\', oopb.float, np.array([0.5], dtype=\'float32\'))\n                                  ],\n                                  operator.inputs[0].full_name + \'_half_height_0\')\n    half_width_0 = oopb.add_node(\'Mul\',\n                                  [width,\n                                   (\'_mul_constant\', oopb.float, np.array([0.5], dtype=\'float32\'))\n                                  ],\n                                  operator.inputs[0].full_name + \'_half_width_0\')\n    center_y_0 = oopb.add_node(\'Add\',\n                               [box_0, half_height_0],\n                               operator.inputs[0].full_name + \'_center_y_0\')\n    center_x_0 = oopb.add_node(\'Add\',\n                               [box_1, half_width_0],\n                               operator.inputs[0].full_name + \'_center_x_0\')\n\n    delta_height = oopb.add_node(\'Mul\',\n                               [delta_0, height],\n                               operator.inputs[0].full_name + \'_delta_height\')\n    delta_width = oopb.add_node(\'Mul\',\n                               [delta_1, width],\n                               operator.inputs[0].full_name + \'_delta_width\')\n    center_y_1 = oopb.add_node(\'Add\',\n                               [center_y_0, delta_height],\n                               operator.inputs[0].full_name + \'_center_y_1\')\n    center_x_1 = oopb.add_node(\'Add\',\n                               [center_x_0, delta_width],\n                               operator.inputs[0].full_name + \'_center_x_1\')\n\n    delta_2_exp = oopb.add_node(\'Exp\',\n                                [delta_2],\n                                operator.inputs[0].full_name + \'_delta_2_exp\')\n    delta_3_exp = oopb.add_node(\'Exp\',\n                                [delta_3],\n                                operator.inputs[0].full_name + \'_delta_3_exp\')\n    height_exp = oopb.add_node(\'Mul\',\n                                 [height, delta_2_exp],\n                                 operator.inputs[0].full_name + \'_height_exp\')\n    width_exp = oopb.add_node(\'Mul\',\n                                [width, delta_3_exp],\n                                operator.inputs[0].full_name + \'_width_exp\')\n\n    half_height_1 = oopb.add_node(\'Mul\',\n                                  [height_exp,\n                                   (\'_mul_constant\', oopb.float, np.array([0.5], dtype=\'float32\'))\n                                  ],\n                                  operator.inputs[0].full_name + \'_half_height_1\')\n    half_width_1 = oopb.add_node(\'Mul\',\n                                  [width_exp,\n                                   (\'_mul_constant\', oopb.float, np.array([0.5], dtype=\'float32\'))\n                                  ],\n                                  operator.inputs[0].full_name + \'_half_width_1\')\n    y1 = oopb.add_node(\'Sub\',\n                          [center_y_1, half_height_1],\n                          operator.inputs[0].full_name + \'_y1\')\n    x1 = oopb.add_node(\'Sub\',\n                          [center_x_1, half_width_1],\n                          operator.inputs[0].full_name + \'_x1\')\n    y2 = oopb.add_node(\'Add\',\n                               [y1, height_exp],\n                               operator.inputs[0].full_name + \'_y2\')\n    x2 = oopb.add_node(\'Add\',\n                               [x1, width_exp],\n                               operator.inputs[0].full_name + \'_x2\')\n\n    windows_squeeze = scope.get_unique_variable_name(\'windows_squeeze\')\n    attrs = {\'axes\': [0]}\n    container.add_node(\'Squeeze\', windows_transpose, windows_squeeze, op_version=operator.target_opset,\n                       **attrs)\n    wy1 = oopb.add_node(\'Slice\',\n                         [windows_squeeze,\n                          (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                          (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                          (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                          ],\n                         operator.inputs[0].full_name + \'_windows_0\')\n    wx1 = oopb.add_node(\'Slice\',\n                          [windows_squeeze,\n                           (\'_start\', oopb.int64, np.array([1], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([2], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                           ],\n                          operator.inputs[0].full_name + \'_windows_1\')\n    wy2 = oopb.add_node(\'Slice\',\n                          [windows_squeeze,\n                           (\'_start\', oopb.int64, np.array([2], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([3], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                           ],\n                          operator.inputs[0].full_name + \'_windows_2\')\n    wx2 = oopb.add_node(\'Slice\',\n                          [windows_squeeze,\n                           (\'_start\', oopb.int64, np.array([3], dtype=\'int64\')),\n                           (\'_end\', oopb.int64, np.array([4], dtype=\'int64\')),\n                           (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                           ],\n                          operator.inputs[0].full_name + \'_windows_3\')\n    y1_min = oopb.add_node(\'Min\',\n                       [y1, wy2],\n                       operator.inputs[0].full_name + \'_y1_min\')\n    x1_min = oopb.add_node(\'Min\',\n                       [x1, wx2],\n                       operator.inputs[0].full_name + \'_x1_min\')\n    y2_min = oopb.add_node(\'Min\',\n                       [y2, wy2],\n                       operator.inputs[0].full_name + \'_y2_min\')\n    x2_min = oopb.add_node(\'Min\',\n                       [x2, wx2],\n                       operator.inputs[0].full_name + \'_x2_min\')\n    y1_max = oopb.add_node(\'Max\',\n                           [y1_min, wy1],\n                           operator.inputs[0].full_name + \'_y1_max\')\n    x1_max = oopb.add_node(\'Max\',\n                           [x1_min, wx1],\n                           operator.inputs[0].full_name + \'_x1_max\')\n    y2_max = oopb.add_node(\'Max\',\n                           [y2_min, wy1],\n                           operator.inputs[0].full_name + \'_y2_max\')\n    x2_max = oopb.add_node(\'Max\',\n                           [x2_min, wx1],\n                           operator.inputs[0].full_name + \'_x2_max\')\n    concat_result = scope.get_unique_variable_name(operator.output_full_names[0] + \'_concat_result\')\n    attrs = {\'axis\': 1}\n    container.add_node(""Concat"",\n                       [y1_max, x1_max, y2_max, x2_max],\n                       concat_result,\n                       op_version=operator.target_opset,\n                       name=operator.outputs[0].full_name + \'_concat_result\', **attrs)\n\n    concat_unsqueeze = scope.get_unique_variable_name(\'concat_unsqueeze\')\n    attrs = {\'axes\': [0]}\n    container.add_node(\'Unsqueeze\', concat_result, concat_unsqueeze, op_version=operator.target_opset,\n                       **attrs)\n    return concat_unsqueeze\n\n\ndef norm_boxes_graph(scope, operator, container, oopb, image_meta):\n    image_shapes = oopb.add_node(\'Slice\',\n                         [image_meta,\n                          (\'_start\', oopb.int64, np.array([4], dtype=\'int64\')),\n                          (\'_end\', oopb.int64, np.array([7], dtype=\'int64\')),\n                          (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                          ],\n                         operator.inputs[0].full_name + \'_image_shapes\')\n    image_shape = oopb.add_node(\'Slice\',\n                                 [image_shapes,\n                                  (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                                  (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                                  (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                  ],\n                                 operator.inputs[0].full_name + \'_image_shape\')\n    image_shape_squeeze = scope.get_unique_variable_name(\'image_shape_squeeze\')\n    attrs = {\'axes\': [0]}\n    container.add_node(\'Squeeze\', image_shape, image_shape_squeeze, op_version=operator.target_opset,\n                       **attrs)\n    window = oopb.add_node(\'Slice\',\n                            [image_meta,\n                             (\'_start\', oopb.int64, np.array([7], dtype=\'int64\')),\n                             (\'_end\', oopb.int64, np.array([11], dtype=\'int64\')),\n                             (\'_axes\', oopb.int64, np.array([1], dtype=\'int64\'))\n                             ],\n                            operator.inputs[0].full_name + \'_window\')\n    h_norm = oopb.add_node(\'Slice\',\n                         [image_shape_squeeze,\n                          (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                          (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                          (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                          ],\n                         operator.inputs[0].full_name + \'_h_norm\')\n    w_norm = oopb.add_node(\'Slice\',\n                           [image_shape_squeeze,\n                            (\'_start\', oopb.int64, np.array([1], dtype=\'int64\')),\n                            (\'_end\', oopb.int64, np.array([2], dtype=\'int64\')),\n                            (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                            ],\n                           operator.inputs[0].full_name + \'_w_norm\')\n    h_norm_float = scope.get_unique_variable_name(\'h_norm_float\')\n    attrs = {\'to\': 1}\n    container.add_node(\'Cast\', h_norm, h_norm_float, op_version=operator.target_opset,\n                       **attrs)\n    w_norm_float = scope.get_unique_variable_name(\'w_norm_float\')\n    attrs = {\'to\': 1}\n    container.add_node(\'Cast\', w_norm, w_norm_float, op_version=operator.target_opset,\n                       **attrs)\n    hw_concat = scope.get_unique_variable_name(operator.inputs[0].full_name + \'_hw_concat\')\n    attrs = {\'axis\': -1}\n    container.add_node(""Concat"",\n                       [h_norm_float, w_norm_float, h_norm_float, w_norm_float],\n                       hw_concat,\n                       op_version=operator.target_opset,\n                       name=operator.inputs[0].full_name + \'_hw_concat\', **attrs)\n    scale = oopb.add_node(\'Sub\',\n                          [hw_concat,\n                           (\'_sub\', oopb.float, np.array([1.0], dtype=\'float32\'))\n                           ],\n                          operator.inputs[0].full_name + \'_scale\')\n    boxes_shift = oopb.add_node(\'Sub\',\n                          [window,\n                           (\'_sub\', oopb.float, np.array([0.0, 0.0, 1.0, 1.0], dtype=\'float32\'))\n                           ],\n                          operator.inputs[0].full_name + \'_boxes_shift\')\n    divide = oopb.add_node(\'Div\',\n                            [boxes_shift, scale],\n                            operator.inputs[0].full_name + \'_divide\')\n    # output shape: [batch, 4]\n    return divide\n\n\ndef convert_DetectionLayer(scope, operator, container):\n    # type: (keras2onnx.common.InterimContext, keras2onnx.common.Operator, keras2onnx.common.OnnxObjectContainer) -> None\n    DETECTION_MAX_INSTANCES = 100\n    DETECTION_NMS_THRESHOLD = 0.3\n    DETECTION_MIN_CONFIDENCE = 0.7\n\n    oopb = OnnxOperatorBuilder(container, scope)\n    box_transpose = scope.get_unique_variable_name(operator.inputs[0].full_name + \'_tx\')\n    score_transpose = scope.get_unique_variable_name(operator.inputs[1].full_name + \'_tx\')\n\n    # apply_transpose(scope, operator.inputs[0].full_name, box_transpose, container, perm=[2, 0, 1])\n    apply_identity(scope, operator.inputs[0].full_name, box_transpose, container)\n    # output shape: [num_batches, spatial_dimension, 4]\n    score_identity = scope.get_unique_variable_name(operator.inputs[1].full_name + \'_id\')\n    apply_identity(scope, operator.inputs[1].full_name, score_identity, container)\n    # output shape: [num_batches, spatial_dimension, num_classes]\n\n    deltas_transpose = scope.get_unique_variable_name(operator.inputs[2].full_name + \'_tx\')\n    apply_identity(scope, operator.inputs[2].full_name, deltas_transpose, container)\n    image_meta = scope.get_unique_variable_name(operator.inputs[3].full_name + \'_tx\')\n    apply_identity(scope, operator.inputs[3].full_name, image_meta, container)\n    windows_transpose = norm_boxes_graph(scope, operator, container, oopb, image_meta)\n    delta_mul_output = convert_apply_box_deltas_graph(scope, operator, container, oopb, box_transpose, score_identity, deltas_transpose, windows_transpose)\n\n    sliced_score = oopb.add_node(\'Slice\',\n                                 [score_identity,\n                                  (\'_start\', oopb.int64, np.array([1], dtype=\'int64\')),\n                                  (\'_end\', oopb.int64, np.array([81], dtype=\'int64\')),\n                                  (\'_axes\', oopb.int64, np.array([2], dtype=\'int64\'))\n                                  ],\n                                 operator.inputs[1].full_name + \'_sliced\')\n    apply_transpose(scope, sliced_score, score_transpose, container, perm=[0, 2, 1])\n    # output shape: [num_batches, num_classes, spatial_dimension]\n\n    max_output_size = scope.get_unique_variable_name(\'max_output_size\')\n    iou_threshold = scope.get_unique_variable_name(\'iou_threshold\')\n    score_threshold = scope.get_unique_variable_name(\'layer.score_threshold\')\n\n    container.add_initializer(max_output_size, onnx_proto.TensorProto.INT64,\n                              [], [DETECTION_MAX_INSTANCES])\n    container.add_initializer(iou_threshold, onnx_proto.TensorProto.FLOAT,\n                              [], [DETECTION_NMS_THRESHOLD])\n    container.add_initializer(score_threshold, onnx_proto.TensorProto.FLOAT,\n                              [], [DETECTION_MIN_CONFIDENCE])\n\n    nms_node = next((nd_ for nd_ in operator.nodelist if nd_.type == \'NonMaxSuppressionV3\'), operator.nodelist[0])\n    nms_output = scope.get_unique_variable_name(operator.output_full_names[0] + \'_nms\')\n    container.add_node(""NonMaxSuppression"",\n                       [delta_mul_output, score_transpose, max_output_size, iou_threshold, score_threshold],\n                       nms_output,\n                       op_version=operator.target_opset,\n                       name=nms_node.name)\n\n    add_init = scope.get_unique_variable_name(\'add\')\n    container.add_initializer(add_init, onnx_proto.TensorProto.INT64,\n                              [1, 3], [0, 1, 0])\n    nms_output_add = scope.get_unique_variable_name(operator.output_full_names[0] + \'_class_add\')\n    container.add_node(""Add"",\n                       [nms_output, add_init],\n                       nms_output_add,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_class_idx_add\')\n\n    starts_init = scope.get_unique_variable_name(\'starts\')\n    ends_init = scope.get_unique_variable_name(\'ends\')\n    axes_init = scope.get_unique_variable_name(\'axes\')\n\n    container.add_initializer(starts_init, onnx_proto.TensorProto.INT32,\n                              [1], [1])\n    container.add_initializer(ends_init, onnx_proto.TensorProto.INT32,\n                              [1], [2])\n    container.add_initializer(axes_init, onnx_proto.TensorProto.INT32,\n                              [1], [1])\n\n    class_idx_output = scope.get_unique_variable_name(operator.output_full_names[0] + \'_class_idx\')\n    container.add_node(""Slice"",\n                       [nms_output_add, starts_init, ends_init, axes_init],\n                       class_idx_output,\n                       op_version=operator.target_opset,\n                       name=nms_node.name+\'_class_idx\')\n    # output shape: [num_selected_indices, 1]\n\n    starts_init_2 = scope.get_unique_variable_name(\'starts\')\n    ends_init_2 = scope.get_unique_variable_name(\'ends\')\n    axes_init_2 = scope.get_unique_variable_name(\'axes\')\n\n    container.add_initializer(starts_init_2, onnx_proto.TensorProto.INT32,\n                              [1], [2])\n    container.add_initializer(ends_init_2, onnx_proto.TensorProto.INT32,\n                              [1], [3])\n    container.add_initializer(axes_init_2, onnx_proto.TensorProto.INT32,\n                              [1], [1])\n\n    box_idx_output = scope.get_unique_variable_name(operator.output_full_names[0] + \'_box_idx\')\n    container.add_node(""Slice"",\n                       [nms_output_add, starts_init_2, ends_init_2, axes_init_2],\n                       box_idx_output,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_box_idx\')\n    # output shape: [num_selected_indices, 1]\n\n    box_idx_squeeze = scope.get_unique_variable_name(operator.output_full_names[0] + \'_box_idx_squeeze\')\n    attrs = {\'axes\': [1]}\n    container.add_node(""Squeeze"",\n                       box_idx_output,\n                       box_idx_squeeze,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_box_idx_squeeze\', **attrs)\n    # output shape: [num_selected_indices]\n\n    starts_init_3 = scope.get_unique_variable_name(\'starts\')\n    ends_init_3 = scope.get_unique_variable_name(\'ends\')\n    axes_init_3 = scope.get_unique_variable_name(\'axes\')\n    step_init_3 = scope.get_unique_variable_name(\'steps\')\n\n    container.add_initializer(starts_init_3, onnx_proto.TensorProto.INT32,\n                              [1], [2])\n    container.add_initializer(ends_init_3, onnx_proto.TensorProto.INT32,\n                              [1], [0])\n    container.add_initializer(axes_init_3, onnx_proto.TensorProto.INT32,\n                              [1], [1])\n    container.add_initializer(step_init_3, onnx_proto.TensorProto.INT32,\n                              [1], [-1])\n    from keras2onnx.common.data_types import Int32TensorType, FloatTensorType\n    class_box_idx_output = scope.get_local_variable_or_declare_one(operator.output_full_names[0] + \'_class_box_idx\',\n                                                            type=Int32TensorType(shape=[None, 2]))\n    container.add_node(""Slice"",\n                       [nms_output_add, starts_init_3, ends_init_3, axes_init_3, step_init_3],\n                       class_box_idx_output.full_name,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_class_box_idx\')\n    # output shape: [num_selected_indices, 2]\n\n    box_squeeze = scope.get_unique_variable_name(operator.output_full_names[0] + \'_box_squeeze\')\n    attrs = {\'axes\': [0]}\n    container.add_node(""Squeeze"",\n                       delta_mul_output,\n                       box_squeeze,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_box_squeeze\', **attrs)\n    # output shape: [spatial_dimension, 4]\n\n    score_squeeze = scope.get_local_variable_or_declare_one(operator.output_full_names[0] + \'_score_squeeze\',\n                                                             type=FloatTensorType(shape=[None]))\n    attrs = {\'axes\': [0]}\n    container.add_node(""Squeeze"",\n                       score_identity,\n                       score_squeeze.full_name,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_score_squeeze\', **attrs)\n    # output shape: [spatial_dimension, num_classes]\n\n    box_gather = scope.get_unique_variable_name(operator.output_full_names[0] + \'_box_gather\')\n    attrs = {\'axis\': 0}\n    container.add_node(""Gather"",\n                       [box_squeeze, box_idx_squeeze],\n                       box_gather,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_box_gather\', **attrs)\n    # output shape: [num_selected_indices, 4]\n\n    score_gather = scope.get_unique_variable_name(operator.output_full_names[0] + \'_score_gather\')\n    container.add_node(""GatherND"",\n                       [score_squeeze.full_name, class_box_idx_output.full_name],\n                       score_gather,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_score_gather\')\n    # output shape: [num_selected_indices]\n\n    score_gather_unsqueeze = scope.get_unique_variable_name(operator.output_full_names[0] + \'_score_gather_unsqueeze\')\n    attrs = {\'axes\': [1]}\n    container.add_node(""Unsqueeze"",\n                       score_gather,\n                       score_gather_unsqueeze,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_score_gather_unsqueeze\', **attrs)\n    # output shape: [num_selected_indices, 1]\n\n\n    top_k_var = scope.get_unique_variable_name(\'topK\')\n    container.add_initializer(top_k_var, onnx_proto.TensorProto.FLOAT,\n                              [1], [100.0])\n\n    score_gather_shape = oopb.add_node(\'Shape\',\n                                       [score_gather],\n                                       operator.inputs[1].full_name + \'_score_gather_shape\')\n    attrs = {\'to\': 1}\n    scope_gather_float = oopb.add_node(\'Cast\',\n                                       [score_gather_shape],\n                                       operator.inputs[1].full_name + \'_scope_gather_float\', **attrs)\n    top_k_min = oopb.add_node(\'Min\',\n                              [scope_gather_float, top_k_var],\n                              operator.inputs[1].full_name + \'_top_k_min\')\n    attrs = {\'to\': 7}\n    top_k_min_int = oopb.add_node(\'Cast\',\n                                   [top_k_min],\n                                   operator.inputs[1].full_name + \'_top_k_min_int\', **attrs)\n\n\n    score_top_k_output_val = scope.get_unique_variable_name(operator.output_full_names[0] + \'_score_top_k_output_val\')\n    # output shape: [num_top_K]\n    score_top_k_output_idx = scope.get_unique_variable_name(operator.output_full_names[0] + \'_score_top_k_output_idx\')\n    # output shape: [num_top_K]\n    attrs = {\'axis\': 0}\n    container.add_node(\'TopK\', [score_gather, top_k_min_int], [score_top_k_output_val, score_top_k_output_idx],\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_topK\', **attrs)\n\n    class_idx_cast = scope.get_unique_variable_name(operator.output_full_names[0] + \'_class_idx_cast\')\n    attrs = {\'to\': 1}\n    container.add_node(\'Cast\', class_idx_output, class_idx_cast, op_version=operator.target_opset,\n                       name=nms_node.name+\'_class_idx_cast\', **attrs)\n    # output shape: [num_selected_indices, 1]\n\n    concat_var = scope.get_unique_variable_name(operator.output_full_names[0] + \'_concat_var\')\n    concat_node = next((nd_ for nd_ in operator.nodelist if nd_.type == \'Concat\'), operator.nodelist[0])\n    attrs = {\'axis\': 1}\n    container.add_node(""Concat"",\n                       [box_gather, class_idx_cast, score_gather_unsqueeze],\n                       concat_var,\n                       op_version=operator.target_opset,\n                       name=concat_node.name, **attrs)\n    # output shape: [num_selected_indices, 6]\n\n    all_gather = scope.get_unique_variable_name(operator.output_full_names[0] + \'_all_gather\')\n    attrs = {\'axis\': 0}\n    container.add_node(""Gather"",\n                       [concat_var, score_top_k_output_idx],\n                       all_gather,\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_all_gather\', **attrs)\n    # output shape: [num_top_K, 6]\n    padded_result = oopb.add_node(\'Pad\',\n                                  [all_gather,\n                                   np.array([0, 0, DETECTION_MAX_INSTANCES, 0],\n                                            dtype=np.int64)],\n                                  nms_node.name + \'_padded_result\')\n    detection_final = oopb.add_node(\'Slice\',\n                                 [padded_result,\n                                  (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                                  (\'_end\', oopb.int64, np.array([DETECTION_MAX_INSTANCES], dtype=\'int64\')),\n                                  (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                  ],\n                                 nms_node.name + \'_detection_final\'\n                                 )\n\n    attrs = {\'axes\': [0]}\n    container.add_node(""Unsqueeze"",\n                       detection_final,\n                       operator.output_full_names[0],\n                       op_version=operator.target_opset,\n                       name=nms_node.name + \'_concat_unsqueeze\', **attrs)\n    # output shape: [1, num_top_K, 6]\n\n\nset_converter(DetectionLayer, convert_DetectionLayer)\nset_converter(BatchNorm, convert_BatchNorm)\n\n\n# Run detection\nclass_names = [\'BG\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\',\n               \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\',\n               \'fire hydrant\', \'stop sign\', \'parking meter\', \'bench\', \'bird\',\n               \'cat\', \'dog\', \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\',\n               \'zebra\', \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n               \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\',\n               \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\',\n               \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\', \'cup\',\n               \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\',\n               \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n               \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\',\n               \'dining table\', \'toilet\', \'tv\', \'laptop\', \'mouse\', \'remote\',\n               \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\',\n               \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\',\n               \'teddy bear\', \'hair drier\', \'toothbrush\']\n\n\ndef generate_image(images, molded_images, windows, results):\n    results_final = []\n    for i, image in enumerate(images):\n        final_rois, final_class_ids, final_scores, final_masks = \\\n            model.unmold_detections(results[0][i], results[3][i], # detections[i], mrcnn_mask[i]\n                                   image.shape, molded_images[i].shape,\n                                   windows[i])\n        results_final.append({\n            ""rois"": final_rois,\n            ""class_ids"": final_class_ids,\n            ""scores"": final_scores,\n            ""masks"": final_masks,\n        })\n        r = results_final[i]\n        visualize.display_instances(image, r[\'rois\'], r[\'masks\'], r[\'class_ids\'],\n                                    class_names, r[\'scores\'])\n    return results_final\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        print(""Need an image file for object detection."")\n        exit(-1)\n\n    model_file_name = \'./mrcnn.onnx\'\n    if not os.path.exists(model_file_name):\n        # use opset 11 or later\n        set_converter(\'CropAndResize\', convert_tf_crop_and_resize)\n        oml = keras2onnx.convert_keras(model.keras_model, target_opset=11)\n        onnx.save_model(oml, model_file_name)\n\n    # run with ONNXRuntime\n    import onnxruntime\n    filename = sys.argv[1]\n    image = skimage.io.imread(filename)\n    images = [image]\n\n    sess = onnxruntime.InferenceSession(model_file_name)\n\n    # preprocessing\n    molded_images, image_metas, windows = model.mold_inputs(images)\n    anchors = model.get_anchors(molded_images[0].shape)\n    anchors = np.broadcast_to(anchors, (model.config.BATCH_SIZE,) + anchors.shape)\n\n    results = \\\n        sess.run(None, {""input_image"": molded_images.astype(np.float32),\n                        ""input_anchors"": anchors,\n                        ""input_image_meta"": image_metas.astype(np.float32)})\n\n    # postprocessing\n    results_final = generate_image(images, molded_images, windows, results)\n'"
applications/nightly_build/run_all.py,0,"b'import os\nfrom os import listdir\nfrom os.path import isfile, join\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exclude\')\nargs = parser.parse_args()\nexclude_set = set(args.exclude.split()) if args.exclude is not None else set()\n\nmypath = \'.\'\nfiles = [f for f in listdir(mypath) if isfile(join(mypath, f)) and f.find(""test_"") == 0]\nfiles.sort()\n\nres_final = True\nfor f_ in files:\n    if f_ not in exclude_set:\n        res = os.system(""pytest "" + f_ +  "" --doctest-modules --junitxml=junit/test-results-"" + f_[5:-3] + "".xml"")\n        if res > 0:\n            res_final = False\n\nif res_final:\n    assert(True)\nelse:\n    assert(False)\n'"
applications/nightly_build/test_aae.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport numpy as np\nfrom keras2onnx.proto import keras, is_tf_keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nadd = keras.layers.add\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\nimport keras.backend as K\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/aae/aae.py\nclass AdversarialAutoencoder():\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 10\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the encoder / decoder\n        self.encoder = self.build_encoder()\n        self.decoder = self.build_decoder()\n\n        img = Input(shape=self.img_shape)\n        # The generator takes the image, encodes it and reconstructs it\n        # from the encoding\n        encoded_repr = self.encoder(img)\n        reconstructed_img = self.decoder(encoded_repr)\n\n        # For the adversarial_autoencoder model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The discriminator determines validity of the encoding\n        validity = self.discriminator(encoded_repr)\n\n        # The adversarial_autoencoder model  (stacked generator and discriminator)\n        self.adversarial_autoencoder = Model(img, [reconstructed_img, validity])\n\n    def build_encoder(self):\n        # Encoder\n\n        img = Input(shape=self.img_shape)\n\n        h = Flatten()(img)\n        h = Dense(512)(h)\n        h = LeakyReLU(alpha=0.2)(h)\n        h = Dense(512)(h)\n        h = LeakyReLU(alpha=0.2)(h)\n        mu = Dense(self.latent_dim)(h)\n        log_var = Dense(self.latent_dim)(h)\n        # merge is deprecated, use add to replace it.\n        latent_repr = add([mu, log_var])\n\n        return Model(img, latent_repr)\n\n    def build_decoder(self):\n        model = Sequential()\n\n        model.add(Dense(512, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(np.prod(self.img_shape), activation=\'tanh\'))\n        model.add(Reshape(self.img_shape))\n\n        z = Input(shape=(self.latent_dim,))\n        img = model(z)\n\n        return Model(z, img)\n\n    def build_discriminator(self):\n        model = Sequential()\n\n        model.add(Dense(512, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(1, activation=""sigmoid""))\n\n        encoded_repr = Input(shape=(self.latent_dim,))\n        validity = model(encoded_repr)\n\n        return Model(encoded_repr, validity)\n\n\nclass TestAdversarialAutoencoder(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_AdversarialAutoencoder(self):\n        keras_model = AdversarialAutoencoder().adversarial_autoencoder\n        x = np.random.rand(5, 28, 28, 1).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_acgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\nfrom distutils.version import StrictVersion\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/acgan/acgan.py\nclass ACGAN():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.num_classes = 10\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise and the target label as input\n        # and generates the corresponding digit of that label\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,))\n        img = self.generator([noise, label])\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The discriminator takes generated image as input and determines validity\n        # and the label of that image\n        valid, target_label = self.discriminator(img)\n\n        # The combined model  (stacked generator and discriminator)\n        # Trains the generator to fool the discriminator\n        self.combined = Model([noise, label], [valid, target_label])\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(128 * 7 * 7, activation=""relu"", input_dim=self.latent_dim))\n        model.add(Reshape((7, 7, 128)))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=3, padding=""same""))\n        model.add(Activation(""relu""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=3, padding=""same""))\n        model.add(Activation(""relu""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(self.channels, kernel_size=3, padding=\'same\'))\n        model.add(Activation(""tanh""))\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,), dtype=\'int32\')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n\n        model_input = multiply([noise, label_embedding])\n        img = model(model_input)\n\n        return Model([noise, label], img)\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(32, kernel_size=3, strides=2, padding=""same""))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(128, kernel_size=3, strides=1, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n\n        model.add(Flatten())\n\n        img = Input(shape=self.img_shape)\n\n        # Extract feature representation\n        features = model(img)\n\n        # Determine validity and label of the image\n        validity = Dense(1, activation=""sigmoid"")(features)\n        label = Dense(self.num_classes, activation=""softmax"")(features)\n\n        return Model(img, [validity, label])\n\n\nclass TestACGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    @unittest.skipIf(StrictVersion(onnx.__version__) < StrictVersion(""1.5.0""),\n                     ""Not supported before onnx 1.5.0"")\n    def test_ACGAN(self):\n        keras_model = ACGAN().combined\n        batch = 5\n        x = np.random.rand(batch, 100).astype(np.float32)\n        y = np.random.rand(batch, 1).astype(np.float32)\n        expected = keras_model.predict([x, y])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, {keras_model.input_names[0]: x, keras_model.input_names[1]: y}, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_bgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport numpy as np\nfrom keras2onnx.proto import keras, is_tf_keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/bgan/bgan.py\nclass BGAN():\n    """"""Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/""""""\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise as input and generated imgs\n        z = Input(shape=(self.latent_dim,))\n        img = self.generator(z)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The valid takes generated images as input and determines validity\n        valid = self.discriminator(img)\n\n        # The combined model  (stacked generator and discriminator)\n        # Trains the generator to fool the discriminator\n        self.combined = Model(z, valid)\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.img_shape), activation=\'tanh\'))\n        model.add(Reshape(self.img_shape))\n\n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Flatten(input_shape=self.img_shape))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(1, activation=\'sigmoid\'))\n\n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        return Model(img, validity)\n\nclass TestBGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_BGAN(self):\n        keras_model = BGAN().combined\n        x = np.random.rand(5, 100).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_bigan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport numpy as np\nfrom keras2onnx.proto import keras, is_tf_keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nconcatenate = keras.layers.concatenate\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/bigan/bigan.py\nclass BIGAN():\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # Build the encoder\n        self.encoder = self.build_encoder()\n\n        # The part of the bigan that trains the discriminator and encoder\n        self.discriminator.trainable = False\n\n        # Generate image from sampled noise\n        z = Input(shape=(self.latent_dim, ))\n        img_ = self.generator(z)\n\n        # Encode image\n        img = Input(shape=self.img_shape)\n        z_ = self.encoder(img)\n\n        # Latent -> img is fake, and img -> latent is valid\n        fake = self.discriminator([z, img_])\n        valid = self.discriminator([z_, img])\n\n        # Set up and compile the combined model\n        # Trains generator to fool the discriminator\n        self.bigan_generator = Model([z, img], [fake, valid])\n\n\n    def build_encoder(self):\n        model = Sequential()\n\n        model.add(Flatten(input_shape=self.img_shape))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(self.latent_dim))\n\n        img = Input(shape=self.img_shape)\n        z = model(img)\n\n        return Model(img, z)\n\n    def build_generator(self):\n        model = Sequential()\n\n        model.add(Dense(512, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.img_shape), activation=\'tanh\'))\n        model.add(Reshape(self.img_shape))\n\n        z = Input(shape=(self.latent_dim,))\n        gen_img = model(z)\n\n        return Model(z, gen_img)\n\n    def build_discriminator(self):\n\n        z = Input(shape=(self.latent_dim, ))\n        img = Input(shape=self.img_shape)\n        d_in = concatenate([z, Flatten()(img)])\n\n        model = Dense(1024)(d_in)\n        model = LeakyReLU(alpha=0.2)(model)\n        model = Dropout(0.5)(model)\n        model = Dense(1024)(model)\n        model = LeakyReLU(alpha=0.2)(model)\n        model = Dropout(0.5)(model)\n        model = Dense(1024)(model)\n        model = LeakyReLU(alpha=0.2)(model)\n        model = Dropout(0.5)(model)\n        validity = Dense(1, activation=""sigmoid"")(model)\n\n        return Model([z, img], validity)\n\n\nclass TestBIGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_BIGAN(self):\n        keras_model = BIGAN().bigan_generator\n        batch = 5\n        x = np.random.rand(batch, 100).astype(np.float32)\n        y = np.random.rand(batch, 28, 28, 1).astype(np.float32)\n        expected = keras_model.predict([x, y])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, {keras_model.input_names[0]: x, keras_model.input_names[1]: y}, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_ccgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport keras_contrib\nimport numpy as np\nfrom keras2onnx import set_converter\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime, convert_InstanceNormalizationLayer\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConcatenate = keras.layers.Concatenate\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInstanceNormalization = keras_contrib.layers.InstanceNormalization\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/ccgan/ccgan.py\nclass CCGAN():\n    def __init__(self):\n        self.img_rows = 32\n        self.img_cols = 32\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.mask_height = 10\n        self.mask_width = 10\n        self.num_classes = 10\n\n        # Number of filters in first layer of generator and discriminator\n        self.gf = 32\n        self.df = 32\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise as input and generates imgs\n        masked_img = Input(shape=self.img_shape)\n        gen_img = self.generator(masked_img)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The valid takes generated images as input and determines validity\n        valid, _ = self.discriminator(gen_img)\n        # The combined model  (stacked generator and discriminator)\n        # Trains the generator to fool the discriminator\n        self.combined = Model(masked_img , valid)\n\n    def build_generator(self):\n        """"""U-Net Generator""""""\n\n        def conv2d(layer_input, filters, f_size=4, bn=True):\n            """"""Layers used during downsampling""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = BatchNormalization(momentum=0.8)(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            """"""Layers used during upsampling""""""\n            u = UpSampling2D(size=2)(layer_input)\n            u = Conv2D(filters, kernel_size=f_size, strides=1, padding=\'same\', activation=\'relu\')(u)\n            if dropout_rate:\n                u = Dropout(dropout_rate)(u)\n            u = BatchNormalization(momentum=0.8)(u)\n            u = Concatenate()([u, skip_input])\n            return u\n\n        img = Input(shape=self.img_shape)\n\n        # Downsampling\n        d1 = conv2d(img, self.gf, bn=False)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d4, d3, self.gf*4)\n        u2 = deconv2d(u1, d2, self.gf*2)\n        u3 = deconv2d(u2, d1, self.gf)\n\n        u4 = UpSampling2D(size=2)(u3)\n        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding=\'same\', activation=\'tanh\')(u4)\n\n        return Model(img, output_img)\n\n    def build_discriminator(self):\n\n        img = Input(shape=self.img_shape)\n\n        model = Sequential()\n        model.add(Conv2D(64, kernel_size=4, strides=2, padding=\'same\', input_shape=self.img_shape))\n        model.add(LeakyReLU(alpha=0.8))\n        model.add(Conv2D(128, kernel_size=4, strides=2, padding=\'same\'))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(InstanceNormalization())\n        model.add(Conv2D(256, kernel_size=4, strides=2, padding=\'same\'))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(InstanceNormalization())\n\n        features = model(img)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding=\'same\')(features)\n\n        label = Flatten()(features)\n        label = Dense(self.num_classes+1, activation=""softmax"")(label)\n\n        return Model(img, [validity, label])\n\n\nset_converter(keras_contrib.layers.InstanceNormalization, convert_InstanceNormalizationLayer)\n\n\nclass TestCCGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_CCGAN(self):\n        keras_model = CCGAN().combined\n        x = np.random.rand(2, 32, 32, 1).astype(np.float32)\n        expected = keras_model.predict([x])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files, rtol=1.e-2, atol=1.e-4))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_cogan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/cogan/cogan.py\nclass COGAN():\n    """"""Reference: https://wiseodd.github.io/techblog/2017/02/18/coupled_gan/""""""\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.d1, self.d2 = self.build_discriminators()\n\n        # Build the generator\n        self.g1, self.g2 = self.build_generators()\n\n        # The generator takes noise as input and generated imgs\n        z = Input(shape=(self.latent_dim,))\n        img1 = self.g1(z)\n        img2 = self.g2(z)\n\n        # For the combined model we will only train the generators\n        self.d1.trainable = False\n        self.d2.trainable = False\n\n        # The valid takes generated images as input and determines validity\n        valid1 = self.d1(img1)\n        valid2 = self.d2(img2)\n\n        # The combined model  (stacked generators and discriminators)\n        # Trains generators to fool discriminators\n        self.combined = Model(z, [valid1, valid2])\n\n    def build_generators(self):\n\n        # Shared weights between generators\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        noise = Input(shape=(self.latent_dim,))\n        feature_repr = model(noise)\n\n        # Generator 1\n        g1 = Dense(1024)(feature_repr)\n        g1 = LeakyReLU(alpha=0.2)(g1)\n        g1 = BatchNormalization(momentum=0.8)(g1)\n        g1 = Dense(np.prod(self.img_shape), activation=\'tanh\')(g1)\n        img1 = Reshape(self.img_shape)(g1)\n\n        # Generator 2\n        g2 = Dense(1024)(feature_repr)\n        g2 = LeakyReLU(alpha=0.2)(g2)\n        g2 = BatchNormalization(momentum=0.8)(g2)\n        g2 = Dense(np.prod(self.img_shape), activation=\'tanh\')(g2)\n        img2 = Reshape(self.img_shape)(g2)\n\n        return Model(noise, img1), Model(noise, img2)\n\n    def build_discriminators(self):\n\n        img1 = Input(shape=self.img_shape)\n        img2 = Input(shape=self.img_shape)\n\n        # Shared discriminator layers\n        model = Sequential()\n        model.add(Flatten(input_shape=self.img_shape))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n\n        img1_embedding = model(img1)\n        img2_embedding = model(img2)\n\n        # Discriminator 1\n        validity1 = Dense(1, activation=\'sigmoid\')(img1_embedding)\n        # Discriminator 2\n        validity2 = Dense(1, activation=\'sigmoid\')(img2_embedding)\n\n        return Model(img1, validity1), Model(img2, validity2)\n\n\nclass TestCOGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_COGAN(self):\n        keras_model = COGAN().combined\n        x = np.random.rand(5, 100).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_cyclegan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport keras_contrib\nimport numpy as np\nfrom keras2onnx import set_converter\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime, convert_InstanceNormalizationLayer\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConcatenate = keras.layers.Concatenate\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInstanceNormalization = keras_contrib.layers.InstanceNormalization\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/cyclegan/cyclegan.py\nclass CycleGAN():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 128\n        self.img_cols = 128\n        self.channels = 3\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows / 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 32\n        self.df = 64\n\n        # Loss weights\n        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n\n        # Build and compile the discriminators\n        self.d_A = self.build_discriminator()\n        self.d_B = self.build_discriminator()\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generators\n        #-------------------------\n\n        # Build the generators\n        self.g_AB = self.build_generator()\n        self.g_BA = self.build_generator()\n\n        # Input images from both domains\n        img_A = Input(shape=self.img_shape)\n        img_B = Input(shape=self.img_shape)\n\n        # Translate images to the other domain\n        fake_B = self.g_AB(img_A)\n        fake_A = self.g_BA(img_B)\n        # Translate images back to original domain\n        reconstr_A = self.g_BA(fake_B)\n        reconstr_B = self.g_AB(fake_A)\n        # Identity mapping of images\n        img_A_id = self.g_BA(img_A)\n        img_B_id = self.g_AB(img_B)\n\n        # For the combined model we will only train the generators\n        self.d_A.trainable = False\n        self.d_B.trainable = False\n\n        # Discriminators determines validity of translated images\n        valid_A = self.d_A(fake_A)\n        valid_B = self.d_B(fake_B)\n\n        # Combined model trains generators to fool discriminators\n        self.combined = Model(inputs=[img_A, img_B],\n                              outputs=[ valid_A, valid_B,\n                                        reconstr_A, reconstr_B,\n                                        img_A_id, img_B_id ])\n\n    def build_generator(self):\n        """"""U-Net Generator""""""\n\n        def conv2d(layer_input, filters, f_size=4):\n            """"""Layers used during downsampling""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            d = InstanceNormalization()(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            """"""Layers used during upsampling""""""\n            u = UpSampling2D(size=2)(layer_input)\n            u = Conv2D(filters, kernel_size=f_size, strides=1, padding=\'same\', activation=\'relu\')(u)\n            if dropout_rate:\n                u = Dropout(dropout_rate)(u)\n            u = InstanceNormalization()(u)\n            u = Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d0 = Input(shape=self.img_shape)\n\n        # Downsampling\n        d1 = conv2d(d0, self.gf)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d4, d3, self.gf*4)\n        u2 = deconv2d(u1, d2, self.gf*2)\n        u3 = deconv2d(u2, d1, self.gf)\n\n        u4 = UpSampling2D(size=2)(u3)\n        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding=\'same\', activation=\'tanh\')(u4)\n\n        return Model(d0, output_img)\n\n    def build_discriminator(self):\n\n        def d_layer(layer_input, filters, f_size=4, normalization=True):\n            """"""Discriminator layer""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if normalization:\n                d = InstanceNormalization()(d)\n            return d\n\n        img = Input(shape=self.img_shape)\n\n        d1 = d_layer(img, self.df, normalization=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding=\'same\')(d4)\n\n        return Model(img, validity)\n\n\nset_converter(keras_contrib.layers.InstanceNormalization, convert_InstanceNormalizationLayer)\n\n\nclass TestCycleGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_CycleGAN(self):\n        keras_model = CycleGAN().combined\n        batch = 5\n        x = np.random.rand(batch, 128, 128, 3).astype(np.float32)\n        y = np.random.rand(batch, 128, 128, 3).astype(np.float32)\n        expected = keras_model.predict([x, y])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, {keras_model.input_names[0]: x, keras_model.input_names[1]: y}, expected, self.model_files, rtol=1.e-2, atol=5.e-3))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_dcgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py\nclass DCGAN():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise as input and generates imgs\n        z = Input(shape=(self.latent_dim,))\n        img = self.generator(z)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The discriminator takes generated images as input and determines validity\n        valid = self.discriminator(img)\n\n        # The combined model  (stacked generator and discriminator)\n        # Trains the generator to fool the discriminator\n        self.combined = Model(z, valid)\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(128 * 7 * 7, activation=""relu"", input_dim=self.latent_dim))\n        model.add(Reshape((7, 7, 128)))\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=3, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(""relu""))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=3, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(""relu""))\n        model.add(Conv2D(self.channels, kernel_size=3, padding=""same""))\n        model.add(Activation(""tanh""))\n\n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=""same""))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size=3, strides=2, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(256, kernel_size=3, strides=1, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n        model.add(Dense(1, activation=\'sigmoid\'))\n\n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        return Model(img, validity)\n\nclass TestDCGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_DCGAN(self):\n        keras_model = DCGAN().combined\n        x = np.random.rand(5, 100).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_densenet_1.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras_segmentation\nfrom os.path import dirname, abspath\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image\n\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\nfrom keras2onnx.proto import is_keras_older_than\n\nclass TestDenseNet_1(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    @unittest.skipIf(is_keras_older_than(""2.2.3""),\n                     ""Cannot import normalize_data_format from keras.backend"")\n    def test_densenet(self):\n        # From https://github.com/titu1994/DenseNet/blob/master/densenet.py\n        sys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../model_source/densenet_1/\'))\n        import densenet_1\n        image_dim = (224, 224, 3)\n        model = densenet_1.DenseNetImageNet121(input_shape=image_dim)\n        res = run_image(model, self.model_files, img_path, target_size=(224, 224))\n        self.assertTrue(*res)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_densenet_2.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras_segmentation\nfrom os.path import dirname, abspath\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image\n\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\nfrom keras2onnx.proto import is_keras_older_than\n\nclass TestDenseNet_2(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_densenet(self):\n        # From https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/DenseNet/densenet.py\n        sys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../model_source/densenet_2/\'))\n        import densenet_2\n        model = densenet_2.DenseNet(20,\n                                    (224, 224, 3),\n                                    4,\n                                    1,\n                                    1,\n                                    nb_filter=10)\n        res = run_image(model, self.model_files, img_path, target_size=(224, 224))\n        self.assertTrue(*res)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_discogan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport keras_contrib\nimport numpy as np\nfrom keras2onnx import set_converter\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime, convert_InstanceNormalizationLayer\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConcatenate = keras.layers.Concatenate\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInstanceNormalization = keras_contrib.layers.InstanceNormalization\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/discogan/discogan.py\nclass DiscoGAN():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 128\n        self.img_cols = 128\n        self.channels = 3\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows / 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 64\n        self.df = 64\n\n        # Build and compile the discriminators\n        self.d_A = self.build_discriminator()\n        self.d_B = self.build_discriminator()\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generators\n        #-------------------------\n\n        # Build the generators\n        self.g_AB = self.build_generator()\n        self.g_BA = self.build_generator()\n\n        # Input images from both domains\n        img_A = Input(shape=self.img_shape)\n        img_B = Input(shape=self.img_shape)\n\n        # Translate images to the other domain\n        fake_B = self.g_AB(img_A)\n        fake_A = self.g_BA(img_B)\n        # Translate images back to original domain\n        reconstr_A = self.g_BA(fake_B)\n        reconstr_B = self.g_AB(fake_A)\n\n        # For the combined model we will only train the generators\n        self.d_A.trainable = False\n        self.d_B.trainable = False\n\n        # Discriminators determines validity of translated images\n        valid_A = self.d_A(fake_A)\n        valid_B = self.d_B(fake_B)\n\n        # Objectives\n        # + Adversarial: Fool domain discriminators\n        # + Translation: Minimize MAE between e.g. fake B and true B\n        # + Cycle-consistency: Minimize MAE between reconstructed images and original\n        self.combined = Model(inputs=[img_A, img_B],\n                              outputs=[ valid_A, valid_B,\n                                        fake_B, fake_A,\n                                        reconstr_A, reconstr_B ])\n\n    def build_generator(self):\n        """"""U-Net Generator""""""\n\n        def conv2d(layer_input, filters, f_size=4, normalize=True):\n            """"""Layers used during downsampling""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if normalize:\n                d = InstanceNormalization()(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            """"""Layers used during upsampling""""""\n            u = UpSampling2D(size=2)(layer_input)\n            u = Conv2D(filters, kernel_size=f_size, strides=1, padding=\'same\', activation=\'relu\')(u)\n            if dropout_rate:\n                u = Dropout(dropout_rate)(u)\n            u = InstanceNormalization()(u)\n            u = Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d0 = Input(shape=self.img_shape)\n\n        # Downsampling\n        d1 = conv2d(d0, self.gf, normalize=False)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n        d5 = conv2d(d4, self.gf*8)\n        d6 = conv2d(d5, self.gf*8)\n        d7 = conv2d(d6, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d7, d6, self.gf*8)\n        u2 = deconv2d(u1, d5, self.gf*8)\n        u3 = deconv2d(u2, d4, self.gf*8)\n        u4 = deconv2d(u3, d3, self.gf*4)\n        u5 = deconv2d(u4, d2, self.gf*2)\n        u6 = deconv2d(u5, d1, self.gf)\n\n        u7 = UpSampling2D(size=2)(u6)\n        output_img = Conv2D(self.channels, kernel_size=4, strides=1,\n                            padding=\'same\', activation=\'tanh\')(u7)\n\n        return Model(d0, output_img)\n\n    def build_discriminator(self):\n\n        def d_layer(layer_input, filters, f_size=4, normalization=True):\n            """"""Discriminator layer""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if normalization:\n                d = InstanceNormalization()(d)\n            return d\n\n        img = Input(shape=self.img_shape)\n\n        d1 = d_layer(img, self.df, normalization=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding=\'same\')(d4)\n\n        return Model(img, validity)\n\n\nset_converter(keras_contrib.layers.InstanceNormalization, convert_InstanceNormalizationLayer)\n\n\nclass TestDiscoGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_DiscoGAN(self):\n        keras_model = DiscoGAN().combined\n        batch = 5\n        x = np.random.rand(batch, 128, 128, 3).astype(np.float32)\n        y = np.random.rand(batch, 128, 128, 3).astype(np.float32)\n        expected = keras_model.predict([x, y])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, {keras_model.input_names[0]: x, keras_model.input_names[1]: y}, expected, self.model_files, rtol=1.e-2, atol=1.e-2))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_dualgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/dualgan/dualgan.py\nclass DUALGAN():\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_dim = self.img_rows*self.img_cols\n\n        # Build and compile the discriminators\n        self.D_A = self.build_discriminator()\n        self.D_B = self.build_discriminator()\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generators\n        #-------------------------\n\n        # Build the generators\n        self.G_AB = self.build_generator()\n        self.G_BA = self.build_generator()\n\n        # For the combined model we will only train the generators\n        self.D_A.trainable = False\n        self.D_B.trainable = False\n\n        # The generator takes images from their respective domains as inputs\n        imgs_A = Input(shape=(self.img_dim,))\n        imgs_B = Input(shape=(self.img_dim,))\n\n        # Generators translates the images to the opposite domain\n        fake_B = self.G_AB(imgs_A)\n        fake_A = self.G_BA(imgs_B)\n\n        # The discriminators determines validity of translated images\n        valid_A = self.D_A(fake_A)\n        valid_B = self.D_B(fake_B)\n\n        # Generators translate the images back to their original domain\n        recov_A = self.G_BA(fake_B)\n        recov_B = self.G_AB(fake_A)\n\n        # The combined model  (stacked generators and discriminators)\n        self.combined = Model(inputs=[imgs_A, imgs_B], outputs=[valid_A, valid_B, recov_A, recov_B])\n\n    def build_generator(self):\n\n        X = Input(shape=(self.img_dim,))\n\n        model = Sequential()\n        model.add(Dense(256, input_dim=self.img_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dropout(0.4))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dropout(0.4))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dropout(0.4))\n        model.add(Dense(self.img_dim, activation=\'tanh\'))\n\n        X_translated = model(X)\n\n        return Model(X, X_translated)\n\n    def build_discriminator(self):\n\n        img = Input(shape=(self.img_dim,))\n\n        model = Sequential()\n        model.add(Dense(512, input_dim=self.img_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1))\n\n        validity = model(img)\n\n        return Model(img, validity)\n\n\nclass TestDualGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_DualGAN(self):\n        keras_model = DUALGAN().combined\n        batch = 5\n        x = np.random.rand(batch, 784).astype(np.float32)\n        y = np.random.rand(batch, 784).astype(np.float32)\n        expected = keras_model.predict([x, y])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, {keras_model.input_names[0]: x, keras_model.input_names[1]: y}, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_efn.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nfrom os.path import dirname, abspath\nfrom keras2onnx.proto import keras, is_tensorflow_older_than\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image\n\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\n\n@unittest.skipIf(is_tensorflow_older_than(\'2.1.0\'), ""efficientnet needs tensorflow >= 2.1.0"")\nclass TestEfn(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    @unittest.skip(""TODO: model discrepancy"")\n    def test_custom(self):\n        from efficientnet import tfkeras as efn\n        keras.backend.set_learning_phase(0)\n        base_model = efn.EfficientNetB0(input_shape=(600, 600, 3), weights=None)\n        backbone = keras.Model(base_model.input, base_model.get_layer(""top_activation"").output)\n        res = run_image(backbone, self.model_files, img_path, target_size=(600, 600),\n                        rtol=1e-2, atol=1e-1, tf_v2=True)\n        self.assertTrue(*res)\n\n    def test_efn(self):\n        from efficientnet import tfkeras as efn\n        keras.backend.set_learning_phase(0)\n        model = efn.EfficientNetB0(weights=None)\n        res = run_image(model, self.model_files, img_path, target_size=(224, 224), rtol=1e-2, tf_v2=True)\n        self.assertTrue(*res)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_fcn.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras_segmentation\nfrom os.path import dirname, abspath\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\n\nclass TestFCN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_fcn(self):\n        # From https://github.com/divamgupta/image-segmentation-keras/models/fcn.py\n        model = keras_segmentation.models.fcn.fcn_8(101)\n        res = run_image(model, self.model_files, img_path, target_size=(416, 608))\n        self.assertTrue(*res)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_gan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\nclass GAN():\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise as input and generates imgs\n        z = Input(shape=(self.latent_dim,))\n        img = self.generator(z)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The discriminator takes generated images as input and determines validity\n        validity = self.discriminator(img)\n\n        # The combined model  (stacked generator and discriminator)\n        # Trains the generator to fool the discriminator\n        self.combined = Model(z, validity)\n\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.img_shape), activation=\'tanh\'))\n        model.add(Reshape(self.img_shape))\n\n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Flatten(input_shape=self.img_shape))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(1, activation=\'sigmoid\'))\n\n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        return Model(img, validity)\n\n\nclass TestGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_GAN(self):\n        keras_model = GAN().combined\n        x = np.random.rand(5, 100).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_infogan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/infogan/infogan.py\nclass INFOGAN():\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.num_classes = 10\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 72\n\n        # Build and the discriminator and recognition network\n        self.discriminator, self.auxilliary = self.build_disk_and_q_net()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise and the target label as input\n        # and generates the corresponding digit of that label\n        gen_input = Input(shape=(self.latent_dim,))\n        img = self.generator(gen_input)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The discriminator takes generated image as input and determines validity\n        valid = self.discriminator(img)\n        # The recognition network produces the label\n        target_label = self.auxilliary(img)\n\n        # The combined model  (stacked generator and discriminator)\n        self.combined = Model(gen_input, [valid, target_label])\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(128 * 7 * 7, activation=""relu"", input_dim=self.latent_dim))\n        model.add(Reshape((7, 7, 128)))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=3, padding=""same""))\n        model.add(Activation(""relu""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=3, padding=""same""))\n        model.add(Activation(""relu""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(self.channels, kernel_size=3, padding=\'same\'))\n        model.add(Activation(""tanh""))\n\n        gen_input = Input(shape=(self.latent_dim,))\n        img = model(gen_input)\n\n        return Model(gen_input, img)\n\n    def build_disk_and_q_net(self):\n        img = Input(shape=self.img_shape)\n\n        # Shared layers between discriminator and recognition network\n        model = Sequential()\n        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=self.img_shape, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size=3, strides=2, padding=""same""))\n        model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(256, kernel_size=3, strides=2, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(512, kernel_size=3, strides=2, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Flatten())\n\n        img_embedding = model(img)\n\n        # Discriminator\n        validity = Dense(1, activation=\'sigmoid\')(img_embedding)\n\n        # Recognition\n        q_net = Dense(128, activation=\'relu\')(img_embedding)\n        label = Dense(self.num_classes, activation=\'softmax\')(q_net)\n\n        # Return discriminator and recognition network\n        return Model(img, validity), Model(img, label)\n\n\nclass TestInfoGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_InfoGAN(self):\n        keras_model = INFOGAN().combined\n        x = np.random.rand(5, 72).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_keras_applications.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport onnx\nimport unittest\nimport keras2onnx\nimport keras_segmentation\nimport numpy as np\nfrom keras2onnx.proto import keras, is_keras_older_than\nfrom distutils.version import StrictVersion\nfrom os.path import dirname, abspath\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image, run_onnx_runtime\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\nActivation = keras.layers.Activation\nAveragePooling2D = keras.layers.AveragePooling2D\nBatchNormalization = keras.layers.BatchNormalization\nBidirectional = keras.layers.Bidirectional\nConcatenate = keras.layers.Concatenate\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nLSTM = keras.layers.LSTM\nMaxPooling2D = keras.layers.MaxPooling2D\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\n\nclass TestKerasApplications(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_MobileNet(self):\n        mobilenet = keras.applications.mobilenet\n        model = mobilenet.MobileNet(weights=\'imagenet\')\n        res = run_image(model, self.model_files, img_path)\n        self.assertTrue(*res)\n\n    @unittest.skipIf(is_keras_older_than(""2.2.3""),\n                     ""There is no mobilenet_v2 module before keras 2.2.3."")\n    def test_MobileNetV2(self):\n        mobilenet_v2 = keras.applications.mobilenet_v2\n        model = mobilenet_v2.MobileNetV2(weights=\'imagenet\')\n        res = run_image(model, self.model_files, img_path)\n        self.assertTrue(*res)\n\n    def test_ResNet50(self):\n        from keras.applications.resnet50 import ResNet50\n        model = ResNet50(include_top=True, weights=\'imagenet\')\n        res = run_image(model, self.model_files, img_path)\n        self.assertTrue(*res)\n\n    def test_InceptionV3(self):\n        from keras.applications.inception_v3 import InceptionV3\n        model = InceptionV3(include_top=True, weights=\'imagenet\')\n        res = run_image(model, self.model_files, img_path, target_size=299)\n        self.assertTrue(*res)\n\n    def test_DenseNet121(self):\n        from keras.applications.densenet import DenseNet121\n        model = DenseNet121(include_top=True, weights=\'imagenet\')\n        res = run_image(model, self.model_files, img_path)\n        self.assertTrue(*res)\n\n    def test_Xception(self):\n        from keras.applications.xception import Xception\n        model = Xception(include_top=True, weights=\'imagenet\')\n        res = run_image(model, self.model_files, img_path, atol=5e-3, target_size=299)\n        self.assertTrue(*res)\n\n    def test_SmileCNN(self):\n        # From https://github.com/kylemcdonald/SmileCNN/blob/master/2%20Training.ipynb\n        nb_filters = 32\n        nb_pool = 2\n        nb_conv = 3\n        nb_classes = 2\n\n        model = Sequential()\n\n        model.add(Conv2D(nb_filters, (nb_conv, nb_conv), activation=\'relu\', input_shape=(32, 32, 3)))\n        model.add(Conv2D(nb_filters, (nb_conv, nb_conv), activation=\'relu\'))\n        model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n        model.add(Dense(128, activation=\'relu\'))\n        model.add(Dropout(0.5))\n        model.add(Dense(nb_classes, activation=\'softmax\'))\n        res = run_image(model, self.model_files, img_path, atol=5e-3, target_size=32)\n        self.assertTrue(*res)\n\n    @unittest.skipIf(is_keras_older_than(""2.2.4""),\n                     ""keras-resnet requires keras 2.2.4 or later."")\n    def test_keras_resnet_batchnormalization(self):\n        N, C, H, W = 2, 3, 120, 120\n        import keras_resnet\n\n        model = Sequential()\n        model.add(ZeroPadding2D(padding=((3, 3), (3, 3)), input_shape=(H, W, C), data_format=\'channels_last\'))\n        model.add(Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding=\'valid\', dilation_rate=(1, 1), use_bias=False,\n                         data_format=\'channels_last\'))\n        model.add(keras_resnet.layers.BatchNormalization(freeze=True, axis=3))\n\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        data = np.random.rand(N, H, W, C).astype(np.float32).reshape((N, H, W, C))\n        expected = model.predict(data)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, data, expected, self.model_files))\n\n    def test_tcn(self):\n        from tcn import TCN\n        batch_size, timesteps, input_dim = None, 20, 1\n        actual_batch_size = 3\n        i = Input(batch_shape=(batch_size, timesteps, input_dim))\n        np.random.seed(1000)  # set the random seed to avoid the output result discrepancies.\n        for return_sequences in [True, False]:\n            o = TCN(return_sequences=return_sequences)(i)  # The TCN layers are here.\n            o = Dense(1)(o)\n            model = keras.models.Model(inputs=[i], outputs=[o])\n            onnx_model = keras2onnx.convert_keras(model, model.name)\n            data = np.random.rand(actual_batch_size, timesteps, input_dim).astype(np.float32).reshape((actual_batch_size, timesteps, input_dim))\n            expected = model.predict(data)\n            self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, data, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_keras_applications_v2.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nfrom keras2onnx.proto import keras\nfrom keras2onnx.proto.tfcompat import is_tf2\nfrom os.path import dirname, abspath\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image\n\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\n\n@unittest.skipIf(not is_tf2, ""Tensorflow 2.x only tests"")\nclass TestKerasApplications(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_DenseNet121(self):\n        DenseNet121 = keras.applications.densenet.DenseNet121\n        model = DenseNet121(include_top=True, weights=None)\n        res = run_image(model, self.model_files, img_path, tf_v2=True)\n        self.assertTrue(*res)\n\n    def test_MobileNet(self):\n        MobileNet = keras.applications.mobilenet.MobileNet\n        model = MobileNet(weights=None)\n        res = run_image(model, self.model_files, img_path, tf_v2=True)\n        self.assertTrue(*res)\n\n    def test_MobileNetV2(self):\n        MobileNetV2 = keras.applications.mobilenet_v2.MobileNetV2\n        model = MobileNetV2(weights=None)\n        res = run_image(model, self.model_files, img_path, tf_v2=True)\n        self.assertTrue(*res)\n\n    def test_NASNetMobile(self):\n        NASNetMobile = keras.applications.nasnet.NASNetMobile\n        model = NASNetMobile(weights=None)\n        res = run_image(model, self.model_files, img_path, tf_v2=True)\n        self.assertTrue(*res)\n\n    def test_InceptionV3(self):\n        keras.backend.set_learning_phase(0)\n        InceptionV3 = keras.applications.inception_v3.InceptionV3\n        model = InceptionV3(include_top=True)\n        res = run_image(model, self.model_files, img_path, target_size=299, tf_v2=True)\n        self.assertTrue(*res)\n\n    def test_ResNet50(self):\n        ResNet50 = keras.applications.resnet_v2.ResNet50V2\n        model = ResNet50(include_top=True, weights=None)\n        res = run_image(model, self.model_files, img_path, tf_v2=True)\n        self.assertTrue(*res)\n\n    def test_Xception(self):\n        Xception = keras.applications.xception.Xception\n        model = Xception(include_top=True, weights=None)\n        res = run_image(model, self.model_files, img_path, atol=5e-3, target_size=299, tf_v2=True)\n        self.assertTrue(*res)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_lsgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/lsgan/lsgan.py\nclass LSGAN():\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise as input and generated imgs\n        z = Input(shape=(self.latent_dim,))\n        img = self.generator(z)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The valid takes generated images as input and determines validity\n        valid = self.discriminator(img)\n\n        # The combined model  (stacked generator and discriminator)\n        # Trains generator to fool discriminator\n        self.combined = Model(z, valid)\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(256, input_dim=self.latent_dim))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(1024))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(np.prod(self.img_shape), activation=\'tanh\'))\n        model.add(Reshape(self.img_shape))\n\n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Flatten(input_shape=self.img_shape))\n        model.add(Dense(512))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(256))\n        model.add(LeakyReLU(alpha=0.2))\n        # (!!!) No softmax\n        model.add(Dense(1))\n\n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        return Model(img, validity)\n\n\nclass TestLSGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_LSGAN(self):\n        keras_model = LSGAN().combined\n        x = np.random.rand(5, 100).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_mask_rcnn.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nfrom keras2onnx import set_converter\nfrom keras2onnx.proto import keras\nimport onnx\nimport numpy as np\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime, print_mismatches, convert_tf_crop_and_resize\n\nimport urllib.request\nMASKRCNN_WEIGHTS_PATH = r\'https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\'\nmodel_file_name = \'mask_rcnn_coco.h5\'\nif not os.path.exists(model_file_name):\n    urllib.request.urlretrieve(MASKRCNN_WEIGHTS_PATH, model_file_name)\n\nkeras.backend.clear_session()\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../mask_rcnn/\'))\nfrom mask_rcnn import model\nfrom distutils.version import StrictVersion\n\nworking_path = os.path.abspath(os.path.dirname(__file__))\ntmp_path = os.path.join(working_path, \'temp\')\n\n# mask rcnn code From https://github.com/matterport/Mask_RCNN\nclass TestMaskRCNN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    @unittest.skipIf(StrictVersion(onnx.__version__.split(\'-\')[0]) < StrictVersion(""1.6.0""),\n                     ""Mask-rcnn conversion needs contrib op for onnx < 1.6.0."")\n    def test_mask_rcnn(self):\n        set_converter(\'CropAndResize\', convert_tf_crop_and_resize)\n        onnx_model = keras2onnx.convert_keras(model.keras_model)\n\n        import skimage\n        img_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n        image = skimage.io.imread(img_path)\n        images = [image]\n        case_name = \'mask_rcnn\'\n\n        if not os.path.exists(tmp_path):\n            os.mkdir(tmp_path)\n        temp_model_file = os.path.join(tmp_path, \'temp_\' + case_name + \'.onnx\')\n        onnx.save_model(onnx_model, temp_model_file)\n        try:\n            import onnxruntime\n            sess = onnxruntime.InferenceSession(temp_model_file)\n        except ImportError:\n            return True\n\n        # preprocessing\n        molded_images, image_metas, windows = model.mold_inputs(images)\n        anchors = model.get_anchors(molded_images[0].shape)\n        anchors = np.broadcast_to(anchors, (model.config.BATCH_SIZE,) + anchors.shape)\n\n        expected = model.keras_model.predict(\n            [molded_images.astype(np.float32), image_metas.astype(np.float32), anchors])\n\n        actual = \\\n            sess.run(None, {""input_image"": molded_images.astype(np.float32),\n                            ""input_anchors"": anchors,\n                            ""input_image_meta"": image_metas.astype(np.float32)})\n\n        rtol = 1.e-3\n        atol = 1.e-6\n        compare_idx = [0, 3]\n        res = all(np.allclose(expected[n_], actual[n_], rtol=rtol, atol=atol) for n_ in compare_idx)\n        if res and temp_model_file not in self.model_files:  # still keep the failed case files for the diagnosis.\n            self.model_files.append(temp_model_file)\n        if not res:\n            for n_ in compare_idx:\n                expected_list = expected[n_].flatten()\n                actual_list = actual[n_].flatten()\n                print_mismatches(case_name, n_, expected_list, actual_list, atol, rtol)\n\n        self.assertTrue(res)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_nlp.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport numpy as np\nfrom keras2onnx.proto import keras, is_tf_keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\nfrom onnxconverter_common.onnx_ex import get_maximum_opset_supported\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nBidirectional = keras.layers.Bidirectional\nconcatenate = keras.layers.concatenate\nConv1D = keras.layers.Conv1D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nLSTM = keras.layers.LSTM\nMaxPooling1D = keras.layers.MaxPooling1D\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n\nclass TestNLP(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_addition_rnn(self):\n        # An implementation of sequence to sequence learning for performing addition\n        # from https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py\n        DIGITS = 3\n        MAXLEN = DIGITS + 1 + DIGITS\n        HIDDEN_SIZE = 128\n        BATCH_SIZE = 128\n        CHARS_LENGTH = 12\n\n        for RNN in [keras.layers.LSTM, keras.layers.GRU, keras.layers.SimpleRNN]:\n            model = keras.models.Sequential()\n            model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, CHARS_LENGTH)))\n            model.add(keras.layers.RepeatVector(DIGITS + 1))\n            model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n            model.add(keras.layers.TimeDistributed(keras.layers.Dense(CHARS_LENGTH, activation=\'softmax\')))\n            onnx_model = keras2onnx.convert_keras(model, model.name)\n            x = np.random.rand(BATCH_SIZE, MAXLEN, CHARS_LENGTH).astype(np.float32)\n            expected = model.predict(x)\n            self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n    def test_babi_rnn(self):\n        # two recurrent neural networks based upon a story and a question.\n        # from https://github.com/keras-team/keras/blob/master/examples/babi_rnn.py\n        RNN = keras.layers.recurrent.LSTM\n        EMBED_HIDDEN_SIZE = 50\n        SENT_HIDDEN_SIZE = 100\n        QUERY_HIDDEN_SIZE = 100\n        BATCH_SIZE = 32\n        story_maxlen = 15\n        vocab_size = 27\n        query_maxlen = 17\n\n        sentence = Input(shape=(story_maxlen,), dtype=\'int32\')\n        encoded_sentence = Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n        encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence)\n\n        question = Input(shape=(query_maxlen,), dtype=\'int32\')\n        encoded_question = Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n        encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question)\n\n        merged = concatenate([encoded_sentence, encoded_question])\n        preds = Dense(vocab_size, activation=\'softmax\')(merged)\n\n        model = Model([sentence, question], preds)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.randint(5, 10, size=(BATCH_SIZE, story_maxlen)).astype(np.int32)\n        y = np.random.randint(5, 10, size=(BATCH_SIZE, query_maxlen)).astype(np.int32)\n        expected = model.predict([x, y])\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, {model.input_names[0]: x, model.input_names[1]: y}, expected, self.model_files))\n\n    @unittest.skipIf(get_maximum_opset_supported() < 9,\n                     ""None seq_length LSTM is not supported before opset 9."")\n    def test_imdb_bidirectional_lstm(self):\n        # A Bidirectional LSTM on the IMDB sentiment classification task.\n        # from https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py\n        max_features = 20000\n        maxlen = 100\n        batch_size = 32\n        model = Sequential()\n        model.add(Embedding(max_features, 128, input_length=maxlen))\n        model.add(Bidirectional(LSTM(64)))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation=\'sigmoid\'))\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.rand(batch_size, maxlen).astype(np.float32)\n        expected = model.predict(x)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n    def test_imdb_cnn_lstm(self):\n        # A recurrent convolutional network on the IMDB sentiment classification task.\n        # from https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py\n        max_features = 20000\n        maxlen = 100\n        embedding_size = 128\n        kernel_size = 5\n        filters = 64\n        pool_size = 4\n        lstm_output_size = 70\n        batch_size = 30\n\n        model = Sequential()\n        model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n        model.add(Dropout(0.25))\n        model.add(Conv1D(filters,\n                         kernel_size,\n                         padding=\'valid\',\n                         activation=\'relu\',\n                         strides=1))\n        model.add(MaxPooling1D(pool_size=pool_size))\n        model.add(LSTM(lstm_output_size))\n        model.add(Dense(1))\n        model.add(Activation(\'sigmoid\'))\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.rand(batch_size, maxlen).astype(np.float32)\n        expected = model.predict(x)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n    @unittest.skipIf(get_maximum_opset_supported() < 9,\n                     ""None seq_length LSTM is not supported before opset 9."")\n    def test_imdb_lstm(self):\n        # An LSTM model on the IMDB sentiment classification task.\n        # from https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n        max_features = 20000\n        maxlen = 80\n        batch_size = 32\n        model = Sequential()\n        model.add(Embedding(max_features, 128))\n        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n        model.add(Dense(1, activation=\'sigmoid\'))\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.rand(batch_size, maxlen).astype(np.float32)\n        expected = model.predict(x)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n    def test_lstm_text_generation(self):\n        # Generate text from Nietzsche\'s writings.\n        # from https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n        maxlen = 40\n        chars_len = 20\n        batch_size = 32\n        model = Sequential()\n        model.add(LSTM(128, input_shape=(maxlen, chars_len)))\n        model.add(Dense(chars_len, activation=\'softmax\'))\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.rand(batch_size, maxlen, chars_len).astype(np.float32)\n        expected = model.predict(x)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n    def test_reuters_mlp(self):\n        # An MLP on the Reuters newswire topic classification task.\n        # from https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n        max_words = 1000\n        batch_size = 32\n        num_classes = 20\n        model = Sequential()\n        model.add(Dense(512, input_shape=(max_words,)))\n        model.add(Activation(\'relu\'))\n        model.add(Dropout(0.5))\n        model.add(Dense(num_classes))\n        model.add(Activation(\'softmax\'))\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        x = np.random.rand(batch_size, max_words).astype(np.float32)\n        expected = model.predict(x)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\n    if __name__ == ""__main__"":\n        unittest.main()\n'"
applications/nightly_build/test_pix2pix.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConcatenate = keras.layers.Concatenate\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\nclass Pix2Pix():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 256\n        self.img_cols = 256\n        self.channels = 3\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows / 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 64\n        self.df = 64\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generator\n        #-------------------------\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # Input images and their conditioning images\n        img_A = Input(shape=self.img_shape)\n        img_B = Input(shape=self.img_shape)\n\n        # By conditioning on B generate a fake version of A\n        fake_A = self.generator(img_B)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # Discriminators determines validity of translated images / condition pairs\n        valid = self.discriminator([fake_A, img_B])\n\n        self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n\n    def build_generator(self):\n        """"""U-Net Generator""""""\n\n        def conv2d(layer_input, filters, f_size=4, bn=True):\n            """"""Layers used during downsampling""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = BatchNormalization(momentum=0.8)(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            """"""Layers used during upsampling""""""\n            u = UpSampling2D(size=2)(layer_input)\n            u = Conv2D(filters, kernel_size=f_size, strides=1, padding=\'same\', activation=\'relu\')(u)\n            if dropout_rate:\n                u = Dropout(dropout_rate)(u)\n            u = BatchNormalization(momentum=0.8)(u)\n            u = Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d0 = Input(shape=self.img_shape)\n\n        # Downsampling\n        d1 = conv2d(d0, self.gf, bn=False)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n        d5 = conv2d(d4, self.gf*8)\n        d6 = conv2d(d5, self.gf*8)\n        d7 = conv2d(d6, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d7, d6, self.gf*8)\n        u2 = deconv2d(u1, d5, self.gf*8)\n        u3 = deconv2d(u2, d4, self.gf*8)\n        u4 = deconv2d(u3, d3, self.gf*4)\n        u5 = deconv2d(u4, d2, self.gf*2)\n        u6 = deconv2d(u5, d1, self.gf)\n\n        u7 = UpSampling2D(size=2)(u6)\n        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding=\'same\', activation=\'tanh\')(u7)\n\n        return Model(d0, output_img)\n\n    def build_discriminator(self):\n\n        def d_layer(layer_input, filters, f_size=4, bn=True):\n            """"""Discriminator layer""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = BatchNormalization(momentum=0.8)(d)\n            return d\n\n        img_A = Input(shape=self.img_shape)\n        img_B = Input(shape=self.img_shape)\n\n        # Concatenate image and conditioning image by channels to produce input\n        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n\n        d1 = d_layer(combined_imgs, self.df, bn=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding=\'same\')(d4)\n\n        return Model([img_A, img_B], validity)\n\n\nclass TestPix2Pix(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_Pix2Pix(self):\n        keras_model = Pix2Pix().combined\n        batch = 5\n        x = np.random.rand(batch, 256, 256, 3).astype(np.float32)\n        y = np.random.rand(batch, 256, 256, 3).astype(np.float32)\n        expected = keras_model.predict([x, y])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, {keras_model.input_names[0]: x, keras_model.input_names[1]: y}, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_pixelda.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport keras_contrib\nimport numpy as np\nfrom keras2onnx import set_converter\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime, convert_InstanceNormalizationLayer\n\nActivation = keras.layers.Activation\nAdd = keras.layers.Add\nBatchNormalization = keras.layers.BatchNormalization\nConcatenate = keras.layers.Concatenate\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInstanceNormalization = keras_contrib.layers.InstanceNormalization\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/pixelda/pixelda.py\nclass PixelDA():\n    def __init__(self):\n        # Input shape\n        self.img_rows = 32\n        self.img_cols = 32\n        self.channels = 3\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.num_classes = 10\n\n        # Loss weights\n        lambda_adv = 10\n        lambda_clf = 1\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows / 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of residual blocks in the generator\n        self.residual_blocks = 6\n\n        # Number of filters in first layer of discriminator and classifier\n        self.df = 64\n        self.cf = 64\n\n        # Build and compile the discriminators\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # Build the task (classification) network\n        self.clf = self.build_classifier()\n\n        # Input images from both domains\n        img_A = Input(shape=self.img_shape)\n        img_B = Input(shape=self.img_shape)\n\n        # Translate images from domain A to domain B\n        fake_B = self.generator(img_A)\n\n        # Classify the translated image\n        class_pred = self.clf(fake_B)\n\n        # For the combined model we will only train the generator and classifier\n        self.discriminator.trainable = False\n\n        # Discriminator determines validity of translated images\n        valid = self.discriminator(fake_B)\n\n        self.combined = Model(img_A, [valid, class_pred])\n\n    def build_generator(self):\n        """"""Resnet Generator""""""\n\n        def residual_block(layer_input):\n            """"""Residual block described in paper""""""\n            d = Conv2D(64, kernel_size=3, strides=1, padding=\'same\')(layer_input)\n            d = BatchNormalization(momentum=0.8)(d)\n            d = Activation(\'relu\')(d)\n            d = Conv2D(64, kernel_size=3, strides=1, padding=\'same\')(d)\n            d = BatchNormalization(momentum=0.8)(d)\n            d = Add()([d, layer_input])\n            return d\n\n        # Image input\n        img = Input(shape=self.img_shape)\n\n        l1 = Conv2D(64, kernel_size=3, padding=\'same\', activation=\'relu\')(img)\n\n        # Propogate signal through residual blocks\n        r = residual_block(l1)\n        for _ in range(self.residual_blocks - 1):\n            r = residual_block(r)\n\n        output_img = Conv2D(self.channels, kernel_size=3, padding=\'same\', activation=\'tanh\')(r)\n\n        return Model(img, output_img)\n\n\n    def build_discriminator(self):\n\n        def d_layer(layer_input, filters, f_size=4, normalization=True):\n            """"""Discriminator layer""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if normalization:\n                d = InstanceNormalization()(d)\n            return d\n\n        img = Input(shape=self.img_shape)\n\n        d1 = d_layer(img, self.df, normalization=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding=\'same\')(d4)\n\n        return Model(img, validity)\n\n    def build_classifier(self):\n\n        def clf_layer(layer_input, filters, f_size=4, normalization=True):\n            """"""Classifier layer""""""\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if normalization:\n                d = InstanceNormalization()(d)\n            return d\n\n        img = Input(shape=self.img_shape)\n\n        c1 = clf_layer(img, self.cf, normalization=False)\n        c2 = clf_layer(c1, self.cf*2)\n        c3 = clf_layer(c2, self.cf*4)\n        c4 = clf_layer(c3, self.cf*8)\n        c5 = clf_layer(c4, self.cf*8)\n\n        class_pred = Dense(self.num_classes, activation=\'softmax\')(Flatten()(c5))\n\n        return Model(img, class_pred)\n\n\nset_converter(keras_contrib.layers.InstanceNormalization, convert_InstanceNormalizationLayer)\n\n\nclass TestPixelDA(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_PixelDA(self):\n        keras_model = PixelDA().combined\n        x = np.random.rand(5, 32, 32, 3).astype(np.float32)\n        expected = keras_model.predict([x])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files, atol=1.e-5))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_pspnet.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras\nimport keras_segmentation\nimport numpy as np\nfrom os.path import dirname, abspath\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\nActivation = keras.layers.Activation\nAveragePooling2D = keras.layers.AveragePooling2D\nBatchNormalization = keras.layers.BatchNormalization\nBidirectional = keras.layers.Bidirectional\nConcatenate = keras.layers.Concatenate\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nLSTM = keras.layers.LSTM\nMaxPooling1D = keras.layers.MaxPooling1D\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\n\nclass TestPSPNet(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def _pool_block(self, feats, pool_factor, IMAGE_ORDERING):\n        import keras.backend as K\n        if IMAGE_ORDERING == \'channels_first\':\n            h = K.int_shape(feats)[2]\n            w = K.int_shape(feats)[3]\n        elif IMAGE_ORDERING == \'channels_last\':\n            h = K.int_shape(feats)[1]\n            w = K.int_shape(feats)[2]\n        pool_size = strides = [int(np.round(float(h) / pool_factor)), int(np.round(float(w) / pool_factor))]\n        x = AveragePooling2D(pool_size, data_format=IMAGE_ORDERING, strides=strides, padding=\'same\')(feats)\n        x = Conv2D(512, (1, 1), data_format=IMAGE_ORDERING, padding=\'same\', use_bias=False)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n        x = keras_segmentation.models.model_utils.resize_image(x, strides, data_format=IMAGE_ORDERING)\n        return x\n\n    def test_pspnet(self):\n        # From https://github.com/divamgupta/image-segmentation-keras/models/pspnet.py\n        from keras_segmentation.models.basic_models import vanilla_encoder\n        img_input, levels = vanilla_encoder(input_height=384, input_width=576)\n        o = levels[4]\n        pool_factors = [1, 2, 3, 6]\n        pool_outs = [o]\n        IMAGE_ORDERING = \'channels_last\'\n        if IMAGE_ORDERING == \'channels_first\':\n            MERGE_AXIS = 1\n        elif IMAGE_ORDERING == \'channels_last\':\n            MERGE_AXIS = -1\n        for p in pool_factors:\n            pooled = self._pool_block(o, p, IMAGE_ORDERING)\n            pool_outs.append(pooled)\n        o = Concatenate(axis=MERGE_AXIS)(pool_outs)\n        o = Conv2D(512, (1, 1), data_format=IMAGE_ORDERING, use_bias=False)(o)\n        o = BatchNormalization()(o)\n        o = Activation(\'relu\')(o)\n        o = Conv2D(101, (3, 3), data_format=IMAGE_ORDERING, padding=\'same\')(o)\n        o = keras_segmentation.models.model_utils.resize_image(o, (8, 8), data_format=IMAGE_ORDERING)\n\n        model = keras_segmentation.models.model_utils.get_segmentation_model(img_input, o)\n        model.model_name = ""pspnet""\n\n        res = run_image(model, self.model_files, img_path, target_size=(384, 576))\n        self.assertTrue(*res)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_segnet.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras_segmentation\nfrom os.path import dirname, abspath\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\n\nclass TestSegNet(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_segnet(self):\n        # From https://github.com/divamgupta/image-segmentation-keras/blob/master/keras_segmentation/models/segnet.py\n        model = keras_segmentation.models.segnet.segnet(101)\n        res = run_image(model, self.model_files, img_path, target_size=(416, 608))\n        self.assertTrue(*res)\n\n    def test_vgg_segnet(self):\n        # From https://github.com/divamgupta/image-segmentation-keras/blob/master/keras_segmentation/models/segnet.py\n        model = keras_segmentation.models.segnet.vgg_segnet(101)\n        res = run_image(model, self.model_files, img_path, rtol=3.e-3, target_size=(416, 608))\n        self.assertTrue(*res)\n\n    def test_mobilenet_segnet(self):\n        # From https://github.com/divamgupta/image-segmentation-keras/blob/master/keras_segmentation/models/segnet.py\n        model = keras_segmentation.models.segnet.mobilenet_segnet(101)\n        res = run_image(model, self.model_files, img_path, target_size=(224, 224))\n        self.assertTrue(*res)\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_sgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras, is_keras_older_than\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/sgan/sgan.py\nclass SGAN:\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.num_classes = 10\n        self.latent_dim = 100\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise as input and generates imgs\n        noise = Input(shape=(100,))\n        img = self.generator(noise)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The valid takes generated images as input and determines validity\n        valid, _ = self.discriminator(img)\n\n        # The combined model  (stacked generator and discriminator)\n        # Trains generator to fool discriminator\n        self.combined = Model(noise, valid)\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(128 * 7 * 7, activation=""relu"", input_dim=self.latent_dim))\n        model.add(Reshape((7, 7, 128)))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=3, padding=""same""))\n        model.add(Activation(""relu""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=3, padding=""same""))\n        model.add(Activation(""relu""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(1, kernel_size=3, padding=""same""))\n        model.add(Activation(""tanh""))\n\n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=""same""))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(128, kernel_size=3, strides=2, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(256, kernel_size=3, strides=1, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n\n        img = Input(shape=self.img_shape)\n\n        features = model(img)\n        valid = Dense(1, activation=""sigmoid"")(features)\n        label = Dense(self.num_classes+1, activation=""softmax"")(features)\n\n        return Model(img, [valid, label])\n\n\nclass TestSGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    @unittest.skipIf(is_keras_older_than(""2.2.4""),\n                     ""keras version older than 2.2.4 not supported for SGAN"")\n    def test_SGAN(self):\n        keras_model = SGAN().combined\n        x = np.random.rand(5, 100).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_srgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom keras.applications import VGG19\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nAdd = keras.layers.Add\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/srgan/srgan.py\nclass SRGAN():\n    def __init__(self):\n        # Input shape\n        self.channels = 3\n        self.lr_height = 64                 # Low resolution height\n        self.lr_width = 64                  # Low resolution width\n        self.lr_shape = (self.lr_height, self.lr_width, self.channels)\n        self.hr_height = self.lr_height*4   # High resolution height\n        self.hr_width = self.lr_width*4     # High resolution width\n        self.hr_shape = (self.hr_height, self.hr_width, self.channels)\n\n        # Number of residual blocks in the generator\n        self.n_residual_blocks = 16\n\n        # We use a pre-trained VGG19 model to extract image features from the high resolution\n        # and the generated high resolution images and minimize the mse between them\n        self.vgg = self.build_vgg()\n        self.vgg.trainable = False\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.hr_height / 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 64\n        self.df = 64\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # High res. and low res. images\n        img_hr = Input(shape=self.hr_shape)\n        img_lr = Input(shape=self.lr_shape)\n\n        # Generate high res. version from low res.\n        fake_hr = self.generator(img_lr)\n\n        # Extract image features of the generated img\n        fake_features = self.vgg(fake_hr)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # Discriminator determines validity of generated high res. images\n        validity = self.discriminator(fake_hr)\n\n        self.combined = Model([img_lr, img_hr], [validity, fake_features])\n\n    def build_vgg(self):\n        """"""\n        Builds a pre-trained VGG19 model that outputs image features extracted at the\n        third block of the model\n        """"""\n        vgg = VGG19(weights=""imagenet"")\n        # Set outputs to outputs of last conv. layer in block 3\n        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n        vgg.outputs = [vgg.layers[9].output]\n\n        img = Input(shape=self.hr_shape)\n\n        # Extract image features\n        img_features = vgg(img)\n\n        return Model(img, img_features)\n\n    def build_generator(self):\n\n        def residual_block(layer_input, filters):\n            """"""Residual block described in paper""""""\n            d = Conv2D(filters, kernel_size=3, strides=1, padding=\'same\')(layer_input)\n            d = Activation(\'relu\')(d)\n            d = BatchNormalization(momentum=0.8)(d)\n            d = Conv2D(filters, kernel_size=3, strides=1, padding=\'same\')(d)\n            d = BatchNormalization(momentum=0.8)(d)\n            d = Add()([d, layer_input])\n            return d\n\n        def deconv2d(layer_input):\n            """"""Layers used during upsampling""""""\n            u = UpSampling2D(size=2)(layer_input)\n            u = Conv2D(256, kernel_size=3, strides=1, padding=\'same\')(u)\n            u = Activation(\'relu\')(u)\n            return u\n\n        # Low resolution image input\n        img_lr = Input(shape=self.lr_shape)\n\n        # Pre-residual block\n        c1 = Conv2D(64, kernel_size=9, strides=1, padding=\'same\')(img_lr)\n        c1 = Activation(\'relu\')(c1)\n\n        # Propogate through residual blocks\n        r = residual_block(c1, self.gf)\n        for _ in range(self.n_residual_blocks - 1):\n            r = residual_block(r, self.gf)\n\n        # Post-residual block\n        c2 = Conv2D(64, kernel_size=3, strides=1, padding=\'same\')(r)\n        c2 = BatchNormalization(momentum=0.8)(c2)\n        c2 = Add()([c2, c1])\n\n        # Upsampling\n        u1 = deconv2d(c2)\n        u2 = deconv2d(u1)\n\n        # Generate high resolution output\n        gen_hr = Conv2D(self.channels, kernel_size=9, strides=1, padding=\'same\', activation=\'tanh\')(u2)\n\n        return Model(img_lr, gen_hr)\n\n    def build_discriminator(self):\n\n        def d_block(layer_input, filters, strides=1, bn=True):\n            """"""Discriminator layer""""""\n            d = Conv2D(filters, kernel_size=3, strides=strides, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = BatchNormalization(momentum=0.8)(d)\n            return d\n\n        # Input img\n        d0 = Input(shape=self.hr_shape)\n\n        d1 = d_block(d0, self.df, bn=False)\n        d2 = d_block(d1, self.df, strides=2)\n        d3 = d_block(d2, self.df*2)\n        d4 = d_block(d3, self.df*2, strides=2)\n        d5 = d_block(d4, self.df*4)\n        d6 = d_block(d5, self.df*4, strides=2)\n        d7 = d_block(d6, self.df*8)\n        d8 = d_block(d7, self.df*8, strides=2)\n\n        d9 = Dense(self.df*16)(d8)\n        d10 = LeakyReLU(alpha=0.2)(d9)\n        validity = Dense(1, activation=\'sigmoid\')(d10)\n\n        return Model(d0, validity)\n\n\nclass TestSRGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_SRGAN(self):\n        keras_model = SRGAN().combined\n        x = np.random.rand(5, 64, 64, 3).astype(np.float32)\n        y = np.random.rand(5, 256, 256, 3).astype(np.float32)\n        expected = keras_model.predict([x, y])\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, {keras_model.input_names[0]: x, keras_model.input_names[1]: y}, expected, self.model_files, rtol=1.e-2, atol=1.e-3))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_transformers.py,3,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport json\nimport urllib.request\nimport pickle\nfrom os.path import dirname, abspath\nfrom keras2onnx.proto import keras\nimport numpy as np\nimport tensorflow as tf\nfrom onnxconverter_common.onnx_ex import get_maximum_opset_supported\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\nfrom keras2onnx.proto import is_tensorflow_older_than\n\nenable_full_transformer_test = False\nif os.environ.get(\'ENABLE_FULL_TRANSFORMER_TEST\', \'0\') != \'0\':\n    enable_transformer_test = True\n\n\n@unittest.skipIf(is_tensorflow_older_than(\'2.1.0\'),\n                 ""Transformers conversion need tensorflow 2.1.0+"")\nclass TestTransformers(unittest.TestCase):\n\n    text_str = \'The quick brown fox jumps over lazy dog.\'\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def _get_token_path(self, file_name):\n        return \'https://lotus.blob.core.windows.net/converter-models/transformer_tokenizer/\' + file_name\n\n    def _get_tokenzier(self, tokenizer_file):\n        token_path = self._get_token_path(tokenizer_file)\n        if not os.path.exists(tokenizer_file):\n            urllib.request.urlretrieve(token_path, tokenizer_file)\n        with open(tokenizer_file, \'rb\') as handle:\n            tokenizer = pickle.load(handle)\n        return tokenizer\n\n    def _prepare_inputs(self, tokenizer, batch_size=3):\n        raw_data = json.dumps({\n            \'text\': self.text_str\n        })\n        text = json.loads(raw_data)[\'text\']\n        # The tokenizers are generated using transformers 2.5.0, but model_max_length is introduced and needed in 2.9.0. \n        if not hasattr(tokenizer, \'model_max_length\'):\n            tokenizer.model_max_length = 1024\n        inputs_raw = tokenizer.encode_plus(text, add_special_tokens=True)\n        inputs_onnx = {k_: np.repeat(np.expand_dims(v_, axis=0), batch_size, axis=0) for k_, v_ in inputs_raw.items()}\n        inputs = {k_: tf.constant(v_) for k_, v_ in inputs_onnx.items()}\n        return text, inputs, inputs_onnx\n\n    @unittest.skip(""Output shape mismatch for tf model prediction."")\n    def test_3layer_gpt2(self):\n        from transformers import GPT2Config, TFGPT2Model, BertTokenizer\n        keras2onnx.proto.keras.backend.set_learning_phase(0)\n        config = GPT2Config(n_layer=3)\n        model = TFGPT2Model(config)\n        tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\'tf\')\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFBertModel(self):\n        from transformers import BertConfig, TFBertModel\n        keras.backend.clear_session()\n        # pretrained_weights = \'bert-base-uncased\'\n        tokenizer_file = \'bert_bert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = BertConfig()\n        model = TFBertModel(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(\n            run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    @unittest.skipIf(not enable_full_transformer_test, ""Full transfomer test is not enabled"")\n    def test_TFBertForPreTraining(self):\n        from transformers import BertConfig, TFBertForPreTraining\n        keras.backend.clear_session()\n        # pretrained_weights = \'bert-base-uncased\'\n        tokenizer_file = \'bert_bert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = BertConfig()\n        model = TFBertForPreTraining(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(\n            run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    def test_TFBertForMaskedLM(self):\n        from transformers import BertConfig, TFBertForMaskedLM\n        keras.backend.clear_session()\n        # pretrained_weights = \'bert-base-uncased\'\n        tokenizer_file = \'bert_bert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = BertConfig()\n        model = TFBertForMaskedLM(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(\n            run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    @unittest.skipIf(not enable_full_transformer_test, ""Full transfomer test is not enabled"")\n    def test_TFBertForNextSentencePrediction(self):\n        from transformers import BertConfig, TFBertForNextSentencePrediction\n        keras.backend.clear_session()\n        # pretrained_weights = \'bert-base-uncased\'\n        tokenizer_file = \'bert_bert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = BertConfig()\n        model = TFBertForNextSentencePrediction(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFBertForSequenceClassification(self):\n        from transformers import BertConfig, TFBertForSequenceClassification\n        keras.backend.clear_session()\n        # pretrained_weights = \'bert-base-uncased\'\n        tokenizer_file = \'bert_bert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = BertConfig()\n        model = TFBertForSequenceClassification(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFBertForTokenClassification(self):\n        from transformers import BertConfig, TFBertForTokenClassification\n        keras.backend.clear_session()\n        # pretrained_weights = \'bert-base-uncased\'\n        tokenizer_file = \'bert_bert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = BertConfig()\n        model = TFBertForTokenClassification(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFBertForQuestionAnswering(self):\n        from transformers import BertConfig, TFBertForQuestionAnswering\n        keras.backend.clear_session()\n        # pretrained_weights = \'bert-base-uncased\'\n        tokenizer_file = \'bert_bert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = BertConfig()\n        model = TFBertForQuestionAnswering(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFGPT2(self):\n        if enable_full_transformer_test:\n            from transformers import GPT2Config, TFGPT2Model, TFGPT2LMHeadModel, TFGPT2DoubleHeadsModel\n            model_list = [TFGPT2Model, TFGPT2LMHeadModel, TFGPT2DoubleHeadsModel]\n        else:\n            from transformers import GPT2Config, TFGPT2Model\n            model_list = [TFGPT2Model]\n        # pretrained_weights = \'gpt2\'\n        tokenizer_file = \'gpt2_gpt2.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = GPT2Config()\n        for model_instance_ in model_list:\n            keras.backend.clear_session()\n            model = model_instance_(config)\n            model._set_inputs(inputs)\n            predictions_original = model(inputs)\n            predictions = [predictions_original[0]] + list(v_.numpy() for v_ in predictions_original[1])\n            onnx_model = keras2onnx.convert_keras(model, model.name)\n            self.assertTrue(\n                run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                                 atol=1.e-4))\n\n    @unittest.skipIf(get_maximum_opset_supported() < 12, ""Einsum is not supported until opset 12."")\n    def test_TFXLNet(self):\n        if enable_full_transformer_test:\n            from transformers import XLNetConfig, TFXLNetModel, TFXLNetLMHeadModel, TFXLNetForSequenceClassification, \\\n                TFXLNetForTokenClassification, TFXLNetForQuestionAnsweringSimple, XLNetTokenizer\n            model_list = [TFXLNetModel, TFXLNetLMHeadModel, TFXLNetForSequenceClassification, \\\n                TFXLNetForTokenClassification, TFXLNetForQuestionAnsweringSimple]\n        else:\n            from transformers import XLNetConfig, TFXLNetModel, XLNetTokenizer\n            model_list = [TFXLNetModel]\n\n        # XLNetTokenizer need SentencePiece, so the pickle file does not work here.\n        tokenizer = XLNetTokenizer.from_pretrained(\'xlnet-large-cased\')\n        config = XLNetConfig(n_layer=2)\n        # The model with input mask has MatrixDiagV3 which is not a registered function/op\n        token = np.asarray(tokenizer.encode(self.text_str, add_special_tokens=True), dtype=np.int32)\n        inputs_onnx = {\'input_1\': np.expand_dims(token, axis=0)}\n        inputs = tf.constant(token)[None, :]  # Batch size 1\n\n        for model_instance_ in model_list:\n            keras.backend.clear_session()\n            model = model_instance_(config)\n            predictions = model.predict(inputs)\n            onnx_model = keras2onnx.convert_keras(model)\n            self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    @unittest.skipIf(not enable_full_transformer_test, ""Full transfomer test is not enabled"")\n    def test_TFOpenAIGPTModel(self):\n        from transformers import OpenAIGPTConfig, TFOpenAIGPTModel\n        keras.backend.clear_session()\n        # pretrained_weights = \'openai-gpt\'\n        tokenizer_file = \'openai_openai-gpt.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = OpenAIGPTConfig()\n        model = TFOpenAIGPTModel(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFOpenAIGPTLMHeadModel(self):\n        from transformers import OpenAIGPTConfig, TFOpenAIGPTLMHeadModel\n        keras.backend.clear_session()\n        # pretrained_weights = \'openai-gpt\'\n        tokenizer_file = \'openai_openai-gpt.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = OpenAIGPTConfig()\n        model = TFOpenAIGPTLMHeadModel(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    def test_TFOpenAIGPTDoubleHeadsModel(self):\n        from transformers import OpenAIGPTConfig, TFOpenAIGPTDoubleHeadsModel\n        keras.backend.clear_session()\n        # pretrained_weights = \'openai-gpt\'\n        tokenizer_file = \'openai_openai-gpt.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        # tf.gather(hidden_states, cls_index, batch_dims=len(hidden_shape) - 2), batch_dims = 1 in this case\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer, batch_size=1)\n        config = OpenAIGPTConfig()\n        model = TFOpenAIGPTDoubleHeadsModel(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(\n            run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    @unittest.skip(\'tensorflow.GraphDef exceeds maximum protobuf size of 2GB\')\n    def test_TFXLMModel(self):\n        from transformers import XLMConfig, TFXLMModel\n        keras.backend.clear_session()\n        # pretrained_weights = \'xlm-mlm-enfr-1024\'\n        tokenizer_file = \'xlm_xlm-mlm-enfr-1024.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = XLMConfig()\n        model = TFXLMModel(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(\n            run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    @unittest.skip(\'tensorflow.GraphDef exceeds maximum protobuf size of 2GB\')\n    def test_TFXLMWithLMHeadModel(self):\n        from transformers import XLMConfig, TFXLMWithLMHeadModel\n        keras.backend.clear_session()\n        # pretrained_weights = \'xlm-mlm-enfr-1024\'\n        tokenizer_file = \'xlm_xlm-mlm-enfr-1024.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = XLMConfig()\n        model = TFXLMWithLMHeadModel(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(\n            run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    @unittest.skip(\'tensorflow.GraphDef exceeds maximum protobuf size of 2GB\')\n    def test_TFXLMForSequenceClassification(self):\n        from transformers import XLMConfig, TFXLMForSequenceClassification\n        keras.backend.clear_session()\n        # pretrained_weights = \'xlm-mlm-enfr-1024\'\n        tokenizer_file = \'xlm_xlm-mlm-enfr-1024.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = XLMConfig()\n        model = TFXLMForSequenceClassification(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    @unittest.skip(\'tensorflow.GraphDef exceeds maximum protobuf size of 2GB\')\n    def test_TFXLMForQuestionAnsweringSimple(self):\n        from transformers import XLMConfig, TFXLMForQuestionAnsweringSimple\n        keras.backend.clear_session()\n        # pretrained_weights = \'xlm-mlm-enfr-1024\'\n        tokenizer_file = \'xlm_xlm-mlm-enfr-1024.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = XLMConfig()\n        model = TFXLMForQuestionAnsweringSimple(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFDistilBertModel(self):\n        from transformers import DistilBertConfig, TFDistilBertModel\n        keras.backend.clear_session()\n        # pretrained_weights = \'distilbert-base-uncased\'\n        tokenizer_file = \'distilbert_distilbert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = DistilBertConfig()\n        model = TFDistilBertModel(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFDistilBertForMaskedLM(self):\n        from transformers import DistilBertConfig, TFDistilBertForMaskedLM\n        keras.backend.clear_session()\n        # pretrained_weights = \'distilbert-base-uncased\'\n        tokenizer_file = \'distilbert_distilbert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = DistilBertConfig()\n        model = TFDistilBertForMaskedLM(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(\n            run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    @unittest.skipIf(not enable_full_transformer_test, ""Full transfomer test is not enabled"")\n    def test_TFDistilBertForSequenceClassification(self):\n        from transformers import DistilBertConfig, TFDistilBertForSequenceClassification\n        keras.backend.clear_session()\n        # pretrained_weights = \'distilbert-base-uncased\'\n        tokenizer_file = \'distilbert_distilbert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = DistilBertConfig()\n        model = TFDistilBertForSequenceClassification(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFDistilBertForTokenClassification(self):\n        from transformers import DistilBertConfig, TFDistilBertForTokenClassification\n        keras.backend.clear_session()\n        # pretrained_weights = \'distilbert-base-uncased\'\n        tokenizer_file = \'distilbert_distilbert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = DistilBertConfig()\n        model = TFDistilBertForTokenClassification(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFDistilBertForQuestionAnswering(self):\n        from transformers import DistilBertConfig, TFDistilBertForQuestionAnswering\n        keras.backend.clear_session()\n        # pretrained_weights = \'distilbert-base-uncased\'\n        tokenizer_file = \'distilbert_distilbert-base-uncased.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = DistilBertConfig()\n        model = TFDistilBertForQuestionAnswering(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    @unittest.skipIf(not enable_full_transformer_test, ""Full transfomer test is not enabled"")\n    def test_TFRobertaModel(self):\n        from transformers import RobertaConfig, TFRobertaModel\n        keras.backend.clear_session()\n        # pretrained_weights = \'roberta-base\'\n        tokenizer_file = \'roberta_roberta-base.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = RobertaConfig()\n        model = TFRobertaModel(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    def test_TFRobertaForMaskedLM(self):\n        from transformers import RobertaConfig, TFRobertaForMaskedLM\n        keras.backend.clear_session()\n        # pretrained_weights = \'roberta-base\'\n        tokenizer_file = \'roberta_roberta-base.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = RobertaConfig()\n        model = TFRobertaForMaskedLM(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(\n            run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files, rtol=1.e-2,\n                             atol=1.e-4))\n\n    def test_TFRobertaForSequenceClassification(self):\n        from transformers import RobertaConfig, TFRobertaForSequenceClassification\n        keras.backend.clear_session()\n        # pretrained_weights = \'roberta-base\'\n        tokenizer_file = \'roberta_roberta-base.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = RobertaConfig()\n        model = TFRobertaForSequenceClassification(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n    @unittest.skipIf(not enable_full_transformer_test, ""Full transfomer test is not enabled"")\n    def test_TFRobertaForTokenClassification(self):\n        from transformers import RobertaConfig, TFRobertaForTokenClassification\n        keras.backend.clear_session()\n        # pretrained_weights = \'roberta-base\'\n        tokenizer_file = \'roberta_roberta-base.pickle\'\n        tokenizer = self._get_tokenzier(tokenizer_file)\n        text, inputs, inputs_onnx = self._prepare_inputs(tokenizer)\n        config = RobertaConfig()\n        model = TFRobertaForTokenClassification(config)\n        predictions = model.predict(inputs)\n        onnx_model = keras2onnx.convert_keras(model, model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, inputs_onnx, predictions, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_unet.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras_segmentation\nfrom os.path import dirname, abspath\nfrom keras2onnx.proto import keras, is_keras_older_than\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_image\nimg_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n\n\nInput = keras.layers.Input\nconcatenate = keras.layers.concatenate\nConv2D = keras.layers.Conv2D\nConv2DTranspose = keras.layers.Conv2DTranspose\nMaxPooling2D = keras.layers.MaxPooling2D\n\nModel = keras.models.Model\n\nclass TestUnet(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_unet_1(self):\n        # From https://github.com/divamgupta/image-segmentation-keras/models/unet.py\n        model = keras_segmentation.models.unet.unet(101)\n        res = run_image(model, self.model_files, img_path, target_size=(416, 608))\n        self.assertTrue(*res)\n\n    @unittest.skipIf(is_keras_older_than(""2.2.3""),\n                     ""Cannot import normalize_data_format from keras.backend"")\n    def test_unet_2(self):\n        # From https://github.com/jocicmarko/ultrasound-nerve-segmentation\n        img_rows = 96\n        img_cols = 96\n\n        inputs = Input((img_rows, img_cols, 1))\n        conv1 = Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\')(inputs)\n        conv1 = Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\')(conv1)\n        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n        conv2 = Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\')(pool1)\n        conv2 = Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\')(conv2)\n        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n        conv3 = Conv2D(128, (3, 3), activation=\'relu\', padding=\'same\')(pool2)\n        conv3 = Conv2D(128, (3, 3), activation=\'relu\', padding=\'same\')(conv3)\n        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n        conv4 = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\')(pool3)\n        conv4 = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\')(conv4)\n        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n        conv5 = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\')(pool4)\n        conv5 = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\')(conv5)\n\n        up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=\'same\')(conv5), conv4], axis=3)\n        conv6 = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\')(up6)\n        conv6 = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\')(conv6)\n\n        up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=\'same\')(conv6), conv3], axis=3)\n        conv7 = Conv2D(128, (3, 3), activation=\'relu\', padding=\'same\')(up7)\n        conv7 = Conv2D(128, (3, 3), activation=\'relu\', padding=\'same\')(conv7)\n\n        up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=\'same\')(conv7), conv2], axis=3)\n        conv8 = Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\')(up8)\n        conv8 = Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\')(conv8)\n\n        up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding=\'same\')(conv8), conv1], axis=3)\n        conv9 = Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\')(up9)\n        conv9 = Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\')(conv9)\n\n        conv10 = Conv2D(1, (1, 1), activation=\'sigmoid\')(conv9)\n\n        model = Model(inputs=[inputs], outputs=[conv10])\n        res = run_image(model, self.model_files, img_path, color_mode=""grayscale"", target_size=(img_rows, img_cols))\n        self.assertTrue(*res)\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_wgan.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan/wgan.py\nclass WGAN():\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 100\n\n        # Following parameter and optimizer set as recommended in paper\n        self.n_critic = 5\n        self.clip_value = 0.01\n\n        # Build and compile the critic\n        self.critic = self.build_critic()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise as input and generated imgs\n        z = Input(shape=(self.latent_dim,))\n        img = self.generator(z)\n\n        # For the combined model we will only train the generator\n        self.critic.trainable = False\n\n        # The critic takes generated images as input and determines validity\n        valid = self.critic(img)\n\n        # The combined model  (stacked generator and critic)\n        self.combined = Model(z, valid)\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(128 * 7 * 7, activation=""relu"", input_dim=self.latent_dim))\n        model.add(Reshape((7, 7, 128)))\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=4, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(""relu""))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=4, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(""relu""))\n        model.add(Conv2D(self.channels, kernel_size=4, padding=""same""))\n        model.add(Activation(""tanh""))\n\n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)\n\n    def build_critic(self):\n\n        model = Sequential()\n\n        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(32, kernel_size=3, strides=2, padding=""same""))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size=3, strides=1, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n        model.add(Dense(1))\n\n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        return Model(img, validity)\n\n\nclass TestWGAN(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_WGAN(self):\n        keras_model = WGAN().combined\n        x = np.random.rand(5, 100).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_wgan_gp.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom keras2onnx.proto import keras\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom test_utils import run_onnx_runtime\nfrom keras.layers.merge import _Merge\n\nActivation = keras.layers.Activation\nBatchNormalization = keras.layers.BatchNormalization\nConv2D = keras.layers.Conv2D\nDense = keras.layers.Dense\nDropout = keras.layers.Dropout\nEmbedding = keras.layers.Embedding\nFlatten = keras.layers.Flatten\nInput = keras.layers.Input\nLeakyReLU = keras.layers.LeakyReLU\nmultiply = keras.layers.multiply\nReshape = keras.layers.Reshape\nUpSampling2D = keras.layers.UpSampling2D\nZeroPadding2D = keras.layers.ZeroPadding2D\n\nSequential = keras.models.Sequential\nModel = keras.models.Model\nK = keras.backend\n\nclass RandomWeightedAverage(_Merge):\n    """"""Provides a (random) weighted average between real and generated image samples""""""\n    def _merge_function(self, inputs):\n        alpha = K.random_uniform((32, 1, 1, 1))\n        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n\n# From https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py\nclass WGANGP():\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channels = 1\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = 100\n\n        # Following parameter and optimizer set as recommended in paper\n        self.n_critic = 5\n\n        # Build the generator and critic\n        self.generator = self.build_generator()\n        self.critic = self.build_critic()\n\n        #-------------------------------\n        # Construct Computational Graph\n        #       for the Critic\n        #-------------------------------\n\n        # Freeze generator\'s layers while training critic\n        self.generator.trainable = False\n\n        # Image input (real sample)\n        real_img = Input(shape=self.img_shape)\n\n        # Noise input\n        z_disc = Input(shape=(self.latent_dim,))\n        # Generate image based of noise (fake sample)\n        fake_img = self.generator(z_disc)\n\n        # Discriminator determines validity of the real and fake images\n        fake = self.critic(fake_img)\n        valid = self.critic(real_img)\n\n        # Construct weighted average between real and fake images\n        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n        # Determine validity of weighted sample\n        validity_interpolated = self.critic(interpolated_img)\n\n        self.critic_model = Model(inputs=[real_img, z_disc],\n                            outputs=[valid, fake, validity_interpolated])\n        #-------------------------------\n        # Construct Computational Graph\n        #         for Generator\n        #-------------------------------\n\n        # For the generator we freeze the critic\'s layers\n        self.critic.trainable = False\n        self.generator.trainable = True\n\n        # Sampled noise for input to generator\n        z_gen = Input(shape=(self.latent_dim,))\n        # Generate images based of noise\n        img = self.generator(z_gen)\n        # Discriminator determines validity\n        valid = self.critic(img)\n        # Defines generator model\n        self.generator_model = Model(z_gen, valid)\n\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(128 * 7 * 7, activation=""relu"", input_dim=self.latent_dim))\n        model.add(Reshape((7, 7, 128)))\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=4, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(""relu""))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=4, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(""relu""))\n        model.add(Conv2D(self.channels, kernel_size=4, padding=""same""))\n        model.add(Activation(""tanh""))\n\n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)\n\n    def build_critic(self):\n\n        model = Sequential()\n\n        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=""same""))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(32, kernel_size=3, strides=2, padding=""same""))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size=3, strides=1, padding=""same""))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n        model.add(Dense(1))\n\n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        return Model(img, validity)\n\n\nclass TestWGANGP(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def test_WGANGP(self):\n        keras_model = WGANGP().generator_model\n        x = np.random.rand(5, 100).astype(np.float32)\n        expected = keras_model.predict(x)\n        onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\n        self.assertTrue(run_onnx_runtime(onnx_model.graph.name, onnx_model, x, expected, self.model_files))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/nightly_build/test_yolov3.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport sys\nimport unittest\nimport keras2onnx\nimport onnx\nimport numpy as np\nfrom os.path import dirname, abspath\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../../tests/\'))\nfrom keras.models import load_model\n\nimport urllib.request\nYOLOV3_WEIGHTS_PATH = r\'https://lotus.blob.core.windows.net/converter-models/yolov3.h5\'\nmodel_file_name = \'yolov3.h5\'\nYOLOV3_TINY_WEIGHTS_PATH = r\'https://lotus.blob.core.windows.net/converter-models/yolov3-tiny.h5\'\ntiny_model_file_name = \'yolov3-tiny.h5\'\n\nsys.path.insert(0, os.path.join(dirname(abspath(__file__)), \'../yolov3\'))\nfrom yolov3 import YOLO\n\nfrom distutils.version import StrictVersion\n\nworking_path = os.path.abspath(os.path.dirname(__file__))\ntmp_path = os.path.join(working_path, \'temp\')\n\n\nclass TestYoloV3(unittest.TestCase):\n\n    def setUp(self):\n        self.model_files = []\n\n    def tearDown(self):\n        for fl in self.model_files:\n            os.remove(fl)\n\n    def post_compute(self, all_boxes, all_scores, indices):\n        out_boxes, out_scores, out_classes = [], [], []\n        for idx_ in indices[0]:\n            out_classes.append(idx_[1])\n            out_scores.append(all_scores[tuple(idx_)])\n            idx_1 = (idx_[0], idx_[2])\n            out_boxes.append(all_boxes[idx_1])\n        return [out_boxes, out_scores, out_classes]\n\n    @unittest.skipIf(StrictVersion(onnx.__version__.split(\'-\')[0]) < StrictVersion(""1.5.0""),\n                     ""NonMaxSuppression op is not supported for onnx < 1.5.0."")\n    def test_yolov3(self):\n        img_path = os.path.join(os.path.dirname(__file__), \'../data\', \'street.jpg\')\n        yolo3_yolo3_dir = os.path.join(os.path.dirname(__file__), \'../../keras-yolo3/yolo3\')\n        try:\n            import onnxruntime\n        except ImportError:\n            return True\n\n        from PIL import Image\n\n        for is_tiny_yolo in [True, False]:\n            if is_tiny_yolo:\n                if not os.path.exists(tiny_model_file_name):\n                    urllib.request.urlretrieve(YOLOV3_TINY_WEIGHTS_PATH, tiny_model_file_name)\n                yolo_weights = load_model(tiny_model_file_name)\n                model_path = tiny_model_file_name  # model path or trained weights path\n                anchors_path = \'model_data/tiny_yolo_anchors.txt\'\n                case_name = \'yolov3-tiny\'\n            else:\n                if not os.path.exists(model_file_name):\n                    urllib.request.urlretrieve(YOLOV3_WEIGHTS_PATH, model_file_name)\n                yolo_weights = load_model(model_file_name)\n                model_path = model_file_name  # model path or trained weights path\n                anchors_path = \'model_data/yolo_anchors.txt\'\n                case_name = \'yolov3\'\n\n            my_yolo = YOLO(model_path, anchors_path, yolo3_yolo3_dir)\n            my_yolo.load_model(yolo_weights)\n            onnx_model = keras2onnx.convert_keras(my_yolo.final_model, channel_first_inputs=[\'input_1\'])\n\n            if not os.path.exists(tmp_path):\n                os.mkdir(tmp_path)\n            temp_model_file = os.path.join(tmp_path, \'temp_\' + case_name + \'.onnx\')\n            onnx.save_model(onnx_model, temp_model_file)\n\n            sess = onnxruntime.InferenceSession(temp_model_file)\n\n            image = Image.open(img_path)\n            image_data = my_yolo.prepare_keras_data(image)\n\n            all_boxes_k, all_scores_k, indices_k = my_yolo.final_model.predict([image_data, np.array([image.size[1], image.size[0]], dtype=\'float32\').reshape(1, 2)])\n\n            image_data_onnx = np.transpose(image_data, [0, 3, 1, 2])\n\n            feed_f = dict(zip([\'input_1\', \'image_shape\'],\n                              (image_data_onnx, np.array([image.size[1], image.size[0]], dtype=\'float32\').reshape(1, 2))))\n            all_boxes, all_scores, indices = sess.run(None, input_feed=feed_f)\n\n            expected = self.post_compute(all_boxes_k, all_scores_k, indices_k)\n            actual = self.post_compute(all_boxes, all_scores, indices)\n\n            res = all(np.allclose(expected[n_], actual[n_]) for n_ in range(3))\n            self.assertTrue(res)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
applications/yolov3/yolov3.py,13,"b'import os\nimport sys\nimport inspect\nimport colorsys\nimport onnx\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom PIL import Image, ImageFont, ImageDraw\nfrom keras import backend as K\nfrom keras.layers import Input\nfrom keras.models import load_model\nfrom keras2onnx import convert_keras\nfrom keras2onnx import set_converter\nfrom keras2onnx.common.onnx_ops import apply_transpose, apply_identity, apply_cast\nfrom keras2onnx.proto import onnx_proto\n\nfrom os.path import dirname, abspath\nyolo3_dir = os.path.join(os.path.dirname(__file__), \'../../keras-yolo3\')\nif os.path.exists(yolo3_dir):\n    sys.path.insert(0, yolo3_dir)\n\nimport yolo3\nfrom yolo3.model import yolo_body, tiny_yolo_body, yolo_boxes_and_scores\nfrom yolo3.utils import letterbox_image\n\n\nclass YOLOEvaluationLayer(keras.layers.Layer):\n\n    def __init__(self, **kwargs):\n        super(YOLOEvaluationLayer, self).__init__()\n        self.anchors = np.array(kwargs.get(\'anchors\'))\n        self.num_classes = kwargs.get(\'num_classes\')\n\n    def get_config(self):\n        config = {\n            ""anchors"": self.anchors,\n            ""num_classes"": self.num_classes,\n        }\n\n        return config\n\n    def call(self, inputs, **kwargs):\n        """"""Evaluate YOLO model on given input and return filtered boxes.""""""\n        yolo_outputs = inputs[0:-1]\n        input_image_shape = K.squeeze(inputs[-1], axis=0)\n        num_layers = len(yolo_outputs)\n        anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]] if num_layers == 3 else [[3, 4, 5],\n                                                                                 [1, 2, 3]]  # default setting\n        input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n        boxes = []\n        box_scores = []\n        for l in range(num_layers):\n            _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l], self.anchors[anchor_mask[l]], self.num_classes,\n                                                        input_shape, input_image_shape)\n            boxes.append(_boxes)\n            box_scores.append(_box_scores)\n        boxes = K.concatenate(boxes, axis=0)\n        box_scores = K.concatenate(box_scores, axis=0)\n        return [boxes, box_scores]\n\n    def compute_output_shape(self, input_shape):\n        assert isinstance(input_shape, list)\n        return [(None, 4), (None, None)]\n\n\nclass YOLONMSLayer(keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(YOLONMSLayer, self).__init__()\n        self.max_boxes = kwargs.get(\'max_boxes\', 20)\n        self.score_threshold = kwargs.get(\'score_threshold\', .6)\n        self.iou_threshold = kwargs.get(\'iou_threshold\', .5)\n        self.num_classes = kwargs.get(\'num_classes\')\n\n    def get_config(self):\n        config = {\n            ""max_boxes"": self.max_boxes,\n            ""score_threshold"": self.score_threshold,\n            ""iou_threshold"": self.iou_threshold,\n            ""num_classes"": self.num_classes,\n        }\n\n        return config\n\n    def call(self, inputs, **kwargs):\n        boxes = inputs[0]\n        box_scores = inputs[1]\n        box_scores_transpose = tf.transpose(box_scores, perm=[1, 0])\n        boxes_number = tf.shape(boxes)[0]\n        box_range = tf.range(boxes_number)\n\n        mask = box_scores >= self.score_threshold\n        max_boxes_tensor = K.constant(self.max_boxes, dtype=\'int32\')\n        classes_ = []\n        batch_indexs_ = []\n        nms_indexes_ = []\n        class_box_range_ = []\n        for c in range(self.num_classes):\n            class_boxes = tf.boolean_mask(boxes, mask[:, c])\n            class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n            class_box_range = tf.boolean_mask(box_range, mask[:, c])\n            nms_index = tf.image.non_max_suppression(\n                class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=self.iou_threshold)\n            class_box_scores = K.gather(class_box_scores, nms_index)\n            class_box_range = K.gather(class_box_range, nms_index)\n            classes = K.ones_like(class_box_scores, \'int32\') * c\n            batch_index = K.zeros_like(class_box_scores, \'int32\')\n            batch_indexs_.append(batch_index)\n            classes_.append(classes)\n            nms_indexes_.append(nms_index)\n            class_box_range_.append(class_box_range)\n\n        classes_ = K.concatenate(classes_, axis=0)\n        batch_indexs_ = K.concatenate(batch_indexs_, axis=0)\n        class_box_range_ = K.concatenate(class_box_range_, axis=0)\n\n        boxes_1 = tf.expand_dims(boxes, 0)\n        classes_1 = tf.expand_dims(classes_, 1)\n        batch_indexs_ = tf.expand_dims(batch_indexs_, 1)\n        class_box_range_ = tf.expand_dims(class_box_range_, 1)\n        box_scores_transpose_1 = tf.expand_dims(box_scores_transpose, 0)\n        nms_final_ = K.concatenate([batch_indexs_, classes_1, class_box_range_], axis=1)\n        nms_final_1 = tf.expand_dims(nms_final_, 0)\n        return [boxes_1, box_scores_transpose_1, nms_final_1]\n\n    def compute_output_shape(self, input_shape):\n        assert isinstance(input_shape, list)\n        return [(None, None, 4), (None, self.num_classes, None), (None, None, 3)]\n\n\nclass YOLO(object):\n    def __init__(self, model_path=\'model_data/yolo.h5\', anchors_path=\'model_data/yolo_anchors.txt\', yolo3_dir=None):\n        self.yolo3_dir = yolo3_dir\n        self.model_path = model_path\n        self.anchors_path = anchors_path\n        self.classes_path = \'model_data/coco_classes.txt\'\n        self.score = 0.3\n        self.iou = 0.45\n        self.class_names = self._get_class()\n        self.anchors = self._get_anchors()\n        self.sess = K.get_session()\n        self.model_image_size = (416, 416)  # fixed size or (None, None), hw\n        self.session = None\n        self.final_model = None\n\n        # Generate colors for drawing bounding boxes.\n        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n                      for x in range(len(self.class_names))]\n        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n        self.colors = list(\n            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n                self.colors))\n        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n        np.random.seed(None)  # Reset seed to default.\n        K.set_learning_phase(0)\n\n    @staticmethod\n    def _get_data_path(name, yolo3_dir):\n        path = os.path.expanduser(name)\n        if not os.path.isabs(path):\n            if yolo3_dir is None:\n                yolo3_dir = os.path.dirname(inspect.getabsfile(yolo3))\n            path = os.path.join(yolo3_dir, os.path.pardir, path)\n        return path\n\n    def _get_class(self):\n        classes_path = self._get_data_path(self.classes_path, self.yolo3_dir)\n        with open(classes_path) as f:\n            class_names = f.readlines()\n        class_names = [c.strip() for c in class_names]\n        return class_names\n\n    def _get_anchors(self):\n        anchors_path = self._get_data_path(self.anchors_path, self.yolo3_dir)\n        with open(anchors_path) as f:\n            anchors = f.readline()\n        anchors = [float(x) for x in anchors.split(\',\')]\n        return np.array(anchors).reshape(-1, 2)\n\n    def load_model(self, yolo_weights=None):\n        model_path = self._get_data_path(self.model_path, self.yolo3_dir)\n        assert model_path.endswith(\'.h5\'), \'Keras model or weights must be a .h5 file.\'\n        if yolo_weights is None:\n            # Load model, or construct model and load weights.\n            num_anchors = len(self.anchors)\n            num_classes = len(self.class_names)\n            is_tiny_version = num_anchors == 6  # default setting\n\n            try:\n                self.yolo_model = load_model(model_path, compile=False)\n            except:\n                self.yolo_model = tiny_yolo_body(Input(shape=(None, None, 3)), num_anchors // 2, num_classes) \\\n                    if is_tiny_version else yolo_body(Input(shape=(None, None, 3)), num_anchors // 3, num_classes)\n                self.yolo_model.load_weights(self.model_path)  # make sure model, anchors and classes match\n            else:\n                assert self.yolo_model.layers[-1].output_shape[-1] == \\\n                    num_anchors / len(self.yolo_model.output) * (num_classes + 5), \\\n                    \'Mismatch between model and given anchor and class sizes\'\n        else:\n            self.yolo_model = yolo_weights\n\n        input_image_shape = keras.Input(shape=(2,), name=\'image_shape\')\n        image_input = keras.Input((None, None, 3), dtype=\'float32\', name=\'input_1\')\n        y = list(self.yolo_model(image_input))\n        y.append(input_image_shape)\n\n        boxes, box_scores = \\\n            YOLOEvaluationLayer(anchors=self.anchors, num_classes=len(self.class_names))(inputs=y)\n\n        out_boxes, out_scores, out_indices = \\\n            YOLONMSLayer(anchors=self.anchors, num_classes=len(self.class_names))(\n                inputs=[boxes, box_scores])\n        self.final_model = keras.Model(inputs=[image_input, input_image_shape],\n                                       outputs=[out_boxes, out_scores, out_indices])\n\n        self.final_model.save(\'final_model.h5\')\n        print(\'{} model, anchors, and classes loaded.\'.format(model_path))\n\n    def prepare_keras_data(self, image):\n        if self.model_image_size != (None, None):\n            assert self.model_image_size[0] % 32 == 0, \'Multiples of 32 required\'\n            assert self.model_image_size[1] % 32 == 0, \'Multiples of 32 required\'\n            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n        else:\n            new_image_size = (image.width - (image.width % 32),\n                              image.height - (image.height % 32))\n            boxed_image = letterbox_image(image, new_image_size)\n        image_data = np.array(boxed_image, dtype=\'float32\')\n        image_data /= 255.\n        image_data = np.expand_dims(image_data, 0) # Add batch dimension.\n        return image_data\n\n    def detect_with_onnx(self, image):\n        self.load_model()\n        image_data = self.prepare_keras_data(image)\n        all_boxes_k, all_scores_k, indices_k = self.final_model.predict([image_data, np.array([image.size[1], image.size[0]], dtype=\'float32\').reshape(1, 2)])\n\n        image_data_onnx = np.transpose(image_data, [0, 3, 1, 2])\n        feed_f = dict(zip([\'input_1\', \'image_shape\'],\n                          (image_data_onnx, np.array([image.size[1], image.size[0]], dtype=\'float32\').reshape(1, 2))))\n        all_boxes, all_scores, indices = self.session.run(None, input_feed=feed_f)\n\n        out_boxes, out_scores, out_classes = [], [], []\n        for idx_ in indices[0]:\n            out_classes.append(idx_[1])\n            out_scores.append(all_scores[tuple(idx_)])\n            idx_1 = (idx_[0], idx_[2])\n            out_boxes.append(all_boxes[idx_1])\n\n        font = ImageFont.truetype(font=self._get_data_path(\'font/FiraMono-Medium.otf\', self.yolo3_dir),\n                                  size=np.floor(3e-2 * image.size[1] + 0.5).astype(\'int32\'))\n        thickness = (image.size[0] + image.size[1]) // 300\n\n        for i, c in reversed(list(enumerate(out_classes))):\n            predicted_class = self.class_names[c]\n            box = out_boxes[i]\n            score = out_scores[i]\n\n            label = \'{} {:.2f}\'.format(predicted_class, score)\n            draw = ImageDraw.Draw(image)\n            label_size = draw.textsize(label, font)\n\n            top, left, bottom, right = box\n            top = max(0, np.floor(top + 0.5).astype(\'int32\'))\n            left = max(0, np.floor(left + 0.5).astype(\'int32\'))\n            bottom = min(image.size[1], np.floor(bottom + 0.5).astype(\'int32\'))\n            right = min(image.size[0], np.floor(right + 0.5).astype(\'int32\'))\n\n            if top - label_size[1] >= 0:\n                text_origin = np.array([left, top - label_size[1]])\n            else:\n                text_origin = np.array([left, top + 1])\n\n            for i in range(thickness):\n                draw.rectangle(\n                    [left + i, top + i, right - i, bottom - i],\n                    outline=self.colors[c])\n            draw.rectangle(\n                [tuple(text_origin), tuple(text_origin + label_size)],\n                fill=self.colors[c])\n            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n            del draw\n\n        return image\n\n\ndef detect_img(yolo, img_url, model_file_name):\n    import onnxruntime\n    image = Image.open(img_url)\n    yolo.session = onnxruntime.InferenceSession(model_file_name)\n\n    r_image = yolo.detect_with_onnx(image)\n    n_ext = img_url.rindex(\'.\')\n    score_file = img_url[0:n_ext] + \'_score\' + img_url[n_ext:]\n    r_image.save(score_file, ""JPEG"")\n\n\ndef convert_NMSLayer(scope, operator, container):\n    # type: (keras2onnx.common.InterimContext, keras2onnx.common.Operator, keras2onnx.common.OnnxObjectContainer) -> None\n    box_transpose = scope.get_unique_variable_name(operator.inputs[0].full_name + \'_tx\')\n    score_transpose = scope.get_unique_variable_name(operator.inputs[1].full_name + \'_tx\')\n\n    apply_identity(scope, operator.inputs[0].full_name, box_transpose, container)\n    apply_transpose(scope, operator.inputs[1].full_name, score_transpose, container, perm=[1, 0])\n\n    box_batch = scope.get_unique_variable_name(operator.inputs[0].full_name + \'_btc\')\n    score_batch = scope.get_unique_variable_name(operator.inputs[1].full_name + \'_btc\')\n\n    container.add_node(""Unsqueeze"", box_transpose,\n                       box_batch, op_version=operator.target_opset, axes=[0])\n    container.add_node(""Unsqueeze"", score_transpose,\n                       score_batch, op_version=operator.target_opset, axes=[0])\n\n    layer = operator.raw_operator  # type: YOLONMSLayer\n\n    max_output_size = scope.get_unique_variable_name(\'max_output_size\')\n    iou_threshold = scope.get_unique_variable_name(\'iou_threshold\')\n    score_threshold = scope.get_unique_variable_name(\'layer.score_threshold\')\n\n    container.add_initializer(max_output_size, onnx_proto.TensorProto.INT64,\n                              [], [layer.max_boxes])\n    container.add_initializer(iou_threshold, onnx_proto.TensorProto.FLOAT,\n                              [], [layer.iou_threshold])\n    container.add_initializer(score_threshold, onnx_proto.TensorProto.FLOAT,\n                              [], [layer.score_threshold])\n\n    cast_name = scope.get_unique_variable_name(\'casted\')\n    nms_node = next((nd_ for nd_ in operator.nodelist if nd_.type == \'NonMaxSuppressionV3\'), operator.nodelist[0])\n    container.add_node(""NonMaxSuppression"",\n                       [box_batch, score_batch, max_output_size, iou_threshold, score_threshold],\n                       cast_name,\n                       op_version=operator.target_opset,\n                       name=nms_node.name)\n\n    cast_batch = scope.get_unique_variable_name(operator.output_full_names[2] + \'_btc\')\n    container.add_node(""Unsqueeze"", cast_name,\n                       cast_batch, op_version=operator.target_opset, axes=[0])\n    apply_cast(scope, cast_batch, operator.output_full_names[2], container, to=onnx_proto.TensorProto.INT32)\n\n    apply_identity(scope, box_batch, operator.output_full_names[0], container)\n    apply_identity(scope, score_batch, operator.output_full_names[1], container)\n\n\nset_converter(YOLONMSLayer, convert_NMSLayer)\n\n\ndef convert_model(yolo, model_file_name, target_opset):\n    yolo.load_model()\n    onnxmodel = convert_keras(yolo.final_model, target_opset=target_opset, channel_first_inputs=[\'input_1\'])\n    onnx.save_model(onnxmodel, model_file_name)\n    return onnxmodel\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        print(""Need an image file for object detection."")\n        exit(-1)\n\n    target_opset = 10\n\n    model_file_name = \'model_data/yolov3.onnx\'\n    model_path = \'model_data/yolo.h5\'  # model path or trained weights path\n    anchors_path = \'model_data/yolo_anchors.txt\'\n    \'\'\'\n    # For tiny yolov3 case, use:\n    model_file_name = \'model_data/yolov3-tiny.onnx\'\n    model_path = \'model_data/yolo-tiny.h5\'\n    anchors_path = \'model_data/tiny_yolo_anchors.txt\'\n    \'\'\'\n\n    if not os.path.exists(model_file_name):\n        onnxmodel = convert_model(YOLO(model_path, anchors_path), model_file_name, target_opset)\n\n    detect_img(YOLO(), sys.argv[1], model_file_name)\n'"
docs/examples/plot_transfer_learning.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License.\n\n""""""\n.. _l-transfer-learning:\n\nTransfer learning with ONNX\n===========================\n\n`Transfer learning <https://en.wikipedia.org/wiki/Transfer_learning>`_\nis usually useful to adapt a deep learning model to some\nnew problem for which the number of images is not enough\nto train a deep learning model. The proposed solution\nimplies the use of class *OnnxTransformer* which wraps\n*OnnxRuntime* into a *scikit-learn* transformer\neasily pluggable into a pipeline.\n\n.. contents::\n    :local:\n\nTrain a model\n+++++++++++++\n\nA very basic example using random forest and\nthe iris dataset.\n""""""\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nclr = RandomForestClassifier(n_estimators=1, max_depth=2)\nclr.fit(X_train, y_train)\nprint(clr)\n\n###########################\n# Convert a model into ONNX\n# +++++++++++++++++++++++++\n\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\ninitial_type = [(\'float_input\', FloatTensorType([1, 4]))]\nonx = convert_sklearn(clr, initial_types=initial_type)\n\nwith open(""rf_iris.onnx"", ""wb"") as f:\n    f.write(onx.SerializeToString())\n\n###################################\n# Compute ONNX prediction similarly as scikit-learn transformer\n# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nfrom onnxruntime.sklapi import OnnxTransformer\n\nwith open(""rf_iris.onnx"", ""rb"") as f:\n    content = f.read()\n\not = OnnxTransformer(content, output_name=""output_probability"")\not.fit(X_train, y_train)\n\nprint(ot.transform(X_test[:5]))\n\n###################################\n# .. index:: transfer learning, MobileNet, ImageNet\n#\n# Transfer Learning with MobileNet\n# ++++++++++++++++++++++++++++++++\n#\n# Deep learning models started to win\n# the `ImageNet <http://www.image-net.org/>`_\n# competition in 2012 and most the winners\n# are available on the web as pre-trained models.\n# Transfer Learning is computed by wrapping\n# a backend into a *scikit-learn*\n# transformers which *onnxruntime* does.\n\nimport os\nmodel_file = ""mobilenetv2-1.0.onnx""\nif not os.path.exists(model_file):\n    print(""Download \'{0}\'..."".format(model_file))\n    import urllib.request\n    url = ""https://s3.amazonaws.com/onnx-model-zoo/mobilenet/mobilenetv2-1.0/mobilenetv2-1.0.onnx""\n    urllib.request.urlretrieve(url, model_file)\n    print(""Done."")\n\nclass_names = ""imagenet_class_index.json""\nif not os.path.exists(class_names):\n    print(""Download \'{0}\'..."".format(class_names))\n    import urllib.request\n    url = ""https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json""\n    urllib.request.urlretrieve(url, class_names)\n    print(""Done."")\n\nimport json\nwith open(class_names, ""r"", encoding=""utf-8"") as f:\n    content_classes = f.read()\nlabels = json.loads(content_classes)\n\n#####################################\n# Let\'s consider one image form *wikipedia*.\n\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nimport numpy\n\nimg = Image.open(\'daisy_wikipedia.jpg\')\nplt.imshow(img)\nplt.axis(\'off\')\n\n#####################################\n# Let\'s create the OnnxTransformer\n# which we apply on that particular image.\n\nwith open(model_file, ""rb"") as f:\n    model_bytes = f.read()\n    \not = OnnxTransformer(model_bytes)\n\nimg2 = img.resize((224, 224))\nX = numpy.asarray(img2).transpose((2, 0, 1))\nX = X[numpy.newaxis, :, :, :] / 255.0\nprint(X.shape, X.min(), X.max())\n\npred = ot.fit_transform(X)[0, :]\nprint(pred.shape)\n\n#############################\n# And the best classes are...\n\nfrom heapq import nlargest\nresults = nlargest(10, range(pred.shape[0]), pred.take)\nprint(results)\n\nimport pandas\ndata=[{""index"": i, ""label"": labels.get(str(i), (\'?\', \'?\'))[1], \'score\': pred[i]} \\\n      for i in results]\ndf = pandas.DataFrame(data)\nprint(df)\n\n###################################\n# .. index:: Yolo\n# \n# Transfer Learning with Yolo\n# +++++++++++++++++++++++++++\n#\n# `yolo <https://pjreddie.com/darknet/yolo/>`_\n# is quite popular among the framework\n# which can identity objects in images in real time.\n# One of the models is available in \n# `ONNX zoo <https://github.com/onnx/models>`_.\n\nimport os\nfilename = ""tiny_yolov2.tar.gz""\nif not os.path.exists(filename):\n    print(""Download \'{0}\'..."".format(filename))\n    import urllib.request\n    url = ""https://onnxzoo.blob.core.windows.net/models/opset_8/tiny_yolov2/tiny_yolov2.tar.gz""\n    urllib.request.urlretrieve(url, filename)\n    print(""Done."")\n\nmodel_file = ""tiny_yolov2/model.onnx""\nif not os.path.exists(model_file):\n    print(""Unzip \'{0}\'."".format(model_file))\n    import tarfile\n    tfile = tarfile.open(filename, \'r:gz\')\n    tfile.extractall()\n    print(""Done."")\n\n#######################################""\n# Let\'s retrieve an image.\n\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nimport numpy\n\nimg = Image.open(\'Au-Salon-de-l-agriculture-la-campagne-recrute.jpg\')\nplt.imshow(img)\nplt.axis(\'off\')\n\n#################################\n# It needs to be zoomed, converted into an array,\n# resized, transposed.\nimg2 = img.resize((416, 416))\nX = numpy.asarray(img2)\nX = X.transpose(2,0,1)\nX = X.reshape(1,3,416,416)\nX = X.astype(numpy.float32)\n\n#####################################\n# Let\'s create the OnnxTransformer\n\nwith open(model_file, ""rb"") as f:\n    model_bytes = f.read()\n    \not = OnnxTransformer(model_bytes)\npred = ot.fit_transform(X)\nprint(pred.shape)\n\n######################################\n# Let\'s display the results on the image itself\n\ndef display_yolo(img, out, threshold):\n    """"""\n    Displays yolo results *out* on an image *img*.\n    *threshold* filters out uncertain results.\n    """"""\n    import numpy as np\n    numClasses = 20\n    anchors = [1.08, 1.19, 3.42, 4.41, 6.63, 11.38, 9.42, 5.11, 16.62, 10.52]\n\n    def sigmoid(x, derivative=False):\n        return x*(1-x) if derivative else 1/(1+np.exp(-x))\n\n    def softmax(x):\n        scoreMatExp = np.exp(np.asarray(x))\n        return scoreMatExp / scoreMatExp.sum(0)\n\n    clut = [(0,0,0),(255,0,0),(255,0,255),(0,0,255),(0,255,0),(0,255,128),\n            (128,255,0),(128,128,0),(0,128,255),(128,0,128),\n            (255,0,128),(128,0,255),(255,128,128),(128,255,128),(255,255,0),\n            (255,128,128),(128,128,255),(255,128,128),(128,255,128),(128,255,128)]\n    label = [""aeroplane"",""bicycle"",""bird"",""boat"",""bottle"",\n             ""bus"",""car"",""cat"",""chair"",""cow"",""diningtable"",\n             ""dog"",""horse"",""motorbike"",""person"",""pottedplant"",\n             ""sheep"",""sofa"",""train"",""tvmonitor""]\n\n    draw = ImageDraw.Draw(img)\n    for cy in range(0,13):\n        for cx in range(0,13):\n            for b in range(0,5):\n                channel = b*(numClasses+5)\n                tx = out[channel  ][cy][cx]\n                ty = out[channel+1][cy][cx]\n                tw = out[channel+2][cy][cx]\n                th = out[channel+3][cy][cx]\n                tc = out[channel+4][cy][cx]\n\n                x = (float(cx) + sigmoid(tx))*32\n                y = (float(cy) + sigmoid(ty))*32\n\n                w = np.exp(tw) * 32 * anchors[2*b  ]\n                h = np.exp(th) * 32 * anchors[2*b+1]\n\n                confidence = sigmoid(tc)\n\n                classes = np.zeros(numClasses)\n                for c in range(0, numClasses):\n                    classes[c] = out[channel + 5 +c][cy][cx]\n                    classes = softmax(classes)\n                detectedClass = classes.argmax()\n\n                if threshold < classes[detectedClass] * confidence:\n                    color = clut[detectedClass]\n                    x = x - w/2\n                    y = y - h/2\n                    draw.line((x  ,y  ,x+w,y ),fill=color, width=3)\n                    draw.line((x  ,y  ,x  ,y+h),fill=color, width=3)\n                    draw.line((x+w,y  ,x+w,y+h),fill=color, width=3)\n                    draw.line((x  ,y+h,x+w,y+h),fill=color, width=3)\n\n    return img\n\nimg_results = display_yolo(img2, pred[0], 0.038)\nplt.imshow(img_results)\nplt.axis(\'off\')\n\n#################################\n# **Versions used for this example**\n\nimport numpy, sklearn\nprint(""numpy:"", numpy.__version__)\nprint(""scikit-learn:"", sklearn.__version__)\nimport onnx, onnxruntime, skl2onnx, onnxmltools, lightgbm\nprint(""onnx: "", onnx.__version__)\nprint(""onnxruntime: "", onnxruntime.__version__)\nprint(""skl2onnx: "", skl2onnx.__version__)\n'"
keras2onnx/common/__init__.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom .utils import with_variable, get_default_batch_size\nfrom .utils import k2o_logger, set_logger_level\nfrom .cvtfunc import cvtfunc\nfrom .intop import Operator\nfrom .interim import OnnxObjectContainer, InterimContext, Variable\n\n\n# keras2onnx common code has been refactored into onnxconverter-common.\n\ndef name_func(scope, operator):\n    """"""Returns a function that can generate unique names for an operator based on the\n    scope.\n    """"""\n\n    def _name_func(name):\n        return scope.get_unique_operator_name(operator.full_name + \'_\' + name)\n\n    return _name_func\n'"
keras2onnx/common/cvtfunc.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\n\nclass ConverterFunction(object):\n    """"""\n    The converter function decoration class\n    """"""\n\n    @staticmethod\n    def _functor_more(functor, pattern):\n        functor.patterns.append(pattern)\n        return functor\n\n    def __init__(self, *args, **kwargs):\n        self.name = args\n        self.pattern = kwargs.get(\'pattern\')\n        self.shape_infer = kwargs.get(\'shape_infer\')\n\n    def __call__(self, func):\n        setattr(func, \'shape_infer\', self.shape_infer)\n        setattr(func, \'patterns\', [] if self.pattern is None else [self.pattern])\n        func.more = functools.partial(ConverterFunction._functor_more, func)\n        return func\n\n\ncvtfunc = functools.partial(ConverterFunction)\n'"
keras2onnx/common/data_types.py,0,b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\n\nfrom onnxconverter_common.data_types import *  # noqa\n'
keras2onnx/common/interim.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\n# the interim objects for the model conversion.\nimport six\nfrom ..proto import helper\nfrom .intop import Operator\n\n\nclass OnnxObjectContainer(object):\n    """"""\n    In the conversion phase, this class is used to collect all materials required to build an ONNX GraphProto, which is\n    encapsulated in a ONNX ModelProto.\n    """"""\n\n    def __init__(self, target_opset):\n        \'\'\'\n        :param targeted_onnx: A string, for example, \'1.1.2\' and \'1.2\'.\n        \'\'\'\n        # Inputs of ONNX graph. They are ValueInfoProto in ONNX.\n        self.inputs = []\n        # Outputs of ONNX graph. They are ValueInfoProto in ONNX.\n        self.outputs = []\n        # ONNX tensors (type: TensorProto). They are initializers of ONNX GraphProto.\n        self.initializers = []\n        # Intermediate variables in ONNX computational graph. They are ValueInfoProto in ONNX.\n        self.value_info = []\n        # ONNX nodes (type: NodeProto) used to define computation structure\n        self.nodes = []\n        # ONNX operators\' domain-version pair set. They will be added into opset_import field in the final ONNX model.\n        self.node_domain_version_pair_sets = set()\n        # The targeted ONNX version. All produced operators should be supported by the targeted ONNX version.\n        self.target_opset = target_opset\n        # ONNX node name list\n        self.node_names = {}\n\n    @staticmethod\n    def _make_value_info(variable):\n        value_info = helper.ValueInfoProto()\n        value_info.name = variable.full_name\n        value_info.type.CopyFrom(variable.type.to_onnx_type())\n        if variable.type.doc_string:\n            value_info.doc_string = variable.type.doc_string\n        return value_info\n\n    def add_input(self, variable):\n        \'\'\'\n        Add our Variable object defined _parser.py into the the input list of the final ONNX model\n\n        :param variable: The Variable object to be added\n        \'\'\'\n        self.inputs.append(self._make_value_info(variable))\n\n    def add_output(self, variable):\n        \'\'\'\n        Add our Variable object defined _parser.py into the the output list of the final ONNX model\n\n        :param variable: The Variable object to be added\n        \'\'\'\n        self.outputs.append(self._make_value_info(variable))\n\n    def add_initializer(self, name, onnx_type, shape, content):\n        \'\'\'\n        Add a TensorProto into the initializer list of the final ONNX model\n\n        :param name: Variable name in the produced ONNX model.\n        :param onnx_type: Element types allowed in ONNX tensor, e.g., TensorProto.FLOAT and TensorProto.STRING.\n        :param shape: Tensor shape, a list of integers.\n        :param content: Flattened tensor values (i.e., a float list or a float array).\n        \'\'\'\n        if any(d is None for d in shape):\n            raise ValueError(\'Shape of initializer cannot contain None\')\n        tensor = helper.make_tensor(name, onnx_type, shape, content)\n        self.initializers.append(tensor)\n\n    def add_initializer_by_name(self, scope, original_name, onnx_type, shape, content):\n        if original_name not in scope.variable_name_mapping:\n            onnx_name = scope.get_unique_variable_name(original_name)\n            scope.variable_name_mapping[original_name] = [onnx_name]\n\n            if any(d is None for d in shape):\n                raise ValueError(\'Shape of initializer cannot contain None\')\n            tensor = helper.make_tensor(onnx_name, onnx_type, shape, content)\n            self.initializers.append(tensor)\n        else:\n            onnx_name = scope.get_onnx_variable_name(original_name)\n            assert next(ts_ for ts_ in self.initializers if ts_.name == onnx_name)\n        return onnx_name\n\n    def add_initializer_from_tensor(self, tensor):\n        self.initializers.append(tensor)\n\n    def add_value_info(self, variable):\n        self.value_info.append(self._make_value_info(variable))\n\n    def add_node(self, op_type, inputs, outputs, op_domain=\'\', op_version=1, **attrs):\n        \'\'\'\n        Add a NodeProto into the node list of the final ONNX model. If the input operator\'s domain-version information\n        cannot be found in our domain-version pool (a Python set), we may add it.\n\n        :param op_type: A string (e.g., Pool and Conv) indicating the type of the NodeProto\n        :param inputs: A list of strings. They are the input variables\' names of the considered NodeProto\n        :param outputs: A list of strings. They are the output variables\' names of the considered NodeProto\n        :param op_domain: The domain name (e.g., ai.onnx.ml) of the operator we are trying to add.\n        :param op_version: The version number (e.g., 0 and 1) of the operator we are trying to add.\n        :param attrs: A Python dictionary. Keys and values are attributes\' names and attributes\' values, respectively.\n        \'\'\'\n\n        if isinstance(inputs, (six.string_types, six.text_type)):\n            inputs = [inputs]\n        if isinstance(outputs, (six.string_types, six.text_type)):\n            outputs = [outputs]\n        if not isinstance(inputs, list) or not all(isinstance(s, (six.string_types, six.text_type)) for s in inputs):\n            type_list = \',\'.join(list(str(type(s)) for s in inputs))\n            raise ValueError(\'Inputs must be a list of string but get [%s]\' % type_list)\n        if not isinstance(outputs, list) or not all(isinstance(s, (six.string_types, six.text_type)) for s in outputs):\n            type_list = \',\'.join(list(str(type(s)) for s in outputs))\n            raise ValueError(\'Outputs must be a list of string but get [%s]\' % type_list)\n        for k, v in attrs.items():\n            if v is None:\n                raise ValueError(\'Failed to create ONNX node. Undefined attribute pair (%s, %s) found\' % (k, v))\n\n        if \'name\' in attrs.keys():\n            if attrs[\'name\'] in self.node_names:\n                cur_count = self.node_names[attrs[\'name\']] + 1\n                self.node_names.update({attrs[\'name\']: cur_count})\n                attrs[\'name\'] = attrs[\'name\'] + ""_"" + str(cur_count)\n            else:\n                self.node_names.update({attrs[\'name\']: 0})\n\n        node = helper.make_node(op_type, inputs, outputs, **attrs)\n        node.domain = op_domain\n\n        self.node_domain_version_pair_sets.add((op_domain, op_version))\n        self.nodes.append(node)\n\n    def add_onnx_node(self, proto_node, op_domain=\'\', op_version=1):\n        self.node_domain_version_pair_sets.add((op_domain, op_version))\n        self.nodes.append(proto_node)\n\n\nclass InterimContext:\n    """"""\n    The interim context was kept to re-use the keras converter code from onnxmltools, which has\n    an input parameter called as \'scope\'.\n    """"""\n\n    def __init__(self, name, parent_scopes=None, variable_name_set=None, operator_name_set=None,\n                 target_opset=None):\n        """"""\n        :param name:  A string, the unique ID of this scope in a Topology object\n        :param parent_scopes: A list of Scope objects. The last element should be the direct parent scope (i.e., where\n        this scope is declared).\n        :param variable_name_set: A set of strings serving as the name pool of variables\n        :param operator_name_set: A set of strings serving as the name pool of operators\n        :param targeted_onnx_version: A StrictVersion object indicating the ONNX version used\n        """"""\n        self.name = name\n        self.parent_scopes = parent_scopes if parent_scopes else list()\n        self.onnx_variable_names = variable_name_set if variable_name_set is not None else set()\n        self.onnx_operator_names = operator_name_set if operator_name_set is not None else set()\n        self.target_opset = target_opset\n\n        # An one-to-many map from raw variable name to ONNX variable names. It looks like\n        #   (key, value) = (raw_name, [onnx_name, onnx_name1, onnx_name2, ..., onnx_nameN])\n        # The last name may hide all other names in this scope.\n        self.variable_name_mapping = {}\n\n        # A map of local variables defined in this scope. (key, value) = (onnx_name, variable)\n        self.variables = {}\n\n        # A map of local operators defined in this scope. (key, value) = (onnx_name, operator)\n        self.operators = {}\n\n        self.prefix = name + \'/\'\n\n    def get_onnx_variable_name(self, seed):\n        """"""\n        Retrieve the variable ID of the given seed or create one if it is the first time of seeing this seed\n        """"""\n        if seed in self.variable_name_mapping:\n            return self.variable_name_mapping[seed][-1]\n        else:\n            return self.get_unique_variable_name(seed)\n\n    def get_unique_variable_name(self, seed):\n        """"""\n        Create a unique variable ID based on the given seed\n        """"""\n        if seed.startswith(self.prefix):\n            seed = seed[len(self.prefix):]\n        return Variable.generate_unique_name(seed, self.onnx_variable_names)\n\n    def get_unique_operator_name(self, seed):\n        """"""\n        Create a unique operator ID based on the given seed\n        """"""\n        if seed.startswith(self.prefix):\n            seed = seed[len(self.prefix):]\n        return Variable.generate_unique_name(seed, self.onnx_operator_names)\n\n    def find_sink_variables(self):\n        """"""\n        Find sink variables in this scope\n        """"""\n        # First we assume all variables are sinks\n        is_sink = {name: True for name in self.variables.keys()}\n        # Then, we remove those variables which are inputs of some operators\n        for operator in self.operators.values():\n            for variable in operator.inputs:\n                is_sink[variable.onnx_name] = False\n        return [variable for name, variable in self.variables.items() if is_sink[name]]\n\n    def declare_local_variable(self, raw_name, type=None, prepend=False):\n        """"""\n        This function may create a new variable in this scope. If raw_name has been used to create other variables,\n        the new variable will hide all other variables created using raw_name.\n        """"""\n        # Get unique ID for the new variable\n        onnx_name = self.get_unique_variable_name(raw_name)\n\n        # Create the variable\n        variable = Variable(raw_name, onnx_name, self.name, type)\n        self.variables[onnx_name] = variable\n\n        if raw_name in self.variable_name_mapping:\n            # Hide existing variables with the same raw_name\n            if not prepend:\n                self.variable_name_mapping[raw_name].append(onnx_name)\n            else:\n                self.variable_name_mapping[raw_name].insert(0, onnx_name)\n        else:\n            self.variable_name_mapping[raw_name] = [onnx_name]\n        return variable\n\n    def get_local_variable_or_declare_one(self, raw_name, type=None):\n        """"""\n        This function will first check if raw_name has been used to create some variables. If yes, the latest one\n        named in self.variable_name_mapping[raw_name] will be returned. Otherwise, a new variable will be created and\n        then returned.\n        """"""\n        onnx_name = self.get_onnx_variable_name(raw_name)\n        if onnx_name in self.variables:\n            return self.variables[onnx_name]\n        else:\n            variable = Variable(raw_name, onnx_name, self.name, type)\n            self.variables[onnx_name] = variable\n            if raw_name in self.variable_name_mapping:\n                self.variable_name_mapping[raw_name].append(onnx_name)\n            else:\n                self.variable_name_mapping[raw_name] = [onnx_name]\n            return variable\n\n    def declare_local_operator(self, type, raw_model=None, op_name=None, **attrs):\n        """"""\n        This function is used to declare new local operator.\n        """"""\n        onnx_name = self.get_unique_operator_name(str(type) if op_name is None else op_name)\n        operator = Operator(onnx_name, self.name, type, raw_model, self.target_opset)\n        self.operators[onnx_name] = operator\n        operator.update_attrs(**attrs)\n        return operator\n\n    def delete_local_operator(self, onnx_name):\n        """"""\n        Remove the operator whose onnx_name is the input onnx_name\n        """"""\n        if onnx_name not in self.onnx_operator_names or onnx_name not in self.operators:\n            raise RuntimeError(\'The operator to be removed not found\')\n        self.onnx_operator_names.discard(onnx_name)\n        del self.operators[onnx_name]\n\n    def delete_local_variable(self, onnx_name):\n        """"""\n        Remove the variable whose onnx_name is the input onnx_name\n        """"""\n        if onnx_name not in self.onnx_variable_names or onnx_name not in self.variables:\n            raise RuntimeError(\'The variable to be removed not found\')\n        self.onnx_variable_names.discard(onnx_name)\n        raw_name = self.variables[onnx_name].raw_name\n        self.variable_name_mapping[raw_name].remove(onnx_name)\n        del self.variables[onnx_name]\n\n\nclass Variable:\n    """"""\n    The tensor or other data types\n    """"""\n\n    def __init__(self, raw_name, onnx_name, scope, type=None):\n        """"""\n        :param raw_name: A string indicating the variable\'s name in the original model. Usually, it\'s the seed string\n        used to created its ONNX name (i.e., the field onnx_name below).\n        :param onnx_name: A string indicating the variable\'s name in the converted model\n        :param scope: A string. It\'s the name of the scope where this variable is declared\n        :param type: A type object defined in onnxmltools.convert.common.data_types.py; e.g., FloatTensorType\n        """"""\n        self.raw_name = raw_name  #\n        self.onnx_name = onnx_name  #\n        self.scope = scope\n        self.type = type\n        # The following fields are bool variables used in parsing and compiling stages\n        self.is_fed = None\n        self.is_root = None\n        self.is_leaf = None\n        self.is_abandoned = False\n        self.op_from = None\n        self.op_to = []\n\n    @property\n    def full_name(self):\n        """"""\n        Return a globally unique variable ID\n        """"""\n        return self.onnx_name\n\n    def __repr__(self):\n        return self.onnx_name\n\n    @staticmethod\n    def tfname_to_onnx(name):\n        # tf2onnx does not change name but still keep \'/\'.\n        # We should not modify name here, otherwise it causes issues in subgraph in operators.\n        return name\n\n    @staticmethod\n    def generate_unique_name(seed, existing_names):\n        """"""\n        Produce an unique string based on the seed\n        :param seed: a string\n        :param existing_names: a set containing strings which cannot be produced\n        :return: a string similar to the seed\n        """"""\n        if seed == \'\':\n            raise ValueError(\'Name seed must be an non-empty string\')\n\n        # Make the seed meet C-style naming convention\n        seed = Variable.tfname_to_onnx(seed)\n\n        # If seed has never been seen, we return it as it is. Otherwise, we will append an number to make it unique.\n        if seed not in existing_names:\n            existing_names.add(seed)\n            return seed\n        else:\n            i = 1\n            while seed + str(i) in existing_names:\n                i += 1\n            new_name = seed + str(i)\n            existing_names.add(new_name)\n            return new_name\n'"
keras2onnx/common/intop.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\n\n\nclass Operator:\n    """"""\n    The intermediate object to store the information for the final ONNX operator generation.\n    """"""\n\n    def __init__(self, onnx_name, scope, type, raw_operator, target_opset):\n        """"""\n        :param onnx_name: A unique ID, which is a string\n        :param scope: The name of the scope where this operator is declared. It\'s a string.\n        :param type: A object which uniquely characterizes the type of this operator.\n        :param raw_operator: The original operator which defines this operator;\n        :param target_opset: the target opset.\n        """"""\n        self.onnx_name = onnx_name  # operator name in the converted model\n        self.scope = scope\n        self.type = type\n        self.raw_operator = raw_operator\n        self.inputs = []\n        self.input_masks = []\n        self.outputs = []\n        self.output_masks = []\n        self.mask_value = None\n        self.nodelist = None\n        self.is_evaluated = None\n        self.target_opset = target_opset\n        self.shape_infer = None\n        self.tf2onnx_graph = None\n        self.attrs = {}\n\n    @property\n    def full_name(self):\n        """"""\n        Return a globally unique operator ID\n        """"""\n        return self.onnx_name\n\n    @property\n    def input_full_names(self):\n        """"""\n        Return all input variables\' names\n        """"""\n        return [variable.full_name for variable in self.inputs]\n\n    @property\n    def output_full_names(self):\n        """"""\n        Return all output variables\' names\n        """"""\n        return [variable.full_name for variable in self.outputs]\n\n    @property\n    def original_operator(self):\n        """"""\n        Return the original operator/layer\n        """"""\n        return self.raw_operator\n\n    def add_input(self, var):\n        if self not in var.op_to:\n            var.op_to.append(self)\n        self.inputs.append(var)\n\n    def add_output(self, var):\n        if var.op_from is not None:\n            assert False, ""Tensor {} already processed"".format(var.full_name)\n        var.op_from = self\n        self.outputs.append(var)\n\n    def add_input_mask(self, var):\n        if self not in var.op_to:\n            var.op_to.append(self)\n        self.input_masks.append(var)\n\n    def add_output_mask(self, var):\n        if var.op_from is None:\n            var.op_from = self\n        self.output_masks.append(var)\n\n    def update_attrs(self, **attrs):\n        self.attrs.update(attrs)\n\n    def get_attr(self, key):\n        return self.attrs.get(key, None)\n\n    def get_input_shape(self):\n        input_shape = self.inputs[0].type.shape\n        if input_shape is None:\n            input_shape = self.raw_operator.input_shape\n        return input_shape\n\n    def get_output_shape(self):\n        output_shape = self.outputs[0].type.shape\n        if output_shape is None:\n            output_shape = self.raw_operator.output_shape\n        return output_shape\n'"
keras2onnx/common/onnx_ops.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport functools\nimport numpy as np\nimport onnxconverter_common\nfrom onnx.mapping import NP_TYPE_TO_TENSOR_TYPE\nfrom onnxconverter_common.onnx_ops import *  # noqa:\n\nfrom .interim import OnnxObjectContainer, InterimContext\nfrom ..proto import onnx_proto\n\n\nclass OnnxOperatorBuilder:\n    def __init__(self, container, scope):\n        # type: (OnnxOperatorBuilder, OnnxObjectContainer, InterimContext) -> None\n        self._container = container\n        self._scope = scope\n        self.int32 = onnx_proto.TensorProto.INT32\n        self.int64 = onnx_proto.TensorProto.INT64\n        self.float = onnx_proto.TensorProto.FLOAT\n        self.float16 = onnx_proto.TensorProto.FLOAT16\n        self.double = onnx_proto.TensorProto.DOUBLE\n        self.bool = onnx_proto.TensorProto.BOOL\n\n        apply_operations = onnxconverter_common.onnx_ops.__dict__\n        for k_, m_ in apply_operations.items():\n            if k_.startswith(""apply_"") and callable(m_):\n                setattr(self, k_, functools.partial(self.apply_op, m_))\n\n    def _process_inputs(self, inputs, name):\n        if not isinstance(inputs, (list, tuple)):\n            inputs = [inputs]\n        ox_inputs = []\n        for i_ in inputs:\n            ox_n = i_\n            if isinstance(i_, np.ndarray):\n                ox_n = self._scope.get_unique_variable_name(name + \'_i\')\n                self._container.add_initializer(\n                    ox_n,\n                    NP_TYPE_TO_TENSOR_TYPE[i_.dtype],\n                    i_.shape,\n                    i_.flatten()\n                )\n            elif isinstance(i_, (tuple, list)):\n                ox_n = self._scope.get_unique_variable_name(name + i_[0])\n                self._container.add_initializer(\n                    ox_n,\n                    i_[1],\n                    i_[2].shape,\n                    i_[2].flatten()\n                )\n            elif isinstance(i_, str):\n                pass\n            else:\n                raise RuntimeError(\'Unknown type for ONNX initializer: {}\'.format(type(i_)))\n            ox_inputs.append(ox_n)\n\n        return ox_inputs\n\n    def add_node_all(self, op_type, inputs, name, outputs_num=1, op_domain=\'\', op_version=None, **attrs):\n        if op_version is None:\n            op_version = self._container.target_opset\n        outputs = [self._scope.get_unique_variable_name(name + str(i_)) for i_ in range(outputs_num)]\n        ox_inputs = self._process_inputs(inputs, name)\n        self._container.add_node(op_type, ox_inputs, outputs, op_domain, op_version, name=name, **attrs)\n        return outputs\n\n    def add_node(self, op_type, inputs, name, op_domain=\'\', op_version=None, **attrs):\n        return self.add_node_all(op_type, inputs, name, 1, op_domain, op_version, **attrs)[0]\n\n    def add_node_with_output(self, op_type, inputs, outputs, name, op_domain=\'\', op_version=None, **attrs):\n        if op_version is None:\n            op_version = self._container.target_opset\n        ox_inputs = self._process_inputs(inputs, name)\n        self._container.add_node(op_type, ox_inputs, outputs, op_domain, op_version, name=name, **attrs)\n        return outputs\n\n    def apply_op(self, apply_func, inputs, name, outputs_num=1, **attrs):\n        outputs = [self._scope.get_unique_variable_name(name + str(i_)) for i_ in range(outputs_num)]\n        ox_inputs = self._process_inputs(inputs, name)\n        apply_func(self._scope, ox_inputs, outputs, self._container, operator_name=name, **attrs)\n        return outputs\n\n    def apply_op_with_output(self, apply_func_name, inputs, outputs, name, **attrs):\n        apply_operations = onnxconverter_common.onnx_ops.__dict__\n        apply_func = apply_operations[apply_func_name]\n        assert apply_func is not None, ""{} not implemented in onnx_ops.py."".format(apply_func_name)\n        ox_inputs = self._process_inputs(inputs, name)\n        apply_func(self._scope, ox_inputs, outputs, self._container, operator_name=name, **attrs)\n'"
keras2onnx/common/utils.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport functools\n\n\nclass FunctionStaticVariable(object):\n    def __init__(self, *args, **kwargs):\n        self.variable = args[0]\n\n    def __call__(self, func):\n        return functools.partial(FunctionStaticVariable.retrieve_attr, func, self.variable)\n\n    @staticmethod\n    def retrieve_attr(func, var_name, *args, **kwargs):\n        if not hasattr(func, var_name):\n            result = func(*args, **kwargs)\n            setattr(func, var_name, result)\n\n        return getattr(func, var_name)\n\n\nwith_variable = functools.partial(FunctionStaticVariable)\n\n\n@with_variable(\'logger\')\ndef k2o_logger():  # type: () -> logging.Logger\n    logger = logging.getLogger(\'keras2onnx\')\n    if not logger.handlers:\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.WARNING)\n        logger.addHandler(ch)\n    logger.setLevel(logging.WARNING)\n    return logger\n\n\ndef set_logger_level(lvl):\n    logger = k2o_logger()\n    if logger.level != lvl:\n        logger.setLevel(lvl)\n        for h_ in logger.handlers:\n            h_.setLevel(lvl)\n\n\n@with_variable(\'batch_size\')\ndef get_default_batch_size():\n    return \'N\'\n\n\ndef count_dynamic_dim(shape):\n    num = 0\n    for s_ in shape:\n        if isinstance(s_, int) and s_ >= 0:\n            num += 1\n    return len(shape) - num\n\n\ndef get_producer():\n    """"""\n    Internal helper function to return the producer\n    """"""\n    from .. import __producer__\n    return __producer__\n\n\ndef get_producer_version():\n    """"""\n    Internal helper function to return the producer version\n    """"""\n    from .. import __producer_version__\n    return __producer_version__\n\n\ndef get_domain():\n    """"""\n    Internal helper function to return the model domain\n    """"""\n    from .. import __domain__\n    return __domain__\n\n\ndef get_model_version():\n    """"""\n    Internal helper function to return the model version\n    """"""\n    from .. import __model_version__\n    return __model_version__\n'"
keras2onnx/ke2onnx/__init__.py,0,b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom .main import keras_layer_to_operator\nfrom .main import static_set_ke2onnx_converters\nfrom .layer_spec import keras_layer_spec\n'
keras2onnx/ke2onnx/activation.py,17,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy as np\nimport tensorflow as tf\nfrom ..proto import keras, is_tf_keras\nfrom ..common.onnx_ops import apply_elu, apply_hard_sigmoid, apply_leaky_relu, apply_relu, apply_relu_6, \\\n    apply_tanh, apply_softmax, apply_identity, apply_selu, apply_mul, apply_prelu, apply_sigmoid\nfrom onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\n\nactivation_get = keras.activations.get\n\nrelu6 = None\nif not is_tf_keras:\n    try:\n        from keras_applications.mobilenet import relu6\n    except ImportError:\n        pass\nif not relu6 and hasattr(keras.applications.mobilenet, \'relu6\'):\n    relu6 = keras.applications.mobilenet.relu6\n\n\ndef apply_leaky_relu_keras(scope, input_name, output_name, container, operator_name=None, alpha=0.2):\n    apply_leaky_relu(scope, input_name, output_name, container, operator_name, alpha)\n\n\nactivation_map = {activation_get(\'sigmoid\'): apply_sigmoid,\n                  activation_get(\'softmax\'): apply_softmax,\n                  activation_get(\'linear\'): apply_identity,\n                  activation_get(\'relu\'): apply_relu,\n                  activation_get(\'elu\'): apply_elu,\n                  activation_get(\'selu\'): apply_selu,\n                  activation_get(\'tanh\'): apply_tanh,\n                  activation_get(\'hard_sigmoid\'): apply_hard_sigmoid,\n                  tf.nn.leaky_relu: apply_leaky_relu_keras,\n                  tf.nn.sigmoid: apply_sigmoid,\n                  tf.nn.softmax: apply_softmax,\n                  tf.nn.relu: apply_relu,\n                  tf.nn.relu6: apply_relu_6,\n                  tf.nn.elu: apply_elu,\n                  tf.nn.selu: apply_selu,\n                  tf.nn.tanh: apply_tanh}\n\nif hasattr(tf.compat, \'v1\'):\n    activation_map.update({tf.compat.v1.nn.sigmoid: apply_sigmoid})\n    activation_map.update({tf.compat.v1.nn.softmax: apply_softmax})\n    activation_map.update({tf.compat.v1.nn.leaky_relu: apply_leaky_relu_keras})\n    activation_map.update({tf.compat.v1.nn.relu: apply_relu})\n    activation_map.update({tf.compat.v1.nn.relu6: apply_relu_6})\n    activation_map.update({tf.compat.v1.nn.elu: apply_elu})\n    activation_map.update({tf.compat.v1.nn.selu: apply_selu})\n    activation_map.update({tf.compat.v1.nn.tanh: apply_tanh})\n\n\ndef convert_keras_activation(scope, operator, container):\n    input_name = operator.input_full_names[0]\n    output_name = operator.output_full_names[0]\n    activation = operator.raw_operator.activation\n    activation_type = type(activation)\n    if activation in [activation_get(\'sigmoid\'), keras.activations.sigmoid]:\n        apply_sigmoid(scope, input_name, output_name, container)\n    elif activation in [activation_get(\'tanh\'), keras.activations.tanh]:\n        apply_tanh(scope, input_name, output_name, container)\n    elif activation in [activation_get(\'relu\'), keras.activations.relu] or \\\n            (hasattr(keras.layers.advanced_activations, \'ReLU\') and\n             activation_type == keras.layers.advanced_activations.ReLU):\n        apply_relu(scope, input_name, output_name, container)\n    elif activation in [activation_get(\'softmax\'), keras.activations.softmax] or \\\n            activation_type == keras.layers.advanced_activations.Softmax:\n        apply_softmax(scope, input_name, output_name, container, axis=-1)\n    elif activation in [activation_get(\'elu\'), keras.activations.elu] or \\\n            activation_type == keras.layers.advanced_activations.ELU:\n        apply_elu(scope, input_name, output_name, container, alpha=1.0)\n    elif activation in [activation_get(\'hard_sigmoid\'), keras.activations.hard_sigmoid]:\n        apply_hard_sigmoid(scope, input_name, output_name, container, alpha=0.2, beta=0.5)\n    elif activation in [activation_get(\'linear\'), keras.activations.linear]:\n        apply_identity(scope, input_name, output_name, container)\n    elif activation in [activation_get(\'selu\'), keras.activations.selu]:\n        apply_selu(scope, input_name, output_name, container, alpha=1.673263, gamma=1.050701)\n    elif activation_type == keras.layers.advanced_activations.LeakyReLU:\n        apply_leaky_relu(scope, input_name, output_name, container, alpha=activation.alpha.item(0))\n    elif activation_type == keras.layers.advanced_activations.PReLU:\n        apply_prelu(scope, input_name, output_name, container, slope=operator.raw_operator.get_weights()[0])\n    elif activation in [relu6] or (hasattr(activation, \'__name__\') and activation.__name__ == \'relu6\'):\n        # relu6(x) = min(relu(x), 6)\n        np_type = TENSOR_TYPE_TO_NP_TYPE[operator.inputs[0].type.to_onnx_type().tensor_type.elem_type]\n        zero_value = np.zeros(shape=(1,), dtype=np_type)\n        apply_relu_6(scope, input_name, output_name, container,\n                     zero_value=zero_value)\n    elif hasattr(activation, \'__name__\') and activation.__name__ == \'swish\':\n        apply_sigmoid(scope, input_name, output_name + \'_sig\', container)\n        apply_mul(scope, [input_name, output_name + \'_sig\'], output_name, container)\n    else:\n        if activation in [activation_get(\'softsign\'), keras.activations.softsign]:\n            op_type = \'Softsign\'\n        elif activation in [activation_get(\'softplus\'), keras.activations.softplus]:\n            op_type = \'Softplus\'\n        else:\n            raise RuntimeError(""Unsupported activation method within Activation layer \'{}\'"".format(activation))\n\n        container.add_node(op_type, operator.input_full_names, operator.output_full_names, name=operator.full_name)\n'"
keras2onnx/ke2onnx/adv_activation.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom ..proto import keras, is_keras_older_than\nfrom ..common.onnx_ops import apply_elu, apply_leaky_relu, apply_prelu, apply_thresholded_relu, apply_clip\n\n\nactivations = keras.layers.advanced_activations\n\n\ndef convert_keras_advanced_activation(scope, operator, container):\n    op = operator.raw_operator\n    if isinstance(op, activations.LeakyReLU):\n        alpha = op.get_config()['alpha']\n        apply_leaky_relu(scope, operator.input_full_names[0], operator.output_full_names[0], container,\n                         operator_name=operator.full_name, alpha=alpha)\n    elif isinstance(op, activations.ELU):\n        alpha = op.get_config()['alpha']\n        apply_elu(scope, operator.input_full_names[0], operator.output_full_names[0], container,\n                  operator_name=operator.full_name, alpha=alpha)\n    elif isinstance(op, activations.PReLU):\n        weights = op.get_weights()[0]\n        apply_prelu(scope, operator.input_full_names[0], operator.output_full_names[0], container,\n                    operator_name=operator.full_name, slope=weights)\n    elif isinstance(op, activations.ThresholdedReLU):\n        alpha = op.get_config()['theta']\n        apply_thresholded_relu(scope, operator.input_full_names[0], operator.output_full_names[0], container,\n                               operator_name=operator.full_name, alpha=[alpha])\n    else:\n        attrs = {'name': operator.full_name}\n        ver_opset = 6\n        input_tensor_names = [operator.input_full_names[0]]\n        if not is_keras_older_than('2.1.3') and \\\n                isinstance(op, activations.Softmax):\n            op_type = 'Softmax'\n            attrs['axis'] = op.get_config()['axis']\n        elif not is_keras_older_than('2.2.0') and \\\n                isinstance(op, activations.ReLU):\n            apply_clip(scope, operator.input_full_names[0], operator.output_full_names[0], container,\n                       operator_name=operator.full_name+'_clip', max=op.max_value, min=op.threshold)\n            return\n        else:\n            raise RuntimeError('Unsupported advanced layer found %s' % type(op))\n\n        container.add_node(op_type, input_tensor_names, operator.output_full_names, op_version=ver_opset, **attrs)\n"""
keras2onnx/ke2onnx/batch_norm.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy as np\nfrom ..common.onnx_ops import apply_batch_norm, apply_transpose\nfrom ..proto import onnx_proto\n\n\ndef convert_keras_batch_normalization(scope, operator, container):\n    op = operator.raw_operator\n    shape_len = len(operator.get_input_shape())\n\n    if isinstance(op.axis, list):\n        if len(op.axis) == 1:\n            axis = op.axis[0]\n        else:\n            raise AttributeError('No support for more than one axis in: ' + operator.full_name)\n    else:\n        axis = op.axis\n\n    skip_transpose = (axis != shape_len - 1 and axis != -1) or shape_len <= 2\n    if not skip_transpose:\n        perm_1 = list(range(1, shape_len - 1))\n        perm_1 = [0, shape_len - 1] + perm_1\n        perm_2 = list(range(2, shape_len))\n        perm_2 = [0] + perm_2 + [1]\n\n    if skip_transpose:\n        adjusted_input_name = operator.inputs[0].full_name\n    else:\n        adjusted_input_name = scope.get_unique_variable_name(operator.inputs[0].full_name + '_transposed')\n        apply_transpose(scope, operator.inputs[0].full_name, adjusted_input_name, container, perm=perm_1)\n\n    input_tensor_names = [adjusted_input_name]\n\n    params = op.get_weights()\n    # If scale and/or center flag is set in keras node, use keras default values for gamma and/or beta\n    if not op.scale:\n        params.insert(0, np.ones(params[0].shape, dtype=float))\n    if not op.center:\n        params.insert(1, np.zeros(params[1].shape, dtype=float))\n\n    gamma = params[0] / np.sqrt(params[3] + op.epsilon)\n    beta = params[1] - params[0] * params[2] / np.sqrt(params[3] + op.epsilon)\n\n    scale_tensor_name = scope.get_unique_variable_name('scale')\n    container.add_initializer(scale_tensor_name, onnx_proto.TensorProto.FLOAT, params[0].shape, gamma)\n    input_tensor_names.append(scale_tensor_name)\n\n    bias_tensor_name = scope.get_unique_variable_name('bias')\n    container.add_initializer(bias_tensor_name, onnx_proto.TensorProto.FLOAT, params[1].shape, beta)\n    input_tensor_names.append(bias_tensor_name)\n\n    mean_tensor_name = scope.get_unique_variable_name('mean')\n    container.add_initializer(mean_tensor_name, onnx_proto.TensorProto.FLOAT, params[2].shape, 0 * params[2])\n    input_tensor_names.append(mean_tensor_name)\n\n    var_tensor_name = scope.get_unique_variable_name('var')\n    container.add_initializer(var_tensor_name, onnx_proto.TensorProto.FLOAT, params[3].shape, 1 + 0 * params[3])\n    input_tensor_names.append(var_tensor_name)\n\n    epsilon = op.epsilon * 1e-3  # We use a much smaller epsilon because the original epsilon is absorbed in gamma\n    is_test = 1\n    momentum = op.momentum\n    spatial = 1\n\n    if skip_transpose:\n        # If no transpose is required, we can simply use the output of ONNX BatchNorm as the final outcome\n        apply_batch_norm(scope, input_tensor_names, operator.output_full_names[0:5], container,\n                         operator_name=operator.full_name, epsilon=epsilon, is_test=is_test,\n                         momentum=momentum, spatial=spatial)\n    else:\n        # If transpose is required, we need to put BatchNorm's output to an intermediate tensor for applying a transpose\n        intermediate_output_name = scope.get_unique_variable_name('batch_norm_output_buffer')\n        apply_batch_norm(scope, input_tensor_names, intermediate_output_name, container,\n                         operator_name=operator.full_name, epsilon=epsilon, is_test=is_test,\n                         momentum=momentum, spatial=spatial)\n\n        # For 4D case, this is to permute [N,C,H,W] to [N,H,W,C]\n        apply_transpose(scope, intermediate_output_name, operator.outputs[0].full_name, container, perm=perm_2)\n"""
keras2onnx/ke2onnx/bidirectional.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport collections\nimport numbers\nfrom ..common import cvtfunc\nfrom ..proto import keras\nfrom . import simplernn, gru, lstm\n\n\ndef _calculate_keras_bidirectional_output_shapes(operator):\n    op = operator.raw_operator\n    if isinstance(op.output_shape[0], collections.abc.Iterable):\n        operator.outputs[0].type.shape = list(i if isinstance(i, numbers.Integral) else None\n                                              for i in op.output_shape[0])\n        if op.merge_mode is None:\n            operator.outputs[1].type.shape = list(i if isinstance(i, numbers.Integral) else None\n                                                  for i in op.output_shape[1])\n    else:\n        operator.outputs[0].type.shape = list(i if isinstance(i, numbers.Integral) else None for i in op.output_shape)\n\n\n@cvtfunc(shape_infer=_calculate_keras_bidirectional_output_shapes)\ndef convert_bidirectional(scope, operator, container):\n    op_type = type(operator.raw_operator.forward_layer)\n    bidirectional = True\n\n    if op_type == keras.layers.LSTM:\n        lstm.convert_keras_lstm(scope, operator, container, bidirectional)\n    elif op_type == keras.layers.GRU:\n        gru.convert_keras_gru(scope, operator, container, bidirectional)\n    elif op_type == keras.layers.SimpleRNN:\n        simplernn.convert_keras_simple_rnn(scope, operator, container, bidirectional)\n    else:\n        raise ValueError('Unsupported class for Bidirectional layer: {}'.format(op_type))\n"""
keras2onnx/ke2onnx/common.py,1,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\n\nfrom ..proto import keras\nfrom ..proto.tfcompat import tensorflow as tf\nfrom ..common.onnx_ops import apply_relu_6, apply_softmax\nfrom .activation import activation_map\nfrom onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\nimport numpy as np\nactivation_get = keras.activations.get\n\n\ndef get_permutation_config(n_dims):\n    input_perm_axes = [0, n_dims + 1] + list(range(1, n_dims + 1))\n    output_perm_axes = [0] + list(range(2, n_dims + 2)) + [1]\n    return input_perm_axes, output_perm_axes\n\n\ndef activation_process(scope, operator, container, biased_tensor_name):\n    # Create an activation function node and apply activation function to the intermediate tensor\n    apply_activation_function = activation_map[operator.raw_operator.activation]\n    if operator.raw_operator.activation in [activation_get('softmax'), keras.activations.softmax]:\n        apply_softmax(scope, biased_tensor_name, operator.outputs[0].full_name, container, axis=-1)\n    elif operator.raw_operator.activation in [tf.nn.relu6]:\n        np_type = TENSOR_TYPE_TO_NP_TYPE[operator.inputs[0].type.to_onnx_type().tensor_type.elem_type]\n        zero_value = np.zeros(shape=(1,), dtype=np_type)\n        apply_relu_6(scope, biased_tensor_name, operator.outputs[0].full_name, container,\n                     zero_value=zero_value)\n    else:\n        apply_activation_function(scope, biased_tensor_name, operator.outputs[0].full_name, container)\n"""
keras2onnx/ke2onnx/conv.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy\nfrom .common import activation_process\nfrom ..proto import keras\nfrom ..proto import onnx_proto\nfrom ..common.utils import count_dynamic_dim\nfrom ..common.onnx_ops import (apply_identity, apply_pad,\n                               apply_transpose, apply_mul, apply_sigmoid)\n\nactivation_get = keras.activations.get\nSeparableConv2D = keras.layers.SeparableConv2D\nDepthwiseConv2D = keras.layers.DepthwiseConv2D if \\\n    hasattr(keras.layers, \'DepthwiseConv2D\') else None\nSeparableConv1D = keras.layers.SeparableConv1D if \\\n    hasattr(keras.layers, \'SeparableConv1D\') else None\n\n\ndef _calc_explicit_padding(input_size, output_shape, is_transpose, output_padding, kernel_shape, stride, dilation,\n                           perm):\n    def to_nchw(x, perm):\n        return [x[perm[n_]] for n_ in range(len(x))]\n    input_size = to_nchw(input_size, perm)[2:]\n    output_shape = to_nchw(output_shape, perm)[2:]\n\n    spatial = len(kernel_shape)\n    total_padding = []\n    pads = [None] * 2 * spatial\n    for i in range(spatial):\n        # padding is calculated differently for Conv and ConvTranspose\n        if is_transpose:\n            total_padding[i:] = [stride[i] * (output_shape[i] - 1) +\n                                 output_padding[i] + kernel_shape[i] * dilation[i] - input_size[i]]\n        else:\n            total_padding[i:] = [stride[i] * ((input_size[i] - 1) // stride[i]) + 1 +\n                                 output_padding[i] + (kernel_shape[i] - 1) * dilation[i] - input_size[i]]\n        total_padding[i] = max(total_padding[i], 0)\n        pads[i] = total_padding[i] // 2\n        pads[i + spatial] = total_padding[i] - (total_padding[i] // 2)\n\n    return pads\n\n\ndef process_separable_conv_2nd(scope, operator, container, convolution_input_names, n_dims,\n                               weight_perm_axes, parameters, auto_pad):\n    attrs = {\'name\': operator.full_name + \'1\'}\n\n    weight_tensor_name = scope.get_unique_variable_name(\'W\')\n    weight_params = parameters[1].transpose(weight_perm_axes)\n    container.add_initializer(weight_tensor_name, onnx_proto.TensorProto.FLOAT,\n                              weight_params.shape, weight_params.flatten())\n    convolution_input_names.append(weight_tensor_name)\n\n    if len(parameters) == 3:\n        bias_tensor_name = scope.get_unique_variable_name(\'B\')\n        container.add_initializer(bias_tensor_name, onnx_proto.TensorProto.FLOAT,\n                                  parameters[2].shape, parameters[2].flatten())\n        convolution_input_names.append(bias_tensor_name)\n\n    all_ones = numpy.ones(n_dims, numpy.int8)\n    attrs[\'dilations\'] = all_ones\n    attrs[\'strides\'] = all_ones\n    attrs[\'kernel_shape\'] = all_ones\n    attrs[\'group\'] = 1\n    attrs[\'auto_pad\'] = auto_pad\n\n    intermediate_output_name = scope.get_unique_variable_name(\'convolution_output\')\n    container.add_node(\'Conv\', convolution_input_names,\n                       intermediate_output_name, **attrs)\n    return intermediate_output_name\n\n\ndef convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm_axes,\n                            output_perm_axes, weight_perm_axes):\n    op = operator.raw_operator\n\n    is_separable_conv = isinstance(op, SeparableConv2D) or isinstance(op, SeparableConv1D)\n\n    channels_first = n_dims > 1 and op.data_format == \'channels_first\'\n\n    # Unless channels_first is the Keras data format, the inputs and weights in Keras v.s. ONNX\n    # are reversed. This is annoying, and inefficient as we\'ll have to use transposes.\n    if channels_first:\n        adjusted_input_name = operator.inputs[0].full_name\n    else:\n        adjusted_input_name = scope.get_unique_variable_name(\'adjusted_input\')\n        apply_transpose(scope, operator.inputs[0].full_name, adjusted_input_name, container, perm=input_perm_axes)\n\n    op_type = \'ConvTranspose\' if is_transpose else \'Conv\'\n    convolution_input_names = [adjusted_input_name]\n    parameters = op.get_weights()\n\n    if is_separable_conv:\n        attrs = {\'name\': operator.full_name + \'0\'}\n        assert (len(parameters) == 3 if op.use_bias else 2)\n    else:\n        attrs = {\'name\': operator.full_name}\n        assert (len(parameters) == 2 if op.use_bias else 1)\n\n    weight_params = parameters[0]\n\n    input_channels, output_channels = weight_params.shape[-2:]\n    kernel_size = weight_params.shape[:-2]\n    assert (kernel_size == op.kernel_size)\n\n    if isinstance(op, DepthwiseConv2D):\n        # see https://github.com/onnx/onnx-tensorflow/pull/266/files\n        dm = op.depth_multiplier\n        output_channels *= dm\n        group = input_channels\n        shape = weight_params.shape\n        # weight_params = weight_params.transpose(weight_perm_axes)\n        new_shape = shape[:2] + (1, shape[2] * shape[3])\n        weight_params = numpy.reshape(weight_params, new_shape)\n        weight_params = weight_params.transpose(weight_perm_axes)\n    elif is_separable_conv:\n        group = weight_params.shape[-2]\n        shape = weight_params.shape\n        new_shape = shape[:-2] + (1, shape[-2] * shape[-1])\n        weight_params = numpy.reshape(weight_params, new_shape).transpose(weight_perm_axes)\n    else:\n        weight_params = weight_params.transpose(weight_perm_axes)\n        group = 1\n\n    weight_tensor_name = container.add_initializer_by_name(scope, op.weights[0].name, onnx_proto.TensorProto.FLOAT,\n                                                           weight_params.shape, weight_params.flatten())\n    convolution_input_names.append(weight_tensor_name)\n\n    if len(parameters) == 2 and not is_separable_conv:\n        bias_tensor_name = container.add_initializer_by_name(scope, op.weights[1].name, onnx_proto.TensorProto.FLOAT,\n                                                             parameters[1].shape, parameters[1].flatten())\n        convolution_input_names.append(bias_tensor_name)\n\n    attrs[\'dilations\'] = list(op.dilation_rate)\n    attrs[\'strides\'] = list(op.strides)\n    attrs[\'kernel_shape\'] = op.kernel_size\n    attrs[\'group\'] = group\n\n    input_shape = operator.get_input_shape()\n    output_shape = operator.get_output_shape()\n    padded_result = None\n\n    if op.padding == \'valid\':\n        attrs[\'auto_pad\'] = \'VALID\'\n    elif op.padding == \'same\':\n        if count_dynamic_dim(input_shape) > 1:\n            if is_transpose:\n                attrs[\'auto_pad\'] = \'SAME_LOWER\'  # the controversial def in onnx spec.\n            else:\n                attrs[\'auto_pad\'] = \'SAME_UPPER\'\n        else:\n            attrs[\'auto_pad\'] = \'NOTSET\'\n            output_padding = [0] * len(op.kernel_size)\n            if hasattr(op, \'output_padding\') and op.output_padding is not None:\n                output_padding = op.output_padding\n            attrs[\'pads\'] = _calc_explicit_padding(output_shape if is_transpose else input_shape,\n                                                   input_shape if is_transpose else output_shape,\n                                                   is_transpose,\n                                                   output_padding,\n                                                   op.kernel_size,\n                                                   op.strides,\n                                                   op.dilation_rate,\n                                                   list(range(\n                                                       len(input_shape))) if channels_first else input_perm_axes)\n    elif op.padding == \'causal\':\n        assert n_dims == 1\n        attrs[\'auto_pad\'] = \'VALID\'\n        left_pad = op.dilation_rate[0] * (op.kernel_size[0] - 1)\n        padded_result = scope.get_unique_variable_name(\'padded_result\')\n        apply_pad(scope, convolution_input_names[0], padded_result, container, pads=[0, 0, left_pad, 0, 0, 0], value=0.)\n    else:\n        raise RuntimeError(""Unsupported padding type \'{}\'"".format(op.padding))\n\n    intermediate_output_name = scope.get_unique_variable_name(\'convolution_output\')\n    if padded_result:\n        container.add_node(op_type, [padded_result, convolution_input_names[1]],\n                           intermediate_output_name, **attrs)\n    else:\n        container.add_node(op_type, convolution_input_names,\n                           intermediate_output_name, **attrs)\n\n    if is_separable_conv:\n        intermediate_output_name = process_separable_conv_2nd(scope, operator, container, [intermediate_output_name],\n                                                              n_dims,\n                                                              weight_perm_axes, parameters, attrs[\'auto_pad\'])\n\n    # Permute the output back of its original format\n    transpose_output_name = scope.get_unique_variable_name(\'transpose_output\')\n    if not channels_first:\n        # Generate a final transposer.\n        apply_transpose(scope, intermediate_output_name, transpose_output_name, container, perm=output_perm_axes)\n    else:\n        apply_identity(scope, intermediate_output_name, transpose_output_name, container)\n\n    # The construction of convolution is done. Now, we create an activation operator to apply the activation specified\n    # in this Keras layer.\n    if hasattr(op.activation, \'__name__\') and op.activation.__name__ == \'swish\':\n        apply_sigmoid(scope, transpose_output_name, transpose_output_name + \'_sig\', container)\n        apply_mul(scope, [transpose_output_name, transpose_output_name + \'_sig\'], operator.outputs[0].full_name,\n                  container)\n    else:\n        activation_process(scope, operator, container, transpose_output_name)\n\n\ndef get_converter_config(dims, is_conv_transpose):\n    assert (dims in [1, 2, 3])\n    input_perm = [0, dims + 1] + list(range(1, dims + 1))\n    output_perm = [0] + list(range(2, dims + 2)) + [1]\n    weight_perm = [dims + 1, dims] + list(range(dims))\n    return is_conv_transpose, dims, input_perm, output_perm, weight_perm\n\n\ndef convert_keras_conv1d(scope, operator, container):\n    is_transpose, n_dims, input_perm, output_perm, weight_perm = get_converter_config(1, False)\n    convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm, output_perm, weight_perm)\n\n\ndef convert_keras_conv2d(scope, operator, container):\n    is_transpose, n_dims, input_perm, output_perm, weight_perm = get_converter_config(2, False)\n    convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm, output_perm, weight_perm)\n\n\ndef convert_keras_depthwise_conv_2d(scope, operator, container):\n    is_transpose, n_dims, input_perm, output_perm, weight_perm = get_converter_config(2, False)\n    convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm, output_perm, weight_perm)\n\n\ndef convert_keras_conv3d(scope, operator, container):\n    is_transpose, n_dims, input_perm, output_perm, weight_perm = get_converter_config(3, False)\n    convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm, output_perm, weight_perm)\n\n\ndef convert_keras_conv_transpose_2d(scope, operator, container):\n    is_transpose, n_dims, input_perm, output_perm, weight_perm = get_converter_config(2, True)\n    convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm, output_perm, weight_perm)\n\n\ndef convert_keras_conv_transpose_3d(scope, operator, container):\n    is_transpose, n_dims, input_perm, output_perm, weight_perm = get_converter_config(3, True)\n    convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm, output_perm, weight_perm)\n\n\ndef convert_keras_separable_conv1d(scope, operator, container):\n    is_transpose, n_dims, input_perm, output_perm, weight_perm = get_converter_config(1, False)\n    convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm, output_perm, weight_perm)\n\n\ndef convert_keras_separable_conv2d(scope, operator, container):\n    is_transpose, n_dims, input_perm, output_perm, weight_perm = get_converter_config(2, False)\n    convert_keras_conv_core(scope, operator, container, is_transpose, n_dims, input_perm, output_perm, weight_perm)\n'"
keras2onnx/ke2onnx/crop.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy as np\nfrom ..proto import keras\nfrom ..common.onnx_ops import apply_transpose, apply_identity, OnnxOperatorBuilder\nfrom .common import get_permutation_config\n\n\ndef convert_keras_crop_v1(scope, operator, container, n_dims):\n    op = operator.raw_operator\n    op_type = 'Crop'\n    attrs = {'name': operator.full_name}\n\n    input_perm_axes, output_perm_axes = get_permutation_config(n_dims)\n    channels_first = n_dims > 1 and op.data_format == 'channels_first'\n\n    # Before creating the main Crop operator, we need to permute the input tensor if the original operator is working\n    # under channels_last mode.\n    if channels_first:\n        input_tensor_name = operator.inputs[0].full_name\n    else:\n        input_tensor_name = scope.get_unique_variable_name(operator.inputs[0].full_name + '_permuted')\n        apply_transpose(scope, operator.inputs[0].full_name, input_tensor_name, container, perm=input_perm_axes)\n\n    param = op.cropping\n    if isinstance(param, int):\n        param = [param, param]\n\n    if len(param) == 2:\n        if isinstance(param[0], int):\n            attrs['scale'] = param\n        elif len(param[0]) == 2 and len(param[1]) == 2:\n            # If tuple of 2 tuples of 2 ints: interpreted as ((top_crop, bottom_crop), (left_crop, right_crop))\n            top = param[0][0]\n            bottom = param[0][1]\n            left = param[1][0]\n            right = param[1][1]\n            attrs['border'] = [left, top, right, bottom]\n        else:\n            raise RuntimeError('Unknown crop parameter %s in CroppingLayer' % str(param))\n    else:\n        raise RuntimeError('Unknown crop parameter %s in CroppingLayer' % str(param))\n\n    if not channels_first:\n        cropped_tensor_name = scope.get_unique_variable_name(input_tensor_name + '_cropped')\n        container.add_node(op_type, input_tensor_name, cropped_tensor_name, **attrs)\n        apply_transpose(scope, cropped_tensor_name, operator.outputs[0].full_name, container, perm=output_perm_axes)\n    else:\n        container.add_node(op_type, input_tensor_name, operator.outputs[0].full_name, **attrs)\n\n\ndef convert_keras_crop_v9(scope, operator, container, n_dims):\n    op = operator.raw_operator\n    channels_first = n_dims > 1 and op.data_format == 'channels_first'\n    param = op.cropping\n\n    if isinstance(param, int):\n        param = [param, param]\n\n    ori_shape = list(op.input_shape[1:])\n    if isinstance(op, keras.layers.Cropping1D):\n        if isinstance(param[0], int):  # tuple of ints\n            start_border = [param[0], 0]\n            end_border = [param[1], 0]\n        else:  # tuple of type of ints\n            start_border = [param[0][0], 0]\n            end_border = [param[0][1], 0]\n        axes_v = [0, 1, 2]\n    elif isinstance(op, keras.layers.Cropping2D):\n        axes_v = [0, 1, 2, 3]\n        if isinstance(param[0], int):  # tuple of ints\n            start_border = [param[0]] * 2\n            end_border = start_border\n        else:  # tuple of type of ints\n            start_border = [param[0][0], param[1][0]]\n            end_border = [param[0][1], param[1][1]]\n        if channels_first:\n            start_border = [0] + start_border\n            end_border = [0] + end_border\n        else:\n            start_border = start_border + [0]\n            end_border = end_border + [0]\n    else:\n        axes_v = [0, 1, 2, 3, 4]\n        if isinstance(param[0], int):  # tuple of ints\n            start_border = [param[0]] * 3\n            end_border = start_border\n        else:  # tuple of type of ints\n            start_border = [param[0][0], param[1][0], param[2][0]]\n            end_border = [param[0][1], param[1][1], param[2][1]]\n        if channels_first:\n            start_border = [0] + start_border\n            end_border = [0] + end_border\n        else:\n            start_border = start_border + [0]\n            end_border = end_border + [0]\n\n    start_v = [0] + start_border\n    end_v = [np.iinfo(np.int32).max] + list(np.array(ori_shape) - np.array(end_border))\n\n    oopb = OnnxOperatorBuilder(container, scope)\n\n    cropped_tensor_name = oopb.add_node('Slice' if container.target_opset >= 10 else 'DynamicSlice',\n                                        [operator.inputs[0].full_name,\n                                         ('_start', oopb.int64, np.array(start_v, dtype='int64')),\n                                         ('_end', oopb.int64, np.array(end_v, dtype='int64')),\n                                         ('_axes', oopb.int64, np.array(axes_v, dtype='int64'))\n                                         ],\n                                        operator.inputs[0].full_name + '_cropping')\n    apply_identity(scope, cropped_tensor_name, operator.outputs[0].full_name, container)\n\n\ndef convert_keras_crop(scope, operator, container, n_dims):\n    if container.target_opset >= 9:\n        convert_keras_crop_v9(scope, operator, container, n_dims)\n    else:\n        convert_keras_crop_v1(scope, operator, container, n_dims)\n\n\ndef convert_keras_crop_1d(scope, operator, container):\n    convert_keras_crop(scope, operator, container, n_dims=1)\n\n\ndef convert_keras_crop_2d(scope, operator, container):\n    convert_keras_crop(scope, operator, container, n_dims=2)\n\n\ndef convert_keras_crop_3d(scope, operator, container):\n    convert_keras_crop(scope, operator, container, n_dims=3)\n"""
keras2onnx/ke2onnx/dense.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy as np\nfrom .common import activation_process\nfrom ..proto import onnx_proto, keras\nfrom ..common.onnx_ops import apply_add, OnnxOperatorBuilder\nactivation_get = keras.activations.get\n\n\ndef convert_keras_dense(scope, operator, container):\n    op = operator.raw_operator\n    parameters = op.get_weights()\n\n    # Allocate weight matrix\n    weight = parameters[0]\n    weight_name = container.add_initializer_by_name(scope, op.weights[0].name, onnx_proto.TensorProto.FLOAT,\n                                                    weight.shape, weight.flatten())\n\n    # Do a numpy matmul. If the input is 2-D, it will be a standard matrix multiplication. Otherwise, it follows Numpy's\n    # matmul behavior.\n    oopb = OnnxOperatorBuilder(container, scope)\n    transformed_tensor_name = oopb.apply_matmul([operator.inputs[0].full_name, weight_name],\n                                                name=operator.raw_operator.name)\n\n    # Allocate bias vector\n    if len(parameters) == 1:\n        bias = np.zeros((weight.shape[1],), dtype=np.float32)\n        bias_name = scope.get_unique_variable_name('B')\n        container.add_initializer(bias_name, onnx_proto.TensorProto.FLOAT, bias.shape, bias.flatten())\n    else:\n        bias = parameters[1]\n        bias_name = container.add_initializer_by_name(scope, op.weights[1].name, onnx_proto.TensorProto.FLOAT,\n                                                      bias.shape, bias.flatten())\n\n    # Add bias\n    biased_tensor_name = scope.get_unique_variable_name('biased_tensor_name')\n    apply_add(scope, transformed_tensor_name + [bias_name], biased_tensor_name, container,\n              axis=-1, broadcast=1)\n\n    activation_process(scope, operator, container, biased_tensor_name)\n"""
keras2onnx/ke2onnx/dot.py,1,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numbers\nimport numpy as np\n\nfrom ..proto import is_keras_later_than, is_tf_keras\nfrom ..proto.tfcompat import is_tf2\nfrom ..common import cvtfunc\nfrom ..common.onnx_ops import OnnxOperatorBuilder\n\n\n# There is a breaking logic change for keras tensorflow_backend batch_dot after keras 2.2.4\n# Assume input shape is (2, 3, 4, 12, 3) and (2, 3, 4, 15, 3), with axes 4\n# For keras 2.2.4 and before, the output shape is (2, 3, 4, 12, 15)\n# After that, the output shape is (2, 3, 4, 12, 3, 4, 15)\n# See https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py for details.\ndef _calculate_keras_dot_output_shapes(operator):\n    if not is_keras_later_than(""2.2.4""):\n        op = operator.raw_operator\n        shape = []\n        for i in op.output.shape:\n            n = i if is_tf2 else i.value\n            if isinstance(n, numbers.Integral):\n                shape.append(n)\n            else:\n                shape.append(None)\n        operator.outputs[0].type.shape = shape\n\n\ndef _preprocessing(op):\n    if len(op.input_shape) > 2:\n        raise RuntimeError(\'Unsupported number of input = %s > 2\' % len(op.input_shape))\n    x_shape = op.input_shape[0]\n    y_shape = op.input_shape[1]\n    x_shape = [x_ if x_ is not None else -1 for x_ in x_shape]\n    y_shape = [y_ if y_ is not None else -1 for y_ in y_shape]\n    x_shape = np.asarray(x_shape, dtype=np.int64)\n    y_shape = np.asarray(y_shape, dtype=np.int64)\n\n    x_ndim = len(x_shape)\n    y_ndim = len(y_shape)\n    x_batch_size = x_shape[0]\n    y_batch_size = y_shape[0]\n\n    if x_batch_size != y_batch_size:\n        raise RuntimeError(\n            \'Can not do batch_dot on inputs with different batch sizes.\' + str(x_shape) + \' and \' + str(y_shape))\n\n    return x_ndim, y_ndim, x_shape, y_shape\n\n\ndef convert_keras_dot_224(scope, operator, container):\n    op = operator.raw_operator\n    x_ndim, y_ndim, _, _ = _preprocessing(op)\n    oopb = OnnxOperatorBuilder(container, scope)\n\n    axes = op.axes\n    if isinstance(axes, int):\n        axes = (axes, axes)\n    if axes is None:\n        # behaves like tf.batch_matmul as default\n        axes = [x_ndim - 1, y_ndim - 2]\n    if any([isinstance(a, (list, tuple)) for a in axes]):\n        raise ValueError(\'Multiple target dimensions are not supported. \' +\n                         \'Expected: None, int, (int, int), \' +\n                         \'Provided: \' + str(axes))\n\n    normalized_input_names = []\n    if op.normalize:\n        for i_, tensor_name in enumerate(operator.input_full_names):\n            normalized_tensor_name = oopb.apply_normalization(tensor_name, name=operator.full_name + \'_normalize\',\n                                                              axis=axes[i_])\n            normalized_input_names += normalized_tensor_name\n    else:\n        normalized_input_names = operator.input_full_names\n\n    if x_ndim > y_ndim:\n        diff = x_ndim - y_ndim\n        y_shape_node = oopb.add_node(\'Shape\',\n                                     [normalized_input_names[1]],\n                                     operator.inputs[0].full_name + \'_y_shape\')\n        y_shape_concat = oopb.add_node(\'Concat\',\n                                       [y_shape_node,\n                                        (\'_ones\', oopb.int64, np.array([1] * (diff), dtype=\'int64\'))],\n                                       operator.inputs[0].full_name + \'_y_shape_concat\',\n                                       axis=0)\n        y_reshape = oopb.add_node(\'Reshape\',\n                                  [normalized_input_names[1], y_shape_concat],\n                                  operator.inputs[0].full_name + \'_y_reshape\')\n        x_reshape = normalized_input_names[0]\n    elif y_ndim > x_ndim:\n        diff = y_ndim - x_ndim\n        x_shape_node = oopb.add_node(\'Shape\',\n                                     [normalized_input_names[0]],\n                                     operator.inputs[0].full_name + \'_y_shape\')\n        x_shape_concat = oopb.add_node(\'Concat\',\n                                       [x_shape_node,\n                                        (\'_ones\', oopb.int64, np.array([1] * (diff), dtype=\'int64\'))],\n                                       operator.inputs[0].full_name + \'_x_shape_concat\',\n                                       axis=0)\n        x_reshape = oopb.add_node(\'Reshape\',\n                                  [normalized_input_names[0], x_shape_concat],\n                                  operator.inputs[0].full_name + \'_x_reshape\')\n        y_reshape = normalized_input_names[1]\n    else:\n        diff = 0\n        x_reshape = normalized_input_names[0]\n        y_reshape = normalized_input_names[1]\n\n    max_ndim = max([x_ndim, y_ndim])\n    if x_ndim == 2 and y_ndim == 2:\n        if axes[0] == axes[1]:\n            result_mul = oopb.add_node(\'Mul\',\n                                       [x_reshape, y_reshape],\n                                       operator.inputs[0].full_name + \'_result_mul\')\n            out = oopb.add_node(\'ReduceSum\',\n                                [result_mul],\n                                operator.inputs[0].full_name + \'_out\',\n                                axes=[axes[0]])\n        else:\n            x_transpose = oopb.add_node(\'Transpose\',\n                                        [x_reshape],\n                                        operator.inputs[0].full_name + \'_x_transpose\',\n                                        perm=[1, 0])\n            result_mul = oopb.add_node(\'Mul\',\n                                       [x_transpose, y_reshape],\n                                       operator.inputs[0].full_name + \'_result_mul\')\n            out = oopb.add_node(\'ReduceSum\',\n                                [result_mul],\n                                operator.inputs[0].full_name + \'_out\',\n                                axes=[axes[1]])\n    else:\n        if axes is not None:\n            adj_x = None if axes[0] == max_ndim - 1 else True\n            adj_y = True if axes[1] == max_ndim - 1 else None\n        else:\n            adj_x = None\n            adj_y = None\n\n        transpose_perm = list(range(max_ndim))\n        temp = transpose_perm[-1]\n        transpose_perm[-1] = transpose_perm[-2]\n        transpose_perm[-2] = temp\n\n        if adj_x:\n            x_transpose_2 = oopb.add_node(\'Transpose\',\n                                          [x_reshape],\n                                          operator.inputs[0].full_name + \'_x_transpose_2\',\n                                          perm=transpose_perm)\n        else:\n            x_transpose_2 = x_reshape\n        if adj_y:\n            y_transpose_2 = oopb.add_node(\'Transpose\',\n                                          [y_reshape],\n                                          operator.inputs[0].full_name + \'_y_transpose_2\',\n                                          perm=transpose_perm)\n        else:\n            y_transpose_2 = y_reshape\n        out = oopb.add_node(\'MatMul\',\n                            [x_transpose_2, y_transpose_2],\n                            operator.inputs[0].full_name + \'_out\')\n    matrix_len = max_ndim\n    if diff:\n        if x_ndim > y_ndim:\n            idx = x_ndim + y_ndim - 3\n        else:\n            idx = x_ndim - 1\n        out_squeeze = oopb.add_node(\'Squeeze\',\n                                    [out],\n                                    operator.inputs[0].full_name + \'_out_squeeze\',\n                                    axes=list(range(idx, idx + diff)))\n        matrix_len = matrix_len - diff\n    else:\n        out_squeeze = out\n\n    if matrix_len == 1:\n        out_expand = oopb.add_node(\'Unsqueeze\',\n                                   [out_squeeze],\n                                   operator.inputs[0].full_name + \'_out_expand\',\n                                   axes=[1])\n    else:\n        out_expand = out_squeeze\n    container.add_node(\'Identity\', out_expand, operator.output_full_names,\n                       name=scope.get_unique_operator_name(\'Identity\'))\n\n\ndef convert_keras_dot_post_224(scope, operator, container):\n    op = operator.raw_operator\n    x_ndim, y_ndim, x_shape, y_shape = _preprocessing(op)\n    oopb = OnnxOperatorBuilder(container, scope)\n\n    orig_x_ndim = x_ndim\n    orig_y_ndim = y_ndim\n\n    if isinstance(op.axes, int):\n        if op.axes < 0:\n            axes = [op.axes % len(x_shape), op.axes % len(y_shape)]\n        else:\n            axes = [op.axes] * 2\n    else:\n        axes = op.axes\n    a0, a1 = axes\n\n    normalized_input_names = []\n    if op.normalize:\n        for i_, tensor_name in enumerate(operator.input_full_names):\n            normalized_tensor_name = oopb.apply_normalization(tensor_name, name=operator.full_name, axis=axes[i_])\n            normalized_input_names += normalized_tensor_name\n    else:\n        normalized_input_names = operator.input_full_names\n\n    if x_shape[a0] != y_shape[a1]:\n        raise RuntimeError(\'Dimension incompatibility: %s != %s\' % (x_shape[axes[0]], y_shape[axes[1]]))\n\n    if x_ndim == 2:\n        x_expand = oopb.add_node(\'Unsqueeze\',\n                                 [normalized_input_names[0]],\n                                 operator.inputs[0].full_name + \'_expand\',\n                                 axes=[1])\n        a0 += 1\n        x_ndim += 1\n    else:\n        x_expand = normalized_input_names[0]\n\n    if y_ndim == 2:\n        y_expand = oopb.add_node(\'Unsqueeze\',\n                                 [normalized_input_names[1]],\n                                 operator.inputs[1].full_name + \'_expand\',\n                                 axes=[2])\n        y_ndim += 1\n    else:\n        y_expand = normalized_input_names[1]\n\n    # bring x\'s dimension to be reduced to last axis.\n    if a0 != x_ndim - 1:\n        pattern = list(range(x_ndim))\n        for i in range(a0, x_ndim - 1):\n            pattern[i] = pattern[i + 1]\n        pattern[-1] = a0\n        x_transpose = oopb.add_node(\'Transpose\',\n                                    [x_expand],\n                                    operator.inputs[0].full_name + \'_transpose\',\n                                    perm=pattern)\n    else:\n        x_transpose = x_expand\n\n    # bring y\'s dimension to be reduced to axis 1.\n    if a1 != 1:\n        pattern = list(range(y_ndim))\n        for i in range(a1, 1, -1):\n            pattern[i] = pattern[i - 1]\n        pattern[1] = a1\n        y_transpose = oopb.add_node(\'Transpose\',\n                                    [y_expand],\n                                    operator.inputs[1].full_name + \'_transpose\',\n                                    perm=pattern)\n    else:\n        y_transpose = y_expand\n\n    # normalize both inputs to rank 3.\n    if x_ndim > 3:\n        # squash middle dimensions of x.\n        x_shape_node = oopb.add_node(\'Shape\',\n                                     [x_transpose],\n                                     operator.inputs[0].full_name + \'_x_shape\')\n        x_mid_dims = oopb.add_node(\'Slice\',\n                                   [x_shape_node,\n                                    (\'_start\', oopb.int64, np.array([1], dtype=\'int64\')),\n                                    (\'_end\', oopb.int64, np.array([-1], dtype=\'int64\')),\n                                    (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                    ],\n                                   operator.inputs[0].full_name + \'_x_mid_dims\')\n        x_mid_dims_cast = oopb.add_node(\'Cast\',\n                                        [x_mid_dims],\n                                        operator.inputs[0].full_name + \'_x_mid_dims_cast\',\n                                        to=6)\n        x_squashed_dim = oopb.add_node(\'ReduceProd\',\n                                       [x_mid_dims_cast],\n                                       operator.inputs[0].full_name + \'_x_squashed_dim\')\n        x_squashed_dim_cast = oopb.add_node(\'Cast\',\n                                            [x_squashed_dim],\n                                            operator.inputs[0].full_name + \'_x_squashed_dim_cast\',\n                                            to=7)\n        x_shape_0 = oopb.add_node(\'Slice\',\n                                  [x_shape_node,\n                                   (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                                   (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                                   (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                   ],\n                                  operator.inputs[0].full_name + \'_x_shape_0\')\n        x_shape_1 = oopb.add_node(\'Slice\',\n                                  [x_shape,\n                                   (\'_start\', oopb.int64, np.array([-1], dtype=\'int64\')),\n                                   (\'_end\', oopb.int64, np.array([np.iinfo(np.int64).max], dtype=\'int64\')),\n                                   (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                   ],\n                                  operator.inputs[0].full_name + \'_x_shape_1\')\n        x_squashed_shape = oopb.add_node(\'Concat\',\n                                         [x_shape_0, x_squashed_dim_cast, x_shape_1],\n                                         operator.inputs[0].full_name + \'_x_squashed_shape\',\n                                         axis=0)\n        x_reshape = oopb.add_node(\'Reshape\',\n                                  [x_transpose, x_squashed_shape],\n                                  operator.inputs[0].full_name + \'_x_reshape\')\n        x_squashed = True\n    else:\n        x_reshape = x_transpose\n        x_squashed = False\n\n    if y_ndim > 3:\n        # squash trailing dimensions of y.\n        y_shape_node = oopb.add_node(\'Shape\',\n                                     [y_transpose],\n                                     operator.inputs[0].full_name + \'_y_shape\')\n        y_trail_dims = oopb.add_node(\'Slice\',\n                                     [y_shape_node,\n                                      (\'_start\', oopb.int64, np.array([2], dtype=\'int64\')),\n                                      (\'_end\', oopb.int64, np.array([np.iinfo(np.int64).max], dtype=\'int64\')),\n                                      (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                      ],\n                                     operator.inputs[0].full_name + \'_y_trail_dims\')\n        y_trail_dims_cast = oopb.add_node(\'Cast\',\n                                          [y_trail_dims],\n                                          operator.inputs[0].full_name + \'_y_trail_dims_cast\',\n                                          to=6)\n        y_squashed_dim = oopb.add_node(\'ReduceProd\',\n                                       [y_trail_dims_cast],\n                                       operator.inputs[0].full_name + \'_y_squashed_dim\')\n        y_squashed_dim_cast = oopb.add_node(\'Cast\',\n                                            [y_squashed_dim],\n                                            operator.inputs[0].full_name + \'_y_squashed_dim_cast\',\n                                            to=7)\n        y_shape_0 = oopb.add_node(\'Slice\',\n                                  [y_shape_node,\n                                   (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                                   (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                                   (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                   ],\n                                  operator.inputs[0].full_name + \'_y_shape_0\')\n        y_shape_1 = oopb.add_node(\'Slice\',\n                                  [y_shape_node,\n                                   (\'_start\', oopb.int64, np.array([1], dtype=\'int64\')),\n                                   (\'_end\', oopb.int64, np.array([2], dtype=\'int64\')),\n                                   (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                   ],\n                                  operator.inputs[0].full_name + \'_y_shape_1\')\n        y_squashed_shape = oopb.add_node(\'Concat\',\n                                         [y_shape_0, y_shape_1, y_squashed_dim_cast],\n                                         operator.inputs[0].full_name + \'_y_squashed_shape\',\n                                         axis=0)\n        y_reshape = oopb.add_node(\'Reshape\',\n                                  [y_transpose, y_squashed_shape],\n                                  operator.inputs[0].full_name + \'_y_reshape\')\n\n        y_squashed = True\n    else:\n        y_reshape = y_transpose\n        y_squashed = False\n\n    matmul = oopb.add_node(\'MatMul\',\n                           [x_reshape, y_reshape],\n                           operator.inputs[0].full_name + \'_matmul\')\n\n    # if inputs were squashed, we have to reshape the matmul output.\n    if x_squashed or y_squashed:\n        output_shape = oopb.add_node(\'Shape\',\n                                     [matmul],\n                                     operator.inputs[0].full_name + \'_output_shape\')\n    else:\n        output_shape = matmul\n    do_reshape = False\n\n    if x_squashed:\n        output_shape_x_0 = oopb.add_node(\'Slice\',\n                                         [output_shape,\n                                          (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                                          (\'_end\', oopb.int64, np.array([1], dtype=\'int64\')),\n                                          (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                          ],\n                                         operator.inputs[0].full_name + \'_output_shape_x_0\')\n        output_shape_x_1 = oopb.add_node(\'Slice\',\n                                         [output_shape,\n                                          (\'_start\', oopb.int64, np.array([-1], dtype=\'int64\')),\n                                          (\'_end\', oopb.int64, np.array([np.iinfo(np.int64).max], dtype=\'int64\')),\n                                          (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                          ],\n                                         operator.inputs[0].full_name + \'_output_shape_x_1\')\n        output_shape_x = oopb.add_node(\'Concat\',\n                                       [output_shape_x_0, x_mid_dims, output_shape_x_1],\n                                       operator.inputs[0].full_name + \'_output_shape_x\',\n                                       axis=0)\n        do_reshape = True\n    else:\n        output_shape_x = output_shape\n\n    if y_squashed:\n        output_shape_y_0 = oopb.add_node(\'Slice\',\n                                         [output_shape_x,\n                                          (\'_start\', oopb.int64, np.array([0], dtype=\'int64\')),\n                                          (\'_end\', oopb.int64, np.array([-1], dtype=\'int64\')),\n                                          (\'_axes\', oopb.int64, np.array([0], dtype=\'int64\'))\n                                          ],\n                                         operator.inputs[0].full_name + \'_output_shape_y_0\')\n        output_shape_y = oopb.add_node(\'Concat\',\n                                       [output_shape_y_0, y_trail_dims],\n                                       operator.inputs[0].full_name + \'_output_shape_y\',\n                                       axis=0)\n        do_reshape = True\n    else:\n        output_shape_y = output_shape_x\n\n    if do_reshape:\n        output_reshape = oopb.add_node(\'Reshape\',\n                                       [matmul, output_shape_y],\n                                       operator.inputs[0].full_name + \'_output_reshape\')\n    else:\n        output_reshape = matmul\n\n    # if the inputs were originally rank 2, we remove the added 1 dim.\n    if orig_x_ndim == 2:\n        container.add_node(\'Squeeze\', output_reshape, operator.output_full_names,\n                           name=scope.get_unique_operator_name(\'Squeeze\'), axes=[1])\n    elif orig_y_ndim == 2:\n        container.add_node(\'Squeeze\', output_reshape, operator.output_full_names,\n                           name=scope.get_unique_operator_name(\'Squeeze\'), axes=[y_ndim - 1])\n    else:\n        container.add_node(\'Identity\', output_reshape, operator.output_full_names,\n                           name=scope.get_unique_operator_name(\'Identity\'))\n\n\n@cvtfunc(shape_infer=_calculate_keras_dot_output_shapes)\ndef convert_keras_dot(scope, operator, container):\n    if (is_tf2 and is_tf_keras) or is_keras_later_than(""2.2.4""):\n        convert_keras_dot_post_224(scope, operator, container)\n    else:\n        convert_keras_dot_224(scope, operator, container)\n'"
keras2onnx/ke2onnx/embedding.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom ..common.onnx_ops import apply_cast, OnnxOperatorBuilder\nfrom ..proto import onnx_proto\n\nimport numpy as np\n\n\ndef convert_keras_embed(scope, operator, container):\n    op = operator.raw_operator  # Keras Embedding layer object\n    #  if mask_zero specified, the output_mask tensor needed by calculated\n    if hasattr(op, 'mask_zero') and op.mask_zero is True:\n        oopb = OnnxOperatorBuilder(container, scope)\n        # Keras embed layer compute mask\n        # output_mask = K.not_equal(inputs, 0)\n        if container.target_opset >= 11:\n            equal_out = oopb.add_node('Equal', [operator.inputs[0].full_name, np.array([0], dtype='float32')],\n                                      operator.full_name + 'mask')\n            container.add_node('Not', equal_out, operator.output_masks[0].full_name,\n                               name=operator.full_name + 'mask_not')\n        else:\n            equal_input_0 = oopb.add_node('Cast', [operator.inputs[0].full_name],\n                                          operator.full_name + '_input_cast', to=6)\n            equal_out = oopb.add_node('Equal', [equal_input_0, np.array([0], dtype='int32')],\n                                      operator.full_name + 'mask')\n            container.add_node('Not', equal_out, operator.output_masks[0].full_name,\n                               name=operator.full_name + 'mask_not')\n\n    cast_name = scope.get_unique_variable_name('casted')\n    apply_cast(scope, operator.inputs[0].full_name, cast_name, container, to=onnx_proto.TensorProto.INT32)\n\n    # Prepare the weight matrix (i.e., the vectors of all input indices) as an initializer so that the following main\n    # operator can access it.\n    op_output_shape_last_dim = operator.get_output_shape()[-1]\n    weights = np.array(op.get_weights()[0].T).reshape(op_output_shape_last_dim,\n                                                      op.input_dim).transpose().flatten().tolist()\n    embedding_tensor_name = container.add_initializer_by_name(scope, op.weights[0].name, onnx_proto.TensorProto.FLOAT,\n                                                              [op.input_dim, op_output_shape_last_dim], weights)\n    # Create a Gather operator to extract the latent representation of each index\n    container.add_node('Gather', [embedding_tensor_name, cast_name], operator.output_full_names[0],\n                       name=operator.full_name)\n"""
keras2onnx/ke2onnx/gru.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy as np\nfrom ..proto import onnx_proto\nfrom ..common import name_func\nfrom ..common.onnx_ops import apply_transpose, OnnxOperatorBuilder\nfrom . import simplernn\n\nTensorProto = onnx_proto.TensorProto\n\n\ndef extract_params(op):\n    """"""Returns a tuple of the GRU paramters, and converts them into the format for ONNX.\n    """"""\n    params = op.get_weights()\n    W = params[0].T\n    R = params[1].T\n\n    B = None\n    if op.use_bias:\n        B = params[2]\n\n    return W, R, B\n\n\ndef build_parameters(scope, operator, container, bidirectional=False):\n    """"""Returns the parameter initialization values after extracting them from the GRU layer.\n    """"""\n    op = operator.raw_operator\n    _, seq_length, input_size = simplernn.extract_input_shape(op)\n\n    _name = name_func(scope, operator)\n\n    tensor_w = _name(\'W\')\n    tensor_r = _name(\'R\')\n    tensor_b = \'\'\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        backward_layer = op.backward_layer\n        hidden_size = forward_layer.units\n\n        W, R, B = extract_params(forward_layer)\n        W_back, R_back, B_back = extract_params(backward_layer)\n\n        W = np.concatenate([W, W_back])\n        W_shape = [2, 3 * hidden_size, input_size]\n\n        R = np.concatenate([R, R_back])\n        R_shape = [2, 3 * hidden_size, hidden_size]\n\n        if B is not None:\n            if B.size == 3 * hidden_size:\n                B = np.concatenate([B, np.zeros(3 * hidden_size)])\n            if B_back.size == 3 * hidden_size:\n                B_back = np.concatenate([B_back, np.zeros(3 * hidden_size)])\n            B = np.concatenate([B, B_back])\n            B_shape = [2, 6 * hidden_size]\n\n    else:\n        hidden_size = op.units\n\n        W, R, B = extract_params(op)\n        W_shape = [1, 3 * hidden_size, input_size]\n        R_shape = [1, 3 * hidden_size, hidden_size]\n\n        if B is not None:\n            if B.size == 3 * hidden_size:\n                B = np.concatenate([B, np.zeros(3 * hidden_size)])\n            B_shape = [1, 6 * hidden_size]\n\n    # Create initializers\n    container.add_initializer(tensor_w, TensorProto.FLOAT, W_shape, W.flatten())\n    container.add_initializer(tensor_r, TensorProto.FLOAT, R_shape, R.flatten())\n\n    if B is not None:\n        tensor_b = _name(\'B\')\n        container.add_initializer(tensor_b, TensorProto.FLOAT, B_shape, B.flatten())\n\n    return tensor_w, tensor_r, tensor_b\n\n\ndef build_attributes(scope, operator, container, bidirectional=False):\n    """"""Returns a dictionary of attributes for the GRU layer.\n    """"""\n    op = operator.raw_operator\n\n    attrs = {}\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        backward_layer = op.backward_layer\n\n        attrs[\'direction\'] = \'bidirectional\'\n        attrs[\'hidden_size\'] = forward_layer.units\n        attrs.update(simplernn.extract_activations([\n            forward_layer.recurrent_activation,\n            forward_layer.activation,\n            backward_layer.recurrent_activation,\n            backward_layer.activation,\n\n        ]))\n\n    else:\n        attrs[\'direction\'] = \'reverse\' if op.go_backwards else \'forward\'\n        attrs[\'hidden_size\'] = op.units\n        attrs.update(simplernn.extract_activations([\n            op.recurrent_activation,\n            op.activation\n        ]))\n\n    return attrs\n\n\ndef convert_keras_gru(scope, operator, container, bidirectional=False):\n    op = operator.raw_operator\n\n    _name = name_func(scope, operator)\n\n    if bidirectional:\n        output_seq = op.forward_layer.return_sequences\n        reset_after = op.forward_layer.reset_after\n    else:\n        output_seq = op.return_sequences\n        reset_after = op.reset_after\n\n    time_major = simplernn.is_time_major(op, bidirectional)\n\n    # Inputs\n    gru_x = operator.inputs[0].full_name\n    if not time_major:\n        gru_x = _name(\'X\')\n        apply_transpose(scope, operator.inputs[0].full_name, gru_x, container, perm=[1, 0, 2])\n    tensor_w, tensor_r, tensor_b = build_parameters(scope, operator, container, bidirectional)\n    sequence_lengths = simplernn.build_sequence_lengths(scope, operator, container)\n    initial_h = simplernn.build_initial_states(scope, operator, container, bidirectional)\n\n    input_names = [\n        gru_x,\n        tensor_w,\n        tensor_r,\n        tensor_b,\n        sequence_lengths,\n        initial_h,\n    ]\n\n    # Attributes\n    attrs = build_attributes(scope, operator, container, bidirectional)\n\n    # Outputs\n    output_names = [_name(\'Y\'), _name(\'Y_h\')]\n\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.apply_op_with_output(\'apply_gru\',\n                              input_names,\n                              output_names,\n                              name=op.name,\n                              output_seq=output_seq,\n                              reset_after=reset_after,\n                              **attrs)\n\n    simplernn.build_output(scope, operator, container, output_names, bidirectional)\n    simplernn.build_output_states(scope, operator, container, output_names, bidirectional)\n'"
keras2onnx/ke2onnx/layer_spec.py,6,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport tensorflow as tf\nfrom ..proto import keras, is_tf_keras, is_keras_older_than\nfrom ..proto.tfcompat import is_tf2\n\n_layer = keras.layers\n_adv_activations = keras.layers.advanced_activations\n\n\ndef _default_layer_name_extractor(fstr_list, node_name):\n    for fstr in fstr_list:\n        idx = fstr.rfind(\'{}/\')\n        if node_name.endswith(fstr[idx + 3:]):\n            klen = len(fstr) + idx - 2  # 2 = len(\'{}\')\n            return node_name[:len(node_name) - klen]\n\n    return None\n\n\ndef _simple_layer_name_extractor(fstr_list, node_name):\n    ri = node_name.rindex(\'/\')\n    return node_name[:ri]\n\n\ndef _conv_layer_spec_outputs(layer, node):\n    if type(layer) == _layer.Conv1D:\n        return node.name + \'/Squeeze\'\n\n    activation_map = {\n        keras.activations.linear: \'\',\n        tf.nn.sigmoid: \'Sigmoid\',\n        tf.nn.softmax: \'Softmax\',\n        tf.nn.relu: \'Relu\',\n        tf.nn.elu: \'Elu\',\n        tf.nn.tanh: \'Tanh\',\n        tf.nn.swish: \'mul\'}\n\n    node_act = activation_map.get(layer.activation, None)\n    if node_act is None:\n        actname_map = {a_.__name__: a_ for a_ in activation_map if hasattr(a_, ""__name__"")}\n        act_trans = actname_map.get(layer.activation.__name__, None)\n        if act_trans is not None:\n            node_act = activation_map.get(act_trans)\n\n    assert node_act is not None, ""Unsupported activation in the layer({})"".format(layer.activation)\n    if node_act:\n        ri = node.name.rindex(\'/\')\n        return node.name[:ri + 1] + node_act\n    else:\n        if not layer.use_bias:\n            if node.inputs[0].op.type == \'SpaceToBatchND\':\n                return node.name + \'/BatchToSpaceND\'\n            else:\n                return node.name\n        else:\n            ri = node.name.rindex(\'/\')\n            return node.name[:ri + 1] + \'BiasAdd\'\n\n\ndef _relu_like_spec_outputs(layer, node):\n    if isinstance(layer, _adv_activations.PReLU):\n        ri = node.name.rindex(\'/\')\n        return node.name[:ri + 1] + \'add\'\n\n    return node.name\n\n\n_keras_layer_spec = {\n    # layer-type: ([pattern-list], [extract-layer-name, output-name-generator(optional)]\n    _layer.AveragePooling1D: ([""{}/AvgPool""], [_default_layer_name_extractor]),\n    _layer.AveragePooling2D: ([""{}/AvgPool""], [_default_layer_name_extractor]),\n    _layer.AveragePooling3D: ([""{}/AvgPool""], [_default_layer_name_extractor]),\n    _layer.MaxPooling1D: ([""{}/MaxPool""], [_default_layer_name_extractor]),\n    _layer.MaxPooling2D: ([""{}/MaxPool""], [_default_layer_name_extractor]),\n    _layer.MaxPooling3D: ([""{}/MaxPool""], [_default_layer_name_extractor]),\n\n    _layer.Conv1D: ([""{}/conv1d""], [_simple_layer_name_extractor, _conv_layer_spec_outputs]),\n    _layer.Conv2D: ([""{}/Conv2D""], [_simple_layer_name_extractor, _conv_layer_spec_outputs]),\n\n    _layer.Conv2DTranspose: ([""{}/conv2d_transpose""], [_simple_layer_name_extractor, _conv_layer_spec_outputs]),\n    _layer.DepthwiseConv2D: ([""{}/depthwise""], [_simple_layer_name_extractor, _conv_layer_spec_outputs]),\n\n    _layer.LeakyReLU: ([""{}/LeakyRelu""], [_default_layer_name_extractor]),\n    _adv_activations.PReLU: ([""{}/Relu""], [_simple_layer_name_extractor, _relu_like_spec_outputs]),\n\n    _layer.Reshape: ([""{}/Reshape""], [_default_layer_name_extractor])\n}\n\nif not is_keras_older_than(\'2.2.0\'):\n    _keras_layer_spec.update({\n        _adv_activations.ReLU: ([""{}/Relu""], [_simple_layer_name_extractor, _relu_like_spec_outputs]),\n    })\n\nif is_tf_keras and is_tf2:\n    _keras_layer_spec.update({\n        _layer.normalization_v2.BatchNormalization: (\n            [""{}/FusedBatchNormV3"", ""{}/batchnorm/add_1""], [_default_layer_name_extractor])\n    })\n\n\ndef keras_layer_spec(layer_type):\n    return _keras_layer_spec.get(layer_type, (None, []))\n'"
keras2onnx/ke2onnx/lstm.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numbers\nimport numpy as np\nfrom collections.abc import Iterable\nfrom ..common import cvtfunc, name_func\nfrom ..common.onnx_ops import (\n    apply_concat,\n    apply_reshape,\n    apply_split,\n    apply_squeeze,\n    apply_unsqueeze,\n    apply_transpose,\n    OnnxOperatorBuilder\n)\nfrom ..proto import onnx_proto, keras\nfrom . import simplernn\n\nLSTM = keras.layers.LSTM\nTensorProto = onnx_proto.TensorProto\n\n\ndef convert_ifco_to_iofc(tensor_ifco):\n    """"""Returns a tensor in input (i), output (o), forget (f), cell (c) ordering. The\n    Keras ordering is ifco, while the ONNX ordering is iofc.\n    """"""\n    splits = np.split(tensor_ifco, 4)\n    return np.concatenate((splits[0], splits[3], splits[1], splits[2]))\n\n\ndef extract_params(op, hidden_size, input_size):\n    """"""Returns a tuple of the LSTM parameters, and converts them into the format for ONNX.\n    """"""\n    params = op.get_weights()\n\n    # Keras: [W_x, W_h, b] each in I F C O\n    # ONNX: W[iofc] I O F C\n    W_x = convert_ifco_to_iofc(params[0].T).reshape(4, hidden_size, input_size)\n    W_h = convert_ifco_to_iofc(params[1].T).reshape(4, hidden_size, hidden_size)\n\n    b = None\n    if op.use_bias:\n        b = np.zeros((8, hidden_size), dtype=np.float32)\n        b[:4] = convert_ifco_to_iofc(params[2]).reshape(4, hidden_size)\n\n    return W_x, W_h, b\n\n\ndef build_parameters(scope, operator, container, bidirectional=False):\n    """"""Returns the parameter initialization values after extracting them from the LSTM layer.\n    """"""\n    op = operator.raw_operator\n    _, seq_length, input_size = simplernn.extract_input_shape(op)\n\n    _name = name_func(scope, operator)\n\n    tensor_w = _name(\'W\')\n    tensor_r = _name(\'R\')\n    tensor_b = \'\'\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        backward_layer = op.backward_layer\n        hidden_size = forward_layer.units\n\n        W_x, W_h, b = extract_params(forward_layer, hidden_size, input_size)\n        W_x_back, W_h_back, b_back = extract_params(backward_layer, hidden_size, input_size)\n\n        W = np.concatenate([W_x, W_x_back]).flatten()\n        W_shape = [2, 4 * hidden_size, input_size]\n\n        R = np.concatenate([W_h, W_h_back]).flatten()\n        R_shape = [2, 4 * hidden_size, hidden_size]\n\n        if (b is None and b_back is not None) or (b is not None and b_back is None):\n            raise ValueError(\'Bidirectional bias must be enabled (or disabled) for both forward \'\n                             \'and backward layers.\')\n\n        if b is not None:\n            B = np.concatenate([b, b_back]).flatten()\n            B_shape = [2, 8 * hidden_size]\n\n    else:\n        hidden_size = op.units\n\n        W_x, W_h, b = extract_params(op, hidden_size, input_size)\n\n        W = W_x.flatten()\n        W_shape = [1, 4 * hidden_size, input_size]\n\n        R = W_h.flatten()\n        R_shape = [1, 4 * hidden_size, hidden_size]\n\n        if b is not None:\n            B = b.flatten()\n            B_shape = [1, 8 * hidden_size]\n\n    # Create initializers\n    container.add_initializer(tensor_w, TensorProto.FLOAT, W_shape, W)\n    container.add_initializer(tensor_r, TensorProto.FLOAT, R_shape, R)\n\n    if b is not None:\n        tensor_b = _name(\'B\')\n        container.add_initializer(tensor_b, TensorProto.FLOAT, B_shape, B)\n\n    return tensor_w, tensor_r, tensor_b\n\n\ndef build_initial_states(scope, operator, container, bidirectional=False):\n    """"""Builds the initial hidden and cell states for the LSTM layer.\n    """"""\n    _name = name_func(scope, operator)\n\n    initial_h = simplernn.build_initial_states(scope, operator, container, bidirectional)\n\n    # Determine if the cell states are set\n    has_c = (\n            (len(operator.inputs) > 1 and not bidirectional) or\n            (len(operator.inputs) > 3 and bidirectional)\n    )\n    if not has_c:\n        return initial_h, \'\'\n\n    op = operator.raw_operator\n    initial_c = _name(\'initial_c\')\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        hidden_size = forward_layer.units\n        desired_shape = [1, -1, hidden_size]\n\n        # Combine the forward and backward_layers\n        forward_h = _name(\'initial_c_forward\')\n        backward_h = _name(\'initial_c_backward\')\n        apply_reshape(scope, operator.inputs[2].full_name, forward_h, container, desired_shape=desired_shape)\n        apply_reshape(scope, operator.inputs[4].full_name, backward_h, container, desired_shape=desired_shape)\n\n        apply_concat(scope, [forward_h, backward_h], initial_c, container)\n\n    else:\n        # Unsqueeze dim 0 to represent num_directions\n        input_c = operator.inputs[2].full_name\n        apply_unsqueeze(scope, input_c, initial_c, container, axes=[0])\n\n    return initial_h, initial_c\n\n\ndef build_attributes(scope, operator, container, bidirectional=False):\n    """"""Returns a dictionary of attributes for the LSTM layer.\n    """"""\n    op = operator.raw_operator\n\n    attrs = {}\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        backward_layer = op.backward_layer\n\n        attrs[\'direction\'] = \'bidirectional\'\n        attrs[\'hidden_size\'] = forward_layer.units\n        attrs.update(simplernn.extract_activations([\n            forward_layer.recurrent_activation,\n            forward_layer.activation,\n            forward_layer.activation,\n            backward_layer.recurrent_activation,\n            backward_layer.activation,\n            backward_layer.activation,\n        ]))\n\n    else:\n        attrs[\'direction\'] = \'reverse\' if op.go_backwards else \'forward\'\n        attrs[\'hidden_size\'] = op.units\n        attrs.update(simplernn.extract_activations([\n            op.recurrent_activation,\n            op.activation,\n            op.activation,\n        ]))\n    return attrs\n\n\ndef build_output(scope, operator, container, output_names, bidirectional=False):\n    """"""Builds the output operators for the LSTM layer.\n    """"""\n    if bidirectional:\n        return simplernn.build_output(scope, operator, container, output_names[:-1], bidirectional)\n\n    lstm_y, lstm_h, lstm_c = output_names\n\n    op = operator.raw_operator\n    output_seq = op.return_sequences\n    _, seq_length, input_size = simplernn.extract_input_shape(op)\n\n    _name = name_func(scope, operator)\n\n    output_name = operator.outputs[0].full_name\n\n    time_major = simplernn.is_time_major(op, bidirectional)\n    # Create output-adjusting operators\n    if output_seq:\n        # Squeeze the num_direction dim as we know its size is 1 for\n        # lstm(forward/reverse).\n        lstm_out = output_name if time_major else _name(\'y_squeezed\')\n        apply_squeeze(scope, lstm_y, lstm_out, container, axes=[1])\n        if not time_major:\n            # Onnx LSTM produces time major output. Add a transpose operator to\n            # make it batch_major, if the keras op was not time_major.\n            # This transforms [ S, B, I] -> [ B, S, I ] where B is\n            # batch_size and S is seq_len.\n            perm = [1, 0, 2]\n            apply_transpose(scope, lstm_out, output_name, container, perm=perm)\n    else:\n        apply_squeeze(scope, lstm_h, output_name, container, axes=[0])\n\n\ndef build_output_states(scope, operator, container, output_names, bidirectional=False):\n    """"""Builds the output hidden states for the LSTM layer.\n    """"""\n    _, lstm_h, lstm_c = output_names\n    op = operator.raw_operator\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        output_state = forward_layer.return_state\n\n        if not output_state:\n            return\n\n        # Split lstm_h and lstm_c into forward and backward components\n        squeeze_names = []\n        output_names = [o.full_name for o in operator.outputs[1:]]\n        name_map = {lstm_h: output_names[::2], lstm_c: output_names[1::2]}\n\n        for state_name, outputs in name_map.items():\n            split_names = [\'{}_{}\'.format(state_name, d) for d in (\'forward\', \'backward\')]\n\n            apply_split(scope, state_name, split_names, container)\n            squeeze_names.extend(list(zip(split_names, outputs)))\n\n        for split_name, output_name in squeeze_names:\n            apply_squeeze(scope, split_name, output_name, container)\n\n    else:\n        output_state = op.return_state\n\n        if not output_state:\n            return\n\n        output_h = operator.outputs[1].full_name\n        output_c = operator.outputs[2].full_name\n        apply_squeeze(scope, lstm_h, output_h, container)\n        apply_squeeze(scope, lstm_c, output_c, container)\n\n\ndef _calculate_keras_lstm_output_shapes(operator):\n    op = operator.raw_operator\n    if isinstance(op.output_shape[0], Iterable):\n        operator.outputs[0].type.shape = list(i if isinstance(i, numbers.Integral) else None\n                                              for i in op.output_shape[0])\n    else:\n        operator.outputs[0].type.shape = list(i if isinstance(i, numbers.Integral) else None for i in op.output_shape)\n\n\n@cvtfunc(shape_infer=_calculate_keras_lstm_output_shapes)\ndef convert_keras_lstm(scope, operator, container, bidirectional=False):\n    op = operator.raw_operator\n    _name = name_func(scope, operator)\n\n    if bidirectional:\n        output_seq = op.forward_layer.return_sequences\n    else:\n        output_seq = op.return_sequences\n\n    time_major = simplernn.is_time_major(op, bidirectional)\n\n    # Inputs\n    lstm_x = operator.inputs[0].full_name\n    if not time_major:\n        # If the keras op was not time_major, we add a transpose op to make the\n        # input time_major as ONNX lstm expects time_major input.\n        # Transform [ B, S, I ] -> [ S, B, I] where B is batch_size and S is\n        # seq_len.\n        lstm_x = _name(\'X\')\n        apply_transpose(scope, operator.inputs[0].full_name, lstm_x, container, perm=[1, 0, 2])\n\n    tensor_w, tensor_r, tensor_b = build_parameters(scope, operator, container, bidirectional)\n    sequence_lengths = simplernn.build_sequence_lengths(scope, operator, container)\n    initial_h, initial_c = build_initial_states(scope, operator, container, bidirectional)\n\n    input_names = [\n        lstm_x,\n        tensor_w,\n        tensor_r,\n        tensor_b,\n        sequence_lengths,\n        initial_h,\n        initial_c,\n        \'\',  # P (optional) : No peep hole in Keras.\n    ]\n\n    # Attributes\n    attrs = build_attributes(scope, operator, container, bidirectional)\n\n    # Outputs\n    output_names = [_name(\'Y\'), _name(\'Y_h\'), _name(\'Y_c\')]\n\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.apply_op_with_output(\'apply_lstm\',\n                              input_names,\n                              output_names,\n                              name=op.name,\n                              output_seq=output_seq,\n                              **attrs)\n\n    build_output(scope, operator, container, output_names, bidirectional)\n    build_output_states(scope, operator, container, output_names, bidirectional)\n'"
keras2onnx/ke2onnx/main.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy as np\nfrom ..proto import keras, is_tf_keras, is_keras_older_than\nfrom ..proto.tfcompat import is_tf2\nfrom ..common import with_variable, k2o_logger\nfrom ..common.onnx_ops import apply_identity, apply_tile\nfrom ..common.onnx_ops import apply_reshape, apply_concat, apply_transpose, apply_flatten, OnnxOperatorBuilder\n\nfrom .activation import convert_keras_activation\nfrom .adv_activation import convert_keras_advanced_activation\nfrom .batch_norm import convert_keras_batch_normalization\nfrom .merge import convert_keras_merge_layer\nfrom .dense import convert_keras_dense\nfrom .dot import convert_keras_dot\nfrom .upsample import convert_keras_upsample_1d, convert_keras_upsample_2d, convert_keras_upsample_3d\nfrom .conv import convert_keras_conv1d, convert_keras_conv2d, convert_keras_conv3d\nfrom .conv import convert_keras_conv_transpose_2d, convert_keras_conv_transpose_3d, convert_keras_depthwise_conv_2d\nfrom .conv import convert_keras_separable_conv1d, convert_keras_separable_conv2d\nfrom .pooling import convert_keras_max_pooling_1d, convert_keras_max_pooling_2d, convert_keras_max_pooling_3d\nfrom .pooling import convert_keras_average_pooling_1d, convert_keras_average_pooling_2d, \\\n    convert_keras_average_pooling_3d\nfrom .crop import convert_keras_crop_1d, convert_keras_crop_2d, convert_keras_crop_3d\nfrom .zeropad import convert_keras_zero_pad_1d, convert_keras_zero_pad_2d, convert_keras_zero_pad_3d\nfrom .embedding import convert_keras_embed\nfrom .simplernn import convert_keras_simple_rnn\nfrom .gru import convert_keras_gru\nfrom .lstm import convert_keras_lstm\nfrom .bidirectional import convert_bidirectional\n\n\ndef convert_keras_reshape(scope, operator, container):\n    iop = operator.raw_operator\n    target_shape = [-1 if i_ is None else i_ for i_ in iop.output_shape]\n    if target_shape[0] == -1:\n        target_shape[0] = 0\n\n    apply_reshape(scope, operator.inputs[0].full_name, operator.outputs[0].full_name, container,\n                  operator_name=operator.raw_operator.name, desired_shape=target_shape)\n\n\ndef convert_keras_concat(scope, operator, container):\n    axis = operator.raw_operator.axis\n    if axis < 0:\n        axis += len(operator.raw_operator.output.shape)\n    apply_concat(scope, operator.input_full_names, operator.output_full_names, container,\n                 operator_name=operator.full_name, axis=axis)\n\n\ndef convert_keras_flatten(scope, operator, container):\n    iop = operator.raw_operator\n    shape_len = len(operator.get_input_shape())\n\n    if iop.data_format == \'channels_last\' or shape_len < 3:\n        apply_flatten(scope, operator.inputs[0].full_name, operator.outputs[0].full_name, container,\n                      operator_name=operator.raw_operator.name)\n    else:\n        perm = list(range(2, shape_len))\n        perm = [0] + perm + [1]\n        input_tensor_name = scope.get_unique_variable_name(operator.inputs[0].full_name + \'_permuted\')\n        apply_transpose(scope, operator.inputs[0].full_name, input_tensor_name, container,\n                        operator_name=operator.raw_operator.name + ""_transpose"", perm=perm)\n        apply_flatten(scope, input_tensor_name, operator.outputs[0].full_name, container,\n                      operator_name=operator.raw_operator.name)\n\n\ndef _apply_not_equal(oopb, target_opset, operator):\n    if operator.mask_value is None:\n        raise ValueError(""Masking value was not properly parsed for layer \'{}\'"".format(operator.full_name))\n    if target_opset >= 11:\n        equal_out = oopb.add_node(\'Equal\', [operator.inputs[0].full_name,\n                                            np.array([operator.mask_value], dtype=\'float32\')],\n                                  operator.full_name + \'mask\')\n        not_o = oopb.add_node(\'Not\', equal_out,\n                              name=operator.full_name + \'_not\')\n    else:\n        k2o_logger().warning(""On converting a model with opset < 11, "" +\n                             ""the masking layer result may be incorrect if the model input is in range (0, 1.0)."")\n        equal_input_0 = oopb.add_node(\'Cast\', [operator.inputs[0].full_name],\n                                      operator.full_name + \'_input_cast\', to=oopb.int32)\n        equal_out = oopb.add_node(\'Equal\', [equal_input_0, np.array([operator.mask_value], dtype=\'int32\')],\n                                  operator.full_name + \'mask\')\n        not_o = oopb.add_node(\'Not\', equal_out,\n                              name=operator.full_name + \'_not\')\n    return not_o\n\n\ndef convert_keras_masking(scope, operator, container):\n    oopb = OnnxOperatorBuilder(container, scope)\n    not_o = _apply_not_equal(oopb, container.target_opset, operator)\n    cast_o = oopb.apply_cast(not_o, to=oopb.float, name=operator.full_name + \'_cast\')\n    if operator.output_masks:\n        reduce_node = oopb.add_node(""ReduceSum"",\n                                    cast_o[0], keepdims=False, axes=[-1], name=operator.full_name + \'_reduced\')\n        oopb.add_node_with_output(""Greater"", [reduce_node, np.array(0, dtype=np.float32)],\n                                  [operator.output_masks[0].full_name], name=operator.full_name + \'_greater\')\n\n    reduce_node2 = oopb.add_node(""ReduceSum"",\n                                 cast_o, keepdims=True, axes=[-1], name=operator.full_name + \'reduced2\')\n    greater_o = oopb.add_node(""Greater"",\n                              [reduce_node2, np.array(0, dtype=np.float32)], name=operator.full_name + \'_greater2\')\n    cast2_o = oopb.apply_cast(greater_o, to=oopb.float, name=operator.full_name + \'_cast2\')\n\n    oopb.add_node_with_output(\'Mul\', [cast2_o[0], operator.inputs[0].full_name], [operator.outputs[0].full_name],\n                              name=operator.outputs[0].full_name)\n\n\ndef convert_keras_permute(scope, operator, container):\n    axes = [0] + list(operator.raw_operator.dims)\n    apply_transpose(scope, operator.inputs[0].full_name, operator.outputs[0].full_name, container, perm=axes)\n\n\ndef convert_keras_repeat_vector(scope, operator, container):\n    op = operator.raw_operator\n\n    intermediate_tensor_name = scope.get_unique_variable_name(operator.inputs[0].full_name + \'_reshaped\')\n    apply_reshape(scope, operator.inputs[0].full_name, intermediate_tensor_name, container,\n                  desired_shape=[-1, 1, op.input_shape[1]])\n\n    repeats = [1, int(op.n), 1]\n    apply_tile(scope, intermediate_tensor_name, operator.outputs[0].full_name, container, repeats=repeats)\n\n\ndef convert_keras_training_only_layer(scope, operator, container):\n    apply_identity(scope, operator.inputs[0].full_name, operator.outputs[0].full_name, container)\n\n\n_layer = keras.layers\n_adv_activations = keras.layers.advanced_activations\n\nkeras_layer_to_operator = {\n    _layer.UpSampling1D: convert_keras_upsample_1d,\n    _layer.UpSampling2D: convert_keras_upsample_2d,\n    _layer.UpSampling3D: convert_keras_upsample_3d,\n    _layer.BatchNormalization: convert_keras_batch_normalization,\n\n    _adv_activations.LeakyReLU: convert_keras_advanced_activation,\n    _adv_activations.ThresholdedReLU: convert_keras_advanced_activation,\n    _adv_activations.ELU: convert_keras_advanced_activation,\n    _adv_activations.PReLU: convert_keras_advanced_activation,\n\n    _layer.Activation: convert_keras_activation,\n\n    _layer.Conv1D: convert_keras_conv1d,\n    _layer.Conv2D: convert_keras_conv2d,\n    _layer.Conv3D: convert_keras_conv3d,\n    _layer.Conv2DTranspose: convert_keras_conv_transpose_2d,\n    _layer.Conv3DTranspose: convert_keras_conv_transpose_3d,\n    _layer.DepthwiseConv2D: convert_keras_depthwise_conv_2d,\n    _layer.SeparableConv1D: convert_keras_separable_conv1d,\n    _layer.SeparableConv2D: convert_keras_separable_conv2d,\n\n    _layer.Add: convert_keras_merge_layer,\n    _layer.Multiply: convert_keras_merge_layer,\n    _layer.Subtract: convert_keras_merge_layer,\n    _layer.Average: convert_keras_merge_layer,\n    _layer.Maximum: convert_keras_merge_layer,\n    _layer.Concatenate: convert_keras_concat,\n\n    _layer.Dense: convert_keras_dense,\n    _layer.Dot: convert_keras_dot,\n    _layer.Embedding: convert_keras_embed,\n    _layer.Masking: convert_keras_masking,\n\n    _layer.MaxPooling1D: convert_keras_max_pooling_1d,\n    _layer.MaxPooling2D: convert_keras_max_pooling_2d,\n    _layer.MaxPooling3D: convert_keras_max_pooling_3d,\n    _layer.GlobalMaxPooling1D: convert_keras_max_pooling_1d,\n    _layer.GlobalMaxPooling2D: convert_keras_max_pooling_2d,\n    _layer.GlobalMaxPooling3D: convert_keras_max_pooling_3d,\n    _layer.AveragePooling1D: convert_keras_average_pooling_1d,\n    _layer.AveragePooling2D: convert_keras_average_pooling_2d,\n    _layer.AveragePooling3D: convert_keras_average_pooling_3d,\n    _layer.GlobalAveragePooling1D: convert_keras_average_pooling_1d,\n    _layer.GlobalAveragePooling2D: convert_keras_average_pooling_2d,\n    _layer.GlobalAveragePooling3D: convert_keras_average_pooling_3d,\n\n    _layer.Cropping1D: convert_keras_crop_1d,\n    _layer.Cropping2D: convert_keras_crop_2d,\n    _layer.Cropping3D: convert_keras_crop_3d,\n\n    _layer.ZeroPadding1D: convert_keras_zero_pad_1d,\n    _layer.ZeroPadding2D: convert_keras_zero_pad_2d,\n    _layer.ZeroPadding3D: convert_keras_zero_pad_3d,\n\n    _layer.Flatten: convert_keras_flatten,\n    _layer.Reshape: convert_keras_reshape,\n    _layer.Permute: convert_keras_permute,\n    _layer.RepeatVector: convert_keras_repeat_vector,\n\n    _layer.AlphaDropout: convert_keras_training_only_layer,\n    _layer.Dropout: convert_keras_training_only_layer,\n    _layer.GaussianDropout: convert_keras_training_only_layer,\n    _layer.GaussianNoise: convert_keras_training_only_layer,\n    _layer.SpatialDropout1D: convert_keras_training_only_layer,\n    _layer.SpatialDropout2D: convert_keras_training_only_layer,\n    _layer.SpatialDropout3D: convert_keras_training_only_layer,\n\n    _layer.SimpleRNN: convert_keras_simple_rnn,\n    _layer.GRU: convert_keras_gru,\n    _layer.LSTM: convert_keras_lstm,\n    _layer.Bidirectional: convert_bidirectional\n}\n\nif not is_keras_older_than(\'2.1.3\'):\n    keras_layer_to_operator.update({\n        _adv_activations.Softmax: convert_keras_advanced_activation\n    })\n\nif not is_keras_older_than(\'2.2.0\'):\n    keras_layer_to_operator.update({\n        _adv_activations.ReLU: convert_keras_advanced_activation,\n    })\n\nif is_tf_keras and is_tf2:\n    keras_layer_to_operator.update({\n        _layer.recurrent_v2.GRU: convert_keras_gru,\n        _layer.recurrent_v2.LSTM: convert_keras_lstm,\n        _layer.normalization_v2.BatchNormalization: convert_keras_batch_normalization,\n    })\n\n\n@with_variable(\'loaded\')\ndef static_set_ke2onnx_converters(func_set_converter):\n    for ky_, val_ in keras_layer_to_operator.items():\n        func_set_converter(ky_, val_)\n'"
keras2onnx/ke2onnx/merge.py,0,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy as np\nfrom ..proto import keras\nfrom ..common.onnx_ops import apply_add, apply_mul, apply_sub\nfrom ..common.onnx_ops import apply_mean, apply_max, OnnxOperatorBuilder\n\n_merge_layer_handlers = {keras.layers.Add: apply_add, keras.layers.Multiply: apply_mul,\n                         keras.layers.Subtract: apply_sub, keras.layers.Average: apply_mean,\n                         keras.layers.Maximum: apply_max}\n\n\ndef convert_keras_merge_layer(scope, operator, container):\n    op = operator.raw_operator\n    if isinstance(op, keras.layers.Subtract) and len(operator.inputs) > 2:\n        raise RuntimeError(\n            'Expected two inputs but got %s. Their names are %s' % (len(operator.inputs), operator.input_full_names))\n\n    apply_merge_operation = _merge_layer_handlers[type(op)]\n\n    intermediate_tensor_name = None\n    for i in range(len(operator.inputs) - 1):\n        if i == 0:\n            left_tensor_name = operator.inputs[0].full_name\n            right_tensor_name = operator.inputs[1].full_name\n        else:\n            if intermediate_tensor_name is None:\n                raise RuntimeError('Tensor name cannot be None')\n            left_tensor_name = intermediate_tensor_name\n            right_tensor_name = operator.inputs[i + 1].full_name\n\n        if (len(operator.inputs) == 2 and i == 0) or (len(operator.inputs) > 2 and i == len(operator.inputs) - 2):\n            # At the last iteration, we need to put the result to Keras layer's output tensor\n            intermediate_tensor_name = operator.outputs[0].full_name\n        else:\n            # Keep accumulate changes through iterations using buffer tensors\n            intermediate_tensor_name = scope.get_unique_variable_name('intermediate_tensor')\n        apply_merge_operation(scope, [left_tensor_name, right_tensor_name], intermediate_tensor_name, container)\n\n    if operator.output_masks:\n        # Keras merge layer compute mask\n        #    masks = [array_ops.expand_dims(m, axis=0) for m in mask if m is not None]\n        #    return K.all(K.concatenate(masks, axis=0), axis=0, keepdims=False)\n        oopb = OnnxOperatorBuilder(container, scope)\n        expanded = []\n        for idx_, i_ in enumerate(operator.input_masks):\n            expanded.append(oopb.add_node('Unsqueeze', i_.full_name, i_.full_name + '_i' + str(idx_), axes=[0]))\n\n        if len(expanded) > 1:\n            concat = oopb.apply_concat(expanded, name=operator.full_name + '_concat')\n        else:\n            concat = expanded[0]\n        cast = oopb.add_node('Cast', concat, name=operator.full_name + '_cast', to=1)\n        reduced = oopb.add_node('ReduceSum', cast, name=operator.full_name + '_reduced', op_version=1, axes=[0],\n                                keepdims=0)\n        oopb.apply_op_with_output('apply_greater',\n                                  [reduced, np.array([0], dtype=np.float32)],\n                                  [operator.output_masks[0].full_name],\n                                  name=operator.raw_operator.name)\n"""
keras2onnx/ke2onnx/pooling.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom .common import get_permutation_config\n\n\ndef convert_keras_pooling_core(scope, operator, container, n_dims,\n                               op_type, input_perm_axes, output_perm_axes):\n    op = operator.raw_operator\n    no_permutation_required = op.data_format == \'channels_first\' if hasattr(op, \'data_format\') else False\n\n    if no_permutation_required:\n        adjusted_pooling_input = operator.inputs[0].full_name\n    else:\n        adjusted_pooling_input = scope.get_unique_variable_name(\'input_transposed\')\n        preprocessor_type = \'Transpose\'\n        preprocessor_attrs = {\'name\': scope.get_unique_operator_name(preprocessor_type), \'perm\': input_perm_axes}\n        container.add_node(preprocessor_type, operator.inputs[0].full_name,\n                           adjusted_pooling_input, **preprocessor_attrs)\n\n    is_global = type(op).__name__.startswith(\'Global\')\n    op_type_prefix = \'Global\' if is_global else \'\'\n    onnx_op_type = ""AveragePool"" if op_type == \'Avg\' else \'MaxPool\'\n    attrs = {}\n    if container.target_opset < 10:\n        op_version = 7\n    elif container.target_opset < 11:\n        op_version = 10\n    else:\n        op_version = 11\n    if not is_global:\n        attrs[\'strides\'] = list(op.strides)\n        attrs[\'kernel_shape\'] = op.pool_size\n        attrs[\'op_version\'] = op_version\n        # In ONNX opset 10, the ceil_mode attribute was added to local MaxPool and AveragePool\n        if container.target_opset >= 10:\n            attrs[\'ceil_mode\'] = 0\n        if op.padding == \'valid\':\n            attrs[\'auto_pad\'] = \'VALID\'\n        elif op.padding == \'same\':\n            attrs[\'auto_pad\'] = \'SAME_UPPER\'\n        else:\n            raise RuntimeError(""Unsupported padding type \'{0}\'"".format(op.padding))\n\n    from keras2onnx.common.onnx_ops import OnnxOperatorBuilder\n    oopb = OnnxOperatorBuilder(container, scope)\n    if no_permutation_required:\n        # In this case, the output of our Pool operator just match what Keras produces.\n        pool_result = oopb.add_node(op_type_prefix + onnx_op_type, adjusted_pooling_input,\n                                    operator.inputs[0].full_name + \'_pooling\', **attrs)\n    else:\n        # Put the output of Pool operator to an intermediate tensor. Laster we will apply a Transpose to match the\n        # original Keras output format\n        pool_result_1 = oopb.add_node(op_type_prefix + onnx_op_type, adjusted_pooling_input,\n                                      operator.inputs[0].full_name + \'_pooling\', **attrs)\n\n        # Generate a final Transpose\n        pool_result = oopb.add_node(\'Transpose\', pool_result_1,\n                                    operator.inputs[0].full_name + \'_transpose\', perm=output_perm_axes)\n\n    if is_global:\n        import numpy as np\n        squeeze_result = oopb.add_node(\'Reshape\',\n                                       [pool_result,\n                                        (\'_start\', oopb.int64, np.array([0, -1], dtype=\'int64\'))],\n                                       operator.inputs[0].full_name + \'_reshape\')\n    else:\n        squeeze_result = pool_result\n\n    container.add_node(\'Identity\', squeeze_result, operator.outputs[0].full_name)\n\n\ndef convert_keras_max_pooling_1d(scope, operator, container):\n    input_perm_axes, output_perm_axes = get_permutation_config(1)\n    convert_keras_pooling_core(scope, operator, container, n_dims=1, op_type=\'Max\',\n                               input_perm_axes=input_perm_axes, output_perm_axes=output_perm_axes)\n\n\ndef convert_keras_max_pooling_2d(scope, operator, container):\n    input_perm_axes, output_perm_axes = get_permutation_config(2)\n    convert_keras_pooling_core(scope, operator, container, n_dims=2, op_type=\'Max\',\n                               input_perm_axes=input_perm_axes, output_perm_axes=output_perm_axes)\n\n\ndef convert_keras_max_pooling_3d(scope, operator, container):\n    input_perm_axes, output_perm_axes = get_permutation_config(3)\n    convert_keras_pooling_core(scope, operator, container, n_dims=3, op_type=\'Max\',\n                               input_perm_axes=input_perm_axes, output_perm_axes=output_perm_axes)\n\n\ndef convert_keras_average_pooling_1d(scope, operator, container):\n    input_perm_axes, output_perm_axes = get_permutation_config(1)\n    convert_keras_pooling_core(scope, operator, container, n_dims=1, op_type=\'Avg\',\n                               input_perm_axes=input_perm_axes, output_perm_axes=output_perm_axes)\n\n\ndef convert_keras_average_pooling_2d(scope, operator, container):\n    input_perm_axes, output_perm_axes = get_permutation_config(2)\n    convert_keras_pooling_core(scope, operator, container, n_dims=2, op_type=\'Avg\',\n                               input_perm_axes=input_perm_axes, output_perm_axes=output_perm_axes)\n\n\ndef convert_keras_average_pooling_3d(scope, operator, container):\n    input_perm_axes, output_perm_axes = get_permutation_config(3)\n    convert_keras_pooling_core(scope, operator, container, n_dims=3, op_type=\'Avg\',\n                               input_perm_axes=input_perm_axes, output_perm_axes=output_perm_axes)\n'"
keras2onnx/ke2onnx/simplernn.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport numpy as np\nfrom ..proto import onnx_proto, keras\nfrom ..common import name_func\nfrom ..common.onnx_ops import (\n    apply_cast,\n    apply_concat,\n    apply_reshape,\n    apply_slice,\n    apply_split,\n    apply_squeeze,\n    apply_transpose,\n    apply_unsqueeze,\n    OnnxOperatorBuilder,\n)\n\nTensorProto = onnx_proto.TensorProto\n\n\ndef extract_input_shape(op):\n    """"""Returns the input shape for a RNN class.\n    """"""\n    input_shape = op.get_input_shape_at(0)\n    if isinstance(input_shape, list):\n        input_shape = input_shape[0]\n    return input_shape\n\n\ndef extract_params(op, hidden_size):\n    """"""Returns a tuple of the SimpleRNN parameters, and converts them into the format for ONNX.\n    """"""\n    params = op.get_weights()\n\n    W = params[0].T\n    R = params[1].T\n\n    B = None\n    if op.use_bias:\n        B = np.zeros((2, hidden_size), dtype=np.float32)\n        B[0] = params[2]\n\n    return W, R, B\n\n\ndef extract_recurrent_activation(activation):\n    activations = keras.activations\n    alpha = None\n    beta = None\n    if activation == activations.sigmoid:\n        onnx_op_type = \'Sigmoid\'\n    elif activation == activations.hard_sigmoid:\n        onnx_op_type = \'HardSigmoid\'\n        alpha = 0.2\n        beta = 0.5\n    elif activation == activations.tanh:\n        onnx_op_type = \'Tanh\'\n    elif activation == activations.relu:\n        onnx_op_type = \'Relu\'\n    elif activation == activations.linear:\n        onnx_op_type = \'Affine\'\n        alpha = 1.0\n    else:\n        raise NotImplementedError(\'The activation %s not supported\' % activation)\n\n    return onnx_op_type, alpha, beta\n\n\ndef extract_activations(fields):\n    """"""Returns a dictionary with the appropriate activations set\n    """"""\n    activation_types = []\n    alphas = []\n    betas = []\n    activations = [extract_recurrent_activation(f) for f in fields]\n    for (activation_type, alpha, beta) in activations:\n        activation_types.append(activation_type.encode(\'utf-8\'))\n        if alpha is not None:\n            alphas.append(alpha)\n        if beta is not None:\n            betas.append(beta)\n\n    attrs = {}\n    attrs[\'activations\'] = activation_types\n    if alphas:\n        attrs[\'activation_alpha\'] = alphas\n    if betas:\n        attrs[\'activation_beta\'] = betas\n    return attrs\n\n\ndef build_parameters(scope, operator, container, bidirectional=False):\n    """"""Returns the parameter initialization values after extracting them from the RNN layer.\n    """"""\n    op = operator.raw_operator\n    _, seq_length, input_size = extract_input_shape(op)\n\n    _name = name_func(scope, operator)\n\n    tensor_w = _name(\'W\')\n    tensor_r = _name(\'R\')\n    tensor_b = \'\'\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        backward_layer = op.backward_layer\n        hidden_size = forward_layer.units\n\n        W, R, B = extract_params(forward_layer, hidden_size)\n        W_back, R_back, B_back = extract_params(backward_layer, hidden_size)\n\n        W = np.concatenate([W, W_back])\n        W_shape = [2, hidden_size, input_size]\n\n        R = np.concatenate([R, R_back])\n        R_shape = [2, hidden_size, hidden_size]\n\n        if (B is None and B_back is not None) or (B is not None and B_back is None):\n            raise ValueError(\'Bidirectional bias must be enabled (or disabled) for both forward \'\n                             \'and backward layers.\')\n\n        if B is not None:\n            B = np.concatenate([B, B_back])\n            B_shape = [2, 2 * hidden_size]\n\n    else:\n        hidden_size = op.units\n\n        W, R, B = extract_params(op, hidden_size)\n        W_shape = [1, hidden_size, input_size]\n        R_shape = [1, hidden_size, hidden_size]\n\n        if B is not None:\n            B_shape = [1, 2 * hidden_size]\n\n    # Create initializers\n    container.add_initializer(tensor_w, TensorProto.FLOAT, W_shape, W.flatten())\n    container.add_initializer(tensor_r, TensorProto.FLOAT, R_shape, R.flatten())\n\n    if B is not None:\n        tensor_b = _name(\'B\')\n        container.add_initializer(tensor_b, TensorProto.FLOAT, B_shape, B.flatten())\n\n    return tensor_w, tensor_r, tensor_b\n\n\ndef build_sequence_lengths(scope, operator, container):\n    """"""Uses the masking layer to calculate the sequence lengths. If there is no\n    masking layer, then it returns an empty input for the sequence lengths.\n    """"""\n    # Masking input must be present\n    if len(operator.input_masks) != 1:\n        return \'\'\n\n    input_mask_name = operator.input_masks[0].full_name\n    mask_cast = scope.get_unique_operator_name(operator.full_name + \'_mask_cast\')\n    sequence_lengths = scope.get_unique_operator_name(operator.full_name + \'_seq_lens\')\n\n    apply_cast(scope, input_mask_name, mask_cast, container, to=TensorProto.INT32)\n    container.add_node(\'ReduceSum\', mask_cast, sequence_lengths, keepdims=False, axes=[-1])\n    return sequence_lengths\n\n\ndef build_initial_states(scope, operator, container, bidirectional=False):\n    """"""Reshapes the initial input states. If there are no states present as inputs, then\n    it returns an empty input for the initial hidden states.\n    """"""\n    # Initial hidden states\n    if len(operator.inputs) == 1:\n        return \'\'\n\n    op = operator.raw_operator\n    _name = name_func(scope, operator)\n\n    initial_h = _name(\'initial_h\')\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        hidden_size = forward_layer.units\n        desired_shape = [1, -1, hidden_size]\n\n        # Combine the forward and backward layers\n        forward_h = _name(\'initial_h_forward\')\n        backward_h = _name(\'initial_h_backward\')\n\n        # Handle LSTM initial hidden case to enable code reuse\n        if len(operator.inputs) > 4:\n            f, b = 1, 3\n        else:\n            f, b = 1, 2\n\n        apply_reshape(scope, operator.inputs[f].full_name, forward_h, container, desired_shape=desired_shape)\n        apply_reshape(scope, operator.inputs[b].full_name, backward_h, container, desired_shape=desired_shape)\n\n        apply_concat(scope, [forward_h, backward_h], initial_h, container)\n\n    else:\n        # Unsqueeze dim 0 to represent num_directions\n        input_h = operator.inputs[1].full_name\n        apply_unsqueeze(scope, input_h, initial_h, container, axes=[0])\n    return initial_h\n\n\ndef build_attributes(scope, operator, container, bidirectional=False):\n    """"""Returns a dictionary of attributes for the RNN layer.\n    """"""\n    op = operator.raw_operator\n\n    attrs = {}\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        backward_layer = op.backward_layer\n\n        attrs[\'direction\'] = \'bidirectional\'\n        attrs[\'hidden_size\'] = forward_layer.units\n\n        activations = []\n        if hasattr(forward_layer, \'activation\'):\n            activations.append(forward_layer.activation)\n\n        if hasattr(backward_layer, \'activation\'):\n            activations.append(backward_layer.activation)\n\n        if len(activations) > 0:\n            attrs.update(extract_activations(activations))\n\n    else:\n        attrs[\'direction\'] = \'reverse\' if op.go_backwards else \'forward\'\n        attrs[\'hidden_size\'] = op.units\n\n        if hasattr(op, \'activation\'):\n            attrs.update(extract_activations([op.activation]))\n\n    return attrs\n\n\ndef build_output(scope, operator, container, output_names, bidirectional=False):\n    """"""Builds the output stages for the RNN.\n    """"""\n    rnn_y, rnn_h = output_names\n\n    op = operator.raw_operator\n    _, seq_length, input_size = extract_input_shape(op)\n    is_static_shape = seq_length is not None\n\n    _name = name_func(scope, operator)\n\n    oopb = OnnxOperatorBuilder(container, scope)\n\n    # Define seq_dim\n    if not is_static_shape:\n        input_name = operator.inputs[0].full_name\n        input_shape_tensor = oopb.add_node(\'Shape\', [input_name], input_name + \'_input_shape_tensor\')\n\n        seq_dim = input_name + \'_seq_dim\'\n        apply_slice(scope, input_shape_tensor, seq_dim, container, [1], [2], axes=[0])\n\n    if bidirectional:\n        time_major = is_time_major(op, bidirectional)\n        forward_layer = op.forward_layer\n\n        hidden_size = forward_layer.units\n        output_seq = forward_layer.return_sequences\n\n        merge_concat = False\n        if hasattr(op, \'merge_mode\'):\n            if op.merge_mode not in [\'concat\', None]:\n                raise ValueError(\'Bidirectional only supports merge_mode=\\\'concat\\\' \'\n                                 \'but got %s\' % op.merge_mode)\n            if op.merge_mode is not None:\n                merge_concat = True\n\n        if output_seq:\n            lstm_out = _name(\'y_transposed\')\n            if not time_major:\n                # Transpose ONNX RNN Y with shape (T, D, N, C\') into (N, T, D, C\')\n                apply_transpose(scope, rnn_y, lstm_out, container, perm=[2, 0, 1, 3])\n            else:\n                # Transpose RNN Y with shape (T, D, N, C) into (T, N, D, C)\n                apply_transpose(scope, rnn_y, lstm_out, container, perm=[0, 2, 1, 3])\n            if merge_concat:\n                # In this case, only one Keras output with shape (N, T, 2 * C\') should be produced.\n                # ( T, N, 2*C ) if it was time major.\n                apply_reshape(scope, lstm_out, operator.outputs[0].full_name, container,\n                              desired_shape=[0, 0, 2 * hidden_size])\n            else:\n                # If merge_mode=None, two tensors should be generated. The first/second tensor is the output of\n                # forward/backward pass.\n\n                # Split the transposed Y with shape (T, N, D, C\') into (T, N, 1, C\') and (T, N, 1, C\')\n                forward_y = _name(\'Y_forward\')\n                backward_y = _name(\'Y_backward\')\n                axis_direction = 2\n                apply_split(scope, lstm_out, [forward_y, backward_y], container, axis=axis_direction)\n\n                # Change (T, N, 1, C\') into (T, N, C\') to meet Keras spec\n                apply_squeeze(scope, forward_y, operator.outputs[0].full_name, container, axes=[axis_direction])\n                apply_squeeze(scope, backward_y, operator.outputs[1].full_name, container, axes=[axis_direction])\n        else:\n            perm = [1, 0, 2]\n            if merge_concat:\n                # In this case, only one Keras output with shape (N, 2 * C\') should be produced\n\n                # Transpose ONNX RNN Y_h with shape (D, N, C\') into (N, D, C\')\n                transposed_h = _name(\'Y_h_transposed\')\n                apply_transpose(scope, rnn_h, transposed_h, container, perm=perm)\n\n                # Flatten ONNX (N, D, C\') into (N, D * C\')\n                oopb.apply_op_with_output(""apply_flatten"",\n                                          transposed_h,\n                                          operator.outputs[0].full_name,\n                                          name=operator.full_name + \'_flatten\',\n                                          axis=1)\n            else:\n                # If merge_mode=None, two tensors should be generated. The first/second tensor is the output of\n                # forward/backward pass.\n\n                # Transpose ONNX RNN Y_h with shape (D, N, C\') into (N, D, C\')\n                transposed_h = _name(\'Y_h_transposed\')\n                apply_transpose(scope, rnn_h, transposed_h, container, perm=perm)\n\n                # Split the transposed Y with shape (T, N, D, C\') into (T, N, 1, C\') and (T, N, 1, C\')\n                forward_y = _name(\'Y_forward\')\n                backward_y = _name(\'Y_backward\')\n                axis_direction = 1\n                apply_split(scope, transposed_h, [forward_y, backward_y], container, axis=axis_direction)\n\n                # Change (T, N, 1, C\') into (T, N, C\') to meet Keras spec\n                apply_squeeze(scope, forward_y, operator.outputs[0].full_name, container, axes=[axis_direction])\n                apply_squeeze(scope, backward_y, operator.outputs[1].full_name, container, axes=[axis_direction])\n    else:\n        hidden_size = op.units\n        output_seq = op.return_sequences\n\n        output_name = operator.outputs[0].full_name\n        transposed_y = scope.get_unique_variable_name(operator.full_name + \'_y_transposed\')\n\n        # Determine the source, transpose permutation, and output shape\n        if output_seq:\n            source = rnn_y\n            perm = [2, 0, 1, 3]\n            if is_static_shape:\n                desired_shape = [-1, seq_length, hidden_size]\n            elif container.target_opset < 5:\n                # Before Reshape-5 you can not take the sequence dimension in as an input\n                raise ValueError(\'At least opset 5 is required for output sequences\')\n            else:\n                # Dynamically determine the output shape based on the sequence dimension\n                shape_values = [\n                    (\'_a\', oopb.int64, np.array([-1], dtype=\'int64\')),\n                    seq_dim,\n                    (\'_b\', oopb.int64, np.array([hidden_size], dtype=\'int64\')),\n                ]\n                shape_name = _name(\'_output_seq_shape\')\n                desired_shape = oopb.add_node(\'Concat\', shape_values, shape_name, axis=0)\n        else:\n            # Use the last hidden states directly\n            source = rnn_h\n            perm = [1, 0, 2]\n            desired_shape = [-1, hidden_size]\n\n        apply_transpose(scope, source, transposed_y, container, perm=perm)\n        apply_reshape(scope, transposed_y, output_name, container, desired_shape=desired_shape)\n\n\ndef build_output_states(scope, operator, container, output_names, bidirectional=False):\n    """"""Builds the output hidden states for the RNN layer.\n    """"""\n    _, rnn_h = output_names\n    op = operator.raw_operator\n\n    if bidirectional:\n        forward_layer = op.forward_layer\n        output_state = forward_layer.return_state\n\n        if output_state:\n            # Split rnn_h into forward and backward directions\n            output_names = [o.full_name for o in operator.outputs[1:]]\n            split_names = [\'{}_{}\'.format(rnn_h, d) for d in (\'forward\', \'backward\')]\n\n            apply_split(scope, rnn_h, split_names, container)\n\n            for split_name, output_name in zip(split_names, output_names):\n                apply_squeeze(scope, split_name, output_name, container)\n\n    else:\n        output_state = op.return_state\n\n        if output_state:\n            output_h = operator.outputs[1].full_name\n            apply_squeeze(scope, rnn_h, output_h, container)\n\n\ndef is_time_major(op, bidirectional):\n    if bidirectional:\n        time_major = op.forward_layer.time_major if hasattr(op.forward_layer, ""time_major"") else False\n    else:\n        time_major = op.time_major if hasattr(op, ""time_major"") else False\n    return time_major\n\n\ndef convert_keras_simple_rnn(scope, operator, container, bidirectional=False):\n    op = operator.raw_operator\n\n    _name = name_func(scope, operator)\n\n    if bidirectional:\n        output_seq = op.forward_layer.return_sequences\n    else:\n        output_seq = op.return_sequences\n    time_major = is_time_major(op, bidirectional)\n\n    # Inputs\n    rnn_x = operator.inputs[0].full_name\n    if not time_major:\n        rnn_x = _name(\'X\')\n        apply_transpose(scope, operator.inputs[0].full_name, rnn_x, container, perm=[1, 0, 2])\n    tensor_w, tensor_r, tensor_b = build_parameters(scope, operator, container, bidirectional)\n    sequence_lengths = build_sequence_lengths(scope, operator, container)\n    initial_h = build_initial_states(scope, operator, container, bidirectional)\n\n    input_names = [\n        rnn_x,\n        tensor_w,\n        tensor_r,\n        tensor_b,\n        sequence_lengths,\n        initial_h,\n    ]\n\n    # Attributes\n    attrs = build_attributes(scope, operator, container, bidirectional)\n\n    # Outputs\n    output_names = [_name(\'Y\'), _name(\'Y_h\')]\n\n    oopb = OnnxOperatorBuilder(container, scope)\n    oopb.apply_op_with_output(\'apply_rnn\',\n                              input_names,\n                              output_names,\n                              name=op.name,\n                              output_seq=output_seq,\n                              **attrs)\n\n    build_output(scope, operator, container, output_names, bidirectional)\n    build_output_states(scope, operator, container, output_names, bidirectional)\n'"
keras2onnx/ke2onnx/upsample.py,1,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport collections\nfrom ..common.onnx_ops import apply_transpose, apply_upsample\nfrom .common import get_permutation_config\nfrom ..proto import is_tf_keras\nif is_tf_keras:\n    from ..proto import is_tensorflow_older_than\nfrom ..proto.tfcompat import is_tf2\n\n\ndef convert_keras_upsample(scope, operator, container, n_dims):\n    op = operator.raw_operator\n    # op.size type is tuple in keras.io, even if we set a int in keras.layers API.\n    # op.size type can be int in tf.keras.\n    op_size = op.size if isinstance(op.size, collections.abc.Iterable) else [op.size]\n    scales_sub = list(d for d in op_size)\n    if n_dims == 1:\n        shape_gap = len(op.input_shape) - len(scales_sub)\n        if shape_gap == 1:\n            scales = [1] + scales_sub\n        elif shape_gap == 2:\n            scales = [1] + scales_sub + [1]\n        else:\n            raise ValueError('shape_gap should be 1 or 2 for UpSampling1D')\n    elif n_dims == 2 or n_dims == 3:\n        # Always create the list of sampling factors in channels_first format because the input will be converted into\n        # channels_first if it's in channels_last\n        scales = [1, 1] + list(d for d in op_size)\n    else:\n        raise ValueError('Unsupported dimension %s when converting Keras Upsampling layer' % n_dims)\n\n    mode = 'nearest'\n    if hasattr(op, 'interpolation'):\n        mode = 'linear' if op.interpolation.endswith('linear') else op.interpolation\n\n    # Derive permutation configuration. If the Keras input format is not channels_first, this configuration may be used\n    # to manipulate the input and output of ONNX Upsample.\n    input_perm_axes, output_perm_axes = get_permutation_config(n_dims)\n    channels_first = n_dims > 1 and op.data_format == 'channels_first'\n    no_permutation_required = channels_first or n_dims < 2\n\n    # Before creating the main Upsample operator, we need to permute the input tensor if the original operator is\n    # working under channels_last mode.\n    if no_permutation_required:\n        # No permutation is required. Use input as it is.\n        input_tensor_name = operator.inputs[0].full_name\n    else:\n        # Permute the original input and then use the permuted result as the input of ONNX Upsample\n        input_tensor_name = scope.get_unique_variable_name(operator.inputs[0].full_name + '_permuted')\n        apply_transpose(scope, operator.inputs[0].full_name, input_tensor_name, container, perm=input_perm_axes)\n\n    # If no_permutation_required is True, we don't need to permute the output of ONNX Upsample.\n    # Otherwise, similar to Crop's conversion, a Transpose would be added.\n    coordinate_transformation_mode = None\n    if mode == 'linear':\n        if is_tf_keras:\n            if is_tf2:\n                coordinate_transformation_mode = 'half_pixel'\n            else:\n                if not is_tensorflow_older_than('1.15.0'):\n                    if operator.target_opset < 11:\n                        raise ValueError('tf_keras upsample bilinear mode is not supported until opset 11')\n                    else:\n                        coordinate_transformation_mode = 'half_pixel'\n\n    if coordinate_transformation_mode is None:\n        coordinate_transformation_mode = 'asymmetric'\n\n    if no_permutation_required:\n        apply_upsample(scope, input_tensor_name, operator.outputs[0].full_name, container,\n                       mode=mode, coordinate_transformation_mode=coordinate_transformation_mode, scales=scales)\n    else:\n        upsampled_tensor_name = scope.get_unique_variable_name(input_tensor_name + '_upsampled')\n        apply_upsample(scope, input_tensor_name, upsampled_tensor_name, container,\n                       mode=mode, coordinate_transformation_mode=coordinate_transformation_mode, scales=scales)\n        apply_transpose(scope, upsampled_tensor_name, operator.outputs[0].full_name, container, perm=output_perm_axes)\n\n\ndef convert_keras_upsample_1d(scope, operator, container):\n    convert_keras_upsample(scope, operator, container, n_dims=1)\n\n\ndef convert_keras_upsample_2d(scope, operator, container):\n    convert_keras_upsample(scope, operator, container, n_dims=2)\n\n\ndef convert_keras_upsample_3d(scope, operator, container):\n    convert_keras_upsample(scope, operator, container, n_dims=3)\n"""
keras2onnx/ke2onnx/zeropad.py,0,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nfrom ..common.onnx_ops import apply_pad, apply_transpose\nfrom .common import get_permutation_config\n\n\ndef get_padding_config(op, n_dims):\n    pads = op.padding\n    if isinstance(pads, int):\n        pads = [pads] * n_dims\n    else:\n        pads = list(pads)\n    pads[:0] = [0, 0]\n\n    extended_n_dims = n_dims + 2  # adding N, C\n    if len(pads) == extended_n_dims:\n        full_pads = [None] * extended_n_dims\n        for i in range(extended_n_dims):\n            if isinstance(pads[i], int):\n                full_pads[i] = [pads[i]] * 2\n            else:\n                full_pads[i] = pads[i]\n    else:\n        raise RuntimeError(""padding parameter\'s dim({0}) is not correct."".format(n_dims))\n\n    # keras schema ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad))\n    # ONNX\'s [x1_begin, x2_begin...x1_end, x2_end,...], need re-shuffle.\n    onnx_pads = [None] * 2 * extended_n_dims\n    for i in range(extended_n_dims):\n        onnx_pads[i] = full_pads[i][0]\n        onnx_pads[extended_n_dims + i] = full_pads[i][1]\n\n    return onnx_pads\n\n\ndef convert_keras_zero_pad(scope, operator, container, n_dims):\n    op = operator.raw_operator\n\n    # Derive permutation configuration. If the Keras input format is not channels_first, this configuration may be used\n    # to manipulate the input and output of ONNX Upsample.\n    input_perm_axes, output_perm_axes = get_permutation_config(n_dims)\n    channels_first = n_dims > 1 and op.data_format == \'channels_first\'\n\n    # Before creating the main Upsample operator, we need to permute the input tensor if the original operator is\n    # working under channels_last mode.\n    if channels_first:\n        # No permutation is required. Use input as it is.\n        input_tensor_name = operator.inputs[0].full_name\n    else:\n        # Permute the original input and then use the permuted result as the input of ONNX Upsample\n        input_tensor_name = scope.get_unique_variable_name(operator.inputs[0].full_name + \'_permuted\')\n        apply_transpose(scope, operator.inputs[0].full_name, input_tensor_name, container, perm=input_perm_axes)\n\n    # Prepare attributes for ONNX Pad\n    mode = \'constant\'\n    pads = get_padding_config(op, n_dims)\n\n    # If channels_first is True, we don\'t need to permute the output of ONNX Upsample. Otherwise, similar to Crop\'s\n    # conversion, a Transpose would be added.\n    if channels_first:\n        apply_pad(scope, input_tensor_name, operator.outputs[0].full_name, container, mode=mode, pads=pads, value=0.)\n    else:\n        intermediate_tensor_name = scope.get_unique_variable_name(input_tensor_name + \'_padded\')\n        apply_pad(scope, input_tensor_name, intermediate_tensor_name, container, mode=mode, pads=pads, value=0.)\n        apply_transpose(scope, intermediate_tensor_name, operator.outputs[0].full_name, container,\n                        perm=output_perm_axes)\n\n\ndef convert_keras_zero_pad_1d(scope, operator, container):\n    convert_keras_zero_pad(scope, operator, container, 1)\n\n\ndef convert_keras_zero_pad_2d(scope, operator, container):\n    convert_keras_zero_pad(scope, operator, container, 2)\n\n\ndef convert_keras_zero_pad_3d(scope, operator, container):\n    convert_keras_zero_pad(scope, operator, container, 3)\n'"
keras2onnx/proto/__init__.py,1,"b""###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport onnx\nimport tensorflow\nfrom distutils.version import StrictVersion\n\n# Rather than using ONNX protobuf definition throughout our codebase, we import ONNX protobuf definition here so that\n# we can conduct quick fixes by overwriting ONNX functions without changing any lines elsewhere.\nfrom onnx import onnx_pb as onnx_proto\nfrom onnx import helper\nfrom onnx import save_model as save_model\n\n\ndef _check_onnx_version():\n    import pkg_resources\n    min_required_version = pkg_resources.parse_version('1.0.1')\n    current_version = pkg_resources.get_distribution('onnx').parsed_version\n    assert current_version >= min_required_version, 'Keras2ONNX requires ONNX version 1.0.1 or a newer one'\n\n\n_check_onnx_version()\n\n\ndef is_tensorflow_older_than(version_str):\n    return StrictVersion(tensorflow.__version__.split('-')[0]) < StrictVersion(version_str)\n\n\ndef is_tensorflow_later_than(version_str):\n    return StrictVersion(tensorflow.__version__.split('-')[0]) > StrictVersion(version_str)\n\n\nis_tf_keras = False\nstr_tk_keras = os.environ.get('TF_KERAS', None)\nif str_tk_keras is None:\n    # With tensorflow 2.x, be default we loaded tf.keras as the framework, instead of Keras\n    is_tf_keras = not is_tensorflow_older_than('2.0.0')\nelse:\n    is_tf_keras = str_tk_keras != '0'\n\nif is_tf_keras:\n    from tensorflow.python import keras\nelse:\n    try:\n        import keras\n    except ImportError:\n        is_tf_keras = True\n        from tensorflow.python import keras\n\n\ndef is_keras_older_than(version_str):\n    return StrictVersion(keras.__version__.split('-')[0]) < StrictVersion(version_str)\n\n\ndef is_keras_later_than(version_str):\n    return StrictVersion(keras.__version__.split('-')[0]) > StrictVersion(version_str)\n"""
keras2onnx/proto/tfcompat.py,4,"b'###############################################################################\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n###############################################################################\nimport os\nimport tensorflow as _tf\n\nfrom distutils.version import StrictVersion\n\nis_tf2 = StrictVersion(_tf.__version__.split(\'-\')[0]) >= StrictVersion(\'2.0.0\')\n\n\ndef normalize_tensor_shape(tensor_shape):\n    if is_tf2:\n        return [d for d in tensor_shape]\n    else:\n        return [d.value for d in tensor_shape]\n\n\ndef dump_graph_into_tensorboard(tf_graph):\n    # type: (_tf.Graph) -> None\n    _tb_log_dir = os.environ.get(\'TB_LOG_DIR\')\n    if _tb_log_dir:\n        if is_tf2:\n            from tensorflow.python.ops.summary_ops_v2 import graph as write_graph\n            pb_visual_writer = _tf.summary.create_file_writer(_tb_log_dir)\n            with pb_visual_writer.as_default():\n                write_graph(tf_graph)\n        else:\n            from tensorflow.python.summary import summary\n            pb_visual_writer = summary.FileWriter(_tb_log_dir)\n            pb_visual_writer.add_graph(tf_graph)\n\n\nif is_tf2:\n    tensorflow = _tf.compat.v1\n\n    def is_subclassed(layer):\n        """"""Returns True if the object is a subclassed layer or subclassed model.""""""\n        return (layer.__module__.find(\'keras.engine\') == -1 and\n                layer.__module__.find(\'keras.layers\') == -1)\nelse:\n    tensorflow = _tf\n\n    def is_subclassed(layer):\n        return False\n'"
applications/model_source/densenet_1/densenet_1.py,0,"b'# From https://github.com/titu1994/DenseNet/blob/master/densenet.py\n# Modifications Copyright (c) Microsoft.\n\n\'\'\'DenseNet models for Keras.\n# Reference\n- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)\n- [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326.pdf)\n\'\'\'\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport warnings\n\nfrom keras2onnx.proto import keras\nfrom keras.models import Model\nfrom keras.layers.core import Dense, Dropout, Activation, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\nfrom keras.layers.pooling import AveragePooling2D, MaxPooling2D\nfrom keras.layers.pooling import GlobalAveragePooling2D\nfrom keras.layers import Input\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.utils.layer_utils import convert_all_kernels_in_model, convert_dense_weights_data_format\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nfrom keras.applications.imagenet_utils import decode_predictions\nimport keras.backend as K\n\nfrom subpixel import SubPixelUpscaling\n\nDENSENET_121_WEIGHTS_PATH = r\'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-121-32.h5\'\nDENSENET_161_WEIGHTS_PATH = r\'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-161-48.h5\'\nDENSENET_169_WEIGHTS_PATH = r\'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-169-32.h5\'\nDENSENET_121_WEIGHTS_PATH_NO_TOP = r\'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-121-32-no-top.h5\'\nDENSENET_161_WEIGHTS_PATH_NO_TOP = r\'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-161-48-no-top.h5\'\nDENSENET_169_WEIGHTS_PATH_NO_TOP = r\'https://github.com/titu1994/DenseNet/releases/download/v3.0/DenseNet-BC-169-32-no-top.h5\'\n\ndef preprocess_input(x, data_format=None):\n    """"""Preprocesses a tensor encoding a batch of images.\n\n    # Arguments\n        x: input Numpy tensor, 4D.\n        data_format: data format of the image tensor.\n\n    # Returns\n        Preprocessed tensor.\n    """"""\n    if data_format is None:\n        data_format = K.image_data_format()\n    assert data_format in {\'channels_last\', \'channels_first\'}\n\n    if data_format == \'channels_first\':\n        if x.ndim == 3:\n            # \'RGB\'->\'BGR\'\n            x = x[::-1, ...]\n            # Zero-center by mean pixel\n            x[0, :, :] -= 103.939\n            x[1, :, :] -= 116.779\n            x[2, :, :] -= 123.68\n        else:\n            x = x[:, ::-1, ...]\n            x[:, 0, :, :] -= 103.939\n            x[:, 1, :, :] -= 116.779\n            x[:, 2, :, :] -= 123.68\n    else:\n        # \'RGB\'->\'BGR\'\n        x = x[..., ::-1]\n        # Zero-center by mean pixel\n        x[..., 0] -= 103.939\n        x[..., 1] -= 116.779\n        x[..., 2] -= 123.68\n\n    x *= 0.017 # scale values\n\n    return x\n\n\ndef DenseNet(input_shape=None, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1, nb_layers_per_block=-1,\n             bottleneck=False, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, subsample_initial_block=False,\n             include_top=True, weights=None, input_tensor=None,\n             classes=10, activation=\'softmax\'):\n    \'\'\'Instantiate the DenseNet architecture,\n        optionally loading weights pre-trained\n        on CIFAR-10. Note that when using TensorFlow,\n        for best performance you should set\n        `image_data_format=\'channels_last\'` in your Keras config\n        at ~/.keras/keras.json.\n        The model and the weights are compatible with both\n        TensorFlow and Theano. The dimension ordering\n        convention used by the model is the one\n        specified in your Keras config file.\n        # Arguments\n            input_shape: optional shape tuple, only to be specified\n                if `include_top` is False (otherwise the input shape\n                has to be `(32, 32, 3)` (with `channels_last` dim ordering)\n                or `(3, 32, 32)` (with `channels_first` dim ordering).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 8.\n                E.g. `(200, 200, 3)` would be one valid value.\n            depth: number or layers in the DenseNet\n            nb_dense_block: number of dense blocks to add to end (generally = 3)\n            growth_rate: number of filters to add per dense block\n            nb_filter: initial number of filters. -1 indicates initial\n                number of filters is 2 * growth_rate\n            nb_layers_per_block: number of layers in each dense block.\n                Can be a -1, positive integer or a list.\n                If -1, calculates nb_layer_per_block from the network depth.\n                If positive integer, a set number of layers per dense block.\n                If list, nb_layer is used as provided. Note that list size must\n                be (nb_dense_block + 1)\n            bottleneck: flag to add bottleneck blocks in between dense blocks\n            reduction: reduction factor of transition blocks.\n                Note : reduction value is inverted to compute compression.\n            dropout_rate: dropout rate\n            weight_decay: weight decay rate\n            subsample_initial_block: Set to True to subsample the initial convolution and\n                add a MaxPool2D before the dense blocks are added.\n            include_top: whether to include the fully-connected\n                layer at the top of the network.\n            weights: one of `None` (random initialization) or\n                \'imagenet\' (pre-training on ImageNet)..\n            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n                to use as image input for the model.\n            classes: optional number of classes to classify images\n                into, only to be specified if `include_top` is True, and\n                if no `weights` argument is specified.\n            activation: Type of activation at the top layer. Can be one of \'softmax\' or \'sigmoid\'.\n                Note that if sigmoid is used, classes must be 1.\n        # Returns\n            A Keras model instance.\n        \'\'\'\n\n    if weights not in {\'imagenet\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `cifar10` \'\n                         \'(pre-training on CIFAR-10).\')\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` as ImageNet with `include_top`\'\n                         \' as true, `classes` should be 1000\')\n\n    if activation not in [\'softmax\', \'sigmoid\']:\n        raise ValueError(\'activation must be one of ""softmax"" or ""sigmoid""\')\n\n    if activation == \'sigmoid\' and classes != 1:\n        raise ValueError(\'sigmoid activation can only be used when classes = 1\')\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=32,\n                                      min_size=8,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = __create_dense_net(classes, img_input, include_top, depth, nb_dense_block,\n                           growth_rate, nb_filter, nb_layers_per_block, bottleneck, reduction,\n                           dropout_rate, weight_decay, subsample_initial_block, activation)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name=\'densenet\')\n\n    # load weights\n    if weights == \'imagenet\':\n        weights_loaded = False\n\n        if (depth == 121) and (nb_dense_block == 4) and (growth_rate == 32) and (nb_filter == 64) and \\\n                (bottleneck is True) and (reduction == 0.5) and (dropout_rate == 0.0) and (subsample_initial_block):\n            if include_top:\n                weights_path = get_file(\'DenseNet-BC-121-32.h5\',\n                                        DENSENET_121_WEIGHTS_PATH,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'a439dd41aa672aef6daba4ee1fd54abd\')\n            else:\n                weights_path = get_file(\'DenseNet-BC-121-32-no-top.h5\',\n                                        DENSENET_121_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'55e62a6358af8a0af0eedf399b5aea99\')\n            model.load_weights(weights_path)\n            weights_loaded = True\n\n        if (depth == 161) and (nb_dense_block == 4) and (growth_rate == 48) and (nb_filter == 96) and \\\n                (bottleneck is True) and (reduction == 0.5) and (dropout_rate == 0.0) and (subsample_initial_block):\n            if include_top:\n                weights_path = get_file(\'DenseNet-BC-161-48.h5\',\n                                        DENSENET_161_WEIGHTS_PATH,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'6c326cf4fbdb57d31eff04333a23fcca\')\n            else:\n                weights_path = get_file(\'DenseNet-BC-161-48-no-top.h5\',\n                                        DENSENET_161_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'1a9476b79f6b7673acaa2769e6427b92\')\n            model.load_weights(weights_path)\n            weights_loaded = True\n\n        if (depth == 169) and (nb_dense_block == 4) and (growth_rate == 32) and (nb_filter == 64) and \\\n                (bottleneck is True) and (reduction == 0.5) and (dropout_rate == 0.0) and (subsample_initial_block):\n            if include_top:\n                weights_path = get_file(\'DenseNet-BC-169-32.h5\',\n                                        DENSENET_169_WEIGHTS_PATH,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'914869c361303d2e39dec640b4e606a6\')\n            else:\n                weights_path = get_file(\'DenseNet-BC-169-32-no-top.h5\',\n                                        DENSENET_169_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'89c19e8276cfd10585d5fadc1df6859e\')\n            model.load_weights(weights_path)\n            weights_loaded = True\n\n        if weights_loaded:\n            if K.backend() == \'theano\':\n                convert_all_kernels_in_model(model)\n\n            if K.image_data_format() == \'channels_first\' and K.backend() == \'tensorflow\':\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image data format convention \'\n                              \'(`image_data_format=""channels_first""`). \'\n                              \'For best performance, set \'\n                              \'`image_data_format=""channels_last""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n\n            print(""Weights for the model were loaded successfully"")\n\n    return model\n\n\ndef DenseNetFCN(input_shape, nb_dense_block=5, growth_rate=16, nb_layers_per_block=4,\n                reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, init_conv_filters=48,\n                include_top=True, weights=None, input_tensor=None, classes=1, activation=\'softmax\',\n                upsampling_conv=128, upsampling_type=\'deconv\'):\n    \'\'\'Instantiate the DenseNet FCN architecture.\n        Note that when using TensorFlow,\n        for best performance you should set\n        `image_data_format=\'channels_last\'` in your Keras config\n        at ~/.keras/keras.json.\n        # Arguments\n            nb_dense_block: number of dense blocks to add to end (generally = 3)\n            growth_rate: number of filters to add per dense block\n            nb_layers_per_block: number of layers in each dense block.\n                Can be a positive integer or a list.\n                If positive integer, a set number of layers per dense block.\n                If list, nb_layer is used as provided. Note that list size must\n                be (nb_dense_block + 1)\n            reduction: reduction factor of transition blocks.\n                Note : reduction value is inverted to compute compression.\n            dropout_rate: dropout rate\n            init_conv_filters: number of layers in the initial convolution layer\n            include_top: whether to include the fully-connected\n                layer at the top of the network.\n            weights: one of `None` (random initialization) or\n                \'cifar10\' (pre-training on CIFAR-10)..\n            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n                to use as image input for the model.\n            input_shape: optional shape tuple, only to be specified\n                if `include_top` is False (otherwise the input shape\n                has to be `(32, 32, 3)` (with `channels_last` dim ordering)\n                or `(3, 32, 32)` (with `channels_first` dim ordering).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 8.\n                E.g. `(200, 200, 3)` would be one valid value.\n            classes: optional number of classes to classify images\n                into, only to be specified if `include_top` is True, and\n                if no `weights` argument is specified.\n            activation: Type of activation at the top layer. Can be one of \'softmax\' or \'sigmoid\'.\n                Note that if sigmoid is used, classes must be 1.\n            upsampling_conv: number of convolutional layers in upsampling via subpixel convolution\n            upsampling_type: Can be one of \'upsampling\', \'deconv\' and\n                \'subpixel\'. Defines type of upsampling algorithm used.\n            batchsize: Fixed batch size. This is a temporary requirement for\n                computation of output shape in the case of Deconvolution2D layers.\n                Parameter will be removed in next iteration of Keras, which infers\n                output shape of deconvolution layers automatically.\n        # Returns\n            A Keras model instance.\n    \'\'\'\n\n    if weights not in {None}:\n        raise ValueError(\'The `weights` argument should be \'\n                         \'`None` (random initialization) as no \'\n                         \'model weights are provided.\')\n\n    upsampling_type = upsampling_type.lower()\n\n    if upsampling_type not in [\'upsampling\', \'deconv\', \'subpixel\']:\n        raise ValueError(\'Parameter ""upsampling_type"" must be one of ""upsampling"", \'\n                         \'""deconv"" or ""subpixel"".\')\n\n    if input_shape is None:\n        raise ValueError(\'For fully convolutional models, input shape must be supplied.\')\n\n    if type(nb_layers_per_block) is not list and nb_dense_block < 1:\n        raise ValueError(\'Number of dense layers per block must be greater than 1. Argument \'\n                         \'value was %d.\' % (nb_layers_per_block))\n\n    if activation not in [\'softmax\', \'sigmoid\']:\n        raise ValueError(\'activation must be one of ""softmax"" or ""sigmoid""\')\n\n    if activation == \'sigmoid\' and classes != 1:\n        raise ValueError(\'sigmoid activation can only be used when classes = 1\')\n\n    # Determine proper input shape\n    min_size = 2 ** nb_dense_block\n\n    if K.image_data_format() == \'channels_first\':\n        if input_shape is not None:\n            if ((input_shape[1] is not None and input_shape[1] < min_size) or\n                    (input_shape[2] is not None and input_shape[2] < min_size)):\n                raise ValueError(\'Input size must be at least \' +\n                                 str(min_size) + \'x\' + str(min_size) + \', got \'\n                                                                       \'`input_shape=\' + str(input_shape) + \'`\')\n        else:\n            input_shape = (classes, None, None)\n    else:\n        if input_shape is not None:\n            if ((input_shape[0] is not None and input_shape[0] < min_size) or\n                    (input_shape[1] is not None and input_shape[1] < min_size)):\n                raise ValueError(\'Input size must be at least \' +\n                                 str(min_size) + \'x\' + str(min_size) + \', got \'\n                                                                       \'`input_shape=\' + str(input_shape) + \'`\')\n        else:\n            input_shape = (None, None, classes)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = __create_fcn_dense_net(classes, img_input, include_top, nb_dense_block,\n                               growth_rate, reduction, dropout_rate, weight_decay,\n                               nb_layers_per_block, upsampling_conv, upsampling_type,\n                               init_conv_filters, input_shape, activation)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name=\'fcn-densenet\')\n\n    return model\n\n\ndef DenseNetImageNet121(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=\'imagenet\',\n                        input_tensor=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=121, nb_dense_block=4, growth_rate=32, nb_filter=64,\n                    nb_layers_per_block=[6, 12, 24, 16], bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n                    include_top=include_top, weights=weights, input_tensor=input_tensor,\n                    classes=classes, activation=activation)\n\n\ndef DenseNetImageNet169(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=\'imagenet\',\n                        input_tensor=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=169, nb_dense_block=4, growth_rate=32, nb_filter=64,\n                    nb_layers_per_block=[6, 12, 32, 32], bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n                    include_top=include_top, weights=weights, input_tensor=input_tensor,\n                    classes=classes, activation=activation)\n\n\ndef DenseNetImageNet201(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=None,\n                        input_tensor=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=201, nb_dense_block=4, growth_rate=32, nb_filter=64,\n                    nb_layers_per_block=[6, 12, 48, 32], bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n                    include_top=include_top, weights=weights, input_tensor=input_tensor,\n                    classes=classes, activation=activation)\n\n\ndef DenseNetImageNet264(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=None,\n                        input_tensor=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=201, nb_dense_block=4, growth_rate=32, nb_filter=64,\n                    nb_layers_per_block=[6, 12, 64, 48], bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n                    include_top=include_top, weights=weights, input_tensor=input_tensor,\n                    classes=classes, activation=activation)\n\n\ndef DenseNetImageNet161(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=\'imagenet\',\n                        input_tensor=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=161, nb_dense_block=4, growth_rate=48, nb_filter=96,\n                    nb_layers_per_block=[6, 12, 36, 24], bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay, subsample_initial_block=True,\n                    include_top=include_top, weights=weights, input_tensor=input_tensor,\n                    classes=classes, activation=activation)\n\n\ndef __conv_block(ip, nb_filter, bottleneck=False, dropout_rate=None, weight_decay=1e-4):\n    \'\'\' Apply BatchNorm, Relu, 3x3 Conv2D, optional bottleneck block and dropout\n    Args:\n        ip: Input keras tensor\n        nb_filter: number of filters\n        bottleneck: add bottleneck block\n        dropout_rate: dropout rate\n        weight_decay: weight decay factor\n    Returns: keras tensor with batch_norm, relu and convolution2d added (optional bottleneck)\n    \'\'\'\n    concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(ip)\n    x = Activation(\'relu\')(x)\n\n    if bottleneck:\n        inter_channel = nb_filter * 4  # Obtained from https://github.com/liuzhuang13/DenseNet/blob/master/densenet.lua\n\n        x = Conv2D(inter_channel, (1, 1), kernel_initializer=\'he_normal\', padding=\'same\', use_bias=False,\n                   kernel_regularizer=l2(weight_decay))(x)\n        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n        x = Activation(\'relu\')(x)\n\n    x = Conv2D(nb_filter, (3, 3), kernel_initializer=\'he_normal\', padding=\'same\', use_bias=False)(x)\n    if dropout_rate:\n        x = Dropout(dropout_rate)(x)\n\n    return x\n\n\ndef __dense_block(x, nb_layers, nb_filter, growth_rate, bottleneck=False, dropout_rate=None, weight_decay=1e-4,\n                  grow_nb_filters=True, return_concat_list=False):\n    \'\'\' Build a dense_block where the output of each conv_block is fed to subsequent ones\n    Args:\n        x: keras tensor\n        nb_layers: the number of layers of conv_block to append to the model.\n        nb_filter: number of filters\n        growth_rate: growth rate\n        bottleneck: bottleneck block\n        dropout_rate: dropout rate\n        weight_decay: weight decay factor\n        grow_nb_filters: flag to decide to allow number of filters to grow\n        return_concat_list: return the list of feature maps along with the actual output\n    Returns: keras tensor with nb_layers of conv_block appended\n    \'\'\'\n    concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    x_list = [x]\n\n    for i in range(nb_layers):\n        cb = __conv_block(x, growth_rate, bottleneck, dropout_rate, weight_decay)\n        x_list.append(cb)\n\n        x = concatenate([x, cb], axis=concat_axis)\n\n        if grow_nb_filters:\n            nb_filter += growth_rate\n\n    if return_concat_list:\n        return x, nb_filter, x_list\n    else:\n        return x, nb_filter\n\n\ndef __transition_block(ip, nb_filter, compression=1.0, weight_decay=1e-4):\n    \'\'\' Apply BatchNorm, Relu 1x1, Conv2D, optional compression, dropout and Maxpooling2D\n    Args:\n        ip: keras tensor\n        nb_filter: number of filters\n        compression: calculated as 1 - reduction. Reduces the number of feature maps\n                    in the transition block.\n        dropout_rate: dropout rate\n        weight_decay: weight decay factor\n    Returns: keras tensor, after applying batch_norm, relu-conv, dropout, maxpool\n    \'\'\'\n    concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(ip)\n    x = Activation(\'relu\')(x)\n    x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer=\'he_normal\', padding=\'same\', use_bias=False,\n               kernel_regularizer=l2(weight_decay))(x)\n    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n\n    return x\n\n\ndef __transition_up_block(ip, nb_filters, type=\'deconv\', weight_decay=1E-4):\n    \'\'\' SubpixelConvolutional Upscaling (factor = 2)\n    Args:\n        ip: keras tensor\n        nb_filters: number of layers\n        type: can be \'upsampling\', \'subpixel\', \'deconv\'. Determines type of upsampling performed\n        weight_decay: weight decay factor\n    Returns: keras tensor, after applying upsampling operation.\n    \'\'\'\n\n    if type == \'upsampling\':\n        x = UpSampling2D()(ip)\n    elif type == \'subpixel\':\n        x = Conv2D(nb_filters, (3, 3), activation=\'relu\', padding=\'same\', kernel_regularizer=l2(weight_decay),\n                   use_bias=False, kernel_initializer=\'he_normal\')(ip)\n        x = SubPixelUpscaling(scale_factor=2)(x)\n        x = Conv2D(nb_filters, (3, 3), activation=\'relu\', padding=\'same\', kernel_regularizer=l2(weight_decay),\n                   use_bias=False, kernel_initializer=\'he_normal\')(x)\n    else:\n        x = Conv2DTranspose(nb_filters, (3, 3), activation=\'relu\', padding=\'same\', strides=(2, 2),\n                            kernel_initializer=\'he_normal\', kernel_regularizer=l2(weight_decay))(ip)\n\n    return x\n\n\ndef __create_dense_net(nb_classes, img_input, include_top, depth=40, nb_dense_block=3, growth_rate=12, nb_filter=-1,\n                       nb_layers_per_block=-1, bottleneck=False, reduction=0.0, dropout_rate=None, weight_decay=1e-4,\n                       subsample_initial_block=False, activation=\'softmax\'):\n    \'\'\' Build the DenseNet model\n    Args:\n        nb_classes: number of classes\n        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n        include_top: flag to include the final Dense layer\n        depth: number or layers\n        nb_dense_block: number of dense blocks to add to end (generally = 3)\n        growth_rate: number of filters to add per dense block\n        nb_filter: initial number of filters. Default -1 indicates initial number of filters is 2 * growth_rate\n        nb_layers_per_block: number of layers in each dense block.\n                Can be a -1, positive integer or a list.\n                If -1, calculates nb_layer_per_block from the depth of the network.\n                If positive integer, a set number of layers per dense block.\n                If list, nb_layer is used as provided. Note that list size must\n                be (nb_dense_block + 1)\n        bottleneck: add bottleneck blocks\n        reduction: reduction factor of transition blocks. Note : reduction value is inverted to compute compression\n        dropout_rate: dropout rate\n        weight_decay: weight decay rate\n        subsample_initial_block: Set to True to subsample the initial convolution and\n                add a MaxPool2D before the dense blocks are added.\n        subsample_initial:\n        activation: Type of activation at the top layer. Can be one of \'softmax\' or \'sigmoid\'.\n                Note that if sigmoid is used, classes must be 1.\n    Returns: keras tensor with nb_layers of conv_block appended\n    \'\'\'\n\n    concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    if reduction != 0.0:\n        assert reduction <= 1.0 and reduction > 0.0, \'reduction value must lie between 0.0 and 1.0\'\n\n    # layers in each dense block\n    if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n        nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n\n        assert len(nb_layers) == (nb_dense_block), \'If list, nb_layer is used as provided. \' \\\n                                                   \'Note that list size must be (nb_dense_block)\'\n        final_nb_layer = nb_layers[-1]\n        nb_layers = nb_layers[:-1]\n    else:\n        if nb_layers_per_block == -1:\n            assert (depth - 4) % 3 == 0, \'Depth must be 3 N + 4 if nb_layers_per_block == -1\'\n            count = int((depth - 4) / 3)\n\n            if bottleneck:\n                count = count // 2\n\n            nb_layers = [count for _ in range(nb_dense_block)]\n            final_nb_layer = count\n        else:\n            final_nb_layer = nb_layers_per_block\n            nb_layers = [nb_layers_per_block] * nb_dense_block\n\n    # compute initial nb_filter if -1, else accept users initial nb_filter\n    if nb_filter <= 0:\n        nb_filter = 2 * growth_rate\n\n    # compute compression factor\n    compression = 1.0 - reduction\n\n    # Initial convolution\n    if subsample_initial_block:\n        initial_kernel = (7, 7)\n        initial_strides = (2, 2)\n    else:\n        initial_kernel = (3, 3)\n        initial_strides = (1, 1)\n\n    x = Conv2D(nb_filter, initial_kernel, kernel_initializer=\'he_normal\', padding=\'same\',\n               strides=initial_strides, use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n\n    if subsample_initial_block:\n        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n        x = Activation(\'relu\')(x)\n        x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n\n    # Add dense blocks\n    for block_idx in range(nb_dense_block - 1):\n        x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter, growth_rate, bottleneck=bottleneck,\n                                     dropout_rate=dropout_rate, weight_decay=weight_decay)\n        # add transition_block\n        x = __transition_block(x, nb_filter, compression=compression, weight_decay=weight_decay)\n        nb_filter = int(nb_filter * compression)\n\n    # The last dense_block does not have a transition_block\n    x, nb_filter = __dense_block(x, final_nb_layer, nb_filter, growth_rate, bottleneck=bottleneck,\n                                 dropout_rate=dropout_rate, weight_decay=weight_decay)\n\n    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n    x = Activation(\'relu\')(x)\n    x = GlobalAveragePooling2D()(x)\n\n    if include_top:\n        x = Dense(nb_classes, activation=activation)(x)\n\n    return x\n\n\ndef __create_fcn_dense_net(nb_classes, img_input, include_top, nb_dense_block=5, growth_rate=12,\n                           reduction=0.0, dropout_rate=None, weight_decay=1e-4,\n                           nb_layers_per_block=4, nb_upsampling_conv=128, upsampling_type=\'upsampling\',\n                           init_conv_filters=48, input_shape=None, activation=\'deconv\'):\n    \'\'\' Build the DenseNet model\n    Args:\n        nb_classes: number of classes\n        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n        include_top: flag to include the final Dense layer\n        nb_dense_block: number of dense blocks to add to end (generally = 3)\n        growth_rate: number of filters to add per dense block\n        reduction: reduction factor of transition blocks. Note : reduction value is inverted to compute compression\n        dropout_rate: dropout rate\n        weight_decay: weight decay\n        nb_layers_per_block: number of layers in each dense block.\n            Can be a positive integer or a list.\n            If positive integer, a set number of layers per dense block.\n            If list, nb_layer is used as provided. Note that list size must\n            be (nb_dense_block + 1)\n        nb_upsampling_conv: number of convolutional layers in upsampling via subpixel convolution\n        upsampling_type: Can be one of \'upsampling\', \'deconv\' and \'subpixel\'. Defines\n            type of upsampling algorithm used.\n        input_shape: Only used for shape inference in fully convolutional networks.\n        activation: Type of activation at the top layer. Can be one of \'softmax\' or \'sigmoid\'.\n                    Note that if sigmoid is used, classes must be 1.\n    Returns: keras tensor with nb_layers of conv_block appended\n    \'\'\'\n\n    concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    if concat_axis == 1:  # channels_first dim ordering\n        _, rows, cols = input_shape\n    else:\n        rows, cols, _ = input_shape\n\n    if reduction != 0.0:\n        assert reduction <= 1.0 and reduction > 0.0, \'reduction value must lie between 0.0 and 1.0\'\n\n    # check if upsampling_conv has minimum number of filters\n    # minimum is set to 12, as at least 3 color channels are needed for correct upsampling\n    assert nb_upsampling_conv > 12 and nb_upsampling_conv % 4 == 0, \'Parameter `upsampling_conv` number of channels must \' \\\n                                                                    \'be a positive number divisible by 4 and greater \' \\\n                                                                    \'than 12\'\n\n    # layers in each dense block\n    if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n        nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n\n        assert len(nb_layers) == (nb_dense_block + 1), \'If list, nb_layer is used as provided. \' \\\n                                                       \'Note that list size must be (nb_dense_block + 1)\'\n\n        bottleneck_nb_layers = nb_layers[-1]\n        rev_layers = nb_layers[::-1]\n        nb_layers.extend(rev_layers[1:])\n    else:\n        bottleneck_nb_layers = nb_layers_per_block\n        nb_layers = [nb_layers_per_block] * (2 * nb_dense_block + 1)\n\n    # compute compression factor\n    compression = 1.0 - reduction\n\n    # Initial convolution\n    x = Conv2D(init_conv_filters, (7, 7), kernel_initializer=\'he_normal\', padding=\'same\', name=\'initial_conv2D\',\n               use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n    x = Activation(\'relu\')(x)\n\n    nb_filter = init_conv_filters\n\n    skip_list = []\n\n    # Add dense blocks and transition down block\n    for block_idx in range(nb_dense_block):\n        x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate,\n                                     weight_decay=weight_decay)\n\n        # Skip connection\n        skip_list.append(x)\n\n        # add transition_block\n        x = __transition_block(x, nb_filter, compression=compression, weight_decay=weight_decay)\n\n        nb_filter = int(nb_filter * compression)  # this is calculated inside transition_down_block\n\n    # The last dense_block does not have a transition_down_block\n    # return the concatenated feature maps without the concatenation of the input\n    _, nb_filter, concat_list = __dense_block(x, bottleneck_nb_layers, nb_filter, growth_rate,\n                                              dropout_rate=dropout_rate, weight_decay=weight_decay,\n                                              return_concat_list=True)\n\n    skip_list = skip_list[::-1]  # reverse the skip list\n\n    # Add dense blocks and transition up block\n    for block_idx in range(nb_dense_block):\n        n_filters_keep = growth_rate * nb_layers[nb_dense_block + block_idx]\n\n        # upsampling block must upsample only the feature maps (concat_list[1:]),\n        # not the concatenation of the input with the feature maps (concat_list[0].\n        l = concatenate(concat_list[1:], axis=concat_axis)\n\n        t = __transition_up_block(l, nb_filters=n_filters_keep, type=upsampling_type, weight_decay=weight_decay)\n\n        # concatenate the skip connection with the transition block\n        x = concatenate([t, skip_list[block_idx]], axis=concat_axis)\n\n        # Dont allow the feature map size to grow in upsampling dense blocks\n        x_up, nb_filter, concat_list = __dense_block(x, nb_layers[nb_dense_block + block_idx + 1], nb_filter=growth_rate,\n                                                     growth_rate=growth_rate, dropout_rate=dropout_rate,\n                                                     weight_decay=weight_decay, return_concat_list=True,\n                                                     grow_nb_filters=False)\n\n    if include_top:\n        x = Conv2D(nb_classes, (1, 1), activation=\'linear\', padding=\'same\', use_bias=False)(x_up)\n\n        if K.image_data_format() == \'channels_first\':\n            channel, row, col = input_shape\n        else:\n            row, col, channel = input_shape\n\n        x = Reshape((row * col, nb_classes))(x)\n        x = Activation(activation)(x)\n        x = Reshape((row, col, nb_classes))(x)\n    else:\n        x = x_up\n\n    return x\n\n\n\n\nif __name__ == \'__main__\':\n\n    from keras.utils.vis_utils import plot_model\n    #model = DenseNetFCN((32, 32, 3), growth_rate=16, nb_layers_per_block=[4, 5, 7, 10, 12, 15], upsampling_type=\'deconv\')\n    model = DenseNet((32, 32, 3), depth=100, nb_dense_block=3,\n                     growth_rate=12, bottleneck=True, reduction=0.5, weights=None)\n    model.summary()\n\n    from keras.callbacks import ModelCheckpoint, TensorBoard\n    #plot_model(model, \'test.png\', show_shapes=True)\n'"
applications/model_source/densenet_1/subpixel.py,0,"b'# From https://github.com/titu1994/DenseNet/blob/master/subpixel.py\n# Modifications Copyright (c) Microsoft.\n\nfrom __future__ import absolute_import\n\nfrom keras2onnx.proto import keras\nfrom keras import backend as K\nfrom keras.engine import Layer\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.backend import normalize_data_format\n\nif K.backend() == \'theano\':\n    import theano_backend as K_BACKEND\nelse:\n    import tensorflow_backend as K_BACKEND\n\nclass SubPixelUpscaling(Layer):\n    """""" Sub-pixel convolutional upscaling layer based on the paper ""Real-Time Single Image\n    and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network""\n    (https://arxiv.org/abs/1609.05158).\n    This layer requires a Convolution2D prior to it, having output filters computed according to\n    the formula :\n        filters = k * (scale_factor * scale_factor)\n        where k = a user defined number of filters (generally larger than 32)\n              scale_factor = the upscaling factor (generally 2)\n    This layer performs the depth to space operation on the convolution filters, and returns a\n    tensor with the size as defined below.\n    # Example :\n    ```python\n        # A standard subpixel upscaling block\n        x = Convolution2D(256, 3, 3, padding=\'same\', activation=\'relu\')(...)\n        u = SubPixelUpscaling(scale_factor=2)(x)\n        [Optional]\n        x = Convolution2D(256, 3, 3, padding=\'same\', activation=\'relu\')(u)\n    ```\n        In practice, it is useful to have a second convolution layer after the\n        SubPixelUpscaling layer to speed up the learning process.\n        However, if you are stacking multiple SubPixelUpscaling blocks, it may increase\n        the number of parameters greatly, so the Convolution layer after SubPixelUpscaling\n        layer can be removed.\n    # Arguments\n        scale_factor: Upscaling factor.\n        data_format: Can be None, \'channels_first\' or \'channels_last\'.\n    # Input shape\n        4D tensor with shape:\n        `(samples, k * (scale_factor * scale_factor) channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows, cols, k * (scale_factor * scale_factor) channels)` if data_format=\'channels_last\'.\n    # Output shape\n        4D tensor with shape:\n        `(samples, k channels, rows * scale_factor, cols * scale_factor))` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows * scale_factor, cols * scale_factor, k channels)` if data_format=\'channels_last\'.\n    """"""\n\n    def __init__(self, scale_factor=2, data_format=None, **kwargs):\n        super(SubPixelUpscaling, self).__init__(**kwargs)\n\n        self.scale_factor = scale_factor\n        self.data_format = normalize_data_format(data_format)\n\n    def build(self, input_shape):\n        pass\n\n    def call(self, x, mask=None):\n        y = K_BACKEND.depth_to_space(x, self.scale_factor, self.data_format)\n        return y\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == \'channels_first\':\n            b, k, r, c = input_shape\n            return (b, k // (self.scale_factor ** 2), r * self.scale_factor, c * self.scale_factor)\n        else:\n            b, r, c, k = input_shape\n            return (b, r * self.scale_factor, c * self.scale_factor, k // (self.scale_factor ** 2))\n\n    def get_config(self):\n        config = {\'scale_factor\': self.scale_factor,\n                  \'data_format\': self.data_format}\n        base_config = super(SubPixelUpscaling, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nget_custom_objects().update({\'SubPixelUpscaling\': SubPixelUpscaling})\n'"
applications/model_source/densenet_1/tensorflow_backend.py,1,"b""# From https://github.com/titu1994/DenseNet/blob/master/tensorflow_backend.py\n# Modifications Copyright (c) Microsoft.\n\nimport tensorflow as tf\n\nfrom keras2onnx.proto import keras\nfrom keras.backend import tensorflow_backend as KTF\nfrom keras.backend.common import image_data_format\n\npy_all = all\n\ndef depth_to_space(input, scale, data_format=None):\n    ''' Uses phase shift algorithm to convert channels/depth for spatial resolution '''\n    if data_format is None:\n        data_format = image_data_format()\n\n    if data_format == 'channels_first':\n        data_format = 'NCHW'\n    else:\n        data_format = 'NHWC'\n\n    data_format = data_format.lower()\n    out = tf.depth_to_space(input, scale, data_format=data_format)\n    return out\n"""
applications/model_source/densenet_2/densenet_2.py,0,"b'# From https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/DenseNet/densenet.py\n# Modifications Copyright (c) Microsoft.\n\nfrom keras2onnx.proto import keras\nfrom keras.models import Model\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import AveragePooling2D\nfrom keras.layers.pooling import GlobalAveragePooling2D\nfrom keras.layers import Input, Concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nimport keras.backend as K\n\n\ndef conv_factory(x, concat_axis, nb_filter,\n                 dropout_rate=None, weight_decay=1E-4):\n    """"""Apply BatchNorm, Relu 3x3Conv2D, optional dropout\n\n    :param x: Input keras network\n    :param concat_axis: int -- index of contatenate axis\n    :param nb_filter: int -- number of filters\n    :param dropout_rate: int -- dropout rate\n    :param weight_decay: int -- weight decay factor\n\n    :returns: keras network with b_norm, relu and Conv2D added\n    :rtype: keras network\n    """"""\n\n    x = BatchNormalization(axis=concat_axis,\n                           gamma_regularizer=l2(weight_decay),\n                           beta_regularizer=l2(weight_decay))(x)\n    x = Activation(\'relu\')(x)\n    x = Conv2D(nb_filter, (3, 3),\n               kernel_initializer=""he_uniform"",\n               padding=""same"",\n               use_bias=False,\n               kernel_regularizer=l2(weight_decay))(x)\n    if dropout_rate:\n        x = Dropout(dropout_rate)(x)\n\n    return x\n\n\ndef transition(x, concat_axis, nb_filter,\n               dropout_rate=None, weight_decay=1E-4):\n    """"""Apply BatchNorm, Relu 1x1Conv2D, optional dropout and Maxpooling2D\n\n    :param x: keras model\n    :param concat_axis: int -- index of contatenate axis\n    :param nb_filter: int -- number of filters\n    :param dropout_rate: int -- dropout rate\n    :param weight_decay: int -- weight decay factor\n\n    :returns: model\n    :rtype: keras model, after applying batch_norm, relu-conv, dropout, maxpool\n\n    """"""\n\n    x = BatchNormalization(axis=concat_axis,\n                           gamma_regularizer=l2(weight_decay),\n                           beta_regularizer=l2(weight_decay))(x)\n    x = Activation(\'relu\')(x)\n    x = Conv2D(nb_filter, (1, 1),\n               kernel_initializer=""he_uniform"",\n               padding=""same"",\n               use_bias=False,\n               kernel_regularizer=l2(weight_decay))(x)\n    if dropout_rate:\n        x = Dropout(dropout_rate)(x)\n    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n\n    return x\n\n\ndef denseblock(x, concat_axis, nb_layers, nb_filter, growth_rate,\n               dropout_rate=None, weight_decay=1E-4):\n    """"""Build a denseblock where the output of each\n       conv_factory is fed to subsequent ones\n\n    :param x: keras model\n    :param concat_axis: int -- index of contatenate axis\n    :param nb_layers: int -- the number of layers of conv_\n                      factory to append to the model.\n    :param nb_filter: int -- number of filters\n    :param dropout_rate: int -- dropout rate\n    :param weight_decay: int -- weight decay factor\n\n    :returns: keras model with nb_layers of conv_factory appended\n    :rtype: keras model\n\n    """"""\n\n    list_feat = [x]\n\n    for i in range(nb_layers):\n        x = conv_factory(x, concat_axis, growth_rate,\n                         dropout_rate, weight_decay)\n        list_feat.append(x)\n        x = Concatenate(axis=concat_axis)(list_feat)\n        nb_filter += growth_rate\n\n    return x, nb_filter\n\n\ndef denseblock_altern(x, concat_axis, nb_layers, nb_filter, growth_rate,\n                      dropout_rate=None, weight_decay=1E-4):\n    """"""Build a denseblock where the output of each conv_factory\n       is fed to subsequent ones. (Alternative of a above)\n\n    :param x: keras model\n    :param concat_axis: int -- index of contatenate axis\n    :param nb_layers: int -- the number of layers of conv_\n                      factory to append to the model.\n    :param nb_filter: int -- number of filters\n    :param dropout_rate: int -- dropout rate\n    :param weight_decay: int -- weight decay factor\n\n    :returns: keras model with nb_layers of conv_factory appended\n    :rtype: keras model\n\n    * The main difference between this implementation and the implementation\n    above is that the one above\n    """"""\n\n    for i in range(nb_layers):\n        merge_tensor = conv_factory(x, concat_axis, growth_rate,\n                                    dropout_rate, weight_decay)\n        x = Concatenate(axis=concat_axis)([merge_tensor, x])\n        nb_filter += growth_rate\n\n    return x, nb_filter\n\n\ndef DenseNet(nb_classes, img_dim, depth, nb_dense_block, growth_rate,\n             nb_filter, dropout_rate=None, weight_decay=1E-4):\n    """""" Build the DenseNet model\n\n    :param nb_classes: int -- number of classes\n    :param img_dim: tuple -- (channels, rows, columns)\n    :param depth: int -- how many layers\n    :param nb_dense_block: int -- number of dense blocks to add to end\n    :param growth_rate: int -- number of filters to add\n    :param nb_filter: int -- number of filters\n    :param dropout_rate: float -- dropout rate\n    :param weight_decay: float -- weight decay\n\n    :returns: keras model with nb_layers of conv_factory appended\n    :rtype: keras model\n\n    """"""\n\n    if K.common.image_dim_ordering() == ""th"":\n        concat_axis = 1\n    elif K.common.image_dim_ordering() == ""tf"":\n        concat_axis = -1\n\n    model_input = Input(shape=img_dim)\n\n    assert (depth - 4) % 3 == 0, ""Depth must be 3 N + 4""\n\n    # layers in each dense block\n    nb_layers = int((depth - 4) / 3)\n\n    # Initial convolution\n    x = Conv2D(nb_filter, (3, 3),\n               kernel_initializer=""he_uniform"",\n               padding=""same"",\n               name=""initial_conv2D"",\n               use_bias=False,\n               kernel_regularizer=l2(weight_decay))(model_input)\n\n    # Add dense blocks\n    for block_idx in range(nb_dense_block - 1):\n        x, nb_filter = denseblock(x, concat_axis, nb_layers,\n                                  nb_filter, growth_rate, \n                                  dropout_rate=dropout_rate,\n                                  weight_decay=weight_decay)\n        # add transition\n        x = transition(x, nb_filter, dropout_rate=dropout_rate,\n                       weight_decay=weight_decay)\n\n    # The last denseblock does not have a transition\n    x, nb_filter = denseblock(x, concat_axis, nb_layers,\n                              nb_filter, growth_rate, \n                              dropout_rate=dropout_rate,\n                              weight_decay=weight_decay)\n\n    x = BatchNormalization(axis=concat_axis,\n                           gamma_regularizer=l2(weight_decay),\n                           beta_regularizer=l2(weight_decay))(x)\n    x = Activation(\'relu\')(x)\n    x = GlobalAveragePooling2D(data_format=K.image_data_format())(x)\n    x = Dense(nb_classes,\n              activation=\'softmax\',\n              kernel_regularizer=l2(weight_decay),\n              bias_regularizer=l2(weight_decay))(x)\n\n    densenet = Model(inputs=[model_input], outputs=[x], name=""DenseNet"")\n\n    return densenet'"
