file_path,api_count,code
setup.py,1,"b'import os\nimport setuptools\n\nTOP_DIR = os.path.realpath(os.path.dirname(__file__))\n\nwith open(os.path.join(TOP_DIR, \'VERSION_NUMBER\')) as version_file:\n  version = version_file.read().strip()\n\nif os.getenv(\'TRAVIS\'):\n  # On travis, we install from source, therefore no need to specify version.\n  onnx_dep = ""onnx""\nelse:\n  # For user, we install the onnx release known to work with our release.\n  with open(os.path.join(TOP_DIR, \'ONNX_VERSION_NUMBER\')) as onnx_version_file:\n    onnx_version = onnx_version_file.read().strip()\n    onnx_dep = ""onnx>="" + onnx_version\n\nsetuptools.setup(\n    name=\'onnx-tf\',\n    version=version,\n    description=\n    \'Tensorflow backend for ONNX (Open Neural Network Exchange).\',\n    install_requires=[onnx_dep, ""PyYAML""],\n    entry_points={\n        ""console_scripts"": [\n            ""onnx-tf=onnx_tf.cli:main"",\n        ],\n    },\n    url=\'https://github.com/onnx/onnx-tensorflow/\',\n    author=\'Arpith Jacob, Tian Jin, Gheorghe-Teodor Bercea, Wenhao Hu\',\n    author_email=\'tian.jin1@ibm.com\',\n    license=\'Apache License 2.0\',\n    packages=setuptools.find_packages(),\n    zip_safe=False, \n    classifiers=[\n         ""Programming Language :: Python :: 2"",\n         ""Programming Language :: Python :: 3""\n    ]\n)\n'"
example/onnx_to_tf.py,1,"b'import onnx\n\nfrom onnx_tf.backend import prepare\n\nonnx_model = onnx.load(""input_path"")  # load onnx model\ntf_rep = prepare(onnx_model)  # prepare tf representation\ntf_rep.export_graph(""output_path"")  # export the model\n'"
example/relu.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom onnx_tf.backend import run_node, prepare\nfrom onnx import helper\n\nnode_def = helper.make_node(""Relu"", [""X""], [""Y""])\noutput = run_node(node_def, [[-0.1, 0.1]])\nprint(output[""Y""])\n'"
example/test_model_large_stepping.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport unittest\nimport numpy as np\n\nimport caffe2.python.onnx.backend as c2\nimport onnx\nimport onnx_tf.backend as tf\nfrom onnx import helper\nfrom onnx import TensorProto\n\n\ndef find_between(s, first, last):\n  try:\n    start = s.index(first) + len(first)\n    end = s.index(last, start)\n    return s[start:end]\n  except ValueError:\n    return """"\n\n\nclass TestLargeModel(unittest.TestCase):\n  MODEL_PATH = ""../../onnx_models/""\n\n  def test(self):\n    _model = onnx.load(self.MODEL_PATH + ""shufflenet/model.onnx"")\n    node_count = len(_model.graph.node)\n    more_outputs = []\n    output_to_check = []\n    for node in _model.graph.node:\n      more_outputs.append(\n          helper.make_tensor_value_info(node.output[0], TensorProto.FLOAT,\n                                        (100, 100)))\n      output_to_check.append(node.output[0])\n    _model.graph.output.extend(more_outputs)\n\n    tf_rep = tf.prepare(_model)\n    cf_rep = c2.prepare(_model)\n\n    sample = np.load(\n        self.MODEL_PATH + ""shufflenet/test_data_{}.npz"".format(str(1)),\n        encoding=\'bytes\')\n    inputs = list(sample[\'inputs\'])\n    outputs = list(sample[\'outputs\'])\n\n    my_out = tf_rep.run(inputs)\n    cf_out = cf_rep.run(inputs)\n\n    for op in output_to_check:\n      try:\n        np.savetxt(\n            op.replace(""/"", ""__"") + "".cf"", cf_out[op].flatten(), delimiter=\'\\t\')\n        np.savetxt(\n            op.replace(""/"", ""__"") + "".tf"", my_out[op].flatten(), delimiter=\'\\t\')\n        np.testing.assert_allclose(my_out[op], cf_out[op], rtol=1e-2)\n        print(op, ""results of this layer are correct within tolerence."")\n      except Exception as e:\n        np.set_printoptions(threshold=np.inf)\n        mismatch_percent = (find_between(str(e), ""(mismatch"", ""%)""))\n        print(op, ""mismatch with percentage {} %"".format(mismatch_percent))\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n  pass\n'"
onnx_tf/__init__.py,0,b'from . import backend\n'
onnx_tf/backend.py,18,"b'""""""Backend for running ONNX on Tensorflow\n\nTo run this, you will need to have Tensorflow installed as well.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\ntry:\n  from itertools import izip as zip\nexcept ImportError:  # will be 3.x series\n  pass\n\nfrom onnx import defs\nfrom onnx import numpy_helper\nfrom onnx.backend.base import Backend\nfrom onnx.backend.base import Device\nfrom onnx.backend.base import namedtupledict\nfrom onnx.helper import make_opsetid\nimport tensorflow as tf\n\nfrom onnx_tf.backend_rep import TensorflowRep\nfrom onnx_tf.common import data_type\nfrom onnx_tf.common import exception\nfrom onnx_tf.common import get_device_option\nfrom onnx_tf.common import get_unique_suffix\nfrom onnx_tf.common import supports_device as common_supports_device\nfrom onnx_tf.common.handler_helper import get_all_backend_handlers\nfrom onnx_tf.pb_wrapper import OnnxNode\nimport onnx_tf.common as common\n\n\nclass TensorflowBackend(Backend):\n  """""" Tensorflow Backend for ONNX\n  """"""\n\n  @classmethod\n  def prepare(cls,\n              model,\n              device=\'CPU\',\n              strict=True,\n              logging_level=\'INFO\',\n              **kwargs):\n    """"""Prepare an ONNX model for Tensorflow Backend.\n\n    This function converts an ONNX model to an internel representation\n    of the computational graph called TensorflowRep and returns\n    the converted representation.\n\n    :param model: The ONNX model to be converted.\n    :param device: The device to execute this model on.\n    :param strict: Whether to enforce semantic equivalence between the original model\n      and the converted tensorflow model, defaults to True (yes, enforce semantic equivalence).\n      Changing to False is strongly discouraged.\n      Currently, the strict flag only affects the behavior of MaxPool and AveragePool ops.\n    :param logging_level: The logging level, default is INFO. Change it to DEBUG\n      to see more conversion details or to WARNING to see less\n\n    :returns: A TensorflowRep class object representing the ONNX model\n    """"""\n    super(TensorflowBackend, cls).prepare(model, device, **kwargs)\n    common.logger.setLevel(logging_level)\n    common.logger.handlers[0].setLevel(logging_level)\n\n    return cls.onnx_model_to_tensorflow_rep(model, strict)\n\n  @classmethod\n  def onnx_model_to_tensorflow_rep(cls, model, strict):\n    """""" Convert ONNX model to TensorflowRep.\n\n    :param model: ONNX ModelProto object.\n    :param strict: whether to enforce semantic equivalence between the original model\n      and the converted tensorflow model.\n    :return: TensorflowRep object.\n    """"""\n\n    # Models with IR_VERSION less than 3 does not have opset_import set.\n    # We default to minimum opset, this behavior is consistent with\n    # onnx checker.\n    # c.f. https://github.com/onnx/onnx/blob/427ac0c1b792363d373e3d7e4eef97fa46458420/onnx/checker.cc#L478\n    if model.ir_version < 3:\n      opset_import = [make_opsetid(defs.ONNX_DOMAIN, 1)]\n    else:\n      opset_import = model.opset_import\n    return cls._onnx_graph_to_tensorflow_rep(model.graph, opset_import, strict)\n\n  @classmethod\n  def _onnx_graph_to_tensorflow_rep(cls, graph_def, opset, strict):\n    """""" Convert ONNX graph to TensorflowRep.\n\n    :param graph_def: ONNX GraphProto object.\n    :param opset: ONNX OperatorSetIdProto list.\n    :param strict: whether to enforce semantic equivalence between the original model\n      and the converted tensorflow model.\n    :return: TensorflowRep object.\n    """"""\n    handlers = cls._get_handlers(opset)\n\n    tf_rep_graph = tf.Graph()\n    with tf_rep_graph.as_default():\n      # initializer: TensorProtos representing the values to initialize\n      # a given tensor.\n      # initialized: A list of names of the initialized tensors.\n      if graph_def.initializer:\n        input_dict_items = cls._onnx_initializer_to_input_dict_items(\n            graph_def.initializer)\n        initialized = {init.name for init in graph_def.initializer}\n      else:\n        input_dict_items = []\n        initialized = set()\n\n      # creating placeholders for currently unknown inputs\n      for value_info in graph_def.input:\n        if value_info.name in initialized:\n          continue\n        shape = list(\n            d.dim_value if (d.dim_value > 0 and d.dim_param == """") else None\n            for d in value_info.type.tensor_type.shape.dim)\n        value_info_name = value_info.name.replace(\n            "":"", ""_tf_"") + ""_"" + get_unique_suffix(\n            ) if "":"" in value_info.name else value_info.name\n\n        x = tf.compat.v1.placeholder(data_type.onnx2tf(\n            value_info.type.tensor_type.elem_type),\n                                     name=value_info_name,\n                                     shape=shape)\n        input_dict_items.append((value_info.name, x))\n\n      # tensor dict: this dictionary is a map from variable names\n      # to the latest produced TF tensors of the given name.\n      # This dictionary will get updated as we build the graph to\n      # record the names of newly produced tensors.\n      tensor_dict = dict(input_dict_items)\n      # Since tensor dict may be updated, we need to keep a copy\n      # of the original input dict where we track the earliest\n      # defined tensors so we can have access to the placeholders\n      # to feed in input tensors when we run the graph.\n      input_dict = dict(input_dict_items)\n\n      for node in graph_def.node:\n        onnx_node = OnnxNode(node)\n        output_ops = cls._onnx_node_to_tensorflow_op(onnx_node,\n                                                     tensor_dict,\n                                                     handlers,\n                                                     opset=opset,\n                                                     strict=strict)\n        curr_node_output_map = dict(zip(onnx_node.outputs, output_ops))\n        tensor_dict.update(curr_node_output_map)\n\n    tf_rep = TensorflowRep()\n    tf_rep.graph = tf_rep_graph\n    tf_rep.inputs = [\n        value_info.name\n        for value_info in graph_def.input\n        if value_info.name not in initialized\n    ]\n    tf_rep.outputs = [value_info.name for value_info in graph_def.output]\n    tf_rep.tensor_dict = tensor_dict\n    return tf_rep\n\n  @classmethod\n  def run_node(cls, node, inputs, device=\'CPU\', outputs_info=None, **kwargs):\n    """""" Run ONNX node.\n\n    :param node: ONNX NodeProto object.\n    :param inputs: Inputs.\n    :param device: Device run on.\n    :param outputs_info: None.\n    :param kwargs: Other args.\n    :return: Outputs.\n    """"""\n    super(TensorflowBackend, cls).run_node(node, inputs, device)\n    node_graph = tf.Graph()\n    with node_graph.as_default():\n      node = OnnxNode(node)\n      device_option = get_device_option(Device(device))\n      input_tensors = []\n      for i in inputs:\n        input_tensors.append(tf.constant(i))\n\n      if isinstance(inputs, dict):\n        feed_dict_raw = inputs\n      else:\n        assert len(node.inputs) == len(inputs)\n        feed_dict_raw = dict(zip(node.inputs, inputs))\n\n      # TODO: is constant the best way for feeding inputs?\n      input_dict = dict([\n          (x[0], tf.constant(x[1])) for x in feed_dict_raw.items()\n      ])\n      ops = cls._onnx_node_to_tensorflow_op(node, input_dict)\n\n      with tf.compat.v1.Session() as sess:\n        with tf.device(device_option):\n          sess.run(tf.compat.v1.global_variables_initializer())\n          output_vals = sess.run(ops)\n\n    return namedtupledict(\'Outputs\', node.outputs)(*output_vals)\n\n  @classmethod\n  def _onnx_initializer_to_input_dict_items(cls, initializer):\n    """""" Convert ONNX graph initializer to input dict items.\n\n    :param initializer: ONNX graph initializer, list of TensorProto.\n    :return: List of input dict items.\n    """"""\n\n    def tensor2list(onnx_tensor):\n      # Use the onnx.numpy_helper because the data may be raw\n      return numpy_helper.to_array(onnx_tensor).flatten().tolist()\n\n    def validate_initializer_name(name):\n      # Replace "":"" with ""_tf_"" and append a unique suffix for\n      # traceability\n      return name.replace(\n          "":"", ""_tf_"") + ""_"" + get_unique_suffix() if "":"" in name else name\n\n    return [(init.name,\n             tf.constant(tensor2list(init),\n                         shape=init.dims,\n                         dtype=data_type.onnx2tf(init.data_type),\n                         name=validate_initializer_name(init.name)))\n            for init in initializer]\n\n  @classmethod\n  def _onnx_node_to_tensorflow_op(cls,\n                                  node,\n                                  tensor_dict,\n                                  handlers=None,\n                                  opset=None,\n                                  strict=True):\n    """"""\n    Convert onnx node to tensorflow op.\n\n    Args:\n      node: Onnx node object.\n      tensor_dict: Tensor dict of graph.\n      opset: Opset version of the operator set. Default 0 means using latest version.\n      strict: whether to enforce semantic equivalence between the original model\n        and the converted tensorflow model, defaults to True (yes, enforce semantic equivalence).\n        Changing to False is strongly discouraged.\n    Returns:\n      Tensorflow op\n    """"""\n    handlers = handlers or cls._get_handlers(opset)\n    handler = handlers[node.domain].get(node.op_type, None)\n    if handler:\n      return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    else:\n      exception.OP_UNIMPLEMENTED_EXCEPT(node.op_type)\n\n  @classmethod\n  def _get_handlers(cls, opset):\n    """""" Get all backend handlers with opset.\n\n    :param opset: ONNX OperatorSetIdProto list.\n    :return: All backend handlers.\n    """"""\n    opset = opset or [make_opsetid(defs.ONNX_DOMAIN, defs.onnx_opset_version())]\n    opset_dict = dict([(o.domain, o.version) for o in opset])\n    return get_all_backend_handlers(opset_dict)\n\n  @classmethod\n  def supports_device(cls, device):\n    return common_supports_device(device)\n\n  @classmethod\n  def onnx_graph_to_tensorflow_ops(cls,\n                                   subgraph,\n                                   input_values,\n                                   tensor_dict,\n                                   opset=None,\n                                   strict=True):\n    """"""\n    Converts ONNX graph to Tensorflow operations\n    Args:\n      subgraph:         the ONNX graph to be converted\n      input_values:     dictionary with values/tensors to initialize\n                        the subgraph inputs. if the subgraph.input\n                        are send in as parameters then it is required,\n                        otherwise this can be empty dictionary\n      tensor_dict:      the dictionary that contain values for all the\n                        node.inputs in the subgraph that are not defined\n                        in the subgraph or input_values.\n      opset:            opset version of the operator set.\n      strict:           whether to enforce semantic equivalence between the\n                        original model and the converted tensorflow model,\n                        defaults to True (yes, enforce semantic equivalence).\n    Returns:\n      array of Tensorflow Tensors\n    """"""\n    # get the subgraph.input from input_values\n    subgraph_tensor_dict = input_values.copy()\n    # get the rest of the subgraph input from tensor_dict\n    for i in subgraph.input:\n      if i.name not in subgraph_tensor_dict.keys():\n        subgraph_tensor_dict[i.name] = tensor_dict[i.name]\n    # get the required initializer constant node(s) for the subgraph\n    # Need to get the initializer constant nodes from tensor_dict here\n    # because input from initializer will not be send in as inputs\n    # to the subgraph and those nodes are not in the subgraph\n    nodes_outputs = []\n    for node in subgraph.node:\n      for o_name in node.output:\n        nodes_outputs.append(o_name)\n    for node in subgraph.node:\n      for i_name in node.input:\n        if i_name not in nodes_outputs and i_name not in subgraph_tensor_dict.keys():\n          subgraph_tensor_dict[i_name] = tensor_dict[i_name]\n      onnx_node = OnnxNode(node)\n      output_ops = cls._onnx_node_to_tensorflow_op(onnx_node,\n                                                   subgraph_tensor_dict,\n                                                   opset=opset,\n                                                   strict=strict)\n      curr_node_output_map = dict(zip(onnx_node.outputs, output_ops))\n      subgraph_tensor_dict.update(curr_node_output_map)\n    return subgraph_tensor_dict\n\n  @classmethod\n  def onnx_graph_to_tensorflow_rep(cls, graph_def, strict=True):\n    """"""\n    Converts ONNX graph to TensorflowRep\n    Args:\n      graph_def:        the ONNX graph to be converted\n      strict:           whether to enforce semantic equivalence between the\n                        original model and the converted tensorflow model,\n                        defaults to True (yes, enforce semantic equivalence).\n    Returns:\n      TensorflowRep object.\n    """"""\n    # get the opset of the installed ONNX\n    opset = [make_opsetid(defs.ONNX_DOMAIN, defs.onnx_opset_version())]\n    return cls._onnx_graph_to_tensorflow_rep(graph_def, opset, strict)\n\n\nprepare = TensorflowBackend.prepare\n\nrun_node = TensorflowBackend.run_node\n\nrun_model = TensorflowBackend.run_model\n\nsupports_device = TensorflowBackend.supports_device\n\nonnx_graph_to_tensorflow_ops = TensorflowBackend.onnx_graph_to_tensorflow_ops\n\nonnx_graph_to_tensorflow_rep = TensorflowBackend.onnx_graph_to_tensorflow_rep\n'"
onnx_tf/backend_rep.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom onnx.backend.base import BackendRep, namedtupledict\n\n\nclass TensorflowRep(BackendRep):\n\n  def __init__(self, graph=None, inputs=None, outputs=None, tensor_dict=None):\n    super(TensorflowRep, self).__init__()\n    self._graph = graph\n    self._inputs = inputs or []\n    self._outputs = outputs or []\n    self._tensor_dict = tensor_dict or {}\n\n  @property\n  def graph(self):\n    return self._graph\n\n  @graph.setter\n  def graph(self, graph):\n    self._graph = graph\n\n  @property\n  def inputs(self):\n    return self._inputs\n\n  @inputs.setter\n  def inputs(self, inputs):\n    self._inputs = inputs\n\n  @property\n  def outputs(self):\n    return self._outputs\n\n  @outputs.setter\n  def outputs(self, outputs):\n    self._outputs = outputs\n\n  @property\n  def tensor_dict(self):\n    return self._tensor_dict\n\n  @tensor_dict.setter\n  def tensor_dict(self, tensor_dict):\n    self._tensor_dict = tensor_dict\n\n  def run(self, inputs, **kwargs):\n    """""" Run TensorflowRep.\n\n    :param inputs: Given inputs.\n    :param kwargs: Other args.\n    :return: Outputs.\n    """"""\n    super(TensorflowRep, self).run(inputs, **kwargs)\n\n    # TODO: handle name scope if necessary\n    with self.graph.as_default():\n      with tf.compat.v1.Session() as sess:\n        if isinstance(inputs, dict):\n          feed_dict = inputs\n        elif isinstance(inputs, list) or isinstance(inputs, tuple):\n          if len(self.inputs) != len(inputs):\n            raise RuntimeError(\'Expected {} values for uninitialized \'\n                               \'graph inputs ({}), but got {}.\'.format(\n                                   len(self.inputs), \', \'.join(self.inputs),\n                                   len(inputs)))\n          feed_dict = dict(zip(self.inputs, inputs))\n        else:\n          # single input\n          feed_dict = dict([(self.inputs[0], inputs)])\n\n        feed_dict = {\n            self.tensor_dict[key]: feed_dict[key] for key in self.inputs\n        }\n\n        sess.run(tf.compat.v1.global_variables_initializer())\n        outputs = [self.tensor_dict[output] for output in self.outputs]\n\n        output_values = sess.run(outputs, feed_dict=feed_dict)\n        return namedtupledict(\'Outputs\', self.outputs)(*output_values)\n\n  def export_graph(self, path):\n    """"""Export backend representation to a Tensorflow proto file.\n\n    This function obtains the graph proto corresponding to the ONNX\n    model associated with the backend representation and serializes\n    to a protobuf file.\n\n    :param path: The path to the output TF protobuf file.\n\n    :returns: none.\n    """"""\n    graph_proto = self.graph.as_graph_def()\n    # rename the output nodes\n    meaningful_names = {}\n    for output_name in self.outputs:\n      meaningful_names[self.tensor_dict[output_name].name.replace(\':0\', \'\')] = output_name\n    for node in graph_proto.node:\n      if node.name in meaningful_names.keys():\n        node.name = meaningful_names[node.name]\n\n    file = open(path, ""wb"")\n    file.write(graph_proto.SerializeToString())\n    file.close()\n'"
onnx_tf/cli.py,2,"b'import argparse\nimport sys\n\nimport onnx_tf.converter\n\n\ndef main():\n  args = sys.argv[1:]\n  parser = argparse.ArgumentParser(\n      description=""ONNX-Tensorflow Command Line Interface"")\n  parser.add_argument(\n      ""command"",\n      choices=[""convert""],\n      help=""Available commands."")\n\n  if len(args) == 0:\n    parser.parse_args([""-h""])\n  cli_tool = parser.parse_args([args[0]])\n  if cli_tool.command == ""convert"":\n    return onnx_tf.converter.main(args[1:])\n\n\nif __name__ == \'__main__\':\n  main()\n'"
onnx_tf/converter.py,4,"b'import argparse\nimport inspect\nimport logging\nimport os\nimport shutil\n\nimport onnx\nimport tensorflow as tf\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.python.tools import freeze_graph\n\nimport onnx_tf.backend as backend\nimport onnx_tf.common as common\nfrom onnx_tf.common import get_unique_suffix\nfrom onnx_tf.pb_wrapper import TensorflowGraph\n\n\ndef main(args):\n  args = parse_args(args)\n  convert(**{k: v for k, v in vars(args).items() if v is not None})\n\n\ndef parse_args(args):\n\n  class ListAction(argparse.Action):\n    """""" Define how to convert command line list strings to Python objects.\n    """"""\n\n    def __call__(self, parser, namespace, values, option_string=None):\n      values = values if values[0] not in (""("", ""["") or values[-1] not in (\n          "")"", ""]"") else values[1:-1]\n      res = []\n      for value in values.split("",""):\n        if value.isdigit():\n          res.append(int(value))\n        else:\n          res.append(value)\n      setattr(namespace, self.dest, res)\n\n  class OpsetAction(argparse.Action):\n    """""" Define how to convert command line opset strings to Python objects.\n    """"""\n\n    def __call__(self, parser, namespace, values, option_string=None):\n      if values.isdigit():\n        setattr(namespace, ""opset"", int(values))\n      else:\n        res = []\n        while values and values[0] in (""("", ""[""):\n          values = values[1:]\n        while values and values[-1] in ("")"", ""]""):\n          values = values[:-1]\n        for value in values.split(""),(""):\n          l, r = value.split("","")\n          res.append((l, int(r)))\n        setattr(namespace, ""opset"", res)\n\n  def get_param_doc_dict(funcs):\n    """"""Get doc of funcs params.\n\n    Args:\n      funcs: Target funcs.\n\n    Returns:\n      Dict of params doc.\n    """"""\n\n    # TODO(fumihwh): support google doc format\n    def helper(doc, func):\n      first_idx = doc.find("":param"")\n      last_idx = doc.find("":return"")\n      last_idx = last_idx if last_idx != -1 else len(doc)\n      param_doc = doc[first_idx:last_idx]\n      params_doc = param_doc.split("":param "")[1:]\n      return {\n          p[:p.find("": "")]: p[p.find("": "") + len("": ""):] +\n          "" (from {})"".format(func.__module__ + ""."" + func.__name__)\n          for p in params_doc\n      }\n\n    param_doc_dict = {}\n    for func, persists in funcs:\n      doc = inspect.getdoc(func)\n      doc_dict = helper(doc, func)\n      for k, v in doc_dict.items():\n        if k not in persists:\n          continue\n        param_doc_dict[k] = {""doc"": v, ""params"": persists[k]}\n    return param_doc_dict\n\n  parser = argparse.ArgumentParser(\n      description=\n      ""This is the converter for converting protocol buffer between tf and onnx.""\n  )\n\n  # required two args, source and destination path\n  parser.add_argument(""--infile"", ""-i"", help=""Input file path."", required=True)\n  parser.add_argument(\n      ""--outfile"", ""-o"", help=""Output file path."", required=True)\n\n  def add_argument_group(parser, group_name, funcs):\n    group = parser.add_argument_group(group_name)\n    param_doc_dict = get_param_doc_dict(funcs)\n    for k, v in param_doc_dict.items():\n      group.add_argument(""--{}"".format(k), help=v[""doc""], **v[""params""])\n\n  # backend args\n  # Args must be named consistently with respect to backend.prepare.\n  add_argument_group(parser, ""backend arguments (onnx -> tf)"",\n                     [(backend.prepare, {\n                         ""device"": {},\n                         ""strict"": {},\n                         ""logging_level"": {}\n                     })])\n\n  return parser.parse_args(args)\n\n\ndef convert(infile, outfile, **kwargs):\n  """"""Convert pb.\n\n  Args:\n    infile: Input path.\n    outfile: Output path.\n    **kwargs: Other args for converting.\n\n  Returns:\n    None.\n  """"""\n  logging_level = kwargs.get(""logging_level"", ""INFO"")\n  common.logger.setLevel(logging_level)\n  common.logger.handlers[0].setLevel(logging_level)\n\n  common.logger.info(""Start converting onnx pb to tf pb:"")\n  onnx_model = onnx.load(infile)\n  tf_rep = backend.prepare(onnx_model, **kwargs)\n  tf_rep.export_graph(outfile)\n  common.logger.info(""Converting completes successfully."")\n'"
onnx_tf/gen_doc.py,7,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport re\nimport subprocess\n\nimport onnx_tf.backend\nimport onnx_tf.backend_rep\nfrom third_party import get_info\n\n\ndef main(docs_dir):\n  gen_api(docs_dir)\n  gen_cli(docs_dir)\n\n\ndef gen_api(docs_dir):\n  gen_doc_for = {\n      \'onnx_tf.backend\': [\n          onnx_tf.backend.prepare,\n      ],\n      \'onnx_tf.backend_rep.TensorflowRep\': [\n          onnx_tf.backend_rep.TensorflowRep.export_graph,\n      ]\n  }\n  with open(os.path.join(docs_dir, \'API.md\'), \'w\') as doc_file:\n    doc_file.write(\'ONNX-Tensorflow API\\n\')\n    doc_file.write(\'======\\n\\n\')\n\n    for scope, funcs in sorted(gen_doc_for.items()):\n      for func in funcs:\n        doc_parsed = get_info.parse_docstring(func.__doc__)\n        doc_file.write(\'#### `\' + scope + \'.\' + func.__name__ + \'`\\n\\n\')\n        doc_file.write(\'<details>\\n\')\n        doc_file.write(\'  <summary>\')\n        doc_file.write(doc_parsed[\'short_description\'] + \'\\n\\n\')\n        doc_file.write(\'  </summary>\\n\')\n        doc_file.write(doc_parsed[\'long_description\'] + \'\\n\\n\')\n        doc_file.write(\'</details>\\n\\n\\n\\n\')\n\n        doc_file.write(\'_params_:\\n\\n\')\n        for param in doc_parsed[\'params\']:\n          doc_file.write(\'`\' + param[\'name\'] + \'` : \' + param[\'doc\'] + \'\\n\\n\')\n\n        doc_file.write(\'_returns_:\\n\\n\')\n        doc_file.write(doc_parsed[\'returns\'] + \'\\n\\n\')\n\n\ndef gen_cli(docs_dir):\n  with open(os.path.join(docs_dir, \'CLI_template.md\'), \'r\') as cli_temp_file:\n    temp_lines = cli_temp_file.readlines()\n\n  lines = []\n  for line in temp_lines:\n    matched = re.match(r""{onnx-tf.*}"", line)\n    if matched:\n      command = matched.string.strip()[1:-1]\n      output = subprocess.check_output(command.split("" "")).decode(""UTF-8"")\n      lines.append(output)\n    else:\n      lines.append(line)\n\n  with open(os.path.join(docs_dir, \'CLI.md\'), \'w\') as cli_file:\n    cli_file.writelines(lines)\n\n\nif __name__ == \'__main__\':\n  base_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n  docs_dir = os.path.join(base_dir, \'doc\')\n  main(docs_dir)\n'"
onnx_tf/gen_opset.py,2,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport pprint\n\nfrom onnx import defs\n\nfrom onnx_tf.common.handler_helper import get_backend_coverage\nfrom onnx_tf.common.handler_helper import get_backend_partial_support_detail\n\n\ndef main():\n  backend_opset_dict = {}\n\n  for schema in defs.get_all_schemas():\n    op_name = schema.name\n    backend_opset_dict[op_name] = []\n\n  backend_onnx_coverage, backend_experimental_op = get_backend_coverage()\n  backend_opset_dict.update(backend_onnx_coverage.get(defs.ONNX_DOMAIN, {}))\n  backend_ps_dict = get_backend_partial_support_detail()\n\n  with open(\'opset_version.py\', \'w\') as version_file:\n    pp = pprint.PrettyPrinter(indent=4)\n    version_file.write(""backend_opset_version = {\\n "" +\n                       pp.pformat(backend_opset_dict)[1:-1] + ""\\n}\\n\\n"")\n    version_file.write(""backend_partial_support = {\\n "" +\n                       pp.pformat(backend_ps_dict)[1:-1] + ""\\n}\\n"")\n\n\nif __name__ == \'__main__\':\n  main()\n'"
onnx_tf/gen_status.py,1,"b""#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport getopt\nimport os\nimport subprocess\nimport sys\n\nimport onnx\n\nimport tensorflow as tf\n\nfrom onnx_tf import opset_version\n\n\ndef main(docs_dir):\n  base_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n  docs_dir = os.path.join(base_dir, 'doc')\n  onnx_version = ''\n  onnx_tf_release_build = False\n\n  try:\n    opts, args = getopt.getopt(sys.argv[1:], 'hv:r',\n                               ['onnx_version=', 'onnx_tf_release_build'])\n  except getopt.GetoptError:\n    print('Usage:')\n    print('     gen_status.py -v <onnx_version> [-r]')\n    print('     gen_status.py -h')\n    print('Description:')\n    print('  -v, --onnx_version             installed ONNX version')\n    print('  -r, --onnx_tf_release_build    create report for ONNX-TF release with version stated in the VERSION_NUMBER file')\n    print('                                 when omitted, the report is for ONNX-TF master')\n    print('  -h                             show this help message and exit')\n    print('eg. generate support_status.md for onnx-tf master and onnx master')\n    print('        gen_status.py -v master')\n    print('    generate support_status_<onnx_tf_version>.md for onnx-tf ' +\n          'version stated in the VERSION_NUMBER file and onnx v1.5.0 ')\n    print('        gen_status.py -v v1.5.0 -r')\n    sys.exit(2)\n  for opt, arg in opts:\n    if opt == '-h':\n      print('Usage:')\n      print('     gen_status.py -v <onnx_version> [-r]')\n      print('     gen_status.py -h')\n      print('Description:')\n      print('  -v, --onnx_version             installed ONNX version')\n      print('  -r, --onnx_tf_release_build    create report for ONNX-TF release with version stated in the VERSION_NUMBER file')\n      print('                                 when omitted, the report is for ONNX-TF master')\n      print('  -h                             show this help message and exit')\n      print('eg. generate support_status.md for onnx-tf master and onnx master')\n      print('        gen_status.py -v master')\n      print('    generate support_status_<onnx_tf_version>.md for onnx-tf ' +\n            'version stated in the VERSION_NUMBER file and onnx v1.5.0')\n      print('        gen_status.py -v v1.5.0 -r')\n      sys.exit()\n    elif opt in ('-v', '--onnx_version'):\n      onnx_version = arg\n    elif opt in ('-r', '--onnx_tf_release_build'):\n      onnx_tf_release_build = True\n  if onnx_version == '':\n    print('Please provide the onnx_version.')\n    print('Usage:')\n    print('     gen_status.py -v <onnx_version> [-r]')\n    print('     gen_status.py -h')\n    print('Description:')\n    print('  -v, --onnx_version             installed ONNX version')\n    print('  -r, --onnx_tf_release_build    create report for ONNX-TF release with version stated in the VERSION_NUMBER file')\n    print('                                 when omitted, the report is for ONNX-TF master')\n    print('  -h                             show this help message and exit')\n    print('eg. generate support_status.md for onnx-tf master and onnx master')\n    print('        gen_status.py -v master')\n    print('    generate support_status_<onnx_tf_version>.md for onnx-tf ' +\n          'version stated in the VERSION_NUMBER file and onnx v1.5.0')\n    print('        gen_status.py -v v1.5.0 -r')\n    sys.exit(2)\n\n  gen_support_status(docs_dir, onnx_version, onnx_tf_release_build)\n\n\ndef gen_support_status(docs_dir, onnx_version, onnx_tf_release_build):\n\n  # set filename\n  if onnx_tf_release_build:\n    # get onnx-tf version from VERSION_NUMBER file\n    version_dir = os.path.dirname(\n        os.path.dirname(os.path.realpath('VERSION_NUMBER')))\n    version_file = os.path.join(version_dir, 'VERSION_NUMBER')\n    onnx_tf_version = subprocess.check_output('cat ' + version_file, shell=True)\n    onnx_tf_version = 'v' + onnx_tf_version.decode().strip('\\n')\n    filename = 'support_status_' + onnx_tf_version.replace('.', '_') + '.md'\n  else:  # onnx-tf = master\n    # get onnx-tf commit id\n    onnx_tf_commit_id = subprocess.check_output('git rev-parse HEAD',\n                                                shell=True)\n    onnx_tf_commit_id = onnx_tf_commit_id.decode().strip('\\n')\n    onnx_tf_version = 'Master ( commit id: {} )'.format(onnx_tf_commit_id)\n    filename = 'support_status.md'\n\n  with open(os.path.join(docs_dir, filename), 'w') as status_file:\n    status_file.write('# ONNX-Tensorflow Support Status\\n')\n    status_file.write('|||\\n')\n    status_file.write('|-:|:-|\\n')\n    status_file.write('|ONNX-Tensorflow Version|{}|\\n'.format(onnx_tf_version))\n\n    # get onnx commit id\n    if onnx_version == 'master':\n      onnx_path = os.path.dirname(\n          os.path.dirname(os.path.realpath(onnx.__file__)))\n      onnx_commit_id = subprocess.check_output('cd ' + onnx_path +\n                                               '; git rev-parse HEAD',\n                                               shell=True)\n      onnx_commit_id = onnx_commit_id.decode().strip('\\n')\n      status_file.write(\n          '|ONNX Version|Master ( commit id: {} )|\\n'.format(onnx_commit_id))\n    else:\n      status_file.write('|ONNX Version|{}|\\n'.format(onnx_version))\n\n    # get tf_version\n    status_file.write('|Tensorflow Version|v{}|\\n\\n'.format(tf.__version__))\n\n    # display the table legend\n    status_file.write('Notes:\\n')\n    status_file.write('* Values that are new or updated from a ')\n    status_file.write('previous opset version are in bold.\\n')\n    status_file.write('* -: not defined in corresponding ONNX ')\n    status_file.write('opset version\\n')\n    status_file.write('* \\*: the operator is deprecated\\n')\n    status_file.write('* :small_red_triangle:: not supported yet\\n')\n    status_file.write('* :small_orange_diamond:: partially supported\\n')\n    status_file.write('* the rest are all supported\\n\\n')\n\n    # get oll onnx ops\n    onnx_ops = {}\n    for schema in onnx.defs.get_all_schemas():\n      if schema.domain == '':  # only get onnx ops\n        onnx_ops[schema.name] = {\n            'versions': [],\n            'deprecated': schema.since_version if schema.deprecated else -1\n        }\n    for schema in onnx.defs.get_all_schemas_with_history():\n      if schema.domain == '':  # only get onnx ops\n        op = onnx_ops[schema.name]\n        versions = op['versions']\n        versions.append(schema.since_version)\n\n    # get all onnx-tf supported ops\n    onnx_tf_ops = opset_version.backend_opset_version\n    onnx_tf_ops_ps = opset_version.backend_partial_support\n\n    # get the cureent opset version\n    current_opset = onnx.defs.onnx_opset_version()\n\n    # setup table header\n    status_file.write('||')\n    for i in range(current_opset):\n      status_file.write('|')\n    status_file.write('\\n|:-:|')\n    for i in range(current_opset):\n      status_file.write(':-:|')\n    status_file.write('\\n|**ONNX Operator**|')\n    for opset in range(1, current_opset + 1):\n      status_file.write('**Opset {}**|'.format(opset))\n\n    ops_count = len(onnx_ops)\n    # fill in data for the table\n    for key, val in sorted(onnx_ops.items()):\n      try:\n        status_file.write('\\n|{}|'.format(key))\n        i = 0\n        vers = val['versions']\n        deprecated = val['deprecated']\n        for opset in range(1, current_opset + 1):\n          if i <= len(vers) - 1:\n            lb = vers[i]\n            ub = vers[i + 1] if i < len(vers) - 1 else vers[i]\n            if opset < lb:\n              if i == 0:\n                status_file.write('-')\n            elif opset == lb:\n              status_file.write('**{}**'.format(lb))\n              if lb == deprecated:\n                status_file.write('\\*')\n              elif lb not in onnx_tf_ops[key]:\n                status_file.write(':small_red_triangle:')\n                if opset == current_opset:\n                  ops_count -= 1\n              elif key in onnx_tf_ops_ps:\n                status_file.write(':small_orange_diamond:')\n            else:  # opset > lb\n              if opset < ub:\n                status_file.write('{}'.format(lb))\n                if lb == deprecated:\n                  status_file.write('\\*')\n                elif lb not in onnx_tf_ops[key]:\n                  status_file.write(':small_red_triangle:')\n                  if opset == current_opset:\n                    ops_count -= 1\n                elif key in onnx_tf_ops_ps:\n                  status_file.write(':small_orange_diamond:')\n              elif opset == ub:\n                status_file.write('**{}**'.format(ub))\n                if ub == deprecated:\n                  status_file.write('\\*')\n                elif ub not in onnx_tf_ops[key]:\n                  status_file.write(':small_red_triangle:')\n                  if opset == current_opset:\n                    ops_count -= 1\n                elif key in onnx_tf_ops_ps:\n                  status_file.write(':small_orange_diamond:')\n                i += 1\n              else:  #opset > ub\n                status_file.write('{}'.format(ub))\n                if ub == deprecated:\n                  status_file.write('\\*')\n                elif ub not in onnx_tf_ops[key]:\n                  status_file.write(':small_red_triangle:')\n                  if opset == current_opset:\n                    ops_count -= 1\n                elif key in onnx_tf_ops_ps:\n                  status_file.write(':small_orange_diamond:')\n            status_file.write('|')\n      except:\n        # ops defined in onnx but not in opset_version.backend_opset_versionn\n        status_file.write(':small_red_triangle:|')\n\n    status_file.write(\n        '\\n\\nONNX-TF Supported Operators / ONNX Operators: {} / {}'.format(\n            ops_count, len(onnx_ops)))\n\n    # display partial support footnote\n    status_file.write('\\n\\nNotes:\\n')\n    index = 1\n    for key in onnx_tf_ops_ps:\n      status_file.write(\n          str(index) + '. ' + key + ': ' + onnx_tf_ops_ps[key] + '\\n')\n      index += 1\n\n\nif __name__ == '__main__':\n  main(sys.argv[1:])\n"""
onnx_tf/opset_version.py,0,"b""backend_opset_version = {\n    'Abs': [1, 6],\n    'Acos': [7],\n    'Acosh': [9],\n    'Adagrad': [],\n    'Adam': [],\n    'Add': [1, 6, 7],\n    'And': [1, 7],\n    'ArgMax': [1, 11, 12],\n    'ArgMin': [1, 11, 12],\n    'ArrayFeatureExtractor': [],\n    'Asin': [7],\n    'Asinh': [9],\n    'Atan': [7],\n    'Atanh': [9],\n    'AveragePool': [1, 7, 10, 11],\n    'BatchNormalization': [1, 6, 7, 9],\n    'Binarizer': [],\n    'BitShift': [11],\n    'Cast': [1, 6, 9],\n    'CastMap': [],\n    'CategoryMapper': [],\n    'Ceil': [1, 6],\n    'Celu': [],\n    'Clip': [1, 6, 11],\n    'Compress': [9, 11],\n    'Concat': [1, 4, 11],\n    'ConcatFromSequence': [],\n    'Constant': [1, 9, 11, 12],\n    'ConstantFill': [1],\n    'ConstantOfShape': [9],\n    'Conv': [1, 11],\n    'ConvInteger': [10],\n    'ConvTranspose': [1, 11],\n    'Cos': [7],\n    'Cosh': [9],\n    'CumSum': [11],\n    'DepthToSpace': [1, 11],\n    'DequantizeLinear': [10],\n    'Det': [11],\n    'DictVectorizer': [],\n    'Div': [1, 6, 7],\n    'Dropout': [1, 6, 7, 10],\n    'DynamicQuantizeLinear': [11],\n    'Einsum': [],\n    'Elu': [1, 6],\n    'Equal': [1, 7, 11],\n    'Erf': [9],\n    'Exp': [1, 6],\n    'Expand': [8],\n    'EyeLike': [9],\n    'FeatureVectorizer': [],\n    'Flatten': [1, 9, 11],\n    'Floor': [1, 6],\n    'GRU': [1, 3, 7],\n    'Gather': [1, 11],\n    'GatherElements': [],\n    'GatherND': [11],\n    'Gemm': [1, 6, 7, 9, 11],\n    'GlobalAveragePool': [1],\n    'GlobalLpPool': [1, 2],\n    'GlobalMaxPool': [1],\n    'Gradient': [],\n    'GraphCall': [],\n    'Greater': [1, 7, 9],\n    'GreaterOrEqual': [],\n    'HardSigmoid': [1, 6],\n    'Hardmax': [1, 11],\n    'Identity': [1],\n    'If': [1, 11],\n    'ImageScaler': [1],\n    'Imputer': [],\n    'InstanceNormalization': [1, 6],\n    'IsInf': [10],\n    'IsNaN': [9],\n    'LRN': [1],\n    'LSTM': [1, 7],\n    'LabelEncoder': [],\n    'LeakyRelu': [1, 6],\n    'Less': [1, 7, 9],\n    'LessOrEqual': [],\n    'LinearClassifier': [],\n    'LinearRegressor': [],\n    'Log': [1, 6],\n    'LogSoftmax': [1, 11],\n    'Loop': [1, 11],\n    'LpNormalization': [1],\n    'LpPool': [],\n    'MatMul': [1, 9],\n    'MatMulInteger': [10],\n    'Max': [1, 6, 8, 12, 13],\n    'MaxPool': [1, 8, 10, 11, 12],\n    'MaxRoiPool': [],\n    'MaxUnpool': [9, 11],\n    'Mean': [1, 6, 8],\n    'MeanVarianceNormalization': [1, 9],\n    'Min': [1, 6, 8, 12, 13],\n    'Mod': [10],\n    'Momentum': [],\n    'Mul': [1, 6, 7],\n    'Multinomial': [],\n    'Neg': [1, 6],\n    'NegativeLogLikelihoodLoss': [],\n    'NonMaxSuppression': [10, 11],\n    'NonZero': [9],\n    'Normalizer': [],\n    'Not': [1],\n    'OneHot': [9, 11],\n    'OneHotEncoder': [],\n    'Or': [1, 7],\n    'PRelu': [1, 6, 7, 9],\n    'Pad': [1, 2, 11],\n    'Pow': [1, 7],\n    'QLinearConv': [10],\n    'QLinearMatMul': [10],\n    'QuantizeLinear': [10],\n    'RNN': [1, 7],\n    'RandomNormal': [1],\n    'RandomNormalLike': [1],\n    'RandomUniform': [1],\n    'RandomUniformLike': [1],\n    'Range': [11],\n    'Reciprocal': [1, 6],\n    'ReduceL1': [1, 11],\n    'ReduceL2': [1, 11],\n    'ReduceLogSum': [1, 11],\n    'ReduceLogSumExp': [1, 11],\n    'ReduceMax': [1, 11],\n    'ReduceMean': [1, 11],\n    'ReduceMin': [1, 11],\n    'ReduceProd': [1, 11],\n    'ReduceSum': [1, 11],\n    'ReduceSumSquare': [1, 11],\n    'Relu': [1, 6],\n    'Reshape': [1, 5],\n    'Resize': [10],\n    'ReverseSequence': [10],\n    'RoiAlign': [],\n    'Round': [11],\n    'SVMClassifier': [],\n    'SVMRegressor': [],\n    'Scaler': [],\n    'Scan': [8, 9, 11],\n    'Scatter': [9],\n    'ScatterElements': [11],\n    'ScatterND': [11],\n    'Selu': [1, 6],\n    'SequenceAt': [11],\n    'SequenceConstruct': [11],\n    'SequenceEmpty': [11],\n    'SequenceErase': [11],\n    'SequenceInsert': [11],\n    'SequenceLength': [11],\n    'Shape': [1],\n    'Shrink': [9],\n    'Sigmoid': [1, 6],\n    'Sign': [9],\n    'Sin': [7],\n    'Sinh': [9],\n    'Size': [1],\n    'Slice': [1, 10, 11],\n    'Softmax': [1, 11],\n    'SoftmaxCrossEntropyLoss': [],\n    'Softplus': [1],\n    'Softsign': [1],\n    'SpaceToDepth': [1],\n    'Split': [1, 2, 11],\n    'SplitToSequence': [],\n    'Sqrt': [1, 6],\n    'Squeeze': [1, 11],\n    'StringNormalizer': [],\n    'Sub': [1, 6, 7],\n    'Sum': [1, 6, 8],\n    'Tan': [7],\n    'Tanh': [1, 6],\n    'TfIdfVectorizer': [9],\n    'ThresholdedRelu': [1, 10],\n    'Tile': [1, 6],\n    'TopK': [1, 10, 11],\n    'Transpose': [1],\n    'TreeEnsembleClassifier': [],\n    'TreeEnsembleRegressor': [],\n    'Unique': [],\n    'Unsqueeze': [1, 11],\n    'Upsample': [7, 9],\n    'Where': [9],\n    'Xor': [1, 7],\n    'ZipMap': []\n}\n\nbackend_partial_support = {\n    'Cast': 'Cast string to float32/float64/int32/int64 are not supported in '\n            'Tensorflow.',\n    'ConvTranspose': 'ConvTranspose with dilations != 1, or transposed '\n                     'convolution for 4D or higher are not supported in '\n                     'Tensorflow.',\n    'CumSum': 'CumSum inputs in uint32/uint64 are not supported in Tensorflow.',\n    'Equal': 'Equal inputs in uint16/uint32/uint64 are not supported in '\n             'Tensorflow.',\n    'GRU': 'GRU with clip or GRU with linear_before_reset, or GRU not using '\n           'sigmoid for z and r, or GRU using Elu as the activation function '\n           'with alpha != 1, or GRU using HardSigmoid as the activation '\n           'function with alpha != 0.2 or beta != 0.5 are not supported in '\n           'TensorFlow.',\n    'LSTM': 'LSTM not using sigmoid for `f`, or LSTM not using the same '\n            'activation for `g` and `h` are not supported in Tensorflow.',\n    'MaxPool': 'MaxPoolWithArgmax with pad is None or incompatible mode, or '\n               'MaxPoolWithArgmax with 4D or higher input, orMaxPoolWithArgmax '\n               'with column major are not supported in Tensorflow.',\n    'Mod': 'Mod Dividend or Divisor in int8/int16/uint8/uint16/uint32/uint64 '\n           'are not supported in Tensorflow.',\n    'OneHot': 'OneHot indices in '\n              'uint16/uint32/uint64/int8/int16/float16/float/double, or OneHot '\n              'depth in '\n              'uint8/uint16/uint32/uint64/int8/int16/int64/float16/float/double '\n              'are not supported in Tensorflow.',\n    'RNN': 'RNN with clip is not supported in Tensorflow.',\n    'Resize': 'Resize required 4D input in Tensorflow.',\n    'Upsample': 'Upsample required 4D input in Tensorflow.'\n}\n"""
onnx_tf/pb_wrapper.py,15,"b'import inspect\nfrom itertools import chain\n\nimport numpy as np\nfrom onnx import NodeProto\nfrom onnx import TensorProto\nfrom onnx import ValueInfoProto\nfrom onnx import numpy_helper\nfrom onnx.helper import make_graph\nfrom onnx.helper import make_tensor\nfrom onnx.helper import make_tensor_value_info\nfrom onnx.helper import mapping\nimport tensorflow as tf\nfrom tensorflow.core.framework.attr_value_pb2 import AttrValue\nfrom tensorflow.core.framework.node_def_pb2 import NodeDef\n\nfrom onnx_tf.common import attr_converter\nfrom onnx_tf.common import attr_translator\nfrom onnx_tf.common import CONST_MINUS_ONE_INT32\nfrom onnx_tf.common import CONST_ONE_FP32\nfrom onnx_tf.common import CONST_ONE_INT32\nfrom onnx_tf.common import CONST_ZERO_INT32\nfrom onnx_tf.common import IS_PYTHON3\nfrom onnx_tf.common import logger\nfrom onnx_tf.common.data_type import any_dtype_to_onnx_dtype\n\nclass TensorflowNode(object):\n\n  def __init__(self,\n               node=None,\n               name=None,\n               inputs=None,\n               outputs=None,\n               attr=None,\n               domain=None,\n               op_type=None):\n    # storing a reference to the original protobuf object\n    if node is None:\n      self.node = None\n      self.name = name or """"\n      self.inputs = inputs or []\n      self.attr = attr or {}\n      self.domain = domain or """"\n      self.op_type = op_type or """"\n      self.outputs = outputs or self.get_outputs_names()\n    elif isinstance(node, (OnnxNode, NodeProto)):\n      self._load_onnx_node(node)\n    elif isinstance(node, NodeDef):\n      self._load_tf_node(node)\n\n  def _load_onnx_node(self, node):\n    if isinstance(node, NodeProto):\n      node = OnnxNode(node)\n    self.name = node.name\n    self.inputs = node.inputs\n    self.outputs = node.outputs\n    self.attr = node.attrs\n    self.domain = node.domain\n    self.op_type = node.op_type\n\n  def _load_tf_node(self, node):\n    self.node = node\n    self.name = node.name\n    self.inputs = list(node.input)\n    self.attr = {}\n    for key, val in node.attr.items():\n      new_val = attr_translator.translate_tf(key, val)\n      if isinstance(new_val, AttrValue):\n        new_val = attr_converter.convert_tf(new_val)\n      self.attr[key] = new_val\n    splitted_op_name = node.op.split(""."")\n    self.domain = """" if len(splitted_op_name) == 1 else ""."".join(\n        splitted_op_name[:-1])\n    self.op_type = splitted_op_name[-1]\n    self.outputs = self.get_outputs_names()\n\n  def get_outputs_names(self, num=None):\n    """""" Helper method to get outputs names.\n    e.g. tf.split: [Split, Split:1, Split:2]\n\n    :param num: Force to get `num` outputs names.\n    :return: List of outputs names.\n    """"""\n    if num is None:\n      if ""_output_shapes"" in self.attr:\n        num = len(self.attr[""_output_shapes""])\n      else:\n        num = 1\n        logger.warning(""_output_shapes is not in node.attr. ""\n                      ""The num of output is set to 1 for commonly. ""\n                      ""It will cause problem with case of multiple outputs."")\n    return [\n        self.name + "":{}"".format(i) if i > 0 else self.name for i in range(num)\n    ]\n\n\nclass TensorflowGraph(object):\n\n  def __init__(self, graph_def, outputs=(), graph_name=""graph""):\n    self._graph_name = graph_name\n    self._graph_def = self._process_graph_def(graph_def)\n    self._nodes = self._create_util_nodes() + [\n        TensorflowNode(node) for node in self.graph_def.node\n    ]\n    self._nodes_dict = {n.name: n for n in self._nodes}\n    self._outputs = outputs or self.get_output_node_names(self.graph_def)\n\n  @staticmethod\n  def _create_util_nodes():\n    util_nodes = [(CONST_MINUS_ONE_INT32, np.array([-1]).astype(np.int32)),\n                  (CONST_ZERO_INT32, np.array([0]).astype(np.int32)),\n                  (CONST_ONE_INT32, np.array([1]).astype(np.int32))]\n    return [\n        TensorflowNode(\n            op_type=""Const"",\n            name=name,\n            attr={\n                ""value"": value,\n                ""dtype"": any_dtype_to_onnx_dtype(value.dtype),\n                ""_output_shapes"": [value.shape]\n            }) for name, value in util_nodes\n    ]\n\n  def get_node_by_name(self, name):\n    node = self._nodes_dict.get(name, None)\n    if node is None:\n      raise ValueError(\n          ""Node {} is not found in the graph provided"".format(name))\n    return node\n\n  def _process_graph_def(self, graph_def):\n    if ""_output_shapes"" not in TensorflowNode(graph_def.node[0]).attr:\n      graph_def = self._add_infer_shapes(graph_def)\n    return graph_def\n\n  @staticmethod\n  def _add_infer_shapes(graph_def):\n    with tf.Graph().as_default():\n      with tf.Session(\n          config=tf.ConfigProto(\n              graph_options=tf.GraphOptions(infer_shapes=True))) as sess:\n        tf.import_graph_def(graph_def, name="""")\n      return sess.graph_def\n\n  @staticmethod\n  def get_output_node_names(graph_def):\n    """"""Get output node names from GraphDef.\n\n    Args:\n      graph_def: GraphDef object.\n\n    Returns:\n      List of output node names.\n    """"""\n    input_names, output_names = set(), set()\n    for node in graph_def.node:\n      output_names.add(node.name)\n      input_names.update(set(node.input))\n    return list(output_names - input_names)\n\n  def update_nodes(self, nodes):\n    self._nodes = nodes\n    self._nodes_dict = {n.name: n for n in self._nodes}\n\n  @property\n  def graph_def(self):\n    return self._graph_def\n\n  @property\n  def graph_name(self):\n    return self._graph_name\n\n  @property\n  def nodes(self):\n    return self._nodes\n\n  @property\n  def nodes_dict(self):\n    return self._nodes_dict\n\n  @property\n  def outputs(self):\n    return self._outputs\n\n\n# TODO: Move this into ONNX main library\nclass OnnxNode(object):\n  """"""\n  Reimplementation of NodeProto from ONNX, but in a form\n  more convenient to work with from Python.\n  """"""\n\n  def __init__(self, node):\n    self.name = str(node.name)\n    self.op_type = str(node.op_type)\n    self.domain = str(node.domain)\n    self.attrs = dict([(attr.name,\n                        attr_translator.translate_onnx(\n                            attr.name, attr_converter.convert_onnx(attr)))\n                       for attr in node.attribute])\n    self.inputs = list(node.input)\n    self.outputs = list(node.output)\n    self.node_proto = node\n\n\nclass OnnxGraph(object):\n  """""" A helper class for making ONNX graph.\n  This class holds all information ONNX graph needs.\n  """"""\n\n  def __init__(self, name=None, graph_proto=None):\n    if graph_proto:\n      self._name = graph_proto.name\n      self._inputs_proto = list(graph_proto.input)\n      self._outputs_proto = list(graph_proto.output)\n      self._nodes_proto = list(graph_proto.node)\n      self._consts_proto = list(graph_proto.initializer)\n      self._value_info_proto = list(graph_proto.value_info)\n      self._consts = dict([(init.name, numpy_helper.to_array(init))\n                           for init in graph_proto.initializer])\n    else:\n      self._name = name or """"\n      self._inputs_proto = []\n      self._outputs_proto = []\n      self._nodes_proto = []\n      self._consts = {}\n      self._consts_proto = []\n      self._value_info_proto = []\n    # Either way, data_type_cast_map is empty when initialized.\n    self._data_type_cast_map = {}\n\n    self._add_utility_constants()\n\n  def _add_utility_constants(self):\n    util_consts = {CONST_ONE_FP32: np.array([1.0]).astype(np.float32)}\n    # Add a few useful utility constants:\n    for name, value in util_consts.items():\n      self.add_const_explicit(name=name, value=value)\n      self.add_const_proto_explicit(\n          name=name, value=value, np_dtype=value.dtype)\n      self.add_input_proto_explicit(\n          name=name, shape=value.shape, np_dtype=value.dtype)\n\n  # This list holds the protobuf objects of type ValueInfoProto\n  # representing the input to the converted ONNX graph.\n  @property\n  def inputs_proto(self):\n    return self._inputs_proto\n\n  @inputs_proto.setter\n  def inputs_proto(self, inputs_proto):\n    self._inputs_proto = inputs_proto\n\n  @property\n  def all_node_inputs(self):\n    return list(chain.from_iterable(map(lambda p: p.input, self._nodes_proto)))\n\n  @property\n  def outputs(self):\n    return list(map(lambda p: p.name, self._outputs_proto))\n\n  @property\n  def outputs_proto(self):\n    return self._outputs_proto\n\n  # This list holds the protobuf objects of type NodeProto\n  # representing the ops in the converted ONNX graph.\n  @property\n  def nodes_proto(self):\n    return self._nodes_proto\n\n  @nodes_proto.setter\n  def nodes_proto(self, nodes_proto):\n    self._nodes_proto = nodes_proto\n\n  # This dictionary contains a map from the name of the constant\n  # op to the array of values it holds. This is useful because\n  # tensorflow is less eager to know about input values at\n  # graph construction time than ONNX. That is to say, some ONNX\n  # attributes are input tensors in TF. This dictionary extracts\n  # those values of constant tensors that are known at graph\n  # construction time.\n  @property\n  def consts(self):\n    return self._consts\n\n  @consts.setter\n  def consts(self, consts):\n    self._consts = consts\n\n  # Sometimes the constants are used as inputs to ops. This list\n  # holds initializers that creates global constant tensors available\n  # to be accessed by ops as inputs (as oppose to attributes which\n  # is supplied by the `consts` map above).\n  @property\n  def consts_proto(self):\n    return self._consts_proto\n\n  @consts_proto.setter\n  def consts_proto(self, consts_proto):\n    self._consts_proto = consts_proto\n\n  # A map holds nodes name and new data type. Will be used to\n  # process protos to match ONNX type constraints.\n  @property\n  def data_type_cast_map(self):\n    return self._data_type_cast_map\n\n  @data_type_cast_map.setter\n  def data_type_cast_map(self, data_type_cast_map):\n    self._data_type_cast_map = data_type_cast_map\n\n  # This list holds the protobuf objects of type ValueInfoProto\n  # representing the all nodes\' outputs to the converted ONNX graph.\n  @property\n  def value_info_proto(self):\n    return self._value_info_proto\n\n  def add_input_proto_explicit(self,\n                               name,\n                               shape,\n                               np_dtype=None,\n                               tf_dtype=None,\n                               onnx_dtype=None):\n    onnx_dtype = any_dtype_to_onnx_dtype(\n        np_dtype=np_dtype, tf_dtype=tf_dtype, onnx_dtype=onnx_dtype)\n    input_proto = make_tensor_value_info(name, onnx_dtype, shape)\n    self._inputs_proto.append(input_proto)\n\n  def add_input_proto(self, node):\n    name = node.name\n    onnx_dtype = node.attr[""dtype""]\n    shape = node.attr[""shape""] if node.op_type != ""Const"" else node.attr[\n        \'value\'].shape\n    self.add_input_proto_explicit(name, shape, onnx_dtype=onnx_dtype)\n\n  def add_output_proto(self, node):\n    output_onnx_type = node.attr.get(""T"", TensorProto.BOOL)\n    for i, output_shape in enumerate(node.attr[""_output_shapes""]):\n      output_name = node.name + "":{}"".format(i) if i > 0 else node.name\n      self._outputs_proto.append(\n          make_tensor_value_info(output_name, output_onnx_type, output_shape))\n\n  def add_node_proto(self, node_proto):\n    if not isinstance(node_proto, (list, tuple)):\n      node_proto = [node_proto]\n    self._nodes_proto.extend(node_proto)\n\n  def remove_node_proto(self, names):\n    if not isinstance(names, (list, tuple)):\n      names = [names]\n    self._nodes_proto = list(\n        filter(lambda x: x.name not in names, self._nodes_proto))\n\n  def add_const_explicit(self, name, value):\n    self._consts[name] = value\n\n  def add_const(self, node):\n    self.add_const_explicit(node.name, node.attr[""value""])\n\n  def add_const_proto_explicit(self,\n                               name,\n                               value,\n                               np_dtype=None,\n                               tf_dtype=None,\n                               onnx_dtype=None):\n    onnx_dtype = any_dtype_to_onnx_dtype(\n        np_dtype=np_dtype, tf_dtype=tf_dtype, onnx_dtype=onnx_dtype)\n\n    const_dim = len(value.shape)\n\n    if const_dim == 0:\n      raw_values = [value.tolist()]\n      values = [value]\n    else:\n      raw_values = value.flatten().tolist()\n      values = value\n\n    shape = np.array(values).shape\n    const_proto = make_tensor(\n        name=name, data_type=onnx_dtype, dims=shape, vals=raw_values)\n    self._consts_proto.append(const_proto)\n\n  def add_const_proto(self, node):\n    self.add_const_proto_explicit(\n        node.name, node.attr[""value""], onnx_dtype=node.attr[""dtype""])\n\n  def add_value_info_proto(self, node):\n    node_onnx_type = node.attr.get(""T"", TensorProto.BOOL)\n    for i, output_shape in enumerate(node.attr[""_output_shapes""]):\n      node_name = node.name + "":{}"".format(i) if i > 0 else node.name\n      value_info_proto = make_tensor_value_info(node_name, node_onnx_type,\n                                                output_shape)\n      self._value_info_proto.append(value_info_proto)\n\n  # Remove proto in inputs_proto and consts_proto\n  # if proto is not used as input or an output in ONNX\n  def _clean_graph(self):\n    in_out = self.all_node_inputs + self.outputs\n    self._inputs_proto = list(\n        filter(lambda x: x.name in in_out, self.inputs_proto))\n    self._consts_proto = list(\n        filter(lambda x: x.name in in_out, self.consts_proto))\n\n  def _fix_data_type(self):\n    self.inputs_proto = self._data_type_caster(self.inputs_proto,\n                                               self.data_type_cast_map)\n    self.consts_proto = self._data_type_caster(self.consts_proto,\n                                               self.data_type_cast_map)\n\n  @classmethod\n  def _data_type_caster(cls, protos, data_type_cast_map):\n    """"""Cast to a new data type if node name is in data_type_cast_map.\n    Be used to process protos to match ONNX type constraints.\n\n    :param protos: Target protos.\n      TensorProto for inputs and ValueInfoProto for consts.\n    :param data_type_cast_map: A {node.name: new_data_type} dict.\n    :return: Processed protos.\n    """"""\n    if not data_type_cast_map:\n      return protos\n    result = []\n    for proto in protos:\n      new_proto = proto\n      if proto.name in data_type_cast_map:\n        new_data_type = data_type_cast_map[proto.name]\n        if type(proto) == TensorProto and proto.data_type != new_data_type:\n          field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[\n              mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[proto.data_type]]\n          vals = getattr(proto, field)\n          new_proto = make_tensor(\n              name=proto.name,\n              data_type=new_data_type,\n              dims=proto.dims,\n              vals=vals)\n        elif type(\n            proto\n        ) == ValueInfoProto and proto.type.tensor_type.elem_type != new_data_type:\n          new_proto.type.tensor_type.elem_type = new_data_type\n      result.append(new_proto)\n    return result\n\n  def make_graph_proto(self):\n    self._clean_graph()\n    self._fix_data_type()\n\n    if IS_PYTHON3:\n      params = list(inspect.signature(make_graph).parameters.keys())\n    else:\n      params = inspect.getargspec(make_graph).args\n\n    kwargs = {\n        ""initializer"": self.consts_proto,\n        ""value_info"": self.value_info_proto\n    }\n\n    return make_graph(self.nodes_proto, self._name, self.inputs_proto,\n                      self.outputs_proto,\n                      **dict([(k, kwargs[k]) for k in kwargs if k in params]))\n'"
test/__init__.py,0,b''
test/test_cli.py,3,"b'import inspect\nimport os\nimport subprocess\nimport unittest\n\nimport onnx\nfrom onnx.backend.test.runner import Runner\nfrom onnx.backend.test.case.model import TestCase\n\nfrom onnx_tf.backend import TensorflowBackend\nfrom onnx_tf.common import IS_PYTHON3\nfrom onnx_tf.common.legacy import legacy_onnx_pre_ver\n\n_ONNX_MODELS = [(\n    ""mobilenetv2-1.0"",\n    ""https://s3.amazonaws.com/onnx-model-zoo/mobilenet/mobilenetv2-1.0/mobilenetv2-1.0.tar.gz""\n)]\n\n\nclass TestCli(unittest.TestCase):\n\n  @staticmethod\n  def prepare_model(model_name, url):\n    if legacy_onnx_pre_ver(1, 5, 0):\n      prepare_model_data = Runner._prepare_model_data\n    else:\n      prepare_model_data = Runner.prepare_model_data\n    if IS_PYTHON3:\n      params = list(\n          inspect.signature(prepare_model_data).parameters.keys())\n    else:\n      params = inspect.getargspec(prepare_model_data).args\n    runner_class = Runner\n    if params[0] == ""self"":\n      runner_class = Runner(TensorflowBackend)\n      if legacy_onnx_pre_ver(1, 5, 0):\n        prepare_model_data = runner_class._prepare_model_data\n      else:\n        prepare_model_data = runner_class.prepare_model_data\n    if legacy_onnx_pre_ver(1, 4, 0):\n      tc = TestCase(\n          name=""test_{}"".format(model_name),\n          model_name=model_name,\n          url=url,\n          model_dir=None,\n          model=None,\n          data_sets=None,\n          kind=\'real\')\n    else:\n      tc = TestCase(\n          name=""test_{}"".format(model_name),\n          model_name=model_name,\n          url=url,\n          model_dir=None,\n          model=None,\n          data_sets=None,\n          kind=\'real\',\n          rtol=1e-3,\n          atol=1e-7)\n    return prepare_model_data(model_test=tc)\n\n  def test_convert_to_tf(self):\n    if legacy_onnx_pre_ver(1, 2, 1):\n      raise unittest.SkipTest(\n          ""The current version of ONNX uses dead model link."")\n    for model_name, url in _ONNX_MODELS:\n      model_dir = self.prepare_model(model_name, url)\n      subprocess.check_call([\n          ""onnx-tf"",\n          ""convert"",\n          ""-i"",\n          os.path.join(model_dir, \'{}.onnx\'.format(model_name)),\n          ""-o"",\n          os.path.join(model_dir, \'{}.pb\'.format(model_name)),\n      ])\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
third_party/__init__.py,0,b''
third_party/get_info.py,0,"b'# Copyright 2015: Mirantis Inc.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the ""License""); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport re\nimport sys\n\nPARAM_OR_RETURNS_REGEX = re.compile("":(?:param|returns)"")\nRETURNS_REGEX = re.compile("":returns: (?P<doc>.*)"", re.S)\nPARAM_REGEX = re.compile("":param (?P<name>[\\*\\w]+): (?P<doc>.*?)""\n                         ""(?:(?=:param)|(?=:return)|(?=:raises)|\\Z)"", re.S)\n\n\ndef trim(docstring):\n    """"""trim function from PEP-257""""""\n    if not docstring:\n        return """"\n    # Convert tabs to spaces (following the normal Python rules)\n    # and split into a list of lines:\n    lines = docstring.expandtabs().splitlines()\n    # Determine minimum indentation (first line doesn\'t count):\n    indent = sys.maxsize\n    for line in lines[1:]:\n        stripped = line.lstrip()\n        if stripped:\n            indent = min(indent, len(line) - len(stripped))\n    # Remove indentation (first line is special):\n    trimmed = [lines[0].strip()]\n    if indent < sys.maxsize:\n        for line in lines[1:]:\n            trimmed.append(line[indent:].rstrip())\n    # Strip off trailing and leading blank lines:\n    while trimmed and not trimmed[-1]:\n        trimmed.pop()\n    while trimmed and not trimmed[0]:\n        trimmed.pop(0)\n\n    # Current code/unittests expects a line return at\n    # end of multiline docstrings\n    # workaround expected behavior from unittests\n    if ""\\n"" in docstring:\n        trimmed.append("""")\n\n    # Return a single string:\n    return ""\\n"".join(trimmed)\n\n\ndef reindent(string):\n    return ""\\n"".join(l.strip() for l in string.strip().split(""\\n""))\n\n\ndef parse_docstring(docstring):\n    """"""Parse the docstring into its components.\n\n    :returns: a dictionary of form\n              {\n                  ""short_description"": ...,\n                  ""long_description"": ...,\n                  ""params"": [{""name"": ..., ""doc"": ...}, ...],\n                  ""returns"": ...\n              }\n    """"""\n\n    short_description = long_description = returns = """"\n    params = []\n\n    if docstring:\n        docstring = trim(docstring)\n\n        lines = docstring.split(""\\n"", 1)\n        short_description = lines[0]\n\n        if len(lines) > 1:\n            long_description = lines[1].strip()\n\n            params_returns_desc = None\n\n            match = PARAM_OR_RETURNS_REGEX.search(long_description)\n            if match:\n                long_desc_end = match.start()\n                params_returns_desc = long_description[long_desc_end:].strip()\n                long_description = long_description[:long_desc_end].rstrip()\n\n            if params_returns_desc:\n                params = [\n                    {""name"": name, ""doc"": trim(doc)}\n                    for name, doc in PARAM_REGEX.findall(params_returns_desc)\n                ]\n\n                match = RETURNS_REGEX.search(params_returns_desc)\n                if match:\n                    returns = reindent(match.group(""doc""))\n\n    return {\n        ""short_description"": short_description,\n        ""long_description"": long_description,\n        ""params"": params,\n        ""returns"": returns\n    }\n'"
util/get_version.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport sys\nimport onnx\nimport tensorflow\nimport onnx_tf\nimport pkg_resources\n\nprint(""Python version:"")\nprint(sys.version)\nprint(""ONNX version:"")\nprint(onnx.version.version)\nprint(""ONNX-TF version:"")\nprint(pkg_resources.get_distribution(""onnx-tf"").version)\nprint(""Tensorflow version:"")\nprint(tensorflow.__version__)'"
onnx_tf/common/__init__.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport inspect\nimport re\nimport sys\nimport uuid\nimport warnings\nimport logging\n\nfrom onnx.backend.base import DeviceType\nfrom tensorflow.python.client import device_lib\n\nIS_PYTHON3 = sys.version_info > (3,)\nlogger = logging.getLogger(\'onnx-tf\')\n\n# create console handler and formatter for logger\nconsole = logging.StreamHandler()\nformatter = logging.Formatter(\n    \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\nconsole.setFormatter(formatter)\nlogger.addHandler(console)\n\n\nclass Deprecated:\n  """"""Add deprecated message when function is called.\n\n  Usage:\n    from onnx_tf.common import deprecated\n\n    @deprecated\n    def func():\n      pass\n\n    UserWarning: func is deprecated. It will be removed in future release.\n\n    @deprecated(""Message"")\n    def func():\n      pass\n\n    UserWarning: Message\n\n    @deprecated({""arg"": ""Message"",\n                 ""arg_1"": deprecated.MSG_WILL_REMOVE,\n                 ""arg_2"": """",})\n    def func(arg, arg_1, arg_2):\n      pass\n\n    UserWarning: Message\n    UserWarning: arg_1 of func is deprecated. It will be removed in future release.\n    UserWarning: arg_2 of func is deprecated.\n  """"""\n\n  MSG_WILL_REMOVE = "" It will be removed in future release.""\n\n  def __call__(self, *args, **kwargs):\n    return self.deprecated_decorator(*args, **kwargs)\n\n  @staticmethod\n  def messages():\n    return {v for k, v in inspect.getmembers(Deprecated) if k.startswith(""MSG"")}\n\n  @staticmethod\n  def deprecated_decorator(arg=None):\n    # deprecate function with default message MSG_WILL_REMOVE\n    # @deprecated\n    if inspect.isfunction(arg):\n\n      def wrapper(*args, **kwargs):\n        warnings.warn(""{} is deprecated.{}"".format(\n            arg.__module__ + ""."" + arg.__name__, Deprecated.MSG_WILL_REMOVE))\n        return arg(*args, **kwargs)\n\n      return wrapper\n\n    deprecated_arg = arg if arg is not None else Deprecated.MSG_WILL_REMOVE\n\n    def deco(func):\n      # deprecate arg\n      # @deprecated({...})\n      if isinstance(deprecated_arg, dict):\n        for name, message in deprecated_arg.items():\n          if message in Deprecated.messages():\n            message = ""{} of {} is deprecated.{}"".format(\n                name, func.__module__ + ""."" + func.__name__, message or """")\n          warnings.warn(message)\n      # deprecate function with message\n      # @deprecated(""message"")\n      elif isinstance(deprecated_arg, str):\n        message = deprecated_arg\n        if message in Deprecated.messages():\n          message = ""{} is deprecated.{}"".format(\n              func.__module__ + ""."" + func.__name__, message)\n        warnings.warn(message)\n      return func\n\n    return deco\n\n\ndeprecated = Deprecated()\n\n\n# This function inserts an underscore before every upper\n# case letter and lowers that upper case letter except for\n# the first letter.\ndef op_name_to_lower(name):\n  return re.sub(\'(?<!^)(?=[A-Z])\', \'_\', name).lower()\n\n\ndef get_unique_suffix():\n  """""" Get unique suffix by using first 8 chars from uuid.uuid4\n  to make unique identity name.\n\n  :return: Unique suffix string.\n  """"""\n  return str(uuid.uuid4())[:8]\n\n\ndef get_perm_from_formats(from_, to_):\n  """""" Get perm from data formats.\n  For example:\n    get_perm_from_formats(\'NHWC\', \'NCHW\') = [0, 3, 1, 2]\n\n  :param from_: From data format string.\n  :param to_: To data format string.\n  :return: Perm. Int list.\n  """"""\n  return list(map(lambda x: from_.find(x), to_))\n\n\n# TODO: allow more flexible placement\ndef get_device_option(device):\n  m = {DeviceType.CPU: \'/cpu\', DeviceType.CUDA: \'/gpu\'}\n  return m[device.type]\n\n\ndef get_data_format(x_rank):\n  """""" Get data format by input rank.\n  Channel first if support CUDA.\n\n  :param x_rank: Input rank.\n  :return: Data format.\n  """"""\n  sp_dim_names = [""D"", ""H"", ""W""]\n  sp_dim_lst = []\n  for i in range(x_rank - 2):\n    sp_dim_lst.append(sp_dim_names[-i - 1])\n\n  sp_dim_string = """".join(reversed(sp_dim_lst))\n  storage_format = ""NC"" + sp_dim_string\n\n  if supports_device(""CUDA""):\n    compute_format = ""NC"" + sp_dim_string\n  else:\n    compute_format = ""N"" + sp_dim_string + ""C""\n  return storage_format, compute_format\n\n\ndef supports_device(device):\n  """""" Check if support target device.\n\n  :param device: CUDA or CPU.\n  :return: If supports.\n  """"""\n  if device == ""CUDA"":\n    local_device_protos = device_lib.list_local_devices()\n    return len([x.name for x in local_device_protos if x.device_type == \'GPU\'\n               ]) > 0\n  elif device == ""CPU"":\n    return True\n  return False\n\n\n@deprecated(""onnx_tf.common.get_outputs_names is deprecated.{} {}"".format(\n    deprecated.MSG_WILL_REMOVE,\n    ""Use TensorflowGraph.get_outputs_names instead.""))\ndef get_output_node_names(graph_def):\n  """"""Get output node names from GraphDef.\n  Args:\n    graph_def: GraphDef object.\n  Returns:\n    List of output node names.\n  """"""\n  nodes, input_names = dict(), set()\n  for node in graph_def.node:\n    nodes[node.name] = node\n    input_names.update(set(node.input))\n  return list(set(nodes) - input_names)\n\n\nCONST_MINUS_ONE_INT32 = ""_onnx_tf_internal_minus_one_int32""\nCONST_ZERO_INT32 = ""_onnx_tf_internal_zero_int32""\nCONST_ONE_INT32 = ""_onnx_tf_internal_one_int32""\nCONST_ONE_FP32 = ""_onnx_tf_internal_one_fp32""\n'"
onnx_tf/common/attr_converter.py,1,"b'from onnx_tf.common import IS_PYTHON3\n\n\ndef convert_tf(attr):\n  return __convert_tf_attr_value(attr)\n\n\ndef convert_onnx(attr):\n  return __convert_onnx_attribute_proto(attr)\n\n\ndef __convert_tf_attr_value(attr):\n  """""" convert Tensorflow AttrValue object to Python object\n  """"""\n  if attr.HasField(\'list\'):\n    return __convert_tf_list_value(attr.list)\n  if attr.HasField(\'s\'):\n    return attr.s\n  elif attr.HasField(\'i\'):\n    return attr.i\n  elif attr.HasField(\'f\'):\n    return attr.f\n  elif attr.HasField(\'b\'):\n    return attr.b\n  elif attr.HasField(\'type\'):\n    return attr.type\n  elif attr.HasField(\'shape\'):\n    return attr.type\n  elif attr.HasField(\'tensor\'):\n    return attr.tensor\n  else:\n    raise ValueError(""Unsupported Tensorflow attribute: {}"".format(attr))\n\n\ndef __convert_tf_list_value(list_value):\n  """""" convert Tensorflow ListValue object to Python object\n  """"""\n  if list_value.s:\n    return list_value.s\n  elif list_value.i:\n    return list_value.i\n  elif list_value.f:\n    return list_value.f\n  elif list_value.b:\n    return list_value.b\n  elif list_value.tensor:\n    return list_value.tensor\n  elif list_value.type:\n    return list_value.type\n  elif list_value.shape:\n    return list_value.shape\n  elif list_value.func:\n    return list_value.func\n  else:\n    raise ValueError(""Unsupported Tensorflow attribute: {}"".format(list_value))\n\n\ndef __convert_onnx_attribute_proto(attr_proto):\n  """"""\n  Convert an ONNX AttributeProto into an appropriate Python object\n  for the type.\n  NB: Tensor attribute gets returned as the straight proto.\n  """"""\n  if attr_proto.HasField(\'f\'):\n    return attr_proto.f\n  elif attr_proto.HasField(\'i\'):\n    return attr_proto.i\n  elif attr_proto.HasField(\'s\'):\n    return str(attr_proto.s, \'utf-8\') if IS_PYTHON3 else attr_proto.s\n  elif attr_proto.HasField(\'t\'):\n    return attr_proto.t  # this is a proto!\n  elif attr_proto.HasField(\'g\'):\n    return attr_proto.g\n  elif attr_proto.floats:\n    return list(attr_proto.floats)\n  elif attr_proto.ints:\n    return list(attr_proto.ints)\n  elif attr_proto.strings:\n    str_list = list(attr_proto.strings)\n    if IS_PYTHON3:\n      str_list = list(map(lambda x: str(x, \'utf-8\'), str_list))\n    return str_list\n  elif attr_proto.HasField(\'sparse_tensor\'):\n    return attr_proto.sparse_tensor\n  else:\n    raise ValueError(""Unsupported ONNX attribute: {}"".format(attr_proto))\n'"
onnx_tf/common/attr_translator.py,1,"b'from tensorflow.python.framework.tensor_util import MakeNdarray\n\nfrom onnx_tf.common import data_type\n\n# Keyed by old attribute names.\n__tf_attr_translator = {\n    ""_output_shapes"": lambda x: list(map(lambda shape: get_tf_shape_as_list(shape.dim), x.list.shape)),\n    ""shape"": lambda x: get_tf_shape_as_list(x.shape.dim),\n    ""T"": lambda x: data_type.tf2onnx(list(x.list.type) or x.type),\n    ""dtype"": lambda x: data_type.tf2onnx(list(x.list.type) or x.type),\n    ""component_types"": lambda x: data_type.tf2onnx(list(x.list.type) or x.type),\n    ""value"": lambda x: MakeNdarray(x.tensor),\n    ""seed2"": lambda x: float(x.i),\n    ""seed"": lambda x: float(x.i),\n    ""keep_dims"": lambda x: int(x.b),\n    ""squeeze_dims"": lambda x: list(x.list.i),\n}\n\n__onnx_attr_translator = {\n    ""axis"": lambda x: int(x),\n    ""axes"": lambda x: [int(a) for a in x],\n    ""dtype"": lambda x: data_type.onnx2tf(x),\n    ""keepdims"": lambda x: bool(x),\n    ""to"": lambda x: data_type.onnx2tf(x),\n}\n\n\ndef translate_tf(key, val):\n  return __tf_attr_translator.get(key, lambda x: x)(val)\n\n\ndef translate_onnx(key, val):\n  return __onnx_attr_translator.get(key, lambda x: x)(val)\n\n\ndef get_tf_shape_as_list(tf_shape_dim):\n  return list(map(lambda x: x.size, list(tf_shape_dim)))\n'"
onnx_tf/common/data_type.py,6,"b'from numbers import Number\n\nimport numpy as np\nfrom onnx import mapping\nfrom onnx import TensorProto\nimport tensorflow as tf\n\n\ndef tf2onnx(dtype):\n  if isinstance(dtype, Number):\n    tf_dype = tf.as_dtype(dtype)\n  elif isinstance(dtype, tf.DType):\n    tf_dype = dtype\n  elif isinstance(dtype, list):\n    return [tf2onnx(t) for t in dtype]\n  else:\n    raise RuntimeError(""dtype should be number or tf.DType."")\n\n  # Usually, tf2onnx is done via tf_type->numpy_type->onnx_type\n  # to leverage existing type conversion infrastructure;\n  # However, we need to intercept the string type early because\n  # lowering tf.string type to numpy dtype results in loss of\n  # information. <class \'object\'> is returned instead of the\n  # numpy string type desired.\n  if tf_dype is tf.string:\n    return TensorProto.STRING\n\n  onnx_dtype = None\n  try:\n    onnx_dtype = mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype(\n        tf_dype.as_numpy_dtype)]\n  finally:\n    if onnx_dtype is None:\n      common.logger.warning(\n          ""Can\'t convert tf dtype {} to ONNX dtype. Return 0 (TensorProto.UNDEFINED).""\n          .format(tf_dype))\n      onnx_dtype = TensorProto.UNDEFINED\n    return onnx_dtype\n\n\ndef onnx2tf(dtype):\n  return tf.as_dtype(mapping.TENSOR_TYPE_TO_NP_TYPE[_onnx_dtype(dtype)])\n\n\ndef onnx2field(dtype):\n  return mapping.STORAGE_TENSOR_TYPE_TO_FIELD[_onnx_dtype(dtype)]\n\n\ndef _onnx_dtype(dtype):\n  if isinstance(dtype, Number):\n    onnx_dype = dtype\n  elif isinstance(dtype, str):\n    onnx_dype = TensorProto.DataType.Value(dtype)\n  else:\n    raise RuntimeError(""dtype should be number or str."")\n  return onnx_dype\n\n\n# TODO (tjingrant) unify _onnx_dtype into any_dtype_to_onnx_dtype\ndef any_dtype_to_onnx_dtype(np_dtype=None, tf_dtype=None, onnx_dtype=None):\n  dtype_mask = [1 if val else 0 for val in [np_dtype, tf_dtype, onnx_dtype]]\n  num_type_set = sum(dtype_mask)\n  assert num_type_set == 1, ""One and only one type must be set. However, {} set."".format(\n      sum(num_type_set))\n\n  if np_dtype:\n    onnx_dtype = mapping.NP_TYPE_TO_TENSOR_TYPE[np_dtype]\n  if tf_dtype:\n    onnx_dtype = tf2onnx(tf_dtype)\n\n  return onnx_dtype\n'"
onnx_tf/common/exception.py,1,"b'import inspect\nimport onnx_tf.common as common\n\nclass CustomException(object):\n\n  def __init__(self):\n    self._func = RuntimeError\n    self._message = """"\n\n  def __call__(self, *args, **kwargs):\n    if inspect.isclass(self._func) and issubclass(self._func, Exception):\n      raise self._func(self.get_message(*args, **kwargs))\n    elif callable(self._func):\n      self._func(self.get_message(*args, **kwargs))\n\n  def get_message(self, *args, **kwargs):\n    return self._message\n\n\nclass OpUnimplementedException(CustomException):\n\n  def __init__(self):\n    super(OpUnimplementedException, self).__init__()\n    self._func = NotImplementedError\n    self._message = ""{} is not implemented.""\n\n  def __call__(self, op, version=None, domain=None):\n    if IGNORE_UNIMPLEMENTED:\n      self._func = common.logger.warning\n    super(OpUnimplementedException, self).__call__(op, version, domain)\n\n  def get_message(self, op, version=None, domain=None):\n    insert_message = op\n    if version is not None:\n      insert_message += "" version {}"".format(version)\n    if domain is not None:\n      insert_message += "" in domain `{}`"".format(domain)\n    return self._message.format(insert_message)\n\n\nclass OpUnsupportedException(object):\n\n  def __init__(self):\n    super(OpUnsupportedException, self).__init__()\n    self._func = RuntimeError\n    self._message = ""{} is not supported in {}.""\n\n  def __call__(self, op, framework):\n    raise self._func(self.get_message(op, framework))\n\n  def get_message(self, op, framework):\n    return self._message.format(op, framework)\n\n\nclass ConstNotFoundException(CustomException):\n\n  def __init__(self):\n    super(ConstNotFoundException, self).__init__()\n    self._func = RuntimeError\n    self._message = ""{} of {} is not found in graph consts.""\n\n  def __call__(self, name, op):\n    super(ConstNotFoundException, self).__call__(name, op)\n\n  def get_message(self, name, op):\n    return self._message.format(name, op)\n\n\nIGNORE_UNIMPLEMENTED = False\nOP_UNIMPLEMENTED_EXCEPT = OpUnimplementedException()\nOP_UNSUPPORTED_EXCEPT = OpUnsupportedException()\nCONST_NOT_FOUND_EXCEPT = ConstNotFoundException()\n'"
onnx_tf/common/handler_helper.py,3,"b'from onnx import defs\n\nfrom onnx_tf.handlers.backend import *  # noqa\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nimport onnx_tf.common as common\n\ndef get_all_backend_handlers(opset_dict):\n  """""" Get a dict of all backend handler classes.\n  e.g. {\'domain\': {\'Abs\': Abs handler class}, ...}, }.\n\n  :param opset_dict: A dict of opset. e.g. {\'domain\': version, ...}\n  :return: Dict.\n  """"""\n  handlers = {}\n  for handler in BackendHandler.__subclasses__():\n    handler.check_cls()\n\n    domain = handler.DOMAIN\n    version = opset_dict[domain]\n    handler.VERSION = version\n\n    since_version = 1\n    if defs.has(handler.ONNX_OP, domain=handler.DOMAIN):\n      try:\n        since_version = defs.get_schema(\n            handler.ONNX_OP,\n            domain=handler.DOMAIN,\n            max_inclusive_version=version).since_version\n      except RuntimeError:\n        common.logger.info(""Fail to get since_version of {} in domain `{}` ""\n                      ""with max_inclusive_version={}. Set to 1."".format(\n                          handler.ONNX_OP, handler.DOMAIN, version))\n    else:\n      common.logger.info(""Unknown op {} in domain `{}`."".format(\n          handler.ONNX_OP, handler.DOMAIN or ""ai.onnx""))\n    handler.SINCE_VERSION = since_version\n    handlers.setdefault(domain, {})[handler.ONNX_OP] = handler\n  return handlers\n\n\ndef get_backend_coverage():\n  """""" Get backend coverage for document.\n\n  :return: onnx_coverage: e.g. {\'domain\': {\'ONNX_OP\': [versions], ...}, ...}\n  """"""\n\n  onnx_coverage = {}\n  experimental_op = set()\n  for handler in BackendHandler.__subclasses__():\n    handler.check_cls()\n\n    versions = handler.get_versions()\n    domain = handler.DOMAIN\n    if getattr(handler, ""EXPERIMENTAL"", False):\n      experimental_op.add(handler.ONNX_OP)\n    _update_coverage(onnx_coverage, domain, handler.ONNX_OP, versions)\n  return onnx_coverage, experimental_op\n\n\ndef _update_coverage(coverage, domain, key, versions):\n  domain_coverage = coverage.setdefault(domain, {})\n  vers = domain_coverage.get(key, [])\n  vers.extend(versions)\n  domain_coverage[key] = sorted(list(set(vers)))\n\n\ndef get_backend_partial_support_detail():\n  ps_dict = {}\n  opset_dict = dict([(defs.ONNX_DOMAIN, defs.onnx_opset_version())])\n  handlers = get_all_backend_handlers(opset_dict)[defs.ONNX_DOMAIN]\n  for op_name in handlers:\n    if handlers[op_name].PARTIAL_SUPPORT:\n      ps_dict[op_name] = handlers[op_name].PS_DESCRIPTION\n  return ps_dict\n'"
onnx_tf/common/legacy.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport onnx\n\n\ndef get_onnx_version():\n  return tuple(map(int, onnx.version.version.split(""."")))\n\n\n# Returns whether onnx version is prior to major.minor.patch\ndef legacy_onnx_pre_ver(major=0, minor=0, patch=0):\n  return get_onnx_version() < (major, minor, patch)\n\n\n# Returns whether the opset version accompanying the\n# onnx installation is prior to version passed.\ndef legacy_opset_pre_ver(version):\n  return onnx.defs.onnx_opset_version() < version\n'"
onnx_tf/common/pooling_helper.py,2,"b'from __future__ import division\n\nfrom collections import namedtuple\nimport numpy as np\nimport tensorflow as tf\n\nimport itertools\n\n\npad_ops = namedtuple(""pad_ops"",\n                     [""max_op"", ""ceil_op"", ""floor_op"", ""cast_int_op""])\n\npad_numpy_ops = pad_ops(np.maximum, np.ceil, np.floor,\n                        lambda arr: arr.astype(np.int64))\npad_tf_ops = pad_ops(tf.maximum, tf.math.ceil, tf.math.floor,\n                     lambda tensor: tf.cast(tensor, tf.int64))\n\n\ndef calc_pads_same(in_spatial_shape, kernel_shape, strides,\n                   dilations, padding, padding_ops=pad_numpy_ops,\n                   pads_order=1):\n    """"""\n        Calculates the SAME paddings that need to be added to the input\n\n        Args:\n            in_spatial_shape:   input spatial shape\n            kernel_shape:       the size of the kernel along each axis\n            strides:            stride along each spatial axis\n            dilations:          dilations value along each spatial axis\n            padding:            padding to calculate: SAME_UPPER or\n                                SAME_LOWER\n            padding_ops:        namedtuple with ops to be used during\n                                calculations. there are two sets of ops\n                                defined pad_numpy_ops and pad_tf_ops with\n                                numpy and tensorflow ops\n            pads_order:         order of returned pads. possible options are:\n                                    1 - b1, b2, ..., bn, e1, e2, ..., en\n                                    2 - b1, e1, b2, e2, ..., bn, en\n                                where n = len(kernel_shape) * 2,\n                                b1, b2, ..., bn define pads at the begging of\n                                                axis\n                                e1, e2, ..., en define pads at the end of\n                                                axis\n        Return:\n            pads:               array with calculated pads. the order of the\n                                values is determined by `pads_order`\n\n    """"""\n    spatial_size = len(kernel_shape)\n    pads = [0] * (spatial_size * 2)\n    for i in range(spatial_size):\n        in_size = in_spatial_shape[i]\n        filter_size = (kernel_shape[i] - 1) * dilations[i] + 1\n\n        out_size = padding_ops.ceil_op(in_size / strides[i])\n        out_size = padding_ops.cast_int_op(out_size)\n        pad_along_axis = \\\n            padding_ops.max_op((out_size - 1) * strides[i] +\n                               filter_size - in_size, 0)\n        if padding.lower() == ""same_lower"":\n            pad_op = padding_ops.ceil_op\n        else:\n            pad_op = padding_ops.floor_op\n        pad_begin = pad_op(pad_along_axis / 2)\n\n        pad_begin = padding_ops.cast_int_op(pad_begin)\n        pad_along_axis = padding_ops.cast_int_op(pad_along_axis)\n\n        pad_end = pad_along_axis - pad_begin\n\n        pads[i * pads_order] = pad_begin\n        pads[i * pads_order +\n             (spatial_size if pads_order == 1 else 1)] = pad_end\n\n    return pads\n\n\ndef calc_output_shape(input_spatial_shape, kernel_shape, strides, dilations,\n                      padding, ceil_mode=False):\n    """"""\n        Calculate output shape\n\n        Args:\n            input_spatial_shape: input spatial shape\n            kernel_shape:        the size of the kernel along each axis\n            strides:             stride along each spatial axis\n            dilations:           dilations value along each spatial axis\n            padding:             can be explicit paddings, ""SAME_UPPER"" or\n                                 ""SAME_LOWER""\n        Return:\n            output_shape:        calculated output shape\n    """"""\n    spatial_size = len(input_spatial_shape)\n\n    if type(padding) is not list and type(padding) is not np.ndarray:\n        if padding.lower().startswith(""same""):\n            padding = calc_pads_same(input_spatial_shape, kernel_shape,\n                                     strides, dilations, padding)\n        else:\n            padding = [0] * spatial_size * 2\n\n    output_shape = []\n    for dim in range(spatial_size):\n        output_shape.append(_pooling_output_shape(input_spatial_shape[dim],\n                            kernel_shape[dim], strides[dim], dilations[dim],\n                            padding[dim] + padding[dim + spatial_size],\n                            ceil_mode))\n\n    return output_shape\n\n\ndef _pooling_output_shape(input_size, ksize, stride, dilation, pad, ceil_mode):\n    output_size = (input_size + pad - ((ksize - 1) * dilation + 1) +\n                   ((stride-1) if ceil_mode else 0)) // stride + 1\n    if (pad):\n        if ((output_size - 1) * stride >= input_size + pad):\n            output_size -= 1\n    return output_size\n\n\ndef py_pool(input, kernel_shape, strides=None, dilations=None,\n            padding=None, ceil_mode=False, pooling_type=""MAX"",\n            include_indices=True):\n    """"""\n        Implementation of Max and Average pool operations in Python\n        Args:\n            input:        input N-D data array in NC* format\n            kernel_shape: the size of the kernel along each axis\n            strides:      stride along each spatial axis\n            dilations:    dilations value along each spatial axis of filter\n            padding:      padding for the beginning and ending along each\n                          spatial axis. `padding` format should be as follow\n                          [x1_begin, x2_begin...x1_end, x2_end,...]\n            ceil_mode:    whether to use ceil or floor (default) to compute\n                          the output shape.\n            pooling_type: specify pooling type. Values can be ""MAX"" or ""AVG"".\n            include_indices: should indices be included in the output\n      Return:\n            pooled:       output data from max pooling across the input\n            ind:          indices of the selected max values from the input\n    """"""\n\n    if type(pooling_type) is not str:\n        pooling_type = pooling_type.decode(""UTF-8"")\n\n    input_shape = np.shape(input)\n    inp_sp_shape = input_shape[2:]\n    input_dtype = input.dtype\n    if np.issubdtype(input_dtype, np.integer):\n        input_dtype_min = np.iinfo(input_dtype).min\n    else:\n        input_dtype_min = np.finfo(input_dtype).min\n\n    def _loop_over_output(batch, channel):\n        dims = [range(output_sp_shape[d]) for d in range(spatial_size)]\n        for counters in itertools.product(*dims):\n            input_ranges = []\n            for dim in range(spatial_size):\n                dim_start = \\\n                    counters[dim] * strides[dim] - pads[dim * 2]\n                dim_end = \\\n                    min(dim_start + (kernel_shape[dim] - 1) * dilations[dim]\n                        + 1, inp_sp_shape[dim])\n                while dim_start < 0:\n                    dim_start += dilations[dim]\n\n                cur_range = [i for i in range(dim_start,\n                                              dim_end, dilations[dim])]\n                input_ranges.append(cur_range)\n            if pooling_type == ""AVG"":\n                val_sum = 0\n                val_count = 0\n            else:\n                maxval = input_dtype_min\n                maxind = -1\n            for input_ind in itertools.product(*input_ranges):\n                ind = (batch, channel) + input_ind\n                val = input[ind]\n                if pooling_type == ""AVG"":\n                    val_sum += val\n                    val_count += 1\n                else:\n                    if val > maxval:\n                        maxval = val\n                        ind = 0\n                        for i in range(spatial_size):\n                            coef = 1\n                            for j in range(i+1, spatial_size):\n                                coef *= inp_sp_shape[j]\n                            ind += input_ind[i] * coef\n                        maxind = ind\n            ind = (batch, channel) + counters\n            if pooling_type == ""AVG"":\n                out_pool[ind] = val_sum / val_count\n            else:\n                out_pool[ind] = maxval\n                out_ind[ind] = maxind\n\n    spatial_size = len(kernel_shape)\n\n    batch_size = input_shape[0]\n    channels_num = input_shape[1]\n\n    if strides is None:\n        strides = kernel_shape\n\n    if dilations is None:\n        dilations = [1] * spatial_size\n\n    if padding is None:\n        padding = [0] * spatial_size * 2\n\n    if type(padding) is bytes:\n        padding = padding.decode()\n\n    if type(padding) is not list and type(padding) is not np.ndarray:\n        if padding.lower().startswith(""same""):\n            padding = calc_pads_same(inp_sp_shape, kernel_shape, strides,\n                                     dilations, padding)\n        else:\n            padding = [0] * spatial_size * 2\n\n    pads = []\n    pad_along_axis = []\n    output_sp_shape = []\n\n    for dim in range(spatial_size):\n        pads.append(padding[dim])\n        pads.append(padding[dim + spatial_size])\n        pad_along_axis.append(padding[dim] + padding[dim + spatial_size])\n\n        input_size = input_shape[dim + 2]\n        output_size = \\\n            _pooling_output_shape(input_size, kernel_shape[dim],\n                                  strides[dim], dilations[dim],\n                                  pad_along_axis[dim], ceil_mode)\n        output_sp_shape.append(output_size)\n\n    out_pool = np.zeros([input_shape[0], input_shape[1]] +\n                        output_sp_shape, input_dtype)\n    out_ind = np.zeros([input_shape[0], input_shape[1]] +\n                       output_sp_shape, np.int64)\n\n    for batch in range(batch_size):\n        for channel in range(channels_num):\n            _loop_over_output(batch, channel)\n\n    if not include_indices:\n        return out_pool\n    else:\n        return out_pool, out_ind\n'"
onnx_tf/common/tf_helper.py,7,"b'import tensorflow as tf\nimport numpy as np\n\n\ndef tf_shape(tensor):\n    """"""\n        Helper function returning the shape of a Tensor.\n        The function will check for fully defined shape and will return\n        numpy array or if the shape is not fully defined will use tf.shape()\n        to return the shape as a Tensor.\n    """"""\n    if tensor.shape.is_fully_defined():\n        return np.array(tensor.shape.as_list(), dtype=np.int64)\n    else:\n        return tf.shape(tensor, out_type=tf.int64)\n\n\ndef tf_product(a, b):\n    """"""\n        Calculates the cartesian product of two column vectors a and b\n\n        Example:\n\n        a = [[1]\n             [2]\n             [3]]\n\n        b = [[0]\n             [1]]\n\n        result = [[1 0]\n                  [1 1]\n                  [2 0]\n                  [2 1]\n                  [3 0]\n                  [3 1]]\n    """"""\n    tile_a = tf.tile(a, [1, tf.shape(b)[0]])\n    tile_a = tf.expand_dims(tile_a, 2)\n    tile_a = tf.reshape(tile_a, [-1, 1])\n\n    b = tf.tile(b, [tf.shape(a)[0], 1])\n    b = tf.concat([tile_a, b], axis=1)\n\n    return b\n'"
onnx_tf/handlers/__init__.py,0,b''
onnx_tf/handlers/backend_handler.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport inspect\n\nimport tensorflow as tf\n\nfrom onnx_tf.common import IS_PYTHON3\nfrom onnx_tf.common import get_data_format\nfrom onnx_tf.common import get_perm_from_formats\nfrom onnx_tf.common import supports_device\nfrom .handler import Handler\n\n\nclass BackendHandler(Handler):\n  """""" This class is base backend handler class.\n  All backend operator handler class MUST inherit this class.\n  In backend, operator handler class\'s name should be pascal case of file name\n  which should be snake case.\n  Use ONNX operator name as class name.\n  """"""\n\n  TF_FUNC = None\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    """""" Get param for attrs processor.\n\n    :return: Dict.\n    """"""\n    return {}\n\n  @classmethod\n  def _process_attrs(cls, attrs):\n    """""" Private method for processing attrs.\n    Param for this processor got from `get_attrs_processor_param`.\n    Param is dict contains two key: `default` and `raname`.\n    First add default value to attrs if key does not exist.\n    Second rename key to new key.\n\n    For example:\n      attrs = {""keep_dims"": True}\n      param = {""default"": {""axis"": 1},\n               ""rename"": {""keep_dims"": ""keepdims""}}\n\n      processed_attrs = {""axis"": ""1"", ""keepdims"": True}\n\n    :param attrs: Process target attrs.\n    :return: Processed attrs.\n    """"""\n    param = {""rename"": {}, ""default"": {}}\n    param.update(cls.get_attrs_processor_param())\n\n    for k, v in param[""default""].items():\n      attrs.setdefault(k, v)\n\n    for k, new_k in param[""rename""].items():\n      if k in attrs:\n        attrs[new_k] = attrs.pop(k)\n\n    return attrs\n\n  @classmethod\n  def make_tensor_from_onnx_node(cls,\n                                 node,\n                                 tf_func=None,\n                                 inputs=None,\n                                 attrs=None,\n                                 name="""",\n                                 c_first_cuda_only=False,\n                                 c_last_only=False,\n                                 **kwargs):\n    """""" Helper method to make tensor.\n\n    :param node: OnnxNode object.\n    :param tf_func: Callable Tf function. Default is cls.TF_FUNC.\n    :param inputs: Inputs tensor. Default is got from node.inputs.\n    :param attrs: Attributes. Default is node.attrs.\n    :param name: Node name.\n    :param c_first_cuda_only: If channel first is only supported by cuda.\n    If true and not cuda, do pre and post transpose.\n    :param c_last_only: If only channel last is support,\n    do pre and post transpose.\n    :param kwargs: Other args.\n    :return: Tensor.\n    """"""\n    tensor_dict = kwargs.get(""tensor_dict"", {})\n    tf_func = tf_func or cls.TF_FUNC\n    if tf_func is None:\n      raise RuntimeError(""No Tensorflow function is given."")\n    if inputs is None:\n      inputs = [tensor_dict.get(inp, None) for inp in node.inputs]\n    if attrs is None:\n      attrs = copy.deepcopy(node.attrs)\n    name = name or node.name\n    if name != """":\n      attrs[""name""] = name\n\n    if c_first_cuda_only and c_last_only:\n      raise ValueError(\n          ""c_first_cuda_only and c_last_only can not both be True."")\n\n    if c_first_cuda_only:\n      return cls.c_first_cuda_only(tf_func, inputs, attrs)\n    elif c_last_only:\n      return cls.c_last_only(tf_func, inputs, attrs)\n\n    return cls._run_tf_func(tf_func, inputs, attrs)\n\n  @classmethod\n  def c_first_cuda_only(cls, tf_func, inputs, attrs):\n    """""" Handle operator that channel first is only supported by CUDA.\n    When using CPU, two transposes should be added.\n\n    :param tf_func: Callable Tf function.\n    :param inputs: Inputs tensor.\n    :param attrs: Attributes.\n    :return: Tensor.\n    """"""\n    support_cuda = supports_device(""CUDA"")\n    if not support_cuda:\n      return cls._tuck_transpose(tf_func, inputs, attrs)\n    return cls._run_tf_func(tf_func, inputs, attrs)\n\n  @classmethod\n  def c_last_only(cls, tf_func, inputs, attrs):\n    """""" Handle operator that channel last only is supported.\n    Add two transposes anyway.\n\n    :param tf_func: Callable Tf function.\n    :param inputs: Inputs tensor.\n    :param attrs: Attributes.\n    :return: Tensor.\n    """"""\n    storage_format, compute_format = get_data_format(len(inputs[0].get_shape()))\n    compute_format = compute_format.replace(""C"", """") + ""C""\n    return cls._tuck_transpose(tf_func, inputs, attrs,\n                               (storage_format, compute_format))\n\n  @classmethod\n  def _tuck_transpose(cls, tf_func, inputs, attrs, data_format=None):\n    x = inputs[0]\n    x_rank = len(x.get_shape())\n    if not data_format:\n      data_format = get_data_format(x_rank)\n    pre_perm = get_perm_from_formats(data_format[0], data_format[1])\n    post_perm = get_perm_from_formats(data_format[1], data_format[0])\n    attrs[""data_format""] = data_format[1]\n    if pre_perm != list(range(x_rank)):\n      x_t = tf.transpose(x, perm=pre_perm)\n      y = cls._run_tf_func(tf_func, [x_t] + inputs[1:], attrs)\n      y_t = tf.transpose(y, perm=post_perm)\n      return y_t\n    return cls._run_tf_func(tf_func, inputs, attrs)\n\n  @classmethod\n  def _run_tf_func(cls, tf_func, inputs, attrs):\n    """""" Run Tensorflow function.\n    Use only acceptable attributes of function from attrs.\n\n    :param tf_func: Tensorflow function.\n    :param inputs: Inputs.\n    :param attrs: Attributes.\n    :return: Tensor.\n    """"""\n    if IS_PYTHON3:\n      params = list(inspect.signature(tf_func).parameters.keys())\n    else:\n      # use closure to get args for function using decorator\n      if tf_func.__closure__ is not None:\n        while ""__wrapped__"" in tf_func.func_dict:\n          tf_func = tf_func.func_dict[""__wrapped__""]\n        params = inspect.getargspec(tf_func).args\n      else:\n        params = inspect.getargspec(tf_func).args\n\n    attrs = cls._process_attrs(attrs)\n    return tf_func(*inputs,\n                   **dict([(p, attrs[p]) for p in params if p in attrs]))\n'"
onnx_tf/handlers/handler.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport inspect\n\nfrom onnx import defs\n\nfrom onnx_tf.common import exception\nfrom onnx_tf.common import IS_PYTHON3\n\n\nclass Handler(object):\n  """""" This class is base handler class.\n  Base backend and frontend base handler class inherit this class.\n\n  All operator handler MUST put decorator @onnx_op to register corresponding op.\n  """"""\n\n  ONNX_OP = None\n\n  DOMAIN = defs.ONNX_DOMAIN\n  VERSION = 0\n  SINCE_VERSION = 0\n  PARTIAL_SUPPORT = False\n  PS_DESCRIPTION = \'\'\n\n  @classmethod\n  def check_cls(cls):\n    if not cls.ONNX_OP:\n      common.logger.warning(\n          ""{} doesn\'t have ONNX_OP. ""\n          ""Please use Handler.onnx_op decorator to register ONNX_OP."".format(\n              cls.__name__))\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    """""" Check args. e.g. if shape info is in graph.\n    Raise exception if failed.\n\n    :param node: NodeProto for backend.\n    :param kwargs: Other args.\n    """"""\n    pass\n\n  @classmethod\n  def handle(cls, node, **kwargs):\n    """""" Main method in handler. It will find corresponding versioned handle method,\n    whose name format is `version_%d`. So prefix `version_` is reserved in onnx-tensorflow.\n    DON\'T use it for other purpose.\n\n    :param node: NodeProto for backend.\n    :param kwargs: Other args.\n    :return: TensorflowNode for backend.\n    """"""\n    ver_handle = getattr(cls, ""version_{}"".format(cls.SINCE_VERSION), None)\n    if ver_handle:\n      cls.args_check(node, **kwargs)\n      return ver_handle(node, **kwargs)\n    exception.OP_UNIMPLEMENTED_EXCEPT(node.op_type, cls.SINCE_VERSION)\n    return None\n\n  @classmethod\n  def get_versions(cls):\n    """""" Get all support versions.\n\n    :return: Version list.\n    """"""\n    versions = []\n    for k, v in inspect.getmembers(cls, inspect.ismethod):\n      if k.startswith(""version_""):\n        versions.append(int(k.replace(""version_"", """")))\n    return versions\n\n  @staticmethod\n  def onnx_op(op):\n    return Handler.property_register(""ONNX_OP"", op)\n\n  @staticmethod\n  def tf_func(func):\n    return Handler.property_register(""TF_FUNC"", func)\n\n  @staticmethod\n  def domain(d):\n    return Handler.property_register(""DOMAIN"", d)\n\n  @staticmethod\n  def partial_support(ps):\n    return Handler.property_register(""PARTIAL_SUPPORT"", ps)\n\n  @staticmethod\n  def ps_description(psd):\n    return Handler.property_register(""PS_DESCRIPTION"", psd)\n\n  @staticmethod\n  def property_register(name, value):\n\n    def deco(cls):\n      if inspect.isfunction(value) and not IS_PYTHON3:\n        setattr(cls, name, staticmethod(value))\n      else:\n        setattr(cls, name, value)\n      return cls\n\n    return deco\n\n\ndomain = Handler.domain\nonnx_op = Handler.onnx_op\ntf_func = Handler.tf_func\npartial_support = Handler.partial_support\nps_description = Handler.ps_description\nproperty_register = Handler.property_register\n'"
test/backend/__init__.py,0,b''
test/backend/test_dynamic_shape.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\nimport unittest\n\nfrom onnx_tf.backend import onnx_graph_to_tensorflow_rep\nfrom onnx_tf.common.legacy import legacy_opset_pre_ver\nfrom onnx_tf.common.pooling_helper import py_pool\nfrom onnx import defs\nfrom onnx import helper\nfrom onnx import TensorProto\n\n\nclass TestDynamicShape(unittest.TestCase):\n  """""" Tests for dynamic shape support\n  """"""\n\n  def _get_rnd_float32(self, low=-1.0, high=1.0, shape=None):\n    output = np.random.uniform(low, high, shape)\n    if shape == None:\n      return np.float32(output)\n    else:\n      return output.astype(np.float32)\n\n  def _get_rnd_int(self, low, high=None, shape=None, dtype=np.int32):\n    return np.random.randint(low, high, size=shape, dtype=dtype)\n\n  def test_arg_max(self):\n    if legacy_opset_pre_ver(12):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support select_last_index attribute for ArgMax that depends on shape."".format(\n              defs.onnx_opset_version()))\n    axis = 1\n    node_def = helper.make_node(""ArgMax"",\n                                inputs=[\'X\'],\n                                outputs=[\'Y\'],\n                                axis=axis,\n                                keepdims=0,\n                                select_last_index=1)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [None, None])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [None, None])\n        ])\n    x = np.array([[ 1, 2, 3, 5, 3, 4, 5, 1 ], [ 2, 9, 3, 5, 9, 4, 5, 1 ]])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""X"": x})\n    expected_output = np.argmax(np.flip(x, axis), axis=axis)\n    expected_output = x.shape[axis] - expected_output - 1\n    np.testing.assert_almost_equal(output[\'Y\'], expected_output)\n\n  def test_arg_min(self):\n    if legacy_opset_pre_ver(12):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support select_last_index attribute for ArgMin that depends on shape."".format(\n              defs.onnx_opset_version()))\n    axis = 1\n    node_def = helper.make_node(""ArgMin"",\n                                inputs=[\'X\'],\n                                outputs=[\'Y\'],\n                                axis=axis,\n                                keepdims=0,\n                                select_last_index=1)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [None, None])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [None, None])\n        ])\n    x = np.array([[ 1, 2, 3, 5, 3, 4, 5, 1 ], [ 2, 7, 3, 5, 2, 4, 5, 6 ]])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""X"": x})\n    expected_output = np.argmin(np.flip(x, axis), axis=axis)\n    expected_output = x.shape[axis] - expected_output - 1\n    np.testing.assert_almost_equal(output[\'Y\'], expected_output)\n\n  def test_compress(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support Compress."".format(\n              defs.onnx_opset_version()))\n    axis = 1\n    node_def = helper.make_node(""Compress"",\n                                inputs=[\'X\', \'condition\'],\n                                outputs=[\'Y\'],\n                                axis=axis)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                          [None, None, None]),\n            helper.make_tensor_value_info(""condition"", TensorProto.BOOL, [None])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Y"", TensorProto.FLOAT,\n                                          [None, None, None])\n        ])\n    x = self._get_rnd_float32(shape=[5, 5, 5])\n    cond = np.array([1, 0, 1])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""X"": x, ""condition"": cond})\n    np.testing.assert_almost_equal(output[\'Y\'], np.compress(cond, x, axis=axis))\n\n  def test_eye_like(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support EyeLike."".format(\n          defs.onnx_opset_version()))\n    shape = [6, 10]\n    off_diagonal_offset = -3\n    x = self._get_rnd_int(0, 100, shape=shape)\n    y = np.eye(shape[0], shape[1], k=off_diagonal_offset, dtype=np.float32)\n    node_def = helper.make_node(""EyeLike"", [""x""], [""y""],\n                                dtype=TensorProto.FLOAT,\n                                k=off_diagonal_offset)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""x"", TensorProto.INT32, [None, None])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""y"", TensorProto.FLOAT, [None, None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""x"": x})\n    np.testing.assert_equal(output[""y""], y)\n\n  def test_flatten(self):\n    shape = [2, 3, 4]\n    x = self._get_rnd_float32(shape=shape)\n    axis = 1\n    node_def = helper.make_node(""Flatten"", [""X""], [""Y""], axis=axis)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                          [None, None, None])\n        ],\n        outputs=[helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [None])])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""X"": x})\n    new_shape = (np.prod(shape[0:axis]).astype(int), -1)\n    np.testing.assert_almost_equal(output[""Y""], np.reshape(x, new_shape))\n\n  def test_gather_nd(self):\n    if legacy_opset_pre_ver(11):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support GatherND."".format(\n              defs.onnx_opset_version()))\n    # valid positive and negative indices for elements\n    data = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int32)\n    indices = np.array([[0, 0], [1, -3]], dtype=np.int64)\n    ref_output = np.array([1, 4], dtype=np.int32)\n    node_def = helper.make_node(""GatherND"", [""data"", ""indices""], [""outputs""])\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""data"", TensorProto.INT32,\n                                          [None, None]),\n            helper.make_tensor_value_info(""indices"", TensorProto.INT64,\n                                          [None, None])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""outputs"", TensorProto.INT32, [None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""data"": data, ""indices"": indices})\n    np.testing.assert_almost_equal(output[""outputs""], ref_output)\n\n  def test_is_inf(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support IsInf."".format(\n          defs.onnx_opset_version()))\n    inp = np.array([-1.2, np.nan, np.inf, 2.8, np.NINF, np.inf],\n                   dtype=np.float32)\n    expected_output = np.isinf(inp)\n    node_def = helper.make_node(""IsInf"", [""X""], [""Y""])\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [None]),\n        ],\n        outputs=[helper.make_tensor_value_info(""Y"", TensorProto.BOOL, [None])])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""X"": inp})\n    np.testing.assert_equal(output[""Y""], expected_output)\n\n  def test_matmul_integer(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support MatMulInteger."".format(\n              defs.onnx_opset_version()))\n\n    node_def = helper.make_node(""MatMulInteger"",\n                                [""A"", ""B"", ""a_zero_point"", ""b_zero_point""],\n                                [""Z""])\n    # A & B are 3-D tensor and a_zero_point & b_zero_point are scalar\n    A = self._get_rnd_int(-20, 20, shape=(2, 3, 4), dtype=np.int8)\n    B = self._get_rnd_int(-20, 20, shape=(2, 4, 6), dtype=np.int8)\n    a_zero_point = self._get_rnd_int(-20, 20, dtype=np.int8)\n    b_zero_point = self._get_rnd_int(-20, 20, dtype=np.int8)\n    A_minus_zero_point = np.subtract(A.astype(np.int32),\n                                     a_zero_point.astype(np.int32))\n    B_minus_zero_point = np.subtract(B.astype(np.int32),\n                                     b_zero_point.astype(np.int32))\n    z = np.matmul(A_minus_zero_point, B_minus_zero_point)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""A"", TensorProto.INT8,\n                                          [None, None, None]),\n            helper.make_tensor_value_info(""B"", TensorProto.INT8,\n                                          [None, None, None]),\n            helper.make_tensor_value_info(""a_zero_point"", TensorProto.INT8, []),\n            helper.make_tensor_value_info(""b_zero_point"", TensorProto.INT8, [])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Z"", TensorProto.INT32,\n                                          [None, None, None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({\n        ""A"": A,\n        ""B"": B,\n        ""a_zero_point"": a_zero_point,\n        ""b_zero_point"": b_zero_point\n    })\n    np.testing.assert_almost_equal(output[""Z""], z)\n    # A & B are 4-D tensor and a_zero_point & b_zero_point are 1-D tensor\n    A = self._get_rnd_int(-20, 20, shape=(2, 5, 3, 4), dtype=np.int8)\n    B = self._get_rnd_int(-20, 20, shape=(2, 1, 4, 6), dtype=np.int8)\n    a_zero_point = self._get_rnd_int(-20,\n                                     20,\n                                     shape=(A.shape[-2]),\n                                     dtype=np.int8)\n    b_zero_point = self._get_rnd_int(-20,\n                                     20,\n                                     shape=(B.shape[-1]),\n                                     dtype=np.int8)\n    a_zero_point_with_reshape = np.reshape(a_zero_point, [A.shape[-2], 1])\n    A_minus_zero_point = np.subtract(A.astype(np.int32),\n                                     a_zero_point_with_reshape.astype(np.int32))\n    B_minus_zero_point = np.subtract(B.astype(np.int32),\n                                     b_zero_point.astype(np.int32))\n    z = np.matmul(A_minus_zero_point, B_minus_zero_point)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""A"", TensorProto.INT8,\n                                          [None, None, None, None]),\n            helper.make_tensor_value_info(""B"", TensorProto.INT8,\n                                          [None, None, None, None]),\n            helper.make_tensor_value_info(""a_zero_point"", TensorProto.INT8,\n                                          [None]),\n            helper.make_tensor_value_info(""b_zero_point"", TensorProto.INT8,\n                                          [None])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Z"", TensorProto.INT32,\n                                          [None, None, None, None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({\n        ""A"": A,\n        ""B"": B,\n        ""a_zero_point"": a_zero_point,\n        ""b_zero_point"": b_zero_point\n    })\n    np.testing.assert_almost_equal(output[""Z""], z)\n\n  def test_non_max_suppression(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support NonMaxSuppression."".format(\n              defs.onnx_opset_version()))\n    boxes = np.array([[[0.0, 0.0, 1.0, 1.0], [0.0, 0.1, 1.0, 1.1],\n                       [0.0, -0.1, 1.0, 0.9], [0.0, 10.0, 1.0, 11.0],\n                       [0.0, 10.1, 1.0, 11.1], [0.0, 100.0, 1.0, 101.0]],\n                      [[0.0, 0.0, 1.0, 1.0], [0.0, 0.1, 1.0, 1.1],\n                       [0.0, -0.1, 1.0, 0.9], [0.0, 10.0, 1.0, 11.0],\n                       [0.0, 10.1, 1.0, 11.1], [0.0, 100.0, 1.0,\n                                                101.0]]]).astype(np.float32)\n    scores = np.array([[[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]],\n                       [[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]]]).astype(np.float32)\n    max_output_boxes_per_class = np.array([2]).astype(np.int64)\n    iou_threshold = np.array([0.5]).astype(np.float32)\n    score_threshold = np.array([0.0]).astype(np.float32)\n    selected_indices = np.array([[0, 0, 3], [0, 0, 0], [1, 0, 3],\n                                 [1, 0, 0]]).astype(np.int64)\n    node_def = helper.make_node(""NonMaxSuppression"", [\n        ""boxes"", ""scores"", ""max_output_boxes_per_class"", ""iou_threshold"",\n        ""score_threshold""\n    ], [""selected_indices""],\n                                center_point_box=0)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""boxes"", TensorProto.FLOAT,\n                                          [None, None, None]),\n            helper.make_tensor_value_info(""scores"", TensorProto.FLOAT,\n                                          [None, None, None]),\n            helper.make_tensor_value_info(""max_output_boxes_per_class"",\n                                          TensorProto.INT64, [None]),\n            helper.make_tensor_value_info(""iou_threshold"", TensorProto.FLOAT,\n                                          [None]),\n            helper.make_tensor_value_info(""score_threshold"", TensorProto.FLOAT,\n                                          [None])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""selected_indices"", TensorProto.INT64,\n                                          [None, None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({\n        ""boxes"": boxes,\n        ""scores"": scores,\n        ""max_output_boxes_per_class"": max_output_boxes_per_class,\n        ""iou_threshold"": iou_threshold,\n        ""score_threshold"": score_threshold\n    })\n    np.testing.assert_almost_equal(output[""selected_indices""], selected_indices)\n\n  def test_scatter_nd(self):\n    if legacy_opset_pre_ver(11):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support ScatterND."".format(\n              defs.onnx_opset_version()))\n    # valid positive and negative indices for slices\n    data = np.reshape(np.arange(1, 25, dtype=np.float32), [2, 3, 4])\n    indices = np.array([[-1]], dtype=np.int64)\n    updates = np.array([[[43, 44, 45, 46], [47, 48, 49, 50], [51, 52, 53, 54]]],\n                       dtype=np.float32)\n    ref_output = np.array(\n        [[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n         [[43, 44, 45, 46], [47, 48, 49, 50], [51, 52, 53, 54]]],\n        dtype=np.float32)\n    node_def = helper.make_node(""ScatterND"", [""data"", ""indices"", ""updates""],\n                                [""outputs""])\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""data"", TensorProto.FLOAT,\n                                          [None, None, None]),\n            helper.make_tensor_value_info(""indices"", TensorProto.INT64,\n                                          [None, None]),\n            helper.make_tensor_value_info(""updates"", TensorProto.FLOAT,\n                                          [None, None, None])\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""outputs"", TensorProto.FLOAT,\n                                          [None, None, None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""data"": data, ""indices"": indices, ""updates"": updates})\n    np.testing.assert_almost_equal(output[""outputs""], ref_output)\n\n  def test_max_pool_2d_dilations_ceil_pads(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations nor ceil mode."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    dilations = [3, 3]\n    pads = [1, 1, 2, 2]\n    ceil_mode = 1\n\n    input_shape = [10, 3, 23, 23]\n    x = self._get_rnd_float32(shape=input_shape)\n\n    test_output = py_pool(x,\n                          kernel_shape=kernel_shape,\n                          strides=strides,\n                          dilations=dilations,\n                          padding=pads,\n                          ceil_mode=ceil_mode,\n                          pooling_type=""MAX"",\n                          include_indices=False)\n\n    node_def = helper.make_node(op_type=""MaxPool"",\n                                inputs=[""X""],\n                                outputs=[""Y""],\n                                kernel_shape=kernel_shape,\n                                strides=strides,\n                                dilations=dilations,\n                                pads=pads,\n                                ceil_mode=ceil_mode)\n\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                          [None, None, None, None]),\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Y"", TensorProto.FLOAT,\n                                          [None, None, None, None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""X"": x})\n\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_average_pool_2d(self):\n    kernel_shape = [1, 2]\n    strides = [1, 2]\n\n    input_shape = [10, 10, 4, 4]\n    x = self._get_rnd_float32(shape=input_shape)\n\n    test_output = py_pool(x,\n                          kernel_shape=kernel_shape,\n                          strides=strides,\n                          pooling_type=""AVG"",\n                          include_indices=False)\n\n    node_def = helper.make_node(op_type=""AveragePool"",\n                                inputs=[""X""],\n                                outputs=[""Y""],\n                                kernel_shape=kernel_shape,\n                                strides=strides)\n\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                          [None, None, None, None]),\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Y"", TensorProto.FLOAT,\n                                          [None, None, None, None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output = tf_rep.run({""X"": x})\n\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_max_unpool(self):\n    input_shape = [10, 3, 24, 24]\n    x = self._get_rnd_float32(shape=input_shape)\n\n    kernel_shape = [2, 2]\n    strides = [2, 2]\n\n    maxpool_node_def = helper.make_node(\n            op_type=""MaxPool"",\n            inputs=[""X""],\n            outputs=[""Pool"", ""Indices""],\n            kernel_shape=kernel_shape,\n            strides=strides)\n\n    maxunpool_node_def = helper.make_node(\n        ""MaxUnpool"", [""Pool"", ""Indices""], [""Y""],\n        kernel_shape=kernel_shape,\n        strides=strides)\n\n    graph_def = helper.make_graph(\n        [maxpool_node_def,maxunpool_node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                          [None, None, None, None]),\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Y"", TensorProto.FLOAT,\n                                          [None, None, None, None])\n        ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    output_unpool = tf_rep.run({""X"": x})\n\n    test_output = np.zeros(input_shape)\n    for i1 in range(0, input_shape[0]):\n      for i2 in range(0, input_shape[1]):\n        for i3 in range(0, input_shape[2], 2):\n          for i4 in range(0, input_shape[3], 2):\n            max_val = float(\'-inf\')\n            for j1 in range(i3,i3+2):\n              for j2 in range(i4,i4+2):\n                if x[i1][i2][j1][j2] > max_val:\n                  max_val = x[i1][i2][j1][j2]\n                  max_ind = (j1, j2)\n            j1, j2 = max_ind\n            test_output[i1][i2][j1][j2] = max_val\n    np.testing.assert_almost_equal(output_unpool[""Y""], test_output)\n\n  def test_slice(self):\n    # test case 1 with normal inputs\n    axes = [0, 1, 2]\n    starts = [0, 0, 0]\n    ends = [2, 2, 2]\n\n    if legacy_opset_pre_ver(10):\n      node_def = helper.make_node(""Slice"", [""X""], [""S""],\n                                  axes=axes,\n                                  starts=starts,\n                                  ends=ends)\n      graph_def = helper.make_graph(\n          [node_def],\n          name=""test_unknown_shape"",\n          inputs=[\n              helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                            [None, None, None])\n          ],\n          outputs=[\n              helper.make_tensor_value_info(""S"", TensorProto.FLOAT,\n                                            [None, None, None])\n          ])\n    else:\n      node_def = helper.make_node(""Slice"", [""X"", ""starts"", ""ends"", ""axes""],\n                                  [""S""])\n      graph_def = helper.make_graph(\n          [node_def],\n          name=""test_unknown_shape"",\n          inputs=[\n              helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                            [None, None, None]),\n              helper.make_tensor_value_info(""starts"", TensorProto.INT32,\n                                            [None]),\n              helper.make_tensor_value_info(""ends"", TensorProto.INT32, [None]),\n              helper.make_tensor_value_info(""axes"", TensorProto.INT32, [None]),\n          ],\n          outputs=[\n              helper.make_tensor_value_info(""S"", TensorProto.FLOAT,\n                                            [None, None, None])\n          ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n\n    if legacy_opset_pre_ver(10):\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = tf_rep.run({""X"": x})\n      np.testing.assert_almost_equal(output[""S""], x[0:2, 0:2, 0:2])\n    else:\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = tf_rep.run({\n          ""X"": x,\n          ""starts"": starts,\n          ""ends"": ends,\n          ""axes"": axes\n      })\n      np.testing.assert_almost_equal(output[""S""], x[0:2, 0:2, 0:2])\n\n    # test case 2 with negative, out-of-bound and default inputs\n    axes = [0, 2]\n    starts = [0, -7]\n    ends = [-8, 20]\n    steps = [1, 1]\n\n    if legacy_opset_pre_ver(10):\n      node_def = helper.make_node(""Slice"", [""X""], [""S""],\n                                  axes=axes,\n                                  starts=starts,\n                                  ends=ends)\n      graph_def = helper.make_graph(\n          [node_def],\n          name=""test_unknown_shape"",\n          inputs=[\n              helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                            [None, None, None])\n          ],\n          outputs=[\n              helper.make_tensor_value_info(""S"", TensorProto.FLOAT,\n                                            [None, None, None])\n          ])\n    else:\n      node_def = helper.make_node(""Slice"",\n                                  [""X"", ""starts"", ""ends"", ""axes"", ""steps""],\n                                  [""S""])\n      graph_def = helper.make_graph(\n          [node_def],\n          name=""test_unknown_shape"",\n          inputs=[\n              helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                            [None, None, None]),\n              helper.make_tensor_value_info(""starts"", TensorProto.INT32,\n                                            [None]),\n              helper.make_tensor_value_info(""ends"", TensorProto.INT32, [None]),\n              helper.make_tensor_value_info(""axes"", TensorProto.INT32, [None]),\n              helper.make_tensor_value_info(""steps"", TensorProto.INT32, [None]),\n          ],\n          outputs=[\n              helper.make_tensor_value_info(""S"", TensorProto.FLOAT,\n                                            [None, None, None])\n          ])\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    if legacy_opset_pre_ver(10):\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = tf_rep.run({""X"": x})\n      np.testing.assert_almost_equal(output[""S""], x[0:-8, :, -7:20])\n    else:\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = tf_rep.run({\n          ""X"": x,\n          ""starts"": starts,\n          ""ends"": ends,\n          ""axes"": axes,\n          ""steps"": steps\n      })\n      np.testing.assert_almost_equal(output[""S""], x[0:-8, :, -7:20])\n\n    # test case 3 with non-default steps\n    axes = [0, 1, 2]\n    starts = [0, 0, 0]\n    ends = [2, 2, 2]\n    steps = [2, -2, -1]\n\n    if not legacy_opset_pre_ver(10):\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = tf_rep.run({\n          ""X"": x,\n          ""starts"": starts,\n          ""ends"": ends,\n          ""axes"": axes,\n          ""steps"": steps\n      })\n      np.testing.assert_almost_equal(output[""S""], x[0:2:2, 0:2:-2, 0:2:-1])\n\n  def test_split(self):\n    shape = [12, 12]\n    axis = 0\n    output_count = 3\n    node_def = helper.make_node(""Split"", [""X""],\n                                [""Z%i"" % i for i in range(output_count)],\n                                axis=axis)\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test_unknown_shape"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT,\n                                          [None] * len(shape))\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""Z%i"" % i, TensorProto.FLOAT,\n                                          [None] * len(shape))\n            for i in range(output_count)\n        ])\n\n    tf_rep = onnx_graph_to_tensorflow_rep(graph_def)\n    x = self._get_rnd_float32(shape=shape)\n    output = tf_rep.run({""X"": x})\n\n    per_part = shape[axis] // output_count\n    split = [per_part] * output_count\n    for a, b in zip(list(output), np.split(x, np.cumsum(split))[:-1]):\n      np.testing.assert_almost_equal(a, b)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
test/backend/test_model.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport unittest\n\nimport numpy as np\nimport onnx\nfrom onnx_tf.backend import prepare\nfrom onnx import helper\nfrom onnx import TensorProto\n\nfrom onnx_tf.common.legacy import legacy_onnx_pre_ver\n\n\nclass TestModel(unittest.TestCase):\n  """""" Tests for models\n  """"""\n\n  def _get_rnd(self, shape, low=-1.0, high=1.0):\n    return np.random.uniform(low, high, np.prod(shape)) \\\n                      .reshape(shape) \\\n                      .astype(np.float32)\n\n  def test_sequence_ops(self):\n    # test SequenceConstruct and SequenceAt\n    a = np.random.randn(2, 1, 2).astype(np.float32)\n    b = np.random.randn(1, 1, 2).astype(np.float32)\n    c = np.random.randn(3, 1, 2).astype(np.float32)\n    seq_construct_node = helper.make_node(\'SequenceConstruct\', [\'a\', \'b\', \'c\'], [\'S\'])\n    seq_at_node = helper.make_node(\'SequenceAt\', [\'S\',\'at\'], [\'Y\'])\n    out_value_info = helper.make_tensor_value_info(\'Y\',onnx.TensorProto.FLOAT,[None])\n    a_value_info =  helper.make_tensor_value_info(\'a\',onnx.TensorProto.FLOAT,[2, 1, 2])\n    b_value_info =  helper.make_tensor_value_info(\'b\',onnx.TensorProto.FLOAT,[1, 1, 2])\n    c_value_info =  helper.make_tensor_value_info(\'c\',onnx.TensorProto.FLOAT,[3, 1, 2])\n    at_value_info =  helper.make_tensor_value_info(\'at\',onnx.TensorProto.INT32,[])\n\n    graph = helper.make_graph([seq_construct_node, seq_at_node],\n            name=\'seq_construct_at_test\',\n            inputs=[a_value_info, b_value_info, c_value_info, at_value_info],\n            outputs=[out_value_info])\n    model = helper.make_model(graph, producer_name=\'backend-test\')\n    tf_rep = prepare(model)\n    output = tf_rep.run({\'a\':a, \'b\':b, \'c\':c, \'at\':0})\n    np.testing.assert_almost_equal(output[""Y""], a)\n    output = tf_rep.run({\'a\':a, \'b\':b, \'c\':c, \'at\':-2})\n    np.testing.assert_almost_equal(output[""Y""], b)\n    output = tf_rep.run({\'a\':a, \'b\':b, \'c\':c, \'at\':2})\n    np.testing.assert_almost_equal(output[""Y""], c)\n\n    # test SequenceEmpty, SequenceInsert, and SequenceAt\n    p = np.int32(0)\n    seq_empty_node = helper.make_node(\'SequenceEmpty\', [], [\'S\'])\n    seq_insert_node1 = helper.make_node(\'SequenceInsert\', [\'S\',\'a\'], [\'S1\'])\n    seq_insert_node2 = helper.make_node(\'SequenceInsert\', [\'S1\',\'b\'], [\'S2\'])\n    seq_insert_node3 = helper.make_node(\'SequenceInsert\', [\'S2\',\'c\',\'p\'], [\'S3\'])\n    seq_at_node = helper.make_node(\'SequenceAt\', [\'S3\',\'at\'], [\'Y\'])\n\n    p_value_info = helper.make_tensor_value_info(\'p\',onnx.TensorProto.INT32,[])\n\n    graph = helper.make_graph([seq_empty_node, seq_insert_node1, seq_insert_node2, seq_insert_node3, seq_at_node],\n            name=\'seq_empty_insert_at_test\',\n            inputs=[a_value_info, b_value_info, c_value_info, p_value_info, at_value_info],\n            outputs=[out_value_info])\n    model = helper.make_model(graph, producer_name=\'backend-test\')\n    tf_rep = prepare(model)\n    output = tf_rep.run({\'a\':a, \'b\':b, \'c\':c, \'p\':p, \'at\':0})\n    np.testing.assert_almost_equal(output[""Y""], c)\n\n    # test SequenceConstruct, SequenceErase, and SequenceLength\n    seq_construct_node = helper.make_node(\'SequenceConstruct\', [\'a\', \'b\', \'c\'], [\'S\'])\n    seq_erase_node = helper.make_node(\'SequenceErase\', [\'S\',\'p\'], [\'S1\'])\n    seq_length_node = helper.make_node(\'SequenceLength\', [\'S1\'], [\'Y\'])\n\n    graph = helper.make_graph([seq_construct_node, seq_erase_node, seq_length_node],\n            name=\'seq_construct_erase_length_test\',\n            inputs=[a_value_info, b_value_info, c_value_info, p_value_info],\n            outputs=[out_value_info])\n    model = helper.make_model(graph, producer_name=\'backend-test\')\n    tf_rep = prepare(model)\n    output = tf_rep.run({\'a\':a, \'b\':b, \'c\':c, \'p\':p})\n    np.testing.assert_almost_equal(output[""Y""], 2)\n\n    # test SequenceConstruct and SequenceErase\n    seq_construct_node = helper.make_node(\'SequenceConstruct\', [\'a\', \'b\', \'c\'], [\'S\'])\n    seq_erase_node = helper.make_node(\'SequenceErase\', [\'S\',\'p\'], [\'S1\'])\n    seq_at_node = helper.make_node(\'SequenceAt\', [\'S1\', \'at\'], [\'Y\'])\n\n    graph = helper.make_graph([seq_construct_node, seq_erase_node, seq_at_node],\n            name=\'seq_construct_erase_test\',\n            inputs=[a_value_info, b_value_info, c_value_info, p_value_info, at_value_info],\n            outputs=[out_value_info])\n    model = helper.make_model(graph, producer_name=\'backend-test\')\n    tf_rep = prepare(model)\n    output = tf_rep.run({\'a\':a, \'b\':b, \'c\':c, \'p\':p, \'at\':0})\n    np.testing.assert_almost_equal(output[""Y""], b)\n    output = tf_rep.run({\'a\':a, \'b\':b, \'c\':c, \'p\':p, \'at\':1})\n    np.testing.assert_almost_equal(output[""Y""], c)\n\n  def test_relu_node_inplace(self):\n    X = np.random.randn(3, 2).astype(np.float32)\n    Y_ref = np.clip(X, 0, np.inf)\n\n    node_def = helper.make_node(""Relu"", [""X""], [""X1""])\n\n    graph_def = helper.make_graph(\n        [node_def],\n        name=""test"",\n        inputs=[helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [3, 2])],\n        outputs=[\n            helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, [3, 2])\n        ])\n    tf_rep = prepare(helper.make_model(graph_def))\n    output = tf_rep.run({""X"": X})\n    np.testing.assert_almost_equal(output.X1, Y_ref)\n\n  def test_initializer(self):\n    if legacy_onnx_pre_ver(1, 2):\n      raise unittest.SkipTest(\n          ""The current version of ONNX does not record correctly the opset of Cast.""\n      )\n    X = np.array([[1, 2], [3, 4]]).astype(np.float32)\n    Y = np.array([[1, 2], [3, 4]]).astype(np.float32)\n    weight = np.array([[1, 0], [0, 1]])\n    graph_def = helper.make_graph(\n        [\n            helper.make_node(""Add"", [""X"", ""Y""], [""Z0""]),\n            helper.make_node(""Cast"", [""Z0""], [""Z""], to=TensorProto.FLOAT),\n            helper.make_node(""Mul"", [""Z"", ""weight""], [""W""]),\n            helper.make_node(""Tanh"", [""W""], [""W1""]),\n            helper.make_node(""Sigmoid"", [""W1""], [""W2""])\n        ],\n        name=""test_initializer"",\n        inputs=[\n            helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 2)),\n            helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 2)),\n            helper.make_tensor_value_info(""weight"", TensorProto.FLOAT, (2, 2)),\n        ],\n        outputs=[\n            helper.make_tensor_value_info(""W2"", TensorProto.FLOAT, (2, 2))\n        ],\n        initializer=[\n            helper.make_tensor(""weight"", TensorProto.FLOAT, [2, 2],\n                               weight.flatten().astype(float))\n        ])\n\n    def sigmoid(x):\n      return 1 / (1 + np.exp(-x))\n\n    W_ref = sigmoid(np.tanh((X + Y) * weight))\n    tf_rep = prepare(helper.make_model(graph_def))\n    output = tf_rep.run({""X"": X, ""Y"": Y})\n    np.testing.assert_almost_equal(output[""W2""], W_ref)\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
test/backend/test_node.py,33,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport math\nimport unittest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework.errors_impl import InvalidArgumentError\nfrom onnx_tf.backend import onnx_graph_to_tensorflow_rep\nfrom onnx_tf.backend import run_node\nfrom onnx_tf.common import supports_device\nfrom onnx_tf.common.legacy import legacy_onnx_pre_ver, legacy_opset_pre_ver\nfrom onnx_tf.common.pooling_helper import py_pool\nfrom onnx import helper\nfrom onnx import TensorProto\nfrom onnx import defs\n\n\nclass TestNode(unittest.TestCase):\n  """""" Tests for nodes\n  """"""\n\n  def _get_rnd_float32(self, low=-1.0, high=1.0, shape=None):\n    output = np.random.uniform(low, high, shape)\n    if shape == None:\n      return np.float32(output)\n    else:\n      return output.astype(np.float32)\n\n  def _get_rnd_int(self, low, high=None, shape=None, dtype=np.int32):\n    return np.random.randint(low, high, size=shape, dtype=dtype)\n\n  def _elu(self, x):\n    # f(x) = alpha * (exp(x) - 1.) for x < 0,\n    # f(x) = x for x >= 0\n    if x < 0.:\n      return np.expm1(x)\n    return x\n\n  def _leaky_relu(self, x, alpha):\n    # f(x) = alpha * x for x < 0,\n    # f(x) = x for x >= 0\n    if x < 0.:\n      return alpha * x\n    return x\n\n  def test_abs(self):\n    node_def = helper.make_node(""Abs"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.abs(x))\n\n  def test_acosh(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Acosh."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Acosh"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.arccosh(x))\n\n  def test_add(self):\n    node_def = helper.make_node(""Add"", [""X"", ""Y""], [""Z""])\n    x = self._get_rnd_float32(shape=[5, 10, 5, 5])\n    y = self._get_rnd_float32(shape=[10, 1, 1])\n    output = run_node(node_def, [x, y])\n    np.testing.assert_almost_equal(output[""Z""],\n                                   np.add(x, y.reshape([1, 10, 1, 1])))\n\n    # node_def = helper.make_node(""Add"", [""A"", ""B""], [""C""], broadcast=1)\n    # a = self._get_rnd([10, 10])\n    # b = self._get_rnd([10, 10])\n    # output = run_node(node_def, [a, b])\n    # np.testing.assert_almost_equal(output[""C""], np.add(a, b))\n\n    # node_def = helper.make_node(""Add"", [""A"", ""B""], [""C""], broadcast=1)\n    # a = self._get_rnd([10, 10])\n    # b = self._get_rnd([10,])\n    # output = run_node(node_def, [a, b])\n    # np.testing.assert_almost_equal(output[""C""], np.add(a, b))\n\n  def test_arg_max(self):\n    for axis in [0, 1]:\n      node_def = helper.make_node(""ArgMax"", [""data""], [""reduced""],\n                                  axis=axis,\n                                  keepdims=0)\n      data = self._get_rnd_float32(shape=[10, 10])\n      output = run_node(node_def, [data])\n      np.testing.assert_almost_equal(output[""reduced""],\n                                     np.argmax(data, axis=axis))\n\n      # test select_last_index\n      if not legacy_opset_pre_ver(12):\n        # select_last_index = 0\n        node_def = helper.make_node(""ArgMax"", [""data""], [""reduced""],\n                                    axis=axis,\n                                    keepdims=0,\n                                    select_last_index=0)\n        data = self._get_rnd_float32(shape=[10, 10])\n        output = run_node(node_def, [data])\n        np.testing.assert_almost_equal(output[""reduced""],\n                                       np.argmax(data, axis=axis))\n        # select_last_index = 1\n        node_def = helper.make_node(""ArgMax"", [""data""], [""reduced""],\n                                    axis=axis,\n                                    keepdims=0,\n                                    select_last_index=1)\n        data = np.array([[1, 2, 3, 5, 3, 4, 5, 1], [2, 9, 3, 5, 9, 4, 5, 1]])\n        output = run_node(node_def, [data])\n        data = np.flip(data, axis)\n        result = np.argmax(data, axis=axis)\n        result = data.shape[axis] - result - 1\n        np.testing.assert_almost_equal(output[""reduced""], result)\n\n  def test_arg_min(self):\n    for axis in [0, 1]:\n      node_def = helper.make_node(""ArgMin"", [""data""], [""reduced""],\n                                  axis=axis,\n                                  keepdims=0)\n      data = self._get_rnd_float32(shape=[10, 10])\n      output = run_node(node_def, [data])\n      np.testing.assert_almost_equal(output[""reduced""],\n                                     np.argmin(data, axis=axis))\n\n      # test select_last_index\n      if not legacy_opset_pre_ver(12):\n        # select_last_index = 0\n        node_def = helper.make_node(""ArgMin"", [""data""], [""reduced""],\n                                    axis=axis,\n                                    keepdims=0,\n                                    select_last_index=0)\n        data = self._get_rnd_float32(shape=[10, 10])\n        output = run_node(node_def, [data])\n        np.testing.assert_almost_equal(output[""reduced""],\n                                       np.argmin(data, axis=axis))\n        # select_last_index = 1\n        node_def = helper.make_node(""ArgMin"", [""data""], [""reduced""],\n                                    axis=axis,\n                                    keepdims=0,\n                                    select_last_index=1)\n        data = np.array([[1, 2, 3, 5, 3, 4, 5, 1], [2, 7, 3, 5, 2, 4, 5, 6]])\n        output = run_node(node_def, [data])\n        data = np.flip(data, axis)\n        result = np.argmin(data, axis=axis)\n        result = data.shape[axis] - result - 1\n        np.testing.assert_almost_equal(output[""reduced""], result)\n\n  def test_asinh(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Asinh."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Asinh"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.arcsinh(x))\n\n  def test_atanh(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Atanh."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Atanh"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.arctanh(x))\n\n  def _batch_normalization(self, x, mean, variance, bias, scale,\n                           variance_epsilon):\n    inv = np.reciprocal(np.sqrt(variance + variance_epsilon))\n    if scale is not None:\n      inv *= scale\n    return x * inv + (bias - mean * inv if bias is not None else -mean * inv)\n\n  def test_batch_normalization(self):\n    if legacy_opset_pre_ver(6):\n      raise unittest.SkipTest(""Backend doesn\'t support consumed flag"")\n    node_def = helper.make_node(""BatchNormalization"",\n                                [""X"", ""scale"", ""bias"", ""mean"", ""var""], [""Y""],\n                                epsilon=0.001)\n    x_shape = [3, 5, 4, 2]\n    param_shape = [5]\n    _param_shape = [1, 5, 1, 1]\n    x = self._get_rnd_float32(0, 1, shape=x_shape)\n    m = self._get_rnd_float32(0, 1, shape=param_shape)\n    _m = m.reshape(_param_shape)\n    v = self._get_rnd_float32(0, 1, shape=param_shape)\n    _v = v.reshape(_param_shape)\n    scale = self._get_rnd_float32(0, 1, shape=param_shape)\n    _scale = scale.reshape(_param_shape)\n    bias = self._get_rnd_float32(0, 1, shape=param_shape)\n    _bias = bias.reshape(_param_shape)\n    golden = self._batch_normalization(x, _m, _v, _bias, _scale, 0.001)\n    output = run_node(node_def, [x, scale, bias, m, v])\n    np.testing.assert_almost_equal(output[""Y""], golden, decimal=5)\n\n  def test_cast(self):\n    if legacy_onnx_pre_ver(1, 2) or legacy_opset_pre_ver(6):\n      test_cases = [(""FLOAT"", tf.float32), (""UINT8"", tf.uint8),\n                    (""INT8"", tf.int8),\n                    (""UINT16"", tf.uint16), (""INT16"", tf.int16),\n                    (""INT32"", tf.int32), (""INT64"", tf.int64), (""BOOL"", tf.bool),\n                    (""FLOAT16"", tf.float16), (""DOUBLE"", tf.float64),\n                    (""COMPLEX64"", tf.complex64), (""COMPLEX128"", tf.complex128)]\n    else:\n      test_cases = [(TensorProto.FLOAT, tf.float32),\n                    (TensorProto.UINT8, tf.uint8), (TensorProto.INT8, tf.int8),\n                    (TensorProto.UINT16, tf.uint16),\n                    (TensorProto.INT16, tf.int16),\n                    (TensorProto.INT32, tf.int32),\n                    (TensorProto.INT64, tf.int64), (TensorProto.BOOL, tf.bool),\n                    (TensorProto.FLOAT16, tf.float16),\n                    (TensorProto.DOUBLE, tf.float64),\n                    (TensorProto.COMPLEX64, tf.complex64),\n                    (TensorProto.COMPLEX128, tf.complex128)]\n      if not legacy_opset_pre_ver(9):\n        test_cases.append((TensorProto.STRING, tf.string))\n    for ty, tf_type in test_cases:\n      node_def = helper.make_node(""Cast"", [""input""], [""output""], to=ty)\n      vector = [2, 3]\n      output = run_node(node_def, [vector])\n      np.testing.assert_equal(output[""output""].dtype, tf_type)\n\n    if not legacy_opset_pre_ver(9):\n      test_cases2 = [(TensorProto.FLOAT, tf.float32),\n                     (TensorProto.INT32, tf.int32),\n                     (TensorProto.INT64, tf.int64),\n                     (TensorProto.DOUBLE, tf.float64)]\n      for ty, tf_type in test_cases2:\n        node_def = helper.make_node(""Cast"", [""input""], [""output""], to=ty)\n        vector = [\'2\', \'3\']\n        output = run_node(node_def, [vector])\n        np.testing.assert_equal(output[""output""].dtype, tf_type)\n\n  def test_ceil(self):\n    node_def = helper.make_node(""Ceil"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.ceil(x))\n\n  def test_compress(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support Compress."".format(\n              defs.onnx_opset_version()))\n    axis = 1\n    node_def = helper.make_node(""Compress"",\n                                inputs=[\'X\', \'condition\'],\n                                outputs=[\'Y\'],\n                                axis=axis)\n    x = self._get_rnd_float32(shape=[5, 5, 5])\n    cond = np.array([1, 0, 1])\n    output = run_node(node_def, inputs=[x, cond])\n    np.testing.assert_almost_equal(output[\'Y\'], np.compress(cond, x, axis=axis))\n\n  def test_concat(self):\n    shape = [10, 20, 5]\n    for axis in range(len(shape)):\n      node_def = helper.make_node(""Concat"", [""X1"", ""X2""], [""Y""], axis=axis)\n      x1 = self._get_rnd_float32(shape=shape)\n      x2 = self._get_rnd_float32(shape=shape)\n      output = run_node(node_def, [x1, x2])\n      np.testing.assert_almost_equal(output[""Y""], np.concatenate((x1, x2),\n                                                                 axis))\n\n  def test_constant(self):\n    shape = [16, 16]\n    values = np.random.randn(*shape).flatten().astype(float)\n    const2_onnx = helper.make_tensor(""const2"", TensorProto.DOUBLE, shape,\n                                     values)\n    node_def = helper.make_node(""Constant"", [], [""Y""], value=const2_onnx)\n    output = run_node(node_def, [])\n    np.testing.assert_equal(output[""Y""].shape, shape)\n    np.testing.assert_almost_equal(output[""Y""].flatten(), values)\n\n    # test sparse tensor\n    if not legacy_opset_pre_ver(11):\n      expected = np.array([[1, 0, 0, 0], [0, 0, 2, 0], [0, 0, 0, 0]])\n      x = np.array([[0, 0], [1, 2]]).flatten().astype(np.int64)\n      values = helper.make_tensor(""values"", TensorProto.INT32, [2], [1, 2])\n      indices = helper.make_tensor(""indices"", TensorProto.INT64, [2, 2], x)\n      a = helper.make_sparse_tensor(values, indices, [3, 4])\n      node_def = helper.make_node(""Constant"", [], [""Y""], sparse_value=a)\n      output = run_node(node_def, [])\n      b = tf.compat.v1.sparse_to_dense(output[""Y""].indices,\n                                       output[""Y""].dense_shape,\n                                       output[""Y""].values)\n      result = b.numpy()\n      np.testing.assert_equal(result, expected)\n\n    if not legacy_opset_pre_ver(12):\n      float_attr = 1.0\n      floats_attr = [1.0, 2.0, 3.0]\n      int_attr = np.int64(123)\n      ints_attr = [np.int64(4), np.int64(5), np.int64(6)]\n      string_attr = \'The Cat in the Hat\'\n      strings_attr = [\n          \'Green Eggs and Ham\', \'How the Grinch Stole Christmas!\',\n          \'The Cat in the Hat Comes Back\'\n      ]\n      testcases = [(helper.make_node(""Constant"", [], [""Y""],\n                                     value_float=float_attr), float_attr),\n                   (helper.make_node(""Constant"", [], [""Y""],\n                                     value_floats=floats_attr), floats_attr),\n                   (helper.make_node(""Constant"", [], [""Y""],\n                                     value_int=int_attr), int_attr),\n                   (helper.make_node(""Constant"", [], [""Y""],\n                                     value_ints=ints_attr), ints_attr),\n                   (helper.make_node(""Constant"", [], [""Y""],\n                                     value_string=string_attr), string_attr),\n                   (helper.make_node(""Constant"", [], [""Y""],\n                                     value_strings=strings_attr), strings_attr)]\n      for node_def, expected in testcases:\n        output = run_node(node_def, [])\n        if isinstance(expected, str):\n          np.testing.assert_string_equal(output[""Y""].decode(\'UTF-8\'), expected)\n        elif isinstance(expected, list) and isinstance(expected[0], str):\n          for i in range(len(expected)):\n            np.testing.assert_string_equal(output[\'Y\'][i].decode(\'UTF-8\'),\n                                           expected[i])\n        else:\n          np.testing.assert_equal(output[""Y""], expected)\n\n  def test_constant_fill(self):\n    if not legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support ConstantFill."".format(\n              defs.onnx_opset_version()))\n    shape = [1, 2, 3, 4]\n    extra_shape = [5, 6]\n    value = 3.\n    node_def = helper.make_node(\n        ""ConstantFill"",\n        [""X""],\n        [""Y""],\n        value=value,\n        extra_shape=extra_shape,\n        dtype=1,\n    )\n    x = self._get_rnd_float32(shape=shape)\n    y = np.zeros(shape + extra_shape)\n    y.fill(value)\n    output = run_node(node_def, [x])\n    np.testing.assert_equal(output[""Y""].dtype, tf.float32)\n    np.testing.assert_equal(output[""Y""], y)\n\n  def test_constant_of_shape(self):\n    if defs.onnx_opset_version() < 9:\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support ConstantOfShape."".format(\n              defs.onnx_opset_version()))\n    v = helper.make_tensor(""value"", TensorProto.FLOAT, [1], [1])\n    node_def = helper.make_node(""ConstantOfShape"", [""X""], [""Y""], value=v)\n    x = np.array([4, 3, 2])\n    output = run_node(node_def, inputs=[x])\n    np.testing.assert_almost_equal(output[""Y""], np.ones(x, dtype=np.float32))\n    v = helper.make_tensor(""value"", TensorProto.INT32, [1], [0])\n    node_def = helper.make_node(""ConstantOfShape"", [""X""], [""Y""], value=v)\n    x = np.array([10, 6])\n    output = run_node(node_def, inputs=[x])\n    np.testing.assert_almost_equal(output[""Y""], np.zeros(x, dtype=np.int32))\n\n  def test_conv(self):\n    device = ""CUDA"" if supports_device(""CUDA"") else ""CPU""\n\n    N, C, H, W = 4, 3, 5, 5\n    x_shape = [N, C, H, W]\n    K, kH, kW = 6, 3, 3\n    weight_shape = [K, C, kH, kW]\n    node_def = helper.make_node(""Conv"", [""X"", ""weights""], [""Y""],\n                                pads=[1, 1, 1, 1],\n                                kernel_shape=[kH, kW])\n\n    x = self._get_rnd_float32(shape=x_shape)\n    weights = self._get_rnd_float32(shape=weight_shape)\n    output = run_node(node_def, [x, weights], device=device)\n\n    out_shape = [N, K, H, W]\n    test_output = np.zeros(out_shape)\n    for n in range(N):\n      for c in range(C):\n        for h in range(H):\n          for w in range(W):\n            for k in range(K):\n              for kh in range(kH):\n                for kw in range(kW):\n                  h_in_range = (h - kH // 2 + kh) < H and (h - kH // 2 +\n                                                           kh) >= 0\n                  w_in_range = (w - kW // 2 + kw) < W and (w - kW // 2 +\n                                                           kw) >= 0\n                  if h_in_range and w_in_range:\n                    test_output[n][k][h][w] += (\n                        x[n][c][h - kH // 2 + kh][w - kW // 2 + kw] *\n                        weights[k][c][kh][kw])\n\n    np.testing.assert_almost_equal(output[""Y""], test_output, decimal=5)\n\n  def test_conv_integer(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support ConvInteger."".format(\n              defs.onnx_opset_version()))\n\n    # Test w_zero_point\n    x = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10]).astype(np.int8).reshape(\n        (1, 1, 3, 3))\n    w = np.array([2, 2, 2, 2]).astype(np.int8).reshape((1, 1, 2, 2))\n    w_zero_point = np.int8(1)\n    y = np.array([16, 20, 28, 32]).astype(np.int32).reshape((1, 1, 2, 2))\n\n    node = helper.make_node(""ConvInteger"", [""X"", ""W"", ""w_zero_point""], [""Y""],\n                            kernel_shape=[2, 2],\n                            pads=[0, 0, 0, 0],\n                            dilations=[1, 1])\n    output = run_node(node, [x, w, w_zero_point])\n    np.testing.assert_almost_equal(output[""Y""], y)\n\n    # Test x_zero_point and w_zero_point\n    x = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10]).astype(np.int8).reshape(\n        (1, 1, 3, 3))\n    x_zero_point = np.int8(1)\n    w = np.array([2, 2, 2, 2]).astype(np.int8).reshape((1, 1, 2, 2))\n    w_zero_point = np.int8(1)\n    y = np.array([12, 16, 24, 28]).astype(np.int32).reshape((1, 1, 2, 2))\n\n    node = helper.make_node(""ConvInteger"",\n                            [""X"", ""W"", ""x_zero_point"", ""w_zero_point""], [""Y""],\n                            kernel_shape=[2, 2],\n                            pads=[0, 0, 0, 0],\n                            dilations=[1, 1])\n    output = run_node(node, [x, w, x_zero_point, w_zero_point])\n    np.testing.assert_almost_equal(output[""Y""], y)\n\n    # Test w_zero_point as 1d tensor\n    x = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10]).astype(np.int8).reshape(\n        (1, 1, 3, 3))\n    w = np.array([2, 2, 2, 2]).astype(np.int8).reshape((1, 1, 2, 2))\n    w_zero_point = np.array([1]).astype(np.int8)\n    y = np.array([16, 20, 28, 32]).astype(np.int32).reshape((1, 1, 2, 2))\n\n    node = helper.make_node(""ConvInteger"", [""X"", ""W"", ""w_zero_point""], [""Y""],\n                            kernel_shape=[2, 2],\n                            pads=[0, 0, 0, 0],\n                            dilations=[1, 1])\n    output = run_node(node, [x, w, w_zero_point])\n    np.testing.assert_almost_equal(output[""Y""], y)\n\n    # Test w_zero_point as 1d tensor shape 2\n    x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int8).reshape(\n        (1, 1, 3, 3))\n    w = np.array([2, 2, 2, 2, 2, 2, 2, 2]).astype(np.int8).reshape((2, 1, 2, 2))\n    w_zero_point = np.array([1, 2]).astype(np.int8)\n    y = np.array([12, 16, 24, 28, 0, 0, 0, 0]).astype(np.int32).reshape(\n        (1, 2, 2, 2))\n\n    node = helper.make_node(""ConvInteger"", [""X"", ""W"", ""w_zero_point""], [""Y""],\n                            kernel_shape=[2, 2],\n                            pads=[0, 0, 0, 0],\n                            dilations=[1, 1])\n    output = run_node(node, [x, w, w_zero_point])\n    np.testing.assert_almost_equal(output[""Y""], y)\n\n  def test_conv_transpose(self):\n    device = ""CUDA"" if supports_device(""CUDA"") else ""CPU""\n\n    pads = [1, 1]\n    node_def = helper.make_node(""ConvTranspose"", [""X"", ""weights""], [""Y""],\n                                pads=pads)\n    x_shape = [1, 3, 4]\n    x = self._get_rnd_float32(shape=x_shape)\n    weight_shape = [3, 5, 2]\n    weights = self._get_rnd_float32(shape=weight_shape)\n    output = run_node(node_def, [x, weights], device=device)\n\n    padh_left = weight_shape[2] - 1 - pads[0]\n    padh_right = weight_shape[2] - 1 - pads[1]\n    kh = weight_shape[2]\n    outh = x_shape[2] + padh_right + padh_right - (kh - 1)\n\n    out_shape = [x_shape[0], weight_shape[1], outh]\n\n    test_output = np.zeros(out_shape)\n    for b in range(0, x_shape[0]):\n      for m in range(0, weight_shape[1]):\n        for c in range(0, x_shape[1]):\n          for h in range(0, outh):\n            for k in range(h, h + kh):\n              if (k - padh_left >= 0):\n                test_output[b][m][h] += x[b][c][k - padh_left] * weights[c][m][\n                    kh + h - 1 - k]\n\n    np.testing.assert_almost_equal(output[""Y""], test_output, decimal=5)\n\n    # test for spatial dimension of colnolution is 2\n    pads = [1, 1, 1, 1]\n    node_def = helper.make_node(""ConvTranspose"", [""X"", ""weights""], [""Y""],\n                                pads=pads)\n    x_shape = [1, 3, 4, 6]\n    x = self._get_rnd_float32(shape=x_shape)\n    weight_shape = [3, 5, 2, 2]\n    weights = self._get_rnd_float32(shape=weight_shape)\n    output = run_node(node_def, [x, weights], device=device)\n\n    padh_left = weight_shape[2] - 1 - pads[0]\n    padh_right = weight_shape[2] - 1 - pads[1]\n    padw_left = weight_shape[3] - 1 - pads[2]\n    padw_right = weight_shape[3] - 1 - pads[3]\n\n    kh = weight_shape[2]\n    kw = weight_shape[3]\n    outh = x_shape[2] + padh_right + padh_right - (kh - 1)\n    outw = x_shape[3] + padw_right + padw_right - (kw - 1)\n\n    out_shape = [x_shape[0], weight_shape[1], outh, outw]\n\n    test_output = np.zeros(out_shape)\n    for b in range(0, x_shape[0]):\n      for m in range(0, weight_shape[1]):\n        for c in range(0, x_shape[1]):\n          for h in range(0, outh):\n            for w in range(0, outw):\n              for k1 in range(h, h + kh):\n                for k2 in range(w, w + kw):\n                  if (k1 - padh_left >= 0 and k2 - padw_left >= 0):\n                    test_output[b][m][h][w] += x[b][c][k1 - padh_left][\n                        k2 - padw_left] * weights[c][m][kh + h - 1 -\n                                                        k1][kw + w - 1 - k2]\n\n    np.testing.assert_almost_equal(output[""Y""], test_output, decimal=5)\n\n  def test_cosh(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Cosh."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Cosh"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.cosh(x))\n\n  def test_cumsum(self):\n    if legacy_opset_pre_ver(11):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support CumSum."".format(\n          defs.onnx_opset_version()))\n    x = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.int32)\n    axis = 0\n    node_def = helper.make_node(""CumSum"", [""x"", ""axis""], [""y""])\n    # note: if axis is not provided, np.cumsum() will compute over flattened array,\n    # which is different than the TensorFlow behavior\n    y = np.cumsum(x, axis).astype(np.int32)\n    output = run_node(node_def, [x, axis])\n    np.testing.assert_almost_equal(output[""y""], y)\n\n  def test_depth_to_space(self):\n    node_def = helper.make_node(""DepthToSpace"", [""X""], [""Y""], blocksize=2)\n    x_shape = [1, 12, 1, 1]\n    x = self._get_rnd_float32(shape=x_shape)\n    output = run_node(node_def, [x])\n    x = np.transpose(x, (0, 2, 3, 1))\n    y = np.reshape(np.swapaxes(x.reshape(1, 1, 1, 2, 2, 3), 2, 3), (1, 2, 2, 3))\n    y = np.transpose(y, (0, 3, 1, 2))\n    np.testing.assert_almost_equal(output[""Y""], y, decimal=5)\n\n  def test_dequantize_linear(self):\n    node_def = helper.make_node(""DequantizeLinear"",\n                                [""x"", ""x_scale"", ""x_zero_point""], [""y""])\n    for x, x_zero_point in [[\n        self._get_rnd_int(-128, 127, [2, 6], np.int8),\n        self._get_rnd_int(-128, 127, dtype=np.int8)\n    ],\n                            [\n                                self._get_rnd_int(0, 255, [2, 6], np.uint8),\n                                self._get_rnd_int(0, 255, dtype=np.uint8)\n                            ],\n                            [self._get_rnd_int(-512, 512, [2, 6]),\n                             np.int32(0)]]:\n      x_scale = self._get_rnd_float32(-10., 10)\n      y = np.subtract(np.float32(x), np.float32(x_zero_point))\n      y = np.multiply(y, x_scale)\n      output = run_node(node_def, [x, x_scale, x_zero_point])\n      np.testing.assert_almost_equal(output[""y""], y)\n\n  def test_div(self):\n    node_def = helper.make_node(""Div"", [""X"", ""Y""], [""Z""])\n    x = self._get_rnd_float32(shape=[10, 10])\n    y = self._get_rnd_float32(shape=[10, 10])\n    output = run_node(node_def, [x, y])\n    np.testing.assert_almost_equal(output[""Z""], np.divide(x, y))\n\n  def test_dropout(self):\n    # Since current ONNX only support inference and\n    # dropout at inference mode is a no-op,\n    # therefore dropout is always a no-op operator\n    # in ONNX.\n    node_def = helper.make_node(""Dropout"", [""X""], [""Y""])\n    if legacy_opset_pre_ver(7):\n      # at inference mode, is_test is always set to 1\n      node_def = helper.make_node(""Dropout"", [""X""], [""Y""], is_test=1)\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    y = x\n    output = run_node(node_def, [x])\n    np.testing.assert_equal(output[""Y""], y)\n\n  def test_dot(self):\n    # this op is removed\n    # remove this test in the future\n    return\n    node_def = helper.make_node(""Dot"", [""X"", ""Y""], [""Z""])\n    x = np.floor(self._get_rnd_float32(shape=[10, 10]))\n    y = np.floor(self._get_rnd_float32(shape=[10, 10]))\n    output = run_node(node_def, [x, y])\n    np.testing.assert_almost_equal(output[""Z""], np.dot(x, y))\n\n  def test_dynamic_quantize_linear(self):\n    if legacy_opset_pre_ver(11):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support DynamicQuantizeLinear."".format(\n              defs.onnx_opset_version()))\n    node_def = helper.make_node(""DynamicQuantizeLinear"", [""X""],\n                                [""Y"", ""Y_Scale"", ""Y_Zero_Point""])\n    x = self._get_rnd_float32(shape=[3, 4])\n    min_x = np.minimum(0, np.min(x))\n    max_x = np.maximum(0, np.max(x))\n    y_scale = np.float32((max_x - min_x) / (255 - 0))  # uint8 -> [0, 255]\n    y_zero_point = np.clip(round((0 - min_x) / y_scale), 0,\n                           255).astype(np.uint8)\n    y = np.clip(np.round(x / y_scale) + y_zero_point, 0, 255).astype(np.uint8)\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], y)\n    np.testing.assert_almost_equal(output[""Y_Scale""], y_scale)\n    np.testing.assert_almost_equal(output[""Y_Zero_Point""], y_zero_point)\n\n  def test_elu(self):\n    node_def = helper.make_node(""Elu"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[100])\n    output = run_node(node_def, [x])\n    test_output = [self._elu(a) for a in x]\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_equal(self):\n    node_def = helper.make_node(""Equal"", [""X"", ""Y""], [""Z""])\n    x = self._get_rnd_float32(shape=[5, 3, 3, 2])\n    y = self._get_rnd_float32(shape=[3, 3, 1])\n    output = run_node(node_def, [x, y])\n    np.testing.assert_equal(output[""Z""], np.equal(x,\n                                                  np.reshape(y, [1, 3, 3, 1])))\n\n  def test_erf(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Erf."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Erf"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    output = run_node(node_def, [x])\n    exp_output = np.vectorize(math.erf)(x).astype(np.float32)\n    np.testing.assert_almost_equal(output[""Y""], exp_output)\n\n  def test_exp(self):\n    node_def = helper.make_node(""Exp"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[100])\n    x = x - 3.6\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.exp(x))\n\n  def test_eye_like(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support EyeLike."".format(\n          defs.onnx_opset_version()))\n    for shape in [[6, 10], [10, 6]]:\n      for off_diagonal_offset in [-10, -6, -3, 0, 3, 6, 7, 10]:\n        node_def = helper.make_node(""EyeLike"", [\'x\'], [\'y\'],\n                                    dtype=1,\n                                    k=off_diagonal_offset)\n        x = self._get_rnd_int(0, 100, shape=shape)\n        y = np.eye(shape[0], shape[1], k=off_diagonal_offset, dtype=np.float32)\n        output = run_node(node_def, [x])\n        np.testing.assert_equal(output[\'y\'], y)\n\n  def test_flatten(self):\n    shape = [10, 2, 3, 4, 5]\n    x = self._get_rnd_float32(shape=shape)\n    for axis in range(-len(shape), len(shape)):\n      node_def = helper.make_node(""Flatten"", [""X""], [""Y""], axis=axis)\n      output = run_node(node_def, [x])\n      if axis == 0:\n        new_shape = (1, -1)\n      else:\n        new_shape = (np.prod(shape[0:axis]).astype(int), -1)\n      np.testing.assert_almost_equal(output[""Y""], np.reshape(x, new_shape))\n\n  def test_gather(self):\n    node_def = helper.make_node(""Gather"", [""X"", ""Y""], [""Z""])\n    x = self._get_rnd_float32(shape=[10, 10])\n    y = [[0, 1], [1, 2]]\n    output = run_node(node_def, [x, y])\n    test_output = np.zeros((2, 2, 10))\n    for i in range(0, 2):\n      for j in range(0, 10):\n        test_output[0][i][j] = x[i][j]\n    for i in range(0, 2):\n      for j in range(0, 10):\n        test_output[1][i][j] = x[i + 1][j]\n    np.testing.assert_almost_equal(output[""Z""], test_output)\n    if defs.onnx_opset_version() >= 11:\n      # test negative indices\n      y = [[-10, -9], [1, -8]]\n      output = run_node(node_def, [x, y])\n      np.testing.assert_almost_equal(output[""Z""], test_output)\n      # test out of bound indices\n      for y in ([[-10, 11], [1, -8]], [[-10, -11], [1, -8]]):\n        try:\n          output = run_node(node_def, [x, y])\n          np.testing.assert_almost_equal(output[""Z""], test_output)\n          raise AssertionError(""Expected ValueError not raised for indices %d"" %\n                               str(y))\n        except InvalidArgumentError as e:\n          assert \'Gather indices are out of bound\' in str(e), str(y)\n      # test non-0 and negative axis\n      axis = -3\n      node_def = helper.make_node(""Gather"", [""X"", ""Y""], [""Z""], axis=axis)\n      x = np.reshape(np.arange(5 * 4 * 3 * 2), (5, 4, 3, 2))\n      y = np.array([0, 1, 3])\n      test_output = np.take(x, y, axis=axis)\n      output = run_node(node_def, [x, y])\n      np.testing.assert_almost_equal(output[""Z""], test_output)\n      # test axis attribute validation\n      for axis in [-5, 4, 10]:\n        try:\n          node_def = helper.make_node(""Gather"", [""X"", ""Y""], [""Z""], axis=axis)\n          run_node(node_def, [x, y])\n          raise AssertionError(\n              ""Expected ValueError not raised for axis value %d"" % axis)\n        except ValueError as e:\n          assert \'out of bounds\' in str(e), str(e) + \' for axis \' + str(axis)\n\n  def test_gather_nd(self):\n    if legacy_opset_pre_ver(11):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support GatherND."".format(\n              defs.onnx_opset_version()))\n\n    # valid positive and negative indices for elements\n    data = np.array([[0, 1], [2, 3]], dtype=np.int64)\n    indices = np.array([[0, 0], [1, 1], [-1, -2]], dtype=np.int64)\n    ref_output = np.array([0, 3, 2], dtype=np.int64)\n    node_def = helper.make_node(""GatherND"", [""data"", ""indices""], [""outputs""])\n    output = run_node(node_def, [data, indices])\n    np.testing.assert_almost_equal(output[""outputs""], ref_output)\n\n    # valid positive and negative indices for slices\n    data = np.arange(16, dtype=np.int32).reshape([2, 2, 4])\n    indices = np.array([[0, 0], [-1, -2]], dtype=np.int64)\n    ref_output = np.array([[0, 1, 2, 3], [8, 9, 10, 11]], dtype=np.int32)\n    output = run_node(node_def, [data, indices])\n    np.testing.assert_almost_equal(output[""outputs""], ref_output)\n    indices = np.array([[[0, 0]], [[-1, 0]]], dtype=np.int64)\n    ref_output = np.array([[[0, 1, 2, 3]], [[8, 9, 10, 11]]], dtype=np.int32)\n    output = run_node(node_def, [data, indices])\n    np.testing.assert_almost_equal(output[""outputs""], ref_output)\n\n    # indices out of bounds\n    indices = np.array([[5, 0], [-1, -3]], dtype=np.int64)\n    with np.testing.assert_raises(tf.errors.InvalidArgumentError):\n      output = run_node(node_def, [data, indices])\n    indices = np.array([[1, 1, 6], [-2, -1, -9]], dtype=np.int32)\n    with np.testing.assert_raises(tf.errors.InvalidArgumentError):\n      output = run_node(node_def, [data, indices])\n\n  def test_gemm(self):\n    # Compute Y = alpha * A * B + beta * C\n    node_def = helper.make_node(""Gemm"", [""A"", ""B"", ""C""], [""Y""],\n                                transA=0,\n                                transB=0,\n                                alpha=1.0,\n                                beta=1.0)\n    x = np.floor(self._get_rnd_float32(shape=[10, 10]))\n    y = np.floor(self._get_rnd_float32(shape=[10, 10]))\n    z = np.floor(self._get_rnd_float32(shape=[10, 10]))\n    output = run_node(node_def, [x, y, z])\n    test_output = np.matmul(x, y) + z\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_global_average_pool(self):\n    #   Image case:  (N x C x H x W), where N is the batch size,\n    # C is the number of channels, and H and W are the height\n    # and the width of the data\n    #\n    #   Non-image case: (N x C x D1 x D2 ... Dn)\n    #\n    #   Output data tensor from pooling across the input tensor.\n    # Dimensions will be N x C x 1 x 1\n    node_def = helper.make_node(""GlobalAveragePool"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[10, 10, 2, 3])\n    output = run_node(node_def, [x])\n    test_output = np.zeros([10, 10, 1, 1])\n    for i1 in range(0, 10):\n      for i2 in range(0, 10):\n        sum = 0\n        for j1 in range(0, 2):\n          for j2 in range(0, 3):\n            sum += x[i1][i2][j1][j2]\n        test_output[i1][i2][0][0] = sum / 6.\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_hardmax(self):\n    shape = [2, 3, 4, 5]\n    x = self._get_rnd_float32(shape=shape)\n    for axis in range(-len(shape), len(shape)):\n      node_def = helper.make_node(""Hardmax"", [""X""], [""Y""], axis=axis)\n      output = run_node(node_def, [x])\n      shape_in_2d = (np.prod(shape[0:axis]).astype(int),\n                     np.prod(shape[axis:len(shape)]))\n      x_in_2d = np.reshape(x, shape_in_2d)\n      y = np.eye(x_in_2d.shape[1], dtype=x.dtype)[np.argmax(x_in_2d, axis=1)]\n      np.testing.assert_almost_equal(output[""Y""], np.reshape(y, shape))\n\n  def test_if(self):\n    true_val = np.int64(1)\n    false_val = np.int64(0)\n    true_node = helper.make_node(\'Constant\',\n                                 inputs=[],\n                                 outputs=[\'true\'],\n                                 value_int=true_val)\n    false_node = helper.make_node(\'Constant\',\n                                  inputs=[],\n                                  outputs=[\'false\'],\n                                  value_int=false_val)\n\n    true_out = helper.make_tensor_value_info(\'true\', TensorProto.INT64, [])\n    false_out = helper.make_tensor_value_info(\'false\', TensorProto.INT64, [])\n\n    true_graph = helper.make_graph(nodes=[true_node],\n                                   name=""true_graph"",\n                                   inputs=[],\n                                   outputs=[true_out])\n    false_graph = helper.make_graph(nodes=[false_node],\n                                    name=""false_graph"",\n                                    inputs=[],\n                                    outputs=[false_out])\n\n    node_def = helper.make_node(\'If\', [\'cond\'], [\'outputs\'],\n                                then_branch=true_graph,\n                                else_branch=false_graph)\n\n    for cond, exp in [[True, true_val], [False, false_val]]:\n      output = run_node(node_def, [cond])\n      np.testing.assert_equal(output[\'outputs\'], [exp])\n\n    x = self._get_rnd_int(low=-50, high=50, dtype=np.int64)\n    y = self._get_rnd_int(low=-50, high=50, dtype=np.int64)\n    z = self._get_rnd_int(low=-50, high=50, dtype=np.int64)\n    x_node = helper.make_node(\'Constant\', inputs=[], outputs=[\'x\'], value_int=x)\n    y_node = helper.make_node(\'Constant\', inputs=[], outputs=[\'y\'], value_int=y)\n    z_node = helper.make_node(\'Constant\', inputs=[], outputs=[\'z\'], value_int=z)\n    add_node = helper.make_node(\'Add\', inputs=[\'x\', \'y\'], outputs=[\'sum\'])\n    sub_node = helper.make_node(\'Sub\', inputs=[\'x\', \'y\'], outputs=[\'diff\'])\n    mul1_node = helper.make_node(\'Mul\', inputs=[\'sum\', \'z\'], outputs=[\'prod1\'])\n    mul2_node = helper.make_node(\'Mul\', inputs=[\'diff\', \'z\'], outputs=[\'prod2\'])\n\n    x_out = helper.make_tensor_value_info(\'x\', TensorProto.INT64, [])\n    y_out = helper.make_tensor_value_info(\'y\', TensorProto.INT64, [])\n    z_out = helper.make_tensor_value_info(\'z\', TensorProto.INT64, [])\n    sum_out = helper.make_tensor_value_info(\'sum\', TensorProto.INT64, [])\n    diff_out = helper.make_tensor_value_info(\'diff\', TensorProto.INT64, [])\n    prod1_out = helper.make_tensor_value_info(\'prod1\', TensorProto.INT64, [])\n    prod2_out = helper.make_tensor_value_info(\'prod2\', TensorProto.INT64, [])\n\n    true_graph = helper.make_graph(nodes=[add_node, mul1_node],\n                                   name=""true_graph"",\n                                   inputs=[x_out, y_out, z_out],\n                                   outputs=[sum_out, prod1_out])\n    false_graph = helper.make_graph(nodes=[sub_node, mul2_node],\n                                    name=""false_graph"",\n                                    inputs=[x_out, y_out, z_out],\n                                    outputs=[diff_out, prod2_out])\n\n    less_node = helper.make_node(\'Less\', inputs=[\'x\', \'y\'], outputs=[\'cond\'])\n    if_node = helper.make_node(\'If\',\n                               inputs=[\'cond\'],\n                               outputs=[\'result\'],\n                               then_branch=true_graph,\n                               else_branch=false_graph)\n\n    result_out = helper.make_tensor_value_info(\'result\', TensorProto.INT64, [])\n\n    graph = helper.make_graph(\n        nodes=[x_node, y_node, z_node, less_node, if_node],\n        name=""test_if"",\n        inputs=[],\n        outputs=[result_out])\n\n    tf_rep = onnx_graph_to_tensorflow_rep(graph)\n    output = tf_rep.run({})\n    expected = [x + y, (x + y) * z] if x < y else [x - y, (x - y) * z]\n    np.testing.assert_equal(output[\'result\'], expected)\n\n  def test_image_sacler(self):\n    # Input:  (N x C x H x W), where N is the batch size,\n    # C is the number of channels, and H and W are the height\n    # and the width of the data\n    # Scale: (flout, default 1.0) the scale to apply\n    # Bias: applied to each channel, same size as C\n    # Output has same shape and type as input\n    x = self._get_rnd_float32(shape=[1, 3, 224, 224])\n    #random distribution over [0,1), so add 0.1\n    scale = np.random.rand(1)[0] + 0.1\n    bias = np.random.rand(3)\n    node_def = helper.make_node(""ImageScaler"", [""X""], [""Y""],\n                                scale=scale,\n                                bias=bias)\n    output = run_node(node_def, [x])\n    test_out = np.multiply(x, scale)\n    test_out = np.transpose(test_out, [0, 2, 3, 1])\n    test_out = np.add(test_out, bias)\n    test_out = np.transpose(test_out, [0, 3, 1, 2])\n    np.testing.assert_almost_equal(output[""Y""], test_out)\n\n  def test_is_inf(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support IsInf."".format(\n          defs.onnx_opset_version()))\n    input = np.array([-1.2, np.nan, np.inf, 2.8, np.NINF, np.inf],\n                     dtype=np.float32)\n    expected_output = {\n        ""node_def"": np.isinf(input),\n        ""node_def_neg_false"": np.isposinf(input),\n        ""node_def_pos_false"": np.isneginf(input)\n    }\n    node_defs = {\n        ""node_def"":\n            helper.make_node(""IsInf"", [""X""], [""Y""]),\n        ""node_def_neg_false"":\n            helper.make_node(""IsInf"", [""X""], [""Y""], detect_negative=0),\n        ""node_def_pos_false"":\n            helper.make_node(""IsInf"", [""X""], [""Y""], detect_positive=0)\n    }\n    for key in node_defs:\n      output = run_node(node_defs[key], [input])\n      np.testing.assert_equal(output[""Y""], expected_output[key])\n\n  def test_isnan(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support IsNaN."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""IsNaN"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 3])\n    x[0][1] = x[1][0] = x[2][2] = np.nan\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.isnan(x))\n\n  def test_global_lp_pool(self):\n    #   Image case:  (N x C x H x W), where N is the batch size,\n    # C is the number of channels, and H and W are the height\n    # and the width of the data\n    #\n    #   Non-image case: (N x C x D1 x D2 ... Dn)\n    #\n    #   Output data tensor from pooling across the input tensor.\n    # Dimensions will be N x C x 1 x 1\n    node_def = helper.make_node(""GlobalLpPool"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[10, 10, 2, 3])\n    output = run_node(node_def, [x])\n    test_output = np.zeros([10, 10, 1, 1])\n    for i1 in range(0, 10):\n      for i2 in range(0, 10):\n        tmp = np.zeros([2, 3])\n        for j1 in range(0, 2):\n          for j2 in range(0, 3):\n            tmp[j1][j2] = x[i1][i2][j1][j2]\n        test_output[i1][i2][0][0] = np.linalg.norm(tmp)\n    np.testing.assert_almost_equal(output[""Y""], test_output, decimal=5)\n\n  def test_global_max_pool(self):\n    #   Image case:  (N x C x H x W), where N is the batch size,\n    # C is the number of channels, and H and W are the height\n    # and the width of the data\n    #\n    #   Non-image case: (N x C x D1 x D2 ... Dn)\n    #\n    #   Output data tensor from pooling across the input tensor.\n    # Dimensions will be N x C x 1 x 1\n    node_def = helper.make_node(""GlobalMaxPool"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[10, 10, 2, 3])\n    output = run_node(node_def, [x])\n    test_output = np.zeros([10, 10, 1, 1])\n    for i1 in range(0, 10):\n      for i2 in range(0, 10):\n        max = x[i1][i2][0][0]\n        for j1 in range(0, 2):\n          for j2 in range(0, 3):\n            if max < x[i1][i2][j1][j2]:\n              max = x[i1][i2][j1][j2]\n        test_output[i1][i2][0][0] = max\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_less(self):\n    node_def = helper.make_node(""Less"", [""X"", ""Y""], [""Z""])\n    x = self._get_rnd_float32(shape=[5, 3, 3, 2])\n    y = self._get_rnd_float32(shape=[3, 3, 1])\n    output = run_node(node_def, [x, y])\n    np.testing.assert_equal(output[""Z""], np.less(x, np.reshape(y,\n                                                               [1, 3, 3, 1])))\n\n  def test_lp_normalization(self):\n    for ordr in range(1, 3):\n      node_def = helper.make_node(""LpNormalization"", [""X""], [""Y""], p=ordr)\n      x = self._get_rnd_float32(shape=[2, 2, 3, 2])\n      output = run_node(node_def, [x])\n      np.testing.assert_allclose(\n          output[""Y""],\n          x / np.expand_dims(np.linalg.norm(x, axis=-1, ord=ordr), -1),\n          rtol=1e-3)\n\n  def test_l_r_n(self):\n    # Each input value is divided by:\n    #\n    # (bias+(alpha/size)*sum(xi^2 for every xi in the local region))^beta\n    alpha = 2.0\n    beta = 1.0\n    bias = 5.0\n    size = 3\n    node_def = helper.make_node(""LRN"", [""X""], [""Y""],\n                                alpha=alpha,\n                                beta=beta,\n                                bias=bias,\n                                size=size)\n    x = self._get_rnd_float32(shape=[10, 2, 10, 10])\n    output = run_node(node_def, [x])\n    test_output = np.zeros([10, 10, 10, 2])\n    x = np.transpose(x, axes=[0, 2, 3, 1])\n    for i1 in range(0, 10):\n      for i2 in range(0, 10):\n        for j1 in range(0, 10):\n          for j2 in range(0, 2):\n            sqr_sum = 0.\n            # size of 3 means radius 1 in TF speak\n            # i.e. the immediate neighbouring values\n            # if ""previous"" neighbour exists\n            if j2 > 0:\n              sqr_sum += x[i1][i2][j1][j2 - 1] * x[i1][i2][j1][j2 - 1]\n            # current value\n            sqr_sum += x[i1][i2][j1][j2] * x[i1][i2][j1][j2]\n            # if ""next"" neighbour exists\n            if j2 < 2 - 1:\n              sqr_sum += x[i1][i2][j1][j2 + 1] * x[i1][i2][j1][j2 + 1]\n            test_output[i1][i2][j1][j2] = \\\n              x[i1][i2][j1][j2] / ((bias + (alpha * 1. / size) * sqr_sum) ** beta)\n    test_output = np.transpose(test_output, axes=[0, 3, 1, 2])\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_floor(self):\n    node_def = helper.make_node(""Floor"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[100])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.floor(x))\n\n  def test_leakyrelu(self):\n    node_def = helper.make_node(""LeakyRelu"", [""X""], [""Y""], alpha=0.8)\n    x = np.floor(self._get_rnd_float32(shape=[100]))\n    output = run_node(node_def, [x])\n    test_output = [self._leaky_relu(a, 0.8) for a in x]\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_log(self):\n    node_def = helper.make_node(""Log"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[100])\n    x = x + 3.6\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.log(x))\n\n  def test_loop(self):\n    add1_node = helper.make_node(\'Add\', inputs=[\'x\', \'x\'], outputs=[\'sum1\'])\n    neg_node = helper.make_node(\'Neg\', inputs=[\'sum1\'], outputs=[\'neg_sum1\'])\n    add2_node = helper.make_node(\'Add\',\n                                 inputs=[\'y\', \'neg_sum1\'],\n                                 outputs=[\'sum2\'])\n    add3_node = helper.make_node(\'Add\',\n                                 inputs=[\'sum1\', \'sum2\'],\n                                 outputs=[\'sum3\'])\n    less_node = helper.make_node(\'Less\',\n                                 inputs=[\'sum1\', \'sum2\'],\n                                 outputs=[\'new_cond\'])\n    greater_node = helper.make_node(\'Greater\',\n                                    inputs=[\'sum1\', \'sum2\'],\n                                    outputs=[\'new_cond\'])\n\n    m_in = helper.make_tensor_value_info(\'M\', TensorProto.INT64, [])\n    cond_in = helper.make_tensor_value_info(\'cond\', TensorProto.BOOL, [])\n    cond_int_in = helper.make_tensor_value_info(\'cond\', TensorProto.INT32, [])\n    x_in = helper.make_tensor_value_info(\'x\', TensorProto.INT32, [None])\n    y_in = helper.make_tensor_value_info(\'y\', TensorProto.INT32, [None])\n\n    cond_out = helper.make_tensor_value_info(\'cond\', TensorProto.BOOL, [])\n    new_cond_out = helper.make_tensor_value_info(\'new_cond\', TensorProto.BOOL,\n                                                 [])\n    sum1_out = helper.make_tensor_value_info(\'sum1\', TensorProto.INT32, [None])\n    sum2_out = helper.make_tensor_value_info(\'sum2\', TensorProto.INT32, [None])\n    sum3_out = helper.make_tensor_value_info(\'sum3\', TensorProto.INT32, [None])\n\n    v1_initial = np.array([1, 1], dtype=np.int32)\n    v2_initial = np.array([100, 100], dtype=np.int32)\n\n    # test for loop\n    M = np.int64(10)\n    cond = True # value will be ignore because optional ""cond"" input will be skip\n    graph = helper.make_graph(nodes=[add1_node, neg_node, add2_node, add3_node],\n                              name=""for_loop_graph"",\n                              inputs=[m_in, cond_in, x_in, y_in],\n                              outputs=[cond_out, sum1_out, sum2_out, sum3_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'M\', \'\', \'v1_initial\', \'v2_initial\'],\n                                [\'v_final\', \'scan_outputs\'],\n                                body=graph)\n    output = run_node(node_def, [M, cond, v1_initial, v2_initial])\n    v_final = [\n        np.array([1024, 1024], dtype=np.int32),\n        np.array([-1946, -1946], dtype=np.int32)\n    ]\n    scan_outputs = [\n        np.array([[100, 100], [98, 98], [94, 94], [86, 86], [70, 70], [38, 38],\n                  [-26, -26], [-154, -154], [-410, -410], [-922, -922]],\n                 dtype=np.int32)\n    ]\n    np.testing.assert_almost_equal(output[\'v_final\'], v_final)\n    np.testing.assert_almost_equal(output[\'scan_outputs\'], scan_outputs)\n\n    # test while loop\n    M = 0 # value will be ignore because optional ""M"" input will be skip\n    cond = v1_initial < v2_initial\n    graph = helper.make_graph(\n        nodes=[add1_node, neg_node, add2_node, add3_node, less_node],\n        name=""while_loop_graph"",\n        inputs=[m_in, cond_in, x_in, y_in],\n        outputs=[new_cond_out, sum1_out, sum2_out, sum3_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'\', \'cond\', \'v1_initial\', \'v2_initial\'],\n                                [\'v_final\', \'scan_outputs\'],\n                                body=graph)\n    output = run_node(node_def, [M, cond, v1_initial, v2_initial])\n    v_final = [\n        np.array([64, 64], dtype=np.int32),\n        np.array([-26, -26], dtype=np.int32)\n    ]\n    scan_outputs = [\n        np.array([[100, 100], [98, 98], [94, 94], [86, 86], [70, 70], [38, 38]],\n                 dtype=np.int32)\n    ]\n    np.testing.assert_almost_equal(output[\'v_final\'], v_final)\n    np.testing.assert_almost_equal(output[\'scan_outputs\'], scan_outputs)\n\n    # test do-while loop\n    M = 0 # value will be ignore because optional ""M"" input will be skip\n    cond = 1\n    graph = helper.make_graph(\n        nodes=[add1_node, neg_node, add2_node, add3_node, greater_node],\n        name=""do_while_loop_graph"",\n        inputs=[m_in, cond_int_in, x_in, y_in],\n        outputs=[new_cond_out, sum1_out, sum2_out, sum3_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'\', \'cond\', \'v1_initial\', \'v2_initial\'],\n                                [\'v_final\', \'scan_outputs\'],\n                                body=graph)\n    output = run_node(node_def, [M, cond, v1_initial, v2_initial])\n    v_final = [\n        np.array([2, 2], dtype=np.int32),\n        np.array([98, 98], dtype=np.int32)\n    ]\n    scan_outputs = [np.array([[100, 100]], dtype=np.int32)]\n    np.testing.assert_almost_equal(output[\'v_final\'], v_final)\n    np.testing.assert_almost_equal(output[\'scan_outputs\'], scan_outputs)\n\n    # test for loop and while loop conbine\n    M = np.int64(4)\n    cond = v1_initial < v2_initial\n    graph = helper.make_graph(\n        nodes=[add1_node, neg_node, add2_node, add3_node, less_node],\n        name=""for_and_while_loop_graph"",\n        inputs=[m_in, cond_in, x_in, y_in],\n        outputs=[new_cond_out, sum1_out, sum2_out, sum3_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'M\', \'cond\', \'v1_initial\', \'v2_initial\'],\n                                [\'v_final\', \'scan_outputs\'],\n                                body=graph)\n    output = run_node(node_def, [M, cond, v1_initial, v2_initial])\n    v_final = [\n        np.array([16, 16], dtype=np.int32),\n        np.array([70, 70], dtype=np.int32)\n    ]\n    scan_outputs = [\n        np.array([[100, 100], [98, 98], [94, 94], [86, 86]], dtype=np.int32)\n    ]\n    np.testing.assert_almost_equal(output[\'v_final\'], v_final)\n    np.testing.assert_almost_equal(output[\'scan_outputs\'], scan_outputs)\n\n    # test for loop that doesn\'t run at all (M = 0)\n    M = np.int64(0)\n    cond = True # value will be ignore because optional ""cond"" input will be skip\n    graph = helper.make_graph(nodes=[add1_node, neg_node, add2_node, add3_node],\n                              name=""for_loop_graph"",\n                              inputs=[m_in, cond_in, x_in, y_in],\n                              outputs=[cond_out, sum1_out, sum2_out, sum3_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'M\', \'\', \'v1_initial\', \'v2_initial\'],\n                                [\'v_final\', \'scan_outputs\'],\n                                body=graph)\n    output = run_node(node_def, [M, cond, v1_initial, v2_initial])\n    v_final = [\n        np.array([1, 1], dtype=np.int32),\n        np.array([100, 100], dtype=np.int32)\n    ]\n    scan_outputs = np.array([], dtype=np.int32).reshape(1, 0, 2)\n    np.testing.assert_almost_equal(output[\'v_final\'], v_final)\n    np.testing.assert_almost_equal(output[\'scan_outputs\'], scan_outputs)\n\n    # test while loop that doesn\'t run at all (cond = False)\n    M = 0 # value will be ignore because optional ""M"" input will be skip\n    cond = False\n    graph = helper.make_graph(\n        nodes=[add1_node, neg_node, add2_node, add3_node, less_node],\n        name=""while_loop_graph"",\n        inputs=[m_in, cond_in, x_in, y_in],\n        outputs=[new_cond_out, sum1_out, sum2_out, sum3_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'\', \'cond\', \'v1_initial\', \'v2_initial\'],\n                                [\'v_final\', \'scan_outputs\'],\n                                body=graph)\n    output = run_node(node_def, [M, cond, v1_initial, v2_initial])\n    v_final = [\n        np.array([1, 1], dtype=np.int32),\n        np.array([100, 100], dtype=np.int32)\n    ]\n    scan_outputs = np.array([], dtype=np.int32).reshape(1, 0, 2)\n    np.testing.assert_almost_equal(output[\'v_final\'], v_final)\n    np.testing.assert_almost_equal(output[\'scan_outputs\'], scan_outputs)\n\n    # test while loop that doesn\'t have any scan_outputs\n    M = np.int64(4)\n    cond = v1_initial < v2_initial\n    graph = helper.make_graph(nodes=[add1_node, neg_node, add2_node, less_node],\n                              name=""while_loop_graph"",\n                              inputs=[m_in, cond_in, x_in, y_in],\n                              outputs=[new_cond_out, sum1_out, sum2_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'M\', \'cond\', \'v1_initial\', \'v2_initial\'],\n                                [\'v_final\'],\n                                body=graph)\n    output = run_node(node_def, [M, cond, v1_initial, v2_initial])\n    v_final = [\n        np.array([16, 16], dtype=np.int32),\n        np.array([70, 70], dtype=np.int32)\n    ]\n    np.testing.assert_almost_equal(output[\'v_final\'], v_final)\n\n    # test for loop that doesn\'t run at all (M = 0)\n    # and the scan_outputs shape is not the same as the inputs\n    v1_initial = np.array([[1, 1, 1], [2, 2, 2]], dtype=np.int32)\n    v3_initial = np.array([[1, 1], [2, 2], [3, 3]], dtype=np.int32)\n    matmul_node = helper.make_node(\'MatMul\',\n                                   inputs=[\'x\', \'z\'],\n                                   outputs=[\'product\'])\n    x_in = helper.make_tensor_value_info(\'x\', TensorProto.INT32, [None, None])\n    z_in = helper.make_tensor_value_info(\'z\', TensorProto.INT32, [None, None])\n    sum1_out = helper.make_tensor_value_info(\'sum1\', TensorProto.INT32,\n                                             [None, None])\n    z_out = helper.make_tensor_value_info(\'z\', TensorProto.INT32, [None, None])\n    product_out = helper.make_tensor_value_info(\'product\', TensorProto.INT32,\n                                                [None, None])\n\n    M = np.int64(0)\n    cond = True # value will be ignore because optional ""cond"" input will be skip\n    graph = helper.make_graph(nodes=[add1_node, matmul_node],\n                              name=""for_loop_graph"",\n                              inputs=[m_in, cond_in, x_in, z_in],\n                              outputs=[cond_out, sum1_out, z_out, product_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'M\', \'\', \'v1_initial\', \'v3_initial\'],\n                                [\'v_final\', \'scan_outputs\'],\n                                body=graph)\n    output = run_node(node_def, [M, cond, v1_initial, v3_initial])\n    v_final = [v1_initial, v3_initial]\n    scan_outputs = np.array([], dtype=np.int32).reshape(1, 0, 2, 2)\n    for i in range(len(output[\'v_final\'])):\n      np.testing.assert_almost_equal(output[\'v_final\'][i], v_final[i])\n    np.testing.assert_almost_equal(output[\'scan_outputs\'], scan_outputs)\n\n    # verify infinite loop will get exception\n    M = 0 # value will be ignore because optional ""M"" input will be skip\n    cond = True # value will be ignore because optional ""cond"" input will be skip\n    graph = helper.make_graph(\n        nodes=[add1_node, neg_node, add2_node, add3_node, less_node],\n        name=""while_loop_graph"",\n        inputs=[m_in, cond_in, x_in, y_in],\n        outputs=[cond_out, sum1_out, sum2_out, sum3_out])\n    node_def = helper.make_node(\'Loop\',\n                                [\'\', \'\', \'v1_initial\', \'v2_initial\'],\n                                [\'v_final\', \'scan_outputs\'],\n                                body=graph)\n    try:\n      output = run_node(node_def, [M, cond, v1_initial, v2_initial])\n      raise AssertionError(""Expected RuntimeError not raise when Loop inputs "" +\n                           ""M and cond are both not set at the same time"")\n    except RuntimeError as e:\n      assert ""M and cond in Loop are not set"" in str(e)\n\n  def test_matmul_integer(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support MatMulInteger."".format(\n              defs.onnx_opset_version()))\n\n    node_def = helper.make_node(""MatMulInteger"",\n                                [""A"", ""B"", ""a_zero_point"", ""b_zero_point""],\n                                [""Z""])\n    lower_bound = {np.uint8: 0, np.int8: -20}\n    for dtype in [np.uint8, np.int8]:\n      # A & B are 3-D tensor and a_zero_point & b_zero_point are scalar\n      A = self._get_rnd_int(lower_bound[dtype],\n                            20,\n                            shape=(2, 3, 4),\n                            dtype=dtype)\n      B = self._get_rnd_int(lower_bound[dtype],\n                            20,\n                            shape=(2, 4, 6),\n                            dtype=dtype)\n      a_zero_point = self._get_rnd_int(lower_bound[dtype], 20, dtype=dtype)\n      b_zero_point = self._get_rnd_int(lower_bound[dtype], 20, dtype=dtype)\n      A_minus_zero_point = np.subtract(A.astype(np.int32),\n                                       a_zero_point.astype(np.int32))\n      B_minus_zero_point = np.subtract(B.astype(np.int32),\n                                       b_zero_point.astype(np.int32))\n      z = np.matmul(A_minus_zero_point, B_minus_zero_point)\n      output = run_node(node_def, [A, B, a_zero_point, b_zero_point])\n      np.testing.assert_almost_equal(output[""Z""], z)\n      # A & B are 4-D tensor and a_zero_point & b_zero_point are 1-D tensor\n      A = self._get_rnd_int(lower_bound[dtype],\n                            20,\n                            shape=(2, 5, 3, 4),\n                            dtype=dtype)\n      B = self._get_rnd_int(lower_bound[dtype],\n                            20,\n                            shape=(2, 1, 4, 6),\n                            dtype=dtype)\n      a_zero_point = self._get_rnd_int(lower_bound[dtype],\n                                       20,\n                                       shape=(A.shape[-2]),\n                                       dtype=dtype)\n      b_zero_point = self._get_rnd_int(lower_bound[dtype],\n                                       20,\n                                       shape=(B.shape[-1]),\n                                       dtype=dtype)\n      a_zero_point_with_reshape = np.reshape(a_zero_point, [A.shape[-2], 1])\n      A_minus_zero_point = np.subtract(\n          A.astype(np.int32), a_zero_point_with_reshape.astype(np.int32))\n      B_minus_zero_point = np.subtract(B.astype(np.int32),\n                                       b_zero_point.astype(np.int32))\n      z = np.matmul(A_minus_zero_point, B_minus_zero_point)\n      output = run_node(node_def, [A, B, a_zero_point, b_zero_point])\n      np.testing.assert_almost_equal(output[""Z""], z)\n\n    node_def = helper.make_node(""MatMulInteger"", [""A"", ""B""], [""Z""])\n    for dtype in [np.uint8, np.int8]:\n      # A & B are 3-D tensor\n      A = self._get_rnd_int(lower_bound[dtype],\n                            20,\n                            shape=(2, 3, 4),\n                            dtype=dtype)\n      B = self._get_rnd_int(lower_bound[dtype],\n                            20,\n                            shape=(2, 4, 6),\n                            dtype=dtype)\n      z = np.matmul(A.astype(np.int32), B.astype(np.int32))\n      output = run_node(node_def, [A, B])\n      np.testing.assert_almost_equal(output[""Z""], z)\n      # A & B are 4-D tensor\n      A = self._get_rnd_int(lower_bound[dtype],\n                            20,\n                            shape=(2, 5, 3, 4),\n                            dtype=dtype)\n      B = self._get_rnd_int(lower_bound[dtype],\n                            20,\n                            shape=(2, 1, 4, 6),\n                            dtype=dtype)\n      z = np.matmul(A.astype(np.int32), B.astype(np.int32))\n      output = run_node(node_def, [A, B])\n      np.testing.assert_almost_equal(output[""Z""], z)\n\n  def test_max(self):\n    node_def = helper.make_node(""Max"", [""X1"", ""X2"", ""X3"", ""X4""], [""Z""])\n    x1 = self._get_rnd_float32(shape=[10, 10])\n    x2 = self._get_rnd_float32(shape=[10, 10])\n    x3 = self._get_rnd_float32(shape=[10, 10])\n    x4 = self._get_rnd_float32(shape=[10, 10])\n    output = run_node(node_def, [x1, x2, x3, x4])\n    test_output = np.maximum(np.maximum(np.maximum(x1, x2), x3), x4)\n    np.testing.assert_almost_equal(output[""Z""], test_output)\n\n  def _test_pooling(self,\n                    input_shape,\n                    kernel_shape,\n                    strides=None,\n                    dilations=None,\n                    pads=None,\n                    auto_pad=None,\n                    ceil_mode=None,\n                    count_include_pad=None,\n                    pooling_type=""MAX"",\n                    input_dtype=np.float32):\n\n    op = ""MaxPool"" if pooling_type.upper().startswith(""MAX"") else ""AveragePool""\n    node_def_kwargs = {\n        ""op_type"": op,\n        ""inputs"": [""X""],\n        ""outputs"": [""Y""],\n        ""kernel_shape"": kernel_shape\n    }\n\n    if strides is not None:\n      node_def_kwargs[""strides""] = strides\n    if dilations is not None:\n      node_def_kwargs[""dilations""] = dilations\n    if pads is not None:\n      node_def_kwargs[""pads""] = pads\n    if auto_pad is not None:\n      node_def_kwargs[""auto_pad""] = auto_pad\n      pads = auto_pad\n    if ceil_mode is not None:\n      node_def_kwargs[""ceil_mode""] = ceil_mode\n    else:\n      ceil_mode = 0\n    if count_include_pad is not None:\n      node_def_kwargs[""count_include_pad""] = count_include_pad\n\n    node_def = helper.make_node(**node_def_kwargs)\n\n    if input_dtype == np.float32:\n      x = self._get_rnd_float32(shape=input_shape)\n    else:\n      x = self._get_rnd_int(low=np.iinfo(input_dtype).min,\n                            high=np.iinfo(input_dtype).max,\n                            shape=input_shape,\n                            dtype=input_dtype)\n\n    output = run_node(node_def, [x])\n\n    test_output = py_pool(x,\n                          kernel_shape=kernel_shape,\n                          strides=strides,\n                          dilations=dilations,\n                          padding=pads,\n                          ceil_mode=ceil_mode,\n                          pooling_type=pooling_type,\n                          include_indices=False)\n\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n\n  def test_max_pool_2d(self):\n    kernel_shape = [1, 2]\n    strides = [1, 2]\n\n    input_shape = [10, 10, 4, 4]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides)\n\n  def test_max_pool_2d_same_lower(self):\n    kernel_shape = [1, 2]\n    strides = [1, 2]\n    auto_pad = ""SAME_LOWER""\n\n    input_shape = [10, 10, 7, 7]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       auto_pad=auto_pad)\n\n  def test_max_pool_2d_ceil_same_lower(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support ceil mode."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [2, 1]\n    strides = [1, 2]\n    auto_pad = ""SAME_LOWER""\n    ceil_mode = 1\n\n    input_shape = [10, 10, 7, 7]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       auto_pad=auto_pad,\n                       ceil_mode=ceil_mode)\n\n  def test_max_pool_2d_same_upper(self):\n    kernel_shape = [1, 2]\n    strides = [1, 2]\n    auto_pad = ""SAME_UPPER""\n\n    input_shape = [10, 10, 7, 7]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       auto_pad=auto_pad)\n\n  def test_max_pool_2d_ceil(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support ceil mode."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    ceil_mode = 1\n\n    input_shape = [10, 3, 24, 24]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       ceil_mode=ceil_mode)\n\n  def test_max_pool_2d_dilations(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    dilations = [3, 3]\n    node_def = helper.make_node(""MaxPool"", [""X""], [""Y""],\n                                kernel_shape=kernel_shape,\n                                strides=strides,\n                                dilations=dilations)\n\n    input_shape = [10, 3, 24, 24]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations)\n\n  def test_max_pool_2d_dilations_ceil(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations nor ceil mode."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    dilations = [3, 3]\n    ceil_mode = 1\n\n    input_shape = [10, 3, 23, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       ceil_mode=ceil_mode)\n\n  def test_max_pool_2d_dilations_pads(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    dilations = [3, 3]\n    pads = [1, 1, 2, 2]\n\n    input_shape = [10, 3, 24, 24]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       pads=pads)\n\n  def test_max_pool_2d_dilations_ceil_pads(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations nor ceil mode."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    dilations = [3, 3]\n    pads = [1, 1, 2, 2]\n    ceil_mode = 1\n\n    input_shape = [10, 3, 23, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       pads=pads,\n                       ceil_mode=ceil_mode)\n\n  def test_max_pool_2d_dilations_same_lower(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    dilations = [3, 3]\n    auto_pad = ""same_lower""\n\n    input_shape = [10, 3, 24, 24]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       auto_pad=auto_pad)\n\n  def test_max_pool_2d_dilations_same_upper(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [2, 3]\n    strides = [4, 2]\n    dilations = [3, 5]\n    auto_pad = ""SAME_UPPER""\n\n    input_shape = [10, 3, 24, 24]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       auto_pad=auto_pad)\n\n  def test_max_pool_2d_dilations_ceil_pads_int8(self):\n    if legacy_opset_pre_ver(12):\n      raise unittest.SkipTest(\n          ""ONNX version {} does not support int8 input type."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    dilations = [3, 3]\n    pads = [1, 1, 2, 2]\n    ceil_mode = 1\n\n    input_shape = [10, 3, 23, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       pads=pads,\n                       ceil_mode=ceil_mode,\n                       input_dtype=np.int8)\n\n  def test_max_pool_3d(self):\n    kernel_shape = [3, 3, 3]\n    strides = [2, 2, 2]\n\n    input_shape = [10, 3, 23, 23, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides)\n\n  def test_max_pool_3d_dilations_ceil_pads(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations nor ceil mode."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3, 3]\n    strides = [2, 2, 2]\n    dilations = [3, 3, 3]\n    pads = [1, 1, 2, 2, 1, 1]\n    ceil_mode = 1\n\n    input_shape = [10, 3, 23, 23, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       pads=pads,\n                       ceil_mode=ceil_mode)\n\n  def test_max_pool_3d_dilations_same_lower(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 1, 2]\n    strides = [2, 2, 1]\n    dilations = [3, 2, 5]\n    auto_pad = ""SAME_LOWER""\n\n    input_shape = [10, 3, 23, 23, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       auto_pad=auto_pad)\n\n  def test_max_pool_1d_dilations_ceil_pads(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations nor ceil mode."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3]\n    strides = [2]\n    dilations = [3]\n    pads = [1, 2]\n    ceil_mode = 1\n\n    input_shape = [10, 3, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       dilations=dilations,\n                       pads=pads,\n                       ceil_mode=ceil_mode)\n\n  def test_max_pool_1d(self):\n    kernel_shape = [3]\n    strides = [2]\n\n    input_shape = [10, 3, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides)\n\n  def test_max_pool_with_argmax_2d_dilations_ceil_pads(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support dilations nor ceil mode."".format(\n              defs.onnx_opset_version()))\n\n    kernel_shape = [3, 3]\n    strides = [2, 2]\n    dilations = [3, 3]\n    pads = [1, 1, 2, 2]\n    ceil_mode = True\n    node_def = helper.make_node(""MaxPool"", [""X""], [""Y"", ""Ind""],\n                                kernel_shape=kernel_shape,\n                                strides=strides,\n                                dilations=dilations,\n                                pads=pads,\n                                ceil_mode=ceil_mode)\n\n    input_shape = [10, 1, 23, 23]\n    x = self._get_rnd_float32(shape=input_shape) - 2\n    output = run_node(node_def, [x])\n\n    test_output, test_ind = py_pool(x,\n                                    kernel_shape=kernel_shape,\n                                    strides=strides,\n                                    dilations=dilations,\n                                    padding=pads,\n                                    ceil_mode=ceil_mode,\n                                    pooling_type=""MAX"")\n\n    np.testing.assert_almost_equal(output[""Y""], test_output)\n    np.testing.assert_almost_equal(output[""Ind""], test_ind)\n\n  def test_max_pool_with_argmax_3d(self):\n    kernel_shape = [3, 3, 3]\n    strides = [2, 2, 2]\n    node_def = helper.make_node(""MaxPool"", [""X""], [""Y"", ""Ind""],\n                                kernel_shape=kernel_shape,\n                                strides=strides)\n\n    input_shape = [10, 1, 23, 23, 23]\n    x = self._get_rnd_float32(shape=input_shape)\n    self.assertRaises(RuntimeError, run_node, node_def, [x])\n\n  def test_max_pool_4d(self):\n    kernel_shape = [3, 3, 3, 3]\n    strides = [2, 2, 2, 2]\n    node_def = helper.make_node(""MaxPool"", [""X""], [""Y"", ""Ind""],\n                                kernel_shape=kernel_shape,\n                                strides=strides)\n\n    input_shape = [1, 1, 4, 4, 4, 4]\n    x = self._get_rnd_float32(shape=input_shape)\n    self.assertRaises(RuntimeError, run_node, node_def, [x])\n\n  def test_max_unpool(self):\n    input_shape = [10, 10, 4, 4]\n    x = self._get_rnd_float32(shape=input_shape)\n\n    X = helper.make_tensor_value_info(\'X\', TensorProto.FLOAT, input_shape)\n    Y = helper.make_tensor_value_info(\'Y\', TensorProto.FLOAT, input_shape)\n\n    node_def = helper.make_node(""MaxPool"", [""X""], [""Pool"", ""Indices""],\n                                kernel_shape=[2, 2],\n                                strides=[2, 2])\n    output_pool = run_node(node_def, [x])\n\n    node_def = helper.make_node(""MaxUnpool"", [""Pool"", ""Indices""], [""Y""],\n                                kernel_shape=[2, 2],\n                                strides=[2, 2])\n    output_unpool = run_node(node_def,\n                             [output_pool[""Pool""], output_pool[""Indices""]])\n\n    test_output = np.zeros(input_shape)\n    for i1 in range(0, input_shape[0]):\n      for i2 in range(0, input_shape[1]):\n        for i3 in range(0, input_shape[2], 2):\n          for i4 in range(0, input_shape[3], 2):\n            max_val = float(\'-inf\')\n            for j1 in range(i3, i3 + 2):\n              for j2 in range(i4, i4 + 2):\n                if x[i1][i2][j1][j2] > max_val:\n                  max_val = x[i1][i2][j1][j2]\n                  max_ind = (j1, j2)\n            j1, j2 = max_ind\n            test_output[i1][i2][j1][j2] = max_val\n    np.testing.assert_almost_equal(output_unpool[""Y""], test_output)\n\n  def test_average_pool_1d(self):\n    kernel_shape = [3]\n    strides = [2]\n\n    input_shape = [10, 3, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       pooling_type=""AVG"")\n\n  def test_average_pool_2d(self):\n    kernel_shape = [1, 2]\n    strides = [1, 2]\n\n    input_shape = [10, 10, 4, 4]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       pooling_type=""AVG"")\n\n  def test_average_pool_2d_same_upper(self):\n    kernel_shape = [1, 2]\n    strides = [1, 2]\n    auto_pad = ""SAME_UPPER""\n\n    input_shape = [10, 10, 7, 7]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       auto_pad=auto_pad,\n                       pooling_type=""AVG"")\n\n  def test_average_pool_3d(self):\n    kernel_shape = [3, 3, 3]\n    strides = [2, 2, 2]\n\n    input_shape = [10, 3, 23, 23, 23]\n    self._test_pooling(input_shape=input_shape,\n                       kernel_shape=kernel_shape,\n                       strides=strides,\n                       pooling_type=""AVG"")\n\n  def test_mean_variance_normalization(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t have test for MeanVarianceNormalization"".\n          format(defs.onnx_opset_version()))\n\n    input_data = self._get_rnd_float32(shape=[2, 2, 2, 2])\n    # Calculate expected output data using formula:\n    # (Input - Mean)/SD\n    mean = np.mean(input_data, keepdims=1, axis=(0, 2, 3))\n    std = np.std(input_data, keepdims=1, axis=(0, 2, 3))\n    expected_output = (input_data - mean) / std\n    # Testing without ""axes"" argument should default to axes=[0,2,3]\n    node_def = helper.make_node(""MeanVarianceNormalization"", [""X""], [""Y""])\n    output = run_node(node_def, [input_data])\n    np.testing.assert_almost_equal(output[""Y""], expected_output, decimal=5)\n\n  def test_min(self):\n    node_def = helper.make_node(""Min"", [""X1"", ""X2"", ""X3"", ""X4""], [""Z""])\n    x1 = self._get_rnd_float32(shape=[10, 10])\n    x2 = self._get_rnd_float32(shape=[10, 10])\n    x3 = self._get_rnd_float32(shape=[10, 10])\n    x4 = self._get_rnd_float32(shape=[10, 10])\n    output = run_node(node_def, [x1, x2, x3, x4])\n    test_output = np.minimum(np.minimum(np.minimum(x1, x2), x3), x4)\n    np.testing.assert_almost_equal(output[""Z""], test_output)\n\n  def test_mul(self):\n    node_def = helper.make_node(""Mul"", [""X"", ""Y""], [""Z""])\n    x = self._get_rnd_float32(shape=[5, 10, 5, 5])\n    y = self._get_rnd_float32(shape=[10, 1, 1])\n    output = run_node(node_def, [x, y])\n    np.testing.assert_almost_equal(output[""Z""],\n                                   np.multiply(x, y.reshape([1, 10, 1, 1])))\n\n  def test_mod(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Mod."".format(\n          defs.onnx_opset_version()))\n    x = self._get_rnd_float32(shape=[5, 5])\n    y = self._get_rnd_float32(shape=[5, 5])\n    node_def = helper.make_node(""Mod"", [""X"", ""Y""], [""Z""], fmod=0)\n    output = run_node(node_def, [x, y])\n    np.testing.assert_almost_equal(output[""Z""], np.mod(x, y))\n    node_def = helper.make_node(""Mod"", [""X"", ""Y""], [""Z""], fmod=1)\n    output = run_node(node_def, [x, y])\n    np.testing.assert_almost_equal(output[""Z""], np.fmod(x, y))\n\n  def test_neg(self):\n    node_def = helper.make_node(""Neg"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.negative(x))\n\n  def test_non_zero(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support NonZero."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""NonZero"", [""x""], [""y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    y = np.array(np.nonzero(x))\n    output = run_node(node_def, [x])\n    np.testing.assert_equal(output[""y""], y)\n\n  def test_onehot(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support OneHot."".format(\n          defs.onnx_opset_version()))\n    indices = np.array([[0, 2], [1, 2], [0, 1]])\n    depth = np.int32(5)\n    on_value = 6.0\n    off_value = 2.0\n    values = np.array([off_value, on_value])\n    node_def = helper.make_node(\'OneHot\',\n                                inputs=[\'indices\', \'depth\', \'values\'],\n                                outputs=[\'y\'],\n                                axis=-1)\n    y = (np.arange(depth) == indices[..., None]).astype(int)\n    y = y * (on_value - off_value) + off_value\n    output = run_node(node_def, inputs=[indices, depth, values])\n    np.testing.assert_equal(output[\'y\'], y)\n\n  def test_range(self):\n    if legacy_opset_pre_ver(11):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Range."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Range"", [\'start\', \'limit\', \'delta\'], [\'y\'])\n    # test positive_delta\n    start = self._get_rnd_int(low=0, high=3)\n    limit = self._get_rnd_int(low=10, high=30)\n    delta = np.int32(3)\n    output = run_node(node_def, [start, limit, delta])\n    np.testing.assert_equal(output[\'y\'], range(start, limit, delta))\n    # test negative_delta\n    start = self._get_rnd_int(low=20, high=30)\n    limit = self._get_rnd_int(low=1, high=5)\n    delta = np.int32(-2)\n    output = run_node(node_def, [start, limit, delta])\n    np.testing.assert_equal(output[\'y\'], range(start, limit, delta))\n\n  def test_round(self):\n    if legacy_opset_pre_ver(11):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Round."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Round"", [""X""], [""Y""])\n    x = self._get_rnd_float32(-20.0, 20.0, shape=[1000])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.round(x))\n\n  def test_qLinearMatMul(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support QLinearMatMul."".format(\n              defs.onnx_opset_version()))\n\n    def qLinearMatMul(a, a_scale, a_zero_point, b, b_scale, b_zero_point,\n                      y_scale, y_zero_point):\n      y_dtype = y_zero_point.dtype\n      # reshape 1-D a_scale, a_zero_point, y_scale and\n      # y_zero_point so it can broadcast in arithmetic\n      # operations later\n      a_scale_shape = a_scale.shape\n      if a_scale_shape and a_scale_shape[0] > 1:\n        a_scale = np.reshape(a_scale, [a_scale_shape[0], 1])\n        a_zero_point = np.reshape(a_zero_point, [a_scale_shape[0], 1])\n      y_scale_shape = y_scale.shape\n      if y_scale_shape and y_scale_shape[0] > 1:\n        y_scale = np.reshape(y_scale, [y_scale_shape[0], 1])\n        y_zero_point = np.reshape(y_zero_point, [y_scale_shape[0], 1])\n      # cast everything to float32\n      a = a.astype(np.float32)\n      a_zero_point = a_zero_point.astype(np.float32)\n      b = b.astype(np.float32)\n      b_zero_point = b_zero_point.astype(np.float32)\n      y_zero_point = y_zero_point.astype(np.float32)\n      # dequantize a and b\n      dequantized_a = np.subtract(a, a_zero_point)\n      dequantized_a = np.multiply(dequantized_a, a_scale)\n      dequantized_b = np.subtract(b, b_zero_point)\n      dequantized_b = np.multiply(dequantized_b, b_scale)\n      # matmul a and b\n      x = np.matmul(dequantized_a, dequantized_b)\n      # quantize x\n      y = np.divide(x, y_scale)\n      y = np.round(y)\n      y = np.add(y, y_zero_point)\n      y = np.clip(y, np.iinfo(y_dtype).min, np.iinfo(y_dtype).max)\n      y = y.astype(y_dtype)\n      return y\n\n    node_def = helper.make_node(\'QLinearMatMul\', [\n        \'a\', \'a_scale\', \'a_zero_point\', \'b\', \'b_scale\', \'b_zero_point\',\n        \'y_scale\', \'y_zero_point\'\n    ], [\'y\'])\n    for dtype in [np.int8, np.uint8]:\n      low = np.iinfo(dtype).min\n      high = np.iinfo(dtype).max\n      a = self._get_rnd_int(low, high, [3, 4, 5, 6], dtype)\n      a_scale = self._get_rnd_float32(-0.005, 0.005, [5])\n      a_zero_point = self._get_rnd_int(low, high, [5], dtype)\n      b = self._get_rnd_int(low, high, [3, 4, 6, 2], dtype)\n      b_scale = self._get_rnd_float32(-0.005, 0.005, [2])\n      b_zero_point = self._get_rnd_int(low, high, [2], dtype)\n      y_scale = self._get_rnd_float32(-0.05, 0.05, [5])\n      y_zero_point = self._get_rnd_int(low, high, [5], dtype)\n      y = qLinearMatMul(a, a_scale, a_zero_point, b, b_scale, b_zero_point,\n                        y_scale, y_zero_point)\n      output = run_node(node_def, [\n          a, a_scale, a_zero_point, b, b_scale, b_zero_point, y_scale,\n          y_zero_point\n      ])\n      np.testing.assert_almost_equal(output[\'y\'], y)\n\n  def test_relu(self):\n    node_def = helper.make_node(""Relu"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.maximum(x, 0))\n\n  def test_pad(self):\n    x = self._get_rnd_float32(shape=[100, 100])\n    if legacy_opset_pre_ver(11):  # for opset = 1 or 2\n      # mode = constant\n      node_def = helper.make_node(""Pad"", [""X""], [""Y""],\n                                  mode=""constant"",\n                                  pads=[1, 1, 1, 1],\n                                  value=2.0)\n      output = run_node(node_def, [x])\n      y = np.pad(x, ((1, 1), (1, 1)), \'constant\', constant_values=(2, 2))\n      np.testing.assert_almost_equal(output[""Y""], y)\n      # mode = reflect and edge\n      for mode in [\'edge\', \'reflect\']:\n        node_def = helper.make_node(""Pad"", [""X""], [""Y""],\n                                    mode=mode,\n                                    pads=[1, 1, 1, 1])\n        output = run_node(node_def, [x])\n        y = np.pad(x, ((1, 1), (1, 1)), mode)\n        np.testing.assert_almost_equal(output[""Y""], y)\n    else:  # for opset = 11\n      # mode = constant\n      node_def = helper.make_node(""Pad"", [""X"", ""pads"", ""constant_values""],\n                                  [""Y""],\n                                  mode=""constant"")\n      pads = np.array([1, 1, 1, 1], dtype=np.int64)\n      constant_values = 2.0\n      output = run_node(node_def, [x, pads, constant_values])\n      y = np.pad(x, ((1, 1), (1, 1)), \'constant\', constant_values=(2, 2))\n      np.testing.assert_almost_equal(output[""Y""], y)\n      # mode = reflect and edge\n      for mode in [\'edge\', \'reflect\']:\n        node_def = helper.make_node(""Pad"", [""X"", ""pads""], [""Y""], mode=mode)\n        output = run_node(node_def, [x, pads])\n        y = np.pad(x, ((1, 1), (1, 1)), mode)\n        np.testing.assert_almost_equal(output[""Y""], y)\n\n  def test_qlinearconv(self):\n    if legacy_opset_pre_ver(10):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support QLinearConv."".format(\n              defs.onnx_opset_version()))\n\n    # Test w_scale and w_zero_point as scalar\n    node_def = helper.make_node(""QLinearConv"",\n                                inputs=[\n                                    ""x"", ""x_scale"", ""x_zero_point"", ""w"",\n                                    ""w_scale"", ""w_zero_point"", ""y_scale"",\n                                    ""y_zero_point""\n                                ],\n                                outputs=[""Y""])\n    x = np.array([\n        [255, 174, 162, 25, 203, 168, 58],\n        [15, 59, 237, 95, 129, 0, 64],\n        [56, 242, 153, 221, 168, 12, 166],\n        [232, 178, 186, 195, 237, 162, 237],\n        [188, 39, 124, 77, 80, 102, 43],\n        [127, 230, 21, 83, 41, 40, 134],\n        [255, 154, 92, 141, 42, 148, 247],\n    ],\n                 dtype=np.uint8).reshape((1, 1, 7, 7))\n    x_scale = np.float32(0.00369204697)\n    x_zero_point = np.uint8(132)\n\n    w = np.array([0], dtype=np.uint8).reshape((1, 1, 1, 1))\n    w_scale = np.float32(0.00172794575)\n    w_zero_point = np.uint8(255)\n\n    y = np.array([\n        [0, 81, 93, 230, 52, 87, 197],\n        [240, 196, 18, 160, 126, 255, 191],\n        [199, 13, 102, 34, 87, 243, 89],\n        [23, 77, 69, 60, 18, 93, 18],\n        [67, 216, 131, 178, 175, 153, 212],\n        [128, 25, 234, 172, 214, 215, 121],\n        [0, 101, 163, 114, 213, 107, 8],\n    ],\n                 dtype=np.uint8).reshape((1, 1, 7, 7))\n    y_scale = np.float32(0.00162681262)\n    y_zero_point = np.uint8(123)\n\n    output = run_node(node_def, [\n        x, x_scale, x_zero_point, w, w_scale, w_zero_point, y_scale,\n        y_zero_point\n    ])\n    np.testing.assert_almost_equal(output[""Y""], y)\n\n  def test_quantize_linear(self):\n    node_def = helper.make_node(""QuantizeLinear"",\n                                [""x"", ""y_scale"", ""y_zero_point""], [""y""])\n    for x in [\n        self._get_rnd_float32(-512., 512., [2, 6]),\n        self._get_rnd_int(-512, 512, [2, 6])\n    ]:\n      y_scale = self._get_rnd_float32(-10., 10.)\n      for y_zero_point in [\n          self._get_rnd_int(-128, 127, dtype=np.int8),\n          self._get_rnd_int(0, 255, dtype=np.uint8)\n      ]:\n        y = np.divide(x, y_scale)\n        y = np.round(y)\n        y = np.add(y, y_zero_point)\n        if y_zero_point.dtype.type is np.int8:\n          y = np.clip(y, -128, 127).astype(np.int8)\n        else:\n          y = np.clip(y, 0, 255).astype(np.uint8)\n        output = run_node(node_def, [x, y_scale, y_zero_point])\n        np.testing.assert_almost_equal(output[""y""], y)\n\n  def test_reciprocal(self):\n    node_def = helper.make_node(""Reciprocal"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], 1.0 / x)\n\n  def test_reduce_l1(self):\n    node_def = helper.make_node(""ReduceL1"", [""X""], [""Y""], axes=[1, 2])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""],\n                                   np.linalg.norm(x, 1, (1, 2), True))\n\n  def test_reduce_log_sum_exp(self):\n    node_def = helper.make_node(""ReduceLogSumExp"", [""X""], [""Y""], axes=[1, 2])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_allclose(output[""Y""],\n                               np.log(\n                                   np.sum(np.exp(x), axis=(1, 2),\n                                          keepdims=True)),\n                               rtol=1e-3)\n\n  def test_reduce_max(self):\n    node_def = helper.make_node(""ReduceMax"", [""X""], [""Y""], axes=[1, 2])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_allclose(output[""Y""],\n                               np.max(x, (1, 2), keepdims=True),\n                               rtol=1e-3)\n\n  def test_reduce_mean(self):\n    node_def = helper.make_node(""ReduceMean"", [""X""], [""Y""], axes=[1, 2])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_allclose(output[""Y""],\n                               np.mean(x, (1, 2), keepdims=True),\n                               rtol=1e-3)\n\n  def test_reduce_min(self):\n    node_def = helper.make_node(""ReduceMin"", [""X""], [""Y""], axes=[1, 2])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_allclose(output[""Y""],\n                               np.min(x, (1, 2), keepdims=True),\n                               rtol=1e-3)\n\n  def test_reduce_prod(self):\n    node_def = helper.make_node(""ReduceProd"", [""X""], [""Y""], axes=[1, 2])\n    x = self._get_rnd_float32(shape=[1, 5, 5, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_allclose(output[""Y""],\n                               np.prod(x, (1, 2), keepdims=True),\n                               rtol=1e-3)\n\n  def test_reduce_sum(self):\n    node_def = helper.make_node(""ReduceSum"", [""X""], [""Y""], axes=[1, 2])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_allclose(output[""Y""],\n                               np.sum(x, (1, 2), keepdims=True),\n                               rtol=1e-3)\n\n  def test_reduce_sum_square(self):\n    node_def = helper.make_node(""ReduceSumSquare"", [""X""], [""Y""], axes=[1, 2])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_allclose(output[""Y""],\n                               np.sum(np.square(x), (1, 2), keepdims=True),\n                               rtol=1e-3)\n\n  def test_pow(self):\n    node_def = helper.make_node(""Pow"", [""X"", ""Y""], [""Z""])\n    x = self._get_rnd_float32(shape=1000) / 2.0 + 0.5\n    y = self._get_rnd_float32(shape=1000) / 2.0 + 0.5\n    output = run_node(node_def, [x, y])\n    np.testing.assert_almost_equal(output[""Z""], np.power(x, y))\n\n  def test_reshape(self):\n    x = self._get_rnd_float32(shape=100)\n    shape = [10, 10]\n    if defs.onnx_opset_version() < 5:\n      node_def = helper.make_node(""Reshape"", [""X""], [""Z""], shape=shape)\n      output = run_node(node_def, [x])\n    else:\n      node_def = helper.make_node(""Reshape"", [""X"", ""Y""], [""Z""])\n      output = run_node(node_def, [x, shape])\n\n    np.testing.assert_almost_equal(output[""Z""], x.reshape([10, 10]))\n\n  def test_reshape_with_copy(self):\n    x = self._get_rnd_float32(shape=[10, 20 * 30])\n    shape = [0, 20, 30]\n    if defs.onnx_opset_version() < 5:\n      node_def = helper.make_node(""Reshape"", [""X""], [""Z""], shape=shape)\n      output = run_node(node_def, [x])\n    else:\n      node_def = helper.make_node(""Reshape"", [""X"", ""Y""], [""Z""])\n      output = run_node(node_def, [x, shape])\n\n    np.testing.assert_almost_equal(output[""Z""], x.reshape([10, 20, 30]))\n\n  def test_selu(self):\n    node_def = helper.make_node(""Selu"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000])\n    output = run_node(node_def, [x])\n    alpha = 1.6732\n    gamma = 1.0507\n    x[x <= 0] = gamma * (alpha * np.exp(x[x <= 0]) - alpha)\n    x[x > 0] = gamma * x[x > 0]\n    np.testing.assert_allclose(output[""Y""], x, rtol=1e-3, atol=1e-7)\n\n  def _run_scan_node(self,\n                     initial,\n                     x1,\n                     x2,\n                     input_shape,\n                     output_shape,\n                     scan_input_axes=None,\n                     scan_input_directions=None,\n                     scan_output_axes=None,\n                     scan_output_directions=None,\n                     sequence_lens=None,\n                     directions=None):\n    """"""\n      Subgraph looks like this.\n\n      [const1]       state_in                    concat1_in    concat2_in_\n            \\           |                                 \\     /\n             \\--------[Add]                               [Concat]\n                        |                                    |\n                        |                                 concat_out\n                        |                                    |\n                        |                                  [Add]----------[const1]\n                        |                                    |\n                        |                                add_out_1\n                        |                                    |\n                        |                                 [Split]\n                        |                               /  |   |   \\\n                   state_out                  split1_out    ...     split4_out\n    """"""\n    val_1 = helper.make_tensor(\n        name=\'const_tensor\',\n        data_type=TensorProto.FLOAT,\n        dims=[1],\n        vals=[1],\n    )\n    constant_node = helper.make_node(""Constant"", [], [""const_1""], value=val_1)\n    state_add_node = helper.make_node(""Add"", [""state_in"", ""const_1""],\n                                      [""state_out""])\n    concat_node = helper.make_node(""Concat"", [""concat1_in"", ""concat2_in""],\n                                   [""concat_out""],\n                                   axis=0)\n    add_node = helper.make_node(""Add"", [""concat_out"", ""const_1""], [""add_out""])\n    split_node = helper.make_node(\n        ""Split"", [""add_out""],\n        [""split1_out"", ""split2_out"", ""split3_out"", ""split4_out""])\n\n    state_in = helper.make_tensor_value_info(\'state_in\', TensorProto.FLOAT, [1])\n    concat1_in = helper.make_tensor_value_info(\'concat1_in\', TensorProto.FLOAT,\n                                               input_shape)\n    concat2_in = helper.make_tensor_value_info(\'concat2_in\', TensorProto.FLOAT,\n                                               input_shape)\n    state_out = helper.make_tensor_value_info(\'state_out\', TensorProto.FLOAT,\n                                              [1])\n    split1_out = helper.make_tensor_value_info(\'split1_out\', TensorProto.FLOAT,\n                                               output_shape)\n    split2_out = helper.make_tensor_value_info(\'split2_out\', TensorProto.FLOAT,\n                                               output_shape)\n    split3_out = helper.make_tensor_value_info(\'split3_out\', TensorProto.FLOAT,\n                                               output_shape)\n    split4_out = helper.make_tensor_value_info(\'split4_out\', TensorProto.FLOAT,\n                                               output_shape)\n\n    scan_body = helper.make_graph(\n        [constant_node, state_add_node, concat_node, add_node, split_node],\n        ""scan_body"",\n        [state_in, concat1_in, concat2_in],\n        [state_out, split1_out, split2_out, split3_out, split4_out],\n    )\n\n    node_kwargs = {\n        ""op_type"": ""Scan"",\n        ""inputs"": [""initial"", ""x1"", ""x2""],\n        ""outputs"": [""y"", ""z1"", ""z2"", ""z3"", ""z4""],\n        ""num_scan_inputs"": 2,\n        ""body"": scan_body\n    }\n    if sequence_lens is not None:\n      node_kwargs[""inputs""] = ["""" if sequence_lens is str else ""seq_lens""\n                              ] + node_kwargs[""inputs""]\n\n    if scan_input_axes is not None:\n      node_kwargs[""scan_input_axes""] = scan_input_axes\n    if scan_input_directions is not None:\n      node_kwargs[""scan_input_directions""] = scan_input_directions\n    if scan_output_axes is not None:\n      node_kwargs[""scan_output_axes""] = scan_output_axes\n    if scan_output_directions is not None:\n      node_kwargs[""scan_output_directions""] = scan_output_directions\n    if directions is not None:\n      node_kwargs[""directions""] = directions\n\n    scan_node = helper.make_node(**node_kwargs)\n\n    if sequence_lens is None:\n      inputs = [initial, x1, x2]\n    else:\n      inputs = [sequence_lens, initial, x1, x2]\n\n    return run_node(scan_node, inputs)\n\n  def test_scan_v8(self):\n    if legacy_opset_pre_ver(8) or not legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} not supported."".format(\n          defs.onnx_opset_version()))\n\n    initial = self._get_rnd_int(0, 100, shape=[5, 1]).astype(np.float32)\n    x1 = self._get_rnd_float32(0, 1000, shape=[5, 20, 6, 2])\n    x2 = self._get_rnd_float32(0, 1000, shape=[5, 20, 6, 2])\n\n    directions = [0, 1]\n    sequence_lens = np.array([15, 20, 14, 18, 20]).astype(np.int32)\n\n    Y = initial + (np.shape(x1)[1] if sequence_lens is str else \\\n                  np.reshape(sequence_lens,[-1, 1]))\n    x1_out = x1 + 1\n    # left-right flip x2 (reverse direction)\n    x2_out = x2[:, ::-1] + 1\n\n    Z = np.concatenate([x1_out, x2_out], 2)\n    if sequence_lens is not str:\n      for batch in range(len(sequence_lens)):\n        # zero pad from the sequence_lens\n        shape = list(np.shape(Z[batch]))\n        seq_len = sequence_lens[batch]\n\n        zero_pad = np.zeros([shape[0] - seq_len] + shape[1:])\n        Z[batch] = np.concatenate([Z[batch][:seq_len], zero_pad])\n\n    output = self._run_scan_node(initial,\n                                 x1,\n                                 x2, [6, 4], [3, 2],\n                                 sequence_lens=sequence_lens,\n                                 directions=directions)\n    output_z = np.concatenate(\n        [output[""z1""], output[""z2""], output[""z3""], output[""z4""]], 2)\n\n    np.testing.assert_almost_equal(output[""y""], Y)\n    np.testing.assert_almost_equal(output_z, Z)\n\n  def test_scan(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} not supported."".format(\n          defs.onnx_opset_version()))\n\n    initial = self._get_rnd_int(0, 100, shape=[2]).astype(np.float32)\n    x1 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n    x2 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n\n    Y = initial + np.shape(x1)[0]\n    Z = np.concatenate([x1, x2], 1) + 1\n\n    output = self._run_scan_node(initial, x1, x2, [6, 2], [3, 2])\n    output_z = np.concatenate(\n        [output[""z1""], output[""z2""], output[""z3""], output[""z4""]], 1)\n\n    np.testing.assert_almost_equal(output[""y""], Y)\n    np.testing.assert_almost_equal(output_z, Z)\n\n  def test_scan_input_directions(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} not supported."".format(\n          defs.onnx_opset_version()))\n\n    initial = self._get_rnd_int(0, 100, shape=[1]).astype(np.float32)\n    x1 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n    x2 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n\n    Y = initial + np.shape(x1)[0]\n    Z = np.concatenate([x1[::-1], x2], 1) + 1\n\n    output = self._run_scan_node(initial,\n                                 x1,\n                                 x2, [6, 2], [3, 2],\n                                 scan_input_directions=[1, 0])\n    output_z = np.concatenate(\n        [output[""z1""], output[""z2""], output[""z3""], output[""z4""]], 1)\n\n    np.testing.assert_almost_equal(output[""y""], Y)\n    np.testing.assert_almost_equal(output_z, Z)\n\n  def test_scan_input_axes(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} not supported."".format(\n          defs.onnx_opset_version()))\n\n    initial = self._get_rnd_int(0, 100, shape=[1]).astype(np.float32)\n    x1 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n    x2 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n\n    Y = initial + np.shape(x1)[1]\n    x1_transpose = np.transpose(x1, (1, 0, 2))\n    x2_transpose = np.transpose(x2, (1, 0, 2))\n    Z = np.concatenate([x1_transpose, x2_transpose], 1) + 1\n\n    output = self._run_scan_node(initial,\n                                 x1,\n                                 x2, [3, 2], [10, 2],\n                                 scan_input_axes=[1, 1])\n    output_z = np.concatenate(\n        [output[""z1""], output[""z2""], output[""z3""], output[""z4""]], 1)\n\n    np.testing.assert_almost_equal(output[""y""], Y)\n    np.testing.assert_almost_equal(output_z, Z)\n\n  def test_scan_output_directions(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} not supported."".format(\n          defs.onnx_opset_version()))\n\n    initial = self._get_rnd_int(0, 100, shape=[1]).astype(np.float32)\n    x1 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n    x2 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n\n    Y = initial + np.shape(x1)[0]\n    Z = np.concatenate([x1, x2], 1) + 1\n\n    output = self._run_scan_node(initial,\n                                 x1,\n                                 x2, [6, 2], [3, 2],\n                                 scan_output_directions=[1, 0, 0, 1])\n    output_z = np.concatenate(\n        [output[""z1""][::-1], output[""z2""], output[""z3""], output[""z4""][::-1]], 1)\n\n    np.testing.assert_almost_equal(output[""y""], Y)\n    np.testing.assert_almost_equal(output_z, Z)\n\n  def test_scan_output_axes(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} not supported."".format(\n          defs.onnx_opset_version()))\n\n    initial = self._get_rnd_int(0, 100, shape=[1]).astype(np.float32)\n    x1 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n    x2 = self._get_rnd_float32(0, 1000, shape=[20, 6, 2])\n\n    Y = initial + np.shape(x1)[0]\n    Z = np.concatenate([x1, x2], 1) + 1\n    Z = np.transpose(Z, (1, 0, 2))\n\n    output = self._run_scan_node(initial,\n                                 x1,\n                                 x2, [10, 2], [3, 2],\n                                 scan_output_axes=[1, 1, 1, 1])\n    output_z = np.concatenate(\n        [output[""z1""], output[""z2""], output[""z3""], output[""z4""]], 0)\n\n    np.testing.assert_almost_equal(output[""y""], Y)\n    np.testing.assert_almost_equal(output_z, Z)\n\n  def test_scatter_elements1(self):\n    data = np.array([[1.0, 2.0, 3.0, 4.0, 5.0]], dtype=np.float32)\n    indices = np.array([[1, 3]], dtype=np.int64)\n    updates = np.array([[1.1, 2.1]], dtype=np.float32)\n    axis = 1\n    ref_output = np.array([[1.0, 1.1, 3.0, 2.1, 5.0]], dtype=np.float32)\n\n    if legacy_opset_pre_ver(11):\n      node_def = helper.make_node(""Scatter"", [""data"", ""indices"", ""updates""],\n                                  [""outputs""],\n                                  axis=axis)\n      output = run_node(node_def, [data, indices, updates])\n      np.testing.assert_almost_equal(output[""outputs""], ref_output)\n    else:\n      node_def = helper.make_node(""ScatterElements"",\n                                  [""data"", ""indices"", ""updates""], [""outputs""],\n                                  axis=axis)\n      output = run_node(node_def, [data, indices, updates])\n      np.testing.assert_almost_equal(output[""outputs""], ref_output)\n\n  def test_scatter_elements2(self):\n    data = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n    ],\n                    dtype=np.float32)\n    indices = np.array([\n        [1, 0, 2],\n        [0, 2, 1],\n    ], dtype=np.int64)\n    updates = np.array([\n        [1.0, 1.1, 1.2],\n        [2.0, 2.1, 2.2],\n    ], dtype=np.float32)\n    ref_output = np.array([\n        [2.0, 1.1, 0.0],\n        [1.0, 0.0, 2.2],\n        [0.0, 2.1, 1.2],\n    ],\n                          dtype=np.float32)\n\n    if legacy_opset_pre_ver(11):\n      node_def = helper.make_node(""Scatter"", [""data"", ""indices"", ""updates""],\n                                  [""outputs""])\n      output = run_node(node_def, [data, indices, updates])\n      np.testing.assert_almost_equal(output[""outputs""], ref_output)\n    else:\n      node_def = helper.make_node(""ScatterElements"",\n                                  [""data"", ""indices"", ""updates""], [""outputs""])\n      output = run_node(node_def, [data, indices, updates])\n      np.testing.assert_almost_equal(output[""outputs""], ref_output)\n\n  def test_scatter_elements3(self):\n    # indices out of bounds\n    data = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=np.float32)\n    indices = np.array([[0, 1, 2]], dtype=np.int64)\n    updates = np.array([[1.1, 2.1, 3.1]], dtype=np.float32)\n\n    if legacy_opset_pre_ver(11):\n      node_def = helper.make_node(""Scatter"", [""data"", ""indices"", ""updates""],\n                                  [""outputs""])\n    else:\n      node_def = helper.make_node(""ScatterElements"",\n                                  [""data"", ""indices"", ""updates""], [""outputs""])\n    with np.testing.assert_raises(tf.errors.InvalidArgumentError):\n      output = run_node(node_def, [data, indices, updates])\n\n  def test_scatter_nd(self):\n    if legacy_opset_pre_ver(11):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support ScatterND."".format(\n              defs.onnx_opset_version()))\n\n    # valid positve and negative indices for elements\n    data = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=np.float32)\n    indices = np.array([[4], [3], [1], [7]], dtype=np.int64)\n    updates = np.array([9, 10, 11, 12], dtype=np.float32)\n    ref_output = np.array([1, 11, 3, 10, 9, 6, 7, 12], dtype=np.float32)\n    node_def = helper.make_node(""ScatterND"", [""data"", ""indices"", ""updates""],\n                                [""outputs""])\n    output = run_node(node_def, [data, indices, updates])\n    np.testing.assert_almost_equal(output[""outputs""], ref_output)\n\n    # valid positive and negative indices for slices\n    data = np.reshape(np.arange(1, 25, dtype=np.float32), [2, 3, 4])\n    indices = np.array([[-2, -1], [1, 0]], dtype=np.int64)\n    updates = np.array([[39, 40, 41, 42], [43, 44, 45, 46]], dtype=np.float32)\n    ref_output = np.array(\n        [[[1, 2, 3, 4], [5, 6, 7, 8], [39, 40, 41, 42]],\n         [[43, 44, 45, 46], [17, 18, 19, 20], [21, 22, 23, 24]]],\n        dtype=np.float32)\n    output = run_node(node_def, [data, indices, updates])\n    np.testing.assert_almost_equal(output[""outputs""], ref_output)\n    indices = np.array([[-1]], dtype=np.int64)\n    updates = np.array([[[43, 44, 45, 46], [47, 48, 49, 50], [51, 52, 53, 54]]],\n                       dtype=np.float32)\n    ref_output = np.array(\n        [[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n         [[43, 44, 45, 46], [47, 48, 49, 50], [51, 52, 53, 54]]],\n        dtype=np.float32)\n    output = run_node(node_def, [data, indices, updates])\n    np.testing.assert_almost_equal(output[""outputs""], ref_output)\n\n    # indices out of bounds\n    indices = np.array([[0, 1, 2], [-1, -1, -3], [-2, -3, -4], [0, 2, -5]],\n                       dtype=np.int64)\n    updates = np.array([37, 52, 30, 39], dtype=np.float32)\n    with np.testing.assert_raises(tf.errors.InvalidArgumentError):\n      output = run_node(node_def, [data, indices, updates])\n    indices = np.array([[0, 1], [-1, -1], [-2, -4]], dtype=np.int64)\n    updates = np.array([[35, 36, 37, 38], [51, 52, 53, 54], [31, 32, 33, 34]],\n                       dtype=np.float32)\n    with np.testing.assert_raises(tf.errors.InvalidArgumentError):\n      output = run_node(node_def, [data, indices, updates])\n\n  def test_shape(self):\n    node_def = helper.make_node(""Shape"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_allclose(output[""Y""], np.shape(x))\n\n  def test_shrink(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Shrink."".format(\n          defs.onnx_opset_version()))\n\n    node_def = helper.make_node(""Shrink"", [""X""], [""Y""], bias=1.5, lambd=1.5)\n\n    X = np.arange(-2.0, 2.1, dtype=np.float32)\n    Y = np.array([-0.5, 0, 0, 0, 0.5], dtype=np.float32)\n    output = run_node(node_def, [X])\n    np.testing.assert_almost_equal(output[""Y""], Y)\n\n  def test_sigmoid(self):\n    node_def = helper.make_node(""Sigmoid"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], 1 / (1 + np.exp(-x)))\n\n  def test_sign(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Sign."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Sign"", [""X""], [""Y""])\n    x = self._get_rnd_float32(-10, 10, [3, 5])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.sign(x))\n\n  def test_sinh(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Sinh."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Sinh"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.sinh(x))\n\n  def test_size(self):\n    node_def = helper.make_node(""Size"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[5, 10, 10, 3])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.size(x))\n\n  def test_slice(self):\n    # test case 1 with normal inputs\n    axes = [0, 1, 2]\n    starts = [0, 0, 0]\n    ends = [2, 2, 2]\n    steps = [1, 1, 1]\n\n    if legacy_opset_pre_ver(10):\n      node_def = helper.make_node(""Slice"", [""X""], [""S""],\n                                  axes=axes,\n                                  starts=starts,\n                                  ends=ends)\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = run_node(node_def, [x])\n      np.testing.assert_almost_equal(output[""S""], x[0:2, 0:2, 0:2])\n    else:\n      node_def = helper.make_node(""Slice"",\n                                  [""X"", ""starts"", ""ends"", ""axes"", ""steps""],\n                                  [""S""])\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = run_node(node_def, [x, starts, ends, axes, steps])\n      np.testing.assert_almost_equal(output[""S""], x[0:2, 0:2, 0:2])\n\n    # test case 2 with negative, out-of-bound and default inputs\n    axes = [0, 2]\n    starts = [0, -7]\n    ends = [-8, 20]\n\n    if legacy_opset_pre_ver(10):\n      node_def = helper.make_node(""Slice"", [""X""], [""S""],\n                                  axes=axes,\n                                  starts=starts,\n                                  ends=ends)\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = run_node(node_def, [x])\n      np.testing.assert_almost_equal(output[""S""], x[0:-8, :, -7:20])\n    else:\n      node_def = helper.make_node(""Slice"", [""X"", ""starts"", ""ends"", ""axes""],\n                                  [""S""])\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = run_node(node_def, [x, starts, ends, axes])\n      np.testing.assert_almost_equal(output[""S""], x[0:-8, :, -7:20])\n\n    # test case 3 with non-default steps\n    axes = [0, 1, 2]\n    starts = [0, 0, 0]\n    ends = [2, 2, 2]\n    steps = [2, -2, -1]\n\n    if legacy_opset_pre_ver(10) == False:\n      node_def = helper.make_node(""Slice"",\n                                  [""X"", ""starts"", ""ends"", ""axes"", ""steps""],\n                                  [""S""])\n      x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n      output = run_node(node_def, [x, starts, ends, axes, steps])\n      np.testing.assert_almost_equal(output[""S""], x[0:2:2, 0:2:-2, 0:2:-1])\n\n  def test_softplus(self):\n    node_def = helper.make_node(""Softplus"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.log(np.exp(x) + 1))\n\n  def test_softsign(self):\n    node_def = helper.make_node(""Softsign"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[3, 4, 5])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], x / (1 + np.abs(x)))\n\n  def test_space_to_depth(self):\n    node_def = helper.make_node(""SpaceToDepth"", [""X""], [""Y""], blocksize=2)\n    x_shape = [1, 3, 2, 2]\n    x = self._get_rnd_float32(shape=x_shape)\n    output = run_node(node_def, [x])\n    x = np.transpose(x, (0, 2, 3, 1))\n    y = np.reshape(np.swapaxes(x.reshape(1, 1, 1, 1, 1, 12), 2, 3),\n                   (1, 1, 1, 12))\n    y = np.transpose(y, (0, 3, 1, 2))\n    np.testing.assert_allclose(output[""Y""], y, rtol=1e-3)\n\n  def test_split(self):\n    split = [3, 3, 4]\n    node_def = helper.make_node(""Split"", [""X""],\n                                [""Z%i"" % i for i in range(len(split))],\n                                axis=0,\n                                split=split)\n    x = self._get_rnd_float32(shape=[100]).reshape([10, 10])\n\n    output = run_node(node_def, [x])\n    for a, b in zip(list(output), np.split(x, np.cumsum(split))[:-1]):\n      np.testing.assert_almost_equal(a, b)\n\n    # test axis out of bound\n    node_def = helper.make_node(""Split"", [""X""],\n                                [""Z%i"" % i for i in range(len(split))],\n                                axis=3,\n                                split=split)\n    with np.testing.assert_raises(ValueError):\n      output = run_node(node_def, [x])\n\n  def test_sqrt(self):\n    node_def = helper.make_node(""Sqrt"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000]) + 1.0\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.sqrt(x), decimal=5)\n\n  def test_squeeze(self):\n    node_def = helper.make_node(""Squeeze"", [""X""], [""Y""], axes=[2])\n    x = np.array([[[0], [1], [2]]])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.squeeze(x, axis=2))\n\n  def test_sub(self):\n    node_def = helper.make_node(""Sub"", [""X"", ""Y""], [""Z""])\n    x = self._get_rnd_float32(shape=[10, 10])\n    y = self._get_rnd_float32(shape=[10, 10])\n    output = run_node(node_def, [x, y])\n    np.testing.assert_almost_equal(output[""Z""], np.subtract(x, y))\n\n  def test_sum(self):\n    node_def = helper.make_node(""Sum"", [""X1"", ""X2"", ""X3"", ""X4""], [""Z""])\n    x1 = self._get_rnd_float32(shape=[10, 10])\n    x2 = self._get_rnd_float32(shape=[10, 10])\n    x3 = self._get_rnd_float32(shape=[10, 10])\n    x4 = self._get_rnd_float32(shape=[10, 10])\n    output = run_node(node_def, [x1, x2, x3, x4])\n    test_output = x1 + x2 + x3 + x4\n    np.testing.assert_almost_equal(output[""Z""], test_output)\n\n  def test_tanh(self):\n    node_def = helper.make_node(""Tanh"", [""X""], [""Y""])\n    x = self._get_rnd_float32(shape=[1000]) + 1.0\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.tanh(x), decimal=5)\n\n  def test_tfidf_vectorizer(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(\n          ""ONNX version {} doesn\'t support TfIdfVectorizer."".format(\n              defs.onnx_opset_version()))\n\n    def run_test_ints():\n      node_def = helper.make_node(""TfIdfVectorizer"", [""X""], [""Y""],\n                                  mode=mode,\n                                  min_gram_length=min_gram_len,\n                                  max_gram_length=max_gram_len,\n                                  max_skip_count=max_skip,\n                                  ngram_counts=ngram_counts,\n                                  ngram_indexes=ngram_indexes,\n                                  weights=weights,\n                                  pool_int64s=pool_int64s)\n      output = run_node(node_def, [x])\n      np.testing.assert_almost_equal(output[""Y""], y)\n\n    def run_test_strings():\n      node_def = helper.make_node(""TfIdfVectorizer"", [""X""], [""Y""],\n                                  mode=mode,\n                                  min_gram_length=min_gram_len,\n                                  max_gram_length=max_gram_len,\n                                  max_skip_count=max_skip,\n                                  ngram_counts=ngram_counts,\n                                  ngram_indexes=ngram_indexes,\n                                  weights=weights,\n                                  pool_strings=pool_strings)\n      output = run_node(node_def, [x])\n      np.testing.assert_almost_equal(output[""Y""], y)\n\n    # test 2d inputs with 3 elements, output contains 1-grams and 2-grams\n    x = np.array([[1, 1, 3, 3, 3, 7], [8, 6, 7, 5, 6, 8], [8, 6, 7, 5, 6,\n                                                           8]]).astype(np.int32)\n    y = np.array([[0., 3., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 1., 0., 1.],\n                  [0., 0., 1., 0., 1., 0., 1.]]).astype(np.float32)\n    ngram_counts = np.array([0, 4]).astype(np.int64)\n    ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n    pool_int64s = np.array([2, 3, 5, 4, 5, 6, 7, 8, 6, 7]).astype(np.int64)\n    min_gram_len = 1\n    max_gram_len = 2\n    max_skip = 0\n    mode = \'TF\'\n    weights = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    run_test_ints()\n\n    # test 1d inputs with indexes in non-default order, max_skip=3, output 2-grams\n    x = np.array([1, 1, 3, 3, 3, 7, 8, 6, 7, 5, 6, 8]).astype(np.int32)\n    y = np.array([0., 1., 0., 1., 0., 0., 2.]).astype(np.float32)\n    ngram_counts = np.array([0, 4]).astype(np.int64)\n    ngram_indexes = np.array([5, 0, 2, 4, 1, 6, 3]).astype(np.int64)\n    pool_int64s = np.array([2, 3, 5, 4, 5, 6, 7, 8, 6, 7]).astype(np.int64)\n    min_gram_len = 2\n    max_gram_len = 2\n    max_skip = 3\n    mode = \'TF\'\n    weights = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    run_test_ints()\n\n    # test IDF mode with weights, max_skip=5, output contains 1-grams and 2-grams\n    x = np.array([[1, 1, 3, 3, 3, 7], [8, 6, 7, 5, 6, 8]]).astype(np.int32)\n    y = np.array([[0., 0.1, 0., 0., 0., 0., 0.],\n                  [0., 0., 0.1, 0., 0.5, 0.5, 0.5]]).astype(np.float32)\n    ngram_counts = np.array([0, 4]).astype(np.int64)\n    ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n    pool_int64s = np.array([2, 3, 5, 4, 5, 6, 7, 8, 6, 7]).astype(np.int64)\n    min_gram_len = 1\n    max_gram_len = 2\n    max_skip = 5\n    mode = \'IDF\'\n    weights = np.array([0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5])\n    run_test_ints()\n\n    # test strings inputs, max_skip=5, output contains 1-grams and 2-grams\n    x = np.array([\'a\', \'a\', \'b\', \'b\', \'b\', \'c\', \'d\', \'e\', \'c\', \'f\', \'e\', \'d\'])\n    y = np.array([0., 3., 1., 0., 1., 3., 1.]).astype(np.float32)\n    ngram_counts = np.array([0, 4]).astype(np.int64)\n    ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n    pool_strings = np.array([\'x\', \'b\', \'f\', \'y\', \'f\', \'e\', \'c\', \'d\', \'e\', \'c\'])\n    min_gram_len = 1\n    max_gram_len = 2\n    max_skip = 5\n    mode = \'TF\'\n    run_test_strings()\n\n  def test_thresholded_relu(self):\n    alpha = 2.0\n    node_def = helper.make_node(""ThresholdedRelu"", [""X""], [""Y""], alpha=alpha)\n    x = self._get_rnd_float32(-3.0, 3.0, [10])\n    y = np.clip(x, alpha, np.inf)\n    y[y == alpha] = 0\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], y)\n\n  def test_tile(self):\n    if legacy_onnx_pre_ver(1, 2):\n      raise unittest.SkipTest(\n          ""The current version of ONNX does not record correctly the opset of Tile.""\n      )\n    node_def = helper.make_node(""Tile"", [""X1"", ""X2""], [""Z""])\n    x = self._get_rnd_float32(shape=[3, 5, 5, 3])\n    repeats = [1, 1, 2, 1]\n    output = run_node(node_def, [x, repeats])\n    np.testing.assert_allclose(output[""Z""], np.tile(x, repeats), rtol=1e-3)\n\n  def test_transpose(self):\n    node_def = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 1])\n    x = self._get_rnd_float32(shape=[1000]).reshape([10, 10, 10])\n    output = run_node(node_def, [x])\n    np.testing.assert_almost_equal(output[""Y""], np.transpose(x, (0, 2, 1)))\n\n  def test_topk(self):\n    x = np.arange(15, dtype=np.float32).reshape(3, 5)\n    values = np.array([[4, 3], [9, 8], [14, 13]], dtype=np.float32)\n    indices = np.array([[4, 3], [4, 3], [4, 3]], dtype=np.int64)\n    if legacy_opset_pre_ver(10):  # for opset = 1\n      node_def = helper.make_node(""TopK"", [""x""], [""values"", ""indices""], k=2)\n      output = run_node(node_def, [x])\n    elif legacy_opset_pre_ver(11):  # for opset = 10\n      k = np.array([2], dtype=np.int64)\n      node_def = helper.make_node(""TopK"", [""x"", ""k""], [""values"", ""indices""])\n      output = run_node(node_def, [x, k])\n    else:  # for opset = 11\n      x = np.array([[3, 2, 5, 10, 7], [12, 15, 10, 7, 20], [21, 16, 5, 3, 6]],\n                   dtype=np.float32)\n      values = np.array([[3, 2], [10, 7], [5, 3]], dtype=np.float32)\n      indices = np.array([[0, 1], [2, 3], [2, 3]], dtype=np.int64)\n      k = np.array([2], dtype=np.int64)\n      node_def = helper.make_node(""TopK"", [""x"", ""k""], [""values"", ""indices""],\n                                  largest=0,\n                                  sorted=0)\n      output = run_node(node_def, [x, k])\n    np.testing.assert_almost_equal(output[""values""], values)\n    np.testing.assert_almost_equal(output[""indices""], indices)\n\n  def test_where(self):\n    if legacy_opset_pre_ver(9):\n      raise unittest.SkipTest(""ONNX version {} doesn\'t support Where."".format(\n          defs.onnx_opset_version()))\n    node_def = helper.make_node(""Where"", [""C"", ""X"", ""Y""], [""Z""])\n    c = np.array([[1, 0], [1, 1]], dtype=np.bool)\n    x = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    y = np.array([[9, 8], [7, 6]], dtype=np.float32)\n    output = run_node(node_def, [c, x, y])\n    np.testing.assert_almost_equal(output[""Z""], np.where(c, x, y))\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
test/backend/test_onnx_backend.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport re\nimport unittest\n\nimport onnx.backend.test\n\nfrom onnx import defs\n\nfrom onnx_tf import opset_version\nfrom onnx_tf.backend import TensorflowBackend\nfrom onnx_tf.common.legacy import legacy_onnx_pre_ver\nfrom onnx_tf.common.legacy import legacy_opset_pre_ver\n\ndef get_onnxtf_supported_ops():\n  return opset_version.backend_opset_version\n\ndef get_onnx_supported_ops():\n  onnx_ops_dict = {}\n  for schema in defs.get_all_schemas():\n    onnx_ops_dict[schema.name] = {\n        \'version\': schema.since_version,\n        \'deprecated\': schema.deprecated\n    }\n  return onnx_ops_dict\n\ndef skip_not_implemented_ops_test(test):\n  onnxtf_ops_list = get_onnxtf_supported_ops()\n  onnx_ops_list = get_onnx_supported_ops()\n  for op in onnx_ops_list:\n    op_name = op\n    i = 1\n    while i < len(op_name):\n      if op_name[i].isupper():\n        op_name = op_name[:i] + \'_\' + op_name[i:]\n        i += 2\n      else:\n        i += 1\n    if not onnx_ops_list[op][\'deprecated\']:\n      if op in onnxtf_ops_list:\n        if onnx_ops_list[op][\'version\'] not in onnxtf_ops_list[op]:\n          test.exclude(r\'test_\' + op.lower() + \'_[a-z,_]*\')\n          test.exclude(r\'test_\' + op_name.lower() + \'_[a-z,_]*\')\n      else:\n        test.exclude(r\'test_\' + op.lower() + \'_[a-z,_]*\')\n        test.exclude(r\'test_\' + op_name.lower() + \'_[a-z,_]*\')\n  return test\n\n# This is a pytest magic variable to load extra plugins\npytest_plugins = \'onnx.backend.test.report\',\n\nbackend_test = onnx.backend.test.BackendTest(TensorflowBackend, __name__)\n\n# exclude tests of not-implemented-ops\nbackend_test = skip_not_implemented_ops_test(backend_test)\n\n# manually exclude tests of not-implemented-ops that are using ""short name"" in their testcase name\n# need to remove these lines once those ops support are added into onnx-tf\n# temporary exclude StringNormalizer test\nbackend_test.exclude(r\'[a-z,_]*strnorm[a-z,_]*\')\n\n# https://github.com/onnx/onnx/issues/349\nbackend_test.exclude(r\'[a-z,_]*GLU[a-z,_]*\')\n\n# TF does not support dialation and strides at the same time:\n# Will produce strides > 1 not supported in conjunction with dilation_rate > 1\nbackend_test.exclude(r\'[a-z,_]*dilated_strided[a-z,_]*\')\nbackend_test.exclude(r\'[a-z,_]*Conv2d_dilated[a-z,_]*\')\n\n# TF does not have column major max_pool_with_argmax\nbackend_test.exclude(\n    r\'[a-z,_]*maxpool_with_argmax_2d_precomputed_strides[a-z,_]*\')\n\n# PRelu OnnxBackendPyTorchConvertedModelTest has wrong dim for broadcasting\nbackend_test.exclude(r\'[a-z,_]*PReLU_[0-9]d_multiparam[a-z,_]*\')\n\n# TF does not support int8, int16, uint8, uint16, uint32, uint64 for\n# tf.floormod and tf.truncatemod\nbackend_test.exclude(r\'test_mod_[a-z,_]*uint[0-9]+\')\nbackend_test.exclude(r\'test_mod_[a-z,_]*int(8|(16))+\')\n\n# TF only support uint8, int32, int64 for indices and int32 for depth in\n# tf.one_hot\nbackend_test.exclude(r\'test_onehot_[a-z,_]*\')\n\n# skip resize downsample with mode=linear\nbackend_test.exclude(r\'test_resize_downsample_linear[a-z,_]*\')\n\n# range is using loop in the model test but all the outputs datatype are\n# missing in the body attribute of the loop\nbackend_test.exclude(\n    r\'test_range_float_type_positive_delta_expanded[a-z,_]*\')\nbackend_test.exclude(\n    r\'test_range_int32_type_negative_delta_expanded[a-z,_]*\')\n\n# skip all the cumsum testcases because all the axis in the testcases\n# are created as a 1-D 1 element tensor, but the spec clearly state\n# that axis should be a 0-D tensor(scalar)\nbackend_test.exclude(r\'test_cumsum_[a-z,_]*\')\n\nif legacy_opset_pre_ver(7):\n  backend_test.exclude(r\'[a-z,_]*Upsample[a-z,_]*\')\n\nif \'TRAVIS\' in os.environ:\n  backend_test.exclude(\'test_vgg19\')\n  backend_test.exclude(\'zfnet512\')\n\nif legacy_onnx_pre_ver(1, 2):\n  # These following tests fails by a tiny margin with onnx<1.2:\n  backend_test.exclude(\'test_operator_add_broadcast_cpu\')\n  backend_test.exclude(\'test_operator_add_size1_broadcast_cpu\')\n  backend_test.exclude(\'test_operator_add_size1_right_broadcast_cpu\')\n  backend_test.exclude(\'test_operator_add_size1_singleton_broadcast_cpu\')\n  backend_test.exclude(\'test_averagepool_3d_default_cpu\')\n  # Do not support consumed flag:\n  backend_test.exclude(\'test_batch_normalization\')\n  # Do not support RNN testing on onnx<1.2 due to incorrect tests:\n  backend_test.exclude(r\'test_operator_rnn_cpu\')\n  backend_test.exclude(r\'test_operator_lstm_cpu\')\n  backend_test.exclude(r\'test_operator_rnn_single_layer_cpu\')\n\n# The onnx test for cast, float to string, does not work\nif not legacy_opset_pre_ver(9):\n  backend_test.exclude(r\'[a-z,_]*cast[a-z,_]*\')\n\nif not legacy_opset_pre_ver(10):\n  # Do not support dilations != 1 for ConvTranspose, test is added in opset 10\n  backend_test.exclude(r\'[a-z,_]*convtranspose_dilations[a-z,_]*\')\n\n# import all test cases at global scope to make them visible to python.unittest\nglobals().update(backend_test.enable_report().test_cases)\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
onnx_tf/handlers/backend/__init__.py,0,"b'import os\nimport pkgutil\n\n__all__ = [\n    modname for _, modname, _ in pkgutil.walk_packages(\n        path=[os.path.split(__file__)[0]])\n]\n'"
onnx_tf/handlers/backend/abs.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Abs"")\n@tf_func(tf.abs)\nclass Abs(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/acos.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Acos"")\n@tf_func(tf.acos)\nclass Acos(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/acosh.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Acosh"")\n@tf_func(tf.acosh)\nclass Acosh(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/add.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ArithmeticMixin\n\n\n@onnx_op(""Add"")\n@tf_func(tf.add)\nclass Add(ArithmeticMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/and.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .control_flow_mixin import LogicalMixin\n\n\n@onnx_op(""And"")\n@tf_func(tf.logical_and)\nclass Add(LogicalMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/arg_max.py,7,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom onnx_tf.common.tf_helper import tf_shape\n\n\n@onnx_op(""ArgMax"")\n@tf_func(tf.argmax)\nclass ArgMax(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""axis"": 0}}\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    axis = node.attrs.get(""axis"", 0)\n    keepdims = node.attrs.get(""keepdims"", 1)\n    select_last_index = node.attrs.get(""select_last_index"", 0)\n    if select_last_index == 0:\n      arg_max = cls.make_tensor_from_onnx_node(node, **kwargs)\n    else:\n      # reverse the input and apply argmax on that to get last occurrence of max\n      x = kwargs[""tensor_dict""][node.inputs[0]]\n      x = tf.reverse(x, axis=[axis])\n      arg_max = cls.make_tensor_from_onnx_node(node, inputs=[x], **kwargs)\n      # adjust indices to account for the reverse\n      arg_max = tf_shape(x)[axis] - arg_max - 1\n    if keepdims == 1:\n      return [tf.expand_dims(arg_max, axis=axis)]\n    return [arg_max]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_12(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/arg_min.py,7,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom onnx_tf.common.tf_helper import tf_shape\n\n\n@onnx_op(""ArgMin"")\n@tf_func(tf.argmin)\nclass ArgMin(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""axis"": 0}}\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    axis = node.attrs.get(""axis"", 0)\n    keepdims = node.attrs.get(""keepdims"", 1)\n    select_last_index = node.attrs.get(""select_last_index"", 0)\n    if select_last_index == 0:\n      arg_min = cls.make_tensor_from_onnx_node(node, **kwargs)\n    else:\n      # reverse the input and apply argmax on that to get last occurrence of max\n      x = kwargs[""tensor_dict""][node.inputs[0]]\n      x = tf.reverse(x, axis=[axis])\n      arg_min = cls.make_tensor_from_onnx_node(node, inputs=[x], **kwargs)\n      # adjust indices to account for the reverse\n      arg_min = tf_shape(x)[axis] - arg_min - 1\n    if keepdims == 1:\n      return [tf.expand_dims(arg_min, axis=axis)]\n    return [arg_min]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_12(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/asin.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Asin"")\n@tf_func(tf.asin)\nclass Asin(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/asinh.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Asinh"")\n@tf_func(tf.asinh)\nclass Asinh(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/atan.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Atan"")\n@tf_func(tf.atan)\nclass Atan(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/atanh.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Atanh"")\n@tf_func(tf.atanh)\nclass Atanh(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/average_pool.py,2,"b'from onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .pool_mixin import PoolMixin\n\n\n@onnx_op(""AveragePool"")\nclass AveragePool(PoolMixin, BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    return cls.pool(node, kwargs[""tensor_dict""], ""AVG"",\n                    kwargs.get(""strict"", True))\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/batch_normalization.py,9,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""BatchNormalization"")\n@tf_func(tf.nn.batch_normalization)\nclass BatchNormalization(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {\n        ""default"": {\n            ""epsilon"": 1e-5\n        },\n        ""rename"": {\n            ""epsilon"": ""variance_epsilon""\n        }\n    }\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    x_shape = x.get_shape().as_list()\n    x_rank = len(x_shape)\n\n    params_shape_broadcast = list(\n        [1, x_shape[1]] + [1 for _ in range(2, x_rank)])\n\n    total_num_dim = len(x.get_shape())\n    scale = tf.reshape(tensor_dict[node.inputs[1]], params_shape_broadcast)\n    bias = tf.reshape(tensor_dict[node.inputs[2]], params_shape_broadcast)\n    running_mean = tf.reshape(tensor_dict[node.inputs[3]],\n                              params_shape_broadcast)\n    running_variance = tf.reshape(tensor_dict[node.inputs[4]],\n                                  params_shape_broadcast)\n\n    # from version 7, force to use test mode\n    if cls.SINCE_VERSION >= 7 or node.attrs.get(""is_test"", 0):\n      inputs = [x, running_mean, running_variance, bias, scale]\n      return [cls.make_tensor_from_onnx_node(node, inputs=inputs)]\n    spatial = node.attrs.get(""spatial"", 1) == 1\n    momentum = node.attrs.get(""momentum"", 0.9)\n    axis = [0] if spatial else [0] + list(range(2, total_num_dim))\n    mean, variance = tf.nn.moments(x, axis)\n    running_mean = running_mean * momentum + mean * (1 - momentum)\n    running_variance = running_variance * momentum + variance * (1 - momentum)\n    # TODO: need to conform to the documentation here\n    inputs = [x, running_mean, running_variance, bias, scale]\n    return [cls.make_tensor_from_onnx_node(node, inputs=inputs)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/bitshift.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""BitShift"")\nclass BitShift(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tf_func = tf.bitwise.left_shift if node.attrs.get(\n        ""direction"") == ""LEFT"" else tf.bitwise.right_shift\n    return [\n        cls.make_tensor_from_onnx_node(node, tf_func=tf_func, **kwargs)\n    ]\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/broadcast_mixin.py,4,"b'import numpy as np\nimport tensorflow as tf\n\n\nclass BroadcastMixin(object):\n\n  @classmethod\n  def explicit_broadcast(cls, inputs, axis=None, tensor_dict=None):\n    x = inputs[0] if isinstance(inputs[0],\n                                tf.Tensor) else tensor_dict[inputs[0]]\n    y = inputs[1] if isinstance(inputs[1],\n                                tf.Tensor) else tensor_dict[inputs[1]]\n\n    if np.prod(y.shape) == 1:\n      return y\n\n    if not isinstance(x, tf.Tensor) or not isinstance(y, tf.Tensor):\n      raise ValueError(""Targets for explicit broadcasting need to be Tensor."")\n\n    if axis is None:\n      return y\n\n    total_num_dim = len(x.get_shape())\n    if axis < 0:\n      axis += total_num_dim\n\n    if axis + len(y.get_shape()) == total_num_dim:\n      return y\n\n    dims = [axis + i for i in range(len(y.get_shape()))]\n    new_y = y\n    for i in range(total_num_dim):\n      if i not in dims:\n        new_y = tf.expand_dims(new_y, i)\n    return new_y\n\n  @classmethod\n  def limited_broadcast(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    y = tensor_dict[node.inputs[1]]\n    if node.attrs.get(""broadcast"") == 1:\n      y = cls.explicit_broadcast([x, y], node.attrs.get(""axis"", None))\n      return [cls.make_tensor_from_onnx_node(node, inputs=[x, y], **kwargs)]\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/cast.py,11,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\n\n\n@onnx_op(""Cast"")\n@tf_func(tf.cast)\n@partial_support(True)\n@ps_description(""Cast string to float32/float64/int32/int64 "" +\n                ""are not supported in Tensorflow."")\nclass Cast(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""rename"": {""to"": ""dtype""}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    inp = kwargs[""tensor_dict""][node.inputs[0]]\n    to_type = node.attrs.get(""to"")\n\n    if to_type == tf.string:\n      return [tf.as_string(inp)]\n\n    if inp.dtype == tf.string:\n      if to_type not in [tf.float32, tf.float64, tf.int32, tf.int64]:\n        raise RuntimeError(\n            ""Cast string to type {} is not supported in Tensorflow."".format(\n                to_type))\n      return [tf.strings.to_number(inp, to_type)]\n\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/ceil.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Ceil"")\n@tf_func(tf.math.ceil)\nclass Ceil(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/clip.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Clip"")\n@tf_func(tf.clip_by_value)\nclass Clip(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n\n    if cls.SINCE_VERSION < 11:\n      # min/max were required and passed as attributes\n      clip_value_min = node.attrs.get(""min"", tf.reduce_min(x))\n      clip_value_max = node.attrs.get(""max"", tf.reduce_max(x))\n    else:\n      # min/max are optional and passed as inputs\n      clip_value_min = tensor_dict[\n          ""min""] if ""min"" in tensor_dict else x.dtype.min\n      clip_value_max = tensor_dict[\n          ""max""] if ""max"" in tensor_dict else x.dtype.max\n\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, inputs=[x, clip_value_min, clip_value_max])\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/compress.py,10,"b'import copy\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Compress"")\n@tf_func(tf.gather)\nclass Compress(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    attrs = copy.deepcopy(node.attrs)\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    condition = tensor_dict[node.inputs[1]]\n\n    x = tf.reshape(x, [-1]) if node.attrs.get(""axis"") is None else x\n    if condition.shape.is_fully_defined():\n      condition_shape = condition.shape[0]\n      indices = tf.constant(list(range(condition_shape)), dtype=tf.int64)\n    else:\n      condition_shape = tf.shape(condition, out_type=tf.int64)[0]\n      indices = tf.range(condition_shape, dtype=tf.int64)\n    not_zero = tf.not_equal(condition, tf.zeros_like(condition))\n    attrs[\'indices\'] = tf.boolean_mask(indices, not_zero)\n    return [\n        cls.make_tensor_from_onnx_node(node, inputs=[x], attrs=attrs, **kwargs)\n    ]\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/concat.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Concat"")\n@tf_func(tf.concat)\nclass Concat(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    inputs = [kwargs[""tensor_dict""][inp] for inp in node.inputs]\n    return [cls.make_tensor_from_onnx_node(node, inputs=[inputs])]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_4(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/constant.py,12,"b'import numpy as np\n\nfrom onnx import numpy_helper\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom onnx_tf.common import data_type\n\n\n@onnx_op(""Constant"")\n@tf_func(tf.constant)\nclass Constant(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    attr_value = node.attrs[""value""]\n    dtype = data_type.onnx2tf(attr_value.data_type)\n    value = numpy_helper.to_array(attr_value)\n    return [\n        cls.make_tensor_from_onnx_node(node,\n                                       inputs=[value],\n                                       attrs={""dtype"": dtype})\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    # either value or sparse_value\n    if ""value"" in node.attrs:\n      return cls._common(node, **kwargs)\n    else:\n      sparse_value = node.attrs[""sparse_value""]\n      indices = numpy_helper.to_array(sparse_value.indices)\n      values = numpy_helper.to_array(sparse_value.values)\n      shape = np.array(sparse_value.dims)\n    return [tf.SparseTensor(indices, values, shape)]\n\n  @classmethod\n  def version_12(cls, node, **kwargs):\n    if ""value"" in node.attrs or ""sparse_value"" in node.attrs:\n      return cls.version_11(node, **kwargs)\n    elif ""value_float"" in node.attrs:\n      value = node.attrs[""value_float""]\n      dtype = tf.float32\n    elif ""value_floats"" in node.attrs:\n      value = node.attrs[""value_floats""]\n      dtype = tf.float32\n    elif ""value_int"" in node.attrs:\n      value = node.attrs[""value_int""]\n      dtype = tf.int64\n    elif ""value_ints"" in node.attrs:\n      value = node.attrs[""value_ints""]\n      dtype = tf.int64\n    elif ""value_string"" in node.attrs:\n      value = node.attrs[""value_string""]\n      dtype = tf.string\n    elif ""value_strings"" in node.attrs:\n      value = node.attrs[""value_strings""]\n      dtype = tf.string\n    return [\n        cls.make_tensor_from_onnx_node(node,\n                                       inputs=[value],\n                                       attrs={""dtype"": dtype})\n    ]\n'"
onnx_tf/handlers/backend/constant_fill.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""ConstantFill"")\n@tf_func(tf.fill)\nclass ConstantFill(BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    if node.inputs and ""shape"" in node.attrs:\n      raise ValueError(\n          ""Cannot set the shape argument and pass in an input at the same time.""\n      )\n    if not node.inputs and ""extra_shape"" in node.attrs:\n      raise ValueError(""Cannot set extra_shape when there is no input."")\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""value"": 0.}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n\n    if ""shape"" in node.attrs:\n      shape = node.attrs[""shape""]\n    else:\n      shape = tensor_dict[\n          node.inputs[0]].get_shape().as_list() if node.attrs.get(\n              ""input_as_shape"", 0) == 0 else tensor_dict[node.inputs[0]]\n\n    if ""extra_shape"" in node.attrs:\n      shape = tf.concat([shape, node.attrs[""extra_shape""]], 0)\n\n    value = node.attrs.get(""value"", 0.)\n\n    if ""dtype"" in node.attrs:\n      return [tf.cast(tf.fill(shape, value), dtype=node.attrs[""dtype""])]\n\n    return [cls.make_tensor_from_onnx_node(node, inputs=[shape], **kwargs)]\n'"
onnx_tf/handlers/backend/constant_of_shape.py,6,"b'import copy\n\nimport tensorflow as tf\n\nfrom onnx import numpy_helper\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""ConstantOfShape"")\n@tf_func(tf.fill)\nclass ConstantOfShape(BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    attrs = copy.deepcopy(node.attrs)\n\n    shape = kwargs[""tensor_dict""][node.inputs[0]]\n\n    # make sure the shape dtype is either int32 or int64\n    if shape.dtype not in [tf.int64, tf.int32]:\n      shape = tf.cast(shape, tf.int64)\n\n    # the default value is 0, float32\n    if ""value"" in node.attrs:\n      attr_value = node.attrs[""value""]\n      value = numpy_helper.to_array(attr_value)\n      attrs[""value""] = value[0]\n    else:\n      attrs[""value""] = 0.\n\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, inputs=[shape], attrs=attrs, **kwargs)\n    ]\n'"
onnx_tf/handlers/backend/control_flow_mixin.py,0,b'from .broadcast_mixin import BroadcastMixin\n\n\nclass LogicalMixin(BroadcastMixin):\n  pass\n\n\nclass ComparisonMixin(BroadcastMixin):\n  pass\n'
onnx_tf/handlers/backend/conv.py,2,"b'from onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .conv_mixin import ConvMixin\n\n\n@onnx_op(""Conv"")\nclass Conv(ConvMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.conv(node, kwargs[""tensor_dict""])\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls.conv(node, kwargs[""tensor_dict""])\n'"
onnx_tf/handlers/backend/conv_integer.py,10,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .conv_mixin import ConvMixin\n\n\n@onnx_op(""ConvInteger"")\nclass ConvInteger(ConvMixin, BackendHandler):\n\n  @classmethod\n  def _apply_zero_point(cls, base, zero_point):\n    base = tf.cast(base, tf.float32)\n    zero_point = tf.cast(zero_point, tf.float32)\n    return base - zero_point\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    w = tensor_dict[node.inputs[1]]\n\n    def process_conv(new_x, new_w):\n      # Remove zero-points from inputs\n      if ""x_zero_point"" in node.inputs:\n        node.inputs.remove(""x_zero_point"")\n      if ""w_zero_point"" in node.inputs:\n        node.inputs.remove(""w_zero_point"")\n\n      new_dict = {node.inputs[0]: new_x, node.inputs[1]: new_w}\n\n      # Use common conv handling\n      conv_node = cls.conv(node, new_dict)\n\n      return conv_node\n\n    # Apply x_zero_point first\n    x = cls._apply_zero_point(x, tensor_dict[\n        ""x_zero_point""]) if ""x_zero_point"" in node.inputs else tf.cast(\n            x, tf.float32)\n\n    # Apply w_zero_point next\n    if ""w_zero_point"" in node.inputs:\n      w_zero_point = tensor_dict[""w_zero_point""]\n      if w_zero_point.shape.rank == 0:\n        # Simply apply w_zero_point for scalar\n        w = cls._apply_zero_point(w, w_zero_point)\n      elif w_zero_point.shape.rank == 1:\n        # Need additional processing for 1d w_zero_point\n        tensor_list = []\n        process_shape = [1] + [w.shape[i] for i in range(1, len(w.shape))]\n        for i in range(w.shape.as_list()[0]):\n          # Apply w_zero_point for each element in 1d tensor\n          out_tensor = cls._apply_zero_point(w[i], w_zero_point[i])\n          tensor_list.append(tf.reshape(out_tensor, process_shape))\n        w = tf.concat(tensor_list, 0)\n      else:\n        raise ValueError(""Unsupported w zero point: {}"".format(w_zero_point))\n    else:\n      # Just cast without processing w\n      w = tf.cast(w, tf.float32)\n\n    return [tf.cast(process_conv(x, w)[0], tf.int32)]\n'"
onnx_tf/handlers/backend/conv_mixin.py,26,"b'import tensorflow as tf\n\nfrom onnx_tf.common import get_data_format\nfrom onnx_tf.common import get_perm_from_formats\nfrom onnx_tf.common import supports_device\nfrom onnx_tf.common import exception\nfrom .broadcast_mixin import BroadcastMixin\nfrom .pad_mixin import PadMixin\n\n# Constant string used to indicate that requested padding\n# is not natively supported in Tensorflow.\nPAD_TF_INCOMPATIBLE = ""PAD_TF_INCOMPATIBLE""\n\n\nclass ConvMixin(BroadcastMixin):\n\n  @classmethod\n  def conv(cls, node, input_dict, transpose=False):\n    """""" Convolution method for both conv and transposed conv\n    For transposed conv,\n      Attr pads is not used for input, but declares how much output is padded.\n      Here, output means output from transposed conv which already pad output_padding if set.\n      So the pseudo explanation for output should be:\n        output = conv_transpose_output + output_padding - pads\n      And conv_transpose_output shape should be:\n        conv_transpose_output_shape[i] = strides[i] * (input_shape[i] - 1) + kernel_shape[i]\n    """"""\n    x = input_dict[node.inputs[0]]\n    x_rank = len(x.get_shape())\n    x_shape = x.get_shape().as_list()\n    spatial_size = x_rank - 2\n\n    support_cuda = supports_device(""CUDA"")\n    storage_format, compute_format = get_data_format(x_rank)\n    compute_c_idx = compute_format.find(""C"")\n    spatial_format = """".join([d for d in compute_format if d not in [""N"", ""C""]])\n\n    in_weights = input_dict[node.inputs[1]]\n    weights_rank = len(in_weights.get_shape())\n    if transpose:\n      # Translate weights from (C x M x KH x KW) to (KH x KW X M X C)\n      perm = list(range(2, weights_rank)) + [1, 0]\n    else:\n      # Translate weights from (M x C x KH x KW) to (KH x KW X C X M)\n      perm = list(range(2, weights_rank)) + [1, 0]\n\n    if ""kernel_shape"" in node.attrs.keys():\n      kernel_shape = node.attrs[""kernel_shape""]\n      assert in_weights.get_shape().as_list()[2:] == kernel_shape, (\n          ""kernel_shape ""\n          ""attr of convolution does not match the actual weight ""\n          ""passed to this operation, attr {}, actual {}"").format(\n              kernel_shape,\n              in_weights.get_shape().as_list())\n\n    weights = tf.transpose(in_weights, perm)\n    dilations = node.attrs.get(""dilations"", [1] * spatial_size)\n    strides = node.attrs.get(""strides"", [1] * spatial_size)\n\n    pads = node.attrs.get(""pads"", [0, 0] * spatial_size)\n\n    # Check auto_pad nonexistent or NOTSET first\n    if ""auto_pad"" not in node.attrs or node.attrs[""auto_pad""] == ""NOTSET"":\n      if not transpose:\n        if pads != [0, 0] * spatial_size:\n          x = PadMixin.get_padding_as_op(x, pads)\n        pad_mode = ""VALID""\n      else:\n        pad_mode = ""NOTSET""\n    # Then we use auto_pad to setup pad_mode\n    elif node.attrs[""auto_pad""] == ""SAME_UPPER"":\n      pad_mode = ""SAME""\n    elif node.attrs[""auto_pad""] == ""VALID"":\n      pad_mode = ""VALID""\n    elif node.attrs[""auto_pad""] == ""SAME_LOWER"":\n      pad_mode = PAD_TF_INCOMPATIBLE\n    else:\n      raise ValueError(""Invalid auto_pad attribute: {}"".format(\n          node.attrs[""auto_pad""]))\n\n    # Currently auto_pad = SAME_LOWER is not supported\n    if pad_mode is PAD_TF_INCOMPATIBLE:\n      if transpose:\n        exception.OP_UNSUPPORTED_EXCEPT(\n            ""ConvTranspose with auto_pad `SAME_LOWER`"", ""Tensorflow"")\n      else:\n        exception.OP_UNSUPPORTED_EXCEPT(""Conv with auto_pad `SAME_LOWER`"",\n                                        ""Tensorflow"")\n\n    group = node.attrs.get(""group"", 1)\n\n    weight_groups = tf.split(weights, num_or_size_splits=group, axis=-1)\n\n    if support_cuda:\n      xs = tf.split(x, num_or_size_splits=group, axis=1)\n    else:\n      x = tf.transpose(\n          x, perm=get_perm_from_formats(storage_format, compute_format))\n      xs = tf.split(x, num_or_size_splits=group, axis=-1)\n\n    if transpose:\n      if dilations != [1] * spatial_size:\n        raise RuntimeError(""Cannot set non-1 dilation for conv transpose."")\n      convolved = []\n      for (x, weight) in zip(xs, weight_groups):\n        x_spatial_shape = [\n            x_shape[storage_format.find(d)] for d in spatial_format\n        ]\n        weights_shape = weights.get_shape().as_list()\n        output_shape = node.attrs.get(""output_shape"", None)\n        conv_output_shape = [x_shape[storage_format.find(""N"")]]\n\n        # calculate output shape\n        if pad_mode == ""NOTSET"":\n          if output_shape is None:\n            conv_output_shape += [\n                strides[i] * x_spatial_shape[i] +\n                max(weights_shape[i] - strides[i], 0)\n                for i in list(range(spatial_size))\n            ]\n          else:\n            conv_output_shape += [\n                s + pads[i] + pads[spatial_size + i]\n                for i, s in enumerate(output_shape[-2:])\n            ]\n          conv_output_shape.insert(compute_c_idx, weights_shape[-2])\n\n          # make strides to match input rank\n          strides_full = [1] + strides\n          strides_full.insert(compute_c_idx, 1)\n\n          # get corresponding function in tf\n          if spatial_size == 1:\n            conv_func = tf.nn.conv1d_transpose\n            strides_full = strides[0]\n          elif spatial_size == 2:\n            conv_func = tf.nn.conv2d_transpose\n          elif spatial_size == 3:\n            conv_func = tf.nn.conv3d_transpose\n          else:\n            raise NotImplementedError(\n                ""Transposed convolution for {}d is not implemented in Tensorflow"".\n                format(spatial_size))\n\n          # use raw input x to do transposed conv\n          conv_rs = conv_func(\n              x,\n              weight,\n              conv_output_shape,\n              strides_full,\n              padding=""VALID"",\n              data_format=compute_format)\n\n          # pad output first by output_padding attr\n          if ""output_padding"" in node.attrs and output_shape is None:\n            output_padding = [[0, 0]\n                             ] + [[0, p] for p in node.attrs[""output_padding""]]\n            output_padding.insert(compute_c_idx, [0, 0])\n            conv_rs = tf.pad(conv_rs, output_padding)\n\n          # remove pads set in pads attr\n          conv_rs_shape = conv_rs.get_shape().as_list()\n          begin = [0] + pads[:spatial_size]\n          begin.insert(compute_c_idx, 0)\n          size = [\n              s if d in [""N"", ""C""] else s - pads[spatial_format.find(d)] -\n              pads[spatial_format.find(d) + spatial_size]\n              for d, s in zip(compute_format, conv_rs_shape)\n          ]\n          conv_rs = tf.slice(conv_rs, begin=begin, size=size)\n\n          convolved.append(conv_rs)\n        else:\n          # No need to check pads if auto_pad is specifically provided.\n          # The assumption is that once auto_pad is provided as either VALID\n          # or SAME_UPPER (SAME_LOWER is currently not supported in TF) the\n          # output_shape will always be inferred. That is, the output_shape\n          # and output_padding will not be used in this case.\n          if pad_mode == ""VALID"":\n            conv_output_shape += [\n                strides[i] * (x_spatial_shape[i] - 1) + weights_shape[i]\n                for i in list(range(spatial_size))\n            ]\n          else:\n            conv_output_shape += [\n                strides[i] * x_spatial_shape[i]\n                for i in list(range(spatial_size))\n            ]\n          conv_output_shape.insert(compute_c_idx, weights_shape[-2])\n\n          # make strides to match input rank\n          strides_full = [1] + strides\n          strides_full.insert(compute_c_idx, 1)\n\n          # get corresponding function in tf\n          if spatial_size == 1:\n            conv_func = tf.contrib.nn.conv1d_transpose\n            strides_full = strides[0]\n          elif spatial_size == 2:\n            conv_func = tf.nn.conv2d_transpose\n          elif spatial_size == 3:\n            conv_func = tf.nn.conv3d_transpose\n          else:\n            raise NotImplementedError(\n                ""Transposed convolution for {}d is not implemented in Tensorflow"".\n                format(spatial_size))\n\n          # use raw input x to do transposed conv\n          conv_rs = conv_func(\n              x,\n              weight,\n              conv_output_shape,\n              strides_full,\n              padding=pad_mode,\n              data_format=compute_format)\n          convolved.append(conv_rs)\n\n    else:\n      convolved = [\n          tf.nn.convolution(\n              x,\n              weight,\n              padding=pad_mode,\n              strides=strides,\n              dilations=dilations,\n              data_format=compute_format)\n          for (x, weight) in zip(xs, weight_groups)\n      ]\n\n    if len(node.inputs) == 2:\n      if support_cuda:\n        output = tf.concat(convolved, axis=1)\n      else:\n        output = tf.concat(convolved, axis=-1)\n        output = tf.transpose(\n            output, perm=get_perm_from_formats(compute_format, storage_format))\n    else:\n      bias = input_dict[node.inputs[2]]\n      bias = cls.explicit_broadcast([x, bias], compute_c_idx)\n\n      if support_cuda:\n        output = tf.concat(convolved, axis=1)\n        output = tf.add(output, bias)\n      else:\n        output = tf.concat(convolved, axis=-1)\n        output = tf.add(output, bias)\n        output = tf.transpose(\n            output, perm=get_perm_from_formats(compute_format, storage_format))\n\n    return [output]\n'"
onnx_tf/handlers/backend/conv_transpose.py,4,"b'from onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom .conv_mixin import ConvMixin\n\n\n@onnx_op(""ConvTranspose"")\n@partial_support(True)\n@ps_description(""ConvTranspose with dilations != 1, or "" +\n                ""transposed convolution for 4D or higher "" +\n                ""are not supported in Tensorflow."")\nclass ConvTranspose(ConvMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.conv(node, kwargs[""tensor_dict""], transpose=True)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls.conv(node, kwargs[""tensor_dict""], transpose=True)\n'"
onnx_tf/handlers/backend/cos.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Cos"")\n@tf_func(tf.cos)\nclass Cos(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/cosh.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Cosh"")\n@tf_func(tf.cosh)\nclass Cosh(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/cumsum.py,9,"b'import tensorflow as tf\n\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""CumSum"")\n@tf_func(tf.math.cumsum)\n@partial_support(True)\n@ps_description(\n    ""CumSum inputs in uint32/uint64 "" + ""are not supported in Tensorflow.""\n)\nclass CumSum(BackendHandler):\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    supported_dtype = [\n        tf.bfloat16, tf.half, tf.float32, tf.float64, tf.uint8, tf.uint16,\n        tf.int8, tf.int16, tf.int32, tf.int64, tf.complex64, tf.complex128\n    ]\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    if x.dtype not in supported_dtype:\n      exception.OP_UNSUPPORTED_EXCEPT(\n          ""CumSum input in "" + str(x.dtype) + "" which"", ""Tensorflow"")\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    inputs = [x]\n\n    if len(node.inputs) > 1:\n      # optional 0-D tensor, range [-rank(x), rank(x)-1]\n      axis = tensor_dict[""axis""]\n      inputs.append(axis)\n\n    attrs = {\n        ""exclusive"": bool(node.attrs.get(""exclusive"", 0)),\n        ""reverse"": bool(node.attrs.get(""reverse"", 0))\n    }\n\n    return [cls.make_tensor_from_onnx_node(node, inputs=inputs, attrs=attrs)]\n'"
onnx_tf/handlers/backend/depth_to_space.py,8,"b'import copy\n\nimport tensorflow as tf\n\nfrom onnx_tf.common import get_data_format\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""DepthToSpace"")\n@tf_func(tf.nn.depth_to_space)\nclass DepthToSpace(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""rename"": {""blocksize"": ""block_size""}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_rank = len(x.get_shape())\n    storage_format, compute_format = get_data_format(x_rank)\n    attrs = copy.deepcopy(node.attrs)\n    attrs[""data_format""] = storage_format\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, attrs=attrs, c_first_cuda_only=True, **kwargs)\n    ]\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_rank = len(x.get_shape())\n    storage_format, compute_format = get_data_format(x_rank)\n    attrs = copy.deepcopy(node.attrs)\n    attrs[""data_format""] = storage_format\n    mode = attrs.get(""mode"", ""DCR"")\n\n    if mode == ""CRD"":\n      # need native computation\n      bsize = attrs.get(""blocksize"")\n      batch, channel, height, width = x.shape\n      csize = channel // (bsize**2)\n\n      reshape_node = tf.reshape(x, [batch, csize, bsize, bsize, height, width])\n      transpose_node = tf.transpose(reshape_node, perm=[0, 1, 4, 2, 5, 3])\n      return [\n          tf.reshape(transpose_node,\n                     [batch, csize, height * bsize, width * bsize])\n      ]\n\n    else:\n      return [\n          cls.make_tensor_from_onnx_node(\n              node, attrs=attrs, c_first_cuda_only=True, **kwargs)\n      ]\n'"
onnx_tf/handlers/backend/dequantize_linear.py,7,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""DequantizeLinear"")\nclass DequantizeLinear(BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    if len(node.inputs) == 3:\n      x = tensor_dict[node.inputs[0]]\n      x_scale = tensor_dict[node.inputs[1]]\n      x_zero_point = tensor_dict[node.inputs[2]]\n      if x_scale.shape != x_zero_point.shape:\n        raise ValueError(""DequantizeLinear x_scale(shape="" + str(\n            x_scale.shape) + "") and x_zero_point(shape="" + str(\n                x_zero_point.shape) + "") must be in the same shape"")\n      if x_zero_point.dtype != x.dtype:\n        raise ValueError(\n            ""DequantizeLinear x_zero_point("" + str(x_zero_point.dtype) +\n            "") and x("" + str(x.dtype) + "") must be in the same dtype"")\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    x = tf.cast(x, tf.float32)\n    x_scale = tensor_dict[node.inputs[1]]\n    if len(node.inputs) == 3 and x.dtype != tf.int32:\n      x_zero_point = tensor_dict[node.inputs[2]]\n      x_zero_point = tf.cast(x_zero_point, tf.float32)\n      x = tf.subtract(x, x_zero_point)\n\n    y = tf.multiply(x, x_scale)\n\n    return [y]\n'"
onnx_tf/handlers/backend/det.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Det"")\n@tf_func(tf.linalg.det)\nclass Det(BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return [\n        cls.make_tensor_from_onnx_node(node, **kwargs)\n    ]\n'"
onnx_tf/handlers/backend/dilated_pooling.py,39,"b'from __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom onnx_tf.common import pooling_helper\nfrom onnx_tf.common.tf_helper import tf_shape\nfrom onnx_tf.common.tf_helper import tf_product\n\n\nclass DilatedPooling(object):\n  """"""\n        This class implements two main methods:\n            dilated_pool:\n                calculates a max or average pool over the input\n\n            dilated_maxpool_with_argmax:\n                calculates a maxpool over the input and returns the\n                indices/argmax of the selected values\n\n        In addition to the standard features of pooling operations in\n        Tensorflow, these methods support dilations, ceil mode, SAME_LOWER and\n        explicit padding.\n\n        Dilations are partly supported in Tensorflow in `tf.nn.pool` and\n        `tf.nn.dilation2d`. The code will try to use the Tensoflow build-in\n        functions as much as poosible.\n\n        In cases, not supported by Tensorflow there is a custom algorith of\n        dilated pooling `_remove_dilations`.\n\n        The idea behind `_remove_dilations` is to transform the input N-D data\n        into a supported input for the standard tf.nn.pool operation.\n        This is achieved by calculating N-D indicies for the values which will\n        be selected from the input when applying the dilations and\n        then extracting the values using tf.gather_nd. Next step is to execute\n        `tf.nn.pool` on this new input data with **strides=kernel_shape** and\n        no dilations. The resulting pool will be the result we are looking for.\n\n        In case of `deilated_maxpool_with_argmax` an additional step is needed\n        to recalculated the resulting indices back into the original\n        data indices. It is done with `_calc_orig_argmax`\n\n        Here is a simple example of how the algorithm works:\n\n        kernel_shape = [3]\n        strides = [2]\n        dilations = [3]\n\n        Input 1D data:\n\n            x-----x-----x-----x-----x-----x-----x-----x-----x-----x-----x\n            |  *  |     | **  |  *  |     | **  |  *  |     | **  |     |\n            | 10  |  9  | 30  |  7  |  6  | 15  | 16  | 17  | 18  | 19  |\n            x-----x-----x-----x-----x-----x-----x-----x-----x-----x-----x\n              (0)   (1)   (2)   (3)   (4)   (5)   (6)   (7)   (8)   (9)\n\n        where * represents the values selected during the first sliding window\n        step and ** during the second sliding window step\n\n        the resulting indices will be:\n\n            [0, 3, 6, 2, 5, 8]\n             |     |  |     |\n              First    Second\n              step     step\n\n        after tf.gather_nd operation we get a new input data with\n        removed dilations:\n\n            [10, 7, 16, 30, 15, 18]\n\n        and apllying tf.nn.maxpool (or avgpool) with strides = kernel_shape = 3\n        will result into:\n\n            [16, 30]\n\n        which is the result of the dilated maxpooling.\n\n        Here is pseudo code of the algorithm with comments:\n\n        FUNCTION _remove_dilations:\n            /* Calculate N-D index of the values to be selected by the\n               dilations and strides */\n\n            /* Do a loop over the input spatial dimensions starting from the\n               last (most internal) going up to the first dimension\n\n               On every step of the loop calculate the input indices and\n               ""combine"" them with the already calculated indices from the\n               previous dimensions using cartesian product.\n            */\n            LOOP with **dimension** from **dimensions_count** to **0**:\n\n                // Initialize empty gather_nd index\n                gather_ind = []\n\n                // Calculate the output size for the current dimension\n                dim_filter_size = (dim_kernel_size - 1) * dim_dilations\n                dim_output_size = (((dim_input_size - dim_filter_size) //\n                                   dim_strides) + 1) * dim_kernel_size)\n\n                /* For every output index, calculate the corresponding index\n                   into the input data */\n                dim_input_indices = range(0, dim_output_size)\n                dim_input_indices = calculate_input_indicies(dim_input_indices)\n\n                /* combine the calculated indices with the previous dimensions\n                */\n                gather_ind = cartesian_product(dim_input_indices, gather_ind)\n            END LOOP\n\n            /* For example for 2D input the resulting gather_ind will\n               look like this:\n\n               [[y1, x1], [y2, x2], ..., [yn, xm]]\n\n               where:\n               n is the height\n               m is the width and\n               [xi, yi] are the 2D indices in the input data\n            */\n\n            new_data = tf.gather_nd(input, gather_ind)\n\n            reshape new_data to the correct output shape\n\n            RETURN new_data\n\n\n        Before executing _remove_dilations the code will apply paddings to the\n        input data if needed. Padding is done using tf.pad with -inf values.\n        Check `_remove_dilations` code for more details explanation of the\n        implementation\n\n        In case of dilated_maxpool_with_argmax the returned indices from\n        tf.nn.max_pool_with_argmax will point into our ""no dilations"" data.\n        That is why they need to be mapped back to the original input data.\n        It is done with `_calc_orig_argmax` function which will apply the same\n        calculations, that are used in _remove_dilations when calculating the\n        input data indices from output indices (check `_calc_orig_argmax` for\n        detailed inline comments explaining the calculations)\n\n    """"""\n\n  def __init__(self,\n               input,\n               kernel_shape,\n               strides,\n               dilations,\n               padding=""VALID"",\n               ceil_mode=False,\n               count_include_pad=False,\n               pooling_type=""MAX""):\n    self.input = tf.convert_to_tensor(input)\n\n    self.kernel_shape = kernel_shape\n    self.strides = strides\n    self.dilations = dilations\n    self.padding = padding\n    self.is_explicit_padding = type(padding) is list\n    self.ceil_mode = ceil_mode\n    self.count_include_pad = count_include_pad\n    self.pooling_type = pooling_type.upper()\n\n    self.is_known_shape = self.input.shape.is_fully_defined()\n    self.spatial_size = len(kernel_shape)\n    self.input_rank = self.spatial_size + 2\n\n    # if the rank is not defined, set it to the calculated input_rank\n    # rank should be known for ops like tf.gather_nd\n    if not input.shape.rank:\n      input.set_shape([None] * self.input_rank)\n    self.orig_input_shape = tf_shape(input)\n    self.input_shape = self.orig_input_shape\n\n    if pooling_type.startswith(""MAX""):\n      self.padding_constant = input.dtype.min\n    else:\n      self.padding_constant = 0\n\n  def _calc_input_ind(self, output_ind, kernel, dilation, stride):\n    """"""\n            This function maps index from the output of _remove_dilations\n            to index from the original input along single axis. It calculates\n            the index inside the input data from the index of the output.\n            It is used to generate the correct indexes of the values to be\n            extracted by gather_nd.\n\n            Args:\n                output_ind: vector with indices from the output to be mapped\n                kernel:     kernel size along the axis\n                dilation:   dilations along the axis\n                stride:     strides along the axis\n            Return:\n                input_ind: calculated indices\n\n            The formula is:\n                input_ind = (output_ind // kernel) * stride +\n                            (output_ind % kernel) * dilation\n\n            Example:\n              If we have following 2D input to _remove_dilations:\n                         [[  0,  1,  2,  3],\n                          [  4,  5,  6,  7],\n                          [  8,  9, 10, 11],\n                          [ 12, 13, 14, 15]]\n              and Kernel = [2, 2], Dilations: [2, 2], Strides: [1, 1]\n\n              the output of _remove_dilations will have shape [4, 4] and\n              _calc_input_ind will be called twice for the two axis 0 (along\n              height) and axis 1 (along width) with\n\n                  output_ind = [0, 1, 2, 3]\n\n              which will result in:\n\n                  input_ind = [0, 2, 1, 3]\n        """"""\n    return (output_ind // kernel) * (stride - kernel * dilation) + \\\n        output_ind * dilation\n\n  def _calc_orig_argmax(self, ind):\n    """"""\n            Map result argxmax to the original input indices\n\n            Maps indices generated by maxpool_with_argmax on top of the\n            dilation reduced input to the orignal input indices\n        """"""\n\n    in_width = self.orig_input_shape[2]\n    num_channels = self.orig_input_shape[3]\n    output_width = self.output_shape[2]\n\n    # mod_floor op is not implemented on GPU\n    # implement it using: a % b = a - (a // b) * b\n\n    # inRow = (ind // num_channels) // output_width\n    # inCol = (ind // num_channels) % output_width\n    # ind_channel = ind % num_channels\n\n    ind_nhw = ind // num_channels\n\n    inRow = ind_nhw // output_width\n    inCol = ind_nhw - (ind_nhw // output_width) * output_width\n\n    ind_channel = ind - ind_nhw * num_channels\n\n    row = self._calc_input_ind(inRow, self.kernel_shape[0], self.dilations[0],\n                               self.strides[0]) - self.pads[0]\n    col = self._calc_input_ind(inCol, self.kernel_shape[1], self.dilations[1],\n                               self.strides[1]) - self.pads[2]\n\n    new_ind = num_channels * (row * in_width + col) + ind_channel\n    return new_ind\n\n  def _remove_dilations(self):\n    """"""\n            This method removes the dilations by extracting the values from\n            the input for every sliding window according to the dilations,\n            strides and kernel size and generates output that can be used by\n            pooling operations with strides = kernel_shape to accomplish\n            dilated pooling\n\n            Example:\n              Input:     [[  0,  1,  2,  3],\n                          [  4,  5,  6,  7],\n                          [  8,  9, 10, 11],\n                          [ 12, 13, 14, 15]]\n\n              Kernel:    [2, 2]\n              Dilations: [2, 2]\n              Strides:   [1, 1]\n\n              Will return:\n                         [[  0,  2,  1,  3],\n                          [  8, 10,  9, 11],\n                          [  4,  6,  5,  7],\n                          [ 12, 14, 13, 15]]\n\n              After max_pool2d with kernel_shape = strides = [2, 2]\n              the result is:\n                         [[ 10, 11],\n                          [ 14, 15]]\n        """"""\n\n    input_shape = tf_shape(self.input)\n    in_spatial_shape = input_shape[1:self.spatial_size + 1]\n\n    channels_count = input_shape[self.spatial_size + 1]\n    # Initialize gather_ind with the range of channels\n    # e.g. [0 1]\n    gather_ind = tf.range(channels_count, dtype=tf.int64)\n    # convert the vector to column vector\n    # in the following logic we use column vectors\n    gather_ind = tf.expand_dims(gather_ind, 1)\n\n    # initilize the output_shape with zeros\n    # self.output_shape will contain the shape of the\n    # output tensor after the loop below is executed\n    self.output_shape = [0] * (self.spatial_size + 2)\n    self.output_shape[0] = input_shape[0]\n    """"""\n            Loop over the input spatial dimensions starting from the\n            last (most internal) going up to the first dimension\n\n            On every step of the loop calculate the output indices and\n            map them to the input indices using `_calc_input_ind`,\n            then ""combine"" with the already calculated indices from the\n            previous dimensions using cartesian product.\n\n            For the following example input:\n\n              Input:     [[  0,  1,  2,  3],\n                          [  4,  5,  6,  7],\n                          [  8,  9, 10, 11],\n                          [ 12, 13, 14, 15]]\n\n              Kernel:    [2, 2]\n              Dilations: [2, 2]\n              Strides:   [1, 1]\n\n            these are the steps that will be executed:\n\n            1. Initilize gather_ind = [[0]]     # we have only 1 channel\n\n            2. Loop step 0 (axis 1):\n                  filter_size = 3\n                  output_size = 4\n                  dim_ind = [[0]\n                             [2]\n                             [1]\n                             [3]]\n\n                  gather_ind = [[0 0]\n                                [2 0]\n                                [1 0]\n                                [3 0]]\n\n            3. Loop step 1 (axis 0):\n                  filter_size = 3\n                  output_size = 4\n                  dim_ind = [[0]\n                             [2]\n                             [1]\n                             [3]]\n\n                  gather_ind = [[0 0 0]\n                                [0 2 0]\n                                [0 1 0]\n                                [0 3 0]\n                                [2 0 0]\n                                [2 2 0]\n                                [2 1 0]\n                                [2 3 0]\n                                [1 0 0]\n                                [1 2 0]\n                                [1 1 0]\n                                [1 3 0]\n                                [3 0 0]\n                                [3 2 0]\n                                [3 1 0]\n                                [3 3 0]]\n\n            These are the indices used for gather_nd operation to collect\n            the values from the input data.\n        """"""\n\n    for dim in range(self.spatial_size - 1, -1, -1):\n      filter_size = (self.kernel_shape[dim] - 1) * \\\n                     self.dilations[dim] + 1\n      output_size = ((\n          (in_spatial_shape[dim] - filter_size) // self.strides[dim]) + 1\n                    ) * self.kernel_shape[dim]\n      self.output_shape[dim + 1] = output_size\n\n      # initialize the output dimension index with the range of the\n      # dimension output size (e.g. 4): [0, 1, 2, 3]\n      dim_ind = tf.range(output_size)\n\n      # calculate the matching indices in the input data\n      # [0, 1, 2, 3] will calculate to [0, 2, 1, 3]\n      # from the above example\n      dim_ind = self._calc_input_ind(dim_ind, self.kernel_shape[dim],\n                                     self.dilations[dim], self.strides[dim])\n      # convert to column vector\n      dim_ind = tf.expand_dims(dim_ind, 1)\n\n      # ""combine"" current dimension indices with the previous dimensions\n      # using cartesian product\n      gather_ind = tf_product(dim_ind, gather_ind)\n\n    # The result from the above loop for 2D data will be:\n    # [[y1, x1, c], [y2, x2, c], ..., [yn, xm, c]] where n is the height,\n    # m is the width and c is the channel number.\n\n    # set the channels count in the output_shape\n    self.output_shape[self.spatial_size + 1] = channels_count\n\n    # expand the dimensions to match the input dimensions + 1\n    for x in range(self.spatial_size):\n      gather_ind = tf.expand_dims(gather_ind, 0)\n    # dublicate the indices for every batch\n    gather_ind = tf.tile(gather_ind,\n                         [input_shape[0]] + [1] * (self.spatial_size + 1))\n\n    # extract the selected values from the input\n    output = tf.gather_nd(self.input, gather_ind, batch_dims=1)\n    # reshape the output to the correct shape calculated earlier\n    output = tf.reshape(output, self.output_shape)\n\n    return output\n\n  def _calc_pads_same(self, in_spatial_shape):\n    """"""\n            Calculate SAME_* paddings.\n        """"""\n\n    pad_ops = pooling_helper.pad_numpy_ops if self.is_known_shape else \\\n        pooling_helper.pad_tf_ops\n\n    return pooling_helper.calc_pads_same(in_spatial_shape, self.kernel_shape,\n                                         self.strides, self.dilations,\n                                         self.padding, pad_ops, 2)\n\n  def _calc_pads_explicit(self):\n    """"""\n            Calculate explicit padding\n        """"""\n    assert type(self.padding) is list\n\n    pads = []\n    for i in range(self.spatial_size):\n      pads += [self.padding[i], self.padding[i + self.spatial_size]]\n    return pads\n\n  def _calc_pads_ceil_mode(self, in_spatial_shape):\n    """"""\n            Calculate padding in ceil_mode\n        """"""\n\n    pads = []\n    for i in range(self.spatial_size):\n      dim_size = in_spatial_shape[i]\n      filter_size = (self.kernel_shape[i] - 1) * self.dilations[i] + 1\n      out_size = (dim_size - filter_size) / self.strides[i]\n      if self.is_known_shape:\n        pad_size = (np.ceil(out_size) - np.floor(out_size)).astype(np.int64)\n      else:\n        pad_size = tf.cast(\n            tf.math.ceil(out_size) - tf.math.floor(out_size), tf.int64)\n\n      pads += [0, pad_size * self.strides[i]]\n    return pads\n\n  def _calc_pads(self, in_spatial_shape):\n    if self.is_known_shape:\n      pads = np.zeros([self.spatial_size * 2], np.int64)\n    else:\n      pads = tf.zeros([self.spatial_size * 2], tf.int64)\n\n    # check for explicit padding\n    if type(self.padding) is list:\n      pads += self._calc_pads_explicit()\n    elif self.padding.lower().startswith(""same""):\n      pads += self._calc_pads_same(in_spatial_shape)\n\n    # when padding is set to SAME, ceil_mode will not do anything\n    # because output sizes will be multiple of the strides\n    if self.ceil_mode and (type(self.padding) is list or\n                           not self.padding.lower().startswith(""same"")):\n      new_spatial_shape = [\n          in_spatial_shape[i] + pads[i * 2] + pads[i * 2 + 1]\n          for i in range(self.spatial_size)\n      ]\n      pads += self._calc_pads_ceil_mode(new_spatial_shape)\n    return pads\n\n  def _pad_input(self):\n    """"""\n            Pad the input according to the parameters\n        """"""\n    # check if we need to do any padding at all\n    if not self.ceil_mode and ((type(self.padding) is list and\n                                self.padding == [0] * self.spatial_size * 2) or\n                               self.padding == ""VALID""):\n      self.pads = np.array([0] * self.spatial_size * 2)\n      return (self.input, self.pads)\n\n    in_spatial_shape = self.input_shape[1:self.spatial_size + 1]\n    pads = self._calc_pads(in_spatial_shape)\n\n    if self.is_known_shape and np.count_nonzero(pads) == 0:\n      self.pads = pads\n      return (self.input, pads)\n\n    tf_paddings = [[0, 0]]\n    for i in range(self.spatial_size):\n      tf_paddings += [[pads[i * 2], pads[i * 2 + 1]]]\n    tf_paddings += [[0, 0]]\n\n    self.input = tf.pad(\n        self.input,\n        tf_paddings,\n        mode=\'CONSTANT\',\n        constant_values=self.padding_constant)\n    # update input shape and pads values\n    self.input_shape = tf_shape(self.input)\n    self.pads = pads\n\n  def _calc_argmax_without_padding(self, ind):\n    """"""\n            Calculate the original indices as they would be without padding\n        """"""\n    in_width = self.orig_input_shape[2]\n    padded_width = self.input_shape[2]\n    num_channels = self.input_shape[3]\n\n    # mod_floor op is not implemented on GPU\n    # implement it using: a % b = a - (a // b) * b\n\n    # ind_nhw = ind // num_channels\n    # ind_channel = ind % num_channels\n\n    ind_nhw = ind // num_channels\n    ind_channel = ind - ind_nhw * num_channels\n\n    new_ind = (ind_nhw // padded_width) * (self.pads[2] + self.pads[3])\n    new_ind = ind_nhw - new_ind - self.pads[0] * in_width - self.pads[2]\n    new_ind = num_channels * new_ind + ind_channel\n    return new_ind\n\n  def dilated_maxpool_with_argmax(self, force_custom_impl=False):\n    """"""\n            Do a dilated maxpool and return indices/argmax\n        """"""\n    # Tensorflow does not support maxpool_with_argmax on\n    # spatial_size != 2\n    assert self.spatial_size == 2\n\n    if list(self.dilations) != [1] * self.spatial_size or \\\n       force_custom_impl:\n      # pad the input\n      self._pad_input()\n\n      new_input = self._remove_dilations()\n      kernel_shape = [1] + list(self.kernel_shape) + [1]\n      pooled, new_ind = tf.nn.max_pool_with_argmax(\n          new_input, ksize=kernel_shape, strides=kernel_shape, padding=""VALID"")\n      new_ind = self._calc_orig_argmax(new_ind)\n    else:\n      self.pads = np.array([0] * self.spatial_size * 2)\n      if type(self.padding) is list or \\\n        self.padding.lower() == ""same_lower"":\n        # pad the input\n        self._pad_input()\n\n        padding_ = ""VALID""\n      elif self.padding.lower() == ""same_upper"":\n        padding_ = ""SAME""\n      else:\n        padding_ = self.padding\n\n      strides = [1] + list(self.strides) + [1]\n      kernel_shape = [1] + list(self.kernel_shape) + [1]\n      pooled, new_ind = tf.nn.max_pool_with_argmax(\n          self.input, ksize=kernel_shape, strides=strides, padding=padding_)\n      # if there was padding, recalculate the returned index\n      # to exclude the padding\n      if np.count_nonzero(self.pads) != 0:\n        new_ind = self._calc_argmax_without_padding(new_ind)\n\n    return (pooled, new_ind)\n\n  def dilated_pool(self, force_custom_impl=False):\n    """"""\n            Does N-D dilated max/avg pooling. Pads the input if explicit or\n            SAME_* padding is provided or ceil_mode is True\n        """"""\n\n    assert self.is_supported()\n\n    if self.is_explicit_padding or self.padding.lower() == ""same_lower"" \\\n            or (self.padding.lower() == ""same_upper"" and\n                self.count_include_pad):\n      # pad the input\n      self._pad_input()\n\n      padding_ = ""VALID""\n    elif self.padding.lower() == ""same_upper"":\n      padding_ = ""SAME""\n    else:\n      padding_ = self.padding\n\n    # if maxpool op with dialtions != 1 and spatial_size == 2\n    # we can use tf.nn.dilation2d directly\n    if self.spatial_size == 2 and self.pooling_type.startswith(""MAX"") \\\n            and self.dilations != [1] * self.spatial_size and \\\n            not force_custom_impl:\n      strides = [1] + list(self.strides) + [1]\n      dilations = [1] + list(self.dilations) + [1]\n\n      filter = tf.zeros(\n          [self.kernel_shape[0], self.kernel_shape[1], self.input_shape[3]],\n          self.input.dtype)\n      pooled = tf.nn.dilation2d(\n          input=self.input,\n          filters=filter,\n          strides=strides,\n          dilations=dilations,\n          padding=padding_,\n          data_format=""NHWC"")\n    # if spatial_size < 4 and strides == 1 or dilation == 1 use tf.nn.pool\n    elif self.spatial_size < 4 and (self.strides == [1] * self.spatial_size or\n            self.dilations == [1] * self.spatial_size) and \\\n            not force_custom_impl:\n      # if strides == 1 use tf.nn.pool directly\n      if self.strides == [1] * self.spatial_size:\n        pooled = tf.nn.pool(\n            self.input,\n            window_shape=self.kernel_shape,\n            dilations=self.dilations,\n            strides=self.strides,\n            padding=padding_,\n            pooling_type=self.pooling_type)\n      else:\n        # othwerwise check the pooling_type and use the correct op\n        if self.pooling_type.startswith(""MAX""):\n          op = tf.nn.max_pool\n        elif self.pooling_type == ""AVG"":\n          op = tf.nn.avg_pool\n        else:\n          raise ValueError(""%d-D %s pooling is not supported."" %\n                           (self.spatial_size, self.pooling_type))\n        pooled = op(self.input, ksize=self.kernel_shape, strides=self.strides,\n                    padding=padding_)\n    # in any other case we use custom implementation _remove_dilations\n    # to reduce atrous/dilated pooling into regular pooling and selecting\n    # only the values of the input that should have been selected by\n    # applying the strides and dilations. Then use tf.nn.pool with\n    # strides = kernel_shape and no dilations\n    else:\n      if padding_ == ""SAME"":\n        # pad the input\n        self._pad_input()\n      input_ = self._remove_dilations()\n      pooled = tf.nn.pool(\n          input_,\n          window_shape=self.kernel_shape,\n          strides=self.kernel_shape,\n          padding=""VALID"",\n          pooling_type=self.pooling_type)\n    return pooled\n\n  def is_supported(self):\n    """"""\n            Function to check if the current set of arguments are\n            supported for average pool\n        """"""\n    # check for maxpool\n    if self.pooling_type.startswith(""MAX""):\n      return True\n    else:\n      # if count_include_pad is true it is fully supported\n      if self.count_include_pad:\n        return True\n      # ceil mode is not supported\n      elif self.ceil_mode:\n        return False\n      # explicit padding with padding values set to 0 is supported\n      elif (self.is_explicit_padding and\n            self.padding == [0] * self.spatial_size * 2):\n        return True\n      # ""valid"" and ""same_upper"" auto padding is supported\n      elif (not self.is_explicit_padding and\n            self.padding.lower() in [""valid"", ""same_upper""]):\n        return True\n      # any other case is not supported\n      else:\n        return False\n'"
onnx_tf/handlers/backend/div.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ArithmeticMixin\n\n\n@onnx_op(""Div"")\n@tf_func(tf.math.truediv)\nclass Div(ArithmeticMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/dropout.py,4,"b'import copy\n\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Dropout"")\n@tf_func(tf.nn.dropout)\nclass Dropout(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    attrs = copy.deepcopy(node.attrs)\n    if cls.SINCE_VERSION >= 7 or attrs.pop(""is_test"", 0) == 1:\n      return [x]\n    attrs[""keep_prob""] = 1 - attrs.pop(""ratio"", 0.5)\n    return [cls.make_tensor_from_onnx_node(node, attrs=attrs, **kwargs)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/dynamic_quantize_linear.py,9,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""DynamicQuantizeLinear"")\nclass DynamicQuantizeLinear(BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    # A Function to fuse calculation for Scale, Zero Point and FP32->8Bit convertion of FP32 Input data.\n\n    # Scale is calculated as:\n    #   y_scale = (max(x) - min(x))/(qmax - qmin)\n    # Zero point is calculated as:\n    #   intermediate_zero_point = qmin - min(x)/y_scale\n    #   y_zero_point = cast(round(saturate(intermediate_zero_point)))\n    # Data quantization formula is:\n    #   y = saturate(round(x / y_scale) + y_zero_point)\n    # Only uint8 is supported, so saturation range is [0, 255]\n\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    dtype = tf.uint8\n    qmin = dtype.min\n    qmax = dtype.max\n    min_x = tf.math.minimum(0., tf.math.reduce_min(x))\n    max_x = tf.math.maximum(0., tf.math.reduce_max(x))\n    y_scale = (max_x - min_x) / (qmax - qmin)\n    intermediate_zero_point = qmin - (min_x / y_scale)\n    y_zero_point = tf.clip_by_value(tf.round(intermediate_zero_point), qmin,\n                                    qmax)\n    y = tf.cast(\n        tf.clip_by_value((tf.round(x / y_scale) + y_zero_point), qmin, qmax),\n        dtype)\n\n    return [y, y_scale, tf.cast(y_zero_point, dtype)]\n'"
onnx_tf/handlers/backend/elu.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Elu"")\n@tf_func(tf.nn.elu)\nclass Elu(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    alpha = node.attrs.get(""alpha"", 1.0)\n    if alpha != 1.0:\n      return [\n          tf.cast(x < 0.0, tf.float32) * alpha *\n          (tf.exp(x) - 1.0) + tf.cast(x >= 0.0, tf.float32) * x\n      ]\n    else:\n      return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/equal.py,10,"b'import tensorflow as tf\n\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom .control_flow_mixin import ComparisonMixin\n\n\n@onnx_op(""Equal"")\n@tf_func(tf.equal)\n@partial_support(True)\n@ps_description(\n    ""Equal inputs in uint16/uint32/uint64 "" + ""are not supported in Tensorflow.""\n)\nclass Equal(ComparisonMixin, BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    supported_dtype = [\n        tf.bfloat16, tf.half, tf.float32, tf.float64, tf.uint8, tf.int8,\n        tf.int16, tf.int32, tf.int64, tf.complex64, tf.quint8, tf.qint8,\n        tf.qint32, tf.string, tf.bool, tf.complex128\n    ]\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    if x.dtype not in supported_dtype:\n      exception.OP_UNSUPPORTED_EXCEPT(\n          ""Equal inputs in "" + str(x.dtype) + "" which"", ""Tensorflow"")\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/erf.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Erf"")\n@tf_func(tf.math.erf)\nclass Erf(BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/exp.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Exp"")\n@tf_func(tf.exp)\nclass Exp(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/expand.py,3,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""Expand"")\nclass Expand(BackendHandler):\n\n  @classmethod\n  def version_8(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x, shape = tensor_dict[node.inputs[0]], tensor_dict[node.inputs[1]]\n    ones = tf.ones(shape, dtype=x.dtype)\n    return [x * ones]\n'"
onnx_tf/handlers/backend/eye_like.py,14,"b'import numpy as np\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""EyeLike"")\nclass EyeLike(BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n\n    inp = kwargs[""tensor_dict""][node.inputs[0]]\n    dtype = node.attrs.pop(""dtype"", inp.dtype)\n    offset = node.attrs.pop(""k"", 0)\n\n    # If the shape of input is static, then the handler\n    # can use python code to calculate the eye shape and\n    # paddings for pad in the graph generating phase.\n    # Then the graph will only contains two nodes:\n    # tf.eye and tf.pad\n    if inp.shape.is_fully_defined():\n\n      shape = inp.shape.as_list()\n      # calculate upper and lower bound of max eye shape\n      max_eye_shape_ub = shape[1] if offset > 0 else shape[0]\n      max_eye_shape_lb = shape[0] if offset > 0 else shape[1]\n      # adjust offset base on max_eye_shape_ub\n      offset = max_eye_shape_ub * np.sign(offset) if abs(\n          offset) > max_eye_shape_ub else offset\n      abs_offset = abs(offset)\n      eye_shape = min(max_eye_shape_ub - abs_offset, max_eye_shape_lb)\n      tensor = tf.eye(eye_shape, num_columns=eye_shape, dtype=dtype)\n      if offset > 0:\n        tb_paddings = [0, shape[0] - eye_shape]\n        lr_paddings = [offset, shape[1] - offset - eye_shape]\n      else:\n        tb_paddings = [abs_offset, shape[0] - abs_offset - eye_shape]\n        lr_paddings = [0, shape[1] - eye_shape]\n      paddings = tf.constant([tb_paddings, lr_paddings], dtype=tf.int32)\n      return [\n          cls.make_tensor_from_onnx_node(node,\n                                         tf_func=tf.pad,\n                                         inputs=[tensor, paddings],\n                                         **kwargs)\n      ]\n\n    # if the input shape is not defined yet during the graph generaring\n    # phase, then need to perform the eye shape and paddings calculation\n    # during the execution phase. Therefore the graph will be bigger.\n    # Using tf.funtion to let Tensorflow auto generate a graph out of\n    # our python code would be the best option in this case to get the\n    # smallest graph possible\n    else:\n\n      @tf.function\n      def create_nodes(inp, offset, paddings):\n        shape = tf.shape(inp, out_type=tf.int32)\n        # calculate upper and lower bound of max eye shape\n        max_eye_shape_ub = shape[1] if offset > 0 else shape[0]\n        max_eye_shape_lb = shape[0] if offset > 0 else shape[1]\n        # adjust offset base on max_eye_shape_ub\n        offset = max_eye_shape_ub * np.sign(offset) if abs(\n            offset) > max_eye_shape_ub else offset\n        abs_offset = abs(offset)\n        eye_shape = tf.minimum(max_eye_shape_ub - abs_offset, max_eye_shape_lb)\n        tensor = tf.eye(eye_shape, num_columns=eye_shape, dtype=dtype)\n        if offset > 0:\n          tb_paddings = [0, shape[0] - eye_shape]\n          lr_paddings = [offset, shape[1] - offset - eye_shape]\n        else:\n          tb_paddings = [abs_offset, shape[0] - abs_offset - eye_shape]\n          lr_paddings = [0, shape[1] - eye_shape]\n        paddings = paddings.assign([tb_paddings, lr_paddings])\n        return tensor, paddings\n\n      paddings = tf.Variable([[0, 0], [0, 0]], dtype=tf.int32)\n      tensor, paddings = create_nodes(inp, offset, paddings)\n      return [\n          cls.make_tensor_from_onnx_node(node,\n                                         tf_func=tf.pad,\n                                         inputs=[tensor, paddings],\n                                         **kwargs)\n      ]\n'"
onnx_tf/handlers/backend/flatten.py,8,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Flatten"")\n@tf_func(tf.keras.backend.flatten)\nclass Flatten(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    shape = tf.shape(x)\n    axis = node.attrs.get(""axis"", 1)\n\n    if axis == 0:\n      cal_shape = (1, -1)\n    else:\n      cal_shape = (tf.reduce_prod(shape[0:axis]),\n                   tf.reduce_prod(shape[axis:tf.size(shape)]))\n    return [tf.reshape(x, cal_shape)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/floor.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Floor"")\n@tf_func(tf.floor)\nclass Floor(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/gather.py,5,"b'import copy\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .gather_and_scatter_mixin import GatherAndScatterMixin\n\n\n@onnx_op(""Gather"")\n@tf_func(tf.gather)\nclass Gather(GatherAndScatterMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    indices = kwargs[""tensor_dict""][node.inputs[1]]\n    attrs = copy.deepcopy(node.attrs)\n    axis = attrs.get(""axis"", 0)\n    result = cls.chk_idx_out_of_bounds_along_axis(x, axis, indices)\n    msg = \'Gather indices are out of bounds, please double check the indices and retry.\'\n    with tf.control_dependencies([tf.compat.v1.assert_equal(result, True, message=msg)]):\n      indices = cls.process_neg_idx_along_axis(x, axis, indices)\n      attrs[\'axis\'] = axis\n      return [cls.make_tensor_from_onnx_node(node, attrs=attrs, inputs=[x, indices], **kwargs)]\n'"
onnx_tf/handlers/backend/gather_and_scatter_mixin.py,17,"b'import tensorflow as tf\n\nfrom onnx_tf.common.tf_helper import tf_shape\n\n\nclass GatherAndScatterMixin(object):\n\n  @classmethod\n  def chk_idx_out_of_bounds(cls, data, indices):\n    """""" Check indices out of bounds for ScatterND and GatherND\n    In Tensorflow GPU version, if an out of bound index is found,\n    a 0 is stored in the corresponding output value for GatherND;\n    and the index is ignored for ScatterND/TensorScatterNDUpdate.\n    But ONNX spec state that it is an error if any index values\n    are out of bounds. Therefore the converter need to run this\n    function to verify all the indices are in bounds before send\n    it to Tensoflow. If out of bound is detected then the caller\n    of this function need to throw InvalidArgumentError exception.\n    """"""\n    data_shape = tf_shape(data)\n    indices_shape = tf_shape(indices)\n\n    def _chk_idx_out_of_bounds(i, result):\n      indices_i = tf.transpose(indices)[i]\n      limit_i = tf.cast(data_shape, indices.dtype)[i]\n      cond1 = tf.greater_equal(indices_i, tf.negative(limit_i))\n      cond2 = tf.less(indices_i, limit_i)\n      result = tf.reduce_all(tf.logical_and(cond1, cond2))\n      return i + 1, result\n\n    _, result = tf.while_loop(\n        lambda i, result: tf.logical_and(tf.less(i, indices_shape[-1]), result),\n        _chk_idx_out_of_bounds, [tf.zeros([], tf.int64), True])\n    return result\n\n  @classmethod\n  def chk_idx_out_of_bounds_along_axis(cls, data, axis, indices):\n    """""" Check indices out of bounds for ScatterElement\n    In Tensorflow GPU version, if an out of bound index is found,\n    the index is ignored for ScatterND/TensorScatterNDUpdate.\n    But ONNX spec state that it is an error if any index values\n    are out of bounds. Therefore the converter need to run this\n    function to verify all the indices are in bounds along the\n    axis before send it to Tensoflow. If out of bound is detected\n    then the caller of this function need to throw\n    InvalidArgumentError exception.\n    """"""\n    data_shape = tf.cast(tf_shape(data), indices.dtype)\n    limit = data_shape[axis]\n    cond1 = tf.greater_equal(indices, tf.negative(limit))\n    cond2 = tf.less(indices, limit)\n    return tf.logical_and(cond1, cond2)\n\n  @classmethod\n  def process_neg_idx(cls, data, indices):\n    """""" Convert all the negative indices to positive\n    GatherND and ScatterND/TensorScatterNDUpdate in Tensorflow\n    doesn\'t support negative indices. Therefore need to run this\n    function to convert all the negative indices to positive before\n    send it to Tensorflow.\n    """"""\n    data_shape = tf_shape(data)\n    indices_shape = tf_shape(indices)\n    max_i = tf.cast(data_shape[:indices_shape[-1]], indices.dtype)\n    return tf.math.floormod(tf.add(indices, max_i), max_i)\n\n  @classmethod\n  def process_neg_idx_along_axis(cls, data, axis, indices):\n    """""" Convert all the negative indices to positive\n    ScatterND/TensorScatterNDUpdate in Tensorflow doesn\'t support\n    negative indices. Therefore need to run this function to convert\n    all the negative indices to positive before send it to Tensorflow.\n    """"""\n    data_shape = tf_shape(data)\n    max_i = tf.cast(data_shape[axis], indices.dtype)\n    return tf.math.floormod(tf.add(indices, max_i), max_i)\n'"
onnx_tf/handlers/backend/gather_nd.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .gather_and_scatter_mixin import GatherAndScatterMixin\n\n\n@onnx_op(""GatherND"")\n@tf_func(tf.gather_nd)\nclass GatherND(GatherAndScatterMixin, BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    data = kwargs[""tensor_dict""][node.inputs[0]]\n    indices = kwargs[""tensor_dict""][node.inputs[1]]\n\n    result = cls.chk_idx_out_of_bounds(data, indices)\n    msg = \'GatherND indices are out of bounds, please double check the indices and retry.\'\n    with tf.control_dependencies(\n        [tf.compat.v1.assert_equal(result, True, message=msg)]):\n      indices = cls.process_neg_idx(data, indices)\n      return [\n          cls.make_tensor_from_onnx_node(node, inputs=[data, indices], **kwargs)\n      ]\n'"
onnx_tf/handlers/backend/gemm.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""Gemm"")\nclass Gemm(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    x = tf.keras.layers.Flatten()(x)\n    y = tensor_dict[node.inputs[1]]\n\n    if len(node.inputs) > 2:\n      z = tensor_dict[node.inputs[2]]\n    else:\n      z = 0\n\n    if node.attrs.get(""transA"", 0):\n      x = tf.transpose(x)\n    if node.attrs.get(""transB"", 0):\n      y = tf.transpose(y)\n    alpha = node.attrs.get(""alpha"", 1.0)\n    beta = node.attrs.get(""beta"", 1.0)\n\n    return [alpha * tf.matmul(x, y) + beta * z]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/global_average_pool.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""GlobalAveragePool"")\nclass GlobalAveragePool(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    dims = tf.range(tf.rank(x))\n    _, dim_window = tf.split(dims, [2, tf.size(dims) - 2])\n    return [tf.reduce_mean(x, axis=dim_window, keepdims=True)]\n'"
onnx_tf/handlers/backend/global_lp_pool.py,3,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""GlobalLpPool"")\nclass GlobalLpPool(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    p = node.attrs.get(""p"", 2.)\n    dims = list(range(len(x.shape)))\n    dim_window = dims[2:]\n    if len(dim_window) > 1 and p == 2:\n      p = ""euclidean""\n    return [tf.norm(x, ord=p, axis=dim_window, keepdims=True)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_2(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/global_max_pool.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""GlobalMaxPool"")\nclass GlobalMaxPool(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    dims = tf.range(tf.rank(x))\n    _, dim_window = tf.split(dims, [2, tf.size(dims) - 2])\n    return [tf.reduce_max(x, axis=dim_window, keepdims=True)]\n'"
onnx_tf/handlers/backend/greater.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .control_flow_mixin import ComparisonMixin\n\n\n@onnx_op(""Greater"")\n@tf_func(tf.greater)\nclass Greater(ComparisonMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/gru.py,38,"b'from functools import partial\n\nimport tensorflow as tf\n\nfrom onnx_tf.common import get_unique_suffix\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom .rnn_mixin import RNNMixin\n\n\n@onnx_op(""GRU"")\n@partial_support(True)\n@ps_description(\n    ""GRU with clip or GRU with linear_before_reset, or "" +\n    ""GRU not using sigmoid for z and r, or "" +\n    ""GRU using Elu as the activation function "" + ""with alpha != 1, or "" +\n    ""GRU using HardSigmoid as the activation function "" +\n    ""with alpha != 0.2 or beta != 0.5 "" + ""are not supported in TensorFlow."")\nclass GRU(RNNMixin, BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    direction = node.attrs.get(""direction"", ""forward"")\n    num_directions = 2 if direction == ""bidirectional"" else 1\n    if ""clip"" in node.attrs:\n      exception.OP_UNSUPPORTED_EXCEPT(""GRU with clip"", ""Tensorflow"")\n    if node.attrs.get(""linear_before_reset"", 0):\n      exception.OP_UNSUPPORTED_EXCEPT(""GRU with linear_before_reset"",\n                                      ""Tensorflow"")\n    if ""activations"" in node.attrs:\n      activations = list(map(lambda x: x.lower(), node.attrs[""activations""]))\n      if activations[0] != ""sigmoid"":\n        exception.OP_UNSUPPORTED_EXCEPT(""GRU without sigmoid for `z` and `r`"",\n                                        ""Tensorflow"")\n      if num_directions == 2:\n        if activations[2] != ""sigmoid"":\n          exception.OP_UNSUPPORTED_EXCEPT(""GRU without sigmoid for `z` and `r`"",\n                                          ""Tensorflow"")\n\n  @classmethod\n  def _custom_getter(cls,\n                     getter,\n                     name,\n                     node=None,\n                     tensor_dict=None,\n                     is_bidirectional=None,\n                     *args,\n                     **kwargs):\n    names = name.split(""/"")\n    if is_bidirectional:\n      if ""fw"" in names:\n        index = 0\n      elif ""bw"" in names:\n        index = 1\n      else:\n        raise RuntimeError(""Can not get {} for bidirectional. ""\n                           ""Either fw and bw is not in name scope."".format(\n                               names[-1]))\n    if names[-1] == ""kernel"":\n      # onnx W[zrh], R[zrh]\n      if is_bidirectional:\n        w = tf.split(tensor_dict[node.inputs[1]], 2)[index]\n        r = tf.split(tensor_dict[node.inputs[2]], 2)[index]\n      else:\n        w = tensor_dict[node.inputs[1]]\n        r = tensor_dict[node.inputs[2]]\n      w_z, w_r, w_h = tf.split(tf.squeeze(w), 3)\n      r_z, r_r, r_h = tf.split(tf.squeeze(r), 3)\n      if names[-2] == ""gates"":\n        new_w = tf.transpose(tf.concat([w_r, w_z], 0))\n        new_r = tf.transpose(tf.concat([r_r, r_z], 0))\n      elif names[-2] == ""candidate"":\n        new_w = tf.transpose(w_h)\n        new_r = tf.transpose(r_h)\n      kernel = tf.concat([new_w, new_r], 0)\n      return kernel\n    if names[-1] == ""bias"":\n      if len(node.inputs) >= 4:\n        # onnx Wb[zrh], Rb[zrh]\n        if is_bidirectional:\n          b = tf.split(tensor_dict[node.inputs[3]], 2)[index]\n        else:\n          b = tensor_dict[node.inputs[3]]\n        w_b, r_b = tf.split(tf.squeeze(b), 2)\n        w_b_z, w_b_r, w_b_h = tf.split(w_b, 3)\n        r_b_z, r_b_r, r_b_h = tf.split(r_b, 3)\n        if names[-2] == ""gates"":\n          w_b = tf.transpose(tf.concat([w_b_r, w_b_z], 0))\n          r_b = tf.transpose(tf.concat([r_b_r, r_b_z], 0))\n        elif names[-2] == ""candidate"":\n          w_b = tf.transpose(w_b_h)\n          r_b = tf.transpose(r_b_h)\n        return tf.add(w_b, r_b)\n      return getter(name, *args, **kwargs)\n    return getter(name, *args, **kwargs)\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    input_shape = x.get_shape().as_list()\n    input_size = len(node.inputs)\n    hidden_size = node.attrs[""hidden_size""]\n    direction = node.attrs.get(""direction"", ""forward"")\n    num_directions = 2 if direction == ""bidirectional"" else 1\n\n    # removed from version 7, default is 0\n    output_sequence = node.attrs.get(""output_sequence"", 0)\n\n    # TODO(fumihwh): check if prev node is one of RNN\n    # process input if it comes from other previous cell\n    # which has shape [seq_length, num_directions, batch_size, hidden_size]\n    if len(input_shape) == 4 and input_shape[1] == 1:\n      x = tf.squeeze(x)\n\n    sequence_length = None\n    if input_size >= 5 and node.inputs[4] in tensor_dict:\n      sequence_length = tensor_dict[node.inputs[4]]\n\n    cell_kwargs = {}\n\n    tf_activations = [tf.nn.tanh]\n    if ""activations"" in node.attrs:\n      activations = list(map(lambda x: x.lower(), node.attrs[""activations""]))\n      activation_alpha = node.attrs.get(""activation_alpha"", [None] * 4)\n      activation_beta = node.attrs.get(""activation_beta"", [None] * 4)\n      tf_activations = [\n          cls.rnn_get_activation(activations[1], activation_alpha[1],\n                                 activation_beta[1])\n      ]\n      if num_directions == 2:\n        tf_activations.append(\n            cls.rnn_get_activation(activations[3], activation_alpha[3],\n                                   activation_beta[3]))\n\n    # TODO(fumihwh): check if reverse and bidirectional works\n    with tf.compat.v1.variable_scope(\n        ""GRU_"" + get_unique_suffix(),\n        custom_getter=partial(\n            cls._custom_getter,\n            node=node,\n            tensor_dict=tensor_dict,\n            is_bidirectional=num_directions == 2)):\n\n      cell_kwargs[""num_units""] = hidden_size\n      if input_size < 4 or node.inputs[3] not in tensor_dict:\n        cell_kwargs[""bias_initializer""] = tf.zeros_initializer\n      initial_state = None\n      initial_state_bw = None\n      if input_size == 6:\n        initial_h = tensor_dict.get(node.inputs[5], None)\n        if initial_h is not None:\n          initial_state = (initial_h[0],)\n          if num_directions == 2:\n            initial_state_bw = (initial_h[1],)\n\n      rnn_kwargs = {}\n      if num_directions == 1:\n        rnn_kwargs[""initial_state""] = initial_state\n      elif num_directions == 2:\n        rnn_kwargs[""initial_state_fw""] = initial_state\n        rnn_kwargs[""initial_state_bw""] = initial_state_bw\n      rnn_kwargs[""sequence_length""] = sequence_length\n      rnn_kwargs[""time_major""] = True\n      rnn_kwargs[""dtype""] = tf.float32\n\n      outputs, states = cls.rnn(x, tf.compat.v1.nn.rnn_cell.GRUCell,\n                                cell_kwargs, rnn_kwargs, tf_activations,\n                                direction)\n\n    if num_directions == 1:\n      state = states[0]\n      h = tf.expand_dims(state, 0)\n      output = tf.expand_dims(outputs, 1)\n    else:\n      state_fw = states[0][0]\n      state_bw = states[1][0]\n      output_fw = outputs[0]\n      output_bw = outputs[1]\n      h_fw = tf.expand_dims(state_fw, 0)\n      h_bw = tf.expand_dims(state_bw, 0)\n      h = tf.concat((h_fw, h_bw), axis=0)\n      output_fw = tf.expand_dims(output_fw, 1)\n      output_bw = tf.expand_dims(output_bw, 1)\n      output = tf.concat((output_fw, output_bw), axis=1)\n\n    return [output, h] if output_sequence == 0 else [h]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_3(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/hard_sigmoid.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""HardSigmoid"")\nclass HardSigmoid(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    if ""alpha"" not in node.attrs and ""beta"" not in node.attrs:\n      return [tf.keras.backend.hard_sigmoid(x)]\n\n    alpha = node.attrs.get(""alpha"", 0.2)\n    beta = node.attrs.get(""beta"", 0.5)\n    return [tf.clip_by_value(x * alpha + beta, 0, 1)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/hardmax.py,8,"b'import numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Hardmax"")\n@tf_func(tfa.seq2seq.hardmax)\nclass Hardmax(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    axis = node.attrs.get(""axis"", 1)\n    axis = axis if axis >= 0 else len(np.shape(x)) + axis\n\n    if axis == len(np.shape(x)) - 1:\n      return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n    shape = tf.shape(x)\n    cal_shape = (tf.reduce_prod(shape[0:axis]),\n                 tf.reduce_prod(shape[axis:tf.size(shape)]))\n    x = tf.reshape(x, cal_shape)\n\n    return [tf.reshape(tfa.seq2seq.hardmax(x), shape)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/identity.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Identity"")\n@tf_func(tf.identity)\nclass Identity(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/if.py,6,"b'import tensorflow as tf\n\nimport onnx_tf\nfrom onnx.helper import make_opsetid\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""If"")\n@tf_func(tf.cond)\nclass If(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    cond = kwargs[""tensor_dict""][node.inputs[0]]\n    then_branch = node.attrs[""then_branch""]\n    else_branch = node.attrs[""else_branch""]\n    current_opset = [make_opsetid(cls.DOMAIN, cls.VERSION)]\n\n    def true_fn():\n      subgraph_tensor_dict = onnx_tf.backend.onnx_graph_to_tensorflow_ops(\n          subgraph=then_branch,\n          input_values={}, # all inputs of then_branch are in tensor_dict\n          tensor_dict=kwargs[""tensor_dict""],\n          opset=current_opset)\n      return [subgraph_tensor_dict[o.name] for o in then_branch.output]\n\n    def false_fn():\n      subgraph_tensor_dict = onnx_tf.backend.onnx_graph_to_tensorflow_ops(\n          subgraph=else_branch,\n          input_values={}, # all inputs of else_branch are in tensor_dict\n          tensor_dict=kwargs[""tensor_dict""],\n          opset=current_opset)\n      return [subgraph_tensor_dict[o.name] for o in else_branch.output]\n\n    return [\n        cls.make_tensor_from_onnx_node(node, inputs=[cond, true_fn, false_fn])\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/image_scaler.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""ImageScaler"")\nclass ImageScaler(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    input_dict = kwargs[""tensor_dict""]\n    x = input_dict[node.inputs[0]]\n    scale = node.attrs.get(""scale"", 1.0)\n    output = tf.multiply(x, scale)\n    if ""bias"" in node.attrs:\n      bias = node.attrs[""bias""]\n      output = tf.nn.bias_add(output, bias, data_format=""NCHW"")\n    return [output]\n'"
onnx_tf/handlers/backend/instance_normalization.py,7,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""InstanceNormalization"")\n@tf_func(tf.nn.batch_normalization)\nclass InstanceNormalization(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {\n        ""default"": {\n            ""epsilon"": 1e-5\n        },\n        ""rename"": {\n            ""epsilon"": ""variance_epsilon""\n        }\n    }\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    # this file is adapted from :\n    # https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/layers/python/layers/normalization.py.\n    # We do not use the tf layer instance_norm because there is no way\n    # to pass in tensor as beta or gamma.\n    gamma = tensor_dict[node.inputs[1]]\n    beta = tensor_dict[node.inputs[2]]\n\n    inputs = tensor_dict[node.inputs[0]]\n    inputs_shape = inputs.shape\n    inputs_rank = inputs.shape.ndims\n\n    moments_axes = list(range(inputs_rank))[2:]\n    params_shape_broadcast = list(\n        [1, inputs_shape[1]] + [1 for _ in range(2, inputs_rank)])\n\n    beta = tf.reshape(beta, params_shape_broadcast)\n    gamma = tf.reshape(gamma, params_shape_broadcast)\n\n    # Calculate the moments (instance activations).\n    mean, variance = tf.nn.moments(inputs, moments_axes, keepdims=True)\n\n    # Compute instance normalization.\n    inputs = [inputs, mean, variance, beta, gamma]\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, inputs=inputs, name=""instancenorm"", **kwargs)\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/is_inf.py,8,"b'import tensorflow as tf\n\nfrom onnx_tf.common.tf_helper import tf_shape\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""IsInf"")\n@tf_func(tf.math.is_inf)\nclass IsInf(BackendHandler):\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    inp = kwargs[""tensor_dict""][node.inputs[0]]\n    dtype = inp.dtype\n    shape = tf_shape(inp)\n    zero = tf.zeros(shape, dtype)\n    dn = node.attrs.get(""detect_negative"", 1)\n    dp = node.attrs.get(""detect_positive"", 1)\n    # detecting only positive infinity, zero out elements < 0\n    if dn == 0:\n      inp = tf.maximum(zero, inp)\n    # detecting only negative infinity, zero out elements > 0\n    if dp == 0:\n      inp = tf.minimum(zero, inp)\n    return [cls.make_tensor_from_onnx_node(node, inputs=[inp], **kwargs)]\n'"
onnx_tf/handlers/backend/is_nan.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""IsNaN"")\n@tf_func(tf.math.is_nan)\nclass IsNaN(BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/leaky_relu.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""LeakyRelu"")\n@tf_func(tf.nn.leaky_relu)\nclass Identity(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""alpha"": 0.01}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/less.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .control_flow_mixin import ComparisonMixin\n\n\n@onnx_op(""Less"")\n@tf_func(tf.less)\nclass Less(ComparisonMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/log.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Log"")\n@tf_func(tf.math.log)\nclass Log(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/log_softmax.py,9,"b'import numpy as np\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""LogSoftmax"")\n@tf_func(tf.nn.log_softmax)\nclass LogSoftmax(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    axis = node.attrs.get(""axis"", 1)\n    axis = axis if axis >= 0 else len(np.shape(x)) + axis\n\n    if axis == len(np.shape(x)) - 1:\n      return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n    shape = tf.shape(x)\n    cal_shape = (tf.reduce_prod(shape[0:axis]),\n                 tf.reduce_prod(shape[axis:tf.size(shape)]))\n    x = tf.reshape(x, cal_shape)\n\n    return [tf.reshape(tf.nn.log_softmax(x - tf.reduce_max(x)), shape)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/loop.py,21,"b'import tensorflow as tf\n\nimport onnx_tf\nfrom onnx.helper import make_opsetid\nfrom onnx_tf.common import data_type\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""Loop"")\nclass Loop(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    body = node.attrs[""body""]\n    tensor_dict = kwargs[""tensor_dict""]\n    M = tensor_dict[node.inputs[0]] if node.inputs[0] != """" else None\n    cond = tf.cast(tensor_dict[node.inputs[1]],\n                   tf.bool) if node.inputs[1] != """" else None\n    v_initial = [tensor_dict[graph_input] for graph_input in node.inputs[2:]]\n    v_shapes = [v.get_shape() for v in v_initial]\n    current_opset = [make_opsetid(cls.DOMAIN, cls.VERSION)]\n    # outputs of the body will be in this format:\n    # (condition, loop carried dependencies..., scan_outputs...)\n    scan_outputs_start_index = 1 + len(v_initial)\n    scan_outputs = [\n        tf.TensorArray(dtype=data_type.onnx2tf(\n            body.output[i].type.tensor_type.elem_type),\n                       size=0,\n                       dynamic_size=True)\n        for i in range(scan_outputs_start_index, len(body.output))\n    ]\n    scan_outputs_shapes = [tf.TensorShape(None) for o in scan_outputs]\n\n    def run_subgraph(cond, v, scan_outputs):\n      input_values = {}\n      input_values[body.input[0].name] = M\n      input_values[body.input[1].name] = cond\n      for i in range(2, len(body.input)):\n        input_values[body.input[i].name] = v[i - 2]\n      subgraph_tensor_dict = onnx_tf.backend.onnx_graph_to_tensorflow_ops(\n          subgraph=body,\n          input_values=input_values,\n          tensor_dict=tensor_dict,\n          opset=current_opset)\n      outputs = [subgraph_tensor_dict[output.name] for output in body.output]\n      for i in range(scan_outputs_start_index, len(outputs)):\n        s_index = i - scan_outputs_start_index\n        insert_index = scan_outputs[s_index].size()\n        scan_outputs[s_index] = scan_outputs[s_index].write(\n            insert_index, outputs[i])\n      return outputs[0], outputs[1:scan_outputs_start_index], scan_outputs\n\n    # for loop\n    if M is not None and cond is None:\n      M = tf.cast(M, tf.int32)\n      condition = lambda cond, v, scan_outputs: True\n      _, v_final, scan_outputs = tf.while_loop(\n          cond=condition,\n          body=run_subgraph,\n          loop_vars=["""", v_initial, scan_outputs],\n          shape_invariants=[\n              tf.TensorShape(None), v_shapes, scan_outputs_shapes\n          ],\n          maximum_iterations=M)\n    # while and do-while loop\n    elif M is None and cond is not None:\n      condition = lambda cond, v, scan_outputs: tf.reduce_all(\n          tf.equal(cond, True))\n      cond, v_final, scan_outputs = tf.while_loop(\n          cond=condition,\n          body=run_subgraph,\n          loop_vars=[cond, v_initial, scan_outputs],\n          shape_invariants=[\n              tf.TensorShape(None), v_shapes, scan_outputs_shapes\n          ])\n    # combine for loop and while loop together\n    elif M is not None and cond is not None:\n      M = tf.cast(M, tf.int32)\n      condition = lambda cond, v, scan_outputs: tf.reduce_all(\n          tf.equal(cond, True))\n      cond, v_final, scan_outputs = tf.while_loop(\n          cond=condition,\n          body=run_subgraph,\n          loop_vars=[cond, v_initial, scan_outputs],\n          shape_invariants=[\n              tf.TensorShape(None), v_shapes, scan_outputs_shapes\n          ],\n          maximum_iterations=M)\n    else: # M is None and cond is None\n      exception.OP_UNSUPPORTED_EXCEPT(\n          ""Both M and cond in Loop are not set at the same time"",\n          ""Tensorflow.(PS. if you want to create a do-while loop "" +\n          ""then please set cond to True or 1)"")\n\n    scan_outputs_tensors = [o.stack() for o in scan_outputs]\n    if scan_outputs_start_index == len(body.output):\n      # there is no scan_output in the body graph\n      return [v_final]\n    else:\n      return [v_final, scan_outputs_tensors]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/lp_normalization.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""LpNormalization"")\n@tf_func(tf.norm)\nclass LpNormalization(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {\n        ""default"": {\n            ""axis"": -1,\n            ""p"": 2,\n            ""keepdims"": True\n        },\n        ""rename"": {\n            ""p"": ""ord""\n        }\n    }\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    input_tensor = kwargs[""tensor_dict""][node.inputs[0]]\n    tf_norm = cls.make_tensor_from_onnx_node(node, **kwargs)\n\n    return [\n        cls.make_tensor_from_onnx_node(\n            node,\n            tf_func=tf.math.truediv,\n            inputs=[input_tensor, tf_norm],\n            **kwargs)\n    ]\n'"
onnx_tf/handlers/backend/lrn.py,4,"b'import copy\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""LRN"")\n@tf_func(tf.nn.lrn)\nclass LRN(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    attrs = copy.deepcopy(node.attrs)\n    alpha = attrs.get(""alpha"", 1e-4)\n    attrs.setdefault(""beta"", 0.75)\n    size = attrs[""size""]\n    attrs[""alpha""] = alpha / size\n    attrs[""depth_radius""] = np.floor([(size - 1) / 2.])[0]\n    # TODO: LRN in tf accepts radius\n    # but in ONNX/Caffe accepts diameter.\n    # This could be a problem.\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, attrs=attrs, c_last_only=True, **kwargs)\n    ]\n'"
onnx_tf/handlers/backend/lstm.py,44,"b'from functools import partial\n\nimport tensorflow as tf\n\nfrom onnx_tf.common import get_unique_suffix\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom .rnn_mixin import RNNMixin\n\n\n@onnx_op(""LSTM"")\n@partial_support(True)\n@ps_description(""LSTM not using sigmoid for `f`, or "" +\n                ""LSTM not using the same activation for `g` and `h` "" +\n                ""are not supported in Tensorflow."")\nclass LSTM(RNNMixin, BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    direction = node.attrs.get(""direction"", ""forward"")\n    num_directions = 2 if direction == ""bidirectional"" else 1\n    if node.attrs.get(""input_forget"", 0):\n      # TODO(fumihwh): warning\n      pass\n    if ""activations"" in node.attrs:\n      activations = list(map(lambda x: x.lower(), node.attrs[""activations""]))\n      if activations[0] != ""sigmoid"":\n        exception.OP_UNSUPPORTED_EXCEPT(""LSTM without sigmoid for `f`"",\n                                        ""Tensorflow"")\n      if activations[1] != activations[2]:\n        exception.OP_UNSUPPORTED_EXCEPT(\n            ""LSTM without same activation for `g` and `h`"", ""Tensorflow"")\n      if num_directions == 2:\n        if activations[3] != ""sigmoid"":\n          exception.OP_UNSUPPORTED_EXCEPT(""LSTM without sigmoid for `f`"",\n                                          ""Tensorflow"")\n        if activations[4] != activations[5]:\n          exception.OP_UNSUPPORTED_EXCEPT(\n              ""LSTM without same activation for `g` and `h`"", ""Tensorflow"")\n\n  @classmethod\n  def _custom_getter(cls,\n                     getter,\n                     name,\n                     node=None,\n                     tensor_dict=None,\n                     is_bidirectional=None,\n                     *args,\n                     **kwargs):\n    names = name.split(""/"")\n    if is_bidirectional:\n      if ""fw"" in names:\n        index = 0\n      elif ""bw"" in names:\n        index = 1\n      else:\n        raise RuntimeError(""Can not get {} for bidirectional. ""\n                           ""Either fw and bw is not in name scope."".format(\n                               names[-1]))\n\n    if names[-1] == ""kernel"":\n      # onnx W[iofc], R[iofc]\n      if is_bidirectional:\n        w = tf.split(tensor_dict[node.inputs[1]], 2)[index]\n        r = tf.split(tensor_dict[node.inputs[2]], 2)[index]\n      else:\n        w = tensor_dict[node.inputs[1]]\n        r = tensor_dict[node.inputs[2]]\n      w_i, w_o, w_f, w_c = tf.split(tf.squeeze(w), 4)\n      r_i, r_o, r_f, r_c = tf.split(tf.squeeze(r), 4)\n      new_w = tf.transpose(tf.concat([w_i, w_c, w_f, w_o], 0))\n      new_r = tf.transpose(tf.concat([r_i, r_c, r_f, r_o], 0))\n      kernel = tf.concat([new_w, new_r], 0)\n      return kernel\n    if names[-1] == ""bias"":\n      if len(node.inputs) >= 4:\n        # onnx Wb[iofc], Rb[iofc]\n        if is_bidirectional:\n          b = tf.split(tensor_dict[node.inputs[3]], 2)[index]\n        else:\n          b = tensor_dict[node.inputs[3]]\n        w_b, r_b = tf.split(tf.squeeze(b), 2)\n        w_b_i, w_b_o, w_b_f, w_b_c = tf.split(w_b, 4)\n        r_b_i, r_b_o, r_b_f, r_b_c = tf.split(r_b, 4)\n        w_b = tf.transpose(tf.concat([w_b_i, w_b_c, w_b_f, w_b_o], 0))\n        r_b = tf.transpose(tf.concat([r_b_i, r_b_c, r_b_f, r_b_o], 0))\n        return tf.add(w_b, r_b)\n      return getter(name, *args, **kwargs)\n    # Only use_peepholes is True,\n    # will try to get w_f_diag, w_i_diag, w_o_diag\n    # onnx P[iof]\n    if names[-1] in [""w_f_diag"", ""w_i_diag"", ""w_o_diag""]:\n      if is_bidirectional:\n        p = tf.split(tensor_dict[node.inputs[7]], 2)[index]\n      else:\n        p = tensor_dict[node.inputs[7]]\n      if names[-1] == ""w_f_diag"":\n        return tf.split(p, 3, axis=1)[2]\n      if names[-1] == ""w_i_diag"":\n        return tf.split(p, 3, axis=1)[0]\n      if names[-1] == ""w_o_diag"":\n        return tf.split(p, 3, axis=1)[1]\n    return getter(name, *args, **kwargs)\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    input_shape = x.get_shape().as_list()\n    input_size = len(node.inputs)\n    hidden_size = node.attrs[""hidden_size""]\n    direction = node.attrs.get(""direction"", ""forward"")\n    num_directions = 2 if direction == ""bidirectional"" else 1\n\n    # removed from version 7, default is 0\n    output_sequence = node.attrs.get(""output_sequence"", 0)\n\n    # TODO(fumihwh): check if prev node is one of RNN\n    # process input if it comes from other previous cell\n    # which has shape [seq_length, num_directions, batch_size, hidden_size]\n    if len(input_shape) == 4 and input_shape[1] == 1:\n      x = tf.squeeze(x)\n\n    sequence_length = None\n    if input_size >= 5 and node.inputs[4] in tensor_dict:\n      sequence_length = tensor_dict[node.inputs[4]]\n\n    cell_kwargs = {}\n\n    if ""clip"" in node.attrs:\n      cell_kwargs[""cell_clip""] = node.attrs[""clip""]\n\n    tf_activations = [tf.nn.tanh] * num_directions\n    if ""activations"" in node.attrs:\n      activations = list(map(lambda x: x.lower(), node.attrs[""activations""]))\n      activation_alpha = node.attrs.get(""activation_alpha"", [None] * 6)\n      activation_beta = node.attrs.get(""activation_beta"", [None] * 6)\n\n      # tf only supports cutomizing hidden states activation function,\n      # which correspond to activation functions specified at position 1\n      # and 4 in onnx\'s activations attribute.\n      activation_idxs = [1, 4] if num_directions == 2 else [1]\n      tf_activations = [\n          cls.rnn_get_activation(activations[i], activation_alpha[i],\n                                 activation_beta[i]) for i in activation_idxs\n      ]\n\n    # TODO(fumihwh): check if reverse and bidirectional works\n    with tf.compat.v1.variable_scope(\n        ""LSTM_"" + get_unique_suffix(),\n        custom_getter=partial(\n            cls._custom_getter,\n            node=node,\n            tensor_dict=tensor_dict,\n            is_bidirectional=num_directions == 2)):\n\n      cell_kwargs[\n          ""use_peepholes""] = input_size == 8 and node.inputs[7] in tensor_dict\n      cell_kwargs[""forget_bias""] = 0.\n      cell_kwargs[""num_units""] = hidden_size\n      initial_state = None\n      initial_state_bw = None\n      if input_size >= 6:\n        initial_h = tensor_dict.get(node.inputs[5], None)\n        initial_c = tensor_dict.get(\n            node.inputs[6],\n            None) if input_size >= 7 else tf.zeros_like(initial_h)\n        if initial_h is not None and initial_c is not None:\n          initial_state = (tf.compat.v1.nn.rnn_cell.LSTMStateTuple(\n              initial_c[0], initial_h[0]),)\n          if num_directions == 2:\n            initial_state_bw = (tf.compat.v1.nn.rnn_cell.LSTMStateTuple(\n                initial_c[1], initial_h[1]),)\n\n      rnn_kwargs = {}\n      if num_directions == 1:\n        rnn_kwargs[""initial_state""] = initial_state\n      elif num_directions == 2:\n        rnn_kwargs[""initial_state_fw""] = initial_state\n        rnn_kwargs[""initial_state_bw""] = initial_state_bw\n      rnn_kwargs[""sequence_length""] = sequence_length\n      rnn_kwargs[""time_major""] = True\n      rnn_kwargs[""dtype""] = tf.float32\n\n      outputs, states = cls.rnn(x, tf.compat.v1.nn.rnn_cell.LSTMCell,\n                                cell_kwargs, rnn_kwargs, tf_activations,\n                                direction)\n\n    if num_directions == 1:\n      state = states[0]\n      c = tf.expand_dims(state[0], 0)\n      h = tf.expand_dims(state[1], 0)\n      output = tf.expand_dims(outputs, 1)\n    else:\n      state_fw = states[0][0]\n      state_bw = states[1][0]\n      output_fw = outputs[0]\n      output_bw = outputs[1]\n      c_fw = tf.expand_dims(state_fw[0], 0)\n      c_bw = tf.expand_dims(state_bw[0], 0)\n      c = tf.concat((c_fw, c_bw), axis=0)\n      h_fw = tf.expand_dims(state_fw[1], 0)\n      h_bw = tf.expand_dims(state_bw[1], 0)\n      h = tf.concat((h_fw, h_bw), axis=0)\n      output_fw = tf.expand_dims(output_fw, 1)\n      output_bw = tf.expand_dims(output_bw, 1)\n      output = tf.concat((output_fw, output_bw), axis=1)\n\n    return [output, h, c] if output_sequence == 0 else [h, c]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/mat_mul.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""MatMul"")\n@tf_func(tf.matmul)\nclass MatMul(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/mat_mul_integer.py,15,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""MatMulInteger"")\n@tf_func(tf.matmul)\nclass MatMulInteger(BackendHandler):\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    A = tensor_dict[node.inputs[0]]\n    B = tensor_dict[node.inputs[1]]\n    # tf.matmul doesn\'t support int8 and uint8 for A and B,\n    # therefore need to cast them to int32\n    A = tf.cast(A, tf.int32)\n    B = tf.cast(B, tf.int32)\n\n    if \'a_zero_point\' in tensor_dict:\n      a_zero_point = tensor_dict[\'a_zero_point\']\n\n      if a_zero_point.shape.is_fully_defined():\n        shape = a_zero_point.get_shape().as_list()\n        if len(shape) > 0  and shape[0] > 1:\n          # reshape a_zero_point before subtract it from A\n          a_zero_point = tf.reshape(a_zero_point, [shape[0], 1])\n      else:\n        @tf.function\n        def get_a_zero_point(a_zero_point):\n          shape = tf.shape(a_zero_point)\n          if len(shape) > 0 and shape[0] > 1:\n            # reshape a_zero_point before subtract it from A\n            a_zero_point = tf.reshape(a_zero_point, [shape[0], 1])\n          return a_zero_point\n        a_zero_point = get_a_zero_point(a_zero_point)\n\n      a_zero_point = tf.cast(a_zero_point, tf.int32)\n      A = tf.subtract(A, a_zero_point)\n\n    if \'b_zero_point\' in tensor_dict:\n      b_zero_point = tensor_dict[\'b_zero_point\']\n      b_zero_point = tf.cast(b_zero_point, tf.int32)\n      B = tf.subtract(B, b_zero_point)\n\n    return [cls.make_tensor_from_onnx_node(node, inputs=[A, B], **kwargs)]\n'"
onnx_tf/handlers/backend/math_mixin.py,0,"b'import copy\n\nfrom .broadcast_mixin import BroadcastMixin\n\n\nclass BasicMathMixin(BroadcastMixin):\n  pass\n\n\nclass ArithmeticMixin(BroadcastMixin):\n  pass\n\n\nclass ReductionMixin(BroadcastMixin):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    attrs = copy.deepcopy(node.attrs)\n    axis = attrs.pop(""axes"", None)\n    if isinstance(axis, (list, tuple)) and len(axis) == 1:\n      axis = axis[0]\n    attrs[""axis""] = axis\n    # https://github.com/onnx/onnx/issues/585\n    attrs[""keepdims""] = attrs.pop(""keepdims"", 1) == 1\n    return [cls.make_tensor_from_onnx_node(node, attrs=attrs, **kwargs)]\n'"
onnx_tf/handlers/backend/max.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Max"")\n@tf_func(tf.reduce_max)\nclass Max(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""axis"": 0}}\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    values = [kwargs[""tensor_dict""][inp] for inp in node.inputs]\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, inputs=[tf.stack(values)], **kwargs)\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_8(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_12(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_13(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/max_pool.py,4,"b'from onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom .pool_mixin import PoolMixin\n\n\n@onnx_op(""MaxPool"")\n@partial_support(True)\n@ps_description(\n    ""MaxPoolWithArgmax with pad is None or incompatible mode, or "" +\n    ""MaxPoolWithArgmax with 4D or higher input, or"" +\n    ""MaxPoolWithArgmax with column major "" + ""are not supported in Tensorflow."")\nclass MaxPool(PoolMixin, BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    pool_type = ""MAX"" if len(node.outputs) == 1 else ""MAX_WITH_ARGMAX""\n    return cls.pool(node, kwargs[""tensor_dict""], pool_type,\n                    kwargs.get(""strict"", True))\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_8(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_12(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/max_unpool.py,2,"b'from onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .unpool_mixin import UnpoolMixin\n\n\n@onnx_op(""MaxUnpool"")\nclass MaxUnpool(UnpoolMixin, BackendHandler):\n\n    @classmethod\n    def version_9(cls, node, **kwargs):\n        return cls.max_unpool(node, kwargs[""tensor_dict""])\n\n    @classmethod\n    def version_11(cls, node, **kwargs):\n        return cls.max_unpool(node, kwargs[""tensor_dict""])\n'"
onnx_tf/handlers/backend/mean.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Mean"")\n@tf_func(tf.reduce_mean)\nclass Mean(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""axis"": 0}}\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    values = [kwargs[""tensor_dict""][inp] for inp in node.inputs]\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, inputs=[tf.stack(values)], **kwargs)\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_8(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/mean_variance_normalization.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""MeanVarianceNormalization"")\nclass MeanVarianceNormalization(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    inputs = tensor_dict[node.inputs[0]]\n    inputs_rank = inputs.shape.ndims\n\n    across_channels = node.attrs.get(""across_channels"", 0)\n    normalize_variance = node.attrs.get(""normalize_variance"", 1)\n\n    moments_axes = [0] if not across_channels else [0, 1]\n    moments_axes += list(range(inputs_rank))[2:]\n\n    mean, variance = tf.nn.moments(inputs, moments_axes, keepdims=True)\n\n    if not normalize_variance:\n      return [inputs - mean]\n    return [(inputs - mean) / tf.sqrt(variance)]\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    inputs = tensor_dict[node.inputs[0]]\n    inputs_rank = inputs.shape.ndims\n    # To satisfy default axes=[0,2,3], also assume the\n    # following default when rank is not 4\n    # rank1 -> axes=[0]\n    # rank2 -> axes=[0]\n    # rank3 -> axes=[0,2]\n    # rank4 -> axes=[0,2,3]\n    # rankN -> axes=[0,2,3,..,N-1]\n    # TODO(tedhtchang): Since input tensor is no longer limited\n    # to shape [N,C,H,W], consider using ""[0]"" or ""[]"" as default axes.\n    # See issue https://github.com/onnx/onnx/issues/2047\n    default_axes = [0] if inputs_rank < 3 else [0, 2]\n    default_axes += list(range(inputs_rank))[3:]\n    moments_axes = node.attrs.get(""axes"", default_axes)\n    mean, variance = tf.nn.moments(inputs, moments_axes, keepdims=True)\n    return [(inputs - mean) / tf.sqrt(variance)]\n'"
onnx_tf/handlers/backend/min.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Min"")\n@tf_func(tf.reduce_min)\nclass Min(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""axis"": 0}}\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    values = [kwargs[""tensor_dict""][inp] for inp in node.inputs]\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, inputs=[tf.stack(values)], **kwargs)\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_8(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_12(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_13(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/mod.py,8,"b'import tensorflow as tf\n\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom .math_mixin import ArithmeticMixin\n\n\n@onnx_op(""Mod"")\n@partial_support(True)\n@ps_description(\n    ""Mod Dividend or Divisor in "" + ""int8/int16/uint8/uint16/uint32/uint64 "" +\n    ""are not supported in Tensorflow."")\nclass Mod(ArithmeticMixin, BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    unsupported_dtype = [\n        tf.int8, tf.int16, tf.uint8, tf.uint16, tf.uint32, tf.uint64\n    ]\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    y = kwargs[""tensor_dict""][node.inputs[1]]\n    if x.dtype in unsupported_dtype:\n      exception.OP_UNSUPPORTED_EXCEPT(""Mod Dividend in "" + str(x.dtype),\n                                      ""Tensorflow"")\n    if y.dtype in unsupported_dtype:\n      exception.OP_UNSUPPORTED_EXCEPT(""Mod Divisor in "" + str(y.dtype),\n                                      ""Tensorflow"")\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    fmod = node.attrs.get(""fmod"", 0)\n    tf_func = tf.math.floormod\n    if fmod == 1:\n      tf_func = tf.truncatemod\n    return [cls.make_tensor_from_onnx_node(node, tf_func=tf_func, **kwargs)]\n'"
onnx_tf/handlers/backend/mul.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ArithmeticMixin\n\n\n@onnx_op(""Mul"")\n@tf_func(tf.multiply)\nclass Mul(ArithmeticMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/neg.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Neg"")\n@tf_func(tf.negative)\nclass Neg(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/non_max_suppression.py,44,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""NonMaxSuppression"")\nclass NonMaxSuppression(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    boxes = tensor_dict[node.inputs[0]]\n    scores = tensor_dict[node.inputs[1]]\n    # in ONNX spec max_output_boxes_per_class need to be in int64 but\n    # max_output_boxes for tf.image.non_max_suppression must be in tf.int32\n    # therefore need to cast this input to tf.int32\n    max_output_boxes_per_class = tf.cast(\n        tensor_dict[\'max_output_boxes_per_class\'],\n        tf.int32) if \'max_output_boxes_per_class\' in tensor_dict else tf.cast(\n            boxes.shape[1], tf.int32)\n    iou_threshold = tensor_dict[\n        \'iou_threshold\'] if \'iou_threshold\' in tensor_dict else tf.constant(\n            [0.5], tf.float32)\n    score_threshold = tensor_dict[\n        \'score_threshold\'] if \'score_threshold\' in tensor_dict else tf.constant(\n            [float(\'-inf\')], tf.float32)\n    center_point_box = node.attrs.get(""center_point_box"", 0)\n\n    if center_point_box == 1:\n      boxes_t = tf.transpose(boxes, perm=[0, 2, 1])\n      x_centers = tf.slice(boxes_t, [0, 0, 0], [-1, 1, -1])\n      y_centers = tf.slice(boxes_t, [0, 1, 0], [-1, 1, -1])\n      widths = tf.slice(boxes_t, [0, 2, 0], [-1, 1, -1])\n      heights = tf.slice(boxes_t, [0, 3, 0], [-1, 1, -1])\n      y1 = tf.subtract(y_centers, tf.divide(heights, 2))\n      x1 = tf.subtract(x_centers, tf.divide(widths, 2))\n      y2 = tf.add(y_centers, tf.divide(heights, 2))\n      x2 = tf.add(x_centers, tf.divide(widths, 2))\n      boxes_t = tf.concat([y1, x1, y2, x2], 1)\n      boxes = tf.transpose(boxes_t, perm=[0, 2, 1])\n\n    @tf.function\n    def create_nodes(boxes, scores, max_output_boxes_per_class, iou_threshold,\n                     score_threshold, result):\n      # get number of batches in boxes\n      num_batches = tf.shape(boxes)[0]\n      for batch_i in tf.range(num_batches):\n        # get boxes in batch_i only\n        tf_boxes = tf.squeeze(tf.gather(boxes, [batch_i]), axis=0)\n        # get scores of all classes in batch_i only\n        batch_i_scores = tf.squeeze(tf.gather(scores, [batch_i]), axis=0)\n        # get number of classess in batch_i only\n        num_classes = tf.shape(batch_i_scores)[0]\n        for class_j in tf.range(num_classes):\n          # get scores in class_j for batch_i only\n          tf_scores = tf.squeeze(tf.gather(batch_i_scores, [class_j]), axis=0)\n          # get the selected boxes indices\n          selected_indices = tf.image.non_max_suppression(\n              tf_boxes, tf_scores, max_output_boxes_per_class[0],\n              iou_threshold[0], score_threshold[0])\n          # add batch and class information into the indices\n          output = tf.transpose([tf.cast(selected_indices, dtype=tf.int64)])\n          paddings = tf.constant([[0, 0], [1, 0]])\n          output = tf.pad(output,\n                          paddings,\n                          constant_values=tf.cast(class_j, dtype=tf.int64))\n          output = tf.pad(output,\n                          paddings,\n                          constant_values=tf.cast(batch_i, dtype=tf.int64))\n          # tf.function will auto convert ""result"" from variable to placeholder\n          # therefore don\'t need to use assign here\n          result = output if tf.equal(batch_i, 0) and tf.equal(\n              class_j, 0) else tf.concat([result, output], 0)\n\n      return result\n\n    # Since tf.function doesn\'t support locals() and it require all the variables\n    # are defined before use in the ""for loop"" before it will perform any auto\n    # convertion of the python code. Therefore need to define ""result"" as a\n    # Variable here and send it in as a parameter to ""create_nodes""\n    result = tf.Variable([[0, 0, 0]],\n                         dtype=tf.int64,\n                         shape=tf.TensorShape([None, 3]))\n    return [\n        create_nodes(boxes, scores, max_output_boxes_per_class, iou_threshold,\n                     score_threshold, result)\n    ]\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/non_zero.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""NonZero"")\nclass NonZero(BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    input_tensor = kwargs[""tensor_dict""][node.inputs[0]]\n    condition = tf.not_equal(input_tensor, tf.zeros_like(input_tensor))\n    nonzero_indices = tf.where(condition)\n    return [tf.transpose(nonzero_indices)]\n'"
onnx_tf/handlers/backend/not.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .control_flow_mixin import LogicalMixin\n\n\n@onnx_op(""Not"")\n@tf_func(tf.logical_not)\nclass Not(LogicalMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/onehot.py,9,"b'import copy\nimport tensorflow as tf\n\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""OneHot"")\n@tf_func(tf.one_hot)\n@partial_support(True)\n@ps_description(\n    ""OneHot indices in uint16/uint32/uint64/int8/int16/"" +\n    ""float16/float/double, or "" +\n    ""OneHot depth in uint8/uint16/uint32/uint64/int8/"" +\n    ""int16/int64/float16/float/double "" + ""are not supported in Tensorflow."")\nclass OneHot(BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    indices = tensor_dict[node.inputs[0]]\n    depth = tensor_dict[node.inputs[1]]\n    if indices.dtype not in [tf.uint8, tf.int32, tf.int64]:\n      exception.OP_UNSUPPORTED_EXCEPT(\n          ""OneHot indices must be in uint8 or int32 or int64 "" +\n          ""but it is currently in "" + str(indices.dtype) + "" which"",\n          ""Tensorflow"")\n    if depth.dtype not in [tf.int32]:\n      exception.OP_UNSUPPORTED_EXCEPT(\n          ""OneHot depth must be in int32 but it is currently in "" +\n          str(depth.dtype) + "" which"", ""Tensorflow"")\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    attrs = copy.deepcopy(node.attrs)\n    tensor_dict = kwargs[""tensor_dict""]\n    indices = tensor_dict[node.inputs[0]]\n    depth = tensor_dict[node.inputs[1]]\n    off_value = tensor_dict[node.inputs[2]][0]\n    on_value = tensor_dict[node.inputs[2]][1]\n    attrs[""dtype""] = on_value.dtype\n    return [\n        cls.make_tensor_from_onnx_node(\n            node,\n            inputs=[indices, depth, on_value, off_value],\n            attrs=attrs,\n            **kwargs)\n    ]\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/or.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .control_flow_mixin import LogicalMixin\n\n\n@onnx_op(""Or"")\n@tf_func(tf.logical_or)\nclass Or(LogicalMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/p_relu.py,3,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .broadcast_mixin import BroadcastMixin\n\n\n@onnx_op(""PRelu"")\nclass PRelu(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    """"""\n    Reference implementation at\n    https://github.com/tflearn/tflearn/blob/4ba8c8d78bf1bbdfc595bf547bad30580cb4c20b/tflearn/activations.py#L191\n    """"""\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    slope = BroadcastMixin.explicit_broadcast([x, tensor_dict[node.inputs[1]]])\n    pos = tf.nn.relu(x)\n    neg = slope * (x - abs(x)) * 0.5\n    return [pos + neg]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/pad.py,15,"b'import numpy as np\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Pad"")\n@tf_func(tf.pad)\nclass Pad(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    num_dim = len(tensor_dict[node.inputs[0]].get_shape())\n    mode = node.attrs.pop(""mode"", ""constant"")\n\n    if cls.SINCE_VERSION < 11:  # for opset 1 and opset 2\n      paddings = node.attrs.pop(""pads"", None)\n      # tf requires int32 paddings\n      paddings = tf.constant(\n          np.transpose(\n              np.array(paddings).reshape([2, num_dim]).astype(np.int32)))\n      constant_values = node.attrs.pop(""value"", 0.)\n\n    else:  # for opset 11\n      paddings = tensor_dict[node.inputs[1]]\n      # tf requires int32 paddings\n      paddings = tf.cast(\n          tf.transpose(tf.reshape(paddings, [2, num_dim])), dtype=tf.int32)\n      constant_values = tensor_dict[node.inputs[2]] if len(\n          node.inputs) == 3 else 0\n\n    def _symmetric_pad(i, x):\n      paddings_i = tf.map_fn(lambda e: tf.where(i < e, 1, 0), paddings)\n      paddings_i = tf.reshape(paddings_i, [num_dim, 2])\n      x = tf.pad(x, paddings_i, \'SYMMETRIC\')\n      return i + 1, x\n\n    if mode.lower() == ""edge"":\n      paddings = tf.reshape(paddings, [-1])\n      max_i = tf.reduce_max(paddings)\n      _, x = tf.while_loop(\n          lambda i, x: tf.less(i, max_i), _symmetric_pad, [0, x],\n          [tf.TensorShape([]), tf.TensorShape(None)])\n      return [x]\n\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, inputs=[x, paddings, mode, constant_values], **kwargs)\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_2(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/pad_mixin.py,2,"b'import numpy as np\nimport tensorflow as tf\n\n\nclass PadMixin(object):\n\n  @classmethod\n  def get_padding_as_op(cls, x, pads):\n    num_dim = int(len(pads) / 2)\n\n    tf_pads = np.transpose(np.array(pads).reshape([2, num_dim]))\n    tf_pads = [0, 0, 0, 0] + tf_pads.flatten().tolist()\n\n    padding = tf.constant(\n        np.array(tf_pads).reshape([num_dim + 2, 2])\n        .astype(np.int32))  # tf requires int32 paddings\n    return tf.pad(x, padding)\n'"
onnx_tf/handlers/backend/pool_mixin.py,11,"b'import tensorflow as tf\n\nfrom onnx_tf.common import exception\nfrom onnx_tf.common import get_data_format\nfrom onnx_tf.common import get_perm_from_formats\nfrom onnx_tf.common import logger \nfrom .dilated_pooling import DilatedPooling\nfrom onnx_tf.common.pooling_helper import py_pool\nfrom onnx_tf.common.pooling_helper import calc_pads_same\nfrom onnx_tf.common.pooling_helper import calc_output_shape\n\nclass PoolMixin(object):\n\n  @classmethod\n  def pool(cls, node, input_dict, pooling_type, strict=True):\n    x = input_dict[node.inputs[0]]\n    orig_x = x\n\n    kernel_shape = node.attrs[""kernel_shape""]\n\n    spatial_size = len(kernel_shape)\n    x_rank = spatial_size + 2\n\n    kernel_shape = node.attrs[""kernel_shape""]\n    strides = node.attrs.get(""strides"", [1] * spatial_size)\n    dilations = node.attrs.get(""dilations"", [1] * spatial_size)\n    ceil_mode = bool(node.attrs.get(""ceil_mode"", 0))\n    pads = node.attrs.get(""auto_pad"", ""NOTSET"")\n    if pads == ""NOTSET"":\n      pads = node.attrs.get(""pads"", [0] * spatial_size * 2)\n      # In case shape is fully defined, check if pads match\n      # SAME padding in Tensorflow\n      if x.shape.is_fully_defined() and pads != [0] * spatial_size * 2:\n        in_shape = x.get_shape()\n        same_paddings = calc_pads_same(in_shape[1:x_rank-1], kernel_shape,\n                   strides, dilations, ""SAME_UPPER"")\n        if pads == same_paddings:\n          pads = ""SAME_UPPER""\n\n    count_include_pad = bool(node.attrs.get(""count_include_pad"", 0))\n    if pooling_type == ""AVG"":\n      pooling_name = ""AveragePool""\n    elif pooling_type == ""MAX"":\n      pooling_name = ""MaxPool""\n    elif pooling_type == ""MAX_WITH_ARGMAX"":\n      pooling_name = ""MaxPoolWithArgmax""\n\n    if spatial_size > 3:\n      exception.OP_UNSUPPORTED_EXCEPT(\n          pooling_name + "" with {}D input"".format(x_rank), ""Tensorflow"")\n    if pooling_type == ""MAX_WITH_ARGMAX"" and x_rank != 4:\n      exception.OP_UNSUPPORTED_EXCEPT(\n          pooling_name + "" with {}D input"".format(x_rank), ""Tensorflow"")\n    if node.attrs.get(""storage_order"", 0) != 0:\n      exception.OP_UNSUPPORTED_EXCEPT(pooling_name + "" with column major"",\n                                      ""Tensorflow"")\n\n    storage_format, _ = get_data_format(x_rank)\n\n    need_trans = storage_format.startswith(""NC"")\n    if need_trans:\n      compute_format = ""N"" + storage_format[2:] + ""C""\n      x = tf.transpose(\n          x, perm=get_perm_from_formats(storage_format, compute_format))\n\n    dp = DilatedPooling(\n        input=x,\n        kernel_shape=kernel_shape,\n        strides=strides,\n        dilations=dilations,\n        padding=pads,\n        ceil_mode=ceil_mode,\n        pooling_type=pooling_type,\n        count_include_pad=count_include_pad)\n    if not dp.is_supported():\n      if strict:\n        logger.warning(""Using the pooling op in compatibility mode. ""\n                      ""This means your graph cannot be serialized."",\n                      UserWarning)\n\n        result = tf.numpy_function(py_pool, [\n                orig_x, kernel_shape, strides, dilations, pads, ceil_mode,\n                ""AVG"", False\n            ], orig_x.dtype)\n\n        if orig_x.shape.is_fully_defined():\n          shape = orig_x.get_shape()\n          output_shape = shape[0:2] + calc_output_shape(shape[2:x_rank],\n                  kernel_shape, strides, dilations, pads, ceil_mode)\n        else:\n          output_shape = [None] * x_rank\n        result.set_shape(output_shape)\n        return [result]\n      else:\n        exception.OP_UNSUPPORTED_EXCEPT(""strict == 0 and average pool""\n                                        "" arguments not compatible"",\n                                        ""Tensorflow"")\n\n    def dilated_pool():\n      return (dp.dilated_pool(), None)\n\n    # select correct op depending on the pooling type\n    pooling_op = dilated_pool if pooling_type in [""MAX"", ""AVG""] else \\\n        dp.dilated_maxpool_with_argmax\n\n    # select the correct transpose ops depending on the input storage format\n    perm = get_perm_from_formats(compute_format, storage_format)\n\n    def postprocess(pooled, argmax):\n      return (tf.transpose(pooled, perm=perm) if need_trans else pooled,\n              tf.transpose(argmax, perm=perm)\n              if need_trans and argmax is not None else argmax)\n\n    pooled, argmax = pooling_op()\n    pooled, argmax = postprocess(pooled, argmax)\n\n    result = [pooled] if argmax is None else [pooled, argmax]\n\n    return result\n'"
onnx_tf/handlers/backend/pow.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Pow"")\n@tf_func(tf.pow)\nclass Pow(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/q_linear_conv.py,11,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .conv_mixin import ConvMixin\n\n\n@onnx_op(""QLinearConv"")\nclass QLinearConv(ConvMixin, BackendHandler):\n\n  @classmethod\n  def _dequantize_tensor(cls, base, zero_point, scale):\n    # Do computation in float32\n    base = tf.cast(base, tf.float32)\n    zero_point = tf.cast(zero_point, tf.float32)\n    return (base - zero_point) * scale\n\n  @classmethod\n  def _dequantize_w(cls, base, zero_point, scale):\n    tensor_list = [\n        cls._dequantize_tensor(base[i][j], zero_point[j], scale[j])\n        for i in range(base.shape.as_list()[0])\n        for j in range(zero_point.shape.as_list()[0])\n    ]\n\n    out_tensor = tf.concat(tensor_list, 0)\n    return tf.reshape(out_tensor, base.shape)\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    x_scale = tensor_dict[node.inputs[1]]\n    x_zero_point = tensor_dict[node.inputs[2]]\n    w = tensor_dict[node.inputs[3]]\n    w_scale = tensor_dict[node.inputs[4]]\n    w_zero_point = tensor_dict[node.inputs[5]]\n    y_scale = tensor_dict[node.inputs[6]]\n    y_zero_point = tensor_dict[node.inputs[7]]\n\n    output_dtype = x.dtype\n\n    # Convert w_zero_point and w_scale to 1-D if scalar\n    if len(w_zero_point.shape) == 0:\n      w_zero_point = tf.fill([x.shape[1]], w_zero_point)\n    elif len(w_zero_point.shape) > 1:\n      raise ValueError(""Unsupported zero point: {}"".format(w_zero_point))\n\n    if len(w_scale.shape) == 0:\n      w_scale = tf.fill([x.shape[1]], w_scale)\n    elif len(w_scale.shape) > 1:\n      raise ValueError(""Unsupported scale: {}"".format(w_scale))\n\n    # Dequantize variables to float32\n    x = cls._dequantize_tensor(x, x_zero_point, x_scale)\n    w = cls._dequantize_w(w, w_zero_point, w_scale)\n    y_zero_point = tf.cast(y_zero_point, tf.float32)\n\n    new_dict = tensor_dict.copy()\n    new_dict[node.inputs[0]] = x\n    new_dict[node.inputs[3]] = w\n\n    # Remove scales and zero-points from inputs\n    for i in [7, 6, 5, 4, 2, 1]:\n      node.inputs.remove(node.inputs[i])\n\n    # Use common conv handling\n    conv_node = cls.conv(node, new_dict)[0]\n\n    # Process output\n    y = tf.round(conv_node / y_scale) + y_zero_point\n\n    return [tf.cast(y, output_dtype)]\n'"
onnx_tf/handlers/backend/q_linear_mat_mul.py,20,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""QLinearMatMul"")\nclass QLinearMatMul(BackendHandler):\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    a = tensor_dict[node.inputs[0]]\n    a_scale = tensor_dict[node.inputs[1]]\n    a_zero_point = tensor_dict[node.inputs[2]]\n    b = tensor_dict[node.inputs[3]]\n    b_scale = tensor_dict[node.inputs[4]]\n    b_zero_point = tensor_dict[node.inputs[5]]\n    y_scale = tensor_dict[node.inputs[6]]\n    y_zero_point = tensor_dict[node.inputs[7]]\n    y_dtype = y_zero_point.dtype\n\n    # reshape 1-D a_scale, a_zero_point, y_scale and\n    # y_zero_point so it can broadcast in arithmetic\n    # operations later\n    a_scale_shape = a_scale.get_shape().as_list()\n    if a_scale_shape and a_scale_shape[0] > 1:\n      a_scale = tf.reshape(a_scale, [a_scale_shape[0], 1])\n      a_zero_point = tf.reshape(a_zero_point, [a_scale_shape[0], 1])\n    y_scale_shape = y_scale.get_shape().as_list()\n    if y_scale_shape and y_scale_shape[0] > 1:\n      y_scale = tf.reshape(y_scale, [y_scale_shape[0], 1])\n      y_zero_point = tf.reshape(y_zero_point, [y_scale_shape[0], 1])\n\n    # cast all inputs to float32\n    a = tf.cast(a, tf.float32)\n    a_zero_point = tf.cast(a_zero_point, tf.float32)\n    b = tf.cast(b, tf.float32)\n    b_zero_point = tf.cast(b_zero_point, tf.float32)\n    y_zero_point = tf.cast(y_zero_point, tf.float32)\n\n    # dequantize a and b\n    dequantized_a = tf.subtract(a, a_zero_point)\n    dequantized_a = tf.multiply(dequantized_a, a_scale)\n    dequantized_b = tf.subtract(b, b_zero_point)\n    dequantized_b = tf.multiply(dequantized_b, b_scale)\n\n    # matmul\n    x = tf.matmul(dequantized_a, dequantized_b)\n\n    # quantize x\n    y = tf.divide(x, y_scale)\n    y = tf.round(y)\n    y = tf.add(y, y_zero_point)\n    y = tf.saturate_cast(y, y_dtype)\n\n    return [y]\n'"
onnx_tf/handlers/backend/quantize_linear.py,9,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""QuantizeLinear"")\nclass QuantizeLinear(BackendHandler):\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    y_scale = tensor_dict[node.inputs[1]]\n\n    x = tf.cast(x, tf.float32)\n    y = tf.divide(x, y_scale)\n    y = tf.round(y)\n    if len(node.inputs) == 3:\n      y_zero_point = tensor_dict[node.inputs[2]]\n      y_dtype = y_zero_point.dtype\n      y_zero_point = tf.cast(y_zero_point, tf.float32)\n      y = tf.add(y, y_zero_point)\n    else:  # y_zero_point default dtype = uint8\n      y_dtype = tf.uint8\n\n    y = tf.saturate_cast(y, y_dtype)\n\n    return [y]\n'"
onnx_tf/handlers/backend/random_normal.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""RandomNormal"")\n@tf_func(tf.random.normal)\nclass RandomNormal(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""mean"": 0., ""scale"": 1.}, ""rename"": {""scale"": ""stddev""}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/random_normal_like.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""RandomNormalLike"")\n@tf_func(tf.random.normal)\nclass RandomNormalLike(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""mean"": 0., ""scale"": 1.}, ""rename"": {""scale"": ""stddev""}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    inputs = [kwargs[""tensor_dict""][node.inputs[0]].get_shape()]\n    return [cls.make_tensor_from_onnx_node(node, inputs=inputs, **kwargs)]\n'"
onnx_tf/handlers/backend/random_uniform.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""RandomUniform"")\n@tf_func(tf.random.uniform)\nclass RandomUniform(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {\n        ""default"": {\n            ""low"": 0.,\n            ""high"": 1.\n        },\n        ""rename"": {\n            ""low"": ""minval"",\n            ""high"": ""maxval""\n        }\n    }\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/random_uniform_like.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""RandomUniformLike"")\n@tf_func(tf.random.uniform)\nclass RandomUniformLike(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {\n        ""default"": {\n            ""low"": 0.,\n            ""high"": 1.\n        },\n        ""rename"": {\n            ""low"": ""minval"",\n            ""high"": ""maxval""\n        }\n    }\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    inputs = [kwargs[""tensor_dict""][node.inputs[0]].get_shape()]\n    return [cls.make_tensor_from_onnx_node(node, inputs=inputs, **kwargs)]\n'"
onnx_tf/handlers/backend/range.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Range"")\n@tf_func(tf.range)\nclass Round(BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/reciprocal.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Reciprocal"")\n@tf_func(tf.math.reciprocal)\nclass Reciprocal(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/reduce_l1.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceL1"")\n@tf_func(tf.norm)\nclass ReduceL1(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""ord"": 1}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_l2.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceL2"")\n@tf_func(tf.norm)\nclass ReduceL2(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""ord"": 2}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_log_sum.py,3,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceLogSum"")\nclass ReduceLogSum(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    axis = node.attrs.get(""axes"", list(range(len(x.get_shape().as_list()))))\n    keepdims = node.attrs.get(""keepdims"", 1) == 1\n    return [tf.math.log(tf.reduce_sum(x, axis=axis, keepdims=keepdims))]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_log_sum_exp.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceLogSumExp"")\n@tf_func(tf.reduce_logsumexp)\nclass ReduceLogSumExp(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_max.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceMax"")\n@tf_func(tf.reduce_max)\nclass ReduceMax(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_mean.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceMean"")\n@tf_func(tf.reduce_mean)\nclass ReduceMean(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_min.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceMin"")\n@tf_func(tf.reduce_min)\nclass ReduceMin(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_prod.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceProd"")\n@tf_func(tf.reduce_prod)\nclass ReduceProd(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_sum.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceSum"")\n@tf_func(tf.reduce_sum)\nclass ReduceSum(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/reduce_sum_square.py,3,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .math_mixin import ReductionMixin\n\n\n@onnx_op(""ReduceSumSquare"")\nclass ReduceSumSquare(ReductionMixin, BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    axis = node.attrs.get(""axes"", list(range(len(x.get_shape().as_list()))))\n    keepdims = node.attrs.get(""keepdims"", 1) == 1\n    return [tf.reduce_sum(tf.square(x), axis=axis, keepdims=keepdims)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/relu.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Relu"")\n@tf_func(tf.nn.relu)\nclass Relu(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/reshape.py,12,"b'import copy\n\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Reshape"")\n@tf_func(tf.reshape)\nclass Reshape(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor = kwargs[""tensor_dict""][node.inputs[0]]\n    if cls.SINCE_VERSION == 1:\n      shape = tf.constant(node.attrs[""shape""], dtype=tf.int64)\n    else:  # since_version >= 5\n      shape = tf.cast(kwargs[""tensor_dict""][node.inputs[1]], tf.int64)\n    input_shape = tf.shape(tensor, out_type=tf.int64)\n\n    # Extract indicies of the shape parameter where\n    # a copy from the original dimension size is needed.\n    copy_indices = tf.squeeze(\n        tf.where(tf.equal(shape, tf.constant(0, dtype=tf.int64))), -1)\n\n    indices_gathered = tf.gather(input_shape, copy_indices)\n    indices_scattered = tf.compat.v1.sparse_to_dense(\n        copy_indices, tf.cast(tf.shape(shape), tf.int64), indices_gathered)\n\n    # Perform the copy wherever requested (wherever dim_size == 0)\n    copied_shape = shape + indices_scattered\n    attrs = copy.deepcopy(node.attrs)\n    attrs.pop(""shape"", None)\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, inputs=[tensor, copied_shape], attrs=attrs, **kwargs)\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_5(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/resize.py,24,"b'import tensorflow as tf\n\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom onnx_tf.common.tf_helper import tf_shape\n\n@onnx_op(""Resize"")\n@partial_support(True)\n@ps_description(""Resize required 4D input in Tensorflow."")\nclass Resize(BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_shape = x.get_shape().as_list()\n    if len(x_shape) != 4:\n      exception.OP_UNSUPPORTED_EXCEPT(""Resize required 4D input"", ""Tensorflow"")\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_shape = tf_shape(x)\n    scales = kwargs[""tensor_dict""][node.inputs[1]]\n\n    n_in_scales_is_one = tf.equal(scales[0], 1)\n    c_in_scales_is_one = tf.logical_or(\n        tf.equal(scales[1], 1), tf.equal(scales[3], 1))\n    assert_n_c_in_scales_are_ones = tf.Assert(\n        tf.logical_and(n_in_scales_is_one, c_in_scales_is_one), [scales])\n\n    with tf.control_dependencies([assert_n_c_in_scales_are_ones]):\n      x_in_NCHW_format = tf.equal(scales[1], 1)\n      h_w_scale = tf.where(x_in_NCHW_format, scales[2:], scales[1:3])\n      h_w_shape = tf.where(x_in_NCHW_format, x_shape[2:], x_shape[1:3])\n      new_h_w_shape = tf.cast(h_w_scale * tf.cast(h_w_shape, scales.dtype),\n                              tf.int32)\n\n      mode = node.attrs.get(""mode"", ""nearest"")\n      if mode.lower() == ""linear"":\n        mode = tf.image.ResizeMethod.BILINEAR\n      else:\n        mode = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n\n      def process_NCHW_format(x):\n        x_t = tf.transpose(x, perm=[0, 2, 3, 1])\n        y = tf.image.resize(x_t, size=new_h_w_shape, method=mode)\n        y_t = tf.transpose(y, perm=[0, 3, 1, 2])\n        return y_t\n\n      def process_NHWC_format(x):\n        y = tf.image.resize(x, size=new_h_w_shape, method=mode)\n        return y\n\n      output = tf.cond(x_in_NCHW_format, lambda: process_NCHW_format(x),\n                       lambda: process_NHWC_format(x))\n\n      return [output]\n'"
onnx_tf/handlers/backend/reverse_sequence.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""ReverseSequence"")\n@tf_func(tf.reverse_sequence)\nclass ReverseSequence(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {\n        ""default"": {\n            ""time_axis"": 0,\n            ""batch_axis"": 1\n        },\n        ""rename"": {\n            ""time_axis"": ""seq_axis""\n        }\n    }\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/rnn.py,29,"b'from functools import partial\n\nimport tensorflow as tf\n\nfrom onnx_tf.common import get_unique_suffix\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom .rnn_mixin import RNNMixin\n\n\n@onnx_op(""RNN"")\n@partial_support(True)\n@ps_description(""RNN with clip is not supported in Tensorflow."")\nclass RNN(RNNMixin, BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    if ""clip"" in node.attrs:\n      exception.OP_UNSUPPORTED_EXCEPT(""RNN with clip"", ""Tensorflow"")\n\n  @classmethod\n  def _custom_getter(cls,\n                     getter,\n                     name,\n                     node=None,\n                     tensor_dict=None,\n                     is_bidirectional=None,\n                     *args,\n                     **kwargs):\n    names = name.split(""/"")\n    if is_bidirectional:\n      if ""fw"" in names:\n        index = 0\n      elif ""bw"" in names:\n        index = 1\n      else:\n        raise RuntimeError(""Can not get {} for bidirectional. ""\n                           ""Either fw and bw is not in name scope."".format(\n                               names[-1]))\n    if names[-1] == ""kernel"":\n      if is_bidirectional:\n        w = tf.split(tensor_dict[node.inputs[1]], 2)[index]\n        r = tf.split(tensor_dict[node.inputs[2]], 2)[index]\n      else:\n        w = tensor_dict[node.inputs[1]]\n        r = tensor_dict[node.inputs[2]]\n      new_w = tf.transpose(tf.squeeze(w))\n      new_r = tf.transpose(tf.squeeze(r))\n      kernel = tf.concat([new_w, new_r], 0)\n      return kernel\n    if names[-1] == ""bias"":\n      if len(node.inputs) >= 4:\n        if is_bidirectional:\n          b = tf.split(tensor_dict[node.inputs[3]], 2)[index]\n        else:\n          b = tensor_dict[node.inputs[3]]\n        w_b, r_b = tf.split(tf.squeeze(b), 2)\n        w_b = tf.transpose(w_b)\n        r_b = tf.transpose(r_b)\n        return tf.add(w_b, r_b)\n      return getter(name, *args, **kwargs)\n    return getter(name, *args, **kwargs)\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n    input_shape = x.get_shape().as_list()\n    input_size = len(node.inputs)\n    hidden_size = node.attrs[""hidden_size""]\n    direction = node.attrs.get(""direction"", ""forward"")\n    num_directions = 2 if direction == ""bidirectional"" else 1\n\n    output_sequence = node.attrs.get(""output_sequence"", 0)\n\n    # TODO(fumihwh): check if prev node is one of RNN\n    # process input if it comes from other previous cell\n    # which has shape [seq_length, num_directions, batch_size, hidden_size]\n    if len(input_shape) == 4 and input_shape[1] == 1:\n      x = tf.squeeze(x)\n\n    sequence_length = None\n    if input_size >= 5 and node.inputs[4] in tensor_dict:\n      sequence_length = tensor_dict[node.inputs[4]]\n\n    cell_kwargs = {}\n\n    tf_activations = [tf.nn.tanh]\n    if ""activations"" in node.attrs:\n      activations = list(map(lambda x: x.lower(), node.attrs[""activations""]))\n      activation_alpha = node.attrs.get(""activation_alpha"", [None] * 2)\n      activation_beta = node.attrs.get(""activation_beta"", [None] * 2)\n      tf_activations = [\n          cls.rnn_get_activation(activations[0], activation_alpha[0],\n                                 activation_beta[0])\n      ]\n      if num_directions == 2:\n        tf_activations.append(\n            cls.rnn_get_activation(activations[1], activation_alpha[1],\n                                   activation_beta[1]))\n\n    # TODO(fumihwh): check if reverse and bidirectional works\n    with tf.compat.v1.variable_scope(\n        ""RNN_"" + get_unique_suffix(),\n        custom_getter=partial(\n            cls._custom_getter,\n            node=node,\n            tensor_dict=tensor_dict,\n            is_bidirectional=num_directions == 2)):\n\n      cell_kwargs[""num_units""] = hidden_size\n      initial_state = None\n      initial_state_bw = None\n      if input_size == 6:\n        initial_h = tensor_dict.get(node.inputs[5], None)\n        if initial_h is not None:\n          initial_state = (initial_h[0],)\n          if num_directions == 2:\n            initial_state_bw = (initial_h[1],)\n\n      rnn_kwargs = {}\n      if num_directions == 1:\n        rnn_kwargs[""initial_state""] = initial_state\n      elif num_directions == 2:\n        rnn_kwargs[""initial_state_fw""] = initial_state\n        rnn_kwargs[""initial_state_bw""] = initial_state_bw\n      rnn_kwargs[""sequence_length""] = sequence_length\n      rnn_kwargs[""time_major""] = True\n      rnn_kwargs[""dtype""] = tf.float32\n\n      outputs, states = cls.rnn(x, tf.compat.v1.nn.rnn_cell.BasicRNNCell,\n                                cell_kwargs, rnn_kwargs, tf_activations,\n                                direction)\n\n    if num_directions == 1:\n      state = states[0]\n      h = tf.expand_dims(state, 0)\n      output = tf.expand_dims(outputs, 1)\n    else:\n      state_fw = states[0][0]\n      state_bw = states[1][0]\n      output_fw = outputs[0]\n      output_bw = outputs[1]\n      h_fw = tf.expand_dims(state_fw, 0)\n      h_bw = tf.expand_dims(state_bw, 0)\n      h = tf.concat((h_fw, h_bw), axis=0)\n      output_fw = tf.expand_dims(output_fw, 1)\n      output_bw = tf.expand_dims(output_bw, 1)\n      output = tf.concat((output_fw, output_bw), axis=1)\n\n    return [output, h] if output_sequence == 0 else [h]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/rnn_mixin.py,17,"b'from functools import partial\n\nimport tensorflow as tf\n# import tensorflow_probability as tfp\nfrom tensorflow.python.ops import array_ops\n\nfrom onnx_tf.common import exception\n\n\nclass RNNMixin(object):\n\n  ONNX_ACTIVATION_MAPPING = {\n      # Added from tf 1.8\n      # ""affine"": tf.contrib.distributions.bijectors.AffineScalar,\n      # tf.contrib was removed since tf 2.0,\n      # Class Affine had been move to the following module\n      # ""affine"": tfp.bijectors.Affine,\n      ""elu"": tf.nn.elu,\n      ""hard_sigmoid"": tf.keras.backend.hard_sigmoid,\n      ""leaky_relu"": tf.nn.leaky_relu,\n      ""relu"": tf.nn.relu,\n      ""sigmoid"": tf.sigmoid,\n      ""softsign"": tf.nn.softsign,\n      ""softplus"": tf.nn.softplus,\n      ""tanh"": tf.tanh,\n      ""thresholded_relu"": tf.keras.layers.ThresholdedReLU,\n  }\n\n  @classmethod\n  def rnn(cls, x, cell_class, cell_kwargs, rnn_kwargs, activations, direction):\n    cell_kwargs[""activation""] = activations[0]\n\n    rnn_cell = [cell_class(**cell_kwargs)]\n    cell_fw = tf.compat.v1.nn.rnn_cell.MultiRNNCell(rnn_cell)\n\n    if direction == ""bidirectional"":\n      cell_kwargs[""activation""] = activations[1]\n      rnn_cell_bw = [cell_class(**cell_kwargs)]\n      cell_bw = tf.compat.v1.nn.rnn_cell.MultiRNNCell(rnn_cell_bw)\n\n    if direction == ""forward"":\n      outputs, states = tf.compat.v1.nn.dynamic_rnn(cell_fw, x, **rnn_kwargs)\n    elif direction == ""bidirectional"":\n      outputs, states = tf.compat.v1.nn.bidirectional_dynamic_rnn(\n          cell_fw, cell_bw, x, **rnn_kwargs)\n    elif direction == ""reverse"":\n\n      def _reverse(input_, seq_dim):\n        return array_ops.reverse(input_, axis=[seq_dim])\n\n      time_dim = 0\n      inputs_reverse = _reverse(x, time_dim)\n      outputs, states = tf.compat.v1.nn.dynamic_rnn(cell_fw, inputs_reverse,\n                                                    **rnn_kwargs)\n      outputs = _reverse(outputs, time_dim)\n\n    return outputs, states\n\n  @classmethod\n  def rnn_get_activation(cls, name, alpha, beta):\n    if name not in cls.ONNX_ACTIVATION_MAPPING:\n      exception.OP_UNSUPPORTED_EXCEPT(""Activation function {} for {}"".format(\n          name, cls.__name__), ""Tensorflow"")\n    activation = cls.ONNX_ACTIVATION_MAPPING[name]\n    kwargs = {}\n    if name == ""affine"":\n      kwargs[""scale""] = alpha\n      kwargs[""shift""] = beta\n      activation = activation(**kwargs)\n    elif name == ""elu"":\n      if alpha != 1:\n        exception.OP_UNSUPPORTED_EXCEPT(\n            ""Activation function {} with alpha={} for {}"".format(\n                name, alpha, cls.__name__), ""Tensorflow"")\n    elif name == ""hard_sigmoid"":\n      if alpha != 0.2 or beta != 0.5:\n        exception.OP_UNSUPPORTED_EXCEPT(\n            ""Activation function {} with alpha={}, beta={} for {}"".format(\n                name, alpha, beta, cls.__name__), ""Tensorflow"")\n    elif name == ""leaky_relu"":\n      kwargs[""alpha""] = alpha or 0.01\n      activation = partial(activation, **kwargs)\n    elif name == ""thresholded_relu"":\n      kwargs[""theta""] = alpha\n      activation = activation(**kwargs)\n    return activation\n'"
onnx_tf/handlers/backend/round.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Round"")\n@tf_func(tf.round)\nclass Round(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/scan.py,2,"b'from onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .scan_mixin import ScanMixin\n\n\n@onnx_op(""Scan"")\nclass Scan(ScanMixin, BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    return cls.scan(node, kwargs[""tensor_dict""], kwargs.get(""strict"", True))\n\n  @classmethod\n  def version_8(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/scan_mixin.py,25,"b'import tensorflow as tf\nfrom onnx.helper import make_opsetid\nimport onnx_tf\nfrom onnx_tf.common import data_type\n\n\nclass ScanMixin(object):\n\n  @classmethod\n  def scan(cls, node, input_dict, strict):\n    current_opset = [make_opsetid(cls.DOMAIN, cls.VERSION)]\n\n    body = node.attrs[""body""]\n\n    # in version 8, node.inputs[0] is the sequence_lens\n    node_inputs = node.inputs if cls.SINCE_VERSION != 8 else \\\n        node.inputs[1:]\n    # M\n    num_scan_inputs = int(node.attrs[""num_scan_inputs""])\n    # N = num_inputs - M\n    num_state_vars = len(node_inputs) - num_scan_inputs\n    # K = num_outputs - N\n    num_scan_outputs = len(node.outputs) - num_state_vars\n\n    """"""\n        Function to run subgraph used with tf.scan\n    """"""\n\n    def run_subgraph(a, b):\n      input_values = {}\n      # set the input values for the subgraph\n      # set the values for the state variables\n      for i in range(num_state_vars):\n        input_values[body.input[i].name] = a[i]\n      # set the values for the scan inputs\n      for i in range(num_scan_inputs):\n        input_values[body.input[i + num_state_vars].name] = b[i]\n\n      # get the tensor operations for the onnx graph\n      subgraph_tensor_dict = onnx_tf.backend.onnx_graph_to_tensorflow_ops(\n          subgraph=body,\n          input_values=input_values,\n          tensor_dict=input_dict,\n          opset=current_opset,\n          strict=strict)\n      # return sequence of tensors for every subgraph output\n      outputs = [subgraph_tensor_dict[output.name] for output in body.output]\n      return outputs\n\n    scan_input_axes = node.attrs.get(""scan_input_axes"", [0] * num_scan_inputs)\n    scan_input_directions = node.attrs.get(""directions""\n                                           if cls.SINCE_VERSION == 8 else\n                                           ""scan_input_directions"",\n                                           [0] * num_scan_inputs)\n    scan_output_axes = node.attrs.get(""scan_output_axes"",\n                                      [0] * num_scan_outputs)\n    scan_output_directions = node.attrs.get(""scan_output_directions"",\n                                            [0] * num_scan_outputs)\n\n    # if version 8 read the sequnce_lens from the first input\n    if cls.SINCE_VERSION == 8:\n      sequence_lens = input_dict[node.inputs[0]] \\\n                      if node.inputs[0] != \'\' else None\n\n    inputs = [input_dict[node_input] for node_input in node_inputs]\n\n    scan_inputs = inputs[num_state_vars:]\n    # loop over all the scan inputs and apply transpose depending\n    # on input axes provided and also reverse the scan inputs if\n    # reverse direction for scan is provided\n    for i in range(num_scan_inputs):\n      # if input axes are different than 0, use transpose to scan over\n      # the provided axes\n      if scan_input_axes[i] != 0:\n        transpose_perm = cls._calc_transpose_perm_input(\n            tf.rank(scan_inputs[i]), scan_input_axes[i])\n        scan_inputs[i] = tf.transpose(scan_inputs[i], transpose_perm)\n\n      # check for reverse direction scans\n      if scan_input_directions[i] == 1:\n        # version 8 has a batch dimension\n        axis = 0 if cls.SINCE_VERSION != 8 else 1\n        scan_inputs[i] = tf.reverse(scan_inputs[i], [axis])\n\n    state_vars_init = inputs[:num_state_vars]\n\n    scan_outputs_init = []\n    # generate sequence of zero tensors for all scan outputs\n    # with the correct shape and dtype\n    for scan_output in body.output[num_state_vars:]:\n      tensor_type = scan_output.type.tensor_type\n      shape = [\n          d.dim_value if (d.dim_value > 0 and d.dim_param == """") else None\n          for d in tensor_type.shape.dim\n      ]\n      dtype = data_type.onnx2tf(tensor_type.elem_type)\n      scan_outputs_init.append(tf.zeros(shape, dtype=dtype))\n\n    # tf.scan initilizer is state_variables_init + scan_outputs_init\n    initializer = state_vars_init + scan_outputs_init\n\n    if cls.SINCE_VERSION == 8:\n      # version == 8\n      # function to process the batches. it is used with tf.map_fn\n      def run_batches(x):\n        # state vars initial values per batch\n        initial = x[0]\n        # scan inputs per batch\n        scan_inputs = x[1]\n        # sequence length for the batch\n        seq_len = x[2]\n\n        # slice the input to the current sequence len\n        scan_inputs = [scan_input[:seq_len, ...] for scan_input in scan_inputs]\n\n        # run scan on the current batch\n        out = tf.scan(\n            run_subgraph, scan_inputs, initializer=initial + scan_outputs_init)\n\n        # pad to the original shape with zeros\n        paddings = [[0, tf.shape(x[1][0], out_type=seq_len.dtype)[0] - seq_len]]\n        for i in range(len(out)):\n          pads = tf.concat(\n              [paddings,\n               tf.zeros([(tf.rank(out[i]) - 1), 2], dtype=tf.int32)],\n              axis=0)\n          out[i] = tf.pad(out[i], pads)\n        return out\n\n      if sequence_lens is None:\n        # if sequence_lens is None, fill it with the shape of\n        # the input axis 1\n        sequence_lens = tf.fill([tf.shape(scan_inputs[0])[0]],\n                                tf.shape(scan_inputs[0], out_type=tf.int32)[1])\n\n      output_types = [\n          data_type.onnx2tf(output.type.tensor_type.elem_type)\n          for output in body.output\n      ]\n      # run scan for every batch\n      out = tf.map_fn(\n          run_batches, (state_vars_init, scan_inputs, sequence_lens),\n          dtype=output_types)\n\n      state_vars_outputs = []\n      # extract the final values of the state variables\n      for state_var in out[:num_state_vars]:\n        state_vars_outputs.append(\n            tf.map_fn(lambda x: x[0][x[1] - 1], (state_var, sequence_lens),\n                      state_var.dtype))\n    else:\n      # version > 8\n      # run the scan\n      out = tf.scan(run_subgraph, scan_inputs, initializer=initializer)\n\n      # extract the final values of the state variables\n      state_vars_outputs = [\n          state_var[tf.shape(state_var)[0] - 1]\n          for state_var in out[:num_state_vars]\n      ]\n\n    scan_outputs = out[num_state_vars:]\n\n    # post process the scan outputs depending on the directions and\n    # axes provided.\n    for i in range(num_scan_outputs):\n      # check for reverse direction scan outputs\n      if scan_output_directions[i] == 1:\n        scan_outputs[i] = tf.reverse(scan_outputs[i], [0])\n\n      if scan_output_axes[i] != 0:\n        transpose_perm = cls._calc_transpose_perm_output(\n            tf.rank(scan_outputs[i]), scan_output_axes[i])\n        scan_outputs[i] = tf.transpose(scan_outputs[i], transpose_perm)\n\n    return state_vars_outputs + scan_outputs\n\n  @classmethod\n  def _calc_transpose_perm_input(cls, rank, axis):\n    if axis < 0:\n      axis = rank + axis\n    return tf.concat([[axis], tf.range(axis), tf.range(axis + 1, rank)], 0)\n\n  @classmethod\n  def _calc_transpose_perm_output(cls, rank, axis):\n    if axis < 0:\n      axis = rank + axis\n    return tf.concat([tf.range(1, axis + 1), [0], tf.range(axis + 1, rank)], 0)\n'"
onnx_tf/handlers/backend/scatter.py,3,"b'from onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\nfrom onnx_tf.handlers.backend.scatter_elements import ScatterElements\n\n\n@onnx_op(""Scatter"")\nclass Scatter(BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return ScatterElements.version_11(node, **kwargs)\n'"
onnx_tf/handlers/backend/scatter_elements.py,13,"b'import tensorflow as tf\n\nfrom onnx_tf.common.tf_helper import tf_shape\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom .gather_and_scatter_mixin import GatherAndScatterMixin\n\n\n@onnx_op(""ScatterElements"")\nclass ScatterElements(GatherAndScatterMixin, BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    axis = node.attrs.get(""axis"", 0)\n    data = kwargs[""tensor_dict""][node.inputs[0]]\n    indices = kwargs[""tensor_dict""][node.inputs[1]]\n    updates = kwargs[""tensor_dict""][node.inputs[2]]\n\n    # poocess negative axis\n    axis = axis if axis >= 0 else tf.add(tf.rank(data), axis)\n\n    # check are there any indices are out of bounds\n    result = cls.chk_idx_out_of_bounds_along_axis(data, axis, indices)\n    msg = \'ScatterElements indices are out of bounds, please double check the indices and retry.\'\n    with tf.control_dependencies(\n        [tf.compat.v1.assert_equal(result, True, message=msg)]):\n      # process negative indices\n      indices = cls.process_neg_idx_along_axis(data, axis, indices)\n\n      # Calculate shape of the tensorflow version of indices tensor.\n      sparsified_dense_idx_shape = tf_shape(updates)\n\n      # Move on to convert ONNX indices to tensorflow indices in 2 steps:\n      #\n      # Step 1:\n      #   What would the index tensors look like if updates are all\n      #   dense? In other words, produce a coordinate tensor for updates:\n      #\n      #   coordinate[i, j, k ...] = [i, j, k ...]\n      #   where the shape of ""coordinate"" tensor is same as that of updates.\n      #\n      # Step 2:\n      #   But the coordinate tensor needs some correction because coord\n      #   vector at position axis is wrong (since we assumed update is dense,\n      #   but it is not at the axis specified).\n      #   So we update coordinate vector tensor elements at psotion=axis with\n      #   the sparse coordinate indices.\n\n      idx_tensors_per_axis = tf.meshgrid(\n          *list(\n              map(lambda x: tf.range(x, dtype=tf.dtypes.int64),\n                  sparsified_dense_idx_shape)),\n          indexing=\'ij\')\n      idx_tensors_per_axis[axis] = indices\n      dim_expanded_idx_tensors_per_axis = list(\n          map(lambda x: tf.expand_dims(x, axis=-1), idx_tensors_per_axis))\n      coordinate = tf.concat(dim_expanded_idx_tensors_per_axis, axis=-1)\n\n      # Now the coordinate tensor is in the shape\n      # [updates.shape, updates.rank]\n      # we need it to flattened into the shape:\n      # [product(updates.shape), updates.rank]\n      indices = tf.reshape(coordinate, [-1, tf.rank(data)])\n      updates = tf.reshape(updates, [-1])\n\n      return [tf.tensor_scatter_nd_update(data, indices, updates)]\n'"
onnx_tf/handlers/backend/scatter_nd.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .gather_and_scatter_mixin import GatherAndScatterMixin\n\n\n@onnx_op(""ScatterND"")\n@tf_func(tf.tensor_scatter_nd_update)\nclass ScatterND(GatherAndScatterMixin, BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    data = kwargs[""tensor_dict""][node.inputs[0]]\n    indices = kwargs[""tensor_dict""][node.inputs[1]]\n    updates = kwargs[""tensor_dict""][node.inputs[2]]\n\n    result = cls.chk_idx_out_of_bounds(data, indices)\n    msg = \'ScatterND indices are out of bounds, please double check the indices and retry.\'\n    with tf.control_dependencies(\n        [tf.compat.v1.assert_equal(result, True, message=msg)]):\n      indices = cls.process_neg_idx(data, indices)\n      return [\n          cls.make_tensor_from_onnx_node(node,\n                                         inputs=[data, indices, updates],\n                                         **kwargs)\n      ]\n'"
onnx_tf/handlers/backend/selu.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Selu"")\n@tf_func(tf.nn.selu)\nclass Selu(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    if ""alpha"" not in node.attrs and ""gamma"" not in node.attrs:\n      return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n    x = tensor_dict[node.inputs[0]]\n    alpha = node.attrs.get(""alpha"", 1.67326319217681884765625)\n    gamma = node.attrs.get(""gamma"", 1.05070102214813232421875)\n\n    return [\n        tf.clip_by_value(x, 0, tf.reduce_max(x)) * gamma +\n        (tf.exp(tf.clip_by_value(x, tf.reduce_min(x), 0)) - 1) * alpha * gamma\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/sequence_at.py,9,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""SequenceAt"")\nclass SequenceAt(BackendHandler):\n\n  @classmethod\n  def chk_pos_in_bounds(cls, input_seq, pos):\n    """"""\n    Check the position is in-bounds with respect to the sequence.\n    Accepted range for \'position\' is in [-n, n - 1], where n is the\n    number of tensors in \'input_sequence\'.\n\n    :param input_seq: input sequence\n    :param pos: position of the output tensor\n\n    :return: True if position is in-bounds or input length is dynamic.\n    """"""\n    seq_length = input_seq.shape[0]\n\n    if seq_length is None: return True\n\n    seq_length = tf.cast(seq_length, tf.int32)\n    pos = tf.cast(pos, tf.int32)\n    cond1 = tf.greater_equal(pos, tf.negative(seq_length))\n    cond2 = tf.less_equal(pos, seq_length - 1)\n\n    # pos >= -n and pos < n\n    return tf.reduce_all(tf.logical_and(cond1, cond2))\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    input_sequence = tensor_dict[node.inputs[0]]\n    position = tensor_dict[node.inputs[1]]\n\n    # check whether position is in-bounds and assert if not\n    result = cls.chk_pos_in_bounds(input_sequence, position)\n    assert_pos = tf.Assert(tf.equal(result, True), [result])\n\n    with tf.control_dependencies([assert_pos]):\n      return [input_sequence[position]]\n'"
onnx_tf/handlers/backend/sequence_construct.py,6,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""SequenceConstruct"")\nclass SequenceConstruct(BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    # create an empty sequence first\n    tensor_dict = kwargs[""tensor_dict""]\n    dtype = tensor_dict[node.inputs[0]].dtype\n    input_sequence = tf.ragged.constant([], dtype=dtype)\n\n    # insert tensors at the end of sequence\n    for i in range(len(node.inputs)):\n      input_tensor = tf.expand_dims(tensor_dict[node.inputs[i]], 0)\n      if input_sequence.shape[0] == 0:\n        output_seq = tf.RaggedTensor.from_tensor(input_tensor)\n      else:\n        output_seq = tf.concat([input_sequence, input_tensor], axis=0)\n      input_sequence = output_seq\n\n    return [output_seq]\n'"
onnx_tf/handlers/backend/sequence_empty.py,6,"b'import numpy as np\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.common import data_type\nfrom onnx import mapping\n\n\n@onnx_op(""SequenceEmpty"")\nclass SequenceEmpty(BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    default_dtype = mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype(\'float32\')]\n    dtype = data_type.onnx2tf(node.attrs.get(""dtype"", default_dtype))\n\n    ragged = tf.RaggedTensor.from_row_lengths(values=[], row_lengths=[])\n    sparse = tf.cast(ragged.to_sparse(), dtype)\n    return [tf.RaggedTensor.from_sparse(sparse)]\n'"
onnx_tf/handlers/backend/sequence_erase.py,10,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""SequenceErase"")\nclass SequenceErase(BackendHandler):\n\n  @classmethod\n  def chk_pos_in_bounds(cls, input_seq, pos):\n    """"""\n    Check the position is in-bounds with respect to the sequence.\n    Accepted range for \'position\' is in [-n, n - 1], where n is the\n    number of tensors in \'input_sequence\'.\n\n    :param input_seq: input sequence\n    :param pos: position of the output tensor\n\n    :return: True if position is in-bounds \n    """"""\n    seq_length = tf.shape(input_seq.to_sparse())[0]\n    pos = tf.cast(pos, tf.int32)\n    cond1 = tf.greater_equal(pos, tf.negative(seq_length))\n    cond2 = tf.less_equal(pos, seq_length - 1)\n\n    # pos >= -n and pos < n\n    return tf.reduce_all(tf.logical_and(cond1, cond2))\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    input_sequence = tensor_dict[node.inputs[0]]\n    position = tensor_dict[node.inputs[1]]\n\n    # check whether position is in-bounds and assert if not\n    result = cls.chk_pos_in_bounds(input_sequence, position)\n    assert_pos = tf.Assert(tf.equal(result, True), [result])\n\n    with tf.control_dependencies([assert_pos]):\n      s1 = input_sequence[:position]\n      s2 = input_sequence[position + 1:]\n      return [tf.concat([s1, s2], axis=0)]\n'"
onnx_tf/handlers/backend/sequence_insert.py,13,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n@onnx_op(""SequenceInsert"")\nclass SequenceInsert(BackendHandler):\n\n  @classmethod\n  def chk_pos_in_bounds(cls, input_seq, pos):\n    """""" \n    Check the position is in-bounds with respect to the sequence.\n    Accepted range for \'position\' is in [-n, n], where n is the \n    number of tensors in \'input_sequence\'. \n\n    :param input_seq: input sequence\n    :param pos: position to insert the tensor\n\n    :return: True if position is in-bounds.\n    """"""\n    seq_length = tf.shape(input_seq.to_sparse())[0]\n    pos = tf.cast(pos, tf.int32)\n\n    cond1 = tf.greater_equal(pos, tf.negative(seq_length))\n    cond2 = tf.less_equal(pos, seq_length)\n\n    # pos >= -n and pos <= n\n    return tf.reduce_all(tf.logical_and(cond1, cond2))\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    input_sequence = tensor_dict[node.inputs[0]]\n    input_tensor = tensor_dict[node.inputs[1]]\n\n    position = tensor_dict[node.inputs[2]] if len(node.inputs) > 2 else tf.shape(input_sequence.to_sparse())[0]\n\n    # check whether position is in-bounds and assert if not\n    result = cls.chk_pos_in_bounds(input_sequence, position)\n    assert_pos = tf.Assert(tf.equal(result, True), [result])\n\n    with tf.control_dependencies([assert_pos]):\n      input_tensor = tf.expand_dims(input_tensor, 0)\n      if input_sequence.shape[0] == 0:\n        output_seq = tf.RaggedTensor.from_tensor(input_tensor) \n      else:\n        s1 = input_sequence[:position]\n        s2 = input_sequence[position:]\n        output_seq = tf.concat([s1, input_tensor, s2], axis = 0)\n\n      return [output_seq]\n'"
onnx_tf/handlers/backend/sequence_length.py,3,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""SequenceLength"")\nclass SequenceLength(BackendHandler):\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    input_sequence = tensor_dict[node.inputs[0]]\n\n    return [tf.shape(input_sequence.to_sparse())[0]]\n'"
onnx_tf/handlers/backend/shape.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Shape"")\n@tf_func(tf.shape)\nclass Shape(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""out_type"": tf.int64}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/shrink.py,14,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""Shrink"")\nclass Shrink(BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    input_tensor = tensor_dict[node.inputs[0]]\n    input_shape = tf.shape(input_tensor, out_type=tf.int64)\n\n    # handle defaults for attributes\n    lambd = node.attrs[""lambd""] if ""lambd"" in node.attrs else 0.5\n    bias = node.attrs[""bias""] if ""bias"" in node.attrs else 0.0\n\n    # make tensors in the right shape\n    lambd_tensor = tf.fill(input_shape, tf.constant(lambd, input_tensor.dtype))\n    lambd_neg_tensor = tf.fill(input_shape,\n                               tf.constant(lambd * -1, input_tensor.dtype))\n    bias_tensor = tf.fill(input_shape, tf.constant(bias, input_tensor.dtype))\n    zeros_tensor = tf.zeros(input_shape, input_tensor.dtype)\n\n    # prepare return values and conditions\n    input_plus = tf.add(input_tensor, bias_tensor)\n    input_minus = tf.subtract(input_tensor, bias_tensor)\n    greater_cond = tf.greater(input_tensor, lambd_tensor)\n    less_cond = tf.less(input_tensor, lambd_neg_tensor)\n\n    return [\n        tf.where(less_cond, input_plus,\n                 tf.where(greater_cond, input_minus, zeros_tensor))\n    ]\n'"
onnx_tf/handlers/backend/sigmoid.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Sigmoid"")\n@tf_func(tf.nn.sigmoid)\nclass Sigmoid(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/sign.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Sign"")\n@tf_func(tf.sign)\nclass Sign(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/sin.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Sin"")\n@tf_func(tf.sin)\nclass Sin(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/sinh.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Sinh"")\n@tf_func(tf.sinh)\nclass Sinh(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/size.py,5,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Size"")\n@tf_func(tf.size)\nclass Size(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""out_type"": tf.int64}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/slice.py,30,"b'import numpy as np\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Slice"")\n@tf_func(tf.strided_slice)\nclass Slice(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    x = tensor_dict[node.inputs[0]]\n\n    full_sizes = x.get_shape().as_list()\n    full_begin = [0] * len(full_sizes)\n\n    starts = node.attrs.get(""starts"")\n    ends = node.attrs.get(""ends"")\n    slice_len = len(starts)\n    axes = node.attrs.get(""axes"", list(range(slice_len)))\n\n    for i in range(slice_len):\n      starts[i] = full_sizes[axes[i]] + starts[i] if starts[i] < 0 else starts[\n          i]\n      ends[i] = full_sizes[axes[i]] + ends[i] if ends[i] < 0 else ends[i]\n      if full_sizes[axes[i]] is not None:\n        ends[i] = np.min([full_sizes[axes[i]], ends[i]])\n        starts[i] = np.min([full_sizes[axes[i]], starts[i]])\n      full_begin[axes[i]] = starts[i]\n      full_sizes[axes[i]] = ends[i] - starts[i]\n\n    return [\n        cls.make_tensor_from_onnx_node(\n            node,\n            tf_func=tf.slice,\n            inputs=[\n                tensor_dict[node.inputs[0]],\n                tf.constant(full_begin),\n                tf.constant(full_sizes)\n            ],\n            **kwargs)\n    ]\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    input_tensor = tensor_dict[node.inputs[0]]\n    starts = tensor_dict[node.inputs[1]]\n    ends = tensor_dict[node.inputs[2]]\n\n    # first of all, get the input tensor shape\n    input_tensor_shape = tf.shape(input_tensor, out_type=ends.dtype)\n\n    axes = tensor_dict[node.inputs[3]] if len(\n        node.inputs) >= 4 else tf.range(tf.shape(starts)[0], dtype=ends.dtype)\n\n    is_axes_negative = tf.less(axes, tf.zeros_like(axes))\n    axes = tf.where(is_axes_negative, axes + tf.cast(tf.rank(input_tensor), axes.dtype), axes)\n\n    # expand a dimension of 1 at the end\n    sparse_indices = tf.expand_dims(axes, -1)\n\n    # build the indexed dimension sizes as sparse_shape\n    sparse_shape = tf.gather_nd(\n        params=input_tensor_shape, indices=sparse_indices)\n    sparse_shape = tf.cast(sparse_shape, ends.dtype)\n\n    # take care of starts, ends that are larger than the dim size.\n    starts_min = tf.minimum(starts, sparse_shape)\n    ends_min = tf.minimum(ends, sparse_shape)\n\n    # take care of starts, ends that are negative\n    is_starts_negative = tf.less(starts_min, tf.zeros_like(starts_min))\n    starts_final = tf.where(is_starts_negative, starts_min + sparse_shape,\n                            starts_min)\n    is_ends_negative = tf.less(ends_min, tf.zeros_like(ends_min))\n    ends_final = tf.where(is_ends_negative, ends_min + sparse_shape, ends_min)\n\n    # need to densify everything for the inputs to slice\n    # the output shape is the input_tensor rank\n    output_shape = tf.reshape(tf.rank(input_tensor), [1])\n    output_shape = tf.cast(output_shape, ends.dtype)\n\n    # create dense tensor, pad 0 as default begins\n    dense_begins = tf.compat.v1.sparse_to_dense(sparse_indices, output_shape,\n                                                starts_final)\n    # create dense tensor, pad -1 for next step\n    dense_ends = tf.compat.v1.sparse_to_dense(\n        sparse_indices,\n        output_shape,\n        ends_final,\n        default_value=tf.constant(-1, dtype=dense_begins.dtype))\n    # replace -1 with respective dimension sizes\n    dense_ends = tf.where(\n        tf.equal(dense_ends, tf.constant(-1, dtype=dense_begins.dtype)),\n        input_tensor_shape, dense_ends)\n\n    # create dense tensor for steps if not already so\n    if len(node.inputs) >= 5:\n      dense_steps = tf.compat.v1.sparse_to_dense(\n          sparse_indices,\n          output_shape,\n          tensor_dict[node.inputs[4]],\n          default_value=tf.constant(1, dtype=tensor_dict[node.inputs[4]].dtype))\n    else:\n      dense_steps = tf.ones(input_tensor_shape.shape, ends.dtype)\n\n    return [\n        cls.make_tensor_from_onnx_node(\n            node,\n            inputs=[\n                tensor_dict[node.inputs[0]], dense_begins, dense_ends,\n                dense_steps\n            ],\n            **kwargs)\n    ]\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls.version_10(node, **kwargs)\n'"
onnx_tf/handlers/backend/softmax.py,9,"b'import numpy as np\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Softmax"")\n@tf_func(tf.nn.softmax)\nclass Softmax(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    axis = node.attrs.get(""axis"", 1)\n    axis = axis if axis >= 0 else len(np.shape(x)) + axis\n\n    if axis == len(np.shape(x)) - 1:\n      return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n    shape = tf.shape(x)\n    cal_shape = (tf.reduce_prod(shape[0:axis]),\n                 tf.reduce_prod(shape[axis:tf.size(shape)]))\n    x = tf.reshape(x, cal_shape)\n\n    return [tf.reshape(tf.nn.softmax(x), shape)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/softplus.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Softplus"")\n@tf_func(tf.nn.softplus)\nclass Softplus(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/softsign.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Softsign"")\n@tf_func(tf.nn.softsign)\nclass Softsign(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/space_to_depth.py,5,"b'import copy\n\nimport tensorflow as tf\n\nfrom onnx_tf.common import get_data_format\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""SpaceToDepth"")\n@tf_func(tf.nn.space_to_depth)\nclass SpaceToDepth(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""rename"": {""blocksize"": ""block_size""}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_rank = len(x.get_shape())\n    storage_format, compute_format = get_data_format(x_rank)\n    attrs = copy.deepcopy(node.attrs)\n    attrs[""data_format""] = storage_format\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, attrs=attrs, c_first_cuda_only=True, **kwargs)\n    ]\n'"
onnx_tf/handlers/backend/split.py,6,"b'import copy\n\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom onnx_tf.common.tf_helper import tf_shape\n\n\n@onnx_op(""Split"")\n@tf_func(tf.split)\nclass Split(BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    axis = node.attrs.get(""axis"", 0)\n    x_rank =  len(kwargs[""tensor_dict""][node.inputs[0]].get_shape().as_list())\n    if axis > x_rank - 1 or axis < -x_rank:\n        raise ValueError(""Axis is out of bound"")\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""default"": {""axis"": 0}}\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    input = tensor_dict[node.inputs[0]]\n    x_shape = tf_shape(input)\n    attrs = copy.deepcopy(node.attrs)\n    axis = attrs.get(""axis"", 0)\n    axis = axis if axis >= 0 else len(x_shape) + axis\n    if ""split"" in node.attrs:\n      split = attrs[""split""]\n    elif len(node.inputs) == 2:  # since version 1\n      split = tensor_dict[node.inputs[1]]\n    else:\n      per_part = x_shape[axis] / len(node.outputs)\n      if input.shape.is_fully_defined():\n        if int(per_part) != per_part:\n          raise ValueError(""Split can not be evenly divided."")\n        split = [int(per_part)] * len(node.outputs)\n      else:\n        split = [tf.cast(per_part, tf.int32)] * len(node.outputs)\n    attrs[""num_or_size_splits""] = split\n    return list(\n        cls.make_tensor_from_onnx_node(\n            node, inputs=[input], attrs=attrs, **kwargs))\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_2(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/sqrt.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Sqrt"")\n@tf_func(tf.sqrt)\nclass Sqrt(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/squeeze.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Squeeze"")\n@tf_func(tf.squeeze)\nclass Squeeze(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""rename"": {""axes"": ""axis""}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/sub.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ArithmeticMixin\n\n\n@onnx_op(""Sub"")\n@tf_func(tf.subtract)\nclass Sub(ArithmeticMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/sum.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import ArithmeticMixin\n\n\n@onnx_op(""Sum"")\n@tf_func(tf.add_n)\nclass Sum(ArithmeticMixin, BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    tensor_dict = kwargs[""tensor_dict""]\n    return [\n        cls.make_tensor_from_onnx_node(\n            node,\n            inputs=[[tensor_dict.get(inp, None) for inp in node.inputs]],\n            **kwargs)\n    ]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_8(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/tan.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Tan"")\n@tf_func(tf.tan)\nclass Tan(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/tanh.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .math_mixin import BasicMathMixin\n\n\n@onnx_op(""Tanh"")\n@tf_func(tf.tanh)\nclass Tanh(BasicMathMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/tfidf_vectorizer.py,13,"b'import numpy as np\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""TfIdfVectorizer"")\nclass TfIdfVectorizer(BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    if ""pool_int64s"" in node.attrs and ""pool_strings"" in node.attrs:\n      raise ValueError(\n          ""Cannot set the pool_int64s and pool_strings in an input at the same time.""\n      )\n\n  @classmethod\n  def _prepare_ngrams(cls, x, n, skip):\n    # This method transform input into n-grams for a specific skip\n    # input x is a 1D tensor\n    # ex1: x=[1,2,3,4] n=1 skip=0 output=[[1],[2],[3],[4]]\n    # ex2: x=[1,2,3,4] n=2 skip=0 output=[[1,2],[2,3],[3,4]]\n    # ex3: x=[1,2,3,4] n=2 skip=1 output=[[1,3],[2,4]]\n    count = x.shape[0] - n + 1 - skip\n    multiplier = skip + 1\n    ngrams = [x[i * multiplier:i * multiplier + count] for i in range(n)]\n    ngrams = tf.stack(ngrams)\n    ngrams = tf.transpose(ngrams, [1, 0])\n    return ngrams\n\n  @classmethod\n  def _calc_ngram_skip(cls, x, pool, n, skip=0):\n    # This method calculates ngram counts for specific n and skip\n\n    # Make pool into an array of ngrams\n    pool = np.reshape(pool, (int(len(pool) / n), n))\n\n    # Make input as an array of ngrams\n    new_x = cls._prepare_ngrams(x, n, skip)\n\n    # Loop through the ngram targets in the pool\n    tensor_list = []\n    for i in range(len(pool)):\n      ngram_count = tf.map_fn(lambda in_x: tf.where(tf.reduce_all(tf.equal(in_x, tf.constant(pool[i], dtype=new_x.dtype))), tf.constant([1]), tf.constant([0])), new_x, dtype=tf.int32)\n      ngram_count = tf.math.count_nonzero(ngram_count, dtype=tf.int32)\n      ngram_count = tf.reshape(ngram_count, [1])\n      tensor_list.append(ngram_count)\n\n    return tf.concat(tensor_list, 0)\n\n  @classmethod\n  def _calc_ngram(cls, x, pool, n, max_skip):\n    # This method calculates ngram counts for a specific n and\n    # all allowable skips\n\n    # For 1gram, skip is not in use. Not clearly described in ONNX\n    # spec, this code logic is based on observation of ONNX examples,\n    # tf_batch_uniandbigrams_skip5 and tf_uniandbigrams_skip5,\n    # where the 1-gram results [0, 3, 0, 0] and [0, 3, 1, 0]\n    # are not the accumulated counts from multiple skips.\n    if n == 1:\n      return cls._calc_ngram_skip(x, pool, n)\n\n    # Loop through maximum allowable skip count and sum up the results\n    result = tf.zeros([int(len(pool) / n)], dtype=tf.int32)\n    max_allowable_skip = np.minimum(max_skip,\n                                    int((int(x.shape[0]) - 1) / (n - 1) - 1))\n\n    for skip in range(max_allowable_skip + 1):\n      # For each skip calculate the ngram counts\n      result += cls._calc_ngram_skip(x, pool, n, skip)\n\n    return result\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    input_tensor = kwargs[""tensor_dict""][node.inputs[0]]\n    mode = node.attrs.get(""mode"")\n    max_skip_count = node.attrs.get(""max_skip_count"")\n    min_gram_len = node.attrs.get(""min_gram_length"")\n    max_gram_len = node.attrs.get(""max_gram_length"")\n    ngram_counts = node.attrs.get(""ngram_counts"")\n    ngram_indexes = node.attrs.get(""ngram_indexes"")\n    pool_int64s = node.attrs.get(""pool_int64s"")\n    pool_strings = node.attrs.get(""pool_strings"")\n    weights = node.attrs.get(""weights"", np.ones(len(ngram_indexes)))\n\n    def process_ngram(input_t):\n      # This is the main method that processes and produces ngram counts\n      # for one row of inputs regardless of the operator input dimension.\n      size = len(ngram_indexes)\n      new_ngram_counts = np.append(ngram_counts, size)\n      result_ngram = np.zeros(size)\n      for i in range(len(new_ngram_counts) - 1):\n        gram_len = i + 1\n        count = new_ngram_counts[i + 1] - new_ngram_counts[i]\n        total_len = count * gram_len\n        if gram_len >= min_gram_len and gram_len <= max_gram_len:\n          idx = ngram_indexes[new_ngram_counts[i]:new_ngram_counts[i + 1]]\n          process_pool = pool_int64s[\n              new_ngram_counts[i]:new_ngram_counts[i] +\n              total_len] if pool_int64s is not None else pool_strings[\n                  new_ngram_counts[i]:new_ngram_counts[i] + total_len]\n          result = cls._calc_ngram(input_t, process_pool, gram_len,\n                                   max_skip_count)\n          idx = tf.constant(idx, shape=[len(idx), 1])\n          result_ngram = result_ngram + tf.scatter_nd(idx, result, [size])\n      return result_ngram\n\n    # The input can be either 1d or 2d. Need to loop through\n    # each element for 2d inputs\n    n = len(input_tensor.shape)\n    final_out = [\n        process_ngram(input_tensor[i]) for i in range(input_tensor.shape[0])\n    ] if n > 1 else process_ngram(input_tensor)\n    tf_out = tf.cast(final_out, tf.float32)\n\n    # Apply the mode based of the TF output\n    if mode == \'IDF\':\n      return [tf.minimum(tf_out, 1) * weights]\n    elif mode == \'TFIDF\':\n      return [tf_out * weights]\n    else:\n      return [tf_out]\n'"
onnx_tf/handlers/backend/thresholded_relu.py,3,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\n\n\n@onnx_op(""ThresholdedRelu"")\nclass ThresholdedRelu(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    alpha = node.attrs.get(""alpha"", 1.0)\n    epsilon = 1e-5\n    return [tf.nn.relu(x) - tf.nn.relu(tf.sign(alpha - x + epsilon) * x)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/tile.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Tile"")\n@tf_func(tf.tile)\nclass Tile(BackendHandler):\n\n  @classmethod\n  def get_attrs_processor_param(cls):\n    return {""rename"": {""axes"": ""axis""}}\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_rank = len(x.get_shape())\n    multiples = [1] * x_rank\n    axis = node.attrs[""axis""]\n    tiles = node.attrs[""tiles""]\n    multiples[axis] = tiles\n    inputs = [x, multiples]\n    return [cls.make_tensor_from_onnx_node(node, inputs=inputs, **kwargs)]\n\n  @classmethod\n  def version_6(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/top_k.py,25,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""TopK"")\n@tf_func(tf.nn.top_k)\nclass TopK(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_rank = len(x.get_shape())\n    axes = list(range(x_rank))\n    axis = node.attrs.get(""axis"", -1)\n    axis = axis if axis >= 0 else axis + x_rank\n\n    if axis != x_rank - 1:\n      pre_perm = [a for a in axes if a != axis] + [axis]\n      post_perm = axes[:axis] + [x_rank - 1] + axes[axis:x_rank - 1]\n      x = tf.transpose(x, perm=pre_perm)\n      values, indices = tf.nn.top_k(x, k=node.attrs[""k""])\n      values = tf.transpose(values, perm=post_perm)\n      return [values, tf.cast(indices, dtype=tf.int64)]\n\n    values, indices = tf.nn.top_k(x, k=node.attrs[""k""])\n    return [values, tf.cast(indices, dtype=tf.int64)]\n\n  @classmethod\n  def version_10(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_rank = len(x.get_shape())\n    axes = list(range(x_rank))\n    axis = node.attrs.get(""axis"", -1)\n    axis = axis if axis >= 0 else axis + x_rank\n    k = kwargs[""tensor_dict""][node.inputs[1]][0]\n    k = tf.cast(k, dtype=tf.int32)\n\n    if axis != x_rank - 1:\n      pre_perm = [a for a in axes if a != axis] + [axis]\n      post_perm = axes[:axis] + [x_rank - 1] + axes[axis:x_rank - 1]\n      x = tf.transpose(x, perm=pre_perm)\n      values, indices = tf.nn.top_k(x, k)\n      values = tf.transpose(values, perm=post_perm)\n      return [values, tf.cast(indices, dtype=tf.int64)]\n\n    values, indices = tf.nn.top_k(x, k)\n    return [values, tf.cast(indices, dtype=tf.int64)]\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_rank = len(x.get_shape())\n    axes = list(range(x_rank))\n    axis = node.attrs.get(""axis"", -1)\n    axis = axis if axis >= 0 else axis + x_rank\n    largest = node.attrs.get(""largest"", 1)\n    sort = node.attrs.get(""sorted"", 1)\n    sort = False if sort == 0 else True\n    k = kwargs[""tensor_dict""][node.inputs[1]][0]\n    k = tf.cast(k, dtype=tf.int32)\n\n    if largest == 0:\n      x = tf.negative(x)\n\n    if axis != x_rank - 1:\n      pre_perm = [a for a in axes if a != axis] + [axis]\n      post_perm = axes[:axis] + [x_rank - 1] + axes[axis:x_rank - 1]\n      x = tf.transpose(x, perm=pre_perm)\n      values, indices = tf.nn.top_k(x, k, sort)\n      values = tf.transpose(values, perm=post_perm)\n    else:\n      values, indices = tf.nn.top_k(x, k, sort)\n\n    if largest == 0:\n      values = tf.negative(values)\n\n    return [values, tf.cast(indices, dtype=tf.int64)]\n'"
onnx_tf/handlers/backend/transpose.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Transpose"")\n@tf_func(tf.transpose)\nclass Transpose(BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/unpool_mixin.py,23,"b'import tensorflow as tf\n\nfrom onnx_tf.common import get_data_format\nfrom onnx_tf.common import get_perm_from_formats\nfrom onnx_tf.common.tf_helper import tf_shape\n\n\nclass UnpoolMixin(object):\n\n  @classmethod\n  def max_unpool(cls, node, input_dict):\n    """"""\n            MaxUnpooling operation\n        """"""\n    x = input_dict[node.inputs[0]]\n    ind = input_dict[node.inputs[1]]\n    if len(node.inputs) > 2:\n      output_shape = input_dict.get(node.inputs[2], None)\n    else:\n      output_shape = None\n\n    kernel_shape = node.attrs[""kernel_shape""]\n\n    spatial_size = len(kernel_shape)\n    x_rank = spatial_size + 2\n    storage_format, _ = get_data_format(x_rank)\n\n    # if strides are not provided default is 1 along each spatial axis\n    strides = node.attrs.get(""strides"", [1] * spatial_size)\n    pads = node.attrs.get(""pads"", None)\n\n    input_shape = tf_shape(x)\n    default_shape = cls._get_default_shape(input_shape, kernel_shape, strides)\n\n    need_trans = storage_format != ""NHWC""\n    if need_trans:\n      x = tf.transpose(x, perm=get_perm_from_formats(storage_format, ""NHWC""))\n      ind = tf.transpose(\n          ind, perm=get_perm_from_formats(storage_format, ""NHWC""))\n\n    # default_shape to NHWC storage format\n    default_shape = [input_shape[0]] + default_shape + \\\n                    [input_shape[1]]\n\n    unpooled = cls._unpool(x, ind, default_shape)\n\n    if need_trans:\n      unpooled = tf.transpose(\n          unpooled, perm=get_perm_from_formats(""NHWC"", storage_format))\n\n    if output_shape is not None:\n      pads = cls._get_pads_from_output_shape(unpooled, output_shape)\n    if pads is not None:\n      unpooled = cls._pad_output(unpooled, pads, 0)\n\n    return [unpooled]\n\n  @classmethod\n  def _get_default_shape(cls, input_shape, kernel_shape, strides):\n    """"""\n            Calculates default shape from kernel_shape and strides\n            Args:\n                input_shape:   shape of the input to unpool op\n                kernel_shape:  the size of the kernel along each axis\n                output_shape:  stride along each spatial axis\n          Return:\n            default_shape: calculated default_shape\n        """"""\n    default_shape = []\n    for d in range(len(kernel_shape)):\n      default_shape.append((\n          input_shape[d + 2] - 1) * int(strides[d]) + int(kernel_shape[d]))\n    return default_shape\n\n  @classmethod\n  def _get_pads_from_output_shape(cls, unpool, output_shape):\n    """"""\n            Calculates the paddings from specified output_shape\n            Args:\n                unpool:       result from unpool operation\n                output_shape: expected shape of the output\n            Return:\n                pads:         calculated paddings in format\n                              [x1_begin, x2_begin,.., x1_end, x2_end]\n                              where xi_... represent pads added to begin\n                              or end of axis i\n        """"""\n    unpool_shape = tf.cast(tf.shape(unpool), dtype=tf.int32)\n    new_shape = tf.cast(output_shape, dtype=tf.int32)\n\n    pads_begin = []\n    pads_end = []\n\n    for d in range(len(unpool.get_shape())):\n      pad_total = new_shape[d] - unpool_shape[d]\n      pad_begin = tf.cast(pad_total / 2, tf.int32)\n      pad_end = pad_total - pad_begin\n      pads_begin = pads_begin + [pad_begin]\n      pads_end = pads_end + [pad_end]\n\n    pads = pads_begin + pads_end\n    return pads\n\n  @classmethod\n  def _pad_output(cls, unpool, pads, constant_values):\n    """"""\n            Pad the output from unpool op\n            Args:\n                unpool:         result from unpool op\n                pads:           paddings in format\n                                [x1_begin, x2_begin,..., x1_end, x2_end]\n                constant_values: constant value to fill up the padded spaces\n            Return:\n                padded:         padded tensor\n        """"""\n    unpool_shape = unpool.get_shape()\n    paddings = []\n    for d in range(len(unpool_shape)):\n      paddings = paddings + [[pads[d], pads[d + len(unpool_shape)]]]\n    padded = tf.pad(\n        unpool, paddings, \'CONSTANT\', constant_values=constant_values)\n    return padded\n\n  @classmethod\n  def _unpool(cls, pool, ind, output_shape, scope=\'unpool\'):\n    """"""\n            Unpooling layer after max_pool_with_argmax.\n\n            Args:\n                pool:          max pooled output tensor\n                ind:           argmax indices\n                output_shape:  the shape of the output\n            Return:\n                unpool:        unpooling tensor\n        """"""\n    with tf.compat.v1.variable_scope(scope):\n      input_shape = tf.shape(pool)\n\n      flat_input_size = tf.reduce_prod(input_shape)\n      flat_output_shape = [\n          output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]\n      ]\n\n      pool_ = tf.reshape(pool, [flat_input_size])\n      batch_range = tf.reshape(\n          tf.range(tf.cast(output_shape[0], tf.int64), dtype=ind.dtype),\n          shape=[input_shape[0], 1, 1, 1])\n      b = tf.ones_like(ind) * batch_range\n      b1 = tf.reshape(b, [flat_input_size, 1])\n      ind_ = tf.reshape(ind, [flat_input_size, 1])\n      ind_ = tf.concat([b1, ind_], 1)\n\n      ret = tf.scatter_nd(\n          ind_, pool_, shape=tf.cast(flat_output_shape, tf.int64))\n      ret = tf.reshape(ret, output_shape)\n    return ret\n'"
onnx_tf/handlers/backend/unsqueeze.py,5,"b'import copy\n\nimport tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Unsqueeze"")\n@tf_func(tf.expand_dims)\nclass Unsqueeze(BackendHandler):\n\n  @classmethod\n  def _common(cls, node, **kwargs):\n    attrs = copy.deepcopy(node.attrs)\n    axes = attrs.pop(""axes"")\n    if len(axes) != 1:\n      x = kwargs[""tensor_dict""][node.inputs[0]]\n      for axis in sorted(axes):\n        x = tf.expand_dims(x, axis=axis)\n      return [x]\n    attrs[""axis""] = axes[0]\n    return [cls.make_tensor_from_onnx_node(node, attrs=attrs, **kwargs)]\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n\n  @classmethod\n  def version_11(cls, node, **kwargs):\n    return cls._common(node, **kwargs)\n'"
onnx_tf/handlers/backend/upsample.py,15,"b'import copy\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom onnx_tf.common import exception\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import partial_support\nfrom onnx_tf.handlers.handler import ps_description\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Upsample"")\n@tf_func(tf.image.resize)\n@partial_support(True)\n@ps_description(""Upsample required 4D input in Tensorflow."")\nclass Upsample(BackendHandler):\n\n  @classmethod\n  def args_check(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_shape = x.get_shape().as_list()\n    if len(x_shape) != 4:\n      exception.OP_UNSUPPORTED_EXCEPT(""Upsample without 4D input"", ""Tensorflow"")\n\n    if node.attrs.get(\n        ""mode"", ""nearest"").lower() not in [""nearest"", ""bilinear"", ""linear""]:\n      exception.OP_UNSUPPORTED_EXCEPT(""Upsample without nearest or bilinear"",\n                                      ""Tensorflow"")\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_shape = x.get_shape().as_list()\n    attrs = copy.deepcopy(node.attrs)\n    scales = attrs[""scales""]\n    new_height = np.floor(x_shape[2] * scales[2])\n    new_weight = np.floor(x_shape[3] * scales[3])\n\n    mode = attrs.get(""mode"", ""nearest"")\n    if mode.lower() == ""bilinear"" or mode.lower() == ""linear"":\n      mode = tf.image.ResizeMethod.BILINEAR\n    else:\n      mode = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n\n    attrs[""size""] = np.array((new_height, new_weight), dtype=np.int32)\n    attrs[""method""] = mode\n\n    return [\n        cls.make_tensor_from_onnx_node(\n            node, attrs=attrs, c_last_only=True, **kwargs)\n    ]\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    x = kwargs[""tensor_dict""][node.inputs[0]]\n    x_shape = x.get_shape().as_list()\n    attrs = copy.deepcopy(node.attrs)\n    scales = kwargs[""tensor_dict""][node.inputs[1]]\n\n    assert_n_c_scale_is_one = tf.Assert(\n        tf.logical_and(tf.equal(scales[0], 1), tf.equal(scales[1], 1)),\n        [scales])\n\n    with tf.control_dependencies([assert_n_c_scale_is_one]):\n      h_w_scale = scales[2:]\n      h_w_shape = x_shape[2:]\n      new_h_w_shape = tf.cast(h_w_scale * h_w_shape, tf.int32)\n\n      mode = attrs.get(""mode"", ""nearest"")\n      if mode.lower() == ""bilinear"" or mode.lower() == ""linear"":\n        mode = tf.image.ResizeMethod.BILINEAR\n      else:\n        mode = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n\n      attrs[""size""] = new_h_w_shape\n      attrs[""method""] = mode\n\n      # Remove scale.\n      upsample_node = copy.deepcopy(node)\n      del upsample_node.inputs[1]\n      return [\n          cls.make_tensor_from_onnx_node(\n              upsample_node, attrs=attrs, c_last_only=True, **kwargs)\n      ]\n'"
onnx_tf/handlers/backend/where.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\n\n\n@onnx_op(""Where"")\n@tf_func(tf.where)\nclass Where(BackendHandler):\n\n  @classmethod\n  def version_9(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
onnx_tf/handlers/backend/xor.py,4,"b'import tensorflow as tf\n\nfrom onnx_tf.handlers.backend_handler import BackendHandler\nfrom onnx_tf.handlers.handler import onnx_op\nfrom onnx_tf.handlers.handler import tf_func\nfrom .control_flow_mixin import LogicalMixin\n\n\n@onnx_op(""Xor"")\n@tf_func(tf.math.logical_xor)\nclass Xor(LogicalMixin, BackendHandler):\n\n  @classmethod\n  def version_1(cls, node, **kwargs):\n    return cls.limited_broadcast(node, **kwargs)\n\n  @classmethod\n  def version_7(cls, node, **kwargs):\n    return [cls.make_tensor_from_onnx_node(node, **kwargs)]\n'"
