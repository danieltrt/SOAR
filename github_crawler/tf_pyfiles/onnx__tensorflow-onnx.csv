file_path,api_count,code
setup.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport distutils.command.build\nimport os\nimport subprocess\nfrom collections import namedtuple\nfrom textwrap import dedent\n\nimport setuptools.command.build_py\nimport setuptools.command.develop\nimport setuptools.command.install\nfrom setuptools import setup, find_packages, Command\n\nTOP_DIR = os.path.realpath(os.path.dirname(__file__))\nSRC_DIR = os.path.join(TOP_DIR, \'tf2onnx\')\n\ntry:\n    git_version = subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\'], cwd=TOP_DIR).decode(\'ascii\').strip()\nexcept (OSError, subprocess.CalledProcessError):\n    git_version = None\n\nwith open(os.path.join(TOP_DIR, \'VERSION_NUMBER\')) as version_file:\n    VersionInfo = namedtuple(\'VersionInfo\', [\'version\', \'git_version\'])(\n        version=version_file.read().strip(),\n        git_version=git_version\n    )\n\n\nclass create_version(Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        with open(os.path.join(SRC_DIR, \'version.py\'), \'w\') as f:\n            f.write(dedent(\'\'\'\n            version = \'{version}\'\n            git_version = \'{git_version}\'\n            \'\'\'.format(**dict(VersionInfo._asdict()))))\n\n\nclass build_py(setuptools.command.build_py.build_py):\n    def run(self):\n        self.run_command(\'create_version\')\n        setuptools.command.build_py.build_py.run(self)\n\n\nclass build(distutils.command.build.build):\n    def run(self):\n        self.run_command(\'build_py\')\n\n\nclass develop(setuptools.command.develop.develop):\n    def run(self):\n        self.run_command(\'create_version\')\n        self.run_command(\'build\')\n        setuptools.command.develop.develop.run(self)\n\n\ncmdclass = {\n    \'create_version\': create_version,\n    \'build_py\': build_py,\n    \'build\': build,\n    \'develop\': develop,\n}\n\nsetup(\n    name=""tf2onnx"",\n    version=VersionInfo.version,\n    description=\'Tensorflow to ONNX converter\',\n    setup_requires=[\'pytest-runner\'],\n    tests_require=[\'graphviz\', \'parameterized\', \'pytest\', \'pytest-cov\', \'pyyaml\'],\n    cmdclass=cmdclass,\n    packages=find_packages(),\n    author=\'onnx@microsoft.com\',\n    author_email=\'onnx@microsoft.com\',\n    url=\'https://github.com/onnx/tensorflow-onnx\',\n    install_requires=[\'numpy>=1.14.1\', \'onnx>=1.4.1\', \'requests\', \'six\']\n)\n'"
examples/call_coverter_via_python.py,4,"b'""""""\nA simple example how to call tensorflow-onnx via python.\n""""""\n\nimport tensorflow as tf\nimport tf2onnx\n\nwith tf.Session() as sess:\n    x = tf.placeholder(tf.float32, [2, 3], name=""input"")\n    x_ = tf.add(x, x)\n    _ = tf.identity(x_, name=""output"")\n    onnx_graph = tf2onnx.tfonnx.process_tf_graph(sess.graph, input_names=[""input:0""], output_names=[""output:0""])\n    model_proto = onnx_graph.make_model(""test"")\n    with open(""/tmp/model.onnx"", ""wb"") as f:\n        f.write(model_proto.SerializeToString())\n'"
examples/custom_op_via_python.py,6,"b'""""""\nA simple example how to map a custom op in python.\n""""""\nimport tensorflow as tf\nimport tf2onnx\nfrom onnx import helper\n\n_TENSORFLOW_DOMAIN = ""ai.onnx.converters.tensorflow""\n\n\ndef print_handler(ctx, node, name, args):\n    # replace tf.Print() with Identity\n    #   T output = Print(T input, data, @list(type) U, @string message, @int first_n, @int summarize)\n    # becomes:\n    #   T output = Identity(T Input)\n    node.type = ""Identity""\n    node.domain = _TENSORFLOW_DOMAIN\n    del node.input[1:]\n    return node\n\n\nwith tf.Session() as sess:\n    x = tf.placeholder(tf.float32, [2, 3], name=""input"")\n    x_ = tf.add(x, x)\n    x_ = tf.Print(x_, [x_], ""hello"")\n    _ = tf.identity(x_, name=""output"")\n    onnx_graph = tf2onnx.tfonnx.process_tf_graph(sess.graph,\n                                                 custom_op_handlers={""Print"": (print_handler, [])},\n                                                 extra_opset=[helper.make_opsetid(_TENSORFLOW_DOMAIN, 1)],\n                                                 input_names=[""input:0""],\n                                                 output_names=[""output:0""])\n    model_proto = onnx_graph.make_model(""test"")\n    with open(""/tmp/model.onnx"", ""wb"") as f:\n        f.write(model_proto.SerializeToString())\n'"
tests/backend_test_base.py,10,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Test Base.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,import-outside-toplevel\n# pylint: disable=wrong-import-position\n\nimport logging\nimport os\nimport unittest\n\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import variables as variables_lib\nfrom common import get_test_config\nfrom tf2onnx import utils\nfrom tf2onnx.tfonnx import process_tf_graph\nfrom tf2onnx import optimizer\nfrom tf2onnx.tf_loader import tf_reset_default_graph, tf_session, tf_placeholder, from_function, freeze_session\nfrom tf2onnx.tf_loader import tf_optimize, is_tf2\n\n\nclass Tf2OnnxBackendTestBase(unittest.TestCase):\n    def setUp(self):\n        self.config = get_test_config()\n        tf_reset_default_graph()\n        # reset name generation on every test\n        utils.INTERNAL_NAME = 1\n        np.random.seed(1)  # Make it reproducible.\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    def tearDown(self):\n        if not self.config.is_debug_mode:\n            utils.delete_directory(self.test_data_directory)\n\n    @property\n    def test_data_directory(self):\n        return os.path.join(self.config.temp_dir, self._testMethodName)\n\n    @staticmethod\n    def assertAllClose(expected, actual, **kwargs):\n        np.testing.assert_allclose(expected, actual, **kwargs)\n\n    @staticmethod\n    def assertAllEqual(expected, actual, **kwargs):\n        np.testing.assert_array_equal(expected, actual, **kwargs)\n\n    def run_onnxcaffe2(self, onnx_graph, inputs):\n        """"""Run test against caffe2 backend.""""""\n        import caffe2.python.onnx.backend\n        prepared_backend = caffe2.python.onnx.backend.prepare(onnx_graph)\n        results = prepared_backend.run(inputs)\n        return results\n\n    def run_onnxruntime(self, model_path, inputs, output_names):\n        """"""Run test against onnxruntime backend.""""""\n        import onnxruntime as rt\n        opt = rt.SessionOptions()\n        # in case of issues with the runtime, one can enable more logging\n        # opt.log_severity_level = 0\n        # opt.log_verbosity_level = 255\n        # opt.enable_profiling = True\n        m = rt.InferenceSession(model_path, opt)\n        results = m.run(output_names, inputs)\n        return results\n\n    def run_backend(self, g, outputs, input_dict):\n        model_proto = g.make_model(""test"")\n        model_path = self.save_onnx_model(model_proto, input_dict)\n\n        if self.config.backend == ""onnxruntime"":\n            y = self.run_onnxruntime(model_path, input_dict, outputs)\n        elif self.config.backend == ""caffe2"":\n            y = self.run_onnxcaffe2(model_proto, input_dict)\n        else:\n            raise ValueError(""unknown backend"")\n        return y\n\n    def run_test_case(self, func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-07, atol=1e-5,\n                      convert_var_to_const=True, constant_fold=True, check_value=True, check_shape=True,\n                      check_dtype=True, process_args=None, onnx_feed_dict=None, graph_validator=None, as_session=False):\n        # optional - passed to process_tf_graph\n        if process_args is None:\n            process_args = {}\n        # optional - pass distinct feed_dict to onnx runtime\n        if onnx_feed_dict is None:\n            onnx_feed_dict = feed_dict\n        input_names_with_port = list(feed_dict)\n        tf_reset_default_graph()\n        graph_def = None\n\n        np.random.seed(1)  # Make it reproducible.\n        clean_feed_dict = {utils.node_name(k): v for k, v in feed_dict.items()}\n        if is_tf2() and not as_session:\n            #\n            # use eager to execute the tensorflow func\n            #\n            # numpy doesn\'t work for all ops, make it tf.Tensor()\n            input_tensors = [tf.TensorSpec(shape=v.shape, dtype=tf.as_dtype(v.dtype), name=utils.node_name(k))\n                             for k, v in feed_dict.items()]\n            input_list = [tf.convert_to_tensor(v, dtype=tf.as_dtype(v.dtype), name=utils.node_name(k))\n                          for k, v in feed_dict.items()]\n            tf.random.set_seed(1)\n            expected = func(*input_list)\n            if isinstance(expected, (list, tuple)):\n                # list or tuple\n                expected = [x.numpy() for x in expected]\n            else:\n                # single result\n                expected = [expected.numpy()]\n\n            # now make the eager functions a graph\n            concrete_func = tf.function(func, input_signature=tuple(input_tensors))\n            concrete_func = concrete_func.get_concrete_function()\n            graph_def = from_function(concrete_func,\n                                      input_names=list(feed_dict.keys()), output_names=output_names_with_port)\n        else:\n            #\n            # use graph to execute the tensorflow func\n            #\n            with tf_session() as sess:\n                tf.set_random_seed(1)\n                input_list = []\n                for k, v in clean_feed_dict.items():\n                    input_list.append(tf_placeholder(name=k, shape=v.shape, dtype=tf.as_dtype(v.dtype)))\n                func(*input_list)\n                variables_lib.global_variables_initializer().run()\n                if not is_tf2():\n                    tf.tables_initializer().run()\n                output_dict = []\n                for out_name in output_names_with_port:\n                    output_dict.append(sess.graph.get_tensor_by_name(out_name))\n                expected = sess.run(output_dict, feed_dict=feed_dict)\n                graph_def = freeze_session(sess,\n                                           input_names=list(feed_dict.keys()),\n                                           output_names=output_names_with_port)\n\n            tf_reset_default_graph()\n            with tf_session() as sess:\n                tf.import_graph_def(graph_def, name=\'\')\n                graph_def = tf_optimize(list(feed_dict.keys()), output_names_with_port,\n                                        graph_def, fold_constant=constant_fold)\n\n        tf_reset_default_graph()\n        with tf_session() as sess:\n            tf.import_graph_def(graph_def, name=\'\')\n\n            if self.config.is_debug_mode:\n                model_path = os.path.join(self.test_data_directory, self._testMethodName + ""_after_tf_optimize.pb"")\n                utils.save_protobuf(model_path, graph_def)\n                self.logger.debug(""created file  %s"", model_path)\n\n            g = process_tf_graph(sess.graph, opset=self.config.opset,\n                                 input_names=list(feed_dict.keys()),\n                                 output_names=output_names_with_port,\n                                 target=self.config.target, **process_args)\n            g = optimizer.optimize_graph(g)\n            actual = self.run_backend(g, output_names_with_port, onnx_feed_dict)\n\n        for expected_val, actual_val in zip(expected, actual):\n            if check_value:\n                self.assertAllClose(expected_val, actual_val, rtol=rtol, atol=atol)\n            if check_dtype:\n                self.assertEqual(expected_val.dtype, actual_val.dtype)\n            # why need shape checke: issue when compare [] with scalar\n            # https://github.com/numpy/numpy/issues/11071\n            if check_shape:\n                self.assertEqual(expected_val.shape, actual_val.shape)\n\n        if graph_validator:\n            self.assertTrue(graph_validator(g))\n\n        return g\n\n    def save_onnx_model(self, model_proto, feed_dict, postfix=""""):\n        target_path = utils.save_onnx_model(self.test_data_directory, self._testMethodName + postfix, feed_dict,\n                                            model_proto, include_test_data=self.config.is_debug_mode,\n                                            as_text=self.config.is_debug_mode)\n\n        self.logger.debug(""create model file: %s"", target_path)\n        return target_path\n'"
tests/common.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n"""""" test common utilities.""""""\n\nimport argparse\nimport os\nimport sys\nimport unittest\nfrom collections import defaultdict\n\nfrom distutils.version import LooseVersion\nfrom parameterized import parameterized\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2onnx import constants, logging, utils, tf_utils, tf_loader\n\n# pylint: disable=import-outside-toplevel\n__all__ = [\n    ""TestConfig"",\n    ""get_test_config"",\n    ""unittest_main"",\n    ""check_onnxruntime_backend"",\n    ""check_tf_min_version"",\n    ""check_tf_max_version"",\n    ""skip_tf_versions"",\n    ""skip_tf_cpu"",\n    ""check_onnxruntime_min_version"",\n    ""check_opset_min_version"",\n    ""check_opset_max_version"",\n    ""skip_tf2"",\n    ""check_opset_after_tf_version"",\n    ""check_target"",\n    ""skip_caffe2_backend"",\n    ""skip_onnxruntime_backend"",\n    ""skip_opset"",\n    ""check_onnxruntime_incompatibility"",\n    ""validate_const_node"",\n    ""group_nodes_by_type"",\n    ""test_ms_domain"",\n    ""check_node_domain"",\n    ""check_op_count""\n]\n\n\n# pylint: disable=missing-docstring,unused-argument\n\nclass TestConfig(object):\n    def __init__(self):\n        self.platform = sys.platform\n        self.tf_version = tf_utils.get_tf_version()\n        self.opset = int(os.environ.get(""TF2ONNX_TEST_OPSET"", constants.PREFERRED_OPSET))\n        self.target = os.environ.get(""TF2ONNX_TEST_TARGET"", "","".join(constants.DEFAULT_TARGET)).split(\',\')\n        self.backend = os.environ.get(""TF2ONNX_TEST_BACKEND"", ""onnxruntime"")\n        self.backend_version = self._get_backend_version()\n        self.log_level = logging.WARNING\n        self.temp_dir = utils.get_temp_directory()\n\n    @property\n    def is_mac(self):\n        return self.platform == ""darwin""\n\n    @property\n    def is_onnxruntime_backend(self):\n        return self.backend == ""onnxruntime""\n\n    @property\n    def is_caffe2_backend(self):\n        return self.backend == ""caffe2""\n\n    @property\n    def is_debug_mode(self):\n        return utils.is_debug_mode()\n\n    def _get_backend_version(self):\n        version = None\n        if self.backend == ""onnxruntime"":\n            import onnxruntime as ort\n            version = ort.__version__\n        elif self.backend == ""caffe2"":\n            # TODO: get caffe2 version\n            pass\n\n        if version:\n            version = LooseVersion(version)\n        return version\n\n    def __str__(self):\n        return ""\\n\\t"".join([""TestConfig:"",\n                            ""platform={}"".format(self.platform),\n                            ""tf_version={}"".format(self.tf_version),\n                            ""opset={}"".format(self.opset),\n                            ""target={}"".format(self.target),\n                            ""backend={}"".format(self.backend),\n                            ""backend_version={}"".format(self.backend_version),\n                            ""is_debug_mode={}"".format(self.is_debug_mode),\n                            ""temp_dir={}"".format(self.temp_dir)])\n\n    @staticmethod\n    def load():\n        config = TestConfig()\n        # if not launched by pytest, parse console arguments to override config\n        if ""pytest"" not in sys.argv[0]:\n            parser = argparse.ArgumentParser()\n            parser.add_argument(""--backend"", default=config.backend,\n                                choices=[""caffe2"", ""onnxruntime""],\n                                help=""backend to test against"")\n            parser.add_argument(""--opset"", type=int, default=config.opset, help=""opset to test against"")\n            parser.add_argument(""--target"", default="","".join(config.target), choices=constants.POSSIBLE_TARGETS,\n                                help=""target platform"")\n            parser.add_argument(""--verbose"", ""-v"", help=""verbose output, option is additive"", action=""count"")\n            parser.add_argument(""--debug"", help=""output debugging information"", action=""store_true"")\n            parser.add_argument(""--temp_dir"", help=""temp dir"")\n            parser.add_argument(""unittest_args"", nargs=\'*\')\n\n            args = parser.parse_args()\n            if args.debug:\n                utils.set_debug_mode(True)\n\n            config.backend = args.backend\n            config.opset = args.opset\n            config.target = args.target.split(\',\')\n            config.log_level = logging.get_verbosity_level(args.verbose, config.log_level)\n            if args.temp_dir:\n                config.temp_dir = args.temp_dir\n\n            # Now set the sys.argv to the unittest_args (leaving sys.argv[0] alone)\n            sys.argv[1:] = args.unittest_args\n\n        return config\n\n\n# need to load config BEFORE main is executed when launched from script\n# otherwise, it will be too late for test filters to take effect\n_config = TestConfig.load()\n\n\ndef get_test_config():\n    global _config\n    return _config\n\n\ndef unittest_main():\n    config = get_test_config()\n    logging.basicConfig(level=config.log_level)\n    with logging.set_scope_level(logging.INFO) as logger:\n        logger.info(config)\n    unittest.main()\n\n\ndef _append_message(reason, message):\n    if message:\n        reason = reason + "": "" + message\n    return reason\n\n\ndef check_opset_after_tf_version(tf_version, required_opset, message=""""):\n    """""" Skip if tf_version > max_required_version """"""\n    config = get_test_config()\n    reason = _append_message(""conversion requires opset {} after tf {}"".format(required_opset, tf_version), message)\n    skip = config.tf_version >= LooseVersion(tf_version) and config.opset < required_opset\n    return unittest.skipIf(skip, reason)\n\n\ndef skip_tf2(message=""""):\n    """""" Skip if tf_version > max_required_version """"""\n    reason = _append_message(""test needs to be fixed for tf-2.x"", message)\n    return unittest.skipIf(tf_loader.is_tf2(), reason)\n\n\ndef check_tf_max_version(max_accepted_version, message=""""):\n    """""" Skip if tf_version > max_required_version """"""\n    config = get_test_config()\n    reason = _append_message(""conversion requires tf <= {}"".format(max_accepted_version), message)\n    return unittest.skipIf(config.tf_version > LooseVersion(max_accepted_version), reason)\n\n\ndef check_tf_min_version(min_required_version, message=""""):\n    """""" Skip if tf_version < min_required_version """"""\n    config = get_test_config()\n    reason = _append_message(""conversion requires tf >= {}"".format(min_required_version), message)\n    return unittest.skipIf(config.tf_version < LooseVersion(min_required_version), reason)\n\n\ndef skip_tf_versions(excluded_versions, message=""""):\n    """""" Skip if tf_version SEMANTICALLY matches any of excluded_versions. """"""\n    config = get_test_config()\n    condition = False\n    reason = _append_message(""conversion excludes tf {}"".format(excluded_versions), message)\n\n    current_tokens = str(config.tf_version).split(\'.\')\n    for excluded_version in excluded_versions:\n        exclude_tokens = excluded_version.split(\'.\')\n        # assume len(exclude_tokens) <= len(current_tokens)\n        for i, exclude in enumerate(exclude_tokens):\n            if not current_tokens[i] == exclude:\n                break\n        condition = True\n\n    return unittest.skipIf(condition, reason)\n\n\ndef is_tf_gpu():\n    return tf.test.is_gpu_available()\n\n\ndef skip_tf_cpu(message=""""):\n    is_tf_cpu = not is_tf_gpu()\n    return unittest.skipIf(is_tf_cpu, message)\n\n\ndef check_opset_min_version(min_required_version, message=""""):\n    """""" Skip if opset < min_required_version """"""\n    config = get_test_config()\n    reason = _append_message(""conversion requires opset >= {}"".format(min_required_version), message)\n    return unittest.skipIf(config.opset < min_required_version, reason)\n\n\ndef check_opset_max_version(max_accepted_version, message=""""):\n    """""" Skip if opset > max_accepted_version """"""\n    config = get_test_config()\n    reason = _append_message(""conversion requires opset <= {}"".format(max_accepted_version), message)\n    return unittest.skipIf(config.opset > max_accepted_version, reason)\n\n\ndef skip_opset(opset_v, message=""""):\n    """""" Skip if opset = opset_v """"""\n    config = get_test_config()\n    reason = _append_message(""conversion requires opset != {}"".format(opset_v), message)\n    return unittest.skipIf(config.opset == opset_v, reason)\n\n\ndef check_target(required_target, message=""""):\n    """""" Skip if required_target is NOT specified """"""\n    config = get_test_config()\n    reason = _append_message(""conversion requires target {} specified"".format(required_target), message)\n    return unittest.skipIf(required_target not in config.target, reason)\n\n\ndef skip_onnxruntime_backend(message=""""):\n    """""" Skip if backend is onnxruntime """"""\n    config = get_test_config()\n    reason = _append_message(""not supported by onnxruntime"", message)\n    return unittest.skipIf(config.is_onnxruntime_backend, reason)\n\n\ndef check_onnxruntime_backend(message=""""):\n    """""" Skip if backend is NOT onnxruntime """"""\n    config = get_test_config()\n    reason = _append_message(""only supported by onnxruntime"", message)\n    return unittest.skipIf(not config.is_onnxruntime_backend, reason)\n\n\ndef check_onnxruntime_min_version(min_required_version, message=""""):\n    """""" Skip if onnxruntime version < min_required_version """"""\n    config = get_test_config()\n    reason = _append_message(""conversion requires onnxruntime >= {}"".format(min_required_version), message)\n    return unittest.skipIf(config.is_onnxruntime_backend and\n                           config.backend_version < LooseVersion(min_required_version), reason)\n\n\ndef skip_caffe2_backend(message=""""):\n    """""" Skip if backend is caffe2 """"""\n    config = get_test_config()\n    reason = _append_message(""not supported by caffe2"", message)\n    return unittest.skipIf(config.is_caffe2_backend, reason)\n\n\ndef check_onnxruntime_incompatibility(op):\n    """""" Skip if backend is onnxruntime AND op is NOT supported in current opset """"""\n    config = get_test_config()\n\n    if not config.is_onnxruntime_backend:\n        return unittest.skipIf(False, None)\n\n    support_since = {\n        ""Abs"": 6,  # Abs-1\n        ""Add"": 7,  # Add-1, Add-6\n        ""AveragePool"": 7,  # AveragePool-1\n        ""Div"": 7,  # Div-1, Div-6\n        ""Elu"": 6,  # Elu-1\n        ""Equal"": 7,  # Equal-1\n        ""Exp"": 6,  # Exp-1\n        ""Greater"": 7,  # Greater-1\n        ""Less"": 7,  # Less-1\n        ""Log"": 6,  # Log-1\n        ""Max"": 6,  # Max-1\n        ""Min"": 6,  # Min-1\n        ""Mul"": 7,  # Mul-1, Mul-6\n        ""Neg"": 6,  # Neg-1\n        ""Pow"": 7,  # Pow-1\n        ""Reciprocal"": 6,  # Reciprocal-1\n        ""Relu"": 6,  # Relu-1\n        ""Sqrt"": 6,  # Sqrt-1\n        ""Sub"": 7,  # Sub-1, Sub-6\n        ""Tanh"": 6,  # Tanh-1\n    }\n\n    if op not in support_since or config.opset >= support_since[op]:\n        return unittest.skipIf(False, None)\n\n    reason = ""{} is not supported by onnxruntime before opset {}"".format(op, support_since[op])\n    return unittest.skipIf(True, reason)\n\n\ndef validate_const_node(node, expected_val):\n    if node.is_const():\n        node_val = node.get_tensor_value()\n        np.testing.assert_allclose(expected_val, node_val)\n        return True\n    return False\n\n\ndef group_nodes_by_type(graph):\n    res = defaultdict(list)\n    for node in graph.get_nodes():\n        attr_body_graphs = node.get_body_graphs()\n        if attr_body_graphs:\n            for _, body_graph in attr_body_graphs.items():\n                body_graph_res = group_nodes_by_type(body_graph)\n                for k, v in body_graph_res.items():\n                    res[k].extend(v)\n        res[node.type].append(node)\n    return res\n\n\ndef check_op_count(graph, op_type, expected_count):\n    # return len(group_nodes_by_type(graph)[op_type]) == expected_count\n    # FIXME: after switching to grappler some of the op counts are off. Fix later.\n    return True\n\n\ndef check_lstm_count(graph, expected_count):\n    return len(group_nodes_by_type(graph)[""LSTM""]) == expected_count\n\n\ndef check_gru_count(graph, expected_count):\n    return check_op_count(graph, ""GRU"", expected_count)\n\n\n_MAX_MS_OPSET_VERSION = 1\n\n\ndef test_ms_domain(versions=None):\n    """""" Parameterize test case to apply ms opset(s) as extra_opset. """"""\n\n    @check_onnxruntime_backend()\n    def _custom_name_func(testcase_func, param_num, param):\n        del param_num\n        arg = param.args[0]\n        return ""%s_%s"" % (testcase_func.__name__, arg.version)\n\n    # Test all opset versions in ms domain if versions is not specified\n    if versions is None:\n        versions = list(range(1, _MAX_MS_OPSET_VERSION + 1))\n\n    opsets = []\n    for version in versions:\n        opsets.append([utils.make_opsetid(constants.MICROSOFT_DOMAIN, version)])\n    return parameterized.expand(opsets, testcase_func_name=_custom_name_func)\n\n\ndef check_node_domain(node, domain):\n    # None or empty string means onnx domain\n    if not domain:\n        return not node.domain\n    return node.domain == domain\n'"
tests/conftest.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n"""""" print pytest config.""""""\n\nfrom common import get_test_config\nfrom tf2onnx import logging\n\n\ndef pytest_configure():\n    config = get_test_config()\n    logging.basicConfig(level=config.log_level)\n    with logging.set_scope_level(logging.INFO) as logger:\n        logger.info(config)\n'"
tests/run_pretrained_models.py,3,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Tool to convert and test pre-trained tensorflow models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n# pylint: disable=broad-except,logging-not-lazy,unused-argument,unnecessary-lambda,import-outside-toplevel\n# pylint: disable=wrong-import-position\n\nimport argparse\nimport os\nimport re\nimport sys\nimport tarfile\nimport tempfile\nimport time\nimport zipfile\nfrom collections import namedtuple\nfrom distutils.version import LooseVersion\n\nimport yaml\nimport numpy as np\nimport PIL.Image\nimport six\n\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\nimport tensorflow as tf\n\n# contrib ops are registered only when the module is imported, the following import statement is needed,\n# otherwise tf runtime error will show up when the tf model is restored from pb file because of un-registered ops.\ntry:\n    import tensorflow.contrib.rnn  # pylint: disable=unused-import\nexcept:  # pylint: disable=bare-except\n    # not needed for tf-2.0\n    pass\n\nfrom tf2onnx import tf_loader, logging, optimizer, utils, tf_utils\nfrom tf2onnx.tfonnx import process_tf_graph\nfrom tf2onnx.tf_loader import tf_session, tf_reset_default_graph\n\nlogger = logging.getLogger(""run_pretrained"")\n\nTEMP_DIR = os.path.join(utils.get_temp_directory(), ""run_pretrained"")\nPERFITER = 1000\n\n\ndef get_beach(shape):\n    """"""Get beach image as input.""""""\n    resize_to = shape[1:3]\n    path = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""beach.jpg"")\n    img = PIL.Image.open(path)\n    img = img.resize(resize_to, PIL.Image.ANTIALIAS)\n    img_np = np.array(img).astype(np.float32)\n    img_np = np.stack([img_np] * shape[0], axis=0).reshape(shape)\n    return img_np\n\n\ndef get_random(shape):\n    """"""Get random input.""""""\n    return np.random.sample(shape).astype(np.float32)\n\n\ndef get_random256(shape):\n    """"""Get random imput between 0 and 255.""""""\n    return np.round(np.random.sample(shape) * 256).astype(np.float32)\n\n\ndef get_ramp(shape):\n    """"""Get ramp input.""""""\n    size = np.prod(shape)\n    return np.linspace(1, size, size).reshape(shape).astype(np.float32)\n\n\ndef get_ones(shape):\n    """"""Get ones.""""""\n    return np.ones(shape).astype(np.float32)\n\n\n_INPUT_FUNC_MAPPING = {\n    ""get_beach"": get_beach,\n    ""get_random"": get_random,\n    ""get_random256"": get_random256,\n    ""get_ramp"": get_ramp,\n    ""get_ones"": get_ones\n}\n\nOpsetConstraint = namedtuple(""OpsetConstraint"", ""domain, min_version, max_version, excluded_version"")\n\n\nclass Test(object):\n    """"""Main Test class.""""""\n\n    cache_dir = None\n    target = []\n\n    def __init__(self, url, local, make_input, input_names, output_names,\n                 disabled=False, rtol=0.01, atol=1e-6,\n                 check_only_shape=False, model_type=""frozen"", force_input_shape=False,\n                 skip_tensorflow=False, opset_constraints=None, tf_min_version=None):\n        self.url = url\n        self.make_input = make_input\n        self.local = local\n        self.input_names = input_names\n        self.output_names = output_names\n        self.disabled = disabled\n        self.rtol = rtol\n        self.atol = atol\n        self.check_only_shape = check_only_shape\n        self.perf = None\n        self.tf_runtime = 0\n        self.onnx_runtime = 0\n        self.model_type = model_type\n        self.force_input_shape = force_input_shape\n        self.skip_tensorflow = skip_tensorflow\n        self.opset_constraints = opset_constraints\n        self.tf_min_version = tf_min_version\n\n    def download_model(self):\n        """"""Download model from url.""""""\n        cache_dir = Test.cache_dir\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        url = self.url\n        if url.startswith(r\'module://\'):\n            return self.download_from_module()\n        k = url.rfind(\'/\')\n        fname = self.url[k + 1:]\n        dir_name = fname + ""_dir""\n        ftype = None\n        if url.endswith("".tar.gz"") or url.endswith("".tgz""):\n            ftype = \'tgz\'\n            dir_name = fname.replace("".tar.gz"", """").replace("".tgz"", """")\n        elif url.endswith(\'.zip\'):\n            ftype = \'zip\'\n            dir_name = fname.replace("".zip"", """")\n        dir_name = os.path.join(cache_dir, dir_name)\n        os.makedirs(dir_name, exist_ok=True)\n        fpath = os.path.join(dir_name, fname)\n        if not os.path.exists(fpath):\n            utils.get_url(url, fpath)\n        model_path = os.path.join(dir_name, self.local)\n        if not os.path.exists(model_path):\n            if ftype == \'tgz\':\n                tar = tarfile.open(fpath)\n                tar.extractall(dir_name)\n                tar.close()\n            elif ftype == \'zip\':\n                zip_ref = zipfile.ZipFile(fpath, \'r\')\n                zip_ref.extractall(dir_name)\n                zip_ref.close()\n        return fpath, dir_name\n\n    def download_from_module(self):\n        """"""Download a model from a python module""""""\n        cache_dir = Test.cache_dir\n        from importlib import import_module\n        i = self.url.rfind(\'//\')\n        module, model_name = self.url[i + 2:].split(\'/\')\n        mod_object = import_module(module)\n        model_class = getattr(mod_object, model_name)\n        model = model_class()\n        fpath = os.path.join(cache_dir, self.local)\n        model.save(fpath)\n        return fpath, cache_dir\n\n    def run_tensorflow(self, sess, inputs):\n        """"""Run model on tensorflow so we have a reference output.""""""\n        feed_dict = {}\n        for k, v in inputs.items():\n            k = sess.graph.get_tensor_by_name(k)\n            feed_dict[k] = v\n        result = sess.run(self.output_names, feed_dict=feed_dict)\n        if self.perf:\n            start = time.time()\n            for _ in range(PERFITER):\n                _ = sess.run(self.output_names, feed_dict=feed_dict)\n            self.tf_runtime = time.time() - start\n        return result\n\n    def to_onnx(self, tf_graph, opset=None, extra_opset=None, shape_override=None, input_names=None):\n        """"""Convert graph to tensorflow.""""""\n        return process_tf_graph(tf_graph, continue_on_error=False, opset=opset,\n                                extra_opset=extra_opset, target=Test.target, shape_override=shape_override,\n                                input_names=input_names, output_names=self.output_names)\n\n    def run_caffe2(self, name, model_proto, inputs):\n        """"""Run test again caffe2 backend.""""""\n        import caffe2.python.onnx.backend\n        prepared_backend = caffe2.python.onnx.backend.prepare(model_proto)\n        results = prepared_backend.run(inputs)\n        if self.perf:\n            start = time.time()\n            for _ in range(PERFITER):\n                _ = prepared_backend.run(inputs)\n            self.onnx_runtime = time.time() - start\n        return results\n\n    def run_onnxruntime(self, name, model_proto, inputs):\n        """"""Run test against onnxruntime backend.""""""\n        import onnxruntime as rt\n        model_path = utils.save_onnx_model(TEMP_DIR, name, inputs, model_proto, include_test_data=True,\n                                           as_text=utils.is_debug_mode())\n        logger.info(""Model saved to %s"", model_path)\n        m = rt.InferenceSession(model_path)\n        results = m.run(self.output_names, inputs)\n        if self.perf:\n            start = time.time()\n            for _ in range(PERFITER):\n                _ = m.run(self.output_names, inputs)\n            self.onnx_runtime = time.time() - start\n        return results\n\n    @staticmethod\n    def create_onnx_file(name, model_proto, inputs, outdir):\n        os.makedirs(outdir, exist_ok=True)\n        model_path = os.path.join(outdir, name + "".onnx"")\n        utils.save_protobuf(model_path, model_proto)\n        logger.info(""Created %s"", model_path)\n\n    def run_test(self, name, backend=""caffe2"", onnx_file=None, opset=None, extra_opset=None,\n                 perf=None, fold_const=None):\n        """"""Run complete test against backend.""""""\n        self.perf = perf\n\n        # get the model\n        if self.url:\n            _, dir_name = self.download_model()\n            logger.info(""Downloaded to %s"", dir_name)\n            model_path = os.path.join(dir_name, self.local)\n        else:\n            model_path = self.local\n\n        logger.info(""Load model from %s"", model_path)\n        input_names = list(self.input_names.keys())\n        outputs = self.output_names\n        if self.model_type in [""checkpoint""]:\n            graph_def, input_names, outputs = tf_loader.from_checkpoint(model_path, input_names, outputs)\n        elif self.model_type in [""saved_model""]:\n            graph_def, input_names, outputs = tf_loader.from_saved_model(model_path, input_names, outputs)\n        elif self.model_type in [""keras""]:\n            graph_def, input_names, outputs = tf_loader.from_keras(model_path, input_names, outputs)\n        else:\n            graph_def, input_names, outputs = tf_loader.from_graphdef(model_path, input_names, outputs)\n\n        if utils.is_debug_mode():\n            utils.save_protobuf(os.path.join(TEMP_DIR, name + ""_after_tf_optimize.pb""), graph_def)\n\n        inputs = {}\n        shape_override = {}\n        tf_reset_default_graph()\n        g = tf.import_graph_def(graph_def, name=\'\')\n        # with tf_session(config=tf.ConfigProto(allow_soft_placement=True), graph=g) as sess:\n        with tf_session(graph=g) as sess:\n            # create the input data\n            for k in input_names:\n                v = self.input_names[k]\n                t = sess.graph.get_tensor_by_name(k)\n                expected_dtype = tf.as_dtype(t.dtype).name\n                if isinstance(v, six.text_type) and v.startswith(""np.""):\n                    np_value = eval(v)  # pylint: disable=eval-used\n                    if expected_dtype != np_value.dtype:\n                        logger.warning(""dtype mismatch for input %s: expected=%s, actual=%s"", k, expected_dtype,\n                                       np_value.dtype)\n                    inputs[k] = np_value.astype(expected_dtype)\n                else:\n                    inputs[k] = self.make_input(v).astype(expected_dtype)\n\n            if self.force_input_shape:\n                for k, v in inputs.items():\n                    shape_override[k] = list(v.shape)\n\n            # run the model with tensorflow\n            if self.skip_tensorflow:\n                logger.info(""TensorFlow SKIPPED"")\n            else:\n                tf_results = self.run_tensorflow(sess, inputs)\n                logger.info(""TensorFlow OK"")\n\n        model_proto = None\n        try:\n            # convert model to onnx\n            onnx_graph = self.to_onnx(sess.graph, opset=opset, extra_opset=extra_opset,\n                                      shape_override=shape_override, input_names=inputs.keys())\n            onnx_graph = optimizer.optimize_graph(onnx_graph)\n            model_proto = onnx_graph.make_model(""converted from tf2onnx"")\n            logger.info(""To_ONNX, OK"")\n            if onnx_file:\n                self.create_onnx_file(name, model_proto, inputs, onnx_file)\n        except Exception:\n            logger.error(""To_ONNX FAIL"", exc_info=1)\n            return False\n\n        try:\n            onnx_results = None\n            if backend == ""caffe2"":\n                onnx_results = self.run_caffe2(name, model_proto, inputs)\n            elif backend == ""onnxruntime"":\n                onnx_results = self.run_onnxruntime(name, model_proto, inputs)\n            else:\n                raise ValueError(""unknown backend"")\n            logger.info(""Run_ONNX OK"")\n\n            try:\n                if self.skip_tensorflow:\n                    logger.info(""Results: skipped tensorflow"")\n                else:\n                    if self.check_only_shape:\n                        for tf_res, onnx_res in zip(tf_results, onnx_results):\n                            np.testing.assert_array_equal(tf_res.shape, onnx_res.shape)\n                    else:\n                        for tf_res, onnx_res in zip(tf_results, onnx_results):\n                            np.testing.assert_allclose(tf_res, onnx_res, rtol=self.rtol, atol=self.atol)\n                    logger.info(""Results: OK"")\n                return True\n            except Exception:\n                logger.error(""Results"", exc_info=1)\n\n        except Exception:\n            logger.error(""Run_ONNX FAIL"", exc_info=1)\n\n        return False\n\n    def check_opset_constraints(self, opset, extra_opset=None):\n        """""" Return (condition, reason) tuple, condition is True if constraints are met. """"""\n        if not self.opset_constraints:\n            return True, None\n\n        opsets = {""onnx"": opset}\n        if extra_opset:\n            for e in extra_opset:\n                opsets[e.domain] = e.version\n\n        for constraint in self.opset_constraints:\n            domain = constraint.domain\n            opset_version = opsets.get(domain)\n            if not opset_version:\n                return False, ""conversion requires opset {}"".format(domain)\n\n            if constraint.min_version and opset_version < constraint.min_version:\n                reason = ""conversion requires opset {} >= {}"".format(domain, constraint.min_version)\n                return False, reason\n\n            if constraint.max_version and opset_version > constraint.max_version:\n                reason = ""conversion requires opset {} <= {}"".format(domain, constraint.max_version)\n                return False, reason\n\n            if constraint.excluded_version:\n                if utils.is_list_or_tuple(constraint.excluded_version):\n                    skip = opset_version in constraint.excluded_version\n                else:\n                    skip = opset_version == constraint.excluded_version\n                if skip:\n                    reason = ""conversion requires opset {} != {}"".format(domain, constraint.excluded_version)\n                    return False, reason\n\n        return True, None\n\n\ndef get_args():\n    """"""Parse commandline.""""""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--cache"", default=os.path.join(tempfile.gettempdir(), \'pre-trained\'),\n                        help=""pre-trained models cache dir"")\n    parser.add_argument(""--config"", default=""tests/run_pretrained_models.yaml"", help=""yaml config to use"")\n    parser.add_argument(""--tests"", help=""tests to run"")\n    parser.add_argument(""--target"", default="""", help=""target platform"")\n    parser.add_argument(""--backend"", default=""onnxruntime"",\n                        choices=[""caffe2"", ""onnxruntime""], help=""backend to use"")\n    parser.add_argument(""--opset"", type=int, default=None, help=""opset to use"")\n    parser.add_argument(""--extra_opset"", default=None,\n                        help=""extra opset with format like domain:version, e.g. com.microsoft:1"")\n    parser.add_argument(""--verbose"", ""-v"", help=""verbose output, option is additive"", action=""count"")\n    parser.add_argument(""--debug"", help=""debug mode"", action=""store_true"")\n    parser.add_argument(""--list"", help=""list tests"", action=""store_true"")\n    parser.add_argument(""--onnx-file"", help=""create onnx file in directory"")\n    parser.add_argument(""--perf"", help=""capture performance numbers"")\n    parser.add_argument(""--fold_const"", help=""enable tf constant_folding transformation before conversion"",\n                        action=""store_true"")\n    parser.add_argument(""--include-disabled"", help=""include disabled tests"", action=""store_true"")\n    args = parser.parse_args()\n\n    args.target = args.target.split("","")\n    if args.extra_opset:\n        tokens = args.extra_opset.split(\':\')\n        if len(tokens) != 2:\n            raise ValueError(""invalid extra_opset argument"")\n        args.extra_opset = [utils.make_opsetid(tokens[0], int(tokens[1]))]\n    return args\n\n\ndef load_tests_from_yaml(path):\n    """"""Create test class from yaml file.""""""\n    path = os.path.abspath(path)\n    base_dir = os.path.dirname(path)\n\n    tests = {}\n    config = yaml.safe_load(open(path, \'r\').read())\n    for name, settings in config.items():\n        if name in tests:\n            raise ValueError(""Found duplicated test: {}"".format(name))\n\n        # parse model and url, non-absolute local path is relative to yaml directory\n        model = settings.get(""model"")\n        url = settings.get(""url"")\n        if not url and not os.path.isabs(model):\n            model = os.path.join(base_dir, model)\n\n        # parse input_get\n        input_func = settings.get(""input_get"")\n        input_func = _INPUT_FUNC_MAPPING[input_func]\n\n        # parse inputs, non-absolute npy file path for np.load is relative to yaml directory\n        inputs = settings.get(""inputs"")\n        for k, v in list(inputs.items()):\n            if isinstance(v, str):\n                # assume at most 1 match\n                matches = re.findall(r""np\\.load\\((r?[\'\\""].*?[\'\\""])"", v)\n                if matches:\n                    npy_path = matches[0].lstrip(\'r\').strip(""\'"").strip(\'""\')\n                    if not os.path.isabs(npy_path):\n                        abs_npy_path = os.path.join(base_dir, npy_path)\n                        inputs[k] = v.replace(matches[0], ""r\'{}\'"".format(abs_npy_path))\n\n        # parse opset_constraints\n        opset_constraints = []\n        section = settings.get(""opset_constraints"")\n        if section:\n            for k, v in section.items():\n                c = OpsetConstraint(k, min_version=v.get(""min""), max_version=v.get(""max""),\n                                    excluded_version=v.get(""excluded""))\n                opset_constraints.append(c)\n\n        kwargs = {}\n        for kw in [""rtol"", ""atol"", ""disabled"", ""check_only_shape"", ""model_type"",\n                   ""skip_tensorflow"", ""force_input_shape"", ""tf_min_version""]:\n            if settings.get(kw) is not None:\n                kwargs[kw] = settings[kw]\n\n        test = Test(url, model, input_func, inputs, settings.get(""outputs""),\n                    opset_constraints=opset_constraints, **kwargs)\n        tests[name] = test\n    return tests\n\n\ndef main():\n    args = get_args()\n    logging.basicConfig(level=logging.get_verbosity_level(args.verbose))\n    if args.debug:\n        utils.set_debug_mode(True)\n\n    Test.cache_dir = args.cache\n    Test.target = args.target\n    tests = load_tests_from_yaml(args.config)\n    if args.list:\n        logger.info(sorted(tests.keys()))\n        return 0\n    if args.tests:\n        test_keys = args.tests.split("","")\n    else:\n        test_keys = list(tests.keys())\n\n    failed = 0\n    count = 0\n    for test in test_keys:\n        logger.info(""==================================="")\n\n        t = tests[test]\n        if args.tests is None:\n            if t.disabled and not args.include_disabled:\n                logger.info(""Skip %s: disabled"", test)\n                continue\n\n            condition, reason = t.check_opset_constraints(args.opset, args.extra_opset)\n            if not condition:\n                logger.info(""Skip %s: %s"", test, reason)\n                continue\n\n            if t.tf_min_version:\n                if tf_utils.get_tf_version() < LooseVersion(str(t.tf_min_version)):\n                    logger.info(""Skip %s: %s %s"", test, ""Min TF version needed:"", t.tf_min_version)\n                    continue\n\n        count += 1\n        try:\n            logger.info(""Running %s"", test)\n            ret = t.run_test(test, backend=args.backend, onnx_file=args.onnx_file,\n                             opset=args.opset, extra_opset=args.extra_opset, perf=args.perf,\n                             fold_const=args.fold_const)\n        except Exception:\n            logger.error(""Failed to run %s"", test, exc_info=1)\n            ret = None\n        finally:\n            if not utils.is_debug_mode():\n                utils.delete_directory(TEMP_DIR)\n        if not ret:\n            failed += 1\n\n    logger.info(""==================================="")\n    logger.info(""RESULT: %s failed of %s, backend=%s"", failed, count, args.backend)\n\n    if args.perf:\n        with open(args.perf, ""w"") as f:\n            f.write(""test,tensorflow,onnx\\n"")\n            for test in test_keys:\n                t = tests[test]\n                if t.perf:\n                    f.write(""{},{},{}\\n"".format(test, t.tf_runtime, t.onnx_runtime))\n    return failed\n\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
tests/test_backend.py,689,"b'\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit tests using onnx backends.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport unittest\nfrom distutils.version import LooseVersion\nfrom itertools import product\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import lookup_ops\nfrom backend_test_base import Tf2OnnxBackendTestBase\n# pylint reports unused-wildcard-import which is false positive, __all__ is defined in common\nfrom common import *  # pylint: disable=wildcard-import,unused-wildcard-import\nfrom tf2onnx import constants, utils\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\nfrom tf2onnx.tf_loader import is_tf2\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,function-redefined,cell-var-from-loop\n\n\nNCHW_TO_NHWC = [0, 2, 3, 1]\nNHWC_TO_NCHW = [0, 3, 1, 2]\nHWCN_TO_NCHW = [3, 2, 0, 1]\n\n_STRIDE1x1 = [1, 1, 1, 1]\n_KERNEL3x3 = [3, 3, 1, 1]\n\n# names for input and outputs for tests\n_TFINPUT = ""input""\n_INPUT = ""input:0""\n_TFINPUT1 = ""input1""\n_INPUT1 = ""input1:0""\n_TFINPUT2 = ""input2""\n_INPUT2 = ""input2:0""\n_TFINPUT3 = ""input3""\n_INPUT3 = ""input3:0""\n_TFOUTPUT = ""output""\n_OUTPUT = ""output:0""\n_TFOUTPUT1 = ""output1""\n_OUTPUT1 = ""output1:0""\n_TFOUTPUT2 = ""output2""\n_OUTPUT2 = ""output2:0""\n\n\nif is_tf2():\n    conv2d_backprop_input = tf.compat.v1.nn.conv2d_backprop_input\n    multinomial = tf.compat.v1.random.multinomial\n    space_to_batch_nd = tf.compat.v1.space_to_batch_nd\n    batch_to_space_nd = tf.compat.v1.batch_to_space_nd\n    reverse_v2 = tf.compat.v1.reverse_v2\n    random_normal = tf.compat.v1.random_normal\n    random_uniform = tf.compat.v1.random_uniform\n    fused_batch_norm = tf.compat.v1.nn.fused_batch_norm\n    dropout = tf.compat.v1.nn.dropout\n    resize_nearest_neighbor = tf.compat.v1.image.resize_nearest_neighbor\n    quantize_and_dequantize = tf.quantization.quantize_and_dequantize\n    resize_bilinear = tf.compat.v1.image.resize_bilinear\n    resize_bilinear_v2 = tf.compat.v2.image.resize\n    is_nan = tf.math.is_nan\n    is_inf = tf.math.is_inf\n    floormod = tf.math.floormod\n    matrix_diag_part = tf.compat.v1.matrix_diag_part\nelif LooseVersion(tf.__version__) >= ""1.13"":\n    conv2d_backprop_input = tf.compat.v1.nn.conv2d_backprop_input\n    multinomial = tf.compat.v1.random.multinomial\n    space_to_batch_nd = tf.compat.v1.space_to_batch_nd\n    batch_to_space_nd = tf.compat.v1.batch_to_space_nd\n    reverse_v2 = tf.compat.v1.reverse_v2\n    random_normal = tf.compat.v1.random_normal\n    random_uniform = tf.compat.v1.random_uniform\n    fused_batch_norm = tf.compat.v1.nn.fused_batch_norm\n    dropout = tf.compat.v1.nn.dropout\n    quantize_and_dequantize = tf.compat.v1.quantization.quantize_and_dequantize\n    resize_nearest_neighbor = tf.compat.v1.image.resize_nearest_neighbor\n    resize_bilinear = tf.compat.v1.image.resize_bilinear\n    if LooseVersion(tf.__version__) >= ""1.14"":\n        resize_bilinear_v2 = tf.compat.v2.image.resize\n    is_nan = tf.math.is_nan\n    is_inf = tf.math.is_inf\n    floormod = tf.floormod\n    matrix_diag_part = tf.compat.v1.matrix_diag_part\nelse:\n    conv2d_backprop_input = tf.nn.conv2d_backprop_input\n    multinomial = tf.multinomial\n    space_to_batch_nd = tf.space_to_batch_nd\n    batch_to_space_nd = tf.batch_to_space_nd\n    reverse_v2 = tf.reverse_v2\n    random_normal = tf.random_normal\n    random_uniform = tf.random_uniform\n    fused_batch_norm = tf.nn.fused_batch_norm\n    dropout = tf.nn.dropout\n    resize_nearest_neighbor = tf.image.resize_nearest_neighbor\n    resize_bilinear = tf.image.resize_bilinear\n    is_nan = tf.is_nan\n    is_inf = tf.is_inf\n    floormod = tf.floormod\n    matrix_diag_part = tf.matrix_diag_part\n\n\ndef make_xval(shape):\n    x_val = np.arange(np.prod(shape)).astype(""float32"").reshape(shape)\n    return x_val\n\n\ndef get_conv_getdata(kind=1):\n    if kind == 0:\n        # generate all combinations (costly)\n        dims = [\n            (""padding"", [""SAME"", ""VALID""]),\n            (""input_sizes"", [[32, 35, 35, 3], [32, 17, 17, 3], [1, 28, 28, 3], [32, 8, 8, 3]]),\n            (""filter_sizes"", [[1, 3, 3, 1], [1, 2, 2, 1], [1, 5, 5, 1], [1, 1, 1, 1], [1, 5, 2, 1], [1, 2, 5, 1]]),\n            (""strides"", [[1, 2, 2, 1], [1, 1, 1, 1]]),\n        ]\n        values = [key_values[1] for key_values in dims]\n        for idx, v in enumerate(product(*values)):\n            if True or idx == 30:\n                yield (idx,) + v\n    elif kind == 1:\n        # some combination to that give decent padding coverage\n        data = [\n            (\'SAME\', [32, 35, 35, 3], [1, 3, 3, 1], [1, 2, 2, 1]),\n            (\'SAME\', [32, 35, 35, 3], [1, 2, 2, 1], [1, 2, 2, 1]),\n            (\'SAME\', [32, 35, 35, 3], [1, 1, 1, 1], [1, 1, 1, 1]),\n            (\'SAME\', [32, 35, 35, 3], [1, 5, 2, 1], [1, 2, 2, 1]),\n            (\'SAME\', [32, 35, 35, 3], [1, 2, 5, 1], [1, 2, 2, 1]),\n            (\'SAME\', [32, 35, 35, 3], [1, 2, 5, 1], [1, 1, 1, 1]),\n            (\'SAME\', [1, 28, 28, 3], [1, 3, 3, 1], [1, 2, 2, 1]),\n            (\'SAME\', [1, 28, 28, 3], [1, 3, 3, 1], [1, 1, 1, 1]),\n            (\'SAME\', [1, 28, 28, 3], [1, 2, 2, 1], [1, 2, 2, 1]),\n            (\'SAME\', [1, 28, 28, 3], [1, 2, 2, 1], [1, 1, 1, 1]),\n            (\'SAME\', [1, 28, 28, 3], [1, 5, 5, 1], [1, 2, 2, 1]),\n            (\'SAME\', [1, 28, 28, 3], [1, 5, 5, 1], [1, 1, 1, 1]),\n            (\'SAME\', [1, 28, 28, 3], [1, 5, 2, 1], [1, 2, 2, 1]),\n            (\'SAME\', [32, 8, 8, 3], [1, 3, 3, 1], [1, 2, 2, 1]),\n            (\'SAME\', [32, 8, 8, 3], [1, 3, 3, 1], [1, 1, 1, 1]),\n            (\'VALID\', [32, 35, 35, 3], [1, 3, 3, 1], [1, 1, 1, 1]),\n            (\'VALID\', [32, 35, 35, 3], [1, 2, 2, 1], [1, 2, 2, 1]),\n        ]\n        for idx, v in enumerate(data):\n            yield (idx,) + v\n    else:\n        raise ValueError(""kind not known"")\n\n\ndef get_maxpoolwithargmax_getdata():\n    data = [\n        (\'SAME\', [1, 3, 3, 1], [1, 3, 3, 1], [1, 2, 2, 1]),\n        (\'SAME\', [1, 5, 5, 1], [1, 4, 4, 1], [1, 2, 2, 1]),\n        (\'SAME\', [1, 10, 5, 1], [1, 2, 2, 1], [1, 2, 2, 1]),\n        (\'SAME\', [1, 10, 5, 1], [1, 4, 4, 1], [1, 1, 1, 1]),\n        (\'VALID\', [1, 3, 3, 1], [1, 3, 3, 1], [1, 2, 2, 1]),\n        (\'VALID\', [1, 5, 5, 1], [1, 4, 4, 1], [1, 2, 2, 1]),\n    ]\n    for idx, v in enumerate(data):\n        yield (idx,) + v\n\n\nclass BackendTests(Tf2OnnxBackendTestBase):\n    def _run_test_case(self, func, output_names_with_port, feed_dict, **kwargs):\n        kwargs[""convert_var_to_const""] = False\n        kwargs[""constant_fold""] = False\n        return self.run_test_case(func, feed_dict, [], output_names_with_port, **kwargs)\n\n    def _test_expand_dims_known_rank(self, idx):\n        x_val = make_xval([3, 4])\n        def func(x):\n            op = tf.expand_dims(x, idx)\n            return tf.identity(op, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_expand_dims_known_rank(self):\n        for i in [-1, 0, 1, -2]:\n            self._test_expand_dims_known_rank(i)\n\n    def test_expand_dims_one_unknown_rank(self):\n        x_val = make_xval([3, 4])\n        def func(x):\n            op = tf.expand_dims(x, 0)\n            return tf.identity(op, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_expand_dims_with_list(self):\n        x_val = make_xval([3, 4])\n        def func(x):\n            op = tf.expand_dims(x, [0])\n            return tf.identity(op, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def _test_expand_dims_more_unknown_rank(self, idx):\n        x_val = make_xval([3, 4])\n        def func(x):\n            op = tf.expand_dims(x, idx)\n            return tf.identity(op, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_expand_dims_more_unknown_rank(self):\n        for i in [-1, 0, 1, -2]:\n            self._test_expand_dims_more_unknown_rank(i)\n\n    @check_opset_min_version(9, ""ConstantOfShape"")\n    def test_eye_non_const1(self):\n        # tf.eye(num_rows), num_rows is not const here\n        x_val = np.array(5, dtype=np.int32)\n        def func(x):\n            y = tf.eye(x, dtype=tf.int32)\n            y1 = tf.eye(x, dtype=tf.int64)\n            y2 = tf.eye(x, dtype=tf.float32)\n            return tf.identity(y, name=_TFOUTPUT), tf.identity(y1, name=_TFOUTPUT1), tf.identity(y2, name=_TFOUTPUT2)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1, _OUTPUT2], {_INPUT: x_val}, rtol=0)\n\n        # tf.eye(num_rows, num_columns), both num_rows and num_columns are not const here\n        x_val = np.array([5, 10], dtype=np.int32)\n        def func(x):\n            y = tf.eye(x[0], x[1], dtype=tf.int32)\n            y1 = tf.eye(x[0], x[1], dtype=tf.int64)\n            y2 = tf.eye(x[0], x[1], dtype=tf.float32)\n            return tf.identity(y, name=_TFOUTPUT), tf.identity(y1, name=_TFOUTPUT1), tf.identity(y2, name=_TFOUTPUT2)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1, _OUTPUT2], {_INPUT: x_val}, rtol=0)\n\n    @check_tf_min_version(""1.11"", ""eye has bug when version is below 1.11"")\n    @check_opset_min_version(9, ""ConstantOfShape"")\n    def test_eye_non_const2(self):\n        # tf.eye(num_rows), num_rows is not const here\n        for np_dtype in [np.int32, np.int64, np.float32, np.float64]:\n            x_val = np.array(5, dtype=np_dtype)\n            def func(x):\n                y = tf.eye(x, dtype=tf.int32)\n                y1 = tf.eye(x, dtype=tf.float32)\n                return tf.identity(y, name=_TFOUTPUT),\\\n                       tf.identity(y1, name=_TFOUTPUT1)\n            self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: x_val}, rtol=0)\n\n        # tf.eye(num_rows, num_columns), both num_rows and num_columns are not const here\n        for np_dtype in [np.int32, np.int64, np.float32, np.float64]:\n            x_val = np.array([5, 10], dtype=np_dtype)\n            def func(x):\n                y = tf.eye(x[0], x[1], dtype=tf.int32)\n                y1 = tf.eye(x[0], x[1], dtype=tf.float32)\n                return tf.identity(y, name=_TFOUTPUT), \\\n                       tf.identity(y1, name=_TFOUTPUT1)\n            self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: x_val}, rtol=0)\n\n    @check_opset_min_version(7, ""trig"")\n    def test_trig_ops(self):\n        for op in [tf.sin, tf.cos, tf.tan, tf.asin, tf.acos, tf.atan]:\n            x_val = make_xval([3, 4])\n            def func(x):\n                op_ = op(x)\n                return tf.identity(op_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-06)\n\n    @check_opset_min_version(9, ""trigh"")\n    def test_atrig_ops(self):\n        for op in [tf.sinh, tf.cosh, tf.atanh, tf.asinh, tf.acosh]:\n            x_val = make_xval([3, 4])\n            def func(x):\n                op_ = op(x)\n                return tf.identity(op_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend()\n    @check_opset_min_version(7, ""multinomial"")\n    def test_multinomial(self):\n        x_val = np.array([[10., 10.]], dtype=np.float32)\n        def func(x):\n            op = multinomial(tf.math.log(x), 5, output_dtype=tf.int64)\n            return tf.identity(op, name=_TFOUTPUT)\n\n        # since returned indexes are random we can only check type and shape\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, check_value=False,\n                            check_shape=True, check_dtype=True)\n\n    @skip_caffe2_backend()\n    @check_opset_min_version(7, ""multinomial"")\n    def test_multinomial1(self):\n        shape = [2, 10]\n        x_val = np.ones(np.prod(shape)).astype(""float32"").reshape(shape)\n        def func(x):\n            op = multinomial(x, 2, output_dtype=tf.int64)\n            return tf.identity(op, name=_TFOUTPUT)\n        # since returned indexes are random we can only check type and shape\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, check_value=False,\n                            check_shape=True, check_dtype=True)\n\n    def test_maxpool(self):\n        for p in get_conv_getdata():\n            _, padding, x_shape, ksize, strides = p\n            x_val = make_xval(x_shape)\n            def func(x):\n                mp = tf.nn.max_pool(x, ksize, strides, padding=padding)\n                return tf.identity(mp, name=_TFOUTPUT)\n            self.logger.debug(str(p))\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_tf_cpu(""only tf_gpu can run maxpool with NCHW format"")\n    def test_maxpool_gpu(self):\n        # make sure converter behaves well when data format is NCHW\n        # and when data format is NCHW, only gpu version of tensorflow can run it.\n        ksize = [1, 1, 2, 2]\n        strides = [1, 1, 2, 2]\n        x_val = make_xval([1, 3, 50, 80])\n        for padding in [""SAME"", ""VALID""]:\n            def func(x):\n                mp = tf.nn.max_pool(x, ksize, strides, padding=padding, data_format=""NCHW"")\n                return tf.identity(mp, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""AveragePool"")\n    def test_avgpool(self):\n        for p in get_conv_getdata(kind=0):\n            _, padding, x_shape, ksize, strides = p\n            x_val = make_xval(x_shape)\n            def func(x):\n                mp = tf.nn.avg_pool(x, ksize, strides, padding=padding)\n                return tf.identity(mp, name=_TFOUTPUT)\n\n            self.logger.debug(str(p))\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-06)\n\n    @check_onnxruntime_incompatibility(""AveragePool"")\n    @skip_tf_cpu(""only tf_gpu can run avgpool with NCHW format"")\n    def test_avgpool_gpu(self):\n        ksize = [1, 1, 2, 2]\n        strides = [1, 1, 2, 2]\n        x_val = make_xval([1, 3, 50, 80])\n        for padding in [""SAME"", ""VALID""]:\n            def func(x):\n                mp = tf.nn.avg_pool(x, ksize, strides, padding=padding, data_format=""NCHW"")\n                return tf.identity(mp, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def _conv_test(self, x_val, w, strides=None, padding=""VALID"", dilations=None, rtol=1e-07):\n        if strides is None:\n            strides = _STRIDE1x1\n        if dilations is None:\n            dilations = [1, 1, 1, 1]\n        def func(x):\n            kernel = tf.constant(w, dtype=tf.float32, name=\'k\')\n            conv = tf.nn.conv2d(x, kernel, strides=strides, padding=padding, dilations=dilations)\n            return tf.identity(conv, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=rtol)\n\n    def test_conv2d_1(self):\n        x_val = make_xval((1, 1, 5, 5)).transpose(NCHW_TO_NHWC)\n        w = np.array([[2., 1., 1.],\n                      [1., 3., 1.],\n                      [1., 1., 4.]], dtype=np.float32).reshape(_KERNEL3x3)\n        self._conv_test(x_val, w)\n\n    def test_conv2d_2(self):\n        x_val = np.array([[4, 3, 1, 0],\n                          [2, 1, 0, 1],\n                          [1, 2, 4, 1],\n                          [3, 1, 0, 2]], dtype=np.float32).reshape([1, 4, 4, 1])\n        w = np.array([[1, 0, 1],\n                      [2, 1, 0],\n                      [0, 0, 1]], dtype=np.float32).reshape(_KERNEL3x3)\n        self._conv_test(x_val, w)\n\n    def test_conv2d_3(self):\n        x_val = make_xval((1, 1, 5, 5)).transpose(NCHW_TO_NHWC)\n        w = np.array([[2., 1., 1.],\n                      [1., 3., 1.],\n                      [1., 1., 4.]], dtype=np.float32).reshape(_KERNEL3x3)\n        self._conv_test(x_val, w)\n\n    def test_conv2d_4(self):\n        x_val = make_xval((1, 1, 5, 5)).transpose(NCHW_TO_NHWC)\n        w = np.random.random_sample(_KERNEL3x3).astype(np.float32)\n        self._conv_test(x_val, w, padding=""SAME"", rtol=1e-05)\n\n    def test_conv2d_5(self):\n        x_val = make_xval((1, 1, 5, 5)).transpose(NCHW_TO_NHWC)\n        kernel_shape = [3, 3, 1, 2]\n        w = np.random.random_sample(kernel_shape).astype(np.float32)\n        self._conv_test(x_val, w, padding=""SAME"", rtol=1e-05)\n\n    def test_conv2d_6(self):\n        x_shape = [1, 35, 35, 288]  # out: [1, 17, 17, 384]\n        kernel_shape = [3, 3, 288, 384]\n        strides = [1, 2, 2, 1]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        kernel_val = np.arange(1, 1 + np.prod(kernel_shape)).astype(""float32"").reshape(kernel_shape)\n        self._conv_test(x_val, kernel_val, strides=strides, padding=""VALID"", rtol=1e-05)\n\n    @check_tf_min_version(""1.7"", ""tf only support dilation is 1 for now"")\n    def test_conv2d_7(self):\n        x_shape = [1, 35, 35, 288]  # out: [1, 17, 17, 384]\n        kernel_shape = [3, 3, 288, 384]\n        strides = [1, 2, 2, 1]\n        dilations = [1, 3, 3, 1]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        kernel_val = np.arange(1, 1 + np.prod(kernel_shape)).astype(""float32"").reshape(kernel_shape)\n        self._conv_test(x_val, kernel_val, strides=strides, padding=""VALID"",\n                        dilations=dilations, rtol=1e-05)\n\n    def test_conv2d_8(self):\n        for input_shape in [[10, 10], [5, 5]]:\n            x_val = make_xval((1, 1, *input_shape)).transpose(NCHW_TO_NHWC)\n            w = np.random.random_sample([3, 3, 1, 2]).astype(np.float32)\n            strides = [1, 2, 2, 1]\n            def func(x):\n                kernel = tf.constant(w, dtype=tf.float32, name=\'k\')\n                conv = tf.nn.conv2d(x, kernel, strides=strides, padding=""SAME"")\n                return tf.identity(conv, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-5)\n\n    def test_conv2d_with_pad_valid(self):\n        x_val = make_xval((1, 1, 5, 5)).transpose(NCHW_TO_NHWC)\n        w = np.random.random_sample([3, 3, 1, 2]).astype(np.float32)\n        strides = [1, 1, 1, 1]\n        def func(x):\n            kernel = tf.constant(w, dtype=tf.float32, name=\'k\')\n            x_pad = tf.pad(x, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]])\n            conv = tf.nn.conv2d(x_pad, kernel, strides=strides, padding=""VALID"")\n            return tf.identity(conv, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-5)\n\n    def test_conv2d_with_pad_same(self):\n        x_val = make_xval((1, 1, 5, 5)).transpose(NCHW_TO_NHWC)\n        w = np.random.random_sample([3, 3, 1, 2]).astype(np.float32)\n        strides = [1, 1, 1, 1]\n        def func(x):\n            kernel = tf.constant(w, dtype=tf.float32, name=\'k\')\n            x_pad = tf.pad(x, paddings=[[0, 0], [2, 2], [2, 2], [0, 0]])\n            conv = tf.nn.conv2d(x_pad, kernel, strides=strides, padding=""SAME"")\n            return tf.identity(conv, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-5)\n\n    def test_conv2d_transpose(self):\n        x_shape = [2, 6, 4, 3]\n        output_shape = [2, 13, 9, 2]\n        kernel_shape = [3, 3, 2, 3]\n        strides = [1, 2, 2, 1]\n        x_val = make_xval(x_shape)\n        kernel_val = make_xval(kernel_shape)\n        def func(x):\n            f = tf.constant(kernel_val, name=""kernel"", dtype=tf.float32)\n            conv = tf.nn.conv2d_transpose(x, f, output_shape, strides=strides, padding=""VALID"")\n            return tf.identity(conv, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05)\n\n    @check_onnxruntime_min_version(""0.5.0"", ""conv transpose is added since onnxruntime-0.5.0"")\n    def test_conv2d_transpose2(self):\n        # output_shape is dynamic\n        extra_opset = [utils.make_opsetid(constants.MICROSOFT_DOMAIN, 1)]\n        process_args = {""extra_opset"": extra_opset}\n        x_shape = [2, 6, 4, 3]\n        output_shape = np.array([2, 13, 9, 2]).astype(np.int32)\n        kernel_shape = [3, 3, 2, 3]\n        strides = [1, 2, 2, 1]\n        x_val = make_xval(x_shape)\n        kernel_val = make_xval(kernel_shape)\n        def func(x, output_shape_placeholder):\n            f = tf.constant(kernel_val, name=""kernel"", dtype=tf.float32)\n            conv = tf.nn.conv2d_transpose(x, f, output_shape_placeholder, strides=strides, padding=""VALID"")\n            return tf.identity(conv, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: output_shape},\n                            rtol=1e-05, process_args=process_args)\n\n    def test_depthwiseconv_0(self):\n        x_shape = [1, 3, 4, 3]\n        kernel_shape = [3, 3, 3, 3]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        kernel_val = np.arange(1, 1 + np.prod(kernel_shape)).astype(""float32"").reshape(kernel_shape)\n        def func(x):\n            kernel = tf.constant(kernel_val, dtype=tf.float32, name=\'k\')\n            conv = tf.nn.depthwise_conv2d(x, kernel, strides=[1, 1, 1, 1], padding=\'VALID\')\n            return tf.identity(conv, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=0.08)\n\n    def test_depthwiseconv_1(self):\n        x_shape = [1, 112, 112, 32]\n        kernel_shape = [3, 3, 32, 1]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        kernel_val = np.arange(1, 1 + np.prod(kernel_shape)).astype(""float32"").reshape(kernel_shape)\n        def func(x):\n            kernel = tf.constant(kernel_val, dtype=tf.float32, name=\'k\')\n            conv = tf.nn.depthwise_conv2d(x, kernel, strides=_STRIDE1x1, padding=\'VALID\')\n            return tf.identity(conv, name=_TFOUTPUT)\n        # rtol is a bit high, 2 values have a bit high error. Maybe use different input data.\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=0.08)\n\n    def test_depthwiseconv_3(self):\n        x_shape = [1, 112, 112, 32]\n        kernel_shape = [3, 3, 32, 1]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        kernel_val = np.arange(1, 1 + np.prod(kernel_shape)).astype(""float32"").reshape(kernel_shape)\n        def func(x):\n            kernel = tf.constant(kernel_val, dtype=tf.float32, name=\'k\')\n            conv = tf.nn.depthwise_conv2d(x, kernel, strides=[1, 1, 1, 1], padding=\'VALID\')\n            return tf.identity(conv, name=_TFOUTPUT)\n        # rtol is a bit high, 2 values have a bit high error. Maybe use different input data.\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=0.01)\n\n    @check_tf_max_version(""1.15"", ""not supported in tf-2.0"")\n    def test_dropout(self):\n        x_val = np.ones([1, 24, 24, 3], dtype=np.float32)\n        # Define a scope for reusing the variables\n        def func(x):\n            is_training = tf.constant(False, tf.bool)\n            x_ = tf.identity(x)\n            fc1 = tf.layers.dropout(x_, rate=.1, training=is_training)\n            return tf.identity(fc1, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val},\n                            graph_validator=lambda g: (check_op_count(g, ""RandomUniform"", 0) and\n                                                       check_op_count(g, ""RandomUniformLike"", 0)))\n\n    def test_nn_dropout(self):\n        x_val = np.ones([1, 24, 24, 3], dtype=np.float32)\n        # Define a scope for reusing the variables\n        def func(x, keep_prob):\n            x_ = tf.identity(x)\n            fc1 = dropout(x_, keep_prob)\n            return tf.identity(fc1, name=_TFOUTPUT)\n        # when constant_fold is enabled, PlaceholderWithDefault will be folded into either a const or a placeholder.\n        # here we set it False to test PlaceholderWithDefault bug: https://github.com/onnx/tensorflow-onnx/pull/446\n        # Dropout with ratio 1.0 will be optimized so that only one Identity is left\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: np.array(1., dtype=np.float32)},\n                            graph_validator=lambda g: (check_op_count(g, ""RandomUniform"", 0) and\n                                                       check_op_count(g, ""RandomUniformLike"", 0)))\n\n    @check_tf_min_version(""1.13"")\n    def test_nn_dropout_with_rate(self):\n        rate = tf.constant(0., name=""rate"")\n        x_val = np.ones([1, 24, 24, 3], dtype=np.float32)\n        # Define a scope for reusing the variables\n        def func(x):\n            x_ = tf.identity(x)\n            fc1 = tf.nn.dropout(x_, rate=rate)\n            return tf.identity(fc1, name=""output"")\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, constant_fold=False,\n                           graph_validator=lambda g: (check_op_count(g, ""RandomUniform"", 0) and\n                                                      check_op_count(g, ""RandomUniformLike"", 0)))\n\n    def test_conv2d_with_input_transpose(self):\n        x_shape = [2, 32, 32, 3]\n        kernel_shape = [3, 3, 3, 3]\n        x_val = make_xval(x_shape)\n        x_val_for_onnx = x_val.transpose(NHWC_TO_NCHW)\n        def func(x):\n            kernel = tf.constant(make_xval(kernel_shape), dtype=tf.float32, name=\'k\')\n            conv = tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding=""SAME"")\n            return tf.identity(conv, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05,\n                            process_args={""inputs_as_nchw"": [_INPUT]},\n                            onnx_feed_dict={_INPUT: x_val_for_onnx})\n\n    def test_lrn_default(self):\n        x_shape = [1, 3, 4, 3]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        def func(x):\n            op = tf.nn.local_response_normalization(x)\n            return tf.identity(op, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05)\n\n    def test_lrn(self):\n        # can\'t set bias = 0\n        x_shape = [1, 2, 2, 8]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        def func(x):\n            op = tf.nn.local_response_normalization(x, depth_radius=4, bias=2, alpha=2, beta=1)\n            return tf.identity(op, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05)\n\n    @check_onnxruntime_incompatibility(""Abs"")\n    def test_abs(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.abs(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Add"")\n    def test_const(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            y = tf.constant(x_val, name=""y"")\n            return tf.add(x, y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Add"")\n    def test_add(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.add(x, x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_placeholder(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            return tf.identity(x, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    #@unittest.skip(""doesn\'t work with the new ut func interface, fix later"")\n    #def test_placeholder_with_default_use_default(self):\n    #    x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n    #    def func():\n    #        x = tf.constant(x_val, name=""x"")\n    #        y = tf_placeholder_with_default(x, x_val.shape, name=_TFINPUT)\n    #    return tf.identity(y, name=_TFOUTPUT)\n    #    self._run_test_case(func, [_OUTPUT], {})\n\n    #@unittest.skip(""doesn\'t work with the new ut func interface, fix later"")\n    #def test_placeholder_with_default_use_feed(self):\n    #    x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n    #    def func():\n    #        x = tf.constant(x_val, name=""x"")\n    #        y = tf_placeholder_with_default(x, x_val.shape, name=_TFINPUT)\n    #        return tf.identity(y, name=_TFOUTPUT)\n    #    x_feed_val = np.array([11.0, 22.0, -33.0, -44.0], dtype=np.float32).reshape((2, 2))\n    #    self._run_test_case(func, [_OUTPUT], {_INPUT: x_feed_val})\n\n    @check_onnxruntime_incompatibility(""Add"")\n    def test_add_bcast(self):\n        x1_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        x2_val = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], dtype=np.float32).reshape((2, 2, 2))\n        def func(x1, x2):\n            x_ = tf.add(x1, x2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x1_val, _INPUT1: x2_val})\n\n    @check_onnxruntime_incompatibility(""Add"")\n    def test_add_bcast1(self):\n        # example taken from onnx doc\n        x1_val = np.random.randn(3, 4, 5).astype(np.float32)\n        x2_val = np.random.randn(5).astype(np.float32)\n        def func(x1, x2):\n            x_ = tf.add(x1, x2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x1_val, _INPUT1: x2_val})\n\n    def test_matmul0(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.matmul(x, x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_matmul1(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.matmul(x, x, transpose_a=True)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_matmul2(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        y_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x, y):\n            x_ = tf.matmul(x, y, transpose_b=True)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @unittest.skipIf(get_test_config().is_mac and get_test_config().is_onnxruntime_backend\n                     and get_test_config().backend_version == ""0.2.1"", ""onnxruntime 0.2.1 has bug on mac"")\n    def test_matmul3(self):\n        x_shape = [1, 12, 256, 64]\n        x_val = np.arange(np.prod(x_shape)).astype(""float32"").reshape((x_shape))\n        def func(x, y):\n            x_ = tf.matmul(x, y, transpose_b=True)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: x_val}, rtol=1e-5)\n\n    @check_onnxruntime_incompatibility(""Sub"")\n    def test_sub(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.subtract(x, x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Mul"")\n    def test_multiply(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.multiply(x, x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Div"")\n    def test_div(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.realdiv(x, x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Exp"")\n    def test_exp(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.exp(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05)\n\n    @check_onnxruntime_incompatibility(""Log"")\n    def test_log(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.math.log(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_gather(self):\n        x_val = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\n        idx = np.array([1, 0, 2], dtype=np.int32)\n        idx_flattened = np.array([i * x_val.shape[1] + idx for i in range(0, x_val.shape[0])])\n        def func(x):\n            x_ = tf.gather(tf.reshape(x, [-1]), tf.constant(idx_flattened))\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_target(\'rs6\', \'GatherNd\')\n    def test_gathernd(self):\n        x_val = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\n        indices = np.array([[[0, 1], [1, 1]], [[1, 2], [0, 2]]], dtype=np.int32)\n        def func(x):\n            x_ = tf.gather_nd(x, tf.constant(indices))\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=np.float32)\n        indices = np.array([[[0], [2]], [[4], [7]], [[6], [1]]], dtype=np.int32)\n        def func(x):\n            x_ = tf.gather_nd(x, tf.constant(indices))\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_target(\'rs6\', \'GatherNd\')\n    def test_gathernd_less_index(self):\n        x_val = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\n        indices = np.array([[[0], [1]], [[2], [0]]], dtype=np.int32)\n        def func(x):\n            x_ = tf.gather_nd(x, tf.constant(indices))\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        # shape: 2*2*2\n        x_val = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=np.float32)\n        indices = np.array([[[0, 0], [0, 1]], [[1, 0], [1, 1]]], dtype=np.int32)\n        def func(x):\n            x_ = tf.gather_nd(x, tf.constant(indices))\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend()\n    @check_opset_min_version(7, ""tile"")\n    def test_tile(self):\n        x_val = np.array([[0, 1], [2, 3]], dtype=np.float32)\n        def func(x):\n            multiple = tf.constant([2, 2])\n            x_ = tf.tile(x, multiple)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Neg"")\n    def test_neg(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.negative(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Mul"")\n    def test_square(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.square(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Min"")\n    def test_min(self):\n        x_val1 = np.array([4.0, 16.0, 4.0, 1.6], dtype=np.float32).reshape((2, 2))\n        x_val2 = np.array([4.0, 4.0, 4.0, 4.0], dtype=np.float32).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.minimum(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n        x_val1 = np.array([4.0, 16.0, 4.0, 1.6], dtype=np.int32).reshape((2, 2))\n        x_val2 = np.array([4.0, 4.0, 4.0, 4.0], dtype=np.int32).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.minimum(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    @skip_caffe2_backend(""issue with broadcasting scalar"")\n    @check_onnxruntime_incompatibility(""Sub"")\n    def test_min_broadcast(self):\n        # tests if the broadcast for min/max is working\n        x_val1 = np.array([2.0, 16.0, 5.0, 1.6], dtype=np.float32).reshape((2, 2))\n        x_val2 = np.array([4.0], dtype=np.float32)\n        def func(x1):\n            x2 = tf.constant(x_val2, dtype=tf.float32, name=\'x2\')\n            mi = tf.minimum(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1})\n\n    @check_onnxruntime_incompatibility(""Add"")\n    def test_logicaland(self):\n        x_val1 = np.array([1, 0, 1, 1], dtype=np.bool).reshape((2, 2))\n        x_val2 = np.array([0, 1, 1, 1], dtype=np.bool).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.logical_and(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    @check_onnxruntime_incompatibility(""Greater"")\n    def test_greater(self):\n        for op in [tf.greater, tf.greater_equal]:\n            x_val1 = np.array([4, 2, 4, 1], dtype=np.float32).reshape((2, 2))\n            x_val2 = np.array([2, 4, 4, 1], dtype=np.float32).reshape((2, 2))\n            def func(x1, x2):\n                mi = op(x1, x2)\n                return tf.identity(mi, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    @check_onnxruntime_incompatibility(""Greater"")\n    def test_greater_unsupport_type(self):\n        for op in [tf.greater, tf.greater_equal]:\n            x_val1 = np.array([4, 2, 4, 1], dtype=np.int32).reshape((2, 2))\n            x_val2 = np.array([2, 4, 4, 1], dtype=np.int32).reshape((2, 2))\n            def func(x1, x2):\n                mi = op(x1, x2)\n                return tf.identity(mi, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    @check_onnxruntime_incompatibility(""Less"")\n    def test_less(self):\n        x_val1 = np.array([4, 2, 4, 1], dtype=np.float32).reshape((2, 2))\n        x_val2 = np.array([2, 4, 4, 1], dtype=np.float32).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.less(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    @check_onnxruntime_incompatibility(""Less"")\n    def test_less_unsupport_type(self):\n        x_val1 = np.array([4, 2, 4, 1], dtype=np.int32).reshape((2, 2))\n        x_val2 = np.array([2, 4, 4, 1], dtype=np.int32).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.less(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    @check_opset_min_version(11, ""Equal"")\n    def test_equal_float(self):\n        x_val1 = np.array([0., 1., 2., 3., 4., -1., -2], dtype=np.float32)\n        x_val2 = np.array([0., 1., 2.1, 3.5, 4.6, -1.1, -2.9], dtype=np.float32)\n        def func(x1, x2):\n            mi = tf.equal(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    def test_equal(self):\n        x_val1 = np.array([4, 2, 4, 1], dtype=np.int32).reshape((2, 2))\n        x_val2 = np.array([2, 4, 4, 1], dtype=np.int32).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.equal(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n        x_val1 = np.array([4, 2, 4, 1], dtype=np.float32).reshape((2, 2))\n        x_val2 = np.array([2, 4, 4, 1], dtype=np.float32).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.equal(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    def test_not_equal(self):\n        x_val1 = np.array([4, 2, 4, 1], dtype=np.int32).reshape((2, 2))\n        x_val2 = np.array([2, 4, 4, 1], dtype=np.int32).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.not_equal(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n        x_val1 = np.array([4, 2, 4, 1], dtype=np.float32).reshape((2, 2))\n        x_val2 = np.array([2, 4, 4, 1], dtype=np.float32).reshape((2, 2))\n        def func(x1, x2):\n            mi = tf.not_equal(x1, x2)\n            return tf.identity(mi, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2})\n\n    def test_sequeeze_no_axis_specified(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2, 1))\n        def func(x):\n            x_ = tf.squeeze(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_sequeeze_positive_axis(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2, 1))\n        def func(x):\n            x_ = tf.squeeze(x, [2])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_sequeeze_negative_axis(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2, 1))\n        def func(x):\n            x_ = tf.squeeze(x, [-1])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_sequeeze_mixed_axis(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((1, 2, 2, 1))\n        def func(x):\n            x_ = tf.squeeze(x, [0, -1])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_transpose(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], dtype=np.float32).reshape((2, 3))\n        def func(x):\n            x_ = tf.transpose(x)  # perm=[1,0])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_reshape(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            shape = tf.constant([1, 4])\n            x_ = tf.reshape(x, shape)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, check_shape=True)\n\n    @check_opset_min_version(6, ""cast"")\n    def test_reshape_int(self):\n        x_val = np.array([1, 2, 3, 4], dtype=np.int32).reshape((2, 2))\n        def func(x):\n            shape = tf.constant([1, 4])\n            x_ = tf.reshape(x, shape)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, check_shape=True)\n\n    @check_opset_min_version(6, ""cast"")\n    def test_reshape_dynamic(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2))\n        shape_val = np.array([4, 1], dtype=np.int32)\n        def func(x, shape):\n            x_ = tf.reshape(x, shape)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: shape_val}, check_shape=True)\n\n    @check_onnxruntime_incompatibility(""Relu"")\n    def test_relu(self):\n        x_val = np.array([0.5, 1.0, -0.5, -1.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.nn.relu(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""fails on caffe2 with dim issue"")\n    @check_onnxruntime_incompatibility(""Mul"")\n    @check_tf_min_version(""1.6"")\n    def test_leaky_relu_int(self):\n        # starting from tf 1.6, leaky_relu supports `feature` x of int type\n        x_types = [np.int32, np.int64]\n        for x_type in x_types:\n            x_val = 1000 * np.random.random_sample([1000, 100]).astype(x_type)\n            for alpha in [0.1, -0.1, 1.0, -1.0]:\n                def func(x):\n                    x_ = tf.nn.leaky_relu(x, alpha)\n                    return tf.identity(x_, name=_TFOUTPUT)\n                self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""fails on caffe2 with dim issue"")\n    @check_onnxruntime_incompatibility(""Mul"")\n    def test_leaky_relu_with_dependency(self):\n        x_val = 1000 * np.random.random_sample([1000, 100]).astype(np.float32)\n        def func(x):\n            # simulate leaky_relu\n            alpha = tf.constant(0.5)\n            y = alpha * x\n            x_ = tf.maximum(y, x)\n            dependency = y - 1\n\n            return tf.identity(x_, name=_TFOUTPUT), tf.identity(dependency, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""fails on caffe2 with dim issue"")\n    @check_onnxruntime_incompatibility(""Mul"")\n    def test_leaky_relu_float(self):\n        x_val = 1000 * np.random.random_sample([1000, 100]).astype(np.float32)\n        for alpha in [0.1, -0.1, 1.0, -1.0]:\n            def func(x):\n                x_ = tf.nn.leaky_relu(x, alpha)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Elu"")\n    def test_elu(self):\n        x_val = np.array([0.5, 1.0, -0.5, -1.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.nn.elu(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Tanh"")\n    def test_tanh(self):\n        x_val = np.array([0.5, 1.0, -0.5, -1.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.tanh(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05)\n\n    def test_relu6(self):\n        x_val = np.array([0.5, 1.0, -0.5, -1.0, 6, 7], dtype=np.float32).reshape((2, 3))\n        def func(x):\n            x_ = tf.nn.relu6(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Sub"")\n    def test_relu6_dynamic(self):\n        x_val = np.array([0.5, 1.0, -0.5, -1.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.nn.relu6(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_concat(self):\n        x_val1 = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n        x_val2 = np.array([[7, 8, 9], [10, 11, 12]], dtype=np.float32)\n        x_val3 = np.array([[13, 14, 15], [16, 17, 18]], dtype=np.float32)\n        def func(x1, x2, x3):\n            x_ = tf.concat([x1, x2, x3], 0)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2, ""input3:0"": x_val3})\n\n    def test_concat_empty_const_input(self):\n        x_val1 = np.array([1, 2, 3], dtype=np.float32)\n        x_val2 = np.array([], dtype=np.float32)\n        def func(x1):\n            x2 = tf.constant(x_val2, dtype=tf.float32)\n            x_ = tf.concat([x1, x2], 0)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1})\n\n        x_val1 = np.array([[1, 2, 3]], dtype=np.float32)\n        x_val2 = np.array([[]], dtype=np.float32)\n        def func(x1):\n            x2 = tf.constant(x_val2, dtype=tf.float32)\n            x_ = tf.concat([x1, x2], 1)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1})\n\n        x_val1 = np.array([1, 2, 3], dtype=np.float32)\n        x_val2 = np.array([], dtype=np.float32)\n        x_val3 = np.array([13, 14, 15], dtype=np.float32)\n        def func(x1, x3):\n            x2 = tf.constant(x_val2, dtype=tf.float32)\n            x_ = tf.concat([x1, x2, x3], 0)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val3})\n\n    @check_opset_min_version(6, ""cast"")\n    def test_concat_int64(self):\n        x_val1 = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int64)\n        x_val2 = np.array([[7, 8, 9], [10, 11, 12]], dtype=np.int64)\n        x_val3 = np.array([[13, 14, 15], [16, 17, 18]], dtype=np.int64)\n        def func(x1, x2, x3):\n            x_ = tf.concat([x1, x2, x3], 0)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2, ""input3:0"": x_val3})\n\n    def test_concat_negative_axis(self):\n        x_val1 = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n        x_val2 = np.array([[7, 8, 9], [10, 11, 12]], dtype=np.float32)\n        x_val3 = np.array([[13, 14, 15], [16, 17, 18]], dtype=np.float32)\n        def func(x1, x2, x3):\n            x_ = tf.concat([x1, x2, x3], -1)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2, ""input3:0"": x_val3})\n\n    @check_onnxruntime_incompatibility(""Pow"")\n    def test_pow(self):\n        x_val = np.array([4.0, 16.0, 4.0, 1.6], dtype=np.float32)\n        e = np.array([2.0, 2.0, 2.0, 2.0], dtype=np.float32)\n        def func(x):\n            x_ = tf.pow(x, tf.constant(e))\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_embedding_lookup(self):\n        x_val1 = np.array([[1]], dtype=np.int32)\n        x_val2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=np.float32)\n        def func(x):\n            t = tf.constant(x_val2)\n            x_ = tf.nn.embedding_lookup(t, x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1})\n\n    def test_slice(self):\n        x_val = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.float32)\n        def func(x):\n            t1 = tf.constant([0, 1], dtype=tf.int32)\n            t2 = tf.constant([2, 2], dtype=tf.int32)\n            x_ = tf.slice(x, t1, t2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_slice_neg_size(self):\n        x_val = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.float32)\n        def func(x):\n            t1 = tf.constant([0, 1], dtype=tf.int32)\n            t2 = tf.constant([-1, 2], dtype=tf.int32)\n            x_ = tf.slice(x, t1, t2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""Slice in opset 10 can accept dymaic \'start\' and \'ends\'"")\n    def test_slice_with_non_const(self):\n        x_val = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.float32)\n        t1 = np.array([0, 1], dtype=np.int32)\n        t2 = np.array([2, 2], dtype=np.int32)\n        def func(x, t1, t2):\n            x_ = tf.slice(x, t1, t2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: t1, _INPUT2: t2})\n\n    @check_opset_min_version(10, ""Slice in opset 10 can accept dymaic \'start\' and \'ends\'"")\n    def test_slice_with_size_is_negative_one(self):\n        x_val = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.float32)\n        t1 = np.array([0, 1], dtype=np.int32)\n        # input ""size"" contains -1\n        t2 = np.array([2, -1], dtype=np.int32)\n        def func(x, t1, t2):\n            x_ = tf.slice(x, t1, t2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: t1, _INPUT2: t2})\n\n    @skip_caffe2_backend()\n    def test_slice1(self):\n        # FIXME: only 1 dimension supported by caffe2\n        x_val = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]]], dtype=np.float32)\n        def func(x):\n            t1 = tf.constant([1, 0, 0], dtype=tf.int32)\n            t2 = tf.constant([1, 1, 3], dtype=tf.int32)\n            x_ = tf.slice(x, t1, t2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_split(self):\n        x_val = np.linspace(1.0, 5 * 30.0, 5 * 30).astype(np.float32).reshape((5, 30))\n        def func(x):\n            x_, _, _ = tf.split(x, [4, 15, 11], 1)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_split_with_more_outputs(self):\n        x_val = np.linspace(1.0, 5 * 30.0, 5 * 30).astype(np.float32).reshape((5, 30))\n        def func(x):\n            return tf.split(x, [4, 15, 11], 1, name=""split_test"")\n        self._run_test_case(func, [""split_test:0"", ""split_test:1"", ""split_test:2""], {_INPUT: x_val})\n\n    def test_negative_split(self):\n        x_val = np.linspace(1.0, 5 * 30.0, 5 * 30).astype(np.float32).reshape((5, 30))\n        def func(x):\n            x_, _, _ = tf.split(x, [4, 15, -1], 1)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_reducesum(self):\n        # not supported by onnx-caffe2\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.reduce_sum(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_incompatibility(""Sqrt"")\n    def test_sqrt(self):\n        x_val = np.array([4.0, 16.0, 4.0, 1.6], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.math.sqrt(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def _test_range_const(self, extra_opset=None):\n        process_args = {}\n        if extra_opset is not None:\n            process_args[""extra_opset""] = [extra_opset]\n\n        def func():\n            x = tf.range(5)\n            return tf.identity(x, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {}, process_args=process_args)\n\n        def func():\n            x = tf.range(3, 3, 5)\n            return tf.identity(x, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {}, process_args=process_args)\n\n        def func():\n            x = tf.range(0, -5, -2)\n            return tf.identity(x, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {}, process_args=process_args)\n\n        def func():\n            x = tf.range(-5.0, 5.0, 1.5)\n            return tf.identity(x, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {}, process_args=process_args)\n\n        def func():\n            x = tf.range(2.5, 5.0, 10.0)\n            return tf.identity(x, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {}, process_args=process_args)\n\n    def _test_range_non_const(self, extra_opset=None):\n        process_args = {}\n        if extra_opset is not None:\n            process_args[""extra_opset""] = [extra_opset]\n\n        def func():\n            x = tf.range(5.0)\n            return tf.identity(x, name=_TFOUTPUT)\n        g = self._run_test_case(func, [_OUTPUT], {}, process_args=process_args)\n        # TODO: tf-2.0 uses the optimizer which will most likely make the range const which is not what we want to test\n        # self.assertTrue(extra_opset is None\n        #                or check_node_domain(group_nodes_by_type(g)[""Range""][0], extra_opset.domain))\n\n        def func():\n            x = tf.range(0, -5.0, -2)\n            return tf.identity(x*x, name=_TFOUTPUT)\n        g = self._run_test_case(func, [_OUTPUT], {}, process_args=process_args)\n        # TODO: tf-2.0 uses the optimizer which will most likely make the range const  which is not what we want to test\n        # self.assertTrue(extra_opset is None\n        #                or check_node_domain(group_nodes_by_type(g)[""Range""][0], extra_opset.domain))\n\n        # disable this case due to onnxruntime loop issue\n        # https://github.com/microsoft/onnxruntime/issues/1272\n        # x = tf.range(3.0, 3.0, 5)\n        # return tf.identity(x, name=_TFOUTPUT)\n        # g = self._run_test_case(func, [_OUTPUT], {}, process_args=process_args)\n        # self.assertTrue(extra_opset is None\n        #                 or check_node_domain(group_nodes_by_type(g)[""Range""][0], extra_opset.domain))\n\n        delta_val = np.array(1.5, dtype=np.float32)\n        def func(delta):\n            x = tf.range(-5.0, 5.0, delta)\n            return tf.identity(x, name=_TFOUTPUT)\n        g = self._run_test_case(func, [_OUTPUT], {_INPUT: delta_val}, process_args=process_args)\n        self.assertTrue(extra_opset is None\n                        or check_node_domain(group_nodes_by_type(g)[""Range""][0], extra_opset.domain))\n\n        start_val = np.array(2.5, dtype=np.float32)\n        def func(start):\n            x = tf.range(start, 5.0, 10.0)\n            return tf.identity(x, name=_TFOUTPUT)\n        g = self._run_test_case(func, [_OUTPUT], {_INPUT: start_val}, process_args=process_args)\n        self.assertTrue(extra_opset is None\n                        or check_node_domain(group_nodes_by_type(g)[""Range""][0], extra_opset.domain))\n\n    @check_opset_min_version(7, ""cast"")\n    def test_range_const(self):\n        self._test_range_const()\n\n    def test_range_non_const(self):\n        self._test_range_non_const()\n\n    @test_ms_domain()\n    def test_ms_range_const(self, extra_opset):\n        self._test_range_const(extra_opset)\n\n    @test_ms_domain()\n    def test_ms_range_non_const(self, extra_opset):\n        self._test_range_non_const(extra_opset)\n\n    @check_onnxruntime_incompatibility(""Sqrt"")\n    def test_rsqrt(self):\n        x_val = np.array([4.0, 16.0, 4.0, 1.6], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.math.rsqrt(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05)\n\n    @check_onnxruntime_incompatibility(""Reciprocal"")\n    def test_reciprocal(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.math.reciprocal(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-04)\n\n    def test_reducemax(self):\n        # not supported by onnx-caffe2\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.reduce_max(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05)\n\n    @skip_caffe2_backend()\n    def test_reduceprod(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.reduce_prod(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_reducemean(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.reduce_mean(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend()\n    @check_onnxruntime_incompatibility(""Pow"")\n    def test_pow_scalar(self):\n        x_val = np.array([4.0, 16.0, 4.0, 1.6], dtype=np.float32)\n        e = np.array(2.0, dtype=np.float32)\n        def func(x):\n            x_ = tf.pow(x, tf.constant(e))\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend()\n    def test_pad_const_default_val(self):\n        params = [\n            (""CONSTANT"", [[1, 1], [2, 2]], [[1.0, 1.2], [2.3, 3.4], [4.5, 5.7]]),\n            (""CONSTANT"", [[0, 0], [3, 3], [3, 3], [0, 0]], np.random.randn(1, 3, 4, 5).astype(np.float32)),\n        ]\n        for p in params:\n            mode, pad, xv = p\n            x_val = np.array(xv, dtype=np.float32)\n            def func(x):\n                paddings = tf.constant(pad)\n                op = tf.pad(x, paddings, mode)\n                return tf.identity(op, name=_TFOUTPUT)\n            self.logger.debug(str(p))\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend()\n    def test_pad_const(self):\n        x_val = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n        def func(x):\n            paddings = tf.constant([[1, 1], [2, 2]], name=""paddings"")\n            op = tf.pad(x, paddings, mode=""CONSTANT"", name=""const_with_val"", constant_values=999)\n            return tf.identity(op, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend()\n    def test_pad_reflect(self):\n        x_val = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n        def func(x):\n            paddings = tf.constant([[1, 1], [2, 2]], name=""paddings"")\n            op = tf.pad(x, paddings, mode=""REFLECT"", name=""reflect"")\n            return tf.identity(op, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend()\n    def test_randomuniform(self):\n        def func():\n            shape = tf.constant([2, 3], name=""shape"")\n            x_ = random_uniform(shape, name=""rand"", dtype=tf.float32)\n            x_ = tf.identity(x_, name=""output1"")\n            x_ = tf.identity(x_, name=""output2"")\n            return tf.identity(x_, name=_TFOUTPUT)\n        # since results are random, compare the shapes only\n        self._run_test_case(func, [_OUTPUT], {}, check_value=False, check_shape=True)\n\n    @unittest.skip(""TF RandomUniformInt is not supported"")\n    def test_randomuniform_int(self):\n        def func():\n            shape = tf.constant([2, 3], name=""shape"")\n            x_ = random_uniform(shape, name=""rand"", dtype=tf.int32, maxval=10)\n            x_ = tf.identity(x_, name=""output1"")\n            x_ = tf.identity(x_, name=""output2"")\n            return tf.identity(x_, name=_TFOUTPUT)\n        # since results are random, compare the shapes only\n        self._run_test_case(func, [_OUTPUT], {}, check_value=False, check_shape=True)\n\n    @skip_caffe2_backend()\n    @check_opset_after_tf_version(""2.2"", 9, ""RandomUniform"")\n    def test_randomuniform_dyn_shape(self):\n        # test for dynamic shape coming from a shape op\n        x_val = np.array([0, 1, 2, 3, 5], dtype=np.int64)\n        def func(x):\n            ret = random_uniform(x[3:], dtype=tf.float32)\n            return tf.identity(ret, name=_TFOUTPUT)\n        # since results are random, compare the shapes only\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, check_value=False, check_shape=True)\n\n    @skip_caffe2_backend()\n    def test_randomuniform_calc_shape(self):\n        # test for dynamic shape coming from some subgraph\n        x_val = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\n        def func(x):\n            x_ = tf.identity(x)\n            x_ = tf.shape(x_, name=""shape"")[1:]\n            x_ = random_uniform(x_, name=""rand"", dtype=tf.float32)\n            x_ = tf.identity(x_)\n            return tf.identity(x_, name=_TFOUTPUT)\n        # since results are random, compare the shapes only\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, check_value=False, check_shape=True)\n\n    @skip_caffe2_backend()\n    def test_argminmax(self):\n        x_val = np.array([0.5, 1.0, -0.5, -1.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.argmin(x, axis=0)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val = np.array([1, 2, -2, -1], dtype=np.int32).reshape((2, 2))\n        def func(x):\n            x_ = tf.argmax(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val = np.array([1, 2, -2, -1], dtype=np.int32).reshape((2, 2))\n        def func(x):\n            x_ = tf.argmax(x, output_type=x_val.dtype)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(6, ""cast"")\n    def test_cast(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.cast(x, tf.int32)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(7, ""sign"")\n    def test_sign(self):\n        x_vals = [np.array([1.0, 2.0, 0.0, -1.0, 0.0, -2.0], dtype=np.float32).reshape((2, 3)),\n                  np.array([1, 2, 0, -1, 0, -2], dtype=np.int32).reshape((2, 3)),\n                  np.array([1, 2, 0, -1, 0, -2], dtype=np.int64).reshape((2, 3))]\n        for x_val in x_vals:\n            def func(x):\n                x_ = tf.math.sign(x)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_target(""rs6"", ""onehot"")\n    def test_onehot0(self):\n        x_val = np.array([0, 1, 2], dtype=np.int32)\n        depth = 5\n        for axis in [-1, 0, 1]:\n            def func(x):\n                x_ = tf.one_hot(x, depth, on_value=5.0, axis=axis, off_value=1.0, dtype=tf.float32)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @unittest.skip(""only rank 1 is currently implemented"")\n    def test_onehot1(self):\n        # only rank 1 is currently implemented\n        x_val = np.array([[0, 2], [1, -1]], dtype=np.int32)\n        depth = 3\n        def func(x):\n            x_ = tf.one_hot(x, depth, on_value=5.0, axis=-1, off_value=0.0, dtype=tf.float32)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_target(""rs6"", ""onehot"")\n    def test_onehot2(self):\n        for axis in [-1, 0, 1]:\n            x_val = np.array([0, 1, 2, 1, 2, 0, 1, 2, 1, 2], dtype=np.int32)\n            depth = 20\n            def func(x):\n                x_ = tf.one_hot(x, depth, on_value=5.0, axis=axis, off_value=1.0, dtype=tf.float32)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_target(""rs6"", ""onehot"")\n    @check_opset_min_version(9, ""onehot"")\n    def test_onehot3(self):\n        # rank 1\n        for np_dtype in [np.int32, np.int64]:\n            x_val = np.array([0, 1, 2, 1, 2, 0, 1, 2, 1, 2], dtype=np_dtype)\n            depth = np.array(20).astype(np.int64)\n            def func(x):\n                on_off = np.array([5.6, 1.2]).astype(np_dtype)\n                x_ = tf.one_hot(x, depth, on_value=on_off[0], axis=-1, off_value=on_off[1])\n                return tf.identity(x_, name=_TFOUTPUT)\n            graph = self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n            self.assertTrue(len(group_nodes_by_type(graph)[""OneHot""]) == 1, ""onnx onehot should be used"")\n        # rank 2\n        for aixs in [-1, 0, 1, 2]:\n            for np_dtype in [np.int32, np.int64]:\n                x_val = np.arange(0, 50, dtype=np_dtype).reshape([-1, 10])\n                depth = np.array(20).astype(np.int64)\n                def func(x):\n                    on_off = np.array([5.6, 1.2]).astype(np_dtype)\n                    x_ = tf.one_hot(x, depth, on_value=on_off[0], axis=aixs, off_value=on_off[1])\n                    return tf.identity(x_, name=_TFOUTPUT)\n                graph = self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n                self.assertTrue(len(group_nodes_by_type(graph)[""OneHot""]) == 1, ""onnx onehot should be used"")\n\n    @skip_caffe2_backend(""issue undefined dim 1"")\n    @check_tf_max_version(""1.15"", ""not supported in tf-2.0"")\n    def test_flatten0(self):\n        x_val = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], dtype=np.float32)\n        def func(x):\n            x_ = tf.contrib.layers.flatten(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""issue undefined dim 1"")\n    @check_tf_max_version(""1.15"", ""not supported in tf-2.0"")\n    def test_flatten1(self):\n        x_val = np.array([[[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]], dtype=np.float32)\n        def func(x):\n            x_ = tf.contrib.layers.flatten(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_tf_max_version(""1.15"", ""not supported in tf-2.0"")\n    def test_flatten2(self):\n        x_val = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], dtype=np.float32)\n        def func(x):\n            x_ = tf.contrib.layers.flatten(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_cancel_transpose(self):\n        x_val = np.array([[[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]], dtype=np.float32)\n        def func(x):\n            x_ = tf.identity(x, _TFINPUT)\n            x_ = tf.transpose(x_, perm=NHWC_TO_NCHW)\n            x_ = tf.transpose(x_, perm=NCHW_TO_NHWC)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_onnxruntime_min_version(""0.5.0"", ""topk-10\'s shape inference function has a bug"")\n    @check_opset_min_version(6, ""cast"")\n    def test_topk1(self):\n        x_val = np.arange(3 * 2 * 3).astype(""float32"")\n        def func(x):\n            values, _ = tf.nn.top_k(x, 5, sorted=True)\n            return tf.identity(values, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""TopK with dynamic K"")\n    def test_topk2(self):\n        x_val = np.arange(3 * 2 * 3).astype(""float32"")\n        k_val = np.array(10).astype(np.int32)\n        def func(x, k):\n            values, _ = tf.nn.top_k(x, k, sorted=True)\n            return tf.identity(values, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: k_val})\n\n    @check_onnxruntime_min_version(""0.5.0"", ""topk-10\'s shape inference function has a bug"")\n    def test_topk3(self):\n        # test topk index output\n        x_val = np.arange(3 * 2 * 3).astype(""float32"")\n        def func(x):\n            _, idx = tf.nn.top_k(x, 5, sorted=True)\n            return tf.identity(idx, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_stack_axis(self):\n        for axis in [0, 1]:\n            x_val = [np.random.randn(3, 4).astype(""float32"") for _ in range(10)]\n            def func():\n                x = [tf.constant(x_val[i], dtype=tf.float32) for i in range(10)]\n                x_ = tf.stack(x, axis=axis)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {})\n\n    def test_unstack_axis(self):\n        for axis in [0, 1]:\n            x_val = np.random.randn(10, 3, 4).astype(""float32"")\n            def func():\n                x = tf.constant(x_val, dtype=tf.float32)\n                x_ = tf.unstack(x, axis=axis)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {})\n\n    def _test_reorganize_data(self, op, shape):\n        x_val = make_xval(shape)\n        def func(x):\n            x_ = op(x, block_size=2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""Space2Depth not implemented"")\n    def test_space_to_depth(self):\n        self._test_reorganize_data(tf.nn.space_to_depth, [1, 28, 28, 3])\n\n    @skip_caffe2_backend(""Depth2Space not implemented"")\n    def test_depth_to_space(self):\n        self._test_reorganize_data(tf.nn.depth_to_space, [1, 14, 14, 12])\n\n    def _test_reorganize_data_gpu(self, op, shape):\n        x_val = make_xval(shape)\n        def func(x):\n            x_ = op(x, block_size=2, data_format=""NCHW"")\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_tf_cpu(""only tf_gpu can run Space2Depth with NCHW format"")\n    @skip_caffe2_backend(""Space2Depth not implemented"")\n    def test_space_to_depth_gpu(self):\n        self._test_reorganize_data_gpu(tf.nn.space_to_depth, [1, 3, 50, 80])\n\n    @skip_tf_cpu(""only tf_gpu can run Depth2Space with NCHW format"")\n    @skip_caffe2_backend(""Depth2Space not implemented"")\n    def test_depth_to_space_gpu(self):\n        self._test_reorganize_data_gpu(tf.nn.depth_to_space, [1, 120, 25, 40])\n\n    @check_opset_min_version(6, ""addn"")\n    def test_addn(self):\n        x_val = np.arange(3 * 2 * 3).astype(""float32"")\n        def func(x):\n            x_ = tf.add_n([x, x, x])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice1(self):\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        def func(x):\n            x_ = tf.strided_slice(x, [1, 0, 0], [2, 1, 3], [1, 1, 1])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_strided_slice2(self):\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        def func(x):\n            x_ = tf.strided_slice(x, [1, 0, 0], [2, 2, 3], [1, 1, 1])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_strided_slice3(self):\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        def func(x):\n            x_ = x[1:]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_strided_slice4(self):\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        def func(x):\n            x_ = x[:2]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice5(self):\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        def func(x):\n            x_ = x[:2, 0:1, 1:]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice6(self):\n        # example from here:\n        # https://www.tensorflow.org/versions/r1.0/api_docs/cc/class/tensorflow/ops/strided-slice\n        x_val = np.arange(5 * 6).astype(""float32"").reshape((5, 6))\n        def func(x):\n            x_ = x[2, :]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice7(self):\n        x_val = np.arange(5 * 6).astype(""float32"").reshape((5, 6))\n        def func(x):\n            x_ = tf.strided_slice(x, [0, 1], [3, 4], [1, 1], begin_mask=2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        def func(x):\n            x_ = tf.strided_slice(x, [0, 1], [3, 4], [1, 1], end_mask=2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        def func(x):\n            x_ = tf.strided_slice(x, [0, 1], [3, 4], [1, 1], shrink_axis_mask=2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        def func(x):\n            x_ = tf.strided_slice(x, [0, 1], [3, 4], [1, 1], ellipsis_mask=2)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice8(self):\n        x_val = np.arange(1 * 2 * 3 * 4 * 5 * 6).astype(""float32"").reshape((1, 2, 3, 4, 5, 6))\n        def func(x):\n            x_ = x[0:1, ..., 1, 2:, :6]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val = np.arange(1 * 2 * 3 * 4 * 5 * 6).astype(""float32"").reshape((1, 2, 3, 4, 5, 6))\n        def func(x):\n            x_ = x[0:1, 1, 2:, :6, ...]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val = np.arange(1 * 2 * 3 * 4 * 5 * 6).astype(""float32"").reshape((1, 2, 3, 4, 5, 6))\n        def func(x):\n            x_ = x[..., 0:1, 1, 2:, :6]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice_dynamic_1(self):\n        # simple case\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        y_val = np.array([0, 1, 2], dtype=np.int32)\n        def func(x, y):\n            x_ = tf.strided_slice(x, y, [2, 2, 3], [1, 1, 1])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice_dynamic_2(self):\n        # int32\n        x_val = np.arange(3 * 2 * 3).astype(""int32"").reshape((3, 2, 3))\n        y_val = np.array([0, 1, 2], dtype=np.int32)\n        def func(x, y):\n            x_ = tf.strided_slice(x, y, [2, 2, 3], [1, 1, 1])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice_dynamic_3(self):\n        # common usage, ellipsis_mask\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        y_val = np.array(1, dtype=np.int32)\n        def func(x, y):\n            x_ = x[y:2, :, :]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice_dynamic_4(self):\n        # begin_mask, end_mask\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        y_val = np.array(1, dtype=np.int32)\n        def func(x, y):\n            x_ = x[y:, :y]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice_dynamic_5(self):\n        # only slice the first axis\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        y_val = np.array(1, dtype=np.int32)\n        def func(x, y):\n            x_ = x[y:2]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice_dynamic_6(self):\n        # shrink mask\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        y_val = np.array(1, dtype=np.int32)\n        def func(x, y):\n            x_ = x[y]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n        x_val = np.arange(3 * 2 * 3).astype(""float32"").reshape((3, 2, 3))\n        y_val = np.array(-1, dtype=np.int32)\n        def func(x, y):\n            x_ = x[y]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    @skip_caffe2_backend(""multiple dims not supported"")\n    def test_strided_slice_dynamic_7(self):\n        x_val = np.arange(1 * 2 * 3 * 4 * 5 * 6).astype(""float32"").reshape((1, 2, 3, 4, 5, 6))\n        y_val = np.array(1, dtype=np.int32)\n        def func(x, y):\n            x_ = x[0:y, ..., y, y:, :y]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n        x_val = np.arange(1 * 2 * 3 * 4 * 5 * 6).astype(""float32"").reshape((1, 2, 3, 4, 5, 6))\n        y_val = np.array(1, dtype=np.int32)\n        def func(x, y):\n            x_ = x[0:y, y, y:, :y, ...]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n        x_val = np.arange(1 * 2 * 3 * 4 * 5 * 6).astype(""float32"").reshape((1, 2, 3, 4, 5, 6))\n        y_val = np.array(1, dtype=np.int32)\n        def func(x, y):\n            x_ = x[..., 0:y, y, y:, :y]\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    def test_strided_slice_reverse_1(self):\n        x_val = np.arange(16 * 32).astype(np.float32).reshape((1, 16, 32, 1))\n        def func(x):\n            return tf.concat([x[:, :, :10], x[:, :, :21:-1]], axis=0, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    def test_strided_slice_reverse_2(self):\n        x_val = np.arange(16 * 32).astype(np.float32).reshape((1, 16, 32, 1))\n        def func(x):\n            return tf.concat([x[:, :, :10], x[:, :, 9::-1]], axis=0, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    def test_strided_slice_reverse_3(self):\n        x_val = np.zeros((1, 16, 32, 1)).astype(np.float32)\n        y_val = np.array(9).astype(np.int32)\n        z_val = np.array(-1).astype(np.int32)\n        def func(x, y, z):\n            return tf.concat([x[:, :, :10], x[:, :, y::z]], axis=0, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val, _INPUT2: z_val})\n\n    @check_opset_min_version(10, ""Slice"")\n    def test_new_axis_mask(self):\n        def func(x, y):\n            x_ = x[tf.newaxis, 0:y, y::2, tf.newaxis, :, tf.newaxis, :y, tf.newaxis, ..., 9]\n            return tf.identity(x_, name=_TFOUTPUT)\n        x_val = np.arange(5*10*10*10*10*20*30).astype(""float32"").reshape((5, 10, 10, 10, 10, 20, 30))\n        y_val = np.array(9, dtype=np.int32)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(7, ""batchnorm"")\n    def test_fused_batchnorm(self):\n        x_shape = [1, 28, 28, 2]\n        x_dtype = np.float32\n        scale_dtype = np.float32\n        scale_shape = [2]\n        # only nhwc is support on cpu for tensorflow\n        data_format = ""NHWC""\n        x_val = np.random.random_sample(x_shape).astype(x_dtype)\n        scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        def func(x):\n            scale = tf.constant(scale_val, name=\'scale\')\n            offset = tf.constant(offset_val, name=\'offset\')\n            mean = tf.constant(mean_val, name=\'mean\')\n            var = tf.constant(var_val, name=\'variance\')\n            epsilon = 0.001\n            y, _, _ = fused_batch_norm(\n                x, scale, offset, mean=mean, variance=var,\n                epsilon=epsilon, data_format=data_format, is_training=False)\n            return tf.identity(y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=1e-04)\n\n    @check_opset_min_version(7, ""batchnorm"")\n    @check_tf_min_version(""1.13"")\n    def test_batchnorm(self):\n        x_shape = [1, 128, 128, 2]\n        x_dtype = np.float32\n        scale_dtype = np.float32\n        scale_shape = [2]\n        x_val = np.random.random_sample(x_shape).astype(x_dtype)\n        scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        def func(x, mean, offset, var):\n            scale = tf.constant(scale_val, name=\'scale\')\n            epsilon = 0.001\n            y = tf.nn.batch_normalization(x, mean, var, offset, scale, epsilon)\n            return tf.identity(y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: mean_val, _INPUT2: offset_val, _INPUT3: var_val})\n\n    @check_opset_min_version(7, ""batchnorm"")\n    def test_conv2d_batchnorm_fusion(self):\n        x_shape = [1, 28, 28, 2]\n        x_val = np.random.random_sample(x_shape).astype(np.float32)\n        w = np.array([[2., 1., 1.],\n                      [1., 3., 1.],\n                      [1., 1., 4.]], dtype=np.float32).reshape(_KERNEL3x3)\n        # 2 channels for input and output\n        w = np.concatenate([w, w, w, w]).reshape([3, 3, 2, 2])\n        scale_dtype = np.float32\n        scale_shape = x_shape[-1:]\n        scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n        var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n        def func_conv2d(x):\n            kernel = tf.constant(w, dtype=tf.float32, name=\'k\')\n            conv = tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding=\'VALID\')\n            return conv\n\n        def func_fusedbn(x):\n            scale = tf.constant(scale_val, name=\'scale\')\n            offset = tf.constant(offset_val, name=\'offset\')\n            mean = tf.constant(mean_val, name=\'mean\')\n            var = tf.constant(var_val, name=\'variance\')\n            epsilon = 0.1234\n            y, _, _ = fused_batch_norm(\n                func_conv2d(x), scale, offset, mean=mean, variance=var,\n                epsilon=epsilon, data_format=\'NHWC\', is_training=False)\n            return tf.identity(y, name=_TFOUTPUT)\n\n        def graph_validator(g):\n            if \'BatchNormalization\' in [n.type for n in g.get_nodes()]:\n                return False\n            return True\n\n        self._run_test_case(func_fusedbn, [_OUTPUT], {_INPUT: x_val}, rtol=1e-05, graph_validator=graph_validator)\n\n    @check_tf_min_version(""1.15"")\n    @check_opset_min_version(10, ""quantize_and_dequantize"")\n    def test_qdq_unsigned_input(self):\n        x_shape = [3, 3, 2]\n        x_val = np.arange(1, 1+np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        def func(x):\n            x_ = quantize_and_dequantize(x, 1.0, 6.0, signed_input=False, range_given=True)\n            return tf.identity(x_, name=_TFOUTPUT)\n        _ = self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_tf_min_version(""1.15"")\n    @check_opset_min_version(10, ""quantize_and_dequantize"")\n    def test_qdq_signed_input(self):\n        x_shape = [3, 3, 2]\n        x_val = np.arange(-np.prod(x_shape)/2, np.prod(x_shape)/2).astype(""float32"").reshape(x_shape)\n        def func(x):\n            x_ = quantize_and_dequantize(x, -6.0, 6.0, signed_input=True, narrow_range=False, range_given=True)\n            return tf.identity(x_, name=_TFOUTPUT)\n        _ = self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @skip_caffe2_backend()\n    @check_opset_min_version(7, ""resize_nearest_neighbor"")\n    def test_resize_nearest_neighbor(self):\n        x_shape = [1, 15, 20, 2]\n        x_new_size = [30, 40]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        def func(x):\n            x_new_size_ = tf.constant(x_new_size)\n            x_ = resize_nearest_neighbor(x, x_new_size_)\n            return tf.identity(x_, name=_TFOUTPUT)\n        _ = self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(9, ""resize_nearest_neighbor"")\n    def test_resize_nearest_neighbor_with_non_const(self):\n        x_shape = [3, 10, 8, 5]\n        x_val = np.arange(1, 1 + np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n        x_new_size = np.array([20, 16]).astype(np.int32)\n        def func(x, x_new_size_):\n            x_ = resize_nearest_neighbor(x, x_new_size_)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: x_new_size})\n\n    @skip_caffe2_backend()\n    @check_opset_min_version(7, ""resize_bilinear"")\n    def test_resize_bilinear(self):\n        x_shape = [1, 15, 20, 2]\n        x_new_size = [30, 40]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        def func(x):\n            x_new_size_ = tf.constant(x_new_size)\n            x_ = resize_bilinear(x, x_new_size_)\n            return tf.identity(x_, name=_TFOUTPUT)\n        _ = self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(9, ""resize_bilinear"")\n    def test_resize_bilinear_with_non_const(self):\n        x_shape = [3, 10, 8, 5]\n        x_val = np.arange(1, 1 + np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n        x_new_size = np.array([20, 16]).astype(np.int32)\n        def func(x, x_new_size_):\n            x_ = resize_bilinear(x, x_new_size_)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: x_new_size})\n\n    @check_opset_min_version(10, ""resize scale can less than 1"")\n    def test_resize_bilinear_with_non_const2(self):\n        # scales has an element larger than 1 and also has an element less that 1\n        x_shape = [3, 100, 8, 5]\n        x_val = np.arange(1, 1 + np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n        x_new_size = np.array([20, 16]).astype(np.int32)\n        def func(x, x_new_size_):\n            x_ = resize_bilinear(x, x_new_size_)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: x_new_size})\n\n    @check_tf_min_version(""1.14"")\n    @check_opset_min_version(11, ""resize_bilinear_v2"")\n    def test_resize_bilinear_v2_with_non_const(self):\n        x_shape = [3, 10, 8, 5]\n        x_val = np.arange(1, 1 + np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n        x_new_size = np.array([20, 16]).astype(np.int32)\n        def func(x, x_new_size_):\n            x_ = resize_bilinear_v2(x, x_new_size_)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: x_new_size})\n\n    @check_opset_min_version(10, ""resize scale can less than 1"")\n    def test_resize_nearest_neighbor2(self):\n        x_shape = [1, 300, 20, 2]\n        x_new_size = [30, 40]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        def func(x):\n            x_new_size_ = tf.constant(x_new_size)\n            x_ = resize_nearest_neighbor(x, x_new_size_)\n            return tf.identity(x_, name=_TFOUTPUT)\n        _ = self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(9, ""fill"")\n    def test_fill_float32(self):\n        x_shape = [1, 15, 20, 2]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        def func(x0):\n            x1 = tf.fill(x_val.shape, 9.0)\n            x2 = tf.add(x0, x1)\n            return tf.identity(x2, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(9, ""fill"")\n    def test_fill_int32(self):\n        x_shape = [1, 15, 20, 2]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""int32"").reshape(x_shape)\n        def func(x0):\n            x1 = tf.fill(x_val.shape, 9)\n            x2 = tf.add(x0, x1)\n            return tf.identity(x2, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(7, ""fill"")\n    def test_fill7_float32(self):\n        x_shape = [1, 15, 20, 2]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""float32"").reshape(x_shape)\n        def func(x0):\n            x1 = tf.fill(x_val.shape, 9.0)\n            x2 = tf.add(x0, x1)\n            return tf.identity(x2, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(7, ""fill"")\n    def test_fill7_int32(self):\n        x_shape = [1, 15, 20, 2]\n        x_val = np.arange(1, 1 + np.prod(x_shape)).astype(""int32"").reshape(x_shape)\n        def func(x0):\n            x1 = tf.fill(x_val.shape, 9)\n            x2 = tf.add(x0, x1)\n            return tf.identity(x2, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(7, ""div"")\n    def test_tf_div(self):\n        # pylint: disable=E0001,C0415\n        from tensorflow.python.ops.gen_math_ops import div\n        shape = 1000\n        # test floating data\n        x_val = (np.random.sample(shape) + 1e-6).astype(np.float32)\n        y_val = (np.random.sample(shape) + 1e-6).astype(np.float32)\n        def func(x, y):\n            output = div(x, y, name=_TFOUTPUT)\n            # assert output.op.type == ""Div""\n            return output\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n        # test integer data\n        x_val = (100 * np.random.sample(shape) + 1).astype(np.int32)\n        y_val = (100 * np.random.sample(shape) + 1).astype(np.int32)\n        def func(x, y):\n            output = div(x, y, name=_TFOUTPUT)\n            # assert output.op.type == ""Div""\n            return output\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(7, ""erf"")\n    def test_erf(self):\n        x_shape = [2, 2]\n        x_val0 = np.random.random(np.prod(x_shape)).astype(np.float32).reshape(x_shape)\n        x_val1 = np.array([[-1, -0.5], [1, 0.5]]).astype(np.float32)\n        for x_val in [x_val0, x_val1]:\n            def func(x):\n                x_ = tf.math.erf(x)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=0.01)\n\n    @check_opset_min_version(8, ""Scan"")\n    @skip_opset(9, ""ReverseSequence"")\n    def test_reverse_sequence_batch_major(self):\n        x_val = np.array([[[1, 2, 3], [4, 5, 6], [0, 0, 0]],\n                          [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                          [[1, 2, 3], [0, 0, 0], [0, 0, 0]]],\n                         dtype=np.float32)\n        def func(x):\n            x_ = tf.reverse_sequence(x, seq_axis=1, batch_axis=0, seq_lengths=[2, 3, 1])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val = np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3],\n                          [4, 5, 6], [4, 5, 6], [1, 1, 1],\n                          [0, 0, 0], [7, 8, 9], [0, 0, 0]\n                          ],\n                         dtype=np.float32)\n        def func(x):\n            x_ = tf.reverse_sequence(x, seq_axis=1, batch_axis=0, seq_lengths=[3] * 9)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val_shape = [5, 5, 7, 8, 9]\n        x_val = np.random.randint(0, 100, x_val_shape).astype(np.float32)\n        def func(x):\n            x_ = tf.reverse_sequence(x, seq_axis=1, batch_axis=0, seq_lengths=[5, 5, 5, 5, 5])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(8, ""Scan"")\n    @skip_opset(9, ""ReverseSequence"")\n    def test_reverse_sequence_time_major(self):\n        x_val = np.array([[[1, 2, 3], [1, 2, 3], [1, 2, 3]],\n                          [[4, 5, 6], [4, 5, 6], [0, 0, 0]],\n                          [[0, 0, 0], [7, 8, 9], [0, 0, 0]]],\n                         dtype=np.float32)\n        def func(x):\n            x_ = tf.reverse_sequence(x, seq_axis=0, batch_axis=1, seq_lengths=[2, 3, 1])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n        x_val = np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3],\n                          [4, 5, 6], [4, 5, 6], [1, 1, 1],\n                          [0, 0, 0], [7, 8, 9], [0, 0, 0]],\n                         dtype=np.float32)\n        def func(x):\n            x_ = tf.reverse_sequence(x, seq_axis=0, batch_axis=1, seq_lengths=[9, 9, 9])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n        x_val_shape = [5, 5, 7, 8, 9]\n        x_val = np.random.randint(0, 100, x_val_shape).astype(np.float32)\n        def func(x):\n            x_ = tf.reverse_sequence(x, seq_axis=0, batch_axis=1, seq_lengths=[5, 5, 5, 5, 5])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""ReverseSequence"")\n    def test_reversev2_constant_axis(self):\n        # Tests for constant axis.\n        x_val_shape = [1, 2, 3, 4]\n        x_val = np.random.randint(0, 100, x_val_shape).astype(np.float32)\n        def func(x):\n            x_ = reverse_v2(x, axis=[3])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        # Empty axis vector.\n        x_val_shape = [2, 3, 4]\n        x_val = np.random.randint(0, 100, x_val_shape).astype(np.float32)\n        def func(x):\n            x_ = reverse_v2(x, axis=[])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""ReverseSequence"")\n    def test_reversev2_vector_axis(self):\n        x_val_shape = [1, 2, 3, 4]\n        x_val = np.random.randint(0, 100, x_val_shape).astype(np.float32)\n        def func(x):\n            x_ = reverse_v2(x, axis=[0, -3, 2, 3])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val_shape = [2, 3, 4]\n        x_val = np.random.randint(0, 100, x_val_shape).astype(np.float32)\n        def func(x):\n            x_ = reverse_v2(x, axis=[-3, 1, 2])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val_shape = [5, 5, 9, 7, 8, 9]\n        x_val = np.random.randint(0, 100, x_val_shape).astype(np.float32)\n        def func(x):\n            x_ = reverse_v2(x, axis=[0, 1, -2, 3, 5])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""ReverseSequence"")\n    def test_reversev2_1D_tensor(self):\n        # For tensors with 1 dimension and no axis to reverse.\n        # Adds an identity block.\n        x_val_shape = [4]\n        x_val = np.random.randint(0, 100, x_val_shape).astype(np.float32)\n        def func(x):\n            x_ = reverse_v2(x, axis=[])\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(8, ""where"")\n    def test_where(self):\n        x_val = np.array([1, 2, -3, 4, -5, -6, -7, 8, 9, 0], dtype=np.float32)\n        true_result = np.array([111, 222, 333, 444, 555, 666, 777, 888, 999, 1000],\n                               dtype=np.float32)\n        false_result = np.array([-111, -222, -333, -444, -555, -666, -777, -888, -999, -1000],\n                                dtype=np.float32)\n        def func(x):\n            picks = tf.where(x > -1, true_result, false_result)\n            return tf.identity(picks, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        x_val = np.array(1, dtype=np.float32)\n        true_result = np.array(100, dtype=np.float32)\n        false_result = np.array(-111, dtype=np.float32)\n        def func(x):\n            picks = tf.where(x > -1, true_result, false_result)\n            return tf.identity(picks, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(8, ""where"")\n    @check_target(""rs6"", ""onnxruntime Where type limitation"")\n    def test_where_int32(self):\n        x_val = np.array([1, 2, -3, 4, -5, -6, -7, 8, 9, 0], dtype=np.int32)\n        true_result = np.array([111, 222, 333, 444, 555, 666, 777, 888, 999, 1000],\n                               dtype=np.int32)\n        false_result = np.array([-111, -222, -333, -444, -555, -666, -777, -888, -999, -1000],\n                                dtype=np.int32)\n        def func(x):\n            picks = tf.where(tf.greater_equal(x, 0), true_result, false_result)\n            return tf.identity(picks, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(8, ""where"")\n    @check_tf_max_version(""1.15"", ""issues in tf-2.0, fix later"")\n    def test_where_with_two_rank_input(self):\n        x_val = np.array([1, 2, -3, 4, -5, -6, -7, 8, 9, 0], dtype=np.float32)\n        true_result = np.array([[111, 111], [222, 222], [333, 333], [444, 444], [555, 555],\n                                [666, 666], [777, 777], [888, 888], [999, 999], [1000, 1000]],\n                               dtype=np.float32)\n        false_result = np.array([[-111, -111], [-222, -222], [-333, -333], [-444, -444], [-555, -555],\n                                 [-666, -666], [-777, -777], [-888, -888], [-999, -999], [-1000, -1000]],\n                                dtype=np.float32)\n        def func(x):\n            cond = tf.greater_equal(x, 0)\n            picks = tf.where(cond, true_result, false_result)\n            return tf.identity(picks, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(8, ""where"")\n    def test_where_with_two_rank_condition(self):\n        x_val = np.array([[1, 2, -3, 4, -5, -6, -7, 8, 9, 0]], dtype=np.float32)\n        true_result = np.array([[111, 222, 333, 444, 555, 666, 777, 888, 999, 1000]],\n                               dtype=np.float32)\n        false_result = np.array([[-111, -222, -333, -444, -555, -666, -777, -888, -999, -1000]],\n                                dtype=np.float32)\n        def func(x):\n            picks = tf.where(tf.greater_equal(x, 0), true_result, false_result)\n            return tf.identity(picks, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(8, ""where"")\n    def test_where_with_three_rank_condition(self):\n        x_val = np.array([[[1, 2, -3, 4, -5, -6, -7, 8, 9, 0]]], dtype=np.float32)\n        true_result = np.array([[[111, 222, 333, 444, 555, 666, 777, 888, 999, 1000]]],\n                               dtype=np.float32)\n        false_result = np.array([[[-111, -222, -333, -444, -555, -666, -777, -888, -999, -1000]]],\n                                dtype=np.float32)\n        def func(x):\n            picks = tf.where(tf.greater_equal(x, 0), true_result, false_result)\n            return tf.identity(picks, name=_TFOUTPUT)\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(8, ""where"")\n    def test_where_scalar(self):\n        x_val = np.array(6, dtype=np.float32)\n        true_result = np.array([111, 222, 333, 444, 555, 666, 777, 888, 999, 1000],\n                               dtype=np.float32)\n        false_result = np.array([-111, -222, -333, -444, -555, -666, -777, -888, -999, -1000],\n                                dtype=np.float32)\n        def func(x):\n            picks = tf.where(tf.greater_equal(x, 0), true_result, false_result)\n            return tf.identity(picks, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(9, ""NonZero"")\n    @check_target(""rs6"", ""onnxruntime Transpose type limitation"")\n    def test_where_with_cond_only(self):\n        for np_type in [np.int32, np.float32]:\n            x_val = np.random.randint(0, 2, size=[10, 20, 30]).astype(np_type)\n            def func(x):\n                # FIXME: was tf_placeholder(tf_type, shape=[None] * x_val.ndim, name=_TFINPUT)\n                res = tf.where(x)\n                return tf.identity(res, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(6, ""cast"")\n    def test_shape_int32(self):\n        x_val = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]], dtype=np.float32)\n        def func(x):\n            x_ = tf.multiply(x, x)\n            x_ = tf.shape(x_, out_type=tf.int32)\n            return tf.identity(x_, name=_TFOUTPUT)\n        kwargs = {""check_dtype"": True}\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, **kwargs)\n\n    @unittest.skipIf(get_test_config().is_onnxruntime_backend and get_test_config().opset < 7,\n                     ""mul-1, mul-6 not supported in onnxruntime. conversion is covered since opset6"")\n    def test_shape_int64(self):\n        x_val = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]], dtype=np.float32)\n        def func(x):\n            x_ = tf.multiply(x, x)\n            x_ = tf.shape(x_, out_type=tf.int64)\n            return tf.identity(x_, name=_TFOUTPUT)\n        kwargs = {""check_dtype"": True}\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, **kwargs)\n\n    # @check_opset_min_version(7, ""broadcasting op"")\n    @unittest.skip(""disable it for now, since fold const has bug"")\n    def test_softmax_cross_entropy_with_logits(self):\n        num_class = 5\n        data_shape = [100, num_class]\n        for np_dtype in [np.int32, np.int64]:\n            label_val = np.random.randint(0, num_class - 1, data_shape).astype(np_dtype)\n            logits_val = np.random.random(data_shape).astype(np.float32)\n\n            def func(label, logits):\n                res1 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logits)\n                return tf.identity(res1, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: label_val, _INPUT1: logits_val}, atol=1e-5)\n\n    @check_opset_min_version(9, ""sparse_softmax_cross_entropy_with_logits"")\n    def test_sparse_softmax_cross_entropy_with_logits(self):\n        # FIXME: fails for opset 8 on onnxruntime-1.0, disable for now\n        num_class = 5\n        label_val = np.array([3, 2, 0, 4]).astype(np.int32)\n        logits_val = np.random.random((len(label_val), num_class)).astype(np.float32)\n        def func(label, logits):\n            res1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logits)\n            return tf.identity(res1, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: label_val, _INPUT1: logits_val})\n\n    @check_target(\'rs6\', \'SparseSoftmaxCrossEntropyWithLogits\')\n    def test_sparse_softmax_cross_entropy_with_logits_large_class(self):\n        num_class = 30000\n        label_val = np.array([3374, 2127, 10002, 48]).astype(np.int32)\n        logits_val = np.random.random((len(label_val), num_class)).astype(np.float32)\n\n        def func(label, logits):\n            res = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logits)\n            return tf.identity(res, name=_TFOUTPUT)\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: label_val, _INPUT1: logits_val}, rtol=1e-6)\n\n    def test_matrix_band_part(self):\n        input_val = np.random.randint(0, 666, (10, 15)).astype(np.int32)\n        def func(input_x):\n            res = tf.linalg.band_part(input_x, -1, 0)\n            res1 = tf.linalg.band_part(input_x, 0, -1)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n    def test_matrix_band_part_2(self):\n        input_val = np.random.randint(0, 666, (1, 1)).astype(np.int32)\n        def func(input_x):\n            res = tf.linalg.band_part(input_x, -1, 0)\n            res1 = tf.linalg.band_part(input_x, 0, -1)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n    def test_floordiv(self):\n        input_val_1 = np.random.random_sample(100).astype(np.int32)\n        input_val_2 = (np.random.random_sample(100) + 1).astype(np.int32)\n        def func(input_1, input_2):\n            res = tf.math.floordiv(input_1, input_2)\n            return tf.identity(res, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val_1, _INPUT1: input_val_2})\n\n        input_val_1 = np.random.random_sample(100).astype(np.float32)\n        input_val_2 = (np.random.random_sample(100) + 1).astype(np.float32)\n        def func(input_1, input_2):\n            res = tf.math.floordiv(input_1, input_2)\n            return tf.identity(res, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val_1, _INPUT1: input_val_2})\n\n        # test broadcasting\n        input_val_1 = np.random.random_sample((10, 50)).astype(np.float32)\n        input_val_2 = (np.random.random_sample(50) + 1).astype(np.float32)\n        def func(input_1, input_2):\n            res = tf.math.floordiv(input_1, input_2)\n            return tf.identity(res, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val_1, _INPUT1: input_val_2})\n\n    def test_floormod(self):\n        input_val_1 = 100 * np.random.random_sample(100).astype(np.int32)\n        input_val_2 = (100 * np.random.random_sample(100) + 1).astype(np.int32)\n        def func(input_1, input_2):\n            res = floormod(input_1, input_2)\n            return tf.identity(res, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val_1, _INPUT1: input_val_2})\n\n        input_val_1 = 100 * np.random.random_sample(100).astype(np.float32)\n        input_val_2 = (100 * np.random.random_sample(100) + 1).astype(np.float32)\n        def func(input_1, input_2):\n            res = floormod(input_1, input_2)\n            return tf.identity(res, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val_1, _INPUT1: input_val_2}, rtol=1e-5)\n\n        # test broadcasting case\n        input_val_1 = (50 * np.random.random_sample((10, 50)) + 1).astype(np.float32)\n        input_val_2 = (50 * np.random.random_sample(50) + 1).astype(np.float32)\n        def func(input_1, input_2):\n            res = floormod(input_1, input_2)\n            return tf.identity(res, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val_1, _INPUT1: input_val_2}, rtol=1e-4)\n\n    def test_logical_not(self):\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(x):\n            res = tf.logical_not(x)\n            return tf.identity(res, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val})\n\n    def test_reduce_all(self):\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(x):\n            res = tf.reduce_all(input_tensor=x, keepdims=False)\n            res1 = tf.reduce_all(input_tensor=x, axis=[0], keepdims=False)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(input_x):\n            res = tf.reduce_all(input_tensor=input_x, keepdims=True)\n            res1 = tf.reduce_all(input_tensor=input_x, axis=[0], keepdims=True)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n    def test_reduce_any(self):\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(x):\n            res = tf.reduce_any(input_tensor=x, keepdims=False)\n            res1 = tf.reduce_any(input_tensor=x, axis=[0], keepdims=False)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(x):\n            res = tf.reduce_any(input_tensor=x, keepdims=True)\n            res1 = tf.reduce_any(input_tensor=x, axis=[0], keepdims=True)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n    @check_opset_min_version(11, ""ReduceMin"")\n    def test_reduce_all_negative_axis(self):\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(x):\n            res = tf.reduce_all(input_tensor=x, keepdims=False)\n            res1 = tf.reduce_all(input_tensor=x, axis=[-1], keepdims=False)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(input_x):\n            res = tf.reduce_all(input_tensor=input_x, keepdims=True)\n            res1 = tf.reduce_all(input_tensor=input_x, axis=[-1], keepdims=True)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n    @check_opset_min_version(11, ""ReduceSum"")\n    def test_reduce_any_negative_axis(self):\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(x):\n            res = tf.reduce_any(input_tensor=x, keepdims=False)\n            res1 = tf.reduce_any(input_tensor=x, axis=[-1], keepdims=False)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n        input_val = np.random.randint(0, 2, (10, 20)).astype(np.bool)\n        def func(x):\n            res = tf.reduce_any(input_tensor=x, keepdims=True)\n            res1 = tf.reduce_any(input_tensor=x, axis=[-1], keepdims=True)\n            return tf.identity(res, name=_TFOUTPUT), tf.identity(res1, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: input_val})\n\n    @check_opset_min_version(7, ""fill"")\n    def test_zeros_like(self):\n        input_x = np.random.random_sample([10, 20]).astype(np.float32)\n        input_y = np.array([20, 10]).astype(np.int64)\n\n        def func(x, y):\n            z = tf.reshape(x, y)\n            return tf.zeros_like(z, name=_TFOUTPUT)\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_x, _INPUT1: input_y})\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_x.astype(np.int32), _INPUT1: input_y})\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_x > 0.5, _INPUT1: input_y})\n\n    @check_opset_min_version(9, ""is_nan"")\n    def test_isnan(self):\n        # only compatible with dtype `float32`\n        x_val1 = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        x_val2 = np.array([np.nan, np.nan, np.nan, np.nan], dtype=np.float32).reshape((2, 2))\n        x_val3 = np.array([1.0, np.nan, -3.0, np.nan], dtype=np.float32).reshape((2, 2))\n        for x_val in [x_val1, x_val2, x_val3]:\n            def func(x):\n                x_ = is_nan(x)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_ceil(self):\n        x_val = np.array([-1.5, 1.2], dtype=np.float32)\n        def func(x):\n            x_ = tf.math.ceil(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_softplus(self):\n        x_val = np.array([-1, 0, 1], dtype=np.float32)\n        def func(x):\n            x_ = tf.math.softplus(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_softsign(self):\n        x_val = np.array([-1, 0, 1], dtype=np.float32)\n        def func(x):\n            x_ = tf.math.softsign(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    def test_batch_to_spacend(self):\n        block_size = [2, 2]\n        crop = [[1, 0], [2, 1]]\n\n        input_val = np.random.random_sample([40, 3, 5, 100]).astype(np.float32)\n        def func(x):\n            return batch_to_space_nd(x, block_size, crop, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val})\n\n    @check_opset_min_version(11, ""BatchToSpaceND"")\n    @unittest.skip(""this was recently removed - but don\'t we want this to work ?"")\n    def test_batch_to_spacend_non_const(self):\n        def func(input_x, block_shape, crops):\n            return batch_to_space_nd(input_x, block_shape, crops, name=_TFOUTPUT)\n        input_x_val = np.random.random_sample([40, 3, 5, 100]).astype(np.float32)  # NHWC\n        block_shape_val = np.array([2, 2]).astype(np.int64)\n        crops_val = np.array([[1, 0], [2, 1]]).astype(np.int64)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_x_val, _INPUT1: block_shape_val, _INPUT2: crops_val})\n\n    @check_opset_min_version(11, ""SpaceToBatchND"")\n    @unittest.skip(""this was recently removed - but don\'t we want this to work ?"")\n    def test_space_to_batchnd_non_const(self):\n        input_x_val = np.random.random_sample([40, 5, 7, 66]).astype(np.float32)  # NHWC\n        def func(input_x, block_size, pad):\n            return batch_to_space_nd(input_x, block_size, pad, name=_TFOUTPUT)\n        block_size_val = np.array([2, 2]).astype(np.int64)\n        pad_val = np.array([[0, 1], [2, 1]]).astype(np.int64)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_x_val, _INPUT1: block_size_val, _INPUT2: pad_val})\n\n    @check_opset_min_version(11, ""BatchToSpaceND"")\n    def test_batch_to_spacend_non_const_7d(self):\n        x_type, y_type, z_type = np.int64, np.int64, np.int64\n        # test 3D upto 7D input tensors\n        for x_shape in [[12, 4, 4], [12, 4, 8, 3], [12, 4, 8, 3, 2], [12, 4, 8, 3, 2, 3], [12, 4, 8, 3, 2, 1, 3]]:\n            # test 1D upto 2D block shapes\n            for block_shape in [[2, 3], [2]]:\n                # crop 1 layer at end of each dim\n                # x and z can be dynamic.\n                # y = block_shape cannot be dynamic without change to Transpose op spec\n                crops = [[0, 1] for dim in block_shape]\n                y_val = np.array(block_shape).astype(y_type)\n                x_val = np.array([x + 1 for x in range(0, np.prod(x_shape))], dtype=x_type).reshape(x_shape)\n                z_val = np.array(crops).astype(z_type)\n                def func(x, z):\n                    y = tf.constant(dtype=y_type, value=y_val, shape=y_val.shape, name=_TFINPUT1)\n                    return batch_to_space_nd(x, y, z, name=_TFOUTPUT)\n                self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT2: z_val})\n\n    @check_opset_min_version(11, ""SpaceToBatchND"")\n    def test_space_to_batchnd_non_const_7d(self):\n        x_type, y_type, z_type = np.int64, np.int64, np.int64\n        # test 3D upto 7D input tensors\n        for x_shape in [[2, 4, 4], [1, 4, 8, 3], [1, 4, 8, 3, 2], [1, 4, 8, 3, 2, 3], [1, 4, 8, 3, 2, 1, 3]]:\n            # test 1D upto 2D block shapes\n            for block_shape in [[2], [2, 2]]:\n                # pad 1 layer at begin and end of each dim\n                pads = [[1, 1] for dim in block_shape]\n                y_val = np.array(block_shape).astype(y_type)\n                x_val = np.array([x + 1 for x in range(0, np.prod(x_shape))], dtype=x_type).reshape(x_shape)\n                z_val = np.array(pads).astype(z_type)\n                # x and z can be dynamic.\n                # y = block_shape cannot be dynamic without change to Transpose op spec\n                def func(x, z):\n                    y = tf.constant(dtype=y_type, value=y_val, shape=y_val.shape, name=_TFINPUT1)\n                    return space_to_batch_nd(x, y, z, name=_TFOUTPUT)\n                self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT2: z_val})\n\n    @check_opset_min_version(10, ""CropAndResize"")\n    def test_crop_and_resize(self):\n        boxes_val = [[0.5, 0.7, 0.7, 0.9], [0.2, 0.4, 0.4, 0.6]]\n        def func(input_x, box_ind):\n            boxes = tf.constant(boxes_val, dtype=tf.float32)\n            corp_size = tf.constant(np.array([20, 20]).astype(np.int32))\n            return tf.image.crop_and_resize(input_x, boxes, box_ind, corp_size, name=_TFOUTPUT, method=\'bilinear\')\n\n        input_x_val = np.random.randint(low=0, high=256, size=[2, 36, 36, 3]).astype(np.float32)  # NHWC\n        box_ind_val = np.array([1, 0]).astype(np.int32)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_x_val, _INPUT2: box_ind_val},\n                            rtol=1e-04, atol=1e-03)\n\n    @check_opset_min_version(11, ""CropAndResize"")\n    def test_crop_and_resize_linear(self):\n        def func(input_x, boxes, box_ind, corp_size):\n            return tf.image.crop_and_resize(input_x, boxes, box_ind, corp_size, name=_TFOUTPUT, method=\'bilinear\')\n\n        input_x_val = np.random.randint(low=0, high=256, size=[2, 36, 36, 3]).astype(np.float32)  # NHWC\n        boxes_val = np.array([[0.5, 0.7, 0.7, 0.9], [0.2, 0.4, 0.4, 0.6]]).astype(np.float32)\n        box_ind_val = np.array([1, 0]).astype(np.int32)\n        corp_size_val = np.array([20, 20]).astype(np.int32)\n        self._run_test_case(func, [_OUTPUT],\n                            {_INPUT: input_x_val, _INPUT1: boxes_val, _INPUT2: box_ind_val, _INPUT3: corp_size_val},\n                            rtol=1e-05, atol=1e-04)\n\n    @check_tf_min_version(""1.9"")\n    @check_opset_min_version(11, ""CropAndResize"")\n    def test_crop_and_resize_nearest(self):\n        def func(input_x, boxes, box_ind, corp_size):\n            return tf.image.crop_and_resize(input_x, boxes, box_ind, corp_size, name=_TFOUTPUT, method=\'nearest\')\n        input_x_val = np.random.randint(low=0, high=256, size=[1, 36, 36, 3]).astype(np.float32)  # NHWC\n        boxes_val = np.array([[0.2, 0.4, 0.6, 0.8]]).astype(np.float32)\n        box_ind_val = np.array([0]).astype(np.int32)\n        corp_size_val = np.array([30, 30]).astype(np.int32)\n        self._run_test_case(func, [_OUTPUT],\n                            {_INPUT: input_x_val, _INPUT1: boxes_val, _INPUT2: box_ind_val, _INPUT3: corp_size_val},\n                            rtol=1e-05, atol=1e-04)\n\n    @check_opset_min_version(11, ""CropAndResize"")\n    def test_crop_and_resize_extrapolation(self):\n        def func(input_x, boxes, box_ind, corp_size):\n            return tf.image.crop_and_resize(input_x, boxes, box_ind, corp_size, name=_TFOUTPUT, extrapolation_value=1.0)\n        input_x_val = np.random.randint(low=0, high=256, size=[1, 36, 36, 3]).astype(np.float32)  # NHWC\n        boxes_val = np.array([[0.2, 0.4, 1.2, 1.4]]).astype(np.float32)\n        box_ind_val = np.array([0]).astype(np.int32)\n        corp_size_val = np.array([40, 40]).astype(np.int32)\n        self._run_test_case(func, [_OUTPUT],\n                            {_INPUT: input_x_val, _INPUT1: boxes_val, _INPUT2: box_ind_val, _INPUT3: corp_size_val},\n                            rtol=1e-04, atol=1e-03)\n\n    def test_batch_to_space3d(self):\n        block_size = [2, 2]\n        crop = [[0, 1], [2, 1]]\n        input_val = np.random.random_sample([40, 3, 100]).astype(np.float32)\n        def func(x):\n            return batch_to_space_nd(x, block_size, crop, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val})\n\n    def test_space_to_batchnd(self):\n        block_size = [2, 2]\n        pad = [[0, 1], [2, 1]]\n        input_val = np.random.random_sample([40, 5, 7, 66]).astype(np.float32)\n        def func(x):\n            return space_to_batch_nd(x, block_size, pad, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val})\n\n        pad = [[0, 0], [1, 2]]\n        input_val = np.random.random_sample([10, 6, 7, 66]).astype(np.float32)\n        def func(x):\n            return space_to_batch_nd(x, block_size, pad, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val})\n\n    @check_opset_min_version(10, ""is_inf"")\n    def test_isinf(self):\n        x_types = [np.float32, np.float64]\n        for x_type in x_types:\n            x_val1 = np.array([1.0, -2.0, 3.0, -4.0], dtype=x_type)\n            x_val2 = np.array([np.inf, np.inf, np.inf, np.inf], dtype=x_type).reshape((2, 2))\n            x_val3 = np.array([1.0, np.inf, -3.0, np.inf, 5.0, np.inf, -7.0, np.inf], dtype=x_type).reshape((2, 2, 2))\n            for x_val in [x_val1, x_val2, x_val3]:\n                def func(x):\n                    x_ = is_inf(x)\n                    return tf.identity(x_, name=_TFOUTPUT)\n                self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""NonMaxSuppression"")\n    def test_non_max_suppression(self):\n        box_num = 10\n        boxes_val = np.random.random_sample([box_num, 4]).astype(np.float32)\n        scores_val = np.random.random_sample([box_num]).astype(np.float32)\n\n        def func(boxes, scores):\n            res1 = tf.image.non_max_suppression(boxes, scores, max_output_size=int(box_num / 2))\n            res2 = tf.image.non_max_suppression(boxes, scores, max_output_size=0)\n            return tf.identity(res1, name=_TFOUTPUT), tf.identity(res2, name=_TFOUTPUT1)\n\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: boxes_val, _INPUT1: scores_val})\n\n    @check_opset_min_version(11, ""NonMaxSuppressionV4"")\n    def test_non_max_suppression_v4(self):\n        box_num = 10\n        boxes_val = np.random.random_sample([box_num, 4]).astype(np.float32)\n        scores_val = np.random.random_sample([box_num]).astype(np.float32)\n\n        def func(boxes, scores):\n            ret1, ret2 = tf.image.non_max_suppression_padded(boxes, scores, max_output_size=int(box_num * 2),\n                                                             pad_to_max_output_size=True)\n            return tf.identity(ret1, name=_TFOUTPUT), tf.identity(ret2, name=_TFOUTPUT1)\n\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: boxes_val, _INPUT1: scores_val})\n\n    @check_opset_min_version(11, ""NonMaxSuppressionV4"")\n    def test_non_max_suppression_v4_no_padding(self):\n        box_num = 10\n        boxes_val = np.random.random_sample([box_num, 4]).astype(np.float32)\n        scores_val = np.random.random_sample([box_num]).astype(np.float32)\n\n        def func(boxes, scores):\n            ret1, ret2 = tf.image.non_max_suppression_padded(boxes, scores, max_output_size=int(box_num),\n                                                             pad_to_max_output_size=False)\n            return tf.identity(ret1, name=_TFOUTPUT), tf.identity(ret2, name=_TFOUTPUT1)\n\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: boxes_val, _INPUT1: scores_val})\n\n    @check_tf_min_version(""1.15"")\n    @check_opset_min_version(11, ""NonMaxSuppressionV5"")\n    def test_non_max_suppression_v5(self):\n        box_num = 10\n        boxes_val = np.random.random_sample([box_num, 4]).astype(np.float32)\n        scores_val = np.random.random_sample([box_num]).astype(np.float32)\n\n        def func(boxes, scores):\n            ret1, ret2 = tf.image.non_max_suppression_with_scores(boxes, scores, max_output_size=int(box_num / 2),\n                                                                  soft_nms_sigma=0.0)\n            return tf.identity(ret1, name=_TFOUTPUT), tf.identity(ret2, name=_TFOUTPUT1)\n\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: boxes_val, _INPUT1: scores_val})\n\n    def _conv1d_test(self, x_val, w, stride=None, padding=""VALID"", rtol=1e-07):\n        if stride is None:\n            stride = 1\n        def func(x):\n            kernel = tf.constant(w, dtype=tf.float32, name=\'k\')\n            conv = tf.nn.conv1d(x, kernel, stride=stride, padding=padding)\n            return tf.identity(conv, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, rtol=rtol)\n\n    def test_conv1d_1(self):\n        x_val = make_xval((1, 7, 1))\n        w = np.array([2., 1., 3.], dtype=np.float32).reshape(3, 1, 1)\n        self._conv1d_test(x_val, w)\n\n    def test_conv1d_2(self):\n        x_val = make_xval((1, 7, 1))\n        w = np.array([2., 1., 3.], dtype=np.float32).reshape(3, 1, 1)\n        self._conv1d_test(x_val, w, stride=2)\n\n    def test_conv1d_3(self):\n        x_val = make_xval((1, 7, 1))\n        w = np.array([2., 1., 3.], dtype=np.float32).reshape(3, 1, 1)\n        self._conv1d_test(x_val, w, padding=""SAME"")\n\n    def test_conv1d_4(self):\n        x_val = make_xval((1, 7, 1))\n        w = np.array([2., 1., 3.], dtype=np.float32).reshape(3, 1, 1)\n        self._conv1d_test(x_val, w, rtol=1e-05)\n\n    def test_conv1d_5(self):\n        x_val = make_xval((1, 7, 1))\n        w = np.array([3., 3., 3.], dtype=np.float32).reshape(3, 1, 1)\n        self._conv1d_test(x_val, w)\n\n    @check_opset_min_version(10, ""ThresholdedRelu"")\n    def test_thresholded_relu(self):\n        # tf.keras.layers.ThresholdedReLU only supports `float32` for x\n        x_val = np.array([0.0, 1.0, -1.0, 2.0, -2.0, 0.5, -0.5, 1.5, -1.5], dtype=np.float32).reshape((3, 3))\n        theta_vals = [-1.0, -0.5, 0.0, 0.5, 1.0]\n        for theta_val in theta_vals:\n            def func(x):\n                t = tf.keras.layers.ThresholdedReLU(theta=theta_val)\n                x_ = t.call(x)\n                return tf.identity(x_, name=_TFOUTPUT)\n            self._run_test_case(func, [_OUTPUT], {_INPUT: x_val},\n                                graph_validator=lambda g: check_op_count(g, ""ThresholdedRelu"", 1))\n\n    @check_tf_min_version(""1.13"")\n    @check_opset_min_version(8, ""MaxPoolWithArgmax"")\n    def test_maxpoolwithargmax(self):\n        for p in get_maxpoolwithargmax_getdata():\n            _, padding, x_shape, ksize, strides = p\n            x_val = make_xval(x_shape)\n            def func(x):\n                mp = tf.nn.max_pool_with_argmax(x, ksize, strides, padding=padding)\n                return tf.identity(mp[0], name=_TFOUTPUT), tf.identity(mp[1], name=_TFOUTPUT1)\n            self.logger.debug(str(p))\n            self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""Selu"")\n    def test_selu(self):\n        x_val = np.random.random_sample([3]).astype(np.float32)\n        def func(x):\n            y = tf.nn.selu(x)\n            return tf.identity(y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(8, ""ClipByValue (needs broadcast)"")\n    def test_clip_by_value(self):\n        # float32, dynamic min/max\n        x_val = np.arange(0, 24, dtype=np.float32).reshape([3, 8])\n        x_minval = np.array(8.5, dtype=np.float32)\n        x_maxval = np.array(16.5, dtype=np.float32)\n        def func(x, x_min, x_max):\n            y = tf.clip_by_value(x, x_min, x_max)\n            return tf.identity(y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: x_minval, _INPUT2: x_maxval})\n\n        # float32, const min/max\n        x_val = np.arange(0, 24, dtype=np.float32).reshape([3, 8])\n        def func(x):\n            y = tf.clip_by_value(x, 8.5, 16.5)\n            return tf.identity(y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n        # int32, converter needs to cast, const min/max\n        x_val = np.arange(0, 24, dtype=np.int32).reshape([3, 8])\n        def func(x):\n            y = tf.clip_by_value(x, 8, 16)\n            return tf.identity(y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    # test for gemm pattern0: alpha*A*B + beta*C\n    def test_gemm_pattern0(self):\n        max_number = 10\n        m = np.random.randint(max_number)\n        n = np.random.randint(max_number)\n        k = np.random.randint(max_number)\n        x_val1 = np.random.rand(m, n).astype(""float32"")\n        x_val2 = np.random.rand(n, k).astype(""float32"")\n        x_val3 = np.random.rand(m, k).astype(""float32"")\n        def func(a, b, c):\n            alpha = tf.constant(1.0, dtype=tf.float32)\n            beta = tf.constant(2.0, dtype=tf.float32)\n            mul1 = tf.multiply(alpha, tf.matmul(a, b))\n            mul2 = tf.multiply(beta, c)\n            x_ = mul1 + mul2\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2, _INPUT2: x_val3},\n                            graph_validator=lambda g: check_op_count(g, ""Gemm"", 1))\n\n    # test for gemm pattern1: alpha*A*B + C\n    def test_gemm_pattern1(self):\n        max_number = 10\n        m = np.random.randint(max_number)\n        n = np.random.randint(max_number)\n        k = np.random.randint(max_number)\n        x_val1 = np.random.rand(m, n).astype(""float32"")\n        x_val2 = np.random.rand(n, k).astype(""float32"")\n        x_val3 = np.random.rand(m, k).astype(""float32"")\n        def func(a, b, c):\n            alpha = tf.constant(1.0, dtype=tf.float32)\n            x_ = tf.multiply(alpha, tf.matmul(a, b)) + c\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2, _INPUT2: x_val3},\n                            graph_validator=lambda g: check_op_count(g, ""Gemm"", 1))\n\n    # test for gemm pattern2: A*B + beta*C\n    def test_gemm_pattern2(self):\n        max_number = 10\n        m = np.random.randint(max_number)\n        n = np.random.randint(max_number)\n        k = np.random.randint(max_number)\n        x_val1 = np.random.rand(m, n).astype(""float32"")\n        x_val2 = np.random.rand(n, k).astype(""float32"")\n        x_val3 = np.random.rand(m, k).astype(""float32"")\n        def func(a, b, c):\n            beta = tf.constant(2.0, dtype=tf.float32)\n            x_ = tf.matmul(a, b) + tf.multiply(beta, c)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2, _INPUT2: x_val3},\n                            graph_validator=lambda g: check_op_count(g, ""Gemm"", 1))\n\n    # test for gemm pattern3: A*B + C\n    def test_gemm_pattern3(self):\n        max_number = 10\n        m = np.random.randint(max_number)\n        n = np.random.randint(max_number)\n        k = np.random.randint(max_number)\n        x_val1 = np.random.rand(m, n).astype(""float32"")\n        x_val2 = np.random.rand(n, k).astype(""float32"")\n        x_val3 = np.random.rand(m, k).astype(""float32"")\n        def func(a, b, c):\n            x_ = tf.matmul(a, b) + c\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2, _INPUT2: x_val3},\n                            graph_validator=lambda g: check_op_count(g, ""Gemm"", 1))\n\n    # test for gemm pattern0: alpha*A*B + beta*C\n    @check_opset_min_version(12, ""Optimizer bug in ORT 1.2"")\n    def test_gemm_pattern0_fail_broadcast(self):\n        # shapes (3, 3) * (3, 1) + (1, 4) => (3, 1) + (1, 4)\n        # c not uni-broadcastable to a * b, so should not use GEMM\n        m, n, k = 3, 3, 1\n        x_val1 = np.random.rand(m, n).astype(""float32"")\n        x_val2 = np.random.rand(n, k).astype(""float32"")\n        x_val3 = np.random.rand(k, 4).astype(""float32"")\n\n        def func(a, b, c):\n            alpha = tf.constant(1.0, dtype=tf.float32)\n            beta = tf.constant(2.0, dtype=tf.float32)\n            mul1 = tf.multiply(alpha, tf.matmul(a, b))\n            mul2 = tf.multiply(beta, c)\n            x_ = mul1 + mul2\n            return tf.identity(x_, name=_TFOUTPUT)\n\n        def graph_validator(g):\n            if \'Gemm\' in [n.type for n in g.get_nodes()]: return False\n            return True\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val1, _INPUT1: x_val2, _INPUT2: x_val3},\n                            graph_validator=graph_validator)\n\n    def test_graph_matcher(self):\n        shape = [2, 6]\n        x_val = np.random.random(shape).astype(np.float32)\n        y_val = np.random.random(shape).astype(np.float32)\n        z_val = np.random.random(shape).astype(np.float32)\n        def func(x, y, z):\n            tmp1 = x + y\n            tmp2 = x - y\n            tmp3 = tf.multiply(tmp1, z)\n            tmp4 = tf.multiply(tmp2, z)\n            return tf.add(tmp4, tmp3, name=_TFOUTPUT)\n\n        onnx_graph = self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val, _INPUT2: z_val})\n        pattern = \\\n            OpTypePattern(\'Add\', name=\'output\', inputs=[\n                OpTypePattern(\'Mul\', inputs=[\n                    OpTypePattern(\'Add\', name=\'input1\'),\n                    OpTypePattern(\'*\', name=\'input2\')]),\n                OpTypePattern(\'Mul\', inputs=[\n                    OpTypePattern(\'Sub\', name=\'input1\'),\n                    OpTypePattern(\'*\', name=\'input2\')])])\n\n        matcher = GraphMatcher(pattern, allow_reorder=False)\n        match_results = list(matcher.match_ops(onnx_graph.get_nodes()))\n        self.assertTrue(len(match_results) == 0)\n        matcher = GraphMatcher(pattern, allow_reorder=True)\n        match_results = list(matcher.match_ops(onnx_graph.get_nodes()))\n        self.assertTrue(len(match_results) == 1)\n\n    def test_add2(self):\n        x_val = np.array([1.0, 2.0, -3.0, -4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.add(x, x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(11, ""CumSum"")\n    def test_cumsum(self):\n        x_val = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32).reshape((2, 2))\n        def func(x):\n            x_ = tf.cumsum(x, axis=1)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(11, ""CumSum"")\n    def test_cumsum_axis1_reverse_exclusive(self):\n        x_val = np.array([1., 2., 3., 4.,\n                          5., 6., 7., 8.,\n                          9., 10., 11., 12.,\n                          13., 14., 15., 16.,\n                          17., 18., 19., 20.,\n                          21., 22., 23., 24.], dtype=np.float32).reshape((2, 3, 4))\n        def func(x):\n            x_ = tf.cumsum(x, axis=1, reverse=True)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(11, ""Round"")\n    def test_round(self):\n        x_val = np.array([-0.7, -0.5, -0.0, 0.0, +0.0, 0.3, 0.5, 0.7, float(\'nan\')], dtype=np.float32)\n        def func(x):\n            x_ = tf.round(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(11, ""Det"")\n    @unittest.skip(""unclear how this is called in tf-2, fix later"")\n    def test_determinant(self):\n        x_val = np.array([1., 2., 3., 4., 1., 2.,\n                          2., 1., 1., 3., 3., 1.,\n                          1., 2., 3., 4., 1., 2.,\n                          2., 1., 1., 3., 3., 1.],\n                         dtype=np.float32).reshape((1, 2, 3, 2, 2))\n        def func(x):\n            x_ = tf.matrix_determinant(x)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(11, ""BitShift"")\n    def test_bitshift_left(self):\n        x_val = np.array([16, 4, 1], dtype=np.int32)\n        y_val = np.array([1, 2, 3], dtype=np.int32)\n        def func(x, y):\n            x_ = tf.bitwise.left_shift(x, y)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(11, ""BitShift"")\n    def test_bitshift_right(self):\n        info = np.iinfo(np.int32)\n        x_val = np.array([-1, 0, 1, info.max, info.min], dtype=np.int32)\n        def func(x):\n            x_ = tf.bitwise.right_shift(x, 1)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(11, ""ScatterND"")\n    def test_scatternd_1d(self):\n        x_val = np.array([4, 3, 1, 7], dtype=np.int32).reshape((4, 1))\n        y_val = np.array([9, 10, 11, 12], dtype=np.int64).reshape((4))\n        z_val = np.array([8], dtype=np.int32).reshape(1)\n\n        def func(x, y, z):\n            x_ = tf.scatter_nd(x, y, z)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val, _INPUT2: z_val})\n\n    @check_opset_min_version(11, ""ScatterND"")\n    def test_scatternd_3d(self):\n        x_val = np.array([0, 2], dtype=np.int32).reshape((2, 1))\n        y_val = np.array([[[5, 5, 5, 5], [6, 6, 6, 6],\n                           [7, 7, 7, 7], [8, 8, 8, 8]],\n                          [[5, 5, 5, 5], [6, 6, 6, 6],\n                           [7, 7, 7, 7], [8, 8, 8, 8]]], dtype=np.float32).reshape((2, 4, 4))\n        z_val = np.array([4, 4, 4], dtype=np.int32).reshape(3)\n\n        def func(x, y, z):\n            x_ = tf.scatter_nd(x, y, z)\n            return tf.identity(x_, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val, _INPUT2: z_val})\n\n    @check_opset_min_version(11, ""Unique"")\n    def test_unique(self):\n        x_val = np.array([1, 1, 2, 4, 4, 4, 7, 8, 8], dtype=np.float32)\n        def func(x):\n            x1_, _ = tf.unique(x)\n            y1 = tf.identity(x1_, name=_TFOUTPUT)\n            return y1\n            # FIXME: indices in onnx are not the same as in tensorflow so don\'t check for now\n            #self._run_test_case([_OUTPUT, _OUTPUT1], {_INPUT: x_val})\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(10, ""Conv2DBackpropInput"")\n    def test_Conv2DBackpropInput_const(self):\n        input_sizes_val_ = np.array([1, 10, 10, 3], dtype=np.int32)\n        filter_val_ = np.random.randint(low=0, high=256, size=[3, 3, 3, 5])\n        out_backprop_val_ = np.random.randint(low=0, high=256, size=[1, 10, 10, 5])\n        def func():\n            input_sizes_val = tf.constant(input_sizes_val_, dtype=tf.int32)\n            filter_val = tf.constant(filter_val_, dtype=tf.float32)\n            out_backprop_val = tf.constant(out_backprop_val_, dtype=tf.float32)\n            return conv2d_backprop_input(input_sizes=input_sizes_val, filter=filter_val,\n                                         out_backprop=out_backprop_val, strides=[1, 1, 1, 1],\n                                         padding=\'SAME\', name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {})\n\n    @check_opset_min_version(10, ""Conv2DBackpropInput"")\n    def test_Conv2DBackpropInput_const_strided(self):\n        input_sizes_val_ = np.array([1, 10, 10, 3], dtype=np.int32)\n        filter_val_ = np.random.randint(low=0, high=256, size=[3, 3, 3, 5])\n        out_backprop_val_ = np.random.randint(low=0, high=256, size=[1, 5, 5, 5])\n\n        def func():\n            input_sizes_val = tf.constant(input_sizes_val_, dtype=tf.int32)\n            filter_val = tf.constant(filter_val_, dtype=tf.float32)\n            out_backprop_val = tf.constant(out_backprop_val_, dtype=tf.float32)\n            return conv2d_backprop_input(input_sizes=input_sizes_val, filter=filter_val,\n                                         out_backprop=out_backprop_val, strides=[1, 2, 2, 1],\n                                         padding=\'SAME\', name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {})\n\n    @check_opset_min_version(10, ""Conv2DBackpropInput"")\n    def test_Conv2DBackpropInput_const_valid(self):\n        input_sizes_val_ = np.array([1, 12, 12, 3], dtype=np.int32)\n        filter_val_ = np.random.randint(low=0, high=256, size=[3, 3, 3, 5])\n        out_backprop_val_ = np.random.randint(low=0, high=256, size=[1, 10, 10, 5])\n        def func():\n            input_sizes_val = tf.constant(input_sizes_val_, dtype=tf.int32)\n            filter_val = tf.constant(filter_val_, dtype=tf.float32)\n            out_backprop_val = tf.constant(out_backprop_val_, dtype=tf.float32)\n            return conv2d_backprop_input(input_sizes=input_sizes_val, filter=filter_val,\n                                         out_backprop=out_backprop_val, strides=[1, 1, 1, 1],\n                                         padding=\'VALID\', name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {})\n\n    @check_opset_min_version(10, ""Conv2DBackpropInput"")\n    def test_Conv2DBackpropInput(self):\n        def func(input_sizes, filters, out_backprop):\n            return conv2d_backprop_input(input_sizes, filters, out_backprop, strides=[1, 1, 1, 1],\n                                         padding=\'SAME\', name=_TFOUTPUT)\n        filters_val = np.random.randint(low=0, high=256, size=[3, 3, 3, 5]).astype(np.float32)\n        out_backprop_val = np.random.randint(low=0, high=256, size=[1, 10, 10, 5]).astype(np.float32)\n        input_sizes_val = np.array([1, 10, 10, 3], dtype=np.int32)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_sizes_val, _INPUT1: filters_val, _INPUT2: out_backprop_val})\n\n    @check_opset_min_version(10, ""Conv2DBackpropInput"")\n    def test_Conv2DBackpropInput_strided(self):\n        def func(input_sizes, filters, out_backprop):\n            return conv2d_backprop_input(input_sizes, filters, out_backprop, strides=[1, 2, 2, 1], padding=\'SAME\',\n                                         name=_TFOUTPUT)\n        input_sizes_val = np.array([1, 10, 10, 3], dtype=np.int32)\n        filters_val = np.random.randint(low=0, high=256, size=[3, 3, 3, 5]).astype(np.float32)\n        out_backprop_val = np.random.randint(low=0, high=256, size=[1, 5, 5, 5]).astype(np.float32)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_sizes_val, _INPUT1: filters_val, _INPUT2: out_backprop_val})\n\n    @check_opset_min_version(10, ""Conv2DBackpropInput"")\n    def test_Conv2DBackpropInput_valid(self):\n        def func(input_sizes, filters, out_backprop):\n            return conv2d_backprop_input(input_sizes, filters, out_backprop, strides=[1, 1, 1, 1],\n                                         padding=\'VALID\', name=_TFOUTPUT)\n        input_sizes_val = np.array([1, 12, 12, 3], dtype=np.int32)\n        filters_val = np.random.randint(low=0, high=256, size=[3, 3, 3, 5]).astype(np.float32)\n        out_backprop_val = np.random.randint(low=0, high=256, size=[1, 10, 10, 5]).astype(np.float32)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_sizes_val, _INPUT1: filters_val, _INPUT2: out_backprop_val})\n\n    @check_opset_min_version(8, ""CategoryMapper"")\n    @skip_tf2()\n    def test_hashtable_lookup(self):\n        filnm = ""vocab.tmp""\n        words = [""apple"", ""pear"", ""banana"", ""cherry"", ""grape""]\n        query = np.array([\'cherry\'], dtype=np.object)\n        with open(filnm, ""w"") as f:\n            for word in words:\n                f.write(word + ""\\n"")\n        def func(query_holder):\n            hash_table = lookup_ops.index_table_from_file(filnm)\n            lookup_results = hash_table.lookup(query_holder)\n            ret = tf.add(lookup_results, 0, name=_TFOUTPUT)\n            return ret\n        self._run_test_case(func, [_OUTPUT], {_INPUT: query}, constant_fold=False)\n        os.remove(filnm)\n\n    @check_opset_min_version(11)\n    def test_matrix_diag_part(self):\n        input_vals = [\n            np.array([[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]], dtype=np.int64),\n            np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]], dtype=np.int64),\n            np.array([[[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]],\n                      [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]]], dtype=np.int64)]\n\n        def func(input_holder):\n            return matrix_diag_part(input_holder, name=_TFOUTPUT)\n\n        for input_val in input_vals:\n            self._run_test_case(func, [_OUTPUT], {_INPUT: input_val})\n\n    @check_opset_min_version(8)\n    def test_broadcast(self):\n        input_tensor_val = np.random.randint(low=0, high=256, size=[2, 3]).astype(np.float32)\n        new_shape_val = np.array([3, 2, 3]).astype(np.int64)\n\n        def func(input_tensor, new_shape):\n            return tf.broadcast_to(input_tensor, new_shape, _TFOUTPUT)\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_tensor_val, _INPUT1: new_shape_val})\n\n    def test_bfloat(self):\n        x_val = np.array([0, 1, 2], dtype=np.float32)\n        y_val = np.array([3, 4, 5], dtype=np.float32)\n        def func(x, y):\n            x_ = tf.cast(x, tf.bfloat16)\n            y_ = tf.cast(y, tf.bfloat16)\n            s_ = tf.add(x_, y_)\n            return tf.cast(s_, tf.float32, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(11)\n    @check_tf_min_version(""2.2"")\n    def test_matrix_diag_part_v3(self):\n\n        def func(X, K):\n            v2 = tf.raw_ops.MatrixDiagPartV2(input=X, k=K, padding_value=0.123, name=_TFOUTPUT)\n            v3 = tf.raw_ops.MatrixDiagPartV3(input=X, k=K, padding_value=0.123, align=\'LEFT_RIGHT\', name=_TFOUTPUT1)\n            return v2, v3\n\n        for x_shape in ([4, 5], [2, 3, 4, 5], [5, 4], [7, 5]):\n            x_val = np.random.random(x_shape).astype(np.float32)\n            for raw_k in ([0], [1], [3], [-1], [-3], [1, 2], [-2, -1], [-1, 1]):\n                k_val = np.array(raw_k).astype(np.int32)\n                self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: x_val, _INPUT1: k_val})\n\n    @test_ms_domain()\n    def test_inverse(self, extra_opset):\n        # this depends on onnx Inverse which was removed from opset-12 but does exists in the ms-domain\n        x_val = np.random.random([5, 5]).astype(np.float32)\n        def func(x):\n            return tf.linalg.inv(x, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val}, process_args={""extra_opset"": [extra_opset]})\n\n    @check_opset_min_version(12)\n    def test_squared_distance(self):\n        x_val = np.random.random([4, 5]).astype(np.float32)\n        y_val = np.random.random([4, 5]).astype(np.float32)\n        def func(x, y):\n            return tf.math.squared_difference(x, y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(12)\n    @check_tf_min_version(""2.1"")\n    def test_einsum(self):\n        x_val = np.random.random([10]).astype(np.float32)\n        y_val = np.random.random([10]).astype(np.float32)\n        def func(x, y):\n            ret = tf.einsum(""i,j->ij"", x, y)\n            return tf.identity(ret, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_opset_min_version(7)\n    def test_compare(self):\n        x_val = np.random.random([10, 20]).astype(np.float32)\n        y_val = np.random.random([10, 20]).astype(np.float32)\n        def func(x, y):\n            return tf.math.less_equal(x, y, name=_TFOUTPUT), \\\n                   tf.math.greater_equal(x, y, name=_TFOUTPUT1)\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: x_val, _INPUT1: y_val})\n\n    @check_tf_min_version(""1.14"", ""required for tf.math.is_finite"")\n    @check_opset_min_version(10)\n    def test_is_finite(self):\n        x_val = np.array([5.0, 4.8, 6.8, np.inf, np.nan], dtype=np.float32)\n        def func(x):\n            y = tf.math.is_finite(x)\n            return tf.identity(y, name=_TFOUTPUT)\n        self._run_test_case(func, [_OUTPUT], {_INPUT: x_val})\n\n    @check_opset_min_version(12)\n    @check_tf_min_version(""2.2"")\n    def test_matrix_diag_v3_multi_dim(self):\n        raw_diag = [[[1.0, 2.0, 3.0],\n                     [4.0, 5.0, 6.0],\n                     [7.0, 8.0, 9.0]],\n                    [[10.0, 11.0, 12.0],\n                     [13.0, 14.0, 15.0],\n                     [16.0, 17.0, 18.0]]]\n        diag_val = np.array(raw_diag).astype(np.float32)\n        k_val = np.array([-1, 1]).astype(np.int32)\n        row_val = np.array(-1).astype(np.int32)\n        col_val = np.array(-1).astype(np.int32)\n\n        def func(diag, k, row, col):\n            return tf.raw_ops.MatrixDiagV3(diagonal=diag, k=k, num_rows=row, num_cols=col,\n                                           padding_value=0.123, align=\'RIGHT_RIGHT\', name=_TFOUTPUT), \\\n                   tf.raw_ops.MatrixDiagV2(diagonal=diag, k=k, num_rows=row, num_cols=col,\n                                           padding_value=0.123, name=_TFOUTPUT1)\n\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1], {_INPUT: diag_val, _INPUT1: k_val,\n                                                        _INPUT2: row_val, _INPUT3: col_val})\n\n    @check_opset_min_version(12)\n    @check_tf_min_version(""2.2"")\n    def test_matrix_diag_v3_multi_dim_min_row(self):\n        raw_diag = [[[1.0, 2.0, 3.0],\n                     [4.0, 5.0, 6.0]],\n                    [[7.0, 8.0, 9.0],\n                     [10.0, 11.0, 12.0]]]\n        diag_val = np.array(raw_diag).astype(np.float32)\n        k_val = np.array([2, 3]).astype(np.int32)\n        row_val = np.array(-1).astype(np.int32)\n        col_val = np.array(6).astype(np.int32)\n\n        def func(diag, k, row, col):\n            return tf.raw_ops.MatrixDiagV3(diagonal=diag, k=k, num_rows=row, num_cols=col,\n                                           padding_value=0.456, align=\'LEFT_LEFT\', name=_TFOUTPUT)\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: diag_val, _INPUT1: k_val,\n                                              _INPUT2: row_val, _INPUT3: col_val})\n\n    @check_opset_min_version(12)\n    @check_tf_min_version(""2.2"")\n    def test_matrix_diag_v3_single_dim_min_col(self):\n        raw_diag = [1.0, 2.0, 3.0]\n        diag_val = np.array(raw_diag).astype(np.float32)\n        k_val = np.array(-1).astype(np.int32)\n        row_val = np.array(5).astype(np.int32)\n        col_val = np.array(-1).astype(np.int32)\n\n        def func(diag, k, row, col):\n            return tf.raw_ops.MatrixDiagV3(diagonal=diag, k=k, num_rows=row, num_cols=col,\n                                           padding_value=0.789, align=\'LEFT_RIGHT\', name=_TFOUTPUT)\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: diag_val, _INPUT1: k_val,\n                                              _INPUT2: row_val, _INPUT3: col_val})\n\n    @check_opset_min_version(12)\n    @check_tf_min_version(""2.2"")\n    def test_matrix_diag_v3_2single_dim_row_col(self):\n        raw_diag = [[1, 2, 3], [4, 5, 6]]\n        diag_val = np.array(raw_diag).astype(np.int64)\n        k_val = np.array(0).astype(np.int32)\n        row_val = np.array(3).astype(np.int32)\n        col_val = np.array(4).astype(np.int32)\n\n        def func(diag, k, row, col):\n            return tf.raw_ops.MatrixDiagV3(diagonal=diag, k=k, num_rows=row, num_cols=col,\n                                           padding_value=7, align=\'LEFT_RIGHT\', name=_TFOUTPUT), \\\n                   tf.raw_ops.MatrixDiag(diagonal=diag, name=_TFOUTPUT1)\n\n        self._run_test_case(func, [_OUTPUT, _OUTPUT1],\n                            {_INPUT: diag_val, _INPUT1: k_val,\n                             _INPUT2: row_val, _INPUT3: col_val})\n\n    @check_opset_min_version(12)\n    @check_tf_min_version(""2.2"")\n    def test_matrix_diag_v3_1single_dim_row_col(self):\n        raw_diag = [1, 2, 3, 4, 5]\n        diag_val = np.array(raw_diag).astype(np.int64)\n        k_val = np.array(0).astype(np.int32)\n        row_val = np.array(5).astype(np.int32)\n        col_val = np.array(10).astype(np.int32)\n\n        def func(diag, k, row, col):\n            return tf.raw_ops.MatrixDiagV3(diagonal=diag, k=k, num_rows=row, num_cols=col,\n                                           padding_value=7, align=\'LEFT_RIGHT\', name=_TFOUTPUT)\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: diag_val, _INPUT1: k_val,\n                                              _INPUT2: row_val, _INPUT3: col_val})\n\n    @check_opset_min_version(12)\n    @check_tf_min_version(""2.2"")\n    def test_matrix_set_diag_v3(self):\n        input_val = np.array([[[7, 7, 7, 7],\n                               [7, 7, 7, 7],\n                               [7, 7, 7, 7]],\n                              [[7, 7, 7, 7],\n                               [7, 7, 7, 7],\n                               [7, 7, 7, 7]]]).astype(np.int64)\n        diag_val = np.array([[1, 2, 3],\n                             [4, 5, 6]]).astype(np.int64)\n        k_val = np.array([0]).astype(np.int32)\n\n        def func(base_matrix, diag, k):\n            return tf.raw_ops.MatrixSetDiagV3(input=base_matrix, diagonal=diag, k=k, align=\'RIGHT_LEFT\', name=_TFOUTPUT)\n\n        self._run_test_case(func, [_OUTPUT], {_INPUT: input_val, _INPUT1: diag_val, _INPUT2: k_val})\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_cond.py,59,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for tf.cond and tf.case.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_opset_min_version, check_tf_min_version\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test\n# pylint: disable=abstract-method,arguments-differ\n\nclass CondTests(Tf2OnnxBackendTestBase):\n\n    def test_simple_cond(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            x = x + 1\n            y = y + 1\n            res = tf.cond(x[0] < y[0], lambda: x + y, lambda: x - y, name=""test_cond"")\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_cond_with_const_branch(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            true_const = tf.constant(True, name=""true_const"", dtype=tf.bool)\n\n            def cond_graph():\n                return tf.constant(np.array([2, 1, 3], dtype=np.float32), name=""b"", dtype=tf.float32)\n\n            res = tf.cond(true_const, lambda: x + y, cond_graph, name=""test_cond"")\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_cond_with_multi_merge(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            x = x + 1\n            y = y + 1\n            res = tf.cond(x[0] < y[0], lambda: [x, x + y], lambda: [x, x - y], name=""test"")\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_cond_with_replicate_output(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            x = x + 1\n            y = y + 1\n            res = tf.cond(x[0] < y[0], lambda: [x, y], lambda: [y, x], name=""test_cond"")\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_nest_cond(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            x = x + 1\n            y = y + 1\n            def cond_graph():\n                def cond_graph1():\n                    def cond_graph2():\n                        return tf.cond(x[0] < y[0], lambda: x + y, lambda: tf.square(y))\n                    return tf.cond(tf.reduce_any(x < y), cond_graph2, cond_graph2)\n                return tf.cond(x[0] > y[0], cond_graph1, cond_graph1)\n\n            res = tf.cond(x[0] < y[0], cond_graph, cond_graph, name=""test_cond"")\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_while_loop_in_cond(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            def true_fn():\n                return [x]\n            def false_fn():\n                # b = tf.constant(np.array([0], dtype=np.int32), dtype=tf.int32)\n                # while_loop\n                c = lambda y: tf.reduce_any(tf.less(y, 10))\n                b = lambda i: tf.add(y, 1)\n                return tf.while_loop(c, b, [y])\n\n            res = tf.cond(x[0] < y[0], true_fn, false_fn, name=""test_cond"")\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_cond_in_while_loop(self):\n        def func(i, inputs):\n            inputs_2 = tf.identity(inputs)\n            input_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_2)\n            output_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n\n            c = lambda i, *_: tf.logical_and(tf.less(i, 10), i >= 0)\n\n            def b(i, out_ta):\n                new_i = tf.add(i, 1)\n                x = input_ta.read(i)\n                x = tf.cond(x > 0, lambda: x - 1, lambda: x + 3)\n                out_ta_new = out_ta.write(i, x)\n                return new_i, out_ta_new\n\n            i_final, out_final = tf.while_loop(c, b, [i, output_ta])\n            return tf.identity(i_final, name=""i""), tf.identity(out_final.stack(), name=""output_ta"")\n\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        feed_dict = {""input_1:0"": np.array(0, dtype=np.int32),\n                     ""input_2:0"": np.array([2.0, 16.0, 5.0, 1.6, 5.0, 6.0, 7.0, 8.0, 9.0, 10.], dtype=np.float32)}\n\n        output_names_with_port = [""i:0"", ""output_ta:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    def test_simple_case(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            x = tf.add(x, 1, name=""add_x"")\n            y = tf.add(y, 1, name=""add_y"")\n            res = tf.case([(tf.reduce_all(x < 1, name=""red1""), lambda: x + y),\n                           (tf.reduce_all(y > 0, name=""red2""), lambda: tf.square(y))],\n                          default=lambda: x, name=""test_case"")\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_case_with_exclusive(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            x = x + 1\n            y = y + 1\n            res = tf.case([(tf.reduce_all(x < 1), lambda: x + y), (tf.reduce_all(y > 0), lambda: tf.square(y))],\n                          default=lambda: x, name=""test_case"", exclusive=True)\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_case_without_default_branch(self):\n        def func(x, y):\n            x = tf.add(x, 1, name=""add_x"")\n            y = tf.add(y, 1, name=""add_y"")\n            res = tf.case([(tf.reduce_all(x < 1), lambda: x + y),\n                           (tf.reduce_all(y > 0), lambda: tf.square(y))])\n            return tf.identity(res, name=""output"")\n\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_case_with_multi_merge(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            x = x + 1\n            y = y + 1\n            res = tf.case(\n                [(tf.reduce_all(x < 1), lambda: [x + y, x - y]),\n                 (tf.reduce_all(y > 0), lambda: [tf.abs(x), tf.square(y)])],\n                default=lambda: [x, y], name=""test_case""\n            )\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    def test_nest_case(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        def func(x, y):\n            x = x + 1\n            y = y + 1\n            def case_graph():\n                return tf.case(\n                    [(tf.reduce_all(x < 1), lambda: x + y), (tf.reduce_all(y > 0), lambda: tf.square(y))],\n                    default=lambda: x - y,\n                    name=""test_case"")\n            res = tf.case([(x[0] > 0, case_graph), (x[0] < 0, case_graph)], default=lambda: x - y)\n            return tf.identity(res, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port)\n\n    @check_tf_min_version(""1.8"", ""shape inference for Reshape op screws up"")\n    @check_opset_min_version(9, ""ConstantOfShape"")\n    def test_cond_with_different_output_shape(self):\n        input_shape = (10, 5, 20)\n        def func(inputs, shape):\n            # cheat onnx shape inference\n            inputs = tf.reshape(inputs, shape)\n\n            def pad_tensor(t, length):\n                """"""Pads the input tensor with 0s along the first dimension up to the length.\n\n                Args:\n                  t: the input tensor, assuming the rank is at least 1.\n                  length: a tensor of shape [1]  or an integer, indicating the first dimension\n                    of the input tensor t after padding, assuming length <= t.shape[0].\n\n                Returns:\n                  padded_t: the padded tensor, whose first dimension is length. If the length\n                    is an integer, the first dimension of padded_t is set to length\n                    statically.\n                """"""\n                t_rank = tf.rank(t)\n                t_shape = tf.shape(t)\n                t_d0 = t_shape[0]\n                pad_d0 = tf.expand_dims(length - t_d0, 0)\n                pad_shape = tf.cond(\n                    # shape is [3], depending on input shape\n                    tf.greater(t_rank, 1), lambda: tf.concat([pad_d0, t_shape[1:]], 0),\n                    # shape is always [1]\n                    lambda: tf.expand_dims(length - t_d0, 0))\n                padded_t = tf.concat([t, tf.zeros(pad_shape, dtype=t.dtype)], 0)\n                return padded_t\n\n            output = pad_tensor(inputs, 20)\n            return tf.identity(output, name=""output"")\n        input_names_with_port = [""input:0"", ""shape:0""]\n        feed_dict = {\n            ""input:0"": np.ones(input_shape, dtype=np.float32),\n            ""shape:0"": np.array(input_shape, dtype=np.int32)\n        }\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_const_fold.py,34,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit tests using onnx constant folding rewriters.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport tensorflow as tf\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument\n\n# pylint: disable=C0111\nclass ConstantFoldingTests(Tf2OnnxBackendTestBase):\n    def _run_test_case(self, func, output_names_with_port, feed_dict, **kwargs):\n        kwargs[""convert_var_to_const""] = False\n        kwargs[""constant_fold""] = False\n        return self.run_test_case(func, feed_dict, [], output_names_with_port, **kwargs)\n\n    def test_concat(self):\n        def func():\n            t1 = [[1, 2, 3], [4, 5, 6]]\n            t2 = [[7, 8, 9], [10, 11, 12]]\n            x_ = tf.concat([t1, t2], 0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n            y_ = tf.concat([t1, t2], 1)  # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]\n            z_ = tf.concat([t1, t2], -1)\n            return tf.identity(x_, name=""output_0""), tf.identity(y_, name=""output_1""), tf.identity(z_, name=""output_2"")\n        self._run_test_case(func, [""output_0:0"", ""output_1:0"", ""output_2:0""], {})\n\n    def test_range(self):\n        def func0():\n            start = tf.constant(3, dtype=tf.float32, name=\'start\')\n            limit = tf.constant(18, dtype=tf.float32, name=\'limit\')\n            delta = tf.constant(3, dtype=tf.float32, name=\'delta\')\n            x_ = tf.range(start, limit, delta)\n            return tf.identity(x_, name=""output_0"")\n        self._run_test_case(func0, [""output_0:0""], {})\n\n        def func1():\n            start = tf.constant(3, dtype=tf.float32, name=\'start\')\n            limit = tf.constant(1, dtype=tf.float32, name=\'limit\')\n            delta = tf.constant(-0.5, dtype=tf.float32, name=\'delta\')\n            x_ = tf.range(start, limit, delta)\n            return tf.identity(x_, name=""output_0"")\n        self._run_test_case(func1, [""output_0:0""], {})\n\n        def func2():\n            limit = tf.constant(5, dtype=tf.float32, name=\'limit\')\n            x_ = tf.range(limit)\n            return tf.identity(x_, name=""output_0"")\n        self._run_test_case(func2, [""output_0:0""], {})\n\n    @unittest.skip(""tensorflow op ListDiff is not supported"")\n    def test_bahdanau_attention_memory_layer_tensordot(self):\n        def func():\n            rank = tf.constant(3, dtype=tf.int32, name=\'rank\')\n            start = tf.constant(0, dtype=tf.int32, name=\'start\')\n            delta = tf.constant(1, dtype=tf.int32, name=\'delta\')\n            tensor_dot_range = tf.range(start, rank, delta)\n\n            axes = tf.constant([2], dtype=tf.int32, name=\'axes\')\n            ge_y = tf.constant(0, dtype=tf.int32, name=\'ge_y\')\n            ge = tf.greater_equal(axes, ge_y)\n            cast = tf.cast(ge, tf.int32)\n            mul_1 = tf.multiply(cast, axes)\n\n            less_y = tf.constant(0, dtype=tf.int32)\n            less = tf.less(axes, less_y)\n            cast_2 = tf.cast(less, tf.int32)\n            add_1 = tf.add(axes, rank)\n            mul_2 = tf.multiply(cast_2, add_1)\n\n            add_2 = tf.add(mul_1, mul_2)\n            out, _ = tf.setdiff1d(tensor_dot_range, add_2)\n            return tf.identity(out, name=""output_0"")\n        self._run_test_case(func, [""output_0:0""], {})\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_convert.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n"""""" Test convert.py """"""\n\nimport os\nimport sys\nimport unittest\n\nfrom tf2onnx import convert\n\n\ndef run_test_case(args):\n    """""" run case and clean up """"""\n    sys.argv = args\n    convert.main()\n    ret = os.path.exists(args[-1])\n    if ret:\n        os.remove(args[-1])\n    return ret\n\n\nclass Tf2OnnxConvertTest(unittest.TestCase):\n    """""" teat cases for convert.py """"""\n\n    def test_convert_saved_model(self):\n        """""" convert saved model """"""\n        self.assertTrue(run_test_case([\'\',\n                                       \'--saved-model\',\n                                       \'tests/models/regression/saved_model\',\n                                       \'--output\',\n                                       \'converted_saved_model.onnx\']))\n\n    def test_convert_graphdef(self):\n        """""" convert graphdef """"""\n        self.assertTrue(run_test_case([\'\',\n                                       \'--input\',\n                                       \'tests/models/regression/graphdef/frozen.pb\',\n                                       \'--inputs\',\n                                       \'X:0\',\n                                       \'--outputs\',\n                                       \'pred:0\',\n                                       \'--output\',\n                                       \'converted_graphdef.onnx\']))\n\n    def test_convert_checkpoint(self):\n        """""" convert checkpoint """"""\n        self.assertTrue(run_test_case([\'\',\n                                       \'--checkpoint\',\n                                       \'tests/models/regression/checkpoint/model.meta\',\n                                       \'--inputs\',\n                                       \'X:0\',\n                                       \'--outputs\',\n                                       \'pred:0\',\n                                       \'--output\',\n                                       \'converted_checkpoint.onnx\']))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_cudnn.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for cudnn.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import init_ops\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import check_tf_max_version, skip_tf_cpu, check_opset_min_version, unittest_main\n\n\nclass CudnnTests(Tf2OnnxBackendTestBase):\n    """""" test cudnn cases """"""\n    @check_tf_max_version(""1.15.0"", ""not supported in tf-2.0"")\n    @skip_tf_cpu(""only tf_gpu can run CudnnGPU"")\n    @check_opset_min_version(10, ""CudnnGRU"")\n    def test_cudnngru(self):\n        """""" test contrib cudnn gru """"""\n        seq_length = 3\n        batch_size = 5\n        input_size = 2\n        num_layers = 2\n        num_units = 2\n        num_dirs = 2\n        x_val = np.random.randint(0, 100, [seq_length, batch_size, input_size]).astype(np.float32)\n        h_val = np.random.randint(0, 100, [num_layers * num_dirs, batch_size, num_units]).astype(np.float32).reshape(\n            [num_layers * num_dirs, batch_size, num_units])\n\n        def func(x, h):\n            initializer = init_ops.constant_initializer(0.5)\n            cudnngru = tf.contrib.cudnn_rnn.CudnnGRU(num_layers, num_units, \'linear_input\', \'bidirectional\',\n                                                     kernel_initializer=initializer, bias_initializer=initializer)\n            cudnngru.build([seq_length, batch_size, input_size])\n            outputs = cudnngru.call(x, tuple([h]))\n            _ = tf.identity(outputs[0], name=\'output\')\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": h_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-05, atol=1e-04)\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_cudnn_compatible_gru.py,47,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for gru.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import variable_scope\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_gru_count, check_tf_max_version, check_opset_after_tf_version\nfrom tf2onnx.tf_loader import is_tf2\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,cell-var-from-loop\n\nif is_tf2():\n    MultiRNNCell = tf.compat.v1.nn.rnn_cell.MultiRNNCell\n    dynamic_rnn = tf.compat.v1.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.compat.v1.nn.bidirectional_dynamic_rnn\nelse:\n    GRUBlockCell = tf.contrib.rnn.GRUBlockCell\n    MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n    CudnnCompatibleGRUCell = tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell\n    dynamic_rnn = tf.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.nn.bidirectional_dynamic_rnn\n\n\n# TODO: as a workaround, set batch_size to 1 for now to bypass a onnxruntime bug, revert it when the bug is fixed\nclass CudnnCompatibleGRUTests(Tf2OnnxBackendTestBase):\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_single_dynamic_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # no scope\n            cell = CudnnCompatibleGRUCell(units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_multiple_dynamic_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            gru_output_list = []\n            gru_cell_state_list = []\n            # no scope\n            cell = CudnnCompatibleGRUCell(units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n            gru_output_list.append(outputs)\n            gru_cell_state_list.append(cell_state)\n\n            # given scope\n            cell = CudnnCompatibleGRUCell(units)\n            with variable_scope.variable_scope(""root1"") as scope:\n                outputs, cell_state = dynamic_rnn(\n                    cell,\n                    x,\n                    dtype=tf.float32,\n                    sequence_length=[4],\n                    scope=scope)\n            gru_output_list.append(outputs)\n            gru_cell_state_list.append(cell_state)\n\n            return tf.identity(gru_output_list, name=""output""), tf.identity(gru_cell_state_list, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06)\n        # graph_validator=lambda g: check_gru_count(g, 2))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_single_dynamic_gru_seq_length_is_const(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32,\n                sequence_length=[5])\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_single_dynamic_gru_seq_length_is_not_const(self):\n        for np_dtype in [np.int32, np.int64, np.float32]:\n            units = 5\n            batch_size = 1\n            x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n            x_val = np.stack([x_val] * batch_size)\n            y_val = np.array([5], dtype=np_dtype)\n\n            def func(x, seq_length):\n                initializer = init_ops.constant_initializer(0.5)\n                # no scope\n                cell = CudnnCompatibleGRUCell(\n                    units,\n                    kernel_initializer=initializer)\n                outputs, cell_state = dynamic_rnn(\n                    cell,\n                    x,\n                    dtype=tf.float32,\n                    sequence_length=tf.identity(seq_length))\n\n                return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n            feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n            input_names_with_port = [""input_1:0"", ""input_2:0""]\n            output_names_with_port = [""output:0"", ""cell_state:0""]\n            self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                               graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_single_dynamic_gru_placeholder_input(self):\n        units = 5\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * 1)\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n            # no scope\n            cell = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)  # by default zero initializer is used\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_single_dynamic_gru_ch_zero_state_initializer(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n            # no scope\n            cell = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n\n            # defining initial state\n            initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                initial_state=initial_state,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_single_dynamic_gru_random_weights(self):\n        hidden_size = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(-1.0, 1.0)\n\n            # no scope\n            cell = CudnnCompatibleGRUCell(\n                hidden_size,\n                kernel_initializer=initializer)\n\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.0001,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_single_dynamic_gru_random_weights2(self):\n        hidden_size = 128\n        batch_size = 1\n        x_val = np.random.randn(1, 133).astype(\'f\')\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(0.0, 1.0)\n            # no scope\n            cell = CudnnCompatibleGRUCell(\n                hidden_size,\n                kernel_initializer=initializer)\n\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.01,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_dynamic_gru_output_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            initializer = tf.random_uniform_initializer(-1.0, 1.0)\n            cell1 = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n\n            outputs, _ = dynamic_rnn(\n                cell1,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.0001,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_dynamic_gru_state_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(-1.0, 1.0)\n            cell1 = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n\n            _, cell_state = dynamic_rnn(\n                cell1,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=0.0001, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_dynamic_bigru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bigru, no scope\n            cell1 = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            cell2 = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_dynamic_bigru_output_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bigru, no scope\n            cell1 = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            cell2 = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, _ = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_dynamic_bigru_state_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bigru, no scope\n            cell1 = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            cell2 = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_dynamic_bidirectional_but_one_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bigru, no scope\n            cell = CudnnCompatibleGRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, cell_state = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_dynamic_bidirectional_but_one_gru_and_output_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # bigru, no scope\n            cell = CudnnCompatibleGRUCell(\n                units)\n            outputs, _ = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no CudnnCompatibleGRUCell in tf-2.x"")\n    def test_dynamic_bidirectional_but_one_gru_and_state_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n\n            # bigru, no scope\n            cell = CudnnCompatibleGRUCell(\n                units)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_custom_rnncell.py,69,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for custom rnns.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import init_ops\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import check_tf_min_version, check_opset_min_version, unittest_main, skip_opset, skip_tf2\nfrom tf2onnx.tf_loader import is_tf2\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test\n# pylint: disable=abstract-method,arguments-differ\n\nif is_tf2():\n    BasicLSTMCell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell\n    LSTMCell = tf.compat.v1.nn.rnn_cell.LSTMCell\n    GRUCell = tf.compat.v1.nn.rnn_cell.GRUCell\n    RNNCell = tf.compat.v1.nn.rnn_cell.RNNCell\n    MultiRNNCell = tf.compat.v1.nn.rnn_cell.MultiRNNCell\n    dynamic_rnn = tf.compat.v1.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.compat.v1.nn.bidirectional_dynamic_rnn\nelse:\n    LSTMBlockCell = tf.contrib.rnn.LSTMBlockCell\n    LSTMCell = tf.nn.rnn_cell.LSTMCell\n    GRUCell = tf.nn.rnn_cell.LSTMCell\n    RNNCell = tf.nn.rnn_cell.RNNCell\n    MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n    dynamic_rnn = tf.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.nn.bidirectional_dynamic_rnn\n\n\nclass CustomRnnCellTests(Tf2OnnxBackendTestBase):\n    @check_opset_min_version(8, ""Scan"")\n    @skip_tf2()\n    def test_single_dynamic_custom_rnn(self):\n        size = 5  # size of each model layer.\n        batch_size = 1\n        cell = GatedGRUCell(size)\n\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            xs, s = dynamic_rnn(cell=cell, dtype=tf.float32, inputs=x, time_major=False)\n            return tf.identity(xs, name=""output""), tf.identity(s, name=""final_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""final_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.1)\n\n    @check_opset_min_version(8, ""Scan"")\n    @skip_tf2()\n    def test_single_dynamic_custom_rnn_time_major(self):\n        size = 5  # size of each model layer.\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            cell = GatedGRUCell(size)\n            xs, s = dynamic_rnn(cell=cell, dtype=tf.float32, inputs=x, time_major=True)\n            return tf.identity(xs, name=""output""), tf.identity(s, name=""final_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""final_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.1)\n\n    @check_opset_min_version(8, ""Scan"")\n    @skip_tf2()\n    def test_single_dynamic_custom_rnn_with_seq_length(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = GatedGRUCell(units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32,\n                sequence_length=[4, 3, 4, 5, 2, 1])\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    @check_opset_min_version(8, ""Scan"")\n    @skip_tf2()\n    def test_single_dynamic_custom_rnn_with_non_const_seq_length(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        y_val = np.array([4, 3, 4, 5, 2, 1], dtype=np.int32)\n        def func(x, seq_length):\n            # no scope\n            cell = GatedGRUCell(units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32,\n                sequence_length=tf.identity(seq_length))\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_min_version(""1.8"")\n    @skip_tf2()\n    def test_attention_wrapper_const_encoder(self):\n        size = 5\n        time_step = 3\n        input_size = 4\n        attn_size = size\n        batch_size = 9\n        # shape  [batch size, time step, size]\n        # attention_state: usually the output of an RNN encoder.\n        # This tensor should be shaped `[batch_size, max_time, ...]`.\n        decoder_time_step = 6\n        x_val = np.random.randn(decoder_time_step, input_size).astype(\'f\')\n        x_val = np.stack([x_val] * batch_size)\n\n        attention_states = np.random.randn(batch_size, time_step, attn_size).astype(\'f\')\n        def func(x):\n            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(attn_size, attention_states)\n\n            match_input_fn = lambda curr_input, state: tf.concat([curr_input, state], axis=-1)\n            cell = LSTMCell(size)\n            match_cell_fw = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                                attention_mechanism,\n                                                                attention_layer_size=attn_size,\n                                                                cell_input_fn=match_input_fn,\n                                                                output_attention=False)\n            output, attr_state = dynamic_rnn(match_cell_fw, x, dtype=tf.float32)\n\n            return tf.identity(output, name=""output""), tf.identity(attr_state.cell_state, name=""final_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        output_names_with_port = [""output:0"", ""final_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.1)\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_min_version(""1.8"")\n    @skip_tf2()\n    def test_attention_wrapper_lstm_encoder(self):\n        size = 5\n        time_step = 3\n        input_size = 4\n        attn_size = size\n        batch_size = 9\n\n        # shape  [batch size, time step, size]\n        # attention_state: usually the output of an RNN encoder.\n        # This tensor should be shaped `[batch_size, max_time, ...]`\n        encoder_time_step = time_step\n        encoder_x_val = np.random.randn(encoder_time_step, input_size).astype(\'f\')\n        encoder_x_val = np.stack([encoder_x_val] * batch_size)\n        decoder_time_step = 6\n        decoder_x_val = np.random.randn(decoder_time_step, input_size).astype(\'f\')\n        decoder_x_val = np.stack([decoder_x_val] * batch_size)\n\n        def func(encoder_x, decoder_x):\n            encoder_cell = LSTMCell(size)\n            output, attr_state = dynamic_rnn(encoder_cell, encoder_x, dtype=tf.float32)\n            output_0 = tf.identity(output, name=""output_0"")\n            attention_states = output\n            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(attn_size,\n                                                                       attention_states)\n\n            match_input_fn = lambda curr_input, state: tf.concat([curr_input, state], axis=-1)\n            cell = LSTMCell(size)\n            match_cell_fw = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                                attention_mechanism,\n                                                                attention_layer_size=attn_size,\n                                                                cell_input_fn=match_input_fn,\n                                                                output_attention=False)\n\n            output, attr_state = dynamic_rnn(match_cell_fw, decoder_x, dtype=tf.float32)\n\n            return output_0, tf.identity(output, name=""output""), tf.identity(attr_state.cell_state, name=""final_state"")\n\n        feed_dict = {""input_1:0"": encoder_x_val, ""input_2:0"": decoder_x_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output_0:0"", ""output:0"", ""final_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.1)\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_min_version(""1.8"")\n    @skip_tf2()\n    def test_attention_wrapper_gru_encoder(self):\n        size = 5\n        time_step = 3\n        input_size = 4\n        attn_size = size\n        batch_size = 9\n\n        # shape  [batch size, time step, size]\n        # attention_state: usually the output of an RNN encoder.\n        # This tensor should be shaped `[batch_size, max_time, ...]`\n        encoder_time_step = time_step\n        encoder_x_val = np.random.randn(encoder_time_step, input_size).astype(\'f\')\n        encoder_x_val = np.stack([encoder_x_val] * batch_size)\n        decoder_time_step = 6\n        decoder_x_val = np.random.randn(decoder_time_step, input_size).astype(\'f\')\n        decoder_x_val = np.stack([decoder_x_val] * batch_size)\n\n        def func(encoder_x, decoder_x):\n            encoder_cell = GRUCell(size)\n            output, attr_state = dynamic_rnn(encoder_cell, encoder_x, dtype=tf.float32)\n            _ = tf.identity(output, name=""output_0"")\n            attention_states = output\n            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(attn_size,\n                                                                       attention_states)\n\n            match_input_fn = lambda curr_input, state: tf.concat([curr_input, state], axis=-1)\n            cell = GRUCell(size)\n            match_cell_fw = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                                attention_mechanism,\n                                                                attention_layer_size=attn_size,\n                                                                cell_input_fn=match_input_fn,\n                                                                output_attention=False)\n            output, attr_state = dynamic_rnn(match_cell_fw, decoder_x, dtype=tf.float32)\n            return tf.identity(output, name=""output""), tf.identity(attr_state.cell_state, name=""final_state"")\n\n        feed_dict = {""input_1:0"": encoder_x_val, ""input_2:0"": decoder_x_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output_0:0"", ""output:0"", ""final_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.1)\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_min_version(""1.8"")\n    @skip_tf2()\n    def test_attention_wrapper_lstm_encoder_input_has_none_dim(self):\n        size = 5\n        time_step = 3\n        input_size = 4\n        attn_size = size\n        batch_size = 9\n\n        # shape  [batch size, time step, size]\n        # attention_state: usually the output of an RNN encoder.\n        # This tensor should be shaped `[batch_size, max_time, ...]`\n        encoder_time_step = time_step\n        encoder_x_val = np.random.randn(encoder_time_step, input_size).astype(\'f\')\n        encoder_x_val = np.stack([encoder_x_val] * batch_size)\n        decoder_time_step = 6\n        decoder_x_val = np.random.randn(decoder_time_step, input_size).astype(\'f\')\n        decoder_x_val = np.stack([decoder_x_val] * batch_size)\n\n        def func(encoder_x, decoder_x):\n            encoder_cell = LSTMCell(size)\n            output, attr_state = dynamic_rnn(encoder_cell, encoder_x, dtype=tf.float32)\n            _ = tf.identity(output, name=""output_0"")\n            attention_states = output\n            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(attn_size,\n                                                                       attention_states)\n\n            match_input_fn = lambda curr_input, state: tf.concat([curr_input, state], axis=-1)\n            cell = LSTMCell(size)\n            match_cell_fw = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                                attention_mechanism,\n                                                                attention_layer_size=attn_size,\n                                                                cell_input_fn=match_input_fn,\n                                                                output_attention=False)\n\n            output, attr_state = dynamic_rnn(match_cell_fw, decoder_x, dtype=tf.float32)\n\n            return tf.identity(output, name=""output""), tf.identity(attr_state.cell_state, name=""final_state"")\n\n        feed_dict = {""input_1:0"": encoder_x_val, ""input_2:0"": decoder_x_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output_0:0"", ""output:0"", ""final_state:0""]\n\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.1)\n\n    @check_opset_min_version(8, ""Scan"")\n    @skip_tf2()\n    def test_multi_rnn_lstm(self, state_is_tuple=True):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            cell_0 = LSTMCell(units,\n                              initializer=initializer,\n                              state_is_tuple=state_is_tuple)\n\n            cell_1 = LSTMCell(units,\n                              initializer=initializer,\n                              state_is_tuple=state_is_tuple)\n\n            cell_2 = LSTMCell(units,\n                              initializer=initializer,\n                              state_is_tuple=state_is_tuple)\n\n            cells = MultiRNNCell([cell_0, cell_1, cell_2], state_is_tuple=state_is_tuple)\n            outputs, cell_state = dynamic_rnn(cells, x, dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_min_version(""1.8"")\n    @skip_opset(9, ""ReverseSequence"")\n    @skip_tf2()\n    def test_bidrectional_attention_wrapper_lstm_encoder(self):\n        size = 30\n        time_step = 3\n        input_size = 4\n        attn_size = size\n        batch_size = 9\n\n        # shape  [batch size, time step, size]\n        # attention_state: usually the output of an RNN encoder.\n        # This tensor should be shaped `[batch_size, max_time, ...]`\n        encoder_time_step = time_step\n        encoder_x_val = np.random.randn(encoder_time_step, input_size).astype(\'f\')\n        encoder_x_val = np.stack([encoder_x_val] * batch_size)\n        decoder_time_step = 6\n        decoder_x_val = np.random.randn(decoder_time_step, batch_size, input_size).astype(\'f\')\n\n        def func(encoder_x, decoder_x, seq_length):\n            encoder_cell = LSTMCell(size)\n            attention_states, _ = dynamic_rnn(encoder_cell, encoder_x, dtype=tf.float32)\n            # [9, 3, 30], [9, 30]\n            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(attn_size,\n                                                                       attention_states)\n\n            match_input_fn = lambda curr_input, state: tf.concat([curr_input, state], axis=-1)\n            cell = LSTMCell(size)\n            match_cell_fw = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                                attention_mechanism,\n                                                                attention_layer_size=attn_size,\n                                                                cell_input_fn=match_input_fn,\n                                                                output_attention=False)\n            match_cell_bk = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                                attention_mechanism,\n                                                                attention_layer_size=attn_size,\n                                                                cell_input_fn=match_input_fn,\n                                                                output_attention=False)\n            (match_output_fw, match_output_bk), (match_state_fw, match_state_bk) = \\\n                bidirectional_dynamic_rnn(cell_fw=match_cell_fw,\n                                          cell_bw=match_cell_bk,\n                                          inputs=decoder_x,\n                                          sequence_length=tf.identity(seq_length),\n                                          dtype=tf.float32,\n                                          time_major=True)\n\n            matched_output = tf.concat([match_output_fw, match_output_bk], axis=-1)\n            matched_state = tf.concat([match_state_fw.cell_state, match_state_bk.cell_state], -1)\n            return tf.identity(matched_output, name=""output_0""), tf.identity(matched_state, name=""final_state"")\n\n        feed_dict = {""input_1:0"": encoder_x_val, ""input_2:0"": decoder_x_val,\n                     ""input_3:0"": np.array([6, 5, 4, 3, 2, 1, 2, 3, 6], dtype=np.int32)}\n        input_names_with_port = [""input_1:0"", ""input_2:0"", ""input_3:0""]\n        output_names_with_port = [""output_0:0"", ""final_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.1)\n\n\nclass GatedGRUCell(RNNCell):\n    def __init__(self, hidden_dim, reuse=None):\n        super().__init__(self, _reuse=reuse)\n        self._num_units = hidden_dim\n        self._activation = tf.tanh\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n        # inputs shape: [batch size, time step, input size] = [1, 3, 2]\n        # num_units: 5\n        # W shape: [2, 3 * 5] = [2, 15]\n        # U shape: [5, 3 * 5] = [5, 15]\n        # b shape: [1, 3 * 5] = [1, 15]\n        # state shape: [batch size, state size] = [1, 5]\n\n        input_dim = inputs.get_shape()[-1]\n        assert input_dim is not None, ""input dimension must be defined""\n        # W = tf.get_variable(name=""W"", shape=[input_dim, 3 * self._num_units], dtype=tf.float32)\n        W = np.arange(30.0, dtype=np.float32).reshape((2, 15))\n        # U = tf.get_variable(name=\'U\', shape=[self._num_units, 3 * self._num_units], dtype=tf.float32)\n        U = np.arange(75.0, dtype=np.float32).reshape((5, 15))\n        # b = tf.get_variable(name=\'b\', shape=[1, 3 * self._num_units], dtype=tf.float32)\n        b = np.arange(15.0, dtype=np.float32).reshape((1, 15))\n\n        xw = tf.split(tf.matmul(inputs, W) + b, 3, 1)\n        hu = tf.split(tf.matmul(state, U), 3, 1)\n        r = tf.sigmoid(xw[0] + hu[0])\n        z = tf.sigmoid(xw[1] + hu[1])\n        h1 = self._activation(xw[2] + r * hu[2])\n        next_h = h1 * (1 - z) + state * z\n        return next_h, next_h\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_gru.py,70,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for gru.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import variable_scope\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_gru_count, check_opset_after_tf_version, skip_tf2\nfrom tf2onnx.tf_loader import is_tf2\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,cell-var-from-loop\n\nif is_tf2():\n    # There is no LSTMBlockCell in tf-2.x\n    BasicLSTMCell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell\n    LSTMCell = tf.compat.v1.nn.rnn_cell.LSTMCell\n    GRUCell = tf.compat.v1.nn.rnn_cell.GRUCell\n    MultiRNNCell = tf.compat.v1.nn.rnn_cell.MultiRNNCell\n    dynamic_rnn = tf.compat.v1.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.compat.v1.nn.bidirectional_dynamic_rnn\nelse:\n    BasicLSTMCell = tf.contrib.rnn.BasicLSTMCell\n    LSTMCell = tf.contrib.rnn.LSTMCell\n    GRUCell = tf.contrib.rnn.GRUCell\n    LSTMBlockCell = tf.contrib.rnn.LSTMBlockCell\n    MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n    dynamic_rnn = tf.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.nn.bidirectional_dynamic_rnn\n\n\n# TODO: as a workaround, set batch_size to 1 for now to bypass a onnxruntime bug, revert it when the bug is fixed\nclass GRUTests(Tf2OnnxBackendTestBase):\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # no scope\n            cell = GRUCell(\n                units,\n                activation=None)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_multiple_dynamic_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            gru_output_list = []\n            gru_cell_state_list = []\n            # no scope\n            cell = GRUCell(\n                units,\n                activation=None)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n            gru_output_list.append(outputs)\n            gru_cell_state_list.append(cell_state)\n\n            # given scope\n            cell = GRUCell(\n                units,\n                activation=None)\n            with variable_scope.variable_scope(""root1"") as scope:\n                outputs, cell_state = dynamic_rnn(\n                    cell,\n                    x,\n                    dtype=tf.float32,\n                    sequence_length=[4],\n                    scope=scope)\n            gru_output_list.append(outputs)\n            gru_cell_state_list.append(cell_state)\n\n            return tf.identity(gru_output_list, name=""output""), tf.identity(gru_cell_state_list, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06)\n        # graph_validator=lambda g: check_gru_count(g, 2))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Select"")\n    @skip_tf2()\n    def test_single_dynamic_gru_seq_length_is_const(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32,\n                sequence_length=[5])\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Select"")\n    @skip_tf2()\n    def test_single_dynamic_gru_seq_length_is_not_const(self):\n        for np_dtype in [np.int32, np.int64, np.float32]:\n            units = 5\n            batch_size = 1\n            x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n            x_val = np.stack([x_val] * batch_size)\n            y_val = np.array([5], dtype=np_dtype)\n\n            def func(x, seq_length):\n                initializer = init_ops.constant_initializer(0.5)\n\n                # no scope\n                cell = GRUCell(\n                    units,\n                    kernel_initializer=initializer)\n                outputs, cell_state = dynamic_rnn(\n                    cell,\n                    x,\n                    dtype=tf.float32,\n                    sequence_length=tf.identity(seq_length))\n\n                return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n            feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n            input_names_with_port = [""input_1:0"", ""input_2:0""]\n            output_names_with_port = [""output:0"", ""cell_state:0""]\n            self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                               graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_gru_placeholder_input(self):\n        units = 5\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * 1)\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)  # by default zero initializer is used\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_gru_ch_zero_state_initializer(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = GRUCell(\n                units,\n                kernel_initializer=initializer)\n\n            # defining initial state\n            initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                initial_state=initial_state,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_gru_random_weights(self):\n        hidden_size = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(-1.0, 1.0)\n\n            # no scope\n            cell = GRUCell(\n                hidden_size,\n                kernel_initializer=initializer)\n\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.0001,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_gru_random_weights2(self):\n        hidden_size = 128\n        batch_size = 1\n        x_val = np.random.randn(1, 133).astype(\'f\')\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(0.0, 1.0)\n            # no scope\n            cell = GRUCell(\n                hidden_size,\n                kernel_initializer=initializer)\n\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.01,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_dynamic_gru_output_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(-1.0, 1.0)\n            cell1 = GRUCell(\n                units,\n                kernel_initializer=initializer)\n\n            outputs, _ = dynamic_rnn(\n                cell1,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.0001,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_dynamic_gru_state_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(-1.0, 1.0)\n            cell1 = GRUCell(\n                units,\n                kernel_initializer=initializer)\n\n            _, cell_state = dynamic_rnn(\n                cell1,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=0.0001, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bigru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bigru, no scope\n            cell1 = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            cell2 = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bigru_output_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bigru, no scope\n            cell1 = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            cell2 = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, _ = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bigru_state_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bigru, no scope\n            cell1 = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            cell2 = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bidirectional_but_one_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bigru, no scope\n            cell = GRUCell(\n                units,\n                kernel_initializer=initializer)\n            outputs, cell_state = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bidirectional_but_one_gru_and_output_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n\n            # bigru, no scope\n            cell = GRUCell(\n                units)\n            outputs, _ = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bidirectional_but_one_gru_and_state_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n\n            # bigru, no scope\n            cell = GRUCell(\n                units)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bigru_unknown_batch_size(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n\n            cell1 = GRUCell(units)\n            cell2 = GRUCell(units)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32,\n            )\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bigru_outputs_partially_consumed(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n\n            cell1 = GRUCell(units)\n            cell2 = GRUCell(units)\n            (output_fw, _), (_, state_bw) = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(output_fw, name=""output""), tf.identity(state_bw, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_multi_bigru_with_same_input_hidden_size(self):\n        batch_size = 10\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # bigru, no scope\n            units = 5\n            cell1 = GRUCell(units)\n            cell2 = GRUCell(units)\n            outputs_1, cell_state_1 = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32,\n                scope=""bigru_1""\n            )\n\n            units = 10\n            cell1 = GRUCell(units)\n            cell2 = GRUCell(units)\n            outputs_2, cell_state_2 = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32,\n                scope=""bigru_2""\n            )\n\n            return tf.identity(outputs_1, name=""output_1""),  \\\n                   tf.identity(cell_state_1, name=""cell_state_1""), \\\n                   tf.identity(outputs_2, name=""output_2""), \\\n                   tf.identity(cell_state_2, name=""cell_state_2"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output_1:0"", ""cell_state_1:0"", ""output_2:0"", ""cell_state_2:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06)\n        # graph_validator=lambda g: check_gru_count(g, 2))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_multi_bigru_with_same_input_seq_len(self):\n        units = 5\n        batch_size = 10\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        seq_len_val = np.array([3], dtype=np.int32)\n\n        def func(x, y1, y2):\n            seq_len1 = tf.tile(y1, [batch_size])\n            cell1 = GRUCell(units)\n            cell2 = GRUCell(units)\n            outputs_1, cell_state_1 = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                sequence_length=seq_len1,\n                dtype=tf.float32,\n                scope=""bigru_1""\n            )\n            seq_len2 = tf.tile(y2, [batch_size])\n            cell1 = GRUCell(units)\n            cell2 = GRUCell(units)\n            outputs_2, cell_state_2 = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                sequence_length=seq_len2,\n                dtype=tf.float32,\n                scope=""bigru_2""\n            )\n\n            return tf.identity(outputs_1, name=""output_1""), \\\n                   tf.identity(cell_state_1, name=""cell_state_1""), \\\n                   tf.identity(outputs_2, name=""output_2""), \\\n                   tf.identity(cell_state_2, name=""cell_state_2"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": seq_len_val, ""input_3:0"": seq_len_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0"", ""input_3:0""]\n        output_names_with_port = [""output_1:0"", ""cell_state_1:0"", ""output_2:0"", ""cell_state_2:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06)\n        # graph_validator=lambda g: check_gru_count(g, 2))\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_grublock.py,42,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for grublock.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import variable_scope\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_gru_count, check_tf_max_version\nfrom common import check_opset_after_tf_version\nfrom tf2onnx.tf_loader import is_tf2\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test\n# pylint: disable=invalid-name\n\nif is_tf2():\n    MultiRNNCell = tf.compat.v1.nn.rnn_cell.MultiRNNCell\n    dynamic_rnn = tf.compat.v1.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.compat.v1.nn.bidirectional_dynamic_rnn\nelse:\n    GRUBlockCell = tf.contrib.rnn.GRUBlockCell\n    MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n    dynamic_rnn = tf.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.nn.bidirectional_dynamic_rnn\n\n# pylint: enable=invalid-name\n\n\n# TODO: as a workaround, set batch_size to 1 for now to bypass a onnxruntime bug, revert it when the bug is fixed\nclass GRUBlockTests(Tf2OnnxBackendTestBase):\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = GRUBlockCell(\n                units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_multiple_dynamic_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            gru_output_list = []\n            gru_cell_state_list = []\n            # no scope\n            cell = GRUBlockCell(units)\n            outputs, cell_state = dynamic_rnn(cell, x, dtype=tf.float32)\n            gru_output_list.append(outputs)\n            gru_cell_state_list.append(cell_state)\n\n            # given scope\n            cell = GRUBlockCell(units)\n            with variable_scope.variable_scope(""root1"") as scope:\n                outputs, cell_state = dynamic_rnn(cell, x, dtype=tf.float32, sequence_length=[4], scope=scope)\n            gru_output_list.append(outputs)\n            gru_cell_state_list.append(cell_state)\n\n            return tf.identity(gru_output_list, name=""output""), tf.identity(gru_cell_state_list, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 2))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_gru_seq_length_is_const(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = GRUBlockCell(\n                units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32,\n                sequence_length=[5])\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_gru_seq_length_is_not_const(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        y_val = np.array([5], dtype=np.int32)\n\n        def func(x, seq_length):\n        # no scope\n            cell = GRUBlockCell(\n                units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32,\n                sequence_length=tf.identity(seq_length))\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_gru_placeholder_input(self):\n        units = 5\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * 1)\n        def func(x):\n            # no scope\n            cell = GRUBlockCell(\n                units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)  # by default zero initializer is used\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_gru_ch_zero_state_initializer(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = GRUBlockCell(\n                units)\n\n            # defining initial state\n            initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                initial_state=initial_state,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_gru_random_weights(self):\n        hidden_size = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # no scope\n            cell = GRUBlockCell(\n                hidden_size)\n\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.0001,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_gru_random_weights2(self):\n        hidden_size = 128\n        batch_size = 1\n        x_val = np.random.randn(1, 133).astype(\'f\')\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = GRUBlockCell(\n                hidden_size)\n\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.01,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_gru_output_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            cell1 = GRUBlockCell(units)\n            outputs, _ = dynamic_rnn(cell1, x, dtype=tf.float32)\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.0001,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_gru_state_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            cell1 = GRUBlockCell(units)\n            _, cell_state = dynamic_rnn(cell1, x, dtype=tf.float32)\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.0001,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_bigru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # bigru, no scope\n            cell1 = GRUBlockCell(\n                units)\n            cell2 = GRUBlockCell(\n                units)\n            outputs, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_bigru_output_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # bigru, no scope\n            cell1 = GRUBlockCell(\n                units)\n            cell2 = GRUBlockCell(\n                units)\n            outputs, _ = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_bigru_state_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # bigru, no scope\n            cell1 = GRUBlockCell(\n                units)\n            cell2 = GRUBlockCell(\n                units)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_bidirectional_but_one_gru(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # bigru, no scope\n            cell = GRUBlockCell(\n                units)\n            outputs, cell_state = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_bidirectional_but_one_gru_and_output_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # bigru, no scope\n            cell = GRUBlockCell(\n                units)\n            outputs, _ = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-07,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_bidirectional_but_one_gru_and_state_consumed_only(self):\n        units = 5\n        batch_size = 1\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # bigru, no scope\n            cell = GRUBlockCell(\n                units)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell,\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_gru_count(g, 1))\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_internals.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for internal methods.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\n\nimport graphviz as gv\nimport numpy as np\nfrom onnx import TensorProto\nfrom onnx import helper, numpy_helper\n\nimport tensorflow as tf\nfrom tf2onnx import utils, tf_utils\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\nfrom tf2onnx.graph import GraphUtil\nfrom tf2onnx.tf_loader import tf_reset_default_graph, tf_session\n\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main\n\n\n# pylint: disable=missing-docstring\n\ndef onnx_to_graphviz(g):\n    """"""Onnx graph as dot string.""""""\n    g2 = gv.Digraph()\n    for node in g.get_nodes():\n        kwarg = {}\n        attr = node.attr\n        if ""shape"" in attr:\n            kwarg[""shape""] = str(attr[""shape""].ints)\n        if ""broadcast"" in attr:\n            kwarg[""broadcast""] = str(attr[""broadcast""].i)\n        g2.node(node.name, op_type=node.type, **kwarg)\n    for node in g.get_nodes():\n        for i in node.input:\n            if i:\n                g2.edge(i, node.name)\n    return "" "".join(g2.source.split())\n\n\ndef onnx_pretty(g, args=None):\n    """"""Onnx graph pretty print.""""""\n    graph_proto = g.make_model(""converted from {}"".format(args.input))\n    return helper.printable_graph(graph_proto.graph)\n\n\nclass Tf2OnnxInternalTests(Tf2OnnxBackendTestBase):\n    def setUp(self):\n        super().setUp()\n        arg = namedtuple(""Arg"", ""input inputs outputs"")\n        self._args0 = arg(input=""test"", inputs=[], outputs=[""output:0""])\n        self._args1 = arg(input=""test"", inputs=[""input:0""], outputs=[""output:0""])\n        self._args2 = arg(input=""test"", inputs=[""input1:0"", ""input2:0""], outputs=[""output:0""])\n        self._args3 = arg(input=""test"", inputs=[""input1:0"", ""input2:0"", ""prob:0""], outputs=[""output:0""])\n        self._args4 = arg(input=""test"", inputs=[""input1:0"", ""input2:0""], outputs=[""output1:0"", ""output2:0""])\n\n    @staticmethod\n    def sample_net():\n        n1 = helper.make_node(""Abs"", [""input""], [""n1:0""], name=""n1"")\n        n2 = helper.make_node(""Abs"", [""n1:0""], [""n2:0""], name=""n2"")\n        n3 = helper.make_node(""Abs"", [""n1:0""], [""n3:0""], name=""n3"")\n        n4 = helper.make_node(""Add"", [""n2:0"", ""n3:0""], [""n4:0""], name=""n4"")\n        n5 = helper.make_node(""Abs"", [""n4:0""], [""n5:0""], name=""n5"")\n        n6 = helper.make_node(""Identity"", [""n5:0""], [""n6:0""], name=""n6"")\n\n        graph_proto = helper.make_graph(\n            nodes=[n1, n2, n3, n4, n5, n6],\n            name=""test"",\n            inputs=[helper.make_tensor_value_info(""input"", TensorProto.FLOAT, [2, 2])],\n            outputs=[helper.make_tensor_value_info(""n5:0"", TensorProto.FLOAT, [2, 2])],\n            initializer=[]\n        )\n        return graph_proto\n\n    def test_insert_node1(self):\n        graph_proto = self.sample_net()\n        g = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        n2 = g.get_node_by_name(""n2"")\n        g.insert_new_node_on_input(n2, ""Abs"", ""n1:0"", name=""n7"")\n        ops = g.get_nodes()\n        g.topological_sort(ops)\n        result = onnx_to_graphviz(g)\n        expected = \'digraph { Placeholder__4 [op_type=Placeholder] \' \\\n                   \'n1 [op_type=Abs] n7 [op_type=Abs] n2 [op_type=Abs] n3 [op_type=Abs] \' \\\n                   \'n4 [op_type=Add] n5 [op_type=Abs] n6 [op_type=Identity] \' \\\n                   \'n5_graph_outputs_Identity__3 [op_type=Identity] input -> n1 n1:0 -> n7 \' \\\n                   \'n7:0 -> n2 n1:0 -> n3 n2:0 -> n4 n3:0 -> n4 n4:0 -> n5 n5_raw_output___2:0 -> n6 \' \\\n                   \'n5_raw_output___2:0 -> n5_graph_outputs_Identity__3 }\'\n        self.assertEqual(expected, result)\n\n    def test_insert_node2(self):\n        graph_proto = self.sample_net()\n        g = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        g.insert_new_node_on_output(""Abs"", ""n1:0"", name=""n7"")\n        ops = g.get_nodes()\n        g.topological_sort(ops)\n        result = onnx_to_graphviz(g)\n        expected = \'digraph { Placeholder__4 [op_type=Placeholder] n1 [op_type=Abs] n7 [op_type=Abs] \' \\\n                   \'n3 [op_type=Abs] n2 [op_type=Abs] n4 [op_type=Add] n5 [op_type=Abs] \' \\\n                   \'n6 [op_type=Identity] n5_graph_outputs_Identity__3 [op_type=Identity] \' \\\n                   \'input -> n1 n1:0 -> n7 n7:0 -> n3 n7:0 -> n2 n2:0 -> n4 n3:0 -> n4 n4:0 -> n5 \' \\\n                   \'n5_raw_output___2:0 -> n6 n5_raw_output___2:0 -> n5_graph_outputs_Identity__3 }\'\n        self.assertEqual(expected, result)\n\n    def test_remove_input(self):\n        graph_proto = self.sample_net()\n        g = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        n4 = g.get_node_by_name(""n4"")\n        g.remove_input(n4, n4.input[1])\n        ops = g.get_nodes()\n        g.topological_sort(ops)\n        result = onnx_to_graphviz(g)\n        expected = \'digraph { Placeholder__4 [op_type=Placeholder] n1 [op_type=Abs] n3 [op_type=Abs] \' \\\n                   \'n2 [op_type=Abs] n4 [op_type=Add] n5 [op_type=Abs] n6 [op_type=Identity] \' \\\n                   \'n5_graph_outputs_Identity__3 [op_type=Identity] input -> n1 n1:0 -> n3 \' \\\n                   \'n1:0 -> n2 n2:0 -> n4 n4:0 -> n5 n5_raw_output___2:0 -> n6 \' \\\n                   \'n5_raw_output___2:0 -> n5_graph_outputs_Identity__3 }\'\n        self.assertEqual(expected, result)\n\n    def test_rewrite_subgraph(self):\n        graph_proto = self.sample_net()\n        g = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        pattern = \\\n            OpTypePattern(\'Abs\', name=\'output\', inputs=[\n                OpTypePattern(\'Add\', name=\'input\')\n            ])\n        ops = g.get_nodes()\n        matcher = GraphMatcher(pattern)\n        match_results = list(matcher.match_ops(ops))\n        for match in match_results:\n            input_node = match.get_op(\'input\')\n            output_node = match.get_op(\'output\')\n            op_name = utils.make_name(""ReplacedOp"")\n            out_name = utils.port_name(op_name)\n            new_node = g.make_node(""Sub"", inputs=input_node.input, outputs=[out_name], name=op_name)\n            g.replace_all_inputs(ops, output_node.output[0], new_node.output[0])\n            for n in set(match.get_nodes()):\n                g.remove_node(n.name)\n        g.topological_sort(ops)\n        result = onnx_to_graphviz(g)\n        expected = \'digraph { Placeholder__4 [op_type=Placeholder] n1 [op_type=Abs] \' \\\n                   \'n3 [op_type=Abs] n2 [op_type=Abs] ReplacedOp__5 [op_type=Sub] \' \\\n                   \'n6 [op_type=Identity] n5_graph_outputs_Identity__3 [op_type=Identity] \' \\\n                   \'input -> n1 n1:0 -> n3 n1:0 -> n2 n2:0 -> ReplacedOp__5 n3:0 -> ReplacedOp__5 \' \\\n                   \'ReplacedOp__5:0 -> n6 ReplacedOp__5:0 -> n5_graph_outputs_Identity__3 }\'\n        self.assertEqual(expected, result)\n\n    def test_match_flipped(self):\n        n1 = helper.make_node(""Sub"", [""i1"", ""i1""], [""n1:0""], name=""n1"")\n        n2 = helper.make_node(""Add"", [""i2"", ""i2""], [""n2:0""], name=""n2"")\n        n3 = helper.make_node(""Mul"", [""n1:0"", ""n2:0""], [""n3:0""], name=""n3"")\n\n        graph_proto = helper.make_graph(\n            nodes=[n1, n2, n3],\n            name=""test"",\n            inputs=[helper.make_tensor_value_info(""i1"", TensorProto.FLOAT, [2, 2]),\n                    helper.make_tensor_value_info(""i2"", TensorProto.FLOAT, [2, 2])],\n            outputs=[helper.make_tensor_value_info(""n2:0"", TensorProto.FLOAT, [2, 2])],\n            initializer=[]\n        )\n        g = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        pattern = OpTypePattern(\'Mul\', inputs=[\n            OpTypePattern(\'Add\'),\n            OpTypePattern(\'Sub\')\n        ])\n        ops = g.get_nodes()\n        matcher = GraphMatcher(pattern, allow_reorder=True)\n        match_results = list(matcher.match_ops(ops))\n        self.assertEqual(1, len(match_results))\n\n    def test_cmdarg_parse(self):\n        arg = ""input/V-1_2:0,input/X:0[1,2,3],Y:1[4,5],Z:3,A:1,B""\n        expected_inputs = [\'input/V-1_2:0\', \'input/X:0\', \'Y:1\', \'Z:3\', \'A:1\', \'B\']\n        expected_shape = {\'Y:1\': [4, 5], \'input/X:0\': [1, 2, 3]}\n        inputs, shape_override = utils.split_nodename_and_shape(arg)\n        self.assertEqual(expected_inputs, inputs)\n        self.assertEqual(expected_shape, shape_override)\n\n    def test_shape_utils(self):\n        self.assertEqual(utils.merge_shapes(None, None), None)\n        self.assertEqual(utils.merge_shapes([], None), [])\n        self.assertEqual(utils.merge_shapes(None, [1, 2, 3]), [1, 2, 3])\n        self.assertEqual(utils.merge_shapes([1, 3], [None, 3]), [1, 3])\n        self.assertEqual(utils.merge_shapes([1, None, 3], (-1, 2, ""unk"")), [1, 2, 3])\n\n        self.assertTrue(utils.are_shapes_compatible(None, []))\n        self.assertTrue(utils.are_shapes_compatible([1, None, 3], (-1, 2, ""unk"")))\n        self.assertFalse(utils.are_shapes_compatible([1, 2, 3], (2, 3)))\n        self.assertFalse(utils.are_shapes_compatible([1, 2, 3], (4, 5, 6)))\n\n        self.assertTrue(utils.are_shapes_equal(None, None))\n        self.assertFalse(utils.are_shapes_equal(None, []))\n        self.assertTrue(utils.are_shapes_equal([1, 2, 3], (1, 2, 3)))\n\n    def test_data_format(self):\n        n1 = helper.make_node(""Conv"", [""X"", ""W""], [""Y""], name=""n1"", data_format=""NHWC"")\n        graph_proto = helper.make_graph(\n            nodes=[n1],\n            name=""test"",\n            inputs=[helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [2, 2]),\n                    helper.make_tensor_value_info(""W"", TensorProto.FLOAT, [2, 2])],\n            outputs=[helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [2, 2])],\n            initializer=[]\n        )\n        g = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        n = g.get_node_by_name(""n1"")\n        self.assertEqual(n.data_format, ""NHWC"")\n        self.assertTrue(n.is_nhwc())\n\n    def test_node_attr_onnx(self):\n        n1 = helper.make_node(""Conv"", [""X"", ""W""], [""Y""], name=""n1"", my_attr=""my_attr"")\n        graph_proto = helper.make_graph(\n            nodes=[n1],\n            name=""test"",\n            inputs=[helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [2, 2]),\n                    helper.make_tensor_value_info(""W"", TensorProto.FLOAT, [2, 2])],\n            outputs=[helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [2, 2])],\n            initializer=[]\n        )\n        g = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        n1 = g.get_node_by_name(""n1"")\n        self.assertTrue(""my_attr"" in n1.attr)\n        self.assertTrue(""my_attr"" not in n1.attr_onnx)\n\n        n1 = helper.make_node(""Conv"", [""X"", ""W""], [""Y""], name=""n1"", domain=""my_domain"", my_attr=""my_attr"")\n        graph_proto = helper.make_graph(\n            nodes=[n1],\n            name=""test"",\n            inputs=[helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [2, 2]),\n                    helper.make_tensor_value_info(""W"", TensorProto.FLOAT, [2, 2])],\n            outputs=[helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [2, 2])],\n            initializer=[]\n        )\n        g = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        n1 = g.get_node_by_name(""n1"")\n        self.assertTrue(""my_attr"" in n1.attr)\n        self.assertTrue(""my_attr"" in n1.attr_onnx)\n\n    def test_tensor_data(self):\n        tensors = {\n            ""empty_tensor"": np.array([], dtype=np.float32),\n            ""multi_dim_empty_tensor"": np.array([[], []], dtype=np.float32),\n            ""scalar"": np.array(1., dtype=np.float32),\n            ""one_item_array"": np.array([1.], dtype=np.float32),\n            ""normal_array"": np.array([[1., 2.], [2., 3.]], dtype=np.float32)\n        }\n        tf_reset_default_graph()\n        with tf_session() as sess:\n            for n, data in tensors.items():\n                tf.constant(data, dtype=tf.float32, name=n)\n\n        for tf_node in sess.graph.get_operations():\n            name = tf_node.name\n            self.assertTrue(name in tensors.keys())\n\n            self.assertTrue(""value"" in tf_node.node_def.attr)\n            # convert to onnx tensor value\n            tensor_value = tf_utils.tf_to_onnx_tensor(\n                tf_utils.get_tf_node_attr(tf_node, ""value""),\n                name=utils.port_name(tf_node.name)\n            )\n            attr = helper.make_attribute(""value"", tensor_value)\n            # same as node.get_tensor_value(is_list=False)\n            actual = numpy_helper.to_array(helper.get_attribute_value(attr))\n\n            expected = tensors[name]\n\n            self.assertTrue(np.array_equal(expected, actual))\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_loops.py,70,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for while loops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_tf_min_version, check_tf_max_version, check_onnxruntime_min_version, skip_tf2\nfrom tf2onnx.tf_loader import is_tf2\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test\n\n# names for input and outputs for tests\n_TFINPUT = ""input""\n_INPUT = ""input:0""\n_TFINPUT1 = ""input1""\n_INPUT1 = ""input1:0""\n_TFINPUT2 = ""input2""\n_INPUT2 = ""input2:0""\n_TFOUTPUT = ""output""\n_OUTPUT = ""output:0""\n_TFOUTPUT1 = ""output1""\n_OUTPUT1 = ""output1:0""\n\nclass LoopTests(Tf2OnnxBackendTestBase):\n\n    def test_simple_while_loop(self):\n        def func(i):\n            # tf2 works a little different than tf1 - that is why there is a is_tf2() here\n            if is_tf2():\n                one = tf.constant(np.array([1], dtype=np.int32))\n            else:\n                one = tf.constant(np.array(1, dtype=np.int32))\n            c = lambda i: tf.less(i, 10)\n            b = lambda i: tf.add(i, one)\n            r = tf.while_loop(c, b, [i])\n            if is_tf2():\n                r = tf.reshape(r, [-1])\n            return tf.identity(r, name=_TFOUTPUT)\n\n        if is_tf2():\n            x_val = np.array([0], dtype=np.int32)\n        else:\n            x_val = np.array(0, dtype=np.int32)\n        self.run_test_case(func, {_INPUT: x_val}, [], [_OUTPUT], rtol=1e-06)\n\n    def test_simple_while_loop_2(self):\n        def func(i):\n            # tf2 works a little different than tf1 - that is why there is a is_tf2() here\n            if is_tf2():\n                one = tf.constant(np.array([1], dtype=np.int32))\n            else:\n                one = tf.constant(np.array(1, dtype=np.int32))\n            c = lambda i: tf.logical_and(tf.less(i, 10), tf.greater_equal(i, 3))\n            b = lambda i: tf.add(i, one)\n            r = tf.while_loop(c, b, [i])\n            if is_tf2():\n                r = tf.reshape(r, [-1])\n            return tf.identity(r, name=""output"")\n        if is_tf2():\n            x_val = np.array([3], dtype=np.int32)\n        else:\n            x_val = np.array(3, dtype=np.int32)\n        self.run_test_case(func, {_INPUT: x_val}, [], [_OUTPUT], rtol=1e-06)\n\n    def test_while_loop_with_ta_write(self):\n        def func(i):\n            output_ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n\n            # todo: we cannot support i >= 3 for now, because in this case, TensorArray by default will\n            # leave 0 for in the first 3 index.\n            c = lambda i, *_: tf.logical_and(tf.less(i, 10), i >= 0)\n\n            def b(i, out_ta):\n                new_i = tf.add(i, 1)\n                out_ta_new = out_ta.write(i, i)\n                return new_i, out_ta_new\n\n            i_final, ta_final = tf.while_loop(c, b, [i, output_ta])\n            r = ta_final.stack()\n            return tf.identity(r, name=""output""), tf.identity(i_final, name=""i"")\n\n        x_val = np.array(0, dtype=np.int32)\n        output_names_with_port = [""output:0"", ""i:0""]\n        self.run_test_case(func, {_INPUT: x_val}, [], output_names_with_port, rtol=1e-06)\n\n    def test_while_loop_with_ta_read_simple(self):\n        def func(i, inputs_2):\n            input_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_2)\n            c = lambda i, *_: tf.less(i, 10)\n            res = tf.constant(0.)\n            def b(i, r):\n                new_i = tf.add(i, 1)\n                x = input_ta.read(i)\n                r = tf.add(r, x)\n                return new_i, r\n\n            i_final, x_final = tf.while_loop(c, b, [i, res])\n            return tf.identity(i_final, name=""i""), tf.identity(x_final, name=""x"")\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        feed_dict = {""input_1:0"": np.array(0, dtype=np.int32),\n                     ""input_2:0"": np.array([1., 2., 3., 4., 5., 6., 7., 8., 9., 10.], dtype=np.float32)}\n        output_names_with_port = [""i:0"", ""x:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    def test_while_loop_with_ta_read(self):\n        def func(i, input_2, input_3):\n            inputs_2 = tf.identity(input_2)\n            input_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_2)\n\n            inputs_3_identity = tf.identity(input_3)\n            input_ta_3 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_3_identity)\n\n            c = lambda i, *_: tf.logical_and(tf.less(i, 10), i >= 0)\n            res = tf.constant(0.)\n            res2 = tf.constant(1.)\n\n            def b(i, res, res2):\n                new_i = tf.add(i, 1)\n                x = input_ta.read(i)\n                x = x + 3\n                y = input_ta_3.read(i) + res2\n                return new_i, x, y\n\n            i_final, x_final, y_final = tf.while_loop(c, b, [i, res, res2])\n            return tf.identity(i_final, name=""i""), tf.identity(x_final, name=""x""), tf.identity(y_final, name=""y"")\n        input_names_with_port = [""input_1:0"", ""input_2:0"", ""input_3:0""]\n        feed_dict = {""input_1:0"": np.array(0, dtype=np.int32),\n                     ""input_2:0"": np.array([2.0, 16.0, 5.0, 1.6, 5.0, 6.0, 7.0, 8.0, 9.0, 10.], dtype=np.float32),\n                     ""input_3:0"": np.array([2.0, 16.0, 5.0, 1.6, 5.0, 6.0, 7.0, 8.0, 9.0, 10.], dtype=np.float32)}\n        output_names_with_port = [""i:0"", ""x:0"", ""y:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    def test_while_loop_with_ta_read_reference_outer_input_directly(self):\n        def func(i, inputs_1, inputs_3):\n            input_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_1)\n            input_ta_3 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_3)\n\n            c = lambda i, *_: tf.logical_and(tf.less(i, 10), i >= 0)\n            res = tf.constant(0.)\n            res2 = tf.constant(1.)\n\n            def b(i, res, res2):\n                new_i = tf.add(i, 1)\n                x = input_ta.read(i)\n                x = x + 3\n                y = input_ta_3.read(i) + res2\n                return new_i, x, y\n\n            i_final, x_final, y_final = tf.while_loop(c, b, [i, res, res2])\n            return tf.identity(i_final, name=""i""), tf.identity(x_final, name=""x""), tf.identity(y_final, name=""y"")\n        input_names_with_port = [""input_1:0"", ""input_2:0"", ""input_3:0""]\n        feed_dict = {""input_1:0"": np.array(0, dtype=np.int32),\n                     ""input_2:0"": np.array([2.0, 16.0, 5.0, 1.6, 5.0, 6.0, 7.0, 8.0, 9.0, 10.], dtype=np.float32),\n                     ""input_3:0"": np.array([2.0, 16.0, 5.0, 1.6, 5.0, 6.0, 7.0, 8.0, 9.0, 10.], dtype=np.float32)}\n        output_names_with_port = [""i:0"", ""x:0"", ""y:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    def test_while_loop_with_ta_read_and_write(self):\n        def func(i, inputs):\n            inputs_2 = tf.identity(inputs)\n            input_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_2)\n            output_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n\n            c = lambda i, *_: tf.logical_and(tf.less(i, 10), i >= 0)\n\n            def b(i, out_ta):\n                new_i = tf.add(i, 1)\n                x = input_ta.read(i)\n                x = x + 3\n                out_ta_new = out_ta.write(i, x)\n                return new_i, out_ta_new\n\n            i_final, out_final = tf.while_loop(c, b, [i, output_ta])\n            return tf.identity(i_final, name=""i""), tf.identity(out_final.stack(), name=""output_ta"")\n\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        feed_dict = {""input_1:0"": np.array(0, dtype=np.int32),\n                     ""input_2:0"": np.array([2.0, 16.0, 5.0, 1.6, 5.0, 6.0, 7.0, 8.0, 9.0, 10.], dtype=np.float32)}\n        output_names_with_port = [""i:0"", ""output_ta:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    @check_onnxruntime_min_version(\n        ""0.5.0"",\n        ""disable this case due to onnxruntime loop issue: https://github.com/microsoft/onnxruntime/issues/1272""\n    )\n    def test_while_loop_with_cond_init_false(self):\n        def func(i, inputs):\n            inputs_2 = tf.identity(inputs)\n            input_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_2)\n            output_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n\n            c = lambda i, *_: tf.logical_and(i < 10, i >= 0)\n\n            def b(i, out_ta):\n                new_i = tf.add(i, 1)\n                x = input_ta.read(i)\n                y = x + 3\n                out_ta_new = out_ta.write(i, y)\n                return new_i, out_ta_new\n\n            i_final, out_final = tf.while_loop(c, b, [i, output_ta])\n            return tf.identity(i_final, name=""i""), tf.identity(out_final.stack(), name=""output_ta"")\n\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        feed_dict = {""input_1:0"": np.array(0, dtype=np.int32),\n                     ""input_2:0"": np.array([2.0, 16.0, 5.0, 1.6, 5.0, 6.0, 7.0, 8.0, 9.0, 10.], dtype=np.float32)}\n        output_names_with_port = [""i:0"", ""output_ta:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    @skip_tf2()\n    def test_map_fn_1(self):\n        x_val = 100 * np.random.random_sample([2, 10]).astype(np.float32)\n\n        def fn(elem):\n            res = elem + elem * elem\n            return res\n\n        def func(x):\n            x = tf.identity(x)\n            res_ = tf.map_fn(fn, x, dtype=tf.float32)\n            return tf.identity(res_, name=""output_0"")\n\n        feed_dict = {""input_0:0"": x_val}\n        input_names_with_port = [""input_0:0""]\n        output_names_with_port = [""output_0:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-5)\n\n    @skip_tf2()\n    def test_map_fn_2(self):\n        x_val = 100 * np.random.random_sample([2, 10]).astype(np.float32)\n        y_val = 100 * np.random.random_sample([2, 10]).astype(np.float32)\n\n        def fn(elem):\n            res = elem[0] * elem[1] + elem[0]\n            return res\n\n        def func(x, y):\n            x_ = tf.identity(x)\n            y_ = tf.identity(y)\n            res_ = tf.map_fn(fn, (x_, y_), dtype=tf.float32)\n            return tf.identity(res_, name=""output_0"")\n        feed_dict = {""input_0:0"": x_val, ""input_1:0"": y_val}\n        input_names_with_port = [""input_0:0"", ""input_1:0""]\n        output_names_with_port = [""output_0:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-5)\n\n    @check_tf_min_version(""1.9"")\n    @check_tf_max_version(""1.15"")\n    def test_simple_while_loop_var_shape(self):\n        # test for while_loop with variant shape variables\n        # may not meet ONNX Loop spec\n        # Note: this is not working on tf2 itself.\n        def func(i):\n            const = tf.constant(np.array([2], dtype=np.int32))\n            c = lambda i: tf.reduce_all(tf.shape(i) < 10)\n            b = lambda i: tf.concat([i, const], 0)\n            r = tf.while_loop(c, b, [i], shape_invariants=[tf.TensorShape([None])])\n            return tf.identity(r, name=""output"")\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": np.array([0], dtype=np.int32)}\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_lstm.py,69,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for lstm.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import variable_scope\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_lstm_count, check_opset_after_tf_version, skip_tf2\n\nfrom tf2onnx.tf_loader import is_tf2\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,cell-var-from-loop\n# pylint: disable=invalid-name\n\nif is_tf2():\n    # There is no LSTMBlockCell in tf-2.x\n    BasicLSTMCell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell\n    LSTMCell = tf.compat.v1.nn.rnn_cell.LSTMCell\n    MultiRNNCell = tf.compat.v1.nn.rnn_cell.MultiRNNCell\n    dynamic_rnn = tf.compat.v1.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.compat.v1.nn.bidirectional_dynamic_rnn\nelse:\n    BasicLSTMCell = tf.contrib.rnn.BasicLSTMCell\n    LSTMCell = tf.contrib.rnn.LSTMCell\n    LSTMBlockCell = tf.contrib.rnn.LSTMBlockCell\n    MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n    dynamic_rnn = tf.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.nn.bidirectional_dynamic_rnn\n\n# pylint: enable=invalid-name\n\nclass LSTMTests(Tf2OnnxBackendTestBase):\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_test_single_dynamic_lstm_state_is_tuple(self):\n        self.internal_test_single_dynamic_lstm(True)\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_test_single_dynamic_lstm_state_is_not_tuple(self):\n        self.internal_test_single_dynamic_lstm(False)\n\n    def internal_test_single_dynamic_lstm(self, state_is_tuple):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_time_major(self):\n        units = 5\n        seq_len = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * seq_len)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                time_major=True,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_forget_bias(self):\n        units = 5\n        seq_len = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * seq_len)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer,\n                forget_bias=0.5)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                time_major=True,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Select"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_seq_length_is_const(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        state_is_tuple = True\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32,\n                sequence_length=[4, 3, 4, 5, 2, 1])\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Select"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_seq_length_is_not_const(self):\n        for np_dtype in [np.int32, np.int64, np.float32]:\n            units = 5\n            batch_size = 6\n            x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n            x_val = np.stack([x_val] * batch_size)\n            y_val = np.array([4, 3, 4, 5, 2, 1], dtype=np_dtype)\n            state_is_tuple = True\n            def func(x, seq_length):\n                initializer = init_ops.constant_initializer(0.5)\n\n                # no scope\n                cell = LSTMCell(\n                    units,\n                    initializer=initializer,\n                    state_is_tuple=state_is_tuple)\n                outputs, cell_state = dynamic_rnn(\n                    cell,\n                    x,\n                    dtype=tf.float32,\n                    sequence_length=tf.identity(seq_length))\n\n                return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n            feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n            input_names_with_port = [""input_1:0"", ""input_2:0""]\n            output_names_with_port = [""output:0"", ""cell_state:0""]\n            self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                               graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_placeholder_input(self):\n        units = 5\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * 6)\n        state_is_tuple = True\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)  # by default zero initializer is used\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_ch_zero_state_initializer(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        state_is_tuple = True\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # no scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n\n            # defining initial state\n            initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                initial_state=initial_state,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_consume_one_of_ch_tuple(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n            state_is_tuple = True\n            # no scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), \\\n                   tf.identity(cell_state.c, name=""cell_state_c"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state_c:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_random_weights(self, state_is_tuple=True):\n        hidden_size = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(-1.0, 1.0)\n\n            # no scope\n            cell = LSTMCell(\n                hidden_size,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=0.0001,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Select"")\n    @skip_tf2()\n    def test_single_dynamic_lstm_random_weights2(self, state_is_tuple=True):\n        hidden_size = 128\n        batch_size = 1\n        x_val = np.random.randn(1, 133).astype(\'f\')\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = tf.random_uniform_initializer(0.0, 1.0)\n            # no scope\n            cell = LSTMCell(\n                hidden_size,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=0.01,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Select"")\n    @skip_tf2()\n    def test_multiple_dynamic_lstm_state_is_tuple(self):\n        self.internal_test_multiple_dynamic_lstm_with_parameters(True)\n\n    @skip_tf2()\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    def test_multiple_dynamic_lstm_state_is_not_tuple(self):\n        self.internal_test_multiple_dynamic_lstm_with_parameters(False)\n\n    def internal_test_multiple_dynamic_lstm_with_parameters(self, state_is_tuple):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            lstm_output_list = []\n            lstm_cell_state_list = []\n            # no scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n            lstm_output_list.append(outputs)\n            lstm_cell_state_list.append(cell_state)\n\n            # given scope\n            cell = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            with variable_scope.variable_scope(""root1"") as scope:\n                outputs, cell_state = dynamic_rnn(\n                    cell,\n                    x,\n                    dtype=tf.float32,\n                    sequence_length=[4, 4, 4, 4, 4, 4],\n                    scope=scope)\n            lstm_output_list.append(outputs)\n            lstm_cell_state_list.append(cell_state)\n\n            return tf.identity(lstm_output_list, name=""output""), tf.identity(lstm_cell_state_list, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06)\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_dynamic_basiclstm(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            cell1 = BasicLSTMCell(\n                units,\n                state_is_tuple=True)\n            outputs, cell_state = dynamic_rnn(\n                cell1,\n                x,\n                dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=0.0001, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_dynamic_lstm_output_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            cell1 = LSTMCell(\n                units,\n                state_is_tuple=True)\n\n            outputs, _ = dynamic_rnn(\n                cell1,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=0.0001, atol=1e-07,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_dynamic_lstm_state_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            cell1 = LSTMCell(units, state_is_tuple=True)\n            _, cell_state = dynamic_rnn(cell1, x, dtype=tf.float32)\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=0.0001,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bilstm_state_is_tuple(self):\n        self.internal_test_dynamic_bilstm_with_parameters(True)\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bilstm_state_is_not_tuple(self):\n        self.internal_test_dynamic_bilstm_with_parameters(False)\n\n    def internal_test_dynamic_bilstm_with_parameters(self, state_is_tuple):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bilstm, no scope\n            cell1 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)  # state_is_tuple will impact Pack node (for cell_state)\'s usage pattern\n            cell2 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            outputs, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bilstm_output_consumed_only(self, state_is_tuple=True):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bilstm, no scope\n            cell1 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)  # state_is_tuple will impact Pack node (for cell_state)\'s usage pattern\n            cell2 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            outputs, _ = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bilstm_state_consumed_only(self, state_is_tuple=True):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bilstm, no scope\n            cell1 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)  # state_is_tuple will impact Pack node (for cell_state)\'s usage pattern\n            cell2 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bilstm_outputs_partially_consumed(self, state_is_tuple=True):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            # bilstm, no scope\n            cell1 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)  # state_is_tuple will impact Pack node (for cell_state)\'s usage pattern\n            cell2 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            (output_fw, _), (_, state_bw) = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(output_fw, name=""output""), tf.identity(state_bw, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_bilstm_unknown_batch_size(self, state_is_tuple=True):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n\n            cell1 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            cell2 = LSTMCell(\n                units,\n                initializer=initializer,\n                state_is_tuple=state_is_tuple)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32,\n            )\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_multi_bilstm_with_same_input_hidden_size(self):\n        batch_size = 10\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            units = 5\n            cell1 = LSTMCell(units, name=""cell1"")\n            cell2 = LSTMCell(units, name=""cell2"")\n            outputs_1, cell_state_1 = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32,\n                scope=""bilstm_1""\n            )\n\n            units = 10\n            cell3 = LSTMCell(units, name=""cell3"")\n            cell4 = LSTMCell(units, name=""cell4"")\n            outputs_2, cell_state_2 = bidirectional_dynamic_rnn(\n                cell3,\n                cell4,\n                x,\n                dtype=tf.float32,\n                scope=""bilstm_2""\n            )\n\n            return tf.identity(outputs_1, name=""output_1""), \\\n                   tf.identity(cell_state_1, name=""cell_state_1""), \\\n                   tf.identity(outputs_2, name=""output_2""), \\\n                   tf.identity(cell_state_2, name=""cell_state_2"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output_1:0"", ""cell_state_1:0"", ""output_2:0"", ""cell_state_2:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06)\n        # graph_validator=lambda g: check_lstm_count(g, 2))\n\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    @skip_tf2()\n    def test_dynamic_multi_bilstm_with_same_input_seq_len(self):\n        units = 5\n        batch_size = 10\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        seq_len_val = np.array([3], dtype=np.int32)\n\n        def func(x, y1, y2):\n            seq_len1 = tf.tile(y1, [batch_size])\n            cell1 = LSTMCell(units)\n            cell2 = LSTMCell(units)\n            outputs_1, cell_state_1 = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                sequence_length=seq_len1,\n                dtype=tf.float32,\n                scope=""bilstm_1""\n            )\n\n            seq_len2 = tf.tile(y2, [batch_size])\n            cell1 = LSTMCell(units)\n            cell2 = LSTMCell(units)\n            outputs_2, cell_state_2 = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                sequence_length=seq_len2,\n                dtype=tf.float32,\n                scope=""bilstm_2""\n            )\n\n            return tf.identity(outputs_1, name=""output_1""), \\\n                   tf.identity(cell_state_1, name=""cell_state_1""), \\\n                   tf.identity(outputs_2, name=""output_2""), \\\n                   tf.identity(cell_state_2, name=""cell_state_2"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": seq_len_val, ""input_3:0"": seq_len_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0"", ""input_3:0""]\n        output_names_with_port = [""output_1:0"", ""cell_state_1:0"", ""output_2:0"", ""cell_state_2:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06)\n        # graph_validator=lambda g: check_lstm_count(g, 2))\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_lstmblock.py,50,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for lstm block cell.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import variable_scope\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_tf_min_version, check_opset_min_version, check_lstm_count\nfrom common import check_tf_max_version, check_opset_after_tf_version\nfrom tf2onnx.tf_loader import is_tf2\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test\n\nif is_tf2():\n    # There is no LSTMBlockCell in tf-2.x\n    MultiRNNCell = tf.compat.v1.nn.rnn_cell.MultiRNNCell\n    dynamic_rnn = tf.compat.v1.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.compat.v1.nn.bidirectional_dynamic_rnn\nelse:\n    LSTMBlockCell = tf.contrib.rnn.LSTMBlockCell\n    MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n    dynamic_rnn = tf.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.nn.bidirectional_dynamic_rnn\n\n\nclass LSTMBlockTests(Tf2OnnxBackendTestBase):\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = LSTMBlockCell(units, use_peephole=False)\n            outputs, cell_state = dynamic_rnn(cell, x, dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm_seq_length_is_const(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = LSTMBlockCell(units)\n            outputs, cell_state = dynamic_rnn(cell, x, dtype=tf.float32, sequence_length=[4, 3, 4, 5, 2, 1])\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm_seq_length_is_not_const(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        y_val = np.array([4, 3, 4, 5, 2, 1], dtype=np.int32)\n        def func(x, seq_length):\n            # no scope\n            cell = LSTMBlockCell(units)\n            outputs, cell_state = dynamic_rnn(cell, x, dtype=tf.float32, sequence_length=tf.identity(seq_length))\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val, ""input_2:0"": y_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm_time_major(self):\n        units = 5\n        seq_len = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * seq_len)\n        def func(x):\n            # no scope\n            cell = LSTMBlockCell(units)\n            outputs, cell_state = dynamic_rnn(cell, x, time_major=True, dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm_placeholder_input(self):\n        units = 5\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * 6)\n        def func(x):\n            # no scope\n            cell = LSTMBlockCell(units)\n            outputs, cell_state = dynamic_rnn(cell, x, dtype=tf.float32)  # by default zero initializer is used\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm_ch_zero_state_initializer(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.], [5., 5.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = LSTMBlockCell(units)\n            # defining initial state\n            initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n            outputs, cell_state = dynamic_rnn(cell, x, initial_state=initial_state, dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm_consume_one_of_ch_tuple(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # no scope\n            cell = LSTMBlockCell(units)\n            outputs, cell_state = dynamic_rnn(cell, x, dtype=tf.float32)\n            return tf.identity(outputs, name=""output""),\\\n                   tf.identity(cell_state.c, name=""cell_state_c""),\\\n                   tf.identity(cell_state.h, name=""cell_state_h"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state_c:0"", ""cell_state_h:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_multiple_dynamic_lstm(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            lstm_output_list = []\n            lstm_cell_state_list = []\n            # no scope\n            cell = LSTMBlockCell(units)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n            lstm_output_list.append(outputs)\n            lstm_cell_state_list.append(cell_state)\n\n            # given scope\n            cell = LSTMBlockCell(units)\n            with variable_scope.variable_scope(""root1"") as scope:\n                outputs, cell_state = dynamic_rnn(\n                    cell,\n                    x,\n                    dtype=tf.float32,\n                    sequence_length=[4, 4, 4, 4, 4, 4],\n                    scope=scope)\n            lstm_output_list.append(outputs)\n            lstm_cell_state_list.append(cell_state)\n            return tf.identity(lstm_output_list, name=""output""), tf.identity(lstm_cell_state_list, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06)\n        # graph_validator=lambda g: check_lstm_count(g, 2))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_lstm_output_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            cell1 = LSTMBlockCell(units)\n            outputs, _ = dynamic_rnn(\n                cell1,\n                x,\n                dtype=tf.float32)\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_dynamic_lstm_state_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            cell1 = LSTMBlockCell(units)\n            _, cell_state = dynamic_rnn(\n                cell1,\n                x,\n                dtype=tf.float32)\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    def test_dynamic_bilstm(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # bilstm, no scope\n            cell1 = LSTMBlockCell(units)\n            cell2 = LSTMBlockCell(units)\n            outputs, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    def test_dynamic_bilstm_output_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n        def func(x):\n            # bilstm, no scope\n            cell1 = LSTMBlockCell(units)\n            cell2 = LSTMBlockCell(units)\n            outputs, _ = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n            return tf.identity(outputs, name=""output"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""output:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    @check_opset_after_tf_version(""1.15"", 10, ""might need ReverseV2"")\n    def test_dynamic_bilstm_state_consumed_only(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # bilstm, no scope\n            cell1 = LSTMBlockCell(units)\n            cell2 = LSTMBlockCell(units)\n            _, cell_state = bidirectional_dynamic_rnn(\n                cell1,\n                cell2,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(cell_state, name=""cell_state"")\n\n        feed_dict = {""input_1:0"": x_val}\n        input_names_with_port = [""input_1:0""]\n        output_names_with_port = [""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 1))\n\n    # ==============================================================================================\n    # NOTE: the unittest above should be converted into a single LSTM op, while following unittests\n    # should be first converted into a Scan op with LSTMBlockCell, then decoupled into several ops.\n    # ==============================================================================================\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm_with_peephole(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # no scope\n            cell = LSTMBlockCell(units, use_peephole=True)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-3, atol=1e-06)\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_single_dynamic_lstm_with_cell_clip(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            # no scope\n            cell = LSTMBlockCell(units, cell_clip=0.05)\n            outputs, cell_state = dynamic_rnn(\n                cell,\n                x,\n                dtype=tf.float32)\n\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06)\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_min_version(""1.8"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_attention_wrapper_lstm_encoder(self):\n        size = 5\n        time_step = 3\n        input_size = 4\n        attn_size = size\n        batch_size = 9\n        encoder_time_step = time_step\n        encoder_x_val = np.random.randn(encoder_time_step, input_size).astype(\'f\')\n        encoder_x_val = np.stack([encoder_x_val] * batch_size)\n        decoder_time_step = 6\n        decoder_x_val = np.random.randn(decoder_time_step, input_size).astype(\'f\')\n        decoder_x_val = np.stack([decoder_x_val] * batch_size)\n\n        # shape  [batch size, time step, size]\n        # attention_state: usually the output of an RNN encoder.\n        # This tensor should be shaped `[batch_size, max_time, ...]`\n        def func(encoder_x, decoder_x):\n            encoder_cell = LSTMBlockCell(size)\n            output, attr_state = dynamic_rnn(encoder_cell, encoder_x, dtype=tf.float32)\n            output_0 = tf.identity(output, name=""output_0"")\n            attention_states = output\n            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(attn_size,\n                                                                       attention_states)\n\n            match_input_fn = lambda curr_input, state: tf.concat([curr_input, state], axis=-1)\n            cell = LSTMBlockCell(size)\n            match_cell_fw = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                                attention_mechanism,\n                                                                attention_layer_size=attn_size,\n                                                                cell_input_fn=match_input_fn,\n                                                                output_attention=False)\n\n            output, attr_state = dynamic_rnn(match_cell_fw, decoder_x, dtype=tf.float32)\n\n            return output_0, tf.identity(output, name=""output""), tf.identity(attr_state.cell_state, name=""final_state"")\n\n        feed_dict = {""input_1:0"": encoder_x_val, ""input_2:0"": decoder_x_val}\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output_0:0"", ""output:0"", ""final_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, 0.1)\n\n    @check_opset_min_version(8, ""Scan"")\n    @check_tf_max_version(""1.15"", ""no LSTMBlockCell in tf-2.x"")\n    def test_multi_rnn_lstm(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            cell_0 = LSTMBlockCell(units)\n            cell_1 = LSTMBlockCell(units)\n            cell_2 = LSTMBlockCell(units)\n            cells = MultiRNNCell([cell_0, cell_1, cell_2], state_is_tuple=True)\n            outputs, cell_state = dynamic_rnn(cells, x, dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-03, atol=1e-06)\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_onnx_shape_inference.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for shape inference.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nfrom onnx import TensorProto\nfrom tf2onnx import utils\nfrom tf2onnx.graph import Graph\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import *  # pylint: disable=wildcard-import,unused-wildcard-import\n\n# pylint: disable=missing-docstring\n\nINPUT1 = ""input1""\nINPUT2 = ""input2""\nINPUT3 = ""input3""\n\n\nclass ONNXShapeInferenceTests(Tf2OnnxBackendTestBase):\n    """"""\n    Test shape inference, it\'s just a subset of all cases that can be inferred shape.\n    For more information, please refer to onnx shape inference test:\n    https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py\n    """"""\n\n    def _run_test_case(self, graph, feed_dict):\n        """"""Run model with onnxruntime and compare results\' shape with internal shape inference.""""""\n        outputs = graph.outputs\n        results = self.run_backend(graph, outputs, feed_dict)\n\n        for actual, inferred in zip(results, outputs):\n            actual_shape = actual.shape\n            inferred_shape = tuple(graph.get_shape(inferred))\n            self.assertTrue(utils.are_shapes_compatible(actual_shape, inferred_shape))\n\n            actual_dtype = actual.dtype\n            inferred_dtype = utils.ONNX_TO_NUMPY_DTYPE[graph.get_dtype(inferred)]\n            self.assertEqual(actual_dtype, inferred_dtype)\n\n    def _create_empty_graph(self, inputs, shapes, dtypes):\n        graph = Graph([], target=self.config.target, opset=self.config.opset)\n        for inp, shape, dtype in zip(inputs, shapes, dtypes):\n            graph.add_graph_input(inp, dtype, shape)\n        return graph\n\n    def _generate_random_inputs(self, inputs, shapes, dtypes):\n        """"""Generate random input of shape and ONNX dtypes""""""\n        res = {}\n        for inp, shape, dtype in zip(inputs, shapes, dtypes):\n            new_shape = [1 if s == -1 else s for s in shape]\n            res[inp] = np.random.random(new_shape).astype(utils.ONNX_TO_NUMPY_DTYPE[dtype])\n        return res\n\n    # one input\n    def test_identity(self):\n        inputs = [INPUT1]\n        shapes = [[1, 3, 4, 1]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""Identity"", [INPUT1])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_transpose(self):\n        inputs = [INPUT1]\n        shapes = [[1, 3, 4, 1]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""Transpose"", [INPUT1], attr={""perm"": [1, 0, 2, 3]})\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_shape(self):\n        inputs = [INPUT1]\n        shapes = [[1, 3, 4, 1]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""Shape"", [INPUT1])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    @check_opset_min_version(2, ""Split"")\n    def test_split(self):\n        inputs = [INPUT1]\n        shapes = [[5, 6, 7]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""Split"", [INPUT1], output_count=2, attr={""axis"": 1})\n        graph.add_graph_output(node.output[0])\n        graph.add_graph_output(node.output[1])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    # two inputs\n    @check_opset_min_version(6, ""Add"")\n    def test_add(self):\n        inputs = [INPUT1, INPUT2]\n        shapes = [[1, 3, 4, 1], [2, 1, 4, 10]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""Add"", [INPUT1, INPUT2])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_matmul(self):\n        inputs = [INPUT1, INPUT2]\n        shapes = [[1, 3, 4, 1], [2, 1, 1, 10]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""MatMul"", [INPUT1, INPUT2])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def _test_matmul_unknown_shape(self, shapes):\n        data_shapes = [\n            [1 if s == -1 else s for s in shapes[0]],\n            [1 if s == -1 else s for s in shapes[1]]\n        ]\n        inputs = [INPUT1, INPUT2]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""MatMul"", [INPUT1, INPUT2])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, data_shapes, dtypes))\n\n    def test_matmul_unknown(self):\n        self._test_matmul_unknown_shape([[-1], [-1]])\n        self._test_matmul_unknown_shape([[3], [-1]])\n        self._test_matmul_unknown_shape([[2], [2, -1]])\n        self._test_matmul_unknown_shape([[4, 2], [2, -1]])\n        self._test_matmul_unknown_shape([[1, 4, 2], [-1, 2, 5]])\n        self._test_matmul_unknown_shape([[1, 3, 4, 2], [-1, 2, 5]])\n\n    @check_opset_min_version(4, ""Concat"")\n    def test_concat(self):\n        inputs = [INPUT1, INPUT2]\n        shapes = [[10, 20, 9], [12, 20, 9]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""Concat"", [INPUT1, INPUT2], attr={""axis"": 0})\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_conv(self):\n        inputs = [INPUT1, INPUT2]\n        shapes = [[3, 4, 5, 6, 7], [5, 4, 2, 4, 3]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(\n            ""Conv"", [INPUT1, INPUT2],\n            attr={\n                ""pads"": [0, 1, 1, 0, 0, 1],\n                ""dilations"": [1, 2, 2],\n                ""strides"": [1, 1, 2]\n            }\n        )\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    # more than two inputs\n    @check_opset_min_version(8, ""Sum"")\n    def test_sum(self):\n        inputs = [INPUT1, INPUT2, INPUT3]\n        shapes = [[30, 1, 5], [-1, 4, 1], [4, -1]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""Sum"", [INPUT1, INPUT2, INPUT3])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    @check_opset_min_version(7, ""RNN"")\n    def test_rnn(self):\n        seq_len = 64\n        batch_size = 32\n        input_size = 10\n        hidden_size = 4\n        inputs = [INPUT1, INPUT2, INPUT3]\n        shapes = [[seq_len, batch_size, input_size], [1, hidden_size, input_size], [1, hidden_size, hidden_size]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""RNN"", [INPUT1, INPUT2, INPUT3], output_count=2, attr={""hidden_size"": hidden_size})\n        graph.add_graph_output(node.output[0])\n        graph.add_graph_output(node.output[1])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    @check_opset_min_version(7, ""LSTM"")\n    def test_lstm(self):\n        seq_len = 64\n        batch_size = 32\n        input_size = 10\n        hidden_size = 4\n        inputs = [INPUT1, INPUT2, INPUT3]\n        shapes = [\n            [seq_len, batch_size, input_size],\n            [1, 4 * hidden_size, input_size],\n            [1, 4 * hidden_size, hidden_size]\n        ]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""LSTM"", [INPUT1, INPUT2, INPUT3], output_count=3, attr={""hidden_size"": hidden_size})\n        graph.add_graph_output(node.output[0])\n        graph.add_graph_output(node.output[1])\n        graph.add_graph_output(node.output[2])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    # with const input\n    @check_opset_min_version(5, ""Reshape"")\n    def test_reshape(self):\n        inputs = [INPUT1]\n        shapes = [[10, 20]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        const = graph.make_const(""shape"", np.array([5, 40], dtype=np.int64))\n        node = graph.make_node(""Reshape"", [INPUT1, const.output[0]])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_gather(self):\n        inputs = [INPUT1]\n        shapes = [[4, 3]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        const = graph.make_const(""index"", np.array([1, 2], dtype=np.int64))\n        node = graph.make_node(""Gather"", [INPUT1, const.output[0]])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_gather_axis1(self):\n        inputs = [INPUT1]\n        shapes = [[4, 3, 5]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        const = graph.make_const(""index"", np.array([[1, 2]], dtype=np.int64))\n        node = graph.make_node(""Gather"", [INPUT1, const.output[0]], attr={""axis"": 1})\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_gather_into_scalar(self):\n        inputs = [INPUT1]\n        shapes = [[4]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        const = graph.make_const(""index"", np.array(2, dtype=np.int64))\n        node = graph.make_node(""Gather"", [INPUT1, const.output[0]])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    @check_opset_min_version(9, ""ConstantOfShape"")\n    def test_constant_of_shape(self):\n        inputs = [INPUT1]\n        shapes = [[3]]\n        dtypes = [TensorProto.INT64]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        node = graph.make_node(""ConstantOfShape"", [INPUT1])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n        graph = self._create_empty_graph([], [], [])\n        const = graph.make_const(""shape"", np.array([3, 5, 6], dtype=np.int64))\n        node = graph.make_node(""ConstantOfShape"", [const.output[0]])\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs([], [], []))\n\n    # node with subgraph\n    @check_opset_min_version(8, ""Scan"")\n    @check_opset_max_version(8, ""Scan"")\n    def test_scan(self):\n        batch_size = 1\n        seq_len = 16\n        input_size = 2\n        loop_state_size = 3\n        inputs = [INPUT1, INPUT2]\n        shapes = [[batch_size, loop_state_size, loop_state_size],\n                  [batch_size, seq_len, input_size, input_size]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT]\n\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n\n        subgraph = self._create_empty_graph(\n            [""loop_state_in"", ""input""],\n            [[-1, -1], [-1, -1]],\n            [TensorProto.FLOAT, TensorProto.FLOAT],\n        )\n        subgraph.parent_graph = graph\n        loop_state_iden = subgraph.make_node(""Identity"", [""loop_state_in""])\n        input_iden = subgraph.make_node(""Identity"", [""input""])\n        subgraph.add_graph_output(loop_state_iden.output[0])\n        subgraph.add_graph_output(input_iden.output[0])\n\n        seq_len_node = graph.make_const(""seq_len"", np.array(seq_len, dtype=np.int64))\n        scan = graph.make_node(\n            ""Scan"", [seq_len_node.output[0], INPUT1, INPUT2],\n            output_count=2, attr={""num_scan_inputs"": 1}\n        )\n        scan.set_body_graph_as_attr(""body"", subgraph)\n\n        # explicitly infer shape for scan node\n        graph.update_node_shape_dtype(scan)\n\n        graph.add_graph_output(scan.output[0])\n        graph.add_graph_output(scan.output[1])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    @check_opset_min_version(9, ""Scan"")\n    def test_scan_opset9(self):\n        seq_len = 16\n        input_size = 2\n        loop_state_size = 3\n        inputs = [INPUT1, INPUT2]\n        shapes = [[loop_state_size, loop_state_size],\n                  [seq_len, input_size, input_size]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n\n        subgraph = self._create_empty_graph(\n            [""loop_state_in"", ""input""],\n            [[-1, -1], [-1, -1]],\n            [TensorProto.FLOAT, TensorProto.FLOAT],\n        )\n        subgraph.parent_graph = graph\n        loop_state_iden = subgraph.make_node(""Identity"", [""loop_state_in""])\n        input_iden = subgraph.make_node(""Identity"", [""input""])\n        subgraph.add_graph_output(loop_state_iden.output[0])\n        subgraph.add_graph_output(input_iden.output[0])\n\n        scan = graph.make_node(""Scan"", [INPUT1, INPUT2], output_count=2, attr={""num_scan_inputs"": 1})\n        scan.set_body_graph_as_attr(""body"", subgraph)\n\n        # explicitly infer shape for scan node\n        graph.update_node_shape_dtype(scan)\n\n        graph.add_graph_output(scan.output[0])\n        graph.add_graph_output(scan.output[1])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_if(self):\n        inputs = [INPUT1, INPUT2, INPUT3]\n        shapes = [[2, 3, 4], [2, 3, 4], [2, 3, 4]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n\n        then_subgraph = self._create_empty_graph([], [], [])\n        then_subgraph.parent_graph = graph\n        add = then_subgraph.make_node(""Add"", [INPUT1, INPUT2])\n        then_subgraph.add_graph_output(add.output[0])\n\n        else_subgraph = self._create_empty_graph([], [], [])\n        else_subgraph.parent_graph = graph\n        sub = else_subgraph.make_node(""Sub"", [INPUT1, INPUT3])\n        else_subgraph.add_graph_output(sub.output[0])\n\n        cond = graph.make_const(""cond"", np.array(True, dtype=np.bool))\n        if_node = graph.make_node(""If"", [cond.output[0]])\n        if_node.set_body_graph_as_attr(""then_branch"", then_subgraph)\n        if_node.set_body_graph_as_attr(""else_branch"", else_subgraph)\n\n        # explicitly infer shape for if node\n        graph.update_node_shape_dtype(if_node)\n\n        graph.add_graph_output(if_node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    def test_loop(self):\n        inputs = [INPUT1, INPUT2]\n        shapes = [[3, 4], [4, 5]]\n        dtypes = [TensorProto.FLOAT, TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n\n        subgraph = self._create_empty_graph(\n            [""iter_num"", ""cond"", ""loop_state""],\n            [[-1], [-1], [-1, -1]],\n            [TensorProto.INT64, TensorProto.BOOL, TensorProto.FLOAT]\n        )\n        subgraph.parent_graph = graph\n        cond_out = subgraph.make_node(""Identity"", [""cond""])\n        loop_state_out = subgraph.make_node(""Identity"", [""loop_state""])\n        out = subgraph.make_node(""MatMul"", [""loop_state"", INPUT2])\n        subgraph.add_graph_output(cond_out.output[0])\n        subgraph.add_graph_output(loop_state_out.output[0])\n        subgraph.add_graph_output(out.output[0])\n\n        max_iter = graph.make_const(""max_iter"", np.array([10], dtype=np.int64))\n        cond_const = graph.make_const(""cond_const"", np.array([True], dtype=np.bool))\n        loop = graph.make_node(""Loop"", [max_iter.output[0], cond_const.output[0], INPUT1],\n                               output_count=2)\n        loop.set_body_graph_as_attr(""body"", subgraph)\n\n        graph.update_node_shape_dtype(loop)\n\n        # state shape may change between iterations\n        # graph.add_graph_output(loop.output[0])\n        graph.add_graph_output(loop.output[1])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n    # overrider shape\n    def test_override_shape(self):\n        inputs = [INPUT1]\n        shapes = [[1, 3, 4, 1]]\n        dtypes = [TensorProto.FLOAT]\n        graph = self._create_empty_graph(inputs, shapes, dtypes)\n        output_name = utils.make_name(""output"")\n        graph._output_shapes[output_name] = [-1, -1, 2, 3]  # pylint: disable=protected-access\n        node = graph.make_node(""Transpose"", [INPUT1], attr={""perm"": [1, 0, 2, 3]}, outputs=[output_name])\n\n        graph.update_node_shape_dtype(node, override=True)\n\n        graph.add_graph_output(node.output[0])\n        self._run_test_case(graph, self._generate_random_inputs(inputs, shapes, dtypes))\n\n\nif __name__ == ""__main__"":\n    unittest_main()\n'"
tests/test_optimizers.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for optimizers such as TransposeOptimizer.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nfrom onnx import helper, TensorProto, OperatorSetIdProto\nfrom tf2onnx import utils, constants\nfrom tf2onnx.graph import GraphUtil\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, group_nodes_by_type, check_opset_min_version, check_opset_max_version\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test\n\nclass OptimizerTests(Tf2OnnxBackendTestBase):\n    """"""Run original model proto and modified model proto with onnxruntime, compare the results.""""""\n\n    def run_and_compare(self, output_names_with_port, onnx_feed_dict, origin_proto, op_type,\n                        remaining_op_num, debug=False, rtol=1e-07):\n        utils.make_sure(op_type is not None, ""op_type should be specified"")\n        utils.make_sure(remaining_op_num is not None, ""remaining_op_num should be specified"")\n\n        origin_model_path = self.save_onnx_model(origin_proto, onnx_feed_dict, postfix=""_origin"")\n\n        new_proto = GraphUtil.optimize_model_proto(origin_proto)\n\n        self.assertTrue(new_proto, msg=""model proto after optimizer should not be None"")\n\n        new_model_path = self.save_onnx_model(new_proto, onnx_feed_dict, postfix=""_opt"")\n        current = GraphUtil.get_node_count_from_onnx_graph(new_proto.graph)\n\n        self.assertTrue(current[op_type] == remaining_op_num,\n                        msg=""Expect "" + str(remaining_op_num) + "" "" + op_type + "" ops left, but actually "" + str(\n                            current[op_type]) + "" left"")\n\n        if self.config.is_onnxruntime_backend:\n            expected = self.run_onnxruntime(origin_model_path, onnx_feed_dict, output_names_with_port)\n            actual = self.run_onnxruntime(new_model_path, onnx_feed_dict, output_names_with_port)\n        else:\n            raise ValueError(""only onnxruntime is supported to test transpose optimizer"")\n\n        for expected_val, actual_val in zip(expected, actual):\n            self.assertAllClose(expected_val, actual_val, rtol=rtol, atol=1e-5)\n            self.assertEqual(expected_val.dtype, actual_val.dtype)\n            self.assertEqual(expected_val.shape, actual_val.shape)\n\n        return new_proto\n\n    @staticmethod\n    def _make_onnx_const(np_val, output_name):\n        node = helper.make_node(\n            \'Constant\',\n            inputs=[],\n            outputs=[output_name],\n            value=helper.make_tensor(\n                name=output_name,\n                data_type=utils.map_numpy_to_onnx_dtype(np_val.dtype),\n                dims=np_val.shape,\n                vals=np_val.flatten().astype(np_val.dtype),\n            ),\n        )\n        return node\n\n    def make_model(self, graph, producer_name=""onnx-tests""):\n        imp = OperatorSetIdProto()\n        imp.version = self.config.opset\n        model_proto = helper.make_model(graph, producer_name=producer_name, opset_imports=[imp])\n        try:\n            model_proto.ir_version = constants.OPSET_TO_IR_VERSION.get(self.config.opset, model_proto.ir_version)\n        except:  # pylint: disable=bare-except\n            pass\n        return model_proto\n\n    # Tranpose Optimizer Tests Start\n\n    def run_transpose_compare(self, output_names_with_port, onnx_feed_dict, origin_proto,\n                              remaining_transpose_num=None, debug=False, rtol=1e-07):\n        return self.run_and_compare(output_names_with_port, onnx_feed_dict, origin_proto, op_type=""Transpose"",\n                                    remaining_op_num=remaining_transpose_num, debug=debug, rtol=rtol)\n\n    def check_transpose_perm(self, model_proto, expected_perm):\n        for node in model_proto.graph.node:\n            if node.op_type == ""Transpose"":\n                perm = list(node.attribute[0].ints)\n                self.assertEqual(perm, expected_perm)\n\n    def test_transpose_with_concat(self):\n        input_shape = (2, 3, 4, 5)\n        perm = [0, 3, 1, 2]\n        input_shape_with_trans = [input_shape[i] for i in perm]\n        for axis in [0, 1, 2, 3]:\n            output_before_trans = list(input_shape)\n            output_before_trans[axis] *= 2\n            output_shape = [output_before_trans[i] for i in [0, 3, 1, 2]]\n            node1 = helper.make_node(""Transpose"", [""input_data1""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n            node2 = helper.make_node(""Concat"", [""Y"", ""input_data2""], [""Z""], axis=axis, name=""concat"")\n            node3 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans2"")\n\n            graph = helper.make_graph(\n                [node1, node2, node3],\n                ""test_transpose_with_concat"",\n                [helper.make_tensor_value_info(""input_data1"", TensorProto.FLOAT, input_shape_with_trans),\n                 helper.make_tensor_value_info(""input_data2"", TensorProto.FLOAT, input_shape),\n                 ],\n                [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, output_shape)],\n            )\n\n            model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n            feed_dict = {""input_data1"": np.random.randn(*input_shape_with_trans).astype(np.float32),\n                         ""input_data2"": np.random.randn(*input_shape).astype(np.float32),\n                         }\n            self.run_transpose_compare([""res""], feed_dict, model_proto, remaining_transpose_num=1)\n\n    def test_transpose_with_add1(self):\n        # when transpose follows with a broadcasting op\n        # reshape is needed when switching transpose with this op and op need broadcast its inputs\n        node1 = helper.make_node(""Transpose"", [""input_data1""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node2 = helper.make_node(""Add"", [""Y"", ""input_data2""], [""Z""], name=""add"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans2"")\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""transpose_with_shape"",\n            [helper.make_tensor_value_info(""input_data1"", TensorProto.FLOAT, (2, 3, 4, 5)),\n             helper.make_tensor_value_info(""input_data2"", TensorProto.FLOAT, (3,)),\n             ],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (2, 3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        feed_dict = {""input_data1"": np.random.randn(2, 3, 4, 5).astype(np.float32),\n                     ""input_data2"": np.random.randn(3).astype(np.float32),\n                     }\n        self.run_transpose_compare([""res""], feed_dict, model_proto, remaining_transpose_num=0)\n\n    def test_transpose_with_add2(self):\n        node1 = helper.make_node(""Transpose"", [""input_data1""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node2 = helper.make_node(""Add"", [""Y"", ""input_data2""], [""Z""], name=""add"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans2"")\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""transpose_with_shape"",\n            [helper.make_tensor_value_info(""input_data1"", TensorProto.FLOAT, (2, 3, 4, 5)),\n             helper.make_tensor_value_info(""input_data2"", TensorProto.FLOAT, (2, 4, 5, 3)),\n             ],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (2, 3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        feed_dict = {""input_data1"": np.random.randn(2, 3, 4, 5).astype(np.float32),\n                     ""input_data2"": np.random.randn(2, 4, 5, 3).astype(np.float32),\n                     }\n        self.run_transpose_compare([""res""], feed_dict, model_proto, remaining_transpose_num=1)\n\n    def test_transpose_relu(self):\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node2 = helper.make_node(""Relu"", [""Y""], [""Z""], name=""relu"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""Z1""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""relu-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z1"", TensorProto.FLOAT, (2, 3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z1""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_transpose_leaky_relu(self):\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node2 = helper.make_node(""LeakyRelu"", [""Y""], [""Z""], alpha=0.02, name=""relu"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""Z1""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""LeakyRelu-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z1"", TensorProto.FLOAT, (2, 3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z1""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    @check_opset_min_version(10, ""Slice in opset 10 can accept dymaic \'start\' and \'ends\'"")\n    def test_transpose_slice(self):\n        starts = np.array([0, 0, 0, 0], dtype=np.int64)\n        ends = np.array([1, 2, 1, 2], dtype=np.int64)\n        axes = np.array([0, 1, 2, 3], dtype=np.int64)\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node2 = helper.make_node(""Slice"", [""Y"", ""starts"", ""ends"", ""axes""], [""Z""], name=""relu"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""Z1""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""relu-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z1"", TensorProto.FLOAT, (1, 2, 2, 1))],\n            [\n                helper.make_tensor(""starts"", TensorProto.INT64, starts.shape, starts),\n                helper.make_tensor(""ends"", TensorProto.INT64, ends.shape, ends),\n                helper.make_tensor(""axes"", TensorProto.INT64, axes.shape, axes)\n            ]\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z1""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    @check_opset_min_version(8, ""Max in opset 10 supports broadcasting"")\n    def test_transpose_max(self):\n        const_1_val = [2.0]\n        const_1 = helper.make_tensor(""const_1"", TensorProto.FLOAT, (1,), const_1_val)\n        const_1_node = helper.make_node(""Constant"", [], [""const_1""], value=const_1, name=""const_1"")\n\n        const_2_val = np.random.randn(2, 4, 5, 3).astype(np.float32)\n        const_2 = helper.make_tensor(""const_2"", TensorProto.FLOAT, (2, 4, 5, 3), const_2_val.flatten())\n        const_2_node = helper.make_node(""Constant"", [], [""const_2""], value=const_2, name=""const_2"")\n\n        const_3_val = np.random.randn(2, 4, 5, 3).astype(np.float32)\n        const_3 = helper.make_tensor(""const_3"", TensorProto.FLOAT, (2, 4, 5, 3), const_3_val.flatten())\n        const_3_node = helper.make_node(""Constant"", [], [""const_3""], value=const_3, name=""const_3"")\n\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node2 = helper.make_node(""Max"", [""Y"", ""const_3"", ""const_2"", ""const_1""], [""Z""], name=""max"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""Z1""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [const_1_node, const_2_node, const_3_node, node1, node2, node3],\n            ""Max-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z1"", TensorProto.FLOAT, (2, 3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z1""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    @check_opset_min_version(8, ""Max in opset 10 supports broadcasting"")\n    def test_transpose_max_input_non_const(self):\n        const_1_val = [2.0]\n        const_1 = helper.make_tensor(""const_1"", TensorProto.FLOAT, (1,), const_1_val)\n        const_1_node = helper.make_node(""Constant"", [], [""const_1""], value=const_1, name=""const_1"")\n\n        const_2_val = np.random.randn(2, 4, 5, 3).astype(np.float32)\n        const_2 = helper.make_tensor(""const_2"", TensorProto.FLOAT, (2, 4, 5, 3), const_2_val.flatten())\n        const_2_node = helper.make_node(""Constant"", [], [""const_2""], value=const_2, name=""const_2"")\n\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node2 = helper.make_node(""Max"", [""Y"", ""non_const"", ""const_2"", ""const_1""], [""Z""], name=""max"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""Z1""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [const_1_node, const_2_node, node1, node2, node3],\n            ""Max-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5)),\n             helper.make_tensor_value_info(""non_const"", TensorProto.FLOAT, (2, 4, 5, 3))],\n            [helper.make_tensor_value_info(""Z1"", TensorProto.FLOAT, (2, 3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z1""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32),\n                                            ""non_const"": np.random.randn(2, 4, 5, 3).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=1)\n\n    def test_transpose_merge(self):\n        node0 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y_1""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node2 = helper.make_node(""Mul"", [""Y"", ""Y_1""], [""OUT""], name=""mul"")\n\n        graph = helper.make_graph(\n            [node0, node1, node2],\n            ""transpose-merge-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""OUT"", TensorProto.FLOAT, (2, 4, 5, 3))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""OUT""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=1)\n\n    def test_transpose_with_shape(self):\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node2 = helper.make_node(""Shape"", [""Y""], [""Z""], name=""shape"")\n\n        graph = helper.make_graph(\n            [node1, node2],\n            ""transpose_with_shape"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.INT64, [4])],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_transpose_with_identity(self):\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node2 = helper.make_node(""Identity"", [""Y""], [""Z""], name=""identity"")\n\n        graph = helper.make_graph(\n            [node1, node2],\n            ""transpose_with_identity"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (2, 4, 5, 3))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=1)\n\n    def test_transpose_with_squeeze1(self):\n        # squeeze the first dim\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node2 = helper.make_node(""Squeeze"", [""Y""], [""Z""], name=""squeeze"", axes=[0])\n\n        graph = helper.make_graph(\n            [node1, node2],\n            ""transpose_with_squeeze"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (4, 5, 3))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        model_after_opt = self.run_transpose_compare([""Z""], {""X"": np.random.randn(1, 3, 4, 5).astype(np.float32)},\n                                                     model_proto, remaining_transpose_num=1)\n        self.check_transpose_perm(model_after_opt, [1, 2, 0])\n\n    def test_transpose_with_squeeze2(self):\n        # squeeze the second dim\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node2 = helper.make_node(""Squeeze"", [""Y""], [""Z""], name=""squeeze"", axes=[1])\n\n        graph = helper.make_graph(\n            [node1, node2],\n            ""transpose_with_squeeze"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (3, 4, 1, 5))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (3, 5, 4))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        model_after_opt = self.run_transpose_compare([""Z""], {""X"": np.random.randn(3, 4, 1, 5).astype(np.float32)},\n                                                     model_proto, remaining_transpose_num=1)\n        self.check_transpose_perm(model_after_opt, [0, 2, 1])\n\n    def test_transpose_with_squeeze3(self):\n        # squeeze the last dim\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node2 = helper.make_node(""Squeeze"", [""Y""], [""Z""], name=""squeeze"", axes=[3])\n\n        graph = helper.make_graph(\n            [node1, node2],\n            ""transpose_with_squeeze"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (3, 1, 4, 5))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z""], {""X"": np.random.randn(3, 1, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_transpose_with_squeeze4(self):\n        # squeeze the two dims\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans"")\n        node2 = helper.make_node(""Squeeze"", [""Y""], [""Z""], name=""squeeze"", axes=[1, 3])\n\n        graph = helper.make_graph(\n            [node1, node2],\n            ""transpose_with_squeeze"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (3, 1, 1, 5))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (3, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Z""], {""X"": np.random.randn(3, 1, 1, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_transpose_with_loop(self):\n        def _define_loop_graph(external_inputs):\n            # external_inputs: external node which will be used by this graph\n            # graph without loop carried\n            # computation\n            # for(...){a = external_inputs[i]; b = trans(a), c = squeeze(b)}, c is scan output\n            node1 = helper.make_node(""Gather"", [external_inputs[0], ""loop_iter_num""], [""Y0""])\n            node2 = helper.make_node(""Transpose"", [""Y0""], [""Z0""], perm=[0, 2, 3, 1])\n            # graph output\n            node3 = helper.make_node(""Squeeze"", [""Z0""], [""scan_output""], axes=[0])\n            node4 = helper.make_node(""Identity"", [""loop_condition""], [""loop_cond_output""])\n            node5 = helper.make_node(""Identity"", [""loop_condition""], [""loop_carried_output""])\n\n            graph = helper.make_graph(\n                [node1, node2, node3, node4, node5],\n                ""loop_subgraph"",\n                [helper.make_tensor_value_info(""loop_iter_num"", TensorProto.INT64, (1,)),  # iteration_num\n                 helper.make_tensor_value_info(""loop_condition"", TensorProto.BOOL, ()),  # condition\n                 helper.make_tensor_value_info(""loop_carried"", TensorProto.BOOL, ())  # loop_carried\n                 ],\n                [helper.make_tensor_value_info(""loop_cond_output"", TensorProto.BOOL, ()),\n                 helper.make_tensor_value_info(""loop_carried_output"", TensorProto.BOOL, ()),\n                 helper.make_tensor_value_info(""scan_output"", TensorProto.FLOAT, [""unknown""] * 3)\n                 ],\n            )\n            return graph\n\n        def _make_loop(external_inputs, outputs):\n            trip_cnt = self._make_onnx_const(np.array(10, dtype=np.int64), ""trip_cnt"")\n            cond = self._make_onnx_const(np.array(True, dtype=np.bool), ""cond"")\n            sub_graph = _define_loop_graph(external_inputs)\n            loop_node = helper.make_node(""Loop"", [""trip_cnt"", ""cond"", ""cond""], outputs,\n                                         name=""loop"", body=sub_graph)\n            return trip_cnt, cond, loop_node\n\n        nodes = _make_loop([""array""], [""loop_carried"", ""scan_out""])\n        res = helper.make_node(""Transpose"", [""scan_out""], [""Y""], perm=[0, 3, 1, 2], name=""trans"")\n\n        graph = helper.make_graph(\n            [*nodes, res],\n            ""transpose_with_loop"",\n            [helper.make_tensor_value_info(""array"", TensorProto.FLOAT, [""unknow""] * 4)],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [""unknow""] * 4)],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""Y""], {""array"": np.random.randn(10, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_trans_with_sub(self):\n        io_shape = [2, 3, 4, 5]\n        const_shapes = [[2, 4, 5, 3], [4, 5, 3], [5, 3], [3]]\n        for trans_is_first_input in [True, False]:\n            for const_shape in const_shapes:\n                node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_a"")\n                const_tensor = helper.make_tensor(name=\'const\', data_type=TensorProto.FLOAT, dims=const_shape,\n                                                  vals=np.random.randn(*const_shape).flatten().astype(np.float32))\n                node2 = helper.make_node(""Constant"", [], [""const""], value=const_tensor, name=""const"")\n                if trans_is_first_input:\n                    node3 = helper.make_node(""Sub"", [""Y"", ""const""], [""Z""], name=""sub"")\n                else:\n                    node3 = helper.make_node(""Sub"", [""const"", ""Y""], [""Z""], name=""sub"")\n\n                node4 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_b"")\n                graph = helper.make_graph(\n                    [node1, node2, node3, node4],\n                    ""test_trans_with_sub"",\n                    [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, io_shape)],\n                    [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, io_shape)],\n                )\n\n                model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n                self.run_transpose_compare([""res""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                           model_proto, remaining_transpose_num=0)\n\n    def test_trans_with_sub_input_non_const(self):\n        io_shape = [2, 3, 4, 5]\n        non_const_shapes = [[2, 4, 5, 3], [4, 5, 3], [5, 3]]\n        for trans_is_first_input in [True, False]:\n            for non_const_shape in non_const_shapes:\n                node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_a"")\n                if trans_is_first_input:\n                    node2 = helper.make_node(""Sub"", [""Y"", ""non_const""], [""Z""], name=""sub"")\n                else:\n                    node2 = helper.make_node(""Sub"", [""non_const"", ""Y""], [""Z""], name=""sub"")\n\n                node3 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_b"")\n                graph = helper.make_graph(\n                    [node1, node2, node3],\n                    ""test_trans_with_sub_input_non_const"",\n                    [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, io_shape),\n                     helper.make_tensor_value_info(""non_const"", TensorProto.FLOAT, non_const_shape)],\n                    [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, io_shape)],\n                )\n\n                model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n                self.run_transpose_compare([""res""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32),\n                                                     ""non_const"": np.random.randn(*non_const_shape).astype(np.float32)},\n                                           model_proto, remaining_transpose_num=1)\n\n    def test_transpose_add_with_input_non_const(self):\n\n        node0 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node1 = helper.make_node(""Add"", [""Y"", ""A""], [""Z""], name=""add"")\n        node2 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [node0, node1, node2],\n            ""transpose-add-test-input-non-const"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 1, 3, 3)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (1, 3, 3, 1))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (1, 1, 3, 3))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""X"": np.random.randn(1, 1, 3, 3).astype(np.float32),\n                                             ""A"": np.random.randn(1, 3, 3, 1).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_transpose_add_with_input_const(self):\n        const_1_val = np.random.randn(1, 3, 3, 1).astype(np.float32)\n        const_1 = helper.make_tensor(""const_1"", TensorProto.FLOAT, (1, 3, 3, 1), const_1_val.flatten())\n        const_1_node = helper.make_node(""Constant"", [], [""const_1""], value=const_1, name=""const_1"")\n\n        node0 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node1 = helper.make_node(""Add"", [""Y"", ""const_1""], [""Z""], name=""add"")\n        node2 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [const_1_node, node0, node1, node2],\n            ""transpose-add-test-input-const"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 1, 3, 3))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (1, 1, 3, 3))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""X"": np.random.randn(1, 1, 3, 3).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_transpose_add_with_conv_1(self):\n        # case where bias\'s dim is 1D and can be merged into Conv\n        const_b_val = np.random.randn(1, 1, 1, 16).astype(np.float32)\n        const_b = helper.make_tensor(""const_b"", TensorProto.FLOAT, (1, 1, 1, 16), const_b_val.flatten())\n        const_b_node = helper.make_node(""Constant"", [], [""const_b""], value=const_b, name=""const_b"")\n\n        node0 = helper.make_node(""Conv"", [""x"", ""W""], [""X""], name=""conv"", pads=[0, 0, 0, 0])\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node2 = helper.make_node(""Add"", [""Y"", ""const_b""], [""Z""], name=""add"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [const_b_node, node0, node1, node2, node3],\n            ""transpose-add-test-with-conv-1"",\n            [helper.make_tensor_value_info(""x"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(""W"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (1, 16, 1, 1))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""x"": np.random.randn(1, 5, 3, 3).astype(np.float32),\n                                             ""W"": np.random.randn(16, 5, 3, 3).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_transpose_add_with_conv_2(self):\n        # case where bias\'s dim is not 1D and can\'t be merged into Conv\n        # add handler just remove the transpose around Add node\n        const_b_val = np.random.randn(1, 3, 3, 1).astype(np.float32)\n        const_b = helper.make_tensor(""const_b"", TensorProto.FLOAT, (1, 3, 3, 1), const_b_val.flatten())\n        const_b_node = helper.make_node(""Constant"", [], [""const_b""], value=const_b, name=""const_b"")\n\n        node0 = helper.make_node(""Conv"", [""x"", ""W""], [""X""], name=""conv"", pads=[0, 0, 0, 0])\n        node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node2 = helper.make_node(""Add"", [""Y"", ""const_b""], [""Z""], name=""add"")\n        node3 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [const_b_node, node0, node1, node2, node3],\n            ""transpose-add-test-with-conv-2"",\n            [helper.make_tensor_value_info(""x"", TensorProto.FLOAT, (1, 1, 5, 5)),\n             helper.make_tensor_value_info(""W"", TensorProto.FLOAT, (1, 1, 3, 3))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (1, 1, 3, 3))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""x"": np.random.randn(1, 1, 5, 5).astype(np.float32),\n                                             ""W"": np.random.randn(1, 1, 3, 3).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    @check_opset_max_version(10, ""pad"")\n    def test_transpose_pad(self):\n        node0 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node1 = helper.make_node(""Pad"", [""Y""], [""Z""], pads=[1, 0, 1, 3, 0, 0, 2, 0], name=""pad"")\n        node2 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [node0, node1, node2],\n            ""transpose-pad-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 3, 4, 5))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (2, 6, 4, 8))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""X"": np.random.randn(1, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    @check_opset_min_version(11, ""pad"")\n    def test_transpose_pad11(self):\n\n        pads_val = np.array([1, 0, 1, 3, 0, 0, 2, 0], dtype=np.int64)\n        pads_tensor = helper.make_tensor(""Pads"", TensorProto.INT64, [8], pads_val)\n        pads_const = helper.make_node(""Constant"", [], [""Pads""], value=pads_tensor, name=""Pads"")\n\n        node0 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node1 = helper.make_node(""Pad"", [""Y"", ""Pads""], [""Z""], name=""pad"")\n        node2 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [node0, node1, node2, pads_const],\n            ""transpose-pad-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 3, 4, 5))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (2, 6, 4, 8))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""X"": np.random.randn(1, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_transpose_reducemean(self):\n        node0 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node1 = helper.make_node(""ReduceMean"", [""Y""], [""Z""], axes=[1, 2], keepdims=1, name=""reducemean"")\n        node2 = helper.make_node(""Transpose"", [""Z""], [""res""], perm=[0, 3, 1, 2], name=""trans_2"")\n\n        graph = helper.make_graph(\n            [node0, node1, node2],\n            ""transpose-reducemean-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 3, 4, 5))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (1, 3, 1, 1))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""X"": np.random.randn(1, 3, 4, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_trans_output_as_graph_outputs(self):\n        """"""\n        If transpose\'s output is graph\'s output, don\'t optimize it.\n        """"""\n        trans = helper.make_node(""Transpose"", [""X""], [""Y""], name=""trans"", perm=[0, 2, 3, 1])\n        graph_proto = helper.make_graph(\n            [trans],\n            ""trans-to-graph-output"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 4, 5, 3))],\n        )\n\n        graph = GraphUtil.create_graph_from_onnx_graph(graph_proto)\n        # remove identity to graph output\n        identity_op = graph.get_node_by_output(graph.outputs[0])\n        graph.outputs = [identity_op.input[0]]\n        graph.remove_node(identity_op.name)\n\n        optimized_graph = GraphUtil.optimize_graph(graph)\n\n        self.assertTrue(optimized_graph, msg=""graph after optimizer should not be None"")\n\n        trans_cnt = len(group_nodes_by_type(optimized_graph)[""Transpose""])\n\n        self.assertTrue(trans_cnt == 1, msg=""Expect 1 Transpose ops left, but actually "" + str(trans_cnt) + "" left"")\n\n    def test_trans_can_be_replaced_with_reshape1(self):\n        # test trans-NHWC\n        input_shapes_np = [(2, 3, 4, 1), (2, 1, 1, 4), (2, 3, 4, 1)]\n        input_shapes = [(2, 3, 4, 1), (2, 1, 1, 4), (2, -1, -1, 1)]\n        perm = (0, 3, 1, 2)\n        for input_shape_np, input_shape in zip(input_shapes_np, input_shapes):\n            result_shape = [input_shape[i] for i in perm]\n            node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=perm, name=""trans"")\n            graph = helper.make_graph(\n                [node1],\n                ""test_trans_can_be_replaced_with_reshape"",\n                [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, input_shape)],\n                [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, result_shape)],\n            )\n\n            model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n            self.run_transpose_compare([""Y""], {""X"": np.random.randn(*input_shape_np).astype(np.float32)},\n                                       model_proto, remaining_transpose_num=0)\n\n    def test_trans_can_be_replaced_with_reshape2(self):\n        # test trans-NCHW\n        input_shapes_np = [(2, 1, 3, 4), (2, 4, 1, 1), (2, 1, 3, 4)]\n        input_shapes = [(2, 1, 3, 4), (2, 4, 1, 1), (2, 1, -1, -1)]\n        perm = (0, 2, 3, 1)\n        for input_shape_np, input_shape in zip(input_shapes_np, input_shapes):\n            result_shape = [input_shape[i] for i in perm]\n            node1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=perm, name=""trans"")\n            graph = helper.make_graph(\n                [node1],\n                ""test_trans_can_be_replaced_with_reshape"",\n                [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, input_shape)],\n                [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, result_shape)],\n            )\n\n            model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n            self.run_transpose_compare([""Y""], {""X"": np.random.randn(*input_shape_np).astype(np.float32)},\n                                       model_proto, remaining_transpose_num=0)\n\n    def test_two_transposes_switch_with_mul(self):\n        const_node = self._make_onnx_const(np.array(10, dtype=np.float32), ""const_10"")\n        node0 = helper.make_node(""Transpose"", [""u1""], [""v1""], perm=[0, 2, 3, 1], name=""trans_0"")\n        node1 = helper.make_node(""Transpose"", [""u2""], [""v2""], perm=[0, 2, 3, 1], name=""trans_1"")\n\n        node2 = helper.make_node(""Mul"", [""v1"", ""v2""], [""x""], name=""mul_1"")\n        node3 = helper.make_node(""Mul"", [""x"", const_node.output[0]], [""y""], name=""mul_2"")\n        node4 = helper.make_node(""Transpose"", [""y""], [""res""], perm=[0, 3, 1, 2], name=""trans_3"")\n\n        graph = helper.make_graph(\n            [const_node, node0, node1, node2, node3, node4],\n            ""test-transpose-mul"",\n            [helper.make_tensor_value_info(""u1"", TensorProto.FLOAT, (1, 6, 8, 9)),\n             helper.make_tensor_value_info(""u2"", TensorProto.FLOAT, (1, 6, 8, 9))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (1, 6, 8, 9))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""u1"": np.random.randn(1, 6, 8, 9).astype(np.float32),\n                                             ""u2"": np.random.randn(1, 6, 8, 9).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_many_transposes_and_constant_switch_with_sum(self):\n        constnode = self._make_onnx_const(np.array(np.random.random((1, 8, 9, 6)), dtype=np.float32), ""v4"")\n        node0 = helper.make_node(""Transpose"", [""u1""], [""v1""], perm=[0, 2, 3, 1], name=""trans_0"")\n        node1 = helper.make_node(""Transpose"", [""u2""], [""v2""], perm=[0, 2, 3, 1], name=""trans_1"")\n        node11 = helper.make_node(""Transpose"", [""u3""], [""v3""], perm=[0, 2, 3, 1], name=""trans_2"")\n\n        node2 = helper.make_node(""Sum"", [""v1"", ""v2"", ""v3"", ""v4""], [""x""], name=""sum_1"")\n        node3 = helper.make_node(""Sum"", [""x"", ""v1""], [""y""], name=""sum_2"")\n        node4 = helper.make_node(""Transpose"", [""y""], [""res""], perm=[0, 3, 1, 2], name=""trans_4"")\n\n        graph = helper.make_graph(\n            [constnode, node0, node1, node11, node2, node3, node4],\n            ""test-transpose-mul"",\n            [helper.make_tensor_value_info(""u1"", TensorProto.FLOAT, (1, 6, 8, 9)),\n             helper.make_tensor_value_info(""u2"", TensorProto.FLOAT, (1, 6, 8, 9)),\n             helper.make_tensor_value_info(""u3"", TensorProto.FLOAT, (1, 6, 8, 9))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (1, 6, 8, 9))],\n        )\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""u1"": np.random.randn(1, 6, 8, 9).astype(np.float32),\n                                             ""u2"": np.random.randn(1, 6, 8, 9).astype(np.float32),\n                                             ""u3"": np.random.randn(1, 6, 8, 9).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    # Tranpose Optimizer Tests End\n\n    # Identity Optimizer Tests Start\n\n    def run_identity_compare(self, output_names_with_port, onnx_feed_dict, origin_proto,\n                             remaining_identity_num=None, debug=False, rtol=1e-07):\n        self.run_and_compare(output_names_with_port, onnx_feed_dict, origin_proto, op_type=""Identity"",\n                             remaining_op_num=remaining_identity_num, debug=debug, rtol=rtol)\n\n    def test_identity_non_graph_output(self):\n        node1 = helper.make_node(""Add"", [""X"", ""X""], [""Y""], name=""add"")\n        node2 = helper.make_node(""Identity"", [""Y""], [""Z""], name=""identity"")\n        node3 = helper.make_node(""Shape"", [""Z""], [""Z1""], name=""shape"")\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""identity-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z1"", TensorProto.INT64, [4])],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_identity_compare([""Z1""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                  model_proto, remaining_identity_num=0)\n\n    def test_identity_unremovable_identity(self):\n        # should not remove!!\n        node1 = helper.make_node(""Identity"", [""X""], [""Y""], name=""identity"")\n\n        graph = helper.make_graph(\n            [node1],\n            ""identity-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_identity_compare([""Y""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                  model_proto, remaining_identity_num=1)\n\n    def test_identity_output_as_multiple_graph_outputs(self):\n        # handle case like this, both Identity nodes are graph outputs,\n        #            Add\n        #           /   \\\n        #    Identity   Identity\n        # We at most can remove one Identity for this case.\n        node1 = helper.make_node(""Add"", [""X"", ""X""], [""Y""], name=""identity"")\n        node2 = helper.make_node(""Identity"", [""Y""], [""Z1""], name=""identity2"")\n        node3 = helper.make_node(""Identity"", [""Y""], [""Z2""], name=""identity3"")\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""identity-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z1"", TensorProto.FLOAT, (2, 3, 4, 5)),\n             helper.make_tensor_value_info(""Z2"", TensorProto.FLOAT, (2, 3, 4, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_identity_compare([""Z1"", ""Z2""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                  model_proto, remaining_identity_num=1)\n\n    def test_identity_in_subgraph_non_graph_output(self):\n        node1 = helper.make_node(""Add"", [""X"", ""X""], [""Y""], name=""add"")\n\n        iter_num_value = np.array(1, dtype=np.int64)\n        node2 = helper.make_node(\n            \'Constant\',\n            inputs=[],\n            outputs=[\'iterate_num_value\'],\n            value=helper.make_tensor(\n                name=\'iterate_num_value\',\n                data_type=TensorProto.INT64,\n                dims=iter_num_value.shape,\n                vals=iter_num_value.flatten().astype(np.int64),\n            ),\n        )\n\n        cond_value = np.array(True, dtype=np.bool)\n        node3 = helper.make_node(\n            \'Constant\',\n            inputs=[],\n            outputs=[\'cond_value\'],\n            value=helper.make_tensor(\n                name=\'cond_value\',\n                data_type=TensorProto.BOOL,\n                dims=iter_num_value.shape,\n                vals=cond_value.flatten().astype(np.bool),\n            ),\n        )\n\n        # sub graph\n        sub_node1 = helper.make_node(""Add"", [""loop_var_1"", ""loop_var_1""], [""SubY""], name=""sub_add"")\n        sub_node2 = helper.make_node(""Identity"", [""SubY""], [""SubIdentity1""], name=""sub_identity_1"")\n        sub_node3 = helper.make_node(""Identity"", [""SubIdentity1""], [""loop_var_out_1""], name=""sub_identity_2"")\n        sub_node4 = helper.make_node(""Identity"", [""loop_condition""], [""loop_cond_output""], name=""sub_identity_3"")\n        sub_graph = helper.make_graph(\n            [sub_node1, sub_node2, sub_node3, sub_node4],\n            ""identity_subgraph-test"",\n            [helper.make_tensor_value_info(""loop_iter_num"", TensorProto.INT64, (1,)),  # iteration_num\n             helper.make_tensor_value_info(""loop_condition"", TensorProto.BOOL, ()),  # condition\n             helper.make_tensor_value_info(""loop_var_1"", TensorProto.FLOAT, ()),  # loop-carried dependency\n             ],\n            [helper.make_tensor_value_info(""loop_cond_output"", TensorProto.BOOL, ()),\n             helper.make_tensor_value_info(""loop_var_out_1"", TensorProto.FLOAT, ())\n             ],\n        )\n        # sub graph ends\n\n        loop_node = helper.make_node(""Loop"", [""iterate_num_value"", ""cond_value"", ""Y""], [""loop_var_1_output""],\n                                     name=""loop"", body=sub_graph)\n\n        node4 = helper.make_node(""Identity"", [""loop_var_1_output""], [""Z""], name=""identity"")\n        node5 = helper.make_node(""Shape"", [""Z""], [""Z1""], name=""shape"")\n\n        graph = helper.make_graph(\n            [node1, node2, node3, loop_node, node4, node5],\n            ""identity-test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4, 5))],\n            [helper.make_tensor_value_info(""Z1"", TensorProto.INT64, [4])],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_identity_compare([""Z1""], {""X"": np.random.randn(2, 3, 4, 5).astype(np.float32)},\n                                  model_proto, remaining_identity_num=0)\n\n    # Identity Optimizer Tests End\n\n    # Merge Duplicated Nodes Optimizer Tests Start\n\n    def run_merge_duplicated_nodes_compare(self, output_names_with_port, onnx_feed_dict, origin_proto,\n                                           op_type=None, remaining_op_num=None, debug=False, rtol=1e-07,\n                                           graph_validator=None):\n        new_proto = self.run_and_compare(output_names_with_port, onnx_feed_dict, origin_proto, op_type=op_type,\n                                         remaining_op_num=remaining_op_num, debug=debug, rtol=rtol)\n        if graph_validator:\n            self.assertTrue(graph_validator(new_proto.graph))\n\n    def test_duplicated_duplicated_input(self):\n        # same input or not\n        node0 = helper.make_node(\'Add\', inputs=[""X"", ""X""], outputs=[""value0""])\n        node1 = helper.make_node(\'Add\', inputs=[""X"", ""X""], outputs=[""value1""])\n        node2 = helper.make_node(\'Add\', inputs=[""value1"", ""X""], outputs=[""value2""])\n        node3 = helper.make_node(""Mul"", [""value0"", ""value2""], [""value3""])\n        node4 = helper.make_node(""Mul"", [""value1"", ""value3""], [""OUT""])\n\n        graph = helper.make_graph(\n            [node0, node1, node2, node3, node4],\n            ""test_duplicated_duplicated_input"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5, 5))],\n            [helper.make_tensor_value_info(""OUT"", TensorProto.FLOAT, (5, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_merge_duplicated_nodes_compare([""OUT""], {""X"": np.random.randn(5, 5).astype(np.float32)}, model_proto,\n                                                op_type=""Add"", remaining_op_num=2)\n\n    def test_duplicated_duplicated_attributes(self):\n        # same attr or not\n        node0 = helper.make_node(\'ReduceSum\', inputs=[""X""], outputs=[""value0""], axes=[0], keepdims=0)\n        node1 = helper.make_node(\'ReduceSum\', inputs=[""X""], outputs=[""value1""], axes=[0], keepdims=0)\n        node2 = helper.make_node(\'ReduceSum\', inputs=[""X""], outputs=[""value2""], axes=[1], keepdims=0)\n        node3 = helper.make_node(\'Add\', inputs=[""value0"", ""value1""], outputs=[""value3""])\n        node4 = helper.make_node(""Mul"", [""value2"", ""value3""], [""OUT""])\n\n        graph = helper.make_graph(\n            [node0, node1, node2, node3, node4],\n            ""test_duplicated_duplicated_attributes"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5, 5))],\n            [helper.make_tensor_value_info(""OUT"", TensorProto.FLOAT, (5,))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_merge_duplicated_nodes_compare([""OUT""], {""X"": np.random.randn(5, 5).astype(np.float32)}, model_proto,\n                                                op_type=""ReduceSum"", remaining_op_num=2)\n\n    def _check_initializer_num(self, graph_proto, num):\n        print(len(graph_proto.initializer))\n        return num == len(graph_proto.initializer)\n\n    def test_duplicated_duplicated_constant(self):\n        const_val = np.array([1, 2, 3], dtype=np.float32)\n        tensor_1 = helper.make_tensor(""tensor_1"", TensorProto.FLOAT, const_val.shape, const_val)\n        tensor_2 = helper.make_tensor(""tensor_2"", TensorProto.FLOAT, const_val.shape, const_val)\n        tensor_3 = helper.make_tensor(""tensor_3"", TensorProto.FLOAT, const_val.shape, const_val.tobytes(), raw=True)\n        tensor_4 = helper.make_tensor(""tensor_4"", TensorProto.FLOAT, const_val.shape, const_val.tobytes(), raw=True)\n        node0 = helper.make_node(\'Constant\', inputs=[], outputs=[""value0""], value=tensor_1)\n        node1 = helper.make_node(\'Constant\', inputs=[], outputs=[""value1""], value=tensor_2)\n        node2 = helper.make_node(\'Constant\', inputs=[], outputs=[""value2""], value=tensor_3)\n        node3 = helper.make_node(\'Constant\', inputs=[], outputs=[""value3""], value=tensor_4)\n        node4 = helper.make_node(""Mul"", [""value0"", ""value1""], [""output1""])\n        node5 = helper.make_node(""Mul"", [""value2"", ""output1""], [""output2""])\n        node6 = helper.make_node(""Mul"", [""value3"", ""output2""], [""OUT""])\n\n        graph = helper.make_graph(\n            [node0, node1, node2, node3, node4, node5, node6],\n            ""test_duplicated_duplicated_constant"",\n            [],\n            [helper.make_tensor_value_info(""OUT"", TensorProto.FLOAT, (3,))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_merge_duplicated_nodes_compare([""OUT""], {}, model_proto, op_type=""Constant"", remaining_op_num=0,\n                                                graph_validator=lambda g: self._check_initializer_num(g, 1))\n\n    def test_duplicated_duplicated_constant_and_initializer(self):\n        const_val = np.array([1, 2, 3], dtype=np.float32)\n        tensor_1 = helper.make_tensor(""value0"", TensorProto.FLOAT, const_val.shape, const_val)\n        tensor_2 = helper.make_tensor(""value1"", TensorProto.FLOAT, const_val.shape, const_val)\n        tensor_3 = helper.make_tensor(""value2"", TensorProto.FLOAT, const_val.shape, const_val.tobytes(), raw=True)\n        tensor_4 = helper.make_tensor(""value3"", TensorProto.FLOAT, const_val.shape, const_val.tobytes(), raw=True)\n        node0 = helper.make_node(\'Constant\', inputs=[], outputs=[""value0""], value=tensor_1)\n        node1 = helper.make_node(\'Constant\', inputs=[], outputs=[""value1""], value=tensor_2)\n        node4 = helper.make_node(""Mul"", [""value0"", ""value1""], [""output1""])\n        node5 = helper.make_node(""Mul"", [""value2"", ""output1""], [""output2""])\n        node6 = helper.make_node(""Mul"", [""value3"", ""output2""], [""OUT""])\n\n        graph = helper.make_graph(\n            [node0, node1, node4, node5, node6],\n            ""test_duplicated_duplicated_constant"",\n            [helper.make_tensor_value_info(""value2"", TensorProto.FLOAT, (3,))],\n            [helper.make_tensor_value_info(""OUT"", TensorProto.FLOAT, (3,))],\n            [tensor_3, tensor_4]\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_merge_duplicated_nodes_compare([""OUT""], {}, model_proto, op_type=""Constant"", remaining_op_num=0,\n                                                graph_validator=lambda g: self._check_initializer_num(g, 2))\n\n    def test_duplicated_node_is_graph_output(self):\n        node0 = helper.make_node(\'Add\', inputs=[""X"", ""X""], outputs=[""value0""])\n        node1 = helper.make_node(\'Add\', inputs=[""X"", ""X""], outputs=[""value1""])\n        node2 = helper.make_node(\'Add\', inputs=[""value1"", ""X""], outputs=[""value2""])\n\n        graph = helper.make_graph(\n            [node0, node1, node2],\n            ""test_duplicated_node_is_graph_output"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5, 5))],\n            [helper.make_tensor_value_info(""value1"", TensorProto.FLOAT, (5, 5)),\n             helper.make_tensor_value_info(""value2"", TensorProto.FLOAT, (5, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_merge_duplicated_nodes_compare([""value1"", ""value2""],\n                                                {""X"": np.random.randn(5, 5).astype(np.float32)}, model_proto,\n                                                op_type=""Add"", remaining_op_num=2)\n\n    @check_opset_min_version(10, ""Dropout in opset 10 produces mask of \'bool\' type"")\n    def test_duplicated_different_output_length(self):\n        node0 = helper.make_node(\'Dropout\', inputs=[""X""], outputs=[""value0""])\n        node1 = helper.make_node(\'Dropout\', inputs=[""X""], outputs=[""value1"", ""mask""])\n        node2 = helper.make_node(\'Dropout\', inputs=[""value1""], outputs=[""value2""])\n\n        graph = helper.make_graph(\n            [node0, node1, node2],\n            ""test_duplicated_different_output_length"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""value1"", TensorProto.FLOAT, (5,)),\n             helper.make_tensor_value_info(""mask"", TensorProto.BOOL, (5,)),\n             helper.make_tensor_value_info(""value2"", TensorProto.FLOAT, (5,))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_merge_duplicated_nodes_compare([""value1"", ""mask"", ""value2""],\n                                                {""X"": np.random.randn(5).astype(np.float32)},\n                                                model_proto,\n                                                op_type=""Dropout"", remaining_op_num=2)\n\n    def test_duplicated_need_multiple_run(self):\n        node00 = helper.make_node(\'Log\', inputs=[""X""], outputs=[""value00""])\n        node01 = helper.make_node(\'Log\', inputs=[""value00""], outputs=[""value01""])\n        node02 = helper.make_node(\'Log\', inputs=[""value01""], outputs=[""value02""])\n\n        node10 = helper.make_node(\'Log\', inputs=[""X""], outputs=[""value10""])\n        node11 = helper.make_node(\'Log\', inputs=[""value10""], outputs=[""value11""])\n        node12 = helper.make_node(\'Log\', inputs=[""value11""], outputs=[""value12""])\n\n        res = helper.make_node(\'Add\', inputs=[""value02"", ""value12""], outputs=[""res""])\n\n        graph = helper.make_graph(\n            [node00, node01, node02, node10, node11, node12, res],\n            ""test_duplicated_node_is_graph_output"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (5,))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_merge_duplicated_nodes_compare([""res""], {""X"": np.random.randn(5).astype(np.float32)},\n                                                model_proto,\n                                                op_type=""Log"", remaining_op_num=3)\n\n    # Merge Duplicated Nodes Optimizer Tests End\n\n    # Const Fold Optimizer Tests Start\n\n    def test_const_fold_trans_with_const1(self):\n        shape = (6, 6)\n        const_tensor = helper.make_tensor(name=\'const_tensor\', data_type=TensorProto.FLOAT, dims=shape,\n                                          vals=np.random.randn(*shape).flatten().astype(np.float32))\n        node1 = helper.make_node(""Constant"", [], [""const""], value=const_tensor)\n        node2 = helper.make_node(""Transpose"", [""const""], [""value1""])\n        node3 = helper.make_node(""Add"", [""value1"", ""X""], [""res""])\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""test_const_fold_trans_with_const1"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, shape)],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, shape)],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""X"": np.random.randn(*shape).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_const_fold_trans_with_const2(self):\n        # need multiple optimization run\n        shape = (6, 6)\n        const_tensor = helper.make_tensor(name=\'const_tensor\', data_type=TensorProto.FLOAT, dims=shape,\n                                          vals=np.random.randn(*shape).flatten().astype(np.float32))\n        node1 = helper.make_node(""Constant"", [], [""const""], value=const_tensor)\n        node2 = helper.make_node(""Transpose"", [""const""], [""value1""])\n        node3 = helper.make_node(""Transpose"", [""value1""], [""value2""])\n        node4 = helper.make_node(""Add"", [""value2"", ""X""], [""res""])\n\n        graph = helper.make_graph(\n            [node1, node2, node3, node4],\n            ""test_const_fold_trans_with_const2"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, shape)],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, shape)],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""X"": np.random.randn(*shape).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_const_fold_node_is_output(self):\n        # need multiple optimization run\n        shape = (6, 6)\n        const_tensor = helper.make_tensor(name=\'const_tensor\', data_type=TensorProto.FLOAT, dims=shape,\n                                          vals=np.random.randn(*shape).flatten().astype(np.float32))\n        node1 = helper.make_node(""Constant"", [], [""const""], value=const_tensor)\n        node2 = helper.make_node(""Transpose"", [""const""], [""value1""])\n        node3 = helper.make_node(""Transpose"", [""value1""], [""res""])\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""test_const_fold_node_is_output"",\n            [],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, shape)],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {},\n                                   model_proto, remaining_transpose_num=0)\n\n    def test_const_fold_unsqueeze_with_const(self):\n        shape = (6, 6)\n        const_tensor = helper.make_tensor(name=\'const_tensor\', data_type=TensorProto.FLOAT, dims=shape,\n                                          vals=np.random.randn(*shape).flatten().astype(np.float32))\n        node1 = helper.make_node(""Constant"", [], [""const""], value=const_tensor)\n        node2 = helper.make_node(""Unsqueeze"", [""const""], [""value1""], axes=[0, 2, 3])\n        node3 = helper.make_node(""Add"", [""value1"", ""X""], [""res""])\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""test_const_fold_unsqueeze_with_const"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (1, 6, 1, 1, 6))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_and_compare([""res""], {""X"": np.random.randn(1).astype(np.float32)}, model_proto,\n                             ""Unsqueeze"", 0)\n\n    def test_const_fold_cast_with_const(self):\n        shape = (6, 6)\n        const_tensor = helper.make_tensor(name=\'const_tensor\', data_type=TensorProto.FLOAT, dims=shape,\n                                          vals=np.random.randn(*shape).flatten().astype(np.float32))\n        node1 = helper.make_node(""Constant"", [], [""const""], value=const_tensor)\n        node2 = helper.make_node(""Cast"", [""const""], [""value1""], to=TensorProto.INT64)\n        node3 = helper.make_node(""Add"", [""value1"", ""X""], [""res""])\n\n        graph = helper.make_graph(\n            [node1, node2, node3],\n            ""test_const_fold_cast_with_const"",\n            [helper.make_tensor_value_info(""X"", TensorProto.INT64, shape)],\n            [helper.make_tensor_value_info(""res"", TensorProto.INT64, shape)],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_and_compare([""res""], {""X"": np.random.randn(*shape).astype(np.int64)}, model_proto,\n                             ""Cast"", 0)\n\n    # Const Fold Optimizer Tests End\n\n    def test_transpose_back_to_back_non_const(self):\n\n        node0 = helper.make_node(""Transpose"", [""u""], [""v""], perm=[0, 2, 3, 1], name=""trans_0"")\n        node1 = helper.make_node(""Transpose"", [""v""], [""w""], perm=[0, 3, 1, 2], name=""trans_1"")\n        node2 = helper.make_node(""Transpose"", [""w""], [""x""], perm=[0, 3, 2, 1], name=""trans_2"")\n        node3 = helper.make_node(""Transpose"", [""x""], [""res""], perm=[1, 3, 0, 2], name=""trans_3"")\n\n        graph = helper.make_graph(\n            [node0, node1, node2, node3],\n            ""test-transpose-back-to-back-non-const"",\n            [helper.make_tensor_value_info(""u"", TensorProto.FLOAT, (5, 5, 5, 5))],\n            [helper.make_tensor_value_info(""res"", TensorProto.FLOAT, (5, 5, 5, 5))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n        self.run_transpose_compare([""res""], {""u"": np.random.randn(5, 5, 5, 5).astype(np.float32)},\n                                   model_proto, remaining_transpose_num=1)\n\n    @check_opset_min_version(9, ""string type tensor"")\n    def test_cast_back_to_back_non_const_mixed_types(self):\n        node0 = helper.make_node(""Cast"", [""u""], [""v""], to=11, name=""cast_0"")  # double\n        node1 = helper.make_node(""Cast"", [""v""], [""w""], to=6, name=""cast_1"")  # int32\n        node2 = helper.make_node(""Cast"", [""w""], [""x""], to=1, name=""cast_2"")  # float\n        node3 = helper.make_node(""Cast"", [""x""], [""res""], to=7, name=""cast_3"")  # int64\n\n        node4 = helper.make_node(""Cast"", [""w""], [""w2""], to=6, name=""cast_4"")  # int32\n        node5 = helper.make_node(""Cast"", [""w2""], [""res2""], to=7, name=""cast_5"")  # int64\n\n        node6 = helper.make_node(""Cast"", [""x""], [""x2""], to=9, name=""cast_6"")  # bool\n        # TODO: uncomment below after fix\n        # https://github.com/microsoft/onnxruntime/issues/2338\n        # node7 = helper.make_node(""Cast"", [""x2""], [""x3""], to=8, name=""cast_7"")  # string\n        node8 = helper.make_node(""Cast"", [""x2""], [""res3""], to=3, name=""cast_8"")  # int8\n\n        graph = helper.make_graph(\n            [node0, node1, node2, node3, node4, node5, node6, node8],\n            ""test-cast-back-to-back-non-const"",\n            [helper.make_tensor_value_info(""u"", TensorProto.FLOAT, (1, 2, 3))],\n            [helper.make_tensor_value_info(""res"", TensorProto.INT64, (1, 2, 3)),\n             helper.make_tensor_value_info(""res2"", TensorProto.INT64, (1, 2, 3)),\n             helper.make_tensor_value_info(""res3"", TensorProto.INT8, (1, 2, 3))],\n        )\n\n        model_proto = self.make_model(graph, producer_name=""onnx-tests"")\n\n        self.run_and_compare([""res"", ""res2"", ""res3""], {""u"": np.random.randn(1, 2, 3).astype(np.float32)}, model_proto,\n                             ""Cast"", 5)\n\n\nif __name__ == ""__main__"":\n    unittest_main()\n'"
tests/test_seq2seq.py,37,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n"""""" Unit Tests for tf.contrib.seq2seq """"""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import init_ops\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_tf_max_version\nfrom tf2onnx.tf_loader import is_tf2\n\n# pylint: disable=invalid-name\n\nif is_tf2():\n    BasicLSTMCell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell\n    LSTMCell = tf.compat.v1.nn.rnn_cell.LSTMCell\n    RNNCell = tf.compat.v1.nn.rnn_cell.RNNCell\n    MultiRNNCell = tf.compat.v1.nn.rnn_cell.MultiRNNCell\n    dynamic_rnn = tf.compat.v1.nn.dynamic_rnn\n    bidirectional_dynamic_rnn = tf.compat.v1.nn.bidirectional_dynamic_rnn\n    LSTMStateTuple = tf.compat.v1.nn.rnn_cell.LSTMStateTuple\nelse:\n    LSTMCell = tf.contrib.rnn.LSTMCell\n    LSTMBlockCell = tf.contrib.rnn.LSTMBlockCell\n    RNNCell = tf.nn.rnn_cell.RNNCell\n    MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n    dynamic_rnn = tf.nn.dynamic_rnn\n    LSTMStateTuple = tf.nn.rnn_cell.LSTMStateTuple\n\n# pylint: enable=invalid-name\n# pylint: disable=missing-docstring\n\nclass Seq2SeqTests(Tf2OnnxBackendTestBase):\n    @check_tf_max_version(""1.15"", ""FIXME - need replacement for tf.contrib in tf-2.x"")\n    def test_dynamic_decode_maximum_iterations(self):\n        batch_size = 2\n        num_units = 4\n        vocab_size = 5\n        embedding_size = 3\n        go_token = 0\n        end_token = 1\n\n        def func():\n            embedding = tf.constant(np.ones([vocab_size, embedding_size], dtype=np.float32))\n            state_val = np.reshape([np.ones([num_units], dtype=np.float32) * i for i in range(batch_size)],\n                                   [batch_size, num_units])\n            encoder_state = tf.nn.rnn_cell.LSTMStateTuple(state_val, state_val)\n            initializer = init_ops.constant_initializer(0.5)\n            cell = LSTMCell(\n                num_units=num_units,\n                initializer=initializer,\n                state_is_tuple=True)\n            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n                embedding=embedding,\n                start_tokens=tf.tile([go_token], [batch_size]),\n                end_token=end_token)\n            output_layer = tf.layers.Dense(vocab_size, kernel_initializer=initializer)\n            decoder = tf.contrib.seq2seq.BasicDecoder(\n                cell=cell,\n                helper=helper,\n                initial_state=encoder_state,\n                output_layer=output_layer)\n            outputs, state, sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n                decoder=decoder,\n                maximum_iterations=6)\n\n            return tf.identity(outputs.rnn_output, name=""rnn_output""), \\\n                   tf.identity(outputs.sample_id, name=""sample_id""), \\\n                   tf.identity(state, name=""state""), \\\n                   tf.identity(sequence_lengths, name=""sequence_lengths"")\n\n        output_names_with_port = [\n            ""rnn_output:0"",\n            # ""sample_id:0"",  # incomplete type support for Transpose on onnxruntime 0.2.1\n            ""state:0"",\n        ]\n        self.run_test_case(func, {}, [], output_names_with_port, atol=1e-06, rtol=1e-6)\n\n    @check_tf_max_version(""1.15"", ""FIXME - need replacement for tf.contrib in tf-2.x"")\n    def test_dynamic_decode_normal_stop(self):\n        batch_size = 2\n        num_units = 4\n        vocab_size = 5\n        embedding_size = 3\n        go_token = 0\n        end_token = 1\n\n        def func():\n            embedding = tf.constant(np.ones([vocab_size, embedding_size], dtype=np.float32))\n            state_val = np.reshape([np.ones([num_units], dtype=np.float32) * i for i in range(batch_size)],\n                                   [batch_size, num_units])\n            encoder_state = LSTMStateTuple(state_val, state_val)\n\n            cell_initializer = init_ops.constant_initializer(\n                np.array([[-0.9592235, 0.42451382, 0.7437744, -0.54485345, -0.80763197,\n                           0.19663906, -0.22738314, 0.7762785, 0.7464578, 0.27227187,\n                           0.7661047, 0.3596425, -0.8528242, -0.89316916, -0.48946142,\n                           0.87882376],\n                          [0.86586094, -0.75018406, 0.25992537, -0.69368935, 0.2515502,\n                           -0.26379275, 0.8954313, 0.5759742, -0.7753072, -0.4388857,\n                           0.95751476, -0.82085776, -0.9467752, -0.37055635, -0.18570113,\n                           -0.86504984],\n                          [0.02305841, 0.3850248, 0.893692, -0.6866486, -0.83703446,\n                           -0.9828961, 0.3989377, -0.59993076, 0.5330808, 0.6916566,\n                           0.98468065, -0.6047034, 0.10823512, 0.34599304, -0.7834821,\n                           -0.7852347],\n                          [0.81643987, 0.31507468, -0.51369476, -0.12273741, 0.9701307,\n                           -0.79669356, -0.34496522, -0.88750815, -0.17995334, 0.34707904,\n                           -0.09201193, 0.5363934, -0.87229705, -0.5073328, -0.95894027,\n                           0.5481839],\n                          [-0.84093595, -0.2341497, -0.86047816, 0.43370056, -0.39073753,\n                           0.37730122, 0.48026466, 0.3004985, -0.60727096, 0.9043884,\n                           -0.37619448, 0.22490788, -0.03739262, 0.61672115, 0.478899,\n                           -0.40780973],\n                          [0.31202435, -0.22045255, -0.6087918, 0.95115066, 0.00199413,\n                           -0.688287, -0.1103518, 0.4169519, 0.7913246, -0.9844644,\n                           -0.6193857, 0.38659644, -0.4726901, -0.44781208, -0.5174744,\n                           -0.605911],\n                          [0.66771054, 0.34912825, 0.22297978, -0.4990945, 0.24057317,\n                           -0.5540829, 0.92277217, 0.74939895, -0.35278273, -0.21587133,\n                           -0.28613377, -0.8794241, -0.40119147, 0.67175174, -0.22741508,\n                           0.37898326]], dtype=np.float32))\n            dense_initializer = init_ops.constant_initializer(\n                np.array([[0.56177187, -0.6233454, 0.73997784, 0.35032558, 0.6479795],\n                          [0.6831174, -0.34233975, 0.39330363, 0.45177555, -0.49649096],\n                          [-0.98890066, 0.6175642, 0.09800482, -0.6721206, 0.48805737],\n                          [0.19671416, 0.2623148, 0.742548, 0.13555217, 0.56009054]], dtype=np.float32))\n\n            cell = LSTMCell(\n                num_units=num_units,\n                initializer=cell_initializer,\n                state_is_tuple=True)\n\n            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n                embedding=embedding,\n                start_tokens=tf.tile([go_token], [batch_size]),\n                end_token=end_token)\n\n            output_layer = tf.layers.Dense(vocab_size, kernel_initializer=dense_initializer)\n            decoder = tf.contrib.seq2seq.BasicDecoder(\n                cell=cell,\n                helper=helper,\n                initial_state=encoder_state,\n                output_layer=output_layer)\n\n            outputs, state, sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n                decoder=decoder,\n                maximum_iterations=6)\n\n            return tf.identity(outputs.rnn_output, name=""rnn_output""), \\\n                   tf.identity(outputs.sample_id, name=""sample_id""), \\\n                   tf.identity(state, name=""state""), \\\n                   tf.identity(sequence_lengths, name=""sequence_lengths"")\n\n        output_names_with_port = [\n            ""rnn_output:0"",\n            # ""sample_id:0"",  # incomplete type support for Transpose on onnxruntime 0.2.1\n            ""state:0"",\n        ]\n\n        self.run_test_case(func, {}, [], output_names_with_port, atol=1e-06, rtol=1e-6)\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_stacked_lstm.py,9,"b'""""""Unit Tests for layered lstm""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import init_ops\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import unittest_main, check_lstm_count, check_opset_after_tf_version, skip_tf2\n\nfrom tf2onnx.tf_loader import is_tf2\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,cell-var-from-loop\n# pylint: disable=invalid-name\n\nif is_tf2():\n    LSTMCell = tf.compat.v1.nn.rnn_cell.LSTMCell\n    MultiRNNCell = tf.compat.v1.nn.rnn_cell.MultiRNNCell\n    dynamic_rnn = tf.compat.v1.nn.dynamic_rnn\nelse:\n    LSTMCell = tf.contrib.rnn.LSTMCell\n    LSTMBlockCell = tf.contrib.rnn.LSTMBlockCell\n    MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n    dynamic_rnn = tf.nn.dynamic_rnn\n\n\nclass LSTMLayeredTests(Tf2OnnxBackendTestBase):\n    @check_opset_after_tf_version(""1.15"", 8, ""might need Scan"")\n    @skip_tf2()\n    def test_layered_lstm(self):\n        units = 5\n        batch_size = 6\n        x_val = np.array([[1., 1.], [2., 2.], [3., 3.], [4., 4.]], dtype=np.float32)\n        x_val = np.stack([x_val] * batch_size)\n\n        def func(x):\n            initializer = init_ops.constant_initializer(0.5)\n            num_layers = 2\n\n            # no scope\n            def lstm_cell():\n                return LSTMCell(\n                    units,\n                    initializer=initializer,\n                    state_is_tuple=True)\n\n            stacked_lstm = MultiRNNCell(\n                [lstm_cell() for _ in range(num_layers)])\n            outputs, cell_state = dynamic_rnn(\n                stacked_lstm,\n                x,\n                dtype=tf.float32)\n            return tf.identity(outputs, name=""output""), tf.identity(cell_state, name=""cell_state"")\n\n        input_names_with_port = [""input_1:0""]\n        feed_dict = {""input_1:0"": x_val}\n\n        output_names_with_port = [""output:0"", ""cell_state:0""]\n        self.run_test_case(func, feed_dict, input_names_with_port, output_names_with_port, rtol=1e-06,\n                           graph_validator=lambda g: check_lstm_count(g, 2))\n\n\nif __name__ == \'__main__\':\n    unittest_main()\n'"
tests/test_tf_shape_inference.py,72,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Unit Tests for Tensorflow shape inference.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import variables as variables_lib\nfrom tensorflow.python.ops import init_ops\n\nfrom backend_test_base import Tf2OnnxBackendTestBase\nfrom common import *  # pylint: disable=wildcard-import, unused-wildcard-import\nfrom tf2onnx import utils\nfrom tf2onnx.tf_utils import get_tf_tensor_shape\nfrom tf2onnx.shape_inference import infer_shape_for_graph\nfrom tf2onnx.tf_loader import tf_reset_default_graph, tf_session, tf_placeholder, tf_optimize\n\n# pylint: disable=missing-docstring\n\n\nclass TFShapeInferenceTests(Tf2OnnxBackendTestBase):\n    def _run_test_case(self, input_names_with_port, output_names_with_port):\n        try:\n            tf.compat.v1.disable_eager_execution()\n        except:  # pylint: disable=bare-except\n            pass\n        graph_def = None\n        with tf_session() as sess:\n            # freeze graph\n            origin_graph = sess.graph\n            variables_lib.global_variables_initializer().run()\n            output_name_without_port = [n.split(\':\')[0] for n in output_names_with_port]\n            graph_def = tf.graph_util.convert_variables_to_constants(\n                sess, sess.graph_def,\n                output_name_without_port\n            )\n\n        tf_reset_default_graph()\n        tf.import_graph_def(graph_def, name=\'\')\n\n        # optimize graph\n        graph_def = tf_optimize(input_names_with_port, output_names_with_port, sess.graph_def, True)\n\n        with tf_session() as sess:\n            if self.config.is_debug_mode:\n                if not os.path.exists(self.test_data_directory):\n                    os.makedirs(self.test_data_directory)\n                model_path = os.path.join(self.test_data_directory, self._testMethodName + ""_after_tf_optimize.pb"")\n                utils.save_protobuf(model_path, graph_def)\n                self.logger.debug(""created file  %s"", model_path)\n\n        tf_reset_default_graph()\n        tf.import_graph_def(graph_def, name=\'\')\n\n        with tf_session() as sess:\n            inferred_graph = infer_shape_for_graph(sess.graph)\n            # compare each operation\n            for op in origin_graph.get_operations():\n                inferred_op = None\n                try:\n                    inferred_op = inferred_graph.get_operation_by_name(op.name)\n                except KeyError:\n                    continue\n                self._compare_shape_for_op(op, inferred_op)\n\n    def _compare_shape_for_op(self, op1, op2):\n        """"""Align outputs of op2 to op1.""""""\n        for out1, out2 in zip(op1.outputs, op2.outputs):\n            expected_shape = get_tf_tensor_shape(out1)\n            if out1 is not None:\n                actual_shape = get_tf_tensor_shape(out2)\n                self.assertTrue(utils.are_shapes_compatible(expected_shape, actual_shape))\n\n    @check_tf_max_version(""1.15"", ""_run_test_case needs to supported tf-2"")\n    def test_while_loop_with_ta_read_and_write(self):\n        i = tf_placeholder(tf.int32, (), name=""input_1"")\n        inputs = tf_placeholder(tf.float32, (10,), name=""input_2"")\n\n        inputs_2 = tf.identity(inputs)\n        input_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_2)\n        output_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n\n        c = lambda i, *_: tf.logical_and(tf.less(i, 10), i >= 0)\n\n        def b(i, out_ta):\n            new_i = tf.add(i, 1)\n            x = input_ta.read(i)\n            x = x + 3\n            out_ta_new = out_ta.write(i, x)\n            return new_i, out_ta_new\n\n        i_final, out_final = tf.while_loop(c, b, [i, output_ta])\n        _ = tf.identity(i_final, name=""i"")\n        _ = tf.identity(out_final.stack(), name=""output_ta"")\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n\n        output_names_with_port = [""i:0"", ""output_ta:0""]\n        self._run_test_case(input_names_with_port, output_names_with_port)\n\n    @check_tf_max_version(""1.15"", ""_run_test_case needs to supported tf-2"")\n    def test_map_fn(self):\n        def fn0(elem):\n            res = elem + elem * elem\n            return res\n\n        def fn1(elem):\n            res = elem[0] * elem[1] + elem[0]\n            return res\n\n        x_val = 100 * np.random.random_sample([2, 10]).astype(np.float32)\n        y_val = 100 * np.random.random_sample([2, 10]).astype(np.float32)\n\n        # test fn0\n        x = tf_placeholder(tf.float32, shape=x_val.shape, name=""input_0"")\n        x_ = tf.identity(x)\n        res_ = tf.map_fn(fn0, x_, dtype=tf.float32)\n        _ = tf.identity(res_, name=""output_0"")\n        input_names_with_port = [""input_0:0""]\n        output_names_with_port = [""output_0:0""]\n        self._run_test_case(input_names_with_port, output_names_with_port)\n        tf_reset_default_graph()\n\n        # test fn1\n        x = tf_placeholder(tf.float32, shape=x_val.shape, name=""input_0"")\n        y = tf_placeholder(tf.float32, shape=y_val.shape, name=""input_1"")\n        x_ = tf.identity(x)\n        y_ = tf.identity(y)\n        res_ = tf.map_fn(fn1, (x_, y_), dtype=tf.float32)\n        _ = tf.identity(res_, name=""output_0"")\n        input_names_with_port = [""input_0:0"", ""input_1:0""]\n        output_names_with_port = [""output_0:0""]\n        self._run_test_case(input_names_with_port, output_names_with_port)\n\n    @check_tf_max_version(""1.15"", ""_run_test_case needs to supported tf-2"")\n    def test_bidrectional_attention_wrapper_lstm_encoder(self):\n        size = 30\n        time_step = 3\n        input_size = 4\n        attn_size = size\n        batch_size = 9\n\n        # shape  [batch size, time step, size]\n        # attention_state: usually the output of an RNN encoder.\n        # This tensor should be shaped `[batch_size, max_time, ...]`\n        encoder_time_step = time_step\n        encoder_x_val = np.random.randn(encoder_time_step, input_size).astype(\'f\')\n        encoder_x_val = np.stack([encoder_x_val] * batch_size)\n        encoder_x = tf_placeholder(tf.float32, encoder_x_val.shape, name=""input_1"")\n        encoder_cell = tf.nn.rnn_cell.LSTMCell(size)\n        attention_states, _ = tf.nn.dynamic_rnn(encoder_cell, encoder_x, dtype=tf.float32)\n        # [9, 3, 30], [9, 30]\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(attn_size,\n                                                                   attention_states)\n\n        match_input_fn = lambda curr_input, state: tf.concat([curr_input, state], axis=-1)\n        cell = tf.nn.rnn_cell.LSTMCell(size)\n        match_cell_fw = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                            attention_mechanism,\n                                                            attention_layer_size=attn_size,\n                                                            cell_input_fn=match_input_fn,\n                                                            output_attention=False)\n        match_cell_bk = tf.contrib.seq2seq.AttentionWrapper(cell,\n                                                            attention_mechanism,\n                                                            attention_layer_size=attn_size,\n                                                            cell_input_fn=match_input_fn,\n                                                            output_attention=False)\n\n        decoder_time_step = 6\n        decoder_x_val = np.random.randn(decoder_time_step, batch_size, input_size).astype(\'f\')\n\n        decoder_x = tf_placeholder(tf.float32, decoder_x_val.shape, name=""input_2"")\n        seq_length = tf_placeholder(tf.int32, (batch_size), name=""input_3"")\n        (match_output_fw, match_output_bk), (match_state_fw, match_state_bk) = \\\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=match_cell_fw,\n                                            cell_bw=match_cell_bk,\n                                            inputs=decoder_x,\n                                            sequence_length=tf.identity(seq_length),\n                                            dtype=tf.float32,\n                                            time_major=True)\n\n        matched_output = tf.concat([match_output_fw, match_output_bk], axis=-1)\n        matched_state = tf.concat([match_state_fw.cell_state, match_state_bk.cell_state], -1)\n\n        _ = tf.identity(matched_output, name=""output_0"")\n        _ = tf.identity(matched_state, name=""final_state"")\n\n        input_names_with_port = [""input_1:0"", ""input_2:0"", ""input_3:0""]\n        output_names_with_port = [""output_0:0"", ""final_state:0""]\n        self._run_test_case(input_names_with_port, output_names_with_port)\n\n    @check_tf_max_version(""1.15"", ""_run_test_case needs to supported tf-2"")\n    def test_dynamic_decode_normal_stop(self):\n        batch_size = 2\n        num_units = 4\n        vocab_size = 5\n        embedding_size = 3\n        go_token = 0\n        end_token = 1\n\n        embedding = tf.constant(np.ones([vocab_size, embedding_size], dtype=np.float32))\n        state_val = np.reshape([np.ones([num_units], dtype=np.float32) * i for i in range(batch_size)],\n                               [batch_size, num_units])\n        encoder_state = tf.nn.rnn_cell.LSTMStateTuple(state_val, state_val)\n\n        cell_initializer = init_ops.constant_initializer(\n            np.array([[-0.9592235, 0.42451382, 0.7437744, -0.54485345, -0.80763197,\n                       0.19663906, -0.22738314, 0.7762785, 0.7464578, 0.27227187,\n                       0.7661047, 0.3596425, -0.8528242, -0.89316916, -0.48946142,\n                       0.87882376],\n                      [0.86586094, -0.75018406, 0.25992537, -0.69368935, 0.2515502,\n                       -0.26379275, 0.8954313, 0.5759742, -0.7753072, -0.4388857,\n                       0.95751476, -0.82085776, -0.9467752, -0.37055635, -0.18570113,\n                       -0.86504984],\n                      [0.02305841, 0.3850248, 0.893692, -0.6866486, -0.83703446,\n                       -0.9828961, 0.3989377, -0.59993076, 0.5330808, 0.6916566,\n                       0.98468065, -0.6047034, 0.10823512, 0.34599304, -0.7834821,\n                       -0.7852347],\n                      [0.81643987, 0.31507468, -0.51369476, -0.12273741, 0.9701307,\n                       -0.79669356, -0.34496522, -0.88750815, -0.17995334, 0.34707904,\n                       -0.09201193, 0.5363934, -0.87229705, -0.5073328, -0.95894027,\n                       0.5481839],\n                      [-0.84093595, -0.2341497, -0.86047816, 0.43370056, -0.39073753,\n                       0.37730122, 0.48026466, 0.3004985, -0.60727096, 0.9043884,\n                       -0.37619448, 0.22490788, -0.03739262, 0.61672115, 0.478899,\n                       -0.40780973],\n                      [0.31202435, -0.22045255, -0.6087918, 0.95115066, 0.00199413,\n                       -0.688287, -0.1103518, 0.4169519, 0.7913246, -0.9844644,\n                       -0.6193857, 0.38659644, -0.4726901, -0.44781208, -0.5174744,\n                       -0.605911],\n                      [0.66771054, 0.34912825, 0.22297978, -0.4990945, 0.24057317,\n                       -0.5540829, 0.92277217, 0.74939895, -0.35278273, -0.21587133,\n                       -0.28613377, -0.8794241, -0.40119147, 0.67175174, -0.22741508,\n                       0.37898326]], dtype=np.float32))\n        dense_initializer = init_ops.constant_initializer(\n            np.array([[0.56177187, -0.6233454, 0.73997784, 0.35032558, 0.6479795],\n                      [0.6831174, -0.34233975, 0.39330363, 0.45177555, -0.49649096],\n                      [-0.98890066, 0.6175642, 0.09800482, -0.6721206, 0.48805737],\n                      [0.19671416, 0.2623148, 0.742548, 0.13555217, 0.56009054]], dtype=np.float32))\n\n        cell = tf.nn.rnn_cell.LSTMCell(\n            num_units=num_units,\n            initializer=cell_initializer,\n            state_is_tuple=True)\n\n        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n            embedding=embedding,\n            start_tokens=tf.tile([go_token], [batch_size]),\n            end_token=end_token)\n\n        output_layer = tf.layers.Dense(vocab_size, kernel_initializer=dense_initializer)\n        decoder = tf.contrib.seq2seq.BasicDecoder(\n            cell=cell,\n            helper=helper,\n            initial_state=encoder_state,\n            output_layer=output_layer)\n\n        outputs, state, sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n            decoder=decoder,\n            maximum_iterations=6)\n\n        _ = tf.identity(outputs.rnn_output, name=""rnn_output"")\n        _ = tf.identity(outputs.sample_id, name=""sample_id"")\n        _ = tf.identity(state, name=""state"")\n        _ = tf.identity(sequence_lengths, name=""sequence_lengths"")\n\n        output_names_with_port = [\n            ""rnn_output:0"",\n            # ""sample_id:0"",  # incomplete type support for Transpose on onnxruntime 0.2.1\n            ""state:0"",\n        ]\n\n        self._run_test_case([], output_names_with_port)\n\n    @check_tf_max_version(""1.15"", ""_run_test_case needs to supported tf-2"")\n    def test_while_loop_in_cond(self):\n        x_val = np.array([1, 2, 3], dtype=np.float32)\n        y_val = np.array([4, 5, 6], dtype=np.float32)\n        x = tf_placeholder(tf.float32, x_val.shape, name=""input_1"")\n        y = tf_placeholder(tf.float32, y_val.shape, name=""input_2"")\n\n        def cond_graph():\n            b = tf.constant(np.array([0], dtype=np.int32), dtype=tf.int32)\n            # while_loop\n            c = lambda y: tf.reduce_any(tf.less(y, 10))\n            b = lambda i: tf.add(y, 1)\n            return tf.while_loop(c, b, [y])\n\n        res = tf.cond(x[0] < y[0], lambda: x, cond_graph, name=""test_cond"")\n        _ = tf.identity(res, name=""output"")\n\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n        output_names_with_port = [""output:0""]\n        self._run_test_case(input_names_with_port, output_names_with_port)\n\n    @check_tf_max_version(""1.15"", ""_run_test_case needs to supported tf-2"")\n    def test_cond_in_while_loop(self):\n        i = tf.placeholder(tf.int32, (), name=""input_1"")\n        inputs = tf.placeholder(tf.float32, (10,), name=""input_2"")\n\n        inputs_2 = tf.identity(inputs)\n        input_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True).unstack(inputs_2)\n        output_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n\n        c = lambda i, *_: tf.logical_and(tf.less(i, 10), i >= 0)\n\n        def b(i, out_ta):\n            new_i = tf.add(i, 1)\n            x = input_ta.read(i)\n            x = tf.cond(x > 0, lambda: x - 1, lambda: x + 3)\n            out_ta_new = out_ta.write(i, x)\n            return new_i, out_ta_new\n\n        i_final, out_final = tf.while_loop(c, b, [i, output_ta])\n        _ = tf.identity(i_final, name=""i"")\n        _ = tf.identity(out_final.stack(), name=""output_ta"")\n        input_names_with_port = [""input_1:0"", ""input_2:0""]\n\n        output_names_with_port = [""i:0"", ""output_ta:0""]\n        self._run_test_case(input_names_with_port, output_names_with_port)\n\n\nif __name__ == ""__main__"":\n    unittest_main()\n'"
tf2onnx/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n""""""tf2onnx package.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n__all__ = [""utils"", ""graph_matcher"", ""graph"", ""graph_builder"",\n           ""tfonnx"", ""shape_inference"", ""schemas"", ""tf_utils"", ""tf_loader""]\n\nfrom .version import version as __version__\nfrom . import verbose_logging as logging\nfrom tf2onnx import tfonnx, utils, graph, graph_builder, graph_matcher, shape_inference, schemas  # pylint: disable=wrong-import-order\n#from tf2onnx import tf_utils, tf_loader\n'"
tf2onnx/constants.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ncommon constants\n""""""\n\nfrom onnx import helper\n\nTF2ONNX_PACKAGE_NAME = __name__.split(\'.\')[0]\n\n# Built-in supported domains\nONNX_DOMAIN = """"\nAI_ONNX_ML_DOMAIN = ""ai.onnx.ml""\nMICROSOFT_DOMAIN = ""com.microsoft""\n\n# Default opset version for onnx domain\nPREFERRED_OPSET = 8\n\n# Default opset for custom ops\nTENSORFLOW_OPSET = helper.make_opsetid(""ai.onnx.converters.tensorflow"", 1)\n\n# Target for the generated onnx graph. It possible targets:\n# onnx-1.1 = onnx at v1.1 (winml in rs4 is based on this)\n# caffe2 = include some workarounds for caffe2 and winml\nTARGET_RS4 = ""rs4""\nTARGET_RS5 = ""rs5""\nTARGET_RS6 = ""rs6""\nTARGET_CAFFE2 = ""caffe2""\nPOSSIBLE_TARGETS = [TARGET_RS4, TARGET_RS5, TARGET_RS6, TARGET_CAFFE2]\nDEFAULT_TARGET = []\n\nNCHW_TO_NHWC = [0, 2, 3, 1]\nNHWC_TO_NCHW = [0, 3, 1, 2]\nHWCN_TO_NCHW = [3, 2, 0, 1]\nNCHW_TO_HWCN = [2, 3, 1, 0]\n\n# Environment variables\nENV_TF2ONNX_DEBUG_MODE = ""TF2ONNX_DEBUG_MODE""\n\n# Mapping opset to IR version.\n# Note: opset 7 and opset 8 came out with IR3 but we need IR4 because of PlaceholderWithDefault\nOPSET_TO_IR_VERSION = {\n    1: 3, 2: 3, 3: 3, 4: 3, 5: 3, 6: 3, 7: 4, 8: 4, 9: 4, 10: 5, 11: 6, 12: 7\n}\n'"
tf2onnx/convert.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\npython -m tf2onnx.convert : tool to convert a frozen tensorflow graph to onnx\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n# pylint: disable=unused-argument,unused-import,ungrouped-imports,wrong-import-position\n\nimport argparse\nimport os\nimport sys\n\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n\nimport tensorflow as tf\n\nfrom tf2onnx.tfonnx import process_tf_graph\nfrom tf2onnx import constants, logging, utils, optimizer\nfrom tf2onnx import tf_loader\n\n\n# pylint: disable=unused-argument\n\n_HELP_TEXT = """"""\nUsage Examples:\n\npython -m tf2onnx.convert --saved-model saved_model_dir --output model.onnx\npython -m tf2onnx.convert --input frozen_graph.pb  --inputs X:0 --outputs output:0 --output model.onnx\npython -m tf2onnx.convert --checkpoint checkpoint.meta  --inputs X:0 --outputs output:0 --output model.onnx\n\nFor help and additional information see:\n    https://github.com/onnx/tensorflow-onnx\n\nIf you run into issues, open an issue here:\n    https://github.com/onnx/tensorflow-onnx/issues\n""""""\n\n\ndef get_args():\n    """"""Parse commandline.""""""\n    parser = argparse.ArgumentParser(description=""Convert tensorflow graphs to ONNX."",\n                                     formatter_class=argparse.RawDescriptionHelpFormatter, epilog=_HELP_TEXT)\n    parser.add_argument(""--input"", help=""input from graphdef"")\n    parser.add_argument(""--graphdef"", help=""input from graphdef"")\n    parser.add_argument(""--saved-model"", help=""input from saved model"")\n    parser.add_argument(""--signature_def"", help=""signature_def from saved model to use"")\n    parser.add_argument(""--checkpoint"", help=""input from checkpoint"")\n    parser.add_argument(""--keras"", help=""input from keras model"")\n    parser.add_argument(""--output"", help=""output model file"")\n    parser.add_argument(""--inputs"", help=""model input_names"")\n    parser.add_argument(""--outputs"", help=""model output_names"")\n    parser.add_argument(""--opset"", type=int, default=None, help=""opset version to use for onnx domain"")\n    parser.add_argument(""--custom-ops"", help=""list of custom ops"")\n    parser.add_argument(""--extra_opset"", default=None,\n                        help=""extra opset with format like domain:version, e.g. com.microsoft:1"")\n    parser.add_argument(""--target"", default="","".join(constants.DEFAULT_TARGET), choices=constants.POSSIBLE_TARGETS,\n                        help=""target platform"")\n    parser.add_argument(""--continue_on_error"", help=""continue_on_error"", action=""store_true"")\n    parser.add_argument(""--verbose"", ""-v"", help=""verbose output, option is additive"", action=""count"")\n    parser.add_argument(""--debug"", help=""debug mode"", action=""store_true"")\n    parser.add_argument(""--fold_const"", help=""enable tf constant_folding transformation before conversion"",\n                        action=""store_true"")\n    # experimental\n    parser.add_argument(""--inputs-as-nchw"", help=""transpose inputs as from nhwc to nchw"")\n    args = parser.parse_args()\n\n    args.shape_override = None\n    if args.input:\n        # for backward compativility\n        args.graphdef = args.input\n    if args.graphdef or args.checkpoint:\n        if not args.input and not args.outputs:\n            parser.error(""graphdef and checkpoint models need to provide inputs and outputs"")\n    if not any([args.graphdef, args.checkpoint, args.saved_model, args.keras]):\n        parser.print_help()\n        sys.exit(1)\n    if args.inputs:\n        args.inputs, args.shape_override = utils.split_nodename_and_shape(args.inputs)\n    if args.outputs:\n        args.outputs = args.outputs.split("","")\n    if args.inputs_as_nchw:\n        args.inputs_as_nchw = args.inputs_as_nchw.split("","")\n    if args.target:\n        args.target = args.target.split("","")\n    if args.signature_def:\n        args.signature_def = [args.signature_def]\n    if args.extra_opset:\n        tokens = args.extra_opset.split(\':\')\n        if len(tokens) != 2:\n            parser.error(""invalid extra_opset argument"")\n        args.extra_opset = [utils.make_opsetid(tokens[0], int(tokens[1]))]\n\n    return args\n\n\ndef default_custom_op_handler(ctx, node, name, args):\n    node.domain = constants.TENSORFLOW_OPSET.domain\n    return node\n\n\ndef main():\n    args = get_args()\n    logging.basicConfig(level=logging.get_verbosity_level(args.verbose))\n    if args.debug:\n        utils.set_debug_mode(True)\n\n    logger = logging.getLogger(constants.TF2ONNX_PACKAGE_NAME)\n\n    extra_opset = args.extra_opset or []\n    custom_ops = {}\n    if args.custom_ops:\n        # default custom ops for tensorflow-onnx are in the ""tf"" namespace\n        custom_ops = {op: (default_custom_op_handler, []) for op in args.custom_ops.split("","")}\n        extra_opset.append(constants.TENSORFLOW_OPSET)\n\n    # get the frozen tensorflow model from graphdef, checkpoint or saved_model.\n    if args.graphdef:\n        graph_def, inputs, outputs = tf_loader.from_graphdef(args.graphdef, args.inputs, args.outputs)\n        model_path = args.graphdef\n    if args.checkpoint:\n        graph_def, inputs, outputs = tf_loader.from_checkpoint(args.checkpoint, args.inputs, args.outputs)\n        model_path = args.checkpoint\n    if args.saved_model:\n        graph_def, inputs, outputs = tf_loader.from_saved_model(\n            args.saved_model, args.inputs, args.outputs, args.signature_def)\n        model_path = args.saved_model\n    if args.keras:\n        graph_def, inputs, outputs = tf_loader.from_keras(\n            args.keras, args.inputs, args.outputs)\n        model_path = args.keras\n\n    if args.verbose:\n        logger.info(""inputs: %s"", inputs)\n        logger.info(""outputs: %s"", outputs)\n\n    with tf.Graph().as_default() as tf_graph:\n        tf.import_graph_def(graph_def, name=\'\')\n    with tf_loader.tf_session(graph=tf_graph):\n        g = process_tf_graph(tf_graph,\n                             continue_on_error=args.continue_on_error,\n                             target=args.target,\n                             opset=args.opset,\n                             custom_op_handlers=custom_ops,\n                             extra_opset=extra_opset,\n                             shape_override=args.shape_override,\n                             input_names=inputs,\n                             output_names=outputs,\n                             inputs_as_nchw=args.inputs_as_nchw)\n\n    onnx_graph = optimizer.optimize_graph(g)\n    model_proto = onnx_graph.make_model(""converted from {}"".format(model_path))\n\n    # write onnx graph\n    logger.info("""")\n    logger.info(""Successfully converted TensorFlow model %s to ONNX"", model_path)\n    if args.output:\n        utils.save_protobuf(args.output, model_proto)\n        logger.info(""ONNX model is saved at %s"", args.output)\n    else:\n        logger.info(""To export ONNX model to file, please run with `--output` option"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tf2onnx/graph.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.graph - class to manage graph manipulation on top of onnx\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport collections\nimport copy\nimport logging\nimport six\nimport numpy as np\n\nfrom onnx import helper, numpy_helper, shape_inference, OperatorSetIdProto, AttributeProto, TensorProto\nfrom tf2onnx import utils, __version__\nfrom tf2onnx.utils import make_name, port_name, find_opset\nfrom tf2onnx import optimizer\nfrom tf2onnx.schemas import get_schema, infer_onnx_shape_dtype\nfrom tf2onnx import constants\n\nlogger = logging.getLogger(__name__)\n\n\n# todo(pengwa): remove protected-access later\n# pylint: disable=broad-except,protected-access\n\n\nclass Node(object):\n    """"""A Node - wrapper around onnx nodes that we use for graph manipulations.""""""\n\n    def __init__(self, node, graph, skip_conversion=False):\n        """"""Create Node.\n        Args:\n            node: Onnx node in NodeProto\n            graph: Graph() we are part of\n        """"""\n        self._op = node\n        self.graph = graph\n        self._input = list(node.input)\n        self._output = list(node.output)\n        self._attr = {}\n\n        graph.set_node_by_name(self)\n        # dict to original attributes\n        for a in node.attribute:\n            self._attr[a.name] = a\n        self._skip_conversion = skip_conversion\n\n    @property\n    def input(self):\n        return self._input\n\n    @input.setter\n    def input(self, val):\n        self._input = copy.deepcopy(val)\n\n    @property\n    def output(self):\n        return copy.deepcopy(self._output)\n\n    @output.setter\n    def output(self, val):\n        """"""Set op output. Output should be updated explicitly,\n        changing it would require output mapping changed.\n        """"""\n        self._graph_check()\n        for o in self._output:\n            del self.graph._output_to_node_name[o]\n\n        self._output = val\n        for o in self._output:\n            utils.make_sure(o not in self.graph._output_to_node_name, ""output %s already in output mapping"", o)\n            self.graph._output_to_node_name[o] = self.name\n\n    @property\n    def inputs(self):\n        """"""Input node objects.""""""\n        self._graph_check()\n        val = [self.graph.get_node_by_output(n) for n in self._input]\n        return val\n\n    @property\n    def attr(self):\n        return self._attr\n\n    @property\n    def attr_onnx(self):\n        """"""Return onnx valid attributes""""""\n        schema = get_schema(self.type, self.graph.opset, self.domain)\n        if schema is None and not (self.is_const() or self.is_graph_input()):\n            logger.debug(""Node %s uses non-stardard onnx op <%s, %s>, skip attribute check"",\n                         self.name, self.domain, self.type)\n        onnx_attrs = {}\n        for a in self._attr.values():\n            if schema is None or schema.has_attribute(a.name):\n                onnx_attrs[a.name] = a\n        return onnx_attrs\n\n    @property\n    def name(self):\n        return self._op.name\n\n    def child_name(self):\n        return utils.make_name(self.name)\n\n    @property\n    def op(self):\n        """"""TODO: have a better interface for this.""""""\n        return self._op\n\n    @property\n    def type(self):\n        """"""Return Op type.""""""\n        return self._op.op_type\n\n    @type.setter\n    def type(self, val):\n        """"""Set Op type.""""""\n        self._op.op_type = val\n\n    @property\n    def domain(self):\n        """"""Return Op type.""""""\n        return self._op.domain\n\n    @domain.setter\n    def domain(self, val):\n        """"""Set Op type.""""""\n        self._op.domain = val\n\n    @property\n    def data_format(self):\n        """"""Return data_format.""""""\n        attr_str = self.get_attr_value(""data_format"")\n        return ""unkown"" if attr_str is None else attr_str.decode(""utf-8"")\n\n    @data_format.setter\n    def data_format(self, val):\n        """"""Set data_format.""""""\n        self.set_attr(""data_format"", val)\n\n    def is_nhwc(self):\n        """"""Return True if node is in NHWC format.""""""\n        return self.data_format == ""NHWC""\n\n    def is_const(self):\n        """"""Return True if node is a constant.""""""\n        return self.type in [""Const"", ""ConstV2""]\n\n    def is_graph_input(self):\n        return self.type in [""Placeholder"", ""PlaceholderWithDefault"", ""PlaceholderV2""]\n\n    def is_graph_input_default_const(self):\n        return self.is_const() and any(\n            out.is_graph_input() for out in self.graph.find_output_consumers(self.output[0])\n        )\n\n    def is_while(self):\n        return self.type in [""While"", ""StatelessWhile"", ""Loop""]\n\n    def __str__(self):\n        return str(self._op)\n\n    def __repr__(self):\n        return ""<onnx op type=\'%s\' name=%s>"" % (self.type, self._op.name)\n\n    @property\n    def summary(self):\n        """"""Return node summary information.""""""\n        lines = []\n        lines.append(""OP={}"".format(self.type))\n        lines.append(""Name={}"".format(self.name))\n\n        g = self.graph\n        if self.input:\n            lines.append(""Inputs:"")\n            for name in self.input:\n                node = g.get_node_by_output(name)\n                op = node.type if node else ""N/A""\n                lines.append(""\\t{}={}, {}, {}"".format(name, op, g.get_shape(name), g.get_dtype(name)))\n\n        if self.output:\n            for name in self.output:\n                lines.append(""Outpus:"")\n                lines.append(""\\t{}={}, {}"".format(name, g.get_shape(name), g.get_dtype(name)))\n\n        return \'\\n\'.join(lines)\n\n    def get_attr(self, name, default=None):\n        """"""Get raw attribute value.""""""\n        attr = self.attr.get(name, default)\n        return attr\n\n    def get_attr_value(self, name, default=None):\n        attr = self.get_attr(name)\n        if attr:\n            return helper.get_attribute_value(attr)\n        return default\n\n    def get_attr_int(self, name):\n        """"""Get attribute value as int.""""""\n        attr_int = self.get_attr_value(name)\n        utils.make_sure(\n            attr_int is not None and isinstance(attr_int, int),\n            ""attribute %s is None"", name\n        )\n        return attr_int\n\n    def get_attr_str(self, name, encoding=""utf-8""):\n        """"""Get attribute value as string.""""""\n        attr_str = self.get_attr_value(name)\n        utils.make_sure(\n            attr_str is not None and isinstance(attr_str, bytes),\n            ""attribute %s is None"", name\n        )\n        return attr_str.decode(encoding)\n\n    def set_attr(self, name, value):\n        self.attr[name] = helper.make_attribute(name, value)\n\n    def set_attr_onnx(self, value):\n        self.attr[value.name] = value\n\n    @property\n    def skip_conversion(self):\n        return self._skip_conversion\n\n    @skip_conversion.setter\n    def skip_conversion(self, val):\n        self._skip_conversion = val\n\n    # If some Node is created as onnx_node, then we don\'t need convert it\n    def need_skip(self):\n        return self._skip_conversion\n\n    @property\n    def output_shapes(self):\n        """"""Get output shapes.""""""\n        self._graph_check()\n        val = [self.graph.get_shape(n) for n in self._output]\n        return val\n\n    @property\n    def output_dtypes(self):\n        """"""Get output dtypes.""""""\n        self._graph_check()\n        val = [self.graph.get_dtype(n) for n in self._output]\n        return val\n\n    def get_tensor_value(self, as_list=True):\n        """"""Get value for onnx tensor.\n        Args:\n            as_list: whether return numpy ndarray in list.\n        Returns:\n            If as_list=True, return the array as a (possibly nested) list.\n            Otherwise, return data of type np.ndarray.\n\n            If a tensor is a scalar having value 1,\n                when as_list=False, return np.array(1), type is <class \'numpy.ndarray\'>\n                when as_list=True, return 1, type is <class \'int\'>.\n        """"""\n        if not self.is_const():\n            raise ValueError(""get tensor value: {} must be Const"".format(self.name))\n\n        t = self.get_attr(""value"")\n        if t:\n            t = numpy_helper.to_array(helper.get_attribute_value(t))\n            if as_list is True:\n                t = t.tolist()  # t might be scalar after tolist()\n        return t\n\n    def scalar_to_dim1(self):\n        """"""Get value for onnx tensor.""""""\n        if not self.is_const():\n            raise ValueError(""get tensor value: {} must be Const"".format(self.name))\n\n        t = self.get_attr(""value"")\n        if t:\n            t = helper.get_attribute_value(t)\n            if not t.dims:\n                t.dims.extend([1])\n        return t.dims\n\n    def set_tensor_value(self, new_val):\n        """"""Set new value for existing onnx tensor.\n        Args:\n            new_val: value of type numpy ndarray\n        """"""\n        if not self.is_const():\n            raise ValueError(""set tensor value: {} must be Const"".format(self.name))\n        t = self.get_attr(""value"")\n        if not t:\n            raise ValueError(""set tensor value: {} is None"".format(self.name))\n        t = helper.get_attribute_value(t)\n        onnx_tensor = numpy_helper.from_array(new_val, t.name)\n        del t\n        self.set_attr(""value"", onnx_tensor)\n        # track shapes in _output_shapes\n        self._graph_check()\n        self.graph.set_shape(onnx_tensor.name, onnx_tensor.dims)\n\n    def get_body_graphs(self):\n        self._graph_check()\n        return self.graph.contained_graphs.get(self.name, None)\n\n    def set_body_graph_as_attr(self, attr_name, graph):\n        self._graph_check()\n        if self.name not in self.graph.contained_graphs:\n            self.graph.contained_graphs[self.name] = {}\n\n        self.graph.contained_graphs[self.name].update({attr_name: graph})\n        graph.parent_graph = self.graph\n\n    def update_proto(self):\n        """"""Update protobuf from internal structure.""""""\n        nodes = list(self._op.input)\n        for node in nodes:\n            self._op.input.remove(node)\n        self._op.input.extend(self.input)\n        nodes = list(self._op.output)\n        for node in nodes:\n            self._op.output.remove(node)\n        self._op.output.extend(self.output)\n\n        # update attributes to proto\n        del self._op.attribute[:]\n\n        # check attribute of type GraphProto\n        attr_graphs = self.get_body_graphs()\n        if attr_graphs:\n            for attr_name, sub_graph in attr_graphs.items():\n                graph_proto = sub_graph.make_graph(""graph for "" + self.name + "" "" + attr_name)\n                self.set_attr(attr_name, graph_proto)\n\n        attr = list(self.attr_onnx.values())\n        if attr:\n            self._op.attribute.extend(attr)\n\n    def get_implicit_inputs(self, recursive=True):\n        """"""Get implicit inputs if the node has attributes being GraphProto.""""""\n        output_available_in_cur_graph = set()\n        all_node_inputs = set()\n\n        graphs = []\n        body_graphs = self.get_body_graphs()\n        if body_graphs:\n            graphs.extend(body_graphs.values())\n\n        while graphs:\n            graph = graphs.pop()\n            for n in graph.get_nodes():\n                output_available_in_cur_graph |= set(n.output)\n                for i in n.input:\n                    all_node_inputs.add(i)\n\n                if recursive:\n                    b_graphs = n.get_body_graphs()\n                    if b_graphs:\n                        graphs.extend(b_graphs.values())\n\n        outer_scope_node_input_ids = all_node_inputs - output_available_in_cur_graph\n        return list(outer_scope_node_input_ids)\n\n    def _graph_check(self):\n        utils.make_sure(self.graph is not None, ""Node %s not belonging any graph"",\n                        self.name)\n\n    def maybe_cast_input(self, supported, type_map):\n        """""".maybe_cast_input\n        Args:\n            supported: list of supported types for inputs\n            type_map: dict type to supported type mapping\n        """"""\n        did_cast = False\n        for i, name in enumerate(self.input):\n            dtype = self.graph.get_dtype(name)\n            if dtype not in supported[i]:\n                tdtype = type_map.get(dtype)\n                if tdtype is None:\n                    raise RuntimeError(""don\'t know how to cast type {} on node {}"".format(dtype, name))\n                shape = self.graph.get_shape(name)\n                cast_node = self.graph.insert_new_node_on_input(self, ""Cast"", name)\n                cast_node.set_attr(""to"", tdtype)\n                self.graph.set_dtype(cast_node.output[0], [tdtype])\n                self.graph.set_shape(cast_node.output[0], shape)\n                did_cast = True\n        return did_cast\n\n\nclass Graph(object):\n    """"""""Class that provides graph manipulation and matching.""""""\n\n    def __init__(self, nodes, output_shapes=None, dtypes=None, target=None, opset=None, extra_opset=None,\n                 output_names=None, is_subgraph=False, graph_name=None):\n        """"""Create Graph.\n        Args:\n            nodes: list of Node()\n            output_shapes: dict of tensorflow output shapes\n            dtypes: dict of tensorflow dtype\n        """"""\n        if target is None:\n            target = []\n        self._nodes = []\n        self._nodes_by_name = {}\n        self._output_to_node_name = {}\n        self.shapes = {}\n        self.graph_name = graph_name or ""tf2onnx""\n        self._is_subgraph = is_subgraph\n        self.ta_reads = []\n        self.func_inputs = []\n\n        self._target = set(target)\n        self._dtypes = dtypes\n\n        self._output_shapes = output_shapes\n        self._opset = find_opset(opset)\n\n        if extra_opset is not None:\n            utils.make_sure(isinstance(extra_opset, list), ""invalid extra_opset"")\n        self._extra_opset = extra_opset\n\n        self._order_sensitive_inputs = []\n        self.outputs = output_names if output_names is not None else []\n\n        self.parent_graph = None\n        self.contained_graphs = {}  # {node_name: {node_attribute_name: Graph}}\n\n        ops = [Node(node, self) for node in nodes]\n        self.reset_nodes(ops)\n\n        if not is_subgraph:\n            # add identity node after each output, in case it is renamed during conversion.\n            for o in self.outputs:\n                n = self.get_node_by_output_in_current_graph(o)\n                new_output_name = port_name(n.name + ""_"" + utils.make_name(""raw_output_""))\n                n_shapes = n.output_shapes\n                n_dtypes = n.output_dtypes\n                body_graphs = n.graph.contained_graphs.pop(n.name, None)\n                self.remove_node(n.name)\n\n                new_outputs = [output if output != o else new_output_name for output in n.output]\n                # domain should be passed to new node\n                new_node = self.make_node(n.type, n.input, outputs=new_outputs, attr=n.attr, name=n.name,\n                                          skip_conversion=n._skip_conversion, dtypes=n_dtypes, shapes=n_shapes,\n                                          domain=n.domain)\n\n                if body_graphs:\n                    for attr_name, body_graph in body_graphs.items():\n                        body_graph.parent_graph = self\n                        new_node.set_body_graph_as_attr(attr_name, body_graph)\n\n                self.replace_all_inputs(self.get_nodes(), o, new_output_name)\n                self.make_node(""Identity"", [new_output_name], outputs=[o], op_name_scope=n.name + ""_"" + ""graph_outputs"")\n                self.copy_shape(new_output_name, o)\n                self.copy_dtype(new_output_name, o)\n\n    def create_new_graph_with_same_config(self):\n        """"""Create a clean graph inheriting current graph\'s configuration.""""""\n        return Graph([], output_shapes={}, dtypes={}, target=self._target, opset=self._opset,\n                     extra_opset=self.extra_opset, output_names=[])\n\n    @property\n    def opset(self):\n        return self._opset\n\n    @property\n    def extra_opset(self):\n        return self._extra_opset\n\n    def is_target(self, *names):\n        """"""Return True if target platform contains any name.""""""\n        return any(name in self._target for name in names)\n\n    @property\n    def inputs(self):\n        """"""Input to the graph.""""""\n        all_inputs = self._order_sensitive_inputs\n        for n in self._nodes:\n            if n.is_graph_input() and n not in all_inputs:\n                all_inputs.append(n)\n        return all_inputs\n\n    def make_const(self, name, np_val, skip_conversion=False, raw=True):\n        """"""Make a new constant in the graph.\n        Args:\n            name: const node name, must be unique.\n            np_val: value of type numpy ndarray.\n            skip_conversion: bool, indicate whether this created node would be mapped during conversion.\n            raw: whether to store data at field of raw_data or the specific field according to its dtype\n        """"""\n        if raw:\n            onnx_tensor = numpy_helper.from_array(np_val, name)\n        else:\n            onnx_tensor = helper.make_tensor(name, utils.map_numpy_to_onnx_dtype(np_val.dtype),\n                                             np_val.shape, np_val, raw=False)\n        dtype = onnx_tensor.data_type\n        node = self.make_node(""Const"", [], outputs=[name], name=name, attr={""value"": onnx_tensor},\n                              skip_conversion=skip_conversion, dtypes=[dtype], infer_shape_dtype=False)\n        self.set_shape(name, np_val.shape)\n        self.set_dtype(name, utils.map_numpy_to_onnx_dtype(np_val.dtype))\n        return node\n\n    def copy_const(self, node, name=None):\n        """"""Copy a const node, using name if specified""""""\n        # TODO: support attr copy starting at opset 12\n        if name is None:\n            name = utils.make_name(node.name)\n        return self.make_const(name, node.get_tensor_value(as_list=False))\n\n    def make_node(self, op_type, inputs, attr=None, output_count=1, outputs=None, skip_conversion=True,\n                  op_name_scope=None, name=None, shapes=None, dtypes=None, domain=constants.ONNX_DOMAIN,\n                  infer_shape_dtype=True):\n        """"""Make a new onnx node in the graph""""""\n        if attr is None:\n            attr = {}\n        if shapes is None:\n            shapes = []\n        if dtypes is None:\n            dtypes = []\n\n        if name is None:\n            name = utils.make_name(op_type)\n\n        if op_name_scope:\n            name = ""_"".join([op_name_scope, name])\n\n        logger.debug(""Making node: Name=%s, OP=%s"", name, op_type)\n\n        if outputs is None:\n            outputs = [name + "":"" + str(i) for i in range(output_count)]\n\n        output_count = len(outputs)\n        raw_attr = {}\n        onnx_attrs = []\n        for a, v in attr.items():\n            if isinstance(v, AttributeProto):\n                onnx_attrs.append(v)\n            else:\n                raw_attr[a] = v\n\n        n = self.get_node_by_name(name)\n        utils.make_sure(n is None, ""name %s already exists in node: \\n%s"", name, n)\n        for o in outputs:\n            n = self.get_node_by_output_in_current_graph(o)\n            utils.make_sure(n is None, ""output tensor named %s already exists in node: \\n%s"", o, n)\n\n        onnx_node = helper.make_node(op_type, inputs, outputs, name=name, domain=domain, **raw_attr)\n\n        if op_type in [""If"", ""Loop"", ""Scan""]:\n            # we force the op containing inner graphs not skipped during conversion.\n            skip_conversion = False\n\n        node = Node(onnx_node, self, skip_conversion=skip_conversion)\n        if onnx_attrs:\n            _ = [node.set_attr_onnx(a) for a in onnx_attrs]\n\n        if shapes:\n            utils.make_sure(len(shapes) == output_count,\n                            ""output shape count %s not equal to output count %s"", len(shapes), output_count)\n            for i in range(output_count):\n                self.set_shape(node.output[i], shapes[i])\n\n        if dtypes:\n            utils.make_sure(len(dtypes) == output_count,\n                            ""output dtypes count %s not equal to output count %s"", len(dtypes), output_count)\n            for i in range(output_count):\n                self.set_dtype(node.output[i], dtypes[i])\n\n        if (not shapes or not dtypes) and infer_shape_dtype:\n            self.update_node_shape_dtype(node, override=False)\n\n        logger.debug(""Made node: %s\\n%s"", node.name, node.summary)\n        self._nodes.append(node)\n        return node\n\n    def append_node(self, node):\n        output_shapes = node.output_shapes\n        output_dtypes = node.output_dtypes\n        node.graph = self\n        self._nodes.append(node)\n        self._nodes_by_name[node.name] = node\n        for i, name in enumerate(node.output):\n            self._output_to_node_name[name] = node.name\n            self.set_dtype(name, output_dtypes[i])\n            self.set_shape(name, output_shapes[i])\n\n    def remove_node(self, node_name):\n        """"""Remove node in current graph.""""""\n        utils.make_sure(node_name in self._nodes_by_name, ""node %s not in current graph, cannot remove"", node_name)\n        node = self.get_node_by_name(node_name)\n        del self._nodes_by_name[node_name]\n        if node_name in self.contained_graphs:\n            del self.contained_graphs[node_name]\n\n        if node in self._order_sensitive_inputs:\n            self._order_sensitive_inputs.remove(node)\n\n        for op_output in node.output:\n            del self._output_to_node_name[op_output]\n\n            if op_output in self._output_shapes:\n                del self._output_shapes[op_output]\n            if op_output in self._dtypes:\n                del self._dtypes[op_output]\n\n        self._nodes.remove(node)\n        node.graph = None\n\n    def reset_nodes(self, ops):\n        """"""Reset the graph with node list.""""""\n        remained_dtypes = {}\n        remained_shapes = {}\n        remained_sub_graphs = {}\n        for op in ops:\n            for op_output in op.output:\n                # this check should be removed once we make sure all output tensors have dtype/shape.\n                if op_output in self._dtypes:\n                    remained_dtypes[op_output] = self._dtypes[op_output]\n                if op_output in self._output_shapes:\n                    remained_shapes[op_output] = self._output_shapes[op_output]\n\n            if op.name in self.contained_graphs:\n                remained_sub_graphs[op.name] = self.contained_graphs[op.name]\n\n        self._nodes = ops\n        self.contained_graphs = remained_sub_graphs\n        self._nodes_by_name = {op.name: op for op in ops}\n        self._output_to_node_name = {}\n        for op in ops:\n            for op_output in op.output:\n                self._output_to_node_name[op_output] = op.name\n\n        for n in self._order_sensitive_inputs:\n            if n not in ops:\n                self._order_sensitive_inputs.remove(n)\n        for o in self.outputs:\n            if o not in self._output_to_node_name:\n                raise ValueError(""graph output "" + o + "" not exist"")\n\n        self._dtypes = remained_dtypes\n        self._output_shapes = remained_shapes\n\n    def is_empty_input(self, name):\n        # in ONNX, operation may have optional input and an empty string may be used\n        # in the place of an actual argument\'s name to indicate a missing argument\n        return name == utils.ONNX_EMPTY_INPUT\n\n    def check_integrity(self):\n        """"""\n        Check graph integrity. Every node\'s input needs to associate with a node.\n        Return broken outputs.\n        """"""\n        broken_outputs = set()\n        for node in self.get_nodes():\n            for inp in node.input:\n                if self.get_node_by_output(inp) is None and not self.is_empty_input(inp):\n                    broken_outputs.add(inp)\n        return list(broken_outputs)\n\n    def update_node_shape_dtype(self, node, override=False):\n        """"""Try the best to infer shapes and dtypes for outputs of the node,\n        by default, we respect TF shapes and dtypes.\n        """"""\n        if node.is_const() or node.is_graph_input():\n            return\n        # NOTE: only support onnx node for now\n        if not utils.is_onnx_domain(node.domain):\n            return\n\n        logger.debug(""Infer shape and dtype for [%s]"", node.name)\n        # NOTE: shape inference for some ops need the input values of the op, e.g., Reshape\n        # op needs the ""Shape"" value to infer output shape.\n        initializers = []\n        for i, inp in enumerate(node.inputs):\n            if inp is None:\n                if not self.is_empty_input(node.input[i]):\n                    if logger.isEnabledFor(logging.INFO):\n                        logger.warning(\n                            ""[%s] infer a inexistent node: [%s], please check the code"",\n                            node.name, node.input[i]\n                        )\n                continue\n            if inp.is_const():\n                t = inp.get_attr(""value"")\n                tensor = helper.get_attribute_value(t)\n                tensor.name = inp.output[0]\n                initializers.append(tensor)\n\n        input_shapes = [self.get_shape(i) for i in node.input]\n        input_dtypes = [self.get_dtype(i) for i in node.input]\n\n        shapes, dtypes = infer_onnx_shape_dtype(node, self._opset, input_shapes, input_dtypes, initializers)\n        if not shapes or not dtypes:\n            return\n\n        for output, shape, dtype in zip(node.output, shapes, dtypes):\n            if dtype == TensorProto.UNDEFINED:\n                logger.debug(""Inferred dtype for [%s, type: %s] is UNDEFINED, SKIP"", node.name, node.type)\n            else:\n                existing_dtype = self.get_dtype(output)\n                if existing_dtype is not None and existing_dtype != dtype:\n                    if override:\n                        logger.warning(""Override dtype of %s from %s to %s"", output, existing_dtype, dtype)\n                    else:\n                        dtype = existing_dtype\n                self.set_dtype(output, dtype)\n                logger.debug(""Set dtype of [%s] to %s"", output, dtype)\n\n            if shape is None:\n                logger.debug(""Inferred shape for [%s, type: %s] is None, SKIP"", node.name, node.type)\n            else:\n                existing_shape = self.get_shape(output)\n                if existing_shape is not None and not utils.are_shapes_equal(existing_shape, shape):\n                    if override:\n                        logger.warning(""Override shape of %s from %s to %s"", output, existing_shape, shape)\n                    else:\n                        shape = existing_shape\n                self.set_shape(output, shape)\n                logger.debug(""Set shape of [%s] to %s"", output, shape)\n\n    def update_proto(self):\n        """"""Update the onnx protobuf from out internal Node structure.""""""\n        for node in self._nodes:\n            node.update_proto()\n\n    def get_nodes(self):\n        """"""Get node list.""""""\n        return self._nodes\n\n    def get_node_by_output(self, output, search_in_parent_graphs=True):\n        """"""Get node by node output id recursively going through nested graphs.\n        Args:\n            search_in_parent_graphs: search in all parent graphs\n        """"""\n        ret = None\n        g = self\n        while not ret and g:\n            ret = g.get_node_by_output_in_current_graph(output)\n            if ret:\n                return ret\n\n            if not search_in_parent_graphs:\n                break\n            g = g.parent_graph\n        return ret\n\n    def get_node_by_output_in_current_graph(self, output):\n        """"""Get node by node output id.""""""\n        name = self._output_to_node_name.get(output)\n        ret = None\n        if name:\n            ret = self._nodes_by_name.get(name)\n        return ret\n\n    def get_node_by_name(self, name):\n        """"""Get node by name.""""""\n        ret = self._nodes_by_name.get(name)\n        return ret\n\n    def set_node_by_name(self, node):\n        """"""Set node by name.""""""\n        self._nodes_by_name[node.name] = node\n        for op_output in node.output:\n            self._output_to_node_name[op_output] = node.name\n\n    def change_node_name(self, node, new_name):\n        """"""Remove node in current graph.""""""\n        utils.make_sure(new_name not in self._nodes_by_name, ""node %s not unique "", new_name)\n        dtypes = node.output_dtypes\n        shapes = node.output_shapes\n        self.remove_node(node.name)\n        new_node = self.make_node(node.type, node.input, output_count=len(node.output),\n                                  attr=node.attr, dtypes=dtypes, shapes=shapes, name=new_name)\n        for i, old_output in enumerate(node.output):\n            new_output = port_name(new_name, i)\n            for j, k in enumerate(self.outputs):\n                if k == old_output:\n                    self.outputs[j] = new_output\n                    break\n            self.replace_all_inputs(self.get_nodes(), old_output, new_output)\n        return new_node\n\n    def add_graph_input(self, name, dtype=None, shape=None):\n        """"""Add placeholder node as graph\'s input. Order matters only for subgraph.\n           Placeholders in original graph are assumed for main graph, order not matters.\n        """"""\n        if dtype is None:\n            dtype = self.get_dtype(name)\n\n        if shape is None:\n            shape = self.get_shape(name)\n\n        new_node = self.make_node(""Placeholder"", [], outputs=[name], dtypes=[dtype], shapes=[shape])\n        self._order_sensitive_inputs.append(new_node)\n\n    def add_graph_input_with_default(self, name, default_const, dtype=None, shape=None):\n        """"""Add placeholderwithdefault.""""""\n        if dtype is None:\n            dtype = self.get_dtype(name)\n\n        if shape is None:\n            shape = self.get_shape(name)\n\n        default_const_name = port_name(make_name(""{}_default"".format(name)))\n        default_const.output = [default_const_name]\n        new_node = self.make_node(""PlaceholderWithDefault"", [default_const_name], outputs=[name],\n                                  dtypes=[dtype], shapes=[shape])\n        self._order_sensitive_inputs.append(new_node)\n\n    def add_graph_output(self, name, dtype=None, shape=None):\n        """"""Add node output as graph\'s output.""""""\n        utils.make_sure(name in self._output_to_node_name, ""output %s not exist in the graph"", name)\n\n        if dtype is None:\n            dtype = self.get_dtype(name)\n\n        if shape is None:\n            shape = self.get_shape(name)\n\n        if name not in self.outputs:\n            utils.make_sure(shape is not None, ""shape for output %s should not be None"", name)\n            utils.make_sure(dtype is not None, ""dtype for output %s should not be None"", name)\n            self.outputs.append(name)\n            self.set_shape(name, shape)\n            self.set_dtype(name, dtype)\n        else:\n            raise ValueError(""graph output "" + name + "" already exists"")\n\n    def get_dtype(self, name):\n        """"""Get dtype for node.""""""\n        node = self.get_node_by_output(name, search_in_parent_graphs=True)\n        return node.graph._dtypes.get(name) if node else None\n\n    def set_dtype(self, name, dtype):\n        """"""Set dtype for node.""""""\n        node = self.get_node_by_output(name, search_in_parent_graphs=True)\n        node.graph._dtypes[name] = dtype\n\n    def copy_dtype(self, src_name, dst_name):\n        """"""Copy dtype from another node.""""""\n        dtype = self.get_dtype(src_name)\n        self.set_dtype(dst_name, dtype)\n\n    def get_shape(self, name):\n        """"""Get shape for node.""""""\n        utils.make_sure(isinstance(name, six.text_type), ""get_shape name is invalid type: %s"", name)\n        node = self.get_node_by_output(name, search_in_parent_graphs=True)\n        shape = node.graph._output_shapes.get(name) if node else None\n        if shape:\n            for i, v in enumerate(shape):\n                if v is None:\n                    # pylint: disable=unsupported-assignment-operation\n                    shape[i] = -1\n            # hack to allow utils.ONNX_UNKNOWN_DIMENSION to override batchsize if needed.\n            # default is -1.\n            if shape[0] == -1:\n                # pylint: disable=unsupported-assignment-operation\n                shape[0] = utils.ONNX_UNKNOWN_DIMENSION\n            return shape\n        return shape\n\n    def set_shape(self, name, val):\n        """"""Set new shape of node.""""""\n        if isinstance(val, np.ndarray):\n            val = val.tolist()\n        if isinstance(val, tuple):\n            val = list(val)\n        node = self.get_node_by_output(name, search_in_parent_graphs=True)\n        utils.make_sure(node is not None, ""cannot find node by output id %s"", name)\n        node.graph._output_shapes[name] = val\n\n    def copy_shape(self, input_name, output_name):\n        """"""Copy shape from another node.""""""\n        shape = self.get_shape(input_name)\n        # assert shape is not None\n        if shape is not None:\n            self.set_shape(output_name, shape)\n\n    def topological_sort(self, ops):\n        """"""Topological sort of graph.""""""\n        # sort by name, the result will be reversed alphabeta\n        ops.sort(key=lambda op: op.name)\n\n        def _push_stack(stack, node, in_stack):\n            stack.append(node)\n            if node in in_stack:\n                raise ValueError(\'Graph has cycles, node=\' + ops[node].name)\n            in_stack[node] = True\n\n        def _get_unvisited_child(g, node, not_visited):\n            for child in g[node]:\n                if child in not_visited:\n                    return child\n            return -1\n\n        n = len(ops)\n        g = [[] for _ in range(n)]\n        op_name_to_index = {}\n        for i, op in enumerate(ops):\n            op_name_to_index[op.name] = i\n\n        for i, op in enumerate(ops):\n            all_input = set(op.input)\n            implicit_inputs = op.get_implicit_inputs()\n            all_input |= set(implicit_inputs)\n            # remove those empty inputs\n            all_input = list(filter(lambda a: a != \'\', all_input))\n            for inp in sorted(all_input):\n                j = self.get_node_by_output(inp)\n                utils.make_sure(j is not None, ""Cannot find node with output {}"".format(inp))\n                if self.parent_graph and j.name not in op_name_to_index:\n                    # there might be some outer-scoped inputs for an inner Graph.\n                    pass\n                else:\n                    g[op_name_to_index[j.name]].append(i)\n\n        # label for each op. highest = sink nodes.\n        label = [-1 for _ in range(n)]\n        stack = []\n        in_stack = dict()\n        not_visited = dict.fromkeys(range(n))\n        label_counter = n - 1\n\n        while not_visited:\n            node = list(not_visited.keys())[0]\n            _push_stack(stack, node, in_stack)\n            while stack:\n                node = _get_unvisited_child(g, stack[-1], not_visited)\n                if node != -1:\n                    _push_stack(stack, node, in_stack)\n                else:\n                    node = stack.pop()\n                    in_stack.pop(node)\n                    not_visited.pop(node)\n                    label[node] = label_counter\n                    label_counter -= 1\n\n        ret = [x for _, x in sorted(zip(label, ops))]\n        self.reset_nodes(ret)\n\n    def make_graph(self, doc, graph_name=None):\n        """"""\n        Create GraphProto for onnx from internal graph.\n        Args:\n            optimize: optimize graph via onnx\n            doc: text for doc string of the graph\n        """"""\n        graph_name = graph_name or self.graph_name\n        self.delete_unused_nodes(self.outputs)\n        self.topological_sort(self.get_nodes())\n        self.update_proto()\n\n        # TODO: we\'d want to do something like this so that transpose optimizer is active\n        # for  all (unit) tests\n        # if optimize:\n        #    from tf2onnx.optimizer.transpose_optimizer import TransposeOptimizer\n        #    optimizer = TransposeOptimizer(self, False)\n        #    optimizer.optimize()\n        ops = []\n        order_non_sensitive_placeholders = []\n        order_sensitive_placeholders = self._order_sensitive_inputs\n        const_ops = []\n        for op in self.get_nodes():\n            if op.is_const():\n                const_ops.append(op)\n                continue\n            if op.is_graph_input():\n                if op not in self._order_sensitive_inputs:\n                    order_non_sensitive_placeholders.append(op)\n                continue\n            ops.append(op)\n        placeholder_ops = order_sensitive_placeholders + order_non_sensitive_placeholders\n\n        # create initializers for placeholder with default nodes\n        initializers = []\n        placeholder_default_const_ops = []\n        for op in placeholder_ops:\n            if op.type == ""PlaceholderWithDefault"":\n                utils.make_sure(op.inputs[0] is not None, ""Cannot find node with output {}"".format(op.input[0]))\n                utils.make_sure(op.inputs[0].is_const(),\n                                ""non-const default value for PlaceholderWithDefault is not supported."")\n                # copy the tensor value, set its name to current node\'s output, add as initializer\n                value = op.inputs[0].get_tensor_value(as_list=False)\n                tensor = numpy_helper.from_array(value, op.output[0])\n                initializers.append(tensor)\n                placeholder_default_const_ops.append(op.inputs[0])\n\n        # create initializers for constant nodes\n        const_ops = [op for op in const_ops if op not in placeholder_default_const_ops]\n        for op in const_ops:\n            # not to use numpy_helper.from_array to create a new tensor\n            # because sometimes onnx will have a bug that only check the tensor data in specific field\n            # such as at upsample it only checks the float_data field.\n            t = op.get_attr(""value"")\n            tensor = helper.get_attribute_value(t)\n            tensor.name = op.output[0]\n            initializers.append(tensor)\n\n        # create input_tensor_values\n        input_ids = [op.output[0] for op in placeholder_ops]\n        # onnx with IR version below 4 requires initializer should be in inputs.\n        # here we check opset version rather than IR version for the reason:\n        # https://github.com/onnx/tensorflow-onnx/pull/557\n        # opset 9 come with IR 4.\n        if self.opset < 9:\n            input_ids += [op.output[0] for op in const_ops]\n\n        input_tensor_values = self.make_onnx_graph_io(input_ids)\n\n        # create output_tensor_values\n        output_tensor_values = self.make_onnx_graph_io(self.outputs)\n\n        # create graph proto\n        graph = helper.make_graph([op.op for op in ops],\n                                  graph_name,\n                                  input_tensor_values,\n                                  output_tensor_values,\n                                  initializer=initializers,\n                                  doc_string=doc)\n\n        return graph\n\n    def make_model(self, graph_doc, optimize=False, graph_name=""tf2onnx"", **kwargs):\n        """"""\n        Create final ModelProto for onnx from internal graph.\n        Args:\n            optimize: optimize graph via onnx\n            doc: text for doc string of the model\n        """"""\n        graph = self.make_graph(graph_doc, graph_name)\n\n        if ""producer_name"" not in kwargs:\n            kwargs = {""producer_name"": ""tf2onnx"",\n                      ""producer_version"": __version__}\n\n        if ""opset_imports"" not in kwargs:\n            opsets = []\n            imp = OperatorSetIdProto()\n            imp.version = self._opset\n            opsets.append(imp)\n            if self.extra_opset is not None:\n                opsets.extend(self.extra_opset)\n            kwargs[""opset_imports""] = opsets\n        model_proto = helper.make_model(graph, **kwargs)\n\n        # set the IR version based on opset\n        try:\n            model_proto.ir_version = constants.OPSET_TO_IR_VERSION.get(self.opset, model_proto.ir_version)\n        except: # pylint: disable=bare-except\n            logger.error(""ir_version override failed - install the latest onnx version"")\n\n        # optimize the model proto.\n        # TODO: this is disabled by default because of bugs in fuse_consecutive_transposes\n        if optimize:\n            model_proto = optimizer.optimize(model_proto)\n        return model_proto\n\n    def make_onnx_graph_io(self, ids):\n        """"""Create tensor_value_info for passed input/output ids.""""""\n        tensor_value_infos = []\n        for name in ids:\n            dtype = self.get_dtype(name)\n            shape = self.get_shape(name)\n\n            utils.make_sure(dtype is not None, ""missing output dtype for "" + name)\n            # TODO: allow None output shape or not? e.g. shape=(?,)\n            #utils.make_sure(shape is not None, ""missing output shape for "" + name)\n            if shape is None: logger.warning(""missing output shape for %s"", name)\n\n            v = utils.make_onnx_inputs_outputs(name, dtype, shape)\n            tensor_value_infos.append(v)\n        return tensor_value_infos\n\n    def dump_graph(self):\n        """"""Dump graph with shapes (helpful for debugging).""""""\n        for node in self.get_nodes():\n            input_names = [""{}{}"".format(n, self.get_shape(n)) for n in node.input]\n            logger.debug(""%s %s %s %s"",\n                         node.type,\n                         self.get_shape(node.output[0]),\n                         node.name,\n                         "", "".join(input_names))\n\n    def follow_inputs(self, node, num, space=""""):\n        """"""Follow inputs for (helpful for debugging).""""""\n        val = []\n        top = space == """"\n        if num == 0:\n            return []\n        val.append(""{}{} {} {}"".format(space, node.type, node.name, self.get_shape(port_name(node.name))))\n        space += ""    ""\n        for j in node.inputs:\n            val.extend(self.follow_inputs(j, num - 1, space))\n        if top:\n            print(""\\n"".join(reversed(val)))\n            print()\n            return []\n        return val\n\n    def dump_node_statistics(self):\n        op_cnt = collections.Counter()\n        for n in self.get_nodes():\n            op_cnt[n.type] += 1\n            body_graphs = n.get_body_graphs()\n            if body_graphs:\n                for _, b_g in body_graphs.items():\n                    op_cnt += b_g.dump_node_statistics()\n\n        return op_cnt\n\n    @staticmethod\n    def remove_input(node, to_be_removed):\n        """"""Remove input from Node.\n        Args:\n            node: the node we expect the input on\n            to_be_removed: the node name we want to remove\n        """"""\n        assert isinstance(node, Node) and isinstance(to_be_removed, six.text_type)\n        for i, name in enumerate(node.input):\n            if name == to_be_removed:\n                del node.input[i]\n                break\n        # don\'t remove output from parent since others might depend on it\n        return True\n\n    def insert_new_node_on_input(self, node, op_type, input_name, name=None, domain=None, **kwargs):\n        """"""Create and insert a new node into the graph.\n        Args:\n            node: we want to replace the input for this node\n            op_type: type for new operation\n            input_name: the name(s) of the outputs above us\n                if scalar, new node placed above input_name\n                if list, new node placed above input_name[0]. list is inputs into new node\n            name: the name of the new op\n            kwargs: attributes of the new node\n\n        Returns:\n            node that was inserted\n        """"""\n        if name is None:\n            name = utils.make_name(node.name)\n        new_output = port_name(name)\n        if not isinstance(input_name, list):\n            input_name = [input_name]\n\n        new_node = self.make_node(op_type, input_name, attr=kwargs, outputs=[new_output], name=name, domain=domain)\n        for i, n in enumerate(node.input):\n            if n == input_name[0]:\n                node.input[i] = new_output\n                break\n        return new_node\n\n    def insert_new_node_on_output(self, op_type, output_name, name, domain=None, **kwargs):\n        """"""Create and insert a new node into the graph.\n        Args:\n            op_type: type for new operation\n            output_name: the names of the outputs above us\n            name: the name of the new op\n            kwargs: attributes of the new node\n\n        Returns:\n            node that was inserted\n        """"""\n        utils.make_sure(isinstance(output_name, six.text_type), ""output_name\'s type is not expected: %s"",\n                        type(output_name))\n        utils.make_sure(isinstance(op_type, six.text_type), ""op_type\'s type is not expected: %s"",\n                        type(op_type))\n\n        new_output = port_name(name)\n        new_node = self.make_node(op_type, [output_name], attr=kwargs, outputs=[new_output], name=name, domain=domain)\n\n        to_replace = [n for n in self.get_nodes() if n != new_node]\n        self.replace_all_inputs(to_replace, output_name, new_output)\n        return new_node\n\n    def find_output_consumers(self, output_name):\n        """"""Find all nodes consuming a given output.""""""\n        nodes = []\n        for node in self.get_nodes():\n            if output_name in node.input:\n                nodes.append(node)\n\n            # find consumers in sub graphs\n            body_graphs = node.get_body_graphs()\n            if body_graphs:\n                for g in body_graphs.values():\n                    nodes.extend(g.find_output_consumers(output_name))\n        return nodes\n\n    @staticmethod\n    def replace_all_inputs(ops, old_input, new_input):\n        """"""Replace all inputs pointing to old_input with new_input.""""""\n        if old_input == new_input:\n            return\n\n        for node in ops:\n            if old_input in node.input and new_input in node.output:\n                raise RuntimeError(""creating a circle in the graph is not allowed: "" + node.name)\n\n            for i, input_name in enumerate(node.input):\n                if input_name == old_input:\n                    node.input[i] = new_input\n\n            # modify references in sub graphs\n            body_graphs = node.get_body_graphs()\n            if body_graphs:\n                for g in body_graphs.values():\n                    g.replace_all_inputs(g.get_nodes(), old_input, new_input)\n\n    @staticmethod\n    def replace_input(node, old_input, new_input):\n        """"""Replace node.""""""\n        assert isinstance(node, Node) and isinstance(old_input, six.text_type) and isinstance(new_input, six.text_type)\n        is_replaced = False\n        for i, input_name in enumerate(node.input):\n            if input_name == old_input:\n                node.input[i] = new_input\n                is_replaced = True\n        return is_replaced\n\n    def _extract_sub_graph_nodes(self, dest_node, input_checker=None):\n        """"""Return nodes of subgraph ending with dest_node.\n        Args:\n            dest_node: output node of the subgraph to find\n            input_checker: customized input check function: bool func(node)\n\n        Return:\n            a set of nodes\n        """"""\n        res_set = set()\n        if not dest_node or (input_checker and input_checker(dest_node) is False):\n            return res_set\n\n        processing_set = set([dest_node])\n        while processing_set:\n            top_node = processing_set.pop()\n            res_set.add(top_node)\n            all_inputs = top_node.input + list(top_node.get_implicit_inputs())\n            for input_id in all_inputs:\n                # we don\'t care about nested graph here, just handle current graph cropping.\n                node = self.get_node_by_output(input_id, search_in_parent_graphs=False)\n                if not node:\n                    # some nodes (for example Scan) have optional inputs, which\n                    # might have empty input.\n                    # subgraph might have input defined in outer graph\n                    continue\n                if node not in res_set:\n                    if input_checker and input_checker(node) is False:\n                        continue\n                    processing_set.add(node)\n        return res_set\n\n    def extract_sub_graph_nodes(self, outputs_name, input_checker=None, ignore_unused_placeholder=True):\n        """"""Return nodes of subgraph having output_ids as outputs.\n        Args:\n            output_ids: output node output id of the subgraph to find\n            input_checker: customized input check function: bool func(node)\n            ignore_unused_placeholder: bool, indicates whether unused placeholder will be removed\n                in the resulting nodes.\n        Return:\n            a list of nodes\n        """"""\n        res_set = set()\n        if not outputs_name:\n            return list(res_set)\n\n        for output in outputs_name:\n            node = self.get_node_by_output(output, search_in_parent_graphs=False)\n            res_set = res_set.union(self._extract_sub_graph_nodes(node, input_checker))\n\n        if not ignore_unused_placeholder:\n            # add back placeholder nodes if they are not connected to outputs.\n            for node in self.get_nodes():\n                if node.is_graph_input():\n                    res_set.add(node)\n                elif node.type == ""PlaceholderWithDefault"" and node.inputs[0].is_const():\n                    res_set.add(node.inputs[0])\n\n        return list(res_set)\n\n    def delete_unused_nodes(self, outputs_name):\n        """"""Delete nodes not in subgraph ending with output_names.""""""\n        if not outputs_name:\n            logger.debug(""Outputs not specified, delete_unused_nodes not taking effect."")\n            return\n\n        # we need keep those placeholders that are used as input of Loop\'s body graph.\n        # some of them are not used in the graph, but still need be there to keep the graph complete.\n        related_nodes = self.extract_sub_graph_nodes(outputs_name, ignore_unused_placeholder=False)\n        for node in related_nodes:\n            attr_body_graphs = node.get_body_graphs()\n            if attr_body_graphs:\n                for _, body_graph in attr_body_graphs.items():\n                    body_graph.delete_unused_nodes(body_graph.outputs)\n        self.reset_nodes(related_nodes)\n\n    def safe_to_remove_nodes(self, to_delete):\n        """""" List of nodes that safe to delete (i.e. outputs not consumed by other nodes.)""""""\n        safe_to_remove = []\n        delete_set = set(to_delete)\n        for n in delete_set:\n            out_consumers = set()\n            for out in n.output:\n                out_consumers |= set(self.find_output_consumers(out))\n            if out_consumers.issubset(delete_set):\n                safe_to_remove.append(n)\n        return safe_to_remove\n\n    def safe_remove_nodes(self, to_delete):\n        """"""Delete nodes in `to_delete` without third-party node consuming it.""""""\n        delete_set = set(to_delete)\n        for n in delete_set:\n            out_consumers = set()\n            for out in n.output:\n                out_consumers |= set(self.find_output_consumers(out))\n            if out_consumers.issubset(delete_set):\n                self.remove_node(n.name)\n\n\nclass GraphUtil(object):\n    """"""Utilities for Graph manipulation.""""""\n\n    @staticmethod\n    def optimize_graph(graph):\n        return optimizer.optimize_graph(graph)\n\n    @staticmethod\n    def optimize_model_proto(onnx_model_proto):\n        """"""Optimize the model proto, for example: eliminating all useless Transpose pairs.\n\n        Returns:\n            model proto after optimization, if optimizer run successfully\n            or onnx_model_proto, if exceptions happens\n        """"""\n        try:\n            kwargs = GraphUtil.get_onnx_model_properties(onnx_model_proto)\n            graph = GraphUtil.create_graph_from_onnx_model(onnx_model_proto)\n            graph = GraphUtil.optimize_graph(graph)\n            model_proto = graph.make_model(onnx_model_proto.graph.doc_string,\n                                           graph_name=onnx_model_proto.graph.name, **kwargs)\n\n            if onnx_model_proto.metadata_props:\n                metadata_props = {p.key: p.value for p in onnx_model_proto.metadata_props}\n                helper.set_model_props(model_proto, metadata_props)\n            return model_proto\n        except Exception:\n            # sometimes, onnx shape inference will fail for some reason,\n            # return onnx_model_proto for this case\n            logger.warning(""Failed to optimize model proto"", exc_info=1)\n            return onnx_model_proto\n\n    @staticmethod\n    def get_onnx_model_properties(onnx_model_proto):\n        """"""Get ModelProto properties""""""\n        kwargs = {}\n        if onnx_model_proto.HasField(\'ir_version\'):\n            kwargs[""ir_version""] = onnx_model_proto.ir_version\n        if onnx_model_proto.HasField(\'producer_name\'):\n            kwargs[""producer_name""] = onnx_model_proto.producer_name\n        if onnx_model_proto.HasField(\'producer_version\'):\n            kwargs[""producer_version""] = onnx_model_proto.producer_version\n        if onnx_model_proto.HasField(\'domain\'):\n            kwargs[""domain""] = onnx_model_proto.domain\n        if onnx_model_proto.HasField(\'model_version\'):\n            kwargs[""model_version""] = onnx_model_proto.model_version\n        if onnx_model_proto.HasField(\'doc_string\'):\n            kwargs[""doc_string""] = onnx_model_proto.doc_string\n        kwargs[""opset_imports""] = onnx_model_proto.opset_import\n\n        return kwargs\n\n    @staticmethod\n    def create_graph_from_onnx_model(onnx_model_proto):\n        """"""Create Graph loading onnx model proto.""""""\n        # apply shape inference on the model\n        inferred_model = shape_inference.infer_shapes(onnx_model_proto)\n        graph_proto = inferred_model.graph\n\n        opset_version = None\n        extra_opset = []\n        for opset in onnx_model_proto.opset_import:\n            if not opset.domain:\n                # domain field is None or empty means it is onnx domain\n                opset_version = opset.version\n            else:\n                extra_opset.append(opset)\n\n        utils.make_sure(opset_version is not None, ""opset version is not specified for onnx domain"")\n        main_graph = GraphUtil.create_graph_from_onnx_graph(graph_proto, opset_version, extra_opset)\n        return main_graph\n\n    @staticmethod\n    def create_graph_from_onnx_graph(graph_proto, opset_version=None, extra_opset=None):\n        """"""Create Graph loading onnx graph proto.""""""\n        output_shapes = {}\n        output_dtypes = {}\n\n        shapes, dtypes = GraphUtil._parse_shape_and_type_from_value_infos(graph_proto.value_info)\n        output_shapes.update(shapes)\n        output_dtypes.update(dtypes)\n\n        shapes, dtypes = GraphUtil._parse_shape_and_type_from_value_infos(graph_proto.output)\n        output_shapes.update(shapes)\n        output_dtypes.update(dtypes)\n\n        nodes_to_append = []\n        for n in graph_proto.node:\n            if n.op_type == ""Constant"":\n                n.op_type = ""Const""\n\n            # some pytorch model had empty names - make one up\n            if not n.name:\n                n.name = utils.make_name(""was_empty"")\n            nodes_to_append.append(n)\n\n        output_names = []\n        for n in graph_proto.output:\n            output_names.append(n.name)\n\n        g = Graph(nodes_to_append, output_shapes, output_dtypes, None, opset_version, extra_opset, output_names)\n        const_nodes = GraphUtil._parse_graph_initializer(g, graph_proto)\n        GraphUtil._parse_graph_input(g, graph_proto, [n.name for n in const_nodes])\n\n        for n in g.get_nodes():\n            for attr_name, attr_val in n.attr.items():\n                if attr_val.HasField(\'g\'):\n                    # it was assumed that the a.g has inferred shapes/dtypes.\n                    sub_g = GraphUtil.create_graph_from_onnx_graph(attr_val.g, opset_version, extra_opset)\n                    n.set_body_graph_as_attr(attr_name, sub_g)\n        return g\n\n    @staticmethod\n    def get_node_count_from_onnx_graph(graph_proto):\n        op_cnt = collections.Counter()\n        for n in graph_proto.node:\n            op_cnt[n.op_type] += 1\n        return op_cnt\n\n    @staticmethod\n    def _parse_shape_and_type_from_value_infos(value_infos):\n        """"""Get nodes output shapes and types from value infos.""""""\n        output_shapes = {}\n        output_dtypes = {}\n        for shape_info in value_infos:\n            type_proto = shape_info.type\n            elem_type = type_proto.tensor_type.elem_type\n            shape = type_proto.tensor_type.shape\n            tuned_shape = []\n            for d in shape.dim:\n                if d.HasField(\'dim_param\'):\n                    tuned_shape.append(-1)\n                elif d.HasField(\'dim_value\'):\n                    tuned_shape.append(d.dim_value)\n                else:\n                    # it is found, some unknown dims is missing after inference.\n                    tuned_shape.append(-1)\n            output_shapes[shape_info.name] = tuned_shape\n            output_dtypes[shape_info.name] = elem_type\n\n        return output_shapes, output_dtypes\n\n    @staticmethod\n    def _parse_graph_initializer(g, graph_proto):\n        """"""Get graph initializers and put into Graph object.""""""\n        const_nodes = []\n        for initializer in graph_proto.initializer:\n            np_val = numpy_helper.to_array(initializer)\n            const_nodes.append(g.make_const(initializer.name, np_val))\n\n        return const_nodes\n\n    @staticmethod\n    def _parse_graph_input(g, graph_proto, const_node_names):\n        """"""Get graph inputs not defined as initializers and put into Graph object.""""""\n        shapes, dtypes = GraphUtil._parse_shape_and_type_from_value_infos(graph_proto.input)\n        # make sure the input is added in order we read from graph_proto,\n        # because for subgraphs, the input orders matter.\n        for graph_input in graph_proto.input:\n            name = graph_input.name\n            shape = shapes[name]\n            dtype = dtypes[name]\n            if name not in const_node_names:\n                g.add_graph_input(name, dtype, shape)\n            else:\n                g.add_graph_input_with_default(name, g.get_node_by_name(name), dtype, shape)\n'"
tf2onnx/graph_builder.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.graph_helper - class to help building graph, such as helping to make complex node\n""""""\n\nimport numpy as np\nfrom tf2onnx import utils, logging\n\n\n# pylint: disable=missing-docstring\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass GraphBuilder(object):\n    """"""help to build graph""""""\n    def __init__(self, graph):\n        self._g = graph\n\n    @property\n    def graph(self):\n        return self._g\n\n    def make_slice(self, kwargs, name=None, shapes=None, dtypes=None):\n        """"""\n        slice changes its schema at opset 10: it treats some attributes as dynamic input\n        so this function has to process inputs according to graph\'s opset version\n        to get ""inputs"" and ""attr"" to feed ""make_node""\n        kwargs: key could be [""data"", ""starts"", ""ends"", ""axes"", ""steps"", ""outputs""].\n        """"""\n        outputs = kwargs.pop(""outputs"", None)\n\n        if self.graph.opset < 10:\n            # ""data"" is string\n            # ""starts"", ""ends"" and ""axes"" are attributes, and ""axes"" is optional.\n            inputs = [kwargs.pop(""data"")]\n            starts = self.convert_to_attribute(kwargs.pop(""starts""))\n            ends = self.convert_to_attribute(kwargs.pop(""ends""))\n            axes = self.convert_to_attribute(kwargs.pop(""axes"", None), is_optional=True)\n            attr = {""starts"": starts, ""ends"": ends, ""axes"": axes}\n        else:\n            # slice-10 has 3 required inputs ""data"", ""starts"", ""ends""l\n            # and 2 optional inputs ""axes"", ""steps""\n            # input sequence should be ""data"", ""starts"", ""ends"", ""axes"", ""steps""\n            attr = {}\n            data = self.convert_to_input(kwargs.pop(""data""))\n            starts = self.convert_to_input(kwargs.pop(""starts""), dtype=np.int64)\n            ends = self.convert_to_input(kwargs.pop(""ends""), dtype=np.int64)\n            axes = self.convert_to_input(kwargs.pop(""axes"", None), is_optional=True, dtype=np.int64)\n            steps = self.convert_to_input(kwargs.pop(""steps"", None), is_optional=True, dtype=np.int64)\n            inputs = [data, starts, ends, axes, steps]\n\n        # pro-process inputs and attr\n        if kwargs:\n            logger.warning(""kwargs contains un-used key"")\n\n        new_attr = {}\n        for key, val in attr.items():\n            if val is not None:\n                new_attr[key] = val\n        attr = new_attr\n\n        for ind, val in enumerate(inputs):\n            if val is None:\n                inputs[ind] = utils.ONNX_EMPTY_INPUT  # empty string means no connection in ONNX\n        # remove tailing """"\n        while inputs[-1] == utils.ONNX_EMPTY_INPUT:\n            inputs = inputs[:-1]\n\n        if self.graph.opset >= 10:\n            dtype = self.graph.get_dtype(inputs[1])\n            for input_data in inputs[1:]:\n                if input_data != utils.ONNX_EMPTY_INPUT:\n                    utils.make_sure(dtype == self.graph.get_dtype(input_data), ""dtype should be same"")\n\n        return self.graph.make_node(op_type=""Slice"", inputs=inputs, attr=attr, name=name,\n                                    outputs=outputs, shapes=shapes, dtypes=dtypes).output[0]\n\n    def convert_to_input(self, tensor, is_optional=False, dtype=None):\n        """"""in ONNX, input shold come from node, so it must be a string""""""\n        if is_optional and tensor is None:\n            return None\n\n        utils.make_sure(tensor is not None, ""input is required so it couldn\'t be None"")\n\n        res = tensor\n        if isinstance(tensor, list):\n            res = self.graph.make_const(utils.make_name(""const_slice""), np.array(tensor, dtype)).output[0]\n\n        utils.make_sure(isinstance(res, str), ""input is a dynamic input, so a str is needed"")\n\n        return res\n\n    def convert_to_attribute(self, tensor, is_optional=False):\n        if is_optional and tensor is None:\n            return None\n\n        utils.make_sure(tensor is not None, ""input is required so it couldn\'t be None"")\n\n        res = tensor\n        if isinstance(tensor, str):\n            const_node = self.graph.get_node_by_output(tensor)\n            res = const_node.get_tensor_value(as_list=True)\n\n        utils.make_sure(isinstance(res, list), ""input is an attr, so a list is needed"")\n\n        return res\n'"
tf2onnx/graph_matcher.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities that match patterns in a tf.Graph.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom itertools import permutations\nimport six\n\n\nclass OpTypePattern(object):\n    """"""A tree pattern that matches TF expressions with certain op types.""""""\n\n    def __init__(self, op_type, name=None, inputs=None):\n        """"""Initializes an OpTypePattern.\n\n        Args:\n          op_type: string that specifies the allowed types of the root. It can be\n            (1) an op type, e.g. \'Conv2D\',\n            (2) \'*\', i.e. wildcard, or\n            (3) multiple op types separated by \'|\', e.g., \'Relu|Relu6\'.\n            We could use regex strings, which might be worthwhile when we have many\n            similar TF op types.\n          name: Optional string. The name of the pattern that can be looked up in\n            MatchResult.\n          inputs: Optional list of `OpTypePattern`s or strings that specify the\n            patterns for the inputs of a matching op. If None, this pattern accepts\n            any inputs of a matching op.\n        """"""\n        self._op_type = op_type\n        self._name = name\n        if inputs is None:\n            inputs = []\n        self._inputs = [\n            input_pattern if isinstance(input_pattern, OpTypePattern) else\n            OpTypePattern(input_pattern) for input_pattern in inputs\n        ]\n\n    @property\n    def op_type(self):\n        return self._op_type\n\n    @property\n    def inputs(self):\n        return self._inputs\n\n    @property\n    def name(self):\n        return self._name\n\n\nclass MatchResult(object):\n    r""""""Encapsulates the result of a match done by GraphMatcher.\n\n    MatchResult contains a map from OpTypePattern to the matching op and tensor.\n    When the matching op has multiple output tensors, the matching tensor is the\n    output tensor used by the matching op of the parent pattern. E.g., when we\n    match graph\n\n        -         +\n       / \\y0   y1/ \\\n      x    split    z\n            |\n            y         (nodes are ops; edges are going up)\n\n    against add_pattern defined as\n\n      y1_pattern = OpTypePattern(\'*\')\n      z_pattern = OpTypePattern(\'*\')\n      add_pattern = OpTypePattern(\'+\', inputs=[y1_pattern, z_pattern])\n\n    the matching op of `y1_pattern` is `split`, and the matching tensor of\n    `y1_pattern`\n    is `y1` not `y0`.\n    """"""\n\n    def __init__(self):\n        self._pattern_to_op_tensor = {}\n        self._name_to_pattern = {}\n\n    def add(self, pattern, op, tensor):\n        self._pattern_to_op_tensor[pattern] = op, tensor\n        if pattern.name is not None:\n            # allow this so we can apply subgraphs multiple times\n            # if pattern.name in self._name_to_pattern:\n            #   raise ValueError(\n            #       \'Name %s is already bound to another pattern\' % pattern.name)\n            self._name_to_pattern[pattern.name] = pattern\n\n    def _to_pattern(self, pattern_or_name):\n        if isinstance(pattern_or_name, OpTypePattern):\n            return pattern_or_name\n\n        if isinstance(pattern_or_name, six.text_type):\n            return self._name_to_pattern.get(pattern_or_name)\n\n        raise ValueError(\'pattern_or_name has type %s. Expect OpTypePattern or str.\'\n                         % type(pattern_or_name))\n\n    def get_op(self, pattern_or_name, default=None):\n        """"""\n        For now, if the op can not be effectively obtained, then the function will return the default\n        instead of an error.\n        """"""\n        op_and_tensor = self._pattern_to_op_tensor.get(self._to_pattern(pattern_or_name))\n        if op_and_tensor:\n            return op_and_tensor[0]\n        return default\n\n    def get_tensor(self, pattern_or_name, default=None):\n        """"""\n        For now, if the tensor can not be effectively obtained, then the function will return the default\n        instead of an error.\n        """"""\n        op_and_tensor = self._pattern_to_op_tensor.get(self._to_pattern(pattern_or_name))\n        if op_and_tensor:\n            return op_and_tensor[1]\n        return default\n\n    def get_nodes(self):\n        return [n[0] for n in self._pattern_to_op_tensor.values()]\n\n\nclass GraphMatcher(object):\n    """"""Checks if a particular subgraph matches a given pattern.""""""\n\n    def __init__(self, pattern, allow_reorder=False):\n        """"""Initializes a GraphMatcher.\n\n        Args:\n          pattern: The `OpTypePattern` against which `GraphMatcher` matches\n            subgraphs.\n        """"""\n        self._pattern = pattern\n        self._allow_reorder = allow_reorder\n\n    @staticmethod\n    def _is_op_type_same(op, pattern):\n        if pattern.op_type == ""*"":\n            return True\n\n        if op.type in pattern.op_type.split(\'|\'):\n            return True\n\n        return False\n\n    def _match_pattern(self, pattern, op, tensor):\n        """"""Returns whether an TF expression rooted at `op` matches `pattern`.\n\n        If there is a match, adds to `self._match_result` the matching op and tensor\n        with key `pattern`.\n\n        Args:\n          pattern: An `OpTypePattern`.\n          op: A `tf.Operation` to match against the pattern.\n          tensor: the output `tf.Tensor` of `op` that is used by the matching op of\n            `pattern`\'s parent. Can be None if `pattern` is already the root of the\n            pattern tree.\n\n        Returns:\n          if matched return True and match_list whose elem is [pattern, op, tensor]\n          else return False\n        the condition that op is matched with pattern:\n        1 op is same:\n          if pattern.op_type is None or *, then treat as same\n          or op.type in pattern.op_type.split(""|"")\n        2 op.inputs are same with pattern.inputs:\n          if not pattern.inputs, then treat as same\n          otherwise, iteratively compare input nodes with pattern.\n        """"""\n        match_list = []\n        if pattern.op_type is None:\n            return True, match_list\n\n        if self._is_op_type_same(op, pattern):\n            match_list.append([pattern, op, tensor])\n        else:\n            return False, match_list\n\n        if not pattern.inputs:\n            # If pattern.inputs is empty, skips the rest and accepts all the inputs.\n            return True, match_list\n\n        if not op or len(op.inputs) != len(pattern.inputs):\n            return False, match_list\n\n        if self._allow_reorder:\n            pattern_inputs_list = permutations(pattern.inputs)\n        else:\n            pattern_inputs_list = [pattern.inputs]\n\n        for possible_pattern_inputs in pattern_inputs_list:\n            pat = list(zip(op.inputs, possible_pattern_inputs))\n            match_flag_of_inputs = []\n            match_lists_of_inputs = []\n            for input_tensor, input_pattern in pat:\n                # print(""MATCHING"", input_pattern.op_type, input_tensor.type)\n                flag, match_list_of_input = self._match_pattern(input_pattern, input_tensor, input_tensor)\n                match_flag_of_inputs.append(flag)\n                match_lists_of_inputs.extend(match_list_of_input)\n\n            if all(match_flag_of_inputs):\n                match_list.extend(match_lists_of_inputs)\n                return True, match_list\n        return False, match_list\n\n    def _parse_match_list_to_match_result(self, match_list):\n        for pattern, op, tensor in match_list:\n            self._match_result.add(pattern, op, tensor)\n\n    def match_op(self, op):\n        """"""Matches `op` against `self._pattern`.\n\n        Args:\n          op: `tf.Operation` to match against the pattern.\n\n        Returns:\n          Returns a `MatchResult` if `op` matches the pattern; otherwise, returns\n          None.\n        """"""\n        self._match_result = MatchResult()\n        match_flag, match_list = self._match_pattern(self._pattern, op, tensor=None)\n        if not match_flag:\n            return None\n        self._parse_match_list_to_match_result(match_list)\n        return self._match_result\n\n    def match_ops(self, ops):\n        """"""Matches each operation in `ops` against `self._pattern`.\n\n        Args:\n          ops: collection of `tf.Operation` to match against the pattern.\n\n        Yields:\n          `MatchResult` for each `tf.Operation` that matches the pattern.\n        """"""\n        for op in ops:\n            match_result = self.match_op(op)\n            if match_result:\n                yield match_result\n\n    def match_graph(self, graph):\n        """"""Matches each operation in `graph` against `self._pattern`.\n\n        Args:\n          graph: `tf.Graph` containing operations to match.\n\n        Yields:\n          `MatchResult` for each `tf.Operation` in `graph` that matches the pattern.\n        """"""\n        # Python 3.3.2+ implements `yield from`, but for now:\n        for match_result in self.match_ops(graph.get_operations()):\n            yield match_result\n'"
tf2onnx/handler.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Opset registry.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport collections\nimport inspect\n\nfrom tf2onnx import constants\n\n# pylint: disable=unused-argument,missing-docstring,invalid-name\n\n\nclass tf_op:\n    """"""Class to implement the decorator to register handlers that map tf to onnx.""""""\n\n    _OPSETS = collections.OrderedDict()\n    _MAPPING = None\n\n    def __init__(self, name, domain=constants.ONNX_DOMAIN, **kwargs):\n        """"""Called decorator from decorator.\n\n        :param name: The name of the tensorflow operator.\n        :param domain: The domain the operator belongs to, defaults to onnx.\n        :param kwargs: Dictionary that are passed to the handler. A key \'onnx_op\' will change the operator name.\n        """"""\n        if not isinstance(name, list):\n            name = [name]\n        self.name = name\n        self.domain = domain\n        self.kwargs = kwargs\n\n    def __call__(self, func):\n        opset = tf_op._OPSETS.get(self.domain)\n        if not opset:\n            opset = []\n            tf_op._OPSETS[self.domain] = opset\n        for k, v in inspect.getmembers(func, inspect.ismethod):\n            if k.startswith(""version_""):\n                version = int(k.replace(""version_"", """"))\n                while version >= len(opset):\n                    opset.append({})\n                opset_dict = opset[version]\n                for name in self.name:\n                    opset_dict[name] = (v, self.kwargs)\n        return func\n\n    def register_compat_handler(self, func, version):\n        """"""Register old style custom handler.\n\n        :param func: The handler.\n        :param version: The domain the operator belongs to, defaults to onnx.\n        :param version: The version of the handler.\n        """"""\n        opset = tf_op._OPSETS.get(self.domain)\n        if not opset:\n            opset = []\n            tf_op._OPSETS[self.domain] = opset\n            while version >= len(opset):\n                opset.append({})\n            opset_dict = opset[version]\n            opset_dict[self.name[0]] = (func, self.kwargs)\n\n    @staticmethod\n    def get_opsets():\n        return tf_op._OPSETS\n\n    @staticmethod\n    def create_mapping(max_onnx_opset_version, extra_opsets):\n        """"""Create the final mapping dictionary by stacking domains and opset versions.\n\n        :param max_onnx_opset_version: The highest onnx opset the resulting graph may use.\n        :param extra_opsets: Extra opsets the resulting graph may use.\n        """"""\n        mapping = {constants.ONNX_DOMAIN: max_onnx_opset_version}\n        if extra_opsets:\n            for extra_opset in extra_opsets:\n                mapping[extra_opset.domain] = extra_opset.version\n        ops_mapping = {}\n        for domain, opsets in tf_op.get_opsets().items():\n            for target_opset, op_map in enumerate(opsets):\n                m = mapping.get(domain)\n                if m:\n                    if target_opset <= m and op_map:\n                        ops_mapping.update(op_map)\n\n        tf_op._MAPPING = ops_mapping\n        return ops_mapping\n\n    @staticmethod\n    def find_effective_op(name):\n        """"""Find the effective version of an op create_mapping.\n           This is used if we need to compose ops from other ops where we\'d need to find the\n           op that is doing to be used in the final graph, for example there is a custom op\n           that overrides a onnx op ...\n\n        :param name: The operator name.\n        """"""\n        map_info = tf_op._MAPPING.get(name)\n        if map_info is None:\n            return None\n        return map_info\n'"
tf2onnx/schemas.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.schema\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nimport copy\nfrom collections import defaultdict, OrderedDict\nfrom onnx import defs, helper, TensorProto, OperatorSetIdProto, shape_inference\n\nfrom . import constants\nfrom . import utils\n\nlogger = logging.getLogger(__name__)\n\n\nclass OnnxOpSchema(object):\n    """"""Wrapper for Onnx schema.""""""\n\n    def __init__(self, name, domain, since_version, attributes):\n        """"""Create a Onnx schema\n        Args:\n            name (str): op name\n            attributes (List[str]): valid attributes\n            domain (str): default value """" means it\'s Onnx domain\n            since_version (int): opset version, default is 1\n        """"""\n        self._name = name\n        self._domain = domain\n        self._attributes = attributes\n        self._since_version = since_version\n\n    @property\n    def attributes(self):\n        return self._attributes\n\n    @property\n    def domain(self):\n        return self._domain\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def since_version(self):\n        return self._since_version\n\n    @staticmethod\n    def from_onnx_schema(onnx_schema):\n        name = onnx_schema.name\n        domain = onnx_schema.domain\n        since_version = int(onnx_schema.since_version)\n        attributes = onnx_schema.attributes\n        return OnnxOpSchema(name, domain, since_version, attributes)\n\n    def has_attribute(self, attr):\n        return attr in self.attributes\n\n\ndef _register_all_schemas_with_history():\n    """"""Register all schemas with history""""""\n    onnx_schemas = defs.get_all_schemas_with_history()\n    name_domain_version_schema_map = defaultdict(lambda: defaultdict(dict))\n    for s in onnx_schemas:\n        schema = OnnxOpSchema.from_onnx_schema(s)\n        name_domain_version_schema_map[schema.name][schema.domain][schema.since_version] = schema\n\n    ordered_map = defaultdict(lambda: defaultdict(OrderedDict))\n    for name, domain_version_schema_map in name_domain_version_schema_map.items():\n        for domain, version_schema_map in domain_version_schema_map.items():\n            ordered_map[name][domain] = OrderedDict(\n                sorted(version_schema_map.items(), key=lambda x: -x[0])\n            )\n    return ordered_map\n\n\ndef _parse_domain_opset_versions(schemas):\n    """""" Get max opset version among all schemas within each domain. """"""\n    domain_opset_versions = dict()\n    for domain_version_schema_map in schemas.values():\n        for domain, version_schema_map in domain_version_schema_map.items():\n            # version_schema_map is sorted by since_version in descend order\n            max_version = next(iter(version_schema_map))\n            if domain not in domain_opset_versions:\n                domain_opset_versions[domain] = int(max_version)\n            else:\n                domain_opset_versions[domain] = max(domain_opset_versions[domain], int(max_version))\n    return domain_opset_versions\n\n\n# format is <OpName, <Domain, <SinceVersion, OpSchema>>>\n# SinceVersion is sorted from high to low\n_schemas = _register_all_schemas_with_history()\n\n_domain_opset_versions = _parse_domain_opset_versions(_schemas)\n\n\ndef get_schema(name, max_inclusive_opset_version, domain=None):\n    """"""Get schema by name within specific version.""""""\n    domain = domain or constants.ONNX_DOMAIN\n    domain_version_schema_map = _schemas[name]\n    version_schema_map = domain_version_schema_map[domain]\n    for version, schema in version_schema_map.items():\n        if version <= max_inclusive_opset_version:\n            return schema\n    return None\n\n\ndef get_max_supported_opset_version(domain=None):\n    """"""Get max supported opset version by current onnx package given a domain.""""""\n    domain = domain or constants.ONNX_DOMAIN\n    return _domain_opset_versions.get(domain, None)\n\n\ndef infer_onnx_shape_dtype(node, opset_version, input_shapes, input_dtypes, initializers=None):\n    """"""\n    Infer shapes and dtypes for outputs of the node.\n    Sometimes, shape inference needs the values of node\'s inputs, so initializers are used.\n    """"""\n\n    def build_onnx_op(node):\n        """"""Build onnx op""""""\n        onnx_node = helper.make_node(node.type, node.input, node.output, name=node.name)\n        # deal with attributes\n        attr = []\n        attr_graphs = node.get_body_graphs()\n        if attr_graphs:\n            for attr_name, sub_graph in attr_graphs.items():\n                copied_sub_graph = copy.deepcopy(sub_graph)\n                graph_proto = copied_sub_graph.make_graph(""graph for "" + node.name + "" "" + attr_name)\n                attr.append(helper.make_attribute(attr_name, graph_proto))\n        attr.extend(node.attr_onnx.values())\n        if attr:\n            onnx_node.attribute.extend(attr)\n        return onnx_node\n\n    inputs = []\n    outputs = []\n    for inp, shape, dtype in zip(node.input, input_shapes, input_dtypes):\n        inputs.append(utils.make_onnx_inputs_outputs(inp, dtype, shape))\n    for output in node.output:\n        outputs.append(utils.make_onnx_inputs_outputs(output, TensorProto.UNDEFINED, None))\n    graph_proto = helper.make_graph([build_onnx_op(node)], ""infer-graph"", inputs, outputs, initializer=initializers)\n    imp = OperatorSetIdProto()\n    imp.version = opset_version\n    model_proto = helper.make_model(graph_proto, opset_imports=[imp])\n\n    inferred_model = None\n    try:\n        inferred_model = shape_inference.infer_shapes(model_proto)\n    except Exception:  # pylint: disable=broad-except\n        logger.warning(\n            ""ONNX Failed to infer shapes and dtypes for [%s, type: %s]"",\n            node.name, node.type, exc_info=1\n        )\n        return None, None\n\n    shapes = {}\n    dtypes = {}\n    for output in inferred_model.graph.output:\n        tensor_type = output.type.tensor_type\n        if tensor_type.HasField(""elem_type""):\n            dtypes[output.name] = tensor_type.elem_type\n        else:\n            dtypes[output.name] = TensorProto.UNDEFINED\n        # 0 in shapes of onnx means unknown which is -1 in our convertor\n        if tensor_type.HasField(""shape""):\n            shapes[output.name] = [\n                dim.dim_value if dim.dim_value != 0 else utils.ONNX_UNKNOWN_DIMENSION for dim in tensor_type.shape.dim\n            ]\n        else:\n            shapes[output.name] = None\n    output_shapes = []\n    output_dtypes = []\n    for output in node.output:\n        if output in shapes:\n            output_shapes.append(shapes[output])\n        else:\n            output_shapes.append(None)\n        if output in dtypes:\n            output_dtypes.append(dtypes[output])\n        else:\n            output_dtypes.append(TensorProto.UNDEFINED)\n    return output_shapes, output_dtypes\n'"
tf2onnx/shape_inference.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\n""""""\r\ntf2onnx.shape_inference - shape inference function for tf2onnx\r\n""""""\r\n\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\nimport logging\r\nfrom distutils.version import LooseVersion\r\nfrom collections import defaultdict\r\n\r\nfrom tf2onnx import utils\r\nfrom tf2onnx.tf_utils import get_tf_tensor_shape, get_tf_const_value, get_tf_shape_attr, get_tf_version\r\nfrom tf2onnx.tf_loader import tf_reload_graph\r\n\r\n# pylint: disable=logging-not-lazy,missing-docstring,consider-swap-variables\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef infer_shape(tf_graph, shape_override):\r\n    """"""Infer shape for TF graph with shape_override set first.""""""\r\n    if shape_override:\r\n        logger.info(""Apply shape override:"")\r\n        for name, shape in shape_override.items():\r\n            logger.info(""\\tSet %s shape to %s"", name, shape)\r\n            tf_graph.get_tensor_by_name(name).set_shape(shape)\r\n        tf_graph = tf_reload_graph(tf_graph)\r\n\r\n    tf_graph = infer_shape_for_graph(tf_graph)\r\n\r\n    op_outputs_with_none_shape = check_shape_for_tf_graph(tf_graph)\r\n    if op_outputs_with_none_shape:\r\n        if get_tf_version() > LooseVersion(""1.5.0""):\r\n            for op, outs in op_outputs_with_none_shape.items():\r\n                logger.warning(\r\n                    ""Cannot infer shape for %s: %s"",\r\n                    op, "","".join(outs)\r\n                )\r\n        tf_graph = infer_shape_for_graph_legacy(tf_graph)\r\n\r\n    return tf_graph\r\n\r\n\r\ndef check_shape_for_tf_graph(tf_graph):\r\n    """"""\r\n    Check whether TF graph misses any shape,\r\n    and return all ops with None shape outputs for TF graph.\r\n    """"""\r\n    skip_list = {\'FusedBatchNormV3\': 5}\r\n    op_outputs_mapping_none_shape = defaultdict(list)\r\n    for op in tf_graph.get_operations():\r\n        for i, out in enumerate(op.outputs):\r\n            if op.type in skip_list:\r\n                if skip_list[op.type] == i:\r\n                    continue\r\n            if get_tf_tensor_shape(out) is None:\r\n                op_outputs_mapping_none_shape[op.name].append(out.name)\r\n    return op_outputs_mapping_none_shape\r\n\r\n\r\ndef infer_shape_for_graph(tf_graph):\r\n    """"""\r\n    Infer shape for Tensorflow ops.\r\n    Tensorflow explicitly sets shape for some ops in python code, such as Switch, Merge and TensorArrayGather.\r\n    These shapes may be lost after freezing TF graph to graph_def without add_shapes=True.\r\n    To bring these shapes back, we implement our own shape inference for these control flow ops based on one assumption:\r\n    **outputs of Merge op have the same shape (at least the same rank) of its inputs**.\r\n    With this assumption, our shape inference can handle:\r\n        1. in tf.cond, outputs of two branches have the same rank.\r\n        2. in tf.while_loop, loop variables don\'t change their rank.\r\n    """"""\r\n    shape_updated = True\r\n    while shape_updated:\r\n        shape_updated = False\r\n        for o in tf_graph.get_operations():\r\n            updated = infer_shape_for_op(o)\r\n            if updated:\r\n                shape_updated = True\r\n        if shape_updated:\r\n            tf_graph = tf_reload_graph(tf_graph)\r\n    return tf_graph\r\n\r\n\r\ndef infer_shape_for_op(op):\r\n    has_unknown_output_shape = any(get_tf_tensor_shape(out) is None for out in op.outputs)\r\n\r\n    if not has_unknown_output_shape:\r\n        return False\r\n\r\n    if op.type == ""Placeholder"":\r\n        # if placeholder shape is not found, try to get it from ""shape"" attribute.\r\n        attr_shape = get_tf_shape_attr(op)\r\n        if attr_shape is not None:\r\n            new_shape = list(attr_shape)\r\n            op.outputs[0].set_shape(new_shape)\r\n            logger.debug(""set placeholder op [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n            return True\r\n        logger.warning(""Shape of placeholder %s is unknown, treated it as a scalar"", op.name)\r\n        op.outputs[0].set_shape([])\r\n        return True\r\n\r\n    if op.type == ""Merge"":\r\n        s1 = get_tf_tensor_shape(op.inputs[0])\r\n        s2 = get_tf_tensor_shape(op.inputs[1])\r\n        new_shape = None\r\n        if s1 is None and s2 is None:\r\n            return False\r\n        if s1 is None and s2 is not None:\r\n            new_shape = s2\r\n        if s1 is not None and s2 is None:\r\n            new_shape = s1\r\n\r\n        if new_shape is not None:\r\n            op.inputs[0].set_shape(new_shape)\r\n            op.inputs[1].set_shape(new_shape)\r\n            op.outputs[0].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n            return True\r\n\r\n        # inputs\' shapes both exist\r\n        if s1 != s2:\r\n            if len(s1) != len(s2):\r\n                logger.warning(""Shapes of Merge %s have different ranks: %s, %s"", op.name, len(s1), len(s2))\r\n                return False\r\n\r\n            logger.debug(""Inputs of Merge %s have different shapes: %s, %s, but the same rank"", op.name, s1, s2)\r\n            new_shape = _merge_shapes_for_tf(s1, s2)\r\n            op.outputs[0].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n        else:\r\n            new_shape = s1\r\n            op.outputs[0].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n\r\n        return True\r\n\r\n    if op.type == ""Switch"":\r\n        new_shape = get_tf_tensor_shape(op.inputs[0])\r\n        if new_shape is not None:\r\n            op.outputs[0].set_shape(new_shape)\r\n            op.outputs[1].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[1].name, new_shape)\r\n            return True\r\n        return False\r\n\r\n    if op.type == ""Enter"":\r\n        new_shape = get_tf_tensor_shape(op.inputs[0])\r\n        if new_shape is not None:\r\n            op.outputs[0].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n            return True\r\n        return False\r\n\r\n    if op.type == ""TensorArrayGatherV3"":\r\n        # TensorArrayGatherV3\'s output: all of the elem in the TensorArray,\r\n        # concatenated along a new axis (the new dimension 0), so shape of TensorArray should be found first.\r\n        # And TensorArrayWrite will write elem to TensorArray, so shape of TensorArray can be got from TensorArrayWrite\r\n        # so the process is: first find TensorArrayWrite and then get TensorArray\'s shape,\r\n        # and finally add one dim to the shape is shape of TensorArrayGather\r\n\r\n        handle_op = op.inputs[0].op\r\n        if handle_op.type != ""TensorArrayV3"":\r\n            return False\r\n\r\n        # find TensorArrayWrite\r\n        tensor_array_write_op = _find_tensorarray_write(handle_op)\r\n        if not tensor_array_write_op:\r\n            return False\r\n        # get TensorArray shape from input tensor of the found TensorArrayWrite op\r\n        shape = get_tf_tensor_shape(tensor_array_write_op.inputs[2])\r\n        # update TensorArray\'s shape info\r\n        if shape is not None:\r\n            new_shape = [None] + shape\r\n            op.outputs[0].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n            return True\r\n        return False\r\n\r\n    if op.type == ""TensorArrayReadV3"":\r\n        # TensorArrayRead reads an element from the TensorArray into output value.\r\n        # The TensorArray\'s shape can be got from TensorArrayScatter.\r\n        # So the process is: first find TensorArrayScatter\'s shape and then TensorArray\'s\r\n        # and finally take its last n-1 dim.\r\n        flow_in_op = op.inputs[2].op\r\n        if flow_in_op.type != ""Enter"":\r\n            return False\r\n\r\n        scatter_op = flow_in_op.inputs[0].op\r\n        if scatter_op.type != ""TensorArrayScatterV3"":\r\n            return False\r\n\r\n        value_shape_before_scatter = get_tf_tensor_shape(scatter_op.inputs[2])\r\n        if value_shape_before_scatter is None:\r\n            return False\r\n\r\n        new_shape = value_shape_before_scatter[1:]\r\n        if new_shape is not None:\r\n            op.outputs[0].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n            return True\r\n        return False\r\n\r\n    return False\r\n\r\n\r\ndef _find_tensorarray_write(op):\r\n    utils.make_sure(op.type == ""TensorArrayV3"", ""op should be tensorarray"")\r\n\r\n    tensor_array_consumers = op.outputs[0].consumers()\r\n    for i in tensor_array_consumers:\r\n        if i.type == ""Enter"":\r\n            consumer_ops = i.outputs[0].consumers()\r\n            for j in consumer_ops:\r\n                if j.type == ""TensorArrayWriteV3"":\r\n                    return j\r\n    return None\r\n\r\n\r\ndef _merge_shapes_for_tf(shape1, shape2):\r\n    """"""\r\n    Merge 2 shapes, return merged shape, set unknown for dims with different values.\r\n    Raise exception for mismatch.\r\n    """"""\r\n    if shape1 is None:\r\n        return shape2\r\n    if shape2 is None:\r\n        return shape1\r\n\r\n    utils.make_sure(utils.is_list_or_tuple(shape1), ""invalid type for shape1"")\r\n    utils.make_sure(utils.is_list_or_tuple(shape2), ""invalid type for shape2"")\r\n    utils.make_sure(len(shape1) == len(shape2), ""shapes rank mismatch: shape1=%s, shape2=%s"", shape1, shape2)\r\n\r\n    merged = []\r\n    for d1, d2 in zip(shape1, shape2):\r\n        d = d1\r\n        if d1 is None:\r\n            d = d2\r\n        elif not d2 is None:\r\n            # None means unknown in tensorflow\r\n            d = None\r\n        merged.append(d)\r\n    return merged\r\n\r\n\r\n######################################################################\r\n####   Below is our old tf shape inference as a supplementary     ####\r\n####            and a subtitute for TF 1.5.0                      ####\r\n######################################################################\r\n\r\ndirect_ops = [\r\n    ""Cast"",\r\n    ""Exit"",\r\n    ""Floor"",\r\n    ""Identity"",\r\n    ""LogicalNot"",\r\n    ""ReverseSequence"",\r\n    ""Relu6"",\r\n    ""Sigmoid"",\r\n    ""Square"",\r\n    ""Tanh""\r\n]\r\nbroadcast_ops = [\r\n    ""Add"",\r\n    ""Greater"",\r\n    ""GreaterEqual"",\r\n    ""Less"",\r\n    ""LessEqual"",\r\n    ""LogicalAnd"",\r\n    ""LogicalOr"",\r\n    ""Mul"",\r\n    ""RealDiv"",\r\n    ""Sub""\r\n]\r\n\r\n\r\ndef infer_shape_for_graph_legacy(tf_graph):\r\n    shape_updated = True\r\n    while shape_updated:\r\n        shape_updated = False\r\n        for op in tf_graph.get_operations():\r\n            updated = infer_shape_for_op_legacy(op)\r\n            if updated:\r\n                shape_updated = True\r\n\r\n    return tf_graph\r\n\r\n\r\ndef infer_shape_for_op_legacy(op):\r\n    # invoke tf shape inference first\r\n    infer_shape_for_op(op)\r\n\r\n    has_unknown_input_shape = any(get_tf_tensor_shape(inp) is None for inp in op.inputs)\r\n    has_unknown_output_shape = any(get_tf_tensor_shape(out) is None for out in op.outputs)\r\n\r\n    # an input shape may be inferred from op output or other input shapes\r\n    # try to infer it first\r\n    if has_unknown_input_shape:\r\n        if infer_input_shapes(op):\r\n            return True\r\n\r\n    if not has_unknown_output_shape:\r\n        return False\r\n\r\n    # for those ops, we don\'t expect all input shapes available to infer output shapes.\r\n    ret = infer_output_shapes_with_partial_inputs(op)\r\n    if ret is not None:\r\n        return ret\r\n\r\n    # for ops, we need all input shapes ready to infer output shapes.\r\n    are_all_input_shape_ready = True\r\n    no_shape = []\r\n    for i in op.inputs:\r\n        if get_tf_tensor_shape(i) is None:\r\n            are_all_input_shape_ready = False\r\n            no_shape.append(i.name)\r\n\r\n    if not are_all_input_shape_ready:\r\n        logger.debug(""op %s has inputs don\'t have shape specified, they are: %s"", op.name, no_shape)\r\n        return False\r\n\r\n    if op.type in direct_ops:\r\n        return set_shape_from_input(op.inputs[0], op.outputs[0])\r\n\r\n    if op.type in broadcast_ops:\r\n        return set_shape_from_inputs_broadcast(op.inputs, op.outputs[0])\r\n\r\n    if op.type == ""RandomUniform"":\r\n        shape_op = op.inputs[0].op\r\n        if not shape_op or shape_op.type != ""Shape"":\r\n            return False\r\n        return set_shape_from_input(shape_op.inputs[0], op.outputs[0])\r\n\r\n    if op.type == ""Gather"":\r\n        # uses the follwing link to know how to infer shape of output\r\n        # https://www.tensorflow.org/api_docs/python/tf/gather\r\n        shape_params = get_tf_tensor_shape(op.inputs[0])\r\n        shape_indices = get_tf_tensor_shape(op.inputs[1])\r\n        # gather can only have 2 inputs\r\n        # https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/gather.html\r\n        if len(op.inputs) == 3:\r\n            axis_op = op.inputs[2].op\r\n            if not utils.is_tf_const_op(axis_op):\r\n                return False\r\n            axis = get_tf_const_value(axis_op)\r\n        else:\r\n            axis = 0\r\n\r\n        shape = shape_params[:axis] + shape_indices + shape_params[axis + 1:]\r\n        op.outputs[0].set_shape(shape)\r\n        return True\r\n\r\n    if op.type in [""All"", ""Any"", ""Max"", ""Min""]:\r\n        axis_op = op.inputs[1].op\r\n        if not utils.is_tf_const_op(axis_op):\r\n            return False\r\n        axis = get_tf_const_value(axis_op)\r\n        if not isinstance(axis, list):\r\n            axis = [axis]\r\n        keep_dims = op.get_attr(""keep_dims"")\r\n        shape = get_tf_tensor_shape(op.inputs[0])\r\n        for i, _ in enumerate(axis):\r\n            if axis[i] < 0:\r\n                axis[i] += len(shape)\r\n\r\n        new_shape = []\r\n        for i, _ in enumerate(shape):\r\n            if i in axis:\r\n                if keep_dims:\r\n                    new_shape.append(1)\r\n            else:\r\n                new_shape.append(shape[i])\r\n\r\n        op.outputs[0].set_shape(new_shape)\r\n        logger.debug(""set %s op [%s] with new shape %s"", op.type, op.outputs[0].name, new_shape)\r\n        return True\r\n\r\n    if op.type == ""ExpandDims"":\r\n        # https://www.tensorflow.org/api_docs/python/tf/expand_dims\r\n        input_shape = get_tf_tensor_shape(op.inputs[0])\r\n        dim_op = op.inputs[1].op\r\n        if input_shape is None or not utils.is_tf_const_op(dim_op):\r\n            return False\r\n\r\n        dim = get_tf_const_value(dim_op)\r\n        if dim < 0:\r\n            dim = dim + len(input_shape) + 1\r\n\r\n        new_shape = input_shape[:dim] + [1] + input_shape[dim:]\r\n        op.outputs[0].set_shape(new_shape)\r\n        logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n        return True\r\n\r\n    if op.type == ""Unpack"":\r\n        input_shape = get_tf_tensor_shape(op.inputs[0])\r\n        if input_shape is None:\r\n            return False\r\n\r\n        axis = op.get_attr(""axis"")\r\n        axis = axis if axis >= 0 else axis + len(input_shape)\r\n        # the link below says that the rank of output is ""rank(input) -1"",\r\n        # from this statement ""num"" must equal to input_shape[axis], and if not tf will throw a runtime error\r\n        # https://www.tensorflow.org/api_docs/python/tf/unstack\r\n        new_shape = input_shape[:axis] + input_shape[axis + 1:]\r\n        for output in op.outputs:\r\n            output.set_shape(new_shape)\r\n            logger.debug(""set %s op [%s] with new shape %s"", op.type, output.name, new_shape)\r\n        return True\r\n\r\n    if op.type in [""Minimum"", ""Maximum""]:\r\n        # ops that are elementwise and support broadcasting\r\n        input_shapes = [get_tf_tensor_shape(op) for op in op.inputs]\r\n        new_shape = broadcast_shape_inference(*input_shapes)\r\n        op.outputs[0].set_shape(new_shape)\r\n        return True\r\n\r\n    return False\r\n\r\n\r\ndef infer_input_shapes(op):\r\n    if op.type in [""Select"", ""SelectV2""]:\r\n        shape_t = get_tf_tensor_shape(op.inputs[1])\r\n        shape_e = get_tf_tensor_shape(op.inputs[2])\r\n        # copy shape if t OR e does not have a shape, no update if t AND e both have shapes\r\n        if shape_t is None or shape_e is None:\r\n            new_shape = shape_t or shape_e\r\n            if new_shape is not None:\r\n                op.inputs[1].set_shape(new_shape)\r\n                op.inputs[2].set_shape(new_shape)\r\n                logger.debug(""set [%s, %s] with new shape %s"", op.inputs[1].name, op.inputs[2].name, new_shape)\r\n                return True\r\n    return False\r\n\r\n\r\ndef infer_output_shapes_with_partial_inputs(op):\r\n    # output shape of concat op: only the dim val of concatenated dim will be changed\r\n    # so only partial(at least one) input shapes need to be known to infer output shape of concat op\r\n    if utils.is_tf_concat_op(op):\r\n        data_inputs = op.inputs[:-1]\r\n        input_shapes = [get_tf_tensor_shape(inp) for inp in data_inputs]\r\n        input_shapes = [shape for shape in input_shapes if shape is not None]\r\n        if not input_shapes:\r\n            logger.debug(""all input shapes of concat op %s are None, can\'t infer its output shape"", op.name)\r\n            return False\r\n\r\n        new_shape = input_shapes[0]\r\n        axis_op = op.inputs[-1]\r\n        rank = len(new_shape)\r\n        if not utils.is_tf_const_op(axis_op):\r\n            op.outputs[0].set_shape([-1] * rank)\r\n            return True\r\n\r\n        axis = get_tf_const_value(axis_op)\r\n        axis = axis if axis >= 0 else axis + rank\r\n        new_shape[axis] = -1\r\n        if len(input_shapes) == len(data_inputs):  # all input shapes are known\r\n            concat_dim_vals = list(np.array(input_shapes)[:, axis])\r\n            # only when inputs\' shape are known, then val of concat dim can be calculated\r\n            if concat_dim_vals.count(-1) == 0:\r\n                new_shape[axis] = sum(concat_dim_vals)\r\n\r\n        op.outputs[0].set_shape(new_shape)\r\n        logger.debug(""set Concat op [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n        return True\r\n\r\n    if op.type in [""Select"", ""SelectV2""]:\r\n        new_shape = get_tf_tensor_shape(op.inputs[1])\r\n        if new_shape is None:\r\n            new_shape = get_tf_tensor_shape(op.inputs[2])\r\n        if new_shape is not None:\r\n            op.outputs[0].set_shape(new_shape)\r\n            op.inputs[1].set_shape(new_shape)\r\n            op.inputs[2].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n            return True\r\n        return False\r\n\r\n    if op.type == ""Pack"":\r\n        axis = op.get_attr(""axis"")\r\n        input_shape = None\r\n        for i in op.inputs:\r\n            s = get_tf_tensor_shape(i)\r\n            if s is not None:\r\n                input_shape = s\r\n                break\r\n        if input_shape is None:\r\n            return False\r\n        if axis < 0:\r\n            axis += len(input_shape)\r\n        for i in op.inputs:\r\n            if not get_tf_tensor_shape(i):\r\n                i.set_shape(input_shape)\r\n                logger.debug(""set [%s] with new shape %s"", i.name, input_shape)\r\n        new_shape = input_shape[:axis] + [len(op.inputs)] + input_shape[axis:]\r\n        op.outputs[0].set_shape(new_shape)\r\n        logger.debug(""set Pack op [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n        return True\r\n\r\n    if op.type == ""Pow"":\r\n        # https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/pow\r\n        new_shape = get_tf_tensor_shape(op.inputs[0])\r\n        if new_shape is None:\r\n            new_shape = get_tf_tensor_shape(op.inputs[1])\r\n        if new_shape is not None:\r\n            op.outputs[0].set_shape(new_shape)\r\n            logger.debug(""set [%s] with new shape %s"", op.outputs[0].name, new_shape)\r\n            return True\r\n        return False\r\n\r\n    return None\r\n\r\n\r\ndef set_shape_from_input(input_tensor, output_tensor):\r\n    new_shape = get_tf_tensor_shape(input_tensor)\r\n    if new_shape is not None:\r\n        output_tensor.set_shape(new_shape)\r\n        logger.debug(""set [%s] with new shape %s"", output_tensor.name, new_shape)\r\n        return True\r\n    return False\r\n\r\n\r\ndef set_shape_from_inputs_broadcast(input_tensors, output_tensor):\r\n    s1 = get_tf_tensor_shape(input_tensors[0])\r\n    s2 = get_tf_tensor_shape(input_tensors[1])\r\n    new_shape = broadcast_shape_inference(s1, s2)\r\n    if new_shape is not None:\r\n        output_tensor.set_shape(new_shape)\r\n        logger.debug(""set [%s] with new shape %s"", output_tensor.name, new_shape)\r\n        return True\r\n    return False\r\n\r\n\r\ndef broadcast_shape_inference(shape_0, shape_1):\r\n    if shape_0 is None:\r\n        return shape_1\r\n    if shape_1 is None:\r\n        return shape_0\r\n\r\n    # two dimensions are compatible when they are equal, or one of them is 1\r\n    # compare from last dim\r\n    if len(shape_0) > len(shape_1):\r\n        tmp = shape_0\r\n        shape_0 = shape_1\r\n        shape_1 = tmp\r\n\r\n    new_shape = shape_1\r\n    l = len(shape_0)\r\n    if l == 0:\r\n        return new_shape\r\n\r\n    i = l - 1\r\n    while i >= 0:\r\n        if shape_0[i] == shape_1[i]:\r\n            # do nothing\r\n            pass\r\n        elif shape_0[i] == 1:\r\n            # do nothing\r\n            pass\r\n        elif shape_1[i] == 1:\r\n            new_shape[i] = shape_0[i]\r\n        # maybe one of them is -1, we can use the other one as real shape.\r\n        elif shape_0[i] == -1:\r\n            pass\r\n        elif shape_1[i] == -1:\r\n            new_shape[i] = shape_0[i]\r\n        else:\r\n            logger.warning(""two shapes not possible to broadcast, %s, %s"", shape_0, shape_1)\r\n            return None\r\n        i -= 1\r\n    return new_shape\r\n'"
tf2onnx/tf_loader.py,42,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Methods to load tensorflow graph from graphdef, checkpoint or saved_model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nfrom distutils.version import LooseVersion\n\nimport tensorflow as tf\n\nfrom tf2onnx import utils\nfrom tf2onnx.tf_utils import get_tf_version, tflist_to_onnx\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=unused-argument,unused-import,no-value-for-parameter,unexpected-keyword-arg,ungrouped-imports\n# pylint: disable=missing-function-docstring,import-outside-toplevel,useless-import-alias,missing-docstring\n\n\ndef is_tf2():\n    return tf.__version__.startswith(""2."")\n\n\ndef _not_implemented_tf_placeholder(name):\n    """"""Creates a placeholder function for missing Tensorflow imports""""""\n\n    def not_implemented_tf_placeholder(*args, **kwargs):\n        raise NotImplementedError(\n            f\'Tensorflow verison {tf.__version__} does not implement \'\n            f\'`{name}`, try converting your model with a different version.\'\n        )\n\n    return not_implemented_tf_placeholder\n\n\ntry:\n    from tensorflow.python.framework.function_def_to_graph import function_def_to_graph\nexcept ImportError:\n    function_def_to_graph = _not_implemented_tf_placeholder(\'function_def_to_graph\')\n\nif is_tf2():\n    convert_variables_to_constants = tf.compat.v1.graph_util.convert_variables_to_constants\n    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\nelse:\n    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n\n    convert_variables_to_constants_v2 = _not_implemented_tf_placeholder(\'convert_variables_to_constants_v2\')\n\nif is_tf2():\n    tf_reset_default_graph = tf.compat.v1.reset_default_graph\n    tf_global_variables = tf.compat.v1.global_variables\n    tf_session = tf.compat.v1.Session  # pylint: disable=invalid-name\n    tf_graphdef = tf.compat.v1.GraphDef\n    tf_import_meta_graph = tf.compat.v1.train.import_meta_graph\n    tf_gfile = tf.io.gfile\n    tf_placeholder = tf.compat.v1.placeholder\n    extract_sub_graph = tf.compat.v1.graph_util.extract_sub_graph\nelif LooseVersion(tf.__version__) >= ""1.13"":\n    # 1.13 introduced the compat namespace\n    tf_reset_default_graph = tf.compat.v1.reset_default_graph\n    tf_global_variables = tf.compat.v1.global_variables\n    tf_session = tf.compat.v1.Session  # pylint: disable=invalid-name\n    tf_graphdef = tf.compat.v1.GraphDef\n    tf_import_meta_graph = tf.compat.v1.train.import_meta_graph\n    tf_gfile = tf.gfile\n    tf_placeholder = tf.compat.v1.placeholder\n    extract_sub_graph = tf.compat.v1.graph_util.extract_sub_graph\nelse:\n    # older than 1.13\n    tf_reset_default_graph = tf.reset_default_graph\n    tf_global_variables = tf.global_variables\n    tf_session = tf.Session  # pylint: disable=invalid-name\n    tf_graphdef = tf.GraphDef\n    tf_import_meta_graph = tf.train.import_meta_graph\n    tf_gfile = tf.gfile\n    tf_placeholder = tf.placeholder\n    extract_sub_graph = tf.graph_util.extract_sub_graph\n\n\ndef inputs_without_resource(sess, input_names):\n    try:\n        new_input_names = []\n        for n in input_names:\n            t = sess.graph.get_tensor_by_name(n)\n            if t.dtype != tf.dtypes.resource:\n                new_input_names.append(n)\n        input_names = new_input_names\n    except:  # pylint: disable=bare-except\n        pass\n    return input_names\n\n\ndef from_function(func, input_names, output_names):\n    frozen_func = convert_variables_to_constants_v2(func, lower_control_flow=False)\n    graph_def = frozen_func.graph.as_graph_def(add_shapes=True)\n    # output_names = [i.name for i in frozen_func.outputs]\n    tf_reset_default_graph()\n    with tf_session() as sess:\n        tf.import_graph_def(graph_def, name=\'\')\n        input_names = inputs_without_resource(sess, input_names)\n        graph_def = tf_optimize(input_names, output_names, graph_def)\n    return graph_def\n\n\ndef freeze_session(sess, input_names=None, output_names=None):\n    """"""Freezes the state of a session into a pruned computation graph.""""""\n    output_node_names = [i.split(\':\')[:-1][0] for i in output_names]\n    keep_var_names = [i.split(\':\')[:-1][0] for i in input_names]\n    with sess.graph.as_default():\n        output_node_names = output_node_names or []\n        output_node_names += [v.op.name for v in tf_global_variables()]\n        output_node_names += keep_var_names\n        graph_def = sess.graph.as_graph_def(add_shapes=True)\n        for node in graph_def.node:\n            node.device = """"\n        graph_def = convert_variables_to_constants(sess, graph_def, output_node_names)\n    return graph_def\n\n\ndef remove_redundant_inputs(frozen_graph, input_names):\n    """"""Remove redundant inputs not in frozen graph.""""""\n    frozen_inputs = []\n    # get inputs in frozen graph\n    for n in frozen_graph.node:\n        for inp in input_names:\n            if utils.node_name(inp) == n.name:\n                frozen_inputs.append(inp)\n    deleted_inputs = list(set(input_names) - set(frozen_inputs))\n    if deleted_inputs:\n        logger.warning(""inputs [%s] is not in frozen graph, delete them"", "","".join(deleted_inputs))\n    return frozen_inputs\n\n\ndef from_graphdef(model_path, input_names, output_names):\n    """"""Load tensorflow graph from graphdef.""""""\n    # make sure we start with clean default graph\n    tf_reset_default_graph()\n    with tf_session() as sess:\n        graph_def = tf_graphdef()\n        with tf_gfile.GFile(model_path, \'rb\') as f:\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def, name=\'\')\n        input_names = inputs_without_resource(sess, input_names)\n        frozen_graph = freeze_session(sess, input_names=input_names, output_names=output_names)\n        input_names = remove_redundant_inputs(frozen_graph, input_names)\n\n    tf_reset_default_graph()\n    with tf_session() as sess:\n        input_names = inputs_without_resource(sess, input_names)\n        frozen_graph = tf_optimize(input_names, output_names, frozen_graph)\n    tf_reset_default_graph()\n    return frozen_graph, input_names, output_names\n\n\ndef from_checkpoint(model_path, input_names, output_names):\n    """"""Load tensorflow graph from checkpoint.""""""\n    # make sure we start with clean default graph\n    tf_reset_default_graph()\n    # model_path = checkpoint/checkpoint.meta\n    with tf_session() as sess:\n        saver = tf_import_meta_graph(model_path, clear_devices=True)\n        # restore from model_path minus the "".meta""\n        saver.restore(sess, model_path[:-5])\n        input_names = inputs_without_resource(sess, input_names)\n        frozen_graph = freeze_session(sess, input_names=input_names, output_names=output_names)\n        input_names = remove_redundant_inputs(frozen_graph, input_names)\n\n    tf_reset_default_graph()\n    with tf_session() as sess:\n        frozen_graph = tf_optimize(input_names, output_names, frozen_graph)\n    tf_reset_default_graph()\n    return frozen_graph, input_names, output_names\n\n\ndef _from_saved_model_v1(sess, model_path, input_names, output_names, signatures):\n    """"""Load tensorflow graph from saved_model.""""""\n\n    imported = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], model_path)\n    for k in imported.signature_def.keys():\n        if k.startswith(""_""):\n            # consider signatures starting with \'_\' private\n            continue\n        signatures.append(k)\n    try:\n        from tensorflow.contrib.saved_model.python.saved_model import signature_def_utils\n        # pylint: disable=unnecessary-lambda\n        get_signature_def = lambda meta_graph_def, k: \\\n            signature_def_utils.get_signature_def_by_key(meta_graph_def, k)\n    except ImportError:\n        # TF1.12 changed the api\n        get_signature_def = lambda meta_graph_def, k: meta_graph_def.signature_def[k]\n\n    input_names = []\n    output_names = []\n    for k in signatures:\n        inputs_tensor_info = get_signature_def(imported, k).inputs\n        for _, input_tensor in inputs_tensor_info.items():\n            input_names.append(input_tensor.name)\n        outputs_tensor_info = get_signature_def(imported, k).outputs\n        for _, output_tensor in outputs_tensor_info.items():\n            output_names.append(output_tensor.name)\n    frozen_graph = freeze_session(sess, input_names=input_names, output_names=output_names)\n    return frozen_graph, input_names, output_names\n\n\ndef _from_saved_model_v2(model_path, input_names, output_names, signatures):\n    """"""Load tensorflow graph from saved_model.""""""\n    imported = tf.saved_model.load(model_path)  # pylint: disable=no-value-for-parameter\n\n    # f = meta_graph_def.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n    for k in imported.signatures.keys():\n        if k.startswith(""_""):\n            # consider signatures starting with \'_\' private\n            continue\n        signatures.append(k)\n    for k in signatures:\n        concrete_func = imported.signatures[k]\n        input_names = [input_tensor.name for input_tensor in concrete_func.inputs\n                       if input_tensor.dtype != tf.dtypes.resource]\n        output_names = [output_tensor.name for output_tensor in concrete_func.outputs\n                        if output_tensor.dtype != tf.dtypes.resource]\n\n    frozen_graph = from_function(concrete_func, input_names, output_names)\n    return frozen_graph, input_names, output_names\n\n\ndef from_saved_model(model_path, input_names, output_names, signatures=None):\n    """"""Load tensorflow graph from saved_model.""""""\n    if signatures is None:\n        signatures = []\n    tf_reset_default_graph()\n    if is_tf2():\n        frozen_graph, input_names, output_names = \\\n            _from_saved_model_v2(model_path, input_names, output_names, signatures)\n    else:\n        with tf_session() as sess:\n            frozen_graph, input_names, output_names = \\\n                _from_saved_model_v1(sess, model_path, input_names, output_names, signatures)\n\n    if len(signatures) > 1:\n        logger.warning(""found multiple signatures %s in saved_model, pass --signature_def in command line"",\n                       signatures)\n\n    tf_reset_default_graph()\n    return frozen_graph, input_names, output_names\n\n\ndef from_keras(model_path, input_names, output_names):\n    """"""Load keras model - experimental for now.""""""\n    from tensorflow.python import keras as _keras\n    from tensorflow.python.eager import context\n    from tensorflow.python.keras.saving import saving_utils as _saving_utils\n\n    # Handles Keras when Eager mode is enabled.\n    custom_objects = None\n    if context.executing_eagerly():\n        _keras.backend.clear_session()\n        _keras.backend.set_learning_phase(False)\n        keras_model = _keras.models.load_model(model_path, custom_objects)\n\n        function = _saving_utils.trace_model_call(keras_model)\n        concrete_func = function.get_concrete_function()\n        # allow to pass inputs and outputs from caller if we don\'t want all of them\n        input_names = [input_tensor.name for input_tensor in concrete_func.inputs\n                       if input_tensor.dtype != tf.dtypes.resource]\n        output_names = [output_tensor.name for output_tensor in concrete_func.outputs\n                        if output_tensor.dtype != tf.dtypes.resource]\n\n        frozen_graph = from_function(concrete_func, input_names, output_names)\n    else:\n        # Handles Keras when Eager mode is disabled.\n        _keras.backend.clear_session()\n        _keras.backend.set_learning_phase(False)\n        keras_model = _keras.models.load_model(model_path, custom_objects)\n        # allow to pass inputs and outputs from caller if we don\'t want all of them\n        input_names = keras_model.inputs\n        output_names = keras_model.outputs\n        sess = _keras.backend.get_session()\n        input_names = inputs_without_resource(sess, input_names)\n        frozen_graph = freeze_session(sess, input_names=input_names, output_names=output_names)\n        tf_reset_default_graph()\n        with tf_session() as sess:\n            frozen_graph = tf_optimize(input_names, output_names, frozen_graph)\n        tf_reset_default_graph()\n    return frozen_graph, input_names, output_names\n\n\ndef tf_optimize_grappler(input_names, output_names, graph_def, fold_constant=None):\n    from tensorflow.core.protobuf import meta_graph_pb2 as meta_graph_pb2, config_pb2, rewriter_config_pb2\n    from tensorflow.python.grappler import tf_optimizer as tf_opt\n\n    config = config_pb2.ConfigProto()\n    rewrite_options = config.graph_options.rewrite_options\n    config.graph_options.infer_shapes = True\n    # TODO: if we turn on pruning, grappler removes some identities that the tf-1.x lstm rewriter\n    #   depends on so for now don\'t turn this on.\n    rewrite_options.optimizers[:] = [\n        # \'pruning\', \'constfold\', \'arithmetic\', \'dependency\', \'function\',\n        \'constfold\', \'function\'\n    ]\n    meta_graph = tf.compat.v1.train.export_meta_graph(graph_def=graph_def)\n    fetch_collection = meta_graph_pb2.CollectionDef()\n    for t in input_names + output_names:\n        fetch_collection.node_list.value.append(t)\n    meta_graph.collection_def[""train_op""].CopyFrom(fetch_collection)\n    graph_def = tf_opt.OptimizeGraph(config, meta_graph)\n    return graph_def\n\n\ndef tf_optimize(input_names, output_names, graph_def, fold_constant=True):\n    """"""Extract inference subgraph and optimize graph.""""""\n    assert isinstance(input_names, list)\n    assert isinstance(output_names, list)\n\n    # TODO: is this needed ?\n    needed_names = [utils.node_name(i) for i in input_names] + \\\n                   [utils.node_name(i) for i in output_names]\n    graph_def = extract_sub_graph(graph_def, needed_names)\n\n    if fold_constant:\n        want_grappler = is_tf2() or LooseVersion(tf.__version__) >= ""1.15""\n        if want_grappler:\n            graph_def = tf_optimize_grappler(input_names, output_names, graph_def, fold_constant)\n        else:\n            # the older transform path\n            from tensorflow.tools.graph_transforms import TransformGraph  # pylint: disable=redefined-outer-name\n            transforms = []\n            if fold_constant:\n                transforms.extend([\n                    ""fold_constants(ignore_errors=true)"",\n                    ""remove_attribute(attribute_name=_class)"",  # remove node colocation attributes\n                ])\n            transforms.extend([\n                ""fold_batch_norms"",\n                ""fold_old_batch_norms"",\n            ])\n            graph_def = TransformGraph(graph_def, input_names, output_names, transforms)\n\n    return graph_def\n\n\ndef tf_reload_graph(tf_graph):\n    """"""Invoke tensorflow cpp shape inference by reloading graph_def.""""""\n    # invoke c api if tf version is below 1.8\n    if get_tf_version() < LooseVersion(""1.8""):\n        logger.debug(\n            ""On TF < 1.8, graph is constructed by python API, "" \\\n            ""which doesn\'t invoke shape inference, please set "" \\\n            ""TF_C_API_GRAPH_CONSTRUCTION=1 to enable it""\n        )\n\n    graph_def = tf_graph.as_graph_def(add_shapes=True)\n    with tf.Graph().as_default() as inferred_graph:\n        tf.import_graph_def(graph_def, name="""")\n    return inferred_graph\n\n\ndef is_function(g):\n    if is_tf2():\n        return \'tensorflow.python.framework.func_graph.FuncGraph\' in str(type(g))\n    return False\n\n_FUNCTIONS = {}\n\n\ndef resolve_functions(tf_graph):\n    def toposort(data):\n        while True:\n            ordered = set(item for item, dep in data.items() if not dep)\n            if not ordered:\n                break\n            yield ordered\n            data = {item: (dep - ordered) for item, dep in data.items() if item not in ordered}\n\n    _, _, _, _, _, functions = tflist_to_onnx(tf_graph, {})\n    data = {}\n    for k, fdef in tf_graph._functions.items():  # pylint: disable=protected-access\n        input_shapes = functions.get(k)\n        fdef = fdef.definition\n        if input_shapes and len(fdef.signature.input_arg) < len(input_shapes):\n            input_shapes = input_shapes[:len(fdef.signature.input_arg)]\n        try:\n            func = function_def_to_graph(fdef, input_shapes=input_shapes)\n        except:  # pylint: disable=bare-except\n            # if there is a missmatch between caller and function use the functions shape\n            logger.warning(""shape missmatch between caller and function: %s"", k)\n            func = function_def_to_graph(fdef)\n        _FUNCTIONS[k] = func\n        _, _, _, _, _, tfunctions = tflist_to_onnx(func, {})\n        functions.update(tfunctions)\n        data[k] = set(tfunctions.keys())\n\n    result = []\n    for d in toposort(data):\n        result.extend(list(d))\n    return [_FUNCTIONS[k] for k in result]\n\n\ndef set_function(name, func):\n    _FUNCTIONS[name] = func\n\n\ndef find_function(name):\n    return _FUNCTIONS.get(name)\n'"
tf2onnx/tf_utils.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.tf_utils - misc utilities for tf2onnx that interface with tensorflow\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport collections\nfrom distutils.version import LooseVersion\n\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom tensorflow.core.framework import types_pb2, tensor_pb2\nfrom tensorflow.python.framework import tensor_util\n\nfrom onnx import helper, onnx_pb, numpy_helper\n\nfrom tf2onnx.utils import make_sure, is_tf_const_op, port_name\nfrom . import logging\n\nlogger = logging.getLogger(__name__)\n\n#\n#  mapping dtypes from tensorflow to onnx\n#\nTF_TO_ONNX_DTYPE = {\n    types_pb2.DT_FLOAT: onnx_pb.TensorProto.FLOAT,\n    types_pb2.DT_HALF: onnx_pb.TensorProto.FLOAT16,\n    types_pb2.DT_BFLOAT16: onnx_pb.TensorProto.FLOAT16,\n    types_pb2.DT_DOUBLE: onnx_pb.TensorProto.DOUBLE,\n    types_pb2.DT_INT32: onnx_pb.TensorProto.INT32,\n    types_pb2.DT_INT16: onnx_pb.TensorProto.INT16,\n    types_pb2.DT_INT8: onnx_pb.TensorProto.INT8,\n    types_pb2.DT_UINT8: onnx_pb.TensorProto.UINT8,\n    types_pb2.DT_UINT16: onnx_pb.TensorProto.UINT16,\n    types_pb2.DT_INT64: onnx_pb.TensorProto.INT64,\n    types_pb2.DT_STRING: onnx_pb.TensorProto.STRING,\n    types_pb2.DT_COMPLEX64: onnx_pb.TensorProto.COMPLEX64,\n    types_pb2.DT_COMPLEX128: onnx_pb.TensorProto.COMPLEX128,\n    types_pb2.DT_BOOL: onnx_pb.TensorProto.BOOL,\n    types_pb2.DT_RESOURCE: onnx_pb.TensorProto.INT64,  # TODO: hack to allow processing on control flow\n    types_pb2.DT_VARIANT: onnx_pb.TensorProto.UNDEFINED,\n    types_pb2.DT_QUINT8: onnx_pb.TensorProto.UINT8,\n}\n\n\ndef tf_to_onnx_tensor(tensor, name=""""):\n    """"""Convert tensorflow tensor to onnx tensor.""""""\n    np_data = get_tf_tensor_data(tensor)\n    if np_data.dtype == np.object:\n        # assume np_data is string, numpy_helper.from_array accepts ndarray,\n        # in which each item is of str while the whole dtype is of object.\n        try:\n            if len(np_data.shape) > 0:\n                np_data = np_data.astype(np.str).astype(np.object)\n            else:\n                np_data = np.array(str(np_data)).astype(np.object)\n        except:  # pylint: disable=bare-except\n            raise RuntimeError(""Not support type: {}"".format(type(np_data.flat[0])))\n    return numpy_helper.from_array(np_data, name=name)\n\n\ndef get_tf_tensor_data(tensor):\n    """"""Get data from tensor.""""""\n    make_sure(isinstance(tensor, tensor_pb2.TensorProto), ""Require TensorProto"")\n    np_data = tensor_util.MakeNdarray(tensor)\n    make_sure(isinstance(np_data, np.ndarray), ""{} isn\'t ndarray"".format(np_data))\n    return np_data\n\n\ndef get_tf_const_value(op, as_list=True):\n    """"""\n    If as_list=True, return the array as a (possibly nested) list.\n    Otherwise, return data of type np.ndarray.\n\n    If a tensor is a scalar having value 1,\n        when as_list=False, return np.array(1), type is <class \'numpy.ndarray\'>\n        when as_list=True, return 1, type is <class \'int\'>.\n    """"""\n    make_sure(is_tf_const_op(op), ""{} isn\'t a const op"".format(op.name))\n    value = get_tf_tensor_data(op.get_attr(""value""))\n    if as_list:\n        value = value.tolist()\n    return value\n\n\ndef get_tf_shape_attr(node):\n    """"""Get shape from tensorflow attr ""shape"".""""""\n    dims = None\n    try:\n        shape = get_tf_node_attr(node, ""shape"")\n        if not shape.unknown_rank:\n            dims = [int(d.size) for d in shape.dim]\n    except:  # pylint: disable=bare-except\n        pass\n    return dims\n\n\ndef get_tf_tensor_shape(tensor):\n    shape = []\n    try:\n        shape = tensor.get_shape().as_list()\n    except Exception:  # pylint: disable=broad-except\n        shape = None\n    return shape\n\n\ndef map_tf_dtype(dtype):\n    if dtype:\n        dtype = TF_TO_ONNX_DTYPE[dtype]\n    return dtype\n\n\ndef get_tf_node_attr(node, name):\n    """"""Parser TF node attribute.""""""\n    if six.PY2:\n        # For python2, TF get_attr does not accept unicode\n        name = str(name)\n    return node.get_attr(name)\n\n\ndef get_tf_version():\n    return LooseVersion(tf.__version__)\n\n\ndef tflist_to_onnx(g, shape_override):\n    """"""\n    Convert the tf-node list into an onnx graph with minimal rewrites so\n    we can use the onnx graph as intermediate graph.\n    """"""\n\n    # ignore the following attributes\n    ignored_attr = [""unknown_rank"", ""_class"", ""Tshape"", ""use_cudnn_on_gpu"", ""Index"", ""Tpaddings"",\n                    ""TI"", ""Tparams"", ""Tindices"", ""Tlen"", ""Tdim"", ""Tin"", ""dynamic_size"", ""Tmultiples"",\n                    ""Tblock_shape"", ""Tcrops"", ""index_type"", ""Taxis"", ""U"", ""maxval"",\n                    ""Tout"", ""Tlabels"", ""Tindex"", ""element_shape"", ""Targmax"", ""Tperm"", ""Tcond"",\n                    ""T_threshold"", ""element_dtype"", ""shape_type"", ""_lower_using_switch_merge"",\n                    ""parallel_iterations"", ""_num_original_outputs"", ""output_types"", ""output_shapes"",\n                    ""key_dtype"", ""value_dtype"", ""Tin"", ""Tout"", ""capacity"", ""component_types"", ""shapes""]\n\n    node_list = g.get_operations()\n    functions = {}\n\n    # some stats\n    op_cnt = collections.Counter()\n    attr_cnt = collections.Counter()\n    onnx_nodes = []\n    output_shapes = {}\n    dtypes = {}\n\n    # find outputs\n    ops = node_list\n\n    # create dict with output to shape mappings\n    for node in ops:\n        for out in node.outputs:\n            shape = shape_override.get(out.name)\n            if shape is None:\n                shape = get_tf_tensor_shape(out)\n            dtypes[out.name] = map_tf_dtype(out.dtype)\n            output_shapes[out.name] = shape\n\n    # minimal conversion of attributes\n    for node in ops:\n        attr = {}\n        takeit = True\n        op_cnt[node.type] += 1\n        for a in node.node_def.attr:\n            attr_cnt[a] += 1\n            if a == ""dtype"":\n                attr[a] = map_tf_dtype(get_tf_node_attr(node, ""dtype""))\n            elif a in [""T""]:\n                dtype = get_tf_node_attr(node, a)\n                if dtype:\n                    if not isinstance(dtype, list):\n                        dtypes[node.name] = map_tf_dtype(dtype)\n            elif a in [""output_type"", ""output_dtype"", ""out_type"", ""Tidx"", ""out_idx""]:\n                # Tidx is used by Range\n                # out_idx is used by ListDiff\n                attr[a] = map_tf_dtype(get_tf_node_attr(node, a))\n            elif a == ""shape"":\n                shape = get_tf_shape_attr(node)\n                if shape is not None:\n                    attr[a] = shape\n            elif a == ""output_shapes"":\n                # we should not need it since we pull the shapes above already\n                pass\n            elif a in [""body"", ""cond"", ""then_branch"", ""else_branch""]:\n                input_shapes = [inp.get_shape() for inp in node.inputs]\n                nattr = get_tf_node_attr(node, a)\n                attr[a] = nattr.name\n                functions[nattr.name] = input_shapes\n            elif a == ""value"":\n                onnx_tensor = tf_to_onnx_tensor(get_tf_node_attr(node, a), name=port_name(node.name))\n                attr[a] = onnx_tensor\n            elif a == ""DstT"":\n                attr[""to""] = map_tf_dtype(get_tf_node_attr(node, ""DstT""))\n            elif a == ""SrcT"":\n                continue\n            elif a in ignored_attr:\n                continue\n            else:\n                attr[a] = get_tf_node_attr(node, a)\n\n        if takeit:\n            try:\n                input_names = [i.name for i in node.inputs]\n                output_names = [i.name for i in node.outputs]\n                onnx_node = helper.make_node(node.type, input_names, output_names, name=node.name, **attr)\n                onnx_nodes.append(onnx_node)\n            except Exception as ex:\n                logger.error(""pass1 convert failed for %s, ex=%s"", node, ex)\n                raise\n\n    return onnx_nodes, op_cnt, attr_cnt, output_shapes, dtypes, functions\n\n\ndef tensorflow_to_onnx(graph, shape_override):\n    """"""\n    Load tensorflow graph and do a conversion.\n    """"""\n    return tflist_to_onnx(graph, shape_override)\n'"
tf2onnx/tfonnx.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.tf2onnx - rewrite tensorflow graph to onnx graph\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport collections\nimport sys\nimport traceback\n\nimport numpy as np\nfrom onnx import onnx_pb\n\nimport tf2onnx\nimport tf2onnx.onnx_opset  # pylint: disable=unused-import\nimport tf2onnx.custom_opsets  # pylint: disable=unused-import\nfrom tf2onnx.graph import Graph\nfrom tf2onnx.rewriter import *  # pylint: disable=wildcard-import\nfrom tf2onnx.shape_inference import infer_shape\nfrom tf2onnx.tf_loader import is_function, resolve_functions, set_function\nfrom tf2onnx.tf_utils import tensorflow_to_onnx, get_tf_version\n\nfrom . import constants, logging, schemas, utils, handler\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=useless-return,broad-except,logging-not-lazy,unused-argument,missing-docstring\n# pylint: disable=unused-variable\n\n\ndef rewrite_constant_fold(g, ops):\n    """"""\n    We call tensorflow transform with constant folding but in some cases tensorflow does\n    fold all constants. Since there are a bunch of ops in onnx that use attributes where\n    tensorflow has dynamic inputs, we badly want constant folding to work. For cases where\n    tensorflow missed something, make another pass over the graph and fix want we care about.\n    """"""\n    func_map = {\n        ""Add"": np.add,\n        ""GreaterEqual"": np.greater_equal,\n        ""Cast"": np.cast,\n        ""ConcatV2"": np.concatenate,\n        ""Less"": np.less,\n        ""ListDiff"": np.setdiff1d,\n        ""Mul"": np.multiply,\n        ""Pack"": np.stack,\n        ""Range"": np.arange,\n        ""Sqrt"": np.sqrt,\n        ""Sub"": np.subtract,\n    }\n\n    # pylint: disable=too-many-nested-blocks\n    keep_looking = True\n    while keep_looking:\n        keep_looking = False\n        for idx, op in enumerate(ops):\n            func = func_map.get(op.type)\n            if func is None: continue\n            if set(op.output) & set(g.outputs): continue\n            try:\n                inputs = []\n                for node in op.inputs:\n                    if not node.is_const():\n                        break\n                    inputs.append(node.get_tensor_value(as_list=False))\n\n                logger.debug(""op name %s, %s, %s"", op.name, len(op.input), len(inputs))\n                if inputs and len(op.input) == len(inputs):\n                    logger.info(""folding node type=%s, name=%s"" % (op.type, op.name))\n                    if op.type == ""Cast"":\n                        dst = op.get_attr_int(""to"")\n                        np_type = tf2onnx.utils.map_onnx_to_numpy_type(dst)\n                        val = np.cast[np_type](*inputs)\n                    elif op.type == ""ConcatV2"":\n                        axis = inputs[-1]\n                        values = inputs[:-1]\n                        val = func(tuple(values), axis)\n                    elif op.type == ""ListDiff"":\n                        out_type = op.get_attr_int(""out_idx"")\n                        np_type = tf2onnx.utils.map_onnx_to_numpy_type(out_type)\n                        val = func(*inputs)\n                        val = val.astype(np_type)\n                    elif op.type in [""Pack""]:\n                        # handle ops that need input array and axis\n                        axis = op.get_attr_int(""axis"")\n                        val = func(inputs, axis=axis)\n                    elif op.type == ""Range"":\n                        dtype = op.get_attr_int(""Tidx"")\n                        np_type = tf2onnx.utils.map_onnx_to_numpy_type(dtype)\n                        val = func(*inputs, dtype=np_type)\n                    else:\n                        val = func(*inputs)\n\n                    new_node_name = utils.make_name(op.name)\n                    new_output_name = new_node_name\n                    old_output_name = op.output[0]\n                    old_node_name = op.name\n                    logger.debug(""create const node [%s] replacing [%s]"", new_node_name, old_node_name)\n                    ops[idx] = g.make_const(new_node_name, val)\n\n                    logger.debug(""replace old output [%s] with new output [%s]"", old_output_name, new_output_name)\n                    # need to re-write the consumers input name to use the const name\n                    consumers = g.find_output_consumers(old_output_name)\n                    if consumers:\n                        for consumer in consumers:\n                            g.replace_input(consumer, old_output_name, new_output_name)\n\n                    # keep looking until there is nothing we can fold.\n                    # We keep the graph in topological order so if we folded,\n                    # the result might help a following op.\n                    keep_looking = True\n            except Exception as ex:\n                tb = traceback.format_exc()  # pylint: disable=bare-except\n                logger.info(""exception: %s, details: %s"", ex, tb)\n                # ignore errors\n\n        # pylint: enable=too-many-nested-blocks\n    return ops\n\n\ndef rewrite_incomplete_type_support(g, ops, impacted_ops):\n    """"""\n    for ops that have inclomplete type support, insert casts.\n    This is needed for some tensor ops in opset7 and for some ops in winml-rs5.\n    It is not helping performance but better than the model not working at all.\n    """"""\n    ignored_input_index = {\n        ""Tile"": [1],  # Tile\'s second input can only be int64\n        ""Where"": [0],  # Where\'s first input is bool\n    }\n    new_ops = []\n    org_ops = list(ops)\n    for op in org_ops:\n        if op.type in impacted_ops:\n            cast_inserted = []\n            output_dtype = None\n            ignored_inputs = ignored_input_index.get(op.type)\n            # insert casts on inputs if the runtime only supports float\n            for i, input_node in enumerate(op.inputs):\n                if ignored_inputs and i in ignored_inputs:\n                    continue\n\n                input_name = op.input[i]\n                dtype = g.get_dtype(input_name)\n                if dtype is None:\n                    logger.warning(""adding Cast for op %s (type is %s)\' input: %s, dtype should not be None"",\n                                   op.name, op.type, input_name)\n\n                if dtype != onnx_pb.TensorProto.FLOAT:\n                    output_dtype = dtype\n                    logger.debug(""insert cast for node %s on input %s"", op.name, input_name)\n                    if input_node and input_node.type == ""Cast"" \\\n                            and len(g.find_output_consumers(input_node.output[0])) == 1:\n                        input_node.set_attr(""to"", onnx_pb.TensorProto.FLOAT)\n                        g.set_dtype(input_name, onnx_pb.TensorProto.FLOAT)\n                    else:\n                        cast_node = g.insert_new_node_on_input(op, ""Cast"", input_name)\n                        cast_node.set_attr(""to"", onnx_pb.TensorProto.FLOAT)\n                        g.set_dtype(cast_node.output[0], onnx_pb.TensorProto.FLOAT)\n                        g.copy_shape(input_name, cast_node.output[0])\n                        cast_inserted.append(cast_node)\n            if output_dtype:\n                # insert reverse cast if needed\n                for output_name in op.output:\n                    name = utils.make_name(op.name)\n                    logger.debug(""insert cast back for node %s on output %s [dtype=%s]"", op.name, output_name,\n                                 output_dtype)\n                    output_cast = g.insert_new_node_on_output(""Cast"", output_name, name=name)\n                    output_cast.set_attr(""to"", output_dtype)\n                    g.set_dtype(output_cast.output[0], output_dtype)\n                    g.copy_shape(output_name, output_cast.output[0])\n                    cast_inserted.append(output_cast)\n\n            if cast_inserted:\n                new_ops.extend(cast_inserted)\n        new_ops.append(op)\n    return new_ops\n\n\ndef rewrite_incomplete_type_support_rs5(g, ops):\n    return rewrite_incomplete_type_support(g, ops, [""Unsqueeze"", ""Mul"", ""Concat"", ""Slice"", ""Transpose""])\n\n\ndef rewrite_incomplete_type_support_rs6(g, ops):\n    impacted_ops = [\n        ""Div"",\n        ""IsNaN"",\n        ""Max"",\n        ""Min"",\n        ""ReduceSum"",\n        ""Slice"",\n        ""Split"",\n        ""Tile"",\n        ""Transpose"",\n        ""Where""\n    ]\n    # TODO: logic to insert cast has bug, not all inputs of one node need cast\n    # for example, slice\'s input ""starts"" doesn\'t need it.\n    if g.opset == 10:\n        impacted_ops.remove(""Slice"")\n\n    return rewrite_incomplete_type_support(g, ops, impacted_ops)\n\n\ndef tensorflow_onnx_mapping(g, ops_mapping):\n    logger.verbose(""Mapping TF node to ONNX node(s)"")\n    mapped_op = collections.Counter()\n    unmapped_op = collections.Counter()\n    exceptions = []\n\n    ops = list(g.get_nodes())\n    for node in ops:\n        logger.debug(""Process node: %s\\n%s"", node.name, node.summary)\n\n        if node.need_skip():\n            logger.debug(""explicitly skip node "" + node.name)\n            continue\n\n        op = node.type\n        map_info = ops_mapping.get(op)\n        if map_info is None:\n            unmapped_op[op] += 1\n            logger.error(""Tensorflow op [%s: %s] is not supported"", node.name, op)\n            continue\n        mapped_op[op] += 1\n\n        func, kwargs = map_info\n        if kwargs:\n            # if there is a onnx_op key we\'ll map the old type to a new type\n            onnx_op = kwargs.get(""onnx_op"")\n            if onnx_op:\n                node.type = onnx_op\n        body_graphs = node.get_body_graphs()\n        if body_graphs:\n            for attr, b_g in body_graphs.items():\n                logger.debug(""start handling subgraph of %s\'s attribute %s"", node.name, attr)\n                b_g.topological_sort(b_g.get_nodes())\n                # we assume only ONNX nodes have subgraph defined in pre-rewriters.\n                # that means, if we create node having subgraphs in this step, the\n                # created subgraphs\' nodes won\'t be mapped.\n                m_ops, unm_ops, body_exceptions = tensorflow_onnx_mapping(b_g, ops_mapping)\n                mapped_op += m_ops\n                unmapped_op += unm_ops\n                # topological_sort on the body in case processing has changed the order\n                b_g.topological_sort(b_g.get_nodes())\n                exceptions.extend(body_exceptions)\n                logger.debug(""finish handling subgraph of %s\'s attribute %s"", node.name, attr)\n\n        try:\n            func(g, node, **kwargs)\n            node.skip_conversion = True\n        except Exception as ex:\n            logger.error(""Failed to convert node %s\\n%s"", node.name, node.summary, exc_info=1)\n            exceptions.append(ex)\n\n    return mapped_op, unmapped_op, exceptions\n\n\ndef transpose_inputs(ctx, inputs_as_nchw):\n    """"""Insert a transpose from NHWC to NCHW on model input on users request.""""""\n    ops = []\n    for node in ctx.get_nodes():\n        for idx, output_name in enumerate(node.output):\n            if output_name in inputs_as_nchw:\n                shape = ctx.get_shape(output_name)\n                if len(shape) != len(constants.NCHW_TO_NHWC):\n                    logger.warning(""transpose_input for %s: shape must be rank 4, ignored"" % output_name)\n                    ops.append(node)\n                    continue\n                # insert transpose\n                op_name = utils.make_name(node.name)\n                transpose = ctx.insert_new_node_on_output(""Transpose"", output_name, name=op_name)\n                transpose.set_attr(""perm"", constants.NCHW_TO_NHWC)\n                ctx.copy_shape(output_name, transpose.output[0])\n                ctx.set_shape(output_name, np.array(shape)[constants.NHWC_TO_NCHW])\n                ops.append(transpose)\n                ops.append(node)\n                continue\n        ops.append(node)\n    ctx.reset_nodes(ops)\n\n\ndef topological_sort(g, continue_on_error):\n    ops = g.get_nodes()\n    if not continue_on_error:\n        g.topological_sort(ops)\n    else:\n        try:\n            g.topological_sort(ops)\n        except:  # pylint: disable=bare-except\n            # if we continue on error, ignore graph cycles so we can report all missing ops\n            pass\n\n\ndef run_rewriters(g, funcs, continue_on_error):\n    """"""Rewrite the original graph and body graphs of nodes""""""\n    # NOTE(wayuanho):\n    # 1. we don\'t sort graph here, rewriter is expected to do it on its own.\n    # 2. the graph here may have circles, current topological_sort cannot handle it.\n    for func in funcs:\n        try:\n            ops = func(g, g.get_nodes())\n            g.reset_nodes(ops)\n        except Exception as ex:\n            type_, value_, traceback_ = sys.exc_info()\n            logger.error(""rewriter %s: exception %s"", func, ex)\n            ex_ext = traceback.format_exception(type_, value_, traceback_)\n            if continue_on_error:\n                logger.info(ex_ext)\n            else:\n                raise ex\n\n        if utils.is_debug_mode():\n            broken_outputs = g.check_integrity()\n            if broken_outputs:\n                logging.error(\n                    ""After rewriter %s, graph breaks at outputs %s"",\n                    func.__name__, broken_outputs\n                )\n\n    if g.contained_graphs:\n        for dict_val in g.contained_graphs.values():\n            for attr_name, b_g in dict_val.items():\n                run_rewriters(b_g, funcs, attr_name)\n\n\ndef process_tf_graph(tf_graph, continue_on_error=False, verbose=False, target=None,\n                     opset=None, custom_op_handlers=None, custom_rewriter=None,\n                     extra_opset=None, shape_override=None, inputs_as_nchw=None,\n                     input_names=None, output_names=None, is_subgraph=False):\n    """"""Convert tensorflow graph to onnx graph.\n        Args:\n            tf_graph: tensorflow graph\n            continue_on_error: if an op can\'t be processed (aka there is no mapping), continue\n            verbose: print summary stats (deprecated)\n            target: list of workarounds applied to help certain platforms\n            opset: the opset to be used (int, default is latest)\n            custom_op_handlers: dictionary of custom ops handlers\n            custom_rewriter: list of custom graph rewriters\n            extra_opset: list of extra opset\'s, for example the opset\'s used by custom ops\n            shape_override: dict with inputs that override the shapes given by tensorflow\n            inputs_as_nchw: transpose inputs in list from nchw to nchw\n            input_names: list of input node names in graph, input name format as node_name:port_id\n            output_names: list of output node names in graph, output name format as node_name:port_id\n        Return:\n            onnx graph\n    """"""\n    if verbose:\n        logger.warning(""Argument verbose for process_tf_graph is deprecated. Please use --verbose option instead."")\n    del verbose\n\n    opset = utils.find_opset(opset)\n    if not is_subgraph:\n        logger.info(""Using tensorflow=%s, onnx=%s, tf2onnx=%s/%s"",\n                    get_tf_version(), utils.get_onnx_version(), tf2onnx.__version__, tf2onnx.version.git_version[:6])\n        logger.info(""Using opset <onnx, %s>"", opset)\n        if opset > schemas.get_max_supported_opset_version():\n            logger.warning(""Currently installed onnx package %s is too low to support opset %s, ""\n                           ""please upgrade onnx package to avoid potential conversion issue."",\n                           utils.get_onnx_version(), opset)\n\n    is_func = is_function(tf_graph)\n    if not is_func:\n        tf_graph = infer_shape(tf_graph, shape_override)\n\n    if shape_override is None:\n        shape_override = {}\n    if inputs_as_nchw is None:\n        inputs_as_nchw = []\n    if target is None:\n        target = constants.DEFAULT_TARGET\n\n    onnx_nodes, op_cnt, attr_cnt, output_shapes, dtypes, _ = tensorflow_to_onnx(tf_graph, shape_override)\n    if not is_subgraph:\n        # make tf2onnx internal subgraphs from the tensorflow subgraphs\n        ordered_func = resolve_functions(tf_graph)\n        for func in ordered_func:\n            f_inputs_names = [t.name for t in func.inputs]\n            f_output_names = [t.name for t in func.outputs]\n            fg = process_tf_graph(func, continue_on_error, False, target, opset,\n                                  custom_op_handlers, custom_rewriter,\n                                  extra_opset, shape_override, inputs_as_nchw,\n                                  f_inputs_names, f_output_names, is_subgraph=True)\n            fg.graph_name = func.name\n            fg.func_inputs = f_inputs_names\n            set_function(func.name, fg)\n\n    io_to_check = []\n    if input_names:\n        io_to_check.extend(input_names)\n    if output_names:\n        io_to_check.extend(output_names)\n\n    if io_to_check:\n        # check output existence in case user passed in wrong output ids\n        non_exists = set(io_to_check) - set(output_shapes.keys())\n        if non_exists:\n            logger.error(""\\nFailed to convert: inputs/outputs specified do not exist, make sure your passed""\n                         ""in format: input/output_node_name:port_id. Problematical inputs/outputs are: %s \\n"",\n                         non_exists)\n            raise ValueError(""Inputs/Outputs Not Found"")\n\n    g = Graph(onnx_nodes, output_shapes, dtypes, target, opset, extra_opset, output_names, is_subgraph=is_subgraph)\n\n    # create ops mapping for the desired opsets\n    ops_mapping = handler.tf_op.create_mapping(g.opset, g.extra_opset)\n\n    # apply custom ops on top of the assembled opset. We can either complement the opset\n    # or override existing ops with a custom op.\n    if custom_op_handlers is not None:\n        # below is a bit tricky since there are a few api\'s:\n        # 1. the future way we want custom ops to be registered with the @tf_op decorator. THose handlers will be\n        #     registered via the decorator on load of the module ... nothing is required here.\n        # 2. the old custom op api: a dictionary of {name: (func, args[])\n        #     We deal with this by using a compat_handler that wraps to old handler with a new style handler.\n        #     This is tempoary to give people give to move to the new api and after tf2onnx-1.5 we want to remove this\n        custom_opset = {}\n        for k, v in custom_op_handlers.items():\n            # FIXME: remove this after tf2onnx-1.5\n            def compat_handler(ctx, node, **kwargs):\n                # wrap old handler\n                name = node.name\n                args = kwargs[""args""]\n                func = kwargs[""func""]\n                return func(ctx, node, name, args)\n\n            args = v[1]\n            kwargs = {""func"": v[0]}\n            if args:\n                onnx_op = args[0]\n                kwargs[""onnx_op""] = onnx_op\n                args = args[1:]\n            kwargs[""args""] = args\n            new_handler = handler.tf_op(k,\n                                        domain=constants.TENSORFLOW_OPSET.domain,\n                                        kwargs=kwargs)\n            new_handler.register_compat_handler(compat_handler, 1)\n            custom_opset[k] = (compat_handler, kwargs)\n        ops_mapping.update(custom_opset)\n\n    if inputs_as_nchw:\n        transpose_inputs(g, inputs_as_nchw)\n\n    # pre-processing graph rewrites\n    # bi-directional re-writer should be placed after single directional re-writer\n    rewriters = [rewrite_constant_fold, rewrite_quantize_and_dequantize, rewrite_transpose, rewrite_flatten,\n                 rewrite_gemm, rewrite_random_uniform, rewrite_random_uniform_fold_const,\n                 rewrite_random_normal, rewrite_dropout, rewrite_eye,\n                 rewrite_leakyrelu, rewrite_thresholded_relu, rewrite_conv2d_with_pad,\n                 rewrite_single_direction_lstm, rewrite_bi_direction_lstm,\n                 rewrite_single_direction_gru, rewrite_bi_direction_gru,\n                 rewrite_custom_rnn_cell, rewrite_generic_loop, rewrite_cond,\n                 rewrite_biasadd_with_conv2d,\n                 ]\n\n    if custom_rewriter is not None:\n        rewriters.extend(custom_rewriter)\n\n    run_rewriters(g, rewriters, continue_on_error)\n\n    # some nodes may already copied into inner Graph, so remove them from main Graph.\n    g.delete_unused_nodes(output_names)\n    topological_sort(g, continue_on_error)\n\n    mapped_op, unmapped_op, exceptions = tensorflow_onnx_mapping(g, ops_mapping)\n    if unmapped_op:\n        logger.error(""Unsupported ops: %s"", unmapped_op)\n    if exceptions and not continue_on_error:\n        raise exceptions[0]\n\n    # post-processing rewriters\n    late_rewriters = []\n    if constants.TARGET_RS5 in target:\n        late_rewriters.append(rewrite_incomplete_type_support_rs5)\n    if constants.TARGET_RS6 in target:\n        late_rewriters.append(rewrite_incomplete_type_support_rs6)\n    if late_rewriters:\n        run_rewriters(g, late_rewriters, continue_on_error)\n\n    # onnx requires topological sorting\n    topological_sort(g, continue_on_error)\n\n    g.update_proto()\n\n    logger.verbose(\n        ""Summay Stats:\\n""\n        ""\\ttensorflow ops: {}\\n""\n        ""\\ttensorflow attr: {}\\n""\n        ""\\tonnx mapped: {}\\n""\n        ""\\tonnx unmapped: {}"".format(op_cnt, attr_cnt, mapped_op, unmapped_op))\n\n    return g\n\n\ndef tf_optimize(input_names, output_names, graph_def, fold_constant=True):\n    """"""optimize tensorflow graph. This is in tf_loader but some apps call this\n       so we proxy into tf_loader to keep them working.""""""\n    return tf2onnx.tf_loader.tf_optimize(input_names, output_names, graph_def, fold_constant)\n'"
tf2onnx/utils.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.utils - misc utilities for tf2onnx\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport re\nimport shutil\nimport tempfile\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport numpy as np\nfrom google.protobuf import text_format\nfrom onnx import helper, onnx_pb, defs, numpy_helper, __version__\n\nfrom . import constants\n\n\n#\n# mapping dtypes from onnx to numpy\n#\nONNX_TO_NUMPY_DTYPE = {\n    onnx_pb.TensorProto.FLOAT: np.float32,\n    onnx_pb.TensorProto.FLOAT16: np.float16,\n    onnx_pb.TensorProto.DOUBLE: np.float64,\n    onnx_pb.TensorProto.INT32: np.int32,\n    onnx_pb.TensorProto.INT16: np.int16,\n    onnx_pb.TensorProto.INT8: np.int8,\n    onnx_pb.TensorProto.UINT8: np.uint8,\n    onnx_pb.TensorProto.UINT16: np.uint16,\n    onnx_pb.TensorProto.INT64: np.int64,\n    onnx_pb.TensorProto.UINT64: np.uint64,\n    onnx_pb.TensorProto.BOOL: np.bool,\n}\n\n#\n#  onnx dtype names\n#\nONNX_DTYPE_NAMES = {\n    onnx_pb.TensorProto.FLOAT: ""float"",\n    onnx_pb.TensorProto.FLOAT16: ""float16"",\n    onnx_pb.TensorProto.DOUBLE: ""double"",\n    onnx_pb.TensorProto.INT32: ""int32"",\n    onnx_pb.TensorProto.INT16: ""int16"",\n    onnx_pb.TensorProto.INT8: ""int8"",\n    onnx_pb.TensorProto.UINT8: ""uint8"",\n    onnx_pb.TensorProto.UINT16: ""uint16"",\n    onnx_pb.TensorProto.INT64: ""int64"",\n    onnx_pb.TensorProto.STRING: ""string"",\n    onnx_pb.TensorProto.BOOL: ""bool""\n}\n\n\nclass TensorValueInfo(object):\n    def __init__(self, tensor_id, g):\n        self.id = tensor_id\n        if self.id:\n            self.dtype = g.get_dtype(tensor_id)\n            self.shape = g.get_shape(tensor_id)\n\n\nONNX_UNKNOWN_DIMENSION = -1\nONNX_EMPTY_INPUT = """"\n\n# index for internally generated names\nINTERNAL_NAME = 1\n\n# Fake onnx op type which is used for Graph input.\nGRAPH_INPUT_TYPE = ""NON_EXISTENT_ONNX_TYPE""\n\n\ndef make_name(name):\n    """"""Make op name for inserted ops.""""""\n    global INTERNAL_NAME\n    INTERNAL_NAME += 1\n    return ""{}__{}"".format(name, INTERNAL_NAME)\n\n\ndef split_nodename_and_shape(name):\n    """"""input name with shape into name and shape.""""""\n    # pattern for a node name\n    inputs = []\n    shapes = {}\n    # input takes in most cases the format name:0, where 0 is the output number\n    # in some cases placeholders don\'t have a rank which onnx can\'t handle so we let uses override the shape\n    # by appending the same, ie : [1,28,28,3]\n    name_pattern = r""(?:([\\w\\d/\\-\\._:]+)(\\[[\\-\\d,]+\\])?),?""\n    splits = re.split(name_pattern, name)\n    for i in range(1, len(splits), 3):\n        inputs.append(splits[i])\n        if splits[i + 1] is not None:\n            shapes[splits[i]] = [int(n) for n in splits[i + 1][1:-1].split("","")]\n    if not shapes:\n        shapes = None\n    return inputs, shapes\n\n\ndef map_numpy_to_onnx_dtype(np_dtype):\n    for onnx_dtype, numpy_dtype in ONNX_TO_NUMPY_DTYPE.items():\n        if numpy_dtype == np_dtype:\n            return onnx_dtype\n    raise ValueError(""unsupported dtype "" + np_dtype + "" for mapping"")\n\n\ndef map_onnx_to_numpy_type(onnx_type):\n    return ONNX_TO_NUMPY_DTYPE[onnx_type]\n\n\ndef node_name(name):\n    """"""Get node name without io#.""""""\n    pos = name.find("":"")\n    if pos >= 0:\n        return name[:pos]\n    return name\n\n\ndef make_onnx_shape(shape):\n    """"""shape with -1 is not valid in onnx ... make it a name.""""""\n    if shape:\n        # don\'t do this if input is a scalar\n        return [make_name(""unk"") if i == -1 else i for i in shape]\n    return shape\n\n\ndef port_name(name, nr=0):\n    """"""Map node output number to name.""""""\n    return name + "":"" + str(nr)\n\n\ndef make_onnx_inputs_outputs(name, elem_type, shape, **kwargs):\n    """"""Wrapper for creating onnx graph inputs or outputs\n       name,  # type: Text\n       elem_type,  # type: TensorProto.DataType\n       shape,  # type: Optional[Sequence[int]]\n    """"""\n    if elem_type is None:\n        elem_type = onnx_pb.TensorProto.UNDEFINED\n    return helper.make_tensor_value_info(\n        name,\n        elem_type,\n        make_onnx_shape(shape),\n        **kwargs\n    )\n\n\ndef find_opset(opset):\n    """"""Find opset.""""""\n    if opset is None or opset == 0:\n        opset = defs.onnx_opset_version()\n        if opset > constants.PREFERRED_OPSET:\n            # if we use a newer onnx opset than most runtimes support, default to the one most supported\n            opset = constants.PREFERRED_OPSET\n    return opset\n\n\ndef save_onnx_model(save_path_root, onnx_file_name, feed_dict, model_proto, include_test_data=False, as_text=False):\n    """"""Save onnx model as file. Save a pbtxt file as well if as_text is True""""""\n    save_path = save_path_root\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    if include_test_data:\n        data_path = os.path.join(save_path, ""test_data_set_0"")\n        if not os.path.exists(data_path):\n            os.makedirs(data_path)\n\n        i = 0\n        for data_key in feed_dict:\n            data = feed_dict[data_key]\n            t = numpy_helper.from_array(data)\n            t.name = data_key\n            data_full_path = os.path.join(data_path, ""input_"" + str(i) + "".pb"")\n            save_protobuf(data_full_path, t)\n            i += 1\n\n    target_path = os.path.join(save_path, onnx_file_name + "".onnx"")\n    save_protobuf(target_path, model_proto)\n    if as_text:\n        save_protobuf(target_path + "".pbtxt"", model_proto, as_text=True)\n    return target_path\n\n\ndef make_sure(bool_val, error_msg, *args):\n    if not bool_val:\n        raise ValueError(""make_sure failure: "" + error_msg % args)\n\n\ndef construct_graph_from_nodes(parent_g, nodes, outputs, shapes, dtypes):\n    """"""Construct Graph from nodes and outputs with specified shapes and dtypes.""""""\n    # pylint: disable=protected-access\n    g = parent_g.create_new_graph_with_same_config()\n    g.parent_graph = parent_g\n    nodes = set(nodes)\n    all_outputs = set()\n    for op in nodes:\n        all_outputs |= set(op.output)\n\n        new_node = g.make_node(op.type, op.input, outputs=op.output, attr=op.attr, name=op.name,\n                               skip_conversion=op.skip_conversion, infer_shape_dtype=False)\n        body_graphs = op.graph.contained_graphs.pop(op.name, None)\n        if body_graphs:\n            for attr_name, body_graph in body_graphs.items():\n                body_graph.parent_graph = g\n                new_node.set_body_graph_as_attr(attr_name, body_graph)\n\n    for i in all_outputs:\n        if i not in g._output_shapes:\n            g._output_shapes[i] = parent_g._output_shapes[i]\n        if i not in g._dtypes:\n            g._dtypes[i] = parent_g._dtypes[i]\n\n    # handle cell graph: insert identity node, since sometimes we need output same output_id\n    # as state_output and scan_out, but ONNX don\'t allow the same output_id to appear more\n    # than once as output node.\n    new_output_names = []\n    for output, shape, dtype in zip(outputs, shapes, dtypes):\n        node = g.make_node(""Identity"", inputs=[output], op_name_scope=""sub_graph_ending_node"",\n                           shapes=[shape], dtypes=[dtype], infer_shape_dtype=False)\n        new_output_names.append(node.output[0])\n    g.outputs = new_output_names\n    return g\n\n\ndef tf_name_scope(name):\n    return \'/\'.join(name.split(\'/\')[:-1])\n\n\ndef get_temp_directory():\n    return os.environ.get(""TF2ONNX_TEMP_DIRECTORY"", tempfile.mkdtemp())\n\n\ndef delete_directory(path):\n    if os.path.exists(path):\n        shutil.rmtree(path)\n\n\ndef save_protobuf(path, message, as_text=False):\n    dir_name = os.path.dirname(path)\n    if dir_name:\n        os.makedirs(dir_name, exist_ok=True)\n    if as_text:\n        with open(path, ""w"") as f:\n            f.write(text_format.MessageToString(message))\n    else:\n        with open(path, ""wb"") as f:\n            f.write(message.SerializeToString())\n\n\ndef is_list_or_tuple(obj):\n    return isinstance(obj, (list, tuple))\n\n\ndef is_unknown_dimension(dim):\n    """"""  Return true if dim is not a positive integer value. """"""\n    if dim is None or not isinstance(dim, int):\n        return True\n    return dim <= 0\n\n\ndef merge_shapes(shape1, shape2):\n    """"""\n    Merge 2 shapes, return merged shape, choose more specific dimension value from either side.\n    Raise exception for mismatch.\n    """"""\n    if shape1 is None:\n        return shape2\n    if shape2 is None:\n        return shape1\n\n    make_sure(is_list_or_tuple(shape1), ""invalid type for shape1"")\n    make_sure(is_list_or_tuple(shape2), ""invalid type for shape2"")\n    make_sure(len(shape1) == len(shape2), ""shapes rank mismatch: shape1=%s, shape2=%s"", shape1, shape2)\n\n    merged = []\n    for d1, d2 in zip(shape1, shape2):\n        d = d1\n        if is_unknown_dimension(d1):\n            d = d2\n        elif not is_unknown_dimension(d2):\n            make_sure(d1 == d2, ""shapes dimension mismatch: shape1=%s, shape2=%s"", shape1, shape2)\n        merged.append(d)\n    return merged\n\n\ndef are_shapes_compatible(src, dest):\n    """"""\n    Returns True iff src is compatible with dest.\n    None is compatible with all shapes, different ranks are not considered as compatible\n    """"""\n    try:\n        merge_shapes(src, dest)\n        return True\n    except:  # pylint: disable=bare-except\n        return False\n\n\ndef are_shapes_equal(src, dest):\n    """""" Check whether 2 shapes are equal. """"""\n    if src is None:\n        return dest is None\n    if dest is None:\n        return src is None\n\n    make_sure(is_list_or_tuple(src), ""invalid type for src"")\n    make_sure(is_list_or_tuple(dest), ""invalid type for dest"")\n\n    if len(src) != len(dest):\n        return False\n    return all(i == j for i, j in zip(src, dest))\n\n\ndef create_vague_shape_like(shape):\n    make_sure(len(shape) >= 0, ""rank should be >= 0"")\n    return [-1 for i in enumerate(shape)]\n\n\ndef get_onnx_version():\n    return __version__\n\n\ndef make_opsetid(domain, version):\n    make_sure(isinstance(version, int), ""version must be an integer"")\n    return helper.make_opsetid(domain, version)\n\n\ndef is_onnx_domain(domain):\n    if domain is None or domain == """":\n        return True\n    return False\n\n\ndef parse_bool(val):\n    if val is None:\n        return False\n    return val.lower() in (""yes"", ""true"", ""t"", ""y"", ""1"")\n\n\n_is_debug_mode = parse_bool(os.environ.get(constants.ENV_TF2ONNX_DEBUG_MODE))\n\n\ndef is_debug_mode():\n    return _is_debug_mode\n\n\ndef set_debug_mode(enabled):\n    global _is_debug_mode\n    _is_debug_mode = enabled\n\n\ndef get_max_value(np_dtype):\n    return np.iinfo(np_dtype).max\n\n\ndef get_min_value(np_dtype):\n    return np.iinfo(np_dtype).min\n\n\ndef get_url(url, path, max_retries=5):\n    """""" Download url and save to path. """"""\n    retries = Retry(total=max_retries, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retries)\n    session = requests.Session()\n    session.mount(""http://"", adapter)\n    session.mount(""https://"", adapter)\n\n    response = session.get(url, allow_redirects=True)\n    if response.status_code not in [200]:\n        response.raise_for_status()\n\n    dir_name = os.path.dirname(path)\n    if dir_name:\n        os.makedirs(dir_name, exist_ok=True)\n\n    with open(path, ""wb"") as f:\n        f.write(response.content)\n\n\ndef have_same_inference_value(g, output_1, output_2):\n    """"""\n    If two outputs have the same value in inference.\n    Check whether they come from the same subgraph and the same subgraphs\n    contain nodes with the same attributes and share the same ancestors.\n    """"""\n\n    def is_same(node_1, node_2):\n        # go further util two instance isn\'t the same\n        if node_1 == node_2:\n            return True\n        # check body graph\n        if node_1.get_body_graphs() or node_2.get_body_graphs():\n            logger.warning(""Comparing two nodes containing body graph isn\'t supported."")\n            return False\n        # check domain\n        if node_1.domain != node_2.domain:\n            return False\n        # check type\n        if node_1.type != node_2.type:\n            return False\n        # check onnx attributes\n        if node_1.attr_onnx.keys() != node_2.attr_onnx.keys():\n            return False\n        for name in node_1.attr_onnx.keys(): # pylint: disable=consider-iterating-dictionary\n            if node_1.get_attr_value(name) != node_2.get_attr_value(name):\n                return False\n        return True\n\n    if output_1 == output_2:\n        return True\n    node_1 = g.get_node_by_output(output_1)\n    node_2 = g.get_node_by_output(output_2)\n    # compare their domain, attr, etc. see __eq__ in Node class\n    if not is_same(node_1, node_2):\n        return False\n\n    for inp_1, inp_2 in zip(node_1.input, node_2.input):\n        if not have_same_inference_value(g, inp_1, inp_2):\n            return False\n    return True\n\n\ndef is_tf_reverse_op(op):\n    return op.type in (""ReverseV2"", ""ReverseSequence"")\n\n\ndef is_tf_concat_op(op):\n    return op.type in (""Concat"", ""ConcatV2"", ""ConcatV3"")\n\n\ndef is_tf_tensor_array_gather_op(op):\n    return op.type in (""TensorArrayGatherV2"", ""TensorArrayGatherV3"")\n\n\ndef is_tf_tensor_array_write_op(op):\n    return op.type in (""TensorArrayWriteV2"", ""TensorArrayWriteV3"")\n\n\ndef is_tf_tensor_array_op(op):\n    return op.type in (""TensorArrayV2"", ""TensorArrayV3"")\n\n\ndef is_tf_loopcond_op(op):\n    return op.type == ""LoopCond""\n\n\ndef is_tf_select_op(op):\n    return op.type in (""Select"", ""SelectV2"")\n\n\ndef is_tf_slice_op(op):\n    return op.type == ""Slice""\n\n\ndef is_tf_const_op(op):\n    return op.type in [""Const"", ""ConstV2""]\n'"
tf2onnx/verbose_logging.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nA wrapper of built-in logging with custom level support and utilities.\n""""""\n\nfrom contextlib import contextmanager\nimport logging as _logging\nfrom logging import *  # pylint: disable=wildcard-import, unused-wildcard-import\nimport os\nimport types\n\nimport tensorflow as tf\nTF2 = tf.__version__.startswith(""2."")\n\nfrom . import constants  # pylint: disable=wrong-import-position\n\nVERBOSE = 15\n\n_logging.addLevelName(VERBOSE, ""VERBOSE"")\n\n\ndef _verbose(self, message, *args, **kwargs):\n    if self.isEnabledFor(VERBOSE):\n        self._log(VERBOSE, message, args, **kwargs)  # pylint: disable=protected-access\n\n\ndef getLogger(name=None):  # pylint: disable=invalid-name, function-redefined\n    logger = _logging.getLogger(name)\n    # Inject verbose method to logger object instead logging module\n    logger.verbose = types.MethodType(_verbose, logger)\n    return logger\n\n\n_BASIC_LOG_FORMAT = ""%(asctime)s - %(levelname)s - %(message)s""\n_VERBOSE_LOG_FORMAT = ""%(asctime)s - %(levelname)s - %(name)s: %(message)s""\n\n\ndef basicConfig(**kwargs):  # pylint: disable=invalid-name, function-redefined\n    """""" Do basic configuration for the logging system. tf verbosity is updated accordingly. """"""\n    # Choose pre-defined format if format argument is not specified\n    if ""format"" not in kwargs:\n        level = kwargs.get(""level"", _logging.root.level)\n        kwargs[""format""] = _BASIC_LOG_FORMAT if level >= INFO else _VERBOSE_LOG_FORMAT\n    # config will make effect only when root.handlers is empty, so add the following statement to make sure it\n    _logging.root.handlers = []\n    _logging.basicConfig(**kwargs)\n    set_tf_verbosity(_logging.getLogger().getEffectiveLevel())\n\n\n_LOG_LEVELS = [FATAL, ERROR, WARNING, INFO, VERBOSE, DEBUG]\n\n\ndef get_verbosity_level(verbosity, base_level=INFO):\n    """""" If verbosity is specified, return corresponding level, otherwise, return default_level. """"""\n    if verbosity is None:\n        return base_level\n    verbosity = min(max(0, verbosity) + _LOG_LEVELS.index(base_level), len(_LOG_LEVELS) - 1)\n    return _LOG_LEVELS[verbosity]\n\n\ndef set_level(level):\n    """""" Set logging level for tf2onnx package. tf verbosity is updated accordingly. """"""\n    _logging.getLogger(constants.TF2ONNX_PACKAGE_NAME).setLevel(level)\n    set_tf_verbosity(level)\n\n\ndef set_tf_verbosity(level):\n    """""" Set TF logging verbosity.""""""\n    # TF log is too verbose, adjust it\n    if TF2:\n        return\n\n    level = ERROR if level >= INFO else level\n    tf.logging.set_verbosity(level)\n\n    # TF_CPP_MIN_LOG_LEVEL:\n    #   0 = all messages are logged (default behavior)\n    #   1 = INFO messages are not printed\n    #   2 = INFO and WARNING messages are not printed\n    #   3 = INFO, WARNING, and ERROR messages are not printed\n    if level <= INFO:\n        tf_cpp_min_log_level = ""0""\n    elif level <= WARNING:\n        tf_cpp_min_log_level = ""1""\n    elif level <= ERROR:\n        tf_cpp_min_log_level = ""2""\n    else:\n        tf_cpp_min_log_level = ""3""\n    os.environ[""TF_CPP_MIN_LOG_LEVEL""] = tf_cpp_min_log_level\n\n\n@contextmanager\ndef set_scope_level(level, logger=None):\n    """"""\n    Set logging level to logger within context, reset level to previous value when exit context.\n    TF verbosity is NOT affected.\n    """"""\n    if logger is None:\n        logger = getLogger()\n\n    current_level = logger.level\n    logger.setLevel(level)\n\n    try:\n        yield logger\n    finally:\n        logger.setLevel(current_level)\n'"
tf2onnx/version.py,0,"b""\nversion = '1.7.0'\ngit_version = 'aafc8335bf0e3e708840fbaacf8f5fc10059821e'\n"""
tools/aggregate-patterns.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nTool to find common patterns in onnx graphs.\n""""""\n\n# don\'t want to rename the tool\n# pylint: disable=invalid-name,missing-docstring,too-many-nested-blocks\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport csv\nimport os\n\nfrom onnx import ModelProto\n\n\nclass Node():\n    def __init__(self, node=None):\n        self.inputs = []\n        self.outputs = []\n        self.name = """"\n        self.op_type = """"\n        if node:\n            self.op_type = node.op_type\n            self.name = node.name\n\n    def add_input(self, node):\n        self.inputs.append(node)\n\n    def add_output(self, node):\n        self.outputs.append(node)\n\n    def walk(self, n):\n        ret = []\n        for node in self.inputs:\n            if n > 0:\n                next_node = node.walk(n - 1)\n                if next_node:\n                    ret.append(next_node)\n            else:\n                ret.append(""*"")\n        if self.inputs:\n            return self.op_type + ""("" + "","".join(ret) + "")""\n        return self.op_type + ""()""\n\n    def __str__(self):\n        return ""<node = "" + self.op_type + "","" + self.name + "">""\n\n    def __repr__(self):\n        return ""<node = "" + self.op_type + "","" + self.name + "">""\n\n\nclass Graph():\n    def __init__(self):\n        self.nodes = []\n        self.outputs = []\n\n    def add(self, node):\n        self.nodes.append(node)\n\n    def add_output(self, node):\n        self.outputs.append(node)\n\n    def bfs(self, next_to_visit):\n        nodes_to_keep = []\n        while next_to_visit:\n            n = next_to_visit[0]\n            del next_to_visit[0]\n            if n in nodes_to_keep:\n                continue\n            nodes_to_keep.append(n)\n            for i in reversed(n.inputs):\n                next_to_visit.append(i)\n        return nodes_to_keep\n\n\ndef parse_onnx(fname):\n    with open(fname, ""rb"") as f:\n        data = f.read()\n        model = ModelProto()\n        model.ParseFromString(data)\n\n    outputs = {}\n    g = Graph()\n\n    for node in model.graph.initializer:\n        n = Node()\n        n.op_type = ""Const""\n        n.name = node.name\n        g.add(n)\n        outputs[n.name] = n\n    for node in model.graph.node:\n        n = Node(node)\n        for name in node.output:\n            outputs[name] = n\n        for name in node.input:\n            o = outputs.get(name)\n            if o:\n                n.add_input(o)\n        g.add(n)\n    for node in model.graph.output:\n        o = outputs.get(node.name)\n        if o:\n            g.add_output(o)\n    return g\n\n\ndef one_file(fname, summary, max_nodes):\n    g = parse_onnx(fname)\n    for output_node in g.outputs:\n        res = g.bfs([output_node])\n        for node in res:\n            ret = node.walk(max_nodes)\n            if len(ret.split("","")) > 2:\n                summary[ret] += 1\n\n\ndef read_csv(fname, summary):\n    with open(fname, ""r"", encoding=""utf-8"") as fp:\n        rd = csv.reader(fp)\n        next(rd)\n        for row in rd:\n            if len(row) > 4:\n                summary[row[4]] = int(row[1])\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--output"", help=""write to csv"")\n    parser.add_argument(""--summary"", help=""write combined to csv"")\n    parser.add_argument(""--fold"", action=""store_true"", help=""fold smaller sub graphs"")\n    parser.add_argument(""--max-nodes"", type=int, default=6, help=""number of max nodes in a pattern"")\n    parser.add_argument(""--min-frequency"", type=int, default=2, help=""show patterns number seen at least this"")\n    parser.add_argument(""infile"", nargs=""*"", help=\'event files\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = get_args()\n    if args.output:\n        fp = open(args.output, ""w"")\n        fp.write(""model,seen,maxlength,length,pattern\\n"")\n\n    summary_combined = collections.defaultdict(int)\n    for fname in args.infile:\n        summary = collections.defaultdict(int)\n        for n in range(2, args.max_nodes):\n            if fname.endswith("".onnx""):\n                one_file(fname, summary, n)\n                summary_sorted = sorted(summary.items(), key=lambda x: x[1], reverse=True)\n                name = os.path.basename(fname)\n                for k, v in summary_sorted:\n                    if v > args.min_frequency:\n                        print(""{},{},{}"".format(name, v, k))\n                if args.output:\n                    for k, v in summary_sorted:\n                        if v > args.min_frequency:\n                            l = len(k.split("",""))\n                            fp.write(""{},{},{},{},\\""{}\\""\\n"".format(name, v, n, l, k))\n            else:\n                read_csv(fname, summary)\n\n            for k, v in summary.items():\n                summary_combined[k] += v\n\n    if args.output:\n        fp.close()\n\n    if args.summary:\n        summary_sorted = sorted(summary_combined.items(), key=lambda x: x[1], reverse=True)\n        for k, v in summary_sorted:\n            if v > args.min_frequency:\n                print(""combined,{},{}"".format(v, k))\n        with open(args.summary, ""w"") as fp:\n            for k, v in summary_sorted:\n                if v > args.min_frequency:\n                    l = len(k.split("",""))\n                    fp.write(""{},{},{},{},\\""{}\\""\\n"".format(""combined"", v, 0, l, k))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/dump-onnx.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nDump onnx graph.\n""""""\n# don\'t want to rename the tool\n# pylint: disable=invalid-name\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport re\n\nimport onnx\nfrom onnx import ModelProto\nfrom onnx import helper, shape_inference\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--input"", required=True, help=""input model"")\n    parser.add_argument(""--pbtxt"", help=""write pbtxt"")\n    parser.add_argument(""--dot"", help=""write dot file"")\n    parser.add_argument(""--meta"", help=""include meta data"", action=""store_true"")\n    parser.add_argument(""--check"", help=""check onnx model"", action=""store_true"")\n    parser.add_argument(""--stats"", help=""collect stats"", action=""store_true"")\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = get_args()\n\n    with open(args.input, ""rb"") as f:\n        data = f.read()\n        model = ModelProto()\n        model.ParseFromString(data)\n\n    if args.stats:\n        ops = collections.Counter()\n        for node in model.graph.node:\n            ops[node.op_type] += 1\n        print(ops, ""\\n\\n"")\n\n    if args.meta:\n        fields = [""ir_version"", ""producer_name"", ""producer_version"", ""name"", ""opset_import""]\n        for name in fields:\n            value = getattr(model, name, None)\n            if value:\n                print(""{} = {}"".format(name, value))\n        for i in model.metadata_props:\n            print(""meta.{} = {}"", i.key, i.value)\n\n    print(helper.printable_graph(model.graph))\n\n    if args.check:\n        onnx.checker.check_model(model)\n        inferred_model = shape_inference.infer_shapes(model)\n        onnx.checker.check_model(inferred_model)\n\n    if args.pbtxt:\n        with open(args.pbtxt, ""w"") as f:\n            f.write(str(model.graph))\n\n    if args.dot:\n        with open(args.dot, ""w"") as f:\n            f.write(""digraph graphname {\\n"")\n            for node in model.graph.node:\n                output_name = node.name\n                name = node.name\n                color = """"\n                if node.op_type.startswith(""_""):\n                    color = \' color=""yellow""\'\n                if node.op_type == ""CELL"":\n                    color = \' color=""red""\'\n                f.write(\'""{}"" [label=""{},{}""{}];\\n\'.format(output_name, node.op_type, name, color))\n                for input_name in node.input:\n                    parts = input_name.split("":"")\n                    input_name = re.sub(r""^\\^"", """", parts[0])\n                    f.write(\'  ""{}"" -> ""{}"";\\n\'.format(input_name, output_name))\n            f.write(""}\\n"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/gen_doc.py,0,"b'""""""\ntool to list tensorflow op to onnx op mapping in markdown\n""""""\n\nimport argparse\nimport inspect\nimport re\n\nfrom collections import OrderedDict\n\nfrom tf2onnx.handler import tf_op\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'filename\',\n                    type=str,\n                    help=""Path to the documentation file."")\nargs = parser.parse_args()\n\n\nLATEST_OPSET = {\n    """": 12, # default domain\n    ""com.microsoft"": 1 # microsoft domain\n}\n\n\ndef is_unsupported(func_body):\n    source_code = inspect.getsource(func_body)\n    if re.findall(r\'raise NotImplementedError\', source_code):\n        return True\n    return False\n\n\nwith open(args.filename, \'w+\') as doc_file:\n    doc_file.write(""## `tf2onnx` Support Status\\n"")\n\n    for domain, opsets in tf_op.get_opsets().items():\n        comment = ""(default domain)"" if domain == """" else """"\n        doc_file.write(""### Domain: \\""{}\\"" {}\\n"".format(domain, comment))\n        doc_file.write(""| Tensorflow Op | Convertible to ONNX Op Versions |\\n"")\n        doc_file.write(""| ------------- | ------------------------------- |\\n"")\n\n        # Collect a mapping from tf ops to supported handler versions.\n        tf_op_to_versions = OrderedDict()\n        # Some op with NotImplementedError in it is unsupported.\n        unsupported_cases = OrderedDict()\n        for opset in opsets:\n            for name, func in opset.items():\n                handler_ver = int(func[0].__name__.replace(""version_"", """"))\n                if name not in tf_op_to_versions or handler_ver < tf_op_to_versions[name]:\n                    tf_op_to_versions[name] = handler_ver\n\n                if is_unsupported(func[0]):\n                    if name in unsupported_cases:\n                        unsupported_cases[name].append(handler_ver)\n                    else:\n                        unsupported_cases[name] = [handler_ver]\n\n        # Document support status according to the gathered mapping.\n        for tf_op in sorted(tf_op_to_versions):\n            supported_versions = tf_op_to_versions[tf_op]\n            if supported_versions < LATEST_OPSET[domain]:\n                version_text = ""{} ~ {}"".format(supported_versions, LATEST_OPSET[domain])\n                if tf_op in unsupported_cases:\n                    version_text += "" (Except {})"".format(\n                        \',\'.join(str(v) for v in unsupported_cases[tf_op])\n                    )\n            else:\n                version_text = ""{}"".format(LATEST_OPSET[domain])\n            doc_file.write(""| {} | {} |\\n"".format(tf_op, version_text))\n'"
tools/graphtool.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nsimple tool to convert .meta to .pb.\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport tensorflow as tf\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""infile"", nargs=""*"", help=\'event files\')\n    args = parser.parse_args()\n    return args\n\n\ndef to_pb(src):\n    """"""Convert .meta to .pb.""""""\n    _ = tf.train.import_meta_graph(src)\n    graph = tf.get_default_graph()\n\n    fname = os.path.basename(src)[:-5]\n    tf.train.write_graph(graph, os.path.dirname(src), fname + \'.pb\', as_text=False)\n    tf.train.write_graph(graph, os.path.dirname(src), fname + \'.pbtxt\', as_text=True)\n\n    writer = tf.summary.FileWriter(os.path.dirname(src))\n    writer.add_graph(graph)\n    writer.close()\n\n\ndef main():\n    args = get_args()\n    for src in args.infile:\n        to_pb(src)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/make_regression_test_models.py,15,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Make simple test model in all tensorflow formats.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\n\n# pylint: disable=missing-docstring\n\n# Parameters\nlearning_rate = 0.02\ntraining_epochs = 100\n\n# Training Data\n_train_x = np.array(\n    [3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\n_train_y = np.array(\n    [1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\n_test_x = np.array([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n_test_y = np.array([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n\n\ndef freeze_session(sess, keep_var_names=None, output_names=None, clear_devices=True):\n    """"""Freezes the state of a session into a pruned computation graph.""""""\n    output_names = [i.replace("":0"", """") for i in output_names]\n    graph = sess.graph\n    with graph.as_default():\n        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n        output_names = output_names or []\n        output_names += [v.op.name for v in tf.global_variables()]\n        input_graph_def = graph.as_graph_def()\n        if clear_devices:\n            for node in input_graph_def.node:\n                node.device = """"\n        frozen_graph = convert_variables_to_constants(sess, input_graph_def,\n                                                      output_names, freeze_var_names)\n        return frozen_graph\n\n\ndef train(model_path):\n    n_samples = _train_x.shape[0]\n\n    # tf Graph Input\n    x = tf.placeholder(tf.float32, name=""X"")\n    y = tf.placeholder(tf.float32, name=""Y"")\n\n    # Set model weights\n    w = tf.Variable(np.random.randn(), name=""W"")\n    b = tf.Variable(np.random.randn(), name=""b"")\n\n    pred = tf.add(tf.multiply(x, w), b)\n    pred = tf.identity(pred, name=""pred"")\n    cost = tf.reduce_sum(tf.pow(pred - y, 2)) / (2 * n_samples)\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n    saver = tf.train.Saver()\n\n    # Launch the graph\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        # Fit all training data\n        for _ in range(training_epochs):\n            for (ix, iy) in zip(_train_x, _train_y):\n                sess.run(optimizer, feed_dict={x: ix, y: iy})\n        training_cost = sess.run(cost, feed_dict={x: _train_x, y: _train_y})\n        testing_cost = sess.run(cost, feed_dict={x: _test_x, y: _test_y})\n        print(""train_cost={}, test_cost={}, diff={}""\n              .format(training_cost, testing_cost, abs(training_cost - testing_cost)))\n\n        p = os.path.abspath(os.path.join(model_path, ""checkpoint""))\n        os.makedirs(p, exist_ok=True)\n        saver.save(sess, os.path.join(p, ""model""))\n\n        frozen_graph = freeze_session(sess, output_names=[""pred:0""])\n        p = os.path.abspath(os.path.join(model_path, ""graphdef""))\n        tf.train.write_graph(frozen_graph, p, ""frozen.pb"", as_text=False)\n\n        p = os.path.abspath(os.path.join(model_path, ""saved_model""))\n        tf.saved_model.simple_save(sess, p, inputs={""X"": x}, outputs={""pred"": pred})\n\n\ntrain(""models/regression"")\n'"
tools/onnx-experiments.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nA simple tool to try optimizations on onnx graphs.\nThis makes use of the fact that tensorflow-onnx internal graph representation is onnx\nso all graph, rewrite, matching and utility libaries do work which makes things easy.\n""""""\n\n# pylint: disable=invalid-name,missing-docstring, unused-argument\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport logging\nimport traceback\n\nimport numpy as np\nimport onnx\n\nimport tf2onnx.utils\nfrom tf2onnx.graph import GraphUtil\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(""onnx-experiments"")\n\n\ndef get_args():\n    """"""Parse commandline.""""""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--input"", required=True, help=""onnx input model file"")\n    parser.add_argument(""--output"", help=""output model file"")\n    args = parser.parse_args()\n    return args\n\n\ndef load_graph(fname):\n    model_proto = onnx.ModelProto()\n    with open(fname, ""rb"") as f:\n        data = f.read()\n        model_proto.ParseFromString(data)\n    g = GraphUtil.create_graph_from_onnx_model(model_proto)\n    return g, model_proto.producer_name\n\n\ndef sample_rewrite(g, ops):\n    return ops\n\n\ndef rewrite_constant_fold(g, ops):\n\n    func_map = {\n        ""Transpose"": not None,\n        ""Unsqueeze"": not None,\n        ""Slice"": not None,\n        ""Add"": np.add,\n        ""Cast"": np.cast,\n        ""Mul"": np.multiply,\n        ""Sqrt"": np.sqrt,\n        ""Sub"": np.subtract,\n    }\n\n    # pylint: disable=too-many-nested-blocks\n    keep_looking = True\n    while keep_looking:\n        keep_looking = False\n        for idx, op in enumerate(ops):\n            inputs = []\n            for node in op.inputs:\n                if node and node.is_const():\n                    inputs.append(node.get_tensor_value(as_list=False))\n\n            if inputs and len(op.input) == len(inputs):\n                func = func_map.get(op.type)\n                if func is None:\n                    logger.info(""can fold but don\'t know how, type=%s, name=%s"", op.type, op.name)\n                    continue\n                try:\n                    logger.info(""folding node type=%s, name=%s"", op.type, op.name)\n                    if op.type == ""Cast"":\n                        dst = op.get_attr_int(""to"")\n                        np_type = tf2onnx.utils.map_onnx_to_numpy_type(dst)\n                        val = np.cast[np_type](*inputs)\n                    elif op.type == ""Transpose"":\n                        perm = op.get_attr(""perm"").ints\n                        val = np.transpose(inputs[0], perm)\n                    elif op.type == ""Unsqueeze"":\n                        axis = op.get_attr_int(""axis"")\n                        val = np.expand_dims(inputs[0], axis=axis)\n                    elif op.type == ""Slice"":\n                        axis = op.get_attr_int(""axis"")\n                        if axis != 0:\n                            logger.info(""can fold slice with axis!=0, type=%s, name=%s"", op.type, op.name)\n                            continue\n                        starts = op.get_attr_int(""starts"")\n                        ends = op.get_attr_int(""ends"")\n                        if starts == 0 and ends == 0:\n                            val = inputs[0][starts:ends]\n                        else:\n                            val = inputs[0]\n                    else:\n                        val = func(*inputs)\n\n                    new_node_name = tf2onnx.utils.make_name(op.name)\n                    new_output_name = new_node_name\n                    old_output_name = op.output[0]\n                    old_node_name = op.name\n                    logger.debug(""create const node [%s] replacing [%s]"", new_node_name, old_node_name)\n                    ops[idx] = g.make_const(new_node_name, val)\n                    consumers = g.find_output_consumers(old_output_name)\n                    if consumers:\n                        for consumer in consumers:\n                            g.replace_input(consumer, old_output_name, new_output_name)\n                    for i, node in zip(op.input, op.inputs):\n                        if len(g.find_output_consumers(i)) == 1:\n                            g.remove_node(node.name)\n                    keep_looking = True\n                except Exception as ex:  # pylint: disable=broad-except\n                    tb = traceback.format_exc()\n                    logger.info(""exception: %s, details: %s"", ex, tb)\n                    # pylint: enable=too-many-nested-blocks\n    return ops\n\n\ndef main():\n    args = get_args()\n\n    g, producer_name = load_graph(args.input)\n\n    rewriters = [sample_rewrite,\n                 rewrite_constant_fold]\n    ops = g.get_nodes()\n    stats = g.dump_node_statistics()\n\n    for rewrite in rewriters:\n        ops = rewrite(g, ops)\n\n    try:\n        g.topological_sort(ops)\n    except Exception as ex:  # pylint: disable=broad-except,unused-variable\n        logger.error(""graph has cycles, ignored ..."")\n\n    model_proto = g.make_model(producer_name)\n\n    print(""before:"", stats)\n    stats.subtract(g.dump_node_statistics())\n    print(""removed:"", stats)\n\n    # write onnx graph\n    if args.output:\n        with open(args.output, ""wb"") as f:\n            f.write(model_proto.SerializeToString())\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/quantitize_weights.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nquantitize_weights.py - simple script to quantitize weights (not the model) to 8 bits.\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nimport numpy as np\nfrom onnx import ModelProto, helper, onnx_pb, numpy_helper\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(""quantitize_weights"")\n\n\ndef _get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--input"", required=True, help=""input model"")\n    parser.add_argument(""--output"", required=True, help=""output model"")\n    parser.add_argument(""--verbose"", help=""verbose"", action=""store_true"")\n    args = parser.parse_args()\n    return args\n\n\ndef eight_bit_dequantitize(w_in, zp, scale):\n    w = w_in * scale + zp\n    w = w.astype(""float32"")\n    return w\n\n\ndef eight_bit_quantitize(w_in):\n    """"""quantitize to 8 bit as scale and zeropoint""""""\n    low = np.min(w_in)\n    high = np.max(w_in)\n    scale = (high - low) / 256.\n    w = (w_in - low) / scale\n    w_out = w.astype(""uint8"")\n    return w_out, low, scale\n\n\ndef _port_name(name):\n    return name + ""__out""\n\n\ndef _make_node(nodes, op, name, inputs, **kwargs):\n    node = helper.make_node(op, inputs, [_port_name(name)], name=name, **kwargs)\n    nodes.append(node)\n\n\ndef _compose_quantitize(nodes, weights, zp, scale, name):\n    """"""\n    Compose Dequantitize(input, zeropoint, scale).\n    """"""\n    name_zp = name + ""_zeropoint""\n    name_scale = name + ""_scale""\n    name_add = name + ""_add""\n    name_mul = name + ""_mul""\n    name_cast = name + ""_cast""\n\n    # add zeropoint and scale as initializers\n    weights.append(numpy_helper.from_array(np.array(zp, dtype=np.float32), name_zp))\n    weights.append(numpy_helper.from_array(np.array(scale, dtype=np.float32), name_scale))\n\n    # insert ops to dequantitize\n    _make_node(nodes, ""Cast"", name_cast, [name], to=onnx_pb.TensorProto.FLOAT)\n    _make_node(nodes, ""Mul"", name_mul, [_port_name(name_cast), name_scale])\n    _make_node(nodes, ""Add"", name_add, [_port_name(name_mul), name_zp])\n\n    return _port_name(name_add)\n\n\ndef stats(a):\n    return {""mean"": a.mean(), ""std"": a.std(), ""max"": a.max(), ""min"": a.min()}\n\n\ndef quantitize_graph(g, verbose=False):\n    """"""Quantitize graph.""""""\n    new_weights = []\n    quantitized_weights = []\n    nodes = []\n    remap = {}\n    remove = []\n\n    for i, w in enumerate(g.initializer):\n        # only quantitize float32\n        if w.data_type != onnx_pb.TensorProto.FLOAT:\n            continue\n        w_np = numpy_helper.to_array(w)\n        # only look at sizes >= 32 elements\n        if w_np.size < 32:\n            continue\n\n        # weights we want to quantitize\n        remove.append(i)\n        name = w.name\n        if verbose:\n            logger.info(""quantitizing %s"", name)\n        w_quant, zp, scale = eight_bit_quantitize(w_np)\n        nw = numpy_helper.from_array(w_quant, name=name)\n        if verbose:\n            w_dequant = eight_bit_dequantitize(w_quant, zp, scale)\n            rtol = np.abs(w_dequant - w_np)\n            s = {}\n            for j in [1.0, 5.0, 10.0, 20.0]:\n                above_rtol = np.sum(rtol > np.abs(j * w_np / 100.)) / w_np.size\n                s[""> "" + str(j) + ""%""] = ""{:.2f}"".format(100. * above_rtol)\n            logger.info(""above_rtol: %s"", str(s))\n            logger.info(""raw:   %s"", stats(w_np))\n            logger.info(""quant: %s"", stats(w_dequant))\n        output_name = _compose_quantitize(nodes, new_weights, zp, scale, name)\n        remap[name] = output_name\n        quantitized_weights.append(nw)\n\n    # few things to do to initializers and graph inputs:\n\n    # 1. remove initializers that got quantitized\n    for i in reversed(remove):\n        del g.initializer[i]\n\n    # 2. add quantitized to initializers\n    g.initializer.extend(new_weights)\n    g.initializer.extend(quantitized_weights)\n\n    # 3. modify the type of weights that we quantitized\n    modified = {w.name: w for w in quantitized_weights}\n    new_inputs = []\n    remove = []\n    for i, inp in enumerate(g.input):\n        w = modified.get(inp.name)\n        if w is not None:\n            new_inputs.append(helper.make_tensor_value_info(w.name, w.data_type, w.dims))\n            remove.append(i)\n    for i in reversed(remove):\n        del g.input[i]\n\n    # 4. add new weights as inputs\n    for w in new_weights:\n        tv = helper.make_tensor_value_info(w.name, w.data_type, w.dims)\n        new_inputs.append(tv)\n    g.input.extend(new_inputs)\n\n    # 5. rewrite consumers of the quantitized weights\n    for node in g.node:\n        for i, name in enumerate(node.input):\n            new_name = remap.get(name)\n            if new_name is not None:\n                node.input[i] = new_name\n\n    # 6. add composed nodes to graph, new nodes in the front\n    nodes.extend(g.node)\n    del g.node[:]\n    g.node.extend(nodes)\n    return g\n\n\ndef main():\n    args = _get_args()\n\n    # read onnx graph\n    with open(args.input, ""rb"") as f:\n        data = f.read()\n        model_proto = ModelProto()\n        model_proto.ParseFromString(data)\n\n    # quantitize weights\n    g = quantitize_graph(model_proto.graph, args.verbose)\n\n    # write quantitized graph\n    with open(args.output, ""wb"") as f:\n        # create model proto\n        model_proto_out = helper.make_model(g,\n                                            producer_name=""quantized {}"".format(model_proto.producer_name),\n                                            producer_version=model_proto.producer_version,\n                                            opset_imports=model_proto.opset_import)\n        f.write(model_proto_out.SerializeToString())\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/save_pretrained_model.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nSave pre-trained model.\n""""""\nimport tensorflow as tf\nimport numpy as np\n\n\n# pylint: disable=redefined-outer-name,reimported,import-outside-toplevel\n\ndef save_pretrained_model(sess, outputs, feeds, out_dir, model_name=""pretrained""):\n    """"""Save pretrained model and config""""""\n    try:\n        import os\n        import sys\n        import tensorflow as tf\n        import subprocess\n        to_onnx_path = ""{}/to_onnx"".format(out_dir)\n        if not os.path.isdir(to_onnx_path):\n            os.makedirs(to_onnx_path)\n        saved_model = ""{}/saved_model"".format(to_onnx_path)\n        inputs_path = ""{}/inputs.npy"".format(to_onnx_path)\n        pretrained_model_yaml_path = ""{}/pretrained.yaml"".format(to_onnx_path)\n        envars_path = ""{}/environment.txt"".format(to_onnx_path)\n        pip_requirement_path = ""{}/requirements.txt"".format(to_onnx_path)\n\n        print(""===============Save Saved Model========================"")\n        if os.path.exists(saved_model):\n            print(""{} already exists, SKIP"".format(saved_model))\n            return\n\n        print(""Save tf version, python version and installed packages"")\n        tf_version = tf.__version__\n        py_version = sys.version\n        pip_packages = subprocess.check_output([sys.executable, ""-m"", ""pip"", ""freeze"", ""--all""])\n        pip_packages = pip_packages.decode(""UTF-8"")\n        with open(envars_path, ""w"") as fp:\n            fp.write(tf_version + os.linesep)\n            fp.write(py_version)\n        with open(pip_requirement_path, ""w"") as fp:\n            fp.write(pip_packages)\n\n        print(""Save model for tf2onnx: {}"".format(to_onnx_path))\n        # save inputs\n        inputs = {}\n        for inp, value in feeds.items():\n            if isinstance(inp, str):\n                inputs[inp] = value\n            else:\n                inputs[inp.name] = value\n        np.save(inputs_path, inputs)\n        print(""Saved inputs to {}"".format(inputs_path))\n\n        # save graph and weights\n        from tensorflow.saved_model import simple_save\n        # pylint: disable=unnecessary-comprehension\n        simple_save(sess, saved_model,\n                    {n: i for n, i in zip(inputs.keys(), feeds.keys())},\n                    {op.name: op for op in outputs})\n        print(""Saved model to {}"".format(saved_model))\n\n        # generate config\n        pretrained_model_yaml = \'\'\'\n{}:\n  model: ./saved_model\n  model_type: saved_model\n  input_get: get_ramp\n\'\'\'.format(model_name)\n        pretrained_model_yaml += ""  inputs:\\n""\n        for inp, _ in inputs.items():\n            pretrained_model_yaml += \\\n                ""    \\""{input}\\"": np.array(np.load(\\""./inputs.npy\\"")[()][\\""{input}\\""])\\n"".format(input=inp)\n        outputs = [op.name for op in outputs]\n        pretrained_model_yaml += ""  outputs:\\n""\n        for out in outputs:\n            pretrained_model_yaml += ""    - {}\\n"".format(out)\n        with open(pretrained_model_yaml_path, ""w"") as f:\n            f.write(pretrained_model_yaml)\n        print(""Saved pretrained model yaml to {}"".format(pretrained_model_yaml_path))\n        print(""========================================================="")\n    except Exception as ex:  # pylint: disable=broad-except\n        print(""Error: {}"".format(ex))\n\n\ndef test():\n    """"""Test sample.""""""\n    x_val = np.random.rand(5, 20).astype(np.float32)\n    y_val = np.random.rand(20, 10).astype(np.float32)\n    x = tf.placeholder(tf.float32, x_val.shape, name=""x"")\n    y = tf.placeholder(tf.float32, y_val.shape, name=""y"")\n    z = tf.matmul(x, y)\n    w = tf.get_variable(""weight"", [5, 10], dtype=tf.float32)\n    init = tf.global_variables_initializer()\n    outputs = [z + w]\n    feeds = {x: x_val, y: y_val}\n    with tf.Session() as sess:\n        sess.run(init)\n        sess.run(outputs, feeds)\n        # NOTE: NOT override the saved model, so put below snippet after testing the BEST model.\n        # if you perform testing several times.\n        save_pretrained_model(sess, outputs, feeds, ""./tests"", model_name=""test"")\n\n\nif __name__ == ""__main__"":\n    test()\n'"
tools/tf_graph_tool.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n"""""" Tool for common tf graph operations. """"""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nfrom collections import Counter\nimport logging\nimport os\nimport sys\nimport copy\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.core.framework import attr_value_pb2, graph_pb2, node_def_pb2\n\n# pylint: disable=missing-docstring\n\nlogging.basicConfig(stream=sys.stdout, format=\'[%(asctime)s] %(levelname)s: %(message)s\', level=logging.INFO)\n\n\ndef get_file_name(path):\n    return os.path.basename(path)\n\n\ndef get_file_name_without_ext(path):\n    return \'.\'.join(get_file_name(path).split(\'.\')[:-1])\n\n\ndef replace_file_extension(path, ext):\n    tokens = path.split(\'.\')[:-1]\n    tokens.append(ext)\n    return \'.\'.join(tokens)\n\n\ndef append_file_name_suffix(path, suffix):\n    tokens = path.split(\'.\')\n    tokens[-2] += \'_\' + suffix\n    return \'.\'.join(tokens)\n\n\ndef get_file_directory(path):\n    return os.path.dirname(path)\n\n\ndef get_file_directory_name(path):\n    return os.path.basename(get_file_directory(path))\n\n\ndef create_directory(path):\n    if not os.path.isdir(path):\n        os.makedirs(path, exist_ok=True)\n\n\ndef load_graph_def_from_pb(path):\n    tf.reset_default_graph()\n    graph_def = tf.GraphDef()\n    with open(path, ""rb"") as f:\n        graph_def.ParseFromString(f.read())\n    return graph_def\n\n\ndef save_graph_def(graph_def, path, as_text=False):\n    if as_text:\n        with open(path, ""w"") as f:\n            f.write(text_format.MessageToString(graph_def))\n    else:\n        with open(path, ""wb"") as f:\n            f.write(graph_def.SerializeToString())\n\n\ndef get_node_name(tensor_name):\n    if tensor_name.startswith(""^""):\n        return tensor_name[1:]\n    return tensor_name.split("":"")[0]\n\n\ndef get_node_shape(node):\n    shape_attr = node.attr.get(""shape"")\n    shape = [d.size for d in shape_attr.shape.dim]\n    return shape\n\n\ndef get_graph_def_io_nodes(graph_def):\n    consumed = set()\n    inputs = []\n    outputs = []\n    input_shapes = []\n    for node in graph_def.node:\n        for i in node.input:\n            consumed.add(get_node_name(i))\n        if node.op in [""Placeholder"", ""PlaceholderWithDefault"", ""PlaceholderV2""]:\n            inputs.append(node.name)\n            shape = []\n            try:\n                shape = get_node_shape(node)\n            except:  # pylint: disable=bare-except\n                pass\n            input_shapes.append(shape)\n\n    for node in graph_def.node:\n        if node.name not in consumed and node.name not in inputs:\n            outputs.append(node.name)\n\n    return inputs, outputs, input_shapes\n\n\nclass main(object):\n    @staticmethod\n    def convert_pb_to_pbtxt(input_path, output_path=None):\n        if not output_path:\n            output_path = replace_file_extension(input_path, ""pbtxt"")\n\n        logging.info(""load from %s"", input_path)\n        graph_def = load_graph_def_from_pb(input_path)\n\n        logging.info(""save to %s"", output_path)\n        save_graph_def(graph_def, output_path, as_text=True)\n\n    @staticmethod\n    def convert_pb_to_summary(input_path, output_dir=None, start_tensorboard=False, port=6006):\n        if not output_dir:\n            output_dir = input_path + "".summary""\n\n        logging.info(""load from %s"", input_path)\n        graph_def = load_graph_def_from_pb(input_path)\n\n        logging.info(""save to %s"", output_dir)\n        create_directory(output_dir)\n        with tf.Session() as sess:\n            tf.import_graph_def(graph_def, name=get_file_name_without_ext(input_path))\n            train_writer = tf.summary.FileWriter(output_dir)\n            train_writer.add_graph(sess.graph)\n            train_writer.close()\n\n        if start_tensorboard:\n            logging.info(""launch tensorboard"")\n            os.system(""start tensorboard --logdir {} --port {}"".format(output_dir, port))\n            os.system(""start http://localhost:{}"".format(port))\n\n    @staticmethod\n    def get_graph_io_nodes(input_path):\n        logging.info(""load from %s"", input_path)\n        graph_def = load_graph_def_from_pb(input_path)\n        inputs, outputs, input_shapes = get_graph_def_io_nodes(graph_def)\n        logging.info(""graph has:"")\n        logging.info(""\\t%s inputs:"", len(inputs))\n        for input_name, input_shape in zip(inputs, input_shapes):\n            print(""\\""{}:0\\"": {}"".format(input_name, input_shape))\n        logging.info(""\\t%s (possible) outputs:"", len(outputs))\n        for output in outputs:\n            print(""- {}:0"".format(output))\n\n    @staticmethod\n    def print_graph_stat(input_path):\n        logging.info(""load from %s"", input_path)\n        graph_def = load_graph_def_from_pb(input_path)\n\n        op_stat = Counter()\n        for node in graph_def.node:\n            op_stat[node.op] += 1\n\n        logging.info(""graph stat:"")\n        for op, count in sorted(op_stat.items(), key=lambda x: x[0]):\n            logging.info(""\\t%s = %s"", op, count)\n\n    @staticmethod\n    def extract_sub_graph(input_path, dest_nodes=None, output_path=None,\n                          src_nodes=None, name_prefix=""""):\n        """"""\n        Extract the subgraph within the boundary defined by dest_nodes and src_nodes if name_prefix is provided\n        or the subgraph comprising all nodes with name that starts with name_prefix.\n        dest_nodes/src_nodes and name_prefix aren\'t compatible. You only need to supply one of them.\n        """"""\n        logging.info(""load from %s"", input_path)\n        graph_def = load_graph_def_from_pb(input_path)\n        logging.info(""\\ttotal node = %s"", len(graph_def.node))\n\n        if (dest_nodes or src_nodes) and name_prefix:\n            raise RuntimeError(""dest_nodes/src_nodes and name_prefix are incompatible."")\n        if not name_prefix:\n            if not dest_nodes:\n                _, dest_nodes, _ = get_graph_def_io_nodes(graph_def)\n        else:\n            dest_nodes = []\n            for node in graph_def.node:\n                if node.name.startswith(name_prefix):\n                    dest_nodes.append(node.name)\n        if not src_nodes:\n            src_nodes = []\n\n        if not isinstance(dest_nodes, list):\n            raise TypeError(""dest_nodes must be a list."")\n        if not isinstance(src_nodes, list):\n            raise TypeError(""src_nodes must be a list."")\n\n        def extract_graph_summary(graph_def):\n            """"""Extracts useful information from the graph and returns them.""""""\n            name_to_input_name = {}  # Keyed by the dest node name.\n            name_to_node = {}  # Keyed by node name.\n\n            # Keeps track of node sequences. It is important to still output the\n            # operations in the original order.\n            name_to_seq_num = {}  # Keyed by node name.\n            seq = 0\n            for node in graph_def.node:\n                n = get_node_name(node.name)\n                name_to_node[n] = node\n                name_to_input_name[n] = [get_node_name(x) for x in node.input]\n                name_to_seq_num[n] = seq\n                seq += 1\n            return name_to_input_name, name_to_node, name_to_seq_num\n\n\n        def assert_nodes_are_present(name_to_node, nodes):\n            """"""Assert that nodes are present in the graph.""""""\n            for d in nodes:\n                assert d in name_to_node, ""%s is not in graph"" % d\n\n\n        def bfs_for_reachable_nodes(target_nodes, name_to_input_name, checker=None):\n            """"""Breadth first search for reachable nodes from target nodes.""""""\n            nodes_to_keep = set()\n            # Breadth first search to find all the nodes that we should keep.\n            next_to_visit = target_nodes[:]\n            while next_to_visit:\n                n = next_to_visit[0]\n                del next_to_visit[0]\n                if n in nodes_to_keep:\n                    # Already visited this node.\n                    continue\n                if not checker or checker(n):\n                    nodes_to_keep.add(n)\n                    next_to_visit += name_to_input_name[n]\n            return nodes_to_keep\n\n        name_to_input_name, name_to_node, name_to_seq_num = extract_graph_summary(\n            graph_def)\n        assert_nodes_are_present(name_to_node, dest_nodes)\n        assert_nodes_are_present(name_to_node, src_nodes)\n\n        src_ops = []\n        def node_checker(n):\n            if not n.startswith(name_prefix) or n in src_nodes:\n                if name_to_node[n] not in src_ops:\n                    src_ops.append(name_to_node[n])\n                return False\n            return True\n        nodes_to_keep = bfs_for_reachable_nodes(dest_nodes, name_to_input_name, checker=node_checker)\n\n        nodes_to_keep_list = sorted(\n            list(nodes_to_keep), key=lambda n: name_to_seq_num[n])\n        # Now construct the output GraphDef\n        out = graph_pb2.GraphDef()\n        for n in nodes_to_keep_list:\n            out.node.extend([copy.deepcopy(name_to_node[n])])\n\n        # create placeholder\n        with tf.Graph().as_default() as tf_graph:\n            tf.import_graph_def(graph_def, name="""")\n        for op in src_ops:\n            placeholder_node = node_def_pb2.NodeDef()\n            placeholder_node.op = ""Placeholder""\n            placeholder_node.name = op.name\n            dtype = None\n            if str(op.attr[""dtype""]):\n                dtype = op.attr[""dtype""]\n            elif str(op.attr[""T""]):\n                dtype = op.attr[""T""]\n            elif str(op.attr[""output_types""]):\n                dtype = attr_value_pb2.AttrValue()\n                dtype.type = op.attr[""output_types""].list.type[0]\n            if dtype is None:\n                raise RuntimeError(""Cannot find dtype for Placeholder: {}"".format(op.name))\n            placeholder_node.attr[""dtype""].CopyFrom(dtype)\n            shape = graph_util.tensor_shape_from_node_def_name(tf_graph, op.name)\n            placeholder_node.attr[""shape""].CopyFrom(\n                attr_value_pb2.AttrValue(shape=shape.as_proto())\n            )\n            out.node.extend([placeholder_node])\n\n        out.library.CopyFrom(graph_def.library)\n        out.versions.CopyFrom(graph_def.versions)\n\n        if not output_path:\n            output_path = append_file_name_suffix(input_path, ""sub"")\n        logging.info(""save to %s"", output_path)\n        logging.info(""\\ttotal node = %s"", len(out.node))\n        save_graph_def(out, output_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers()\n\n    # pb2txt\n    subparser = subparsers.add_parser(""pb2txt"", help=""convert pb to pbtxt"")\n    subparser.add_argument(""--input"", dest=""input_path"", required=True, help=""input pb path"")\n    subparser.add_argument(""--output"", dest=""output_path"", help=""output pbtxt path"")\n    subparser.set_defaults(func=main.convert_pb_to_pbtxt)\n\n    # pb2summary\n    subparser = subparsers.add_parser(""pb2summary"", help=""create summary from pb"")\n    subparser.add_argument(""--input"", dest=""input_path"", required=True, help=""input pb path"")\n    subparser.add_argument(""--output"", dest=""output_dir"", help=""output summary directory"")\n    subparser.add_argument(""--tb"", dest=""start_tensorboard"", action=""store_true"", default=False,\n                           help=""open with tensorboard"")\n    subparser.add_argument(""--port"", type=int, help=""tensorboard port"")\n    subparser.set_defaults(func=main.convert_pb_to_summary)\n\n    # io\n    subparser = subparsers.add_parser(""io"", help=""get input nodes for graph, guess output nodes"")\n    subparser.add_argument(""--input"", dest=""input_path"", required=True, help=""input pb path"")\n    subparser.set_defaults(func=main.get_graph_io_nodes)\n\n    # stat\n    subparser = subparsers.add_parser(""stat"", help=""print stat"")\n    subparser.add_argument(""--input"", dest=""input_path"", required=True, help=""input pb path"")\n    subparser.set_defaults(func=main.print_graph_stat)\n\n    # extract\n    subparser = subparsers.add_parser(""extract"", help=""extract sub-graph"")\n    subparser.add_argument(""--input"", dest=""input_path"", required=True, help=""input pb path"")\n    subparser.add_argument(""--output"", dest=""output_path"", help=""output pb path"")\n    subparser.add_argument(""--dest_nodes"", help=""dest nodes"", default=None, action=""append"")\n    subparser.add_argument(""--src_nodes"", help=""source nodes"", default=None, action=""append"")\n    subparser.add_argument(\n        ""--name_prefix"",\n        help=""prefix of name scope, incompatible with dest_nodes/src_nodes"",\n        default=None\n    )\n    subparser.set_defaults(func=main.extract_sub_graph)\n\n    if len(sys.argv) <= 2:\n        parser.print_help()\n        sys.exit()\n\n    (args, unknown) = parser.parse_known_args()\n\n    func = args.func\n    del args.func\n\n    args = dict(filter(lambda x: x[1], vars(args).items()))\n    func(**args)\n'"
tools/tfgraph.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Simple tool to guess inputs and outputs of a tensorflow model.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nfrom collections import Counter\n\nimport tensorflow as tf\n\nIGNORE_INPUT = [""Const"", ""ConstV2"", ""Variable"", ""VariableV2"", ""RestoreV2"", ""Restore""]\nIGNORE_OUTPUT = [""NoOp"", ""Assign"", ""TensorSummaryV2"", ""Placeholder""]\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--input"", required=True, help=""input model"")\n    return parser.parse_args()\n\n\ndef cleanup_io_name(name):\n    """"""Cleanup op names.""""""\n    pos = name.find("":"")\n    if pos >= 0:\n        return name[:pos]\n    return name\n\n\ndef main():\n    args = get_args()\n\n    op_cnt = Counter()\n    attr_cnt = Counter()\n\n    graph_def = tf.GraphDef()\n    with tf.gfile.FastGFile(args.input, \'rb\') as f:\n        graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as g:\n        tf.import_graph_def(graph_def, name=\'\')\n    with tf.Session(graph=g):\n        inputs = []\n        outputs = []\n        ops = g.get_operations()\n        input_nodes = {}\n        shapes = {}\n        for node in ops:\n            for i in node.inputs:\n                input_nodes[i.name] = 1\n            for i in node.control_inputs:\n                input_nodes[i.name] = 1\n            for i in node.outputs:\n                try:\n                    shape = i.get_shape().as_list()\n                    shapes[i.name] = shape\n                except:  # pylint: disable=bare-except\n                    pass\n        for node in ops:\n            for i in node.outputs:\n                if i.name not in input_nodes:\n                    outputs.append(i.name)\n            if not node.inputs and not node.control_inputs and node.type not in IGNORE_INPUT:\n                if node.outputs:\n                    inputs.append(node.outputs[0].name)\n            if node.type in [""PlaceHolder""]:\n                inputs.append(node.outputs[0].name)\n            op_cnt[node.type] += 1\n            for a in node.node_def.attr:\n                attr_cnt[a] += 1\n\n    print(""Ops: {}"".format(op_cnt))\n    print(""Attr: {}"".format(attr_cnt))\n    print()\n    for i in inputs:\n        print(""input: {} shape={}"".format(i, shapes.get(i)))\n    for i in outputs:\n        print(""output: {} shape={}"".format(i, shapes.get(i)))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tf2onnx/custom_opsets/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n"""""" custom tf2onnx mapping functions. """"""\n\nfrom . import ms\nfrom . import onnx_ml\n'"
tf2onnx/custom_opsets/ms.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n"""""" tf2onnx mapping functions for ms domain. """"""\n\nimport numpy as np\n\nfrom onnx import onnx_pb\nfrom onnx.onnx_pb import TensorProto\nfrom tf2onnx import constants, utils\nfrom tf2onnx.handler import tf_op\nfrom tf2onnx.onnx_opset import controlflow\nfrom tf2onnx.onnx_opset.nn import conv_convert_inputs, conv_dims_attr\n\n\n# pylint: disable=unused-argument,missing-docstring\n\ndef make_range(ctx, start, limit, delta, output, scope_name, shape, dtype):\n    if all(ctx.get_node_by_output(n).is_const() for n in [start, limit, delta]) is True:\n        controlflow.make_range_const(ctx, start, limit, delta, output, scope_name, shape, dtype)\n    else:\n        _make_range_non_const(ctx, start, limit, delta, output, scope_name, shape, dtype)\n\n\ndef _make_range_non_const(ctx, start, limit, delta, output, scope_name, shape, dtype):\n    utils.make_sure(\n        dtype in [TensorProto.FLOAT, TensorProto.DOUBLE, TensorProto.INT16, TensorProto.INT32, TensorProto.INT64],\n        ""dtype %s is not supported"", dtype)\n    ctx.make_node(""Range"", [start, limit, delta], outputs=[output], name=scope_name, shapes=[shape], dtypes=[dtype],\n                  domain=constants.MICROSOFT_DOMAIN)\n\n\n@tf_op(""Range"", domain=constants.MICROSOFT_DOMAIN)\nclass Range:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        """"""Range.""""""\n        # T range = Range(T start, T limit, T delta)\n        dtype = node.get_attr_int(""Tidx"")\n        shape = node.output_shapes[0]\n        utils.make_sure(dtype is not None, ""Tidx of %s is None"", node.name)\n        ctx.remove_node(node.name)\n        make_range(ctx, node.input[0], node.input[1], node.input[2], node.output[0], node.name, shape, dtype)\n\n\n@tf_op(""Conv2DBackpropInput"", domain=constants.MICROSOFT_DOMAIN, onnx_op=""ConvTransposeWithDynamicPads"")\nclass ConvTransposeWithDynamicPads:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = Conv2DBackpropInput(int32 input_sizes, T filter, T out_backprop,\n        #    @list(int) strides, @bool use_cudnn_on_gpu, @string padding, @string data_format, @list(int) dilations)\n        # T Y = ConvTranspose(T X, T W, T B, T pads, @STRING auto_pad, @INTS dilations,\n        #    @INT group, @INTS kernel_shape, @INTS output_shape, @INTS strides)\n\n        # tf uses ""output_shape"" while onnx uses ""pads"", the equation to calculate pads is:\n        # total_padding[i] = stride[i] * (input_shape[i] - 1)+ kernel_shape[i] - output_shape[i]\n        # pads[i_begin] = total_padding[i]/2\n        # pads[i_end] = total_padding[i] - (total_padding[i]/2)\n        # output dtype of onnx ""shape"" is int64 while in tf dtype could be specified\n        utils.make_sure(node.is_nhwc(), ""only support NHWC for now"")\n        node.domain = constants.MICROSOFT_DOMAIN\n        input_shape = ctx.make_node(""Shape"", [node.input[2]])\n        hw_indices = ctx.make_const(utils.make_name(""hw_indices""), np.array([1, 2]).astype(np.int64))\n        input_shape_hw = ctx.make_node(""Gather"", [input_shape.output[0], hw_indices.output[0]])\n        output_shape = node.inputs[0]\n        if ctx.get_dtype(output_shape.output[0]) != onnx_pb.TensorProto.INT64:\n            output_shape = ctx.make_node(""Cast"", [output_shape.output[0]], attr={""to"": onnx_pb.TensorProto.INT64})\n        output_shape_hw = ctx.make_node(""Gather"", [output_shape.output[0], hw_indices.output[0]])\n        kernel_shape_hw = list(ctx.get_shape(node.input[1]))[0:2]\n        kernel_shape = ctx.make_const(utils.make_name(""const_convtrans""), np.array(kernel_shape_hw).astype(np.int64))\n        strides = conv_dims_attr(node, ""strides"")\n        utils.make_sure(len(strides) == 2, ""only stride of H and W needed"")\n\n        stride_node = ctx.make_const(utils.make_name(""const_convtrans""), np.array(strides).astype(np.int64))\n        const_one = ctx.make_const(utils.make_name(""cosnt_one""), np.array([1]).astype(np.int64))\n        const_two = ctx.make_const(utils.make_name(""cosnt_two""), np.array([2]).astype(np.int64))\n\n        tmp0 = ctx.make_node(""Sub"", [input_shape_hw.output[0], const_one.output[0]])\n        tmp1 = ctx.make_node(""Mul"", [stride_node.output[0], tmp0.output[0]])\n        tmp2 = ctx.make_node(""Add"", [tmp1.output[0], kernel_shape.output[0]])\n        total_pads = ctx.make_node(""Sub"", [tmp2.output[0], output_shape_hw.output[0]],\n                                   dtypes=[onnx_pb.TensorProto.INT64])\n        pads_beg = ctx.make_node(""Div"", [total_pads.output[0], const_two.output[0]], dtypes=[onnx_pb.TensorProto.INT64])\n        pads_end = ctx.make_node(""Sub"", [total_pads.output[0], pads_beg.output[0]])\n        pads = ctx.make_node(""Concat"", [pads_beg.output[0], pads_end.output[0]], attr={""axis"": 0})\n        # set node\'s attrs, Note: output_padding, group are left default.\n        conv_dims_attr(node, ""dilations"")\n        # set node\'s inputs from (output_shape, filter, input_tensor) to (input_tensor, filter, pads, Bias)\n        node.input[0] = node.input[2]\n        node.input[2] = pads.output[0]\n        conv_convert_inputs(ctx, node, with_kernel=True)\n        node.attr.pop(""data_format"")\n        node.attr.pop(""padding"")\n        if ""explicit_paddings"" in node.attr:\n            node.attr.pop(""explicit_paddings"")\n\n@tf_op(""CropAndResize"", domain=constants.MICROSOFT_DOMAIN)\nclass CropAndResize:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        """""" utilize contrib cropandresize """"""\n        node.attr[\'method\'].name = \'mode\'\n        node.domain = constants.MICROSOFT_DOMAIN\n        ctx.insert_new_node_on_input(node, ""Transpose"", node.input[0], perm=constants.NHWC_TO_NCHW)\n        ctx.insert_new_node_on_output(""Transpose"", node.output[0], node.name + \'_transposed\',\n                                      None, perm=constants.NCHW_TO_NHWC)\n\n@tf_op(""MatrixInverse"", domain=constants.MICROSOFT_DOMAIN, onnx_op=""Inverse"")\nclass Inverse:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        utils.make_sure(node.get_attr(\'adjoint\').i == 0, ""adjoint must be false"")\n        del node.attr[""adjoint""]\n        node.domain = constants.MICROSOFT_DOMAIN\n'"
tf2onnx/custom_opsets/onnx_ml.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n"""""" tf2onnx mapping functions for onnx ml domain. """"""\nfrom tf2onnx import constants\nfrom tf2onnx.handler import tf_op\n\n\n# pylint: disable=unused-argument,missing-docstring,unnecessary-pass\n\n@tf_op(""HashTableV2"")\nclass HashTable:\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        """""" HashTable will be removed """"""\n        pass\n\n\n@tf_op(""LookupTableFindV2"")\nclass LookupTableFind:\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        """""" convert lookup to category mapper """"""\n        table_node = node.inputs[0]\n        file_path = table_node.get_attr_value(""shared_name"")[11:-6]\n        cats_int64s = []\n        cats_strings = []\n        with open(file_path, \'r\') as f:\n            for i, s in enumerate(f.readlines()):\n                cats_int64s.append(i)\n                cats_strings.append(s.strip())\n        node_name = node.name\n        node_inputs = node.input\n        node_outputs = node.output\n        ctx.remove_node(node.name)\n        new_node = ctx.make_node(""CategoryMapper"", domain=constants.AI_ONNX_ML_DOMAIN,\n                                 name=node_name, inputs=node_inputs[1: 2], outputs=node_outputs,\n                                 attr={\'cats_int64s\': cats_int64s, \'cats_strings\': cats_strings})\n        ctx.set_shape(new_node.name + "":0"", [-1])\n        customer_nodes = ctx.find_output_consumers(table_node.output[0])\n        if len(customer_nodes) == 0:\n            ctx.remove_node(table_node.name)\n'"
tf2onnx/onnx_opset/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n""""""tf2onnx.onnx_opset module""""""\n\nfrom . import common, controlflow, generator, logical, math, misc, nn, reduction, rnn, tensor, traditionalml\n'"
tf2onnx/onnx_opset/common.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ncommon\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nfrom tf2onnx import constants\n\n\nlogger = logging.getLogger(__name__)\n\n# pylint: disable=unused-argument,missing-docstring\n\nclass BroadcastOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        """"""Elementwise Ops with broadcast flag.""""""\n        if node.type == ""AddV2"":\n            node.type = ""Add""\n        shape0 = ctx.get_shape(node.input[0])\n        shape1 = ctx.get_shape(node.input[1])\n        if shape0 != shape1:\n            node.set_attr(""broadcast"", 1)\n            # this works around shortcomings in the broadcasting code\n            # of caffe2 and winml/rs4.\n            if ctx.is_target(constants.TARGET_RS4):\n                # in rs4 mul and add do not support scalar correctly\n                if not shape0:\n                    if node.inputs[0].is_const():\n                        shape0 = node.inputs[0].scalar_to_dim1()\n                if not shape1:\n                    if node.inputs[1].is_const():\n                        shape1 = node.inputs[1].scalar_to_dim1()\n            if shape0 and shape1 and len(shape0) < len(shape1) and node.type in [""Mul"", ""Add""]:\n                tmp = node.input[0]\n                node.input[0] = node.input[1]\n                node.input[1] = tmp\n        else:\n            node.set_attr(""broadcast"", 0)\n\n    @classmethod\n    def version_6(cls, ctx, node, **kwargs):\n        """"""Elementwise Ops with broadcast flag.""""""\n        if node.type == ""AddV2"":\n            node.type = ""Add""\n        shape0 = ctx.get_shape(node.input[0])\n        shape1 = ctx.get_shape(node.input[1])\n        if shape0 != shape1:\n            # this works around shortcomings in the broadcasting code\n            # of caffe2 and winml/rs4.\n            if ctx.is_target(constants.TARGET_RS4):\n                # in rs4 mul and add do not support scalar correctly\n                if not shape0:\n                    if node.inputs[0].is_const():\n                        shape0 = node.inputs[0].scalar_to_dim1()\n                if not shape1:\n                    if node.inputs[1].is_const():\n                        shape1 = node.inputs[1].scalar_to_dim1()\n            if shape0 and shape1 and len(shape0) < len(shape1) and node.type in [""Mul"", ""Add""]:\n                tmp = node.input[0]\n                node.input[0] = node.input[1]\n                node.input[1] = tmp\n'"
tf2onnx/onnx_opset/controlflow.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ncontrolflow\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport logging\nimport sys\n\nimport numpy as np\n\nfrom onnx import onnx_pb\nfrom onnx.onnx_pb import TensorProto\nfrom tf2onnx import utils\nfrom tf2onnx.handler import tf_op\nfrom tf2onnx.utils import make_sure\nfrom tf2onnx.tf_loader import find_function\n\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=unused-argument,missing-docstring\n\ndef get_inputs_for_current_iteration(g, input_id, iter_index):\n    cond_gather_node = g.make_node(""Gather"", [input_id, iter_index])\n    cur_cond_val_scalar_node = g.make_node(""Squeeze"", [cond_gather_node.output[0]], attr={""axes"": [0]})\n    return cur_cond_val_scalar_node.output[0]\n\n\ndef create_loop_body_graph(parent_g, gather_input_ids, output_data_type, output_shape, trip_count_input_ids,\n                           rank, loop_name):\n    g = parent_g.create_new_graph_with_same_config()\n    g.parent_graph = parent_g\n    iter_name = utils.make_name(""i"")\n    cond_name = utils.make_name(""cond"")\n    fake_var_name = utils.make_name(""fake_var"")\n\n    g.add_graph_input(iter_name, TensorProto.INT64, (1,))  # iteration_num\n    g.add_graph_input(cond_name, TensorProto.BOOL, ())  # condition\n    g.add_graph_input(fake_var_name, TensorProto.FLOAT, ())  # loop-carried dependency\n\n    # get the i\'th value of condition\n    cond_input_id = gather_input_ids[0]\n    cond_input_id_for_current_iter = get_inputs_for_current_iteration(g, cond_input_id, iter_name)\n\n    # get the i\'th value of true values\n    true_input_id = gather_input_ids[1]\n    true_input_id_for_current_iter = get_inputs_for_current_iteration(g, true_input_id, iter_name)\n\n    # get the i\'th value of false values\n    false_input_id = gather_input_ids[2]\n    false_input_id_for_current_iter = get_inputs_for_current_iteration(g, false_input_id, iter_name)\n\n    input_ids_for_current_iter = [cond_input_id_for_current_iter, true_input_id_for_current_iter,\n                                  false_input_id_for_current_iter]\n    output_id = None\n    rank -= 1\n    if rank >= 1:\n        loop_1 = create_loop_op(g, input_ids_for_current_iter, output_data_type, output_shape[1:],\n                                trip_count_input_ids, rank)\n        output_id = loop_1.output[1]\n    elif rank == 0:\n        _, if_node_output_id = create_if_op(g, input_ids_for_current_iter, output_data_type, output_shape[1:])\n        output_id = if_node_output_id\n\n    output_identity_name = utils.make_name(""loop_output"")\n    loop_output_id = utils.port_name(output_identity_name)\n    g.make_node(\n        \'Identity\',\n        [output_id],\n        outputs=[loop_output_id],\n        name=output_identity_name\n    )\n\n    cond_identity_name = utils.make_name(""cond_output"")\n    cond_output_id = utils.port_name(cond_identity_name)\n    g.make_node(\n        \'Identity\',\n        [cond_name],\n        outputs=[cond_output_id],\n        name=cond_identity_name\n    )\n\n    fake_var_identity_name = utils.make_name(""fake_var_output"")\n    fake_var_output_id = utils.port_name(fake_var_identity_name)\n    g.make_node(\n        \'Identity\',\n        [fake_var_name],\n        outputs=[fake_var_output_id],\n        name=fake_var_identity_name\n    )\n\n    g.add_graph_output(cond_output_id, TensorProto.BOOL, ())\n    g.add_graph_output(fake_var_output_id, TensorProto.FLOAT, ())\n\n    # use None for all dims, just keep original rank. Because it is observed, dims might be changed in loop.\n    g.add_graph_output(loop_output_id, output_data_type, utils.create_vague_shape_like(output_shape[1:]))\n\n    return g\n\n\ndef create_if_op(g, input_ids, output_data_type, output_shape):\n    op_name = utils.make_name(""If"")\n    true_graph = create_body_graph_for_if_branch(g, output_data_type, output_shape, input_ids[1], op_name)\n    false_graph = create_body_graph_for_if_branch(g, output_data_type, output_shape, input_ids[2], op_name)\n    out_name = utils.port_name(op_name)\n\n    # output a scalar\n    if_node = g.make_node(""If"", [input_ids[0]], outputs=[out_name], name=op_name, skip_conversion=True)\n    if_node.set_body_graph_as_attr(""then_branch"", true_graph)\n    if_node.set_body_graph_as_attr(""else_branch"", false_graph)\n    return if_node, out_name\n\n\ndef create_body_graph_for_if_branch(parent_g, data_type, output_shape, chosen_cur_cond_val_out_name, op_name):\n    g = parent_g.create_new_graph_with_same_config()\n    g.parent_graph = parent_g\n    name = utils.make_name(""Identity"")\n    g.make_node(\n        \'Identity\',\n        inputs=[chosen_cur_cond_val_out_name],\n        outputs=[\'y\'],\n        name=name\n    )\n    g.add_graph_output(""y"", data_type, utils.create_vague_shape_like(output_shape))\n    return g\n\n\n# gather_input_ids is 1-D tensor, containing 3 elements:\n# 0: condition data to gather on\n# 1: true result to gather on\n# 2: false result to gather on\ndef create_loop_op(g, gather_input_ids, output_type, output_shape, trip_count_input_ids, rank):\n    cond_var_name = utils.make_name(""cond_var"")\n    g.make_const(cond_var_name, np.array(True, dtype=np.bool))\n\n    # Loop requires at least a variable, add a useless fake variable.\n    fake_val_name = utils.make_name(""fake_var"")\n    g.make_const(fake_val_name, np.array(0.0, dtype=np.float32))\n\n    if rank < 1:\n        raise ValueError(""rank is < 1"")\n    trip_count_input_id = trip_count_input_ids[-1 * rank]\n\n    loop_inputs = [trip_count_input_id,  # trip count\n                   cond_var_name,  # termination condition\n                   fake_val_name  # initial value of loop-carried dependencies\n                   ]\n    # define an extra scan output\n    loop_node = g.make_node(""Loop"", loop_inputs, output_count=2, op_name_scope=""select_loop"",\n                            skip_conversion=False)\n    loop_body = create_loop_body_graph(g, gather_input_ids, output_type, output_shape, trip_count_input_ids,\n                                       rank, loop_node.name)\n    loop_node.set_body_graph_as_attr(""body"", loop_body)\n    return loop_node\n\n\ndef make_range_const(ctx, start, limit, delta, output, scope_name, shape, dtype):\n    """"""make Range subgraph if all inputs are const.""""""\n    # T range = Range(T start, T limit, T delta)\n    # V v_final_and_scan_outputs = Loop(int64 M, B cond, V v_initial)\n    base_name = utils.make_name(scope_name)\n    start = ctx.get_node_by_output(start).get_tensor_value(as_list=False)\n    limit = ctx.get_node_by_output(limit).get_tensor_value(as_list=False)\n    delta = ctx.get_node_by_output(delta).get_tensor_value(as_list=False)\n    val = np.arange(start, limit, delta, dtype=start.dtype)\n    const_range = ctx.make_const(base_name, val)\n    ctx.make_node(""Identity"", [const_range.output[0]], shapes=[shape], dtypes=[dtype], outputs=[output])\n\n\ndef make_range_non_const(ctx, start, limit, delta, output, scope_name, shape, dtype):\n    """"""make Range subgraph.""""""\n    # T range = Range(T start, T limit, T delta)\n    # V v_final_and_scan_outputs = Loop(int64 M, B cond, V v_initial)\n    base_name = utils.make_name(scope_name)\n\n    # trip_count\n    diff_node = ctx.make_node(""Sub"",\n                              [limit, start],\n                              op_name_scope=base_name,\n                              name=utils.make_name(""diff""))\n    diff_output = diff_node.output[0]\n\n    delta_cast = delta\n    if dtype in [TensorProto.INT32, TensorProto.INT64]:\n        cast_node = ctx.make_node(""Cast"", [diff_output], op_name_scope=base_name,\n                                  name=""cast_diff"", attr={""to"": TensorProto.FLOAT})\n        diff_output = cast_node.output[0]\n\n        cast_node = ctx.make_node(""Cast"", [delta], op_name_scope=base_name, name=""cast_delta"",\n                                  attr={""to"": TensorProto.FLOAT})\n        delta_cast = cast_node.output[0]\n    div_node = ctx.make_node(""Div"", [diff_output, delta_cast], op_name_scope=base_name, name=""div"")\n    ceil_node = ctx.make_node(""Ceil"", [div_node.output[0]], op_name_scope=base_name, name=""ceil"")\n    trip_count_node = ctx.make_node(""Cast"", [ceil_node.output[0]], op_name_scope=base_name, name=""trip_cnt"",\n                                    attr={""to"": TensorProto.INT64})\n\n    # cond\n    # Use initializer here since Constant OP before opset 9 does not support bool type\n    cond_name = ""{}_cond"".format(base_name)\n    ctx.make_const(cond_name, np.ones((), dtype=bool))\n\n    # body\n    g = ctx.create_new_graph_with_same_config()\n    g.parent_graph = ctx\n    g.add_graph_input(""i"", TensorProto.INT64, [])\n    g.add_graph_input(""cond"", TensorProto.BOOL, [])\n    g.add_graph_input(""prev"", dtype, [])\n\n    g.make_node(""Identity"", [""cond""], outputs=[""cond_out""])\n    g.make_node(""Add"", [""prev"", delta], outputs=[""current""], name=utils.make_name(""add""))\n    g.make_node(""Identity"", [""prev""], outputs=[""range""])\n\n    g.add_graph_output(""cond_out"", TensorProto.BOOL, [])\n    g.add_graph_output(""current"", dtype, [])\n    g.add_graph_output(""range"", dtype, [])\n\n    # loop\n    loop_inputs = [trip_count_node.output[0], cond_name, start]\n    loop_node = ctx.make_node(""Loop"", loop_inputs, output_count=2, op_name_scope=base_name, name=""loop"")\n    loop_node.set_body_graph_as_attr(""body"", g)\n\n    ctx.make_node(""Identity"", [loop_node.output[1]], name=base_name, shapes=[shape], dtypes=[dtype], outputs=[output])\n\n\ndef make_range(ctx, start, limit, delta, output, scope_name, shape, dtype):\n    if all(ctx.get_node_by_output(n).is_const() for n in [start, limit, delta]) is True:\n        make_range_const(ctx, start, limit, delta, output, scope_name, shape, dtype)\n    else:\n        make_range_non_const(ctx, start, limit, delta, output, scope_name, shape, dtype)\n\n\n@tf_op([""Loop"", ""Scan""])\nclass PassThroughOp:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        pass\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # no change needed\n        # loop has 1 less mandatory input\n        # if = only doc changes\n        # scan has 1 less mandatory input and 4 extra attrs\n        pass\n\n\n@tf_op(""Range"")\nclass Range:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        """"""Range.""""""\n        # T range = Range(T start, T limit, T delta)\n        # V v_final_and_scan_outputs = Loop(int64 M, B cond, V v_initial)\n        dtype = node.get_attr_int(""Tidx"")\n        shape = node.output_shapes[0]\n        utils.make_sure(dtype is not None, ""Tidx of %s is None"", node.name)\n        ctx.remove_node(node.name)\n        make_range(ctx, node.input[0], node.input[1], node.input[2],\n                   node.output[0], node.name, shape, dtype)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # opset 11 implements Range op explicitly\n        pass\n\n\n@tf_op([""Select"", ""SelectV2""])\nclass Select:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # T output = Select(bool condition, T x, T y)\n        # Select_res = Add(Multiply(Cast(bool condition, float32), T x,),\n        #                  Multiply(Cast(Not(bool condition), float32), T y)).\n        utils.make_sure(len(node.input) > 1, ""Select with only condition is not supported."")\n        positive_cast = ctx.make_node(""Cast"", [node.input[0]], name=utils.make_name(node.name),\n                                      attr={""to"": TensorProto.FLOAT})\n        negative = ctx.make_node(""Not"", [node.input[0]], name=utils.make_name(node.name))\n        negative_cast = ctx.make_node(""Cast"", [negative.output[0]], name=utils.make_name(node.name),\n                                      attr={""to"": TensorProto.FLOAT})\n        multiply_1 = ctx.make_node(""Mul"", [positive_cast.output[0], node.input[1]], name=utils.make_name(node.name))\n        multiply_2 = ctx.make_node(""Mul"", [node.input[2], negative_cast.output[0]], name=utils.make_name(node.name))\n        add_name = node.name\n        add_out = node.output\n        dtype = ctx.get_dtype(node.output[0])\n        shape = ctx.get_shape(node.output[0])\n        ctx.remove_node(node.name)\n        ctx.make_node(""Add"", [multiply_1.output[0], multiply_2.output[0]], outputs=add_out, name=add_name,\n                      dtypes=[dtype], shapes=[shape])\n\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        # T output = Select(bool condition, T x, T y)\n        # V v_final_and_scan_outputs = Loop(int64 M, B cond, V v_initial)\n        utils.make_sure(len(node.input) > 1, ""Select with only condition is not supported."")\n\n        true_data_type = ctx.get_dtype(node.input[1])\n        true_data_shape = ctx.get_shape(node.input[1])\n        make_sure(true_data_type is not None, ""select true data dtype cannot be None"")\n        make_sure(true_data_shape is not None, ""select true data shape cannot be None"")\n\n        condition_shape = ctx.get_shape(node.input[0])\n        utils.make_sure(condition_shape is not None, ""Shape of {} is None"".format(node.input[0]))\n        rank = len(condition_shape)\n\n        utils.make_sure(rank >= 0, ""rank should be >= 0"")\n        val_output_id = None\n        if rank > 0:\n            # create nodes getting shape of condition\n            shape_node_output_shape = [rank]\n            shape_node = ctx.make_node(""Shape"", [node.input[0]], op_name_scope=node.name,\n                                       shapes=[shape_node_output_shape], dtypes=[TensorProto.INT64])\n\n            # todo(pengwa), move those leveraging rewrite_incomplete_type_support_onnxruntime after shape inferencing\n            # bug is fixed.\n            # workaround: onnxruntime does not support Split-2, add cases before and after.\n            target_dtype = TensorProto.FLOAT\n            shape_f_node = ctx.make_node(""Cast"", [shape_node.output[0]], attr={""to"": target_dtype},\n                                         shapes=[shape_node_output_shape], dtypes=[target_dtype],\n                                         op_name_scope=node.name)\n\n            split_attr = [1 for i in range(rank)]\n            output_shapes = [[1] for i in range(rank)]\n            output_dtypes = [target_dtype for i in range(rank)]\n            split_node = ctx.make_node(""Split"", [shape_f_node.output[0]], output_count=rank,\n                                       attr={""split"": split_attr}, shapes=output_shapes,\n                                       dtypes=output_dtypes, op_name_scope=node.name)\n\n            trip_cnts = []\n            for i in range(rank):\n                output_id = split_node.output[i]\n                output_shape = ctx.get_shape(output_id)\n                target_dtype = TensorProto.INT64\n                shape_i_node = ctx.make_node(""Cast"", [output_id], attr={""to"": target_dtype},\n                                             shapes=[output_shape], dtypes=[target_dtype],\n                                             op_name_scope=node.name)\n                trip_cnts.append(shape_i_node.output[0])\n            # workaround ends\n\n            loop_node = create_loop_op(ctx, node.input, true_data_type, true_data_shape, trip_cnts, rank)\n\n            val_output_id = loop_node.output[1]\n        elif rank == 0:\n            _, val_output_id = create_if_op(ctx, node.input, true_data_type, true_data_shape)\n\n        ctx.copy_shape(node.output[0], val_output_id)\n        ctx.set_dtype(node.output[0], true_data_type)\n        ctx.remove_node(node.name)\n        ctx.make_node(""Identity"", [val_output_id], outputs=node.output,\n                      shapes=[ctx.get_shape(val_output_id)], dtypes=[true_data_type])\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        # T output = Select(bool condition, T x, T y)\n        # T1 output = Where(bool condition, T1 x, T1 y)\n        # NOTE: condition can be 1-dimension in tensorflow, while in onnx,\n        # it should be broadcastable with other two inputs\n        node.type = ""Where""\n        cond_shape = ctx.get_shape(node.input[0])\n        make_sure(cond_shape is not None, ""shape of {} is None"".format(node.input[0]))\n        input_shape = ctx.get_shape(node.input[1])\n        if input_shape is None:\n            input_shape = ctx.get_shape(node.input[2])\n        make_sure(input_shape is not None, ""input shape of {} is None"".format(node.name))\n        input_rank = len(input_shape)\n        # if cond shape is 1-dimensional while input has higher rank, need to be reshaped to broadcast\n        if len(cond_shape) == 1 and input_rank > 1:\n            broadcast_shape = [cond_shape[0]] + [1] * (input_rank - 1)\n            shape_const = ctx.make_const(utils.make_name(node.name), np.array(broadcast_shape, dtype=np.int64))\n            reshape = ctx.make_node(""Reshape"", [node.input[0], shape_const.output[0]])\n            ctx.replace_input(node, node.input[0], reshape.output[0])\n\n\n@tf_op(""Where"")\nclass Where:\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        # T_y output = Where(T_x condition), return indices of elements whose value are True\n        node.type = ""NonZero""\n        # in onnx, indices are returned in this way [[ind_a_0, ind_b_0, ...], [ind_a_1, ind_b_1,...]];\n        # while in tf, the result will be [[ind_a_0, ind_a_1, ...], [ind_b_0, ind_b_1, ...], ...]\n        # this is the reason a transpose node inserted here.\n        transpose_node = ctx.insert_new_node_on_output(""Transpose"",\n                                                       node.output[0], name=utils.make_name(""where_op_added""))\n        ctx.copy_shape(node.output[0], transpose_node.output[0])\n        ctx.copy_dtype(node.output[0], transpose_node.output[0])\n\n\n@tf_op([""StatelessIf""])\nclass StatelessIfOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        """"""V2 control flow - If""""""\n        inputs = node.input[1:]\n\n        output_shapes = node.output_shapes\n        output_dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n\n        # replace the original node\n        if_node = ctx.make_node(""If"", node.input[:1], name=node.name, output_count=len(output_shapes),\n                                shapes=output_shapes, dtypes=output_dtypes, skip_conversion=True)\n\n        for branch in [""then_branch"", ""else_branch""]:\n            func_name = node.get_attr_str(branch)\n            g = find_function(func_name)\n            g.parent_graph = ctx\n            wire_if_branch(ctx, g, inputs, output_shapes, output_dtypes, func_name, node.name)\n            if_node.set_body_graph_as_attr(branch, g)\n\n\n@tf_op([""If""])\nclass IfOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        """"""V2 control flow - If""""""\n        inputs = node.input[1:]\n\n        if node.type == ""If"" and len(inputs) == 0:\n            # this comes from the re-writers\n            return\n\n        output_shapes = node.output_shapes\n        output_dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n\n        # replace the original node\n        if_node = ctx.make_node(""If"", node.input[:1], name=node.name, output_count=len(output_shapes),\n                                shapes=output_shapes, dtypes=output_dtypes, skip_conversion=True)\n\n        for branch in [""then_branch"", ""else_branch""]:\n            func_name = node.get_attr_str(branch)\n            g = find_function(func_name)\n            g.parent_graph = ctx\n            wire_if_branch(ctx, g, inputs, output_shapes, output_dtypes, func_name, node.name)\n            if_node.set_body_graph_as_attr(branch, g)\n\n\n@tf_op([""TensorListSetItem""])\nclass TensorListSetItem:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # handled in \'While\'\n        pass\n\n\n@tf_op([""TensorListGetItem""])\nclass TensorListGetItem:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        ctx.ta_reads.append(node.input[0])\n        node.type = ""Gather""\n        node.input = [node.input[0], node.input[1]]\n        ctx.insert_new_node_on_input(node, ""Unsqueeze"", node.input[1], name=node.child_name(), axes=[0])\n        ctx.insert_new_node_on_output(""Squeeze"", node.output[0], name=node.child_name(), axes=[0])\n\n\n@tf_op([""TensorListLength""])\nclass TensorListLength:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op([""TensorListReserve"", ""TensorListResize""])\nclass TensorListReserve:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op([""TensorListFromTensor""])\nclass TensorListFromTensor:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        consumers = ctx.find_output_consumers(node.output[0])\n        if any([c.is_while() for c in consumers]):\n            node.type = ""Identity""\n            ctx.copy_dtype(node.input[0], node.output[0])\n            ctx.copy_shape(node.input[0], node.output[0])\n\n\n@tf_op([""TensorListStack""])\nclass TensorListStack:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        if node.inputs[0].is_while():\n            ctx.remove_node(node.name)\n            ctx.replace_all_inputs(ctx.get_nodes(), node.output[0], node.input[0])\n\n\n@tf_op([""While"", ""StatelessWhile""])\nclass While:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # the tensorflow while input is:\n        #   loop_counter, max_iterations, [loop_vars]\n        # cond and body use the same inputs\n        # outputs are identical to inputs\n        tf_while_inputs = node.input\n\n        # the onnx loop input is:\n        #   max_iterations, cond, [loop_vars]\n        # body uses the inputs:\n        #   iteration, cond, [loop_vars]\n        # the onnx loop output is:\n        #   cond [v_final_and_scan_outputs]\n\n        output_shapes = node.output_shapes\n        output_dtypes = node.output_dtypes\n\n        # make maximum_iterations int64 and replace -1(tf) with maxsize(onnx)\n        maximum_iterations_name = node.input[1]\n        maximum_iterations = node.inputs[1].get_tensor_value()\n        ctx.remove_node(node.inputs[1].name)\n        if maximum_iterations == -1:\n            maximum_iterations = sys.maxsize\n        ctx.make_const(maximum_iterations_name, np.array(maximum_iterations, dtype=np.int64))\n\n        cond_name = node.get_attr_str(""cond"")\n        cond_graph = find_function(cond_name)\n        cond_graph.parent_graph = ctx\n\n        body_name = node.get_attr_str(""body"")\n        body = find_function(body_name)\n        body.parent_graph = ctx\n\n        loop_vars = [] # passed into the loop\n        state_vars = {} # comes from outer context\n        to_remove = []\n        input_idx_to_remove = []\n        # remove TensorListReserve\n        for idx, name in enumerate(tf_while_inputs):\n            if idx == 1:\n                # onnx does not know maximum_iterations in the body so move this to a state var\n                state_vars[body.func_inputs[idx]] = maximum_iterations_name\n                continue\n            if idx < 2:\n                # skip  [0,1] loop_counter, max_iterations\n                continue\n            n = node.inputs[idx]\n            if n.type in [""TensorListReserve"", ""TensorListResize""]:\n                # there is no equivalent step in onnx and we should remove it.\n                # But we make this an identity to keep the loop_vars the same on input and output\n                # of the body but there should be no access to this argument in the body.\n                to_remove.append((idx, n))\n                continue\n\n            # tensor arrays we read from can\'t be loop_vars and we fetch them from the outer context instead\n            if body.func_inputs[idx] in body.ta_reads:\n                state_vars[body.func_inputs[idx]] = name\n                input_idx_to_remove.append(idx)\n            else:\n                loop_vars.append(name)\n\n        # loop_vars that become state_vars need to be removed from output as well\n        for idx in reversed(input_idx_to_remove):\n            del output_shapes[idx]\n            del output_dtypes[idx]\n            del body.outputs[idx]\n\n        # remove tensor array that are passed in to the loop\n        for idx, n in reversed(to_remove):\n            ctx.remove_node(n.name)\n            # make the node output bad\n            ctx.replace_all_inputs(ctx.get_nodes(), n.output[0], ""@@ALLOC"")\n            del body.func_inputs[idx]\n            del cond_graph.func_inputs[idx]\n            del tf_while_inputs[idx]\n\n        ctx.remove_node(node.name)\n\n        # In onnx \'cond\' is a variable, not a function. We need to inject the subgraph into the main graph\n        # before the loop and into the body.\n        cond_binding = parameter_binding(cond_graph, tf_while_inputs)\n        cond_outputs = inline_subgraph(ctx, cond_graph, cond_name, cond_binding)\n        # onnx Loop op outputs only loop_vars so we need shift output dtypes/shapes and consumers\n        output_map = {node.output[i+2]: node.output[i] for i in range(len(node.output) - 2)}\n        output_shapes = output_shapes[2:]\n        output_dtypes = output_dtypes[2:]\n\n        loop_node = ctx.make_node(""Loop"", [maximum_iterations_name, cond_outputs[0]] + loop_vars,\n                                  output_count=len(output_shapes), name=node.name,\n                                  shapes=output_shapes, dtypes=output_dtypes, skip_conversion=True)\n        # shift output consumers\n        for k, v in output_map.items():\n            ctx.replace_all_inputs(ctx.get_nodes(), k, v)\n\n        wire_while_body(ctx, body, loop_node.inputs, state_vars, output_shapes, output_dtypes, body_name,\n                        node.name, cond_graph, tf_while_inputs)\n\n        # if there was a tensorflow variant type, bind in a real type here\n        for i, n in enumerate(body.inputs):\n            if body.get_dtype(n.output[0]) == onnx_pb.TensorProto.UNDEFINED:\n                body.set_dtype(n.output[0], ctx.get_dtype(loop_node.input[i]))\n        loop_node.set_body_graph_as_attr(""body"", body)\n        # dump_graph(body)\n        # dump_graph(ctx)\n\n\ndef wire_while_body(parent_g, g, loop_node_inputs, state_vars, output_shapes, output_dtypes, scope, parent,\n                    cond_graph, tf_while_inputs):\n    """"""Wire subgraph graph into main.""""""\n    remove_parents = []\n    to_remove = []\n\n    # tensorflow function inputs that are state_vars come from outer context and\n    # we need to remove them from the inputs by makeing the placeholder an identity\n    for n in g.inputs:\n        if n.output[0] in state_vars:\n            n.type = ""Identity""\n            n.input = [state_vars[n.output[0]]]\n\n    # onnx will pass in cond as argument\n    cond_node = g.make_node(""Placeholder"", [], name=utils.make_name(""cond""),\n                            output_count=1, dtypes=[onnx_pb.TensorProto.BOOL], shapes=[[]])\n\n    # in onnx the body inputs are: index, cond, [loop_vars]\n    func_inputs = [i for i in g.func_inputs[2:] if i not in state_vars]\n    func_inputs = [g.func_inputs[0], cond_node.output[0]] + func_inputs\n    g.set_dtype(func_inputs[0], onnx_pb.TensorProto.INT64)\n    # tell graph lib to keep inputs in order\n    g._order_sensitive_inputs = \\\n        [g.get_node_by_output(name) for name in func_inputs]  # pylint: disable=protected-access\n\n    for p, c in zip(loop_node_inputs, func_inputs):\n        shape = p.output_shapes[0]\n        g.set_shape(c, shape)\n\n    for i, node in enumerate(g.inputs):\n        if node.output[0] not in func_inputs:\n            remove_parents.append(node.output[0])\n\n    # this is a tensor array write - make it an identity\n    for node in g.get_nodes():\n        if node.type == ""TensorListSetItem"":\n            remove_parents.append(node.input[0])\n            node.type = ""Identity""\n            g.set_shape(node.output[0], g.get_shape(node.input[2]))\n            g.set_dtype(node.output[0], g.get_dtype(node.input[2]))\n            node.input = [node.input[2]]\n\n    # remove all nodes feeding to TensorListSetItem\'s reserved tensor\n    while remove_parents:\n        output_name = remove_parents[0]\n        del remove_parents[0]\n        node = g.get_node_by_output(output_name)\n        if node:\n            if output_name not in func_inputs:\n                if node.input:\n                    remove_parents.extend(node.input)\n                g.remove_node(node.name)\n\n    for node in to_remove:\n        g.remove_node(node.name)\n\n    # we need to bind the the loop_var output, else we\'d do 1 too much\n    cond_binding = parameter_binding(cond_graph, func_inputs[:2] + g.outputs[2:])\n    cond_outputs = inline_subgraph(g, cond_graph, ""cond__"", cond_binding)\n\n    g.outputs = [cond_outputs[0]] + g.outputs[2:]\n\n    # FIXME: onnx does not have a variant type so we try to fish for the dtype in a prior TensorListSetItem.\n    for o in g.outputs:\n        if g.get_dtype(o) == onnx_pb.TensorProto.UNDEFINED:\n            node = g.get_node_by_output(o)\n            if node.type in [""Identity""]:\n                g.set_dtype(o, node.inputs[0].output_dtypes[0])\n\n    return g\n\n\ndef wire_if_branch(parent_g, g, inputs, output_shapes, output_dtypes, scope, parent):\n    """"""Wire subgraph graph into main.""""""\n    binding = parameter_binding(g, inputs)\n    to_remove = []\n    for node in g.inputs:\n        parent_name = binding.get(node.output[0])\n        if parent_name and parent_name != ""@@ALLOC"":\n            node.input = [parent_name]\n            node.type = ""Identity""\n        else:\n            to_remove.append(node)\n\n    for node in to_remove:\n        g.remove_node(node.name)\n\n    prefix_graph(g, scope)\n\n    for shape, dtype, output_name in zip(output_shapes, output_dtypes, g.outputs):\n        g.set_shape(output_name, shape)\n        g.set_dtype(output_name, dtype)\n\n    return g\n\n\ndef inline_subgraph(parent, g, scope, binding):\n    # make a copy since we don\'t want to change the origianl graph\n    g = copy.deepcopy(g)\n    to_remove = []\n    for node in g.inputs:\n        parent_name = binding.get(node.output[0])\n        if parent_name and parent_name != ""@@ALLOC"":\n            node.input = [parent_name]\n            node.type = ""Identity""\n        else:\n            to_remove.append(node)\n    for node in to_remove:\n        g.remove_node(node.name)\n    prefix_graph(g, scope)\n    for n in g.get_nodes():\n        dtypes = n.output_dtypes\n        shapes = n.output_shapes\n        n.graph = parent\n        for name, shape, dtype in zip(n.output, shapes, dtypes):\n            # FIXME: don\'t access this directly\n            parent._output_shapes[name] = shape  # pylint: disable=protected-access\n            parent._dtypes[name] = dtype  # pylint: disable=protected-access\n\n    ops = parent.get_nodes() + g.get_nodes()\n    parent.reset_nodes(ops)\n\n    # copy output shape and dtype to parent graph\n    for name in g.outputs:\n        parent.set_dtype(name, g.get_dtype(name))\n        parent.set_shape(name, g.get_shape(name))\n\n    return  g.outputs\n\n\ndef parameter_binding(g, inputs, state_vars=None):\n    binding = {}\n    for k, v in zip(g.func_inputs, inputs):\n        if state_vars:\n            v = state_vars.get(v, v)\n        binding[k] = v\n    return binding\n\n\ndef prefix_graph(g, scope):\n    ops = g.get_nodes()[:]\n    to_remove = []\n    for node in ops:\n        output_shapes = node.output_shapes\n        output_dtypes = node.output_dtypes\n        attr = node.attr\n        if node.is_graph_input():\n            continue\n        new_node = g.make_node(node.type, node.input, name=node.name, output_count=len(node.output),\n                               shapes=output_shapes, dtypes=output_dtypes, attr=attr,\n                               op_name_scope=scope, skip_conversion=True)\n        attr_graphs = node.get_body_graphs()\n        if attr_graphs:\n            for k, v in attr_graphs.items():\n                new_node.set_body_graph_as_attr(k, v)\n        for old_output, new_output in zip(node.output, new_node.output):\n            for i, oname in enumerate(g.outputs):\n                if old_output == oname:\n                    g.outputs[i] = new_output\n                    break\n            g.replace_all_inputs(ops, old_output, new_output)\n        to_remove.append(node)\n    for node in to_remove:\n        g.remove_node(node.name)\n\n\ndef dump_graph(g):\n    print()\n    print(""--, graph="", g.graph_name)\n    t = [""{} {}/{}"".format(n.name, g.get_shape(n.output[0]), g.get_dtype(n.output[0])) for n in g.inputs]\n    print(""--, inputs="", "", "".join(t))\n    t = [""{} {}/{}"".format(n, g.get_shape(n), g.get_dtype(n)) for n in g.outputs]\n    print(""--, outputs="", "", "".join(t))\n    for node in g.get_nodes():\n        input_names = "", "".join([""{} {}/{}"".format(n, g.get_shape(n), g.get_dtype(n)) for n in node.input])\n        output_names = "", "".join([""{} {}/{}"".format(n, g.get_shape(n), g.get_dtype(n)) for n in node.output])\n        print(""-- {} n={} i={} o={}"".format(node.type, node.name, input_names, output_names))\n'"
tf2onnx/onnx_opset/generator.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ngenerator\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nimport numpy as np\nfrom onnx import onnx_pb, numpy_helper\nfrom tf2onnx import utils\nfrom tf2onnx.handler import tf_op\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=unused-argument,missing-docstring\n\n@tf_op([""Const"", ""ConstV2""])\nclass DirectOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op([""RandomNormal"", ""RandomUniform""])\nclass RandomOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # in tf-2.0 grappler optimizes the graph pretty well and our matching logic\n        # in the rewriter does not trigger. grappler will send the random uniform\n        # with shape as input so we need to pickup the input here and if the shape is\n        # const we make it an attribute.\n        seed = node.get_attr(""seed"")\n        node.set_attr(""seed"", float(seed.f))\n        if len(node.input) > 0:\n            shape = node.inputs[0].get_tensor_value()\n            ctx.remove_input(node, node.input[0])\n            node.set_attr(""shape"", shape)\n            ctx.set_shape(node.output[0], shape)\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        if node.inputs[0].is_const():\n            cls.version_1(ctx, node, **kwargs)\n        else:\n            seed = node.get_attr(""seed"")\n            node.set_attr(""seed"", float(seed.f))\n            cast_node = ctx.make_node(""Cast"", node.input, attr={\'to\': onnx_pb.TensorProto.INT64})\n            const_node = ctx.make_node(""ConstantOfShape"", cast_node.output)\n            node.input = const_node.output\n            node.type = node.type + \'Like\'\n\n\n@tf_op([""RandomNormalLike"", ""RandomUniformLike""])\nclass PassThroughOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n@tf_op(""Fill"")\nclass Fill:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # T output = Fill(int32 dims, T value, @int32 index_type)\n        # T outputs = Tile(T value, int64 repeats (e.g. dims))\n        fill_shape = ctx.get_shape(node.input[0])\n        utils.make_sure(fill_shape is not None, ""shape of {} is None"".format(node.input[0]))\n        fill_shape_dims = fill_shape[0]\n        utils.make_sure(fill_shape_dims > 0, ""opset 7 requires fill shape length > 0, or please try opset > 7"")\n        val_dtype = ctx.get_dtype(node.input[1])\n        val_shape = ctx.get_shape(node.input[1])\n\n        need_cast = val_dtype != onnx_pb.TensorProto.FLOAT and ctx.opset < 9\n        new_dtype = val_dtype\n        if need_cast:\n            new_dtype = onnx_pb.TensorProto.FLOAT\n            attr = {""to"": new_dtype}\n            cast_to_float = ctx.insert_new_node_on_input(node, ""Cast"", node.input[1], name=None, **attr)\n            ctx.set_dtype(cast_to_float.output[0], new_dtype)\n            ctx.set_shape(cast_to_float.output[0], val_shape)\n\n        for _ in range(fill_shape_dims):\n            attr = {""axes"": [0]}\n            shape = ctx.get_shape(node.input[1])\n            unsqueeze_node = ctx.insert_new_node_on_input(node, ""Unsqueeze"", node.input[1], name=None, **attr)\n            ctx.set_dtype(unsqueeze_node.output[0], new_dtype)\n            if shape:\n                shape = [1] + shape\n            else:\n                shape = [1]\n            ctx.set_shape(unsqueeze_node.output[0], shape)\n\n        # Tile\'s repeats must be INT64\n        attr = {""to"": onnx_pb.TensorProto.INT64}\n        tile_shape_int64 = ctx.insert_new_node_on_input(node, ""Cast"", node.input[0], name=None, **attr)\n        ctx.set_dtype(tile_shape_int64.output[0], onnx_pb.TensorProto.INT64)\n        ctx.set_shape(tile_shape_int64.output[0], fill_shape)\n\n        tmp = node.input[0]\n        node.input[0] = node.input[1]\n        node.input[1] = tmp\n        node.type = ""Tile""\n        ctx.set_dtype(node.output[0], new_dtype)\n\n        if need_cast:\n            attr = {""to"": val_dtype}\n            op_name = utils.make_name(node.name + ""/cast_back"")\n            cast_back = ctx.insert_new_node_on_output(""Cast"", node.output[0], name=op_name, **attr)\n            ctx.set_dtype(cast_back.output[0], val_dtype)\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        node.type = ""ConstantOfShape""\n        # both shape and value in tensorflow are passed as tensor.\n        # In onnx the value is an attribute so we need to fetch the value as const which\n        # sooner or later will be a problem for tensorflow-onnx.\n        # ConstantOfShape in onnxruntime only support int64, so insert cast op\n        input_dtype_is_int64 = utils.map_onnx_to_numpy_type(ctx.get_dtype(node.input[0])) == np.int64\n        if not input_dtype_is_int64:\n            ctx.insert_new_node_on_input(node, ""Cast"", node.input[0], to=onnx_pb.TensorProto.INT64)\n        dtype = ctx.get_dtype(node.output[0])\n        value = np.array([node.inputs[1].get_tensor_value()]).astype(utils.map_onnx_to_numpy_type(dtype))\n        value_proto = numpy_helper.from_array(value)\n        node.set_attr(""value"", value_proto)\n        del node.input[1]\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # cls.version_7(ctx, node, **kwargs)\n        node.type = ""Expand""\n        node.input = [node.input[1], node.input[0]]\n        # cast shape to int64 if needed\n        if ctx.get_dtype(node.input[1]) != onnx_pb.TensorProto.INT64:\n            ctx.insert_new_node_on_input(node, ""Cast"", node.input[1], to=onnx_pb.TensorProto.INT64)\n\n\n@tf_op(""Multinomial"")\nclass Multinomial:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # output_dtype output = Multinomial(T logits, int32 num_samples, @int seed, @int seed2, @type output_dtype)\n        sample_size = node.inputs[1].get_tensor_value()\n        seed = node.get_attr(""seed"")\n        if seed:\n            node.set_attr(""seed"", float(seed.i))\n        output_dtype = node.get_attr(""output_dtype"")\n        if output_dtype:\n            output_dtype = output_dtype.i\n        else:\n            output_dtype = onnx_pb.TensorProto.INT32\n        node.set_attr(""dtype"", output_dtype)\n        node.set_attr(""sample_size"", sample_size)\n        ctx.remove_input(node, node.input[1])\n\n\n@tf_op(""ZerosLike"")\nclass ZerosLike:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        casted_input = ctx.make_node(""Cast"", node.input, attr={\'to\': onnx_pb.TensorProto.INT64})\n        const_zero = ctx.make_const(utils.make_name(""zero""), np.array(0).astype(np.int64))\n        mul_node = ctx.make_node(\'Mul\', inputs=[casted_input.output[0], const_zero.output[0]])\n        ctx.make_node(""Cast"", inputs=[mul_node.output[0]],\n                      attr={\'to\': dtypes[0]},\n                      name=node.name, outputs=node.output,\n                      shapes=shapes, dtypes=dtypes)\n\n\n@tf_op([""IteratorV2"", ""FIFOQueueV2""])\nclass Iterator:\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        ctx.remove_node(node.name)\n\n\n@tf_op([""IteratorGetNext"", ""QueueDequeueV2""])\nclass IteratorGetNext:\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        output_names = node.output\n        type_0 = ctx.get_dtype(output_names[0])\n        type_1 = ctx.get_dtype(output_names[1])\n        shape_0 = ctx.get_shape(output_names[0])\n        shape_1 = ctx.get_shape(output_names[1])\n        ctx.remove_node(node.name)\n        ctx.add_graph_input(output_names[0], type_0, shape_0)\n        ctx.add_graph_input(output_names[1], type_1, shape_1)\n\n\n@tf_op(""QueueDequeueManyV2"")\nclass QueueDequeueManyV2:\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        outputs = node.output\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        for i, output in enumerate(outputs):\n            ctx.add_graph_input(output, dtypes[i], shapes[i])\n'"
tf2onnx/onnx_opset/logical.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nlogical\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nfrom onnx import TensorProto\nfrom tf2onnx import utils\nfrom tf2onnx.handler import tf_op\nfrom tf2onnx.onnx_opset import common\n\n\nlogger = logging.getLogger(__name__)\n\n# pylint: disable=unused-argument,missing-docstring\n\ndef _add_cast_to_inputs(graph, node, supported_dtypes, target_dtype):\n    is_support = True\n    for inp in node.input:\n        if graph.get_dtype(inp) not in supported_dtypes:\n            is_support = False\n            break\n    if not is_support:\n        for inp in node.input:\n            inp_cast = graph.insert_new_node_on_input(node, ""Cast"", inp, to=target_dtype)\n            graph.copy_shape(inp, inp_cast.output[0])\n            graph.set_dtype(inp_cast.output[0], target_dtype)\n\n\n@tf_op(""LogicalNot"", onnx_op=""Not"")\nclass DirectOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""LogicalAnd"", onnx_op=""And"")\n@tf_op(""LogicalOr"", onnx_op=""Or"")\nclass BroadcastOp(common.BroadcastOp):\n    pass\n\n\n@tf_op([""Equal"", ""NotEqual""])\nclass Equal:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        need_not = node.type == ""NotEqual""\n        common.BroadcastOp.version_1(ctx, node, **kwargs)\n        if need_not:\n            node.type = ""Equal""\n            output_name = node.output[0]\n            not_node = ctx.insert_new_node_on_output(""Not"", output_name, name=utils.make_name(node.name))\n            ctx.copy_shape(output_name, not_node.output[0])\n            ctx.copy_dtype(output_name, not_node.output[0])\n\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # T2 output = Equal(T1, x, T1 y), T1 \\in {bool, int32, int64}\n        need_not = node.type == ""NotEqual""\n        supported_dtypes = [\n            TensorProto.BOOL,\n            TensorProto.INT32,\n            TensorProto.INT64\n        ]\n        # FIXME: casting is not the same as equal\n        target_dtype = TensorProto.INT32\n        _add_cast_to_inputs(ctx, node, supported_dtypes, target_dtype)\n        if need_not:\n            node.type = ""Equal""\n            output_name = node.output[0]\n            not_node = ctx.insert_new_node_on_output(""Not"", output_name, name=utils.make_name(node.name))\n            ctx.copy_shape(output_name, not_node.output[0])\n            ctx.copy_dtype(output_name, not_node.output[0])\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # starting with opset-11, equal supports all types\n        need_not = node.type == ""NotEqual""\n        if need_not:\n            node.type = ""Equal""\n            output_name = node.output[0]\n            not_node = ctx.insert_new_node_on_output(""Not"", output_name, name=utils.make_name(node.name))\n            ctx.copy_shape(output_name, not_node.output[0])\n            ctx.copy_dtype(output_name, not_node.output[0])\n\n\n@tf_op([""Greater"", ""Less""])\nclass GreaterLess:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        common.BroadcastOp.version_1(ctx, node, **kwargs)\n\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # T2 output = Greater(T1 x, T1 y), T2=tensor(bool)\n        # T2 output = Less(T1 x, T1 y), T2=tensor(bool)\n        # Great/Less in opset7 only supports limited types, insert Cast if needed\n        supported_dtypes = [\n            TensorProto.FLOAT,\n            TensorProto.FLOAT16,\n            TensorProto.DOUBLE\n        ]\n        target_dtype = TensorProto.FLOAT\n        _add_cast_to_inputs(ctx, node, supported_dtypes, target_dtype)\n\n@tf_op([""GreaterEqual"", ""LessEqual""])\nclass GreaterLessEqual:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        GreaterLess.version_7(ctx, node, **kwargs)\n        output_name = node.output[0]\n        node.op.op_type = ""Less"" if node.op.op_type == ""GreaterEqual"" else ""Greater""\n        new_node = ctx.insert_new_node_on_output(""Not"", output_name, name=utils.make_name(node.name))\n        ctx.copy_shape(output_name, new_node.output[0])\n        ctx.set_dtype(new_node.output[0], ctx.get_dtype(output_name))\n\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        node.op.op_type = ""GreaterOrEqual"" if node.op.op_type == ""GreaterEqual"" else ""LessOrEqual""\n'"
tf2onnx/onnx_opset/math.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nmath\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nimport numpy as np\nfrom onnx import onnx_pb\nfrom tf2onnx import constants, utils\nfrom tf2onnx.handler import tf_op\nfrom tf2onnx.onnx_opset import common\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=unused-argument,missing-docstring\n\n@tf_op([""Add"", ""AddV2"", ""Div"", ""Mul"", ""Sub""])\nclass BroadcastOp(common.BroadcastOp):\n    pass\n\n\n@tf_op([""RealDiv"", ""TruncateDiv""], onnx_op=""Div"")\nclass RealDiv(common.BroadcastOp):\n    pass\n\n\n@tf_op([""LeakyRelu"", ""LogSoftmax"", ""Softplus"", ""Softsign""])\nclass DirectOpSinceOpset1:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op([""Abs"", ""Ceil"", ""Elu"", ""Exp"", ""Floor"", ""Log"", ""Neg"", ""Relu"", ""Sigmoid"", ""Sqrt"",\n        ""Tanh"", ""Reciprocal""])\nclass DirectOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n    @classmethod\n    def version_6(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op([""Acos"", ""Asin"", ""Atan"", ""Cos"", ""Sin"", ""Tan""])\nclass TrigOpSinceOpset7:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op([""Acosh"", ""Asinh"", ""Atanh"", ""Cosh"", ""Sinh""])\nclass TrigOpSinceOpset9:\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        pass\n\n\ndef make_min_or_max_op(ctx, op_type, inputs, outputs,\n                       output_shapes=None, output_dtypes=None):\n    # support more dtype\n    supported_dtypes = [\n        onnx_pb.TensorProto.FLOAT,\n        onnx_pb.TensorProto.FLOAT16,\n        onnx_pb.TensorProto.DOUBLE\n    ]\n    target_dtype = onnx_pb.TensorProto.FLOAT\n    need_cast = False\n    cast_inputs = []\n    for inp in inputs:\n        dtype = ctx.get_dtype(inp)\n        utils.make_sure(dtype is not None, ""dtype of {} is None"".format(inp))\n        if dtype not in supported_dtypes:\n            cast_inp = ctx.make_node(""Cast"", [inp], attr={""to"": target_dtype})\n            cast_inputs.append(cast_inp.output[0])\n            need_cast = True\n        else:\n            cast_inputs.append(inp)\n    node = ctx.make_node(op_type, cast_inputs, shapes=output_shapes)\n    actual_outputs = node.output\n    if need_cast:\n        origin_dtype = ctx.get_dtype(inputs[0])\n        if output_dtypes is not None:\n            origin_dtype = output_dtypes[0]\n        ctx.set_dtype(node.output[0], target_dtype)\n        cast_name = utils.make_name(node.name)\n        cast_node = ctx.insert_new_node_on_output(""Cast"", node.output[0], name=cast_name, to=origin_dtype)\n        ctx.set_dtype(cast_node.output[0], origin_dtype)\n        ctx.copy_shape(node.output[0], cast_node.output[0])\n        actual_outputs = cast_node.output\n    final_node = ctx.make_node(""Identity"", actual_outputs, outputs=outputs,\n                               shapes=output_shapes, dtypes=output_dtypes)\n\n    # tensorflow minimum/maximum does support broadcast, onnx < opset 8 does not.\n    # handle this by doing something like:\n    # y = min(x1, add(x2, sub(x1, x1))), where x1, x2 are the inputs and x2 is a scalar\n    # this will create a tensor of zeros of the shape of x1, adds x2 to it (which broadcasts) and use that for min.\n    shapeo = ctx.get_shape(node.output[0])\n    needs_broadcast_op = []\n    has_correct_shape = []\n    if ctx.opset < 8:\n        for i, input_name in enumerate(node.input):\n            if ctx.get_shape(input_name) != shapeo:\n                needs_broadcast_op.append(i)\n            else:\n                has_correct_shape.append(input_name)\n    if needs_broadcast_op:\n        has_correct_shape = has_correct_shape[0]\n        for i in needs_broadcast_op:\n            input_node = node.inputs[i]\n            # get a tensor with zeros (since there is no Fill op as of opset8)\n            sub_node = ctx.make_node(""Sub"", [has_correct_shape, has_correct_shape],\n                                     op_name_scope=input_node.name)\n            # use add as \'broadcast\' op\n            add_node = ctx.make_node(""Add"", [input_node.output[0], sub_node.output[0]],\n                                     op_name_scope=input_node.name)\n            node.input[i] = add_node.output[0]\n    return final_node\n\n\n@tf_op(""Minimum"", onnx_op=""Min"")\n@tf_op(""Maximum"", onnx_op=""Max"")\nclass MinMaxOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        make_min_or_max_op(ctx, node.type, node.input, node.output, shapes, dtypes)\n\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        pass # support all numeric types and broadcasting\n\n@tf_op(""ClipByValue"")\nclass ClipByValueOp:\n    # in tf-1.8 there was a ClipByValue op which in later versions was replaced by max(min(x, a), b)\n    # To support models generated with tf-1.8 rewrite the tf ClipByValue op to max(min(x, a), b)\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        supported = [onnx_pb.TensorProto.FLOAT16, onnx_pb.TensorProto.FLOAT, onnx_pb.TensorProto.DOUBLE]\n        # fetch those upfront since they are not accessible once we remove \'node\'\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        input_dtype = node.inputs[0].output_dtypes[0]\n        name = node.name\n        min_node = node.inputs[1]\n        if min_node.output_dtypes[0] not in supported:\n            # cast min if needed\n            min_node = ctx.insert_new_node_on_input(node, ""Cast"", min_node.output[0], to=onnx_pb.TensorProto.FLOAT)\n        max_node = node.inputs[2]\n        if max_node.output_dtypes[0] not in supported:\n            # cast max if needed\n            max_node = ctx.insert_new_node_on_input(node, ""Cast"", max_node.output[0], to=onnx_pb.TensorProto.FLOAT)\n        ctx.remove_node(name)\n        new_node = ctx.make_node(""Max"", [node.input[0], min_node.output[0]], outputs=[node.output[0]],\n                                 shapes=shapes, dtypes=dtypes)\n        if input_dtype not in supported:\n            # cast the data tensor if needed\n            ctx.insert_new_node_on_input(new_node, ""Cast"", new_node.input[0], to=onnx_pb.TensorProto.FLOAT)\n\n        new_node = ctx.insert_new_node_on_output(""Min"", new_node.output[0], name=utils.make_name(name))\n        new_node.input.append(max_node.output[0])\n        # copy shape and type\n        ctx.set_dtype(new_node.output[0], dtypes[0])\n        ctx.set_shape(new_node.output[0], shapes[0])\n        if dtypes[0] not in supported:\n            # cast output if needed\n            new_node = ctx.insert_new_node_on_output(""Cast"", new_node.output[0],\n                                                     name=utils.make_name(name), to=dtypes[0])\n            # copy shape and type\n            ctx.set_dtype(new_node.output[0], dtypes[0])\n            ctx.set_shape(new_node.output[0], shapes[0])\n\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        node.name = \'Clip\' # clip supports all types now\n\n@tf_op(""Softmax"")\nclass Softmax:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = Softmax(T logits). The axis softmax would be performed on is always on -1.\n        # T output = Softmax(T input, @int axis). Default axis is 1.\n        logits_rank = len(ctx.get_shape(node.input[0]))\n        node.set_attr(""axis"", logits_rank - 1)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""Square"")\nclass Square:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        node.type = ""Mul""\n        node.input.append(node.input[0])\n\n\n@tf_op(""Relu6"")\nclass Relu6:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # relu6 = min(max(features, 0), 6)\n        # relu6 = min(max(features, 0), 6)\n        node.type = ""Clip""\n        node.set_attr(""min"", 0.0)\n        node.set_attr(""max"", 6.0)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # add min and max as inputs\n        node.type = ""Clip""\n        onnx_dtype = ctx.get_dtype(node.input[0])\n        np_dtype = utils.ONNX_TO_NUMPY_DTYPE[onnx_dtype]\n        clip_min = ctx.make_const(utils.make_name(""{}_min"".format(node.name)), np.array(0.0, dtype=np_dtype))\n        clip_max = ctx.make_const(utils.make_name(""{}_max"".format(node.name)), np.array(6.0, dtype=np_dtype))\n        node.input.append(clip_min.output[0])\n        node.input.append(clip_max.output[0])\n\n\n@tf_op(""Rsqrt"")\nclass Rsqrt:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        node.type = ""Sqrt""\n        op_name = utils.make_name(node.name)\n        reciprocal = ctx.insert_new_node_on_output(""Reciprocal"", node.output[0], name=op_name)\n        ctx.copy_shape(node.output[0], reciprocal.output[0])\n\n\n@tf_op(""SquaredDifference"")\nclass SquaredDifference:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        node.type = ""Sub""\n        op_name = utils.make_name(node.name)\n        mul = ctx.insert_new_node_on_output(""Mul"", node.output[0], name=op_name)\n        mul.input.append(node.output[0])\n\n\n@tf_op(""Sign"")\nclass Sign:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        """"""Sign op.""""""\n        # T sign = Sign(T Input)\n        node_dtype = ctx.get_dtype(node.output[0])\n        utils.make_sure(node_dtype, ""Dtype of {} is None"".format(node.name))\n        if node_dtype in [onnx_pb.TensorProto.COMPLEX64, onnx_pb.TensorProto.COMPLEX128]:\n            raise ValueError(""dtype "" + str(node_dtype) + "" is not supported in onnx for now"")\n        zero_name = utils.make_name(""{}_zero"".format(node.name))\n        ctx.make_const(zero_name, np.array(0, dtype=np.float32))\n        if node_dtype not in [onnx_pb.TensorProto.FLOAT16, onnx_pb.TensorProto.FLOAT, onnx_pb.TensorProto.DOUBLE]:\n            cast_node_0 = ctx.make_node(""Cast"", [node.input[0]], {""to"": onnx_pb.TensorProto.FLOAT})\n            greater_node = ctx.make_node(""Greater"", [cast_node_0.output[0], zero_name])\n            less_node = ctx.make_node(""Less"", [cast_node_0.output[0], zero_name])\n        else:\n            greater_node = ctx.make_node(""Greater"", [node.input[0], zero_name])\n            less_node = ctx.make_node(""Less"", [node.input[0], zero_name])\n        cast_node_1 = ctx.make_node(""Cast"", [greater_node.output[0]], {""to"": node_dtype})\n        cast_node_2 = ctx.make_node(""Cast"", [less_node.output[0]], {""to"": node_dtype})\n\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(""Sub"", [cast_node_1.output[0], cast_node_2.output[0]], outputs=[node.output[0]],\n                      shapes=shapes, dtypes=dtypes)\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        node_dtype = ctx.get_dtype(node.output[0])\n        utils.make_sure(node_dtype, ""dtype of {} is None"".format(node.name))\n        if node_dtype in [onnx_pb.TensorProto.BOOL, onnx_pb.TensorProto.COMPLEX64, onnx_pb.TensorProto.COMPLEX128]:\n            raise ValueError(""dtype "" + str(node_dtype) + "" is not supported in onnx for now"")\n\n\n@tf_op(""Pow"")\nclass Pow:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        if ctx.is_target(constants.TARGET_CAFFE2):\n            # workaround a bug in caffe2 pre Feb2018, pow(a, b) becomes np.exp(np.log(a) * b)\n            node.type = ""Log""\n            b = node.input[1]\n            ctx.remove_input(node, node.input[1])\n            op_name = utils.make_name(node.name)\n            mul_op = ctx.insert_new_node_on_output(""Mul"", node.output[0], name=op_name)\n            mul_op.input.append(b)\n            op_name = utils.make_name(node.name)\n            exp_op = ctx.insert_new_node_on_output(""Exp"", mul_op.output[0], name=op_name)\n            ctx.copy_shape(node.output[0], exp_op.output[0])\n            BroadcastOp.version_1(ctx, mul_op, **kwargs)\n\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""LRN"")\nclass LRN:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # ONNX: Each input value is divided by (bias+(alpha/size)*sum(xi^2 for every xi in the local region))^beta\n        # TF: sqr_sum[a, b, c, d] = sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\n        #     output = input / (bias + alpha * sqr_sum) ** beta\n\n        # by default, depth_radius is 5 in tensorflow\n        size = node.get_attr_value(""depth_radius"", 5) * 2 + 1\n\n        node.set_attr(""size"", size)\n        node.set_attr(""alpha"", size * node.get_attr(""alpha"").f)\n\n        shapes = node.output_shapes[0]\n        dtypes = node.output_dtypes[0]\n\n        ctx.insert_new_node_on_input(node, ""Transpose"", node.input[0], perm=constants.NHWC_TO_NCHW)\n        ctx.update_node_shape_dtype(node, override=True)\n        op_name = utils.make_name(node.name)\n        ctx.insert_new_node_on_output(""Transpose"", node.output[0], perm=constants.NCHW_TO_NHWC,\n                                      name=op_name, shapes=shapes, dtypes=dtypes)\n\n\n@tf_op([""MatMul"", ""BatchMatMul"", ""BatchMatMulV2""])\nclass MatMul:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # tensorflow allows transpose and conjugated. If found, insert the required transpose.\n        # We could use Gemm as well but tensorflow does not pass bias in matmul.\n        node.type = ""MatMul""\n\n        attrs = [""transpose_a"", ""transpose_b"", ""adjoint_a"", ""adjoint_b"", ""adj_x"", ""adj_y""]\n        attrs_val = [node.get_attr(attr) for attr in attrs]\n        attrs_val = [0 if val is None else val.i for val in attrs_val]\n\n        dtype = ctx.get_dtype(node.output[0])\n        if any(attrs_val[2:]):\n            # conjugation operation on complex data not supported in onnx for now\n            # so if it\'s complex than raise exception\n            if dtype not in [onnx_pb.TensorProto.FLOAT, onnx_pb.TensorProto.FLOAT16, onnx_pb.TensorProto.DOUBLE]:\n                raise ValueError(""dtype "" + dtype + "" is not supported in onnx matmul for now"")\n\n        transpose_a = (attrs_val[0] + attrs_val[2] + attrs_val[4]) % 2\n        transpose_b = (attrs_val[1] + attrs_val[3] + attrs_val[5]) % 2\n\n        if transpose_a != 0:\n            shape = ctx.get_shape(node.input[0])\n            if shape:\n                perm = list(range(0, len(shape)))\n                tmp = perm[-1]\n                perm[-1] = perm[-2]\n                perm[-2] = tmp\n                ctx.insert_new_node_on_input(node, ""Transpose"", node.input[0], perm=perm)\n\n        if transpose_b != 0:\n            shape = ctx.get_shape(node.input[1])\n            if shape:\n                perm = list(range(0, len(shape)))\n                tmp = perm[-1]\n                perm[-1] = perm[-2]\n                perm[-2] = tmp\n                ctx.insert_new_node_on_input(node, ""Transpose"", node.input[1], perm=perm)\n\n        unsupported = [""a_is_sparse"", ""b_is_sparse""]\n        for i in unsupported:\n            val = node.get_attr(i)\n            if val is not None and val.i != 0:\n                raise ValueError(node.type + "" attribute "" + i + "" is not supported"")\n\n\n@tf_op(""Erf"")\nclass Erf:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        """"""Error function.""""""\n        # constant names\n        a1 = ""erf_a1""\n        a2 = ""erf_a2""\n        a3 = ""erf_a3""\n        a4 = ""erf_a4""\n        a5 = ""erf_a5""\n        p = ""erf_p""\n        one = ""erf_one""\n        null = ""erf_null""\n\n        n = node.name\n        output_name = node.output[0]\n        erf_a1_node = ctx.get_node_by_output(""erf_a1"")\n        if erf_a1_node is None:\n            # insert the constants for erf once\n            ctx.make_const(a1, np.array(0.254829592, dtype=np.float32))\n            ctx.make_const(a2, np.array(-0.284496736, dtype=np.float32))\n            ctx.make_const(a3, np.array(1.421413741, dtype=np.float32))\n            ctx.make_const(a4, np.array(-1.453152027, dtype=np.float32))\n            ctx.make_const(a5, np.array(1.061405429, dtype=np.float32))\n            ctx.make_const(p, np.array(0.3275911, dtype=np.float32))\n            ctx.make_const(one, np.array(1., dtype=np.float32))\n            ctx.make_const(null, np.array(0., dtype=np.float32))\n\n        x = node.input[0]\n\n        # erf(x):\n        #  sign = 1 if x >= 0 else -1\n        #  x = abs(x)\n        #  # A&S formula 7.1.26\n        #  t = 1.0 / (1.0 + p * x)\n        #  y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) *  t * math.exp(-x * x)\n        #  return sign * y  # erf(-x) = -erf(x)\n\n        x_node = ctx.make_node(""Abs"", [x], op_name_scope=node.name, name=""x"")\n        negx_node = ctx.make_node(""Sub"", [null, x], op_name_scope=node.name, name=""negx"")\n        is_positive_node = ctx.make_node(""Greater"", [x, null], op_name_scope=node.name, name=""isPositive"")\n        is_positive_value_node = ctx.make_node(""Cast"", is_positive_node.output, op_name_scope=node.name,\n                                               name=""isPositiveValue"", attr={""to"": onnx_pb.TensorProto.FLOAT})\n        is_neg_node = ctx.make_node(""Less"", [x, null], op_name_scope=node.name, name=""isNeg"")\n        ig_neg_value_node = ctx.make_node(""Cast"", is_neg_node.output, op_name_scope=node.name, name=""isNegValue"",\n                                          attr={""to"": onnx_pb.TensorProto.FLOAT})\n        sign0_node = ctx.make_node(""Sub"", [is_positive_value_node.output[0], ig_neg_value_node.output[0]],\n                                   op_name_scope=node.name, name=""sign0"")\n        sign_add_one_node = ctx.make_node(""Add"", [sign0_node.output[0], one], op_name_scope=node.name,\n                                          name=""signAddOne"")\n        non_zero_node = ctx.make_node(""Abs"", sign0_node.output, op_name_scope=node.name, name=""nonZero"")\n        sign_node = ctx.make_node(""Sub"", [sign_add_one_node.output[0], non_zero_node.output[0]],\n                                  op_name_scope=node.name, name=""sign"")\n        num_4_node = ctx.make_node(""Mul"", [x_node.output[0], p], op_name_scope=node.name, name=""4"")\n        num_5_node = ctx.make_node(""Add"", [num_4_node.output[0], one], op_name_scope=node.name, name=""5"")\n        t_node = ctx.make_node(""Div"", [one, num_5_node.output[0]], op_name_scope=node.name, name=""t"")\n        xsq_node = ctx.make_node(""Mul"", [x, negx_node.output[0]], op_name_scope=node.name, name=""xsq"")\n        num_6_node = ctx.make_node(""Exp"", xsq_node.output, op_name_scope=node.name, name=""6"")\n        num_7_node = ctx.make_node(""Mul"", [num_6_node.output[0], t_node.output[0]], op_name_scope=node.name, name=""7"")\n        num_8_node = ctx.make_node(""Mul"", [t_node.output[0], a5], op_name_scope=node.name, name=""8"")\n        num_9_node = ctx.make_node(""Add"", [num_8_node.output[0], a4], op_name_scope=node.name, name=""9"")\n        num_10_node = ctx.make_node(""Mul"", [num_9_node.output[0], t_node.output[0]], op_name_scope=node.name, name=""10"")\n        num_11_node = ctx.make_node(""Add"", [num_10_node.output[0], a3], op_name_scope=node.name, name=""11"")\n        num_12_node = ctx.make_node(""Mul"", [num_11_node.output[0], t_node.output[0]], op_name_scope=node.name,\n                                    name=""12"")\n        num_13_node = ctx.make_node(""Add"", [num_12_node.output[0], a2], op_name_scope=node.name, name=""13"")\n        num_14_node = ctx.make_node(""Mul"", [num_13_node.output[0], t_node.output[0]], op_name_scope=node.name,\n                                    name=""14"")\n        num_15_node = ctx.make_node(""Add"", [num_14_node.output[0], a1], op_name_scope=node.name, name=""15"")\n        num_16_node = ctx.make_node(""Mul"", [num_15_node.output[0], num_7_node.output[0]], op_name_scope=node.name,\n                                    name=""16"")\n        num_17_node = ctx.make_node(""Sub"", [one, num_16_node.output[0]], op_name_scope=node.name, name=""17"")\n\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(""Mul"", [num_17_node.output[0], sign_node.output[0]], outputs=[output_name], name=n,\n                      shapes=shapes, dtypes=dtypes)\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""FloorDiv"")\nclass FloorDiv:\n    @classmethod\n    def version_6(cls, ctx, node, **kwargs):\n        # T output = FloorDiv(T x, T y)\n        node.type = ""Div""\n        dtype = ctx.get_dtype(node.input[0])\n        if dtype in [onnx_pb.TensorProto.FLOAT, onnx_pb.TensorProto.FLOAT16, onnx_pb.TensorProto.DOUBLE]:\n            new_node_name = utils.make_name(""floor_div_res"")\n            floor_res = ctx.insert_new_node_on_output(op_type=""Floor"", output_name=node.output[0],\n                                                      name=new_node_name)\n            ctx.copy_dtype(node.output[0], floor_res.output[0])\n            ctx.copy_shape(node.output[0], floor_res.output[0])\n\n\n@tf_op(""FloorMod"")\nclass FloorMod:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # T output = FloorMod(T x, T y)\n        div = ctx.make_node(op_type=""Div"", inputs=node.input)\n        dtype = ctx.get_dtype(node.input[0])\n        if dtype in [onnx_pb.TensorProto.FLOAT, onnx_pb.TensorProto.FLOAT16, onnx_pb.TensorProto.DOUBLE]:\n            div = ctx.make_node(op_type=""Floor"", inputs=div.output)\n\n        mul = ctx.make_node(op_type=""Mul"", inputs=[div.output[0], node.input[1]])\n        # res node will take over shape&dtype&output connection info of original ""node""\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(op_type=""Sub"", inputs=[node.input[0], mul.output[0]],\n                      name=node.name, outputs=node.output, shapes=shapes, dtypes=dtypes)\n\n\n@tf_op(""Selu"")\nclass Selu:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""Cumsum"", onnx_op=""CumSum"")\nclass CumSum:\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""Round"")\nclass Round:\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""MatrixDeterminant"", onnx_op=""Det"")\nclass Det:\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op([""LeftShift"", ""RightShift""])\nclass BitShift:\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        dir_map = {""LeftShift"": ""LEFT"", ""RightShift"": ""RIGHT""}\n        direction = dir_map[node.type]\n        supported = [onnx_pb.TensorProto.UINT8, onnx_pb.TensorProto.UINT16,\n                     onnx_pb.TensorProto.UINT32, onnx_pb.TensorProto.UINT64]\n        type_map = {onnx_pb.TensorProto.INT8: onnx_pb.TensorProto.UINT8,\n                    onnx_pb.TensorProto.INT16: onnx_pb.TensorProto.UINT32,\n                    onnx_pb.TensorProto.INT32: onnx_pb.TensorProto.UINT64}\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n\n        node = ctx.make_node(""BitShift"", inputs=node.input, outputs=node.output, name=node.name,\n                             shapes=shapes, dtypes=dtypes, domain=constants.ONNX_DOMAIN, attr={\'direction\': direction})\n\n        if node.maybe_cast_input([supported, supported], type_map):\n            cast_back_node = ctx.insert_new_node_on_output(""Cast"", node.output[0],\n                                                           name=utils.make_name(node.name) + ""_castback"")\n            cast_back_node.set_attr(""to"", dtypes[0])\n            ctx.set_dtype(cast_back_node.output[0], dtypes[0])\n            ctx.copy_shape(node.name, cast_back_node.output[0])\n\n\n@tf_op(""SquaredDistance"", onnx_op=""MeanSquaredDistance"")\nclass SquaredDistance:\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        node.attr[""reduction""] = ""none""\n\n\n@tf_op(""Einsum"")\nclass Einsum:\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        del node.attr[""N""]\n\n\n@tf_op(""IsFinite"")\nclass IsFinite:\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        # map to onnx as:\n        # not (isinf(x) or isnan(x))\n\n        shapes = node.output_shapes\n        dtypes = [onnx_pb.TensorProto.BOOL] * len(node.output_dtypes)\n\n        ctx.remove_node(node.name)\n\n        inf_node = ctx.make_node(""IsInf"", inputs=node.input, name=utils.make_name(node.name),\n                                 shapes=shapes, dtypes=dtypes)\n        nan_node = ctx.make_node(""IsNaN"", inputs=node.input, name=utils.make_name(node.name),\n                                 shapes=shapes, dtypes=dtypes)\n        or_node = ctx.make_node(""Or"", inputs=[inf_node.output[0], nan_node.output[0]], name=utils.make_name(node.name),\n                                shapes=shapes, dtypes=dtypes)\n        _ = ctx.make_node(""Not"", inputs=or_node.output, name=node.name,\n                          shapes=shapes, dtypes=dtypes)\n'"
tf2onnx/onnx_opset/misc.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nmisc\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nfrom tf2onnx.handler import tf_op\n\n\nlogger = logging.getLogger(__name__)\n\n# pylint: disable=unused-argument,missing-docstring\n\n@tf_op([""CheckNumerics"", ""StopGradient""])\nclass MoveToIdent:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        node.type = ""Identity""\n        if node.inputs[0].is_const():\n            # should not remove the identity node if it is output of the graph\n            if node.output[0] in ctx.outputs:\n                return\n            # if identity has a const as input, remove it\n            input_name = node.input[0]\n            output_name = node.output[0]\n            ctx.replace_all_inputs(ctx.get_nodes(), output_name, input_name)\n            ctx.remove_node(node.name)\n\n\n@tf_op([""Placeholder"", ""PlaceholderV2"", ""PlaceholderWithDefault""])\nclass DirectOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""NoOp"")\nclass NukeNode:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        ctx.remove_node(node.name)\n'"
tf2onnx/onnx_opset/nn.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nnn\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nimport numpy as np\nfrom onnx import onnx_pb\nfrom onnx.onnx_pb import TensorProto\nfrom tf2onnx import constants, utils\nfrom tf2onnx.graph_builder import GraphBuilder\nfrom tf2onnx.handler import tf_op\nfrom tf2onnx.onnx_opset import common, controlflow, tensor\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=unused-argument,missing-docstring,unused-variable\n\ndef spatial_map(shape, perm):\n    new_shape = shape[:]\n    for i in perm:\n        new_shape[i] = shape[perm[i]]\n    return new_shape\n\n\ndef conv_convert_inputs(ctx, node, with_kernel=False, new_kernel_shape=None,\n                        input_indices=None, output_indices=None):\n    """"""Convert input and kernel from tensorflow to onnx. This maybe require to\n        to insert transpose ops for input, kernel and output unless they are constants\n        and we can transpose the constant.\n        We transpose inputs if they are in NHWC. We always transpose the kernel from\n        HWNC to NCHW. Outputs are transposed if the format is NHWC.\n        Some convolutions like depthwise_conv2d require a reshape of the kernel.\n        Args:\n            ctx: the parent graph\n            node: node of the convolution op\n            with_kernel: transpose the kernel\n            new_kernel_shape: reshape the kernel\n    """"""\n\n    if input_indices is None:\n        input_indices = [0]\n    if output_indices is None:\n        output_indices = [0]\n\n    if node.is_nhwc():\n        # transpose input if needed, no need to record shapes on input\n        for idx in input_indices:\n            parent = node.inputs[idx]\n            if node.inputs[idx].is_const() and len(ctx.find_output_consumers(node.input[1])) == 1:\n                # if input is a constant, transpose that one if we are the only consumer\n                val = parent.get_tensor_value(as_list=False)\n                parent.set_tensor_value(val.transpose(constants.NHWC_TO_NCHW))\n            else:\n                # if input comes from a op, insert transpose op\n                input_name = node.input[idx]\n                transpose = ctx.insert_new_node_on_input(node, ""Transpose"", input_name)\n                transpose.set_attr(""perm"", constants.NHWC_TO_NCHW)\n                transpose.skip_conversion = True\n                shape = ctx.get_shape(input_name)\n                if shape is not None:\n                    new_shape = spatial_map(shape, constants.NHWC_TO_NCHW)\n                    ctx.set_shape(transpose.output[0], new_shape)\n\n    # kernel must to be transposed\n    if with_kernel:\n        # some onnx conv ops require the reshape the kernel (ie. depthwise_conv2d)\n        if new_kernel_shape:\n            if ctx.opset < 5:\n                # old reshape takes new shape as attribute\n                input_name = node.input[1]\n                reshape = ctx.insert_new_node_on_input(node, ""Reshape"", input_name)\n                reshape.set_attr(""shape"", new_kernel_shape)\n                reshape.skip_conversion = True\n            else:\n                # new reshape takes new shape as input[1]\n                shape_name = utils.make_name(node.name)\n                ctx.make_const(shape_name, np.array(new_kernel_shape, dtype=np.int64))\n                input_name = node.input[1]\n                reshape = ctx.make_node(""Reshape"", [input_name, shape_name])\n                ctx.replace_input(node, input_name, reshape.output[0])\n                reshape.skip_conversion = True\n            ctx.set_shape(reshape.output[0], new_kernel_shape)\n\n        parent = node.inputs[1]\n        need_transpose = True\n        if node.inputs[1].is_const():\n            # kernel is const - transpose the const if we are the only consumer of const\n            consumers = ctx.find_output_consumers(node.input[1])\n            if len(consumers) == 1:\n                val = parent.get_tensor_value(as_list=False)\n                val = val.transpose(constants.HWCN_TO_NCHW)\n                parent.set_tensor_value(val)\n                need_transpose = False\n\n        if need_transpose:\n            input_name = node.input[1]\n            transpose = ctx.insert_new_node_on_input(node, ""Transpose"", input_name)\n            transpose.set_attr(""perm"", constants.HWCN_TO_NCHW)\n            transpose.skip_conversion = True\n            new_shape = spatial_map(ctx.get_shape(input_name), constants.HWCN_TO_NCHW)\n            ctx.set_shape(transpose.output[0], new_shape)\n\n    # transpose outputs if needed\n    if node.is_nhwc():\n        for idx in output_indices:\n            output_name = node.output[idx]\n            output_shape = ctx.get_shape(node.output[idx])\n            op_name = utils.make_name(node.name)\n            transpose = ctx.insert_new_node_on_output(""Transpose"", output_name, name=op_name)\n            transpose.set_attr(""perm"", constants.NCHW_TO_NHWC)\n            transpose.skip_conversion = True\n            # set TF NHWC shape to transpose node output\n            ctx.set_shape(transpose.output[0], output_shape)\n            # Transpose TF NHWC shape back to NCHW shape for current ONNX conv node output\n            ctx.set_shape(output_name, spatial_map(output_shape, constants.NHWC_TO_NCHW))\n        node.data_format = ""NCHW""\n\n\ndef add_padding(ctx, node, kernel_shape, strides, dilations=None, spatial=2):\n    padding = node.get_attr(""padding"")\n    if padding:\n        if dilations is None:\n            dilations = [1] * spatial * 2\n        padding = padding.s.decode(""utf-8"")\n        if padding == \'SAME\':\n            pads = [0] * spatial * 2\n            input_shape = ctx.get_shape(node.input[0])\n            output_shape = ctx.get_shape(node.output[0])\n            # check if the input shape is valid\n            if len(input_shape) != len(pads):\n                logger.error(""node %s input needs to be rank %d, is %d"", node.name, len(pads), len(input_shape))\n            # transpose shape to nchw\n            if node.is_nhwc():\n                input_shape = spatial_map(input_shape, constants.NHWC_TO_NCHW)\n                output_shape = spatial_map(output_shape, constants.NHWC_TO_NCHW)\n            # calculate pads\n            if any(input_shape[i + 2] == -1 or output_shape[i + 2] == -1 for i in range(spatial)):\n                logger.debug(\n                    ""node %s has unknown dim for pads calculation, fallback to auto_pad: ""\n                    ""input_shape=%s, output_shape=%s"",\n                    node.name, input_shape, output_shape)\n                node.set_attr(""auto_pad"", ""SAME_UPPER"")\n            else:\n                for i in range(spatial):\n                    pad = (output_shape[i + 2] - 1) * strides[i] + dilations[i] * kernel_shape[i] - input_shape[i + 2]\n                    pad = max(pad, 0)\n                    pads[i] = pad // 2\n                    pads[i + spatial] = pad - pad // 2\n                node.set_attr(""pads"", pads)\n\n        elif padding == \'VALID\':\n            pass\n        else:\n            raise ValueError(""invalid padding value: "" + padding)\n\n\ndef conv_dims_attr(node, name, new_name=None):\n    if new_name is None:\n        new_name = name\n    dims = node.get_attr(name)\n    if not dims:\n        return None\n    dims = dims.ints\n    if node.is_nhwc():\n        if len(dims) == 2:\n            h, w = dims\n            c = n = 1\n        else:\n            n, h, w, c = dims\n    else:\n        n, c, h, w = dims\n    dims = [h, w]\n    node.set_attr(new_name, dims)\n    return dims\n\n\ndef conv_kernel_shape(ctx, node, input_idx, spatial=2):\n    kernel_shape = ctx.get_shape(node.input[input_idx])\n    if len(kernel_shape) != 2 * spatial:\n        raise ValueError(""kernel rank must be 2* spatial"")\n    kernel_shape = kernel_shape[0:spatial]\n    node.set_attr(""kernel_shape"", kernel_shape)\n    return kernel_shape\n\n\ndef build_dynamic_target_size(ctx, transposed_intput, target_hw):\n    """"""\n    Build the target tensor shape for the Resize op.\n\n    Args:\n        - ctx: the graph context\n        - transposed_intput: A tensor of rank 4 of shape [n c h w]\n        - target_hw: tensor of rank 2 containing the target size for a resize: [nh nw]\n\n    Returns:\n        A tensor of rank 2 containing [n c nh nw]\n    """"""\n    # We get the first half [n c] of the target shape\n    shape_of_transposed_input = ctx.make_node(""Shape"", [transposed_intput.output[0]])\n    first_half_of_shape = GraphBuilder(ctx).make_slice(\n        {""data"": shape_of_transposed_input.output[0], ""ends"": [2], ""starts"": [0]})\n    target_size_int64 = ctx.make_node(""Cast"", [target_hw.output[0]], attr={\'to\': TensorProto.INT64})\n    # We build a tensor containing [n c nh nw]\n    final_target_size = ctx.make_node(""Concat"", [first_half_of_shape, target_size_int64.output[0]], {\'axis\': 0})\n    return final_target_size\n\n\n@tf_op([""Conv1D"", ""Conv2D"", ""Conv3D""])\nclass ConvOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = Conv2D(T input, T filter, @list(int) strides, @bool use_cudnn_on_gpu,\n        #                       @string padding, @string data_format)\n        # T Y = Conv(T X, T W, T B, @AttrType.STRING auto_pad, @AttrType.INTS dilations, @AttrType.INT group,\n        #                       @AttrType.INTS kernel_shape, @AttrType.INTS pads, @AttrType.INTS strides)\n        node.type = ""Conv""\n        kernel_shape = conv_kernel_shape(ctx, node, 1, spatial=2)\n        strides = conv_dims_attr(node, ""strides"")\n        dilations = conv_dims_attr(node, ""dilations"")\n        add_padding(ctx, node, kernel_shape, strides, dilations=dilations, spatial=2)\n        conv_convert_inputs(ctx, node, with_kernel=True)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # no change\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""Conv2DBackpropInput"")\nclass ConvTranspose:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = Conv2DBackpropInput(int32 input_sizes, T filter, T out_backprop,\n        #    @list(int) strides, @bool use_cudnn_on_gpu, @string padding, @string data_format, @list(int) dilations)\n        # T Y = ConvTranspose(T X, T W, T B, @STRING auto_pad, @INTS dilations,\n        #    @INT group, @INTS kernel_shape, @INTS output_shape, @INTS pads, @INTS strides)\n\n        node.type = ""ConvTranspose""\n        # Note: inputs are reversed from what one would expect.\n        conv_kernel_shape(ctx, node, 1)\n        input_shape = ctx.get_shape(node.input[2])\n        output_shape_orig = node.output_shapes\n\n        # ouput_shape is explicitly specified here, in this case pads values are auto generated/calculated.\n        if node.inputs[0].is_const():\n            output_shape = ctx.get_shape(node.output[0])\n            if node.is_nhwc():\n                new_output_shape = [output_shape[1], output_shape[2]]\n                input_hw = [input_shape[1], input_shape[2]]\n            else:\n                new_output_shape = [output_shape[2], output_shape[3]]\n                input_hw = [input_shape[2], input_shape[3]]\n            utils.make_sure(new_output_shape.count(-1) <= 0, ""output h and w need to be known"")\n            utils.make_sure(new_output_shape[0] >= input_hw[0] and new_output_shape[1] >= input_hw[1],\n                            ""output h and w cannot be smaller than input h and w."")\n            node.set_attr(""output_shape"", new_output_shape)\n        else:\n            input_shape = ctx.make_node(""Cast"", [node.input[0]], attr={\'to\': TensorProto.INT64})\n            output_shape = ctx.make_node(""Shape"", [node.output[0]])\n            output_h = GraphBuilder(ctx).make_slice(\n                {""data"": output_shape.output[0], ""ends"": [2], ""starts"": [1], ""axes"": [0]})\n            output_w = GraphBuilder(ctx).make_slice(\n                {""data"": output_shape.output[0], ""ends"": [3], ""starts"": [2], ""axes"": [0]})\n            expect_h = GraphBuilder(ctx).make_slice(\n                {""data"": input_shape.output[0], ""ends"": [2], ""starts"": [1], ""axes"": [0]})\n            expect_w = GraphBuilder(ctx).make_slice(\n                {""data"": input_shape.output[0], ""ends"": [3], ""starts"": [2], ""axes"": [0]})\n            diff_h = ctx.make_node(""Sub"", [output_h, expect_h])\n            diff_w = ctx.make_node(""Sub"", [output_w, expect_w])\n            const_two = ctx.make_const(utils.make_name(node.name + ""_const_two""), np.array([2], dtype=np.int64))\n            start_h = ctx.make_node(""Div"", [diff_h.output[0], const_two.output[0]])\n            start_w = ctx.make_node(""Div"", [diff_w.output[0], const_two.output[0]])\n            end_h = ctx.make_node(""Add"", [start_h.output[0], expect_h])\n            end_w = ctx.make_node(""Add"", [start_w.output[0], expect_w])\n            starts = ctx.make_node(""Concat"", [start_h.output[0], start_w.output[0]], attr={""axis"": 0})\n            ends = ctx.make_node(""Concat"", [end_h.output[0], end_w.output[0]], attr={""axis"": 0})\n            const_one_two = ctx.make_const(utils.make_name(node.name + ""_const_one_two""),\n                                           np.array([1, 2], dtype=np.int64))\n            slice_node = ctx.make_node(""Slice"",\n                                       [node.output[0], starts.output[0], ends.output[0], const_one_two.output[0]],\n                                       shapes=output_shape_orig)\n            downstream_nodes = ctx.find_output_consumers(node.output[0])\n            downstream_nodes.remove(output_shape)\n            downstream_nodes.remove(slice_node)\n            ctx.replace_all_inputs(downstream_nodes, node.output[0], slice_node.output[0])\n\n        conv_dims_attr(node, ""strides"")\n        conv_dims_attr(node, ""dilations"")\n\n        # remove output_shapes input\n        ctx.remove_input(node, node.input[0])\n        # swap data and kernel\n        t = node.input[0]\n        node.input[0] = node.input[1]\n        node.input[1] = t\n\n        conv_convert_inputs(ctx, node, with_kernel=True)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op([""DepthwiseConv2d"", ""DepthwiseConv2dNative""])\nclass DepthwiseConv2d:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = DepthwiseConv2dNative(T input, T filter, @list(int) strides, @string padding, @string data_format)\n        # T Y = ConvTranspose(T X, T W, T B, @AttrType.STRING auto_pad, @AttrType.INTS dilations, @AttrType.INT group,\n        #        @AttrType.INTS kernel_shape, @AttrType.INTS output_shape, @AttrType.INTS pads, @AttrType.INTS strides)\n        #\n        # this is not documented well in onnx, the hint comes from pytorch documentation:\n        # http://pytorch.org/docs/master/nn.html#torch.nn.Conv2d\n        #   The configuration when groups == in_channels and out_channels = K * in_channels\n        #   where K is a positive integer is termed in literature as depthwise convolution.\n        #   In other words, for an input of size (N,Cin,Hin,Win),\n        #   if you want a depthwise convolution with a depthwise multiplier K,\n        #   then you use the constructor arguments (in_channels=Cin,out_channels=Cin*K,...,groups=Cin)\n        #\n        node.type = ""Conv""\n        input_shape = ctx.get_shape(node.input[0])\n        if len(input_shape) != 4:\n            raise ValueError(""only Conv2D is supported"")\n\n        kernel_shape = ctx.get_shape(node.input[1])\n        if len(kernel_shape) != 4:\n            raise ValueError(""only Conv2D is supported"")\n        k_h, k_w, k_input_channels, k_channel_multiplier = kernel_shape\n        if k_input_channels < 1:\n            raise ValueError(""input channel must be positive"")\n        k_output_channels = k_input_channels * k_channel_multiplier\n\n        node.set_attr(""kernel_shape"", [k_h, k_w])\n        strides = conv_dims_attr(node, ""strides"")\n        conv_dims_attr(node, ""dilations"")\n        node.set_attr(""group"", k_input_channels)\n        add_padding(ctx, node, kernel_shape, strides)\n\n        new_kernel_shape = [k_h, k_w, 1, k_output_channels]\n        conv_convert_inputs(ctx, node, with_kernel=True, new_kernel_shape=new_kernel_shape)\n\n\n@tf_op([""AvgPool"", ""AvgPool3D""], onnx_op=""AveragePool"")\n@tf_op([""MaxPool"", ""MaxPoolV2""], onnx_op=""MaxPool"")\nclass PoolOp:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        cls._convert(ctx, node, **kwargs)\n\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        cls._convert(ctx, node, **kwargs)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # no change\n        cls._convert(ctx, node, **kwargs)\n\n    @classmethod\n    def _convert(cls, ctx, node, **kwargs):\n        # T output = MaxPool(T input, @list(int) ksize, @list(int) strides, @string padding, @string data_format)\n        # T Y = MaxPool(T X, @AttrType.STRING auto_pad, @AttrType.INTS kernel_shape, @AttrType.INTS pads,\n        #               @AttrType.INTS strides)\n        # above seems wrong - input[1] is ksize, input[2] is strides\n        # stride and ksize in tf is not always NHWC, so watch out when converting into onnx\'s NCHW\n        if len(node.input) < 3:\n            kernel_shape_tf = node.get_attr(""ksize"").ints\n            strides_tf = node.get_attr(""strides"").ints\n        else:\n            kernel_shape_tf = node.inputs[1].get_tensor_value()\n            strides_tf = node.inputs[2].get_tensor_value()\n            ctx.remove_input(node, node.input[2])\n            ctx.remove_input(node, node.input[1])\n\n        if node.is_nhwc():\n            kernel_shape_hw = kernel_shape_tf[1:3]\n            strides_hw = strides_tf[1:3]\n        else:\n            kernel_shape_hw = kernel_shape_tf[2:4]\n            strides_hw = strides_tf[2:4]\n        node.set_attr(""kernel_shape"", kernel_shape_hw)\n        node.set_attr(""strides"", strides_hw)\n        conv_dims_attr(node, ""dilations"")\n        add_padding(ctx, node, kernel_shape_hw, strides_hw)\n        conv_convert_inputs(ctx, node, with_kernel=False)\n\n\n@tf_op([""MaxPoolWithArgmax""], onnx_op=""MaxPool"")\nclass MaxPoolWithArgmaxOp:\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        # T output = MaxPool(T input, @list(int) ksize, @list(int) strides, @string padding, @string data_format)\n\n        # Set kernel_shape attribute\n        kernel_shape = node.get_attr(""ksize"").ints\n        kernel_shape = [kernel_shape[1], kernel_shape[2]]\n        node.set_attr(""kernel_shape"", kernel_shape)\n\n        # Set strides attribute\n        strides = node.get_attr(""strides"").ints\n        strides = [strides[1], strides[2]]\n        node.set_attr(""strides"", strides)\n\n        # The input data_format is NHWC for TF MaxPoolWithArgmax\n        node.set_attr(""data_format"", ""NHWC"")\n\n        add_padding(ctx, node, kernel_shape, strides)\n        conv_convert_inputs(ctx, node, with_kernel=False, input_indices=[0], output_indices=[0, 1])\n\n\n@tf_op([""BiasAdd"", ""BiasAddV1""])\nclass BiasAdd:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = BiasAdd(T value, T bias, @string data_format)\n        # T output = BiasAddV1(T value, T bias)\n        # TODO: for now use add. We may need to convert to NCHW.\n        node.type = ""Add""\n        common.BroadcastOp.version_1(ctx, node, **kwargs)\n\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # T output = BiasAdd(T value, T bias, @string data_format)\n        # T output = BiasAddV1(T value, T bias)\n        # According TF bias_add definition, the input dim is always only 1.\n        node.type = ""Add""\n        common.BroadcastOp.version_6(ctx, node, **kwargs)\n\n        # on NHWC, bias will broadcast from largest dim, which is default onnx Add op broadcast behavior.\n        if not node.is_nhwc():\n            # however, in NCHW, bias should be at 2nd dim, which by default onnx Add op has no way to know,\n            # so it needs being reshaped into 3-dim tensor before add\n            shape0 = ctx.get_shape(node.input[0])\n            shape1 = ctx.get_shape(node.input[1])\n            if node.inputs[1].type == \'Const\' and len(shape1) == 1:\n                new_broadcast_shape = [shape1[0]] + [1] * (len(shape0) - 2)\n                shape_name = utils.make_name(node.name)\n                ctx.make_const(shape_name, np.array(new_broadcast_shape, dtype=np.int64))\n                op_name = node.input[1]\n                reshape_node = ctx.make_node(""Reshape"", [op_name, shape_name])\n                ctx.replace_input(node, op_name, reshape_node.output[0])\n                ctx.set_shape(reshape_node.output[0], new_broadcast_shape)\n\n\n@tf_op([""Pad"", ""PadV2"", ""MirrorPad""], onnx_op=""Pad"")\nclass Pad:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        node.type = ""Pad""\n        # T output = Pad(T input, int32 paddings, @type Tpaddings), CONST model using default value\n        #  or PadV2(T input, int32 paddings, T constant_value, @type Tpaddings), CONST mode - default value specified\n        #  or MirrorPad(T input, int32 paddings, @type Tpaddings, @STRING mode), other mode.\n        # T output = Pad(T data, @STRING mode, @INTS pads, @FLOAT value)\n        paddings = np.array(node.inputs[1].get_tensor_value()).transpose().flatten()\n        mode = node.get_attr(""mode"")\n        if mode:\n            mode = mode.s.decode(""utf-8"").lower()\n            node.set_attr(""mode"", mode)\n        if mode not in [None, ""constant"", ""reflect""]:\n            raise ValueError(mode + "" pad mode is not supported"")\n\n        if mode in [None, ""constant""] and len(node.input) == 3:\n            const_val = node.inputs[2].get_tensor_value()\n            node.set_attr(""value"", const_val)\n            ctx.remove_input(node, node.input[2])\n\n        ctx.remove_input(node, node.input[1])\n        node.set_attr(""pads"", paddings)\n\n        origin_dtype = ctx.get_dtype(node.output[0])\n        if origin_dtype not in [onnx_pb.TensorProto.FLOAT16, onnx_pb.TensorProto.FLOAT,\n                                onnx_pb.TensorProto.DOUBLE]:\n            cast_node = ctx.insert_new_node_on_input(node, ""Cast"", node.input[0])\n            cast_node.set_attr(""to"", onnx_pb.TensorProto.FLOAT)\n            ctx.set_dtype(cast_node.output[0], onnx_pb.TensorProto.FLOAT)\n            ctx.copy_shape(node.name, cast_node.output[0])\n\n            cast_back_node = ctx.insert_new_node_on_output(""Cast"", node.output[0],\n                                                           name=utils.make_name(node.name) + ""_castback"")\n            cast_back_node.set_attr(""to"", origin_dtype)\n            ctx.set_dtype(cast_back_node.output[0], origin_dtype)\n            ctx.copy_shape(node.name, cast_back_node.output[0])\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        mode = node.get_attr(""mode"")\n        if mode:\n            mode = mode.s.decode(""utf-8"").lower()\n            node.set_attr(""mode"", mode)\n        if mode not in [None, ""constant"", ""reflect""]:\n            raise ValueError(mode + "" pad mode is not supported"")\n\n        # pads must be int64.\n        if ctx.get_dtype(node.input[1]) != onnx_pb.TensorProto.INT64:\n            ctx.insert_new_node_on_input(node, ""Cast"", node.input[1], to=onnx_pb.TensorProto.INT64)\n        ctx.insert_new_node_on_input(node, ""Transpose"", node.input[1])\n        shape_const = ctx.make_const(utils.make_name(node.name), np.array([-1]).astype(np.int64))\n        ctx.insert_new_node_on_input(node, ""Reshape"", [node.input[1], shape_const.name])\n\n        origin_dtype = ctx.get_dtype(node.output[0])\n        if origin_dtype not in [TensorProto.FLOAT, TensorProto.DOUBLE,\n                                TensorProto.INT32, TensorProto.INT64]:\n            cast_node = ctx.insert_new_node_on_input(node, ""Cast"", node.input[0])\n            cast_node.set_attr(""to"", TensorProto.FLOAT)\n            ctx.set_dtype(cast_node.output[0], TensorProto.FLOAT)\n            ctx.copy_shape(node.name, cast_node.output[0])\n\n            cast_back_node = ctx.insert_new_node_on_output(""Cast"", node.output[0],\n                                                           name=utils.make_name(node.name) + ""_castback"")\n            cast_back_node.set_attr(""to"", origin_dtype)\n            ctx.set_dtype(cast_back_node.output[0], origin_dtype)\n            ctx.copy_shape(node.name, cast_back_node.output[0])\n\n\n@tf_op([""FusedBatchNorm"", ""FusedBatchNormV2"", ""FusedBatchNormV3""])\nclass BatchNorm:\n    @classmethod\n    def version_6(cls, ctx, node, **kwargs):\n        node.type = ""BatchNormalization""\n        # tf inputs: x, scale, bias, mean, variance\n        # tf outputs: y, batch_mean, batch_var\n        # a: data_format, epsilon, is_training\n        # onnx inputs: X, scale, B, mean, variance, attributes: epsilon, momentum=0.9, spatial : 1\n        # output: y, mean, var, savedmean, savedvar,\n        # detach unused outputs. While we could let the unused outputs dangle,\n        # some runtimes like pytorch/caffe2 do complain about it.\n        consumers = [ctx.find_output_consumers(output_name) for output_name in node.output[1:]]\n        if not any(consumers):\n            new_output = [node.output[0]]\n            node.output = new_output\n\n        conv_convert_inputs(ctx, node, with_kernel=False)\n\n        scale_shape = ctx.get_shape(node.input[1])\n        mean_shape = ctx.get_shape(node.input[3])\n        var_shape = ctx.get_shape(node.input[4])\n        val_type = utils.map_onnx_to_numpy_type(ctx.get_dtype(node.input[1]))\n\n        if mean_shape != scale_shape:\n            new_mean_value = np.array(np.resize(node.inputs[3].get_tensor_value(as_list=False), scale_shape),\n                                      dtype=val_type)\n            new_mean_node_name = utils.make_name(node.name)\n            ctx.make_const(new_mean_node_name, new_mean_value)\n            node.input[3] = new_mean_node_name\n\n        if var_shape != scale_shape:\n            new_var_value = np.array(np.resize(node.inputs[4].get_tensor_value(as_list=False), scale_shape),\n                                     dtype=val_type)\n            new_val_node_name = utils.make_name(node.name)\n            ctx.make_const(new_val_node_name, new_var_value)\n            node.input[4] = new_val_node_name\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        # is_test was removed - no change for us\n        cls.version_6(ctx, node, **kwargs)\n\n\n@tf_op([""SpaceToDepth""])\nclass SpaceToDepth:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        block_size = node.get_attr(""block_size"")\n        node.set_attr(""blocksize"", block_size.i)\n        conv_convert_inputs(ctx, node, with_kernel=False)\n\n\n@tf_op([""DepthToSpace""])\nclass DepthToSpace:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        block_size = node.get_attr(""block_size"")\n        node.set_attr(""blocksize"", block_size.i)\n        conv_convert_inputs(ctx, node, with_kernel=False)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # Onnx-11 CRD mode added. No change for tf2onnx\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op([""CropAndResize""])\nclass CropAndResize:\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        utils.make_sure(node.inputs[1].type == ""Const"", ""boxes input must be a Const"")\n        utils.make_sure(node.inputs[3].type == ""Const"", ""boxes input must be a Const"")\n        name = node.name\n        output_height = node.inputs[3].get_tensor_value()[0]\n        output_width = node.inputs[3].get_tensor_value()[1]\n        rois = node.inputs[1].get_tensor_value()\n        rois_shape = ctx.get_shape(node.input[1])\n        img_shape = ctx.get_shape(node.input[0])\n        transform_rois = np.zeros(list(rois_shape), dtype=np.float32)\n        for i in range(rois_shape[0]):\n            y1, x1, y2, x2 = rois[i]\n            y1 = y1 * (img_shape[1] - 1)\n            y2 = y2 * (img_shape[1] - 1)\n            x1 = x1 * (img_shape[2] - 1)\n            x2 = x2 * (img_shape[2] - 1)\n            spacing_h = (y2 - y1)\n            spacing_w = (x2 - x1)\n            b1 = y1 - 0.5 * spacing_h / (output_height - 1)\n            a1 = x1 - 0.5 * spacing_w / (output_width - 1)\n            b2 = y2 + 0.5 * spacing_h / (output_height - 1)\n            a2 = x2 + 0.5 * spacing_w / (output_width - 1)\n            transform_rois[i][0] = a1\n            transform_rois[i][1] = b1\n            transform_rois[i][2] = a2\n            transform_rois[i][3] = b2\n        cast_node = ctx.make_node(""Cast"", [node.input[2]], attr={""to"": onnx_pb.TensorProto.INT64})\n        bbox_node = ctx.make_const(utils.make_name(""bbox""), transform_rois)\n        dtypes = [ctx.get_dtype(node.output[0])]\n        shapes = [ctx.get_shape(node.output[0])]\n        input_nchw = ctx.make_node(""Transpose"", [node.input[0]], {""perm"": [0, 3, 1, 2]},\n                                   name=utils.make_name(node.name))\n        crop_and_resize = ctx.make_node(""RoiAlign"", inputs=[input_nchw.output[0], bbox_node.output[0],\n                                                            cast_node.output[0]],\n                                        attr={""output_height"": output_height, ""output_width"": output_width,\n                                              ""spatial_scale"": 1.0, ""sampling_ratio"": 1},\n                                        name=utils.make_name(node.name), dtypes=dtypes, shapes=shapes)\n        ctx.remove_node(name)\n        ctx.make_node(""Transpose"", crop_and_resize.output, {""perm"": [0, 2, 3, 1]},\n                      name=name, outputs=node.output, shapes=shapes, dtypes=dtypes)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # create loop of resize to cater to tensorflow CropAndResize, one box one iteration\n        mode = ""nearest"" if node.get_attr(""method"") is not None and node.get_attr(\n            ""method"").s == b""nearest"" else ""linear""\n        extrapolation_value = float(node.get_attr(""extrapolation_value"", ""0"").f)\n        input_x = node.inputs[0]\n        boxes = node.inputs[1]\n        box_ind = node.inputs[2]\n        crop_size = node.inputs[3]\n        trip_name = utils.make_name(node.name + ""_i"")\n        cond_name = utils.make_name(node.name + ""_cond"")\n        cond_out_name = utils.make_name(node.name + ""cond_out"")\n        g = ctx.create_new_graph_with_same_config()\n        g.add_graph_input(trip_name, TensorProto.INT64, [1])\n        g.add_graph_input(cond_name, TensorProto.BOOL, [])\n        g.parent_graph = ctx\n        const_zero = g.make_const(utils.make_name(node.name + ""_const_zero""), np.array([0], dtype=np.int32))\n        const_zero_long = g.make_const(utils.make_name(node.name + ""_const_zero_long""), np.array([0], dtype=np.int64))\n        const_one = g.make_const(utils.make_name(node.name + ""_const_one""), np.array([1], dtype=np.int32))\n        const_one_long = g.make_const(utils.make_name(node.name + ""_const_one_long""), np.array([1], dtype=np.int64))\n        index_end = g.make_node(""Add"", [trip_name, const_one_long.output[0]])\n        box_index_from = g.make_node(""Slice"", [box_ind.output[0], trip_name, index_end.output[0]], name=""Slice_a"")\n        box_index_to = g.make_node(""Add"", [box_index_from.output[0], const_one.output[0]])\n        target_x = g.make_node(""Slice"", [input_x.output[0], box_index_from.output[0], box_index_to.output[0],\n                                         const_zero.output[0]], name=""Slice_b"")\n        transposed_x = g.make_node(""Transpose"", [target_x.output[0]], attr={\'perm\': constants.NHWC_TO_NCHW})\n        const_zero_zero = g.make_const(utils.make_name(node.name + ""_const_zero_zero""),\n                                       np.array([0, 0], dtype=np.float32))\n        const_one_one = g.make_const(utils.make_name(node.name + ""_const_one_one""),\n                                     np.array([1, 1], dtype=np.float32))\n        const_four = g.make_const(utils.make_name(node.name + ""_const_four""), np.array([4], dtype=np.int64))\n        const_empty_float = g.make_const(utils.make_name(""const_empty_float""), np.array([], dtype=np.float32))\n        box = g.make_node(""Slice"", [boxes.output[0], trip_name, index_end.output[0], const_zero_long.output[0]],\n                          name=""Slice_c"")\n        roi_raw = g.make_node(""Reshape"", [box.output[0], const_four.output[0]])\n        roi_raw_first_half = GraphBuilder(g).make_slice({""data"": roi_raw.output[0], ""ends"": [2], ""starts"": [0]})\n        roi_raw_second_half = GraphBuilder(g).make_slice({""data"": roi_raw.output[0], ""ends"": [4], ""starts"": [2]})\n        roi_concat_1 = g.make_node(""Concat"", [const_zero_zero.output[0], roi_raw_first_half], attr={\'axis\': 0})\n        roi_concat_2 = g.make_node(""Concat"", [const_one_one.output[0], roi_raw_second_half], attr={\'axis\': 0})\n        final_roi = g.make_node(""Concat"", [roi_concat_1.output[0], roi_concat_2.output[0]], attr={\'axis\': 0})\n        final_crop_size = build_dynamic_target_size(g, transposed_x, crop_size)\n        resized_x = g.make_node(""Resize"", [transposed_x.output[0], final_roi.output[0], const_empty_float.output[0],\n                                           final_crop_size.output[0]],\n                                attr={""mode"": mode, ""extrapolation_value"": extrapolation_value,\n                                      ""coordinate_transformation_mode"": ""tf_crop_and_resize""})\n        recovered_x = g.make_node(""Transpose"", [resized_x.output[0]], attr={\'perm\': constants.NCHW_TO_NHWC})\n        squeeze_x = g.make_node(""Squeeze"", inputs=[recovered_x.output[0]], attr={""axes"": [0]})\n        g.make_node(""Identity"", [cond_name], outputs=[cond_out_name])\n        g.add_graph_output(cond_out_name, TensorProto.BOOL, [])\n        g.add_graph_output(squeeze_x.output[0], ctx.get_dtype(node.input[0]), [-1, -1, -1])\n        trip_node = ctx.make_node(""Size"", [box_ind.output[0]])\n        cond_const = ctx.make_const(utils.make_name(""cond""), np.ones((), dtype=np.bool))\n        ctx.remove_node(node.name)\n        inner_loop = ctx.make_node(""Loop"", [trip_node.output[0], cond_const.output[0]], name=node.name,\n                                   outputs=node.output)\n        inner_loop.set_body_graph_as_attr(""body"", g)\n\n\n@tf_op([""ResizeBilinear"", ""ResizeNearestNeighbor""])\nclass Resize:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        mode = ""linear"" if node.type == ""ResizeBilinear"" else ""nearest""\n        node.type = ""Upsample""\n        shape = ctx.get_shape(node.input[0])\n        target_shape = node.inputs[1].get_tensor_value()\n        # https://www.tensorflow.org/api_docs/python/tf/image/resize_nearest_neighbor\n        # wants the input to be NHWC - adjust target_shape to this.\n        n, h, w, c = shape\n        nh, nw = target_shape\n        utils.make_sure(all(i != -1 for i in [nh, nw]), ""h and w need to be known"")\n        # scaler is nchw\n        scaler = [1., 1., float(nh) / h, float(nw) / w]\n        node.set_attr(""scales"", scaler)\n        node.set_attr(""mode"", mode)\n        ctx.remove_input(node, node.input[1])\n        node.data_format = ""NHWC""\n        conv_convert_inputs(ctx, node, with_kernel=False)\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        cls._convert_since_9(ctx, node, op_type=""Upsample"")\n\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        cls._convert_since_9(ctx, node, op_type=""Resize"")\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        mode = ""linear"" if node.type == ""ResizeBilinear"" else ""nearest""\n        roi = ctx.make_const(utils.make_name(""roi""), np.array([]).astype(np.float32))\n        const_zero = ctx.make_const(utils.make_name(""const_zero""), np.array([0]).astype(np.int64))\n        const_two = ctx.make_const(utils.make_name(""const_two""), np.array([2]).astype(np.int64))\n        const_empty_float = ctx.make_const(utils.make_name(""const_empty_float""), np.array([]).astype(np.float32))\n        input_nchw = ctx.make_node(""Transpose"", [node.input[0]], {""perm"": constants.NHWC_TO_NCHW})\n        shape_input = ctx.make_node(""Shape"", [input_nchw.output[0]])\n        sliced_shape = ctx.make_node(""Slice"", [shape_input.output[0], const_zero.output[0], const_two.output[0]])\n        size_int64 = ctx.make_node(""Cast"", [node.input[1]], attr={""to"": onnx_pb.TensorProto.INT64})\n        concat_shape = ctx.make_node(""Concat"", [sliced_shape.output[0], size_int64.output[0]], {\'axis\': 0})\n        resize_inputs = [\n            input_nchw.output[0],\n            roi.output[0],\n            const_empty_float.output[0],\n            concat_shape.output[0]\n        ]\n        transformation_mode = ""asymmetric""\n        if ""half_pixel_centers"" in node.attr and node.attr[""half_pixel_centers""].i:\n            transformation_mode = ""half_pixel""\n        resize = ctx.make_node(""Resize"", resize_inputs,\n                               attr={""mode"": mode, ""nearest_mode"": ""floor"",\n                                     ""coordinate_transformation_mode"": transformation_mode})\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(""Transpose"", resize.output, {""perm"": constants.NCHW_TO_NHWC},\n                      name=node.name, outputs=node.output, shapes=shapes, dtypes=dtypes)\n\n    @classmethod\n    def _convert_since_9(cls, ctx, node, op_type, use_target_size=False):\n\n        # float32 out = ResizeBilinear/ResizeNearestNeighbor(T images, int size)\n        # https://www.tensorflow.org/api_docs/python/tf/image/resize_nearest_neighbor\n        # wants the input to be NHWC - adjust target_shape to this.\n        mode = ""linear"" if node.type == ""ResizeBilinear"" else ""nearest""\n\n        # because onnxruntime only supports to scale the last two dims so transpose is inserted\n        input_nchw = ctx.make_node(""Transpose"", [node.input[0]], {""perm"": constants.NHWC_TO_NCHW})\n        if use_target_size:\n            final_target_size = build_dynamic_target_size(ctx, input_nchw, node.inputs[1])\n            roi = ctx.make_const(utils.make_name(""roi""), np.array([]).astype(np.float32))\n            const_empty_float = ctx.make_const(utils.make_name(""const_empty_float""), np.array([], dtype=np.float32))\n            resize_inputs = [\n                input_nchw.output[0],\n                roi.output[0],\n                const_empty_float.output[0],\n                final_target_size.output[0]\n            ]\n            upsample = ctx.make_node(""Resize"", resize_inputs,\n                                     attr={""mode"": mode, ""nearest_mode"": ""floor"",\n                                           ""coordinate_transformation_mode"": ""asymmetric""})\n        else:\n            # first create ""scales"" info for onnx upsample\n            # if shape of input and output known then  ""scale"" is calculated statically and set as a const node\n            shape = ctx.get_shape(node.input[0])\n            if shape and shape[2] != -1 and shape[1] != -1 and node.inputs[1].is_const():\n                target_shape = node.inputs[1].get_tensor_value()\n                n, h, w, c = shape\n                nh, nw = target_shape\n                # scales is nchw\n                # the reason not storing data at raw field is because of the bug:\n                # https://github.com/onnx/onnx/issues/1852\n                scale_val = np.array([1.0, 1.0, float(nh) / h, float(nw) / w]).astype(np.float32)\n                scales = ctx.make_const(utils.make_name(""scales""), scale_val, raw=False)\n            else:\n                ori_shape = ctx.make_node(""Shape"", [node.input[0]])\n                attr = {""axes"": [0], ""starts"": [1], ""ends"": [3]}\n                inputs_map = {""data"": ori_shape.output[0], **attr}\n                ori_shape_hw = GraphBuilder(ctx).make_slice(inputs_map)\n                ori_shape_hw_float = ctx.make_node(""Cast"", [ori_shape_hw], attr={""to"": onnx_pb.TensorProto.FLOAT})\n\n                target_hw = node.inputs[1]\n                target_hw_float = ctx.make_node(""Cast"", target_hw.output, attr={""to"": onnx_pb.TensorProto.FLOAT})\n\n                scales_hw = ctx.make_node(""Div"", [target_hw_float.output[0], ori_shape_hw_float.output[0]])\n\n                const_one_array = ctx.make_const(utils.make_name(""one""), np.array([1.0, 1.0]).astype(np.float32))\n                # scales is nchw\n                scales = ctx.make_node(""Concat"", [const_one_array.output[0], scales_hw.output[0]], {""axis"": 0})\n            upsample = ctx.make_node(op_type, [input_nchw.output[0], scales.output[0]], attr={""mode"": mode})\n\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(""Transpose"", upsample.output, {""perm"": constants.NCHW_TO_NHWC},\n                      name=node.name, outputs=node.output, shapes=shapes, dtypes=dtypes)\n\n\n@tf_op(""MatrixBandPart"")\nclass MatrixBandPart:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # T output = MatrixBandPart(T input, int num_lower, int num_upper)\n        # data-flow: first generate mask matrix and then use element-wise mul op\n        input_rank = len(ctx.get_shape(node.input[0]))\n        utils.make_sure(input_rank == 2, error_msg=""MatrixBandPart op: only rank 2 is supported"")\n        bandpart = [node.inputs[ind].get_tensor_value() for ind in [1, 2]]\n        utils.make_sure(bandpart in [[-1, 0], [0, -1]], ""only support Lower/Upper triangular for now"")\n        # methods to generate mask matrix: if lower triangular is needed, then generate column one by one\n        # otherwise row is generated one by one.\n        axis, counter_axis, squeeze_axis = (1, 0, 2) if bandpart == [-1, 0] else (0, 1, 1)\n        # 1: subgraph to implement tf.onelike(input[:, 0]),\n        # no need to worry about the dtype, because bool type is needed as Xor only support bool\n        node_name = utils.make_name(""const_zero"")\n        const_zero = ctx.make_const(name=node_name, np_val=np.array([0]).astype(np.int32))\n        first_col_or_row = ctx.make_node(op_type=""Gather"", inputs=[node.input[0], const_zero.output[0]],\n                                         attr={""axis"": axis})\n        first_col_or_row_casted = ctx.make_node(op_type=""Cast"", inputs=first_col_or_row.output,\n                                                attr={""to"": onnx_pb.TensorProto.BOOL})\n        # line means one col or one row\n        zero_line = ctx.make_node(op_type=""Xor"", inputs=first_col_or_row_casted.output * 2)\n        one_line = ctx.make_node(op_type=""Not"", inputs=zero_line.output)\n\n        # 2: ""loop"" to generate mask matrix: generate col or row of matrix one by one\n        g = ctx.create_new_graph_with_same_config()\n        node_name = utils.make_name(""const_zero_bool"")\n        const_zero_bool = g.make_const(name=node_name, np_val=np.array([[0]]).astype(np.bool))\n        g.set_dtype(const_zero_bool.output[0], onnx_pb.TensorProto.BOOL)\n\n        g.add_graph_input(""trip"", onnx_pb.TensorProto.INT64, [])\n        g.add_graph_input(""cond"", onnx_pb.TensorProto.BOOL, [])\n        g.add_graph_input(""line"", onnx_pb.TensorProto.BOOL, [-1, -1])\n\n        # shift right the line and add zero at the left.\n        new_line = g.make_node(op_type=""Concat"", inputs=[const_zero_bool.output[0], ""line""],\n                               attr={""axis"": counter_axis},\n                               dtypes=[onnx_pb.TensorProto.BOOL])\n        attr = {""axes"": [counter_axis], ""starts"": [0], ""ends"": [-1]}\n        inputs_map = {""data"": new_line.output[0], **attr}\n        slice_node = GraphBuilder(g).make_slice(inputs_map)\n\n        g.make_node(""Identity"", [""cond""], outputs=[""cond_out""])\n        g.make_node(""Identity"", [""line""], outputs=[""res""])\n        g.make_node(""Identity"", [slice_node], outputs=[""line_out""])\n\n        g.add_graph_output(""cond_out"", onnx_pb.TensorProto.BOOL, [])\n        g.add_graph_output(""line_out"", onnx_pb.TensorProto.BOOL, [-1, -1])\n        g.add_graph_output(""res"", onnx_pb.TensorProto.BOOL, [-1, -1])\n\n        # initial value of body vars\n        shape = ctx.make_node(op_type=""Shape"", inputs=[node.input[0]])  # dtype of result is int64\n        node_name = utils.make_name(""line_num_index"")\n        col_or_row_num_index = ctx.make_const(name=node_name, np_val=np.array(axis).astype(np.int32))\n        line_num = ctx.make_node(op_type=""Gather"", inputs=[shape.output[0], col_or_row_num_index.output[0]])\n        trip_cnt = line_num.output[0]\n        node_name = utils.make_name(""true"")\n        cond = ctx.make_const(name=node_name, np_val=np.array(1).astype(np.bool))\n        col_init = one_line.output[0]\n\n        loop_node = ctx.make_node(op_type=""Loop"", inputs=[trip_cnt, cond.output[0], col_init], output_count=2)\n        loop_node.set_body_graph_as_attr(""body"", g)\n        # convert generated mask matrix from bool to right shape and data type\n        squeeze = ctx.make_node(op_type=""Squeeze"", inputs=[loop_node.output[1]], attr={""axes"": [squeeze_axis]})\n        cast1 = ctx.make_node(op_type=""Cast"", inputs=squeeze.output, attr={""to"": onnx_pb.TensorProto.FLOAT})\n        if axis == 1:\n            mask_matrix = ctx.make_node(op_type=""Transpose"", inputs=cast1.output)\n        else:\n            mask_matrix = squeeze\n        cast2 = ctx.make_node(op_type=""Cast"", inputs=mask_matrix.output,\n                              attr={""to"": ctx.get_dtype(node.input[0])})\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(op_type=""Mul"", inputs=[cast2.output[0], node.input[0]],\n                      name=node.name, outputs=node.output, shapes=shapes,\n                      dtypes=dtypes)\n\n\ndef _make_softmax_cross_entropy_with_logits(ctx, label, logit, tf_ori_node):\n    label_dtype = ctx.get_dtype(label.output[0])\n    logit_dtype = ctx.get_dtype(logit.output[0])\n    utils.make_sure(label_dtype == logit_dtype, ""the following logic only works on same dtype of label and logit"")\n\n    log_softmax = ctx.make_node(op_type=""LogSoftmax"", inputs=logit.output)\n    # implement tf.multiply(-1, tf.reduce_sum(tf.multiply(label, log_softmax), axis=1))\n    mul1 = ctx.make_node(op_type=""Mul"", inputs=[label.output[0], log_softmax.output[0]])\n    reduce_sum = ctx.make_node(op_type=""ReduceSum"", inputs=[mul1.output[0]], attr={""axes"": [-1]})\n    const_negative_one = ctx.make_const(name=utils.make_name(""const_negative_one""),\n                                        np_val=np.array(-1).astype(utils.ONNX_TO_NUMPY_DTYPE[logit_dtype]))\n    mul2 = ctx.make_node(op_type=""Mul"", inputs=[const_negative_one.output[0], reduce_sum.output[0]])\n    shapes = tf_ori_node.output_shapes\n    dtypes = tf_ori_node.output_dtypes\n    ctx.remove_node(tf_ori_node.name)\n    ctx.make_node(op_type=""Squeeze"", inputs=[mul2.output[0]], attr={""axes"": [1]},\n                  outputs=[tf_ori_node.output[0]], shapes=[shapes[0]], dtypes=[dtypes[0]])\n\n\ndef sparse_softmax_cross_entropy_with_logits_op_by_gathernd(ctx, node, **kwargs):\n    # make subgraph to implement one_hot, idea comes from onehot_op\n    indices_name = node.input[1]\n    indices_shape = ctx.get_shape(indices_name)\n    if len(indices_shape) != 1:\n        # TODO: this works for rank=1 but tensorflow supports more than this.\n        # Same principle should work but we need to implement our own eye.\n        raise ValueError(""onehot op: only rank1 is supported"")\n    logit_name = node.input[0]\n    logit_dtype = ctx.get_dtype(logit_name)\n    logit_shape = ctx.get_shape(logit_name)\n    utils.make_sure(logit_dtype, ""Dtype of {} is None"".format(logit_name))\n    indices_dtype = ctx.get_dtype(indices_name)\n    if indices_dtype != TensorProto.INT64:\n        indices_cast = ctx.make_node(""Cast"", [indices_name], attr={""to"": TensorProto.INT64})\n        indices_name = indices_cast.output[0]\n    indices_size = ctx.make_node(""Size"", [indices_name])\n    indices_unsqueeze = ctx.make_node(""Unsqueeze"", [indices_name], attr={""axes"": [1]})\n    zero_const = ctx.make_const(utils.make_name(""zero""), np.array(0, dtype=np.int64))\n    one_const = ctx.make_const(utils.make_name(""one""), np.array(1, dtype=np.int64))\n    id_name = utils.make_name(""sparse_softmax_id"")\n    id_output = utils.port_name(id_name)\n    controlflow.make_range(ctx, zero_const.output[0], indices_size.output[0], one_const.output[0],\n                           id_output, id_name, shape=[-1], dtype=TensorProto.INT64)\n    id_unsqueeze = ctx.make_node(""Unsqueeze"", [id_output], attr={""axes"": [1]})\n    indices_with_id = ctx.make_node(""Concat"",\n                                    [id_unsqueeze.output[0], indices_unsqueeze.output[0]],\n                                    attr={""axis"": 1})\n    log_softmax = ctx.make_node(op_type=""LogSoftmax"",\n                                inputs=[logit_name], dtypes=[logit_dtype], shapes=[logit_shape])\n    gathernd_name = utils.make_name(""sparse_softmax_gathernd"")\n    gathernd_output = utils.port_name(gathernd_name)\n    tensor.make_gathernd(ctx, log_softmax.output[0], indices_with_id.output[0], gathernd_output,\n                         gathernd_name, logit_dtype, [logit_shape], [logit_dtype])\n    const_name = utils.make_name(""const_negative_one"")\n    const_negative_one = ctx.make_const(const_name, np.array(-1).astype(utils.map_onnx_to_numpy_type(logit_dtype)))\n    mul2 = ctx.make_node(op_type=""Mul"", inputs=[const_negative_one.output[0], gathernd_output])\n    shapes = node.output_shapes\n    dtypes = node.output_dtypes\n    ctx.remove_node(node.name)\n    ctx.make_node(op_type=""Squeeze"",\n                  inputs=[mul2.output[0]], outputs=[node.output[0]],\n                  attr={""axes"": [1]}, shapes=[shapes[0]], dtypes=[dtypes[0]])\n\n\n@tf_op(""SoftmaxCrossEntropyWithLogits"")\nclass SoftmaxCrossEntropyWithLogits:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        logits = node.inputs[0]\n        logit_dtype = ctx.get_dtype(logits.output[0])\n        labels = node.inputs[1]\n        label_dtype = ctx.get_dtype(labels.output[0])\n        if label_dtype != logit_dtype:\n            labels = ctx.make_node(""Cast"", labels.output, attr={""to"": logit_dtype}, dtypes=[logit_dtype])\n\n        _make_softmax_cross_entropy_with_logits(ctx, labels, logits, node)\n\n\ndef _make_sparse_softmax_cross_entropy_with_logits(ctx, label, logit, tf_ori_node):\n    logit = logit.output[0]\n    label = label.output[0]\n    label_dtype = ctx.get_dtype(label)\n    logit_dtype = ctx.get_dtype(logit)\n    utils.make_sure(label_dtype == logit_dtype, ""the following logic only works on same dtype of label and logit"")\n\n    # when label is onehot, logic ""tf.multiply(-1, tf.reduce_sum(tf.multiply(label, log_softmax), axis=1))"" is equal to\n    # ""-log(q_i)"" where i is the selected index specified by label, q_i = logic_i/sum, the detail process is as follows:\n    # logit_exp=exp(logit) >> sum = tf.reduce_sum(logit_exp, axis = -1), masked_sum = reduce_sum(mul(logit_exp, mul))\n    # >> -log(masked_sum/sum)\n    logit_exp = ctx.make_node(op_type=""Exp"", inputs=[logit]).output[0]\n    logit_exp_sum = ctx.make_node(op_type=""ReduceSum"", inputs=[logit_exp], attr={""axes"": [-1], ""keepdims"": 0}).output[0]\n    masked = ctx.make_node(op_type=""Mul"", inputs=[label, logit_exp]).output[0]\n    masked_sum = ctx.make_node(op_type=""ReduceSum"", inputs=[masked], attr={""axes"": [-1], ""keepdims"": 0}).output[0]\n    probability = ctx.make_node(op_type=""Div"", inputs=[masked_sum, logit_exp_sum]).output[0]\n    log_prob = ctx.make_node(op_type=""Log"", inputs=[probability]).output[0]\n    const_negative_one = ctx.make_const(name=utils.make_name(""const_negative_one""),\n                                        np_val=np.array(-1).astype(utils.ONNX_TO_NUMPY_DTYPE[logit_dtype])).output[0]\n\n    shapes = tf_ori_node.output_shapes\n    dtypes = tf_ori_node.output_dtypes\n    ctx.remove_node(tf_ori_node.name)\n    ctx.make_node(op_type=""Mul"", inputs=[log_prob, const_negative_one],\n                  outputs=[tf_ori_node.output[0]], shapes=[shapes[0]], dtypes=[dtypes[0]])\n\n\n@tf_op(""SparseSoftmaxCrossEntropyWithLogits"")\nclass SparseSoftmaxCrossEntropyWithLogits:\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # make subgraph to implement one_hot, idea comes from onehot_op\n        indices_name = node.input[1]\n        indices_shape = ctx.get_shape(indices_name)\n        if len(indices_shape) != 1:\n            # TODO: this works for rank=1 but tensorflow supports more than this.\n            # Same principle should work but we need to implement our own eye.\n            raise ValueError(""onehot op: only rank1 is supported"")\n        logit_name = node.input[0]\n        depth = ctx.get_shape(logit_name)[-1]\n        # if number of classes is unknown or too large\n        if depth == utils.ONNX_UNKNOWN_DIMENSION or depth > 20000:\n            sparse_softmax_cross_entropy_with_logits_op_by_gathernd(ctx, node, **kwargs)\n            return\n        logit_dtype = ctx.get_dtype(logit_name)\n        utils.make_sure(logit_dtype, ""Dtype of {} is None"".format(logit_name))\n\n        dtype = utils.map_onnx_to_numpy_type(logit_dtype)\n        eye = np.eye(depth).astype(dtype)\n        const_name = utils.make_name(""const_eye"")\n        const_eye = ctx.make_const(name=const_name, np_val=eye)\n        onehot = ctx.make_node(op_type=""Gather"", inputs=[const_eye.output[0], indices_name], attr={""axis"": 0})\n        log_softmax = ctx.make_node(op_type=""LogSoftmax"", inputs=[logit_name])\n        # implement tf.multiply(np.float32(-1.0), tf.reduce_sum(tf.multiply(one_hot, log_softmax), axis=1))\n        mul1 = ctx.make_node(op_type=""Mul"", inputs=[onehot.output[0], log_softmax.output[0]])\n        reduce_sum = ctx.make_node(op_type=""ReduceSum"", inputs=[mul1.output[0]], attr={""axes"": [1]})\n        const_name = utils.make_name(""const_negative_one"")\n        const_negative_one = ctx.make_const(name=const_name, np_val=np.array(-1).astype(dtype))\n        mul2 = ctx.make_node(op_type=""Mul"", inputs=[const_negative_one.output[0], reduce_sum.output[0]])\n\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(op_type=""Squeeze"", inputs=[mul2.output[0]], outputs=[node.output[0]], attr={""axes"": [1]},\n                      shapes=[shapes[0]], dtypes=[dtypes[0]])\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        # float32/64 output = SparseSoftmaxCrossEntropyWithLogits(float32/64 features, int32/64 labels)\n        # the detail math process of this op is: a = onehot(labels), b = logsoftmax(features), reduce_sum(mul(a, b))\n        logit_node = node.inputs[0]\n        logit_shape = ctx.get_shape(node.input[0])\n        logit_dtype = ctx.get_dtype(node.input[0])\n\n        label_name = node.input[1]\n\n        if logit_shape is not None and logit_shape[-1] != -1:\n            num_class = logit_shape[-1]\n            node_nme = utils.make_name(""onehot_depth"")\n            depth_node = ctx.make_const(node_nme, np.array([num_class]).astype(np.int64)).output[0]\n        else:\n            logit_shape = ctx.make_node(""Shape"", [node.input[0]]).output[0]\n            slice_args = {""data"": logit_shape,\n                          ""starts"": [-1], ""ends"": [int(utils.get_max_value(np.int32))]}\n            num_class = GraphBuilder(ctx).make_slice(kwargs=slice_args)\n            depth_node = num_class\n        values_node = ctx.make_const(utils.make_name(""onehot_values""), np.array([0, 1]).astype(np.int64)).output[0]\n        label_dtype = ctx.get_dtype(label_name)\n        if label_dtype != TensorProto.INT64:\n            onehot_indice = ctx.make_node(""Cast"", [label_name], attr={""to"": TensorProto.INT64}).output[0]\n        else:\n            onehot_indice = label_name\n        label_node = ctx.make_node(op_type=""OneHot"",\n                                   inputs=[onehot_indice, depth_node, values_node])\n        # the above logic makes output dtype of label_node now always int64\n        # make sure label has same dtype as logit\n        if logit_dtype != TensorProto.INT64:\n            label_node = ctx.make_node(""Cast"", label_node.output, attr={""to"": logit_dtype}, dtypes=[logit_dtype])\n\n        _make_sparse_softmax_cross_entropy_with_logits(ctx, label_node, logit_node, node)\n'"
tf2onnx/onnx_opset/reduction.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nreduction\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nimport numpy as np\nfrom onnx import onnx_pb, helper\n\nfrom tf2onnx import utils\nfrom tf2onnx.handler import tf_op\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=unused-argument,missing-docstring\n\n@tf_op(""Min"", onnx_op=""ReduceMin"")\n@tf_op(""Max"", onnx_op=""ReduceMax"")\n@tf_op(""Mean"", onnx_op=""ReduceMean"")\n@tf_op(""Sum"", onnx_op=""ReduceSum"")\n@tf_op(""Prod"", onnx_op=""ReduceProd"")\nclass ReduceOpBase:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        axes_node = node.inputs[1]\n        axes = axes_node.get_tensor_value()\n        if np.isscalar(axes):\n            axes = [axes]\n        input_shape = ctx.get_shape(node.input[0])\n        if input_shape is None:\n            if any([val < 0 for val in axes]):\n                raise ValueError(""reduce_op: cannot have negative axis because we don\'t know input rank"")\n        else:\n            input_rank = len(ctx.get_shape(node.input[0]))\n            axes = [val + input_rank if val < 0 else val for val in axes]\n\n        node.set_attr(""axes"", axes)\n        ctx.remove_input(node, node.input[1])\n        keep_dims = node.get_attr(""keep_dims"")\n        if keep_dims:\n            del node.attr[\'keep_dims\']\n            node.set_attr(""keepdims"", keep_dims.i)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # Opset 11 supports negative axis, but core logic is same\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op([""ArgMax"", ""ArgMin""])\nclass ArgMax:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # output_type output = ArgMin(T input, Tidx dimension, @type Tidx, @type output_type)\n        # tensor(int32) reduced = ArgMin(T data, @INT axis, @INT keepdims)\n        axis_node = node.inputs[1]\n        axis = axis_node.get_tensor_value()\n        if axis < 0:\n            # ArgMax|ArgMin in onnx don\'t necessary support negative axis(not in doc explicitly)\n            input_shape = ctx.get_shape(node.input[0])\n            dim_count = len(input_shape) if input_shape else 0\n            axis = dim_count + axis\n\n        # TF ArgMin/ArgMax may return int32 or int64\n        # Onnx ArgMin/ArgMax only supports int64 output, add cast if needed\n        if node.get_attr_int(""output_type"") == onnx_pb.TensorProto.INT32:\n            # current node will return int64 after conversion, which differs from previous dtype got from tf\n            ctx.set_dtype(node.output[0], onnx_pb.TensorProto.INT64)\n            op_name = utils.make_name(""Cast"")\n            cast_node = ctx.insert_new_node_on_output(""Cast"", node.output[0], name=op_name,\n                                                      to=onnx_pb.TensorProto.INT32)\n            ctx.set_dtype(cast_node.output[0], onnx_pb.TensorProto.INT32)\n            ctx.copy_shape(node.output[0], cast_node.output[0])\n\n        node.set_attr(""axis"", axis)\n        node.set_attr(""keepdims"", 0)\n        ctx.remove_input(node, node.input[1])\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # Opset 11 supports negative axis, but core logic same\n        cls.version_1(ctx, node, **kwargs)\n\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        # Opset 12 adds extra attribute \'select_last_index\'\n        # No changes needed\n        cls.version_1(ctx, node, **kwargs)\n\n@tf_op([""All"", ""Any""])\nclass AllAny:\n    @classmethod\n    def version_6(cls, ctx, node, **kwargs):\n        # T output = All(T x, list(int) reduce_indices, @bool keepdims)\n        # T output = Any(T x, list(int) reduce_indices, @bool keepdims)\n        reduce_dim = node.inputs[1].get_tensor_value()\n\n        # for Any, the reduce_indices can be scalar as observed.\n        if np.isscalar(reduce_dim):\n            reduce_dim = [reduce_dim]\n\n        if ctx.opset < 11:\n            utils.make_sure(all(i >= 0 for i in reduce_dim), ""negative reduce axis is not supported in onnx for now"")\n\n        cast = ctx.make_node(op_type=""Cast"", inputs=[node.input[0]], attr={""to"": onnx_pb.TensorProto.FLOAT})\n        keepdims = helper.get_attribute_value(node.get_attr(""keep_dims""))\n        op_type = ""ReduceMin"" if node.type == ""All"" else ""ReduceSum""\n        reduce_node = ctx.make_node(op_type=op_type, inputs=cast.output,\n                                    attr={""axes"": reduce_dim, ""keepdims"": keepdims})\n\n        zero_node = ctx.make_const(utils.make_name(""zero_reduce""), np.array(0, dtype=np.float32))\n\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(op_type=""Greater"", inputs=[reduce_node.output[0], zero_node.output[0]],\n                      name=node.name, outputs=node.output, shapes=shapes, dtypes=dtypes)\n\n\n@tf_op(""AddN"")\nclass AddN():\n    @classmethod\n    def version_6(cls, ctx, node, **kwargs):\n        node.type = ""Sum""\n'"
tf2onnx/onnx_opset/rnn.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\nrnn\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nimport numpy as np\nfrom tf2onnx import utils\nfrom tf2onnx.handler import tf_op\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=unused-argument,missing-docstring\n\n@tf_op(""LSTMBlockCell"")\nclass LSTMBlockCell:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        """"""\n        Args:\n          x: A `Tensor`. Must be one of the following types: `float32`.\n            The input to the LSTM cell, shape (batch_size, num_inputs).\n          cs_prev: A `Tensor`. Must have the same type as `x`.\n            Value of the cell state at previous time step.\n          h_prev: A `Tensor`. Must have the same type as `x`.\n            Output of the previous cell at previous time step.\n          w: A `Tensor`. Must have the same type as `x`. The weight matrix.\n          wci: A `Tensor`. Must have the same type as `x`.\n            The weight matrix for input gate peephole connection.\n          wcf: A `Tensor`. Must have the same type as `x`.\n            The weight matrix for forget gate peephole connection.\n          wco: A `Tensor`. Must have the same type as `x`.\n            The weight matrix for output gate peephole connection.\n          b: A `Tensor`. Must have the same type as `x`. The bias vector.\n          forget_bias: An optional `float`. Defaults to `1`. The forget gate bias.\n          cell_clip: An optional `float`. Defaults to `-1` (no clipping).\n            Value to clip the \'cs\' value to. Disable by setting to negative value.\n          use_peephole: An optional `bool`. Defaults to `False`.\n            Whether to use peephole weights.\n          name: A name for the operation (optional).\n        Returns:\n          A tuple of `Tensor` objects (i, cs, f, o, ci, co, h).\n          i: A `Tensor`. Has the same type as `x`. The input gate.\n          cs: A `Tensor`. Has the same type as `x`. The cell state before the tanh.\n          f: A `Tensor`. Has the same type as `x`. The forget gate.\n          o: A `Tensor`. Has the same type as `x`. The output gate.\n          ci: A `Tensor`. Has the same type as `x`. The cell input.\n          co: A `Tensor`. Has the same type as `x`. The cell after the tanh.\n          h: A `Tensor`. Has the same type as `x`. The output h vector.\n        ```python\n        xh = [x, h_prev]\n        [i, ci, f, o] = xh * w + b\n        f = f + forget_bias\n        if not use_peephole:\n          wci = wcf = wco = 0\n        i = sigmoid(cs_prev .* wci + i)\n        f = sigmoid(cs_prev .* wcf + f)\n        ci = tanh(ci)\n        cs = ci .* i + cs_prev .* f\n        cs = clip(cs, cell_clip)\n        o = sigmoid(cs * wco + o)\n        co = tanh(cs)\n        h = co .* o\n        ```\n        """"""\n        nodes = []\n        x, cs_prev, h_prev, w, wci, wcf, wco, b = node.input\n        forget_bias = float(node.get_attr(""forget_bias"").f)\n        cell_clip = float(node.get_attr(""cell_clip"").f)\n        use_peephole = bool(node.get_attr(""use_peephole"").i)\n\n        def make_sigmoid(i, w, b):\n            i_w_node = ctx.make_node(""Mul"", [i, w])\n            i_w_b_node = ctx.make_node(""Add"", [i_w_node.output[0], b])\n            output_node = ctx.make_node(""Sigmoid"", [i_w_b_node.output[0]])\n            nodes.extend([i_w_node, i_w_b_node, output_node])\n            return output_node.output[0]\n\n        # xh = [x, h]\n        xh_node = ctx.make_node(""Concat"", [x, h_prev], attr={""axis"": 1})\n\n        # i, ci, f, o = xh * w + b\n        xh_w_node = ctx.make_node(""MatMul"", [xh_node.output[0], w])\n        w_shape = ctx.get_shape(w)\n        if len(w_shape) != 2 or w_shape[1] % 4 != 0:\n            raise RuntimeError(""shape of W of LSTMBlockCell {} should be times of 4"".format(node.name))\n        merged_output_node = ctx.make_node(""Add"", [xh_w_node.output[0], b])\n        w_last_dim = int(w_shape[1] / 4)\n        split = [w_last_dim] * 4\n        split_output_node = ctx.make_node(\n            ""Split"", [merged_output_node.output[0]],\n            attr={""axis"": 1, ""split"": split},\n            output_count=4\n        )\n        i, ci, f, o = split_output_node.output\n\n        # f = f + forget_bias\n        forget_bias_const = ctx.make_const(\n            utils.make_name(""{}__forget_bias"".format(node.name)),\n            np.array(forget_bias, dtype=np.float32)\n        )\n        f_node = ctx.make_node(""Add"", [f, forget_bias_const.output[0]])\n\n        if not use_peephole:\n            zeros_const = ctx.make_const(\n                utils.make_name(""{}__zeros_const"".format(node.name)),\n                np.zeros([w_last_dim], dtype=np.float32)\n            )\n            nodes.append(zeros_const)\n            wci = zeros_const.output[0]\n            wcf = zeros_const.output[0]\n            wco = zeros_const.output[0]\n\n        # i = sigmoid(cs_prev .* wci + i)\n        i = make_sigmoid(cs_prev, wci, i)\n        # f = sigmoid(cs_prev .* wcf + f)\n        f = make_sigmoid(cs_prev, wcf, f_node.output[0])\n        # ci = Tanh(ci)\n        ci_node = ctx.make_node(""Tanh"", [ci])\n        # cs = ci .* i + f .* cs_prev\n        ci_i_node = ctx.make_node(""Mul"", [ci_node.output[0], i])\n        cs_prev_f_node = ctx.make_node(""Mul"", [cs_prev, f])\n        cs_node = ctx.make_node(""Add"", [ci_i_node.output[0], cs_prev_f_node.output[0]])\n        cs = cs_node.output[0]\n        # cs = clip(cs)\n        if cell_clip > 0:\n            if ctx.opset < 11:\n                cs_clip_node = ctx.make_node(""Clip"", [cs], attr={""max"": cell_clip, ""min"": -cell_clip})\n                nodes.append(cs_clip_node)\n                cs = cs_clip_node.output[0]\n            else:\n                dtype = utils.map_onnx_to_numpy_type(ctx.get_dtype(cs))\n                name_min = utils.make_name(""{}_min"".format(node.name))\n                name_max = utils.make_name(""{}_max"".format(node.name))\n                min_const = ctx.make_const(name_min, np.array(-cell_clip, dtype=dtype))\n                max_const = ctx.make_const(name_max, np.array(cell_clip, dtype=dtype))\n                cs_clip_node = ctx.make_node(\'Clip\', [cs, min_const.output[0], max_const.output[0]])\n                nodes.append(cs_clip_node)\n                cs = cs_clip_node.output[0]\n\n        # o = cs * wco + o\n        o = make_sigmoid(cs, wco, o)\n        # co = Tanh(cs)\n        co_node = ctx.make_node(""Tanh"", [cs])\n        # h = co .* o\n        h_node = ctx.make_node(""Mul"", [co_node.output[0], o])\n\n        def replace_output(old_output, new_output):\n            ctx.replace_all_inputs(ctx.get_nodes(), old_output, new_output)\n            ctx.copy_dtype(old_output, new_output)\n            ctx.copy_shape(old_output, new_output)\n\n        replace_output(node.output[0], i)\n        replace_output(node.output[1], cs)\n        replace_output(node.output[2], f)\n        replace_output(node.output[3], o)\n        replace_output(node.output[4], ci_node.output[0])\n        replace_output(node.output[5], co_node.output[0])\n        replace_output(node.output[6], h_node.output[0])\n\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""CudnnRNN"")\nclass CudnnRNN:\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        x = node.input[0]\n        x_shape = ctx.get_shape(x)\n        h = node.input[1]\n        h_shape = ctx.get_shape(h)\n        p = node.input[3]\n        utils.make_sure(\n            node.attr[""rnn_mode""].s == b""gru"",\n            ""rnn mode other than gru are not supported yet""\n        )\n        utils.make_sure(\n            node.attr[""dropout""].f == 0,\n            ""dropout not supported yet""\n        )\n        utils.make_sure(\n            node.attr[""input_mode""].s == b""linear_input"",\n            ""input mode must be linear input""\n        )\n        num_dirs = 1 if node.attr[""direction""].s == b""unidirectional"" else 2\n        num_layers = int(h_shape[0] / num_dirs)\n        num_units = hidden_size = h_shape[2]\n        input_size = x_shape[2]\n        w_shape = [num_layers * num_dirs, 3 * hidden_size, input_size]\n        w_shape_const = ctx.make_const(utils.make_name(""w_shape""), np.array(w_shape, dtype=np.int64))\n        r_shape = [num_layers * num_dirs, 3 * hidden_size, hidden_size]\n        r_shape_const = ctx.make_const(utils.make_name(""r_shape""), np.array(r_shape, dtype=np.int64))\n        b_shape = [num_layers * num_dirs, 6 * hidden_size]\n        b_shape_const = ctx.make_const(utils.make_name(""b_shape""), np.array(b_shape, dtype=np.int64))\n        zero_const = ctx.make_const(utils.make_name(""zero""), np.array([0], dtype=np.int64))\n        w_end = np.prod(w_shape)\n        w_end_const = ctx.make_const(utils.make_name(""w_end""), np.array([w_end], dtype=np.int64))\n        r_end = w_end + np.prod(r_shape)\n        r_end_const = ctx.make_const(utils.make_name(""r_end""), np.array([r_end], dtype=np.int64))\n        b_end = r_end + np.prod(b_shape)\n        b_end_const = ctx.make_const(utils.make_name(""b_end""), np.array([b_end], dtype=np.int64))\n\n        def name(nm):\n            return node.name + ""_"" + nm\n\n        ws = [name(\'W_\' + str(i)) for i in range(num_layers * num_dirs)]\n        rs = [name(\'R_\' + str(i)) for i in range(num_layers * num_dirs)]\n        bs = [name(\'B_\' + str(i)) for i in range(num_layers * num_dirs)]\n        hs = [name(\'H_\' + str(i)) for i in range(num_layers * num_dirs)]\n        yhs = [name(\'YH_\' + str(i)) for i in range(num_layers * num_dirs)]\n        w_flattened = ctx.make_node(\'Slice\', [p, zero_const.output[0], w_end_const.output[0]])\n        r_flattened = ctx.make_node(\'Slice\', [p, w_end_const.output[0], r_end_const.output[0]])\n        b_flattened = ctx.make_node(\'Slice\', [p, r_end_const.output[0], b_end_const.output[0]])\n        w = utils.make_name(\'W\')\n        r = utils.make_name(\'R\')\n        b = utils.make_name(\'B\')\n        ctx.make_node(\'Reshape\', [w_flattened.output[0], w_shape_const.output[0]], outputs=[w])\n        ctx.make_node(\'Reshape\', [r_flattened.output[0], r_shape_const.output[0]], outputs=[r])\n        ctx.make_node(\'Reshape\', [b_flattened.output[0], b_shape_const.output[0]], outputs=[b])\n        ctx.make_node(\'Split\', [w], outputs=ws)\n        ctx.make_node(\'Split\', [r], outputs=rs)\n        ctx.make_node(\'Split\', [b], outputs=bs)\n        ctx.make_node(\'Split\', [h], outputs=hs)\n        xnf = xnb = x\n        for i in range(num_layers):\n            suffix = \'_\' + str(i * num_dirs)\n            ctx.make_node(\'GRU\',\n                          [xnf, name(\'W\' + suffix), name(\'R\' + suffix), name(\'B\' + suffix), \'\', name(\'H\' + suffix)],\n                          outputs=[name(\'Y\' + suffix), name(\'YH\' + suffix)],\n                          attr={\'direction\': \'forward\', \'hidden_size\': num_units})\n            xnf = name(x + suffix)\n            ctx.make_node(\'Squeeze\', [name(\'Y\' + suffix)], outputs=[xnf], attr={\'axes\': [1]})\n            if num_dirs == 2:\n                suffix = \'_\' + str(i * 2 + 1)\n                ctx.make_node(\'GRU\',\n                              [xnb, name(\'W\' + suffix), name(\'R\' + suffix), name(\'B\' + suffix), \'\', name(\'H\' + suffix)],\n                              outputs=[name(\'Y\' + suffix), name(\'YH\' + suffix)],\n                              attr={\'direction\': \'reverse\', \'hidden_size\': num_units})\n                xnb = name(x + suffix)\n                ctx.make_node(\'Squeeze\', [name(\'Y\' + suffix)], outputs=[xnb], attr={\'axes\': [1]})\n        ctx.remove_node(node.name)\n        if num_dirs == 2:\n            ctx.make_node(\'Concat\', [xnf, xnb], outputs=[node.output[0]], attr={\'axis\': -1})\n        else:\n            ctx.make_node(\'Identity\', [xnf], outputs=[node.output[0]])\n        ctx.make_node(\'Concat\', yhs, outputs=[node.output[1]], attr={\'axis\': 0})\n'"
tf2onnx/onnx_opset/tensor.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntensor\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nimport sys\n\nimport numpy as np\nfrom onnx import onnx_pb\nfrom onnx.onnx_pb import TensorProto\n\nfrom tf2onnx import constants, utils\nfrom tf2onnx.graph_builder import GraphBuilder\nfrom tf2onnx.handler import tf_op\nfrom tf2onnx.onnx_opset import nn, math\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=unused-argument,missing-docstring,unused-variable,pointless-string-statement,invalid-name\n\n\ndef _convert_shapenode_to_int64(ctx, node, input_number):\n    """"""cast int32 shape into int64 shape.""""""\n    name = node.input[input_number]\n\n    cast_node = ctx.insert_new_node_on_input(node, ""Cast"", name)\n    cast_node.set_attr(""to"", onnx_pb.TensorProto.INT64)\n    ctx.set_dtype(cast_node.output[0], onnx_pb.TensorProto.INT64)\n    ctx.copy_shape(name, cast_node.output[0])\n\n\ndef _wrap_concat_with_cast(ctx, node):\n    """"""wrap concat in casts for opset < 8 since it only supports.""""""\n    supported_types = [onnx_pb.TensorProto.FLOAT, onnx_pb.TensorProto.FLOAT16]\n    dtype = ctx.get_dtype(node.output[0])\n    need_casting = dtype not in supported_types\n    if need_casting:\n        output_name = node.output[0]\n        # cast each inputs to float\n        for i, inp in enumerate(node.inputs):\n            input_cast = ctx.insert_new_node_on_input(node, ""Cast"", node.input[i])\n            input_cast.set_attr(""to"", onnx_pb.TensorProto.FLOAT)\n            ctx.set_dtype(input_cast.output[0], onnx_pb.TensorProto.FLOAT)\n        next_nodes = ctx.find_output_consumers(node.output[0])\n        # cast output back to dtype unless the next op is a cast\n        if next_nodes[0].type != ""Cast"":\n            output_cast = ctx.insert_new_node_on_output(""Cast"", output_name, name=node.child_name())\n            output_cast.set_attr(""to"", dtype)\n            ctx.set_dtype(output_cast.output[0], dtype)\n            ctx.copy_shape(output_name, output_cast.output[0])\n\n\n@tf_op(""Size"")\nclass Size:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""Flatten"")\nclass Flatten:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        # no change for us\n        cls.version_1(ctx, node, **kwargs)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # no change\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""Dropout"")\nclass Dropout:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        pass\n\n    @classmethod\n    def version_6(cls, ctx, node, **kwargs):\n        pass\n\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        pass\n\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        pass\n\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""Identity"")\nclass Identity:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        if node.inputs[0].is_const():\n            # should not remove the identity node if it is output of the graph\n            if node.output[0] in ctx.outputs:\n                return\n            # if identity has a const as input, remove it\n            input_name = node.input[0]\n            output_name = node.output[0]\n            ctx.replace_all_inputs(ctx.get_nodes(), output_name, input_name)\n            ctx.remove_node(node.name)\n\n\n@tf_op(""IdentityN"")\nclass IdentityN:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        ctx.remove_node(node.name)\n        for input_name, output_name in zip(node.input, node.output):\n            ctx.replace_all_inputs(ctx.get_nodes(), output_name, input_name)\n\n\n@tf_op(""Reshape"")\nclass Reshape:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = Reshape(T tensor, Tshape shape, @type Tshape)\n        # T reshaped = Reshape(T data, @INTS shape) - but takes a optional 2nd input for shape\n        shape_node = node.inputs[1]\n        shape = shape_node.get_tensor_value()\n        if shape is None:\n            logger.error(""Reshape on node %s does not have a const shape"", node.name)\n            return\n        ctx.remove_input(node, node.input[1])\n        node.set_attr(""shape"", shape)\n        ctx.set_shape(node.output[0], shape)\n\n    @classmethod\n    def version_5(cls, ctx, node, **kwargs):\n        dtype = ctx.get_dtype(node.output[0])\n        need_casting = dtype in [onnx_pb.TensorProto.INT32,\n                                 onnx_pb.TensorProto.INT16,\n                                 onnx_pb.TensorProto.INT64]\n        # onnx wants reshape.input[1] to have the value be int64 which is not the case for tensorflow.\n        _convert_shapenode_to_int64(ctx, node, 1)\n        if ctx.opset >= 8 or not need_casting:\n            # onnx reshape can handle the type - done\n            return\n\n        # onnx < opset 8 does not know reshape for other types than float*, wrap the reshape in casts\n        input_cast = ctx.insert_new_node_on_input(node, ""Cast"", node.input[0])\n        input_cast.set_attr(""to"", onnx_pb.TensorProto.FLOAT)\n        ctx.copy_shape(node.output[0], input_cast.output[0])\n\n        # if the next node is already a cast we don\'t need to insert another one\n        next_nodes = ctx.find_output_consumers(node.output[0])\n        if len(next_nodes) != 1 or next_nodes[0].type != ""Cast"":\n            output_cast = ctx.insert_new_node_on_output(""Cast"", node.output[0], name=node.child_name())\n            output_cast.set_attr(""to"", dtype)\n            ctx.set_dtype(output_cast.output[0], dtype)\n            ctx.copy_shape(node.output[0], output_cast.output[0])\n\n\n@tf_op(""Squeeze"")\nclass Squeeze:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = Squeeze(T input, @list(int) squeeze_dims)\n        # T squeezed = Squeeze(T data, @AttrType.INTS axes), axes are list of positive integers.\n        axis = node.get_attr(""axis"")\n        if not axis:\n            axis = node.get_attr(""squeeze_dims"")\n            if axis:\n                del node.attr[""squeeze_dims""]\n        else:\n            del node.attr[""axis""]\n\n        if axis and axis.ints:\n            axis = axis.ints\n            neg_axis = any([val < 0 for val in axis])\n            if neg_axis:\n                shape = ctx.get_shape(node.input[0])\n                utils.make_sure(shape is not None, ""squeeze input shape cannot be None"")\n                shape_len = len(shape)\n                axis = [a + shape_len if a < 0 else a for a in axis]\n        else:\n            shape = ctx.get_shape(node.input[0])\n            utils.make_sure(shape is not None, ""squeeze input shape cannot be None"")\n            axis = [i for i, j in enumerate(shape) if j == 1]\n            if not axis: axis = [0]\n        node.set_attr(""axes"", axis)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # Opset 11 supports negative axis, but core logic is same\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""Transpose"")\nclass Transpose:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T y = Transpose(T x, Tperm perm, @type Tperm)\n        # T transposed = Transpose(T data, @INTS perm)\n        if len(node.input) > 1:\n            perm = node.inputs[1]\n            if perm.is_const():\n                # perms is passed as const\n                dims = perm.get_tensor_value()\n                ctx.remove_input(node, node.input[1])\n                node.set_attr(""perm"", dims)\n            else:\n                utils.make_sure(False, ""perm can\'t be dynamic in ONNX"")\n        else:\n            # graph rewrite moved perm to attribute\n            pass\n\n\n@tf_op(""Concat"")\nclass Concat:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # old concat op has axis as input[0]\n        node.type = ""Concat""\n        axis_node = node.inputs[0]\n        axis_val = axis_node.get_tensor_value()\n        ctx.remove_input(node, node.input[0])\n\n        if axis_val < 0:  # onnxruntime does not support -1 axis, but TF supports.\n            input_shape = ctx.get_shape(node.input[0])\n            axis_val = len(input_shape) + axis_val\n        node.set_attr(""axis"", axis_val)\n\n        if ctx.opset < 8:\n            # opset < 8: might need to wrap concat in casts since only float is supported\n            _wrap_concat_with_cast(ctx, node)\n            return\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # Opset 11 supports negative axis, but core logic is same\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""ConcatV2"")\nclass ConcatV2:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = ConcatV2(T values, Tidx axis, @int N, @type Tidx)\n        # T concat_result = Concat(T inputs, @INT axis)\n        # if any input is empty, remove the input and concat the others\n        # NOTE: workaround for https://github.com/Microsoft/onnxruntime/issues/681\n        node.type = ""Concat""\n        for i, inp in enumerate(node.inputs):\n            if inp.is_const() and inp.get_tensor_value(as_list=False).size == 0:\n                ctx.remove_input(node, node.input[i])\n        # all inputs are deleted\n        if not node.input:\n            raise RuntimeError(""all inputs of {} are empty"".format(node.name))\n\n        axis_node = node.inputs[-1]\n        utils.make_sure(axis_node.is_const(), ""{} needs to be const"".format(axis_node.name))\n        axis_val = axis_node.get_tensor_value()\n        ctx.remove_input(node, node.input[-1])\n\n        if axis_val < 0:  # onnxruntime does not support -1 axis, but TF supports.\n            input_shape = ctx.get_shape(node.input[0])\n            utils.make_sure(input_shape is not None, ""shape of {} is None"".format(node.input[0]))\n            axis_val = len(input_shape) + axis_val\n        node.set_attr(""axis"", axis_val)\n\n        if ctx.opset < 8:\n            # opset < 8: might need to wrap concat in casts since only float is supported\n            _wrap_concat_with_cast(ctx, node)\n            return\n\n\n@tf_op(""Slice"")\nclass Slice:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = Slice(T input, Index begin, Index size)\n        # T output = Slice(T input, Tind starts, Tind ends, Tind axes, Tind steps)\n        # ""ends"" are exclusive, ""axes"" and ""steps"" are optional, their default val are [0, ...] and 1\n        input_tensor = node.input[0]\n        starts = node.input[1]\n        size = node.input[2]\n        # in tf, size can be -1 which means all elem are taken, so size can\'t be added starts directly.\n        # the way to make sure size are not less than 0: set ""sizes""\'s elem to be int_max if elem val is -1\n        size_dtype = ctx.get_dtype(size)\n        size_np_dtype = utils.map_onnx_to_numpy_type(size_dtype)\n        if ctx.get_node_by_output(size).is_const() and ctx.get_node_by_output(starts).is_const():\n            starts = ctx.get_node_by_output(starts).get_tensor_value()\n            sizes = ctx.get_node_by_output(size).get_tensor_value()\n            ends = []\n            for start, size in zip(starts, sizes):\n                # get all elements\n                if size == -1:\n                    dtype = ctx.get_dtype(node.input[1])\n                    utils.make_sure(dtype, ""dtype of {} is None"".format(node.input[1]))\n                    utils.make_sure(dtype, ""dtype of {} is None"".format(node.input[1]))\n                    ends.append(np.iinfo(dtype).max)\n                else:\n                    ends.append(start + size)\n\n        else:\n            neg_one_val = np.array([-1]).astype(size_np_dtype)\n            neg_one = ctx.make_const(utils.make_name(""const""), neg_one_val).output[0]\n\n            int_max_val = np.array([utils.get_max_value(size_np_dtype)]).astype(size_np_dtype)\n            int_max = ctx.make_const(utils.make_name(""largest_int_val""), int_max_val).output[0]\n\n            size_are_neg_one_flag = ctx.make_node(""Equal"", [neg_one, size]).output[0]\n            size_are_neg_one_flag = ctx.make_node(""Cast"", [size_are_neg_one_flag], attr={""to"": size_dtype}).output[0]\n            value_to_add = ctx.make_node(""Mul"", [int_max, size_are_neg_one_flag]).output[0]\n            size_processed = ctx.make_node(""Add"", [size, value_to_add]).output[0]\n            ends = ctx.make_node(""Add"", [starts, size_processed]).output[0]\n\n        ctx.remove_node(node.name)\n        inputs_map = {""data"": input_tensor, ""starts"": starts, ""ends"": ends}\n        kwargs = {**inputs_map, ""outputs"": node.output}\n        _ = GraphBuilder(ctx).make_slice(kwargs, name=node.name)\n\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        cls.version_1(ctx, node, **kwargs)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""Gather"")\nclass Gather:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        node.type = ""Gather""\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # no change\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""GatherV2"")\nclass GatherV2:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # for GatherV2 axis come as input\n        node.type = ""Gather""\n        axis = node.inputs[2].get_tensor_value()\n        ctx.remove_input(node, node.input[2])\n        node.set_attr(""axis"", axis)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # no change\n        cls.version_1(ctx, node, **kwargs)\n\n\ndef _make_gathernd_inner_loop(ctx, params, index, dtype):\n    """"""create the inner loop for GatherNd.""""""\n    # gather_cur = params\n    # for (int i = 0; i < size(index); i++)\n    #   gather_res = gather(gather_cur, index[i])\n    scope_name = utils.make_name(""gathernd_inner_loop"")\n    trip_node = ctx.make_node(""Size"", [index.output[0]])\n    cond_const = ctx.make_const(utils.make_name(""cond""), np.ones((), dtype=np.bool))\n    trip_name = utils.make_name(""i"")\n    cond_name = utils.make_name(""cond"")\n    cond_out_name = utils.make_name(""cond_out"")\n    cur_name = utils.make_name(""gather_cur"")\n    result_name = utils.make_name(""res"")\n\n    # body graph creation\n    g = ctx.create_new_graph_with_same_config()\n    g.add_graph_input(trip_name, TensorProto.INT64, [1])\n    g.add_graph_input(cond_name, TensorProto.BOOL, [])\n    g.add_graph_input(cur_name, dtype, [])\n    g.parent_graph = ctx\n\n    index_i = g.make_node(""Gather"", [index.output[0], trip_name], attr={""axis"": 0})\n    gather = g.make_node(""Gather"", [cur_name, index_i.output[0]], attr={""axis"": 0})\n    g.make_node(""Squeeze"", [gather.output[0]], attr={""axes"": [0]}, outputs=[result_name])\n    g.make_node(""Identity"", [cond_name], outputs=[cond_out_name])\n\n    g.add_graph_output(cond_out_name, TensorProto.BOOL, [])\n    g.add_graph_output(result_name, dtype, [])\n\n    inner_loop = ctx.make_node(""Loop"", [trip_node.output[0],\n                                        cond_const.output[0],\n                                        params],\n                               op_name_scope=scope_name, skip_conversion=False)\n    inner_loop.set_body_graph_as_attr(""body"", g)\n    return inner_loop\n\n\ndef make_gathernd(ctx, params, indices, output, scope_name, t_params, shapes, dtypes):\n    """"""make GatherNd op.""""""\n    # Tparams output = GatherNd(Tparams params, Tidx indices)\n    scope_name = utils.make_name(scope_name)\n    # reshape indices into [sum(indices[:-1]), indices[-1]]\n    indices_shape = ctx.make_node(""Shape"", [indices], dtypes=[TensorProto.INT64])\n    indices_size = ctx.make_node(""Size"", [indices])\n    attr = {""axes"": [0], ""ends"": [sys.maxsize], ""starts"": [-1]}\n    inputs_map = {""data"": indices_shape.output[0], **attr}\n    inner_shape = GraphBuilder(ctx).make_slice(inputs_map, dtypes=[TensorProto.INT64])\n    outter_shape = ctx.make_node(""Div"",\n                                 [indices_size.output[0], inner_shape],\n                                 dtypes=[TensorProto.INT64])\n    flatten_shape = ctx.make_node(""Concat"",\n                                  [outter_shape.output[0], inner_shape],\n                                  attr={""axis"": 0},\n                                  dtypes=[TensorProto.INT64])\n    flatten_indices = ctx.make_node(""Reshape"", [indices, flatten_shape.output[0]])\n\n    # outter loop for each index\n    # for (int i=0; i<outter_shape; i++) inner_loop(params, flatten_indices[i])\n    cond_const = ctx.make_const(utils.make_name(""cond""), np.ones((), dtype=np.bool))\n    ctx.make_const(utils.make_name(""dummy""), np.ones((), dtype=np.int64))\n\n    # body graph creation\n    g = ctx.create_new_graph_with_same_config()\n    trip_name = utils.make_name(""i"")\n    cond_name = utils.make_name(""cond"")\n    cond_out_name = utils.make_name(""cond_out"")\n    dummy_name = utils.make_name(""dummy"")\n    dummy_out_name = utils.make_name(""dummy_out"")\n    result_name = utils.make_name(""res"")\n\n    g.add_graph_input(trip_name, TensorProto.INT64, [1])\n    g.add_graph_input(cond_name, TensorProto.BOOL, [])\n    g.add_graph_input(dummy_name, t_params, [])\n    g.parent_graph = ctx\n\n    index = g.make_node(""Gather"", [flatten_indices.output[0], trip_name], attr={""axis"": 0})\n    index_squeeze = g.make_node(""Squeeze"", [index.output[0]], attr={""axes"": [0]})\n    # inner loop to gather result\n    inner_loop = _make_gathernd_inner_loop(g, params, index_squeeze, t_params)\n    g.make_node(""Identity"", [cond_name], outputs=[cond_out_name])\n    g.make_node(""Identity"", [dummy_name], outputs=[dummy_out_name])\n    g.make_node(""Identity"", [inner_loop.output[0]], outputs=[result_name])\n\n    g.add_graph_output(cond_out_name, TensorProto.BOOL, [])\n    g.add_graph_output(dummy_out_name, t_params, [])\n    g.add_graph_output(result_name, t_params, [])\n\n    gathernd_loop = ctx.make_node(""Loop"",\n                                  [outter_shape.output[0], cond_const.output[0], params],\n                                  output_count=2,\n                                  op_name_scope=scope_name, skip_conversion=False)\n    gathernd_loop.set_body_graph_as_attr(""body"", g)\n\n    # reshape to target shape\n    # output shape of gathernd: indices.shape[:-1] + gathernd_output.shape[1:]\n    inner_loop_shape = ctx.make_node(""Shape"", [gathernd_loop.output[1]], dtypes=[TensorProto.INT64])\n    # workaround in case gathernd_loop is 1-dimensional\n    one_const = ctx.make_const(utils.make_name(""one""), np.array([1], dtype=np.int64))\n    inner_loop_shape_ = ctx.make_node(""Concat"",\n                                      [inner_loop_shape.output[0], one_const.output[0]],\n                                      attr={""axis"": 0},\n                                      dtypes=[TensorProto.INT64])\n    attr = {""axes"": [0], ""ends"": [sys.maxsize], ""starts"": [1]}\n    inputs_map = {""data"": inner_loop_shape_.output[0], **attr}\n    output_inner_shape = GraphBuilder(ctx).make_slice(inputs_map, dtypes=[TensorProto.INT64])\n    attr = {""axes"": [0], ""ends"": [-1], ""starts"": [0]}\n    inputs_map = {""data"": indices_shape.output[0], **attr}\n    indices_outter_shape = GraphBuilder(ctx).make_slice(inputs_map, dtypes=[TensorProto.INT64])\n    output_shape_ = ctx.make_node(""Concat"",\n                                  [indices_outter_shape, output_inner_shape],\n                                  attr={""axis"": 0},\n                                  dtypes=[TensorProto.INT64])\n    attr = {""axes"": [0], ""ends"": [-1], ""starts"": [0]}\n    inputs_map = {""data"": output_shape_.output[0], **attr}\n    output_shape = GraphBuilder(ctx).make_slice(inputs_map, dtypes=[TensorProto.INT64])\n    ctx.make_node(""Reshape"",\n                  [gathernd_loop.output[1], output_shape],\n                  outputs=[output],\n                  shapes=shapes,\n                  dtypes=dtypes)\n\n\n@tf_op(""GatherNd"", onnx_op=""GatherND"")\nclass GatherND:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # Tparams output = GatherNd(Tparams params, Tidx indices)\n        params = node.input[0]\n        indices = node.input[1]\n        output = node.output[0]\n        # same as the attr Tparams\n        t_params = ctx.get_dtype(params)\n        utils.make_sure(t_params, ""Dtype of {} is None"".format(indices))\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        make_gathernd(ctx, params, indices, output, node.name, t_params, shapes, dtypes)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # indicies input\n        input1 = node.input[1]\n        target_dtype = TensorProto.INT64\n        if ctx.get_dtype(input1) != TensorProto.INT64:\n            inp_cast = ctx.insert_new_node_on_input(node, ""Cast"", input1, to=target_dtype)\n            ctx.copy_shape(input1, inp_cast.output[0])\n            ctx.set_dtype(inp_cast.output[0], target_dtype)\n\n\n@tf_op(""ScatterNd"", onnx_op=""ScatterND"")\nclass ScatterND:\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        onnxdtype = ctx.get_dtype(node.input[1])\n        const_of_shape = ctx.insert_new_node_on_input(node, ""ConstantOfShape"", node.input[2])\n        ctx.insert_new_node_on_input(const_of_shape, ""Cast"", const_of_shape.input[0], to=TensorProto.INT64)\n        ctx.insert_new_node_on_input(node, ""Cast"", node.input[0], to=TensorProto.INT64)\n        ctx.insert_new_node_on_input(node, ""Cast"", node.input[2], to=onnxdtype)\n        # reorder inputs to match onnx\n        node.input = [node.input[2], node.input[0], node.input[1]]\n\n\n@tf_op(""Split"")\nclass Split:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = Split(int32 split_dim, T value, @int num_split)\n        # T outputs = Split(T input, @INT axis, @INTS split)\n        split_dims = node.inputs[0].get_tensor_value()\n        ctx.remove_input(node, node.input[0])\n        node.set_attr(""axis"", split_dims)\n\n    @classmethod\n    def version_2(cls, ctx, node, **kwargs):\n        cls.version_1(ctx, node, **kwargs)\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # no change\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""SplitV"")\nclass SplitV:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = SplitV(T value, Tlen size_splits, int32 split_dim, @int num_split, @type Tlen)\n        # T outputs = Split(T input, @INT axis, @INTS split)\n        node.type = ""Split""\n        split = node.inputs[1].get_tensor_value()\n        split_dims = node.inputs[2].get_tensor_value()\n        if -1 in split:\n            # negative split = use the remaining size\n            shape = ctx.get_shape(node.input[0])\n            final_sum = shape[split_dims]\n            sums = sum([i for i in split if i >= 0])\n            for i, v in enumerate(split):\n                if v == -1:\n                    split[i] = final_sum - sums\n        ctx.remove_input(node, node.input[2])\n        ctx.remove_input(node, node.input[1])\n        node.set_attr(""split"", split)\n        node.set_attr(""axis"", split_dims)\n\n    @classmethod\n    def version_2(cls, ctx, node, **kwargs):\n        cls.version_1(ctx, node, **kwargs)\n\n\n@tf_op(""ExpandDims"")\nclass ExpandDims:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T output = ExpandDims(T input, Tdim dim, @type Tdim)\n        # T reshaped = Reshape-1(T data, @ints consumed_inputs, @int64 shape)\n        # T expanded = Unsqueeze-1(T data, @ints axes)\n        shape = ctx.get_shape(node.output[0])\n        if shape is not None and shape.count(-1) < 2:\n            # tensorflow already infers the output shape so we can just take it\n            shape = ctx.get_shape(node.output[0])\n            node.type = ""Reshape""\n            ctx.remove_input(node, node.input[1])\n            node.set_attr(""shape"", shape)\n            return\n\n        # if there is more than one -1 in the shape, Reshape won\'t support.\n        dim_node = node.inputs[1]\n        if dim_node.is_const():\n            node.type = ""Unsqueeze""\n            dim = dim_node.get_tensor_value()\n            if dim < 0:\n                input_rank = len(ctx.get_shape(node.input[0]))\n                dim = dim + input_rank + 1\n            node.set_attr(""axes"", [dim])\n            ctx.remove_input(node, node.input[1])\n            return\n        raise ValueError(""non-const dim is not supported"")\n\n    @classmethod\n    def version_7(cls, ctx, node, **kwargs):\n        # T output = ExpandDims(T input, Tdim dim, @type Tdim), dim is 0-D scalar.\n        # T reshaped = Reshape-5(T data, int64 shape)\n        # T expanded = Unsqueeze-1(T data, @ints axes)\n        dim_node = node.inputs[1]\n        if dim_node.is_const():\n            node.type = ""Unsqueeze""\n            dim = dim_node.get_tensor_value()\n            if isinstance(dim, list):\n                dim = dim[0]\n            if dim < 0:\n                input_rank = len(ctx.get_shape(node.input[0]))\n                dim = dim + input_rank + 1\n            node.set_attr(""axes"", [dim])\n            ctx.remove_input(node, node.input[1])\n            return\n        raise ValueError(""non-const dim is not supported"")\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        dim_node = node.inputs[1]\n        if dim_node.is_const():\n            node.type = ""Unsqueeze""\n            dim = dim_node.get_tensor_value()\n            if isinstance(dim, list):\n                # tf.expanddims() wants a scalar per doc but quietly accepts a list too.\n                dim = dim[0]\n            node.set_attr(""axes"", [dim])\n            ctx.remove_input(node, node.input[1])\n            return\n        raise ValueError(""non-const dim is not supported"")\n\n@tf_op(""StridedSlice"")\nclass StridedSlice:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # for now we implement common cases. Things like strides!=1 are not mappable to onnx.\n        not_supported_attr = [""new_axis_mask""]\n        for attr_name in not_supported_attr:\n            attr = node.get_attr(attr_name)\n            if attr is not None and attr.i != 0:\n                raise ValueError(""StridedSlice: attribute "" + attr_name + "" not supported"")\n\n        onnx_dtype = ctx.get_dtype(node.input[1])\n        np_dtype = utils.ONNX_TO_NUMPY_DTYPE[onnx_dtype]\n        max_size = np.iinfo(np_dtype).max\n        begin = node.inputs[1].get_tensor_value()\n        end = node.inputs[2].get_tensor_value()\n        strides = node.inputs[3].get_tensor_value()\n        end_mask = node.get_attr(""end_mask"")\n        end_mask = end_mask.i if end_mask is not None else 0\n        begin_mask = node.get_attr(""begin_mask"")\n        begin_mask = begin_mask.i if begin_mask is not None else 0\n        shrink_axis_mask = node.get_attr(""shrink_axis_mask"")\n        shrink_axis_mask = shrink_axis_mask.i if shrink_axis_mask is not None else 0\n        ellipsis_mask = node.get_attr(""ellipsis_mask"")\n        ellipsis_mask = ellipsis_mask.i if ellipsis_mask is not None else 0\n        new_begin = []\n        new_end = []\n        axes = []\n        # onnx slice op can\'t remove a axis, track axis and add a squeeze op if needed\n        needs_squeeze = []\n        # ellipsis: one bit at most can be 1. An ellipsis implicitly creates as many range specifications as\n        # necessary to fully specify the sliced range for every dimension.\n        # For example for a 4-dimensional tensor foo the slice foo[2, ..., 5:8] implies foo[2, :, :, 5:8]\n        # NOTE: we ignore those axes denoted by ellipsis using `axes` attribute\n        ellipsis_gap = 0\n        for idx, begin_item in enumerate(begin):\n            if strides[idx] != 1:\n                raise ValueError(""StridedSlice: only strides=1 is supported"")\n            if (ellipsis_mask >> idx) & 1:\n                input_shape = ctx.get_shape(node.input[0])\n                utils.make_sure(\n                    input_shape is not None,\n                    ""StridedSlice op {} requires the shape of input"".format(node.name)\n                )\n                ellipsis_gap = len(input_shape) - len(begin)\n                continue\n\n            # ignore ellipsis axes\n            axes.append(idx + ellipsis_gap)\n            end_item = end[idx]\n\n            # an implicit condition is stride == 1 (checked in above)\n            if begin_item < 0 and end_item == 0:\n                end_item = max_size\n\n            mask = (shrink_axis_mask >> idx) & 1\n            if mask != 0:\n                new_begin.append(begin_item)\n                end_item = begin_item + 1 if begin_item != -1 else max_size\n                new_end.append(end_item)\n                needs_squeeze.append(idx + ellipsis_gap)\n                continue\n\n            mask = (begin_mask >> idx) & 1\n            if mask != 0:\n                new_begin.append(0)\n            else:\n                new_begin.append(begin_item)\n\n            mask = (end_mask >> idx) & 1\n            if mask != 0:\n                new_end.append(max_size)\n            else:\n                new_end.append(end_item)\n\n        out_dtypes = [ctx.get_dtype(node.output[0])]\n        out_shapes = [ctx.get_shape(node.output[0])]\n        ctx.remove_node(node.name)\n\n        attr = {""starts"": new_begin, ""ends"": new_end, ""axes"": axes}\n        inputs_map = {""data"": node.input[0], **attr}\n        kwargs = {**inputs_map, ""outputs"": node.output}\n        node = GraphBuilder(ctx).make_slice(kwargs, name=node.name, dtypes=out_dtypes, shapes=out_shapes)\n        node = ctx.get_node_by_output(node)\n        nodes = [node]\n        if needs_squeeze:\n            name = utils.make_name(node.name)\n            squeeze_node = ctx.insert_new_node_on_output(""Squeeze"", node.output[0], name)\n            squeeze_node.set_attr(""axes"", needs_squeeze)\n            nodes.append(squeeze_node)\n            input_dtype = ctx.get_dtype(node.output[0])\n            ctx.set_dtype(squeeze_node.output[0], input_dtype)\n            ctx.copy_shape(node.output[0], squeeze_node.output[0])\n\n        # onnx slice as of opset 7 does only take float tensors ... cast if needed\n        input_dtype = ctx.get_dtype(node.input[0])\n        if ctx.opset < 9:\n            if input_dtype != onnx_pb.TensorProto.FLOAT:\n                if node.inputs[0].type == ""Cast"" and len(ctx.find_output_consumers(node.inputs[0].output[0])) == 1:\n                    # override the previous cast\n                    cast_node = node.inputs[0]\n                else:\n                    cast_node = ctx.insert_new_node_on_input(node, ""Cast"", node.input[0])\n                    nodes.insert(0, cast_node)\n                cast_node.set_attr(""to"", onnx_pb.TensorProto.FLOAT)\n                ctx.set_dtype(cast_node.output[0], onnx_pb.TensorProto.FLOAT)\n                ctx.copy_shape(node.input[0], cast_node.output[0])\n                # undo the cast afer slice\n                name = utils.make_name(node.name)\n                cast_node = ctx.insert_new_node_on_output(""Cast"", nodes[-1].output[0], name)\n                cast_node.set_attr(""to"", input_dtype)\n                ctx.set_dtype(cast_node.output[0], input_dtype)\n                ctx.copy_shape(node.output[0], cast_node.output[0])\n                nodes.append(cast_node)\n\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        # T output = Slice(T input, Index begin, Index end, Index strides\n        #                 @int begin_mask, @int end_mask, @int ellipsis_mask\n        #                 @int shrink_axis_mask, @int new_axis_mask)\n        # T output = Slice(T input, Tind starts, Tind ends, Tind axes, Tind steps)\n        # ""ends"" are exclusive, ""axes"" and ""steps"" are optional, their default val are [0, ...] and 1\n        input_x = node.inputs[0]\n        begin = node.inputs[1]\n        end = node.inputs[2]\n        strides = node.inputs[3]\n        new_axis_mask = node.get_attr(""new_axis_mask"")\n        new_axis_mask = new_axis_mask.i if new_axis_mask is not None else 0\n\n        if begin.is_const() and end.is_const() and strides.is_const() \\\n                and all(val == 1 for val in strides.get_tensor_value()) \\\n                and new_axis_mask == 0:\n            cls.version_1(ctx, node, **kwargs)\n            return\n\n        onnx_dtype = ctx.get_dtype(node.input[1])\n        np_dtype = utils.ONNX_TO_NUMPY_DTYPE[onnx_dtype]\n\n        # NOTE: Max op only supports float32, deal with overflow when cast back to int32\n        # enable it after Max supports int32 and int64\n        # max_size = utils.get_max_value(np_dtype)\n        # min_size = utils.get_min_value(np_dtype)\n        max_size = 1e9\n        min_size = -1e9\n\n        end_mask = node.get_attr(""end_mask"")\n        end_mask = end_mask.i if end_mask is not None else 0\n        begin_mask = node.get_attr(""begin_mask"")\n        begin_mask = begin_mask.i if begin_mask is not None else 0\n        ellipsis_mask = node.get_attr(""ellipsis_mask"")\n        ellipsis_mask = ellipsis_mask.i if ellipsis_mask is not None else 0\n        shrink_axis_mask = node.get_attr(""shrink_axis_mask"")\n        shrink_axis_mask = shrink_axis_mask.i if shrink_axis_mask is not None else 0\n        if new_axis_mask != 0:\n            unqueeze_at = []\n            for bit in range(32):\n                if (new_axis_mask >> bit) & 1 == 1:\n                    unqueeze_at.append(bit)\n                    begin_mask |= 1 << bit\n                    end_mask |= 1 << bit\n            input_x = ctx.make_node(""Unsqueeze"", [input_x.output[0]], {""axes"": unqueeze_at})\n\n        param_shape = ctx.get_shape(node.input[1]) or \\\n                      ctx.get_shape(node.input[2]) or \\\n                      ctx.get_shape(node.input[3])\n        utils.make_sure(\n            param_shape is not None,\n            ""StridedSlice op {} requires the shape of begin/end/strides"".format(node.name)\n        )\n        param_rank = param_shape[0]\n        # use in onnx graph to mask begin\n        new_begin_mask = [1] * param_rank\n        # use in onnx graph to mask end\n        new_end_mask = [min_size] * param_rank\n        # for shrink mask, if shrink mask is 1, set stride to be max_size\n        shrink_strided_mask = [min_size] * param_rank\n        axes = []\n        # onnx slice op can\'t remove a axis, track axis and add a squeeze op if needed\n        needs_squeeze = []\n        ellipsis_gap = 0\n        for idx in range(param_rank):\n            if (ellipsis_mask >> idx) & 1:\n                input_shape = ctx.get_shape(input_x.output[0])\n                utils.make_sure(\n                    input_shape is not None,\n                    ""StridedSlice op {} requires the shape of input"".format(node.name)\n                )\n                ellipsis_gap = len(input_shape) - param_rank\n                # handle the redundant param\n                new_begin_mask[idx] = 0\n                new_end_mask[idx] = max_size\n                axes.append(idx)\n                continue\n\n            # ignore ellipsis axes\n            axes.append(idx + ellipsis_gap)\n\n            mask = (shrink_axis_mask >> idx) & 1\n            if mask != 0:\n                shrink_strided_mask[idx] = max_size\n                new_end_mask[idx] = max_size\n                needs_squeeze.append(idx + ellipsis_gap)\n                continue\n\n            mask = (begin_mask >> idx) & 1\n            if mask != 0:\n                new_begin_mask[idx] = 0\n\n            mask = (end_mask >> idx) & 1\n            if mask != 0:\n                new_end_mask[idx] = max_size\n\n        out_dtypes = [ctx.get_dtype(node.output[0])]\n        out_shapes = [ctx.get_shape(node.output[0])]\n        ctx.remove_node(node.name)\n\n        # mask begin\n        new_begin_mask = np.array(new_begin_mask, dtype=np_dtype)\n        if not np.all(new_begin_mask == 1):\n            if begin.is_const() and strides.is_const():\n                new_begin_vals = np.copy(begin.get_tensor_value(as_list=False))\n                strides_vals = strides.get_tensor_value(as_list=False)\n                idx1 = np.where(new_begin_mask == 0)\n                idx2 = np.where(strides_vals < 0)\n                idx3 = np.intersect1d(idx1, idx2)\n                new_begin_vals[idx3] = max_size\n                begin = ctx.make_const(utils.make_name(""begin_masked""), new_begin_vals)\n            else:\n                begin_mask_const = ctx.make_const(utils.make_name(""begin_mask""), np.equal(new_begin_mask, 0))\n                zero_const = ctx.make_const(utils.make_name(""zero_const""), np.zeros(1, dtype=np_dtype))\n                max_const = ctx.make_const(utils.make_name(""max_const""), np.array(max_size, dtype=np_dtype))\n                op1 = ctx.make_node(""Less"", [strides.output[0], zero_const.output[0]], op_name_scope=node.name)\n                op2 = ctx.make_node(""And"", [op1.output[0], begin_mask_const.output[0]], op_name_scope=node.name)\n                begin = ctx.make_node(""Where"", [op2.output[0], max_const.output[0], begin.output[0]],\n                                      op_name_scope=node.name)\n\n        # mask end\n        new_end_mask = np.array(new_end_mask, dtype=np_dtype)\n        end_output = end.output[0]\n        if not np.all(new_end_mask == min_size):\n            if end.is_const() and strides.is_const():\n                new_end_mask = np.maximum(end.get_tensor_value(as_list=False), new_end_mask)\n                idx = np.where(new_end_mask == max_size)\n                sign = np.sign(strides.get_tensor_value(as_list=False))[idx]\n                new_end_mask[idx] = new_end_mask[idx] * sign\n                end = ctx.make_const(utils.make_name(""end_masked""), new_end_mask)\n                end_output = end.output[0]\n            else:\n                # Overlay new_end_mask with specified end values.\n                # Adjust max_size to min_size if steps are < 0\n                max_const = ctx.make_const(utils.make_name(""max_const""), np.array(max_size, dtype=np_dtype))\n                min_const = ctx.make_const(utils.make_name(""min_const""), np.array(min_size, dtype=np_dtype))\n                zero_const = ctx.make_const(utils.make_name(""zero_const""), np.zeros(1, dtype=np_dtype))\n                end_mask_const = ctx.make_const(utils.make_name(""end_mask""), np.array(new_end_mask, dtype=np_dtype))\n                outputname = utils.make_name(""{}__newendmask"".format(node.name))\n                new_end_mask = math.make_min_or_max_op(ctx, ""Max"", [end.output[0], end_mask_const.output[0]],\n                                                       [outputname])\n                op1 = ctx.make_node(""Less"", [strides.output[0], zero_const.output[0]], op_name_scope=node.name)\n                op2 = ctx.make_node(""Equal"", [new_end_mask.output[0], max_const.output[0]], op_name_scope=node.name)\n                op3 = ctx.make_node(""And"", [op2.output[0], op1.output[0]], op_name_scope=node.name)\n                final_end = ctx.make_node(""Where"", [op3.output[0], min_const.output[0],\n                                                    new_end_mask.output[0]], op_name_scope=node.name)\n                end_output = final_end.output[0]\n\n        # mask strides for shrink\n        shrink_strided_mask = np.array(shrink_strided_mask, dtype=np_dtype)\n        strides_output = strides.output[0]\n        if not np.all(shrink_strided_mask == min_size):\n            if strides.is_const():\n                strides = ctx.make_const(\n                    utils.make_name(""strides_masked""),\n                    np.maximum(strides.get_tensor_value(as_list=False), shrink_strided_mask)\n                )\n                strides_output = strides.output[0]\n            else:\n                shrink_strided_mask_const = ctx.make_const(\n                    utils.make_name(""strides_mask""),\n                    np.array(shrink_strided_mask, dtype=np_dtype)\n                )\n                strides_output = utils.make_name(""{}__strides"".format(node.name))\n                math.make_min_or_max_op(\n                    ctx, ""Max"",\n                    [strides.output[0], shrink_strided_mask_const.output[0]],\n                    [strides_output]\n                )\n        # create axes input\n        axes_const = ctx.make_const(\n            utils.make_name(""slice_axes""),\n            np.array(axes, dtype=np_dtype)\n        )\n        axes_output = axes_const.output[0]\n\n        inputs_map = {\n            ""data"": input_x.output[0],\n            ""starts"": begin.output[0],\n            ""ends"": end_output,\n            ""steps"": strides_output,\n            ""axes"": axes_output\n        }\n        kwargs = {**inputs_map, ""outputs"": node.output}\n        node = GraphBuilder(ctx).make_slice(kwargs, name=node.name, dtypes=out_dtypes, shapes=out_shapes)\n        node = ctx.get_node_by_output(node)\n        if needs_squeeze:\n            squeeze_node = ctx.insert_new_node_on_output(""Squeeze"", node.output[0], node.child_name())\n            squeeze_node.set_attr(""axes"", needs_squeeze)\n            input_dtype = ctx.get_dtype(node.output[0])\n            ctx.set_dtype(squeeze_node.output[0], input_dtype)\n            ctx.copy_shape(node.output[0], squeeze_node.output[0])\n\n\n@tf_op(""Cast"")\nclass Cast:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # DstT y = Cast(SrcT x, @type SrcT, @type DstT)\n        # T2 output = Cast(T1 input, @STRING to)\n        dst = node.get_attr(""to"")\n        dst = tf2onnx.utils.ONNX_DTYPE_NAMES[dst]\n        node.set_attr(""to"", dst)\n\n    @classmethod\n    def version_6(cls, ctx, node, **kwargs):\n        pass\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""TopKV2"", onnx_op=""TopK"")\nclass TopKV2:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # T values, int32 indices = TopKV2(T input, int32 k, @bool sorted=true, @realnumbertype T)\n        # T values, I indices = TopK(T x, @int axis=-1, @int k). I: int64\n        topk_node_name = node.name\n        topk_output1 = node.output[0]\n        topk_output2 = node.output[1]\n\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        k = node.inputs[1].get_tensor_value()\n        ctx.remove_node(topk_node_name)\n        new_topk_name = utils.make_name(topk_node_name)\n        new_topk_node = ctx.make_node(""TopK"", [node.input[0]],\n                                      outputs=[topk_output1, utils.port_name(new_topk_name, 1)],\n                                      name=new_topk_name, attr={""k"": k},\n                                      shapes=shapes, dtypes=[dtypes[0], onnx_pb.TensorProto.INT64])\n\n        new_cast_name = utils.make_name(topk_node_name)\n        ctx.make_node(""Cast"", [new_topk_node.output[1]], outputs=[topk_output2],\n                      name=new_cast_name, attr={""to"": onnx_pb.TensorProto.INT32},\n                      shapes=[shapes[1]], dtypes=[onnx_pb.TensorProto.INT32])\n\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        # onnx only supports input K as a 1D tesor with dtype int64\n        # while in tf, K is a 0D tensor with dtype int32\n        dtypes = node.output_dtypes\n        k_0d = node.input[1]\n        cast = ctx.make_node(""Cast"", [k_0d], attr={""to"": onnx_pb.TensorProto.INT64})\n        k_1d = ctx.make_node(""Unsqueeze"", cast.output, attr={""axes"": [0]})\n        ctx.replace_input(node, k_0d, k_1d.output[0])\n        # cast the index output to int32\n        cast_out = ctx.insert_new_node_on_output(""Cast"", node.output[1], name=utils.make_name(node.name), to=dtypes[1])\n        ctx.set_dtype(cast_out.output[0], dtypes[1])\n        ctx.copy_shape(node.output[1], cast_out.output[0])\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # opset 11 supports negative axis, and new attrs \'largest\' and \'sorted\'\n        # the core logic doesn\'t change, using defaults for new attrs\n        cls.version_10(ctx, node, **kwargs)\n\n\n@tf_op(""Tile"")\nclass Tile:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # onnx wants shape input to be int64\n        _convert_shapenode_to_int64(ctx, node, 1)\n\n\n@tf_op(""Pack"")\nclass Pack:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # hack to make up for the missing onnx pack op\n        axis = node.get_attr(""axis"").i\n        if axis < 0:\n            axis += len(ctx.get_shape(node.input[0])) + 1\n\n        inputs = []\n        dtype = None\n        # insert Unsqueeze on each input\n        for i, n in enumerate(node.inputs):\n            dtype = ctx.get_dtype(node.input[i])\n            shape = ctx.get_shape(node.input[i])\n            new_node = ctx.make_node(""Unsqueeze"", [node.input[i]], op_name_scope=node.name, attr={""axes"": [axis]},\n                                     shapes=[shape], dtypes=[dtype])\n            output_name = new_node.output[0]\n            node.input[i] = output_name\n            inputs.append(output_name)\n\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        # concat all unqueezes\n        concat = ctx.make_node(""Concat"", inputs, op_name_scope=node.name, attr={""axis"": axis},\n                               shapes=shapes, dtypes=dtypes)\n        ctx.replace_all_inputs(ctx.get_nodes(), node.output[0], concat.output[0])\n\n\n@tf_op(""Unpack"")\nclass Unpack:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # hack to make up for the missing onnx unpack op\n        # squeeze does not support negative axis\n        axis = node.get_attr(""axis"").i\n        if axis < 0:\n            shape = ctx.get_shape(node.input[0])\n            utils.make_sure(shape is not None, ""shape of unpack input is None: {}"".format(node.input[0]))\n            axis += len(shape)\n        # split the tensor into n outputs\n        node.type = ""Split""\n        # for each output we need to squeeze axis\n        for n in node.output:\n            op_name = utils.make_name(node.name)\n            squeeze_node = ctx.insert_new_node_on_output(""Squeeze"", n, name=op_name, axes=[axis])\n            ctx.copy_shape(n, squeeze_node.output[0])\n            ctx.copy_dtype(n, squeeze_node.output[0])\n\n\n@tf_op(""OneHot"")\nclass OneHot:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # until there is no onehot op in onnx, a workaround using gather from eye\n        indices_name = node.input[0]\n        indices_shape = ctx.get_shape(indices_name)\n        if len(indices_shape) != 1:\n            # TODO: this works for rank=1 but tensorflow supports more than this.\n            # Same principle should work but we need to implemtn our own eye.\n            raise ValueError(""onehot op: only rank1 is supported"")\n        axis = node.get_attr(""axis"")\n        # axis becomes axis for gather\n        node.set_attr(""axis"", 0)\n        depth = node.inputs[1].get_tensor_value()\n        on_val = node.inputs[2].get_tensor_value(as_list=False)\n        on = on_val.tolist()\n        off = node.inputs[3].get_tensor_value()\n        eye = np.eye(depth, dtype=on_val.dtype)\n        if on != 0:\n            eye[eye == 1] = on\n            eye[eye == 0] = off\n        else:\n            eye[eye == 0] = off\n            eye[eye == 1] = on\n\n        const_name = utils.make_name(node.name)\n        ctx.make_const(const_name, eye)\n        # setup gather inputs\n        del node.input[:]\n        node.input.append(const_name)\n        node.input.append(indices_name)\n        node.type = ""Gather""\n        if axis.i == 0:\n            # TODO: revisit for rank > 1\n            name = utils.make_name(node.name)\n            transpose_node = ctx.insert_new_node_on_output(""Transpose"", node.output[0], name)\n            ctx.copy_shape(node.output[0], transpose_node.output[0])\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        # T output = OneHot(uint8/int32/int64 input, T depth, T on-value, T off-value, @int axis, @dtype)\n        # tf requires that dtype is same as on-value\'s and off-value\'s dtype\n        # in ONNX, op\'s schema is (input, depth, value, @int axis), meaning of ""value"" is [off-value, on-value]\n        # onnxruntime only supports int64\n        output_dtype = ctx.get_dtype(node.input[2])\n        if ctx.is_target(constants.TARGET_RS6) \\\n                and output_dtype not in [onnx_pb.TensorProto.INT64, onnx_pb.TensorProto.INT32]:\n            logger.warning(""unsupported dtype in onnxruntime, onehot-9 can\'t be used directly"")\n            cls.version_1(ctx, node, **kwargs)\n            return\n\n        depth = node.input[1]\n        depth = ctx.make_node(""Unsqueeze"", [depth], attr={""axes"": [0]}).output[0]\n\n        on_value = node.input[2]\n        off_value = node.input[3]\n        on_value = ctx.make_node(""Unsqueeze"", [on_value], attr={""axes"": [0]}).output[0]\n        off_value = ctx.make_node(""Unsqueeze"", [off_value], attr={""axes"": [0]}).output[0]\n        off_on_value = ctx.make_node(""Concat"", [off_value, on_value], attr={""axis"": 0}).output[0]\n\n        indices = node.input[0]\n        if ctx.is_target(constants.TARGET_RS6) \\\n                and ctx.get_dtype(indices) != onnx_pb.TensorProto.INT64:\n            indices = ctx.make_node(""Cast"", [indices], attr={""to"": onnx_pb.TensorProto.INT64}).output[0]\n        node.input[0] = indices\n\n        if ctx.is_target(constants.TARGET_RS6) \\\n                and ctx.get_dtype(depth) != onnx_pb.TensorProto.INT64:\n            depth = ctx.make_node(""Cast"", [depth], attr={""to"": onnx_pb.TensorProto.INT64}).output[0]\n        node.input[1] = depth\n\n        if ctx.is_target(constants.TARGET_RS6) \\\n                and output_dtype != onnx_pb.TensorProto.INT64:\n            off_on_value = ctx.make_node(""Cast"", [off_on_value], attr={""to"": onnx_pb.TensorProto.INT64}).output[0]\n        node.input[2] = off_on_value\n\n        del node.input[3]\n\n        if ctx.is_target(constants.TARGET_RS6) \\\n                and output_dtype != onnx_pb.TensorProto.INT64:\n            new_node_name = utils.make_name(""onehot_output"")\n            new_node = ctx.insert_new_node_on_output(""Cast"", node.output[0], new_node_name, to=output_dtype)\n            ctx.set_dtype(new_node.output[0], output_dtype)\n            ctx.set_shape(new_node.output[0], ctx.get_shape(node.output[0]))\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # Opset 11 supports negative axis, but core logic is same\n        cls.version_9(ctx, node, **kwargs)\n\n\n@tf_op(""Shape"")\nclass Shape:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # out_type output = Shape(T input, @int32|int64 out_type), out_type by default int32\n        # int64 output = Shape(T input)\n        dtype = ctx.get_dtype(node.output[0])\n        if dtype == onnx_pb.TensorProto.INT64:\n            return\n        op_name = utils.make_name(node.name)\n        output_cast = ctx.insert_new_node_on_output(""Cast"", node.output[0], name=op_name)\n        output_cast.set_attr(""to"", dtype)\n        ctx.set_dtype(output_cast.output[0], dtype)\n        ctx.copy_shape(node.output[0], output_cast.output[0])\n\n\n@tf_op(""IsNan"", onnx_op=""IsNaN"")\nclass IsNan:\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        pass\n\n\n@tf_op(""BatchToSpaceND"", onnx_op=""DepthToSpace"")\nclass BatchToSpace:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # block_shape impacts Transpose \'perm\' attribute values.\n        # must be available at compile time\n        utils.make_sure(node.inputs[1].is_const(), \'only support constant block_shape value.\')\n\n        block_shape = node.inputs[1].get_tensor_value(False)\n        blocklen = len(block_shape)\n        xlen = len(ctx.get_shape(node.input[0]))\n\n        # if 3d or 4d tensor & square 2d block_shape , can optimize\n        cond1 = xlen in [3, 4]\n        cond2 = node.inputs[2].is_const()\n        cond3 = blocklen == 2 and block_shape[0] == block_shape[1]\n        if cond1 and cond2 and cond3:\n            # https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d.html\n            # the above link says the data format of input tensor should be (batch, spatial_shape, remaining_shape)\n            # and we only support 3D and 4D here, and the data format is NHC and NHWC\n            # onnx op ""DepthToSpace"" does the same work on input tensor except that it works on ""C"",\n            # and it only supports NCHW\n            # T out = BatchToSpaceND(T input, int32 block_shape, int32 crops)\n            input_tensor = node.inputs[0]\n            input_shape = ctx.get_shape(input_tensor.output[0])\n            crops = node.inputs[2].get_tensor_value()\n\n            # NHWC TO CNHW, so onnx op will work on ""N"" which is the same as tensorflow\n            if len(input_shape) == 3:\n                # insert automatically an Unsqueeze op if the input is 3d\n                unsqz1 = ctx.make_node(""Unsqueeze"", input_tensor.output, {""axes"": [3]})\n                trans1 = ctx.make_node(""Transpose"", unsqz1.output, {""perm"": [3, 0, 1, 2]})\n            else:\n                trans1 = ctx.make_node(""Transpose"", input_tensor.output, {""perm"": [3, 0, 1, 2]})\n            reorganize_node = ctx.make_node(node.type, trans1.output, attr={""blocksize"": block_shape[0]})\n            trans2 = ctx.make_node(""Transpose"", reorganize_node.output, {""perm"": [1, 2, 3, 0]})\n\n            # implement crop logic, the data format is NHWC\n            slice_axis = [1, 2]\n            top, bottom = crops[0]\n            left, right = crops[1]\n            starts = [top, left]\n            ends = []\n            for end in [bottom, right]:\n                if end != 0:\n                    ends.append(-end)\n                else:\n                    ends.append(np.iinfo(np.int32).max)\n\n            attr = {""axes"": slice_axis, ""ends"": ends, ""starts"": starts}\n            inputs_map = {""data"": trans2.output[0], **attr}\n            dtypes = node.output_dtypes\n            shapes = node.output_shapes\n\n            if len(input_shape) == 3:\n                # add a squeeze op to convert output into 3d\n                kwargs = {**inputs_map}\n                ctx.remove_node(node.name)\n                slice1 = GraphBuilder(ctx).make_slice(kwargs)\n                ctx.make_node(""Squeeze"", [slice1], {""axes"": [3]},\n                              outputs=node.output, name=node.name, dtypes=dtypes, shapes=shapes)\n            else:\n                kwargs = {**inputs_map, ""outputs"": node.output}\n                ctx.remove_node(node.name)\n                GraphBuilder(ctx).make_slice(kwargs, name=node.name, dtypes=dtypes, shapes=shapes)\n        else:\n            def mknode(optype, inputs, attrs=None):\n                nodename = utils.make_name(node.name + \'_\' + optype.lower())\n                return ctx.make_node(optype, inputs, attrs, name=nodename)\n\n            def mkconst(desc, val, dtype=np.int64):\n                nodename = utils.make_name(node.name + \'_\' + desc)\n                const_node = ctx.make_const(utils.make_name(nodename), val.astype(dtype))\n                return const_node.output[0]\n\n            # support non 3D/4D tensors and dynamic crop vals\n            # dynamic slice starts at opset 10\n            utils.make_sure(ctx.opset >= 11, \'non-4D tensor or non-const crops require opset 11\')\n\n            input0 = node.input[0]\n            input2 = node.input[2]\n\n            # const vals\n            int_max_const = mkconst(\'int_max\', np.array([utils.get_max_value(np.int64)]))\n            one_const = mkconst(\'_const_one\', np.array([1]))\n            minus1_const = mkconst(\'_const_minus1\', np.array([-1]))\n            blocklen_resize_const = mkconst(\'_const_blocklen_resize\', np.array([-1, blocklen]))\n            blocklenplus1_const = mkconst(\'_const_blocklenplus1\', np.array([blocklen + 1]))\n            block_shape_const = mkconst(\'_const_block_shape\', block_shape)\n\n            x_shape = ctx.insert_new_node_on_input(node, \'Shape\', node.input[0])\n\n            # get the spatial and depth (i.e remaining) dimensions\n            # compute target spatial dimensions by multiplying block_shape\n            spatial = mknode(\'Slice\', [x_shape.output[0], one_const, blocklenplus1_const])\n            depth = mknode(\'Slice\', [x_shape.output[0], blocklenplus1_const, int_max_const])\n            target_spatial = mknode(\'Mul\', [spatial.output[0], block_shape_const])\n\n            # shape to use before shuffle  (part 1)\n            ccat1 = mknode(\'Concat\', [spatial.output[0], block_shape_const], {\'axis\': 0})\n            re1 = mknode(\'Reshape\', [ccat1.output[0], blocklen_resize_const])\n            tr1 = mknode(\'Transpose\', [re1.output[0]])\n            interleave = mknode(\'Reshape\', [tr1.output[0], minus1_const])\n            shape1 = mknode(\'Concat\', [minus1_const, interleave.output[0], depth.output[0]], {\'axis\': 0})\n\n            # shape to use before shuffle (part 2)\n            g1 = list(range(2, 2 * blocklen + 1, 2))\n            g2 = list(range(1, 2 * blocklen + 1, 2))\n            g = g1 + [0] + g2 + list(range(0, xlen + blocklen)[1 + 2 * blocklen:])\n\n            # permutation values for shuffling\n            p = np.asarray(range(0, xlen + blocklen))\n            p[0] = blocklen\n            p[1] = blocklen + 1\n            p[2] = 0\n            for i in range(3, blocklen * 2 + 1):\n                p[i] = p[i - 2] + 1\n\n            # reshape to create moving blocks, shuffle, and reshape to target_spatial\n            indices = mkconst(\'_indicies_const\', np.asarray(g))\n            gather = mknode(\'Gather\', [shape1.output[0], indices])\n            x2 = mknode(\'Reshape\', [input0, gather.output[0]])\n            tr2 = mknode(\'Transpose\', [x2.output[0]], {\'perm\': np.array(p)})\n            shape2 = mknode(\'Concat\', [minus1_const, target_spatial.output[0], depth.output[0]], {\'axis\': 0})\n            x3 = mknode(\'Reshape\', [tr2.output[0], shape2.output[0]])\n\n            # crop axes\n            slice_starts_const1 = mkconst(\'_slicestart1_const\', np.asarray([0, 0]))\n            slice_starts_const2 = mkconst(\'_slicestart2_const\', np.asarray([1, utils.get_max_value(np.int64)]))\n            slice_ends_const1 = mkconst(\'_sliceend1_const\', np.asarray([1, 0]))\n            slice_ends_const2 = mkconst(\'_sliceend2_const\', np.asarray([2, utils.get_max_value(np.int64)]))\n            axes_const = mkconst(\'_sliceaxes_const\', np.asarray(range(1, blocklen + 1)))\n            crop = mknode(\'Cast\', [input2], {\'to\': TensorProto.INT64})\n            crop_transposed = mknode(\'Transpose\', [crop.output[0]])\n            crop_starts = mknode(\'Slice\', [crop_transposed.output[0], slice_starts_const1, slice_starts_const2])\n            crop_ends = mknode(\'Slice\', [crop_transposed.output[0], slice_ends_const1, slice_ends_const2])\n            crop_starts_squeeze = mknode(\'Squeeze\', [crop_starts.output[0]], {\'axes\': [0]})\n            crop_ends_squeeze = mknode(\'Squeeze\', [crop_ends.output[0]], {\'axes\': [0]})\n            end_range = mknode(\'Sub\', [target_spatial.output[0], crop_ends_squeeze.output[0]])\n            orig_shape = node.output_shapes\n            orig_dtypes = node.output_dtypes\n            ctx.remove_node(node.name)\n            ctx.make_node(\'Slice\', [x3.output[0], crop_starts_squeeze.output[0], end_range.output[0], axes_const],\n                          name=node.name, outputs=node.output, shapes=orig_shape, dtypes=orig_dtypes)\n\n\n@tf_op(""SpaceToBatchND"", onnx_op=""SpaceToDepth"")\nclass SpaceToBatch:\n    @classmethod\n    def version_1(cls, ctx, node, **kwargs):\n        # block_shape impacts Transpose \'perm\' attribute values.\n        # must be available at compile time\n        utils.make_sure(node.inputs[1].is_const(), \'only support constant block_shape value.\')\n\n        block_shape = node.inputs[1].get_tensor_value(False)\n        blocklen = len(block_shape)\n        xlen = len(ctx.get_shape(node.input[0]))\n\n        # if 3d or 4d tensor & square 2d block_shape , can optimize\n        cond1 = xlen in [3, 4]\n        cond2 = node.inputs[2].is_const()\n        cond3 = blocklen == 2 and block_shape[0] == block_shape[1]\n        if cond1 and cond2 and cond3:\n            # https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd\n            # the above link says the data format of input tensor should be (batch, spatial_shape, remaining_shape)\n            # and we only support 4D here, so the data format is NHWC\n            # onnx op ""SpaceToDepth"" does the same work on input tensor except that it works on ""C"",\n            # and it only supports NCHW\n            # T out = SpaceToBatchND(T input, int32 block_shape, int32 crops)\n            input_tensor = node.inputs[0]\n            shapes = [ctx.get_shape(node.output[0])]\n            dtypes = [ctx.get_dtype(node.output[0])]\n\n            # implement pads logic, the data format is NHWC\n            paddings = node.inputs[2].get_tensor_value()\n            top, bottom = paddings[0]\n            left, right = paddings[1]\n            pads = [0, top, left, 0,\n                    0, bottom, right, 0]\n            ctx.remove_node(node.name)\n            if ctx.opset <= 10:\n                pad_op = ctx.make_node(""Pad"", input_tensor.output, attr={""pads"": pads})\n            else:\n                # TODO: we should be able to support dynamic input here.\n                pads_name = utils.make_name(node.name)\n                ctx.make_const(name=pads_name, np_val=np.array(pads, dtype=np.int64))\n                pad_op = ctx.make_node(""Pad"", [input_tensor.output[0], pads_name])\n\n            # NHWC TO CNHW, so onnx op will work on ""N"" which is the same as tensorflow\n            trans1 = ctx.make_node(""Transpose"", pad_op.output, {""perm"": [3, 0, 1, 2]})\n            reorganize_node = ctx.make_node(node.type, trans1.output, attr={""blocksize"": block_shape[0]})\n            ctx.make_node(""Transpose"", reorganize_node.output, {""perm"": [1, 2, 3, 0]},\n                          name=node.name, outputs=node.output, shapes=shapes, dtypes=dtypes)\n        else:\n            def mknode(optype, inputs, attrs=None):\n                nodename = utils.make_name(node.name + \'_\' + optype.lower())\n                return ctx.make_node(optype, inputs, attrs, name=nodename)\n\n            def mkconst(desc, val, dtype=np.int64):\n                nodename = utils.make_name(node.name + \'_\' + desc)\n                const_node = ctx.make_const(utils.make_name(nodename), val.astype(dtype))\n                return const_node.output[0]\n\n            # support non 3D/4D tensors and dynamic pad vals\n            # dynamic slice starts at opset 10\n            utils.make_sure(ctx.opset >= 11, \'non-4D tensor or non-const pads require opset 11\')\n\n            input0 = node.input[0]\n            input2 = node.input[2]\n\n            # const vals\n            int_max_const = mkconst(\'int_max\', np.array([utils.get_max_value(np.int64)]))\n            zero_const = mkconst(\'_zero_const\', np.array([0]))\n            one_const = mkconst(\'_one_const\', np.array([1]))\n            minus1_const = mkconst(\'_minus1_const\', np.array([-1]))\n            blocklen_resize_const = mkconst(\'_blocklen_resize_const\', np.array([-1, blocklen]))\n            blocklenplus1_const = mkconst(\'_blocklenplus1_const\', np.array([blocklen + 1]))\n            filltop_const = mkconst(\'_filltop_const\', np.array([1, 0, 0, 0]))\n            fillbottom_const = mkconst(\'_bottom_const\', np.array([0, 0, 1, 0]))\n            block_shape_const = mkconst(\'_block_shape_const\', block_shape)\n\n            x_shape = ctx.insert_new_node_on_input(node, \'Shape\', node.input[0])\n            x_rank = mknode(\'Size\', [x_shape.output[0]])\n\n            # pad x prior to compute\n            pad = mknode(\'Cast\', [input2], {\'to\': TensorProto.INT64})\n            pad_shape = mknode(\'Shape\', [pad.output[0]])\n            pad_rank = mknode(\'Slice\', [pad_shape.output[0], zero_const, one_const])\n            pad_gap = mknode(\'Sub\', [x_rank.output[0], pad_rank.output[0]])\n            gapminus1 = mknode(\'Sub\', [pad_gap.output[0], one_const])\n            gapminus1fillbot = mknode(\'Mul\', [fillbottom_const, gapminus1.output[0]])\n            padfilltop = mknode(\'Pad\', [pad.output[0], filltop_const])\n            padfilltopbottom = mknode(\'Pad\', [padfilltop.output[0], gapminus1fillbot.output[0]])\n            pad_t = mknode(\'Transpose\', [padfilltopbottom.output[0]])\n            pad1d = mknode(\'Reshape\', [pad_t.output[0], minus1_const])\n\n            # get the spatial and depth (i.e remaining) dimensions\n            # compute reduced spatial dimensions by dividing block_shape\n            x1 = mknode(\'Pad\', [input0, pad1d.output[0]])\n            x1_shape = mknode(\'Shape\', [x1.output[0]])\n            spatial = mknode(\'Slice\', [x1_shape.output[0], one_const, blocklenplus1_const])\n            depth = mknode(\'Slice\', [x1_shape.output[0], blocklenplus1_const, int_max_const])\n            reduced = mknode(\'Div\', [spatial.output[0], block_shape_const])\n\n            # reshape x into smaller blocks before shuffle\n            ccat1 = mknode(\'Concat\', [reduced.output[0], block_shape_const], {\'axis\': 0})\n            reshape1 = mknode(\'Reshape\', [ccat1.output[0], blocklen_resize_const])\n            tr1 = mknode(\'Transpose\', [reshape1.output[0]])\n            interleave = mknode(\'Reshape\', [tr1.output[0], minus1_const])\n            shape1 = mknode(\'Concat\', [minus1_const, interleave.output[0], depth.output[0]], {\'axis\': 0})\n            x2 = mknode(\'Reshape\', [x1.output[0], shape1.output[0]])\n\n            # permutation values for shuffling\n            p1 = list(range(2, 2 * blocklen + 1, 2))\n            p2 = list(range(1, 2 * blocklen + 1, 2))\n            perm = p1 + [0] + p2 + list(range(0, xlen + blocklen)[1 + 2 * blocklen:])\n\n            tr2 = mknode(\'Transpose\', [x2.output[0]], {\'perm\': perm})\n            shape2 = mknode(\'Concat\', [minus1_const, reduced.output[0], depth.output[0]], {\'axis\': 0})\n            orig_shape = node.output_shapes\n            orig_dtypes = node.output_dtypes\n            ctx.remove_node(node.name)\n            ctx.make_node(\'Reshape\', [tr2.output[0], shape2.output[0]],\n                          name=node.name, outputs=node.output, shapes=orig_shape,\n                          dtypes=orig_dtypes)\n\n\n@tf_op(""IsInf"", onnx_op=""IsInf"")\nclass IsInf:\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        node_dtype = ctx.get_dtype(node.input[0])\n        utils.make_sure(node_dtype, ""Dtype of {} is None"".format(node.name))\n        if node_dtype not in [onnx_pb.TensorProto.FLOAT, onnx_pb.TensorProto.DOUBLE]:\n            raise ValueError(""dtype "" + str(node_dtype) + "" is not supported in onnx for now"")\n\n\n@tf_op([""NonMaxSuppressionV2"", ""NonMaxSuppressionV3"", ""NonMaxSuppressionV4"", ""NonMaxSuppressionV5""],\n       onnx_op=""NonMaxSuppression"")\nclass NonMaxSuppression:\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        # int32 = NonMaxSuppressionV2(T boxes, T scores, int32 max_output_size, T iou_threshold, T score_threshold)\n        # int64 = NonMaxSuppression(T boxes, T scores, int64 max_output_size, T iou_threshold, T score_threshold),\n        # T means float32 here, the last 3 params are optional\n        # tf boxes is 2D ([boxes_num, 4]) while onnx is 3D ([num_batches, boxes_num, 4])\n        # tf scores is 1D ([boxes_num])while onnx is 2D ([num_batches, num_classes, boxes_num])\n        # onnx output is [num_selected_boxes, 3], the meaning of last dim is [batch_index, class_index, box_index]\n        # while tf\'s output is [num_selected_boxes]\n        ctx.insert_new_node_on_input(node, ""Unsqueeze"", node.input[0], axes=[0])\n        input_score = ctx.insert_new_node_on_input(node, ""Unsqueeze"", node.input[1], axes=[0, 1])\n        ctx.insert_new_node_on_input(node, ""Cast"", node.input[2], to=onnx_pb.TensorProto.INT64)\n        # replace original node with nonmaxsurppress + slice + squeeze + cast\n        dtypes = [[ctx.get_dtype(output)] for output in node.output]\n        shapes = [[ctx.get_shape(output)] for output in node.output]\n        max_output_size = node.input[2]\n        utils.make_sure(len(node.inputs) <= 5 or int(node.inputs[5].get_tensor_value(False)) == 0,\n                        ""soft_nms_sigma must be 0"")\n        ctx.remove_node(node.name)\n        new_nonmaxsurppress = ctx.make_node(node.type, node.input[: 5]).output[0]\n        slice_op = GraphBuilder(ctx).make_slice({""data"": new_nonmaxsurppress,\n                                                 ""axes"": [1], ""ends"": [3], ""starts"": [2]})\n        squeeze_op = ctx.make_node(""Squeeze"", [slice_op], attr={""axes"": [1]})\n        if len(node.input) > 5:  # v5, called by ..._with_scores(), pad_to_max_output_size always False\n            ctx.make_node(""Cast"", inputs=squeeze_op.output, attr={""to"": onnx_pb.TensorProto.INT32},\n                          outputs=[node.output[0]], dtypes=dtypes[0], shapes=shapes[0])\n            ctx.make_node(""Gather"", inputs=[input_score.input[0], squeeze_op.output[0]],\n                          outputs=[node.output[1]], dtypes=dtypes[1], shapes=shapes[1])\n        elif ""pad_to_max_output_size"" in node.attr:  # V4\n            shape_op = ctx.make_node(""Shape"", inputs=[squeeze_op.output[0]])\n            const_zero = ctx.make_const(utils.make_name(""const_zero""), np.array([0], dtype=np.int64))\n            sub_op = ctx.make_node(""Sub"", inputs=[max_output_size, shape_op.output[0]])\n            raw_pad = ctx.make_node(""Concat"", inputs=[const_zero.output[0], sub_op.output[0]], attr={\'axis\': 0})\n            raw_pad_float = ctx.make_node(""Cast"", inputs=[raw_pad.output[0]], attr={""to"": onnx_pb.TensorProto.FLOAT})\n            relu_op = ctx.make_node(""Relu"", inputs=[raw_pad_float.output[0]])\n            pad_val = ctx.make_node(""Cast"", inputs=[relu_op.output[0]], attr={""to"": onnx_pb.TensorProto.INT64})\n            pad_op = ctx.make_node(""Pad"", inputs=[squeeze_op.output[0], pad_val.output[0]])\n            ctx.make_node(""Cast"", inputs=pad_op.output, name=""cast_A"", attr={""to"": onnx_pb.TensorProto.INT32},\n                          outputs=[node.output[0]], dtypes=dtypes[0], shapes=shapes[0], op_name_scope=node.name)\n            reduce_op = ctx.make_node(""ReduceSum"", inputs=shape_op.output, attr={""axes"": [0], ""keepdims"": 0})\n            ctx.make_node(""Cast"", inputs=[reduce_op.output[0]], name=""cast_B"", attr={""to"": onnx_pb.TensorProto.INT32},\n                          outputs=[node.output[1]], dtypes=dtypes[1], shapes=shapes[1], op_name_scope=node.name)\n        else:\n            ctx.make_node(""Cast"", inputs=squeeze_op.output, attr={""to"": onnx_pb.TensorProto.INT32},\n                          name=node.name, outputs=node.output, dtypes=dtypes[0], shapes=shapes[0])\n\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # no change\n        cls.version_10(ctx, node, **kwargs)\n\n\n@tf_op(""ReverseSequence"")\nclass ReverseSequence:\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        # T output = ReverseSequence(T input, int32|int64 seq_lengths, @int seq_dim, @int batch_dim)\n        # T output = Scan(int64 sequence_lens, variadic initial_state_and_scan_inputs, @graph body,\n        #                 @ints directions,@int num_scan_inputs)\n        seq_dim = node.get_attr(""seq_dim"")\n        batch_dim = node.get_attr(""batch_dim"")\n        batch_major = seq_dim.i == 1 and (batch_dim or batch_dim.i == 0)\n        time_major = batch_dim.i == 1 and (seq_dim or seq_dim.i == 0)\n        perm_val = None\n\n        if not batch_major and not time_major:\n            error_msg = ""unsupported attributes, seq_dim:{}, batch_dim:{}"".format(seq_dim, batch_dim)\n            raise ValueError(error_msg)\n\n        if time_major:\n            old_shape = ctx.get_shape(node.input[0])\n            old_dtype = ctx.get_dtype(node.input[0])\n            perm_val = [1, 0]\n            rank = len(old_shape)\n            utils.make_sure(rank >= 2, ""rank of reverse_sequence input {} is at least 2"".format(node.input[0]))\n            perm_val += list(range(2, rank))\n            trans_node = ctx.insert_new_node_on_input(node, ""Transpose"", node.input[0], perm=perm_val)\n            new_shape = nn.spatial_map(old_shape, perm_val)\n            ctx.set_shape(trans_node.output[0], new_shape)\n            ctx.set_dtype(trans_node.output[0], old_dtype)\n\n        # handle batch_major input\n        node.type = ""Scan""\n        node.set_attr(""num_scan_inputs"", 1)\n        input_dtype = ctx.get_dtype(node.input[0])\n        input_shape = ctx.get_shape(node.input[0])\n\n        g = ctx.create_new_graph_with_same_config()\n        g.parent_graph = ctx\n        g.add_graph_input(\'X\', input_dtype, input_shape[2:])\n        g.make_node(\'Identity\', [\'X\'], outputs=[\'Y\'])\n        g.add_graph_output(\'Y\', input_dtype, input_shape[2:])\n\n        node.set_body_graph_as_attr(""body"", g)\n        node.set_attr(""directions"", [1])  # reverse the scan input\n\n        seq_len_dtype = ctx.get_dtype(node.input[1])\n        if seq_len_dtype != onnx_pb.TensorProto.INT64:\n            cast_node = ctx.insert_new_node_on_input(node, ""Cast"", node.input[1])\n            cast_node.set_attr(""to"", onnx_pb.TensorProto.INT64)\n            ctx.set_dtype(cast_node.output[0], onnx_pb.TensorProto.INT64)\n            ctx.copy_shape(node.input[1], cast_node.output[0])\n\n        if time_major:\n            # get back to time_major\n            op_name = utils.make_name(node.name)\n            trans_back_node = ctx.insert_new_node_on_output(""Transpose"", node.output[0],\n                                                            name=op_name, perm=perm_val)\n            ctx.copy_dtype(node.output[0], trans_back_node.output[0])\n\n        tmp = node.input[0]\n        node.input[0] = node.input[1]\n        node.input[1] = tmp\n\n    @classmethod\n    def version_9(cls, ctx, node, **kwargs):\n        # T output = ReverseSequence(T input, int32|int64 seq_lengths, @int seq_dim, @int batch_dim)\n        # we cannot easily construct reverse_sequence equivalence in opset 9, so we will not support it\n        # here. Actually using loops to do that is kind of meaningless since there will be performance\n        # issue there for sure.\n        raise NotImplementedError(""ReverseSequence is not supported to convert in OPSET 9,""\n                                  "" if possible please try using OPSET 8, or OPSET >=10 instead."")\n\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        # T output = ReverseSequence(T input, int32|int64 seq_lengths, @int seq_dim, @int batch_dim)\n        # T output = ReverseSequence(T input, int64 sequence_lens, @int time_axis, @int batch_axis)\n        seq_dim = node.get_attr(""seq_dim"")\n        utils.make_sure(seq_dim is not None, ""sequence dim must be given in {}"".format(node.name))\n        seq_dim = seq_dim.i\n        batch_dim = node.get_attr_value(""batch_dim"", 0)\n\n        ctx.remove_node(node.name)\n        node = ctx.make_node(\n            ""ReverseSequence"",\n            node.input,\n            outputs=node.output,\n            attr={""batch_axis"": batch_dim, ""time_axis"": seq_dim})\n\n        seq_len_dtype = ctx.get_dtype(node.input[1])\n        utils.make_sure(seq_len_dtype is not None, ""dtype of {} is None"".format(node.input[1]))\n        target_dtype = TensorProto.INT64\n        if seq_len_dtype != target_dtype:\n            ctx.insert_new_node_on_input(node, ""Cast"", node.input[1], to=target_dtype)\n\n\n@tf_op(""ReverseV2"")\nclass ReverseV2:\n    @classmethod\n    def version_10(cls, ctx, node, **kwargs):\n        # T output = ReverseV2(T input, int32|int64 seq_lengths, @int seq_dim, @int batch_dim)\n        # Implement tensorflow ReverseV2 op using multiple ReverseSequence (for each axis)\n        # and Transpose ops. We sort the axis vector (if non-empty) at the start. Each axis can\n        # be reversed only once (in tf) and so we can compute the transpose for each axis\n        # (other than 0), feed the tensor to a ReverseSequence node and finally transpose again\n        # to get back the original shape.\n\n        axes_node = node.inputs[1]\n        axes = axes_node.get_tensor_value(as_list=False)\n        # Current support is for when axis is a 1D tensor.\n        utils.make_sure(len(axes.shape) == 1 \\\n                        , ""Currently no support for reverseV2 tensor axis"")\n\n        axes = axes.tolist()\n        len_axes = len(axes)\n\n        # Store input and output parameters of the ReverseV2 node.\n        rv2_in_names = [node.input[0]]\n\n        input_shape = ctx.get_shape(node.input[0])\n        input_rank = len(input_shape)\n        input_shape_node = ctx.make_node(""Shape"", [node.input[0]], op_name_scope=node.name)\n\n        # Make sure input shape is not None\n        utils.make_sure(input_shape is not None, ""shape of {} is None"".format(node.input[0]))\n\n        rv2_node_name = node.name\n        # ReverseV2 has a single output.\n        rv2_output_dtypes = node.output_dtypes\n        rv2_output_shapes = node.output_shapes\n\n        # Remove ReverseV2 node from graph.\n        ctx.remove_node(rv2_node_name)\n\n        # Variable to store input names for the next node.\n        inputs = rv2_in_names\n\n        new_node = None\n\n        # Empty axis vector.\n        if len_axes == 0:\n            # Replace ReverseV2 with an identity block.\n            ctx.make_node(\n                ""Identity"",\n                inputs=inputs,\n                outputs=node.output,\n                shapes=rv2_output_shapes,\n                dtypes=rv2_output_dtypes,\n                op_name_scope=rv2_node_name,\n            )\n\n        else:\n            # For negative indices use the positive counterpart.\n            for i, ax in enumerate(axes):\n                if ax < 0:\n                    axes[i] += input_rank\n\n            axes = sorted(axes)\n\n            orig_perm = list(range(input_rank))\n            curr_perm = []\n\n            # Add ReverseSequence nodes for each element of axis.\n            for i in range(len_axes):\n\n                axis = axes[i]\n\n                curr_perm = orig_perm.copy()\n                # Permutation indices relative to original tensor.\n                curr_perm[axis], curr_perm[0] = curr_perm[0], curr_perm[axis]\n\n                # Add a Transpose node if the axis != 0 (finish first due to sort).\n                if axis != 0:\n                    # Permutation indices for the transpose node relative to IN tensor shape.\n                    new_node = ctx.make_node(\n                        ""Transpose"",\n                        inputs=inputs,\n                        op_name_scope=rv2_node_name,\n                        dtypes=rv2_output_dtypes,\n                        attr={""perm"": curr_perm}\n                    )\n\n                    inputs = [new_node.output[0]]\n\n                const_one_name = utils.make_name(f\'const_one\')\n                const_one = ctx.make_const(name=const_one_name, np_val=np.array([1], dtype=np.int64))\n                const_axis_name = utils.make_name(f\'const_{axis}\')\n                const_axis = ctx.make_const(name=const_axis_name, np_val=np.array([axis], dtype=np.int64))\n\n                # Add a Constant node (seq_len) for ReverseSequence.\n                # Index 1 for the shape should not return 0, since rank(input) >=2\n                input_shape = ctx.make_node(""Shape"", [inputs[-1]], op_name_scope=rv2_node_name)\n                batch_size = ctx.make_node(""Gather"", [input_shape.output[0], const_one.output[0]],\n                                           op_name_scope=rv2_node_name)\n                axis_dim = ctx.make_node(""Gather"", [input_shape_node.output[0], const_axis.output[0]],\n                                         op_name_scope=rv2_node_name)\n                seq_array = ctx.make_node(""Expand"", [axis_dim.output[0], batch_size.output[0]])\n                inputs.append(seq_array.output[0])\n\n                # Add a ReverseSequence node.\n\n                # If processing for the final axis and the tensor shape permutation is\n                # original then the output is fed to the output of the ReverseV2 node.\n                #\n                # Else a new output is created which is fed to a Transpose node.\n                rs_out_name = node.output if \\\n                    ((i == len_axes - 1) and (curr_perm == orig_perm)) \\\n                    else None\n\n                rs_out_shapes = None if rs_out_name is None else rv2_output_shapes\n\n                new_node = ctx.make_node(\n                    ""ReverseSequence"",\n                    inputs=inputs,\n                    op_name_scope=rv2_node_name,\n                    outputs=rs_out_name,\n                    shapes=rs_out_shapes,\n                    dtypes=rv2_output_dtypes,\n                    attr={""batch_axis"": 1, ""time_axis"": 0}\n                )\n\n                inputs = [new_node.output[0]]\n\n            # Additional transpose block is required if the current\n            # permutation list is not the original one.\n            if curr_perm != orig_perm:\n\n                # Compute the required permutation list.\n                if len_axes != 1:\n                    for i, ax in enumerate(axes[::-1][1:]):\n                        curr_perm[0], curr_perm[ax] = \\\n                            curr_perm[ax], curr_perm[0]\n\n                # Add a Transpose node to restore shape.\n                ctx.make_node(\n                    ""Transpose"",\n                    inputs=inputs,\n                    op_name_scope=rv2_node_name,\n                    outputs=node.output,\n                    shapes=rv2_output_shapes,\n                    dtypes=rv2_output_dtypes,\n                    attr={""perm"": curr_perm}\n                )\n\n\n@tf_op(""Unique"", onnx_op=""Unique"")\nclass Unique:\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # opset 11 supports explicitly\n        dtypes = node.output_dtypes\n        if len(node.output) > 1:\n            # cast to int64 if needed\n            if dtypes[1] != onnx_pb.TensorProto.UINT64:\n                cast_node = ctx.insert_new_node_on_output(""Cast"", node.output[1],\n                                                          name=utils.make_name(node.name) + ""_cast"")\n                cast_node.set_attr(""to"", dtypes[1])\n                ctx.set_dtype(cast_node.output[0], dtypes[1])\n                ctx.copy_shape(node.output[1], cast_node.output[0])\n            # FIXME: the indices in onnx are not the same as in tensorflow.\n\n\n@tf_op(""MatrixDiagPart"")\nclass MatrixDiagPart:\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # MatrixDiagPart by slice and gather\n        const_zero = ctx.make_const(utils.make_name(node.name) + \'const_zero\', np.array([0]).astype(np.int64))\n        const_zero_ = ctx.make_const(utils.make_name(node.name) + \'const_zero_\', np.array(0).astype(np.int64))\n\n        const_zero_zero = ctx.make_const(utils.make_name(node.name) + \'const_zero_zero\',\n                                         np.array([0, 0]).astype(np.int64))\n        const_one = ctx.make_const(utils.make_name(node.name) + \'const_one\', np.array([1]).astype(np.int64))\n        const_one_ = ctx.make_const(utils.make_name(node.name) + \'const_one_\', np.array(1).astype(np.int64))\n        const_two = ctx.make_const(utils.make_name(node.name) + \'const_two\', np.array([2]).astype(np.int64))\n        const_negative_one = ctx.make_const(utils.make_name(node.name) + \'const_negative_one\',\n                                            np.array([-1]).astype(np.int64))\n        const_negative_two = ctx.make_const(utils.make_name(node.name) + \'const_negative_two\',\n                                            np.array([-2]).astype(np.int64))\n        const_negative_two_one = ctx.make_const(utils.make_name(node.name) + \'const_negative_two_one\',\n                                                np.array([-2, -1]).astype(np.int64))\n        input_shape = ctx.make_node(\'Shape\', [node.input[0]])\n        input_shape_size = ctx.make_node(\'Shape\', [input_shape.output[0]])\n        matrice_shape = ctx.make_node(\'Slice\',\n                                      [input_shape.output[0], const_negative_two.output[0], input_shape_size.output[0]])\n        matrice_shape_float = ctx.make_node(\'Cast\', [matrice_shape.output[0]], attr={\'to\': TensorProto.FLOAT})\n        matrice_shape_float_x = ctx.make_node(\'Slice\', [matrice_shape_float.output[0], const_zero.output[0],\n                                                        const_one.output[0]])\n        matrice_shape_float_y = ctx.make_node(\'Slice\',\n                                              [matrice_shape_float.output[0], const_one.output[0], const_two.output[0]])\n        min_matrice_dim_float = ctx.make_node(\'Min\', [matrice_shape_float_x.output[0], matrice_shape_float_y.output[0]])\n        min_matrice_dim = ctx.make_node(\'Cast\', [min_matrice_dim_float.output[0]], attr={\'to\': TensorProto.INT64})\n        double_matrice_dim = ctx.make_node(\'Concat\', [min_matrice_dim.output[0], min_matrice_dim.output[0]],\n                                           attr={\'axis\': -1})\n        sliced_input = ctx.make_node(\'Slice\', [node.input[0], const_zero_zero.output[0], double_matrice_dim.output[0],\n                                               const_negative_two_one.output[0]])\n        sliced_input_shape = ctx.make_node(\'Shape\', [sliced_input.output[0]])\n        sliced_input_shape_half = ctx.make_node(\'Slice\', [sliced_input_shape.output[0], const_zero.output[0],\n                                                          const_negative_one.output[0]])\n        sliced_input_shape_new = ctx.make_node(\'Concat\', [sliced_input_shape_half.output[0], const_one.output[0]],\n                                               attr={\'axis\': -1})\n        min_matrice_dim_ = ctx.make_node(\'Squeeze\', [min_matrice_dim.output[0]], {\'axes\': [0]})\n        matrice_range = ctx.make_node(\'Range\', [const_zero_.output[0], min_matrice_dim_.output[0],\n                                                const_one_.output[0]])\n        unsqueezed_matrice_range = ctx.make_node(\'Unsqueeze\', [matrice_range.output[0]], attr={""axes"": [-1]})\n        expanded_range = ctx.make_node(\'Expand\', [unsqueezed_matrice_range.output[0], sliced_input_shape_new.output[0]])\n        gathered_result = ctx.make_node(\'GatherElements\', [sliced_input.output[0], expanded_range.output[0]],\n                                        attr={\'axis\': -1})\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(\'Squeeze\', [gathered_result.output[0]], attr={""axes"": [-1]},\n                      name=node.name, outputs=node.output, shapes=shapes, dtypes=dtypes)\n\n\n@tf_op([""MatrixDiagPartV2"", ""MatrixDiagPartV3""])\nclass MatrixDiagPartV2V3:\n    @classmethod\n    def version_11(cls, ctx, node, **kwargs):\n        # assemble MatrixDiagPart V2&V3 by looping k diagonals with proper pads\n        const_zero = ctx.make_const(utils.make_name(node.name) + \'const_zero\', np.array([0]).astype(np.int64))\n        const_one = ctx.make_const(utils.make_name(node.name) + \'const_one\', np.array([1]).astype(np.int64))\n        const_two = ctx.make_const(utils.make_name(node.name) + \'const_two\', np.array([2]).astype(np.int64))\n        const_neg_one = ctx.make_const(utils.make_name(node.name) + \'const_neg_one\', np.array([-1]).astype(np.int64))\n        const_neg_two = ctx.make_const(utils.make_name(node.name) + \'const_neg_two\', np.array([-2]).astype(np.int64))\n        def normalize():\n            raw_k = ctx.make_node(\'Cast\', [node.input[1]], attr={\'to\': TensorProto.INT64}).output[0]\n            return ctx.make_node(\'Reshape\', [raw_k, const_neg_one.output[0]]).output[0]\n        input_tensor = node.input[0]\n        k = normalize()\n        padding = node.input[2]\n        align = \'LEFT_LEFT\'\n        if node.op.op_type == \'MatrixDiagPartV3\':\n            align = node.get_attr_str(\'align\') if \'align\' in node.attr else \'LEFT_RIGHT\'\n        input_rank = len(ctx.get_shape(input_tensor))\n        raw_input_shape = [-1] * input_rank\n        per_loop_shape = raw_input_shape[:-1]\n        raw_output_shape = raw_input_shape[:-2] + [-1]\n        loop_output_shape = raw_output_shape + [-1]\n        ctx.set_shape(node.output[0], raw_output_shape)\n        for out in ctx.find_output_consumers(node.output[0]):\n            if out.op.op_type == \'Identity\':\n                ctx.set_shape(out.output[0], raw_output_shape)\n\n        # prepare new_shape of input\n        input_shape = ctx.make_node(\'Shape\', [input_tensor])\n        shape_input_shape = ctx.make_node(\'Shape\', [input_shape.output[0]])\n        matrix_shape = ctx.make_node(\'Slice\',\n                                     [input_shape.output[0], const_neg_two.output[0], shape_input_shape.output[0]])\n        min_dim = ctx.make_node(\'ReduceMin\', [matrix_shape.output[0]])\n        input_depth = ctx.make_node(\'Slice\', [matrix_shape.output[0], const_neg_two.output[0], const_neg_one.output[0]])\n        input_width = ctx.make_node(\'Slice\', [matrix_shape.output[0], const_neg_one.output[0], const_two.output[0]])\n        temp_shape = ctx.make_node(\'Concat\', [const_neg_one.output[0], matrix_shape.output[0]], attr={\'axis\': 0})\n        temp_input = ctx.make_node(\'Reshape\', [input_tensor, temp_shape.output[0]])\n        temp_transposed = ctx.make_node(\'Transpose\', [temp_input.output[0]], attr={\'perm\': [0, 2, 1]})\n        half_shape = ctx.make_node(\'Slice\', [input_shape.output[0], const_zero.output[0], const_neg_two.output[0]])\n        new_shape = ctx.make_node(\'Concat\', [half_shape.output[0], input_width.output[0], input_depth.output[0]],\n                                  attr={\'axis\': 0})\n        # define body graph for main loop\n        k_shape = ctx.make_node(\'Shape\', [k])\n        k_start = ctx.make_node(\'Slice\', [k, const_zero.output[0], const_one.output[0]])\n        k_end = ctx.make_node(\'Slice\', [k, const_neg_one.output[0], k_shape.output[0]])\n        raw_total_k = ctx.make_node(\'Sub\', [k_end.output[0], k_start.output[0]])\n        total_k = ctx.make_node(\'Add\', [raw_total_k.output[0], const_one.output[0]])\n        trip_name = utils.make_name(node.name + ""_i"")\n        cond_name = utils.make_name(node.name + ""_cond"")\n        body_graph = ctx.create_new_graph_with_same_config()\n        body_graph.add_graph_input(trip_name, TensorProto.INT64, [1])\n        body_graph.add_graph_input(cond_name, TensorProto.BOOL, [])\n        body_graph.parent_graph = ctx\n        # identity of input\n        identity_input_graph = body_graph.create_new_graph_with_same_config()\n        identity_input_graph.parent_graph = body_graph\n        identity_input = identity_input_graph.make_node(\'Identity\', [input_tensor])\n        identity_input_graph.add_graph_output(identity_input.output[0], ctx.get_dtype(node.input[0]), raw_input_shape)\n        # transposed input\n        transposed_input_graph = body_graph.create_new_graph_with_same_config()\n        transposed_input_graph.parent_graph = body_graph\n        next_shape = transposed_input_graph.make_node(\'Concat\', [half_shape.output[0], input_width.output[0],\n                                                                 input_depth.output[0]], attr={\'axis\': 0})\n        transposed_input = transposed_input_graph.make_node(\'Reshape\',\n                                                            [temp_transposed.output[0], next_shape.output[0]])\n        transposed_input_graph.add_graph_output(transposed_input.output[0], ctx.get_dtype(node.input[0]),\n                                                raw_input_shape)\n        # compute current k of the loop\n        current_k = body_graph.make_node(\'Sub\', [k_end.output[0], trip_name])\n        is_k_noneg = body_graph.make_node(\'Greater\', [current_k.output[0], const_neg_one.output[0]])\n        processed_input = body_graph.make_node(\'If\', [is_k_noneg.output[0]])\n        processed_input.set_body_graph_as_attr(\'then_branch\', identity_input_graph)\n        processed_input.set_body_graph_as_attr(\'else_branch\', transposed_input_graph)\n        processed_shape = body_graph.make_node(\'Shape\', [processed_input.output[0]])\n        shape_processed_shape = body_graph.make_node(\'Shape\', [processed_shape.output[0]])\n        new_depth = body_graph.make_node(\'Slice\',\n                                         [processed_shape.output[0], const_neg_two.output[0], const_neg_one.output[0]])\n        new_width = body_graph.make_node(\'Slice\', [processed_shape.output[0], const_neg_one.output[0],\n                                                   shape_processed_shape.output[0]])\n        abs_k = body_graph.make_node(\'Abs\', [current_k.output[0]])\n\n        range_k = body_graph.make_node(\'Range\', [abs_k.output[0], new_width.output[0], const_one.output[0]],\n                                       domain=""com.microsoft"")\n        sliced_range = body_graph.make_node(\'Slice\', [range_k.output[0], const_zero.output[0], new_depth.output[0]])\n        sliced_shape = body_graph.make_node(\'Shape\', [sliced_range.output[0]])\n        pad_length = body_graph.make_node(\'Sub\', [new_depth.output[0], sliced_shape.output[0]])\n        pad_length_2 = body_graph.make_node(\'Concat\', [const_zero.output[0], pad_length.output[0]], attr={\'axis\': 0})\n        padded_range = body_graph.make_node(\'Pad\', [sliced_range.output[0], pad_length_2.output[0]])\n        unsqueezed_range = body_graph.make_node(\'Unsqueeze\', [padded_range.output[0]], attr={\'axes\': [1]})\n        half_shape_x = body_graph.make_node(\'Slice\',\n                                            [new_shape.output[0], const_zero.output[0], const_neg_two.output[0]])\n        shape_range = body_graph.make_node(\'Shape\', [unsqueezed_range.output[0]])\n        full_shape = body_graph.make_node(\'Concat\', [half_shape_x.output[0], shape_range.output[0]], attr={\'axis\': 0})\n        expanded_range = body_graph.make_node(\'Expand\', [unsqueezed_range.output[0], full_shape.output[0]])\n        gathered_input = body_graph.make_node(\'GatherElements\', [processed_input.output[0], expanded_range.output[0]],\n                                              attr={\'axis\': -1})\n        squeezed_input = body_graph.make_node(\'Squeeze\', [gathered_input.output[0]], attr={\'axes\': [-1]})\n        left_width = body_graph.make_node(\'Sub\', [new_width.output[0], abs_k.output[0]])\n        dims = body_graph.make_node(\'Concat\', [left_width.output[0], new_depth.output[0]], attr={\'axis\': 0})\n        valid_dim = body_graph.make_node(\'ReduceMin\', [dims.output[0]])\n        raw_output = body_graph.make_node(\'Slice\', [squeezed_input.output[0], const_zero.output[0], valid_dim.output[0],\n                                                    const_neg_one.output[0]])\n        gap_output = body_graph.make_node(\'Sub\', [min_dim.output[0], valid_dim.output[0]])\n        gaps = body_graph.make_node(\'Concat\', [const_zero.output[0], gap_output.output[0]], attr={\'axis\': 0})\n        processed_gap = body_graph.make_node(\'ReduceMax\', [gaps.output[0]])\n        pad_zero = body_graph.make_node(\'Mul\', [new_shape.output[0], const_zero.output[0]])\n        sliced_zero = body_graph.make_node(\'Slice\', [pad_zero.output[0], const_zero.output[0], const_neg_two.output[0]])\n        # gap_pos_k_graph\n        gap_pos_k_graph = body_graph.create_new_graph_with_same_config()\n        gap_pos_k_graph.parent_graph = body_graph\n        gap_pos_k = gap_pos_k_graph.make_node(\'Concat\', [const_zero.output[0],\n                                                         processed_gap.output[0]],\n                                              attr={\'axis\': 0}) \\\n            if align.startswith(\'LEFT\') \\\n            else gap_pos_k_graph.make_node(\'Concat\', [processed_gap.output[0],\n                                                      const_zero.output[0]],\n                                           attr={\'axis\': 0})\n        gap_pos_k_graph.add_graph_output(gap_pos_k.output[0], TensorProto.INT64, [-1])\n        # gap_neg_k_graph\n        gap_neg_k_graph = body_graph.create_new_graph_with_same_config()\n        gap_neg_k_graph.parent_graph = body_graph\n        gap_neg_k = gap_neg_k_graph.make_node(\'Concat\', [const_zero.output[0],\n                                                         processed_gap.output[0]],\n                                              attr={\'axis\': 0}) \\\n            if align.endswith(\'LEFT\') \\\n            else gap_neg_k_graph.make_node(\'Concat\', [processed_gap.output[0],\n                                                      const_zero.output[0]],\n                                           attr={\'axis\': 0})\n        gap_neg_k_graph.add_graph_output(gap_neg_k.output[0], TensorProto.INT64, [-1])\n        # pad output with gap\n        gap_k = body_graph.make_node(\'If\', [is_k_noneg.output[0]])\n        gap_k.set_body_graph_as_attr(""then_branch"", gap_pos_k_graph)\n        gap_k.set_body_graph_as_attr(""else_branch"", gap_neg_k_graph)\n        gap_left = body_graph.make_node(\'Slice\', [gap_k.output[0], const_zero.output[0], const_one.output[0]])\n        gap_right = body_graph.make_node(\'Slice\', [gap_k.output[0], const_one.output[0], const_two.output[0]])\n        gap_all = body_graph.make_node(\'Concat\', [sliced_zero.output[0], gap_left.output[0], sliced_zero.output[0],\n                                                  gap_right.output[0]], attr={\'axis\': 0})\n        padded_output = body_graph.make_node(\'Pad\', [raw_output.output[0], gap_all.output[0], padding])\n        cond_output = body_graph.make_node(\'Identity\', [cond_name])\n        body_graph.add_graph_output(cond_output.output[0], TensorProto.BOOL, [])\n        body_graph.add_graph_output(padded_output.output[0], ctx.get_dtype(node.input[0]), per_loop_shape)\n        body_graph.add_graph_output(gap_k.output[0], TensorProto.INT64, [-1])\n        # make loop\n        cond_const = ctx.make_const(utils.make_name(""cond""), np.ones((), dtype=np.bool))\n        main_loop = ctx.make_node(\'Loop\', [total_k.output[0], cond_const.output[0]], output_count=2)\n        main_loop.set_body_graph_as_attr(""body"", body_graph)\n        # reshape output\n        next_padded_shape = ctx.make_node(\'Concat\', [total_k.output[0], const_neg_one.output[0], min_dim.output[0]],\n                                          attr={\'axis\': 0})\n        reshaped_padded = ctx.make_node(\'Reshape\', [main_loop.output[0], next_padded_shape.output[0]])\n        transposed_padded = ctx.make_node(\'Transpose\', [reshaped_padded.output[0]], attr={\'perm\': [1, 0, 2]})\n        output_shape = ctx.make_node(\'Concat\', [half_shape.output[0], total_k.output[0], const_neg_one.output[0]],\n                                     attr={\'axis\': 0})\n        reshaped_output = ctx.make_node(\'Reshape\', [transposed_padded.output[0], output_shape.output[0]])\n        # compute pads\n        left_pads = ctx.make_node(\'Slice\', [main_loop.output[1], const_neg_two.output[0], const_neg_one.output[0],\n                                            const_neg_one.output[0]])\n        flattened_left_pads = ctx.make_node(\'Reshape\', [left_pads.output[0], const_neg_one.output[0]])\n        min_left_pads = ctx.make_node(\'ReduceMin\', [flattened_left_pads.output[0]])\n        right_pads = ctx.make_node(\'Slice\', [main_loop.output[1], const_neg_one.output[0], const_two.output[0],\n                                             const_neg_one.output[0]])\n        flattened_right_pads = ctx.make_node(\'Reshape\', [right_pads.output[0], const_neg_one.output[0]])\n        min_right_pads = ctx.make_node(\'ReduceMin\', [flattened_right_pads.output[0]])\n        # trim left pads\n        identity_left_sliced_graph = ctx.create_new_graph_with_same_config()\n        identity_left_sliced_graph.parent_graph = ctx\n        identity_left_sliced = identity_left_sliced_graph.make_node(\'Identity\', [reshaped_output.output[0]])\n        identity_left_sliced_graph.add_graph_output(identity_left_sliced.output[0], ctx.get_dtype(node.input[0]),\n                                                    loop_output_shape)\n        output_left_sliced_graph = ctx.create_new_graph_with_same_config()\n        output_left_sliced_graph.parent_graph = ctx\n        output_left_sliced = output_left_sliced_graph.make_node(\'Slice\',\n                                                                [reshaped_output.output[0], min_left_pads.output[0],\n                                                                 min_dim.output[0], const_neg_one.output[0]])\n        output_left_sliced_graph.add_graph_output(output_left_sliced.output[0], ctx.get_dtype(node.input[0]),\n                                                  loop_output_shape)\n        left_pads_greater_than_zero = ctx.make_node(\'Greater\', [min_left_pads.output[0], const_zero.output[0]])\n        final_output_left_sliced = ctx.make_node(\'If\', [left_pads_greater_than_zero.output[0]])\n        final_output_left_sliced.set_body_graph_as_attr(""then_branch"", output_left_sliced_graph)\n        final_output_left_sliced.set_body_graph_as_attr(""else_branch"", identity_left_sliced_graph)\n        # trim right pads\n        valid_right_dim = ctx.make_node(\'Sub\', [min_dim.output[0], min_right_pads.output[0]])\n        identity_right_sliced_graph = ctx.create_new_graph_with_same_config()\n        identity_right_sliced_graph.parent_graph = ctx\n        identity_right_sliced = identity_right_sliced_graph.make_node(\'Identity\', [final_output_left_sliced.output[0]])\n        identity_right_sliced_graph.add_graph_output(identity_right_sliced.output[0], ctx.get_dtype(node.input[0]),\n                                                     loop_output_shape)\n        output_right_sliced_graph = ctx.create_new_graph_with_same_config()\n        output_right_sliced_graph.parent_graph = ctx\n        output_right_sliced = output_right_sliced_graph.make_node(\'Slice\', [final_output_left_sliced.output[0],\n                                                                            const_zero.output[0],\n                                                                            valid_right_dim.output[0],\n                                                                            const_neg_one.output[0]])\n        output_right_sliced_graph.add_graph_output(output_right_sliced.output[0], ctx.get_dtype(node.input[0]),\n                                                   loop_output_shape)\n        right_dim_greater_than_valid = ctx.make_node(\'Greater\', [min_dim.output[0], valid_right_dim.output[0]])\n        final_output_right_sliced = ctx.make_node(\'If\', [right_dim_greater_than_valid.output[0]])\n        final_output_right_sliced.set_body_graph_as_attr(""then_branch"", output_right_sliced_graph)\n        final_output_right_sliced.set_body_graph_as_attr(""else_branch"", identity_right_sliced_graph)\n        # squeeze output\n        latest_shape = ctx.make_node(\'Shape\', [final_output_right_sliced.output[0]])\n        latest_depth = ctx.make_node(\'Slice\',\n                                     [latest_shape.output[0], const_neg_two.output[0], const_neg_one.output[0]])\n        need_squeeze = ctx.make_node(\'Equal\', [latest_depth.output[0], const_one.output[0]])\n        identity_sliced_graph = ctx.create_new_graph_with_same_config()\n        identity_sliced_graph.parent_graph = ctx\n        identity_sliced = identity_sliced_graph.make_node(\'Identity\', [final_output_right_sliced.output[0]])\n        identity_sliced_graph.add_graph_output(identity_sliced.output[0], ctx.get_dtype(node.input[0]),\n                                               raw_output_shape + [-1])\n        squeeze_sliced_graph = ctx.create_new_graph_with_same_config()\n        squeeze_sliced_graph.parent_graph = ctx\n        squeeze_sliced = squeeze_sliced_graph.make_node(\'Squeeze\', [final_output_right_sliced.output[0]],\n                                                        attr={\'axes\': [-2]})\n        squeeze_sliced_graph.add_graph_output(squeeze_sliced.output[0], ctx.get_dtype(node.input[0]), raw_output_shape)\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        squeeze_if = ctx.make_node(\'If\', [need_squeeze.output[0]], name=node.name, outputs=node.output, shapes=shapes,\n                                   dtypes=dtypes)\n        squeeze_if.set_body_graph_as_attr(""then_branch"", squeeze_sliced_graph)\n        squeeze_if.set_body_graph_as_attr(""else_branch"", identity_sliced_graph)\n\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n\n        def mkconsts(values, dtype=np.int64):\n            ret = []\n            for value in values:\n                name = utils.make_name(node.name + \'_const\')\n                ret.append(ctx.make_const(name, np.array(value, dtype=dtype)).output[0])\n            return ret\n\n        # assemble MatrixDiagPart V2&V3\n        m = node.input[0]\n        m_shape = ctx.get_shape(m)\n        m_rank = len(m_shape)\n        pads = np.zeros(2 * m_rank, dtype=np.int64)\n        pads[-2:] = [1, 1]\n        utils.make_sure(m_rank > 1, \'Input data should be at least 2D %s\', str(m_shape))\n\n        align = \'LEFT_LEFT\'\n        if node.op.op_type == \'MatrixDiagPartV3\':\n            align = node.get_attr_str(\'align\') if \'align\' in node.attr else \'LEFT_RIGHT\'\n        xalign, yalign = align.split(\'_\')\n\n        # consts\n        const_zero_float, const_neg_one_float = mkconsts([0, -1], np.float32)\n        const_zero, const_one, const_neg_one, const_neg_two, const_pad_vals, const_t = \\\n            mkconsts([[0], [1], [-1], [-2], pads, [-1, 1]])\n        const_zero_scalar, const_one_scalar, const_neg_one_scalar = mkconsts([0, 1, -1])\n\n        m_shape = ctx.make_node(\'Shape\', [node.input[0]]).output[0]\n        xlen = ctx.make_node(\'Gather\', [m_shape, const_neg_one]).output[0]\n        ylen = ctx.make_node(\'Gather\', [m_shape, const_neg_two]).output[0]\n        xlenp = ctx.make_node(\'Add\', [xlen, const_one]).output[0]\n        stride = ctx.make_node(\'Add\', [xlenp, const_one]).output[0]\n        minxy_0 = ctx.make_node(\'Concat\', [xlen, ylen], attr={\'axis\': 0}).output[0]\n        minxy = ctx.make_node(\'ReduceMin\', [minxy_0]).output[0]\n        minxy_float = ctx.make_node(\'Cast\', [minxy], attr={\'to\': TensorProto.FLOAT}).output[0]\n        xmax_0 = ctx.make_node(\'Mul\', [xlen, xlenp]).output[0]\n        xmax_1 = ctx.make_node(\'Add\', [xmax_0, xlenp]).output[0]\n        xmax = ctx.make_node(\'Add\', [xmax_1, const_neg_one]).output[0]\n        ymax_0 = ctx.make_node(\'Mul\', [xlenp, ylen]).output[0]\n        ymax = ctx.make_node(\'Add\', [ymax_0, const_neg_one]).output[0]\n        ymax_float = ctx.make_node(\'Cast\', [ymax], attr={\'to\': TensorProto.FLOAT}).output[0]\n        partial_shape = ctx.make_node(\'Slice\', [m_shape, const_zero, const_neg_two]).output[0]\n        m2_shape = ctx.make_node(\'Concat\', [partial_shape, const_neg_one], attr={\'axis\': 0}).output[0]\n        gather_shape = ctx.make_node(\'Concat\', [partial_shape, const_one], attr={\'axis\': 0}).output[0]\n\n        def normalize():\n            raw_input1 = ctx.make_node(\'Cast\', [node.input[1]], attr={\'to\': TensorProto.INT64}).output[0]\n            return ctx.make_node(\'Reshape\', [raw_input1, const_neg_one])\n\n        # get k0, k1 values. diags to be extracted\n        input1 = normalize()\n        k0 = ctx.make_node(\'ReduceMin\', [input1.output[0]]).output[0]\n        k1 = ctx.make_node(\'ReduceMax\', [input1.output[0]]).output[0]\n        k0_scalar = ctx.make_node(\'Squeeze\', [k0]).output[0]\n        k1_scalar = ctx.make_node(\'Squeeze\', [k1]).output[0]\n        m_padded = ctx.make_node(\'Pad\', [m, const_pad_vals, node.input[2]])\n\n        # starting indexes for super diagonals\n        xstart_0 = ctx.make_node(\'Cast\', [k0_scalar], attr={\'to\': TensorProto.FLOAT})\n        xstart_1 = ctx.make_node(\'Max\', [const_zero_float, xstart_0.output[0]])\n        xstart_2 = ctx.make_node(\'Cast\', [xstart_1.output[0]], attr={\'to\': TensorProto.INT64})\n        xstart_3 = ctx.make_node(\'Add\', [xstart_2.output[0], const_neg_one_scalar])\n        xstart_4 = ctx.make_node(\'Range\', [k1_scalar, xstart_3.output[0], const_neg_one_scalar])\n        xstart = ctx.make_node(\'Reshape\', [xstart_4.output[0], const_t])\n\n        # starting indexes for sub diagonals\n        ystart_0 = ctx.make_node(\'Cast\', [k1_scalar], attr={\'to\': TensorProto.FLOAT})\n        ystart_1 = ctx.make_node(\'Min\', [const_neg_one_float, ystart_0.output[0]])\n        ystart_2 = ctx.make_node(\'Cast\', [ystart_1.output[0]], attr={\'to\': TensorProto.INT64})\n        ystart_3 = ctx.make_node(\'Add\', [k0_scalar, const_neg_one_scalar])\n        ystart_4 = ctx.make_node(\'Range\', [ystart_2.output[0], ystart_3.output[0], const_neg_one_scalar])\n        ystart = ctx.make_node(\'Reshape\', [ystart_4.output[0], const_t])\n\n        xmax_0 = ctx.make_node(\'Mul\', [xstart.output[0], xlenp])\n        xmax = ctx.make_node(\'Sub\', [xmax, xmax_0.output[0]])\n        xmax_float = ctx.make_node(\'Cast\', [xmax.output[0]], attr={\'to\': TensorProto.FLOAT})\n\n        # lengths of super/sub diags to extract\n        xsize_0 = ctx.make_node(\'Sub\', [xlen, xstart.output[0]])\n        xsize_1 = ctx.make_node(\'Cast\', [xsize_0.output[0]], attr={\'to\': TensorProto.FLOAT})\n        xsize_2 = ctx.make_node(\'Min\', [xsize_1.output[0], minxy_float])\n        xsize = ctx.make_node(\'Cast\', [xsize_2.output[0]], attr={\'to\': TensorProto.INT64})\n        ysize_0 = ctx.make_node(\'Add\', [ylen, ystart.output[0]])\n        ysize_1 = ctx.make_node(\'Cast\', [ysize_0.output[0]], attr={\'to\': TensorProto.FLOAT})\n        ysize_2 = ctx.make_node(\'Min\', [ysize_1.output[0], minxy_float])\n        ysize = ctx.make_node(\'Cast\', [ysize_2.output[0]], attr={\'to\': TensorProto.INT64})\n        diagsize = ctx.make_node(\'Concat\', [xsize.output[0], ysize.output[0]], attr={\'axis\': 0})\n        maxsize = ctx.make_node(\'ReduceMax\', [diagsize.output[0]], attr={\'keep_dims\': 0})\n        maxsize_0 = ctx.make_node(\'Reshape\', [maxsize.output[0], const_neg_one])\n        maxsize_scalar = ctx.make_node(\'Squeeze\', [maxsize.output[0]])\n\n        diagdistances_0 = ctx.make_node(\'Range\', [const_zero_scalar, maxsize_scalar.output[0], const_one_scalar])\n        diagdistances = ctx.make_node(\'Mul\', [diagdistances_0.output[0], stride])\n\n        def right_align(sizes, indices, starts, maxval):\n            op1 = ctx.make_node(\'Sub\', [maxsize.output[0], sizes.output[0]])\n            op2 = ctx.make_node(\'Mul\', [op1.output[0], stride])\n            op3 = ctx.make_node(\'Sub\', [indices.output[0], op2.output[0]])\n            op4 = ctx.make_node(\'Less\', [op3.output[0], starts.output[0]])\n            op5 = ctx.make_node(\'Where\', [op4.output[0], maxval, op3.output[0]])\n            return op5\n\n        # xdiags, ydiags contain indices of diagonal elements\n        xdiags_0 = ctx.make_node(\'Add\', [xstart.output[0], diagdistances.output[0]])\n        xdiags_1 = ctx.make_node(\'Cast\', [xdiags_0.output[0]], attr={\'to\': TensorProto.FLOAT})\n        if xalign == \'RIGHT\':\n            xdiags = right_align(xsize, xdiags_0, xstart, ymax)\n        else:\n            xdiags_2 = ctx.make_node(\'Min\', [xdiags_1.output[0], xmax_float.output[0]])\n            xdiags = ctx.make_node(\'Cast\', [xdiags_2.output[0]], attr={\'to\': TensorProto.INT64})\n\n        ydiags_0_ = ctx.make_node(\'Abs\', [ystart.output[0]])\n        ydiags_1 = ctx.make_node(\'Mul\', [ydiags_0_.output[0], xlenp])\n        ydiags_2 = ctx.make_node(\'Add\', [ydiags_1.output[0], diagdistances.output[0]])\n        ydiags_3 = ctx.make_node(\'Cast\', [ydiags_2.output[0]], attr={\'to\': TensorProto.FLOAT})\n        if yalign == \'RIGHT\':\n            ydiags = right_align(ysize, ydiags_2, ydiags_1, ymax)\n        else:\n            ydiags_4 = ctx.make_node(\'Min\', [ydiags_3.output[0], ymax_float])\n            ydiags = ctx.make_node(\'Cast\', [ydiags_4.output[0]], attr={\'to\': TensorProto.INT64})\n\n        # flatten last dimension of matrix\n        m2 = ctx.make_node(\'Reshape\', [m_padded.output[0], m2_shape])\n\n        diags_0 = ctx.make_node(\'Concat\', [xdiags.output[0], ydiags.output[0]], attr={\'axis\': 0})\n        diags_1 = ctx.make_node(\'Reshape\', [diags_0.output[0], const_neg_one])\n        diags_2 = ctx.make_node(\'Expand\', [diags_1.output[0], gather_shape])\n        diags = ctx.make_node(\'GatherElements\', [m2.output[0], diags_2.output[0]], attr={\'axis\': -1})\n\n        def compute_out_shape(k0_k1_same=False):\n            g = ctx.create_new_graph_with_same_config()\n            g.parent_graph = ctx\n            if k0_k1_same:\n                dims = [partial_shape, maxsize_0.output[0]]\n            else:\n                dims = [partial_shape, const_neg_one, maxsize_0.output[0]]\n            outshape = g.make_node(\'Concat\', dims, attr={\'axis\': 0})\n            g.add_graph_output(outshape.output[0], TensorProto.INT64, [-1])\n            return g\n\n        # if k0=k1, rank of output matrix is 1 less than usual\n        # hence, need \'If\' to compute right output matrix shape\n        k0_k1_same = ctx.make_node(\'Equal\', [k1, k0])\n        if_node = ctx.make_node(\'If\', [k0_k1_same.output[0]])\n        if_node.set_body_graph_as_attr(\'then_branch\', compute_out_shape(True))\n        if_node.set_body_graph_as_attr(\'else_branch\', compute_out_shape(False))\n\n        shapes = ctx.get_shape(node.output[0])\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(\'Reshape\', [diags.output[0], if_node.output[0]], name=node.name, outputs=node.output,\n                      shapes=[shapes], dtypes=dtypes)\n\n        for consumer in ctx.find_output_consumers(node.output[0]):\n            if consumer.type == \'Identity\':\n                ctx.set_shape(consumer.output[0], shapes)\n\n\n@tf_op([""MatrixDiag"", ""MatrixDiagV2"", ""MatrixDiagV3""])\nclass MatrixDiag:\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        # Assemble MatrixDiagV3 by ReverseSequence\n        argc = len(node.input)\n\n        def mkconsts(values):\n            return [ctx.make_const(utils.make_name(\'const\'), \\\n                                   np.array(value).astype(np.int64)).output[0] for value in values]\n\n        minus_two, minus_one, zeo, one, two = mkconsts([[-2], [-1], [0], [1], [2]])\n\n        def mknode(op, args, **kwargs):\n            return ctx.make_node(op, args, **kwargs).output[0]\n\n        def mknode2(g, op, args, **kwargs):\n            return g.make_node(op, args, **kwargs).output[0]\n\n        def normalize(name):\n            # normalize arguments\n            casted = mknode(""Cast"", [name], attr={\'to\': TensorProto.INT64})\n            reshaped = mknode(""Reshape"", [casted, minus_one])\n            return reshaped\n\n        def cast(name):\n            return mknode(""Cast"", [name], attr={""to"": ctx.get_dtype(node.input[0])})\n\n        def processdiag():\n            # unsqueeze diag if necessary\n            diag = node.input[0]\n            shape = ctx.get_shape(diag)\n            if len(shape) == 1:\n                diag = mknode(""Unsqueeze"", [diag], attr={""axes"": [0]})\n                shape = [1] + shape\n                ctx.set_shape(diag, shape)\n\n            diag_shape = mknode(""Shape"", [diag])\n            diag_depth = mknode(""Slice"", [diag_shape, minus_two, minus_one])\n            k = normalize(node.input[1]) if argc > 1 else zeo\n            k_min, k_max = mknode(""ReduceMin"", [k]), mknode(""ReduceMax"", [k])\n            k_max_nxt = mknode(""Add"", [k_max, one])\n            k_depth = mknode(""Sub"", [k_max_nxt, k_min])\n            equal = mknode(""Equal"", [k_depth, diag_depth])\n\n            def id_diag():\n                g = ctx.create_new_graph_with_same_config()\n                g.parent_graph = ctx\n                idt = mknode2(g, ""Identity"", [diag])\n                g.add_graph_output(idt, ctx.get_dtype(node.input[0]), ctx.get_shape(diag))\n                return g\n\n            def ex_diag():\n                g = ctx.create_new_graph_with_same_config()\n                g.parent_graph = ctx\n                ex = mknode2(g, ""Unsqueeze"", [diag], attr={""axes"": [-2]})\n                rank = len(ctx.get_shape(diag)) + 1\n                g.add_graph_output(ex, ctx.get_dtype(node.input[0]), [-1] * rank)\n                return g\n\n            expand_diag = ctx.make_node(""If"", [equal])\n            expand_diag.set_body_graph_as_attr(""then_branch"", id_diag())\n            expand_diag.set_body_graph_as_attr(""else_branch"", ex_diag())\n            return expand_diag.output[0], k, k_min, k_max, k_max_nxt\n\n        def squeeze(name):\n            return ctx.make_node(""Squeeze"", [name], attr={""axis"": -1}).output[0]\n\n        # gather inputs\n        diag, k, k_min, k_max, k_max_nxt = processdiag()\n        row, col, pad, align = normalize(node.input[2]) if argc > 2 else minus_one, \\\n                               normalize(node.input[3]) if argc > 3 else minus_one, \\\n                               node.input[4] if argc > 4 else cast(zeo), \\\n                               node.get_attr_str(""align"") if ""align"" in node.attr else ""LEFT_LEFT""\n\n        diag_shape = mknode(""Shape"", [diag])\n        diag_rank = mknode(""Shape"", [diag_shape])\n        head_shape = mknode(""Slice"", [diag_shape, zeo, minus_two])\n        tail_shape = mknode(""Slice"", [diag_shape, minus_two, diag_rank])\n        diag_width = mknode(""Slice"", [diag_shape, minus_one, diag_rank])\n        diag_depth = mknode(""Slice"", [diag_shape, minus_two, minus_one])\n        k_range = mknode(""Range"", [squeeze(k_min), squeeze(k_max_nxt), squeeze(one)])\n        abs_k_range = mknode(""Abs"", [k_range])\n        min_k2zeo = mknode(""ReduceMin"", [abs_k_range])\n        max_diag_len = mknode(""Add"", [min_k2zeo, diag_width])\n\n        def outrowcol():\n            # get output matrix shape\n            row_set = mknode(""Greater"", [row, zeo])\n            col_set = mknode(""Greater"", [col, zeo])\n\n            def rowset():\n                # if row is set\n                g = ctx.create_new_graph_with_same_config()\n                g.parent_graph = ctx\n\n                def rowsetcolset():\n                    # if col is set\n                    gg = g.create_new_graph_with_same_config()\n                    id_row = mknode2(gg, ""Identity"", [row])\n                    id_col = mknode2(gg, ""Identity"", [col])\n                    shape = mknode2(gg, ""Concat"", [id_row, id_col], attr={""axis"": -1})\n                    gg.parent_graph = g\n                    gg.add_graph_output(shape, TensorProto.INT64, [-1])\n                    return gg\n\n                def rowsetcolnotset():\n                    # if col is not set\n                    gg = g.create_new_graph_with_same_config()\n                    gg.parent_graph = g\n                    id_row = mknode2(gg, ""Identity"", [row])\n                    id_diag_width = mknode2(gg, ""Identity"", [diag_width])\n                    shape = mknode2(gg, ""Concat"", [id_row, id_diag_width], attr={""axis"": -1})\n                    gg.add_graph_output(shape, TensorProto.INT64, [-1])\n                    return gg\n\n                if_col_set = g.make_node(""If"", [col_set])\n                if_col_set.set_body_graph_as_attr(""then_branch"", rowsetcolset())\n                if_col_set.set_body_graph_as_attr(""else_branch"", rowsetcolnotset())\n                g.add_graph_output(if_col_set.output[0], TensorProto.INT64, [-1])\n                return g\n\n            def rownotset():\n                # if row is not set\n                g = ctx.create_new_graph_with_same_config()\n                g.parent_graph = ctx\n\n                def rownotsetcolset():\n                    # if col is set\n                    gg = g.create_new_graph_with_same_config()\n                    gg.parent_graph = g\n                    id_diag_width = gg.make_node(""Identity"", [diag_width]).output[0]\n                    id_col = gg.make_node(""Identity"", [col]).output[0]\n                    shape = gg.make_node(""Concat"", [id_diag_width, id_col], attr={""axis"": -1}).output[0]\n                    gg.add_graph_output(shape, TensorProto.INT64, [-1])\n                    return gg\n\n                def rownotsetcolnotset():\n                    # if col is not set\n                    gg = g.create_new_graph_with_same_config()\n                    gg.parent_graph = g\n                    id_max_diag_len = gg.make_node(""Identity"", [max_diag_len]).output[0]\n                    shape = gg.make_node(""Concat"", [id_max_diag_len, id_max_diag_len], attr={""axis"": -1}).output[0]\n                    gg.add_graph_output(shape, TensorProto.INT64, [-1])\n                    return gg\n\n                if_col_set = g.make_node(""If"", [col_set])\n                if_col_set.set_body_graph_as_attr(""then_branch"", rownotsetcolset())\n                if_col_set.set_body_graph_as_attr(""else_branch"", rownotsetcolnotset())\n                g.add_graph_output(if_col_set.output[0], TensorProto.INT64, [-1])\n                return g\n\n            if_row_set = ctx.make_node(""If"", [row_set])\n            if_row_set.set_body_graph_as_attr(""then_branch"", rowset())\n            if_row_set.set_body_graph_as_attr(""else_branch"", rownotset())\n            return if_row_set.output[0]\n\n        out_shape = outrowcol()\n        out_row = mknode(""Slice"", [out_shape, zeo, one])\n        out_col = mknode(""Slice"", [out_shape, one, two])\n        k_btm = mknode(""Sub"", [one, out_row]) # lowest possible k\n\n        def getklens():\n            # return diag len of all ks\n            rwcl_min = mknode(""Min"", [out_row, out_col])\n            rwcl_gap = mknode(""Sub"", [out_row, out_col])\n            absl_gap = mknode(""Abs"", [rwcl_gap])\n            left_btm = mknode(""Range"", [squeeze(one), squeeze(rwcl_min), squeeze(one)])\n            riht_top = mknode(""Abs"", [mknode(""Sub"", [left_btm, rwcl_min])])\n            klen_mid = mknode(""Expand"", [rwcl_min, mknode(""Add"", [absl_gap, one])])\n            return mknode(""Concat"", [left_btm, klen_mid, riht_top], attr={""axis"": -1})\n\n        k_lens = getklens()\n\n        def reverseseq(args):\n            return mknode(""ReverseSequence"", args, attr={""batch_axis"": 0, ""time_axis"": 1})\n\n        def reverse1d(name):\n            # reverse an array\n            shape = mknode(""Shape"", [name])\n            temp_shape = mknode(""Concat"", [minus_one, shape], attr={""axis"": -1})\n            reshaped = mknode(""Reshape"", [name, temp_shape])\n            rev = reverseseq([reshaped, shape])\n            return mknode(""Reshape"", [rev, shape])\n\n        def sortdiag():\n            # sort diag to ""LEFT_RIGHT"" so each col form a line of the out matrix\n            k_sup_stt = mknode(""Sub"", [mknode(""Max"", [zeo, k_min]), k_btm])\n            k_sup_end = mknode(""Sub"", [k_max_nxt, k_btm])\n            k_sup_len = mknode(""Max"", [zeo, mknode(""Sub"", [k_sup_end, k_sup_stt])])\n            k_sub_stt = mknode(""Sub"", [k_min, k_btm])\n            k_sub_end = mknode(""Sub"", [mknode(""Min"", [zeo, k_max_nxt]), k_btm])\n            k_sub_len = mknode(""Max"", [zeo, mknode(""Sub"", [k_sub_end, k_sub_stt])])\n            sup_k_lens = mknode(""Slice"", [k_lens, k_sup_stt, k_sup_end])\n            sub_k_lens = mknode(""Slice"", [k_lens, k_sub_stt, k_sub_end])\n            all_k_lens = mknode(""Concat"", [sub_k_lens, sup_k_lens], attr={""axis"": -1})\n            max_k_len = mknode(""ReduceMax"", [all_k_lens])\n            top_k_len = mknode(""Slice"", [all_k_lens, minus_one, diag_depth])\n            btm_k_len = mknode(""Slice"", [all_k_lens, zeo, one])\n            diag_rev_shap = mknode(""Concat"", [minus_one, diag_width], attr={""axis"": -1})\n            reshaped_diag = mknode(""Reshape"", [diag, diag_rev_shap])\n            rev_shape = mknode(""Slice"", [diag_shape, zeo, minus_one])\n\n            sup_rev_len_1 = mknode(""Expand"", [one, k_sup_len]) if align.startswith(""LEFT"") else mknode(""Expand"",\n                                                                                                       [diag_width,\n                                                                                                        k_sup_len])\n            sub_rev_len_1 = mknode(""Expand"", [one, k_sub_len]) if align.endswith(""RIGHT"") else sub_k_lens\n            cnt_rev_len_1 = mknode(""Concat"", [sub_rev_len_1, sup_rev_len_1], attr={""axis"": -1})\n            exp_rev_len_1 = mknode(""Expand"", [reverse1d(cnt_rev_len_1), rev_shape])\n\n            sup_rev_len_2 = mknode(""Expand"", [one, k_sup_len]) if align.startswith(""LEFT"") else sup_k_lens\n            sub_rev_len_2 = mknode(""Expand"", [one, k_sub_len]) if align.endswith(""RIGHT"") else mknode(""Expand"",\n                                                                                                      [diag_width,\n                                                                                                       k_sub_len])\n            cnt_rev_len_2 = mknode(""Concat"", [sub_rev_len_2, sup_rev_len_2], attr={""axis"": -1})\n            exp_rev_len_2 = mknode(""Expand"", [reverse1d(cnt_rev_len_2), rev_shape])\n\n            reversed_diag_1 = reverseseq([reshaped_diag, mknode(""Reshape"", [exp_rev_len_1, minus_one])])\n            reversed_diag_2 = reverseseq([reversed_diag_1, mknode(""Reshape"", [exp_rev_len_2, minus_one])])\n\n            return mknode(""Reshape"", [reversed_diag_2, diag_shape]), \\\n                   mknode(""Sub"", [max_k_len, top_k_len]), \\\n                   mknode(""Sub"", [max_k_len, btm_k_len])\n\n        sorted_diag, top_pad, btm_pad = sortdiag()\n\n        def trandiag():\n            # transpose last two dim of diag\n            temp_shape = mknode(""Concat"", [minus_one, tail_shape], attr={""axis"": -1})\n            reshaped = mknode(""Reshape"", [sorted_diag, temp_shape])\n            transposed = mknode(""Transpose"", [reshaped], attr={""perm"": [0, 2, 1]})\n            out_shape = mknode(""Concat"", [head_shape, reverse1d(tail_shape)], attr={""axis"": -1})\n            return mknode(""Reshape"", [transposed, out_shape])\n\n        tran_diag = trandiag()\n\n        def relu1(name):\n            # all return values >= 1\n            minusd = mknode(""Sub"", [name, one])\n            casted = mknode(""Cast"", [minusd], attr={""to"": TensorProto.FLOAT})\n            relued = mknode(""Relu"", [casted])\n            casted = mknode(""Cast"", [relued], attr={""to"": TensorProto.INT64})\n            return mknode(""Add"", [casted, one])\n\n        def makediagonal():\n            # padding with required value and move lines so they form diagonals\n            shape = mknode(""Shape"", [tran_diag])\n            rank = mknode(""Shape"", [shape])\n            width = mknode(""Slice"", [shape, minus_one, rank])\n            temp_shape = mknode(""Concat"", [minus_one, width], attr={""axis"": -1})\n            reshaped = mknode(""Reshape"", [tran_diag, temp_shape])\n            left_pad, riht_pad = top_pad, mknode(""Add"", [btm_pad, diag_width])\n            full_pad = mknode(""Concat"", [zeo, left_pad, zeo, riht_pad], attr={""axis"": -1})\n            diag_pad = mknode(""Pad"", [reshaped, full_pad, pad])\n            diag_pad_shape = mknode(""Shape"", [diag_pad])\n            diag_pad_width = mknode(""Slice"", [diag_pad_shape, one, two])\n            exp_shape = mknode(""Concat"", [head_shape, diag_width], attr={""axis"": -1})\n\n            def padleft():\n                # set pads from left\n                fm = mknode(""Add"", [left_pad, left_pad])\n                to = mknode(""Sub"", [fm, diag_width])\n                rg = reverse1d(relu1(mknode(""Range"", [squeeze(fm), squeeze(to), squeeze(minus_one)])))\n                expanded_range = mknode(""Expand"", [rg, exp_shape])\n                reshaped_range = mknode(""Reshape"", [expanded_range, minus_one])\n                pad_left = mknode(""ReverseSequence"", [diag_pad, reshaped_range], attr={""batch_axis"": 0, ""time_axis"": 1})\n                return mknode(""Slice"", [pad_left, left_pad, diag_pad_width, one])\n\n            pad_left = padleft()\n\n            def padright():\n                # set pads from right\n                pad_left_shape = mknode(""Shape"", [pad_left])\n                pad_left_depth = mknode(""Slice"", [pad_left_shape, zeo, one])\n                pad_left_width = mknode(""Slice"", [pad_left_shape, one, two])\n                pad_full_lenth = mknode(""Expand"", [pad_left_width, pad_left_depth])\n                rev = mknode(""ReverseSequence"", [pad_left, pad_full_lenth], attr={""batch_axis"": 0, ""time_axis"": 1})\n                fm = mknode(""Add"", [riht_pad, btm_pad])\n                to = mknode(""Sub"", [fm, diag_width])\n                rg = mknode(""Range"", [squeeze(fm), squeeze(to), squeeze(minus_one)])\n                expanded_range = mknode(""Expand"", [rg, exp_shape])\n                reshaped_range = mknode(""Reshape"", [expanded_range, minus_one])\n                raw_pad_right = mknode(""ReverseSequence"", [rev, reshaped_range],\n                                       attr={""batch_axis"": 0, ""time_axis"": 1})\n                shape = mknode(""Shape"", [raw_pad_right])\n                width = mknode(""Slice"", [shape, one, two])\n                sliced = mknode(""Slice"", [raw_pad_right, btm_pad, width, one])\n                all_width = mknode(""Expand"", [mknode(""Sub"", [width, btm_pad]), mknode(""Shape"", [reshaped_range])])\n                return mknode(""ReverseSequence"", [sliced, all_width], attr={""batch_axis"": 0, ""time_axis"": 1})\n\n            pad_right = padright()\n\n            def diagonize():\n                # move lines to right to form diagonals\n                fm = mknode(""Sub"", [diag_depth, btm_pad])\n                to = mknode(""Add"", [fm, diag_width])\n                rg = mknode(""Range"", [squeeze(fm), squeeze(to), squeeze(one)])\n                expanded_range = mknode(""Expand"", [rg, exp_shape])\n                reshaped_range = mknode(""Reshape"", [expanded_range, minus_one])\n                rev = mknode(""ReverseSequence"", [pad_right, reshaped_range],\n                             attr={""batch_axis"": 0, ""time_axis"": 1})\n                k_max_idx = mknode(""Sub"", [k_max, k_btm])\n                k_max_idx_nxt = mknode(""Add"", [k_max_idx, one])\n                k_max_len = mknode(""Slice"", [k_lens, k_max_idx, k_max_idx_nxt])\n                k_gap = mknode(""Sub"", [mknode(""Abs"", [k_max]), min_k2zeo])\n                width = mknode(""Add"", [k_max_len, k_gap])\n                return mknode(""Slice"", [rev, zeo, width, one]), width\n\n            diag, width = diagonize()\n            shape = mknode(""Concat"", [head_shape, diag_width, minus_one], attr={""axis"": -1})\n            return mknode(""Reshape"", [diag, shape]), diag_width, width\n\n        new_diag, new_depth, new_width = makediagonal()\n\n        def paddiag():\n            # pad to output shape\n            pad_row, pad_col = mknode(""Sub"", [out_row, new_depth]), mknode(""Sub"", [out_col, new_width])\n            pad_top = mknode(""Max"", [zeo, mknode(""Sub"", [zeo, k_max])])\n            pad_lft = mknode(""Max"", [zeo, mknode(""Sub"", [k_min, zeo])])\n            pad_btm = mknode(""Sub"", [pad_row, pad_top])\n            pad_rht = mknode(""Sub"", [pad_col, pad_lft])\n            pad_hlf = mknode(""Mul"", [zeo, head_shape])\n            pad_ful = mknode(""Concat"", [pad_hlf, pad_top, pad_lft, pad_hlf, pad_btm, pad_rht], attr={""axis"": -1})\n            return mknode(""Pad"", [new_diag, pad_ful, pad])\n\n        padded = paddiag()\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        ctx.make_node(""Identity"", [padded], name=node.name,\n                      outputs=node.output, shapes=shapes, dtypes=dtypes)\n\n\n@tf_op(""MatrixSetDiagV3"")\nclass MatrixSetDiagV3:\n    @classmethod\n    def version_12(cls, ctx, node, **kwargs):\n        # Assemble MatrixSetDiagV3 by MatrixDiagPartV3 and MatrixDiagV3\n        def mkconsts(values):\n            return [ctx.make_const(utils.make_name(\'const\'), \\\n                                   np.array(value).astype(np.int64)).output[0] for value in values]\n\n        minus_two, minus_one, zeo, one = mkconsts([[-2], [-1], [0], [1]])\n\n        def mknode(op, args, **kwargs):\n            return ctx.make_node(op, args, **kwargs).output[0]\n\n        def integer(name):\n            return mknode(""Cast"", [name], attr={""to"": TensorProto.INT64})\n\n        def cast(name):\n            return mknode(""Cast"", [name], attr={""to"": ctx.get_dtype(node.input[0])})\n\n        def normalize():\n            k = node.input[2]\n            casted = mknode(""Cast"", [k], attr={""to"": TensorProto.INT64})\n            return mknode(""Reshape"", [casted, minus_one])\n\n        x = node.input[0]\n        diag = node.input[1]\n        k = normalize()\n        attr = {""align"": node.get_attr_str(""align"")}\n\n        shape = mknode(""Shape"", [x])\n        rank = mknode(""Shape"", [shape])\n        row = mknode(""Slice"", [shape, minus_two, minus_one])\n        col = mknode(""Slice"", [shape, minus_one, rank])\n\n        # ones of x shape\n        zeos = mknode(""Mul"", [integer(x), zeo])\n        ones = mknode(""Add"", [zeos, one])\n\n        # make diag of 1s\n        ones_diag = ctx.make_node(""MatrixDiagPartV3"", [ones, k, zeo], attr)\n        MatrixDiagPartV2V3.version_11(ctx, ones_diag)\n        # MatrixDiagPartV2V3.version_12(ctx, ones_diag) # todo: fix exception\n\n        # make matrix of bool\n        ctx.set_dtype(ones_diag.output[0], TensorProto.INT64)\n        ones_matrix = ctx.make_node(""MatrixDiagV3"", [ones_diag.output[0], k, row, col, zeo], attr)\n        MatrixDiag.version_12(ctx, ones_matrix)\n        ones_bool = mknode(""Equal"", [ones_matrix.output[0], one])\n\n        # make matrix out of diag\n        diag_matrix = ctx.make_node(""MatrixDiagV3"", [diag, k, row, col, cast(zeo)], attr)\n        MatrixDiag.version_12(ctx, diag_matrix)\n\n        shapes = node.output_shapes\n        dtypes = node.output_dtypes\n        ctx.remove_node(node.name)\n        mknode(""Where"", [ones_bool, diag_matrix.output[0], x],\n               name=node.name, outputs=node.output, shapes=shapes, dtypes=dtypes)\n\n\n@tf_op(""BroadcastTo"")\nclass BroadcastTo:\n    @classmethod\n    def version_8(cls, ctx, node, **kwargs):\n        # broadcast by expanding\n        node.type = ""Expand""\n        ctx.insert_new_node_on_input(node, ""Cast"", node.input[1], to=TensorProto.INT64)\n'"
tf2onnx/onnx_opset/traditionalml.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntraditional ml\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n'"
tf2onnx/optimizer/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n""""""tf2onnx.optimizer module""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import OrderedDict\nimport copy\n\nfrom .const_fold_optimizer import ConstFoldOptimizer\nfrom .identity_optimizer import IdentityOptimizer\nfrom .merge_duplicated_nodes_optimizer import MergeDuplicatedNodesOptimizer\nfrom .transpose_optimizer import TransposeOptimizer\nfrom .loop_optimizer import LoopOptimizer\nfrom .back_to_back_optimizer import BackToBackOptimizer\nfrom .. import logging\n\n# optimizer sequence need to be considered carefully\n_optimizers = OrderedDict([\n    (""optimize_transpose"", TransposeOptimizer),\n    (""fold_constants"", ConstFoldOptimizer),\n    (""loop_optimizer"", LoopOptimizer),\n    # merge_duplication should be used after optimize_transpose\n    # for optimize_transpose may have some trans nodes that can be merge\n    (""merge_duplication"", MergeDuplicatedNodesOptimizer),\n    (""remove_identity"", IdentityOptimizer),\n    (""remove_back_to_back"", BackToBackOptimizer),\n])\n\n\ndef _get_optimizers():\n    return _optimizers\n\n\ndef optimize_graph(graph):\n    """""" Optimize graph, return optimized graph. No throw. """"""\n    logger = logging.getLogger(__name__)\n    logger.info(""Optimizing ONNX model"")\n\n    before = graph.dump_node_statistics()\n    opts = _get_optimizers()\n    continue_flag = True\n    while continue_flag:\n        continue_flag = False\n        for name, factory in opts.items():\n            try:\n                logger.verbose(""Apply %s"", name)\n                current = copy.deepcopy(graph)\n                opt = factory()\n                graph = opt.optimize(current) or graph\n                continue_flag = continue_flag or opt.graph_been_opt\n\n            except Exception:  # pylint: disable=broad-except\n                # if current optimizer fails, continue with other optimizers\n                logger.warning(""Failed to apply %s"", name, exc_info=1)\n\n    try:\n        graph.topological_sort(graph.get_nodes())\n    except Exception:  # pylint: disable=broad-except\n        logger.warning(""Failed topological_sort"", exc_info=1)\n\n    after = graph.dump_node_statistics()\n    diff = copy.deepcopy(after)\n    diff.subtract(before)\n    diff = [""{} {} ({}->{})"".format(k, str(v) if v < 0 else \'+\' + str(v), before.get(k, 0), after.get(k, 0))\n            for k, v in sorted(diff.items()) if v != 0]\n    logger.info(""After optimization: %s"", \', \'.join(diff) if diff else ""no change"")\n\n    return graph\n'"
tf2onnx/optimizer/back_to_back_optimizer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Back_To_Back Optimizer.\n   Collapse consecutive nodes into 1 node if possible.\n""""""\n\nfrom __future__ import unicode_literals\n\nimport numpy as np\nfrom tf2onnx.utils import ONNX_DTYPE_NAMES  # lgtm[py/unsafe-cyclic-import]\nfrom .optimizer_base import GraphOptimizerBase  # lgtm[py/unsafe-cyclic-import]\n\n# pylint: disable=logging-not-lazy,unused-argument,missing-docstring,unused-variable,arguments-differ\n\n_func_map = {}\n\n\ndef _register_func(op_type):\n    def _internal_fun(func):\n        _func_map[op_type] = func\n        return func\n\n    return _internal_fun\n\n\nclass BackToBackOptimizer(GraphOptimizerBase):\n    """"""Remove back-to-back nodes e.g. \'Cast\'\n    """"""\n\n    def __init__(self):  # pylint: disable=useless-super-delegation\n        super(BackToBackOptimizer, self).__init__()\n\n    def _optimize(self, graph):\n        return self._apply_optimization(graph, self._optimize_at_current_graph_level)\n\n    def _optimize_at_current_graph_level(self, g):\n        for optype, handler in _func_map.items():\n            # candidate nodes for removal/optimization\n            nodes = [n for n in g.get_nodes() if n.type in optype]\n\n            # topological sort of candidates\n            # simplifying assumption for back-to-back-optimizer is\n            # the op_types have 1 input, 1 output, but multiple consumers\n            has_dependencies = set()\n            consumer_node_ids = {n.output[0]: [] for n in nodes}\n            for n in nodes:\n                if n.input[0] in consumer_node_ids:\n                    consumer_node_ids[n.input[0]].extend([n])\n                    has_dependencies.add(n.output[0])\n\n            # q = starting nodes with no dependencies\n            q = list(set(consumer_node_ids.keys()) - has_dependencies)\n            while q:\n                nodeid = q.pop(0)\n                node = g.get_node_by_output(nodeid, False)\n                consumer_nodes = consumer_node_ids[nodeid]\n\n                if len(consumer_nodes) > 0:\n                    all_consumers = g.find_output_consumers(node.output[0])\n                    if len(all_consumers) != len(consumer_nodes):\n                        # if first node is used elsewhere, skip\n                        continue\n                    if set(node.output) & set(g.outputs):\n                        # if this node is part of graph outputs, skip\n                        continue\n                    q2 = handler(g, node, consumer_nodes)\n                    # add more nodes which can now be processed\n                    q.extend(q2)\n        return g\n\n    @staticmethod\n    @_register_func(""Cast"")\n    def _optimize_cast(g, node, consumer_nodes):\n        """"""remove long chains of cast ops""""""\n        q2 = []\n        type1 = node.get_attr(\'to\').i\n        type1_name = ONNX_DTYPE_NAMES[type1] if type1 in ONNX_DTYPE_NAMES else \'\'\n\n        # if parent node is cast node, and same type, delete this one\n        pnode = node.inputs[0]\n        if pnode.type == \'Cast\':\n            type2 = pnode.get_attr(\'to\').i\n            if type1 == type2:\n                for node2 in consumer_nodes:\n                    node2.input[0] = node.input[0]\n                    q2.append(node2.output[0])\n                g.remove_node(node.name)\n                return q2\n\n        # otherwise, check consumer cast nodes for a target type\n        # that contains more information than current type\n        can_reduce = True\n        for node2 in consumer_nodes:\n            type2 = node2.get_attr(\'to\').i\n            type2_name = ONNX_DTYPE_NAMES[type2] if type2 in ONNX_DTYPE_NAMES else \'\'\n\n            if \'float\' in type1_name or type1_name == \'double\':\n                # high information type. ok to eliminate\n                pass\n            elif \'int\' in type1_name:\n                # int* and uint* are mix of high and low information.\n                # for safety, keep the current node, unless type2 is bool,\n                # in which case it\'s ok to remove node\n                if type1 != type2 and type2_name != \'bool\':\n                    can_reduce = False\n            elif type1_name == \'bool\':\n                # bool is low information, so don\'t eliminate\n                if type1 != type2:\n                    can_reduce = False\n            elif type1_name == \'string\':\n                # can always remove string\n                pass\n            else:\n                # some odd type, keep node\n                can_reduce = False\n            q2.append(node2.output[0])\n\n        if can_reduce:\n            for node2 in consumer_nodes:\n                node2.input[0] = node.input[0]\n            g.remove_node(node.name)\n        return q2\n\n    @staticmethod\n    @_register_func(""Transpose"")\n    def _optimize_transpose(g, node, consumer_nodes):\n        """"""remove long chains of transpose ops""""""\n        t1 = list(node.get_attr(\'perm\').ints)\n        q2 = []\n        for node2 in consumer_nodes:\n            node2.input[0] = node.input[0]\n            t2 = list(node2.get_attr(\'perm\').ints)\n            new_perm = [t1[i] for i in t2]\n            # check if node2 can be removed. otherwise only update\n            if new_perm == list(range(len(t2))):\n                # both nodes can be deleted\n                shape = g.get_shape(node2.output[0])\n                dtype = g.get_dtype(node2.output[0])\n                node2_consumers = g.find_output_consumers(node2.output[0])\n                g.replace_all_inputs(node2_consumers, node2.output[0], node.input[0])\n                g.remove_node(node2.name)\n                if set(node2.output) & set(g.outputs):\n                    g.make_node(""Identity"", [node.input[0]],\n                                outputs=node2.output, shapes=[shape], dtypes=[dtype])\n            else:\n                node2.set_attr(\'perm\', [t1[i] for i in t2])\n                q2.append(node2.output[0])\n        g.remove_node(node.name)\n        return q2\n\n    @staticmethod\n    @_register_func((\'Squeeze\', \'Unsqueeze\'))\n    def _optimize_squeeze_unsqueeze(g, node, consumer_nodes):\n        """"""remove pairs of squeeze-unsqueeze nodes""""""\n        if node.type != \'Squeeze\' or len(consumer_nodes) != 1:\n            # no need to return any value, since not removing long chain of nodes\n            return []\n\n        node2 = consumer_nodes[0]\n        if node2.type != \'Unsqueeze\':\n            return []\n\n        axis1 = node.get_attr(\'axes\').ints\n        axis2 = node2.get_attr(\'axes\').ints\n\n        # if squeeze followed by unsqueeze is on diff axes, skip\n        if axis1 != axis2:\n            return []\n\n        # if unsqueeze output is graph output, skip\n        if set(node2.output) & set(g.outputs):\n            return []\n\n        node2_consumers = g.find_output_consumers(node2.output[0])\n        g.replace_all_inputs(node2_consumers, node2.output[0], node.input[0])\n        g.remove_node(node.name)\n        g.remove_node(node2.name)\n        return []\n\n    @staticmethod\n    @_register_func((\'Conv\', \'BatchNormalization\'))\n    def _optimize_conv_batchnorm_fusion(g, node, consumer_nodes):\n        """"""fuse conv and batchnorm""""""\n        if node.type != \'Conv\' or len(consumer_nodes) != 1:\n            # can only fuse 1 conv + batchnorm\n            return []\n\n        node2 = consumer_nodes[0]\n        if node2.type != \'BatchNormalization\':\n            return []\n\n        # if batchnorm is a graph output, skip\n        if set(node2.output) & set(g.outputs):\n            return []\n\n        if not node.inputs[1].is_const():\n            return []\n        weights = node.inputs[1].get_tensor_value(as_list=False)\n        # if not 4D, NCHW skip\n        if len(weights.shape) != 4:\n            return []\n\n        bias = 0\n        # optional bias value\n        if len(node.inputs) > 2:\n            if not node.inputs[2].is_const():\n                return []\n            bias = node.inputs[2].get_tensor_value(as_list=False)\n\n        # scale, offset, mean, var be const, otherwise skip\n        if False in [node2.inputs[i].is_const() for i in [1, 2, 3, 4]]:\n            return []\n\n        # if bn outputs used elsewhere, cannot fuse\n        for i in range(1, len(node2.output)):\n            if g.find_output_consumers(node2.output[i]):\n                return []\n\n        weights = weights.transpose(2, 3, 1, 0)\n        scale = node2.inputs[1].get_tensor_value(as_list=False)\n        offset = node2.inputs[2].get_tensor_value(as_list=False)\n        mean = node2.inputs[3].get_tensor_value(as_list=False)\n        var = node2.inputs[4].get_tensor_value(as_list=False)\n        epsilon = node2.get_attr(\'epsilon\').f\n\n        scale_new = scale / np.sqrt(var + epsilon)\n        weights_new = weights * scale_new\n        weights_new = weights_new.transpose(3, 2, 0, 1)\n        bias_new = (bias - mean) * scale_new + offset\n        bias_new_const = g.make_const(node.name + \'_bias_fused_bn\', bias_new)\n        weights_new_const = g.make_const(node.name + \'_weights_fused_bn\', weights_new)\n        node.input = [node.input[0], weights_new_const.output[0], bias_new_const.output[0]]\n\n        # fuse conv and bn, delete bn\n        node2_output = node2.output[:1]\n        node2_shape = g.get_shape(node2.output[0])\n        node2_dtype = g.get_dtype(node2.output[0])\n        g.remove_node(node2.name)\n        node.output = node2_output\n        g.set_shape(node2_output[0], node2_shape)\n        g.set_dtype(node2_output[0], node2_dtype)\n        return []\n'"
tf2onnx/optimizer/const_fold_optimizer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""const fold Optimizer.\n   if op\'s inputs are all const then do op computation when building the graph to improve performance\n   for example, input of transpose node is const then we can do transpose statically instead of at runtime\n""""""\n\nfrom .. import utils\nfrom .optimizer_base import GraphOptimizerBase\n\n# pylint: disable=logging-not-lazy,unused-argument,missing-docstring\n\n# key is op_type, value is the function to compute outputs\n# the schema of function is: inputs are(node, graph), output is a list of constant values.\n_func_map = {}\n\n\ndef _register_func(op_type):\n    def _internal_fun(func):\n        _func_map[op_type] = func\n        return func\n\n    return _internal_fun\n\n\nclass ConstFoldOptimizer(GraphOptimizerBase):\n\n    def __init__(self):  # pylint: disable=useless-super-delegation\n        super(ConstFoldOptimizer, self).__init__()\n\n    def _optimize(self, graph):\n        return self._apply_optimization(graph, self._optimize_at_current_graph_level)\n\n    def _optimize_at_current_graph_level(self, graph):\n        graph_changed = True\n        while graph_changed:\n            graph_changed = False\n            ops = graph.get_nodes()\n            for op in ops:\n                if self._should_skip(op):\n                    continue\n                if self._fold_node(op, graph):\n                    graph_changed = True\n                    self.graph_been_opt = True\n        return graph\n\n    @staticmethod\n    def _should_skip(node):\n        # only support onnx official op for now, op in other domain is not supported for now\n        if not utils.is_onnx_domain(node.domain):\n            return True\n\n        if node.is_const() or node.is_graph_input():\n            return True\n\n        skip_type = [""Identity""]\n        if node.type in skip_type:\n            return True\n\n        return False\n\n    def _fold_node(self, node, graph):\n        """""" if node\'s input are all const and it\'s not graph\'s output then it can be fold.\n            if node can be fold True will be return indicating that graph is changed\n        """"""\n        if self._all_inputs_are_const(node.inputs) and not self._is_graph_output(node, graph):\n            process_func = _func_map.get(node.type, None)\n            if process_func:\n                const_outputs = process_func(node, graph)\n                self._replace_node_with_const(node, graph, const_outputs)\n                return True\n            self.logger.debug(""need to add function to fold op %s whose op_type is %s"", node.name, node.type)\n        return False\n\n    @staticmethod\n    def _all_inputs_are_const(nodes):\n        return all(node.is_const() for node in nodes if node)\n\n    @staticmethod\n    def _is_graph_output(node, graph):\n        node_out_set = set(node.output)\n        graph_out_set = set(graph.outputs)\n        return node_out_set.intersection(graph_out_set)\n\n    @staticmethod\n    def _replace_node_with_const(node, graph, vals):\n        utils.make_sure(len(node.output) == len(vals), ""length of node outputs and const vals should be same"")\n        for old_input, val in zip(node.output, vals):\n            const_node = graph.make_const(utils.make_name(""const_fold_opt""), val)\n            graph.set_dtype(const_node.output[0], utils.map_numpy_to_onnx_dtype(val.dtype))\n            graph.set_shape(const_node.output[0], val.shape)\n            graph.replace_all_inputs(graph.get_nodes(), old_input, const_node.output[0])\n        graph.remove_node(node.name)\n\n    @staticmethod\n    @_register_func(""Cast"")\n    def _fold_cast(node, graph):\n        const_val = node.inputs[0].get_tensor_value(as_list=False)\n        np_dtype = utils.ONNX_TO_NUMPY_DTYPE[node.get_attr(""to"").i]\n        const_val_after_cast = const_val.astype(np_dtype)\n        return [const_val_after_cast]\n\n    @staticmethod\n    @_register_func(""Transpose"")\n    def _fold_transpose(node, graph) -> list:\n        const_val = node.inputs[0].get_tensor_value(as_list=False)\n        perm_attr = node.get_attr(""perm"")\n        perm = perm_attr.ints if perm_attr else None\n        const_val_after_trans = const_val.transpose(perm)\n        return [const_val_after_trans]\n\n    @staticmethod\n    @_register_func(""Unsqueeze"")\n    def _fold_unsqueeze(node, graph):\n        """"""\n        numpy expand_dims only supports to unsqueeze one dim one time, so reshape is used to simplify the logic\n        """"""\n        const_val = node.inputs[0].get_tensor_value(as_list=False)\n        axes = list(node.get_attr(""axes"").ints)\n        utils.make_sure(all(axis >= 0 for axis in axes), ""onnx spec says it only supports positive axis"")\n        shape_in = const_val.shape\n        dims_out = len(shape_in) + len(axes)\n        # calculate the shape of output accroding to onnx Unsqueeze\'s spec\n        # https://github.com/onnx/onnx/blob/master/docs/Operators.md#Unsqueeze\n        shape_in = iter(shape_in)\n        shape_out = [None] * dims_out\n        for ind in axes:\n            shape_out[ind] = 1\n        for ind, val in enumerate(shape_out):\n            if val is None:\n                shape_out[ind] = next(shape_in)\n\n        const_val_after_unsqueeze = const_val.reshape(shape_out)\n        return [const_val_after_unsqueeze]\n'"
tf2onnx/optimizer/identity_optimizer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\n""""""Identity Optimizer.\r\n   Remove useless Identity node in graphs including subgraphs, but does not hurt model output names.\r\n""""""\r\n\r\nfrom __future__ import unicode_literals\r\n\r\nfrom .optimizer_base import GraphOptimizerBase\r\n\r\n\r\n# pylint: disable=logging-not-lazy,unused-argument,missing-docstring,unused-variable,arguments-differ\r\n\r\n\r\nclass IdentityOptimizer(GraphOptimizerBase):\r\n    """"""Identity Optimizer.""""""\r\n\r\n    def __init__(self):  # pylint: disable=useless-super-delegation\r\n        super(IdentityOptimizer, self).__init__()\r\n\r\n    def _optimize(self, graph):\r\n        return self._apply_optimization(graph, self._optimize_at_current_graph_level)\r\n\r\n    def _optimize_at_current_graph_level(self, g):\r\n        has_update = True\r\n        while has_update:\r\n            has_update = False\r\n            nodes = [n for n in g.get_nodes() if n.type == ""Identity""]\r\n            for n in nodes:\r\n                if n.graph is None:\r\n                    self.logger.debug(""node has been removed from this graph, skip"")\r\n                    continue\r\n\r\n                graph_outputs = set(n.output).intersection(g.outputs)\r\n                ret = False\r\n                if graph_outputs:\r\n                    ret = self._handle_graph_output_identity(g, n, graph_outputs)\r\n                else:\r\n                    ret = self._handle_non_graph_output_identity(g, n)\r\n                has_update = ret\r\n                if ret:\r\n                    self.graph_been_opt = True\r\n        return g\r\n\r\n    @staticmethod\r\n    def _handle_non_graph_output_identity(graph, identity):\r\n        old_name = identity.output[0]\r\n        new_name = identity.input[0]\r\n        graph.replace_all_inputs(graph.get_nodes(), old_name, new_name)\r\n        graph.remove_node(identity.name)\r\n        return True\r\n\r\n    def _handle_graph_output_identity(self, graph, identity, graph_outputs):\r\n        input_id = identity.input[0]\r\n        input_node = identity.inputs[0]\r\n\r\n        if input_node.graph != graph:\r\n            # If input node is in parent graph, we don\'t handle it now\r\n            self.logger.debug(""input node in parent graph, skip"")\r\n            return False\r\n\r\n        if input_node.is_graph_input():\r\n            # Identity between input and output should not be removed.\r\n            self.logger.debug(""skip identity between input and output"")\r\n            return False\r\n\r\n        output_id = identity.output[0]\r\n        output_shape = graph.get_shape(output_id)\r\n        output_dtype = graph.get_dtype(output_id)\r\n        if input_id in graph.outputs:\r\n            # input id already be graph output, so we cannot make that be another graph output.\r\n            # this Identity must be kept.\r\n            self.logger.debug(""identity input already be graph output"")\r\n            return False\r\n\r\n        graph.remove_node(identity.name)\r\n        new_output = [output_id if o == input_id else o for o in input_node.output]\r\n        input_node.output = new_output\r\n\r\n        graph.set_shape(output_id, output_shape)\r\n        graph.set_dtype(output_id, output_dtype)\r\n\r\n        graph.replace_all_inputs(graph.get_nodes(), input_id, output_id)\r\n        return True\r\n'"
tf2onnx/optimizer/loop_optimizer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Loop Optimizer.\n   some op in loop\'s body graph can be moved out to the loop\n""""""\n\nfrom tf2onnx.utils import make_name, make_sure\nfrom .optimizer_base import GraphOptimizerBase\n\n\n# pylint: disable=logging-not-lazy,unused-argument,missing-docstring,unused-variable,arguments-differ\n\n\nclass LoopOptimizer(GraphOptimizerBase):\n    """"""Loop Optimizer.""""""\n\n    # a lot of terms used here come from loop\'s onnx spec\n    # https://github.com/onnx/onnx/blob/master/docs/Operators.md#Loop\n    def __init__(self):  # pylint: disable=useless-super-delegation\n        super(LoopOptimizer, self).__init__()\n\n    def _optimize(self, graph):\n        return self._apply_optimization(graph, self._optimize_at_current_graph_level)\n\n    def _optimize_at_current_graph_level(self, g):\n        has_update = True\n        while has_update:\n            has_update = False\n            nodes = [n for n in g.get_nodes() if n.type == ""Loop""]\n            for n in nodes:\n                has_update_tmp = self._try_move_transpose_out_of_body_graph(n)\n                if has_update_tmp:\n                    has_update = True\n                    self.graph_been_opt = True\n        return g\n\n    @staticmethod\n    def consumer_nodes_num(graph, node):\n        make_sure(len(node.output) == 1, ""only consider node with only one output"")\n        res = len(graph.find_output_consumers(node.output[0]))\n        return res\n\n    def _try_move_transpose_out_of_body_graph(self, loop_node):\n        # output node of body graph can be loop-carried-dependent, if so it can\'t be move out of the body graph\n        # return True if moving some nodes successfully\n        # for now, we only consider moving transpose\n        body_graph = loop_node.get_body_graphs()[""body""]\n        parent_graph = loop_node.graph\n        scan_nodes_name_in_body, scan_node_in_parent = self._scan_outputs(loop_node)\n        scan_nodes = [body_graph.get_node_by_output(name) for name in scan_nodes_name_in_body]\n        graph_is_changed = False\n        for node, name_in_parent in zip(scan_nodes, scan_node_in_parent):\n            # 1 delete node in body graph if possible\n            # only consider two case: trans is output, or transpose > identity > output\n            need_process = False\n            if node.type == ""Transpose"" and self.consumer_nodes_num(body_graph, node) <= 1:\n                trans = node\n                new_output = node.input[0]\n                body_graph.remove_node(node.name)\n                need_process = True\n            elif node.type == ""Identity"" and node.inputs[0].type == ""Transpose"" \\\n                    and self.consumer_nodes_num(body_graph, node) <= 1\\\n                    and self.consumer_nodes_num(body_graph, node.inputs[0]) <= 1:\n                trans = node.inputs[0]\n                new_output = node.inputs[0].input[0]\n                body_graph.remove_node(node.inputs[0].name)\n                body_graph.remove_node(node.name)\n                need_process = True\n\n            if need_process:\n                # 2 correct body graph\'s output\n                body_outputs = body_graph.outputs\n                body_outputs[body_outputs.index(node.output[0])] = new_output\n                # 3 insert new node in parent graph\n                ori_perm = list(trans.get_attr(""perm"").ints)\n                new_perm = [0] + [i + 1 for i in ori_perm]  # body output\'s rank is m > rank of loop\'s output is m+1\n                name = make_name(""trans_moved_from_loop_body"")\n                _ = parent_graph.insert_new_node_on_output(""Transpose"", name_in_parent, name, perm=new_perm)\n                graph_is_changed = True\n\n        return graph_is_changed\n\n    @classmethod\n    def _scan_outputs(cls, loop):\n        # loop has 2+N inputs; loop has N+K outputs;\n        # loop\'s body graph has 1+N+K outputs\n        loop_carried = len(loop.input) - 2\n        body_graph = loop.get_body_graphs()[""body""]\n        return body_graph.outputs[loop_carried + 1:], loop.output[loop_carried:]\n'"
tf2onnx/optimizer/merge_duplicated_nodes_optimizer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Merge Duplicated Nodes Optimizer.\n   Remove duplicate nodes except identity nodes which should be handled by identity optimizer.\n   for example, node a is input of node b and node c, and computation of node b, c are same such as ""abs"" op.\n   then b and c can be merged into one node to avoid duplicated computation\n""""""\n\nfrom collections import defaultdict, namedtuple\n\nimport numpy as np\n\nfrom .optimizer_base import GraphOptimizerBase\n\n# pylint: disable=logging-not-lazy,unused-argument,missing-docstring\n\n_KeyToGroupNodes = namedtuple(""key"", ""type input"")\n\n\nclass MergeDuplicatedNodesOptimizer(GraphOptimizerBase):\n    """"""Remove duplicate nodes.\n    """"""\n\n    def __init__(self):\n        super(MergeDuplicatedNodesOptimizer, self).__init__()\n        # used internally\n        self._graph_can_be_optimized = True\n\n    def _optimize(self, graph):\n        return self._apply_optimization(graph, self._optimize_at_current_graph_level)\n\n    def _optimize_at_current_graph_level(self, graph):\n        while self._graph_can_be_optimized:\n            self._graph_can_be_optimized = False\n            self._merge_duplicated_nodes(graph)\n            if self._graph_can_be_optimized:\n                self.graph_been_opt = True\n        return graph\n\n    def _merge_duplicated_nodes(self, graph):\n        # ""duplicated"" means: op_type, input and attribute are same\n        # while attr is un-hashable so doesn\'t include it when grouping nodes\n        nodes_groups = self._group_nodes_by_type_inputs(graph)\n        for _, nodes_group in nodes_groups.items():\n            if self._skip_node_type(nodes_group[0]):\n                continue\n            self._del_nodes_if_duplicated(nodes_group, graph)\n\n    @staticmethod\n    def _group_nodes_by_type_inputs(graph):\n        res = defaultdict(list)\n        for node in graph.get_nodes():\n            # default const of graph input cannot be merged\n            if node.is_graph_input_default_const():\n                continue\n            res[_KeyToGroupNodes(node.type, tuple(node.input))].append(node)\n        return res\n\n    def _del_nodes_if_duplicated(self, nodes_group, graph):\n        # input and op type of nodes in same group are same,\n        # and if their attributes are also same then they are duplicated\n        while len(nodes_group) > 1:\n            unprocessed_node = []\n            nodes_to_process = [nodes_group[0]]\n            for node in nodes_group[1:]:\n                if self._have_equal_attr(node, nodes_to_process[0], graph):\n                    nodes_to_process.append(node)\n                else:\n                    unprocessed_node.append(node)\n\n            self._merge_nodes_that_are_duplicated(nodes_to_process, graph)\n            nodes_group = unprocessed_node\n\n    def _have_equal_attr(self, node_1, node_2, graph):\n        if node_1.attr == node_2.attr:\n            return True\n        # above check guarantees consts here are able to be merged\n        if node_1.is_const() and node_2.is_const():\n            # get_tensor_value is costly so that we check their shape first\n            shape_1 = graph.get_shape(node_1.output[0])\n            shape_2 = graph.get_shape(node_2.output[0])\n            if shape_1 is not None and shape_2 is not None and \\\n                    shape_1 != shape_2:\n                return False\n            const_1 = node_1.get_tensor_value(as_list=False)\n            const_2 = node_2.get_tensor_value(as_list=False)\n            if const_1.dtype == const_2.dtype and \\\n                    np.array_equal(const_1, const_2):\n                return True\n        return False\n\n    def _merge_nodes_that_are_duplicated(self, nodes_to_process, graph):\n        # node\'s output may not all be used, so have to select the one that uses most of node\'s outputs\n        nodes_to_process.sort(key=self._len_of_node_output, reverse=True)\n        node_to_retain = nodes_to_process[0]\n        for node_to_delete in nodes_to_process[1:]:\n            # if one of the output is graph\'s output then it can\'t be deleted\n            if set(node_to_delete.output).intersection(set(graph.outputs)):\n                continue\n            for old_input, new_input in zip(node_to_delete.output, node_to_retain.output):\n                graph.replace_all_inputs(graph.get_nodes(), old_input, new_input)\n            graph.remove_node(node_to_delete.name)\n            self._graph_can_be_optimized = True\n\n    @staticmethod\n    def _skip_node_type(node):\n        # identity node will be handled by identity optimizer so skip it\n        if node.type in [""Identity""]:\n            return True\n        if node.is_graph_input():\n            return True\n        return False\n\n    @staticmethod\n    def _len_of_node_output(node):\n        return len(node.output)\n'"
tf2onnx/optimizer/optimizer_base.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""Graph Optimizer Base""""""\n\nfrom __future__ import unicode_literals\n\nimport copy\n\nfrom .. import logging, utils\n\n\nclass GraphOptimizerBase(object):\n    """"""optimizer graph to improve performance\n    """"""\n\n    def __init__(self):\n        self._logger = logging.getLogger(\'.\'.join(__name__.split(\'.\')[:-1] + [self.__class__.__name__]))\n        self._graph_been_opt = False\n\n    @property\n    def logger(self):\n        return self._logger\n\n    @property\n    def is_debug_mode(self):\n        return utils.is_debug_mode()\n\n    @property\n    def graph_been_opt(self):\n        return self._graph_been_opt\n\n    @graph_been_opt.setter\n    def graph_been_opt(self, value):\n        self._graph_been_opt = value\n\n    def optimize(self, graph):\n        """""" Optimize graph, return optimized graph. """"""\n        before = graph.dump_node_statistics()\n\n        graph = self._optimize(graph)\n        graph.update_proto()\n        graph.delete_unused_nodes(graph.outputs)\n\n        after = graph.dump_node_statistics()\n        self._print_stat_diff(before, after)\n        return graph\n\n    def _optimize(self, graph):\n        """""" Derived class should override this function. """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def _apply_optimization(graph, optimize_func):\n        """"""\n        optimize graph\n        will also optimize graph of nodes\'\n        Args:\n            graph: the top level graph to be optimized\n            optimize_func: function to optimize graph\n        """"""\n        graph = optimize_func(graph)\n        for node in graph.get_nodes():\n            body_graphs = node.get_body_graphs()\n            if body_graphs:\n                for attr, b_g in body_graphs.items():\n                    b_g = GraphOptimizerBase._apply_optimization(b_g, optimize_func)\n                    node.set_body_graph_as_attr(attr, b_g)\n        return graph\n\n    def _print_stat_diff(self, before, after):\n        diff = copy.deepcopy(after)\n        diff.subtract(before)\n        diff = [""{} {} ({}->{})"".format(k, str(v) if v < 0 else \'+\' + str(v), before.get(k, 0), after.get(k, 0))\n                for k, v in sorted(diff.items()) if v != 0]\n        self.logger.verbose(\', \'.join(diff) if diff else ""no change"")\n'"
tf2onnx/optimizer/transpose_optimizer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\n""""""Transpose Optimizer.""""""\r\n\r\nfrom __future__ import unicode_literals\r\nfrom collections import defaultdict\r\n\r\nimport numpy as np\r\nimport onnx\r\nfrom tf2onnx.constants import NCHW_TO_NHWC, NHWC_TO_NCHW\r\nfrom .. import utils\r\nfrom .optimizer_base import GraphOptimizerBase\r\n\r\n\r\n# pylint: disable=logging-not-lazy,unused-argument,missing-docstring,abstract-method\r\n# FIXME:\r\n# pylint: disable=unused-variable\r\n\r\ndef is_nhwc_transpose(transpose_node):\r\n    perm_attr = transpose_node.get_attr(\'perm\')\r\n    return transpose_node.type == ""Transpose"" and perm_attr and perm_attr.ints == NCHW_TO_NHWC\r\n\r\n\r\ndef is_nchw_transpose(transpose_node):\r\n    perm_attr = transpose_node.get_attr(\'perm\')\r\n    return transpose_node.type == ""Transpose"" and perm_attr and perm_attr.ints == NHWC_TO_NCHW\r\n\r\n\r\ndef is_useless_transpose(transpose_node):\r\n    perm_attr = transpose_node.get_attr(\'perm\')\r\n    return transpose_node.type == ""Transpose"" and perm_attr and perm_attr.ints == list(range(len(perm_attr.ints)))\r\n\r\n\r\nclass TransposeOptimizer(GraphOptimizerBase):\r\n    """"""Transpose Optimizer.""""""\r\n\r\n    def __init__(self):\r\n        super(TransposeOptimizer, self).__init__()\r\n\r\n        self._handler_map = {}\r\n        self._force_stop = {}\r\n\r\n        self._initialize_handlers()\r\n        self._g = None\r\n        self._output_names = None\r\n\r\n    @property\r\n    def nodes(self):\r\n        return self._g.get_nodes()\r\n\r\n    def pre_optimize_action(self):\r\n        # make Reshape into a const, which then can be fused into Conv\'s weight for mobilenet_v1_75_192\r\n        self._output_names = [name.split("":"")[0] for name in self._g.outputs]\r\n        ops = self.nodes\r\n        constable_reshape_ops = [n for n in ops\r\n                                 if (n.type == ""Reshape""\r\n                                     and n.inputs[0].is_const()\r\n                                     and n.inputs[1].is_const())]\r\n        for reshape_op in constable_reshape_ops:\r\n            target_t = reshape_op.inputs[0].get_tensor_value(as_list=False)\r\n            target_shape = reshape_op.inputs[1].get_tensor_value(as_list=False)\r\n            new_data = np.reshape(target_t, tuple(target_shape))\r\n            const_name = reshape_op.output[0]\r\n            self._g.remove_node(reshape_op.name)\r\n            self._g.make_const(const_name, new_data)\r\n\r\n            # point all children nodes inputs to the new node\r\n            for output_name in reshape_op.output:\r\n                for child in ops:\r\n                    for i, name in enumerate(child.input):\r\n                        if name == output_name:\r\n                            child.input[i] = const_name\r\n\r\n            self._g.topological_sort(self._g.get_nodes())\r\n\r\n    def post_optimize_action(self):\r\n        def _calculate_new_shape(graph, op):\r\n            input_shape = graph.get_shape(op.input[0])\r\n            if input_shape.count(-1) <= 1:\r\n                if is_nchw_transpose(op):\r\n                    new_shape = [input_shape[0], input_shape[3], input_shape[1], input_shape[2]]\r\n                else:\r\n                    new_shape = [input_shape[0], input_shape[2], input_shape[3], input_shape[1]]\r\n                return graph.make_const(utils.make_name(""new_shape""), np.array(new_shape, dtype=np.int64)).output[0]\r\n\r\n            # reshape requires tha output shape can only contain one -1, if not some extra op needed.\r\n            input_shape = graph.make_node(""Shape"", [op.input[0]]).output[0]\r\n            if is_nchw_transpose(op):\r\n                indice = graph.make_const(utils.make_name(""indice""), np.array(NHWC_TO_NCHW)).output[0]\r\n            else:\r\n                indice = graph.make_const(utils.make_name(""indice""), np.array(NCHW_TO_NHWC)).output[0]\r\n\r\n            return graph.make_node(""Gather"", [input_shape, indice]).output[0]\r\n\r\n        nodes = self.nodes\r\n        # if channel==1 or height==width==1, replace transpose with reshape\r\n        # replacing trans with reshape is because transpose will copy data even if this transpose doesn\'t nothing\r\n        for op in nodes:\r\n            if op.type == ""Transpose"":\r\n                input_shape = self._g.get_shape(op.input[0])\r\n                if not input_shape:\r\n                    continue\r\n\r\n                if (is_nchw_transpose(op) and (input_shape[3] == 1 or (input_shape[1:3] == [1, 1]))) \\\r\n                        or (is_nhwc_transpose(op) and (input_shape[1] == 1 or (input_shape[2:4] == [1, 1]))):\r\n                    new_shape = _calculate_new_shape(self._g, op)\r\n                    # replace transpose with reshape\r\n                    self._g.remove_node(op.name)\r\n                    self._g.make_node(""Reshape"", [op.input[0], new_shape], name=op.name, outputs=op.output)\r\n                    self._g.topological_sort(self._g.get_nodes())\r\n\r\n    def merge_duplicated_transposes(self):\r\n        # strategy used in previous procedure is to move transpose nodes down if possible,\r\n        # and it means that when a node has n outputs then n transpose will be generated,\r\n        # so we should merge them back to one if they can\'t be eliminated in previous procedure.\r\n        graph = self._g\r\n        input_transposes_map = defaultdict(list)\r\n        for node in graph.get_nodes():\r\n            if node.type == ""Transpose"" and node.get_attr(""perm""):\r\n                key = (node.input[0], str(node.get_attr(""perm"").ints))\r\n                input_transposes_map[key].append(node)\r\n\r\n        for transposes in input_transposes_map.values():\r\n            # merge transpose nodes into one: make nodes use the output of the first transpose node\r\n            transpose_out = transposes[0].output[0]\r\n            for node in transposes[1:]:\r\n                old_transpose_out = node.output[0]\r\n                graph.replace_all_inputs(graph.get_nodes(), old_transpose_out, transpose_out)\r\n\r\n        # dangling transpose nodes can be deleted\r\n        graph.delete_unused_nodes(graph.outputs)\r\n\r\n    def _optimize(self, graph):\r\n        return self._apply_optimization(graph, self._optimize_at_current_graph_level)\r\n\r\n    def _optimize_at_current_graph_level(self, graph):\r\n        self._g = graph\r\n        self.pre_optimize_action()\r\n        no_action = False\r\n        iteration_cnt = 0\r\n        while not no_action:\r\n            no_action = True\r\n            nodes = self.nodes\r\n            self._force_stop = {}\r\n            for n in nodes:\r\n                if is_nhwc_transpose(n):\r\n                    if self._handle_nhwc_tranpose(n):\r\n                        no_action = False\r\n                        self.graph_been_opt = True\r\n                        iteration_cnt += 1\r\n                        # need break, because handler may change nodes set, making the n stale object\r\n                        # referencing already deleted elements\r\n                        break\r\n\r\n                if is_useless_transpose(n):\r\n                    no_action = False\r\n                    iteration_cnt += 1\r\n                    self._remove_useless_tranpose(n)\r\n                    break\r\n            # for debugging purpose\r\n            if ""stop"" in self._force_stop and self._force_stop[""stop""] == 1:\r\n                break\r\n\r\n        self.logger.debug(""finish after "" + str(iteration_cnt) + "" iteration(s)"")\r\n\r\n        self.merge_duplicated_transposes()\r\n        self.post_optimize_action()\r\n        return self._g\r\n\r\n    def _initialize_handlers(self):\r\n        self._handler_map = {\r\n            ""Add"": self._add_handler,\r\n            ""Cast"": self._simple_through_handler,\r\n            ""Clip"": self._simple_through_handler,\r\n            ""Concat"": self._concat_handler,\r\n            ""Elu"": self._simple_through_handler,\r\n            ""Exp"": self._simple_through_handler,\r\n            ""Identity"": self._identity_handler,\r\n            ""LeakyRelu"": self._simple_through_handler,\r\n            ""Log"": self._simple_through_handler,\r\n            ""Max"": self._maxmin_handler,\r\n            ""Min"": self._maxmin_handler,\r\n            ""Mul"": self._mul_handler,\r\n            ""Pad"": self._pad_handler,\r\n            ""ReduceMean"": self._reducemean_handler,\r\n            ""Relu"": self._simple_through_handler,\r\n            ""Shape"": self._shape_handler,\r\n            ""Sigmoid"": self._simple_through_handler,\r\n            ""Sum"": self._sum_handler,\r\n            ""Slice"": self._slice_handler,\r\n            ""Split"": self._split_handler,\r\n            ""Softplus"": self._simple_through_handler,\r\n            ""Squeeze"": self._squeeze_handler,\r\n            ""Sub"": self._sub_handler,\r\n            ""Tanh"": self._simple_through_handler,\r\n            ""Transpose"": self._transpose_handler,\r\n        }\r\n\r\n    def _handle_node_having_branches(self, node):\r\n        # create transpose pairs if some input are not.\r\n        if not self._create_transpose_pairs_before_node(node):\r\n            return False\r\n        # make sure node\'s all input transpose all have only 1 consumer node,\r\n        # otherwise, it would impact their other output nodes\r\n        if self._nodes_has_single_consumer_node(node.inputs) and len(node.output) == 1:\r\n            self._create_transpose_pairs_after_node(node)\r\n            input_transposes = set(node.inputs)\r\n            for n in input_transposes:\r\n                n_input = n.input[0]\r\n                utils.make_sure(len(n.output) == 1, ""only expect single output"")\r\n                self._g.replace_all_inputs(self._g.get_nodes(), n.output[0], n_input)\r\n                self._g.remove_node(n.name)\r\n\r\n            utils.make_sure(len(node.output) == 1, ""only expect single output"")\r\n            # currently we assume node only has 1 output, for cases where it is more than 1 for example Split\r\n            # we need consider the fact that Split\'s multiple output will not always has data in NCHW/NHWC,\r\n            # it might be a different shape.\r\n            output_transposes = self._g.find_output_consumers(node.output[0])\r\n            for n in output_transposes:\r\n                n_input = n.input[0]\r\n                utils.make_sure(len(n.output) == 1, ""only expect single output"")\r\n                self._g.replace_all_inputs(self._g.get_nodes(), n.output[0], n_input)\r\n                self._g.remove_node(n.name)\r\n\r\n            shape = self._g.get_shape(node.output[0])\r\n            if shape:\r\n                # only nhwc transpose can reach here\r\n                new_shape = [shape[i] for i in NHWC_TO_NCHW]\r\n                self._g.set_shape(node.output[0], new_shape)\r\n            return True\r\n\r\n        self.logger.debug(""input transpose does not have single consumer, skipping..."")\r\n        return False\r\n\r\n    # get the input index of transpose op in node\'s inputs.\r\n    def _get_input_index_for_trans(self, node, trans):\r\n        input_index = 0\r\n        for i in node.input:\r\n            if i == trans.output[0]:\r\n                break\r\n            input_index += 1\r\n        return input_index\r\n\r\n    # the assumption is: both node and trans have only 1 output\r\n    def _switch_transpose_and_node(self, node, trans):\r\n        if not self._nodes_has_single_consumer_node([trans]):\r\n            return False\r\n\r\n        input_index = self._get_input_index_for_trans(node, trans)\r\n\r\n        ops = self._g.get_nodes()\r\n        self._g.replace_all_inputs(ops, node.output[0], trans.output[0])\r\n        node.input[input_index] = trans.input[0]\r\n        trans.input[0] = node.output[0]\r\n\r\n        # need to transpose node shape in backward direction as well after switch\r\n        # otherwise, reshape added in post_optimize_action may not work correctly\r\n        shape = self._g.get_shape(node.output[0])\r\n        if shape:\r\n            # only nhwc transpose can reach here\r\n            new_shape = [shape[i] for i in NHWC_TO_NCHW]\r\n            self._g.set_shape(node.output[0], new_shape)\r\n        return True\r\n\r\n    # if return value is True, then it means Transpose is handled as designed\r\n    # otherwise, it means that we skip handling since it is not in our support set\r\n    def _handle_nhwc_tranpose(self, trans):\r\n        if trans.output[0] in self._g.outputs:\r\n            self.logger.debug(""%s connects to graph outputs, skip"", trans.output[0])\r\n            return False\r\n        out_nodes = self._g.find_output_consumers(trans.output[0])\r\n        if len(out_nodes) == 1:\r\n            p = out_nodes[0]\r\n            if p.name in self._output_names:\r\n                self.logger.debug(""cannot move transpose down since it met output node %s"", p.name)\r\n                return False\r\n\r\n            if p.type in self._handler_map:\r\n                op_handler = self._handler_map[p.type]\r\n                return op_handler(trans, p)\r\n            return False\r\n        # move transpose into branches to let Transposes can be ""handled"" in each branch\r\n        for n in out_nodes:\r\n            branch_trans = self._g.make_node(""Transpose"", [trans.input[0]], attr=trans.attr_onnx)\r\n            self._g.replace_input(n, trans.output[0], branch_trans.output[0])\r\n\r\n        self._g.remove_node(trans.name)\r\n        return False\r\n\r\n    def _remove_useless_tranpose(self, trans):\r\n        self._g.replace_all_inputs(self._g.get_nodes(), trans.output[0], trans.input[0])\r\n        self._g.remove_node(trans.name)\r\n\r\n    def _nodes_has_single_consumer_node(self, nodes):\r\n        for n in nodes:\r\n            for output in n.output:\r\n                cnt = len(set(self._g.find_output_consumers(output)))\r\n                if cnt != 1:\r\n                    return False\r\n        return True\r\n\r\n    def _get_non_nchw_transpose_output_nodes(self, node):\r\n        # we just support node having 1 output, we need consider cases where node has more than 1 outputs\r\n        assert len(node.output) == 1\r\n        non_nchw_tranpose_nodes = []\r\n        consumers = self._g.find_output_consumers(node.output[0])\r\n        for o in consumers:\r\n            if not is_nchw_transpose(o) and o not in non_nchw_tranpose_nodes:\r\n                non_nchw_tranpose_nodes.append(o)\r\n        return non_nchw_tranpose_nodes\r\n\r\n    def _create_transpose_pairs_after_node(self, node):\r\n        assert len(node.output) == 1  # just support node who has 1 output\r\n        non_nchw_trans_consumers = self._get_non_nchw_transpose_output_nodes(node)\r\n        # add Transpose(0, 3, 1, 2) and Transpose(0, 2, 3, 1) before each non_nchw_trans_consumers\r\n        for consumer in non_nchw_trans_consumers:\r\n            nchw_node = self._g.make_node(""Transpose"", [node.output[0]], attr={""perm"": NHWC_TO_NCHW})\r\n            nhwc_node = self._g.make_node(""Transpose"", [nchw_node.output[0]], attr={""perm"": NCHW_TO_NHWC})\r\n            self._g.replace_input(consumer, node.output[0], nhwc_node.output[0])\r\n\r\n    def _create_transpose_pairs_before_node(self, node):\r\n        def shape_after_expand(ori_shape):\r\n            # according to broadcasting rule to expand shape to 4D while not tile the tensor here\r\n            # still count on the broadcasting op to tile the tensor\r\n            if ori_shape.count(-1) >= 2:\r\n                self.logger.warning(""%s shape can contain one -1 at most, otherwise reshape op can\'t work"", node.name)\r\n                return None\r\n            ori_rank = len(ori_shape)\r\n            new_shape = [1] * (4 - ori_rank) + ori_shape\r\n            return new_shape\r\n\r\n        non_nhwc_trans_inputs = []\r\n        for input_id, n in zip(node.input, node.inputs):\r\n            if not is_nhwc_transpose(n):\r\n                # check in case node has two inputs coming from a same node output.\r\n                if [input_id, n] not in non_nhwc_trans_inputs:\r\n                    non_nhwc_trans_inputs.append([input_id, n])\r\n\r\n        # add Transpose(0, 3, 1, 2) and Transpose(0, 2, 3, 1) before each non_nhwc_trans_consumers\r\n        shape_unknow = [input_id for input_id, _ in non_nhwc_trans_inputs if self._g.get_shape(input_id) is None]\r\n        if shape_unknow:\r\n            if self._g.opset <= 9:\r\n                msg = ""%s \'s shape is unknown, ConstantOfShape will be used which exists in version 9 or higher"" \\\r\n                      ""while graph\'s opset version is %s"" % (shape_unknow, self._g.opset)\r\n                self.logger.warning(msg)\r\n                return False\r\n\r\n        for input_id, n in non_nhwc_trans_inputs:\r\n            shape = self._g.get_shape(input_id)\r\n            # if rank of n is not 4, then we need to insert a reshape op before inserting a transpose\r\n            # for example shape of n is [x, y], then output shape of reshape will be [1, 1, x, y]\r\n            if shape is None:\r\n                const_4 = self._g.make_const(utils.make_name(""const_4""), np.array([4], np.int64)).output[0]\r\n                tensor_1 = onnx.helper.make_tensor(""value"", onnx.TensorProto.INT64, [1], [1])\r\n                shape_node = self._g.make_node(""Shape"", [input_id]).output[0]\r\n                rank_node = self._g.make_node(""Shape"", [shape_node]).output[0]\r\n                expand_rank = self._g.make_node(""Sub"", [const_4, rank_node]).output[0]\r\n                array_fill_1 = self._g.make_node(""ConstantOfShape"", [expand_rank], attr={""value"": tensor_1}).output[0]\r\n                new_shape = self._g.make_node(""Concat"", [array_fill_1, shape_node], attr={""axis"": 0}).output[0]\r\n                reshape = self._g.make_node(""Reshape"", [input_id, new_shape]).output[0]\r\n                input_of_new_trans = reshape\r\n            elif len(shape) == 4:\r\n                input_of_new_trans = input_id\r\n            else:\r\n                shape_4d = shape_after_expand(shape)\r\n                if shape_4d is None:\r\n                    return False\r\n                const = self._g.make_const(utils.make_name(""reshape_shape""), np.array(shape_4d, np.int64)).output[0]\r\n                reshape = self._g.make_node(""Reshape"", [input_id, const]).output[0]\r\n                input_of_new_trans = reshape\r\n\r\n            nchw_node = self._g.make_node(""Transpose"", [input_of_new_trans], attr={""perm"": NHWC_TO_NCHW})\r\n            nhwc_node = self._g.make_node(""Transpose"", [nchw_node.output[0]], attr={""perm"": NCHW_TO_NHWC})\r\n            self._g.replace_input(node, input_id, nhwc_node.output[0])\r\n        return True\r\n\r\n    def _add_handler(self, trans, node):\r\n        if node.inputs[1].is_const():\r\n            t_p = trans.inputs[0]\r\n            if t_p.type in (""Conv"", ""ConvTranspose"") and len(t_p.input) == 2:\r\n                # if Conv or ConvTranspose\'s bias input is not set, then we set, otherwise, we don\'t set\r\n                # todo: maybe we can add already set bias with the input??? try later\r\n\r\n                if not self._nodes_has_single_consumer_node([t_p]):\r\n                    self.logger.debug(""Conv does not have single consumer, can not merge Conv and Add"")\r\n                    return self._handle_node_having_branches(node)\r\n\r\n                if not self._nodes_has_single_consumer_node([trans]):\r\n                    self.logger.debug(""input transpose does not have single consumer, skipping..."")\r\n                    return False\r\n\r\n                target_node = node.inputs[1]\r\n                numpy_val = target_node.get_tensor_value(as_list=False)\r\n                # Optional 1D bias to be added to the convolution, has size of M\r\n                if len(numpy_val.shape) - numpy_val.shape.count(1) > 1:\r\n                    self.logger.debug(""Bias is not 1D, can not merge Conv and Add"")\r\n                    return self._handle_node_having_branches(node)\r\n\r\n                bias_size = max(numpy_val.shape)\r\n                size_m = t_p.inputs[1].output_shapes[0][0]\r\n                if bias_size != size_m:\r\n                    self.logger.debug(""Bias size is not M, can not merge Conv and Add"")\r\n                    return self._handle_node_having_branches(node)\r\n\r\n                target_val = numpy_val.reshape(bias_size)\r\n                target_node.set_tensor_value(target_val)\r\n\r\n                conv_inputs = [t_p.input[0], t_p.input[1], node.input[1]]\r\n                conv_node = self._g.make_node(t_p.type, conv_inputs, attr=t_p.attr_onnx)\r\n                ops = self._g.get_nodes()\r\n                trans.input[0] = utils.port_name(conv_node.name)\r\n                self._g.replace_all_inputs(ops, node.output[0], trans.output[0])\r\n                self._g.remove_node(t_p.name)\r\n                self._g.remove_node(node.name)\r\n                return True\r\n        return self._handle_node_having_branches(node)\r\n\r\n    def _transpose_handler(self, trans, node):\r\n        if is_nchw_transpose(node):\r\n            for g in {self._g, node.graph}:\r\n                ops = g.get_nodes()\r\n                g.replace_all_inputs(ops, node.output[0], trans.input[0])\r\n\r\n            shape = node.graph.get_shape(node.output[0])\r\n            dtype = node.graph.get_dtype(node.output[0])\r\n            if node.output[0] in node.graph.outputs:\r\n                node.graph.make_node(""Identity"", [trans.input[0]],\r\n                                     outputs=node.output, shapes=[shape], dtypes=[dtype])\r\n            self._g.remove_node(trans.name)\r\n            node.graph.remove_node(node.name)\r\n            return True\r\n        return False\r\n\r\n    def _maxmin_handler(self, trans, node):\r\n        return self._handle_node_having_branches(node)\r\n\r\n    def _mul_handler(self, trans, node):\r\n        multiplier_input_id = None\r\n        multiplier_input_node = None\r\n        for i, input_node in zip(node.input, node.inputs):\r\n            if i != trans.output[0]:\r\n                multiplier_input_id = i\r\n                multiplier_input_node = input_node\r\n\r\n        # node\'s inputs may come from one same node. if so the multiplier_input_node may be none\r\n        if multiplier_input_node is None:\r\n            return False\r\n\r\n        # convert  mul(trans(x), trans(y)) ->  trans(mul(x, y))\r\n        if multiplier_input_node.type == ""Transpose"":\r\n            if is_nhwc_transpose(multiplier_input_node):\r\n                if not self._nodes_has_single_consumer_node([multiplier_input_node]):\r\n                    return False\r\n                input_index = self._get_input_index_for_trans(node, multiplier_input_node)\r\n                if not self._switch_transpose_and_node(node, trans):\r\n                    return False\r\n\r\n                node.input[input_index] = multiplier_input_node.input[0]\r\n                self._g.remove_node(multiplier_input_node.name)\r\n                return True\r\n\r\n        # handle const multipliers\r\n        if not multiplier_input_node.is_const():\r\n            return False\r\n        multiplier = multiplier_input_node.get_tensor_value(as_list=False)\r\n\r\n        # todo: apply this block if we have model case multiplier_input_id==0, and verify that.\r\n        if multiplier_input_id == node.input[1]:\r\n            t_p = trans.inputs[0]\r\n            # make sure conv don\'t have bias set\r\n            if t_p.type == ""Conv"" and t_p.inputs[1].is_const() and len(t_p.input) == 2:\r\n                conv = t_p\r\n                numpy_val = conv.inputs[1].get_tensor_value(as_list=False)\r\n                transposed_val = np.transpose(numpy_val, (2, 3, 1, 0))\r\n                mul_val = multiplier\r\n                result = np.multiply(transposed_val, mul_val)\r\n                conv.inputs[1].set_tensor_value(np.transpose(result, (3, 2, 0, 1)))\r\n\r\n                ops = self._g.get_nodes()\r\n                self._g.replace_all_inputs(ops, node.output[0], trans.output[0])\r\n                self._g.remove_node(node.name)\r\n                return True\r\n\r\n        # if the shape is () or (1), we just move transpose after the mul\r\n        if not multiplier.shape or (len(multiplier.shape) == 1 and multiplier.shape[0] == 1):\r\n            return self._switch_transpose_and_node(node, trans)\r\n\r\n        return False\r\n\r\n    def _sum_handler(self, trans, node):\r\n        inputs = node.inputs\r\n        trans_shape = self._g.get_shape(trans.output[0])\r\n        perm = list(trans.get_attr(\'perm\').ints)\r\n        untrans_idx = [perm.index(i) for i in range(len(perm))]\r\n\r\n        # check if sum(trans(x1), trans(x2), const(x3), ...) can be switched\r\n        for n in inputs:\r\n            if n.type not in [""Transpose"", ""Const""]:\r\n                return False\r\n            if not self._nodes_has_single_consumer_node([n]):\r\n                return False\r\n            if n.is_const():\r\n                # if graph is valid, op shapes should be valid\r\n                # const is special case, in case of broadcasting\r\n                # ensure rank matches\r\n                n_shape = self._g.get_shape(n.output[0])\r\n                if len(n_shape) != len(trans_shape):\r\n                    return False\r\n            else:\r\n                if list(n.get_attr(\'perm\').ints) != perm:\r\n                    return False\r\n\r\n        # switch to trans(sum(x1, x2, x3, ...))\r\n        ops = self._g.get_nodes()\r\n        self._g.replace_all_inputs(ops, node.output[0], trans.output[0])\r\n        node.input = [n.output[0] if n.is_const() else n.input[0] for n in inputs]\r\n        trans.input[0] = node.output[0]\r\n\r\n        # adjust shape if present\r\n        shape = self._g.get_shape(node.output[0])\r\n        if shape:\r\n            self._g.set_shape(node.output[0], [shape[i] for i in untrans_idx])\r\n\r\n        # update constants, remove dangling transposes\r\n        for n in inputs:\r\n            if n.is_const():\r\n                val = n.get_tensor_value(as_list=False)\r\n                new_val = np.transpose(val, untrans_idx)\r\n                n.set_tensor_value(new_val)\r\n            elif n.name != trans.name:\r\n                self._g.remove_node(n.name)\r\n        return True\r\n\r\n    def _identity_handler(self, trans, node):\r\n        if node.output[0] in node.graph.outputs:\r\n            return False\r\n        for g in {self._g, node.graph}:\r\n            ops = g.get_nodes()\r\n            g.replace_all_inputs(ops, node.output[0], trans.output[0])\r\n        node.graph.remove_node(node.name)\r\n        return True\r\n\r\n    def _concat_handler(self, trans, node):\r\n        if self._handle_node_having_branches(node):\r\n            perm = trans.get_attr_value(""perm"")\r\n            axis = node.get_attr_value(""axis"", 0)\r\n            new_axis = perm[axis]\r\n            node.set_attr(""axis"", new_axis)\r\n            return True\r\n        return False\r\n\r\n    def _split_handler(self, trans, node):\r\n        # Todo: need handle cases where Slit node has more than 1 outputs.\r\n        if self._handle_node_having_branches(node):\r\n            node.set_attr(""axis"", 1)\r\n            return True\r\n        return False\r\n\r\n    def _squeeze_handler(self, trans, node):\r\n        def _calculate_new_attr(ori_perm, ori_squeeze_axes):\r\n            new_squeeze_axes = sorted([ori_perm[i] for i in ori_squeeze_axes])\r\n            # calculate output shape after trans and squeeze\r\n            input_shape = ""abcd""\r\n            shape_after_trans = [input_shape[i] for i in ori_perm]\r\n            output_shape = [shape_after_trans[i] for i in range(4) if i not in ori_squeeze_axes]\r\n            # calculate new_perm\r\n            # after switch, the output shape should be same, using this condtion we can figure the new perm\r\n            shape_after_squeeze = [input_shape[i] for i in range(4) if i not in new_squeeze_axes]\r\n            new_perm = [shape_after_squeeze.index(i) for i in output_shape]\r\n\r\n            return new_perm, new_squeeze_axes\r\n\r\n        if not self._nodes_has_single_consumer_node([trans]):\r\n            return False\r\n\r\n        if node.get_attr(""axes""):\r\n            # switch tran and squeeze\r\n            # 1 switch\r\n            ops = self._g.get_nodes()\r\n            self._g.replace_all_inputs(ops, node.output[0], trans.output[0])\r\n            node.input[0] = trans.input[0]\r\n            trans.input[0] = node.output[0]\r\n            # 2 correct attr of nodes\r\n            squeeze_axes = sorted(list(node.get_attr(""axes"").ints))\r\n            trans_perm = list(trans.get_attr(""perm"").ints)\r\n            new_perm, new_squeeze_axes = _calculate_new_attr(ori_perm=trans_perm, ori_squeeze_axes=squeeze_axes)\r\n            trans.set_attr(""perm"", new_perm)\r\n            node.set_attr(""axes"", new_squeeze_axes)\r\n            # 3 set shape\r\n            squeeze_shape = self._g.get_shape(node.output[0])\r\n            self._g.set_shape(trans.output[0], squeeze_shape)\r\n            input_shape = self._g.get_shape(node.input[0])\r\n            if input_shape is not None:\r\n                new_squeeze_output_shape = [input_shape[i] for i in range(4) if i not in new_squeeze_axes]\r\n            else:\r\n                new_squeeze_output_shape = [-1] * 4\r\n                self.logger.warning(""%s\'s shape is unknown, which may interfere further optimization"", node.input[0])\r\n            self._g.set_shape(node.output[0], new_squeeze_output_shape)\r\n            return True\r\n        return False\r\n\r\n    def _sub_handler(self, trans, node):\r\n        return self._handle_node_having_branches(node)\r\n\r\n    def _pad_handler(self, trans, node):\r\n        # [N-start, H-start, W-start, C-start, N-end, H-end,  W-end, C-end]\r\n        if self._g.opset < 11:\r\n            pads = node.get_attr(\'pads\').ints  # [x1_begin, x2_begin...x1_end, x2_end,...]\r\n            # NHWC->NCHW\r\n            new_pads = [pads[0], pads[3], pads[1], pads[2], pads[4], pads[7], pads[5], pads[6]]\r\n            node.set_attr(""pads"", new_pads)\r\n            return self._switch_transpose_and_node(node, trans)\r\n\r\n        input1 = node.inputs[1]\r\n        if input1.is_const():\r\n            if input1.data_format in [""NHWC"", ""unkown""]:\r\n                if not self._nodes_has_single_consumer_node([input1]):\r\n                    input1 = self._g.copy_const(input1)\r\n                    node.input[1] = input1.output[0]\r\n                pads = input1.get_tensor_value()\r\n                # NHWC->NCHW\r\n                new_pads = np.array([pads[0], pads[3], pads[1], pads[2], pads[4], pads[7], pads[5], pads[6]],\r\n                                    dtype=np.int64)\r\n                input1.set_tensor_value(new_pads)\r\n                input1.data_format = ""NCHW""\r\n            return self._switch_transpose_and_node(node, trans)\r\n        return False\r\n\r\n    def _reducemean_handler(self, trans, node):\r\n        axes = node.get_attr(""axes"").ints\r\n        keepdims = node.get_attr(""keepdims"")\r\n        # make sure keepdims is 1, then we can do the swap, otherwise, please don\'t, because\r\n        # once keepdims is not set, original dims are lost, so transpose back won\'t work well.\r\n        # by default, if keepdims is not specified, it is 1\r\n        if axes == [1, 2] and ((keepdims and keepdims.i == 1) or (not keepdims)):\r\n            node.set_attr(""axes"", [2, 3])\r\n            return self._switch_transpose_and_node(node, trans)\r\n        return False\r\n\r\n    def _slice_handler(self, trans, node):\r\n        axes = None\r\n        if self._g.opset < 10:\r\n            axes = node.get_attr(""axes"").ints\r\n            if axes == [0, 1, 2, 3]:\r\n                node.set_attr(""axes"", NCHW_TO_NHWC)\r\n                return self._switch_transpose_and_node(node, trans)\r\n        else:  # in opset 10, axes is input instead of an attribute.\r\n            if len(node.inputs) >= 4 and node.inputs[3].is_const():\r\n                axes = node.inputs[3].get_tensor_value(as_list=True)\r\n                if axes == [0, 1, 2, 3]:\r\n                    # axes node might be shared\r\n                    new_axes = np.array(NCHW_TO_NHWC, dtype=np.int64)\r\n                    if self._nodes_has_single_consumer_node([node.inputs[3]]):\r\n                        node.inputs[3].set_tensor_value(new_axes)\r\n                    else:\r\n                        new_axes_const = self._g.make_const(\r\n                            utils.make_name(node.inputs[3].name), new_axes\r\n                        )\r\n                        self._g.replace_input(node, node.input[3], new_axes_const.output[0])\r\n                    return self._switch_transpose_and_node(node, trans)\r\n        return False\r\n\r\n    def _simple_through_handler(self, trans, node):\r\n        return self._switch_transpose_and_node(node, trans)\r\n\r\n    def _shape_handler(self, trans, node):\r\n        # input > trans > shape  can be changed into  input > shape > gather\r\n        if not self._nodes_has_single_consumer_node([trans]):\r\n            return False\r\n\r\n        output_shape = self._g.get_shape(node.output[0])\r\n        output_dtype = self._g.get_dtype(node.output[0])\r\n        self._g.remove_node(trans.name)\r\n        self._g.remove_node(node.name)\r\n        shape_node = self._g.make_node(""Shape"", [trans.input[0]])\r\n        const_node = self._g.make_const(utils.make_name(""Const""), np.array(trans.get_attr(""perm"").ints))\r\n        gather_node = self._g.make_node(""Gather"", [shape_node.output[0], const_node.output[0]], outputs=node.output)\r\n        self._g.set_shape(gather_node.output[0], output_shape)\r\n        self._g.set_dtype(gather_node.output[0], output_dtype)\r\n        return True\r\n'"
tf2onnx/rewriter/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n""""""tf2onnx.rewriter module.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom tf2onnx.rewriter.cond_rewriter import rewrite_cond\nfrom tf2onnx.rewriter.conv2d_with_pad_rewriter import rewrite_conv2d_with_pad\nfrom tf2onnx.rewriter.dropout_rewriter import rewrite_dropout\nfrom tf2onnx.rewriter.eye_rewriter import rewrite_eye\nfrom tf2onnx.rewriter.flatten_rewriter import rewrite_flatten\nfrom tf2onnx.rewriter.gemm_rewriter import rewrite_gemm\nfrom tf2onnx.rewriter.leakyrelu_rewriter import rewrite_leakyrelu\nfrom tf2onnx.rewriter.random_normal_rewriter import rewrite_random_normal\nfrom tf2onnx.rewriter.random_uniform import rewrite_random_uniform, rewrite_random_uniform_fold_const\nfrom tf2onnx.rewriter.rnn import rewrite_single_direction_lstm, rewrite_bi_direction_lstm, \\\n    rewrite_single_direction_gru, rewrite_bi_direction_gru, \\\n    rewrite_custom_rnn_cell, rewrite_generic_loop\nfrom tf2onnx.rewriter.thresholded_relu_rewriter import rewrite_thresholded_relu\nfrom tf2onnx.rewriter.transpose_rewriter import rewrite_transpose\nfrom tf2onnx.rewriter.conv2d_with_add_rewriter import rewrite_biasadd_with_conv2d\nfrom tf2onnx.rewriter.quantization_ops_rewriter import rewrite_quantize_and_dequantize\n\n\n__all__ = [\n    ""rewrite_cond"",\n    ""rewrite_conv2d_with_pad"",\n    ""rewrite_dropout"",\n    ""rewrite_eye"",\n    ""rewrite_flatten"",\n    ""rewrite_gemm"",\n    ""rewrite_leakyrelu"",\n    ""rewrite_random_normal"",\n    ""rewrite_random_uniform"",\n    ""rewrite_random_uniform_fold_const"",\n    ""rewrite_thresholded_relu"",\n    ""rewrite_transpose"",\n    ""rewrite_single_direction_lstm"",\n    ""rewrite_bi_direction_lstm"",\n    ""rewrite_single_direction_gru"",\n    ""rewrite_bi_direction_gru"",\n    ""rewrite_custom_rnn_cell"",\n    ""rewrite_generic_loop"",\n    ""rewrite_biasadd_with_conv2d"",\n    ""rewrite_quantize_and_dequantize""\n]\n'"
tf2onnx/rewriter/bigru_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.bigru_rewriter - bigru support.\nThis rewriter depends on tf2onnx.rewriter.gru_rewriter\'s results.\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nimport numpy as np\nfrom tf2onnx import utils\nfrom tf2onnx.rewriter import rnn_utils\n\n\nlogger = logging.getLogger(__name__)\n\n# pylint: disable=invalid-name,unused-argument,missing-docstring\n\ndef process_bigru(g, bi_grus):\n    for gru_fw, gru_bw in bi_grus:\n        logger.debug(""========================="")\n        logger.debug(""start handling potential bidirectional gru: %s, %s"", gru_fw.name, gru_bw.name)\n\n        w_fw = rnn_utils.get_np_val_for_const(g, gru_fw, 1)\n        w_bw = rnn_utils.get_np_val_for_const(g, gru_bw, 1)\n        r_fw = rnn_utils.get_np_val_for_const(g, gru_fw, 2)\n        r_bw = rnn_utils.get_np_val_for_const(g, gru_bw, 2)\n        b_fw = rnn_utils.get_np_val_for_const(g, gru_fw, 3)\n        b_bw = rnn_utils.get_np_val_for_const(g, gru_bw, 3)\n        W = np.concatenate((w_fw, w_bw), axis=0)\n        R = np.concatenate((r_fw, r_bw), axis=0)\n        B = np.concatenate((b_fw, b_bw), axis=0)\n\n        all_nodes = g.get_nodes()\n        if len(gru_fw.inputs) == len(gru_bw.inputs):\n            if len(gru_fw.inputs) > 4:\n                initializer_node = process_init_nodes(g, gru_fw, gru_bw, all_nodes)\n        else:\n            logger.error(""fw, bw gru inputs num is not consistent. stop"")\n            continue\n\n        # create node\n        w_name = utils.make_name(""W"")\n        w_node = g.make_const(w_name, W, skip_conversion=True)\n        all_nodes.append(w_node)\n\n        r_name = utils.make_name(""R"")\n        r_node = g.make_const(r_name, R, skip_conversion=True)\n        all_nodes.append(r_node)\n\n        b_name = utils.make_name(""B"")\n        b_node = g.make_const(b_name, B, skip_conversion=True)\n        all_nodes.append(b_node)\n        gru_inputs = [gru_fw.input[0], w_node.output[0],\n                      r_node.output[0], b_node.output[0]]\n        if len(gru_fw.inputs) > 4:\n            gru_inputs.extend([gru_fw.input[4], initializer_node.output[0]])\n\n        direction = ""bidirectional""\n        attr = {}\n        for name in rnn_utils.onnx_rnn_attr_mapping[rnn_utils.ONNX_RNN_TYPE.GRU]:\n            attr_val = gru_fw.get_attr_value(name)\n            if attr_val:\n                attr[name] = attr_val\n        # activation has to be took care, attr here is proto\n        activations = [act.decode(""utf-8"")\n                       for act in gru_fw.get_attr_value(""activations"")]\n        activations += [act.decode(""utf-8"")\n                        for act in gru_bw.get_attr_value(""activations"")]\n        attr.update({""direction"": direction, ""activations"": activations})\n\n        bi_gru_node = g.make_node(""GRU"", gru_inputs, attr=attr, output_count=2)\n        all_nodes.append(bi_gru_node)\n        logger.debug(""processing output nodes"")\n\n        to_remove = [gru_fw.name, gru_fw.input[1], gru_fw.input[2], gru_fw.input[3],\n                     gru_bw.name, gru_bw.input[1], gru_bw.input[2], gru_bw.input[3]]\n        rnn_utils.slice_birnn_for_original_rnn_consumers(\n            g, gru_fw, gru_bw, bi_gru_node, 0, all_nodes, to_remove)\n        rnn_utils.slice_birnn_for_original_rnn_consumers(\n            g, gru_fw, gru_bw, bi_gru_node, 1, all_nodes, to_remove)\n\n        gru_bw_old_x = gru_bw.input[0]\n        for n in to_remove:\n            g.remove_node(n)\n\n        rnn_utils.remove_reverse_in_bw_input(g, gru_bw_old_x, rnn_utils.ONNX_RNN_TYPE.GRU)\n\n    return g.get_nodes()\n\n\ndef process_init_nodes(g, gru_fw, gru_bw, to_append):\n    initializer_node = rnn_utils.process_single_init_node(\n        g, gru_fw.input[5], gru_bw.input[5], to_append)\n\n    return initializer_node\n\n\ndef rewrite_bidirectional_grus(g, ops):\n    bi_grus = rnn_utils.find_bidirectional_rnns(g, ops, rnn_utils.ONNX_RNN_TYPE.GRU)\n\n    return process_bigru(g, bi_grus)\n'"
tf2onnx/rewriter/bilstm_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\n""""""\r\ntf2onnx.rewriter.bilstm_rewriter - bilstm support.\r\nThis rewriter depends on tf2onnx.rewriter.lstm_rewriter\'s results.\r\n""""""\r\n\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\n\r\nimport logging\r\nimport numpy as np\r\nfrom tf2onnx import utils\r\nfrom tf2onnx.rewriter import rnn_utils\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n# pylint: disable=invalid-name,unused-argument,missing-docstring\r\n\r\ndef process_bilstm(g, bi_lstms):\r\n    for lstm_fw, lstm_bw in bi_lstms:\r\n        logger.debug(""========================="")\r\n        logger.debug(""start handling potential bidirectional lstm: %s, %s"", lstm_fw.name, lstm_bw.name)\r\n\r\n        w_fw = rnn_utils.get_np_val_for_const(g, lstm_fw, 1)\r\n        w_bw = rnn_utils.get_np_val_for_const(g, lstm_bw, 1)\r\n        r_fw = rnn_utils.get_np_val_for_const(g, lstm_fw, 2)\r\n        r_bw = rnn_utils.get_np_val_for_const(g, lstm_bw, 2)\r\n        b_fw = rnn_utils.get_np_val_for_const(g, lstm_fw, 3)\r\n        b_bw = rnn_utils.get_np_val_for_const(g, lstm_bw, 3)\r\n        W = np.concatenate((w_fw, w_bw), axis=0)\r\n        R = np.concatenate((r_fw, r_bw), axis=0)\r\n        B = np.concatenate((b_fw, b_bw), axis=0)\r\n\r\n        all_nodes = g.get_nodes()\r\n        if len(lstm_fw.inputs) == len(lstm_bw.inputs):\r\n            if len(lstm_fw.inputs) > 4:\r\n                h_node, c_node = process_ch_init_nodes(g, lstm_fw, lstm_bw, all_nodes)\r\n        else:\r\n            logger.error(""fw, bw lstm inputs num is not consistent. stop"")\r\n            continue\r\n\r\n        # create node\r\n        w_name = utils.make_name(""W"")\r\n        w_node = g.make_const(w_name, W, skip_conversion=True)\r\n        all_nodes.append(w_node)\r\n\r\n        r_name = utils.make_name(""R"")\r\n        r_node = g.make_const(r_name, R, skip_conversion=True)\r\n        all_nodes.append(r_node)\r\n\r\n        b_name = utils.make_name(""B"")\r\n        b_node = g.make_const(b_name, B, skip_conversion=True)\r\n        all_nodes.append(b_node)\r\n        lstm_inputs = [lstm_fw.input[0], w_node.output[0], r_node.output[0], b_node.output[0]]\r\n        if len(lstm_fw.inputs) > 4:\r\n            lstm_inputs.extend([lstm_fw.input[4], h_node.output[0], c_node.output[0]])\r\n\r\n        attr = {""direction"": ""bidirectional""}\r\n        for name in rnn_utils.onnx_rnn_attr_mapping[rnn_utils.ONNX_RNN_TYPE.LSTM]:\r\n            attr_val = lstm_fw.get_attr_value(name)\r\n            if attr_val:\r\n                attr[name] = attr_val\r\n\r\n        bi_lstm_node = g.make_node(""LSTM"", lstm_inputs, attr=attr, output_count=3)\r\n        all_nodes.append(bi_lstm_node)\r\n        logger.debug(""processing output nodes"")\r\n\r\n        to_remove = [lstm_fw.name, lstm_fw.input[1], lstm_fw.input[2], lstm_fw.input[3],\r\n                     lstm_bw.name, lstm_bw.input[1], lstm_bw.input[2], lstm_bw.input[3]]\r\n        rnn_utils.slice_birnn_for_original_rnn_consumers(\r\n            g, lstm_fw, lstm_bw, bi_lstm_node, 0, all_nodes, to_remove\r\n        )\r\n        rnn_utils.slice_birnn_for_original_rnn_consumers(\r\n            g, lstm_fw, lstm_bw, bi_lstm_node, 1, all_nodes, to_remove\r\n        )\r\n        rnn_utils.slice_birnn_for_original_rnn_consumers(\r\n            g, lstm_fw, lstm_bw, bi_lstm_node, 2, all_nodes, to_remove\r\n        )\r\n\r\n        lstm_bw_old_x = lstm_bw.input[0]\r\n        for n in to_remove:\r\n            g.remove_node(n)\r\n\r\n        rnn_utils.remove_reverse_in_bw_input(g, lstm_bw_old_x, rnn_utils.ONNX_RNN_TYPE.LSTM)\r\n\r\n    return g.get_nodes()\r\n\r\n\r\ndef process_ch_init_nodes(g, lstm_fw, lstm_bw, to_append):\r\n    h_node = rnn_utils.process_single_init_node(g, lstm_fw.input[5], lstm_bw.input[5], to_append)\r\n    c_node = rnn_utils.process_single_init_node(g, lstm_fw.input[6], lstm_bw.input[6], to_append)\r\n\r\n    return h_node, c_node\r\n\r\n\r\ndef rewrite_bidirectional_lstms(g, ops):\r\n    bi_lstms = rnn_utils.find_bidirectional_rnns(g, ops, rnn_utils.ONNX_RNN_TYPE.LSTM)\r\n\r\n    return process_bilstm(g, bi_lstms)\r\n'"
tf2onnx/rewriter/cond_rewriter.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.cond_rewriter\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nimport logging\nimport traceback\nfrom collections import OrderedDict\nfrom enum import Enum\nfrom tf2onnx import utils\n\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=missing-docstring,unused-argument,broad-except\n\nclass BranchType(Enum):\n    """"""Type of branch""""""\n    TRUE = 1\n    FALSE = 2\n    # TODO: sometimes, the branch depends on control inputs,\n    # so we just set it unknown\n    UNKNOWN = 3\n\n\nclass CondBranchContext:\n    """"""Context for each branch graph""""""\n\n    def __init__(self):\n        self.output = []\n        self.nodes = set()\n\n\nclass CondContext:\n    def __init__(self, cond_scope, pred_input, true_branch_context,\n                 false_branch_context, switchs, merges):\n        self.cond_scope = cond_scope  # name scope for this tf.cond\n        self.pred_input = pred_input  # condition input\n        self.true_branch_context = true_branch_context\n        self.false_branch_context = false_branch_context\n        self.switchs = set(switchs)\n        self.merges = merges  # list of merges in order\n\n\nclass CondRewriter:\n    def __init__(self, g):\n        self.g = g\n\n    def rewrite(self):\n        logger.debug(""enter cond pre rewrite"")\n        return self.run()\n\n    def run(self):\n        """"""tf.cond rewriter""""""\n        # parse tf.cond in topological sort order.\n        # NOTE: we assume the current graph is a DAG.\n        name_scope_merges = OrderedDict()\n        self.g.topological_sort(self.g.get_nodes())\n        all_nodes = self.g.get_nodes()\n        for n in all_nodes:\n            if self._is_merge(n):\n                name_scope = utils.tf_name_scope(n.name)\n                if name_scope not in name_scope_merges:\n                    name_scope_merges[name_scope] = []\n                name_scope_merges[name_scope].append(n)\n        # check if need rewrite\n        if not name_scope_merges.keys():\n            return all_nodes\n\n        for name_scope, merge_nodes in name_scope_merges.items():\n            cond_context = None\n            try:\n                pred_input, true_branch_context, false_branch_context, switchs = \\\n                    self._parse_cond(name_scope, merge_nodes)\n                cond_context = CondContext(\n                    name_scope,\n                    pred_input,\n                    true_branch_context,\n                    false_branch_context,\n                    switchs,\n                    merge_nodes\n                )\n            except Exception as ex:\n                tb = traceback.format_exc()\n                logger.warning(""tf.cond rewrite failed, due to exception: %s, details:%s"", ex, tb)\n                continue\n\n            self._cut_off_connection(cond_context)\n            self._create_if_node(cond_context)\n            # remove nodes in If branches explicitly\n            for n in list(cond_context.true_branch_context.nodes) + list(cond_context.false_branch_context.nodes):\n                self.g.remove_node(n.name)\n        logger.debug(""cond pre rewrite done"")\n\n        return self.g.get_nodes()\n\n    def _get_output_shape_dtype(self, cond_context):\n        output_shapes = []\n        output_dtypes = []\n        for i, _ in enumerate(cond_context.true_branch_context.output):\n            true_output = cond_context.true_branch_context.output[i]\n            false_output = cond_context.false_branch_context.output[i]\n            true_shape = self.g.get_shape(true_output)\n            utils.make_sure(true_shape is not None, ""Shape of {} is None"".format(true_output))\n            true_rank = len(true_shape)\n            true_dtype = self.g.get_dtype(true_output)\n            false_shape = self.g.get_shape(false_output)\n            utils.make_sure(false_shape is not None, ""Shape of {} is None"".format(false_output))\n            false_rank = len(false_shape)\n            false_dtype = self.g.get_dtype(false_output)\n            # just require rank is equal\n            if true_rank != false_rank:\n                raise RuntimeError(\n                    ""the rank of outputs {} and {} mismatch: {}, {}"".format(\n                        true_output,\n                        false_output,\n                        true_rank,\n                        false_rank\n                    )\n                )\n            if true_dtype != false_dtype:\n                raise RuntimeError(\n                    ""the dtype of outputs {} and {} mismatch: {}, {}"".format(\n                        true_output,\n                        false_output,\n                        true_dtype,\n                        false_dtype\n                    )\n                )\n            output_shapes.append(utils.create_vague_shape_like(true_shape))\n            output_dtypes.append(true_dtype)\n        return output_shapes, output_dtypes\n\n    def _create_if_node(self, cond_context):\n        output_shapes, output_dtypes = self._get_output_shape_dtype(cond_context)\n        if_node = self.g.make_node(\n            ""If"",\n            [cond_context.pred_input],\n            op_name_scope=cond_context.cond_scope,\n            outputs=[m.output[0] for m in cond_context.merges],\n            shapes=output_shapes,\n            dtypes=output_dtypes,\n            skip_conversion=False\n        )\n        logger.debug(""set graph for if branches"")\n        true_graph = utils.construct_graph_from_nodes(\n            self.g,\n            list(cond_context.true_branch_context.nodes),\n            cond_context.true_branch_context.output,\n            output_shapes,\n            output_dtypes\n        )\n        false_graph = utils.construct_graph_from_nodes(\n            self.g,\n            list(cond_context.false_branch_context.nodes),\n            cond_context.false_branch_context.output,\n            output_shapes,\n            output_dtypes\n        )\n        if_node.set_body_graph_as_attr(""then_branch"", true_graph)\n        if_node.set_body_graph_as_attr(""else_branch"", false_graph)\n        return if_node\n\n    def _cut_off_connection(self, cond_context):\n        """"""Cut off switchs and merges, all changes are based on the origin graph""""""\n        nodes_to_add = []\n        logger.debug(""cut off switch connection"")\n        # replace switch with identity node\n        for switch in cond_context.switchs:\n            shapes = switch.output_shapes\n            dtypes = switch.output_dtypes\n            self.g.remove_node(switch.name)\n            false_switch_id = self.g.make_node(\n                ""Identity"",\n                [switch.input[0]],\n                outputs=[switch.output[0]],\n                op_name_scope=cond_context.cond_scope,\n                shapes=[shapes[0]],\n                dtypes=[dtypes[0]],\n            )\n            cond_context.false_branch_context.nodes.add(false_switch_id)\n            true_switch_id = self.g.make_node(\n                ""Identity"",\n                [switch.input[0]],\n                outputs=[switch.output[1]],\n                op_name_scope=cond_context.cond_scope,\n                shapes=[shapes[1]],\n                dtypes=[dtypes[1]],\n            )\n            cond_context.true_branch_context.nodes.add(true_switch_id)\n            nodes_to_add.extend([false_switch_id, true_switch_id])\n        # replace merge with if node\n        logger.debug(""cut off merge connection"")\n        for n in cond_context.merges:\n            self.g.remove_node(n.name)\n\n    def _is_merge(self, node):\n        return node.type == ""Merge""\n\n    def _is_switch(self, node):\n        return node.type == ""Switch""\n\n    def _parse_cond(self, name_scope, merge_nodes):\n        """"""Parse condition subgraph for these merge nodes""""""\n        true_branch_context, false_branch_context, switchs = self._trace_back(name_scope, merge_nodes)\n        # find pred output from any switch\n        pred_input = list(switchs)[0].input[1]\n        return pred_input, true_branch_context, false_branch_context, switchs\n\n    def _trace_back(self, name_scope, merge_nodes):\n        """"""\n        Trace back to the switch from merge nodes and collect the nodes\n        in the true/false branchs of tf.cond respectively, some comments:\n        1. According to tf.cond implementation, We make the hypothesis\n           that one tf.cond cannot comprise successive Switch nodes.\n        2. Thank to construct_graph_from_nodes, in which Identity node\n           will be added to each output of subgraph, we needn\'t deal with the\n           branch with only one const node specially.\n\n        TODO: This implement doesn\'t depend on control inputs. For a price,\n           in the case that true and false branch both only contain a\n           const node, we will throw a Exception.\n        """"""\n        logger.debug(""trace back from [%s]"", "","".join(n.name for n in merge_nodes))\n        true_branch_context = CondBranchContext()\n        false_branch_context = CondBranchContext()\n        total_switchs = set()\n        for merge_node in merge_nodes:\n            true_branch_nodes, true_output, false_branch_nodes, false_output, switchs = \\\n                self._trace_back_from_one_merge(merge_node)\n            true_branch_context.nodes |= set(true_branch_nodes)\n            true_branch_context.output.append(true_output)\n            false_branch_context.nodes |= set(false_branch_nodes)\n            false_branch_context.output.append(false_output)\n            total_switchs |= switchs\n        return true_branch_context, false_branch_context, total_switchs\n\n    def _trace_back_from_one_merge(self, merge_node):\n        """"""Parse the ingredients (nodes and outputs)of true and false branch""""""\n        logger.debug(""trace back from %s"", merge_node.name)\n        true_branch_nodes = None\n        true_output = None\n        false_branch_nodes = None\n        false_output = None\n        merge_input_1 = merge_node.input[0]\n        merge_input_2 = merge_node.input[1]\n        switchs = set()\n\n        def stop_at_switch(node):\n            if self._is_switch(node):\n                switchs.add(node)\n                return False\n            return True\n\n        branch_nodes_1 = self.g.extract_sub_graph_nodes(\n            [merge_input_1],\n            stop_at_switch\n        )\n        branch_nodes_2 = self.g.extract_sub_graph_nodes(\n            [merge_input_2],\n            stop_at_switch\n        )\n        branch_type_1 = self._branch_type(merge_input_1, branch_nodes_1)\n        branch_type_2 = self._branch_type(merge_input_2, branch_nodes_2)\n        # all possible branch types: UU, UT, UF, TU, TF, FU, FT\n        if branch_type_1 == BranchType.UNKNOWN and branch_type_2 == BranchType.UNKNOWN:\n            raise ValueError(""Cannot handle the case both true and false branchs only \\\n                             contain const nodes for now."")\n        if branch_type_1 == branch_type_2:\n            raise ValueError(""true graph and false graph are intersected"")\n        if branch_type_1 == BranchType.TRUE or branch_type_2 == BranchType.FALSE:\n            true_branch_nodes = branch_nodes_1\n            true_output = merge_input_1\n            false_branch_nodes = branch_nodes_2\n            false_output = merge_input_2\n        else:\n            true_branch_nodes = branch_nodes_2\n            true_output = merge_input_2\n            false_branch_nodes = branch_nodes_1\n            false_output = merge_input_1\n        return true_branch_nodes, true_output, false_branch_nodes, false_output, switchs\n\n    def _branch_type(self, branch_output, nodes):\n        """"""Infer the branch type (true, false or unknown)""""""\n        branch = BranchType.UNKNOWN\n        # the branch is empty\n        if not nodes:\n            input_node = self.g.get_node_by_output(branch_output)\n            if self._is_switch(input_node):\n                if branch_output == input_node.output[0]:\n                    branch = BranchType.FALSE\n                else:\n                    branch = BranchType.TRUE\n            return branch\n        for node in nodes:\n            for inp in node.input:\n                input_node = self.g.get_node_by_output(inp)\n                if self._is_switch(input_node):\n                    if inp == input_node.output[0]:\n                        if branch == BranchType.TRUE:\n                            raise ValueError(""true and false graph intersect at {}"".format(node.name))\n                        branch = BranchType.FALSE\n                    else:\n                        if branch == BranchType.FALSE:\n                            raise ValueError(""true and false graph intersect at {}"".format(node.name))\n                        branch = BranchType.TRUE\n        if branch == BranchType.UNKNOWN:\n            logger.debug(\n                ""branch only contains const node: [%s]"",\n                "","".join(n.name for n in nodes)\n            )\n        return branch\n\n\ndef rewrite_cond(g, ops):\n    return CondRewriter(g).rewrite()\n'"
tf2onnx/rewriter/conv2d_with_add_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter - rewrite tensorflow subgraph to onnx conv2d op with BiasAdd\n""""""\nfrom tf2onnx import logging\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=missing-docstring\n\ndef rewrite_biasadd_with_conv2d(g, ops):\n    pattern = \\\n        OpTypePattern(\'BiasAdd\', name=\'biasadd\', inputs=[\n            OpTypePattern(\'Conv2D|Conv2DBackpropInput\', name=\'conv\', inputs=[\'*\', \'*\']), \'*\'])\n    matcher = GraphMatcher(pattern)\n    match_results = list(matcher.match_ops(ops))\n    for match in match_results:\n        biasadd = match.get_op(\'biasadd\')\n        conv = match.get_op(\'conv\')\n\n        #backup the conv and biasadd values\n        conv_type = conv.type\n        conv_input = conv.input\n        conv_attr = conv.attr\n        dtype = g.get_dtype(conv.output[0])\n        shape = g.get_shape(conv.output[0])\n        conv_name = biasadd.name\n        conv_output = biasadd.output\n        conv_inputs = [conv_input[0], conv_input[1], biasadd.input[1]]\n\n        # Remove the Conv and BiasAdd node\n        g.remove_node(conv.name)\n        g.remove_node(biasadd.name)\n\n        g.make_node(conv_type, conv_inputs, attr=conv_attr, name=conv_name, outputs=conv_output,\n                    shapes=[shape], dtypes=[dtype], skip_conversion=False)\n    return ops\n'"
tf2onnx/rewriter/conv2d_with_pad_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter - rewrite tensorflow subgraph to onnx condv2 op with pad\n""""""\n\nimport numpy as np\n\nfrom tf2onnx import handler, logging\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=missing-docstring\n\n\ndef rewrite_conv2d_with_pad(g, ops):\n    pattern = \\\n        OpTypePattern(""Conv2D"", name=""conv"", inputs=[\n            OpTypePattern(""Pad"", name=""pad""),\n            OpTypePattern(""*"")\n        ])\n    matcher = GraphMatcher(pattern)\n    match_results = list(matcher.match_ops(ops))\n    for match in match_results:\n        conv = match.get_op(""conv"")\n        pad = match.get_op(""pad"")\n        paddings = pad.inputs[1]\n\n        if not paddings.is_const():\n            continue\n        mode = pad.get_attr(""mode"")\n        if mode:\n            mode = mode.s.decode(""utf-8"").lower()\n        if mode not in [None, ""constant""] or len(pad.input) >= 3:\n            continue\n        # Conv2D already has a pad\n        if conv.get_attr(""padding"").s.decode(""utf-8"") == ""SAME"":\n            continue\n\n        logger.debug(""merge pad [%s] into conv [%s]"", pad.name, conv.name)\n        paddings_val = np.array(paddings.get_tensor_value())\n        # can\'t pad on batch or channel dimensions\n        if np.any(paddings_val[0]) or np.any(paddings_val[3]):\n            continue\n\n        paddings_val = paddings_val[1:3]\n        paddings_val = paddings_val.transpose().flatten()\n        g.replace_input(conv, conv.input[0], pad.input[0])\n        # convert Conv2D\n        conv.type = ""Conv""\n        func, _ = handler.tf_op.find_effective_op(""Conv2D"")\n        func(g, conv)\n        conv.skip_conversion = True\n        conv.set_attr(""auto_pad"", ""NOTSET"")\n        conv.set_attr(""pads"", paddings_val)\n    return ops\n'"
tf2onnx/rewriter/custom_rnn_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.custom_rnn_rewriter - custom rnn support\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport sys\nimport traceback\n\nfrom onnx import onnx_pb\nimport numpy as np\n\nfrom tf2onnx.graph_builder import GraphBuilder\nfrom tf2onnx.rewriter.loop_rewriter_base import LoopRewriterBase, Context\nfrom tf2onnx.rewriter.rnn_utils import REWRITER_RESULT, get_rnn_scope_name, parse_rnn_loop\nfrom tf2onnx import utils\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,broad-except,protected-access\n\n\nclass CustomRnnContext(Context):\n    def __init__(self):\n        super(CustomRnnContext, self).__init__()\n        self.rnn_scope = None\n        self.time_var = None\n        self.iteration_var = None\n\n\nclass CustomRnnRewriter(LoopRewriterBase):\n    def create_context(self):\n        return CustomRnnContext()\n\n    def run(self):\n        logger.debug(""enter custom rnn rewriter"")\n        return self.run_internal()\n\n    def need_rewrite(self, context):\n        context.rnn_scope = get_rnn_scope_name(context.while_context_scope)\n\n        res = parse_rnn_loop(self.g, context.loop_properties, context.rnn_scope,\n                             context.while_context_scope)\n        if not res:\n            logger.debug(""skip the loop due to parse_rnn_loop failed"")\n            return False\n\n        time_var, iteration_var = res\n        context.time_var = time_var\n        context.iteration_var = iteration_var\n        logger.debug(""time var %s - enter input id (%s) shape: %s, output (%s) shape: %s"", time_var.enter_name,\n                     time_var.enter_input_id, self.g.get_shape(time_var.enter_input_id),\n                     time_var.switch_true_identity_output.id, time_var.switch_true_identity_output.shape)\n\n        return True\n\n    def rewrite(self, context):\n        logger.debug(""enter rewrite function"")\n        try:\n            scan_props = context.loop_properties\n\n            state_inputs_initial_values = []\n            for state_input in scan_props.state_inputs_initial_values:\n                if self.g.opset == 8:\n                    nodes = self._adapt_scan_sequence_input_or_output(""input"", state_input, False)\n                    state_inputs_initial_values.append(nodes[-1].output[0])\n                else:  # since opset 9\n                    state_inputs_initial_values.append(state_input)\n\n            scan_inputs_initial_values = []\n            for scan_input in scan_props.scan_inputs_initial_values:\n                if self.g.opset == 8:\n                    nodes = self._adapt_scan_sequence_input_or_output(""input"", scan_input, False)\n                    scan_inputs_initial_values.append(nodes[-1].output[0])\n                else:  # since opset 9\n                    scan_inputs_initial_values.append(scan_input)\n\n            cell_g_info = context.cell_graph\n            scan_body_g = LoopRewriterBase.construct_graph_from_nodes(self.g, cell_g_info.nodes, cell_g_info.outputs)\n            for input_tensor_info in scan_props.state_inputs:\n                scan_body_g.add_graph_input(input_tensor_info.id, input_tensor_info.dtype, input_tensor_info.shape)\n\n            for input_tensor_info in scan_props.scan_inputs:\n                scan_body_g.add_graph_input(input_tensor_info.id, input_tensor_info.dtype, input_tensor_info.shape)\n\n            scan_node = self._create_scan_node(context, scan_props,\n                                               state_inputs_initial_values + scan_inputs_initial_values)\n            if not scan_node:\n                logger.error(""failed to create scan node during rewrite"")\n                return REWRITER_RESULT.FAIL\n\n            scan_node.set_body_graph_as_attr(""body"", scan_body_g)\n            self._connect_scan_with_output(context, scan_node)\n\n            return REWRITER_RESULT.OK\n\n        except Exception as ex:\n            tb = traceback.format_exc()\n            logger.error(""custom rnn rewrite failed, due to exception: %s, details:%s"", ex, tb)\n            return REWRITER_RESULT.FAIL\n\n    def _create_scan_node(self, context, scan_props, init_values):\n        logger.debug(""create scan node"")\n        # reuse original output connection id (e.g. Exit_XXX), so we don\'t need set shape.\n        loop_outputs_shapes = []\n        loop_outputs_dtypes = []\n        for tensor_value_info in scan_props.state_outputs_exits + scan_props.scan_outputs_exits:\n            if tensor_value_info.id:\n                # in opset 8, the first dim of scan output must be batch\n                if self.g.opset == 8:\n                    loop_outputs_shapes.append([1] + tensor_value_info.shape)\n                else:\n                    loop_outputs_shapes.append(tensor_value_info.shape)\n                loop_outputs_dtypes.append(tensor_value_info.dtype)\n                n = self.g.get_node_by_output(tensor_value_info.id)\n                self.g.remove_node(n.name)\n            else:\n                loop_outputs_shapes.append([-1])\n                loop_outputs_dtypes.append(None)\n\n        if self.g.opset == 8:\n            # here we did not give the sequence_length, because\n            # current batch size is 1, not original batch size\n            # original seq_length will be used by the loop body of Scan op.\n            scan_node = self.g.make_node(""Scan"", [""""] + init_values, op_name_scope=""custom_rnn_scan"",\n                                         attr={""num_scan_inputs"": len(scan_props.scan_inputs)},\n                                         output_count=len(scan_props.state_outputs + scan_props.scan_outputs),\n                                         shapes=loop_outputs_shapes, dtypes=loop_outputs_dtypes,\n                                         skip_conversion=False)\n        else:\n            scan_node = self.g.make_node(""Scan"", init_values, op_name_scope=""custom_rnn_scan"",\n                                         attr={""num_scan_inputs"": len(scan_props.scan_inputs)},\n                                         output_count=len(scan_props.state_outputs + scan_props.scan_outputs),\n                                         shapes=loop_outputs_shapes, dtypes=loop_outputs_dtypes,\n                                         skip_conversion=False)\n\n        return scan_node\n\n    def _connect_scan_with_output(self, context, scan_node):\n        logger.debug(""connect scan output with the graph"")\n\n        index = 0\n        for out_tensor_value_info in context.loop_properties.state_outputs_exits:\n            if out_tensor_value_info.id:\n                if self.g.opset == 8:\n                    nodes = self._adapt_scan_sequence_input_or_output(""state_output_reshape"",\n                                                                      scan_node.output[index], True)\n                    self.g.replace_all_inputs(self.g.get_nodes(), out_tensor_value_info.id, nodes[-1].output[0])\n                else:  # since opset 9\n                    self.g.replace_all_inputs(self.g.get_nodes(), out_tensor_value_info.id, scan_node.output[index])\n            index += 1\n\n        for out_tensor_value_info in context.loop_properties.scan_outputs_exits:\n            if out_tensor_value_info.id:\n                if self.g.opset == 8:\n                    nodes = self._adapt_scan_sequence_input_or_output(""scan_output_reshape"",\n                                                                      scan_node.output[index], True)\n                    self.g.replace_all_inputs(self.g.get_nodes(), out_tensor_value_info.id, nodes[-1].output[0])\n                else:  # since opset 9\n                    self.g.replace_all_inputs(self.g.get_nodes(), out_tensor_value_info.id, scan_node.output[index])\n            index += 1\n\n    def _adapt_scan_sequence_input_or_output(self, target_name, input_id, handle_output=False):\n        nodes_to_add = []\n        shape_node = self.g.make_node(""Shape"", [input_id])\n        nodes_to_add.append(shape_node)\n        inferred_shape = self.g.get_shape(input_id)\n        if handle_output is True:\n            # handle output:\n            # if required dim values don\'t contain more than one -1,\n            # just use a const for Reshape\'s shape input.\n            if inferred_shape is not None and inferred_shape[1:].count(-1) <= 1:\n                new_shape_node = self.g.make_const(utils.make_name(target_name + ""_target_shape""),\n                                                   np.array(inferred_shape[1:], dtype=np.int64))\n                nodes_to_add.append(new_shape_node)\n            else:\n                # otherwise, get the dim dynamically, e.g. remove the fake batch size (e.g.1)\n                # from [1, time, real-batch, ...]\n                origin_shape_node = self.g.make_node(""Cast"", [shape_node.output[0]],\n                                                     {""to"": onnx_pb.TensorProto.FLOAT})\n                nodes_to_add.append(origin_shape_node)\n\n                attr = {""axes"": [0], ""starts"": [1], ""ends"": [sys.maxsize]}\n                inputs_map = {""data"": origin_shape_node.output[0], **attr}\n                sliced_shape_node = GraphBuilder(self.g).make_slice(inputs_map)\n                nodes_to_add.append(self.g.get_node_by_output(sliced_shape_node))\n\n                new_shape_node = self.g.make_node(""Cast"", [sliced_shape_node],\n                                                  {""to"": onnx_pb.TensorProto.INT64})\n                nodes_to_add.append(new_shape_node)\n\n            new_shape = inferred_shape[1:]\n        else:\n            # handle input:\n            if inferred_shape is not None and inferred_shape.count(-1) <= 1:\n                new_shape_node = self.g.make_const(utils.make_name(target_name + ""_target_shape""),\n                                                   np.array([1] + inferred_shape, dtype=np.int64))\n                nodes_to_add.append(new_shape_node)\n            else:\n                # add a fake batch size : 1\n                fake_batch_size_node = self.g.make_const(utils.make_name(target_name + ""_target_shape""),\n                                                         np.array([1], dtype=np.int64))\n                nodes_to_add.append(fake_batch_size_node)\n                new_shape_node = self.g.make_node(""Concat"",\n                                                  [fake_batch_size_node.output[0], shape_node.output[0]],\n                                                  attr={""axis"": 0})\n                nodes_to_add.append(new_shape_node)\n            new_shape = [1] + inferred_shape\n\n        reshape_node = self.g.make_node(""Reshape"", [input_id, new_shape_node.output[0]],\n                                        shapes=[new_shape],\n                                        dtypes=[self.g.get_dtype(input_id)],\n                                        op_name_scope=target_name)\n        nodes_to_add.append(reshape_node)\n        logger.debug(""create Reshape for scan output %s, with output shape %s"",\n                     reshape_node.output[0], new_shape)\n        return nodes_to_add\n'"
tf2onnx/rewriter/dropout_rewriter.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter - rewrite tensorflow subgraph to onnx dropout op\n""""""\n\nfrom tf2onnx import utils\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\n\n# pylint: disable=missing-docstring\n\n\ndef rewrite_dropout(g, ops):\n    patterns = [\n        OpTypePattern(\'Mul\', name=\'outputs\', inputs=[\n            OpTypePattern(\'RealDiv\', name=""input2""),\n            OpTypePattern(\'Floor\', inputs=[\n                OpTypePattern(\'Add\', inputs=[\n                    OpTypePattern(None, name=""input3""),\n                    OpTypePattern(\'RandomUniform|RandomUniformLike\'),\n                ])\n            ]),\n        ]),\n        OpTypePattern(""Mul"", name=""outputs"", inputs=[\n            OpTypePattern(""Mul"", name=""input2""),\n            OpTypePattern(""Cast"", inputs=[\n                OpTypePattern(""GreaterEqual"", inputs=[\n                    OpTypePattern(""RandomUniform|RandomUniformLike""),\n                    OpTypePattern(None, name=""input3"")\n                ])\n            ])\n        ]),\n        # pattern for tf-2.0 tf.nn.dropout()\n        OpTypePattern(""Mul"", name=""outputs"", inputs=[\n            OpTypePattern(""Cast"", inputs=[\n                OpTypePattern(""GreaterEqual"", inputs=[\n                    OpTypePattern(""RandomUniform|RandomUniformLike""),\n                    OpTypePattern(None, name=""input3"")\n                ])\n            ]),\n            OpTypePattern(""Mul"", name=""input2""),\n        ]),\n    ]\n    for pattern in patterns:\n        matcher = GraphMatcher(pattern, allow_reorder=True)\n        match_results = list(matcher.match_ops(ops))\n        for match in match_results:\n            inputs2 = match.get_op(\'input2\')\n            if inputs2.inputs[0].type == ""RealDiv"":\n                data = inputs2.input[1]\n            else:\n                data = inputs2.input[0]\n            outputs = match.get_op(\'outputs\')\n            op_name = utils.make_name(""Dropout"")\n            out_name = utils.port_name(op_name)\n            new_node = g.make_node(\n                ""Dropout"",\n                [data],\n                outputs=[out_name],\n                name=op_name,\n                attr={""ratio"": 1.0},\n                shapes=[g.get_shape(inputs2.input[0])],\n                dtypes=[g.get_dtype(inputs2.input[0])]\n            )\n            g.replace_all_inputs(ops, outputs.output[0], new_node.output[0])\n            g.safe_remove_nodes(match.get_nodes())\n\n    # remove dropout if its ratio is 1.0\n    for node in g.get_nodes():\n        if node.type == ""Dropout"" and node.get_attr(""ratio"").f == 1.0:\n            g.replace_all_inputs(g.get_nodes(), node.output[0], node.input[0])\n            g.remove_node(node.name)\n\n    return ops\n'"
tf2onnx/rewriter/eye_rewriter.py,4,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.eye_rewriter - supports tf.eye\n""""""\n\nfrom onnx import onnx_pb\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\n# pylint: disable=invalid-name,unused-argument,missing-docstring, unused-variable\n\n\ndef rewrite_eye(g, ops):\n    # schema of eye is eye(num_rows, num_columns=None), if num_columns not specified then it\'s equal to num_rows\n    # tf.eye is implemented by a sub_graph which contains op ""MatrixDiag"" or ""MatrixSetDiag"" while\n    # these two ops are un-supported directly in onnx\n    # but onnx op EyeLike can be used to map the sub_graph\n    # ""rewrite_eye"" supports tf.eye(non_const) and tf.eye(non_const1, non_const2).\n    # tf.eye(const) and tf.eye(const1, const2) are not supported in this rewriter\n\n    # ConstantOfShape in opset 9 is used, so if opset less than 9 then do nothing\n    if g.opset < 9:\n        return g.get_nodes()\n\n    pattern1 = \\\n        OpTypePattern(""MatrixDiag"", name=""output_eye_matrix"", inputs=[\n            OpTypePattern(""Fill"", inputs=[\n                OpTypePattern(""Const"", name=""fill_value""),\n                OpTypePattern(""ConcatV2"", inputs=[\n                    ""*"",\n                    ""*"",\n                    OpTypePattern(""Pack"", inputs=[\n                        OpTypePattern(""Minimum|Cast"", name=""min_or_cast"")\n                    ])\n                ])\n            ])\n        ])\n    pattern2 = \\\n        OpTypePattern(""MatrixSetDiag"", name=""output_eye_matrix"", inputs=[\n            OpTypePattern(""Fill""),\n            OpTypePattern(""Fill"", inputs=[\n                OpTypePattern(""Const"", name=""fill_value""),\n                OpTypePattern(""ConcatV2"", inputs=[\n                    ""*"",\n                    ""*"",\n                    OpTypePattern(""Pack"", inputs=[\n                        OpTypePattern(""Minimum|Cast"", name=""min_or_cast"")\n                    ])\n                ])\n            ])\n        ])\n    pattern3 = \\\n        OpTypePattern(""MatrixDiag"", name=""output_eye_matrix"", inputs=[\n            OpTypePattern(""Fill"", inputs=[\n                OpTypePattern(""ConcatV2"", inputs=[\n                    ""*"",\n                    OpTypePattern(""ExpandDims"", inputs=[\n                        OpTypePattern(""Minimum|Cast"", name=""min_or_cast""),\n                        ""*""\n                    ]),\n                    ""*"",\n                ]),\n                OpTypePattern(""Const"", name=""fill_value""),\n            ])\n        ])\n    pattern4 = \\\n        OpTypePattern(""MatrixSetDiag"", name=""output_eye_matrix"", inputs=[\n            OpTypePattern(""Fill""),\n            OpTypePattern(""Fill"", inputs=[\n                OpTypePattern(""ConcatV2"", inputs=[\n                    ""*"",\n                    OpTypePattern(""ExpandDims"", inputs=[\n                        OpTypePattern(""Minimum|Cast"", name=""min_or_cast""),\n                        ""*""\n                    ]),\n                    ""*"",\n                ]),\n                OpTypePattern(""Const"", name=""fill_value""),\n            ]),\n        ])\n    pattern5 = \\\n        OpTypePattern(""MatrixDiagV3"", name=""output_eye_matrix"", inputs=[\n            OpTypePattern(""Fill"", inputs=[\n                OpTypePattern(""ConcatV2"", inputs=[\n                    ""*"",\n                    OpTypePattern(""ExpandDims"", inputs=[\n                        OpTypePattern(""Minimum|Cast"", name=""min_or_cast""),\n                        ""*""\n                    ]),\n                    ""*"",\n                ]),\n                OpTypePattern(""Const"", name=""fill_value""),\n            ]),\n            ""*"", ""*"", ""*"", ""*"",\n        ])\n    pattern6 = \\\n        OpTypePattern(""MatrixSetDiagV3"", name=""output_eye_matrix"", inputs=[\n            OpTypePattern(""Fill""),\n            OpTypePattern(""Fill"", inputs=[\n                OpTypePattern(""ConcatV2"", inputs=[\n                    ""*"",\n                    OpTypePattern(""ExpandDims"", inputs=[\n                        OpTypePattern(""Minimum|Cast"", name=""min_or_cast""),\n                        ""*""\n                    ]),\n                    ""*"",\n                ]),\n                OpTypePattern(""Const"", name=""fill_value""),\n            ]), ""*""\n        ])\n\n    for pattern in [pattern1, pattern2, pattern3, pattern4, pattern5, pattern6]:\n        matcher = GraphMatcher(pattern, allow_reorder=True)\n        match_results = list(matcher.match_ops(ops))\n        for match_result in match_results:\n            if match_result.get_op(""fill_value"").get_tensor_value() != 1:\n                continue\n\n            min_or_cast = match_result.get_op(""min_or_cast"")\n            if min_or_cast.type == ""Minimum"":\n                min_node = min_or_cast\n            elif min_or_cast.type == ""Cast"" and min_or_cast.inputs[0].type == ""Minimum"":\n                min_node = min_or_cast.inputs[0]\n            else:\n                continue\n\n            num_rows = min_node.inputs[0]\n            num_columns = min_node.inputs[1]\n\n            old_output = match_result.get_op(""output_eye_matrix"")\n            output_dtypes = [g.get_dtype(old_output.output[0])]\n            output_shapes = [g.get_shape(old_output.output[0])]\n            g.remove_node(old_output.name)\n\n            # onnx op ""EyeLike"" need a 2D tensor, so generate it\n            num_rows = g.make_node(""Unsqueeze"", num_rows.output, attr={""axes"": [0]})\n            num_columns = g.make_node(""Unsqueeze"", num_columns.output, attr={""axes"": [0]})\n            matrix_shape = g.make_node(""Concat"", [num_rows.output[0], num_columns.output[0]], attr={""axis"": 0})\n            # cast nodes added for ""ConstantOfShape"" in ONNX only accepts int64 data.\n            matrix_shape_int64 = g.make_node(""Cast"", matrix_shape.output, attr={""to"": onnx_pb.TensorProto.INT64})\n            zero_matrix = g.make_node(""ConstantOfShape"", matrix_shape_int64.output)\n\n            g.make_node(""EyeLike"", zero_matrix.output, attr={""dtype"": output_dtypes[0]},\n                        name=old_output.name, shapes=output_shapes, dtypes=output_dtypes)\n\n    return g.get_nodes()\n'"
tf2onnx/rewriter/flatten_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter - rewrite tensorflow subgraph to onnx flatten op\n""""""\n\nimport numpy as np\n\nfrom tf2onnx import utils\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\n\n# pylint: disable=missing-docstring\n\n\ndef rewrite_flatten(g, ops):\n    pattern_fixed_shape_input = \\\n        OpTypePattern(\'Reshape\', name=\'reshape\', inputs=[\n            OpTypePattern(""*"", name=""input""),\n            OpTypePattern(\'Pack\', name=""pack"", inputs=[\n                OpTypePattern(\'StridedSlice\', name=""slice"", inputs=[\n                    ""*"", ""*"", ""*"", ""*"",\n                ]),\n                ""*"",\n            ]),\n        ])\n    pattern_non_fixed_shape_input = \\\n        OpTypePattern(\'Reshape\', name=\'reshape\', inputs=[\n            OpTypePattern(""*"", name=""input""),\n            OpTypePattern(\'Pack\', name=""pack"", inputs=[\n                OpTypePattern(\'StridedSlice\', name=""slice"", inputs=[\n                    OpTypePattern(\'Shape\', inputs=[\n                        OpTypePattern(""*"", name=""input2"")\n                    ]),\n                    ""*"", ""*"", ""*"",\n                ]),\n                ""*"",\n            ]),\n        ])\n    matcher = GraphMatcher(pattern_fixed_shape_input)\n    match_results_1 = list(matcher.match_ops(ops))\n\n    matcher = GraphMatcher(pattern_non_fixed_shape_input)\n    match_results_2 = list(matcher.match_ops(ops))\n\n    match_results = [(match_results_1, True), (match_results_2, False)]\n    for match_results, check_fixed_input_shape in match_results:\n        for match in match_results:\n            input_node = match.get_op(\'input\')\n            reshape_node = match.get_op(\'reshape\')\n            pack_node = match.get_op(\'pack\')\n            slice_node = match.get_op(\'slice\')\n            need_rewrite = pack_node.inputs[1].is_const() and pack_node.inputs[1].get_tensor_value() == -1\n            if not need_rewrite:\n                continue\n\n            input_shape = g.get_shape(reshape_node.input[0])\n            need_rewrite = input_shape is not None\n            if not need_rewrite:\n                continue\n\n            if check_fixed_input_shape:\n                need_rewrite = slice_node.inputs[0].is_const() and \\\n                               np.array_equal(list(input_shape), list(slice_node.inputs[0].get_tensor_value()))\n                if not need_rewrite:\n                    continue\n\n            begin = slice_node.inputs[1].get_tensor_value(as_list=False)\n            end = slice_node.inputs[2].get_tensor_value(as_list=False)\n            strides = slice_node.inputs[3].get_tensor_value(as_list=False)\n            need_rewrite = np.array_equal(begin, [0]) and len(end) == 1 and \\\n                           np.array_equal(strides, [1]) and end[0] - begin[0] == 1\n            if not need_rewrite:\n                continue\n\n            to_remove = [n for n in match.get_nodes() if n != input_node]\n            safe = g.safe_to_remove_nodes(to_remove)\n\n            # Ok if reshape_node is not safe. Will make it safe later.\n            if len(to_remove) - len(safe) > 1:\n                continue\n\n            op_name = utils.make_name(""Flatten"")\n            out_name = utils.port_name(op_name)\n            g.make_node(""Flatten"", [reshape_node.input[0]], outputs=[out_name], name=op_name)\n\n            last_dim = input_shape[-1]\n            sec_last_dim = input_shape[-2]\n            new_dim = None\n            if last_dim > 0 and sec_last_dim > 0:\n                new_dim = last_dim * sec_last_dim\n            else:\n                new_dim = -1\n\n            g.set_shape(out_name, input_shape[:-2] + [new_dim])\n            g.replace_all_inputs(ops, reshape_node.output[0], out_name)\n            for n in to_remove:\n                g.remove_node(n.name)\n\n    return ops\n'"
tf2onnx/rewriter/gemm_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewrite - rewrite tensorflow subgraph to onnx gemm op\n""""""\nimport logging\nfrom onnx import onnx_pb\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\n\n# pylint: disable=missing-docstring\n\ndef rewrite_gemm(g, ops):\n    if g.opset <= 6:\n        return ops\n\n    # pattern0: alpha*A*B + beta*C\n    pattern0 = \\\n        OpTypePattern(\'Add|AddV2\', name=\'add\', inputs=[\n            OpTypePattern(\'Mul\', name=\'mul1\', inputs=[\n                OpTypePattern(\'Const\', name=\'alpha\'),\n                OpTypePattern(\'MatMul\', name=\'matmul\')\n            ]),\n            OpTypePattern(\'Mul\', name=\'mul2\', inputs=[\n                OpTypePattern(\'Const\', name=\'beta\'),\n                OpTypePattern(\'*\', name=\'C\')\n            ])\n        ])\n\n    # pattern1: alpha*A*B + C\n    pattern1 = \\\n        OpTypePattern(\'Add|AddV2\', name=\'add\', inputs=[\n            OpTypePattern(\'Mul\', name=\'mul1\', inputs=[\n                OpTypePattern(\'MatMul\', name=\'matmul\'),\n                OpTypePattern(\'Const\', name=\'alpha\')\n            ]),\n            OpTypePattern(\'*\', name=\'C\'),\n        ])\n\n    # pattern2: A*B + beta*C\n    pattern2 = \\\n        OpTypePattern(\'Add|AddV2\', name=\'add\', inputs=[\n            OpTypePattern(\'MatMul\', name=\'matmul\'),\n            OpTypePattern(\'Mul\', name=\'mul2\', inputs=[\n                OpTypePattern(\'Const\', name=\'beta\'),\n                OpTypePattern(\'*\', name=\'C\')\n            ])\n        ])\n\n    # pattern3: A*B + C\n    pattern3 = \\\n        OpTypePattern(\'Add|AddV2\', name=\'add\', inputs=[\n            OpTypePattern(\'MatMul\', name=\'matmul\'),\n            OpTypePattern(\'*\', name=\'C\'),\n        ])\n\n    pattern_list = [pattern0, pattern1, pattern2, pattern3]\n\n    for pattern in pattern_list:\n        matcher = GraphMatcher(pattern, allow_reorder=True)\n        match_results = list(matcher.match_ops(ops))\n        if match_results:\n            for match in match_results:\n                matmul_node = match.get_op(""matmul"")\n\n                if g.get_dtype(matmul_node.input[0]) != onnx_pb.TensorProto.FLOAT:\n                    logging.warning(u""For now, onnxruntime only support float32 type for Gemm rewriter"")\n                    continue\n\n                attr, is_valid = get_gemm_attr(match)\n                if not is_valid:\n                    continue\n\n                add_node = match.get_op(\'add\')\n                input_c_node = match.get_op(""C"")\n                a_edge_name = matmul_node.input[0]\n                b_edge_name = matmul_node.input[1]\n                c_edge_name = input_c_node.output[0]\n\n                a_mul_b_shape = g.get_shape(matmul_node.output[0])\n                c_shape = g.get_shape(c_edge_name)\n                if c_shape is None: continue\n                if a_mul_b_shape is None: continue\n                if -1 in c_shape + a_mul_b_shape: continue\n                compatible = True\n                for i in range(1, len(c_shape) + 1):\n                    if c_shape[-i] not in [1, a_mul_b_shape[-i]]:\n                        compatible = False\n                if not compatible: continue\n\n                gemm = g.make_node(""Gemm"", inputs=[a_edge_name, b_edge_name, c_edge_name],\n                                   attr=attr,\n                                   shapes=[g.get_shape(add_node.output[0])],\n                                   dtypes=[g.get_dtype(add_node.output[0])], op_name_scope=matmul_node.name)\n\n                ops.append(gemm)\n                g.replace_all_inputs(ops, add_node.output[0], gemm.output[0])\n                to_delete = [add_node, matmul_node]\n                g.safe_remove_nodes(to_delete)\n    return ops\n\n\ndef get_gemm_attr(match):\n    attr = {}\n    for arg in [""alpha"", ""beta""]:\n        arg_op = match.get_op(arg)\n        if arg_op is not None:\n            match_args = arg_op.get_tensor_value()\n            if isinstance(match_args, list):\n                if len(match_args) != 1:\n                    return attr, False\n                match_args = match_args[0]\n            attr[arg] = match_args\n    for arg in [""matmul""]:\n        arg_op = match.get_op(arg)\n        if arg_op is not None:\n            match_args = arg_op.attr\n            if isinstance(match_args, dict):\n                keys = list(match_args.keys())\n                if \'transpose_a\' not in keys and \'transpose_b\' not in keys:\n                    return attr, False\n                match_args_a = match_args[\'transpose_a\'].i\n                attr[\'transA\'] = match_args_a\n                match_args_b = match_args[\'transpose_b\'].i\n                attr[\'transB\'] = match_args_b\n    return attr, True\n'"
tf2onnx/rewriter/gru_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.gru_rewriter\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nimport numpy as np\nfrom tf2onnx import utils\nfrom tf2onnx.rewriter.rnn_utils import RNNUnitType, get_weights_from_const_node\n\nfrom tf2onnx.rewriter.unit_rnn_rewriter_base import UnitRnnRewriterBase\n\n# pylint: disable=invalid-name,unused-argument,missing-docstring\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass GRUUnitRewriter(UnitRnnRewriterBase):\n    def __init__(self, g):\n        super(GRUUnitRewriter, self).__init__(g)\n        self.gru_cell_type = None\n        self.state_variable_handlers = [\n            {""state"": (self._state_variable_finder, self._connect_gru_state_to_graph)}\n        ]\n\n    def run(self):\n        logger.debug(""enter gru rewriter"")\n        return super(GRUUnitRewriter, self).run()\n\n    def find_cell(self, context):\n        gru_cell_types = [RNNUnitType.GRUCell, RNNUnitType.GRUBlockCell, RNNUnitType.CudnnCompatibleGRUCell]\n        for cell_type in gru_cell_types:\n            cell_match = self._match_cell(context, cell_type)\n            if cell_match:\n                self.gru_cell_type = cell_type\n                logger.debug(""parsing unit is %s"", cell_type)\n                return cell_match\n        logger.debug(""cannot parse unit"")\n        return None\n\n    def get_weight_and_bias(self, context):\n        match = context.cell_match\n\n        gate_kernel = get_weights_from_const_node(self.g, match.get_op(""gate_kernel""))\n        gate_bias = get_weights_from_const_node(self.g, match.get_op(""gate_bias""))\n        res = {\n            ""gate_kernel"": gate_kernel,\n            ""gate_bias"": gate_bias\n        }\n\n        # differ on memory gate:\n        # GRUCell: h\'_t = tanh(concat(x_t, r_t .* h_t-1) * W + b)\n        # CudnnCompatibleGRUCell: h\'_t = tanh(x_t * W_x + b_x + r_t .* (h_t-1 * W_h + b_h))\n        if self.gru_cell_type == RNNUnitType.CudnnCompatibleGRUCell:\n            hidden_state_kernel = get_weights_from_const_node(\n                self.g, match.get_op(""hidden_state_kernel"")\n            )\n            hidden_state_bias = get_weights_from_const_node(\n                self.g, match.get_op(""hidden_state_bias"")\n            )\n            hidden_input_kernel = get_weights_from_const_node(\n                self.g, match.get_op(""hidden_input_kernel"")\n            )\n            hidden_input_bias = get_weights_from_const_node(\n                self.g, match.get_op(""hidden_input_bias"")\n            )\n            if not all(val is not None for val in [\n                    hidden_state_kernel, hidden_state_bias,\n                    hidden_input_kernel, hidden_input_bias\n            ]):\n                logger.debug(""rnn weights check failed, skip"")\n                return None\n            hidden_kernel = np.concatenate([hidden_input_kernel, hidden_state_kernel])\n            # apply the linear transformation before multiplying by the output of reset gate\n            context.attributes[""linear_before_reset""] = 1\n            res[""hidden_kernel""] = hidden_kernel\n            res[""hidden_bias""] = hidden_input_bias\n            # recurrence bias for hidden gate\n            res[""Rb_h""] = hidden_state_bias\n        elif self.gru_cell_type in [RNNUnitType.GRUCell, RNNUnitType.GRUBlockCell]:\n            hidden_kernel = get_weights_from_const_node(self.g, match.get_op(""hidden_kernel""))\n            hidden_bias = get_weights_from_const_node(self.g, match.get_op(""hidden_bias""))\n            res[""hidden_kernel""] = hidden_kernel\n            res[""hidden_bias""] = hidden_bias\n\n        if not all(val is not None for val in res.values()):\n            logger.debug(""rnn weights check failed, skip"")\n            return None\n\n        logger.debug(""find needed weights"")\n        return res\n\n    def _state_variable_finder(self, context):\n        if self.gru_cell_type in [\n                RNNUnitType.GRUCell,\n                RNNUnitType.CudnnCompatibleGRUCell\n        ]:\n            gru_cell = context.cell_match\n            return self._find_state_variable_with_select(\n                context,\n                gru_cell.get_op(""cell_output"").output[0],\n                [gru_cell.get_op(""cell_inputs"")]\n            )\n        if self.gru_cell_type == RNNUnitType.GRUBlockCell:\n            gru_block_cell = context.cell_match.get_op(""gru_block_cell"")\n            return self._find_state_variable_with_select(\n                context,\n                gru_block_cell.output[3],\n                [gru_block_cell]\n            )\n        return None\n\n    def parse_attributes(self, context):\n        # in tf, only activation of hidden gate is optional, input and update gate always use sigmoid\n        match = context.cell_match\n        activations = [""Sigmoid"", ""Tanh""]\n        if self.gru_cell_type == RNNUnitType.GRUCell:\n            activation_op = match.get_op(""optional_activation"")\n            activations = [""Sigmoid"", activation_op.type]\n        context.attributes[""activations""] = activations\n        return True\n\n    def is_valid(self, context):\n        # except for ct, ht or ct_ht, there are at most 2 state variables\n        other_state_variables_num = len(context.loop_properties.state_variables) - \\\n            len(context.state_variables)\n        if other_state_variables_num > 2:\n            logger.debug(""found %d other state variables"", other_state_variables_num)\n            return False\n\n        # output should be no more than 1\n        outputs = context.loop_properties.scan_outputs_exits\n        if len(outputs) > 1:\n            logger.debug(""found %d outputs for gru: %s"", len(outputs), outputs)\n            return False\n        return True\n\n    def process_weights_and_bias(self, context):\n        """"""\n        why split the data in this way should refer to code of tensorflow GRU cell and official document of ONNX GRU\n        """"""\n        weights = context.weights\n        # from code of tensorflow GRU cell, it can be known that shape of hidden_kernel(or candidate_kernel)\n        # is (input_size+hidden_unit, hidden_unit)\n        hidden_size = weights[""hidden_kernel""].shape[1]\n        input_size = weights[""hidden_kernel""].shape[0] - hidden_size\n        weight_dtype = weights[""hidden_kernel""].dtype\n        bias_dtype = weights[""hidden_bias""].dtype\n        # below code will use same notation as ONNX document\n        # z means update gate, r means reset gate, h means hidden gate;\n        # at this time weights of gate include input and state, will split it next\n        r_kernel, z_kernel = np.split(weights[""gate_kernel""], [hidden_size], axis=1)\n        h_kernel = weights[""hidden_kernel""]\n        r_bias, z_bias = np.split(weights[""gate_bias""], [hidden_size], axis=0)\n        h_bias = weights[""hidden_bias""]\n        # ONNX GRU split weights of input and state, so have to split *_kernel\n        input_r_kernel, state_r_kernel = np.split(r_kernel, [input_size], axis=0)\n        input_z_kernel, state_z_kernel = np.split(z_kernel, [input_size], axis=0)\n        input_h_kernel, state_h_kernel = np.split(h_kernel, [input_size], axis=0)\n        W_zrh = np.concatenate((input_z_kernel, input_r_kernel, input_h_kernel), axis=1)\n        R_zrh = np.concatenate((state_z_kernel, state_r_kernel, state_h_kernel), axis=1)\n        # transpose weight matrix\n        W_zrh = np.transpose(np.expand_dims(W_zrh, axis=0), axes=(0, 2, 1))\n        R_zrh = np.transpose(np.expand_dims(R_zrh, axis=0), axes=(0, 2, 1))\n        W_zrh = W_zrh.astype(weight_dtype)\n        R_zrh = R_zrh.astype(weight_dtype)\n        assert W_zrh.shape == (1, 3*hidden_size, input_size)\n        assert R_zrh.shape == (1, 3*hidden_size, hidden_size)\n        Wb_zrh = np.concatenate((z_bias, r_bias, h_bias), axis=0)\n        # if tf doesn\'t provide bias for state, use 0\n        zero = np.zeros_like(z_bias)\n        # Rb_h is set in CudnnCompatibleGRUCell\n        Rb_h = weights[""Rb_h""] if ""Rb_h"" in weights else zero\n        Rb_zrh = np.concatenate((zero, zero, Rb_h), axis=0)\n        B_zrh = np.concatenate((Wb_zrh, Rb_zrh), axis=0)\n        B_zrh = np.expand_dims(B_zrh, axis=0)\n        B_zrh = B_zrh.astype(bias_dtype)\n        assert B_zrh.shape == (1, 6*hidden_size)\n        # create const ONNX node\n        w_name = utils.make_name(""W"")\n        w_node = self.g.make_const(w_name, W_zrh, skip_conversion=True)\n\n        r_name = utils.make_name(""R"")\n        r_node = self.g.make_const(r_name, R_zrh, skip_conversion=True)\n\n        b_name = utils.make_name(""B"")\n        b_node = self.g.make_const(b_name, B_zrh, skip_conversion=True)\n\n        context.input_size = input_size\n        context.hidden_size = hidden_size\n        context.onnx_input_ids[""W""] = w_node.output[0]\n        context.onnx_input_ids[""R""] = r_node.output[0]\n        context.onnx_input_ids[""B""] = b_node.output[0]\n\n    def process_var_init_nodes(self, context):\n        assert ""state"" in context.state_variables.keys()\n        initializer_input_id = context.state_variables[""state""].enter_input_id\n        node = self.g.get_node_by_output(initializer_input_id)\n        if node.is_const():\n            val = node.get_tensor_value(as_list=False)\n            initial_name = utils.make_name(""Const"")\n            new_val = np.expand_dims(val, axis=0)\n            const_node = self.g.make_const(initial_name, new_val)\n            context.onnx_input_ids[""initial_state""] = const_node.output[0]\n            return\n        squeeze_node = self.g.make_node(""Unsqueeze"", [initializer_input_id], attr={""axes"": [0]})\n        to_replace = [n for n in self.g.get_nodes() if n != squeeze_node]\n        self.g.replace_all_inputs(to_replace, initializer_input_id, squeeze_node.output[0])\n        context.onnx_input_ids[""initial_state""] = squeeze_node.output[0]\n\n    def create_rnn_node(self, context):\n        # specify if the RNN is forward, reverse, or bidirectional.\n        # Must be one of forward (default), reverse, or bidirectional.\n        # Here we won\'t mark bidirectional/reverse, we will have another rewriter running after this one,\n        # which will based on patterns to combine a forward GRU and a backward GRU into a bidirectional one.\n        num_direction = 1\n        # todo: input_forget\n        context.attributes[""direction""] = ""forward""\n        context.attributes[""hidden_size""] = context.hidden_size\n        inputs = context.onnx_input_ids\n        # sequence length is optional\n        seq_len_input = utils.ONNX_EMPTY_INPUT\n        if inputs[""sequence_lens""]:\n            seq_len_input = inputs[""sequence_lens""]\n        gru_inputs = [\n            inputs[""X""], inputs[""W""], inputs[""R""], inputs[""B""],\n            seq_len_input, inputs[""initial_state""]]\n        x_shape = self.g.get_shape(gru_inputs[0])\n        x_seq_length = x_shape[0]\n        x_batch_size = x_shape[1]\n        out_dtype = self.g.get_dtype(gru_inputs[0])\n        gru_node = self.g.make_node(""GRU"", gru_inputs, attr=context.attributes, output_count=2,\n                                    shapes=[[x_seq_length, num_direction, x_batch_size, context.hidden_size],\n                                            [num_direction, x_batch_size, context.hidden_size]],\n                                    dtypes=[out_dtype, out_dtype], op_name_scope=context.rnn_scope)\n        return gru_node\n\n    def _connect_gru_state_to_graph(self, context):\n        # in tf, state output shape is: [batch, hidden]\n        # in onnx, output shape is: [number_directions, batch, hidden]\n        exit_output_id = context.state_variables[""state""].exit_output.id\n        if not exit_output_id:\n            logger.debug(""no one consume state variable"")\n            return\n        output_id = context.rnn_node.output[1]\n        gru_state_shape = self.g.get_shape(output_id)\n        output_shape = [gru_state_shape[1], gru_state_shape[2]]\n        squeeze_node = self.g.make_node(""Squeeze"", [output_id], attr={""axes"": [0]},\n                                        shapes=[output_shape], dtypes=[self.g.get_dtype(output_id)])\n\n        self.g.replace_all_inputs(self.g.get_nodes(), exit_output_id, squeeze_node.output[0])\n'"
tf2onnx/rewriter/leakyrelu_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter - rewrite tensorflow subgraph to onnx leakyrelu op\n""""""\n\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\n\n# pylint: disable=missing-docstring\n\n\ndef rewrite_leakyrelu(g, ops):\n    if g.opset < 6:\n        return ops\n\n    pattern = \\\n        OpTypePattern(\'Maximum\', name=\'max\', inputs=[\n            OpTypePattern(\'Mul\', name=\'mul\', inputs=[\n                OpTypePattern(\'Const\', name=\'alpha\'),\n                OpTypePattern(\'*\', name=\'mul_input\'),\n            ]),\n            OpTypePattern(\'*\', name=\'max_input\'),\n        ])\n\n    matcher = GraphMatcher(pattern, allow_reorder=True)\n    match_results = list(matcher.match_ops(ops))\n    for match in match_results:\n        max_node = match.get_op(\'max\')\n        max_input_node = match.get_op(\'max_input\')\n        mul_node = match.get_op(""mul"")\n        mul_input_node = match.get_op(\'mul_input\')\n\n        max_input_edge_name = _find_edge_name_between_nodes(max_input_node, max_node)\n        mul_input_edge_name = _find_edge_name_between_nodes(mul_input_node, mul_node)\n        if max_input_edge_name == mul_input_edge_name:\n            alpha = match.get_op(""alpha"").get_tensor_value()\n            if alpha >= 1:\n                continue\n            leakyrelu = g.make_node(""LeakyRelu"", inputs=[max_input_edge_name], attr={""alpha"": alpha},\n                                    shapes=[g.get_shape(max_node.output[0])], dtypes=[g.get_dtype(max_node.output[0])])\n            ops.append(leakyrelu)\n            g.replace_all_inputs(ops, max_node.output[0], leakyrelu.output[0])\n            to_delete = [max_node, mul_node]\n            g.safe_remove_nodes(to_delete)\n\n    return ops\n\n\ndef _find_edge_name_between_nodes(src_node, consumer_node):\n    # find the first edge connection between two nodes.\n    for consumer_end in consumer_node.input:\n        for src_end in src_node.output:\n            if consumer_end == src_end:\n                return consumer_end\n    return None\n'"
tf2onnx/rewriter/loop_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.loop_rewriter - generic loop support\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport sys\nimport traceback\n\nfrom onnx import TensorProto\nimport numpy as np\n\nfrom tf2onnx.rewriter.loop_rewriter_base import LoopRewriterBase, Context\nfrom tf2onnx.rewriter.rnn_utils import REWRITER_RESULT\nfrom tf2onnx import utils\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,broad-except,protected-access\n\n\nclass LoopRewriter(LoopRewriterBase):\n\n    def create_context(self):\n        return Context()\n\n    def run(self):\n        logger.debug(""enter loop rewriter"")\n        return self.run_internal()\n\n    def need_rewrite(self, context):\n        return True\n\n    def rewrite(self, context):\n        logger.debug(""enter rewrite function"")\n        loop_node = None\n        try:\n            loop_props = context.loop_properties\n            cell_g_info = context.cell_graph\n            cond_g_info = context.cond_graph\n\n            # create a dummy loop to calculate the init condition\n            init_cond_output = self._create_subgraph_initial_cond(cond_g_info)\n\n            ## create Loop body graph with existing nodes\n\n            body_nodes = set(cell_g_info.nodes + cond_g_info.nodes)\n            body_outputs = cond_g_info.outputs + cell_g_info.outputs\n            for out_tensor_value_info in body_outputs:\n                shape = out_tensor_value_info.shape\n                utils.make_sure(\n                    shape is not None,\n                    ""Conversion of Loop requries output shape [{}] exists"".format(out_tensor_value_info.id)\n                )\n                out_tensor_value_info.shape = utils.create_vague_shape_like(shape)\n\n            loop_body_g = LoopRewriterBase.construct_graph_from_nodes(self.g, body_nodes, body_outputs)\n\n            # create loop body graph inputs\n            loop_body_g.add_graph_input(utils.make_name(""i""), TensorProto.INT64, ())\n            loop_body_g.add_graph_input(utils.make_name(""cond""), TensorProto.BOOL, ())\n            for i, tensor_value_info in enumerate(loop_props.state_inputs):\n                input_name = tensor_value_info.id\n                if input_name is None:\n                    # if the variable is not used in the body graph, then we created a fake one,\n                    # the same type and shape as its corresponding output.\n                    out_tensor_value_info = loop_props.state_outputs[i]\n                    dtype = out_tensor_value_info.dtype\n                    shape = out_tensor_value_info.shape\n                    input_name = utils.make_name(""unused_state_input_"")\n                else:\n                    dtype = tensor_value_info.dtype\n                    shape = tensor_value_info.shape\n\n                loop_body_g.add_graph_input(input_name, dtype, utils.create_vague_shape_like(shape))\n\n            for input_ta in loop_props.tensor_array_inputs:\n                # Loop does not have scan inputs, so we use Gather to get data for each iteration.\n                index_node = loop_body_g.make_node(""Unsqueeze"", [input_ta.index_input_id], attr={""axes"": [0]})\n                gather_node = loop_body_g.make_node(""Gather"", [input_ta.data_input_id, index_node.output[0]])\n                data_node = loop_body_g.make_node(""Squeeze"", [gather_node.output[0]], attr={""axes"": [0]})\n                loop_body_g.replace_all_inputs(loop_body_g.get_nodes(), input_ta.consumer.id, data_node.output[0])\n\n            ## create Loop node\n            loop_node = self._create_loop_node(context, loop_props, init_cond_output)\n            if not loop_node:\n                logger.error(""failed to create loop node during rewrite"")\n                return REWRITER_RESULT.FAIL\n            loop_node.set_body_graph_as_attr(""body"", loop_body_g)\n\n            logger.debug(""rewrite successfully"")\n            return REWRITER_RESULT.OK\n\n        except Exception as ex:\n            tb = traceback.format_exc()\n            logger.error(""loop rewrite failed, due to exception: %s, details:%s"", ex, tb)\n            return REWRITER_RESULT.FAIL\n\n    def _create_subgraph_initial_cond(self, cond_graph):\n        """"""Create subgraph to calculate initial cond.""""""\n        # copy condition subgraph to parent graph\n        copied_nodes = []\n        name_scope = utils.make_name(""copy"")\n        for node in cond_graph.nodes:\n            new_name = ""{}/{}"".format(name_scope, node.name)\n            new_outputs = [""{}/{}"".format(name_scope, out) for out in node.output]\n            # some inputs are out of cond_graph.nodes, keep them intact\n            new_inputs = []\n            for inp in node.input:\n                if self.g.get_node_by_output(inp) in cond_graph.nodes:\n                    new_inputs.append(""{}/{}"".format(name_scope, inp))\n                else:\n                    new_inputs.append(inp)\n\n            new_node = self.g.make_node(\n                node.type, new_inputs, outputs=new_outputs,\n                attr=node.attr, name=new_name,\n                shapes=node.output_shapes, dtypes=node.output_dtypes,\n                skip_conversion=node.skip_conversion, infer_shape_dtype=False\n            )\n            body_graphs = node.graph.contained_graphs.pop(node.name, None)\n            if body_graphs:\n                for attr_name, body_graph in body_graphs.items():\n                    body_graph.parent_graph = g\n                    new_node.set_body_graph_as_attr(attr_name, body_graph)\n            copied_nodes.append(new_node)\n\n        # replace all inputs of condition graph by initializer (enter_input)\n        for loop_var in cond_graph.dependent_vars:\n            self.g.replace_all_inputs(\n                copied_nodes,\n                loop_var.next_iteration_input.id,\n                loop_var.enter_input_id\n            )\n        init_cond_output = ""{}/{}"".format(name_scope, cond_graph.outputs[0].id)\n        self.g.set_dtype(init_cond_output, cond_graph.outputs[0].dtype)\n        self.g.set_shape(init_cond_output, cond_graph.outputs[0].shape)\n        return init_cond_output\n\n    def _create_loop_node(self, context, loop_props, init_cond_output):\n        loop_outputs = []\n        loop_output_shapes = []\n        loop_output_dtypes = []\n        for tensor_value_info in loop_props.state_outputs_exits + loop_props.scan_outputs_exits:\n            if tensor_value_info.id:\n                loop_outputs.append(tensor_value_info.id)\n                loop_output_shapes.append(tensor_value_info.shape)\n                loop_output_dtypes.append(tensor_value_info.dtype)\n                n = self.g.get_node_by_output(tensor_value_info.id)\n                self.g.remove_node(n.name)\n            else:\n                loop_outputs.append(utils.make_name(""unused_loop_output_""))\n                loop_output_shapes.append([-1])\n                loop_output_dtypes.append(None)\n\n        # trip count and cond are not used, giving them values just because bug\n        # (https://github.com/Microsoft/onnxruntime/issues/255) of onnxruntime.\n        trip_cnt = self.g.make_const(utils.make_name(""trip_count""), np.array(sys.maxsize, dtype=np.int64))\n        loop_node = self.g.make_node(""Loop"", [trip_cnt.output[0]] + [init_cond_output] +\n                                     loop_props.state_inputs_initial_values,  # ONNX Loop support state inputs only\n                                     outputs=loop_outputs, op_name_scope=""generic_loop"",\n                                     shapes=loop_output_shapes, dtypes=loop_output_dtypes,\n                                     skip_conversion=False)\n\n        return loop_node\n'"
tf2onnx/rewriter/loop_rewriter_base.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.loop_rewriter_base\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nfrom collections import OrderedDict\nfrom tf2onnx import utils\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\nfrom tf2onnx.utils import is_tf_loopcond_op, is_tf_tensor_array_op\nfrom tf2onnx.utils import is_tf_tensor_array_gather_op, is_tf_tensor_array_write_op\nfrom tf2onnx.rewriter.rnn_utils import REWRITER_RESULT\nfrom tf2onnx.utils import TensorValueInfo\n\n\nlogger = logging.getLogger(__name__)\nINVALID_INPUT_ID = utils.make_name(""invalid_input_id"")\n\n# todo(pengwa) remove protected-access with changes to Graph/Node later.\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,protected-access\n\n\nclass Context(object):\n    def __init__(self):\n        self.while_context_scope = None\n        self.loop_properties = LoopProperties()\n        self.loop_cond = None\n\n        self.cell_graph = None  # GraphInfo of cell graph\n        self.cond_graph = None  # GraphInfo of condition graph\n\n\nclass GraphInfo(object):\n    def __init__(self, ops, inputs, outputs):\n        self.nodes = ops\n        self.inputs = inputs  # list of TensorValueInfo in order\n        self.outputs = outputs  # list of TensorValueInfo in order\n        self.dependent_vars = None\n\n\nclass LoopProperties(object):\n    def __init__(self):\n        # use enter name as key, they are initial inputs.\n        # we don\'t use enter_input_id because it might be\n        # used as initial input for more than one Enter nodes.\n        self.state_variables = OrderedDict()\n        self.scan_variables = OrderedDict()\n\n        self.tensor_array_inputs = []  # list of type InputTensorArray\n\n    def add_variable(self, var):\n        utils.make_sure(var.enter_name not in self.scan_variables,\n                        ""variable %s already exists as scan variable."", var.enter_name)\n        utils.make_sure(var.enter_name not in self.state_variables,\n                        ""variable %s already exists as state variable."", var.enter_name)\n        if not var.is_tensor_array:\n            self.state_variables[var.enter_name] = var\n        else:\n            self.scan_variables[var.enter_name] = var\n\n    def get_variables(self, checker):\n        if not checker:\n            return self.all_variables.values()\n        return [v for v in self.all_variables.values() if checker(v)]\n\n    @property\n    def all_variables(self):\n        items = self.state_variables.copy()\n        items.update(self.scan_variables)\n        return items\n\n    # state inputs and outputs are in pairs, even though some outputs are not depending on corresponding input,\n    # we leave the input id be None.\n    @property\n    def state_inputs(self):\n        return [v.switch_true_identity_output for v in self.state_variables.values()]\n\n    @property\n    def state_inputs_initial_values(self):\n        return [v.enter_input_id for v in self.state_variables.values()]\n\n    @property\n    def state_outputs(self):\n        return [v.next_iteration_input for v in self.state_variables.values()]\n\n    @property\n    def state_outputs_exits(self):\n        return [v.exit_output for v in self.state_variables.values()]\n\n    # scan output (e.g. tensor array) won\'t be used by next iteration calculation\n    @property\n    def scan_outputs(self):\n        return [v.next_iteration_input for v in self.scan_variables.values()]\n\n    @property\n    def scan_outputs_exits(self):\n        return [v.exit_output for v in self.scan_variables.values()]\n\n    # treat input tensor array as scan inputs\n    def add_scan_input(self, input_tensor_array):\n        self.tensor_array_inputs.append(input_tensor_array)\n\n    # usually it is called TensorArrayReadV3\n    @property\n    def scan_inputs(self):\n        return [i.consumer for i in self.tensor_array_inputs]\n\n    @property\n    def scan_inputs_initial_values(self):\n        return [i.data_input_id for i in self.tensor_array_inputs]\n\nclass LoopVariable(object):\n    """"""In TensorFlow loop, all loop variables are listed both in iteration body graph\'s inputs, and outputs.\n       Loop (state variable 1, state variable 2) {\n           # do the calculation\n           # updated state variable 1 not necessarily only depends on state variable 1, it might depend\n           # on 0, 1 or more state variables.\n           # So if it depends on 0 state variable, then switch_true_identity_output.id is None. For this case,\n           # during conversion, a fake input for ONNX Loop body graph is created, but not consumed by any node.\n           return (updated) state variable 1, (updated) state variable 2, scan variable 1, scan variable 2\n       }\n\n       Here we take the perspective of body graph\'s outputs:\n           1. start from the iteration body graph\'s output (e.g. next_iteration_input.id)\n           2. find body graph generating it (those node between NextIteration and Switch)\n           3. find the variable initial value (e.g. enter_input_id)\n           4. check whether it is a tensor array\n           5. the body graph output might go to next iteration as corresponding input\n              (e.g. switch_true_identity_output.id).\n    """"""\n    def __init__(self, enter_name, enter_input_id, next_iteration_input_id,\n                 switch_true_identity_output_id, exit_output_id, is_tensor_array, ta_index_id, g):\n        self.enter_name = enter_name\n        self.enter_input_id = enter_input_id\n\n        # the output of iteration body graph for this variable\n        # should not be None\n        utils.make_sure(next_iteration_input_id, ""next_iteration_input_id should not be None"")\n        self.next_iteration_input = TensorValueInfo(next_iteration_input_id, g)\n\n        # the starting point of iteration body graph,\n        # might be None when this variable value (either initial value or last iteration output value)\n        # is not consumed iteration body graph nodes.\n        self.switch_true_identity_output = TensorValueInfo(switch_true_identity_output_id, g)\n\n        # the switch_false branch is ended with Exit, which is a boundary for the loop,\n        # might be None when no consumers for the variable output.\n        self.exit_output = TensorValueInfo(exit_output_id, g)\n\n        # only applicable for tensor array variable\n        self.is_tensor_array = is_tensor_array\n        # todo: need check ta\'s index variable is a scalar starting from 1, and increase by 1 each iteration.\n        # then we can be sure this is equivalent to scan output behavior.\n        self.ta_index_id = ta_index_id\n\n\nclass InputTensorArray(object):\n    def __init__(self, data_input_id, index_input_id, consumer_id, g):\n        self.index_input_id = index_input_id\n        self.data_input_id = data_input_id\n\n        # tensor array is unstacked before being used in loop, consumer_id is the node\n        # (in the iteration body graph) consuming one of the element of tensor array.\n        self.consumer = TensorValueInfo(consumer_id, g)\n\n\nclass LoopRewriterBase(object):\n    def __init__(self, g):\n        self.g = g\n        self.ta_read_input_pattern = \\\n            OpTypePattern(""TensorArrayReadV3"", name=""ta_read"", inputs=[\n                OpTypePattern(""Enter"", name=""ta_enter"", inputs=[\n                    OpTypePattern(""TensorArrayV3"")\n                ]),\n                OpTypePattern(""Identity"", name=""ta_index""),\n                OpTypePattern(""Enter"", name=""ta_scatter_enter"", inputs=[\n                    OpTypePattern(""TensorArrayScatterV3"", name=""ta_input_scatter"")\n                ]),\n            ])\n\n    def create_context(self):\n        return Context()\n\n    def need_rewrite(self, context):\n        return False\n\n    def rewrite(self, context):\n        return REWRITER_RESULT.FAIL\n\n    def run_internal(self):\n        loopcond_ops = []\n        for op in self.g.get_nodes():\n            if is_tf_loopcond_op(op):\n                loopcond_ops.append(op)\n\n        # self.g.get_nodes may change inside this loop so that we parse all LoopCond first\n        for op in loopcond_ops:\n            logger.debug(""======================\\n handling loop cond node called %s"", op.name)\n            context = self.create_context()\n            context.loop_cond = op\n\n            self._check_in_read_only_mode(context)\n\n            if self.need_rewrite(context):\n                # cut off connection between cell/cond graphs and useless nodes like Merge, NextIteration.\n                self._cut_off_connection_for_cell(context)\n                context.cell_graph = self._crop_loop_body_sub_graph(context)\n                context.cond_graph = self._crop_loop_condition_sub_graph(context)\n\n                _result = self.rewrite(context)\n                if _result == REWRITER_RESULT.OK:\n                    logger.debug(""rewrite successfully"")\n                elif _result == REWRITER_RESULT.SKIP:\n                    logger.debug(""rewrite skipped for LoopCond called %s"", op.name)\n                    continue\n                elif _result == REWRITER_RESULT.FAIL:\n                    raise ValueError(""rewrite failed, so just fast fail it"")\n\n        if self.g.outputs:\n            # clean the graph based on output names.\n            self.g.delete_unused_nodes(self.g.outputs)\n        return self.g.get_nodes()\n\n    def _check_in_read_only_mode(self, context):\n        self._parse_loop_variables(context)\n        self._parse_input_ta(context)\n\n    def _parse_loop_variables(self, context):\n        loop_cond_op = context.loop_cond\n        parts = loop_cond_op.name.split(\'/\')\n        context.while_context_scope = \'/\'.join(parts[0:-1]) + ""/""\n        logger.debug(""found while loop scope %s"", context.while_context_scope)\n\n        switch_nodes = self.g.find_output_consumers(loop_cond_op.output[0])\n        for s in switch_nodes:\n            if s.type != \'Switch\':\n                raise ValueError(""LoopCond\'s output node should be followed with a Switch node"")\n\n            loop_var = self._get_loop_var_from_switch(s)\n            context.loop_properties.add_variable(loop_var)\n\n    def _parse_input_ta(self, context):\n        graph_inputs = [v.switch_true_identity_output.id for v in context.loop_properties.all_variables.values()\n                        if v.switch_true_identity_output.id]\n        matcher = GraphMatcher(self.ta_read_input_pattern, allow_reorder=False)\n        match_results = matcher.match_ops(self.g.get_nodes())\n        match_results = [r for r in match_results if r.get_op(""ta_index"").output[0] in graph_inputs]\n        for match in match_results:\n            ta_input_scatter = match.get_op(""ta_input_scatter"")\n            # the 3rd input of scatter is the value\n            data_input_id = ta_input_scatter.input[2]\n            ta_read_node = match.get_op(""ta_read"")\n\n            # todo: need check ta\'s index variable is a scalar starting from 1, and increase by 1 each iteration.\n            # then we can be sure this is equivalent to scan input behavior.\n            index_input_id = ta_read_node.input[1]\n            unstacked_ta_consumer = match.get_op(""ta_read"").output[0]\n            ta = InputTensorArray(data_input_id, index_input_id, unstacked_ta_consumer, self.g)\n            context.loop_properties.add_scan_input(ta)\n\n    def _crop_loop_body_sub_graph(self, context):\n        # according to input and output, find the body graph\n        loop_props = context.loop_properties\n        inputs = loop_props.state_inputs + loop_props.scan_inputs\n        input_ids = [input_tensor_value_info.id for input_tensor_value_info in inputs]\n\n        outputs = loop_props.state_outputs + loop_props.scan_outputs\n        output_ids = [out_tensor_value_info.id for out_tensor_value_info in outputs]\n        ops, enter_nodes, _ = self.find_subgraph(set(input_ids), set(output_ids), self.g, merge_as_end=False)\n\n        for enter_node in enter_nodes:\n            # connect Enter\'s output to Enter\'s input\n            self.g.replace_all_inputs(ops, enter_node.output[0], enter_node.input[0])\n\n        return GraphInfo(ops, inputs, outputs)\n\n    def _crop_loop_condition_sub_graph(self, context):\n        input_ids = []\n        output_ids = [context.loop_cond.input[0]]\n        outputs = [TensorValueInfo(o, self.g) for o in output_ids]\n        ops, enter_nodes, merge_nodes = self.find_subgraph(set(input_ids), set(output_ids), self.g, merge_as_end=True)\n\n        for enter_node in enter_nodes:\n            # connect Enter\'s output to Enter\'s input\n            self.g.replace_all_inputs(ops, enter_node.output[0], enter_node.input[0])\n\n        dependent_vars = []\n        for merge_node in merge_nodes:\n            enter_node = [n for n in merge_node.inputs if n.type == ""Enter""][0]\n            loop_var = context.loop_properties.all_variables[enter_node.name]\n\n            # cut off connection between condition graph and Merge node.\n            # replace condition graph\'s inputs to be cell graph\'s outputs, because we want condition graph\n            # to consumer cell graph outputs.\n            non_switch_consumers = [n for n in self.g.find_output_consumers(merge_node.output[0]) if n.type != ""Switch""]\n            self.g.replace_all_inputs(non_switch_consumers, merge_node.output[0],\n                                      loop_var.next_iteration_input.id)\n            dependent_vars.append(loop_var)\n\n        # cut off connection between condition graph and LoopCond node.\n        self.g.replace_all_inputs([context.loop_cond], context.loop_cond.output[0], INVALID_INPUT_ID)\n\n        graph_info = GraphInfo(ops, [], outputs)\n        graph_info.dependent_vars = dependent_vars\n        return graph_info\n\n    def _cut_off_connection_for_cell(self, context):\n        for val in context.loop_properties.all_variables.values():\n            if val.switch_true_identity_output.id:\n                # remove the node to cut off a starting node of the cell (e.g. loop body).\n                n = self.g.get_node_by_output(val.switch_true_identity_output.id)\n                self.g.remove_node(n.name)\n\n            if val.is_tensor_array:\n                # connect NextIteration to an invalid node, to cut off an ending node of the cell.\n                ta_write_nodes = [n for n in self.g.get_nodes() if is_tf_tensor_array_write_op(n)]\n                self.g.replace_all_inputs(ta_write_nodes, val.next_iteration_input.id, INVALID_INPUT_ID)\n            else:\n                # connect NextIteration to an invalid node, to cut off an ending node of the cell.\n                next_iter_nodes = [n for n in self.g.get_nodes() if n.type == ""NextIteration""]\n                self.g.replace_all_inputs(next_iter_nodes, val.next_iteration_input.id, INVALID_INPUT_ID)\n\n        for scan_input in context.loop_properties.scan_inputs:\n            # remove the node to cut off connection between scan_input and the cell.\n            self.g.remove_node(self.g.get_node_by_output(scan_input.id).name)\n\n    def _get_loop_var_from_switch(self, switch_node):\n        if switch_node.type != \'Switch\':\n            logger.error(""not a switch node, skip"")\n            return None\n\n        # the first input is data\n        merge_node = switch_node.inputs[0]\n        if merge_node.type != ""Merge"":\n            logger.error(""switch node does not has Merge as its first input"")\n            return None\n\n        # find the output_true consumers\n        switch_consumers = self.g.find_output_consumers(switch_node.output[1])\n        switch_true_consumer_cnt = len(switch_consumers)\n        if switch_true_consumer_cnt == 0:\n            switch_true_identity_output = None\n        elif switch_true_consumer_cnt == 1:\n            if switch_consumers[0].type == ""Identity"":\n                switch_true_identity_output = switch_consumers[0].output[0]\n            else:\n                # using grappler there is not necessarily an identity behind switch\n                switch_true_identity_output = switch_node.output[1]\n        else:\n            raise ValueError(""switch_true "" + switch_node.name + "" has unexpected count of consumers:"",\n                             [n.name for n in switch_consumers])\n\n        target_node_input_id = None\n        enter_node = [n for n in merge_node.inputs if n.type == \'Enter\'][0]\n        target_node_input_id = enter_node.input[0]\n        logger.debug(""a Switch >> Merge >> Enter is found called %s"", enter_node.inputs[0].name)\n\n        next_iteration_node = [n for n in merge_node.inputs if n.type == \'NextIteration\'][0]\n        last_iteration_output_id = next_iteration_node.input[0]\n\n        # find the output_false consumers to see whether there is consumer for this var\n        switch_false_consumers = self.g.find_output_consumers(switch_node.output[0])\n        false_consumer_count = len(switch_false_consumers)\n        exit_output_id = None\n        if false_consumer_count == 1:\n            exit_node = switch_false_consumers[0]\n            if exit_node.type != ""Exit"":\n                raise ValueError(""switch false branch is followed by non-Exit"")\n            exit_output_id = exit_node.output[0]\n        elif false_consumer_count == 0:\n            # sometime, the variable output won\'t be used in the new iteration as input.\n            exit_output_id = None\n        else:\n            raise ValueError(""unexpected number of switch false consumers"")\n\n        is_ta = False\n        ta_index_id = None\n        if is_tf_tensor_array_op(self.g.get_node_by_output(target_node_input_id)):\n            is_ta = True\n\n            ta_write_node = self.g.get_node_by_output(last_iteration_output_id)\n            utils.make_sure(is_tf_tensor_array_write_op(ta_write_node), ""ta nextiteration is not following ta write op"")\n            last_iteration_output_id = ta_write_node.input[2]\n            ta_index_id = ta_write_node.input[1]\n\n            # here we parse patterns generated by\n            # ta.write(), then ta.stack(), because this is the most frequent usage pattern.\n            if exit_output_id:\n                exit_consumers = self.g.find_output_consumers(exit_output_id)\n                ta_gather_node = [n for n in exit_consumers if is_tf_tensor_array_gather_op(n)][0]\n\n                # update exit output id, treat the gather output as ta\'s output\n                exit_output_id = ta_gather_node.output[0]\n\n        loop_var = LoopVariable(enter_node.name, target_node_input_id, last_iteration_output_id,\n                                switch_true_identity_output, exit_output_id, is_ta, ta_index_id, self.g)\n\n        return loop_var\n\n    @staticmethod\n    def find_subgraph(input_ids, output_ids, g, merge_as_end=False):\n        logger.debug(""input ids %s "", input_ids)\n        logger.debug(""output ids %s "", output_ids)\n\n        enter_nodes = set()\n        merge_nodes = set()\n\n        def find_input_boundary(node):\n            if node.type == ""Enter"":\n                enter_nodes.add(node)\n                logger.debug(""terminate the input search at %s"", node.name)\n                return False\n\n            if merge_as_end is True and node.type == ""Merge"":\n                merge_nodes.add(node)\n                logger.debug(""terminate the input search at %s"", node.name)\n                return False\n\n            if node.is_const():\n                logger.debug(""terminate search at const node %s"", node.name)\n                return False\n\n            for o in node.output:\n                if o in input_ids:\n                    return False\n            return True\n\n        nodes = g.extract_sub_graph_nodes(output_ids, input_checker=find_input_boundary)\n        return nodes, enter_nodes, merge_nodes\n\n    @staticmethod\n    def construct_graph_from_nodes(parent_g, nodes, outputs):\n        return utils.construct_graph_from_nodes(\n            parent_g,\n            nodes,\n            [out.id for out in outputs],\n            [out.shape for out in outputs],\n            [out.dtype for out in outputs]\n        )\n'"
tf2onnx/rewriter/lstm_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.lstm_rewriter\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nimport numpy as np\nfrom tf2onnx import utils\nfrom tf2onnx.graph_builder import GraphBuilder\nfrom tf2onnx.rewriter.rnn_utils import RNNUnitType, get_weights_from_const_node\nfrom tf2onnx.utils import is_tf_concat_op, is_tf_slice_op\n\nfrom tf2onnx.rewriter.lstm_rewriter_base import LSTMRewriterBase\n\n# pylint: disable=invalid-name,unused-argument,missing-docstring\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass LSTMRewriter(LSTMRewriterBase):\n    def __init__(self, g):\n        super(LSTMRewriter, self).__init__(g)\n        self.lstm_cell_type = None\n        self.num_lstm_layers = 0\n\n    def run(self):\n        logger.debug(""enter lstm rewriter"")\n        return super(LSTMRewriter, self).run()\n\n    def find_cell(self, context):\n        lstm_cell_types = [RNNUnitType.LSTMCell, RNNUnitType.LSTMBlockCell]\n        for cell_type in lstm_cell_types:\n            cell_match = self._match_cell(context, cell_type)\n            if cell_match and len(cell_match) >= 1:\n                self.num_lstm_layers = len(cell_match)\n                logger.debug(""number of LSTM layers: %s"", self.num_lstm_layers)\n                for i in range(self.num_lstm_layers):\n                    self.state_variable_handlers.append({\n                        ""ct"" + str(i): (self._ct_variable_finder, self._connect_lstm_yc_to_graph, i),\n                        ""ht"" + str(i): (self._ht_variable_finder, self._connect_lstm_yh_to_graph, i)\n                    })\n                    self.state_variable_handlers.append({\n                        ""ct_ht"" + str(i): (self._ct_ht_shared_variable_finder, self._connect_lstm_ych_to_graph, i)\n                    })\n                logger.debug(""parsing unit is %s, num layers is %d"", cell_type, self.num_lstm_layers)\n            if cell_match:\n                self.lstm_cell_type = cell_type\n                logger.debug(""parsing unit is %s"", cell_type)\n                return cell_match\n        logger.debug(""cannot parse unit"")\n        return None\n\n    def get_weight_and_bias(self, context):\n        weight_and_bias = list()\n        for i in range(self.num_lstm_layers):\n            if self.lstm_cell_type == RNNUnitType.LSTMCell:\n                weight_and_bias.append(self._get_weight_and_bias_for_lstm_cell(context, i))\n            if self.lstm_cell_type == RNNUnitType.LSTMBlockCell:\n                weight_and_bias.append(self._get_weight_and_bias_for_lstmblock_cell(context, i))\n        return weight_and_bias\n\n    def _get_weight_and_bias_for_lstmblock_cell(self, context, i):\n        cell_match = context.cell_match[i]\n\n        w_node = cell_match.get_op(""cell_kernel"")\n        w = get_weights_from_const_node(self.g, w_node)\n        if w is None:\n            logger.warning(""Cannot find weight, SKIP"")\n            return None\n\n        b_node = cell_match.get_op(""cell_bias"")\n        b = get_weights_from_const_node(self.g, b_node)\n        if b is None or b.shape[0] != w.shape[1]:\n            logger.warning(""cell_kernel and cell_bias\'s dimension doesn\'t match, SKIP"")\n            return None\n\n        lstm_block_cell = cell_match.get_op(""lstm_block_cell"")\n        ft_bias_val = np.array(\n            lstm_block_cell.get_attr(""forget_bias"").f,\n            dtype=b.dtype\n        )\n\n        return {\n            ""weight"": w,\n            ""bias"": b,\n            ""ft_bias"": ft_bias_val\n        }\n\n    def _get_weight_and_bias_for_lstm_cell(self, context, i):\n        match = context.cell_match[i]\n\n        w_e = match.get_op(""cell_kernel"")\n        w = get_weights_from_const_node(self.g, w_e)\n        if w is None or w.size == 0:\n            return None\n\n        # check https://www.tensorflow.org/versions/r1.8/api_docs/cc/class/tensorflow/ops/bias-add\n        # for bias_add data format\n        bias_add = match.get_op(""bias_add"")\n        if bias_add is not None and bias_add.data_format != ""NHWC"":\n            logger.debug(""BiasAdd data_format is not NHWC, SKIP"")\n            return None\n\n        b_e = match.get_op(""cell_bias"")\n        if b_e is None:\n            b = np.array([0 for i in range(len(w[0]))]).astype(w.dtype)\n        else:\n            b = get_weights_from_const_node(self.g, b_e)\n            if b is None or b.shape[0] != w.shape[1]:\n                logger.warning(""cell_kernel and cell_bias\'s dimensions does not match, skip"")\n                return None\n\n        ft_bias_node = match.get_op(""ft_bias"")\n        ft_bias = get_weights_from_const_node(self.g, ft_bias_node)\n        if ft_bias is None:\n            return None\n\n        if not b.dtype == ft_bias.dtype:\n            return None\n\n        return {\n            ""weight"": w,\n            ""bias"": b,\n            ""ft_bias"": ft_bias\n        }\n\n    def parse_attributes(self, context):\n        if self.lstm_cell_type == RNNUnitType.LSTMBlockCell:\n            lstm_block_cell = context.cell_match[0].get_op(""lstm_block_cell"")\n            clip = float(lstm_block_cell.get_attr(""cell_clip"").f)\n            # current LSTM op cannot handle clip\n            if clip > 0:\n                return False\n\n            use_peephole = lstm_block_cell.get_attr_value(""use_peephole"")\n            if use_peephole:\n                return False\n        return True\n\n    def _ct_variable_finder(self, context, i):\n        if self.lstm_cell_type == RNNUnitType.LSTMCell:\n            lstm_cell = context.cell_match[i]\n            return self._find_state_variable_with_select(\n                context,\n                lstm_cell.get_op(""ct"").output[0],\n                [lstm_cell.get_op(""ct_identity_consumer"")]\n            )\n        if self.lstm_cell_type == RNNUnitType.LSTMBlockCell:\n            lstm_block_cell = context.cell_match[i].get_op(""lstm_block_cell"")\n            return self._find_state_variable_with_select(\n                context,\n                lstm_block_cell.output[1],\n                [lstm_block_cell]\n            )\n        return None\n\n    def _ht_variable_finder(self, context, i):\n        if self.lstm_cell_type == RNNUnitType.LSTMCell:\n            lstm_cell = context.cell_match[i]\n            return self._find_state_variable_with_select(\n                context,\n                lstm_cell.get_op(""ht"").output[0],\n                [lstm_cell.get_op(""xh"")]\n            )\n        if self.lstm_cell_type == RNNUnitType.LSTMBlockCell:\n            lstm_block_cell = context.cell_match[i].get_op(""lstm_block_cell"")\n            return self._find_state_variable_with_select(\n                context,\n                lstm_block_cell.output[6],\n                [lstm_block_cell]\n            )\n        return None\n\n    def _ct_ht_shared_variable_finder(self, context, i):\n        if self.lstm_cell_type == RNNUnitType.LSTMBlockCell:\n            return None\n\n        lstm_cell = context.cell_match[i]\n        ct = lstm_cell.get_op(""ct"").output[0]\n        ht = lstm_cell.get_op(""ht"").output[0]\n        ct_concat = [c for c in self.g.find_output_consumers(ct) if is_tf_concat_op(c)]\n        ht_concat = [c for c in self.g.find_output_consumers(ht) if is_tf_concat_op(c)]\n        if len(ct_concat) != 1 or len(ht_concat) != 1 or ct_concat[0] != ht_concat[0]:\n            logger.debug(""failed to find ct-ht concat"")\n            return None\n        ct_ht_shared_output = ct_concat[0].output[0]\n\n        consumers = []\n        ct_identity_consumer = lstm_cell.get_op(""ct_identity_consumer"")\n        ht_identity_consumer = lstm_cell.get_op(""xh"")\n        ct_slice = [c for c in ct_identity_consumer.inputs if is_tf_slice_op(c)]\n        ht_slice = [c for c in ht_identity_consumer.inputs if is_tf_slice_op(c)]\n        if len(ct_slice) != 1 or len(ht_slice) != 1:\n            logger.debug(""failed to find slice op before identity consumers"")\n            return None\n        consumers.extend([ct_slice[0], ht_slice[0]])\n\n        return self._find_state_variable_with_select(\n            context,\n            ct_ht_shared_output,\n            consumers\n        )\n\n    def is_valid(self, context):\n        # except for ct, ht or ct_ht, there are at most 2 state variables\n        if len(context.loop_properties.state_variables) - \\\n                len(context.state_variables) > 2:\n            return False\n\n        # output is no more than 1\n        outputs = context.loop_properties.scan_outputs_exits\n        if len(outputs) > 1:\n            logger.debug(""found %d outputs for lstm: %s"", len(outputs), outputs)\n            return False\n        return True\n\n    def process_weights_and_bias_per_layer(self, context, i):\n        weights = context.weights[i]\n        w_r_icfo = weights[""weight""]\n        w_dtype = weights[""weight""].dtype\n        b_r_icfo = weights[""bias""]\n        b_dtype = weights[""bias""].dtype\n        ft_bias_scalar = weights[""ft_bias""]\n\n        # split bias for each hidden unit\n        # b_r_icfo: (4 * num_units,)\n        bias_dim = b_r_icfo.shape[0]\n        hidden_size = int(bias_dim / 4)\n        b_r_icfo = np.reshape(b_r_icfo, (1, bias_dim))\n        bias_gates = np.split(b_r_icfo, 4, axis=1)\n        ft_bias = np.add(bias_gates[2], ft_bias_scalar)\n        wb_bias_iofc = np.concatenate((bias_gates[0], bias_gates[3], ft_bias, bias_gates[1]), axis=1)\n\n        # fill Rb with empty since in TF, we have only one bias.\n        rb_bias_iofc = np.zeros((1, bias_dim), dtype=b_dtype)\n        B = np.concatenate((wb_bias_iofc, rb_bias_iofc), axis=1)\n        assert B.shape == (1, 2 * bias_dim)\n\n        [wx, wh] = np.split(w_r_icfo, [-1 * hidden_size])\n        input_size = wx.shape[0]\n        assert wx.shape[0] == input_size\n        assert int(wx.shape[1] / 4) == hidden_size\n\n        # split weight for gates\n        w_gates = np.split(wx, 4, axis=1)\n        new_wx = np.concatenate((w_gates[0], w_gates[3], w_gates[2], w_gates[1]), axis=1)\n\n        h_gates = np.split(wh, 4, axis=1)\n        new_wh = np.concatenate((h_gates[0], h_gates[3], h_gates[2], h_gates[1]), axis=1)\n        W_iofc = np.transpose(new_wx)\n        R_iofc = np.transpose(new_wh)\n\n        W = np.array([W_iofc], w_dtype)\n        R = np.array([R_iofc], w_dtype)\n\n        # create node\n        w_name = utils.make_name(""W"" + str(i))\n        w_node = self.g.make_const(w_name, W, skip_conversion=True)\n\n        r_name = utils.make_name(""R"" + str(i))\n        r_node = self.g.make_const(r_name, R, skip_conversion=True)\n\n        b_name = utils.make_name(""B"" + str(i))\n        b_node = self.g.make_const(b_name, B, skip_conversion=True)\n\n        context.input_size[i] = input_size\n        context.hidden_size[i] = hidden_size\n        context.onnx_input_ids[i][""W""] = w_node.output[0]\n        context.onnx_input_ids[i][""R""] = r_node.output[0]\n        context.onnx_input_ids[i][""B""] = b_node.output[0]\n\n    def process_weights_and_bias(self, context):\n        for i in range(self.num_lstm_layers):\n            self.process_weights_and_bias_per_layer(context, i)\n\n    def process_var_init_nodes(self, context):\n        for i in range(self.num_lstm_layers):\n            self.process_var_init_nodes_per_layer(context, i)\n\n    def process_var_init_nodes_per_layer(self, context, i):\n        init_h_id = None\n        init_c_id = None\n        if ""ct_ht"" + str(i) in context.state_variables:\n            init_h_id, init_c_id = self._process_non_tuple_ch_init_nodes(context, i)\n        elif ""ct"" + str(i) in context.state_variables and (""ht"" + str(i)) in context.state_variables:\n            init_h_id, init_c_id = self._process_tuple_ch_init_nodes(context, i)\n        else:\n            raise ValueError(""no initializers, unexpected"")\n        assert init_h_id and init_c_id\n        context.onnx_input_ids[i][""initial_h""] = init_h_id\n        context.onnx_input_ids[i][""initial_c""] = init_c_id\n\n    def _process_non_tuple_ch_init_nodes(self, context, i):\n        input_id = context.state_variables[""ct_ht"" + str(i)].enter_input_id\n        hidden_size = context.hidden_size[i]\n\n        attr = {""axes"": [1], ""starts"": [0], ""ends"": [hidden_size]}\n        inputs_map = {""data"": input_id, **attr}\n        slice_node1 = GraphBuilder(self.g).make_slice(inputs_map)\n        unsqueeze_node_1 = self.g.make_node(""Unsqueeze"", [slice_node1], attr={""axes"": [0]})\n\n        attr = {""axes"": [1], ""starts"": [hidden_size], ""ends"": [hidden_size * 2]}\n        inputs_map = {""data"": input_id, **attr}\n        slice_node2 = GraphBuilder(self.g).make_slice(inputs_map)\n        unsqueeze_node_2 = self.g.make_node(""Unsqueeze"", [slice_node2], attr={""axes"": [0]})\n\n        return unsqueeze_node_1.output[0], unsqueeze_node_2.output[0]\n\n    def _process_tuple_ch_init_nodes(self, context, i):\n        h_init_input_id = context.state_variables[""ht"" + str(i)].enter_input_id\n        c_init_input_id = context.state_variables[""ct"" + str(i)].enter_input_id\n        h_node_output = self._process_c_or_h_init_nodes(h_init_input_id, context)\n        c_node_output = self._process_c_or_h_init_nodes(c_init_input_id, context)\n        return h_node_output, c_node_output\n\n    def _process_c_or_h_init_nodes(self, initializer_input_id, context):\n        node = self.g.get_node_by_output(initializer_input_id)\n        if node.is_const():\n            val = node.get_tensor_value(as_list=False)\n            initial_name = utils.make_name(""Const"")\n            new_val = np.expand_dims(val, axis=0)\n            const_node = self.g.make_const(initial_name, new_val)\n            return const_node.output[0]\n        squeeze_node = self.g.make_node(""Unsqueeze"", [initializer_input_id], attr={""axes"": [0]})\n        to_replace = [n for n in self.g.get_nodes() if n != squeeze_node]\n        self.g.replace_all_inputs(to_replace, initializer_input_id, squeeze_node.output[0])\n        return squeeze_node.output[0]\n\n    def create_single_rnn_node(self, context, i):\n        # specify if the RNN is forward, reverse, or bidirectional.\n        # Must be one of forward (default), reverse, or bidirectional.\n        # Here we won\'t mark bidirectional/reverse, we will have another rewriter running\n        # after this one, which will based on patterns to combine a forward LSTM and a\n        # backward LSTM into a bidirectional one.\n        num_direction = 1\n        # todo: input_forget\n        context.attributes[i][""direction""] = ""forward""\n        context.attributes[i][""hidden_size""] = context.hidden_size[i]\n        inputs = context.onnx_input_ids[i]\n        lstm_inputs = [\n            inputs[""X""], inputs[""W""], inputs[""R""], inputs[""B""],\n            inputs[""sequence_lens""], inputs[""initial_h""], inputs[""initial_c""]]\n\n        x_shape = self.g.get_shape(lstm_inputs[0])\n        x_seq_length = x_shape[0]\n        x_batch_size = x_shape[1]\n        out_dtype = self.g.get_dtype(lstm_inputs[0])\n\n        lstm_node = self.g.make_node(""LSTM"", lstm_inputs, attr=context.attributes[i], output_count=3,\n                                     shapes=[[x_seq_length, num_direction, x_batch_size, context.hidden_size],\n                                             [num_direction, x_batch_size, context.hidden_size],\n                                             [num_direction, x_batch_size, context.hidden_size]],\n                                     dtypes=[out_dtype, out_dtype, out_dtype], op_name_scope=context.rnn_scope)\n        return lstm_node\n\n    def create_rnn_node(self, context):\n        rnn_nodes = list()\n        outputs = context.loop_properties.scan_outputs_exits\n        logger.debug(""number of rnn node outputs: %s"", len(outputs))\n\n        for i in range(self.num_lstm_layers):\n            logger.debug(""creating rnn node for layer: %s"", i)\n            rnn_nodes.append(self.create_single_rnn_node(context, i))\n            output_id = rnn_nodes[i].output[0]\n            rnn_output_shape = self.g.get_shape(output_id)\n            squeeze_output_shape = [rnn_output_shape[0], rnn_output_shape[2], rnn_output_shape[3]]\n            squeeze_node = self.g.make_node(""Squeeze"", [output_id], attr={""axes"": [1]},\n                                            shapes=[squeeze_output_shape],\n                                            dtypes=[self.g.get_dtype(output_id)])\n            if i + 1 < self.num_lstm_layers:\n                logger.debug(""setting input for layer: %s"", i + 1)\n                context.onnx_input_ids[i + 1][""X""] = squeeze_node.output[0]\n        return rnn_nodes\n\n    def _connect_lstm_yh_to_graph(self, context, i):\n        # in tf, y_h output shape is: [batch, hidden]\n        # in onnx, output shape is: [number_directions, batch, hidden]\n        exit_output = context.state_variables[""ht"" + str(i)].exit_output\n        output_id = context.rnn_node[i].output[1]\n        lstm_yh_shape = self.g.get_shape(output_id)\n        squeeze_node = self.g.make_node(""Squeeze"", [output_id], attr={""axes"": [0]},\n                                        shapes=[[lstm_yh_shape[1], lstm_yh_shape[2]]],\n                                        dtypes=[self.g.get_dtype(output_id)])\n\n        self.g.replace_all_inputs(self.g.get_nodes(), exit_output.id, squeeze_node.output[0])\n\n    def _connect_lstm_yc_to_graph(self, context, i):\n        # in tf, y_c output shape is: [batch, hidden]\n        # in onnx, output shape is: [number_directions, batch, hidden]\n        exit_output = context.state_variables[""ct"" + str(i)].exit_output\n        output_id = context.rnn_node[i].output[2]\n        lstm_yc_shape = self.g.get_shape(output_id)\n        squeeze_node = self.g.make_node(""Squeeze"", [output_id], attr={""axes"": [0]},\n                                        shapes=[[lstm_yc_shape[1], lstm_yc_shape[2]]],\n                                        dtypes=[self.g.get_dtype(output_id)])\n\n        self.g.replace_all_inputs(self.g.get_nodes(), exit_output.id, squeeze_node.output[0])\n\n    def _connect_lstm_ych_to_graph(self, context, i):\n        # in tf, concat of y_c and y_h output shape is: [batch, hidden *2]\n        # in onnx, y_c/y_h output shape is: [number_directions, batch, hidden]\n        exit_output = context.state_variables[""ct_ht"" + str(i)].exit_output\n        lstm_node = context.rnn_node[i]\n        yc_shape = self.g.get_shape(lstm_node.output[2])\n        concat_output_shape = [yc_shape[0], yc_shape[1], yc_shape[2] * 2]\n        concat = self.g.make_node(""Concat"", [lstm_node.output[2], lstm_node.output[1]],\n                                  attr={""axis"": 2}, shapes=[concat_output_shape],\n                                  dtypes=[self.g.get_dtype(lstm_node.output[2])])\n\n        squeeze_output_shape = [concat_output_shape[1], concat_output_shape[2]]\n        squeeze_node = self.g.make_node(""Squeeze"", [concat.output[0]], attr={""axes"": [0]},\n                                        shapes=[squeeze_output_shape],\n                                        dtypes=[self.g.get_dtype(concat.output[0])])\n\n        self.g.replace_all_inputs(self.g.get_nodes(), exit_output.id, squeeze_node.output[0])\n'"
tf2onnx/rewriter/lstm_rewriter_base.py,0,"b'# Licensed under the MIT license.\n\n# Temporary base class exclusive for LSTMs for stacked LSTM layer support.\n# Once GRU, BiLSTM, BiGRU re-writers will also be enhanced for stacked layer support\n# this will be combined with unit rnn base class.\n\n""""""\ntf2onnx.rewriter.lstm_rewriter_base\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nimport logging\n\nfrom tf2onnx import utils\nfrom tf2onnx.rewriter.loop_rewriter_base import LoopRewriterBase\nfrom tf2onnx.rewriter.rnn_utils import get_pattern\nfrom tf2onnx.graph_matcher import GraphMatcher\nfrom tf2onnx.rewriter.unit_rnn_rewriter_base import UnitRnnRewriterBase, UnitRnnContext\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,broad-except,protected-access,W0223\n\nclass LSTMContext(UnitRnnContext):\n    def __init__(self):\n        super(LSTMContext, self).__init__()\n        self.cell_match = list()  # matched cell\n\n        self.weights = list({})\n        self.input_size = list()\n        self.hidden_size = list()\n\n        self.attributes = list({})  # onnx attributes\n        # onnx inputs: List of [X, W, R, B, sequence_lens, initial_h, initial_c, P]\n        self.onnx_input_ids = list({})\n\n\nclass LSTMRewriterBase(UnitRnnRewriterBase):\n    """"""\n    main procedures:\n    1 check whether extracted loop is a unit LSTM, fall back in necessity:\n        1 parse LSTM\n        2 find needed info from tensorflow graph\n    3 process found info according to ONNX requirement\n    """"""\n\n    def __init__(self, g):\n        super(LSTMRewriterBase, self).__init__(g)\n        # {var_name: (finder, connector)}\n        self.state_variable_handler = list()\n        self.state_variable_handlers = list()\n\n    def create_context(self):\n        return LSTMContext()\n\n    def parse_unit_rnn(self, context):\n        """"""\n        parse needed info from tensorflow graph:\n        1 weight\n        2 state variables used in rnn unit, such as c_t, h_t\n        3 sequence node\n        4 input_x\n        5 attributes, e.g., activation_alpha, activation_beta... optional\n        """"""\n        logger.debug(""parse unit rnn"")\n\n        logger.debug(""match unit cell against loop body graph"")\n        cell_match = self.find_cell(context)\n        if not cell_match:\n            logger.debug(\'failed to match cell pattern\')\n            return False\n        cell_match.sort(key=lambda cmt: cmt.get_op(""cell_kernel"").name)\n        context.cell_match = cell_match\n\n        logger.debug(""get_weight_and_bias starts"")\n        weights = self.get_weight_and_bias(context)\n        if not weights:\n            logger.debug(""rnn weights check failed, SKIP"")\n            return False\n        context.weights = weights\n\n        if not self.get_state_variables(context):\n            logger.debug(""no cell variable initializers found, SKIP"")\n            return False\n\n        seq_len_node = self.find_sequence_length_node(context)\n        if seq_len_node:\n            logger.debug(""find sequence node: %s"", seq_len_node.name)\n\n        # require exact one input\n        inputs = context.loop_properties.scan_inputs_initial_values\n        if len(inputs) != 1:\n            logger.debug(""found %d inputs for the unit rnn: %s"",\n                         len(inputs), inputs)\n            return False\n\n        for i in range(len(context.cell_match)):\n            context.onnx_input_ids.append({})\n            context.input_size.append(None)\n            context.hidden_size.append(None)\n            context.attributes.append({})\n            context.onnx_input_ids[i][""sequence_lens""] = \\\n                seq_len_node.output[0] if seq_len_node else utils.ONNX_EMPTY_INPUT\n\n        context.onnx_input_ids[0][""X""] = inputs[0]\n        if not self.parse_attributes(context):\n            logger.debug(""wrong attributes found"")\n            return False\n\n        return True\n\n    def _match_cell(self, context, unittype):\n        """"""match unit cell""""""\n        for cell_pattern in get_pattern(unittype):\n            matcher = GraphMatcher(cell_pattern, allow_reorder=True)\n\n            loop_props = context.loop_properties\n            inputs = loop_props.state_inputs + loop_props.scan_inputs\n            input_ids = [input_tensor_value_info.id for input_tensor_value_info in inputs]\n            outputs = loop_props.state_outputs + loop_props.scan_outputs\n            output_ids = [out_tensor_value_info.id for out_tensor_value_info in outputs]\n            body_graph_ops, _, _ = LoopRewriterBase.find_subgraph(\n                set(input_ids),\n                set(output_ids),\n                self.g, merge_as_end=True\n            )\n\n            match_results = list(matcher.match_ops(body_graph_ops))\n            logger.debug(""number of match results: %s"", len(match_results))\n            if len(match_results) > 0:\n                return match_results\n        return None\n\n    def get_state_variables(self, context):\n        """"""\n        Get state variables by provided handlers. There maybe several handlers corresponding to\n        different patterns of state variables.\n        The commone method is to find state variables from loop property according to its\n        next_iteration_input and switch_true_identity_output, see lstm_rewriter_v2\n        """"""\n        contains_handler = False\n        for handler in self.state_variable_handlers:\n            can_handle = True\n            for var_name, funcs in handler.items():\n                finder = funcs[0]\n                state_variable = finder(context, funcs[2])\n                if state_variable:\n                    logger.debug(""found state variable %s"", var_name)\n                    context.state_variables[var_name] = state_variable\n                else:\n                    logger.debug(""failed to get state variable %s"", var_name)\n                    can_handle = False\n                    break\n            if can_handle:\n                self.state_variable_handler.append(handler)\n                contains_handler = True\n        return contains_handler\n\n    def process_outputs(self, context):\n        for handler in self.state_variable_handler:\n            for var_name, funcs in handler.items():\n                output_connector = funcs[1]\n                output_connector(context, funcs[2])\n                logger.debug(""connect output of %s to graph"", var_name)\n        logger.debug(""done handling all state variables, now focusing on final output"")\n        self.connect_unit_rnn_output_to_graph(context)\n\n    def connect_unit_rnn_output_to_graph(self, context):\n        outputs = context.loop_properties.scan_outputs_exits\n        if not outputs:\n            logger.debug(""no one consume output"")\n            return\n\n        gather_output_id = outputs[0].id\n        logger.debug(""found output for rnn: %s"", gather_output_id)\n\n        # in tf batch major mode, output shape is : [batch, time, hidden]\n        # in time major mode, output shape is: [time, batch, hidden]\n        # in onnx, output shape is : [time, num_directions, batch, hidden]\n\n        rnn_node = context.rnn_node[len(context.rnn_node) - 1]\n        output_id = rnn_node.output[0]\n        rnn_output_shape = self.g.get_shape(output_id)\n        squeeze_output_shape = [rnn_output_shape[0], rnn_output_shape[2], rnn_output_shape[3]]\n        squeeze_node = self.g.make_node(""Squeeze"", [output_id], attr={""axes"": [1]},\n                                        shapes=[squeeze_output_shape],\n                                        dtypes=[self.g.get_dtype(output_id)])\n        self.g.replace_all_inputs(self.g.get_nodes(), gather_output_id, squeeze_node.output[0])\n'"
tf2onnx/rewriter/quantization_ops_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\n""""""\r\ntf2onnx.rewriter - rewrite tensorflow QuantizeAndDequantizeV3 op\r\n""""""\r\n\r\nimport numpy as np\r\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\r\nfrom tf2onnx import utils\r\n\r\n# pylint: disable=missing-docstring\r\n\r\ndef extract_numpy_array(node):\r\n    return np.frombuffer(node.attr[""value""].t.raw_data, dtype=""float32"")\r\n\r\ndef create_qdq_nodes(g, match_results):\r\n\r\n    for match in match_results:\r\n        qdq_node = match.get_op(\'output\')\r\n        qdq_node_output_dtype = g.get_dtype(qdq_node.output[0])\r\n        qdq_node_output_shape = g.get_shape(qdq_node.output[0])\r\n\r\n        # Get the attributes of qdq node\r\n        narrow_range = qdq_node.attr[\'narrow_range\'].i\r\n        signed_input = qdq_node.attr[\'signed_input\'].i\r\n\r\n        min_quantized, max_quantized = [-127, 127]\r\n        if not narrow_range and signed_input:\r\n            min_quantized = -128\r\n\r\n        if not signed_input:\r\n            min_quantized, max_quantized = [0, 255]\r\n\r\n        # Get the min and max value of the inputs to QDQ op\r\n        min_value = extract_numpy_array(qdq_node.inputs[1])\r\n        max_value = extract_numpy_array(qdq_node.inputs[2])\r\n\r\n        # Calculate scales from the min and max values\r\n        scale_from_min_side = min_quantized/min_value if min_quantized*min_value > 0 else max_quantized\r\n        scale_from_max_side = max_quantized/max_value if max_quantized*max_value > 0 else max_quantized\r\n\r\n        if scale_from_min_side < scale_from_max_side:\r\n            scale = scale_from_min_side\r\n        else:\r\n            scale = scale_from_max_side\r\n\r\n        utils.make_sure(scale > 0, ""Quantize/Dequantize scale must be greater than zero"")\r\n\r\n        if signed_input:\r\n            zero_point = np.int8(0)\r\n        else:\r\n            zero_point = np.uint8(0)\r\n\r\n        # Split it into QuantizeLinear and DequantizeLinear and remove the QDQ node reference\r\n        y_quant_scale = g.make_const(name=utils.make_name(""y_quant_scale""), np_val=1/scale)\r\n        y_zero_point = g.make_const(name=utils.make_name(""y_zero_point""), np_val=zero_point)\r\n        quant_node = g.make_node(op_type=""QuantizeLinear"",\r\n                                 inputs=[qdq_node.input[0], y_quant_scale.output[0],\r\n                                         y_zero_point.output[0]],\r\n                                 shapes=[qdq_node_output_shape],\r\n                                 dtypes=[qdq_node_output_dtype],\r\n                                 name=utils.make_name(""QuantLinearNode""))\r\n\r\n        g.set_shape(quant_node.output[0], qdq_node_output_shape)\r\n\r\n        g.remove_node(qdq_node.name)\r\n\r\n        y_dequant_scale = g.make_const(name=utils.make_name(""y_dequant_scale""), np_val=1/scale)\r\n        y_inv_zero_point = g.make_const(name=utils.make_name(""y_inv_zero_point""), np_val=zero_point)\r\n        dequant_node = g.make_node(op_type=""DequantizeLinear"",\r\n                                   inputs=[quant_node.output[0], y_dequant_scale.output[0],\r\n                                           y_inv_zero_point.output[0]],\r\n                                   outputs=[qdq_node.output[0]],\r\n                                   shapes=[qdq_node_output_shape],\r\n                                   dtypes=[qdq_node_output_dtype],\r\n                                   name=utils.make_name(""DequantLinearNode""))\r\n        g.set_shape(dequant_node.output[0], qdq_node_output_shape)\r\n\r\n    return g.get_nodes()\r\n\r\ndef rewrite_quantize_and_dequantize(g, ops):\r\n\r\n    pattern_for_qdq_v2 = \\\r\n        OpTypePattern(\'QuantizeAndDequantizeV2\', name=\'output\', inputs=[\r\n            OpTypePattern(""*""),\r\n            OpTypePattern(None),\r\n            OpTypePattern(None),\r\n        ])\r\n    pattern_for_qdq_v3 = \\\r\n        OpTypePattern(\'QuantizeAndDequantizeV3\', name=\'output\', inputs=[\r\n            OpTypePattern(""*""),\r\n            OpTypePattern(None),\r\n            OpTypePattern(None),\r\n            OpTypePattern(None),\r\n        ])\r\n\r\n    # Match all the patterns for QDQ ops\r\n    patterns = [pattern_for_qdq_v3, pattern_for_qdq_v2]\r\n    match_results = []\r\n    for pattern in patterns:\r\n        matcher = GraphMatcher(pattern)\r\n        results = list(matcher.match_ops(ops))\r\n        match_results.extend(results)\r\n\r\n    return create_qdq_nodes(g, match_results)\r\n'"
tf2onnx/rewriter/random_normal_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter - rewrite tensorflow subgraph to onnx random normal op\n""""""\n\nfrom tf2onnx import utils\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\n\n# pylint: disable=missing-docstring\n\n\ndef rewrite_random_normal(g, ops):\n    pattern1 = \\\n        OpTypePattern(\'Add\', name=\'output\', inputs=[\n            OpTypePattern(\'Mul\', name=\'input2\', inputs=[\n                OpTypePattern(\'RandomStandardNormal\', name=\'input1\', inputs=[""*""]), ""*""\n            ]), ""*""\n        ])\n\n    pattern2 = \\\n        OpTypePattern(\'Identity\', name=\'output\', inputs=[\n            OpTypePattern(\'Identity\', name=\'input2\', inputs=[\n                OpTypePattern(\'RandomStandardNormal\', name=\'input1\', inputs=[""*""])\n            ])\n        ])\n\n    pattern_list = [pattern1, pattern2]\n    for pattern in pattern_list:\n        matcher = GraphMatcher(pattern)\n        match_results = list(matcher.match_ops(ops))\n        for match in match_results:\n            output = match.get_op(\'output\')\n            if output.type == \'Add\':\n                # pattern 1\n                mean = output.inputs[1].get_tensor_value()\n            else:\n                # pattern 2\n                mean = 0.0\n            dtype = g.get_dtype(output.output[0])\n            op_name = utils.make_name(""RandomNormal"")\n            out_name = utils.port_name(op_name)\n\n            rn_op = match.get_op(\'input1\')\n            seed = rn_op.get_attr(\'seed2\').i\n\n            if rn_op.inputs[0].type == ""Shape"":\n                shape_node = rn_op.inputs[0]\n                new_node = g.make_node(""RandomNormalLike"", [shape_node.input[0]], outputs=[out_name], name=op_name,\n                                       attr={""mean"": mean, ""scale"": 1.0, ""dtype"": dtype, ""seed"": float(seed)})\n            else:\n                shape = g.get_shape(output.output[0])\n                new_node = g.make_node(""RandomNormal"", [], outputs=[out_name], name=op_name,\n                                       attr={""shape"": shape, ""mean"": mean, ""scale"": 1.0, ""dtype"": dtype, ""seed"": seed})\n\n            g.replace_all_inputs(ops, output.output[0], new_node.output[0])\n            g.safe_remove_nodes(match.get_nodes())\n    return ops\n'"
tf2onnx/rewriter/random_uniform.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\n""""""\r\ntf2onnx.rewriter - rewrite tensorflow subgraph to onnx random_uniform op\r\n""""""\r\nimport numpy as np\r\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\r\nfrom tf2onnx import utils, handler\r\n\r\n\r\n# pylint: disable=missing-docstring\r\n\r\n\r\ndef rewrite_random_uniform(g, ops):\r\n    pattern = \\\r\n        OpTypePattern(\'Add\', name=\'output\', inputs=[\r\n            OpTypePattern(\'Mul\', inputs=[\r\n                OpTypePattern(\'RandomUniform\', name=\'input1\', inputs=[""*""]),\r\n                OpTypePattern(\'Sub\', name=\'input2\', inputs=[""*"", ""*""]),\r\n            ]), None\r\n        ])\r\n\r\n    matcher = GraphMatcher(pattern)\r\n    match_results = list(matcher.match_ops(ops))\r\n    for match in match_results:\r\n        input2 = match.get_op(\'input2\')\r\n        output = match.get_op(\'output\')\r\n        ru_op = match.get_op(\'input1\')\r\n        # max is on input 0\r\n        tmax = input2.inputs[0].get_tensor_value()\r\n        tmin = input2.inputs[1].get_tensor_value()\r\n        to_delete = list(set(match.get_nodes()))\r\n        new_node = create_onnx_random_uniform_op(g, tmax, tmin, ru_op, output, to_delete)\r\n        g.replace_all_inputs(ops, output.output[0], new_node.output[0])\r\n        g.safe_remove_nodes(to_delete)\r\n\r\n    return ops\r\n\r\n\r\n# rewriter function when fold_const is enabled\r\ndef rewrite_random_uniform_fold_const(g, ops):\r\n    pattern = \\\r\n        OpTypePattern(\'Add\', name=\'output\', inputs=[\r\n            OpTypePattern(\'Mul\', name=\'mul\', inputs=[\r\n                OpTypePattern(\'RandomUniform\', name=\'input1\', inputs=[""*""]),\r\n                None,\r\n            ]),\r\n            None,\r\n        ])\r\n\r\n    matcher = GraphMatcher(pattern)\r\n    match_results = list(matcher.match_ops(ops))\r\n    for match in match_results:\r\n        output = match.get_op(\'output\')\r\n        mul = match.get_op(\'mul\')\r\n        ru_op = match.get_op(\'input1\')\r\n\r\n        tmax_minus_tmin = mul.inputs[1].get_tensor_value()\r\n        tmin = output.inputs[1].get_tensor_value()\r\n        tmax = tmin + tmax_minus_tmin\r\n        to_delete = list(set(match.get_nodes()))\r\n        new_node = create_onnx_random_uniform_op(g, tmax, tmin, ru_op, output, to_delete)\r\n        g.replace_all_inputs(ops, output.output[0], new_node.output[0])\r\n        g.safe_remove_nodes(to_delete)\r\n\r\n    return ops\r\n\r\n\r\ndef create_onnx_random_uniform_op(g, tmax, tmin, ru_op, output, to_delete):\r\n    dtype = g.get_dtype(output.output[0])\r\n    op_name = utils.make_name(""RandomUniform"")\r\n    shape_node = ru_op.inputs[0]\r\n    shape = g.get_shape(output.output[0])\r\n    if shape_node.is_const():\r\n        # if the tensorflow input (aka the shape) is const we can use the RandomUniform op\r\n        new_node = g.make_node(""RandomUniform"", [], name=op_name,\r\n                               attr={""low"": tmin, ""high"": tmax, ""dtype"": dtype, ""shape"": shape},\r\n                               shapes=[shape], dtypes=[dtype])\r\n    else:\r\n        if shape_node.type == ""Shape"":\r\n            # if shape is dynamic - in tensorflow shape comes as tensor VALUE,\r\n            # in onnx RandomUniformLike finds takes the shape from the tensor itself.\r\n            # In many cases there is a shape op in tensorflow before RandomUniform and\r\n            # to make that work for onnx we just need to remove the shape op.\r\n            new_node = g.make_node(""RandomUniformLike"", inputs=[shape_node.input[0]], name=op_name,\r\n                                   attr={""low"": tmin, ""high"": tmax, ""dtype"": dtype},\r\n                                   shapes=[shape], dtypes=[dtype])\r\n        else:\r\n            # if the shape is calculated we need to create a tensor so RandomUniformLike\r\n            # can take the shape from there. Pre opset9 this is somewhat hacky because there is\r\n            # no real fill op in onnx. In general this is not going to help performance but the tensors\r\n            # created are expected to be small.\r\n\r\n            # tell the caller to not delete the shape node\r\n            to_delete.remove(shape_node)\r\n            # create a fill op with the shape of the value of the input tensor\r\n            zero = g.make_const(utils.make_name(""zero""), np.zeros((), dtype=np.float32))\r\n            fill_node = g.make_node(""Fill"", inputs=[shape_node.output[0], zero.name],\r\n                                    shapes=[shape], dtypes=[dtype])\r\n            func, _ = handler.tf_op.find_effective_op(""Fill"")\r\n            func(g, fill_node)\r\n            # and use RandomUniformLike to create the random tensor\r\n            new_node = g.make_node(""RandomUniformLike"", inputs=[fill_node.output[0]], name=op_name,\r\n                                   attr={""low"": tmin, ""high"": tmax, ""dtype"": dtype},\r\n                                   shapes=[shape], dtypes=[dtype])\r\n    return new_node\r\n'"
tf2onnx/rewriter/rnn.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.rnn - lstm support\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\n\nfrom tf2onnx.rewriter.bilstm_rewriter import rewrite_bidirectional_lstms\nfrom tf2onnx.rewriter.bigru_rewriter import rewrite_bidirectional_grus\nfrom tf2onnx.rewriter.custom_rnn_rewriter import CustomRnnRewriter\nfrom tf2onnx.rewriter.loop_rewriter import LoopRewriter\nfrom tf2onnx.rewriter.lstm_rewriter import LSTMRewriter\nfrom tf2onnx.rewriter.gru_rewriter import GRUUnitRewriter\n\n# pylint: disable=invalid-name,unused-argument,missing-docstring\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef rewrite_single_direction_lstm(g, ops):\n    r = LSTMRewriter(g)\n    return r.run()\n\n\ndef rewrite_bi_direction_lstm(g, ops):\n    return rewrite_bidirectional_lstms(g, ops)\n\n\ndef rewrite_single_direction_gru(g, ops):\n    r = GRUUnitRewriter(g)\n    return r.run()\n\n\ndef rewrite_bi_direction_gru(g, ops):\n    return rewrite_bidirectional_grus(g, ops)\n\n\ndef rewrite_custom_rnn_cell(g, ops):\n    return  CustomRnnRewriter(g).run()\n\n\ndef rewrite_generic_loop(g, ops):\n    return LoopRewriter(g).run()\n'"
tf2onnx/rewriter/rnn_utils.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\n""""""\r\ntf2onnx.rewriter.rnn_utils - rnn support\r\n""""""\r\n\r\nfrom __future__ import unicode_literals\r\nfrom collections import defaultdict\r\nfrom enum import Enum\r\n\r\nimport logging\r\nimport numpy as np\r\nfrom tf2onnx import utils\r\nfrom tf2onnx.graph_builder import GraphBuilder\r\nfrom tf2onnx.graph_matcher import OpTypePattern # pylint: disable=unused-import\r\n\r\n\r\n# pylint: disable=invalid-name,unused-argument,missing-docstring\r\n\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nclass REWRITER_RESULT(Enum):\r\n    SKIP = 1\r\n    OK = 2\r\n    FAIL = 3\r\n\r\n\r\n# TensorFlow LSTMCell/BasicLSTMCell computation graph matching\r\n\r\nxc_pattern = \\\r\n    OpTypePattern(\'Split\', inputs=[\r\n        OpTypePattern(""Const""), # axis for split\r\n        OpTypePattern(""BiasAdd"", name=""bias_add"", inputs=[\r\n            OpTypePattern(""MatMul"", inputs=[\r\n                OpTypePattern(""ConcatV2|Concat"", name=""xh""),\r\n                OpTypePattern(""Enter"", inputs=[\r\n                    OpTypePattern(""*"", name=""cell_kernel""),\r\n                ]),\r\n            ]),\r\n            OpTypePattern(""Enter"", inputs=[\r\n                OpTypePattern(""*"", name=""cell_bias""),\r\n            ]),\r\n        ]),\r\n    ])\r\n\r\nlstmcell_pattern = \\\r\n    OpTypePattern(\'Mul\', name=\'ht\', inputs=[\r\n        OpTypePattern(""Sigmoid"", name=""ot"", inputs=[xc_pattern]),\r\n        OpTypePattern(\'Tanh\', inputs=[\r\n            OpTypePattern(""Add|AddV2"", name=""ct"", inputs=[\r\n                OpTypePattern(""Mul"", name=""ct_identity_consumer"", inputs=[\r\n                    OpTypePattern(""Sigmoid"", name=""ft"", inputs=[\r\n                        OpTypePattern(""Add|AddV2"", inputs=[\r\n                            xc_pattern,\r\n                            OpTypePattern(""*"", name=""ft_bias""),\r\n                        ]),\r\n                    ]),\r\n                    OpTypePattern(""*""),\r\n                ]),\r\n                OpTypePattern(""Mul"", inputs=[\r\n                    OpTypePattern(""Sigmoid"", name=""it"", inputs=[xc_pattern]),\r\n                    OpTypePattern(""Tanh"", name=""gt"", inputs=[xc_pattern]),\r\n                ]),\r\n            ]),\r\n        ]),\r\n    ])\r\n\r\nxc_pattern_optimized = \\\r\n    OpTypePattern(\'Split\', inputs=[\r\n        OpTypePattern(""Const""),\r\n        OpTypePattern(""Identity"", inputs=[\r\n            OpTypePattern(""MatMul"", inputs=[\r\n                OpTypePattern(""ConcatV2|Concat"", name=""xh""),\r\n                OpTypePattern(""Const"", name=""cell_kernel""),\r\n            ]),\r\n        ]),\r\n    ])\r\n\r\nlstmcell_pattern_optimized = \\\r\n    OpTypePattern(\'Mul\', name=\'ht\', inputs=[\r\n        OpTypePattern(""Sigmoid"", name=""ot"", inputs=[xc_pattern_optimized]),\r\n        OpTypePattern(\'Tanh\', inputs=[\r\n            OpTypePattern(""Add|AddV2"", name=""ct"", inputs=[\r\n                OpTypePattern(""Mul"", name=""ct_identity_consumer"", inputs=[\r\n                    OpTypePattern(""Sigmoid"", name=""ft"", inputs=[\r\n                        OpTypePattern(""Add|AddV2"", inputs=[\r\n                            xc_pattern_optimized,\r\n                            OpTypePattern(""*"", name=""ft_bias""),\r\n                        ]),\r\n                    ]),\r\n                    OpTypePattern(""*""),\r\n                ]),\r\n                OpTypePattern(""Mul"", inputs=[\r\n                    OpTypePattern(""Sigmoid"", name=""it"", inputs=[xc_pattern_optimized]),\r\n                    OpTypePattern(""Tanh"", name=""gt"", inputs=[xc_pattern_optimized]),\r\n                ]),\r\n            ]),\r\n        ]),\r\n    ])\r\n\r\n# input sequence: top to down, left to right\r\n# split into update gate and reset gate\r\ngru_split_pattern = \\\r\n    OpTypePattern(""Split"", inputs=[\r\n        OpTypePattern(""Const""),  # split dim, a constant\r\n        OpTypePattern(""Sigmoid"", inputs=[\r\n            OpTypePattern(""BiasAdd"", inputs=[\r\n                OpTypePattern(""Enter"", inputs=[\r\n                    OpTypePattern(""*"", name=""gate_bias"")\r\n                ]),\r\n                OpTypePattern(""MatMul"", name=""update_reset_gate"", inputs=[\r\n                    OpTypePattern(""Enter"", inputs=[\r\n                        OpTypePattern(""*"", name=""gate_kernel"")\r\n                    ]),\r\n                    OpTypePattern(""ConcatV2|Concat"", name=""cell_inputs"")\r\n                ])\r\n            ])\r\n        ])\r\n    ])\r\n\r\n\r\ngrucell_pattern = \\\r\n    OpTypePattern(""Add"", name=""cell_output"", inputs=[\r\n        OpTypePattern(""Mul"", inputs=[\r\n            gru_split_pattern,\r\n            OpTypePattern(""Identity"")\r\n        ]),\r\n        OpTypePattern(""Mul"", inputs=[\r\n            OpTypePattern(""Sub"", inputs=[\r\n                OpTypePattern(""Const""),  # 1-u\r\n                gru_split_pattern\r\n            ]),\r\n            OpTypePattern(""*"", name=""optional_activation"", inputs=[\r\n                OpTypePattern(""BiasAdd"", inputs=[\r\n                    OpTypePattern(""Enter"", inputs=[\r\n                        OpTypePattern(""*"", name=""hidden_bias"")\r\n                    ]),\r\n                    OpTypePattern(""MatMul"", inputs=[\r\n                        OpTypePattern(""Enter"", inputs=[\r\n                            OpTypePattern(""*"", name=""hidden_kernel"")\r\n                        ]),\r\n                        OpTypePattern(""ConcatV2|Concat"")\r\n                    ])\r\n                ])\r\n            ])\r\n        ])\r\n    ])\r\n\r\n\r\ncudnn_compatible_grucell_pattern = \\\r\n    OpTypePattern(""Add"", name=""cell_output"", inputs=[\r\n        OpTypePattern(""Mul"", inputs=[\r\n            OpTypePattern(""Sub"", inputs=[\r\n                OpTypePattern(""Const""),  # 1-u\r\n                gru_split_pattern\r\n            ]),\r\n            OpTypePattern(""*"", name=""optional_activation"", inputs=[\r\n                OpTypePattern(""Add"", inputs=[\r\n                    OpTypePattern(""Mul"", inputs=[\r\n                        gru_split_pattern,\r\n                        OpTypePattern(""BiasAdd"", inputs=[\r\n                            OpTypePattern(""Enter"", inputs=[\r\n                                OpTypePattern(""*"", name=""hidden_state_bias"")\r\n                            ]),\r\n                            OpTypePattern(""MatMul"", inputs=[\r\n                                OpTypePattern(""Enter"", inputs=[\r\n                                    OpTypePattern(""*"", name=""hidden_state_kernel""),\r\n                                ]),\r\n                                OpTypePattern(""Identity"")\r\n                            ])\r\n                        ])\r\n                    ]),\r\n                    OpTypePattern(""BiasAdd"", inputs=[\r\n                        OpTypePattern(""Enter"", inputs=[\r\n                            OpTypePattern(""*"", name=""hidden_input_bias"")\r\n                        ]),\r\n                        OpTypePattern(""MatMul"", inputs=[\r\n                            OpTypePattern(""Enter"", inputs=[\r\n                                OpTypePattern(""*"", name=""hidden_input_kernel""),\r\n                            ]),\r\n                            OpTypePattern(""*"")\r\n                        ])\r\n                    ])\r\n                ])\r\n            ])\r\n        ]),\r\n        OpTypePattern(""Mul"", inputs=[\r\n            gru_split_pattern,\r\n            OpTypePattern(""Identity"")\r\n        ])\r\n    ])\r\n\r\n\r\ngrublockcell_pattern0 = OpTypePattern(""GRUBlockCell"", name=""gru_block_cell"", inputs=[\r\n    OpTypePattern(""*""),\r\n    OpTypePattern(""*""),\r\n    OpTypePattern(""Enter"", inputs=[\r\n        OpTypePattern(""*"", name=""gate_kernel"")\r\n    ]),\r\n    OpTypePattern(""Enter"", inputs=[\r\n        OpTypePattern(""*"", name=""hidden_kernel"")\r\n    ]),\r\n    OpTypePattern(""Enter"", inputs=[\r\n        OpTypePattern(""*"", name=""gate_bias"")\r\n    ]),\r\n    OpTypePattern(""Enter"", inputs=[\r\n        OpTypePattern(""*"", name=""hidden_bias"")\r\n    ])\r\n])\r\n\r\n\r\ngrublockcell_pattern1 = OpTypePattern(""GRUBlockCell"", name=""gru_block_cell"", inputs=[\r\n    OpTypePattern(""*""),\r\n    OpTypePattern(""*""),\r\n    OpTypePattern(""Const"", name=""gate_kernel""),\r\n    OpTypePattern(""Const"", name=""hidden_kernel""),\r\n    OpTypePattern(""Const"", name=""gate_bias""),\r\n    OpTypePattern(""Const"", name=""hidden_bias"")\r\n])\r\n\r\n\r\nlstmblockcell_pattern = \\\r\n    OpTypePattern(""LSTMBlockCell"", name=""lstm_block_cell"", inputs=[\r\n        OpTypePattern(""*""),\r\n        OpTypePattern(""*""),\r\n        OpTypePattern(""*""),\r\n        OpTypePattern(""Enter"", inputs=[\r\n            OpTypePattern(""*"", name=""cell_kernel"")\r\n        ]),\r\n        OpTypePattern(""*"", name=""Pi""),\r\n        OpTypePattern(""*"", name=""Pf""),\r\n        OpTypePattern(""*"", name=""Po""),\r\n        OpTypePattern(""Enter"", inputs=[\r\n            OpTypePattern(""*"", name=""cell_bias"")\r\n        ])\r\n    ])\r\n\r\n\r\nseq_len_pattern0 = OpTypePattern(""Select|SelectV2"", inputs=[\r\n    OpTypePattern(""GreaterEqual"", inputs=[\r\n        OpTypePattern(""*""),\r\n        OpTypePattern(""Enter"", inputs=[\r\n            OpTypePattern(""*"", name=""seq_len_node"")\r\n        ])\r\n    ]),\r\n    OpTypePattern(""*""),\r\n    OpTypePattern(""*"")\r\n])\r\n\r\n\r\nseq_len_pattern1 = OpTypePattern(""Select|SelectV2"", inputs=[\r\n    OpTypePattern(""GreaterEqual"", inputs=[\r\n        OpTypePattern(""*""),\r\n        OpTypePattern(""Const"", name=""seq_len_node"")\r\n    ]),\r\n    OpTypePattern(""*""),\r\n    OpTypePattern(""*"")\r\n])\r\n\r\n\r\nclass RNNUnitType(Enum):\r\n    LSTMCell = 0  # TF LSTMCell and BasicLSTMCell share the same pattern\r\n    LSTMBlockCell = 1\r\n    GRUCell = 2\r\n    GRUBlockCell = 3\r\n    CudnnCompatibleGRUCell = 4\r\n\r\n\r\nrnn_cell_patterns = {\r\n    RNNUnitType.LSTMCell: [lstmcell_pattern, lstmcell_pattern_optimized],\r\n    RNNUnitType.LSTMBlockCell: [lstmblockcell_pattern],\r\n    RNNUnitType.GRUCell: [grucell_pattern],\r\n    RNNUnitType.GRUBlockCell: [grublockcell_pattern0, grublockcell_pattern1],\r\n    RNNUnitType.CudnnCompatibleGRUCell: [cudnn_compatible_grucell_pattern]\r\n}\r\n\r\n\r\ndef get_pattern(cell_type_name):\r\n    return rnn_cell_patterns[cell_type_name]\r\n\r\n\r\ndef get_rnn_scope_name(while_scope_name):\r\n    parts = while_scope_name.split(\'/\')\r\n    rnn_scope = \'/\'.join(parts[0:-2]) + ""/""\r\n    return rnn_scope\r\n\r\n\r\ndef parse_rnn_loop(graph, loop_properties, rnn_scope, while_context_scope):\r\n    """"""check if the while loop is generated by dynamic_rnn or bidirectional_rnn\r\n\r\n    Args:\r\n        loop_properties: LoopProperties\r\n        rnn_scope: rnn scope name\r\n        while_context_scope: while loop scope name\r\n\r\n    check a while loop is generated by dynamic_rnn or bidirectional_rnn by\r\n\r\n    1. some patterns in _time_step in dynamic_rnn: tensor array read, tensor array write\r\n    2. some patterns in control_flow_ops.while_loop in dynamic_rnn:\r\n         cond: time < loop_bound\r\n         loop_vars: (time, output_ta, state)\r\n         time has name called ""time""\r\n         iteration_cnt is added by control flow.\r\n\r\n    be noted:\r\n    1. iteration counter does not exist in tf1.4 or earlier versions\r\n    2. if dynamic_rnn\'s first input is not consumed, output ta does not exist.\r\n    """"""\r\n    time_name = rnn_scope + ""time""\r\n    ta_array_name_prefix = rnn_scope + ""dynamic_rnn/output_""\r\n    iteration_counter_name = while_context_scope + ""iteration_counter""\r\n\r\n    found_time = False\r\n    is_rnn_out_ta = None\r\n    time_var = None\r\n    iteration_var = None\r\n    for val in loop_properties.all_variables.values():\r\n        enter_input_node = graph.get_node_by_output(val.enter_input_id)\r\n        if val.is_tensor_array:\r\n            ta_name = enter_input_node.get_attr(""tensor_array_name"").s.decode(""utf-8"")\r\n            if not ta_name.startswith(ta_array_name_prefix):\r\n                is_rnn_out_ta = False\r\n        elif enter_input_node.name == time_name:\r\n            found_time = True\r\n            time_var = val\r\n        elif enter_input_node.name == iteration_counter_name:\r\n            iteration_var = val\r\n\r\n    if not found_time or is_rnn_out_ta is False:\r\n        logger.debug(""this should not be a dynamic_rnn loop, found_time: %s, is_rnn_out_ta: %s"",\r\n                     found_time, is_rnn_out_ta)\r\n        return None\r\n\r\n    if not loop_properties.tensor_array_inputs:\r\n        logger.debug(""this should not be a dynamic_rnn loop, no ta input is found"")\r\n        return None\r\n\r\n    return time_var, iteration_var\r\n\r\n\r\ndef get_weights_from_const_node(g, node):\r\n    temp = node\r\n    val = None\r\n    # this would help ignore Identity in non-const_folded graph.\r\n    while temp.type == \'Identity\':\r\n        temp = temp.inputs[0]\r\n\r\n    if temp and temp.type == \'Const\':\r\n        val = temp.get_tensor_value(as_list=False)\r\n        dtype = utils.map_onnx_to_numpy_type(g.get_dtype(temp.output[0]))\r\n        val = val.astype(dtype)\r\n        logger.debug(""found weights %s"", temp.name)\r\n    else:\r\n        logger.debug(""weight node seems not to be Const, skip, node name is %s"", temp.name)\r\n        return None\r\n\r\n    return val\r\n\r\n\r\n######################################################\r\n####      Utilities for bidirectional rnn      #######\r\n######################################################\r\nclass ONNX_RNN_TYPE(Enum):\r\n    GRU = 0\r\n    LSTM = 1\r\n\r\n\r\nonnx_rnn_type_mapping = {\r\n    ONNX_RNN_TYPE.GRU: ""GRU"",\r\n    ONNX_RNN_TYPE.LSTM: ""LSTM""\r\n}\r\n\r\nonnx_rnn_attr_mapping = {\r\n    ONNX_RNN_TYPE.LSTM: [\r\n        ""clip"",\r\n        ""hidden_size"",\r\n        ""input_forget""\r\n    ],\r\n    ONNX_RNN_TYPE.GRU: {\r\n        ""clip"",\r\n        ""hidden_size"",\r\n        ""linear_before_reset""\r\n    }\r\n}\r\nonnx_rnn_seq_len_index_mapping = {\r\n    ONNX_RNN_TYPE.LSTM: 4,\r\n    ONNX_RNN_TYPE.GRU: 4\r\n}\r\n\r\n\r\ndef find_bidirectional_rnns(g, ops, rnn_type):\r\n    """"""\r\n    Find possible bidirectional rnns, return: list of tuple,\r\n    Format of tuple is (fw onnx rnn node, bw onnx rnn node).\r\n    """"""\r\n    fw_rnns = defaultdict(list)\r\n    bw_rnns = defaultdict(list)\r\n    for n in g.get_nodes():\r\n        if n.type != onnx_rnn_type_mapping[rnn_type]:\r\n            continue\r\n\r\n        input_id = n.input[0]\r\n        temp = n.inputs[0]\r\n        is_bw = False\r\n        if temp.type == ""Transpose"":\r\n            input_id = temp.input[0]\r\n            temp = temp.inputs[0]\r\n\r\n        if utils.is_tf_reverse_op(temp):\r\n            input_id = temp.input[0]\r\n            is_bw = True\r\n\r\n        if is_bw:\r\n            # if output 0 is consumed and there is no reverse after the 1st output.\r\n            # it\'s not backward rnn.\r\n            if g.find_output_consumers(n.output[0]) and not get_reverse_nodes_after_y_output(g, n):\r\n                logger.warning(""rnn %s following Reverse op isn\'t the part of bi-rnn."", n.name)\r\n                continue\r\n\r\n            logger.debug(""find bw rnn %s"", input_id)\r\n            bw_rnns[input_id].append(n)\r\n        else:\r\n            logger.debug(""find fw rnn %s"", input_id)\r\n            fw_rnns[input_id].append(n)\r\n\r\n    # fw_rnn and bw_rnn must share the same input\r\n    birnn_input = list(set(fw_rnns.keys()).intersection(bw_rnns.keys()))\r\n    bi_rnns = []\r\n    matched_rnn = []\r\n    for inp in birnn_input:\r\n        fw_rnn = fw_rnns[inp]\r\n        bw_rnn = bw_rnns[inp]\r\n        # it\'s possible several bi-rnns share the same input\r\n        for fw_n in fw_rnn:\r\n            for bw_n in bw_rnn:\r\n                if belong_to_birnn(g, fw_n, bw_n, rnn_type) and \\\r\n                        not fw_n in matched_rnn and not bw_n in matched_rnn:\r\n                    logger.debug(""found birnn comprising %s and %s"", fw_n.name, bw_n.name)\r\n                    bi_rnns.append((fw_n, bw_n))\r\n                    matched_rnn.extend([fw_n, bw_n])\r\n    return bi_rnns\r\n\r\n\r\ndef belong_to_birnn(g, fw_rnn, bw_rnn, rnn_type):\r\n    """"""\r\n    Check whether fw_rnn and bw_rnn are part of the same birnn.\r\n    If fw_rnn and bw_rnn have the same attributes except those related to activation\r\n    and share the same seq_len, they are able to be merged into a bi-rnn.\r\n    """"""\r\n    logger.debug(""check whether %s and %s are part of birnn"", fw_rnn.name, bw_rnn.name)\r\n    for name in onnx_rnn_attr_mapping[rnn_type]:\r\n        fw_attr_value = fw_rnn.get_attr_value(name)\r\n        bw_attr_value = bw_rnn.get_attr_value(name)\r\n        if fw_attr_value != bw_attr_value:\r\n            logger.debug(\r\n                ""fw_rnn and bw_rnn mismatch at attr %s: %s, %s"",\r\n                name, fw_attr_value, bw_attr_value\r\n            )\r\n            return False\r\n\r\n    seq_len_index = onnx_rnn_seq_len_index_mapping[rnn_type]\r\n    fw_seq_len = fw_rnn.input[seq_len_index]\r\n    bw_seq_len = bw_rnn.input[seq_len_index]\r\n    if not utils.have_same_inference_value(g, fw_seq_len, bw_seq_len):\r\n        logger.debug(\r\n            ""fw_rnn and bw_rnn have different seq_len input: %s, %s"",\r\n            fw_seq_len, bw_seq_len\r\n        )\r\n        return False\r\n\r\n    return True\r\n\r\n\r\ndef get_reverse_nodes_after_y_output(g, rnn_bw):\r\n    bw_consumers = g.find_output_consumers(rnn_bw.output[0])\r\n\r\n    # todo: figure out a better way to remove reverse op\r\n    squeeze_nodes = [c for c in bw_consumers if c.type == ""Squeeze""]\r\n    s_cnt = len(squeeze_nodes)\r\n    if s_cnt == 1:\r\n        s = squeeze_nodes[0]\r\n        trans_nodes = g.find_output_consumers(s.output[0])\r\n        if len(trans_nodes) == 1:\r\n            if trans_nodes[0].type == ""Transpose"":\r\n                reverse_nodes = g.find_output_consumers(trans_nodes[0].output[0])\r\n            elif utils.is_tf_reverse_op(trans_nodes[0]):\r\n                reverse_nodes = trans_nodes\r\n            else:\r\n                logger.debug(""not found reverse op, unexpected"")\r\n                return []\r\n\r\n            are_all_reverse = all([utils.is_tf_reverse_op(r_op) for r_op in reverse_nodes])\r\n            if are_all_reverse:\r\n                return reverse_nodes\r\n\r\n            logger.debug(""bw y output is used followed by reverse node"")\r\n            return []\r\n\r\n        logger.debug(""unexpected number of transpose after RNN 1st output:%s"", s_cnt)\r\n        return []\r\n\r\n    logger.debug(""unexpected number of squeeze following RNN 1st output:%s"", s_cnt)\r\n    return []\r\n\r\n\r\ndef get_np_val_for_const(g, node, input_index):\r\n    return node.inputs[input_index].get_tensor_value(as_list=False)\r\n\r\n\r\ndef check_const(g, input_id):\r\n    node = g.get_node_by_output(input_id)\r\n    if node and node.is_const():\r\n        return (True, node.get_tensor_value(as_list=False))\r\n    return (None, None)\r\n\r\n\r\ndef process_single_init_node(g, fw_init_input_id, bw_init_input_id, to_append):\r\n    fw_init_is_const, init_fw_val = check_const(g, fw_init_input_id)\r\n    bw_init_is_const, init_bw_val = check_const(g, bw_init_input_id)\r\n    if fw_init_is_const and bw_init_is_const:\r\n        initial_val = np.concatenate((init_fw_val, init_bw_val), axis=0)\r\n        init_name = utils.make_name(""initial"")\r\n        init_node = g.make_const(init_name, initial_val, skip_conversion=True)\r\n    else:\r\n        init_node = g.make_node(""Concat"", [fw_init_input_id, bw_init_input_id], attr={""axis"": 0})\r\n\r\n    to_append.append(init_node)\r\n    return init_node\r\n\r\n\r\ndef slice_birnn_for_original_rnn_consumers(g, rnn_fw, rnn_bw, bi_rnn, rnn_output_index, all_nodes, to_remove):\r\n    fw_consumers = g.find_output_consumers(rnn_fw.output[rnn_output_index])\r\n    bw_consumers = g.find_output_consumers(rnn_bw.output[rnn_output_index])\r\n    if not fw_consumers and not bw_consumers:\r\n        return\r\n\r\n    if rnn_output_index == 0:\r\n        axis = 1\r\n        # remove reverse op for rnn_bw\r\n        reverse_nodes = get_reverse_nodes_after_y_output(g, rnn_bw)\r\n\r\n        for r_op in reverse_nodes:\r\n            logger.debug(""remove reverse op %s"", r_op.name)\r\n            g.replace_all_inputs(all_nodes, r_op.output[0], r_op.input[0])\r\n            to_remove.append(r_op.name)\r\n    elif rnn_output_index in [1, 2]:\r\n        axis = 0\r\n    else:\r\n        raise ValueError(""rnn only should has 3 outputs."")\r\n\r\n    if fw_consumers:\r\n        attr = {""axes"": [axis], ""starts"": [0], ""ends"": [1]}\r\n        inputs_map = {""data"": bi_rnn.output[rnn_output_index], **attr}\r\n        slice_node_fw = GraphBuilder(g).make_slice(inputs_map)\r\n        all_nodes.append(g.get_node_by_output(slice_node_fw))\r\n        g.replace_all_inputs(fw_consumers, rnn_fw.output[rnn_output_index], slice_node_fw)\r\n\r\n    if bw_consumers:\r\n        attr = {""axes"": [axis], ""starts"": [1], ""ends"": [2]}\r\n        inputs_map = {""data"": bi_rnn.output[rnn_output_index], **attr}\r\n        slice_node_bw = GraphBuilder(g).make_slice(inputs_map)\r\n        all_nodes.append(g.get_node_by_output(slice_node_bw))\r\n        g.replace_all_inputs(bw_consumers, rnn_bw.output[rnn_output_index], slice_node_bw)\r\n\r\n\r\ndef remove_reverse_in_bw_input(g, bw_rnn_input_x, rnn_type):\r\n    old_x_consumers = g.find_output_consumers(bw_rnn_input_x)\r\n    # the transpose/reverse here must be followed by RNN if it is still useful.\r\n    # this is guaranteed by dynamic_rnn logic.\r\n    old_x_has_rnn_as_consumer = [n for n in old_x_consumers if n.type == onnx_rnn_type_mapping[rnn_type]]\r\n    if not old_x_has_rnn_as_consumer:\r\n        logger.debug(""plan to remove useless reverse op in bw"")\r\n        reverse_node = g.get_node_by_output(bw_rnn_input_x)\r\n\r\n        if reverse_node.type == ""Transpose"":\r\n            reverse_node = reverse_node.inputs[0]\r\n\r\n        g.replace_all_inputs(g.get_nodes(), reverse_node.output[0], reverse_node.input[0])\r\n        g.remove_node(reverse_node.name)\r\n    else:\r\n        raise ValueError(""Reverse is still used by RNN as input, cannot remove"")\r\n'"
tf2onnx/rewriter/thresholded_relu_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter - rewrite tensorflow subgraph to onnx ThresholdedRelu op\n""""""\n\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\nfrom tf2onnx.rewriter.leakyrelu_rewriter import _find_edge_name_between_nodes\n\n\n# pylint: disable=missing-docstring\n\n\ndef rewrite_thresholded_relu(g, ops):\n    if g.opset < 10:\n        return ops\n\n    pattern = \\\n        OpTypePattern(\'Mul\', name=\'mul\', inputs=[\n            OpTypePattern(\'Cast\', name=\'cast\', inputs=[\n                OpTypePattern(\'Greater\', name=\'greater\', inputs=[\n                    OpTypePattern(\'*\', name=\'greater_input\'),\n                    OpTypePattern(\'Const\', name=\'theta\')\n                ])\n            ]),\n            OpTypePattern(\'*\', name=\'mul_input\')\n        ])\n    matcher = GraphMatcher(pattern, allow_reorder=True)\n    match_results = list(matcher.match_ops(ops))\n\n    for match in match_results:\n        greater_node = match.get_op(\'greater\')\n        greater_input_node = match.get_op(\'greater_input\')\n        mul_node = match.get_op(""mul"")\n        mul_input_node = match.get_op(\'mul_input\')\n        cast_node = match.get_op(\'cast\')\n\n        greater_input_edge_name = _find_edge_name_between_nodes(greater_input_node, greater_node)\n        mul_input_edge_name = _find_edge_name_between_nodes(mul_input_node, mul_node)\n        if greater_input_edge_name == mul_input_edge_name:\n            theta = match.get_op(\'theta\').get_tensor_value()\n            thresholded_relu = g.make_node(""ThresholdedRelu"", inputs=[mul_input_edge_name], attr={""alpha"": theta},\n                                           shapes=[g.get_shape(mul_node.output[0])],\n                                           dtypes=[g.get_dtype(mul_node.output[0])])\n            g.replace_all_inputs(ops, mul_node.output[0], thresholded_relu.output[0])\n            to_delete = [cast_node, mul_node]\n            g.safe_remove_nodes(to_delete)\n    return ops\n'"
tf2onnx/rewriter/transpose_rewriter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter - rewrite tensorflow transpose op\n""""""\n\nfrom tf2onnx.graph_matcher import OpTypePattern, GraphMatcher\n\n\n# pylint: disable=missing-docstring\n\n\ndef rewrite_transpose(g, ops):\n    pattern = \\\n        OpTypePattern(\'Transpose\', name=\'output\', inputs=[\n            OpTypePattern(None),\n            OpTypePattern(\'Sub\', inputs=[\n                OpTypePattern(\'Sub\', inputs=[""*"", ""*""]),\n                OpTypePattern(\'Range\', inputs=[""*"", ""*"", ""*""]),\n            ]),\n        ])\n\n    matcher = GraphMatcher(pattern)\n    match_results = list(matcher.match_ops(ops))\n    for match in match_results:\n        output = match.get_op(\'output\')\n        shape = g.get_shape(output.input[0])\n        dims = range(len(shape) - 1, -1, -1)\n        output.set_attr(""perm"", dims)\n        g.remove_input(output, output.input[1])\n        to_delete = [n for n in match.get_nodes() if n != output]\n        g.safe_remove_nodes(to_delete)\n    return ops\n'"
tf2onnx/rewriter/unit_rnn_rewriter_base.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\ntf2onnx.rewriter.unit_rnn_rewriter_base\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nimport logging\n\nfrom tf2onnx.rewriter.loop_rewriter_base import LoopRewriterBase, Context\nfrom tf2onnx.rewriter.rnn_utils import REWRITER_RESULT, get_pattern, \\\n    get_rnn_scope_name, parse_rnn_loop, seq_len_pattern0, seq_len_pattern1\nfrom tf2onnx.utils import is_tf_select_op, is_tf_tensor_array_write_op\nfrom tf2onnx.graph_matcher import GraphMatcher\n\n\nlogger = logging.getLogger(__name__)\n\n\n# pylint: disable=missing-docstring,invalid-name,unused-argument,using-constant-test,broad-except,protected-access\n\nclass UnitRnnContext(Context):\n    def __init__(self):\n        super(UnitRnnContext, self).__init__()\n        self.rnn_scope = None\n        self.cell_match = None  # matched cell\n\n        self.weights = {}\n        self.seq_len_node = None\n        self.state_variables = {}\n        self.input_size = None\n        self.hidden_size = None\n\n        self.attributes = {} # onnx attributes\n        # onnx inputs: [X, W, R, B, sequence_lens, initial_h, initial_c, P],\n        # sequence_lens is optional, i.e., None\n        self.onnx_input_ids = {}\n\n\nclass UnitRnnRewriterBase(LoopRewriterBase):\n    """"""\n    main procedures:\n    1 extract info of while_loop based on loop_rewriter_base\n    2 check whether extracted loop is a unit rnn, fall back in necessity:\n        1 parse rnn scope name\n        2 check if it\'s a dynamic_rnn\n        3 find needed info from tensorflow graph\n    3 process found info according to ONNX requirement\n    """"""\n    def __init__(self, g):\n        super(UnitRnnRewriterBase, self).__init__(g)\n        # {var_name: (finder, connector)}\n        self.state_variable_handler = None\n        self.state_variable_handlers = None\n\n    def create_context(self):\n        return UnitRnnContext()\n\n    def run(self):\n        return self.run_internal()\n\n    def need_rewrite(self, context):\n        context.rnn_scope = get_rnn_scope_name(context.while_context_scope)\n\n        if not parse_rnn_loop(self.g, context.loop_properties, context.rnn_scope,\n                              context.while_context_scope):\n            logger.debug(""parse_rnn_loop failed, SKIP"")\n            return False\n\n        if not self.parse_unit_rnn(context):\n            logger.debug(""failed to parse unit rnn, SKIP"")\n            return False\n\n        if not self.is_valid(context):\n            logger.debug(""parsed rnn is not valid, SKIP"")\n            return False\n        return True\n\n    def is_valid(self, context):\n        return True\n\n    def parse_unit_rnn(self, context):\n        """"""\n        parse needed info from tensorflow graph:\n        1 weight\n        2 state variables used in rnn unit, such as c_t, h_t\n        3 sequence node\n        4 input_x\n        5 attributes, e.g., activation_alpha, activation_beta... optional\n        """"""\n        logger.debug(""parse unit rnn"")\n\n        logger.debug(""match unit cell against loop body graph"")\n        cell_match = self.find_cell(context)\n        if not cell_match:\n            logger.debug(\'failed to match cell pattern\')\n            return False\n        context.cell_match = cell_match\n\n        logger.debug(""get_weight_and_bias starts"")\n        weights = self.get_weight_and_bias(context)\n        if not weights:\n            logger.debug(""rnn weights check failed, SKIP"")\n            return False\n        context.weights = weights\n\n        if not self.get_state_variables(context):\n            logger.debug(""no cell variable initializers found, SKIP"")\n            return False\n\n        seq_len_node = self.find_sequence_length_node(context)\n        if seq_len_node:\n            logger.debug(""find sequence node: %s"", seq_len_node.name)\n            context.onnx_input_ids[""sequence_lens""] = seq_len_node.output[0]\n        else:\n            context.onnx_input_ids[""sequence_lens""] = None\n\n        # require exact one input\n        inputs = context.loop_properties.scan_inputs_initial_values\n        if len(inputs) != 1:\n            logger.debug(""found %d inputs for the unit rnn: %s"",\n                         len(inputs), inputs)\n            return False\n        context.onnx_input_ids[""X""] = inputs[0]\n\n        if not self.parse_attributes(context):\n            logger.debug(""wrong attributes found"")\n            return False\n\n        return True\n\n    def find_cell(self, context):\n        raise NotImplementedError()\n\n    def _match_cell(self, context, unittype):\n        """"""match unit cell""""""\n        for cell_pattern in get_pattern(unittype):\n            matcher = GraphMatcher(cell_pattern, allow_reorder=True)\n\n            loop_props = context.loop_properties\n            inputs = loop_props.state_inputs + loop_props.scan_inputs\n            input_ids = [input_tensor_value_info.id for input_tensor_value_info in inputs]\n            outputs = loop_props.state_outputs + loop_props.scan_outputs\n            output_ids = [out_tensor_value_info.id for out_tensor_value_info in outputs]\n            body_graph_ops, _, _ = LoopRewriterBase.find_subgraph(\n                set(input_ids),\n                set(output_ids),\n                self.g, merge_as_end=True\n            )\n\n            match_results = list(matcher.match_ops(body_graph_ops))\n            if len(match_results) == 1:\n                return match_results[0]\n        return None\n\n    def get_weight_and_bias(self, context):\n        raise NotImplementedError()\n\n    def parse_attributes(self, context):\n        return True\n\n    def rewrite(self, context):\n        logger.debug(""enter unit rnn rewrite function"")\n\n        logger.debug(""process the weights/bias/ft_bias, to fit onnx weights/bias requirements"")\n        self.process_weights_and_bias(context)\n\n        self.process_var_init_nodes(context)\n\n        logger.debug(""start to build new rnn node"")\n\n        rnn_node = self.create_rnn_node(context)\n        context.rnn_node = rnn_node\n\n        logger.debug(""start to handle outputs"")\n        # format of ONNX output is different with tf\n        self.process_outputs(context)\n\n        logger.debug(""rewrite successfully"")\n        return REWRITER_RESULT.OK\n\n    def get_state_variables(self, context):\n        """"""\n        Get state variables by provided handlers. There maybe several handlers corresponding to\n        different patterns of state variables.\n        The commone method is to find state variables from loop property according to its\n        next_iteration_input and switch_true_identity_output, see lstm_rewriter_v2\n        """"""\n        for handler in self.state_variable_handlers:\n            can_handle = True\n            for var_name, funcs in handler.items():\n                finder = funcs[0]\n                state_variable = finder(context)\n                if state_variable:\n                    logger.debug(""found state variable %s"", var_name)\n                    context.state_variables[var_name] = state_variable\n                else:\n                    logger.debug(""failed to get state variable %s"", var_name)\n                    can_handle = False\n                    break\n            if can_handle:\n                self.state_variable_handler = handler\n                return True\n        return False\n\n    def find_sequence_length_node(self, context):\n        # get any state variable\n        state_variable = list(context.state_variables.values())[0]\n        next_iter_input_node = self.g.get_node_by_output(state_variable.next_iteration_input.id)\n        if not is_tf_select_op(next_iter_input_node):\n            logger.debug(""no sequence length node is given"")\n            return None\n        matcher = GraphMatcher(seq_len_pattern0)\n        match_result = matcher.match_op(next_iter_input_node)\n        if not match_result:\n            matcher = GraphMatcher(seq_len_pattern1)\n            match_result = matcher.match_op(next_iter_input_node)\n            if not match_result:\n                raise RuntimeError(""failed to find sequence length."")\n        return match_result.get_op(""seq_len_node"")\n\n    def process_weights_and_bias(self, context):\n        raise NotImplementedError()\n\n    def process_var_init_nodes(self, context):\n        raise NotImplementedError()\n\n    def create_rnn_node(self, context):\n        raise NotImplementedError()\n\n    def process_outputs(self, context):\n        for var_name, funcs in self.state_variable_handler.items():\n            output_connector = funcs[1]\n            output_connector(context)\n            logger.debug(""connect output of %s to graph"", var_name)\n\n        self.connect_unit_rnn_output_to_graph(context)\n\n    def connect_unit_rnn_output_to_graph(self, context):\n        outputs = context.loop_properties.scan_outputs_exits\n        if not outputs:\n            logger.debug(""no one consume output"")\n            return\n\n        gather_output_id = outputs[0].id\n        logger.debug(""found output for rnn: %s"", gather_output_id)\n\n        # in tf batch major mode, output shape is : [batch, time, hidden]\n        # in time major mode, output shape is: [time, batch, hidden]\n        # in onnx, output shape is : [time, num_directions, batch, hidden]\n\n        rnn_node = context.rnn_node\n        output_id = rnn_node.output[0]\n        rnn_output_shape = self.g.get_shape(output_id)\n        squeeze_output_shape = [rnn_output_shape[0], rnn_output_shape[2], rnn_output_shape[3]]\n        squeeze_node = self.g.make_node(""Squeeze"", [output_id], attr={""axes"": [1]},\n                                        shapes=[squeeze_output_shape],\n                                        dtypes=[self.g.get_dtype(output_id)])\n        self.g.replace_all_inputs(self.g.get_nodes(), gather_output_id, squeeze_node.output[0])\n\n    def _find_state_variable_with_select(self, context,\n                                         next_iteration_input,\n                                         switch_true_identity_consumers):\n        """"""\n        Find state variables from switch_true_identity_consumers to next_iteration_input.\n        Select maybe added after next_iteration_input.\n        """"""\n        # find all select not followed by TensorArrayWrite\n        select = []\n        for c in self.g.find_output_consumers(next_iteration_input):\n            if not is_tf_select_op(c):\n                continue\n            out_ta_writer = [\n                o for o in self.g.find_output_consumers(c.output[0]) if is_tf_tensor_array_write_op(o)\n            ]\n            if out_ta_writer:\n                continue\n            select.append(c)\n        if len(select) == 1:\n            next_iteration_input = select[0].output[0]\n            switch_true_identity_consumers.append(select[0])\n\n        logger.debug(\n            ""try to find state variable from [%s, %s]"",\n            next_iteration_input,\n            switch_true_identity_consumers\n        )\n\n        def checker(state_variable):\n            if state_variable.next_iteration_input.id != next_iteration_input:\n                return False\n            for consumer in switch_true_identity_consumers:\n                if state_variable.switch_true_identity_output.id not in consumer.input:\n                    return False\n            return True\n\n        state_variables = context.loop_properties.get_variables(checker)\n        if len(state_variables) != 1:\n            logger.debug(""found %d state variables"", len(state_variables))\n            return None\n        return state_variables[0]\n'"
