file_path,api_count,code
manage.py,0,"b'#!/usr/bin/env python\n\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport sys\n\nif __name__ == ""__main__"":\n    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""cvat.settings.{}"" \\\n        .format(os.environ.get(""DJANGO_CONFIGURATION"", ""development"")))\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            ""Couldn\'t import Django. Are you sure it\'s installed and ""\n            ""available on your PYTHONPATH environment variable? Did you ""\n            ""forget to activate a virtual environment?""\n        ) from exc\n    execute_from_command_line(sys.argv)\n'"
cvat/__init__.py,0,"b""# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom cvat.utils.version import get_version\n\nVERSION = (1, 1, 0, 'alpha', 0)\n\n__version__ = get_version(VERSION)\n"""
cvat/simpleworker.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom rq import Worker\n\n\nclass BaseDeathPenalty(object):\n    def __init__(self, timeout, exception, **kwargs):\n        pass\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, type, value, traceback):\n        pass\n\n\nclass SimpleWorker(Worker):\n    death_penalty_class = BaseDeathPenalty\n\n    def main_work_horse(self, *args, **kwargs):\n        raise NotImplementedError(""Test worker does not implement this method"")\n\n    def execute_job(self, *args, **kwargs):\n        """"""Execute job in same thread/process, do not fork()""""""\n        return self.perform_job(*args, **kwargs)\n'"
cvat/urls.py,0,"b'\n# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n""""""CVAT URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https://docs.djangoproject.com/en/2.0/topics/http/urls/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  path(\'\', views.home, name=\'home\')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  path(\'\', Home.as_view(), name=\'home\')\nIncluding another URLconf\n    1. Import the include() function: from django.urls import include, path\n    2. Add a URL to urlpatterns:  path(\'blog/\', include(\'blog.urls\'))\n""""""\n\nfrom django.contrib import admin\nfrom django.urls import path, include\nfrom django.apps import apps\n\nurlpatterns = [\n    path(\'admin/\', admin.site.urls),\n    path(\'\', include(\'cvat.apps.engine.urls\')),\n    path(\'django-rq/\', include(\'django_rq.urls\')),\n    path(\'auth/\', include(\'cvat.apps.authentication.urls\')),\n    path(\'documentation/\', include(\'cvat.apps.documentation.urls\')),\n]\n\nif apps.is_installed(\'cvat.apps.tf_annotation\'):\n    urlpatterns.append(path(\'tensorflow/annotation/\', include(\'cvat.apps.tf_annotation.urls\')))\n\nif apps.is_installed(\'cvat.apps.git\'):\n    urlpatterns.append(path(\'git/repository/\', include(\'cvat.apps.git.urls\')))\n\nif apps.is_installed(\'cvat.apps.reid\'):\n    urlpatterns.append(path(\'reid/\', include(\'cvat.apps.reid.urls\')))\n\nif apps.is_installed(\'cvat.apps.auto_annotation\'):\n    urlpatterns.append(path(\'auto_annotation/\', include(\'cvat.apps.auto_annotation.urls\')))\n\nif apps.is_installed(\'cvat.apps.dextr_segmentation\'):\n    urlpatterns.append(path(\'dextr/\', include(\'cvat.apps.dextr_segmentation.urls\')))\n\nif apps.is_installed(\'cvat.apps.log_viewer\'):\n    urlpatterns.append(path(\'analytics/\', include(\'cvat.apps.log_viewer.urls\')))\n\nif apps.is_installed(\'silk\'):\n    urlpatterns.append(path(\'profiler/\', include(\'silk.urls\')))\n\n# new feature by Mohammad\nif apps.is_installed(\'cvat.apps.auto_segmentation\'):\n    urlpatterns.append(path(\'tensorflow/segmentation/\', include(\'cvat.apps.auto_segmentation.urls\')))\n'"
cvat/wsgi.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n""""""\nWSGI config for CVAT project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.0/howto/deployment/wsgi/\n""""""\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""cvat.settings.{}"" \\\n    .format(os.environ.get(""DJANGO_CONFIGURATION"", ""development"")))\n\napplication = get_wsgi_application()\n'"
datumaro/datum.py,0,"b""#!/usr/bin/env python\nimport sys\n\nfrom datumaro.cli.__main__ import main\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n"""
datumaro/setup.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os.path as osp\nimport re\nimport setuptools\n\n\ndef find_version(file_path=None):\n    if not file_path:\n        file_path = osp.join(osp.dirname(osp.abspath(__file__)),\n            \'datumaro\', \'version.py\')\n\n    with open(file_path, \'r\') as version_file:\n        version_text = version_file.read()\n\n    # PEP440:\n    # https://www.python.org/dev/peps/pep-0440/#appendix-b-parsing-version-strings-with-regular-expressions\n    pep_regex = r\'([1-9]\\d*!)?(0|[1-9]\\d*)(\\.(0|[1-9]\\d*))*((a|b|rc)(0|[1-9]\\d*))?(\\.post(0|[1-9]\\d*))?(\\.dev(0|[1-9]\\d*))?\'\n    version_regex = r\'VERSION\\s*=\\s*.(\' + pep_regex + \').\'\n    match = re.match(version_regex, version_text)\n    if not match:\n        raise RuntimeError(""Failed to find version string in \'%s\'"" % file_path)\n\n    version = version_text[match.start(1) : match.end(1)]\n    return version\n\n\nwith open(\'README.md\', \'r\') as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""datumaro"",\n    version=find_version(),\n    author=""Intel"",\n    author_email=""maxim.zhiltsov@intel.com"",\n    description=""Dataset Framework"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/opencv/cvat/datumaro"",\n    packages=setuptools.find_packages(exclude=[\'tests*\']),\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    python_requires=\'>=3.5\',\n    install_requires=[\n        \'defusedxml\',\n        \'GitPython\',\n        \'lxml\',\n        \'matplotlib\',\n        \'numpy\',\n        \'opencv-python\',\n        \'Pillow\',\n        \'pycocotools\',\n        \'PyYAML\',\n        \'scikit-image\',\n        \'tensorboardX\',\n    ],\n    extras_require={\n        \'tf\': [\'tensorflow\'],\n        \'tf-gpu\': [\'tensorflow-gpu\'],\n    },\n    entry_points={\n        \'console_scripts\': [\n            \'datum=datumaro.cli.__main__:main\',\n        ],\n    },\n)\n'"
utils/__init__.py,0,b''
cvat/apps/profiler.py,0,"b""from django.apps import apps\n\nif apps.is_installed('silk'):\n    from silk.profiling.profiler import silk_profile\nelse:\n    from functools import wraps\n    def silk_profile(name=None):\n        def profile(f):\n            @wraps(f)\n            def wrapped(*args, **kwargs):\n                return f(*args, **kwargs)\n            return wrapped\n        return profile"""
cvat/settings/__init__.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/settings/base.py,0,"b'# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n""""""\nDjango settings for CVAT project.\n\nGenerated by \'django-admin startproject\' using Django 2.0.1.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.0/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/2.0/ref/settings/\n""""""\n\nimport os\nimport sys\nimport fcntl\nimport shutil\nimport subprocess\nimport mimetypes\nmimetypes.add_type(""application/wasm"", "".wasm"", True)\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = str(Path(__file__).parents[2])\n\nALLOWED_HOSTS = os.environ.get(\'ALLOWED_HOSTS\', \'localhost,127.0.0.1\').split(\',\')\nINTERNAL_IPS = [\'127.0.0.1\']\n\ntry:\n    sys.path.append(BASE_DIR)\n    from keys.secret_key import SECRET_KEY # pylint: disable=unused-import\nexcept ImportError:\n\n    from django.utils.crypto import get_random_string\n    keys_dir = os.path.join(BASE_DIR, \'keys\')\n    if not os.path.isdir(keys_dir):\n        os.mkdir(keys_dir)\n    with open(os.path.join(keys_dir, \'secret_key.py\'), \'w\') as f:\n        chars = \'abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)\'\n        f.write(""SECRET_KEY = \'{}\'\\n"".format(get_random_string(50, chars)))\n    from keys.secret_key import SECRET_KEY\n\n\ndef generate_ssh_keys():\n    keys_dir = \'{}/keys\'.format(os.getcwd())\n    ssh_dir = \'{}/.ssh\'.format(os.getenv(\'HOME\'))\n    pidfile = os.path.join(ssh_dir, \'ssh.pid\')\n\n    with open(pidfile, ""w"") as pid:\n        fcntl.flock(pid, fcntl.LOCK_EX)\n        try:\n            subprocess.run([\'ssh-add {}/*\'.format(ssh_dir)], shell = True, stderr = subprocess.PIPE)\n            keys = subprocess.run([\'ssh-add -l\'], shell = True,\n                stdout = subprocess.PIPE).stdout.decode(\'utf-8\').split(\'\\n\')\n            if \'has no identities\' in keys[0]:\n                print(\'SSH keys were not found\')\n                volume_keys = os.listdir(keys_dir)\n                if not (\'id_rsa\' in volume_keys and \'id_rsa.pub\' in volume_keys):\n                    print(\'New pair of keys are being generated\')\n                    subprocess.run([\'ssh-keygen -b 4096 -t rsa -f {}/id_rsa -q -N """"\'.format(ssh_dir)], shell = True)\n                    shutil.copyfile(\'{}/id_rsa\'.format(ssh_dir), \'{}/id_rsa\'.format(keys_dir))\n                    shutil.copymode(\'{}/id_rsa\'.format(ssh_dir), \'{}/id_rsa\'.format(keys_dir))\n                    shutil.copyfile(\'{}/id_rsa.pub\'.format(ssh_dir), \'{}/id_rsa.pub\'.format(keys_dir))\n                    shutil.copymode(\'{}/id_rsa.pub\'.format(ssh_dir), \'{}/id_rsa.pub\'.format(keys_dir))\n                else:\n                    print(\'Copying them from keys volume\')\n                    shutil.copyfile(\'{}/id_rsa\'.format(keys_dir), \'{}/id_rsa\'.format(ssh_dir))\n                    shutil.copymode(\'{}/id_rsa\'.format(keys_dir), \'{}/id_rsa\'.format(ssh_dir))\n                    shutil.copyfile(\'{}/id_rsa.pub\'.format(keys_dir), \'{}/id_rsa.pub\'.format(ssh_dir))\n                    shutil.copymode(\'{}/id_rsa.pub\'.format(keys_dir), \'{}/id_rsa.pub\'.format(ssh_dir))\n                subprocess.run([\'ssh-add\', \'{}/id_rsa\'.format(ssh_dir)], shell = True)\n        finally:\n            fcntl.flock(pid, fcntl.LOCK_UN)\n\ntry:\n    if os.getenv(""SSH_AUTH_SOCK"", None):\n        generate_ssh_keys()\nexcept Exception:\n    pass\n\n# Application definition\nJS_3RDPARTY = {}\nCSS_3RDPARTY = {}\n\nINSTALLED_APPS = [\n    \'django.contrib.admin\',\n    \'django.contrib.auth\',\n    \'django.contrib.contenttypes\',\n    \'django.contrib.sessions\',\n    \'django.contrib.messages\',\n    \'django.contrib.staticfiles\',\n    \'cvat.apps.authentication\',\n    \'cvat.apps.documentation\',\n    \'cvat.apps.dataset_manager\',\n    \'cvat.apps.engine\',\n    \'cvat.apps.git\',\n    \'cvat.apps.restrictions\',\n    \'django_rq\',\n    \'compressor\',\n    \'cacheops\',\n    \'sendfile\',\n    \'dj_pagination\',\n    \'revproxy\',\n    \'rules\',\n    \'rest_framework\',\n    \'rest_framework.authtoken\',\n    \'django_filters\',\n    \'drf_yasg\',\n    \'rest_auth\',\n    \'django.contrib.sites\',\n    \'allauth\',\n    \'allauth.account\',\n    \'corsheaders\',\n    \'allauth.socialaccount\',\n    \'rest_auth.registration\'\n]\n\nSITE_ID = 1\n\nREST_FRAMEWORK = {\n    \'DEFAULT_PERMISSION_CLASSES\': [\n        \'rest_framework.permissions.IsAuthenticated\',\n    ],\n    \'DEFAULT_AUTHENTICATION_CLASSES\': [\n        \'cvat.apps.authentication.auth.TokenAuthentication\',\n        \'cvat.apps.authentication.auth.SignatureAuthentication\',\n        \'rest_framework.authentication.SessionAuthentication\',\n        \'rest_framework.authentication.BasicAuthentication\'\n    ],\n    \'DEFAULT_VERSIONING_CLASS\':\n        # Don\'t try to use URLPathVersioning. It will give you /api/{version}\n        # in path and \'/api/docs\' will not collapse similar items (flat list\n        # of all possible methods isn\'t readable).\n        \'rest_framework.versioning.NamespaceVersioning\',\n    # Need to add \'api-docs\' here as a workaround for include_docs_urls.\n    \'ALLOWED_VERSIONS\': (\'v1\', \'api-docs\'),\n    \'DEFAULT_PAGINATION_CLASS\':\n        \'cvat.apps.engine.pagination.CustomPagination\',\n    \'PAGE_SIZE\': 10,\n    \'DEFAULT_FILTER_BACKENDS\': (\n        \'rest_framework.filters.SearchFilter\',\n        \'django_filters.rest_framework.DjangoFilterBackend\',\n        \'rest_framework.filters.OrderingFilter\'),\n\n    # Disable default handling of the \'format\' query parameter by REST framework\n    \'URL_FORMAT_OVERRIDE\': \'scheme\',\n    \'DEFAULT_THROTTLE_CLASSES\': [\n        \'rest_framework.throttling.AnonRateThrottle\',\n    ],\n    \'DEFAULT_THROTTLE_RATES\': {\n        \'anon\': \'100/hour\',\n    },\n}\n\nREST_AUTH_REGISTER_SERIALIZERS = {\n    \'REGISTER_SERIALIZER\': \'cvat.apps.restrictions.serializers.RestrictedRegisterSerializer\'\n}\n\nif \'yes\' == os.environ.get(\'TF_ANNOTATION\', \'no\'):\n    INSTALLED_APPS += [\'cvat.apps.tf_annotation\']\n\nif \'yes\' == os.environ.get(\'OPENVINO_TOOLKIT\', \'no\'):\n    INSTALLED_APPS += [\'cvat.apps.auto_annotation\']\n\nif \'yes\' == os.environ.get(\'OPENVINO_TOOLKIT\', \'no\') and os.environ.get(\'REID_MODEL_DIR\', \'\'):\n    INSTALLED_APPS += [\'cvat.apps.reid\']\n\nif \'yes\' == os.environ.get(\'WITH_DEXTR\', \'no\'):\n    INSTALLED_APPS += [\'cvat.apps.dextr_segmentation\']\n\nif os.getenv(\'DJANGO_LOG_VIEWER_HOST\'):\n    INSTALLED_APPS += [\'cvat.apps.log_viewer\']\n\n# new feature by Mohammad\nif \'yes\' == os.environ.get(\'AUTO_SEGMENTATION\', \'no\'):\n    INSTALLED_APPS += [\'cvat.apps.auto_segmentation\']\n\nMIDDLEWARE = [\n    \'django.middleware.security.SecurityMiddleware\',\n    \'django.contrib.sessions.middleware.SessionMiddleware\',\n    \'corsheaders.middleware.CorsMiddleware\',\n    \'django.middleware.common.CommonMiddleware\',\n    \'django.middleware.csrf.CsrfViewMiddleware\',\n    # FIXME\n    # \'corsheaders.middleware.CorsPostCsrfMiddleware\',\n    \'django.contrib.auth.middleware.AuthenticationMiddleware\',\n    \'django.contrib.messages.middleware.MessageMiddleware\',\n    \'django.middleware.clickjacking.XFrameOptionsMiddleware\',\n    \'dj_pagination.middleware.PaginationMiddleware\',\n]\n\nUI_URL = \'\'\n\nSTATICFILES_FINDERS = [\n    \'django.contrib.staticfiles.finders.FileSystemFinder\',\n    \'django.contrib.staticfiles.finders.AppDirectoriesFinder\',\n    \'compressor.finders.CompressorFinder\',\n]\n\nROOT_URLCONF = \'cvat.urls\'\n\nTEMPLATES = [\n    {\n        \'BACKEND\': \'django.template.backends.django.DjangoTemplates\',\n        \'DIRS\': [],\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'context_processors\': [\n                \'django.template.context_processors.debug\',\n                \'django.template.context_processors.request\',\n                \'django.contrib.auth.context_processors.auth\',\n                \'django.contrib.messages.context_processors.messages\',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \'cvat.wsgi.application\'\n\n# Django Auth\nDJANGO_AUTH_TYPE = \'BASIC\'\nDJANGO_AUTH_DEFAULT_GROUPS = []\nLOGIN_URL = \'login\'\nLOGIN_REDIRECT_URL = \'/\'\nAUTH_LOGIN_NOTE = \'<p>Have not registered yet? <a href=""/auth/register"">Register here</a>.</p>\'\n\nAUTHENTICATION_BACKENDS = [\n    \'rules.permissions.ObjectPermissionBackend\',\n    \'django.contrib.auth.backends.ModelBackend\'\n]\n\n# https://github.com/pennersr/django-allauth\nACCOUNT_EMAIL_VERIFICATION = \'none\'\n\n# Django-RQ\n# https://github.com/rq/django-rq\n\nRQ_QUEUES = {\n    \'default\': {\n        \'HOST\': \'localhost\',\n        \'PORT\': 6379,\n        \'DB\': 0,\n        \'DEFAULT_TIMEOUT\': \'4h\'\n    },\n    \'low\': {\n        \'HOST\': \'localhost\',\n        \'PORT\': 6379,\n        \'DB\': 0,\n        \'DEFAULT_TIMEOUT\': \'24h\'\n    }\n}\n\nRQ_SHOW_ADMIN_LINK = True\nRQ_EXCEPTION_HANDLERS = [\'cvat.apps.engine.views.rq_handler\']\n\n\n# JavaScript and CSS compression\n# https://django-compressor.readthedocs.io\n\nCOMPRESS_CSS_FILTERS = [\n    \'compressor.filters.css_default.CssAbsoluteFilter\',\n    \'compressor.filters.cssmin.rCSSMinFilter\'\n]\nCOMPRESS_JS_FILTERS = []  # No compression for js files (template literals were compressed bad)\n\n# Password validation\n# https://docs.djangoproject.com/en/2.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.UserAttributeSimilarityValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.MinimumLengthValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.CommonPasswordValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.NumericPasswordValidator\',\n    },\n]\n\n# Cache DB access (e.g. for engine.task.get_frame)\n# https://github.com/Suor/django-cacheops\nCACHEOPS_REDIS = {\n    \'host\': \'localhost\', # redis-server is on same machine\n    \'port\': 6379,        # default redis port\n    \'db\': 1,             # SELECT non-default redis database\n}\n\nCACHEOPS = {\n    # Automatically cache any Task.objects.get() calls for 15 minutes\n    # This also includes .first() and .last() calls.\n    \'engine.task\': {\'ops\': \'get\', \'timeout\': 60*15},\n\n    # Automatically cache any Job.objects.get() calls for 15 minutes\n    # This also includes .first() and .last() calls.\n    \'engine.job\': {\'ops\': \'get\', \'timeout\': 60*15},\n}\n\nCACHEOPS_DEGRADE_ON_FAILURE = True\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.0/topics/i18n/\n\nLANGUAGE_CODE = \'en-us\'\n\nTIME_ZONE = os.getenv(\'TZ\', \'Etc/UTC\')\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\nCSRF_COOKIE_NAME = ""csrftoken""\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.0/howto/static-files/\n\nSTATIC_URL = \'/static/\'\nSTATIC_ROOT = os.path.join(BASE_DIR, \'static\')\nos.makedirs(STATIC_ROOT, exist_ok=True)\n\nDATA_ROOT = os.path.join(BASE_DIR, \'data\')\nos.makedirs(DATA_ROOT, exist_ok=True)\n\nMEDIA_DATA_ROOT = os.path.join(DATA_ROOT, \'data\')\nos.makedirs(MEDIA_DATA_ROOT, exist_ok=True)\n\nTASKS_ROOT = os.path.join(DATA_ROOT, \'tasks\')\nos.makedirs(TASKS_ROOT, exist_ok=True)\n\nSHARE_ROOT = os.path.join(BASE_DIR, \'share\')\nos.makedirs(SHARE_ROOT, exist_ok=True)\n\nMODELS_ROOT = os.path.join(DATA_ROOT, \'models\')\nos.makedirs(MODELS_ROOT, exist_ok=True)\n\nLOGS_ROOT = os.path.join(BASE_DIR, \'logs\')\nos.makedirs(MODELS_ROOT, exist_ok=True)\n\nMIGRATIONS_LOGS_ROOT = os.path.join(LOGS_ROOT, \'migrations\')\nos.makedirs(MIGRATIONS_LOGS_ROOT, exist_ok=True)\n\nLOGGING = {\n    \'version\': 1,\n    \'disable_existing_loggers\': False,\n    \'formatters\': {\n        \'standard\': {\n            \'format\': \'[%(asctime)s] %(levelname)s %(name)s: %(message)s\'\n        }\n    },\n    \'handlers\': {\n        \'console\': {\n            \'class\': \'logging.StreamHandler\',\n            \'filters\': [],\n            \'formatter\': \'standard\',\n        },\n        \'server_file\': {\n            \'class\': \'logging.handlers.RotatingFileHandler\',\n            \'level\': \'DEBUG\',\n            \'filename\': os.path.join(BASE_DIR, \'logs\', \'cvat_server.log\'),\n            \'formatter\': \'standard\',\n            \'maxBytes\': 1024*1024*50, # 50 MB\n            \'backupCount\': 5,\n        },\n        \'logstash\': {\n            \'level\': \'INFO\',\n            \'class\': \'logstash.TCPLogstashHandler\',\n            \'host\': os.getenv(\'DJANGO_LOG_SERVER_HOST\', \'localhost\'),\n            \'port\': os.getenv(\'DJANGO_LOG_SERVER_PORT\', 5000),\n            \'version\': 1,\n            \'message_type\': \'django\',\n        }\n    },\n    \'loggers\': {\n        \'cvat.server\': {\n            \'handlers\': [\'console\', \'server_file\'],\n            \'level\': os.getenv(\'DJANGO_LOG_LEVEL\', \'DEBUG\'),\n        },\n\n        \'cvat.client\': {\n            \'handlers\': [],\n            \'level\': os.getenv(\'DJANGO_LOG_LEVEL\', \'DEBUG\'),\n        },\n\n        \'revproxy\': {\n            \'handlers\': [\'console\', \'server_file\'],\n            \'level\': os.getenv(\'DJANGO_LOG_LEVEL\', \'DEBUG\')\n        },\n        \'django\': {\n            \'handlers\': [\'console\', \'server_file\'],\n            \'level\': \'INFO\',\n            \'propagate\': True\n        }\n    },\n}\n\nif os.getenv(\'DJANGO_LOG_SERVER_HOST\'):\n    LOGGING[\'loggers\'][\'cvat.server\'][\'handlers\'] += [\'logstash\']\n    LOGGING[\'loggers\'][\'cvat.client\'][\'handlers\'] += [\'logstash\']\n\nDATA_UPLOAD_MAX_MEMORY_SIZE = 100 * 1024 * 1024  # 100 MB\nDATA_UPLOAD_MAX_NUMBER_FIELDS = None   # this django check disabled\nLOCAL_LOAD_MAX_FILES_COUNT = 500\nLOCAL_LOAD_MAX_FILES_SIZE = 512 * 1024 * 1024  # 512 MB\n\nDATUMARO_PATH = os.path.join(BASE_DIR, \'datumaro\')\nsys.path.append(DATUMARO_PATH)\n\nRESTRICTIONS = {\n    \'user_agreements\': [],\n\n    # this setting limits the number of tasks for the user\n    \'task_limit\': None,\n\n    # this setting reduse task visibility to owner and assignee only\n    \'reduce_task_visibility\': False,\n\n    # allow access to analytics component to users with the following roles\n    \'analytics_access\': (\n        \'engine.role.observer\',\n        \'engine.role.annotator\',\n        \'engine.role.user\',\n        \'engine.role.admin\',\n        ),\n}\n'"
cvat/settings/development.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom .base import *\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nINSTALLED_APPS += [\n    'django_extensions',\n]\n\nALLOWED_HOSTS.append('testserver')\n\n# Django-sendfile:\n# https://github.com/johnsensible/django-sendfile\nSENDFILE_BACKEND = 'sendfile.backends.development'\n\n# Database\n# https://docs.djangoproject.com/en/2.0/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n# Cross-Origin Resource Sharing settings for CVAT UI\nUI_SCHEME = os.environ.get('UI_SCHEME', 'http')\nUI_HOST = os.environ.get('UI_HOST', 'localhost')\nUI_PORT = os.environ.get('UI_PORT', 3000)\nCORS_ALLOW_CREDENTIALS = True\nCSRF_TRUSTED_ORIGINS = [UI_HOST]\nUI_URL = '{}://{}'.format(UI_SCHEME, UI_HOST)\n\nif UI_PORT and UI_PORT != '80':\n    UI_URL += ':{}'.format(UI_PORT)\n\nCORS_ORIGIN_WHITELIST = [UI_URL]\nCORS_REPLACE_HTTPS_REFERER = True\n"""
cvat/settings/production.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom .base import *\n\nDEBUG = False\n\nINSTALLED_APPS += [\n    'mod_wsgi.server',\n]\n\nfor key in RQ_QUEUES:\n    RQ_QUEUES[key]['HOST'] = os.getenv('CVAT_REDIS_HOST', 'cvat_redis')\n\nCACHEOPS_REDIS['host'] = os.getenv('CVAT_REDIS_HOST', 'cvat_redis')\n\n# Django-sendfile:\n# https://github.com/johnsensible/django-sendfile\nSENDFILE_BACKEND = 'sendfile.backends.xsendfile'\n\n# Database\n# https://docs.djangoproject.com/en/2.0/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'HOST': os.getenv('CVAT_POSTGRES_HOST', 'cvat_db'),\n        'NAME': 'cvat',\n        'USER': 'root',\n        'PASSWORD': os.getenv('CVAT_POSTGRES_PASSWORD', ''),\n    }\n}\n"""
cvat/settings/staging.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom .production import *\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nINSTALLED_APPS += [\n    'silk'\n]\n\nMIDDLEWARE += [\n    'silk.middleware.SilkyMiddleware',\n]\n\n# Django profiler\n# https://github.com/jazzband/django-silk\nSILKY_PYTHON_PROFILER = True\nSILKY_PYTHON_PROFILER_BINARY = True\nSILKY_PYTHON_PROFILER_RESULT_PATH = os.path.join(BASE_DIR, 'profiles/')\nos.makedirs(SILKY_PYTHON_PROFILER_RESULT_PATH, exist_ok=True)\nSILKY_AUTHENTICATION = True\nSILKY_AUTHORISATION = True\nSILKY_MAX_REQUEST_BODY_SIZE = 1024\nSILKY_MAX_RESPONSE_BODY_SIZE = 1024\nSILKY_IGNORE_PATHS = ['/admin', '/documentation', '/django-rq', '/auth']\nSILKY_MAX_RECORDED_REQUESTS = 10**4\ndef SILKY_INTERCEPT_FUNC(request):\n    # Ignore all requests which try to get a frame (too many of them)\n    if request.method == 'GET' and '/frames/' in request.path:\n        return False\n\n    return True\n\nSILKY_INTERCEPT_FUNC = SILKY_INTERCEPT_FUNC\n"""
cvat/settings/testing.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom .development import *\nimport tempfile\n\n_temp_dir = tempfile.TemporaryDirectory(suffix=""cvat"")\n\nDATA_ROOT = os.path.join(_temp_dir.name, \'data\')\nos.makedirs(DATA_ROOT, exist_ok=True)\n\nSHARE_ROOT = os.path.join(_temp_dir.name, \'share\')\nos.makedirs(SHARE_ROOT, exist_ok=True)\n\nMEDIA_DATA_ROOT = os.path.join(DATA_ROOT, \'data\')\nos.makedirs(MEDIA_DATA_ROOT, exist_ok=True)\n\nTASKS_ROOT = os.path.join(DATA_ROOT, \'tasks\')\nos.makedirs(TASKS_ROOT, exist_ok=True)\n\nMODELS_ROOT = os.path.join(DATA_ROOT, \'models\')\nos.makedirs(MODELS_ROOT, exist_ok=True)\n\n\n# To avoid ERROR django.security.SuspiciousFileOperation:\n# The joined path (...) is located outside of the base path component\nMEDIA_ROOT = _temp_dir.name\n\n# Suppress all logs by default\nfor logger in LOGGING[""loggers""].values():\n    if isinstance(logger, dict) and ""level"" in logger:\n        logger[""level""] = ""ERROR""\n\nLOGGING[""handlers""][""server_file""] = LOGGING[""handlers""][""console""]\n\nPASSWORD_HASHERS = (\n    \'django.contrib.auth.hashers.MD5PasswordHasher\',\n)\n\n# When you run ./manage.py test, Django looks at the TEST_RUNNER setting to\n# determine what to do. By default, TEST_RUNNER points to\n# \'django.test.runner.DiscoverRunner\'. This class defines the default Django\n# testing behavior.\nTEST_RUNNER = ""cvat.settings.testing.PatchedDiscoverRunner""\n\nfrom django.test.runner import DiscoverRunner\nclass PatchedDiscoverRunner(DiscoverRunner):\n    def __init__(self, *args, **kwargs):\n        # Used fakeredis for testing (don\'t affect production redis)\n        from fakeredis import FakeRedis, FakeStrictRedis\n        import django_rq.queues\n        simple_redis = FakeRedis()\n        strict_redis = FakeStrictRedis()\n        django_rq.queues.get_redis_connection = lambda _, strict: strict_redis \\\n            if strict else simple_redis\n\n        # Run all RQ requests syncroniously\n        for config in RQ_QUEUES.values():\n            config[""ASYNC""] = False\n\n        super().__init__(*args, **kwargs)'"
cvat/utils/version.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n#\n# Note: It is slightly re-implemented Django version of code. We cannot use\n# get_version from django.utils.version module because get_git_changeset will\n# always return empty value (cwd=repo_dir isn\'t correct). Also it gives us a\n# way to define the version as we like.\n\nimport datetime\nimport os\nimport subprocess\n\ndef get_version(version):\n    """"""Return a PEP 440-compliant version number from VERSION.""""""\n    # Now build the two parts of the version number:\n    # main = X.Y[.Z]\n    # sub = .devN - for pre-alpha releases\n    #     | {a|b|rc}N - for alpha, beta, and rc releases\n\n    main = get_main_version(version)\n\n    sub = \'\'\n    if version[3] == \'alpha\' and version[4] == 0:\n        git_changeset = get_git_changeset()\n        if git_changeset:\n            sub = \'.dev%s\' % git_changeset\n\n    elif version[3] != \'final\':\n        mapping = {\'alpha\': \'a\', \'beta\': \'b\', \'rc\': \'rc\'}\n        sub = mapping[version[3]] + str(version[4])\n\n    return main + sub\n\ndef get_main_version(version):\n    """"""Return main version (X.Y[.Z]) from VERSION.""""""\n    parts = 2 if version[2] == 0 else 3\n    return \'.\'.join(str(x) for x in version[:parts])\n\ndef get_git_changeset():\n    """"""Return a numeric identifier of the latest git changeset.\n\n    The result is the UTC timestamp of the changeset in YYYYMMDDHHMMSS format.\n    This value isn\'t guaranteed to be unique, but collisions are very unlikely,\n    so it\'s sufficient for generating the development version numbers.\n    """"""\n    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    git_log = subprocess.Popen(\n        \'git log --pretty=format:%ct --quiet -1 HEAD\',\n        stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n        shell=True, cwd=repo_dir, universal_newlines=True,\n    )\n    timestamp = git_log.communicate()[0]\n    try:\n        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))\n    except ValueError:\n        return None\n    return timestamp.strftime(\'%Y%m%d%H%M%S\')\n\n'"
datumaro/datumaro/__init__.py,0,b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
datumaro/datumaro/__main__.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport sys\n\nfrom datumaro.cli.__main__ import main\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n"""
datumaro/datumaro/version.py,0,"b""VERSION = '0.1.0'"""
datumaro/tests/__init__.py,0,b''
datumaro/tests/test_RISE.py,0,"b""from collections import namedtuple\nimport numpy as np\n\nfrom unittest import TestCase\n\nfrom datumaro.components.extractor import Label, Bbox\nfrom datumaro.components.launcher import Launcher\nfrom datumaro.components.algorithms.rise import RISE\n\n\nclass RiseTest(TestCase):\n    def test_rise_can_be_applied_to_classification_model(self):\n        class TestLauncher(Launcher):\n            def __init__(self, class_count, roi, **kwargs):\n                self.class_count = class_count\n                self.roi = roi\n\n            def launch(self, inputs):\n                for inp in inputs:\n                    yield self._process(inp)\n\n            def _process(self, image):\n                roi = self.roi\n                roi_area = (roi[1] - roi[0]) * (roi[3] - roi[2])\n                if 0.5 * roi_area < np.sum(image[roi[0]:roi[1], roi[2]:roi[3], 0]):\n                    cls = 0\n                else:\n                    cls = 1\n\n                cls_conf = 0.5\n                other_conf = (1.0 - cls_conf) / (self.class_count - 1)\n\n                return [\n                    Label(i, attributes={\n                        'score': cls_conf if cls == i else other_conf }) \\\n                    for i in range(self.class_count)\n                ]\n\n        roi = [70, 90, 7, 90]\n        model = TestLauncher(class_count=3, roi=roi)\n\n        rise = RISE(model, max_samples=(7 * 7) ** 2, mask_width=7, mask_height=7)\n\n        image = np.ones((100, 100, 3))\n        heatmaps = next(rise.apply(image))\n\n        self.assertEqual(1, len(heatmaps))\n\n        heatmap = heatmaps[0]\n        self.assertEqual(image.shape[:2], heatmap.shape)\n\n        h_sum = np.sum(heatmap)\n        h_area = np.prod(heatmap.shape)\n        roi_sum = np.sum(heatmap[roi[0]:roi[1], roi[2]:roi[3]])\n        roi_area = (roi[1] - roi[0]) * (roi[3] - roi[2])\n        roi_den = roi_sum / roi_area\n        hrest_den = (h_sum - roi_sum) / (h_area - roi_area)\n        self.assertLess(hrest_den, roi_den)\n\n    def test_rise_can_be_applied_to_detection_model(self):\n        ROI = namedtuple('ROI',\n            ['threshold', 'x', 'y', 'w', 'h', 'label'])\n\n        class TestLauncher(Launcher):\n            def __init__(self, rois, class_count, fp_count=4, pixel_jitter=20, **kwargs):\n                self.rois = rois\n                self.roi_base_sums = [None, ] * len(rois)\n                self.class_count = class_count\n                self.fp_count = fp_count\n                self.pixel_jitter = pixel_jitter\n\n            @staticmethod\n            def roi_value(roi, image):\n                return np.sum(\n                    image[roi.y:roi.y + roi.h, roi.x:roi.x + roi.w, :])\n\n            def launch(self, inputs):\n                for inp in inputs:\n                    yield self._process(inp)\n\n            def _process(self, image):\n                detections = []\n                for i, roi in enumerate(self.rois):\n                    roi_sum = self.roi_value(roi, image)\n                    roi_base_sum = self.roi_base_sums[i]\n                    first_run = roi_base_sum is None\n                    if first_run:\n                        roi_base_sum = roi_sum\n                        self.roi_base_sums[i] = roi_base_sum\n\n                    cls_conf = roi_sum / roi_base_sum\n\n                    if roi.threshold < roi_sum / roi_base_sum:\n                        cls = roi.label\n                        detections.append(\n                            Bbox(roi.x, roi.y, roi.w, roi.h,\n                                label=cls, attributes={'score': cls_conf})\n                        )\n\n                    if first_run:\n                        continue\n                    for j in range(self.fp_count):\n                        if roi.threshold < cls_conf:\n                            cls = roi.label\n                        else:\n                            cls = (i + j) % self.class_count\n                        box = [roi.x, roi.y, roi.w, roi.h]\n                        offset = (np.random.rand(4) - 0.5) * self.pixel_jitter\n                        detections.append(\n                            Bbox(*(box + offset),\n                                label=cls, attributes={'score': cls_conf})\n                        )\n\n                return detections\n\n        rois = [\n            ROI(0.3, 10, 40, 30, 10, 0),\n            ROI(0.5, 70, 90, 7, 10, 0),\n            ROI(0.7, 5, 20, 40, 60, 2),\n            ROI(0.9, 30, 20, 10, 40, 1),\n        ]\n        model = model = TestLauncher(class_count=3, rois=rois)\n\n        rise = RISE(model, max_samples=(7 * 7) ** 2, mask_width=7, mask_height=7)\n\n        image = np.ones((100, 100, 3))\n        heatmaps = next(rise.apply(image))\n        heatmaps_class_count = len(set([roi.label for roi in rois]))\n        self.assertEqual(heatmaps_class_count + len(rois), len(heatmaps))\n\n        # import cv2\n        # roi_image = image.copy()\n        # for i, roi in enumerate(rois):\n        #     cv2.rectangle(roi_image, (roi.x, roi.y), (roi.x + roi.w, roi.y + roi.h), (32 * i) * 3)\n        # cv2.imshow('img', roi_image)\n\n        for c in range(heatmaps_class_count):\n            class_roi = np.zeros(image.shape[:2])\n            for i, roi in enumerate(rois):\n                if roi.label != c:\n                    continue\n                class_roi[roi.y:roi.y + roi.h, roi.x:roi.x + roi.w] \\\n                    += roi.threshold\n\n            heatmap = heatmaps[c]\n\n            roi_pixels = heatmap[class_roi != 0]\n            h_sum = np.sum(roi_pixels)\n            h_area = np.sum(roi_pixels != 0)\n            h_den = h_sum / h_area\n\n            rest_pixels = heatmap[class_roi == 0]\n            r_sum = np.sum(rest_pixels)\n            r_area = np.sum(rest_pixels != 0)\n            r_den = r_sum / r_area\n\n            # print(r_den, h_den)\n            # cv2.imshow('class %s' % c, heatmap)\n            self.assertLess(r_den, h_den)\n\n        for i, roi in enumerate(rois):\n            heatmap = heatmaps[heatmaps_class_count + i]\n            h_sum = np.sum(heatmap)\n            h_area = np.prod(heatmap.shape)\n            roi_sum = np.sum(heatmap[roi.y:roi.y + roi.h, roi.x:roi.x + roi.w])\n            roi_area = roi.h * roi.w\n            roi_den = roi_sum / roi_area\n            hrest_den = (h_sum - roi_sum) / (h_area - roi_area)\n            # print(hrest_den, h_den)\n            # cv2.imshow('roi %s' % i, heatmap)\n            self.assertLess(hrest_den, roi_den)\n        # cv2.waitKey(0)\n\n    @staticmethod\n    def DISABLED_test_roi_nms():\n        ROI = namedtuple('ROI',\n            ['conf', 'x', 'y', 'w', 'h', 'label'])\n\n        class_count = 3\n        noisy_count = 3\n        rois = [\n            ROI(0.3, 10, 40, 30, 10, 0),\n            ROI(0.5, 70, 90, 7, 10, 0),\n            ROI(0.7, 5, 20, 40, 60, 2),\n            ROI(0.9, 30, 20, 10, 40, 1),\n        ]\n        pixel_jitter = 10\n\n        detections = []\n        for i, roi in enumerate(rois):\n            detections.append(\n                Bbox(roi.x, roi.y, roi.w, roi.h,\n                    label=roi.label, attributes={'score': roi.conf})\n            )\n\n            for j in range(noisy_count):\n                cls_conf = roi.conf * j / noisy_count\n                cls = (i + j) % class_count\n                box = [roi.x, roi.y, roi.w, roi.h]\n                offset = (np.random.rand(4) - 0.5) * pixel_jitter\n                detections.append(\n                    Bbox(*(box + offset),\n                        label=cls, attributes={'score': cls_conf})\n                )\n\n        import cv2\n        image = np.zeros((100, 100, 3))\n        for i, det in enumerate(detections):\n            roi = ROI(det.attributes['score'], *det.get_bbox(), det.label)\n            p1 = (int(roi.x), int(roi.y))\n            p2 = (int(roi.x + roi.w), int(roi.y + roi.h))\n            c = (0, 1 * (i % (1 + noisy_count) == 0), 1)\n            cv2.rectangle(image, p1, p2, c)\n            cv2.putText(image, 'd%s-%s-%.2f' % (i, roi.label, roi.conf),\n                p1, cv2.FONT_HERSHEY_SIMPLEX, 0.25, c)\n        cv2.imshow('nms_image', image)\n        cv2.waitKey(0)\n\n        nms_boxes = RISE.nms(detections, iou_thresh=0.25)\n        print(len(detections), len(nms_boxes))\n\n        for i, det in enumerate(nms_boxes):\n            roi = ROI(det.attributes['score'], *det.get_bbox(), det.label)\n            p1 = (int(roi.x), int(roi.y))\n            p2 = (int(roi.x + roi.w), int(roi.y + roi.h))\n            c = (0, 1, 0)\n            cv2.rectangle(image, p1, p2, c)\n            cv2.putText(image, 'p%s-%s-%.2f' % (i, roi.label, roi.conf),\n                p1, cv2.FONT_HERSHEY_SIMPLEX, 0.25, c)\n        cv2.imshow('nms_image', image)\n        cv2.waitKey(0)"""
datumaro/tests/test_coco_format.py,0,"b""import numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.components.project import Project\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    AnnotationType, Label, Mask, Points, Polygon, Bbox, Caption,\n    LabelCategories, PointsCategories\n)\nfrom datumaro.plugins.coco_format.converter import (\n    CocoConverter,\n    CocoImageInfoConverter,\n    CocoCaptionsConverter,\n    CocoInstancesConverter,\n    CocoPersonKeypointsConverter,\n    CocoLabelsConverter,\n)\nfrom datumaro.plugins.coco_format.importer import CocoImporter\nfrom datumaro.util.image import Image\nfrom datumaro.util.test_utils import TestDir, compare_datasets\n\n\nDUMMY_DATASET_DIR = osp.join(osp.dirname(__file__), 'assets', 'coco_dataset')\n\nclass CocoImporterTest(TestCase):\n    def test_can_import(self):\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.ones((10, 5, 3)), subset='val',\n                        annotations=[\n                            Polygon([0, 0, 1, 0, 1, 2, 0, 2], label=0,\n                                id=1, group=1, attributes={'is_crowd': False}),\n                            Mask(np.array(\n                                [[1, 0, 0, 1, 0]] * 5 +\n                                [[1, 1, 1, 1, 0]] * 5\n                                ), label=0,\n                                id=2, group=2, attributes={'is_crowd': True}),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add('TEST')\n                return { AnnotationType.label: label_cat }\n\n        dataset = Project.import_from(DUMMY_DATASET_DIR, 'coco') \\\n            .make_dataset()\n\n        compare_datasets(self, DstExtractor(), dataset)\n\n    def test_can_detect(self):\n        self.assertTrue(CocoImporter.detect(DUMMY_DATASET_DIR))\n\nclass CocoConverterTest(TestCase):\n    def _test_save_and_load(self, source_dataset, converter, test_dir,\n            target_dataset=None, importer_args=None):\n        converter(source_dataset, test_dir)\n\n        if importer_args is None:\n            importer_args = {}\n        parsed_dataset = CocoImporter()(test_dir, **importer_args).make_dataset()\n\n        if target_dataset is None:\n            target_dataset = source_dataset\n\n        compare_datasets(self, expected=target_dataset, actual=parsed_dataset)\n\n    def test_can_save_and_load_captions(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train',\n                        annotations=[\n                            Caption('hello', id=1, group=1),\n                            Caption('world', id=2, group=2),\n                        ]),\n                    DatasetItem(id=2, subset='train',\n                        annotations=[\n                            Caption('test', id=3, group=3),\n                        ]),\n\n                    DatasetItem(id=3, subset='val',\n                        annotations=[\n                            Caption('word', id=1, group=1),\n                        ]\n                    ),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                CocoCaptionsConverter(), test_dir)\n\n    def test_can_save_and_load_instances(self):\n        label_categories = LabelCategories()\n        for i in range(10):\n            label_categories.add(str(i))\n        categories = { AnnotationType.label: label_categories }\n\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train', image=np.ones((4, 4, 3)),\n                        annotations=[\n                            # Bbox + single polygon\n                            Bbox(0, 1, 2, 2,\n                                label=2, group=1, id=1,\n                                attributes={ 'is_crowd': False }),\n                            Polygon([0, 1, 2, 1, 2, 3, 0, 3],\n                                attributes={ 'is_crowd': False },\n                                label=2, group=1, id=1),\n                        ]),\n                    DatasetItem(id=2, subset='train', image=np.ones((4, 4, 3)),\n                        annotations=[\n                            # Mask + bbox\n                            Mask(np.array([\n                                    [0, 1, 0, 0],\n                                    [0, 1, 0, 0],\n                                    [0, 1, 1, 1],\n                                    [0, 0, 0, 0]],\n                                ),\n                                attributes={ 'is_crowd': True },\n                                label=4, group=3, id=3),\n                            Bbox(1, 0, 2, 2, label=4, group=3, id=3,\n                                attributes={ 'is_crowd': True }),\n                        ]),\n\n                    DatasetItem(id=3, subset='val', image=np.ones((4, 4, 3)),\n                        annotations=[\n                            # Bbox + mask\n                            Bbox(0, 1, 2, 2, label=4, group=3, id=3,\n                                attributes={ 'is_crowd': True }),\n                            Mask(np.array([\n                                    [0, 0, 0, 0],\n                                    [1, 1, 1, 0],\n                                    [1, 1, 0, 0],\n                                    [0, 0, 0, 0]],\n                                ),\n                                attributes={ 'is_crowd': True },\n                                label=4, group=3, id=3),\n                        ]),\n                ])\n\n            def categories(self):\n                return categories\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train', image=np.ones((4, 4, 3)),\n                        annotations=[\n                            Polygon([0, 1, 2, 1, 2, 3, 0, 3],\n                                attributes={ 'is_crowd': False },\n                                label=2, group=1, id=1),\n                        ]),\n                    DatasetItem(id=2, subset='train', image=np.ones((4, 4, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 1, 0, 0],\n                                    [0, 1, 0, 0],\n                                    [0, 1, 1, 1],\n                                    [0, 0, 0, 0]],\n                                ),\n                                attributes={ 'is_crowd': True },\n                                label=4, group=3, id=3),\n                        ]),\n\n                    DatasetItem(id=3, subset='val', image=np.ones((4, 4, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 0, 0],\n                                    [1, 1, 1, 0],\n                                    [1, 1, 0, 0],\n                                    [0, 0, 0, 0]],\n                                ),\n                                attributes={ 'is_crowd': True },\n                                label=4, group=3, id=3),\n                        ]),\n                ])\n\n            def categories(self):\n                return categories\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                CocoInstancesConverter(), test_dir,\n                target_dataset=DstExtractor())\n\n    def test_can_merge_polygons_on_loading(self):\n        label_categories = LabelCategories()\n        for i in range(10):\n            label_categories.add(str(i))\n        categories = { AnnotationType.label: label_categories }\n\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((6, 10, 3)),\n                        annotations=[\n                            Polygon([0, 0, 4, 0, 4, 4],\n                                label=3, id=4, group=4),\n                            Polygon([5, 0, 9, 0, 5, 5],\n                                label=3, id=4, group=4),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                return categories\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((6, 10, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n                                [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],\n                                [0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n                                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n                                # only internal fragment (without the border),\n                                # but not everywhere...\n                            ),\n                            label=3, id=4, group=4,\n                            attributes={ 'is_crowd': False }),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                return categories\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(SrcExtractor(),\n                CocoInstancesConverter(), test_dir,\n                importer_args={'merge_instance_polygons': True},\n                target_dataset=DstExtractor())\n\n    def test_can_crop_covered_segments(self):\n        label_categories = LabelCategories()\n        for i in range(10):\n            label_categories.add(str(i))\n\n        class SrcTestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 1, 1, 1],\n                                    [0, 0, 1, 1, 1],\n                                    [1, 1, 0, 1, 1],\n                                    [1, 1, 1, 0, 0],\n                                    [1, 1, 1, 0, 0]],\n                                ),\n                                label=2, id=1, z_order=0),\n                            Polygon([1, 1, 4, 1, 4, 4, 1, 4],\n                                label=1, id=2, z_order=1),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: label_categories }\n\n        class DstTestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 1, 1, 1],\n                                    [0, 0, 0, 0, 1],\n                                    [1, 0, 0, 0, 1],\n                                    [1, 0, 0, 0, 0],\n                                    [1, 1, 1, 0, 0]],\n                                ),\n                                attributes={ 'is_crowd': True },\n                                label=2, id=1, group=1),\n\n                            Polygon([1, 1, 4, 1, 4, 4, 1, 4],\n                                label=1, id=2, group=2,\n                                attributes={ 'is_crowd': False }),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: label_categories }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(SrcTestExtractor(),\n                CocoInstancesConverter(crop_covered=True), test_dir,\n                target_dataset=DstTestExtractor())\n\n    def test_can_convert_polygons_to_mask(self):\n        label_categories = LabelCategories()\n        for i in range(10):\n            label_categories.add(str(i))\n\n        class SrcTestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((6, 10, 3)),\n                        annotations=[\n                            Polygon([0, 0, 4, 0, 4, 4],\n                                label=3, id=4, group=4),\n                            Polygon([5, 0, 9, 0, 5, 5],\n                                label=3, id=4, group=4),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: label_categories }\n\n        class DstTestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((6, 10, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n                                    [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],\n                                    [0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n                                    # only internal fragment (without the border),\n                                    # but not everywhere...\n                                ),\n                                attributes={ 'is_crowd': True },\n                                label=3, id=4, group=4),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: label_categories }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(SrcTestExtractor(),\n                CocoInstancesConverter(segmentation_mode='mask'), test_dir,\n                target_dataset=DstTestExtractor())\n\n    def test_can_convert_masks_to_polygons(self):\n        label_categories = LabelCategories()\n        for i in range(10):\n            label_categories.add(str(i))\n\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n                                    [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],\n                                    [0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                ]),\n                                label=3, id=4, group=4),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: label_categories }\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Polygon(\n                                [3.0, 2.5, 1.0, 0.0, 3.5, 0.0, 3.0, 2.5],\n                                label=3, id=4, group=4,\n                                attributes={ 'is_crowd': False }),\n                            Polygon(\n                                [5.0, 3.5, 4.5, 0.0, 8.0, 0.0, 5.0, 3.5],\n                                label=3, id=4, group=4,\n                                attributes={ 'is_crowd': False }),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: label_categories }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(SrcExtractor(),\n                CocoInstancesConverter(segmentation_mode='polygons'), test_dir,\n                target_dataset=DstExtractor())\n\n    def test_can_save_and_load_images(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train'),\n                    DatasetItem(id=2, subset='train'),\n\n                    DatasetItem(id=2, subset='val'),\n                    DatasetItem(id=3, subset='val'),\n                    DatasetItem(id=4, subset='val'),\n\n                    DatasetItem(id=5, subset='test'),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                CocoImageInfoConverter(), test_dir)\n\n    def test_can_save_and_load_labels(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train',\n                        annotations=[\n                            Label(4, id=1, group=1),\n                            Label(9, id=2, group=2),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                label_categories = LabelCategories()\n                for i in range(10):\n                    label_categories.add(str(i))\n                return {\n                    AnnotationType.label: label_categories,\n                }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                CocoLabelsConverter(), test_dir)\n\n    def test_can_save_and_load_keypoints(self):\n        label_categories = LabelCategories()\n        points_categories = PointsCategories()\n        for i in range(10):\n            label_categories.add(str(i))\n            points_categories.add(i, joints=[[0, 1], [1, 2]])\n        categories = {\n            AnnotationType.label: label_categories,\n            AnnotationType.points: points_categories,\n        }\n\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train', image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            # Full instance annotations: polygon + keypoints\n                            Points([0, 0, 0, 2, 4, 1], [0, 1, 2],\n                                label=3, group=1, id=1),\n                            Polygon([0, 0, 4, 0, 4, 4],\n                                label=3, group=1, id=1),\n\n                            # Full instance annotations: bbox + keypoints\n                            Points([1, 2, 3, 4, 2, 3], group=2, id=2),\n                            Bbox(1, 2, 2, 2, group=2, id=2),\n\n                            # Solitary keypoints\n                            Points([1, 2, 0, 2, 4, 1], label=5, id=3),\n\n                            # Some other solitary annotations (bug #1387)\n                            Polygon([0, 0, 4, 0, 4, 4], label=3, id=4),\n\n                            # Solitary keypoints with no label\n                            Points([0, 0, 1, 2, 3, 4], [0, 1, 2], id=5),\n                        ])\n                ])\n\n            def categories(self):\n                return categories\n\n        class DstTestExtractor(TestExtractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train', image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Points([0, 0, 0, 2, 4, 1], [0, 1, 2],\n                                label=3, group=1, id=1,\n                                attributes={'is_crowd': False}),\n                            Polygon([0, 0, 4, 0, 4, 4],\n                                label=3, group=1, id=1,\n                                attributes={'is_crowd': False}),\n\n                            Points([1, 2, 3, 4, 2, 3],\n                                group=2, id=2,\n                                attributes={'is_crowd': False}),\n                            Polygon([1, 2, 3, 2, 3, 4, 1, 4],\n                                group=2, id=2,\n                                attributes={'is_crowd': False}),\n\n                            Points([1, 2, 0, 2, 4, 1],\n                                label=5, group=3, id=3,\n                                attributes={'is_crowd': False}),\n                            Polygon([0, 1, 4, 1, 4, 2, 0, 2],\n                                label=5, group=3, id=3,\n                                attributes={'is_crowd': False}),\n\n                            Points([0, 0, 1, 2, 3, 4], [0, 1, 2],\n                                group=5, id=5,\n                                attributes={'is_crowd': False}),\n                            Polygon([1, 2, 3, 2, 3, 4, 1, 4],\n                                group=5, id=5,\n                                attributes={'is_crowd': False}),\n                        ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                CocoPersonKeypointsConverter(), test_dir,\n                target_dataset=DstTestExtractor())\n\n    def test_can_save_dataset_with_no_subsets(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1),\n                    DatasetItem(id=2),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: LabelCategories() }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                CocoConverter(), test_dir)\n\n    def test_can_save_dataset_with_image_info(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=Image(path='1.jpg', size=(10, 15))),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                CocoConverter(tasks='image_info'), test_dir)"""
datumaro/tests/test_command_targets.py,0,"b""import numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.components.project import Project\nfrom datumaro.util.command_targets import ProjectTarget, \\\n    ImageTarget, SourceTarget\nfrom datumaro.util.image import save_image\nfrom datumaro.util.test_utils import TestDir\n\n\nclass CommandTargetsTest(TestCase):\n    def test_image_false_when_no_file(self):\n        target = ImageTarget()\n\n        status = target.test('somepath.jpg')\n\n        self.assertFalse(status)\n\n    def test_image_false_when_false(self):\n        with TestDir() as test_dir:\n            path = osp.join(test_dir, 'test.jpg')\n            with open(path, 'w+') as f:\n                f.write('qwerty123')\n\n            target = ImageTarget()\n\n            status = target.test(path)\n\n            self.assertFalse(status)\n\n    def test_image_true_when_true(self):\n        with TestDir() as test_dir:\n            path = osp.join(test_dir, 'test.jpg')\n            save_image(path, np.ones([10, 7, 3]))\n\n            target = ImageTarget()\n\n            status = target.test(path)\n\n            self.assertTrue(status)\n\n    def test_project_false_when_no_file(self):\n        target = ProjectTarget()\n\n        status = target.test('somepath.jpg')\n\n        self.assertFalse(status)\n\n    def test_project_false_when_no_name(self):\n        target = ProjectTarget(project=Project())\n\n        status = target.test('')\n\n        self.assertFalse(status)\n\n    def test_project_true_when_project_file(self):\n        with TestDir() as test_dir:\n            path = osp.join(test_dir, 'test.jpg')\n            Project().save(path)\n\n            target = ProjectTarget()\n\n            status = target.test(path)\n\n            self.assertTrue(status)\n\n    def test_project_true_when_project_name(self):\n        project_name = 'qwerty'\n        project = Project({\n            'project_name': project_name\n        })\n        target = ProjectTarget(project=project)\n\n        status = target.test(project_name)\n\n        self.assertTrue(status)\n\n    def test_project_false_when_not_project_name(self):\n        project_name = 'qwerty'\n        project = Project({\n            'project_name': project_name\n        })\n        target = ProjectTarget(project=project)\n\n        status = target.test(project_name + '123')\n\n        self.assertFalse(status)\n\n    def test_project_false_when_not_project_file(self):\n        with TestDir() as test_dir:\n            path = osp.join(test_dir, 'test.jpg')\n            with open(path, 'w+') as f:\n                f.write('wqererw')\n\n            target = ProjectTarget()\n\n            status = target.test(path)\n\n            self.assertFalse(status)\n\n    def test_source_false_when_no_project(self):\n        target = SourceTarget()\n\n        status = target.test('qwerty123')\n\n        self.assertFalse(status)\n\n    def test_source_true_when_source_exists(self):\n        source_name = 'qwerty'\n        project = Project()\n        project.add_source(source_name)\n        target = SourceTarget(project=project)\n\n        status = target.test(source_name)\n\n        self.assertTrue(status)\n\n    def test_source_false_when_source_doesnt_exist(self):\n        source_name = 'qwerty'\n        project = Project()\n        project.add_source(source_name)\n        target = SourceTarget(project=project)\n\n        status = target.test(source_name + '123')\n\n        self.assertFalse(status)"""
datumaro/tests/test_cvat_format.py,0,"b""import numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    AnnotationType, Points, Polygon, PolyLine, Bbox, Label,\n    LabelCategories,\n)\nfrom datumaro.plugins.cvat_format.importer import CvatImporter\nfrom datumaro.plugins.cvat_format.converter import CvatConverter\nfrom datumaro.util.image import Image\nfrom datumaro.util.test_utils import TestDir, compare_datasets\n\n\nDUMMY_IMAGE_DATASET_DIR = osp.join(osp.dirname(__file__),\n    'assets', 'cvat_dataset', 'for_images')\n\nDUMMY_VIDEO_DATASET_DIR = osp.join(osp.dirname(__file__),\n    'assets', 'cvat_dataset', 'for_video')\n\nclass CvatImporterTest(TestCase):\n    def test_can_detect_image(self):\n        self.assertTrue(CvatImporter.detect(DUMMY_IMAGE_DATASET_DIR))\n\n    def test_can_detect_video(self):\n        self.assertTrue(CvatImporter.detect(DUMMY_VIDEO_DATASET_DIR))\n\n    def test_can_load_image(self):\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=0, subset='train',\n                        image=np.ones((8, 8, 3)),\n                        annotations=[\n                            Bbox(0, 2, 4, 2, label=0, z_order=1,\n                                attributes={\n                                    'occluded': True,\n                                    'a1': True, 'a2': 'v3'\n                                }),\n                            PolyLine([1, 2, 3, 4, 5, 6, 7, 8],\n                                attributes={'occluded': False}),\n                        ]),\n                    DatasetItem(id=1, subset='train',\n                        image=np.ones((10, 10, 3)),\n                        annotations=[\n                            Polygon([1, 2, 3, 4, 6, 5], z_order=1,\n                                attributes={'occluded': False}),\n                            Points([1, 2, 3, 4, 5, 6], label=1, z_order=2,\n                                attributes={'occluded': False}),\n                        ]),\n                ])\n\n            def categories(self):\n                label_categories = LabelCategories()\n                label_categories.add('label1', attributes={'a1', 'a2'})\n                label_categories.add('label2')\n                return { AnnotationType.label: label_categories }\n\n        parsed_dataset = CvatImporter()(DUMMY_IMAGE_DATASET_DIR).make_dataset()\n\n        compare_datasets(self, DstExtractor(), parsed_dataset)\n\n    def test_can_load_video(self):\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=10, subset='annotations',\n                        image=np.ones((20, 25, 3)),\n                        annotations=[\n                            Bbox(3, 4, 7, 1, label=2,\n                                id=0,\n                                attributes={\n                                    'occluded': True,\n                                    'outside': False, 'keyframe': True,\n                                    'track_id': 0\n                                }),\n                            Points([21.95, 8.00, 2.55, 15.09, 2.23, 3.16],\n                                label=0,\n                                id=1,\n                                attributes={\n                                    'occluded': False,\n                                    'outside': False, 'keyframe': True,\n                                    'track_id': 1, 'hgl': 'hgkf',\n                                }),\n                        ]),\n                    DatasetItem(id=13, subset='annotations',\n                        image=np.ones((20, 25, 3)),\n                        annotations=[\n                            Bbox(7, 6, 7, 2, label=2,\n                                id=0,\n                                attributes={\n                                    'occluded': False,\n                                    'outside': True, 'keyframe': True,\n                                    'track_id': 0\n                                }),\n                            Points([21.95, 8.00, 9.55, 15.09, 5.23, 1.16],\n                                label=0,\n                                id=1,\n                                attributes={\n                                    'occluded': False,\n                                    'outside': True, 'keyframe': True,\n                                    'track_id': 1, 'hgl': 'jk',\n                                }),\n                            PolyLine([7.85, 13.88, 3.50, 6.67, 15.90, 2.00, 13.31, 7.21],\n                                label=2,\n                                id=2,\n                                attributes={\n                                    'occluded': False,\n                                    'outside': False, 'keyframe': True,\n                                    'track_id': 2,\n                                }),\n                        ]),\n                    DatasetItem(id=16, subset='annotations',\n                        image=Image(path='frame_0000016.png',\n                            size=(20, 25)), # no image in the dataset files\n                        annotations=[\n                            Bbox(8, 7, 6, 10, label=2,\n                                id=0,\n                                attributes={\n                                    'occluded': False,\n                                    'outside': True, 'keyframe': True,\n                                    'track_id': 0\n                                }),\n                            PolyLine([7.85, 13.88, 3.50, 6.67, 15.90, 2.00, 13.31, 7.21],\n                                label=2,\n                                id=2,\n                                attributes={\n                                    'occluded': False,\n                                    'outside': True, 'keyframe': True,\n                                    'track_id': 2,\n                                }),\n                        ]),\n                ])\n\n            def categories(self):\n                label_categories = LabelCategories()\n                label_categories.add('klhg', attributes={'hgl'})\n                label_categories.add('z U k')\n                label_categories.add('II')\n                return { AnnotationType.label: label_categories }\n\n        parsed_dataset = CvatImporter()(DUMMY_VIDEO_DATASET_DIR).make_dataset()\n\n        compare_datasets(self, DstExtractor(), parsed_dataset)\n\nclass CvatConverterTest(TestCase):\n    def _test_save_and_load(self, source_dataset, converter, test_dir,\n            target_dataset=None, importer_args=None):\n        converter(source_dataset, test_dir)\n\n        if importer_args is None:\n            importer_args = {}\n        parsed_dataset = CvatImporter()(test_dir, **importer_args).make_dataset()\n\n        if target_dataset is None:\n            target_dataset = source_dataset\n\n        compare_datasets(self, expected=target_dataset, actual=parsed_dataset)\n\n    def test_can_save_and_load(self):\n        label_categories = LabelCategories()\n        for i in range(10):\n            label_categories.add(str(i))\n        label_categories.items[2].attributes.update(['a1', 'a2'])\n        label_categories.attributes.update(['occluded'])\n\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=0, subset='s1', image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Polygon([0, 0, 4, 0, 4, 4],\n                                label=1, group=4,\n                                attributes={ 'occluded': True }),\n                            Points([1, 1, 3, 2, 2, 3],\n                                label=2,\n                                attributes={ 'a1': 'x', 'a2': 42,\n                                    'unknown': 'bar' }),\n                            Label(1),\n                            Label(2, attributes={ 'a1': 'y', 'a2': 44 }),\n                        ]\n                    ),\n                    DatasetItem(id=1, subset='s1',\n                        annotations=[\n                            PolyLine([0, 0, 4, 0, 4, 4],\n                                label=3, id=4, group=4),\n                            Bbox(5, 0, 1, 9,\n                                label=3, id=4, group=4),\n                        ]\n                    ),\n\n                    DatasetItem(id=2, subset='s2', image=np.ones((5, 10, 3)),\n                        annotations=[\n                            Polygon([0, 0, 4, 0, 4, 4], z_order=1,\n                                label=3, group=4,\n                                attributes={ 'occluded': False }),\n                            PolyLine([5, 0, 9, 0, 5, 5]), # will be skipped as no label\n                        ]\n                    ),\n\n                    DatasetItem(id=3, subset='s3', image=Image(\n                        path='3.jpg', size=(2, 4))),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: label_categories }\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=0, subset='s1', image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Polygon([0, 0, 4, 0, 4, 4],\n                                label=1, group=4,\n                                attributes={ 'occluded': True }),\n                            Points([1, 1, 3, 2, 2, 3],\n                                label=2,\n                                attributes={ 'occluded': False,\n                                    'a1': 'x', 'a2': 42 }),\n                            Label(1),\n                            Label(2, attributes={ 'a1': 'y', 'a2': 44 }),\n                        ]\n                    ),\n                    DatasetItem(id=1, subset='s1',\n                        annotations=[\n                            PolyLine([0, 0, 4, 0, 4, 4],\n                                label=3, group=4,\n                                attributes={ 'occluded': False }),\n                            Bbox(5, 0, 1, 9,\n                                label=3, group=4,\n                                attributes={ 'occluded': False }),\n                        ]\n                    ),\n\n                    DatasetItem(id=2, subset='s2', image=np.ones((5, 10, 3)),\n                        annotations=[\n                            Polygon([0, 0, 4, 0, 4, 4], z_order=1,\n                                label=3, group=4,\n                                attributes={ 'occluded': False }),\n                        ]\n                    ),\n\n                    DatasetItem(id=3, subset='s3', image=Image(\n                        path='3.jpg', size=(2, 4))),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: label_categories }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(SrcExtractor(),\n                CvatConverter(save_images=True), test_dir,\n                target_dataset=DstExtractor())\n"""
datumaro/tests/test_datumaro_format.py,0,"b""import numpy as np\n\nfrom unittest import TestCase\n\nfrom datumaro.components.project import Project\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    AnnotationType, Label, Mask, Points, Polygon,\n    PolyLine, Bbox, Caption,\n    LabelCategories, MaskCategories, PointsCategories\n)\nfrom datumaro.plugins.datumaro_format.importer import DatumaroImporter\nfrom datumaro.plugins.datumaro_format.converter import DatumaroConverter\nfrom datumaro.util.mask_tools import generate_colormap\nfrom datumaro.util.image import Image\nfrom datumaro.util.test_utils import TestDir, compare_datasets_strict\n\nclass DatumaroConverterTest(TestCase):\n    def _test_save_and_load(self, source_dataset, converter, test_dir,\n            target_dataset=None, importer_args=None):\n        converter(source_dataset, test_dir)\n\n        if importer_args is None:\n            importer_args = {}\n        parsed_dataset = Project.import_from(\n            test_dir, 'datumaro', **importer_args).make_dataset()\n\n        if target_dataset is None:\n            target_dataset = source_dataset\n\n        compare_datasets_strict(self,\n            expected=target_dataset, actual=parsed_dataset)\n\n    class TestExtractor(Extractor):\n        def __iter__(self):\n            return iter([\n                DatasetItem(id=100, subset='train', image=np.ones((10, 6, 3)),\n                    annotations=[\n                        Caption('hello', id=1),\n                        Caption('world', id=2, group=5),\n                        Label(2, id=3, attributes={\n                            'x': 1,\n                            'y': '2',\n                        }),\n                        Bbox(1, 2, 3, 4, label=4, id=4, z_order=1, attributes={\n                            'score': 1.0,\n                        }),\n                        Bbox(5, 6, 7, 8, id=5, group=5),\n                        Points([1, 2, 2, 0, 1, 1], label=0, id=5, z_order=4),\n                        Mask(label=3, id=5, z_order=2, image=np.ones((2, 3))),\n                    ]),\n                DatasetItem(id=21, subset='train',\n                    annotations=[\n                        Caption('test'),\n                        Label(2),\n                        Bbox(1, 2, 3, 4, 5, id=42, group=42)\n                    ]),\n\n                DatasetItem(id=2, subset='val',\n                    annotations=[\n                        PolyLine([1, 2, 3, 4, 5, 6, 7, 8], id=11, z_order=1),\n                        Polygon([1, 2, 3, 4, 5, 6, 7, 8], id=12, z_order=4),\n                    ]),\n\n                DatasetItem(id=42, subset='test',\n                    attributes={'a1': 5, 'a2': '42'}),\n\n                DatasetItem(id=42),\n                DatasetItem(id=43, image=Image(path='1/b/c.qq', size=(2, 4))),\n            ])\n\n        def categories(self):\n            label_categories = LabelCategories()\n            for i in range(5):\n                label_categories.add('cat' + str(i))\n\n            mask_categories = MaskCategories(\n                generate_colormap(len(label_categories.items)))\n\n            points_categories = PointsCategories()\n            for index, _ in enumerate(label_categories.items):\n                points_categories.add(index, ['cat1', 'cat2'], joints=[[0, 1]])\n\n            return {\n                AnnotationType.label: label_categories,\n                AnnotationType.mask: mask_categories,\n                AnnotationType.points: points_categories,\n            }\n\n    def test_can_save_and_load(self):\n        with TestDir() as test_dir:\n            self._test_save_and_load(self.TestExtractor(),\n                DatumaroConverter(save_images=True), test_dir)\n\n    def test_can_detect(self):\n        with TestDir() as test_dir:\n            DatumaroConverter()(self.TestExtractor(), save_dir=test_dir)\n\n            self.assertTrue(DatumaroImporter.detect(test_dir))\n"""
datumaro/tests/test_diff.py,0,"b""from unittest import TestCase\n\nfrom datumaro.components.extractor import DatasetItem, Label, Bbox\nfrom datumaro.components.comparator import Comparator\n\n\nclass DiffTest(TestCase):\n    def test_no_bbox_diff_with_same_item(self):\n        detections = 3\n        anns = [\n            Bbox(i * 10, 10, 10, 10, label=i,\n                    attributes={'score': (1.0 + i) / detections}) \\\n                for i in range(detections)\n        ]\n        item = DatasetItem(id=0, annotations=anns)\n\n        iou_thresh = 0.5\n        conf_thresh = 0.5\n        comp = Comparator(\n            iou_threshold=iou_thresh, conf_threshold=conf_thresh)\n\n        result = comp.compare_item_bboxes(item, item)\n\n        matches, mispred, a_greater, b_greater = result\n        self.assertEqual(0, len(mispred))\n        self.assertEqual(0, len(a_greater))\n        self.assertEqual(0, len(b_greater))\n        self.assertEqual(len([it for it in item.annotations \\\n                if conf_thresh < it.attributes['score']]),\n            len(matches))\n        for a_bbox, b_bbox in matches:\n            self.assertLess(iou_thresh, a_bbox.iou(b_bbox))\n            self.assertEqual(a_bbox.label, b_bbox.label)\n            self.assertLess(conf_thresh, a_bbox.attributes['score'])\n            self.assertLess(conf_thresh, b_bbox.attributes['score'])\n\n    def test_can_find_bbox_with_wrong_label(self):\n        detections = 3\n        class_count = 2\n        item1 = DatasetItem(id=1, annotations=[\n            Bbox(i * 10, 10, 10, 10, label=i,\n                    attributes={'score': (1.0 + i) / detections}) \\\n                for i in range(detections)\n        ])\n        item2 = DatasetItem(id=2, annotations=[\n            Bbox(i * 10, 10, 10, 10, label=(i + 1) % class_count,\n                    attributes={'score': (1.0 + i) / detections}) \\\n                for i in range(detections)\n        ])\n\n        iou_thresh = 0.5\n        conf_thresh = 0.5\n        comp = Comparator(\n            iou_threshold=iou_thresh, conf_threshold=conf_thresh)\n\n        result = comp.compare_item_bboxes(item1, item2)\n\n        matches, mispred, a_greater, b_greater = result\n        self.assertEqual(len([it for it in item1.annotations \\\n                if conf_thresh < it.attributes['score']]),\n            len(mispred))\n        self.assertEqual(0, len(a_greater))\n        self.assertEqual(0, len(b_greater))\n        self.assertEqual(0, len(matches))\n        for a_bbox, b_bbox in mispred:\n            self.assertLess(iou_thresh, a_bbox.iou(b_bbox))\n            self.assertEqual((a_bbox.label + 1) % class_count, b_bbox.label)\n            self.assertLess(conf_thresh, a_bbox.attributes['score'])\n            self.assertLess(conf_thresh, b_bbox.attributes['score'])\n\n    def test_can_find_missing_boxes(self):\n        detections = 3\n        class_count = 2\n        item1 = DatasetItem(id=1, annotations=[\n            Bbox(i * 10, 10, 10, 10, label=i,\n                    attributes={'score': (1.0 + i) / detections}) \\\n                for i in range(detections) if i % 2 == 0\n        ])\n        item2 = DatasetItem(id=2, annotations=[\n            Bbox(i * 10, 10, 10, 10, label=(i + 1) % class_count,\n                    attributes={'score': (1.0 + i) / detections}) \\\n                for i in range(detections) if i % 2 == 1\n        ])\n\n        iou_thresh = 0.5\n        conf_thresh = 0.5\n        comp = Comparator(\n            iou_threshold=iou_thresh, conf_threshold=conf_thresh)\n\n        result = comp.compare_item_bboxes(item1, item2)\n\n        matches, mispred, a_greater, b_greater = result\n        self.assertEqual(0, len(mispred))\n        self.assertEqual(len([it for it in item1.annotations \\\n                if conf_thresh < it.attributes['score']]),\n            len(a_greater))\n        self.assertEqual(len([it for it in item2.annotations \\\n                if conf_thresh < it.attributes['score']]),\n            len(b_greater))\n        self.assertEqual(0, len(matches))\n\n    def test_no_label_diff_with_same_item(self):\n        detections = 3\n        anns = [\n            Label(i, attributes={'score': (1.0 + i) / detections}) \\\n                for i in range(detections)\n        ]\n        item = DatasetItem(id=1, annotations=anns)\n\n        conf_thresh = 0.5\n        comp = Comparator(conf_threshold=conf_thresh)\n\n        result = comp.compare_item_labels(item, item)\n\n        matches, a_greater, b_greater = result\n        self.assertEqual(0, len(a_greater))\n        self.assertEqual(0, len(b_greater))\n        self.assertEqual(len([it for it in item.annotations \\\n                if conf_thresh < it.attributes['score']]),\n            len(matches))\n\n    def test_can_find_wrong_label(self):\n        item1 = DatasetItem(id=1, annotations=[\n            Label(0),\n            Label(1),\n            Label(2),\n        ])\n        item2 = DatasetItem(id=2, annotations=[\n            Label(2),\n            Label(3),\n            Label(4),\n        ])\n\n        conf_thresh = 0.5\n        comp = Comparator(conf_threshold=conf_thresh)\n\n        result = comp.compare_item_labels(item1, item2)\n\n        matches, a_greater, b_greater = result\n        self.assertEqual(2, len(a_greater))\n        self.assertEqual(2, len(b_greater))\n        self.assertEqual(1, len(matches))"""
datumaro/tests/test_image.py,0,"b""from itertools import product\nimport numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nimport datumaro.util.image as image_module\nfrom datumaro.util.test_utils import TestDir\n\n\nclass ImageOperationsTest(TestCase):\n    def setUp(self):\n        self.default_backend = image_module._IMAGE_BACKEND\n\n    def tearDown(self):\n        image_module._IMAGE_BACKEND = self.default_backend\n\n    def test_save_and_load_backends(self):\n        backends = image_module._IMAGE_BACKENDS\n        for save_backend, load_backend, c in product(backends, backends, [1, 3]):\n            with TestDir() as test_dir:\n                if c == 1:\n                    src_image = np.random.randint(0, 255 + 1, (2, 4))\n                else:\n                    src_image = np.random.randint(0, 255 + 1, (2, 4, c))\n                path = osp.join(test_dir, 'img.png') # lossless\n\n                image_module._IMAGE_BACKEND = save_backend\n                image_module.save_image(path, src_image, jpeg_quality=100)\n\n                image_module._IMAGE_BACKEND = load_backend\n                dst_image = image_module.load_image(path)\n\n                self.assertTrue(np.array_equal(src_image, dst_image),\n                    'save: %s, load: %s' % (save_backend, load_backend))\n\n    def test_encode_and_decode_backends(self):\n        backends = image_module._IMAGE_BACKENDS\n        for save_backend, load_backend, c in product(backends, backends, [1, 3]):\n            if c == 1:\n                src_image = np.random.randint(0, 255 + 1, (2, 4))\n            else:\n                src_image = np.random.randint(0, 255 + 1, (2, 4, c))\n\n            image_module._IMAGE_BACKEND = save_backend\n            buffer = image_module.encode_image(src_image, '.png',\n                jpeg_quality=100) # lossless\n\n            image_module._IMAGE_BACKEND = load_backend\n            dst_image = image_module.decode_image(buffer)\n\n            self.assertTrue(np.array_equal(src_image, dst_image),\n                'save: %s, load: %s' % (save_backend, load_backend))\n\n    def test_save_image_to_inexistent_dir_raises_error(self):\n        with self.assertRaises(FileNotFoundError):\n            image_module.save_image('some/path.jpg', np.ones((5, 4, 3)),\n                create_dir=False)\n\n    def test_save_image_can_create_dir(self):\n        with TestDir() as test_dir:\n            path = osp.join(test_dir, 'some', 'path.jpg')\n            image_module.save_image(path, np.ones((5, 4, 3)), create_dir=True)\n            self.assertTrue(osp.isfile(path))\n"""
datumaro/tests/test_image_dir_format.py,0,"b""import numpy as np\n\nfrom unittest import TestCase\n\nfrom datumaro.components.project import Project\nfrom datumaro.components.extractor import Extractor, DatasetItem\nfrom datumaro.plugins.image_dir import ImageDirConverter\nfrom datumaro.util.test_utils import TestDir, compare_datasets\n\n\nclass ImageDirFormatTest(TestCase):\n    def test_can_load(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.ones((10, 6, 3))),\n                    DatasetItem(id=2, image=np.ones((5, 4, 3))),\n                ])\n\n        with TestDir() as test_dir:\n            source_dataset = TestExtractor()\n\n            ImageDirConverter()(source_dataset, save_dir=test_dir)\n\n            project = Project.import_from(test_dir, 'image_dir')\n            parsed_dataset = project.make_dataset()\n\n            compare_datasets(self, source_dataset, parsed_dataset)\n"""
datumaro/tests/test_images.py,0,"b""import numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.util.test_utils import TestDir\nfrom datumaro.util.image import lazy_image, load_image, save_image, Image\nfrom datumaro.util.image_cache import ImageCache\n\n\nclass LazyImageTest(TestCase):\n    def test_cache_works(self):\n        with TestDir() as test_dir:\n            image = np.ones((100, 100, 3), dtype=np.uint8)\n            image_path = osp.join(test_dir, 'image.jpg')\n            save_image(image_path, image)\n\n            caching_loader = lazy_image(image_path, cache=None)\n            self.assertTrue(caching_loader() is caching_loader())\n\n            non_caching_loader = lazy_image(image_path, cache=False)\n            self.assertFalse(non_caching_loader() is non_caching_loader())\n\nclass ImageCacheTest(TestCase):\n    def test_cache_fifo_displacement(self):\n        capacity = 2\n        cache = ImageCache(capacity)\n\n        loaders = [lazy_image(None, loader=lambda p: object(), cache=cache)\n            for _ in range(capacity + 1)]\n\n        first_request = [loader() for loader in loaders[1 : ]]\n        loaders[0]() # pop something from the cache\n\n        second_request = [loader() for loader in loaders[2 : ]]\n        second_request.insert(0, loaders[1]())\n\n        matches = sum([a is b for a, b in zip(first_request, second_request)])\n        self.assertEqual(matches, len(first_request) - 1)\n\n    def test_global_cache_is_accessible(self):\n        loader = lazy_image(None, loader=lambda p: object())\n\n        ImageCache.get_instance().clear()\n        self.assertTrue(loader() is loader())\n        self.assertEqual(ImageCache.get_instance().size(), 1)\n\nclass ImageTest(TestCase):\n    def test_lazy_image_shape(self):\n        data = np.ones((5, 6, 7))\n\n        image_lazy = Image(data=data, size=(2, 4))\n        image_eager = Image(data=data)\n\n        self.assertEqual((2, 4), image_lazy.size)\n        self.assertEqual((5, 6), image_eager.size)\n\n    def test_ctors(self):\n        with TestDir() as test_dir:\n            path = osp.join(test_dir, 'path.png')\n            image = np.ones([2, 4, 3])\n            save_image(path, image)\n\n            for args in [\n                { 'data': image },\n                { 'data': image, 'path': path },\n                { 'data': image, 'path': path, 'size': (2, 4) },\n                { 'data': image, 'path': path, 'loader': load_image, 'size': (2, 4) },\n                { 'path': path },\n                { 'path': path, 'loader': load_image },\n                { 'path': 'somepath', 'loader': lambda p: image },\n                { 'loader': lambda p: image },\n                { 'path': path, 'size': (2, 4) },\n            ]:\n                with self.subTest(**args):\n                    img = Image(**args)\n                    # pylint: disable=pointless-statement\n                    if img.has_data:\n                        img.data\n                    img.size\n                    # pylint: enable=pointless-statement\n"""
datumaro/tests/test_labelme_format.py,0,"b""import numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    AnnotationType, Bbox, Mask, Polygon, LabelCategories\n)\nfrom datumaro.components.project import Project\nfrom datumaro.plugins.labelme_format import LabelMeImporter, \\\n    LabelMeConverter\nfrom datumaro.util.test_utils import TestDir, compare_datasets\n\n\nclass LabelMeConverterTest(TestCase):\n    def _test_save_and_load(self, source_dataset, converter, test_dir,\n            target_dataset=None, importer_args=None):\n        converter(source_dataset, test_dir)\n\n        if importer_args is None:\n            importer_args = {}\n        parsed_dataset = LabelMeImporter()(test_dir, **importer_args) \\\n            .make_dataset()\n\n        if target_dataset is None:\n            target_dataset = source_dataset\n\n        compare_datasets(self, expected=target_dataset, actual=parsed_dataset)\n\n    def test_can_save_and_load(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train',\n                        image=np.ones((16, 16, 3)),\n                        annotations=[\n                            Bbox(0, 4, 4, 8, label=2, group=2),\n                            Polygon([0, 4, 4, 4, 5, 6], label=3, attributes={\n                                'occluded': True,\n                                'a1': 'qwe',\n                                'a2': True,\n                                'a3': 123,\n                            }),\n                            Mask(np.array([[0, 1], [1, 0], [1, 1]]), group=2,\n                                attributes={ 'username': 'test' }),\n                            Bbox(1, 2, 3, 4, group=3),\n                            Mask(np.array([[0, 0], [0, 0], [1, 1]]), group=3,\n                                attributes={ 'occluded': True }\n                            ),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                for label in range(10):\n                    label_cat.add('label_' + str(label))\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train',\n                        image=np.ones((16, 16, 3)),\n                        annotations=[\n                            Bbox(0, 4, 4, 8, label=0, group=2, id=0,\n                                attributes={\n                                    'occluded': False, 'username': '',\n                                }\n                            ),\n                            Polygon([0, 4, 4, 4, 5, 6], label=1, id=1,\n                                attributes={\n                                    'occluded': True, 'username': '',\n                                    'a1': 'qwe',\n                                    'a2': True,\n                                    'a3': 123,\n                                }\n                            ),\n                            Mask(np.array([[0, 1], [1, 0], [1, 1]]), group=2,\n                                id=2, attributes={\n                                    'occluded': False, 'username': 'test'\n                                }\n                            ),\n                            Bbox(1, 2, 3, 4, group=1, id=3, attributes={\n                                'occluded': False, 'username': '',\n                            }),\n                            Mask(np.array([[0, 0], [0, 0], [1, 1]]), group=1,\n                                id=4, attributes={\n                                    'occluded': True, 'username': ''\n                                }\n                            ),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add('label_2')\n                label_cat.add('label_3')\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                SrcExtractor(), LabelMeConverter(save_images=True),\n                test_dir, target_dataset=DstExtractor())\n\n\nDUMMY_DATASET_DIR = osp.join(osp.dirname(__file__), 'assets', 'labelme_dataset')\n\nclass LabelMeImporterTest(TestCase):\n    def test_can_detect(self):\n        self.assertTrue(LabelMeImporter.detect(DUMMY_DATASET_DIR))\n\n    def test_can_import(self):\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                img1 = np.ones((77, 102, 3)) * 255\n                img1[6:32, 7:41] = 0\n\n                mask1 = np.zeros((77, 102), dtype=int)\n                mask1[67:69, 58:63] = 1\n\n                mask2 = np.zeros((77, 102), dtype=int)\n                mask2[13:25, 54:71] = [\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                ]\n\n                return iter([\n                    DatasetItem(id='img1', image=img1,\n                        annotations=[\n                            Polygon([43, 34, 45, 34, 45, 37, 43, 37],\n                                label=0, id=0,\n                                attributes={\n                                    'occluded': False,\n                                    'username': 'admin'\n                                }\n                            ),\n                            Mask(mask1, label=1, id=1,\n                                attributes={\n                                    'occluded': False,\n                                    'username': 'brussell'\n                                }\n                            ),\n                            Polygon([30, 12, 42, 21, 24, 26, 15, 22, 18, 14, 22, 12, 27, 12],\n                                label=2, group=2, id=2,\n                                attributes={\n                                    'a1': True,\n                                    'occluded': True,\n                                    'username': 'anonymous'\n                                }\n                            ),\n                            Polygon([35, 21, 43, 22, 40, 28, 28, 31, 31, 22, 32, 25],\n                                label=3, group=2, id=3,\n                                attributes={\n                                    'kj': True,\n                                    'occluded': False,\n                                    'username': 'anonymous'\n                                }\n                            ),\n                            Bbox(13, 19, 10, 11, label=4, group=2, id=4,\n                                attributes={\n                                    'hg': True,\n                                    'occluded': True,\n                                    'username': 'anonymous'\n                                }\n                            ),\n                            Mask(mask2, label=5, group=1, id=5,\n                                attributes={\n                                    'd': True,\n                                    'occluded': False,\n                                    'username': 'anonymous'\n                                }\n                            ),\n                            Polygon([64, 21, 74, 24, 72, 32, 62, 34, 60, 27, 62, 22],\n                                label=6, group=1, id=6,\n                                attributes={\n                                    'gfd lkj lkj hi': True,\n                                    'occluded': False,\n                                    'username': 'anonymous'\n                                }\n                            ),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add('window')\n                label_cat.add('license plate')\n                label_cat.add('o1')\n                label_cat.add('q1')\n                label_cat.add('b1')\n                label_cat.add('m1')\n                label_cat.add('hg')\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        parsed = Project.import_from(DUMMY_DATASET_DIR, 'label_me') \\\n            .make_dataset()\n        compare_datasets(self, expected=DstExtractor(), actual=parsed)"""
datumaro/tests/test_masks.py,0,"b""import numpy as np\n\nfrom unittest import TestCase\n\nimport datumaro.util.mask_tools as mask_tools\nfrom datumaro.components.extractor import CompiledMask\n\n\nclass PolygonConversionsTest(TestCase):\n    def test_mask_can_be_converted_to_polygon(self):\n        mask = np.array([\n            [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n            [0, 0, 1, 1, 0, 1, 0, 1, 0, 0],\n            [0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n            [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        ])\n        expected = [\n            [1, 0, 3, 0, 3, 2, 1, 0],\n            [5, 0, 8, 0, 5, 3],\n        ]\n\n        computed = mask_tools.mask_to_polygons(mask)\n\n        self.assertEqual(len(expected), len(computed))\n\n    def test_can_crop_covered_segments(self):\n        image_size = [7, 7]\n        initial = [\n            [1, 1, 6, 1, 6, 6, 1, 6], # rectangle\n            mask_tools.mask_to_rle(np.array([\n                [0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 1, 1, 0],\n                [0, 1, 1, 0, 1, 1, 0],\n                [0, 0, 0, 0, 0, 1, 0],\n                [0, 1, 1, 0, 0, 1, 0],\n                [0, 1, 1, 1, 1, 1, 0],\n                [0, 0, 0, 0, 0, 0, 0],\n            ])),\n            [1, 1, 6, 6, 1, 6], # lower-left triangle\n        ]\n        expected = [\n            np.array([\n                [0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0],\n                [0, 0, 0, 0, 1, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0],\n            ]), # half-covered\n            np.array([\n                [0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 1, 1, 0],\n                [0, 0, 0, 0, 1, 1, 0],\n                [0, 0, 0, 0, 0, 1, 0],\n                [0, 0, 0, 0, 0, 1, 0],\n                [0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0],\n            ]), # half-covered\n            mask_tools.rles_to_mask([initial[2]], *image_size), # unchanged\n        ]\n\n        computed = mask_tools.crop_covered_segments(initial, *image_size,\n            ratio_tolerance=0, return_masks=True)\n\n        self.assertEqual(len(initial), len(computed))\n        for i, (e_mask, c_mask) in enumerate(zip(expected, computed)):\n            self.assertTrue(np.array_equal(e_mask, c_mask),\n                '#%s: %s\\n%s\\n' % (i, e_mask, c_mask))\n\n    def _test_mask_to_rle(self, source_mask):\n        rle_uncompressed = mask_tools.mask_to_rle(source_mask)\n\n        from pycocotools import mask as mask_utils\n        resulting_mask = mask_utils.frPyObjects(\n            rle_uncompressed, *rle_uncompressed['size'])\n        resulting_mask = mask_utils.decode(resulting_mask)\n\n        self.assertTrue(np.array_equal(source_mask, resulting_mask),\n            '%s\\n%s\\n' % (source_mask, resulting_mask))\n\n    def test_mask_to_rle_multi(self):\n        cases = [\n            np.array([\n                [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n                [0, 0, 1, 1, 0, 1, 0, 1, 0, 0],\n                [0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            ]),\n\n            np.array([\n                [0]\n            ]),\n            np.array([\n                [1]\n            ]),\n\n            np.array([\n                [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n                [0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n                [1, 0, 1, 0, 1, 1, 1, 0, 0, 0],\n                [1, 1, 0, 1, 0, 1, 1, 1, 1, 0],\n                [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n                [1, 1, 0, 0, 1, 1, 0, 0, 0, 1],\n                [0, 0, 1, 0, 0, 0, 1, 1, 1, 1],\n                [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n                [1, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n                [0, 1, 0, 1, 1, 1, 1, 1, 0, 0],\n                [0, 1, 0, 0, 0, 1, 0, 0, 1, 0],\n                [1, 1, 0, 1, 0, 0, 1, 1, 1, 1],\n            ])\n        ]\n\n        for case in cases:\n            self._test_mask_to_rle(case)\n\nclass ColormapOperationsTest(TestCase):\n    def test_can_paint_mask(self):\n        mask = np.zeros((1, 3), dtype=np.uint8)\n        mask[:, 0] = 0\n        mask[:, 1] = 1\n        mask[:, 2] = 2\n\n        colormap = mask_tools.generate_colormap(3)\n\n        expected = np.zeros((*mask.shape, 3), dtype=np.uint8)\n        expected[:, 0] = colormap[0][::-1]\n        expected[:, 1] = colormap[1][::-1]\n        expected[:, 2] = colormap[2][::-1]\n\n        actual = mask_tools.paint_mask(mask, colormap)\n\n        self.assertTrue(np.array_equal(expected, actual),\n            '%s\\nvs.\\n%s' % (expected, actual))\n\n    def test_can_unpaint_mask(self):\n        colormap = mask_tools.generate_colormap(3)\n        inverse_colormap = mask_tools.invert_colormap(colormap)\n\n        mask = np.zeros((1, 3, 3), dtype=np.uint8)\n        mask[:, 0] = colormap[0][::-1]\n        mask[:, 1] = colormap[1][::-1]\n        mask[:, 2] = colormap[2][::-1]\n\n        expected = np.zeros((1, 3), dtype=np.uint8)\n        expected[:, 0] = 0\n        expected[:, 1] = 1\n        expected[:, 2] = 2\n\n        actual = mask_tools.unpaint_mask(mask, inverse_colormap)\n\n        self.assertTrue(np.array_equal(expected, actual),\n            '%s\\nvs.\\n%s' % (expected, actual))\n\n    def test_can_remap_mask(self):\n        class_count = 10\n        remap_fn = lambda c: class_count - c\n\n        src = np.empty((class_count, class_count), dtype=np.uint8)\n        for c in range(class_count):\n            src[c:, c:] = c\n\n        expected = np.empty_like(src)\n        for c in range(class_count):\n            expected[c:, c:] = remap_fn(c)\n\n        actual = mask_tools.remap_mask(src, remap_fn)\n\n        self.assertTrue(np.array_equal(expected, actual),\n            '%s\\nvs.\\n%s' % (expected, actual))\n\n    def test_can_merge_masks(self):\n        masks = [\n            np.array([0, 2, 4, 0, 0, 1]),\n            np.array([0, 1, 1, 0, 2, 0]),\n            np.array([0, 0, 2, 3, 0, 0]),\n        ]\n        expected = \\\n            np.array([0, 1, 2, 3, 2, 1])\n\n        actual = mask_tools.merge_masks(masks)\n\n        self.assertTrue(np.array_equal(expected, actual),\n            '%s\\nvs.\\n%s' % (expected, actual))\n\n    def test_can_decode_compiled_mask(self):\n        class_idx = 1000\n        instance_idx = 10000\n        mask = np.array([1])\n        compiled_mask = CompiledMask(mask * class_idx, mask * instance_idx)\n\n        labels = compiled_mask.get_instance_labels()\n\n        self.assertEqual({instance_idx: class_idx}, labels)"""
datumaro/tests/test_mot_format.py,0,"b""import numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    AnnotationType, Bbox, LabelCategories\n)\nfrom datumaro.components.project import Project\nfrom datumaro.plugins.mot_format import MotSeqGtConverter, MotSeqImporter\nfrom datumaro.util.test_utils import TestDir, compare_datasets\n\n\nclass MotConverterTest(TestCase):\n    def _test_save_and_load(self, source_dataset, converter, test_dir,\n            target_dataset=None, importer_args=None):\n        converter(source_dataset, test_dir)\n\n        if importer_args is None:\n            importer_args = {}\n        parsed_dataset = MotSeqImporter()(test_dir, **importer_args) \\\n            .make_dataset()\n\n        if target_dataset is None:\n            target_dataset = source_dataset\n\n        compare_datasets(self, expected=target_dataset, actual=parsed_dataset)\n\n    def test_can_save_bboxes(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train',\n                        image=np.ones((16, 16, 3)),\n                        annotations=[\n                            Bbox(0, 4, 4, 8, label=2, attributes={\n                                'occluded': True,\n                            }),\n                            Bbox(0, 4, 4, 4, label=3, attributes={\n                                'visibility': 0.4,\n                            }),\n                            Bbox(2, 4, 4, 4, attributes={\n                                'ignored': True\n                            }),\n                        ]\n                    ),\n\n                    DatasetItem(id=2, subset='val',\n                        image=np.ones((8, 8, 3)),\n                        annotations=[\n                            Bbox(1, 2, 4, 2, label=3),\n                        ]\n                    ),\n\n                    DatasetItem(id=3, subset='test',\n                        image=np.ones((5, 4, 3)) * 3,\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                for label in range(10):\n                    label_cat.add('label_' + str(label))\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1,\n                        image=np.ones((16, 16, 3)),\n                        annotations=[\n                            Bbox(0, 4, 4, 8, label=2, attributes={\n                                'occluded': True,\n                                'visibility': 0.0,\n                                'ignored': False,\n                            }),\n                            Bbox(0, 4, 4, 4, label=3, attributes={\n                                'occluded': False,\n                                'visibility': 0.4,\n                                'ignored': False,\n                            }),\n                            Bbox(2, 4, 4, 4, attributes={\n                                'occluded': False,\n                                'visibility': 1.0,\n                                'ignored': True,\n                            }),\n                        ]\n                    ),\n\n                    DatasetItem(id=2,\n                        image=np.ones((8, 8, 3)),\n                        annotations=[\n                            Bbox(1, 2, 4, 2, label=3, attributes={\n                                'occluded': False,\n                                'visibility': 1.0,\n                                'ignored': False,\n                            }),\n                        ]\n                    ),\n\n                    DatasetItem(id=3,\n                        image=np.ones((5, 4, 3)) * 3,\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                for label in range(10):\n                    label_cat.add('label_' + str(label))\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                SrcExtractor(), MotSeqGtConverter(save_images=True),\n                test_dir, target_dataset=DstExtractor())\n\n\nDUMMY_DATASET_DIR = osp.join(osp.dirname(__file__), 'assets', 'mot_dataset')\n\nclass MotImporterTest(TestCase):\n    def test_can_detect(self):\n        self.assertTrue(MotSeqImporter.detect(DUMMY_DATASET_DIR))\n\n    def test_can_import(self):\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1,\n                        image=np.ones((16, 16, 3)),\n                        annotations=[\n                            Bbox(0, 4, 4, 8, label=2, attributes={\n                                'occluded': False,\n                                'visibility': 1.0,\n                                'ignored': False,\n                            }),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                for label in range(10):\n                    label_cat.add('label_' + str(label))\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        dataset = Project.import_from(DUMMY_DATASET_DIR, 'mot_seq') \\\n            .make_dataset()\n\n        compare_datasets(self, DstExtractor(), dataset)"""
datumaro/tests/test_project.py,0,"b""import numpy as np\nimport os\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.components.project import Project, Environment, Dataset\nfrom datumaro.components.config_model import Source, Model\nfrom datumaro.components.launcher import Launcher, ModelTransform\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    Label, Mask, Points, Polygon, PolyLine, Bbox, Caption,\n)\nfrom datumaro.util.image import Image\nfrom datumaro.components.config import Config, DefaultConfig, SchemaBuilder\nfrom datumaro.components.dataset_filter import \\\n    XPathDatasetFilter, XPathAnnotationsFilter, DatasetItemEncoder\nfrom datumaro.util.test_utils import TestDir, compare_datasets\n\n\nclass ProjectTest(TestCase):\n    def test_project_generate(self):\n        src_config = Config({\n            'project_name': 'test_project',\n            'format_version': 1,\n        })\n\n        with TestDir() as test_dir:\n            project_path = test_dir\n            Project.generate(project_path, src_config)\n\n            self.assertTrue(osp.isdir(project_path))\n\n            result_config = Project.load(project_path).config\n            self.assertEqual(\n                src_config.project_name, result_config.project_name)\n            self.assertEqual(\n                src_config.format_version, result_config.format_version)\n\n    @staticmethod\n    def test_default_ctor_is_ok():\n        Project()\n\n    @staticmethod\n    def test_empty_config_is_ok():\n        Project(Config())\n\n    def test_add_source(self):\n        source_name = 'source'\n        origin = Source({\n            'url': 'path',\n            'format': 'ext'\n        })\n        project = Project()\n\n        project.add_source(source_name, origin)\n\n        added = project.get_source(source_name)\n        self.assertIsNotNone(added)\n        self.assertEqual(added, origin)\n\n    def test_added_source_can_be_saved(self):\n        source_name = 'source'\n        origin = Source({\n            'url': 'path',\n        })\n        project = Project()\n        project.add_source(source_name, origin)\n\n        saved = project.config\n\n        self.assertEqual(origin, saved.sources[source_name])\n\n    def test_added_source_can_be_dumped(self):\n        source_name = 'source'\n        origin = Source({\n            'url': 'path',\n        })\n        project = Project()\n        project.add_source(source_name, origin)\n\n        with TestDir() as test_dir:\n            project.save(test_dir)\n\n            loaded = Project.load(test_dir)\n            loaded = loaded.get_source(source_name)\n            self.assertEqual(origin, loaded)\n\n    def test_can_import_with_custom_importer(self):\n        class TestImporter:\n            def __call__(self, path, subset=None):\n                return Project({\n                    'project_filename': path,\n                    'subsets': [ subset ]\n                })\n\n        path = 'path'\n        importer_name = 'test_importer'\n\n        env = Environment()\n        env.importers.register(importer_name, TestImporter)\n\n        project = Project.import_from(path, importer_name, env,\n            subset='train')\n\n        self.assertEqual(path, project.config.project_filename)\n        self.assertListEqual(['train'], project.config.subsets)\n\n    def test_can_dump_added_model(self):\n        model_name = 'model'\n\n        project = Project()\n        saved = Model({ 'launcher': 'name' })\n        project.add_model(model_name, saved)\n\n        with TestDir() as test_dir:\n            project.save(test_dir)\n\n            loaded = Project.load(test_dir)\n            loaded = loaded.get_model(model_name)\n            self.assertEqual(saved, loaded)\n\n    def test_can_have_project_source(self):\n        with TestDir() as test_dir:\n            Project.generate(test_dir)\n\n            project2 = Project()\n            project2.add_source('project1', {\n                'url': test_dir,\n            })\n            dataset = project2.make_dataset()\n\n            self.assertTrue('project1' in dataset.sources)\n\n    def test_can_batch_launch_custom_model(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                for i in range(5):\n                    yield DatasetItem(id=i, subset='train', image=np.array([i]))\n\n        class TestLauncher(Launcher):\n            def launch(self, inputs):\n                for i, inp in enumerate(inputs):\n                    yield [ Label(attributes={'idx': i, 'data': inp.item()}) ]\n\n        model_name = 'model'\n        launcher_name = 'custom_launcher'\n\n        project = Project()\n        project.env.launchers.register(launcher_name, TestLauncher)\n        project.add_model(model_name, { 'launcher': launcher_name })\n        model = project.make_executable_model(model_name)\n        extractor = TestExtractor()\n\n        batch_size = 3\n        executor = ModelTransform(extractor, model, batch_size=batch_size)\n\n        for item in executor:\n            self.assertEqual(1, len(item.annotations))\n            self.assertEqual(int(item.id) % batch_size,\n                item.annotations[0].attributes['idx'])\n            self.assertEqual(int(item.id),\n                item.annotations[0].attributes['data'])\n\n    def test_can_do_transform_with_custom_model(self):\n        class TestExtractorSrc(Extractor):\n            def __iter__(self):\n                for i in range(2):\n                    yield DatasetItem(id=i, image=np.ones([2, 2, 3]) * i,\n                        annotations=[Label(i)])\n\n        class TestLauncher(Launcher):\n            def launch(self, inputs):\n                for inp in inputs:\n                    yield [ Label(inp[0, 0, 0]) ]\n\n        class TestConverter(Converter):\n            def __call__(self, extractor, save_dir):\n                for item in extractor:\n                    with open(osp.join(save_dir, '%s.txt' % item.id), 'w') as f:\n                        f.write(str(item.annotations[0].label) + '\\n')\n\n        class TestExtractorDst(Extractor):\n            def __init__(self, url):\n                super().__init__()\n                self.items = [osp.join(url, p) for p in sorted(os.listdir(url))]\n\n            def __iter__(self):\n                for path in self.items:\n                    with open(path, 'r') as f:\n                        index = osp.splitext(osp.basename(path))[0]\n                        label = int(f.readline().strip())\n                        yield DatasetItem(id=index, annotations=[Label(label)])\n\n        model_name = 'model'\n        launcher_name = 'custom_launcher'\n        extractor_name = 'custom_extractor'\n\n        project = Project()\n        project.env.launchers.register(launcher_name, TestLauncher)\n        project.env.extractors.register(extractor_name, TestExtractorSrc)\n        project.env.converters.register(extractor_name, TestConverter)\n        project.add_model(model_name, { 'launcher': launcher_name })\n        project.add_source('source', { 'format': extractor_name })\n\n        with TestDir() as test_dir:\n            project.make_dataset().apply_model(model=model_name,\n                save_dir=test_dir)\n\n            result = Project.load(test_dir)\n            result.env.extractors.register(extractor_name, TestExtractorDst)\n            it = iter(result.make_dataset())\n            item1 = next(it)\n            item2 = next(it)\n            self.assertEqual(0, item1.annotations[0].label)\n            self.assertEqual(1, item2.annotations[0].label)\n\n    def test_source_datasets_can_be_merged(self):\n        class TestExtractor(Extractor):\n            def __init__(self, url, n=0, s=0):\n                super().__init__(length=n)\n                self.n = n\n                self.s = s\n\n            def __iter__(self):\n                for i in range(self.n):\n                    yield DatasetItem(id=self.s + i, subset='train')\n\n        e_name1 = 'e1'\n        e_name2 = 'e2'\n        n1 = 2\n        n2 = 4\n\n        project = Project()\n        project.env.extractors.register(e_name1, lambda p: TestExtractor(p, n=n1))\n        project.env.extractors.register(e_name2, lambda p: TestExtractor(p, n=n2, s=n1))\n        project.add_source('source1', { 'format': e_name1 })\n        project.add_source('source2', { 'format': e_name2 })\n\n        dataset = project.make_dataset()\n\n        self.assertEqual(n1 + n2, len(dataset))\n\n    def test_project_filter_can_be_applied(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                for i in range(10):\n                    yield DatasetItem(id=i, subset='train')\n\n        e_type = 'type'\n        project = Project()\n        project.env.extractors.register(e_type, TestExtractor)\n        project.add_source('source', { 'format': e_type })\n\n        dataset = project.make_dataset().extract('/item[id < 5]')\n\n        self.assertEqual(5, len(dataset))\n\n    def test_can_save_and_load_own_dataset(self):\n        with TestDir() as test_dir:\n            src_project = Project()\n            src_dataset = src_project.make_dataset()\n            item = DatasetItem(id=1)\n            src_dataset.put(item)\n            src_dataset.save(test_dir)\n\n            loaded_project = Project.load(test_dir)\n            loaded_dataset = loaded_project.make_dataset()\n\n            self.assertEqual(list(src_dataset), list(loaded_dataset))\n\n    def test_project_own_dataset_can_be_modified(self):\n        project = Project()\n        dataset = project.make_dataset()\n\n        item = DatasetItem(id=1)\n        dataset.put(item)\n\n        self.assertEqual(item, next(iter(dataset)))\n\n    def test_project_compound_child_can_be_modified_recursively(self):\n        with TestDir() as test_dir:\n            child1 = Project({\n                'project_dir': osp.join(test_dir, 'child1'),\n            })\n            child1.save()\n\n            child2 = Project({\n                'project_dir': osp.join(test_dir, 'child2'),\n            })\n            child2.save()\n\n            parent = Project()\n            parent.add_source('child1', {\n                'url': child1.config.project_dir\n            })\n            parent.add_source('child2', {\n                'url': child2.config.project_dir\n            })\n            dataset = parent.make_dataset()\n\n            item1 = DatasetItem(id='ch1', path=['child1'])\n            item2 = DatasetItem(id='ch2', path=['child2'])\n            dataset.put(item1)\n            dataset.put(item2)\n\n            self.assertEqual(2, len(dataset))\n            self.assertEqual(1, len(dataset.sources['child1']))\n            self.assertEqual(1, len(dataset.sources['child2']))\n\n    def test_project_can_merge_item_annotations(self):\n        class TestExtractor1(Extractor):\n            def __iter__(self):\n                yield DatasetItem(id=1, subset='train', annotations=[\n                    Label(2, id=3),\n                    Label(3, attributes={ 'x': 1 }),\n                ])\n\n        class TestExtractor2(Extractor):\n            def __iter__(self):\n                yield DatasetItem(id=1, subset='train', annotations=[\n                    Label(3, attributes={ 'x': 1 }),\n                    Label(4, id=4),\n                ])\n\n        project = Project()\n        project.env.extractors.register('t1', TestExtractor1)\n        project.env.extractors.register('t2', TestExtractor2)\n        project.add_source('source1', { 'format': 't1' })\n        project.add_source('source2', { 'format': 't2' })\n\n        merged = project.make_dataset()\n\n        self.assertEqual(1, len(merged))\n\n        item = next(iter(merged))\n        self.assertEqual(3, len(item.annotations))\n\nclass DatasetFilterTest(TestCase):\n    @staticmethod\n    def test_item_representations():\n        item = DatasetItem(id=1, subset='subset', path=['a', 'b'],\n            image=np.ones((5, 4, 3)),\n            annotations=[\n                Label(0, attributes={'a1': 1, 'a2': '2'}, id=1, group=2),\n                Caption('hello', id=1),\n                Caption('world', group=5),\n                Label(2, id=3, attributes={ 'x': 1, 'y': '2' }),\n                Bbox(1, 2, 3, 4, label=4, id=4, attributes={ 'a': 1.0 }),\n                Bbox(5, 6, 7, 8, id=5, group=5),\n                Points([1, 2, 2, 0, 1, 1], label=0, id=5),\n                Mask(id=5, image=np.ones((3, 2))),\n                Mask(label=3, id=5, image=np.ones((2, 3))),\n                PolyLine([1, 2, 3, 4, 5, 6, 7, 8], id=11),\n                Polygon([1, 2, 3, 4, 5, 6, 7, 8]),\n            ]\n        )\n\n        encoded = DatasetItemEncoder.encode(item)\n        DatasetItemEncoder.to_string(encoded)\n\n    def test_item_filter_can_be_applied(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                for i in range(4):\n                    yield DatasetItem(id=i, subset='train')\n\n        extractor = TestExtractor()\n\n        filtered = XPathDatasetFilter(extractor, '/item[id > 1]')\n\n        self.assertEqual(2, len(filtered))\n\n    def test_annotations_filter_can_be_applied(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=0),\n                    DatasetItem(id=1, annotations=[\n                        Label(0),\n                        Label(1),\n                    ]),\n                    DatasetItem(id=2, annotations=[\n                        Label(0),\n                        Label(2),\n                    ]),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=0),\n                    DatasetItem(id=1, annotations=[\n                        Label(0),\n                    ]),\n                    DatasetItem(id=2, annotations=[\n                        Label(0),\n                    ]),\n                ])\n\n        extractor = SrcExtractor()\n\n        filtered = XPathAnnotationsFilter(extractor,\n            '/item/annotation[label_id = 0]')\n\n        self.assertListEqual(list(filtered), list(DstExtractor()))\n\n    def test_annotations_filter_can_remove_empty_items(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=0),\n                    DatasetItem(id=1, annotations=[\n                        Label(0),\n                        Label(1),\n                    ]),\n                    DatasetItem(id=2, annotations=[\n                        Label(0),\n                        Label(2),\n                    ]),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=2, annotations=[\n                        Label(2),\n                    ]),\n                ])\n\n        extractor = SrcExtractor()\n\n        filtered = XPathAnnotationsFilter(extractor,\n            '/item/annotation[label_id = 2]', remove_empty=True)\n\n        self.assertListEqual(list(filtered), list(DstExtractor()))\n\nclass ConfigTest(TestCase):\n    def test_can_produce_multilayer_config_from_dict(self):\n        schema_low = SchemaBuilder() \\\n            .add('options', dict) \\\n            .build()\n        schema_mid = SchemaBuilder() \\\n            .add('desc', lambda: Config(schema=schema_low)) \\\n            .build()\n        schema_top = SchemaBuilder() \\\n            .add('container', lambda: DefaultConfig(\n                lambda v: Config(v, schema=schema_mid))) \\\n            .build()\n\n        value = 1\n        source = Config({\n            'container': {\n                'elem': {\n                    'desc': {\n                        'options': {\n                            'k': value\n                        }\n                    }\n                }\n            }\n        }, schema=schema_top)\n\n        self.assertEqual(value, source.container['elem'].desc.options['k'])\n\nclass ExtractorTest(TestCase):\n    def test_custom_extractor_can_be_created(self):\n        class CustomExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=0, subset='train'),\n                    DatasetItem(id=1, subset='train'),\n                    DatasetItem(id=2, subset='train'),\n\n                    DatasetItem(id=3, subset='test'),\n                    DatasetItem(id=4, subset='test'),\n\n                    DatasetItem(id=1),\n                    DatasetItem(id=2),\n                    DatasetItem(id=3),\n                ])\n\n        extractor_name = 'ext1'\n        project = Project()\n        project.env.extractors.register(extractor_name, CustomExtractor)\n        project.add_source('src1', {\n            'url': 'path',\n            'format': extractor_name,\n        })\n\n        dataset = project.make_dataset()\n\n        compare_datasets(self, CustomExtractor(), dataset)\n\nclass DatasetTest(TestCase):\n    def test_create_from_extractors(self):\n        class SrcExtractor1(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train', annotations=[\n                        Bbox(1, 2, 3, 4),\n                        Label(4),\n                    ]),\n                    DatasetItem(id=1, subset='val', annotations=[\n                        Label(4),\n                    ]),\n                ])\n\n        class SrcExtractor2(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='val', annotations=[\n                        Label(5),\n                    ]),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train', annotations=[\n                        Bbox(1, 2, 3, 4),\n                        Label(4),\n                    ]),\n                    DatasetItem(id=1, subset='val', annotations=[\n                        Label(4),\n                        Label(5),\n                    ]),\n                ])\n\n        dataset = Dataset.from_extractors(SrcExtractor1(), SrcExtractor2())\n\n        compare_datasets(self, DstExtractor(), dataset)\n\n\nclass DatasetItemTest(TestCase):\n    def test_ctor_requires_id(self):\n        with self.assertRaises(Exception):\n            # pylint: disable=no-value-for-parameter\n            DatasetItem()\n            # pylint: enable=no-value-for-parameter\n\n    @staticmethod\n    def test_ctors_with_image():\n        for args in [\n            { 'id': 0, 'image': None },\n            { 'id': 0, 'image': 'path.jpg' },\n            { 'id': 0, 'image': np.array([1, 2, 3]) },\n            { 'id': 0, 'image': lambda f: np.array([1, 2, 3]) },\n            { 'id': 0, 'image': Image(data=np.array([1, 2, 3])) },\n        ]:\n            DatasetItem(**args)"""
datumaro/tests/test_tfrecord_format.py,0,"b'import numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase, skipIf\n\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    AnnotationType, Bbox, Mask, LabelCategories\n)\nfrom datumaro.components.project import Project\nfrom datumaro.util.image import Image\nfrom datumaro.util.test_utils import TestDir, compare_datasets\nfrom datumaro.util.tf_util import check_import\n\ntry:\n    from datumaro.plugins.tf_detection_api_format.importer import TfDetectionApiImporter\n    from datumaro.plugins.tf_detection_api_format.extractor import TfDetectionApiExtractor\n    from datumaro.plugins.tf_detection_api_format.converter import TfDetectionApiConverter\n    import_failed = False\nexcept ImportError:\n    import_failed = True\n\n    import importlib\n    module_found = importlib.util.find_spec(\'tensorflow\') is not None\n\n    @skipIf(not module_found, ""Tensorflow package is not found"")\n    class TfImportTest(TestCase):\n        def test_raises_when_crashes_on_import(self):\n            # Should fire if import can\'t be done for any reason except\n            # module unavailability and import crash\n            with self.assertRaisesRegex(ImportError, \'Test process exit code\'):\n                check_import()\n\n@skipIf(import_failed, ""Failed to import tensorflow"")\nclass TfrecordConverterTest(TestCase):\n    def _test_save_and_load(self, source_dataset, converter, test_dir,\n            target_dataset=None, importer_args=None):\n        converter(source_dataset, test_dir)\n\n        if importer_args is None:\n            importer_args = {}\n        parsed_dataset = TfDetectionApiImporter()(test_dir, **importer_args) \\\n            .make_dataset()\n\n        if target_dataset is None:\n            target_dataset = source_dataset\n\n        compare_datasets(self, expected=target_dataset, actual=parsed_dataset)\n\n    def test_can_save_bboxes(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset=\'train\',\n                        image=np.ones((16, 16, 3)),\n                        annotations=[\n                            Bbox(0, 4, 4, 8, label=2),\n                            Bbox(0, 4, 4, 4, label=3),\n                            Bbox(2, 4, 4, 4),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                for label in range(10):\n                    label_cat.add(\'label_\' + str(label))\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                TestExtractor(), TfDetectionApiConverter(save_images=True),\n                test_dir)\n\n    def test_can_save_masks(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset=\'train\', image=np.ones((4, 5, 3)),\n                        annotations=[\n                            Mask(image=np.array([\n                                [1, 0, 0, 1],\n                                [0, 1, 1, 0],\n                                [0, 1, 1, 0],\n                                [1, 0, 0, 1],\n                            ]), label=1),\n                        ]\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                for label in range(10):\n                    label_cat.add(\'label_\' + str(label))\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                TestExtractor(), TfDetectionApiConverter(save_masks=True),\n                test_dir)\n\n    def test_can_save_dataset_with_no_subsets(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1,\n                        image=np.ones((16, 16, 3)),\n                        annotations=[\n                            Bbox(2, 1, 4, 4, label=2),\n                            Bbox(4, 2, 8, 4, label=3),\n                        ]\n                    ),\n\n                    DatasetItem(id=2,\n                        image=np.ones((8, 8, 3)) * 2,\n                        annotations=[\n                            Bbox(4, 4, 4, 4, label=3),\n                        ]\n                    ),\n\n                    DatasetItem(id=3,\n                        image=np.ones((8, 4, 3)) * 3,\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                for label in range(10):\n                    label_cat.add(\'label_\' + str(label))\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                TestExtractor(), TfDetectionApiConverter(save_images=True),\n                test_dir)\n\n    def test_can_save_dataset_with_image_info(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=Image(path=\'1/q.e\', size=(10, 15))),\n                ])\n\n            def categories(self):\n                return { AnnotationType.label: LabelCategories() }\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                TfDetectionApiConverter(), test_dir)\n\n    def test_labelmap_parsing(self):\n        text = """"""\n            {\n                id: 4\n                name: \'qw1\'\n            }\n            {\n                id: 5 name: \'qw2\'\n            }\n\n            {\n                name: \'qw3\'\n                id: 6\n            }\n            {name:\'qw4\' id:7}\n        """"""\n        expected = {\n            \'qw1\': 4,\n            \'qw2\': 5,\n            \'qw3\': 6,\n            \'qw4\': 7,\n        }\n        parsed = TfDetectionApiExtractor._parse_labelmap(text)\n\n        self.assertEqual(expected, parsed)\n\n\nDUMMY_DATASET_DIR = osp.join(osp.dirname(__file__),\n    \'assets\', \'tf_detection_api_dataset\')\n\n@skipIf(import_failed, ""Failed to import tensorflow"")\nclass TfrecordImporterTest(TestCase):\n    def test_can_detect(self):\n        self.assertTrue(TfDetectionApiImporter.detect(DUMMY_DATASET_DIR))\n\n    def test_can_import(self):\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset=\'train\',\n                        image=np.ones((16, 16, 3)),\n                        annotations=[\n                            Bbox(0, 4, 4, 8, label=2),\n                            Bbox(0, 4, 4, 4, label=3),\n                            Bbox(2, 4, 4, 4),\n                        ],\n                    ),\n\n                    DatasetItem(id=2, subset=\'val\',\n                        image=np.ones((8, 8, 3)),\n                        annotations=[\n                            Bbox(1, 2, 4, 2, label=3),\n                        ],\n                    ),\n\n                    DatasetItem(id=3, subset=\'test\',\n                        image=np.ones((5, 4, 3)) * 3,\n                    ),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                for label in range(10):\n                    label_cat.add(\'label_\' + str(label))\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        dataset = Project.import_from(DUMMY_DATASET_DIR, \'tf_detection_api\') \\\n            .make_dataset()\n\n        compare_datasets(self, DstExtractor(), dataset)\n'"
datumaro/tests/test_transforms.py,0,"b'import logging as log\nimport numpy as np\n\nfrom unittest import TestCase\n\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    Mask, Polygon, PolyLine, Points, Bbox, Label,\n    LabelCategories, MaskCategories, AnnotationType\n)\nimport datumaro.util.mask_tools as mask_tools\nimport datumaro.plugins.transforms as transforms\nfrom datumaro.util.test_utils import compare_datasets\n\n\nclass TransformsTest(TestCase):\n    def test_reindex(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=10),\n                    DatasetItem(id=10, subset=\'train\'),\n                    DatasetItem(id=\'a\', subset=\'val\'),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=5),\n                    DatasetItem(id=6, subset=\'train\'),\n                    DatasetItem(id=7, subset=\'val\'),\n                ])\n\n        actual = transforms.Reindex(SrcExtractor(), start=5)\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_mask_to_polygons(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                items = [\n                    DatasetItem(id=1, image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n                                    [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],\n                                    [0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                ]),\n                            ),\n                        ]\n                    ),\n                ]\n                return iter(items)\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Polygon([3.0, 2.5, 1.0, 0.0, 3.5, 0.0, 3.0, 2.5]),\n                            Polygon([5.0, 3.5, 4.5, 0.0, 8.0, 0.0, 5.0, 3.5]),\n                        ]\n                    ),\n                ])\n\n        actual = transforms.MasksToPolygons(SrcExtractor())\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_mask_to_polygons_small_polygons_message(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                items = [\n                    DatasetItem(id=1, image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 0],\n                                    [0, 1, 0],\n                                    [0, 0, 0],\n                                ]),\n                            ),\n                        ]\n                    ),\n                ]\n                return iter(items)\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([ DatasetItem(id=1, image=np.zeros((5, 10, 3))), ])\n\n        with self.assertLogs(level=log.DEBUG) as logs:\n            actual = transforms.MasksToPolygons(SrcExtractor())\n\n            compare_datasets(self, DstExtractor(), actual)\n            self.assertRegex(\'\\n\'.join(logs.output), \'too small polygons\')\n\n    def test_polygons_to_masks(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Polygon([0, 0, 4, 0, 4, 4]),\n                            Polygon([5, 0, 9, 0, 5, 5]),\n                        ]\n                    ),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 10, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n                                    [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n                                    [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                ]),\n                            ),\n                            Mask(np.array([\n                                    [0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n                                    [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n                                    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                ]),\n                            ),\n                        ]\n                    ),\n                ])\n\n        actual = transforms.PolygonsToMasks(SrcExtractor())\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_crop_covered_segments(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            # The mask is partially covered by the polygon\n                            Mask(np.array([\n                                    [0, 0, 1, 1, 1],\n                                    [0, 0, 1, 1, 1],\n                                    [1, 1, 1, 1, 1],\n                                    [1, 1, 1, 0, 0],\n                                    [1, 1, 1, 0, 0]],\n                                ),\n                                z_order=0),\n                            Polygon([1, 1, 4, 1, 4, 4, 1, 4],\n                                z_order=1),\n                        ]\n                    ),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 1, 1, 1],\n                                    [0, 0, 0, 0, 1],\n                                    [1, 0, 0, 0, 1],\n                                    [1, 0, 0, 0, 0],\n                                    [1, 1, 1, 0, 0]],\n                                ),\n                                z_order=0),\n                            Polygon([1, 1, 4, 1, 4, 4, 1, 4],\n                                z_order=1),\n                        ]\n                    ),\n                ])\n\n        actual = transforms.CropCoveredSegments(SrcExtractor())\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_merge_instance_segments(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 1, 1, 1],\n                                    [0, 0, 0, 0, 1],\n                                    [1, 0, 0, 0, 1],\n                                    [1, 0, 0, 0, 0],\n                                    [1, 1, 1, 0, 0]],\n                                ),\n                                z_order=0, group=1),\n                            Polygon([1, 1, 4, 1, 4, 4, 1, 4],\n                                z_order=1, group=1),\n                            Polygon([0, 0, 0, 2, 2, 2, 2, 0],\n                                z_order=1),\n                        ]\n                    ),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 1, 1, 1],\n                                    [0, 1, 1, 1, 1],\n                                    [1, 1, 1, 1, 1],\n                                    [1, 1, 1, 1, 0],\n                                    [1, 1, 1, 0, 0]],\n                                ),\n                                z_order=0, group=1),\n                            Mask(np.array([\n                                    [1, 1, 0, 0, 0],\n                                    [1, 1, 0, 0, 0],\n                                    [0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0]],\n                                ),\n                                z_order=1),\n                        ]\n                    ),\n                ])\n\n        actual = transforms.MergeInstanceSegments(SrcExtractor(),\n            include_polygons=True)\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_map_subsets(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset=\'a\'),\n                    DatasetItem(id=2, subset=\'b\'),\n                    DatasetItem(id=3, subset=\'c\'),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset=\'\'),\n                    DatasetItem(id=2, subset=\'a\'),\n                    DatasetItem(id=3, subset=\'c\'),\n                ])\n\n        actual = transforms.MapSubsets(SrcExtractor(),\n            { \'a\': \'\', \'b\': \'a\' })\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_shapes_to_boxes(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [0, 0, 1, 1, 1],\n                                    [0, 0, 0, 0, 1],\n                                    [1, 0, 0, 0, 1],\n                                    [1, 0, 0, 0, 0],\n                                    [1, 1, 1, 0, 0]],\n                                ), id=1),\n                            Polygon([1, 1, 4, 1, 4, 4, 1, 4], id=2),\n                            PolyLine([1, 1, 2, 1, 2, 2, 1, 2], id=3),\n                            Points([2, 2, 4, 2, 4, 4, 2, 4], id=4),\n                        ]\n                    ),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Bbox(0, 0, 4, 4, id=1),\n                            Bbox(1, 1, 3, 3, id=2),\n                            Bbox(1, 1, 1, 1, id=3),\n                            Bbox(2, 2, 2, 2, id=4),\n                        ]\n                    ),\n                ])\n\n        actual = transforms.ShapesToBoxes(SrcExtractor())\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_id_from_image(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=\'path.jpg\'),\n                    DatasetItem(id=2),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=\'path\', image=\'path.jpg\'),\n                    DatasetItem(id=2),\n                ])\n\n        actual = transforms.IdFromImageName(SrcExtractor())\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_boxes_to_masks(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Bbox(0, 0, 3, 3, z_order=1),\n                            Bbox(0, 0, 3, 1, z_order=2),\n                            Bbox(0, 2, 3, 1, z_order=3),\n                        ]\n                    ),\n                ])\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=np.zeros((5, 5, 3)),\n                        annotations=[\n                            Mask(np.array([\n                                    [1, 1, 1, 0, 0],\n                                    [1, 1, 1, 0, 0],\n                                    [1, 1, 1, 0, 0],\n                                    [0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0]],\n                                ),\n                                z_order=1),\n                            Mask(np.array([\n                                    [1, 1, 1, 0, 0],\n                                    [0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0]],\n                                ),\n                                z_order=2),\n                            Mask(np.array([\n                                    [0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0],\n                                    [1, 1, 1, 0, 0],\n                                    [0, 0, 0, 0, 0],\n                                    [0, 0, 0, 0, 0]],\n                                ),\n                                z_order=3),\n                        ]\n                    ),\n                ])\n\n        actual = transforms.BoxesToMasks(SrcExtractor())\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_random_split(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset=""a""),\n                    DatasetItem(id=2, subset=""a""),\n                    DatasetItem(id=3, subset=""b""),\n                    DatasetItem(id=4, subset=""b""),\n                    DatasetItem(id=5, subset=""b""),\n                    DatasetItem(id=6, subset=""""),\n                    DatasetItem(id=7, subset=""""),\n                ])\n\n        actual = transforms.RandomSplit(SrcExtractor(), splits=[\n            (\'train\', 4.0 / 7.0),\n            (\'test\', 3.0 / 7.0),\n        ])\n\n        self.assertEqual(4, len(actual.get_subset(\'train\')))\n        self.assertEqual(3, len(actual.get_subset(\'test\')))\n\n    def test_random_split_gives_error_on_wrong_ratios(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([DatasetItem(id=1)])\n\n        with self.assertRaises(Exception):\n            transforms.RandomSplit(SrcExtractor(), splits=[\n                (\'train\', 0.5),\n                (\'test\', 0.7),\n            ])\n\n        with self.assertRaises(Exception):\n            transforms.RandomSplit(SrcExtractor(), splits=[])\n\n        with self.assertRaises(Exception):\n            transforms.RandomSplit(SrcExtractor(), splits=[\n                (\'train\', -0.5),\n                (\'test\', 1.5),\n            ])\n\n    def test_remap_labels(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, annotations=[\n                        # Should be remapped\n                        Label(1),\n                        Bbox(1, 2, 3, 4, label=2),\n                        Mask(image=np.array([1]), label=3),\n\n                        # Should be kept\n                        Polygon([1, 1, 2, 2, 3, 4], label=4),\n                        PolyLine([1, 3, 4, 2, 5, 6], label=None)\n                    ]),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add(\'label0\')\n                label_cat.add(\'label1\')\n                label_cat.add(\'label2\')\n                label_cat.add(\'label3\')\n                label_cat.add(\'label4\')\n\n                mask_cat = MaskCategories(\n                    colormap=mask_tools.generate_colormap(5))\n\n                return {\n                    AnnotationType.label: label_cat,\n                    AnnotationType.mask: mask_cat,\n                }\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, annotations=[\n                        Label(1),\n                        Bbox(1, 2, 3, 4, label=0),\n                        Mask(image=np.array([1]), label=1),\n\n                        Polygon([1, 1, 2, 2, 3, 4], label=2),\n                        PolyLine([1, 3, 4, 2, 5, 6], label=None)\n                    ]),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add(\'label0\')\n                label_cat.add(\'label9\')\n                label_cat.add(\'label4\')\n\n                mask_cat = MaskCategories(colormap={\n                    k: v for k, v in mask_tools.generate_colormap(5).items()\n                    if k in { 0, 1, 3, 4 }\n                })\n\n                return {\n                    AnnotationType.label: label_cat,\n                    AnnotationType.mask: mask_cat,\n                }\n\n        actual = transforms.RemapLabels(SrcExtractor(), mapping={\n            \'label1\': \'label9\',\n            \'label2\': \'label0\',\n            \'label3\': \'label9\',\n        }, default=\'keep\')\n\n        compare_datasets(self, DstExtractor(), actual)\n\n    def test_remap_labels_delete_unspecified(self):\n        class SrcExtractor(Extractor):\n            def __iter__(self):\n                return iter([ DatasetItem(id=1, annotations=[ Label(0) ]) ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add(\'label0\')\n\n                return { AnnotationType.label: label_cat }\n\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([ DatasetItem(id=1, annotations=[]) ])\n\n            def categories(self):\n                return { AnnotationType.label: LabelCategories() }\n\n        actual = transforms.RemapLabels(SrcExtractor(),\n            mapping={}, default=\'delete\')\n\n        compare_datasets(self, DstExtractor(), actual)\n'"
datumaro/tests/test_voc_format.py,0,"b""from collections import OrderedDict\nimport numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    AnnotationType, Label, Bbox, Mask, LabelCategories,\n)\nimport datumaro.plugins.voc_format.format as VOC\nfrom datumaro.plugins.voc_format.converter import (\n    VocConverter,\n    VocClassificationConverter,\n    VocDetectionConverter,\n    VocLayoutConverter,\n    VocActionConverter,\n    VocSegmentationConverter,\n)\nfrom datumaro.plugins.voc_format.importer import VocImporter\nfrom datumaro.components.project import Project\nfrom datumaro.util.image import Image\nfrom datumaro.util.test_utils import TestDir, compare_datasets\n\n\nclass VocFormatTest(TestCase):\n    def test_colormap_generator(self):\n        reference = np.array([\n            [  0,   0,   0],\n            [128,   0,   0],\n            [  0, 128,   0],\n            [128, 128,   0],\n            [  0,   0, 128],\n            [128,   0, 128],\n            [  0, 128, 128],\n            [128, 128, 128],\n            [ 64,   0,   0],\n            [192,   0,   0],\n            [ 64, 128,   0],\n            [192, 128,   0],\n            [ 64,   0, 128],\n            [192,   0, 128],\n            [ 64, 128, 128],\n            [192, 128, 128],\n            [  0,  64,   0],\n            [128,  64,   0],\n            [  0, 192,   0],\n            [128, 192,   0],\n            [  0,  64, 128],\n            [224, 224, 192], # ignored\n        ])\n\n        self.assertTrue(np.array_equal(reference, list(VOC.VocColormap.values())))\n\n    def test_can_write_and_parse_labelmap(self):\n        src_label_map = VOC.make_voc_label_map()\n        src_label_map['qq'] = [None, ['part1', 'part2'], ['act1', 'act2']]\n        src_label_map['ww'] = [(10, 20, 30), [], ['act3']]\n\n        with TestDir() as test_dir:\n            file_path = osp.join(test_dir, 'test.txt')\n\n            VOC.write_label_map(file_path, src_label_map)\n            dst_label_map = VOC.parse_label_map(file_path)\n\n            self.assertEqual(src_label_map, dst_label_map)\n\nclass TestExtractorBase(Extractor):\n    def _label(self, voc_label):\n        return self.categories()[AnnotationType.label].find(voc_label)[0]\n\n    def categories(self):\n        return VOC.make_voc_categories()\n\n\nDUMMY_DATASET_DIR = osp.join(osp.dirname(__file__), 'assets', 'voc_dataset')\n\nclass VocImportTest(TestCase):\n    def test_can_import(self):\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id='2007_000001', subset='train',\n                        image=Image(path='2007_000001.jpg', size=(20, 10)),\n                        annotations=[\n                            Label(self._label(l.name))\n                            for l in VOC.VocLabel if l.value % 2 == 1\n                        ] + [\n                            Bbox(1, 2, 2, 2, label=self._label('cat'),\n                                attributes={\n                                    'pose': VOC.VocPose(1).name,\n                                    'truncated': True,\n                                    'difficult': False,\n                                    'occluded': False,\n                                },\n                                id=1, group=1,\n                            ),\n                            Bbox(4, 5, 2, 2, label=self._label('person'),\n                                attributes={\n                                    'truncated': False,\n                                    'difficult': False,\n                                    'occluded': False,\n                                    **{\n                                        a.name: a.value % 2 == 1\n                                        for a in VOC.VocAction\n                                    }\n                                },\n                                id=2, group=2,\n                            ),\n                            Bbox(5.5, 6, 2, 2, label=self._label(\n                                    VOC.VocBodyPart(1).name),\n                                group=2\n                            ),\n                            Mask(image=np.ones([5, 10]),\n                                label=self._label(VOC.VocLabel(2).name),\n                                group=1,\n                            ),\n                        ]\n                    ),\n                    DatasetItem(id='2007_000002', subset='test',\n                        image=np.zeros((20, 10, 3))),\n                ])\n\n        dataset = Project.import_from(DUMMY_DATASET_DIR, 'voc').make_dataset()\n\n        compare_datasets(self, DstExtractor(), dataset)\n\n    def test_can_detect_voc(self):\n        self.assertTrue(VocImporter.detect(DUMMY_DATASET_DIR))\n\nclass VocConverterTest(TestCase):\n    def _test_save_and_load(self, source_dataset, converter, test_dir,\n            target_dataset=None, importer_args=None):\n        converter(source_dataset, test_dir)\n\n        if importer_args is None:\n            importer_args = {}\n        parsed_dataset = VocImporter()(test_dir, **importer_args).make_dataset()\n\n        if target_dataset is None:\n            target_dataset = source_dataset\n\n        compare_datasets(self, expected=target_dataset, actual=parsed_dataset)\n\n    def test_can_save_voc_cls(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=0, subset='a', annotations=[\n                        Label(1),\n                        Label(2),\n                        Label(3),\n                    ]),\n\n                    DatasetItem(id=1, subset='b', annotations=[\n                        Label(4),\n                    ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocClassificationConverter(label_map='voc'), test_dir)\n\n    def test_can_save_voc_det(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Bbox(2, 3, 4, 5, label=2,\n                            attributes={ 'occluded': True }\n                        ),\n                        Bbox(2, 3, 4, 5, label=3,\n                            attributes={ 'truncated': True },\n                        ),\n                    ]),\n\n                    DatasetItem(id=2, subset='b', annotations=[\n                        Bbox(5, 4, 6, 5, label=3,\n                            attributes={ 'difficult': True },\n                        ),\n                    ]),\n                ])\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Bbox(2, 3, 4, 5, label=2, id=1, group=1,\n                            attributes={\n                                'truncated': False,\n                                'difficult': False,\n                                'occluded': True,\n                            }\n                        ),\n                        Bbox(2, 3, 4, 5, label=3, id=2, group=2,\n                            attributes={\n                                'truncated': True,\n                                'difficult': False,\n                                'occluded': False,\n                            },\n                        ),\n                    ]),\n\n                    DatasetItem(id=2, subset='b', annotations=[\n                        Bbox(5, 4, 6, 5, label=3, id=1, group=1,\n                            attributes={\n                                'truncated': False,\n                                'difficult': True,\n                                'occluded': False,\n                            },\n                        ),\n                    ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocDetectionConverter(label_map='voc'), test_dir,\n                target_dataset=DstExtractor())\n\n    def test_can_save_voc_segm(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        # overlapping masks, the first should be truncated\n                        # the second and third are different instances\n                        Mask(image=np.array([[0, 0, 0, 1, 0]]), label=3,\n                            z_order=3),\n                        Mask(image=np.array([[0, 1, 1, 1, 0]]), label=4,\n                            z_order=1),\n                        Mask(image=np.array([[1, 1, 0, 0, 0]]), label=3,\n                            z_order=2),\n                    ]),\n                ])\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Mask(image=np.array([[0, 0, 1, 0, 0]]), label=4,\n                            group=1),\n                        Mask(image=np.array([[1, 1, 0, 0, 0]]), label=3,\n                            group=2),\n                        Mask(image=np.array([[0, 0, 0, 1, 0]]), label=3,\n                            group=3),\n                    ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocSegmentationConverter(label_map='voc'), test_dir,\n                target_dataset=DstExtractor())\n\n    def test_can_save_voc_segm_unpainted(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        # overlapping masks, the first should be truncated\n                        # the second and third are different instances\n                        Mask(image=np.array([[0, 0, 0, 1, 0]]), label=3,\n                            z_order=3),\n                        Mask(image=np.array([[0, 1, 1, 1, 0]]), label=4,\n                            z_order=1),\n                        Mask(image=np.array([[1, 1, 0, 0, 0]]), label=3,\n                            z_order=2),\n                    ]),\n                ])\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Mask(image=np.array([[0, 0, 1, 0, 0]]), label=4,\n                            group=1),\n                        Mask(image=np.array([[1, 1, 0, 0, 0]]), label=3,\n                            group=2),\n                        Mask(image=np.array([[0, 0, 0, 1, 0]]), label=3,\n                            group=3),\n                    ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocSegmentationConverter(label_map='voc', apply_colormap=False),\n                test_dir, target_dataset=DstExtractor())\n\n    def test_can_save_voc_segm_with_many_instances(self):\n        def bit(x, y, shape):\n            mask = np.zeros(shape)\n            mask[y, x] = 1\n            return mask\n\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Mask(image=bit(x, y, shape=[10, 10]),\n                            label=self._label(VOC.VocLabel(3).name),\n                            z_order=10 * y + x + 1\n                        )\n                        for y in range(10) for x in range(10)\n                    ]),\n                ])\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Mask(image=bit(x, y, shape=[10, 10]),\n                            label=self._label(VOC.VocLabel(3).name),\n                            group=10 * y + x + 1\n                        )\n                        for y in range(10) for x in range(10)\n                    ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocSegmentationConverter(label_map='voc'), test_dir,\n                target_dataset=DstExtractor())\n\n    def test_can_save_voc_layout(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Bbox(2, 3, 4, 5, label=2, id=1, group=1,\n                            attributes={\n                                'pose': VOC.VocPose(1).name,\n                                'truncated': True,\n                                'difficult': False,\n                                'occluded': False,\n                            }\n                        ),\n                        Bbox(2, 3, 1, 1, label=self._label(\n                            VOC.VocBodyPart(1).name), group=1),\n                        Bbox(5, 4, 3, 2, label=self._label(\n                            VOC.VocBodyPart(2).name), group=1),\n                    ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocLayoutConverter(label_map='voc'), test_dir)\n\n    def test_can_save_voc_action(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Bbox(2, 3, 4, 5, label=2,\n                            attributes={\n                                'truncated': True,\n                                VOC.VocAction(1).name: True,\n                                VOC.VocAction(2).name: True,\n                            }\n                        ),\n                        Bbox(5, 4, 3, 2, label=self._label('person'),\n                            attributes={\n                                'truncated': True,\n                                VOC.VocAction(1).name: True,\n                                VOC.VocAction(2).name: True,\n                            }\n                        ),\n                    ]),\n                ])\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', annotations=[\n                        Bbox(2, 3, 4, 5, label=2,\n                            id=1, group=1, attributes={\n                                'truncated': True,\n                                'difficult': False,\n                                'occluded': False,\n                                # no attributes here in the label categories\n                            }\n                        ),\n                        Bbox(5, 4, 3, 2, label=self._label('person'),\n                            id=2, group=2, attributes={\n                                'truncated': True,\n                                'difficult': False,\n                                'occluded': False,\n                                VOC.VocAction(1).name: True,\n                                VOC.VocAction(2).name: True,\n                                **{\n                                    a.name: False for a in VOC.VocAction\n                                        if a.value not in {1, 2}\n                                }\n                            }\n                        ),\n                    ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocActionConverter(label_map='voc'), test_dir,\n                target_dataset=DstExtractor())\n\n    def test_can_save_dataset_with_no_subsets(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, annotations=[\n                        Label(2),\n                        Label(3),\n                    ]),\n\n                    DatasetItem(id=2, annotations=[\n                        Label(3),\n                    ]),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocConverter(label_map='voc'), test_dir)\n\n    def test_can_save_dataset_with_images(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='a', image=np.ones([4, 5, 3])),\n                    DatasetItem(id=2, subset='a', image=np.ones([5, 4, 3])),\n\n                    DatasetItem(id=3, subset='b', image=np.ones([2, 6, 3])),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocConverter(label_map='voc', save_images=True), test_dir)\n\n    def test_dataset_with_voc_labelmap(self):\n        class SrcExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(2, 3, 4, 5, label=self._label('cat'), id=1),\n                    Bbox(1, 2, 3, 4, label=self._label('non_voc_label'), id=2),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add(VOC.VocLabel.cat.name)\n                label_cat.add('non_voc_label')\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    # drop non voc label\n                    Bbox(2, 3, 4, 5, label=self._label('cat'), id=1, group=1,\n                        attributes={\n                            'truncated': False,\n                            'difficult': False,\n                            'occluded': False,\n                        }\n                    ),\n                ])\n\n            def categories(self):\n                return VOC.make_voc_categories()\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                SrcExtractor(), VocConverter(label_map='voc'),\n                test_dir, target_dataset=DstExtractor())\n\n    def test_dataset_with_guessed_labelmap(self):\n        class SrcExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(2, 3, 4, 5, label=0, id=1),\n                    Bbox(1, 2, 3, 4, label=1, id=2),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add(VOC.VocLabel(1).name)\n                label_cat.add('non_voc_label')\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(2, 3, 4, 5, label=self._label(VOC.VocLabel(1).name),\n                        id=1, group=1, attributes={\n                            'truncated': False,\n                            'difficult': False,\n                            'occluded': False,\n                        }\n                    ),\n                    Bbox(1, 2, 3, 4, label=self._label('non_voc_label'),\n                        id=2, group=2, attributes={\n                            'truncated': False,\n                            'difficult': False,\n                            'occluded': False,\n                        }\n                    ),\n                ])\n\n            def categories(self):\n                label_map = VOC.make_voc_label_map()\n                label_map['non_voc_label'] = [None, [], []]\n                for label_desc in label_map.values():\n                    label_desc[0] = None # rebuild colormap\n                return VOC.make_voc_categories(label_map)\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                SrcExtractor(), VocConverter(label_map='guess'),\n                test_dir, target_dataset=DstExtractor())\n\n    def test_dataset_with_source_labelmap_undefined(self):\n        class SrcExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(2, 3, 4, 5, label=0, id=1),\n                    Bbox(1, 2, 3, 4, label=1, id=2),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add('Label_1')\n                label_cat.add('label_2')\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(2, 3, 4, 5, label=self._label('Label_1'),\n                        id=1, group=1, attributes={\n                            'truncated': False,\n                            'difficult': False,\n                            'occluded': False,\n                        }\n                    ),\n                    Bbox(1, 2, 3, 4, label=self._label('label_2'),\n                        id=2, group=2, attributes={\n                            'truncated': False,\n                            'difficult': False,\n                            'occluded': False,\n                        }\n                    ),\n                ])\n\n            def categories(self):\n                label_map = OrderedDict()\n                label_map['background'] = [None, [], []]\n                label_map['Label_1'] = [None, [], []]\n                label_map['label_2'] = [None, [], []]\n                return VOC.make_voc_categories(label_map)\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                SrcExtractor(), VocConverter(label_map='source'),\n                test_dir, target_dataset=DstExtractor())\n\n    def test_dataset_with_source_labelmap_defined(self):\n        class SrcExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(2, 3, 4, 5, label=0, id=1),\n                    Bbox(1, 2, 3, 4, label=2, id=2),\n                ])\n\n            def categories(self):\n                label_map = OrderedDict()\n                label_map['label_1'] = [(1, 2, 3), [], []]\n                label_map['background'] = [(0, 0, 0), [], []] # can be not 0\n                label_map['label_2'] = [(3, 2, 1), [], []]\n                return VOC.make_voc_categories(label_map)\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(2, 3, 4, 5, label=self._label('label_1'),\n                        id=1, group=1, attributes={\n                            'truncated': False,\n                            'difficult': False,\n                            'occluded': False,\n                        }\n                    ),\n                    Bbox(1, 2, 3, 4, label=self._label('label_2'),\n                        id=2, group=2, attributes={\n                            'truncated': False,\n                            'difficult': False,\n                            'occluded': False,\n                        }\n                    ),\n                ])\n\n            def categories(self):\n                label_map = OrderedDict()\n                label_map['label_1'] = [(1, 2, 3), [], []]\n                label_map['background'] = [(0, 0, 0), [], []]\n                label_map['label_2'] = [(3, 2, 1), [], []]\n                return VOC.make_voc_categories(label_map)\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                SrcExtractor(), VocConverter(label_map='source'),\n                test_dir, target_dataset=DstExtractor())\n\n    def test_dataset_with_fixed_labelmap(self):\n        class SrcExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(2, 3, 4, 5, label=0, id=1),\n                    Bbox(1, 2, 3, 4, label=1, id=2, group=2,\n                        attributes={'act1': True}),\n                    Bbox(2, 3, 4, 5, label=2, id=3, group=2),\n                    Bbox(2, 3, 4, 6, label=3, id=4, group=2),\n                ])\n\n            def categories(self):\n                label_cat = LabelCategories()\n                label_cat.add('foreign_label')\n                label_cat.add('label', attributes=['act1', 'act2'])\n                label_cat.add('label_part1')\n                label_cat.add('label_part2')\n                return {\n                    AnnotationType.label: label_cat,\n                }\n\n        label_map = {\n            'label': [None, ['label_part1', 'label_part2'], ['act1', 'act2']]\n        }\n\n        class DstExtractor(TestExtractorBase):\n            def __iter__(self):\n                yield DatasetItem(id=1, annotations=[\n                    Bbox(1, 2, 3, 4, label=0, id=1, group=1,\n                        attributes={\n                            'act1': True,\n                            'act2': False,\n                            'truncated': False,\n                            'difficult': False,\n                            'occluded': False,\n                        }\n                    ),\n                    Bbox(2, 3, 4, 5, label=1, group=1),\n                    Bbox(2, 3, 4, 6, label=2, group=1),\n                ])\n\n            def categories(self):\n                return VOC.make_voc_categories(label_map)\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(\n                SrcExtractor(), VocConverter(label_map=label_map),\n                test_dir, target_dataset=DstExtractor())\n\n    def test_can_save_dataset_with_image_info(self):\n        class TestExtractor(TestExtractorBase):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, image=Image(path='1.jpg', size=(10, 15))),\n                ])\n\n        with TestDir() as test_dir:\n            self._test_save_and_load(TestExtractor(),\n                VocConverter(label_map='voc'), test_dir)\n"""
datumaro/tests/test_yolo_format.py,0,"b""import numpy as np\nimport os.path as osp\n\nfrom unittest import TestCase\n\nfrom datumaro.components.extractor import (Extractor, DatasetItem,\n    AnnotationType, Bbox, LabelCategories,\n)\nfrom datumaro.components.project import Project\nfrom datumaro.plugins.yolo_format.importer import YoloImporter\nfrom datumaro.plugins.yolo_format.converter import YoloConverter\nfrom datumaro.util.image import Image, save_image\nfrom datumaro.util.test_utils import TestDir, compare_datasets\n\n\nclass YoloFormatTest(TestCase):\n    def test_can_save_and_load(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train', image=np.ones((8, 8, 3)),\n                        annotations=[\n                            Bbox(0, 2, 4, 2, label=2),\n                            Bbox(0, 1, 2, 3, label=4),\n                        ]),\n                    DatasetItem(id=2, subset='train', image=np.ones((10, 10, 3)),\n                        annotations=[\n                            Bbox(0, 2, 4, 2, label=2),\n                            Bbox(3, 3, 2, 3, label=4),\n                            Bbox(2, 1, 2, 3, label=4),\n                        ]),\n\n                    DatasetItem(id=3, subset='valid', image=np.ones((8, 8, 3)),\n                        annotations=[\n                            Bbox(0, 1, 5, 2, label=2),\n                            Bbox(0, 2, 3, 2, label=5),\n                            Bbox(0, 2, 4, 2, label=6),\n                            Bbox(0, 7, 3, 2, label=7),\n                        ]),\n                ])\n\n            def categories(self):\n                label_categories = LabelCategories()\n                for i in range(10):\n                    label_categories.add('label_' + str(i))\n                return {\n                    AnnotationType.label: label_categories,\n                }\n\n        with TestDir() as test_dir:\n            source_dataset = TestExtractor()\n\n            YoloConverter(save_images=True)(source_dataset, test_dir)\n            parsed_dataset = YoloImporter()(test_dir).make_dataset()\n\n            compare_datasets(self, source_dataset, parsed_dataset)\n\n    def test_can_save_dataset_with_image_info(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train',\n                        image=Image(path='1.jpg', size=(10, 15)),\n                        annotations=[\n                            Bbox(0, 2, 4, 2, label=2),\n                            Bbox(3, 3, 2, 3, label=4),\n                        ]),\n                ])\n\n            def categories(self):\n                label_categories = LabelCategories()\n                for i in range(10):\n                    label_categories.add('label_' + str(i))\n                return {\n                    AnnotationType.label: label_categories,\n                }\n\n        with TestDir() as test_dir:\n            source_dataset = TestExtractor()\n\n            YoloConverter()(source_dataset, test_dir)\n\n            save_image(osp.join(test_dir, 'obj_train_data', '1.jpg'),\n                np.ones((10, 15, 3))) # put the image for dataset\n            parsed_dataset = YoloImporter()(test_dir).make_dataset()\n\n            compare_datasets(self, source_dataset, parsed_dataset)\n\n    def test_can_load_dataset_with_exact_image_info(self):\n        class TestExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train',\n                        image=Image(path='1.jpg', size=(10, 15)),\n                        annotations=[\n                            Bbox(0, 2, 4, 2, label=2),\n                            Bbox(3, 3, 2, 3, label=4),\n                        ]),\n                ])\n\n            def categories(self):\n                label_categories = LabelCategories()\n                for i in range(10):\n                    label_categories.add('label_' + str(i))\n                return {\n                    AnnotationType.label: label_categories,\n                }\n\n        with TestDir() as test_dir:\n            source_dataset = TestExtractor()\n\n            YoloConverter()(source_dataset, test_dir)\n\n            parsed_dataset = YoloImporter()(test_dir,\n                image_info={'1': (10, 15)}).make_dataset()\n\n            compare_datasets(self, source_dataset, parsed_dataset)\n\n\nDUMMY_DATASET_DIR = osp.join(osp.dirname(__file__), 'assets', 'yolo_dataset')\n\nclass YoloImporterTest(TestCase):\n    def test_can_detect(self):\n        self.assertTrue(YoloImporter.detect(DUMMY_DATASET_DIR))\n\n    def test_can_import(self):\n        class DstExtractor(Extractor):\n            def __iter__(self):\n                return iter([\n                    DatasetItem(id=1, subset='train',\n                        image=np.ones((10, 15, 3)),\n                        annotations=[\n                            Bbox(0, 2, 4, 2, label=2),\n                            Bbox(3, 3, 2, 3, label=4),\n                        ]),\n                ])\n\n            def categories(self):\n                label_categories = LabelCategories()\n                for i in range(10):\n                    label_categories.add('label_' + str(i))\n                return {\n                    AnnotationType.label: label_categories,\n                }\n\n        dataset = Project.import_from(DUMMY_DATASET_DIR, 'yolo') \\\n            .make_dataset()\n\n        compare_datasets(self, DstExtractor(), dataset)\n"""
utils/auto_annotation/run_model.py,0,"b'import os\nimport sys\nimport json\nimport argparse\nimport random\nimport logging\nimport fnmatch\nfrom operator import xor\n\nimport numpy as np\nimport cv2\n\nwork_dir = os.path.dirname(os.path.abspath(__file__))\ncvat_dir = os.path.join(work_dir, \'..\', \'..\')\n\nsys.path.insert(0, cvat_dir)\n\nfrom cvat.apps.auto_annotation.inference import run_inference_engine_annotation\n\n\ndef _get_kwargs():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--py\', help=\'Path to the python interpt file\')\n    parser.add_argument(\'--xml\', help=\'Path to the xml file\')\n    parser.add_argument(\'--bin\', help=\'Path to the bin file\')\n    parser.add_argument(\'--json\', help=\'Path to the JSON mapping file\')\n\n    parser.add_argument(\'--model-name\', help=\'Name of the model in the Model Manager\')\n    parser.add_argument(\'--task-id\', type=int, help=\'ID task used to test the model\')\n\n    parser.add_argument(\'--restricted\', dest=\'restricted\', action=\'store_true\')\n    parser.add_argument(\'--unrestricted\', dest=\'restricted\', action=\'store_false\')\n    parser.add_argument(\'--image-files\', nargs=\'*\', help=\'Paths to image files you want to test\')\n\n    parser.add_argument(\'--show-images\', action=\'store_true\', help=\'Show the results of the annotation in a window\')\n    parser.add_argument(\'--show-image-delay\', default=0, type=int, help=\'Displays the images for a set duration in milliseconds, default is until a key is pressed\')\n    parser.add_argument(\'--serialize\', default=False, action=\'store_true\', help=\'Try to serialize the result\')\n    parser.add_argument(\'--show-labels\', action=\'store_true\', help=\'Show the labels on the window\')\n\n    return vars(parser.parse_args())\n\ndef _init_django(settings):\n    import django\n    os.environ[\'DJANGO_SETTINGS_MODULE\'] = settings\n    django.setup()\n\ndef random_color():\n    rgbl=[255,0,0]\n    random.shuffle(rgbl)\n    return tuple(rgbl)\n\n\ndef pairwise(iterable):\n    result = []\n    for i in range(0, len(iterable) - 1, 2):\n        result.append((iterable[i], iterable[i+1]))\n    return np.array(result, dtype=np.int32)\n\ndef find_min_y(array):\n    min_ = sys.maxsize\n    index = None\n    for i, pair in enumerate(array):\n        if pair[1] < min_:\n            min_ = pair[1]\n            index = i\n\n    return array[index]\n\ndef _get_docker_files(model_name: str, task_id: int):\n    _init_django(\'cvat.settings.development\')\n\n    from cvat.apps.auto_annotation.models import AnnotationModel\n    from cvat.apps.engine.models import Task as TaskModel\n\n    task = TaskModel(pk=task_id)\n    model = AnnotationModel.objects.get(name=model_name)\n\n    images_dir = task.data.get_data_dirname()\n\n    py_file = model.interpretation_file.name\n    mapping_file = model.labelmap_file.name\n    xml_file = model.model_file.name\n    bin_file = model.weights_file.name\n\n    image_files = []\n    images_dir = os.path.abspath(images_dir)\n    for root, _, filenames in os.walk(images_dir):\n        for filename in fnmatch.filter(filenames, \'*.jpg\'):\n            image_files.append(os.path.join(root, filename))\n\n    return py_file, mapping_file, bin_file, xml_file, image_files\n\n\ndef main():\n    kwargs = _get_kwargs()\n\n    py_file = kwargs.get(\'py\')\n    bin_file = kwargs.get(\'bin\')\n    mapping_file = os.path.abspath(kwargs.get(\'json\'))\n    xml_file = kwargs.get(\'xml\')\n\n    model_name = kwargs.get(\'model_name\')\n    task_id = kwargs.get(\'task_id\')\n\n    is_docker = model_name and task_id\n\n    # xor is `exclusive or`. English is: if one or the other but not both\n    if xor(bool(model_name), bool(task_id)):\n        logging.critical(\'Must provide both `--model-name` and `--task-id` together!\')\n        return\n\n    if is_docker:\n        files = _get_docker_files(model_name, task_id)\n        py_file = files[0]\n        mapping_file = files[1]\n        bin_file = files[2]\n        xml_file = files[3]\n        image_files = files[4]\n    else:\n        return_ = False\n        if not py_file:\n            logging.critical(\'Must provide --py file!\')\n            return_ = True\n        if not bin_file:\n            logging.critical(\'Must provide --bin file!\')\n            return_ = True\n        if not xml_file:\n            logging.critical(\'Must provide --xml file!\')\n            return_ = True\n        if not mapping_file:\n            logging.critical(\'Must provide --json file!\')\n            return_ = True\n\n        if return_:\n            return\n\n    if not os.path.isfile(py_file):\n        logging.critical(\'Py file not found! Check the path\')\n        return\n\n    if not os.path.isfile(bin_file):\n        logging.critical(\'Bin file is not found! Check path!\')\n        return\n\n    if not os.path.isfile(xml_file):\n        logging.critical(\'XML File not found! Check path!\')\n        return\n\n    if not os.path.isfile(mapping_file):\n        logging.critical(\'JSON file is not found! Check path!\')\n        return\n\n    with open(mapping_file) as json_file:\n        try:\n            mapping = json.load(json_file)\n        except json.decoder.JSONDecodeError:\n            logging.critical(\'JSON file not able to be parsed! Check file\')\n            return\n\n    try:\n        mapping = mapping[\'label_map\']\n    except KeyError:\n        logging.critical(""JSON Mapping file must contain key `label_map`!"")\n        logging.critical(""Exiting"")\n        return\n\n    mapping = {int(k): v for k, v in mapping.items()}\n\n    restricted = kwargs[\'restricted\']\n\n    if not is_docker:\n        image_files = kwargs.get(\'image_files\')\n\n    if image_files:\n        image_data = [cv2.imread(f) for f in image_files]\n    else:\n        test_image = np.ones((1024, 1980, 3), np.uint8) * 255\n        image_data = [test_image,]\n    attribute_spec = {}\n\n    results = run_inference_engine_annotation(image_data,\n                                              xml_file,\n                                              bin_file,\n                                              mapping,\n                                              attribute_spec,\n                                              py_file,\n                                              restricted=restricted)\n\n\n    logging.warning(\'Inference didn\\\'t have any errors.\')\n    show_images = kwargs.get(\'show_images\', False)\n\n    if show_images:\n        if image_files is None:\n            logging.critical(""Warning, no images provided!"")\n            logging.critical(\'Exiting without presenting results\')\n            return\n\n        if not results[\'shapes\']:\n            logging.warning(str(results))\n            logging.critical(""No objects detected!"")\n            return\n\n        show_image_delay = kwargs[\'show_image_delay\']\n        show_labels = kwargs.get(\'show_labels\')\n\n        for index, data in enumerate(image_data):\n            for detection in results[\'shapes\']:\n                if not detection[\'frame\'] == index:\n                    continue\n                points = detection[\'points\']\n                label_str = detection[\'label_id\']\n\n                # Cv2 doesn\'t like floats for drawing\n                points = [int(p) for p in points]\n                color = random_color()\n\n                if detection[\'type\'] == \'rectangle\':\n                    cv2.rectangle(data, (points[0], points[1]), (points[2], points[3]), color, 3)\n\n                    if show_labels:\n                        cv2.putText(data, label_str, (points[0], points[1] - 7), cv2.FONT_HERSHEY_COMPLEX, 0.6, color, 1)\n\n                elif detection[\'type\'] in (\'polygon\', \'polyline\'):\n                    # polylines is picky about datatypes\n                    points = pairwise(points)\n                    cv2.polylines(data, [points], 1, color)\n\n                    if show_labels:\n                        min_point = find_min_y(points)\n                        cv2.putText(data, label_str, (min_point[0], min_point[1] - 7), cv2.FONT_HERSHEY_COMPLEX, 0.6, color, 1)\n\n            cv2.imshow(str(index), data)\n            cv2.waitKey(show_image_delay)\n            cv2.destroyWindow(str(index))\n\n    if kwargs[\'serialize\']:\n        _init_django(\'cvat.settings.production\')\n\n        from cvat.apps.engine.serializers import LabeledDataSerializer\n\n        # NOTE: We\'re actually using `run_inference_engine_annotation`\n        # incorrectly here. The `mapping` dict is supposed to be a mapping\n        # of integers -> integers and represents the transition from model\n        # integers to the labels in the database. We\'re using a mapping of\n        # integers -> strings. For testing purposes, this shortcut is fine.\n        # We just want to make sure everything works. Until, that is....\n        # we want to test using the label serializer. Then we have to transition\n        # back to integers, otherwise the serializer complains about have a string\n        # where an integer is expected. We\'ll just brute force that.\n\n        for shape in results[\'shapes\']:\n            # Change the english label to an integer for serialization validation\n            shape[\'label_id\'] = 1\n\n        serializer = LabeledDataSerializer(data=results)\n\n        if not serializer.is_valid():\n            logging.critical(\'Data unable to be serialized correctly!\')\n            serializer.is_valid(raise_exception=True)\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/cli/cli.py,0,"b""#!/usr/bin/env python3\n#\n# SPDX-License-Identifier: MIT\nimport logging\nimport requests\nimport sys\nfrom http.client import HTTPConnection\nfrom core.core import CLI, CVAT_API_V1\nfrom core.definition import parser\nlog = logging.getLogger(__name__)\n\n\ndef config_log(level):\n    log = logging.getLogger('core')\n    log.addHandler(logging.StreamHandler(sys.stdout))\n    log.setLevel(level)\n    if level <= logging.DEBUG:\n        HTTPConnection.debuglevel = 1\n\n\ndef main():\n    actions = {'create': CLI.tasks_create,\n               'delete': CLI.tasks_delete,\n               'ls': CLI.tasks_list,\n               'frames': CLI.tasks_frame,\n               'dump': CLI.tasks_dump,\n               'upload': CLI.tasks_upload}\n    args = parser.parse_args()\n    config_log(args.loglevel)\n    with requests.Session() as session:\n        api = CVAT_API_V1('%s:%s' % (args.server_host, args.server_port))\n        cli = CLI(session, api, args.auth)\n        try:\n            actions[args.action](cli, **args.__dict__)\n        except (requests.exceptions.HTTPError,\n                requests.exceptions.ConnectionError,\n                requests.exceptions.RequestException) as e:\n            log.critical(e)\n\n\nif __name__ == '__main__':\n    main()\n"""
components/analytics/kibana/setup.py,0,"b'#/usr/bin/env python\n\nimport os\nimport argparse\nimport requests\nimport json\n\ndef import_resources(host, port, cfg_file):\n    with open(cfg_file, \'r\') as f:\n        for saved_object in json.load(f):\n            _id = saved_object[""_id""]\n            _type = saved_object[""_type""]\n            _doc = saved_object[""_source""]\n            import_saved_object(host, port, _type, _id, _doc)\n\ndef import_saved_object(host, port, _type, _id, data):\n    saved_objects_api = ""http://{}:{}/api/saved_objects/{}/{}"".format(\n        host, port, _type, _id)\n    request = requests.get(saved_objects_api)\n    if request.status_code == 404:\n        print(""Creating {} as {}"".format(_type, _id))\n        request = requests.post(saved_objects_api, json={""attributes"": data},\n            headers={\'kbn-xsrf\': \'true\'})\n    else:\n        print(""Updating {} named {}"".format(_type, _id))\n        request = requests.put(saved_objects_api, json={""attributes"": data},\n            headers={\'kbn-xsrf\': \'true\'})\n    request.raise_for_status()\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'import Kibana 6.x resources\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'export_file\', metavar=\'FILE\',\n        help=\'JSON export file with resources\')\n    parser.add_argument(\'-p\', \'--port\', metavar=\'PORT\', default=5601, type=int,\n        help=\'port of Kibana instance\')\n    parser.add_argument(\'-H\', \'--host\', metavar=\'HOST\', default=\'kibana\',\n        help=\'host of Kibana instance\')\n    args = parser.parse_args()\n    import_resources(args.host, args.port, args.export_file)\n'"
cvat/apps/authentication/__init__.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\ndefault_app_config = 'cvat.apps.authentication.apps.AuthenticationConfig'\n\nfrom enum import Enum\n\nclass AUTH_ROLE(Enum):\n    ADMIN = 'admin'\n    USER = 'user'\n    ANNOTATOR = 'annotator'\n    OBSERVER = 'observer'\n\n    def __str__(self):\n        return self.value\n"""
cvat/apps/authentication/admin.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.contrib import admin\nfrom django.contrib.auth.models import Group, User\nfrom django.contrib.auth.admin import GroupAdmin, UserAdmin\nfrom django.utils.translation import ugettext_lazy as _\n\nclass CustomUserAdmin(UserAdmin):\n    fieldsets = (\n        (None, {'fields': ('username', 'password')}),\n        (_('Personal info'), {'fields': ('first_name', 'last_name', 'email')}),\n        (_('Permissions'), {'fields': ('is_active', 'is_staff', 'is_superuser',\n                                       'groups',)}),\n        (_('Important dates'), {'fields': ('last_login', 'date_joined')}),\n    )\n\nclass CustomGroupAdmin(GroupAdmin):\n    fieldsets = ((None, {'fields': ('name',)}),)\n\n\nadmin.site.unregister(User)\nadmin.site.unregister(Group)\nadmin.site.register(User, CustomUserAdmin)\nadmin.site.register(Group, CustomGroupAdmin)"""
cvat/apps/authentication/api_urls.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom django.conf import settings\nfrom rest_auth.views import (\n    LoginView, LogoutView, PasswordChangeView,\n    PasswordResetView, PasswordResetConfirmView)\nfrom rest_auth.registration.views import RegisterView\nfrom .views import SigningView\n\nurlpatterns = [\n    path('login', LoginView.as_view(), name='rest_login'),\n    path('logout', LogoutView.as_view(), name='rest_logout'),\n    path('signing', SigningView.as_view(), name='signing')\n]\n\nif settings.DJANGO_AUTH_TYPE == 'BASIC':\n    urlpatterns += [\n        path('register', RegisterView.as_view(), name='rest_register'),\n        path('password/reset', PasswordResetView.as_view(),\n            name='rest_password_reset'),\n        path('password/reset/confirm', PasswordResetConfirmView.as_view(),\n            name='rest_password_reset_confirm'),\n        path('password/change', PasswordChangeView.as_view(),\n            name='rest_password_change'),\n    ]\n"""
cvat/apps/authentication/apps.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\nclass AuthenticationConfig(AppConfig):\n    name = 'cvat.apps.authentication'\n\n    def ready(self):\n        from .auth import register_signals\n\n        register_signals()\n"""
cvat/apps/authentication/auth.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.conf import settings\nfrom django.db.models import Q\nimport rules\nfrom . import AUTH_ROLE\nfrom . import signature\nfrom rest_framework.permissions import BasePermission\nfrom django.core import signing\nfrom rest_framework import authentication, exceptions\nfrom rest_framework.authentication import TokenAuthentication as _TokenAuthentication\nfrom django.contrib.auth import login\n\n# Even with token authorization it is very important to have a valid session id\n# in cookies because in some cases we cannot use token authorization (e.g. when\n# we redirect to the server in UI using just URL). To overkill that we override\n# the class to call `login` method which restores the session id in cookies.\nclass TokenAuthentication(_TokenAuthentication):\n    def authenticate(self, request):\n        auth = super().authenticate(request)\n        session = getattr(request, \'session\')\n        if auth is not None and session.session_key is None:\n            login(request, auth[0], \'django.contrib.auth.backends.ModelBackend\')\n        return auth\n\ndef register_signals():\n    from django.db.models.signals import post_migrate, post_save\n    from django.contrib.auth.models import User, Group\n\n    def create_groups(sender, **kwargs):\n        for role in AUTH_ROLE:\n            db_group, _ = Group.objects.get_or_create(name=role)\n            db_group.save()\n\n    post_migrate.connect(create_groups, weak=False)\n\n    if settings.DJANGO_AUTH_TYPE == \'BASIC\':\n        from .auth_basic import create_user\n\n        post_save.connect(create_user, sender=User)\n    elif settings.DJANGO_AUTH_TYPE == \'LDAP\':\n        import django_auth_ldap.backend\n        from .auth_ldap import create_user\n\n        django_auth_ldap.backend.populate_user.connect(create_user)\n\nclass SignatureAuthentication(authentication.BaseAuthentication):\n    """"""\n    Authentication backend for signed URLs.\n    """"""\n    def authenticate(self, request):\n        """"""\n        Returns authenticated user if URL signature is valid.\n        """"""\n        signer = signature.Signer()\n        sign = request.query_params.get(signature.QUERY_PARAM)\n        if not sign:\n            return\n\n        try:\n            user = signer.unsign(sign, request.build_absolute_uri())\n        except signing.SignatureExpired:\n            raise exceptions.AuthenticationFailed(\'This URL has expired.\')\n        except signing.BadSignature:\n            raise exceptions.AuthenticationFailed(\'Invalid signature.\')\n        if not user.is_active:\n            raise exceptions.AuthenticationFailed(\'User inactive or deleted.\')\n\n        return (user, None)\n\n# AUTH PREDICATES\nhas_admin_role = rules.is_group_member(str(AUTH_ROLE.ADMIN))\nhas_user_role = rules.is_group_member(str(AUTH_ROLE.USER))\nhas_annotator_role = rules.is_group_member(str(AUTH_ROLE.ANNOTATOR))\nhas_observer_role = rules.is_group_member(str(AUTH_ROLE.OBSERVER))\n\n@rules.predicate\ndef is_project_owner(db_user, db_project):\n    # If owner is None (null) the task can be accessed/changed/deleted\n    # only by admin. At the moment each task has an owner.\n    return db_project is not None and db_project.owner == db_user\n\n@rules.predicate\ndef is_project_assignee(db_user, db_project):\n    return db_project is not None and db_project.assignee == db_user\n\n@rules.predicate\ndef is_project_annotator(db_user, db_project):\n    db_tasks = list(db_project.tasks.prefetch_related(\'segment_set\').all())\n    return any([is_task_annotator(db_user, db_task) for db_task in db_tasks])\n\n@rules.predicate\ndef is_task_owner(db_user, db_task):\n    # If owner is None (null) the task can be accessed/changed/deleted\n    # only by admin. At the moment each task has an owner.\n    return db_task.owner == db_user or is_project_owner(db_user, db_task.project)\n\n@rules.predicate\ndef is_task_assignee(db_user, db_task):\n    return db_task.assignee == db_user or is_project_assignee(db_user, db_task.project)\n\n@rules.predicate\ndef is_task_annotator(db_user, db_task):\n    db_segments = list(db_task.segment_set.prefetch_related(\'job_set__assignee\').all())\n    return any([is_job_annotator(db_user, db_job)\n        for db_segment in db_segments for db_job in db_segment.job_set.all()])\n\n@rules.predicate\ndef is_job_owner(db_user, db_job):\n    return is_task_owner(db_user, db_job.segment.task)\n\n@rules.predicate\ndef is_job_annotator(db_user, db_job):\n    db_task = db_job.segment.task\n    # A job can be annotated by any user if the task\'s assignee is None.\n    has_rights = (db_task.assignee is None and not settings.RESTRICTIONS[\'reduce_task_visibility\']) or is_task_assignee(db_user, db_task)\n    if db_job.assignee is not None:\n        has_rights |= (db_user == db_job.assignee)\n\n    return has_rights\n\n# AUTH PERMISSIONS RULES\nrules.add_perm(\'engine.role.user\', has_user_role)\nrules.add_perm(\'engine.role.admin\', has_admin_role)\nrules.add_perm(\'engine.role.annotator\', has_annotator_role)\nrules.add_perm(\'engine.role.observer\', has_observer_role)\n\nrules.add_perm(\'engine.project.create\', has_admin_role | has_user_role)\nrules.add_perm(\'engine.project.access\', has_admin_role | has_observer_role |\n    is_project_owner | is_project_annotator)\nrules.add_perm(\'engine.project.change\', has_admin_role | is_project_owner |\n    is_project_assignee)\nrules.add_perm(\'engine.project.delete\', has_admin_role | is_project_owner)\n\nrules.add_perm(\'engine.task.create\', has_admin_role | has_user_role)\nrules.add_perm(\'engine.task.access\', has_admin_role | has_observer_role |\n    is_task_owner | is_task_annotator)\nrules.add_perm(\'engine.task.change\', has_admin_role | is_task_owner |\n    is_task_assignee)\nrules.add_perm(\'engine.task.delete\', has_admin_role | is_task_owner)\n\nrules.add_perm(\'engine.job.access\', has_admin_role | has_observer_role |\n    is_job_owner | is_job_annotator)\nrules.add_perm(\'engine.job.change\', has_admin_role | is_job_owner |\n    is_job_annotator)\n\nclass AdminRolePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_permission(self, request, view):\n        return request.user.has_perm(""engine.role.admin"")\n\nclass UserRolePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_permission(self, request, view):\n        return request.user.has_perm(""engine.role.user"")\n\nclass AnnotatorRolePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_permission(self, request, view):\n        return request.user.has_perm(""engine.role.annotator"")\n\nclass ObserverRolePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_permission(self, request, view):\n        return request.user.has_perm(""engine.role.observer"")\n\nclass ProjectCreatePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_permission(self, request, view):\n        return request.user.has_perm(""engine.project.create"")\n\nclass ProjectAccessPermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_object_permission(self, request, view, obj):\n        return request.user.has_perm(""engine.project.access"", obj)\n\nclass ProjectChangePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_object_permission(self, request, view, obj):\n        return request.user.has_perm(""engine.project.change"", obj)\n\nclass ProjectDeletePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_object_permission(self, request, view, obj):\n        return request.user.has_perm(""engine.project.delete"", obj)\n\nclass TaskCreatePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_permission(self, request, view):\n        return request.user.has_perm(""engine.task.create"")\n\nclass TaskAccessPermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_object_permission(self, request, view, obj):\n        return request.user.has_perm(""engine.task.access"", obj)\n\n\nclass ProjectGetQuerySetMixin(object):\n    def get_queryset(self):\n        queryset = super().get_queryset()\n        user = self.request.user\n        # Don\'t filter queryset for admin, observer and detail methods\n        if has_admin_role(user) or has_observer_role(user) or self.detail:\n            return queryset\n        else:\n            return queryset.filter(Q(owner=user) | Q(assignee=user) |\n                Q(task__owner=user) | Q(task__assignee=user) |\n                Q(task__segment__job__assignee=user)).distinct()\n\ndef filter_task_queryset(queryset, user):\n    # Don\'t filter queryset for admin, observer\n    if has_admin_role(user) or has_observer_role(user):\n        return queryset\n\n    query_filter = Q(owner=user) | Q(assignee=user) | \\\n        Q(segment__job__assignee=user)\n    if not settings.RESTRICTIONS[\'reduce_task_visibility\']:\n        query_filter |= Q(assignee=None)\n\n    return queryset.filter(query_filter).distinct()\n\nclass TaskGetQuerySetMixin(object):\n    def get_queryset(self):\n        queryset = super().get_queryset()\n        user = self.request.user\n        # Don\'t filter queryset for detail methods\n        if self.detail:\n            return queryset\n        else:\n            return filter_task_queryset(queryset, user)\n\nclass TaskChangePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_object_permission(self, request, view, obj):\n        return request.user.has_perm(""engine.task.change"", obj)\n\nclass TaskDeletePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_object_permission(self, request, view, obj):\n        return request.user.has_perm(""engine.task.delete"", obj)\n\nclass JobAccessPermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_object_permission(self, request, view, obj):\n        return request.user.has_perm(""engine.job.access"", obj)\n\nclass JobChangePermission(BasePermission):\n    # pylint: disable=no-self-use\n    def has_object_permission(self, request, view, obj):\n        return request.user.has_perm(""engine.job.change"", obj)\n'"
cvat/apps/authentication/auth_basic.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\nfrom . import AUTH_ROLE\nfrom django.conf import settings\n\ndef create_user(sender, instance, created, **kwargs):\n    from django.contrib.auth.models import Group\n\n    if instance.is_superuser and instance.is_staff:\n        db_group = Group.objects.get(name=AUTH_ROLE.ADMIN)\n        instance.groups.add(db_group)\n    for group_name in settings.DJANGO_AUTH_DEFAULT_GROUPS:\n        db_group = Group.objects.get(name=getattr(AUTH_ROLE, group_name))\n        instance.groups.add(db_group)\n'"
cvat/apps/authentication/auth_ldap.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.conf import settings\nfrom . import AUTH_ROLE\n\nAUTH_LDAP_GROUPS = {\n    AUTH_ROLE.ADMIN: settings.AUTH_LDAP_ADMIN_GROUPS,\n    AUTH_ROLE.ANNOTATOR: settings.AUTH_LDAP_ANNOTATOR_GROUPS,\n    AUTH_ROLE.USER: settings.AUTH_LDAP_USER_GROUPS,\n    AUTH_ROLE.OBSERVER: settings.AUTH_LDAP_OBSERVER_GROUPS\n}\n\ndef create_user(sender, user=None, ldap_user=None, **kwargs):\n    from django.contrib.auth.models import Group\n    user_groups = []\n    for role in AUTH_ROLE:\n        db_group = Group.objects.get(name=role)\n\n        for ldap_group in AUTH_LDAP_GROUPS[role]:\n            if ldap_group.lower() in ldap_user.group_dns:\n                user_groups.append(db_group)\n                if role == AUTH_ROLE.ADMIN:\n                    user.is_staff = user.is_superuser = True\n\n    # It is important to save the user before adding groups. Please read\n    # https://django-auth-ldap.readthedocs.io/en/latest/users.html#populating-users\n    # The user instance will be saved automatically after the signal handler\n    # is run.\n    user.save()\n    user.groups.set(user_groups)\n'"
cvat/apps/authentication/decorators.py,0,"b""\n# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom functools import wraps\nfrom django.views.generic import RedirectView\nfrom django.contrib.auth import REDIRECT_FIELD_NAME\nfrom django.http import JsonResponse\nfrom django.conf import settings\nfrom cvat.apps.authentication.auth import TokenAuthentication\n\ndef login_required(function=None, redirect_field_name=REDIRECT_FIELD_NAME,\n    login_url=None, redirect_methods=['GET']):\n    def decorator(view_func):\n        @wraps(view_func)\n        def _wrapped_view(request, *args, **kwargs):\n            if request.user.is_authenticated:\n                return view_func(request, *args, **kwargs)\n            else:\n                tokenAuth = TokenAuthentication()\n                auth = tokenAuth.authenticate(request)\n                if auth is not None:\n                    return view_func(request, *args, **kwargs)\n\n                login_url = '{}/login'.format(settings.UI_URL)\n                if request.method not in redirect_methods:\n                    return JsonResponse({'login_page_url': login_url}, status=403)\n\n                return RedirectView.as_view(\n                    url=login_url,\n                    permanent=True,\n                    query_string=True\n                )(request)\n        return _wrapped_view\n    return decorator(function) if function else decorator\n"""
cvat/apps/authentication/forms.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.contrib.auth.forms import (\n    UsernameField,\n    AuthenticationForm,\n    UserCreationForm,\n)\nfrom django.utils.translation import gettext, gettext_lazy as _\nfrom django.contrib.auth.models import User\n\nfrom django import forms\n\nclass AuthForm(AuthenticationForm):\n    username = UsernameField(\n        widget=forms.TextInput(attrs={\'autofocus\': True, \'placeholder\': ""Username""}),\n    )\n    password = forms.CharField(\n        label=_(""Password""),\n        strip=False,\n        widget=forms.PasswordInput(attrs={\'placeholder\': ""Password""}),\n    )\n\nclass NewUserForm(UserCreationForm):\n    username = UsernameField(\n        widget=forms.TextInput(attrs={\'autofocus\': True, \'placeholder\': ""Username (required)""}),\n        required=True,\n    )\n\n    first_name = UsernameField(\n        widget=forms.TextInput(attrs={\'placeholder\': ""First name""}),\n        required=False,\n    )\n\n    last_name = UsernameField(\n        widget=forms.TextInput(attrs={\'placeholder\': ""Last name""}),\n        required=False,\n    )\n\n    email = forms.EmailField(\n        widget=forms.EmailInput(attrs={\'placeholder\': ""Email (required)""}),\n        required=True,\n    )\n\n    password1 = forms.CharField(\n        label=_(""Password""),\n        strip=False,\n        widget=forms.PasswordInput(attrs={\'placeholder\': ""Password (required)""}),\n    )\n    password2 = forms.CharField(\n        label=_(""Password confirmation""),\n        widget=forms.PasswordInput(attrs={\'placeholder\': ""Password confirmation (required)""}),\n        strip=False,\n    )\n\n    class Meta:\n        model = User\n        fields = (\'username\', \'first_name\', \'last_name\', \'email\', )\n'"
cvat/apps/authentication/models.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.db import models\n\n# Create your models here.\n\n'
cvat/apps/authentication/serializers.py,0,"b""from rest_auth.registration.serializers import RegisterSerializer\nfrom rest_framework import serializers\n\n\nclass RegisterSerializerEx(RegisterSerializer):\n  first_name = serializers.CharField(required=False)\n  last_name = serializers.CharField(required=False)\n\n  def get_cleaned_data(self):\n    data = super().get_cleaned_data()\n    data.update({\n        'first_name': self.validated_data.get('first_name', ''),\n        'last_name': self.validated_data.get('last_name', ''),\n    })\n\n    return data\n"""
cvat/apps/authentication/signature.py,0,"b'from django.contrib.auth import get_user_model\nfrom django.core import signing\nfrom furl import furl\nimport hashlib\n\nQUERY_PARAM = \'sign\'\nMAX_AGE = 30\n\n# Got implementation ideas in https://github.com/marcgibbons/drf_signed_auth\nclass Signer:\n    @classmethod\n    def get_salt(cls, url):\n        normalized_url = furl(url).remove(QUERY_PARAM).url.encode(\'utf-8\')\n        salt = hashlib.sha256(normalized_url).hexdigest()\n        return salt\n\n    def sign(self, user, url):\n        """"""\n        Create a signature for a user object.\n        """"""\n        data = {\n            \'user_id\': user.pk,\n            \'username\': user.get_username()\n        }\n\n        return signing.dumps(data, salt=self.get_salt(url))\n\n    def unsign(self, signature, url):\n        """"""\n        Return a user object for a valid signature.\n        """"""\n        User = get_user_model()\n        data = signing.loads(signature, salt=self.get_salt(url), max_age=MAX_AGE)\n\n        if not isinstance(data, dict):\n            raise signing.BadSignature()\n\n        try:\n            return User.objects.get(**{\n                \'pk\': data.get(\'user_id\'),\n                User.USERNAME_FIELD: data.get(\'username\')\n            })\n        except User.DoesNotExist:\n            raise signing.BadSignature()\n'"
cvat/apps/authentication/tests.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.test import TestCase\n\n# Create your tests here.\n\n'
cvat/apps/authentication/urls.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom django.contrib.auth import views as auth_views\nfrom django.conf import settings\n\nfrom . import forms\nfrom . import views\n\nurlpatterns = [\n    path('login', auth_views.LoginView.as_view(form_class=forms.AuthForm,\n        template_name='login.html', extra_context={'note': settings.AUTH_LOGIN_NOTE}),\n        name='login'),\n    path('logout', auth_views.LogoutView.as_view(next_page='login'), name='logout'),\n]\n\nif settings.DJANGO_AUTH_TYPE == 'BASIC':\n    urlpatterns += [\n        path('register', views.register_user, name='register'),\n    ]\n"""
cvat/apps/authentication/views.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.shortcuts import render, redirect\nfrom django.conf import settings\nfrom django.contrib.auth import login, authenticate\nfrom rest_framework import views\nfrom rest_framework.exceptions import ValidationError\nfrom rest_framework.response import Response\nfrom furl import furl\n\nfrom . import forms\nfrom . import signature\n\nfrom django.utils.decorators import method_decorator\nfrom drf_yasg.utils import swagger_auto_schema\nfrom drf_yasg import openapi\n\ndef register_user(request):\n    if request.method == \'POST\':\n        form = forms.NewUserForm(request.POST)\n        if form.is_valid():\n            form.save()\n            username = form.cleaned_data.get(\'username\')\n            raw_password = form.cleaned_data.get(\'password1\')\n            user = authenticate(username=username, password=raw_password)\n            login(request, user)\n            return redirect(settings.LOGIN_REDIRECT_URL)\n    else:\n        form = forms.NewUserForm()\n    return render(request, \'register.html\', {\'form\': form})\n\n@method_decorator(name=\'post\', decorator=swagger_auto_schema(\n    request_body=openapi.Schema(\n        type=openapi.TYPE_OBJECT,\n        required=[\n            \'url\'\n        ],\n        properties={\n            \'url\': openapi.Schema(type=openapi.TYPE_STRING)\n        }\n    ),\n    responses={\'200\': openapi.Response(description=\'text URL\')}\n))\nclass SigningView(views.APIView):\n    """"""\n    This method signs URL for access to the server.\n\n    Signed URL contains a token which authenticates a user on the server.\n    Signed URL is valid during 30 seconds since signing.\n    """"""\n    def post(self, request):\n        url = request.data.get(\'url\')\n        if not url:\n            raise ValidationError(\'Please provide `url` parameter\')\n\n        signer = signature.Signer()\n        url = self.request.build_absolute_uri(url)\n        sign = signer.sign(self.request.user, url)\n\n        url = furl(url).add({signature.QUERY_PARAM: sign}).url\n        return Response(url)\n'"
cvat/apps/auto_annotation/__init__.py,0,"b""\n# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\ndefault_app_config = 'cvat.apps.auto_annotation.apps.AutoAnnotationConfig'\n"""
cvat/apps/auto_annotation/admin.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.contrib import admin\nfrom .models import AnnotationModel\n\n@admin.register(AnnotationModel)\nclass AnnotationModelAdmin(admin.ModelAdmin):\n    list_display = ('name', 'owner', 'created_date', 'updated_date',\n        'shared', 'primary', 'framework')\n\n    def has_add_permission(self, request):\n        return False\n"""
cvat/apps/auto_annotation/apps.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\n\nclass AutoAnnotationConfig(AppConfig):\n    name = ""cvat.apps.auto_annotation""\n\n    def ready(self):\n        from .permissions import setup_permissions\n\n        setup_permissions()\n'"
cvat/apps/auto_annotation/image_loader.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport cv2\nimport numpy as np\n\nclass ImageLoader():\n    def __init__(self, frame_provider):\n        self._frame_provider = frame_provider\n\n    def __iter__(self):\n        for frame, _ in self._frame_provider.get_frames(self._frame_provider.Quality.ORIGINAL):\n            yield self._load_image(frame)\n\n    def __len__(self):\n        return len(self._frame_provider)\n\n    @staticmethod\n    def _load_image(image):\n        return cv2.imdecode(np.fromstring(image.read(), np.uint8), cv2.IMREAD_COLOR)\n'"
cvat/apps/auto_annotation/inference.py,0,"b'import itertools\nfrom .model_loader import ModelLoader\nfrom cvat.apps.engine.utils import import_modules, execute_python_code\n\ndef _process_detections(detections, path_to_conv_script, restricted=True):\n    results = Results()\n    local_vars = {\n        ""detections"": detections,\n        ""results"": results,\n        }\n    source_code = open(path_to_conv_script).read()\n\n    if restricted:\n        global_vars = {\n            ""__builtins__"": {\n                ""str"": str,\n                ""int"": int,\n                ""float"": float,\n                ""max"": max,\n                ""min"": min,\n                ""range"": range,\n                },\n            }\n    else:\n        global_vars = globals()\n        imports = import_modules(source_code)\n        global_vars.update(imports)\n\n\n    execute_python_code(source_code, global_vars, local_vars)\n\n    return results\n\ndef _process_attributes(shape_attributes, label_attr_spec):\n    attributes = []\n    for attr_text, attr_value in shape_attributes.items():\n        if attr_text in label_attr_spec:\n            attributes.append({\n                ""spec_id"": label_attr_spec[attr_text],\n                ""value"": attr_value,\n            })\n\n    return attributes\n\nclass Results():\n    def __init__(self):\n        self._results = {\n            ""shapes"": [],\n            ""tracks"": []\n        }\n\n    # https://stackoverflow.com/a/50928627/2701402\n    def add_box(self, xtl: float, ytl: float, xbr: float, ybr: float, label: int, frame_number: int, attributes: dict=None):\n        """"""\n        xtl - x coordinate, top left\n        ytl - y coordinate, top left\n        xbr - x coordinate, bottom right\n        ybr - y coordinate, bottom right\n        """"""\n        self.get_shapes().append({\n            ""label"": label,\n            ""frame"": frame_number,\n            ""points"": [xtl, ytl, xbr, ybr],\n            ""type"": ""rectangle"",\n            ""attributes"": attributes or {},\n        })\n\n    def add_points(self, points: list, label: int, frame_number: int, attributes: dict=None):\n        points = self._create_polyshape(points, label, frame_number, attributes)\n        points[""type""] = ""points""\n        self.get_shapes().append(points)\n\n    def add_polygon(self, points: list, label: int, frame_number: int, attributes: dict=None):\n        polygon = self._create_polyshape(points, label, frame_number, attributes)\n        polygon[""type""] = ""polygon""\n        self.get_shapes().append(polygon)\n\n    def add_polyline(self, points: list, label: int, frame_number: int, attributes: dict=None):\n        polyline = self._create_polyshape(points, label, frame_number, attributes)\n        polyline[""type""] = ""polyline""\n        self.get_shapes().append(polyline)\n\n    def get_shapes(self):\n        return self._results[""shapes""]\n\n    def get_tracks(self):\n        return self._results[""tracks""]\n\n    @staticmethod\n    def _create_polyshape(points: list, label: int, frame_number: int, attributes: dict=None):\n        return {\n            ""label"": label,\n            ""frame"": frame_number,\n            ""points"": list(itertools.chain.from_iterable(points)),\n            ""attributes"": attributes or {},\n        }\n\nclass InferenceAnnotationRunner:\n    def __init__(self, data, model_file, weights_file, labels_mapping,\n    attribute_spec, convertation_file):\n        self.data = iter(data)\n        self.data_len = len(data)\n        self.model = ModelLoader(model=model_file, weights=weights_file)\n        self.frame_counter = 0\n        self.attribute_spec = attribute_spec\n        self.convertation_file = convertation_file\n        self.iteration_size = 128\n        self.labels_mapping = labels_mapping\n\n\n    def run(self, job=None, update_progress=None, restricted=True):\n        result = {\n            ""shapes"": [],\n            ""tracks"": [],\n            ""tags"": [],\n            ""version"": 0\n        }\n\n        detections = []\n        for _ in range(self.iteration_size):\n            try:\n                frame = next(self.data)\n            except StopIteration:\n                break\n\n            orig_rows, orig_cols = frame.shape[:2]\n\n            detections.append({\n                ""frame_id"": self.frame_counter,\n                ""frame_height"": orig_rows,\n                ""frame_width"": orig_cols,\n                ""detections"": self.model.infer(frame),\n            })\n\n            self.frame_counter += 1\n            if job and update_progress and not update_progress(job, self.frame_counter * 100 / self.data_len):\n                return None, False\n\n        processed_detections = _process_detections(detections, self.convertation_file, restricted=restricted)\n\n        self._add_shapes(processed_detections.get_shapes(), result[""shapes""])\n\n        more_items = self.frame_counter != self.data_len\n\n        return result, more_items\n\n    def _add_shapes(self, shapes, target_container):\n        for shape in shapes:\n            if shape[""label""] not in self.labels_mapping:\n                    continue\n\n            db_label = self.labels_mapping[shape[""label""]]\n            label_attr_spec = self.attribute_spec.get(db_label)\n            target_container.append({\n                ""label_id"": db_label,\n                ""frame"": shape[""frame""],\n                ""points"": shape[""points""],\n                ""type"": shape[""type""],\n                ""z_order"": 0,\n                ""group"": None,\n                ""occluded"": False,\n                ""attributes"": _process_attributes(shape[""attributes""], label_attr_spec),\n            })\n'"
cvat/apps/auto_annotation/inference_engine.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom openvino.inference_engine import IENetwork, IEPlugin, IECore, get_version\n\nimport subprocess\nimport os\nimport platform\n\n_IE_PLUGINS_PATH = os.getenv(""IE_PLUGINS_PATH"", None)\n\ndef _check_instruction(instruction):\n    return instruction == str.strip(\n        subprocess.check_output(\n            \'lscpu | grep -o ""{}"" | head -1\'.format(instruction), shell=True\n        ).decode(\'utf-8\')\n    )\n\n\ndef make_plugin_or_core():\n    version = get_version()\n    use_core_openvino = False\n    try:\n        major, minor, reference = [int(x) for x in version.split(\'.\')]\n        if major >= 2 and minor >= 1:\n            use_core_openvino = True\n    except Exception:\n        pass\n\n    if use_core_openvino:\n        ie = IECore()\n        return ie\n\n    if _IE_PLUGINS_PATH is None:\n        raise OSError(\'Inference engine plugin path env not found in the system.\')\n\n    plugin = IEPlugin(device=\'CPU\', plugin_dirs=[_IE_PLUGINS_PATH])\n    if (_check_instruction(\'avx2\')):\n        plugin.add_cpu_extension(os.path.join(_IE_PLUGINS_PATH, \'libcpu_extension_avx2.so\'))\n    elif (_check_instruction(\'sse4\')):\n        plugin.add_cpu_extension(os.path.join(_IE_PLUGINS_PATH, \'libcpu_extension_sse4.so\'))\n    elif platform.system() == \'Darwin\':\n        plugin.add_cpu_extension(os.path.join(_IE_PLUGINS_PATH, \'libcpu_extension.dylib\'))\n    else:\n        raise Exception(\'Inference engine requires a support of avx2 or sse4.\')\n\n    return plugin\n\n\ndef make_network(model, weights):\n    return IENetwork(model = model, weights = weights)\n'"
cvat/apps/auto_annotation/model_loader.py,0,"b'\n# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport json\nimport cv2\nimport os\nimport numpy as np\n\nfrom cvat.apps.auto_annotation.inference_engine import make_plugin_or_core, make_network\n\nclass ModelLoader():\n    def __init__(self, model, weights):\n        self._model = model\n        self._weights = weights\n\n        core_or_plugin = make_plugin_or_core()\n        network = make_network(self._model, self._weights)\n\n        if getattr(core_or_plugin, \'get_supported_layers\', False):\n            supported_layers = core_or_plugin.get_supported_layers(network)\n            not_supported_layers = [l for l in network.layers.keys() if l not in supported_layers]\n            if len(not_supported_layers) != 0:\n                raise Exception(""Following layers are not supported by the plugin for specified device {}:\\n {}"".\n                          format(core_or_plugin.device, "", "".join(not_supported_layers)))\n\n        iter_inputs = iter(network.inputs)\n        self._input_blob_name = next(iter_inputs)\n        self._input_info_name = \'\'\n        self._output_blob_name = next(iter(network.outputs))\n\n        self._require_image_info = False\n\n        info_names = (\'image_info\', \'im_info\')\n\n        # NOTE: handeling for the inclusion of `image_info` in OpenVino2019\n        if any(s in network.inputs for s in info_names):\n            self._require_image_info = True\n            self._input_info_name = set(network.inputs).intersection(info_names)\n            self._input_info_name = self._input_info_name.pop()\n        if self._input_blob_name in info_names:\n            self._input_blob_name = next(iter_inputs)\n\n        if getattr(core_or_plugin, \'load_network\', False):\n            self._net = core_or_plugin.load_network(network,\n                                                    ""CPU"",\n                                                    num_requests=2)\n        else:\n            self._net = core_or_plugin.load(network=network, num_requests=2)\n        input_type = network.inputs[self._input_blob_name]\n        self._input_layout = input_type if isinstance(input_type, list) else input_type.shape\n\n    def infer(self, image):\n        _, _, h, w = self._input_layout\n        in_frame = image if image.shape[:-1] == (h, w) else cv2.resize(image, (w, h))\n        in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n        inputs = {self._input_blob_name: in_frame}\n        if self._require_image_info:\n            info = np.zeros([1, 3])\n            info[0, 0] = h\n            info[0, 1] = w\n            # frame number\n            info[0, 2] = 1\n            inputs[self._input_info_name] = info\n\n        results = self._net.infer(inputs)\n        if len(results) == 1:\n            return results[self._output_blob_name].copy()\n        else:\n            return results.copy()\n\n\ndef load_labelmap(labels_path):\n    with open(labels_path, ""r"") as f:\n        return json.load(f)[""label_map""]\n'"
cvat/apps/auto_annotation/model_manager.py,0,"b'# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport django_rq\nimport numpy as np\nimport os\nimport rq\nimport shutil\nimport tempfile\n\nfrom django.db import transaction\nfrom django.utils import timezone\nfrom django.conf import settings\n\nfrom cvat.apps.engine.log import slogger\nfrom cvat.apps.engine.models import Task as TaskModel\nfrom cvat.apps.authentication.auth import has_admin_role\nfrom cvat.apps.engine.serializers import LabeledDataSerializer\nfrom cvat.apps.dataset_manager.task import put_task_data, patch_task_data\nfrom cvat.apps.engine.frame_provider import FrameProvider\n\nfrom .models import AnnotationModel, FrameworkChoice\nfrom .model_loader import load_labelmap\nfrom .image_loader import ImageLoader\nfrom .inference import InferenceAnnotationRunner\n\n\ndef _remove_old_file(model_file_field):\n    if model_file_field and os.path.exists(model_file_field.name):\n        os.remove(model_file_field.name)\n\ndef _update_dl_model_thread(dl_model_id, name, is_shared, model_file, weights_file, labelmap_file,\n        interpretation_file, run_tests, is_local_storage, delete_if_test_fails, restricted=True):\n    def _get_file_content(filename):\n        return os.path.basename(filename), open(filename, ""rb"")\n\n    def _delete_source_files():\n        for f in [model_file, weights_file, labelmap_file, interpretation_file]:\n            if f:\n                os.remove(f)\n\n    def _run_test(model_file, weights_file, labelmap_file, interpretation_file):\n        test_image = np.ones((1024, 1980, 3), np.uint8) * 255\n        try:\n            dummy_labelmap = {key: key for key in load_labelmap(labelmap_file).keys()}\n            runner = InferenceAnnotationRunner(\n                data=[test_image,],\n                model_file=model_file,\n                weights_file=weights_file,\n                labels_mapping=dummy_labelmap,\n                attribute_spec={},\n                convertation_file=interpretation_file)\n\n            runner.run(restricted=restricted)\n        except Exception as e:\n            return False, str(e)\n\n        return True, """"\n\n    job = rq.get_current_job()\n    job.meta[""progress""] = ""Saving data""\n    job.save_meta()\n\n    with transaction.atomic():\n        dl_model = AnnotationModel.objects.select_for_update().get(pk=dl_model_id)\n\n        test_res = True\n        message = """"\n        if run_tests:\n            job.meta[""progress""] = ""Test started""\n            job.save_meta()\n\n            test_res, message = _run_test(\n                model_file=model_file or dl_model.model_file.name,\n                weights_file=weights_file or dl_model.weights_file.name,\n                labelmap_file=labelmap_file or dl_model.labelmap_file.name,\n                interpretation_file=interpretation_file or dl_model.interpretation_file.name,\n            )\n\n            if not test_res:\n                job.meta[""progress""] = ""Test failed""\n                if delete_if_test_fails:\n                    shutil.rmtree(dl_model.get_dirname(), ignore_errors=True)\n                    dl_model.delete()\n            else:\n                job.meta[""progress""] = ""Test passed""\n            job.save_meta()\n\n        # update DL model\n        if test_res:\n            if model_file:\n                _remove_old_file(dl_model.model_file)\n                dl_model.model_file.save(*_get_file_content(model_file))\n            if weights_file:\n                _remove_old_file(dl_model.weights_file)\n                dl_model.weights_file.save(*_get_file_content(weights_file))\n            if labelmap_file:\n                _remove_old_file(dl_model.labelmap_file)\n                dl_model.labelmap_file.save(*_get_file_content(labelmap_file))\n            if interpretation_file:\n                _remove_old_file(dl_model.interpretation_file)\n                dl_model.interpretation_file.save(*_get_file_content(interpretation_file))\n\n            if name:\n                dl_model.name = name\n\n            if is_shared != None:\n                dl_model.shared = is_shared\n\n            dl_model.updated_date = timezone.now()\n            dl_model.save()\n\n    if is_local_storage:\n        _delete_source_files()\n\n    if not test_res:\n        raise Exception(""Model was not properly created/updated. Test failed: {}"".format(message))\n\ndef create_or_update(dl_model_id, name, model_file, weights_file, labelmap_file, interpretation_file, owner, storage, is_shared):\n    def get_abs_path(share_path):\n        if not share_path:\n            return share_path\n        share_root = settings.SHARE_ROOT\n        relpath = os.path.normpath(share_path).lstrip(\'/\')\n        if \'..\' in relpath.split(os.path.sep):\n            raise Exception(\'Permission denied\')\n        abspath = os.path.abspath(os.path.join(share_root, relpath))\n        if os.path.commonprefix([share_root, abspath]) != share_root:\n            raise Exception(\'Bad file path on share: \' + abspath)\n        return abspath\n\n    def save_file_as_tmp(data):\n        if not data:\n            return None\n        fd, filename = tempfile.mkstemp()\n        with open(filename, \'wb\') as tmp_file:\n            for chunk in data.chunks():\n                tmp_file.write(chunk)\n        os.close(fd)\n        return filename\n    is_create_request = dl_model_id is None\n    if is_create_request:\n        dl_model_id = create_empty(owner=owner)\n\n    run_tests = bool(model_file or weights_file or labelmap_file or interpretation_file)\n    if storage != ""local"":\n        model_file = get_abs_path(model_file)\n        weights_file = get_abs_path(weights_file)\n        labelmap_file = get_abs_path(labelmap_file)\n        interpretation_file = get_abs_path(interpretation_file)\n    else:\n        model_file = save_file_as_tmp(model_file)\n        weights_file = save_file_as_tmp(weights_file)\n        labelmap_file = save_file_as_tmp(labelmap_file)\n        interpretation_file = save_file_as_tmp(interpretation_file)\n\n    if owner:\n        restricted = not has_admin_role(owner)\n    else:\n        restricted = not has_admin_role(AnnotationModel.objects.get(pk=dl_model_id).owner)\n\n    rq_id = ""auto_annotation.create.{}"".format(dl_model_id)\n    queue = django_rq.get_queue(""default"")\n    queue.enqueue_call(\n        func=_update_dl_model_thread,\n        args=(\n            dl_model_id,\n            name,\n            is_shared,\n            model_file,\n            weights_file,\n            labelmap_file,\n            interpretation_file,\n            run_tests,\n            storage == ""local"",\n            is_create_request,\n            restricted\n        ),\n        job_id=rq_id\n    )\n\n    return rq_id\n\n@transaction.atomic\ndef create_empty(owner, framework=FrameworkChoice.OPENVINO):\n    db_model = AnnotationModel(\n        owner=owner,\n    )\n    db_model.save()\n\n    model_path = db_model.get_dirname()\n    if os.path.isdir(model_path):\n        shutil.rmtree(model_path)\n    os.mkdir(model_path)\n\n    return db_model.id\n\n@transaction.atomic\ndef delete(dl_model_id):\n    dl_model = AnnotationModel.objects.select_for_update().get(pk=dl_model_id)\n    if dl_model:\n        if dl_model.primary:\n            raise Exception(""Can not delete primary model {}"".format(dl_model_id))\n\n        shutil.rmtree(dl_model.get_dirname(), ignore_errors=True)\n        dl_model.delete()\n    else:\n        raise Exception(""Requested DL model {} doesn\'t exist"".format(dl_model_id))\n\ndef run_inference_thread(tid, model_file, weights_file, labels_mapping, attributes, convertation_file, reset, user, restricted=True):\n    def update_progress(job, progress):\n        job.refresh()\n        if ""cancel"" in job.meta:\n            del job.meta[""cancel""]\n            job.save()\n            return False\n        job.meta[""progress""] = progress\n        job.save_meta()\n        return True\n\n    try:\n        job = rq.get_current_job()\n        job.meta[""progress""] = 0\n        job.save_meta()\n        db_task = TaskModel.objects.get(pk=tid)\n\n        result = None\n        slogger.glob.info(""auto annotation with openvino toolkit for task {}"".format(tid))\n        more_data = True\n        runner = InferenceAnnotationRunner(\n            data=ImageLoader(FrameProvider(db_task.data)),\n            model_file=model_file,\n            weights_file=weights_file,\n            labels_mapping=labels_mapping,\n            attribute_spec=attributes,\n            convertation_file= convertation_file)\n        while more_data:\n            result, more_data = runner.run(\n                job=job,\n                update_progress=update_progress,\n                restricted=restricted)\n\n            if result is None:\n                slogger.glob.info(""auto annotation for task {} canceled by user"".format(tid))\n                return\n\n            serializer = LabeledDataSerializer(data = result)\n            if serializer.is_valid(raise_exception=True):\n                if reset:\n                    put_task_data(tid, result)\n                else:\n                    patch_task_data(tid, result, ""create"")\n\n            slogger.glob.info(""auto annotation for task {} done"".format(tid))\n    except Exception as e:\n        try:\n            slogger.task[tid].exception(""exception was occurred during auto annotation of the task"", exc_info=True)\n        except Exception as ex:\n            slogger.glob.exception(""exception was occurred during auto annotation of the task {}: {}"".format(tid, str(ex)), exc_info=True)\n            raise ex\n\n        raise e\n'"
cvat/apps/auto_annotation/models.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nfrom enum import Enum\n\nfrom django.db import models\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\nfrom django.core.files.storage import FileSystemStorage\n\nfs = FileSystemStorage()\n\ndef upload_path_handler(instance, filename):\n    return os.path.join(settings.MODELS_ROOT, str(instance.id), filename)\n\nclass FrameworkChoice(Enum):\n    OPENVINO = \'openvino\'\n    TENSORFLOW = \'tensorflow\'\n    PYTORCH = \'pytorch\'\n\n    def __str__(self):\n        return self.value\n\n\nclass SafeCharField(models.CharField):\n    def get_prep_value(self, value):\n        value = super().get_prep_value(value)\n        if value:\n            return value[:self.max_length]\n        return value\n\nclass AnnotationModel(models.Model):\n    name = SafeCharField(max_length=256)\n    owner = models.ForeignKey(User, null=True, blank=True,\n        on_delete=models.SET_NULL)\n    created_date = models.DateTimeField(auto_now_add=True)\n    updated_date = models.DateTimeField(auto_now_add=True)\n    model_file = models.FileField(upload_to=upload_path_handler, storage=fs)\n    weights_file = models.FileField(upload_to=upload_path_handler, storage=fs)\n    labelmap_file = models.FileField(upload_to=upload_path_handler, storage=fs)\n    interpretation_file = models.FileField(upload_to=upload_path_handler, storage=fs)\n    shared = models.BooleanField(default=False)\n    primary = models.BooleanField(default=False)\n    framework = models.CharField(max_length=32, default=FrameworkChoice.OPENVINO)\n\n    class Meta:\n        default_permissions = ()\n\n    def get_dirname(self):\n        return ""{models_root}/{id}"".format(models_root=settings.MODELS_ROOT, id=self.id)\n\n    def __str__(self):\n        return self.name\n'"
cvat/apps/auto_annotation/permissions.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport rules\n\nfrom cvat.apps.authentication.auth import has_admin_role, has_user_role\n\n@rules.predicate\ndef is_model_owner(db_user, db_dl_model):\n    return db_dl_model.owner == db_user\n\n@rules.predicate\ndef is_shared_model(_, db_dl_model):\n    return db_dl_model.shared\n\n@rules.predicate\ndef is_primary_model(_, db_dl_model):\n    return db_dl_model.primary\n\ndef setup_permissions():\n    rules.add_perm('auto_annotation.model.create', has_admin_role | has_user_role)\n\n    rules.add_perm('auto_annotation.model.update', (has_admin_role | is_model_owner) & ~is_primary_model)\n\n    rules.add_perm('auto_annotation.model.delete', (has_admin_role | is_model_owner) & ~is_primary_model)\n\n    rules.add_perm('auto_annotation.model.access', has_admin_role | is_model_owner |\n        is_shared_model | is_primary_model)\n"""
cvat/apps/auto_annotation/tests.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/auto_annotation/urls.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path(""create"", views.create_model),\n    path(""update/<int:mid>"", views.update_model),\n    path(""delete/<int:mid>"", views.delete_model),\n\n    path(""start/<int:mid>/<int:tid>"", views.start_annotation),\n    path(""check/<str:rq_id>"", views.check),\n    path(""cancel/<int:tid>"", views.cancel),\n\n    path(""meta/get"", views.get_meta_info),\n]\n'"
cvat/apps/auto_annotation/views.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport django_rq\nimport json\nimport os\n\nfrom django.http import HttpResponse, JsonResponse, HttpResponseBadRequest\nfrom rest_framework.decorators import api_view\nfrom django.db.models import Q\nfrom rules.contrib.views import permission_required, objectgetter\n\nfrom cvat.apps.authentication.decorators import login_required\nfrom cvat.apps.engine.models import Task as TaskModel\nfrom cvat.apps.authentication.auth import has_admin_role\nfrom cvat.apps.engine.log import slogger\n\nfrom .model_loader import load_labelmap\nfrom . import model_manager\nfrom .models import AnnotationModel\n\n@login_required\n@permission_required(perm=[""engine.task.change""],\n    fn=objectgetter(TaskModel, ""tid""), raise_exception=True)\ndef cancel(request, tid):\n    try:\n        queue = django_rq.get_queue(""low"")\n        job = queue.fetch_job(""auto_annotation.run.{}"".format(tid))\n        if job is None or job.is_finished or job.is_failed:\n            raise Exception(""Task is not being annotated currently"")\n        elif ""cancel"" not in job.meta:\n            job.meta[""cancel""] = True\n            job.save()\n\n    except Exception as ex:\n        try:\n            slogger.task[tid].exception(""cannot cancel auto annotation for task #{}"".format(tid), exc_info=True)\n        except Exception as logger_ex:\n            slogger.glob.exception(""exception was occured during cancel auto annotation request for task {}: {}"".format(tid, str(logger_ex)), exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\n    return HttpResponse()\n\n@login_required\n@permission_required(perm=[""auto_annotation.model.create""], raise_exception=True)\ndef create_model(request):\n    if request.method != \'POST\':\n        return HttpResponseBadRequest(""Only POST requests are accepted"")\n\n    try:\n        params = request.POST\n        storage = params[""storage""]\n        name = params[""name""]\n        is_shared = params[""shared""].lower() == ""true""\n        if is_shared and not has_admin_role(request.user):\n            raise Exception(""Only admin can create shared models"")\n\n        files = request.FILES if storage == ""local"" else params\n        model = files[""xml""]\n        weights = files[""bin""]\n        labelmap = files[""json""]\n        interpretation_script = files[""py""]\n        owner = request.user\n\n        rq_id = model_manager.create_or_update(\n            dl_model_id=None,\n            name=name,\n            model_file=model,\n            weights_file=weights,\n            labelmap_file=labelmap,\n            interpretation_file=interpretation_script,\n            owner=owner,\n            storage=storage,\n            is_shared=is_shared,\n        )\n\n        return JsonResponse({""id"": rq_id})\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n\n@login_required\n@permission_required(perm=[""auto_annotation.model.update""],\n    fn=objectgetter(AnnotationModel, ""mid""), raise_exception=True)\ndef update_model(request, mid):\n    if request.method != \'POST\':\n        return HttpResponseBadRequest(""Only POST requests are accepted"")\n\n    try:\n        params = request.POST\n        storage = params[""storage""]\n        name = params.get(""name"")\n        is_shared = params.get(""shared"")\n        is_shared = is_shared.lower() == ""true"" if is_shared else None\n        if is_shared and not has_admin_role(request.user):\n            raise Exception(""Only admin can create shared models"")\n        files = request.FILES\n        model = files.get(""xml"")\n        weights = files.get(""bin"")\n        labelmap = files.get(""json"")\n        interpretation_script = files.get(""py"")\n\n        rq_id = model_manager.create_or_update(\n            dl_model_id=mid,\n            name=name,\n            model_file=model,\n            weights_file=weights,\n            labelmap_file=labelmap,\n            interpretation_file=interpretation_script,\n            owner=None,\n            storage=storage,\n            is_shared=is_shared,\n        )\n\n        return JsonResponse({""id"": rq_id})\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n\n@login_required\n@permission_required(perm=[""auto_annotation.model.delete""],\n    fn=objectgetter(AnnotationModel, ""mid""), raise_exception=True)\ndef delete_model(request, mid):\n    if request.method != \'DELETE\':\n        return HttpResponseBadRequest(""Only DELETE requests are accepted"")\n    model_manager.delete(mid)\n    return HttpResponse()\n\n@api_view([\'POST\'])\n@login_required\ndef get_meta_info(request):\n    try:\n        tids = request.data\n        response = {\n            ""admin"": has_admin_role(request.user),\n            ""models"": [],\n            ""run"": {},\n        }\n        dl_model_list = list(AnnotationModel.objects.filter(Q(owner=request.user) | Q(primary=True) | Q(shared=True)).order_by(\'-created_date\'))\n        for dl_model in dl_model_list:\n            labels = []\n            if dl_model.labelmap_file and os.path.exists(dl_model.labelmap_file.name):\n                with dl_model.labelmap_file.open(\'r\') as f:\n                    labels = list(json.load(f)[""label_map""].values())\n\n            response[""models""].append({\n                ""id"": dl_model.id,\n                ""name"": dl_model.name,\n                ""primary"": dl_model.primary,\n                ""uploadDate"": dl_model.created_date,\n                ""updateDate"": dl_model.updated_date,\n                ""labels"": labels,\n                ""owner"": dl_model.owner.id,\n            })\n\n        queue = django_rq.get_queue(""low"")\n        for tid in tids:\n            rq_id = ""auto_annotation.run.{}"".format(tid)\n            job = queue.fetch_job(rq_id)\n            if job is not None:\n                response[""run""][tid] = {\n                    ""status"": job.get_status(),\n                    ""rq_id"": rq_id,\n                }\n\n        return JsonResponse(response)\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n\n@login_required\n@permission_required(perm=[""engine.task.change""],\n    fn=objectgetter(TaskModel, ""tid""), raise_exception=True)\n@permission_required(perm=[""auto_annotation.model.access""],\n    fn=objectgetter(AnnotationModel, ""mid""), raise_exception=True)\ndef start_annotation(request, mid, tid):\n    slogger.glob.info(""auto annotation create request for task {} via DL model {}"".format(tid, mid))\n    try:\n        db_task = TaskModel.objects.get(pk=tid)\n        queue = django_rq.get_queue(""low"")\n        job = queue.fetch_job(""auto_annotation.run.{}"".format(tid))\n        if job is not None and (job.is_started or job.is_queued):\n            raise Exception(""The process is already running"")\n\n        data = json.loads(request.body.decode(\'utf-8\'))\n\n        should_reset = data[""reset""]\n        user_defined_labels_mapping = data[""labels""]\n\n        dl_model = AnnotationModel.objects.get(pk=mid)\n\n        model_file_path = dl_model.model_file.name\n        weights_file_path = dl_model.weights_file.name\n        labelmap_file = dl_model.labelmap_file.name\n        convertation_file_path = dl_model.interpretation_file.name\n        restricted = not has_admin_role(dl_model.owner)\n\n        db_labels = db_task.label_set.prefetch_related(""attributespec_set"").all()\n        db_attributes = {db_label.id:\n            {db_attr.name: db_attr.id for db_attr in db_label.attributespec_set.all()} for db_label in db_labels}\n        db_labels = {db_label.name:db_label.id for db_label in db_labels}\n\n        model_labels = {value: key for key, value in load_labelmap(labelmap_file).items()}\n\n        labels_mapping = {}\n        for user_model_label, user_db_label in user_defined_labels_mapping.items():\n            if user_model_label in model_labels and user_db_label in db_labels:\n                labels_mapping[int(model_labels[user_model_label])] = db_labels[user_db_label]\n\n        if not labels_mapping:\n            raise Exception(""No labels found for annotation"")\n\n        rq_id=""auto_annotation.run.{}"".format(tid)\n        queue.enqueue_call(func=model_manager.run_inference_thread,\n            args=(\n                tid,\n                model_file_path,\n                weights_file_path,\n                labels_mapping,\n                db_attributes,\n                convertation_file_path,\n                should_reset,\n                request.user,\n                restricted,\n            ),\n            job_id = rq_id,\n            timeout=604800)     # 7 days\n\n        slogger.task[tid].info(""auto annotation job enqueued"")\n\n    except Exception as ex:\n        try:\n            slogger.task[tid].exception(""exception was occurred during annotation request"", exc_info=True)\n        except Exception as logger_ex:\n            slogger.glob.exception(""exception was occurred during create auto annotation request for task {}: {}"".format(tid, str(logger_ex)), exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\n    return JsonResponse({""id"": rq_id})\n\n@login_required\ndef check(request, rq_id):\n    try:\n        target_queue = ""low"" if ""auto_annotation.run"" in rq_id else ""default""\n        queue = django_rq.get_queue(target_queue)\n        job = queue.fetch_job(rq_id)\n        if job is not None and ""cancel"" in job.meta:\n            return JsonResponse({""status"": ""finished""})\n        data = {}\n        if job is None:\n            data[""status""] = ""unknown""\n        elif job.is_queued:\n            data[""status""] = ""queued""\n        elif job.is_started:\n            data[""status""] = ""started""\n            data[""progress""] = job.meta[""progress""] if ""progress"" in job.meta else """"\n        elif job.is_finished:\n            data[""status""] = ""finished""\n            job.delete()\n        else:\n            data[""status""] = ""failed""\n            data[""error""] = job.exc_info\n            job.delete()\n\n    except Exception:\n        data[""status""] = ""unknown""\n\n    return JsonResponse(data)\n'"
cvat/apps/auto_segmentation/__init__.py,0,b'\n# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/auto_segmentation/admin.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\n# Register your models here.\n\n'
cvat/apps/auto_segmentation/apps.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\n\nclass AutoSegmentationConfig(AppConfig):\n    name = 'auto_segmentation'\n\n"""
cvat/apps/auto_segmentation/models.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\n# Create your models here.\n\n'
cvat/apps/auto_segmentation/tests.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\n# Create your tests here.\n\n'
cvat/apps/auto_segmentation/urls.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('create/task/<int:tid>', views.create),\n    path('check/task/<int:tid>', views.check),\n    path('cancel/task/<int:tid>', views.cancel),\n    path('meta/get', views.get_meta_info),\n]\n"""
cvat/apps/auto_segmentation/views.py,2,"b'\n# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\nfrom django.http import HttpResponse, JsonResponse, HttpResponseBadRequest\nfrom rest_framework.decorators import api_view\nfrom rules.contrib.views import permission_required, objectgetter\nfrom cvat.apps.authentication.decorators import login_required\nfrom cvat.apps.dataset_manager.task import put_task_data\nfrom cvat.apps.engine.models import Task as TaskModel\nfrom cvat.apps.engine.serializers import LabeledDataSerializer\nfrom cvat.apps.engine.frame_provider import FrameProvider\n\nimport django_rq\nimport os\nimport rq\n\nimport numpy as np\n\nfrom cvat.apps.engine.log import slogger\n\nimport sys\nimport skimage.io\nfrom skimage.measure import find_contours, approximate_polygon\n\ndef run_tensorflow_auto_segmentation(frame_provider, labels_mapping, treshold):\n    def _convert_to_int(boolean_mask):\n        return boolean_mask.astype(np.uint8)\n\n    def _convert_to_segmentation(mask):\n        contours = find_contours(mask, 0.5)\n        # only one contour exist in our case\n        contour = contours[0]\n        contour = np.flip(contour, axis=1)\n        # Approximate the contour and reduce the number of points\n        contour = approximate_polygon(contour, tolerance=2.5)\n        segmentation = contour.ravel().tolist()\n        return segmentation\n\n    ## INITIALIZATION\n\n    # workarround for tf.placeholder() is not compatible with eager execution\n    # https://github.com/tensorflow/tensorflow/issues/18165\n    import tensorflow as tf\n    tf.compat.v1.disable_eager_execution()\n\n    # Root directory of the project\n    ROOT_DIR = os.environ.get(\'AUTO_SEGMENTATION_PATH\')\n    # Import Mask RCNN\n    sys.path.append(ROOT_DIR)  # To find local version of the library\n    import mrcnn.model as modellib\n\n    # Import COCO config\n    sys.path.append(os.path.join(ROOT_DIR, ""samples/coco/""))  # To find local version\n    import coco\n\n    # Directory to save logs and trained model\n    MODEL_DIR = os.path.join(ROOT_DIR, ""logs"")\n\n    # Local path to trained weights file\n    COCO_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")\n    if COCO_MODEL_PATH is None:\n        raise OSError(\'Model path env not found in the system.\')\n    job = rq.get_current_job()\n\n    ## CONFIGURATION\n\n    class InferenceConfig(coco.CocoConfig):\n        # Set batch size to 1 since we\'ll be running inference on\n        # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n        GPU_COUNT = 1\n        IMAGES_PER_GPU = 1\n\n    # Print config details\n    config = InferenceConfig()\n    config.display()\n\n    ## CREATE MODEL AND LOAD TRAINED WEIGHTS\n\n    # Create model object in inference mode.\n    model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)\n    # Load weights trained on MS-COCO\n    model.load_weights(COCO_MODEL_PATH, by_name=True)\n\n    ## RUN OBJECT DETECTION\n    result = {}\n    frames = frame_provider.get_frames(frame_provider.Quality.ORIGINAL)\n    for image_num, (image_bytes, _) in enumerate(frames):\n        job.refresh()\n        if \'cancel\' in job.meta:\n            del job.meta[\'cancel\']\n            job.save()\n            return None\n        job.meta[\'progress\'] = image_num * 100 / len(frame_provider)\n        job.save_meta()\n\n        image = skimage.io.imread(image_bytes)\n\n        # for multiple image detection, ""batch size"" must be equal to number of images\n        r = model.detect([image], verbose=1)\n\n        r = r[0]\n        # ""r[\'rois\'][index]"" gives bounding box around the object\n        for index, c_id in enumerate(r[\'class_ids\']):\n            if c_id in labels_mapping.keys():\n                if r[\'scores\'][index] >= treshold:\n                    mask = _convert_to_int(r[\'masks\'][:,:,index])\n                    segmentation = _convert_to_segmentation(mask)\n                    label = labels_mapping[c_id]\n                    if label not in result:\n                        result[label] = []\n                    result[label].append(\n                        [image_num, segmentation])\n\n    return result\n\ndef convert_to_cvat_format(data):\n    result = {\n        ""tracks"": [],\n        ""shapes"": [],\n        ""tags"": [],\n        ""version"": 0,\n    }\n\n    for label in data:\n        segments = data[label]\n        for segment in segments:\n            result[\'shapes\'].append({\n                ""type"": ""polygon"",\n                ""label_id"": label,\n                ""frame"": segment[0],\n                ""points"": segment[1],\n                ""z_order"": 0,\n                ""group"": None,\n                ""occluded"": False,\n                ""attributes"": [],\n            })\n\n    return result\n\ndef create_thread(tid, labels_mapping, user):\n    try:\n        # If detected object accuracy bigger than threshold it will returend\n        TRESHOLD = 0.5\n        # Init rq job\n        job = rq.get_current_job()\n        job.meta[\'progress\'] = 0\n        job.save_meta()\n        # Get job indexes and segment length\n        db_task = TaskModel.objects.get(pk=tid)\n        # Get image list\n        frame_provider = FrameProvider(db_task.data)\n\n        # Run auto segmentation by tf\n        result = None\n        slogger.glob.info(""auto segmentation with tensorflow framework for task {}"".format(tid))\n        result = run_tensorflow_auto_segmentation(frame_provider, labels_mapping, TRESHOLD)\n\n        if result is None:\n            slogger.glob.info(\'auto segmentation for task {} canceled by user\'.format(tid))\n            return\n\n        # Modify data format and save\n        result = convert_to_cvat_format(result)\n        serializer = LabeledDataSerializer(data = result)\n        if serializer.is_valid(raise_exception=True):\n            put_task_data(tid, result)\n        slogger.glob.info(\'auto segmentation for task {} done\'.format(tid))\n    except Exception as ex:\n        try:\n            slogger.task[tid].exception(\'exception was occured during auto segmentation of the task\', exc_info=True)\n        except Exception:\n            slogger.glob.exception(\'exception was occured during auto segmentation of the task {}\'.format(tid), exc_info=True)\n        raise ex\n\n@api_view([\'POST\'])\n@login_required\ndef get_meta_info(request):\n    try:\n        queue = django_rq.get_queue(\'low\')\n        tids = request.data\n        result = {}\n        for tid in tids:\n            job = queue.fetch_job(\'auto_segmentation.create/{}\'.format(tid))\n            if job is not None:\n                result[tid] = {\n                    ""active"": job.is_queued or job.is_started,\n                    ""success"": not job.is_failed\n                }\n\n        return JsonResponse(result)\n    except Exception as ex:\n        slogger.glob.exception(\'exception was occured during tf meta request\', exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\n\n@login_required\n@permission_required(perm=[\'engine.task.change\'],\n    fn=objectgetter(TaskModel, \'tid\'), raise_exception=True)\ndef create(request, tid):\n    slogger.glob.info(\'auto segmentation create request for task {}\'.format(tid))\n    try:\n        db_task = TaskModel.objects.get(pk=tid)\n        queue = django_rq.get_queue(\'low\')\n        job = queue.fetch_job(\'auto_segmentation.create/{}\'.format(tid))\n        if job is not None and (job.is_started or job.is_queued):\n            raise Exception(""The process is already running"")\n\n        db_labels = db_task.label_set.prefetch_related(\'attributespec_set\').all()\n        db_labels = {db_label.id:db_label.name for db_label in db_labels}\n\n        # COCO Labels\n        auto_segmentation_labels = { ""BG"": 0,\n            ""person"": 1, ""bicycle"": 2, ""car"": 3, ""motorcycle"": 4, ""airplane"": 5,\n            ""bus"": 6, ""train"": 7, ""truck"": 8, ""boat"": 9, ""traffic_light"": 10,\n            ""fire_hydrant"": 11, ""stop_sign"": 12, ""parking_meter"": 13, ""bench"": 14,\n            ""bird"": 15, ""cat"": 16, ""dog"": 17, ""horse"": 18, ""sheep"": 19, ""cow"": 20,\n            ""elephant"": 21, ""bear"": 22, ""zebra"": 23, ""giraffe"": 24, ""backpack"": 25,\n            ""umbrella"": 26, ""handbag"": 27, ""tie"": 28, ""suitcase"": 29, ""frisbee"": 30,\n            ""skis"": 31, ""snowboard"": 32, ""sports_ball"": 33, ""kite"": 34, ""baseball_bat"": 35,\n            ""baseball_glove"": 36, ""skateboard"": 37, ""surfboard"": 38, ""tennis_racket"": 39,\n            ""bottle"": 40, ""wine_glass"": 41, ""cup"": 42, ""fork"": 43, ""knife"": 44, ""spoon"": 45,\n            ""bowl"": 46, ""banana"": 47, ""apple"": 48, ""sandwich"": 49, ""orange"": 50, ""broccoli"": 51,\n            ""carrot"": 52, ""hot_dog"": 53, ""pizza"": 54, ""donut"": 55, ""cake"": 56, ""chair"": 57,\n            ""couch"": 58, ""potted_plant"": 59, ""bed"": 60, ""dining_table"": 61, ""toilet"": 62,\n            ""tv"": 63, ""laptop"": 64, ""mouse"": 65, ""remote"": 66, ""keyboard"": 67, ""cell_phone"": 68,\n            ""microwave"": 69, ""oven"": 70, ""toaster"": 71, ""sink"": 72, ""refrigerator"": 73,\n            ""book"": 74, ""clock"": 75, ""vase"": 76, ""scissors"": 77, ""teddy_bear"": 78, ""hair_drier"": 79,\n            ""toothbrush"": 80\n            }\n\n        labels_mapping = {}\n        for key, labels in db_labels.items():\n            if labels in auto_segmentation_labels.keys():\n                labels_mapping[auto_segmentation_labels[labels]] = key\n\n        if not len(labels_mapping.values()):\n            raise Exception(\'No labels found for auto segmentation\')\n\n        # Run auto segmentation job\n        queue.enqueue_call(func=create_thread,\n            args=(tid, labels_mapping, request.user),\n            job_id=\'auto_segmentation.create/{}\'.format(tid),\n            timeout=604800)     # 7 days\n\n        slogger.task[tid].info(\'tensorflow segmentation job enqueued with labels {}\'.format(labels_mapping))\n\n    except Exception as ex:\n        try:\n            slogger.task[tid].exception(""exception was occured during tensorflow segmentation request"", exc_info=True)\n        except Exception:\n            pass\n        return HttpResponseBadRequest(str(ex))\n\n    return HttpResponse()\n\n@login_required\n@permission_required(perm=[\'engine.task.access\'],\n    fn=objectgetter(TaskModel, \'tid\'), raise_exception=True)\ndef check(request, tid):\n    try:\n        queue = django_rq.get_queue(\'low\')\n        job = queue.fetch_job(\'auto_segmentation.create/{}\'.format(tid))\n        if job is not None and \'cancel\' in job.meta:\n            return JsonResponse({\'status\': \'finished\'})\n        data = {}\n        if job is None:\n            data[\'status\'] = \'unknown\'\n        elif job.is_queued:\n            data[\'status\'] = \'queued\'\n        elif job.is_started:\n            data[\'status\'] = \'started\'\n            data[\'progress\'] = job.meta[\'progress\']\n        elif job.is_finished:\n            data[\'status\'] = \'finished\'\n            job.delete()\n        else:\n            data[\'status\'] = \'failed\'\n            data[\'stderr\'] = job.exc_info\n            job.delete()\n\n    except Exception:\n        data[\'status\'] = \'unknown\'\n\n    return JsonResponse(data)\n\n\n@login_required\n@permission_required(perm=[\'engine.task.change\'],\n    fn=objectgetter(TaskModel, \'tid\'), raise_exception=True)\ndef cancel(request, tid):\n    try:\n        queue = django_rq.get_queue(\'low\')\n        job = queue.fetch_job(\'auto_segmentation.create/{}\'.format(tid))\n        if job is None or job.is_finished or job.is_failed:\n            raise Exception(\'Task is not being segmented currently\')\n        elif \'cancel\' not in job.meta:\n            job.meta[\'cancel\'] = True\n            job.save()\n\n    except Exception as ex:\n        try:\n            slogger.task[tid].exception(""cannot cancel tensorflow segmentation for task #{}"".format(tid), exc_info=True)\n        except Exception:\n            pass\n        return HttpResponseBadRequest(str(ex))\n\n    return HttpResponse()\n'"
cvat/apps/dataset_manager/annotation.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom copy import copy, deepcopy\n\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom shapely import geometry\n\nfrom cvat.apps.engine.models import ShapeType\nfrom cvat.apps.engine.serializers import LabeledDataSerializer\n\n\nclass AnnotationIR:\n    def __init__(self, data=None):\n        self.reset()\n        if data:\n            self.tags = getattr(data, \'tags\', []) or data[\'tags\']\n            self.shapes = getattr(data, \'shapes\', []) or data[\'shapes\']\n            self.tracks = getattr(data, \'tracks\', []) or data[\'tracks\']\n\n    def add_tag(self, tag):\n        self.tags.append(tag)\n\n    def add_shape(self, shape):\n        self.shapes.append(shape)\n\n    def add_track(self, track):\n        self.tracks.append(track)\n\n    @property\n    def data(self):\n        return {\n            \'version\': self.version,\n            \'tags\': self.tags,\n            \'shapes\': self.shapes,\n            \'tracks\': self.tracks,\n        }\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n\n    @data.setter\n    def data(self, data):\n        self.version = data[\'version\']\n        self.tags = data[\'tags\']\n        self.shapes = data[\'shapes\']\n        self.tracks = data[\'tracks\']\n\n    def serialize(self):\n        serializer = LabeledDataSerializer(data=self.data)\n        if serializer.is_valid(raise_exception=True):\n            return serializer.data\n\n    @staticmethod\n    def _is_shape_inside(shape, start, stop):\n        return start <= int(shape[\'frame\']) <= stop\n\n    @staticmethod\n    def _is_track_inside(track, start, stop):\n        def has_overlap(a, b):\n            # a <= b\n            return 0 <= min(b, stop) - max(a, start)\n\n        prev_shape = None\n        for shape in track[\'shapes\']:\n            if prev_shape and not prev_shape[\'outside\'] and \\\n                has_overlap(prev_shape[\'frame\'], shape[\'frame\']):\n                    return True\n            prev_shape = shape\n\n        if not prev_shape[\'outside\'] and prev_shape[\'frame\'] <= stop:\n            return True\n\n        return False\n\n    @classmethod\n    def _slice_track(cls, track_, start, stop):\n        def filter_track_shapes(shapes):\n            shapes = [s for s in shapes if cls._is_shape_inside(s, start, stop)]\n            drop_count = 0\n            for s in shapes:\n                if s[\'outside\']:\n                    drop_count += 1\n                else:\n                    break\n            # Need to leave the last shape if all shapes are outside\n            if drop_count == len(shapes):\n                drop_count -= 1\n\n            return shapes[drop_count:]\n\n        track = deepcopy(track_)\n        segment_shapes = filter_track_shapes(track[\'shapes\'])\n\n        if len(segment_shapes) < len(track[\'shapes\']):\n            interpolated_shapes = TrackManager.get_interpolated_shapes(\n                track, start, stop)\n            scoped_shapes = filter_track_shapes(interpolated_shapes)\n\n            if scoped_shapes:\n                if not scoped_shapes[0][\'keyframe\']:\n                    segment_shapes.insert(0, scoped_shapes[0])\n                if not scoped_shapes[-1][\'keyframe\']:\n                    segment_shapes.append(scoped_shapes[-1])\n\n            # Should delete \'interpolation_shapes\' and \'keyframe\' keys because\n            # Track and TrackedShape models don\'t expect these fields\n            del track[\'interpolated_shapes\']\n            for shape in segment_shapes:\n                del shape[\'keyframe\']\n\n        track[\'shapes\'] = segment_shapes\n        track[\'frame\'] = track[\'shapes\'][0][\'frame\']\n        return track\n\n    def slice(self, start, stop):\n        #makes a data copy from specified frame interval\n        splitted_data = AnnotationIR()\n        splitted_data.tags = [deepcopy(t)\n            for t in self.tags if self._is_shape_inside(t, start, stop)]\n        splitted_data.shapes = [deepcopy(s)\n            for s in self.shapes if self._is_shape_inside(s, start, stop)]\n        splitted_data.tracks = [self._slice_track(t, start, stop)\n            for t in self.tracks if self._is_track_inside(t, start, stop)]\n\n        return splitted_data\n\n    def reset(self):\n        self.version = 0\n        self.tags = []\n        self.shapes = []\n        self.tracks = []\n\nclass AnnotationManager:\n    def __init__(self, data):\n        self.data = data\n\n    def merge(self, data, start_frame, overlap):\n        tags = TagManager(self.data.tags)\n        tags.merge(data.tags, start_frame, overlap)\n\n        shapes = ShapeManager(self.data.shapes)\n        shapes.merge(data.shapes, start_frame, overlap)\n\n        tracks = TrackManager(self.data.tracks)\n        tracks.merge(data.tracks, start_frame, overlap)\n\n    def to_shapes(self, end_frame):\n        shapes = self.data.shapes\n        tracks = TrackManager(self.data.tracks)\n\n        return shapes + tracks.to_shapes(end_frame)\n\n    def to_tracks(self):\n        tracks = self.data.tracks\n        shapes = ShapeManager(self.data.shapes)\n\n        return tracks + shapes.to_tracks()\n\nclass ObjectManager:\n    def __init__(self, objects):\n        self.objects = objects\n\n    @staticmethod\n    def _get_objects_by_frame(objects, start_frame):\n        objects_by_frame = {}\n        for obj in objects:\n            if obj[""frame""] >= start_frame:\n                if obj[""frame""] in objects_by_frame:\n                    objects_by_frame[obj[""frame""]].append(obj)\n                else:\n                    objects_by_frame[obj[""frame""]] = [obj]\n\n        return objects_by_frame\n\n    @staticmethod\n    def _get_cost_threshold():\n        raise NotImplementedError()\n\n    @staticmethod\n    def _calc_objects_similarity(obj0, obj1, start_frame, overlap):\n        raise NotImplementedError()\n\n    @staticmethod\n    def _unite_objects(obj0, obj1):\n        raise NotImplementedError()\n\n    @staticmethod\n    def _modify_unmached_object(obj, end_frame):\n        raise NotImplementedError()\n\n    def merge(self, objects, start_frame, overlap):\n        # 1. Split objects on two parts: new and which can be intersected\n        # with existing objects.\n        new_objects = [obj for obj in objects\n            if obj[""frame""] >= start_frame + overlap]\n        int_objects = [obj for obj in objects\n            if obj[""frame""] < start_frame + overlap]\n        assert len(new_objects) + len(int_objects) == len(objects)\n\n        # 2. Convert to more convenient data structure (objects by frame)\n        int_objects_by_frame = self._get_objects_by_frame(int_objects, start_frame)\n        old_objects_by_frame = self._get_objects_by_frame(self.objects, start_frame)\n\n        # 3. Add new objects as is. It should be done only after old_objects_by_frame\n        # variable is initialized.\n        self.objects.extend(new_objects)\n\n        # Nothing to merge here. Just add all int_objects if any.\n        if not old_objects_by_frame or not int_objects_by_frame:\n            for frame in old_objects_by_frame:\n                for old_obj in old_objects_by_frame[frame]:\n                    self._modify_unmached_object(old_obj, start_frame + overlap)\n            self.objects.extend(int_objects)\n            return\n\n        # 4. Build cost matrix for each frame and find correspondence using\n        # Hungarian algorithm. In this case min_cost_thresh is stronger\n        # because we compare only on one frame.\n        min_cost_thresh = self._get_cost_threshold()\n        for frame in int_objects_by_frame:\n            if frame in old_objects_by_frame:\n                int_objects = int_objects_by_frame[frame]\n                old_objects = old_objects_by_frame[frame]\n                cost_matrix = np.empty(shape=(len(int_objects), len(old_objects)),\n                    dtype=float)\n                # 5.1 Construct cost matrix for the frame.\n                for i, int_obj in enumerate(int_objects):\n                    for j, old_obj in enumerate(old_objects):\n                        cost_matrix[i][j] = 1 - self._calc_objects_similarity(\n                            int_obj, old_obj, start_frame, overlap)\n\n                # 6. Find optimal solution using Hungarian algorithm.\n                row_ind, col_ind = linear_sum_assignment(cost_matrix)\n                old_objects_indexes = list(range(0, len(old_objects)))\n                int_objects_indexes = list(range(0, len(int_objects)))\n                for i, j in zip(row_ind, col_ind):\n                    # Reject the solution if the cost is too high. Remember\n                    # inside int_objects_indexes objects which were handled.\n                    if cost_matrix[i][j] <= min_cost_thresh:\n                        old_objects[j] = self._unite_objects(int_objects[i], old_objects[j])\n                        int_objects_indexes[i] = -1\n                        old_objects_indexes[j] = -1\n\n                # 7. Add all new objects which were not processed.\n                for i in int_objects_indexes:\n                    if i != -1:\n                        self.objects.append(int_objects[i])\n\n                # 8. Modify all old objects which were not processed\n                # (e.g. generate a shape with outside=True at the end).\n                for j in old_objects_indexes:\n                    if j != -1:\n                        self._modify_unmached_object(old_objects[j],\n                            start_frame + overlap)\n            else:\n                # We don\'t have old objects on the frame. Let\'s add all new ones.\n                self.objects.extend(int_objects_by_frame[frame])\n\nclass TagManager(ObjectManager):\n    @staticmethod\n    def _get_cost_threshold():\n        return 0.25\n\n    @staticmethod\n    def _calc_objects_similarity(obj0, obj1, start_frame, overlap):\n        # TODO: improve the trivial implementation, compare attributes\n        return 1 if obj0[""label_id""] == obj1[""label_id""] else 0\n\n    @staticmethod\n    def _unite_objects(obj0, obj1):\n        # TODO: improve the trivial implementation\n        return obj0 if obj0[""frame""] < obj1[""frame""] else obj1\n\n    @staticmethod\n    def _modify_unmached_object(obj, end_frame):\n        pass\n\ndef pairwise(iterable):\n    a = iter(iterable)\n    return zip(a, a)\n\nclass ShapeManager(ObjectManager):\n    def to_tracks(self):\n        tracks = []\n        for shape in self.objects:\n            shape0 = copy(shape)\n            shape0[""keyframe""] = True\n            shape0[""outside""] = False\n            # TODO: Separate attributes on mutable and unmutable\n            shape0[""attributes""] = []\n            shape0.pop(""group"", None)\n            shape1 = copy(shape0)\n            shape1[""outside""] = True\n            shape1[""frame""] += 1\n\n            track = {\n                ""label_id"": shape[""label_id""],\n                ""frame"": shape[""frame""],\n                ""group"": shape.get(""group"", None),\n                ""attributes"": shape[""attributes""],\n                ""shapes"": [shape0, shape1]\n            }\n            tracks.append(track)\n\n        return tracks\n\n    @staticmethod\n    def _get_cost_threshold():\n        return 0.25\n\n    @staticmethod\n    def _calc_objects_similarity(obj0, obj1, start_frame, overlap):\n        def _calc_polygons_similarity(p0, p1):\n            overlap_area = p0.intersection(p1).area\n            return overlap_area / (p0.area + p1.area - overlap_area)\n\n        has_same_type  = obj0[""type""] == obj1[""type""]\n        has_same_label = obj0.get(""label_id"") == obj1.get(""label_id"")\n        if has_same_type and has_same_label:\n            if obj0[""type""] == ShapeType.RECTANGLE:\n                p0 = geometry.box(*obj0[""points""])\n                p1 = geometry.box(*obj1[""points""])\n\n                return _calc_polygons_similarity(p0, p1)\n            elif obj0[""type""] == ShapeType.POLYGON:\n                p0 = geometry.Polygon(pairwise(obj0[""points""]))\n                p1 = geometry.Polygon(pairwise(obj0[""points""]))\n\n                return _calc_polygons_similarity(p0, p1)\n            else:\n                return 0 # FIXME: need some similarity for points and polylines\n        return 0\n\n    @staticmethod\n    def _unite_objects(obj0, obj1):\n        # TODO: improve the trivial implementation\n        return obj0 if obj0[""frame""] < obj1[""frame""] else obj1\n\n    @staticmethod\n    def _modify_unmached_object(obj, end_frame):\n        pass\n\nclass TrackManager(ObjectManager):\n    def to_shapes(self, end_frame):\n        shapes = []\n        for idx, track in enumerate(self.objects):\n            for shape in TrackManager.get_interpolated_shapes(track, 0, end_frame):\n                shape[""label_id""] = track[""label_id""]\n                shape[""group""] = track[""group""]\n                shape[""track_id""] = idx\n                shape[""attributes""] += track[""attributes""]\n                shapes.append(shape)\n        return shapes\n\n    @staticmethod\n    def _get_objects_by_frame(objects, start_frame):\n        # Just for unification. All tracks are assigned on the same frame\n        objects_by_frame = {0: []}\n        for obj in objects:\n            shape = obj[""shapes""][-1] # optimization for old tracks\n            if shape[""frame""] >= start_frame or not shape[""outside""]:\n                objects_by_frame[0].append(obj)\n\n        if not objects_by_frame[0]:\n            objects_by_frame = {}\n\n        return objects_by_frame\n\n    @staticmethod\n    def _get_cost_threshold():\n        return 0.5\n\n    @staticmethod\n    def _calc_objects_similarity(obj0, obj1, start_frame, overlap):\n        if obj0[""label_id""] == obj1[""label_id""]:\n            # Here start_frame is the start frame of next segment\n            # and stop_frame is the stop frame of current segment\n            # end_frame == stop_frame + 1\n            end_frame = start_frame + overlap\n            obj0_shapes = TrackManager.get_interpolated_shapes(obj0, start_frame, end_frame)\n            obj1_shapes = TrackManager.get_interpolated_shapes(obj1, start_frame, end_frame)\n            obj0_shapes_by_frame = {shape[""frame""]:shape for shape in obj0_shapes}\n            obj1_shapes_by_frame = {shape[""frame""]:shape for shape in obj1_shapes}\n            assert obj0_shapes_by_frame and obj1_shapes_by_frame\n\n            count, error = 0, 0\n            for frame in range(start_frame, end_frame):\n                shape0 = obj0_shapes_by_frame.get(frame)\n                shape1 = obj1_shapes_by_frame.get(frame)\n                if shape0 and shape1:\n                    if shape0[""outside""] != shape1[""outside""]:\n                        error += 1\n                    else:\n                        error += 1 - ShapeManager._calc_objects_similarity(shape0, shape1, start_frame, overlap)\n                    count += 1\n                elif shape0 or shape1:\n                    error += 1\n                    count += 1\n\n            return 1 - error / count\n        else:\n            return 0\n\n    @staticmethod\n    def _modify_unmached_object(obj, end_frame):\n        shape = obj[""shapes""][-1]\n        if not shape[""outside""]:\n            shape = deepcopy(shape)\n            shape[""frame""] = end_frame\n            shape[""outside""] = True\n            obj[""shapes""].append(shape)\n            # Need to update cached interpolated shapes\n            # because key shapes were changed\n            if obj.get(""interpolated_shapes""):\n                last_interpolated_shape = obj[""interpolated_shapes""][-1]\n                for frame in range(last_interpolated_shape[""frame""] + 1, end_frame):\n                    last_interpolated_shape = deepcopy(last_interpolated_shape)\n                    last_interpolated_shape[""frame""] = frame\n                    obj[""interpolated_shapes""].append(last_interpolated_shape)\n                obj[""interpolated_shapes""].append(shape)\n\n    @staticmethod\n    def normalize_shape(shape):\n        points = list(shape[""points""])\n        if len(points) == 2:\n            points.extend(points) # duplicate points for single point case\n        points = np.asarray(points).reshape(-1, 2)\n        broken_line = geometry.LineString(points)\n        points = []\n        for off in range(0, 100, 1):\n            p = broken_line.interpolate(off / 100, True)\n            points.append(p.x)\n            points.append(p.y)\n\n        shape = copy(shape)\n        shape[""points""] = points\n\n        return shape\n\n    @staticmethod\n    def get_interpolated_shapes(track, start_frame, end_frame):\n        def interpolate(shape0, shape1):\n            shapes = []\n            is_same_type = shape0[""type""] == shape1[""type""]\n            is_polygon = shape0[""type""] == ShapeType.POLYGON\n            is_polyline = shape0[""type""] == ShapeType.POLYLINE\n            is_same_size = len(shape0[""points""]) == len(shape1[""points""])\n            if not is_same_type or is_polygon or is_polyline or not is_same_size:\n                shape0 = TrackManager.normalize_shape(shape0)\n                shape1 = TrackManager.normalize_shape(shape1)\n\n            distance = shape1[""frame""] - shape0[""frame""]\n            step = np.subtract(shape1[""points""], shape0[""points""]) / distance\n            for frame in range(shape0[""frame""] + 1, shape1[""frame""]):\n                off = frame - shape0[""frame""]\n                if shape1[""outside""]:\n                    points = np.asarray(shape0[""points""]).reshape(-1, 2)\n                else:\n                    points = (shape0[""points""] + step * off).reshape(-1, 2)\n                shape = deepcopy(shape0)\n                if len(points) == 1:\n                    shape[""points""] = points.flatten()\n                else:\n                    broken_line = geometry.LineString(points).simplify(0.05, False)\n                    shape[""points""] = [x for p in broken_line.coords for x in p]\n\n                shape[""keyframe""] = False\n                shape[""frame""] = frame\n                shapes.append(shape)\n            return shapes\n\n        if track.get(""interpolated_shapes""):\n            return track[""interpolated_shapes""]\n\n        # TODO: should be return an iterator?\n        shapes = []\n        curr_frame = track[""shapes""][0][""frame""]\n        prev_shape = {}\n        for shape in track[""shapes""]:\n            if prev_shape:\n                assert shape[""frame""] > curr_frame\n                for attr in prev_shape[""attributes""]:\n                    if attr[""spec_id""] not in map(lambda el: el[""spec_id""], shape[""attributes""]):\n                        shape[""attributes""].append(deepcopy(attr))\n                if not prev_shape[""outside""]:\n                    shapes.extend(interpolate(prev_shape, shape))\n\n            shape[""keyframe""] = True\n            shapes.append(shape)\n            curr_frame = shape[""frame""]\n            prev_shape = shape\n\n        # TODO: Need to modify a client and a database (append ""outside"" shapes for polytracks)\n        if not prev_shape[""outside""] and (prev_shape[""type""] == ShapeType.RECTANGLE\n               or prev_shape[""type""] == ShapeType.POINTS or prev_shape[""type""] == ShapeType.CUBOID):\n            shape = copy(prev_shape)\n            shape[""frame""] = end_frame\n            shapes.extend(interpolate(prev_shape, shape))\n\n        track[""interpolated_shapes""] = shapes\n\n        return shapes\n\n    @staticmethod\n    def _unite_objects(obj0, obj1):\n        track = obj0 if obj0[""frame""] < obj1[""frame""] else obj1\n        assert obj0[""label_id""] == obj1[""label_id""]\n        shapes = {shape[""frame""]:shape for shape in obj0[""shapes""]}\n        for shape in obj1[""shapes""]:\n            frame = shape[""frame""]\n            if frame in shapes:\n                shapes[frame] = ShapeManager._unite_objects(shapes[frame], shape)\n            else:\n                shapes[frame] = shape\n\n        track[""frame""] = min(obj0[""frame""], obj1[""frame""])\n        track[""shapes""] = list(sorted(shapes.values(), key=lambda shape: shape[""frame""]))\n        track[""interpolated_shapes""] = []\n\n        return track\n'"
cvat/apps/dataset_manager/bindings.py,0,"b'\n# Copyright (C) 2019-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os.path as osp\nfrom collections import OrderedDict, namedtuple\n\nfrom django.utils import timezone\n\nimport datumaro.components.extractor as datumaro\nfrom cvat.apps.engine.frame_provider import FrameProvider\nfrom cvat.apps.engine.models import AttributeType, ShapeType\nfrom datumaro.util.image import Image\n\nfrom .annotation import AnnotationManager, TrackManager\n\n\nclass TaskData:\n    Attribute = namedtuple(\'Attribute\', \'name, value\')\n    LabeledShape = namedtuple(\n        \'LabeledShape\', \'type, frame, label, points, occluded, attributes, group, z_order\')\n    LabeledShape.__new__.__defaults__ = (0, 0)\n    TrackedShape = namedtuple(\n        \'TrackedShape\', \'type, frame, points, occluded, outside, keyframe, attributes, group, z_order, label, track_id\')\n    TrackedShape.__new__.__defaults__ = (0, 0, None, 0)\n    Track = namedtuple(\'Track\', \'label, group, shapes\')\n    Tag = namedtuple(\'Tag\', \'frame, label, attributes, group\')\n    Tag.__new__.__defaults__ = (0, )\n    Frame = namedtuple(\n        \'Frame\', \'idx, frame, name, width, height, labeled_shapes, tags\')\n\n    def __init__(self, annotation_ir, db_task, host=\'\', create_callback=None):\n        self._annotation_ir = annotation_ir\n        self._db_task = db_task\n        self._host = host\n        self._create_callback = create_callback\n        self._MAX_ANNO_SIZE = 30000\n        self._frame_info = {}\n        self._frame_mapping = {}\n        self._frame_step = db_task.data.get_frame_step()\n\n        db_labels = self._db_task.label_set.all().prefetch_related(\n            \'attributespec_set\').order_by(\'pk\')\n\n        self._label_mapping = OrderedDict(\n            (db_label.id, db_label) for db_label in db_labels)\n\n        self._attribute_mapping = {db_label.id: {\n            \'mutable\': {}, \'immutable\': {}} for db_label in db_labels}\n\n        for db_label in db_labels:\n            for db_attribute in db_label.attributespec_set.all():\n                if db_attribute.mutable:\n                    self._attribute_mapping[db_label.id][\'mutable\'][db_attribute.id] = db_attribute.name\n                else:\n                    self._attribute_mapping[db_label.id][\'immutable\'][db_attribute.id] = db_attribute.name\n\n        self._attribute_mapping_merged = {}\n        for label_id, attr_mapping in self._attribute_mapping.items():\n            self._attribute_mapping_merged[label_id] = {\n                **attr_mapping[\'mutable\'],\n                **attr_mapping[\'immutable\'],\n            }\n\n        self._init_frame_info()\n        self._init_meta()\n\n    def _get_label_id(self, label_name):\n        for db_label in self._label_mapping.values():\n            if label_name == db_label.name:\n                return db_label.id\n        raise ValueError(""Label {!r} is not registered for this task"".format(label_name))\n\n    def _get_label_name(self, label_id):\n        return self._label_mapping[label_id].name\n\n    def _get_attribute_name(self, attribute_id):\n        for attribute_mapping in self._attribute_mapping_merged.values():\n            if attribute_id in attribute_mapping:\n                return attribute_mapping[attribute_id]\n\n    def _get_attribute_id(self, label_id, attribute_name, attribute_type=None):\n        if attribute_type:\n            container = self._attribute_mapping[label_id][attribute_type]\n        else:\n            container = self._attribute_mapping_merged[label_id]\n\n        for attr_id, attr_name in container.items():\n            if attribute_name == attr_name:\n                return attr_id\n        return None\n\n    def _get_mutable_attribute_id(self, label_id, attribute_name):\n        return self._get_attribute_id(label_id, attribute_name, \'mutable\')\n\n    def _get_immutable_attribute_id(self, label_id, attribute_name):\n        return self._get_attribute_id(label_id, attribute_name, \'immutable\')\n\n    def _init_frame_info(self):\n        if hasattr(self._db_task.data, \'video\'):\n            self._frame_info = {frame: {\n                ""path"": ""frame_{:06d}"".format(\n                    self._db_task.data.start_frame + frame * self._frame_step),\n                ""width"": self._db_task.data.video.width,\n                ""height"": self._db_task.data.video.height,\n            } for frame in range(self._db_task.data.size)}\n        else:\n            self._frame_info = {db_image.frame: {\n                ""path"": db_image.path,\n                ""width"": db_image.width,\n                ""height"": db_image.height,\n            } for db_image in self._db_task.data.images.all()}\n\n        self._frame_mapping = {\n            self._get_filename(info[""path""]): frame\n            for frame, info in self._frame_info.items()\n        }\n\n    def _init_meta(self):\n        db_segments = self._db_task.segment_set.all().prefetch_related(\'job_set\')\n        self._meta = OrderedDict([\n            (""task"", OrderedDict([\n                (""id"", str(self._db_task.id)),\n                (""name"", self._db_task.name),\n                (""size"", str(self._db_task.data.size)),\n                (""mode"", self._db_task.mode),\n                (""overlap"", str(self._db_task.overlap)),\n                (""bugtracker"", self._db_task.bug_tracker),\n                (""created"", str(timezone.localtime(self._db_task.created_date))),\n                (""updated"", str(timezone.localtime(self._db_task.updated_date))),\n                (""start_frame"", str(self._db_task.data.start_frame)),\n                (""stop_frame"", str(self._db_task.data.stop_frame)),\n                (""frame_filter"", self._db_task.data.frame_filter),\n                (""z_order"", str(self._db_task.z_order)),\n\n                (""labels"", [\n                    (""label"", OrderedDict([\n                        (""name"", db_label.name),\n                        (""attributes"", [\n                            (""attribute"", OrderedDict([\n                                (""name"", db_attr.name),\n                                (""mutable"", str(db_attr.mutable)),\n                                (""input_type"", db_attr.input_type),\n                                (""default_value"", db_attr.default_value),\n                                (""values"", db_attr.values)]))\n                            for db_attr in db_label.attributespec_set.all()])\n                    ])) for db_label in self._label_mapping.values()\n                ]),\n\n                (""segments"", [\n                    (""segment"", OrderedDict([\n                        (""id"", str(db_segment.id)),\n                        (""start"", str(db_segment.start_frame)),\n                        (""stop"", str(db_segment.stop_frame)),\n                        (""url"", ""{}/?id={}"".format(\n                            self._host, db_segment.job_set.all()[0].id))]\n                    )) for db_segment in db_segments\n                ]),\n\n                (""owner"", OrderedDict([\n                    (""username"", self._db_task.owner.username),\n                    (""email"", self._db_task.owner.email)\n                ]) if self._db_task.owner else """"),\n\n                (""assignee"", OrderedDict([\n                    (""username"", self._db_task.assignee.username),\n                    (""email"", self._db_task.assignee.email)\n                ]) if self._db_task.assignee else """"),\n            ])),\n            (""dumped"", str(timezone.localtime(timezone.now())))\n        ])\n\n        if hasattr(self._db_task.data, ""video""):\n            self._meta[""task""][""original_size""] = OrderedDict([\n                (""width"", str(self._db_task.data.video.width)),\n                (""height"", str(self._db_task.data.video.height))\n            ])\n            # Add source to dumped file\n            self._meta[""source""] = str(\n                osp.basename(self._db_task.data.video.path))\n\n    def _export_attributes(self, attributes):\n        exported_attributes = []\n        for attr in attributes:\n            attribute_name = self._get_attribute_name(attr[""spec_id""])\n            exported_attributes.append(TaskData.Attribute(\n                name=attribute_name,\n                value=attr[""value""],\n            ))\n        return exported_attributes\n\n    def _export_tracked_shape(self, shape):\n        return TaskData.TrackedShape(\n            type=shape[""type""],\n            frame=self._db_task.data.start_frame +\n                shape[""frame""] * self._frame_step,\n            label=self._get_label_name(shape[""label_id""]),\n            points=shape[""points""],\n            occluded=shape[""occluded""],\n            z_order=shape.get(""z_order"", 0),\n            group=shape.get(""group"", 0),\n            outside=shape.get(""outside"", False),\n            keyframe=shape.get(""keyframe"", True),\n            track_id=shape[""track_id""],\n            attributes=self._export_attributes(shape[""attributes""]),\n        )\n\n    def _export_labeled_shape(self, shape):\n        return TaskData.LabeledShape(\n            type=shape[""type""],\n            label=self._get_label_name(shape[""label_id""]),\n            frame=self._db_task.data.start_frame +\n                shape[""frame""] * self._frame_step,\n            points=shape[""points""],\n            occluded=shape[""occluded""],\n            z_order=shape.get(""z_order"", 0),\n            group=shape.get(""group"", 0),\n            attributes=self._export_attributes(shape[""attributes""]),\n        )\n\n    def _export_tag(self, tag):\n        return TaskData.Tag(\n            frame=self._db_task.data.start_frame +\n                tag[""frame""] * self._frame_step,\n            label=self._get_label_name(tag[""label_id""]),\n            group=tag.get(""group"", 0),\n            attributes=self._export_attributes(tag[""attributes""]),\n        )\n\n    def group_by_frame(self, include_empty=False):\n        frames = {}\n        def get_frame(idx):\n            frame_info = self._frame_info[idx]\n            frame = self._db_task.data.start_frame + idx * self._frame_step\n            if frame not in frames:\n                frames[frame] = TaskData.Frame(\n                    idx=idx,\n                    frame=frame,\n                    name=frame_info[\'path\'],\n                    height=frame_info[""height""],\n                    width=frame_info[""width""],\n                    labeled_shapes=[],\n                    tags=[],\n                )\n            return frames[frame]\n\n        if include_empty:\n            for idx in self._frame_info:\n                get_frame(idx)\n\n        anno_manager = AnnotationManager(self._annotation_ir)\n        for shape in sorted(anno_manager.to_shapes(self._db_task.data.size),\n                key=lambda shape: shape.get(""z_order"", 0)):\n            if \'track_id\' in shape:\n                exported_shape = self._export_tracked_shape(shape)\n            else:\n                exported_shape = self._export_labeled_shape(shape)\n            get_frame(shape[\'frame\']).labeled_shapes.append(\n                exported_shape)\n\n        for tag in self._annotation_ir.tags:\n            get_frame(tag[\'frame\']).tags.append(self._export_tag(tag))\n\n        return iter(frames.values())\n\n    @property\n    def shapes(self):\n        for shape in self._annotation_ir.shapes:\n            yield self._export_labeled_shape(shape)\n\n    @property\n    def tracks(self):\n        for idx, track in enumerate(self._annotation_ir.tracks):\n            tracked_shapes = TrackManager.get_interpolated_shapes(\n                track, 0, self._db_task.data.size)\n            for tracked_shape in tracked_shapes:\n                tracked_shape[""attributes""] += track[""attributes""]\n                tracked_shape[""track_id""] = idx\n                tracked_shape[""group""] = track[""group""]\n                tracked_shape[""label_id""] = track[""label_id""]\n\n            yield TaskData.Track(\n                label=self._get_label_name(track[""label_id""]),\n                group=track[""group""],\n                shapes=[self._export_tracked_shape(shape)\n                    for shape in tracked_shapes],\n            )\n\n    @property\n    def tags(self):\n        for tag in self._annotation_ir.tags:\n            yield self._export_tag(tag)\n\n    @property\n    def meta(self):\n        return self._meta\n\n    def _import_tag(self, tag):\n        _tag = tag._asdict()\n        label_id = self._get_label_id(_tag.pop(\'label\'))\n        _tag[\'frame\'] = (int(_tag[\'frame\']) -\n            self._db_task.data.start_frame) // self._frame_step\n        _tag[\'label_id\'] = label_id\n        _tag[\'attributes\'] = [self._import_attribute(label_id, attrib)\n            for attrib in _tag[\'attributes\']\n            if self._get_attribute_id(label_id, attrib.name)]\n        return _tag\n\n    def _import_attribute(self, label_id, attribute):\n        return {\n            \'spec_id\': self._get_attribute_id(label_id, attribute.name),\n            \'value\': attribute.value,\n        }\n\n    def _import_shape(self, shape):\n        _shape = shape._asdict()\n        label_id = self._get_label_id(_shape.pop(\'label\'))\n        _shape[\'frame\'] = (int(_shape[\'frame\']) -\n            self._db_task.data.start_frame) // self._frame_step\n        _shape[\'label_id\'] = label_id\n        _shape[\'attributes\'] = [self._import_attribute(label_id, attrib)\n            for attrib in _shape[\'attributes\']\n            if self._get_attribute_id(label_id, attrib.name)]\n        return _shape\n\n    def _import_track(self, track):\n        _track = track._asdict()\n        label_id = self._get_label_id(_track.pop(\'label\'))\n        _track[\'frame\'] = (min(int(shape.frame) for shape in _track[\'shapes\']) -\n            self._db_task.data.start_frame) // self._frame_step\n        _track[\'label_id\'] = label_id\n        _track[\'attributes\'] = []\n        _track[\'shapes\'] = [shape._asdict() for shape in _track[\'shapes\']]\n        for shape in _track[\'shapes\']:\n            shape[\'frame\'] = (int(shape[\'frame\']) - \\\n                self._db_task.data.start_frame) // self._frame_step\n            _track[\'attributes\'] = [self._import_attribute(label_id, attrib)\n                for attrib in shape[\'attributes\']\n                if self._get_immutable_attribute_id(label_id, attrib.name)]\n            shape[\'attributes\'] = [self._import_attribute(label_id, attrib)\n                for attrib in shape[\'attributes\']\n                if self._get_mutable_attribute_id(label_id, attrib.name)]\n\n        return _track\n\n    def _call_callback(self):\n        if self._len() > self._MAX_ANNO_SIZE:\n            self._create_callback(self._annotation_ir.serialize())\n            self._annotation_ir.reset()\n\n    def add_tag(self, tag):\n        imported_tag = self._import_tag(tag)\n        if imported_tag[\'label_id\']:\n            self._annotation_ir.add_tag(imported_tag)\n            self._call_callback()\n\n    def add_shape(self, shape):\n        imported_shape = self._import_shape(shape)\n        if imported_shape[\'label_id\']:\n            self._annotation_ir.add_shape(imported_shape)\n            self._call_callback()\n\n    def add_track(self, track):\n        imported_track = self._import_track(track)\n        if imported_track[\'label_id\']:\n            self._annotation_ir.add_track(imported_track)\n            self._call_callback()\n\n    @property\n    def data(self):\n        return self._annotation_ir\n\n    def _len(self):\n        track_len = 0\n        for track in self._annotation_ir.tracks:\n            track_len += len(track[\'shapes\'])\n\n        return len(self._annotation_ir.tags) + len(self._annotation_ir.shapes) + track_len\n\n    @property\n    def frame_info(self):\n        return self._frame_info\n\n    @property\n    def frame_step(self):\n        return self._frame_step\n\n    @property\n    def db_task(self):\n        return self._db_task\n\n    @staticmethod\n    def _get_filename(path):\n        return osp.splitext(osp.basename(path))[0]\n\n    def match_frame(self, filename):\n        # try to match by filename\n        _filename = self._get_filename(filename)\n        if _filename in self._frame_mapping:\n            return self._frame_mapping[_filename]\n\n        raise Exception(\n            ""Cannot match filename or determine frame number for {} filename"".format(filename))\n\nclass CvatTaskDataExtractor(datumaro.SourceExtractor):\n    def __init__(self, task_data, include_images=False):\n        super().__init__()\n        self._categories = self._load_categories(task_data)\n\n        dm_items = []\n\n        if include_images:\n            frame_provider = FrameProvider(task_data.db_task.data)\n\n        for frame_data in task_data.group_by_frame(include_empty=True):\n            loader = None\n            if include_images:\n                loader = lambda p, i=frame_data.idx: frame_provider.get_frame(i,\n                    quality=frame_provider.Quality.ORIGINAL,\n                    out_type=frame_provider.Type.NUMPY_ARRAY)[0]\n            dm_image = Image(path=frame_data.name, loader=loader,\n                size=(frame_data.height, frame_data.width)\n            )\n            dm_anno = self._read_cvat_anno(frame_data, task_data)\n            dm_item = datumaro.DatasetItem(id=frame_data.frame,\n                annotations=dm_anno, image=dm_image)\n            dm_items.append(dm_item)\n\n        self._items = dm_items\n\n    def __iter__(self):\n        for item in self._items:\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n\n    def categories(self):\n        return self._categories\n\n    @staticmethod\n    def _load_categories(cvat_anno):\n        categories = {}\n\n        label_categories = datumaro.LabelCategories(\n            attributes=[\'occluded\', \'z_order\'])\n\n        for _, label in cvat_anno.meta[\'task\'][\'labels\']:\n            label_categories.add(label[\'name\'])\n            for _, attr in label[\'attributes\']:\n                label_categories.attributes.add(attr[\'name\'])\n\n        categories[datumaro.AnnotationType.label] = label_categories\n\n        return categories\n\n    def _read_cvat_anno(self, cvat_frame_anno, task_data):\n        item_anno = []\n\n        categories = self.categories()\n        label_cat = categories[datumaro.AnnotationType.label]\n        def map_label(name): return label_cat.find(name)[0]\n        label_attrs = {\n            label[\'name\']: label[\'attributes\']\n            for _, label in task_data.meta[\'task\'][\'labels\']\n        }\n\n        def convert_attrs(label, cvat_attrs):\n            cvat_attrs = {a.name: a.value for a in cvat_attrs}\n            dm_attr = dict()\n            for _, a_desc in label_attrs[label]:\n                a_name = a_desc[\'name\']\n                a_value = cvat_attrs.get(a_name, a_desc[\'default_value\'])\n                try:\n                    if a_desc[\'input_type\'] == AttributeType.NUMBER:\n                        a_value = float(a_value)\n                    elif a_desc[\'input_type\'] == AttributeType.CHECKBOX:\n                        a_value = (a_value.lower() == \'true\')\n                    dm_attr[a_name] = a_value\n                except Exception as e:\n                    raise Exception(\n                        ""Failed to convert attribute \'%s\'=\'%s\': %s"" %\n                        (a_name, a_value, e))\n            return dm_attr\n\n        for tag_obj in cvat_frame_anno.tags:\n            anno_group = tag_obj.group\n            anno_label = map_label(tag_obj.label)\n            anno_attr = convert_attrs(tag_obj.label, tag_obj.attributes)\n\n            anno = datumaro.Label(label=anno_label,\n                attributes=anno_attr, group=anno_group)\n            item_anno.append(anno)\n\n        for shape_obj in cvat_frame_anno.labeled_shapes:\n            anno_group = shape_obj.group\n            anno_label = map_label(shape_obj.label)\n            anno_attr = convert_attrs(shape_obj.label, shape_obj.attributes)\n            anno_attr[\'occluded\'] = shape_obj.occluded\n\n            if hasattr(shape_obj, \'track_id\'):\n                anno_attr[\'track_id\'] = shape_obj.track_id\n                anno_attr[\'keyframe\'] = shape_obj.keyframe\n\n            anno_points = shape_obj.points\n            if shape_obj.type == ShapeType.POINTS:\n                anno = datumaro.Points(anno_points,\n                    label=anno_label, attributes=anno_attr, group=anno_group,\n                    z_order=shape_obj.z_order)\n            elif shape_obj.type == ShapeType.POLYLINE:\n                anno = datumaro.PolyLine(anno_points,\n                    label=anno_label, attributes=anno_attr, group=anno_group,\n                    z_order=shape_obj.z_order)\n            elif shape_obj.type == ShapeType.POLYGON:\n                anno = datumaro.Polygon(anno_points,\n                    label=anno_label, attributes=anno_attr, group=anno_group,\n                    z_order=shape_obj.z_order)\n            elif shape_obj.type == ShapeType.RECTANGLE:\n                x0, y0, x1, y1 = anno_points\n                anno = datumaro.Bbox(x0, y0, x1 - x0, y1 - y0,\n                    label=anno_label, attributes=anno_attr, group=anno_group,\n                    z_order=shape_obj.z_order)\n            elif shape_obj.type == ShapeType.CUBOID:\n                continue # Datumaro does not support cuboids\n            else:\n                raise Exception(""Unknown shape type \'%s\'"" % shape_obj.type)\n\n            item_anno.append(anno)\n\n        return item_anno\n\ndef match_frame(item, task_data):\n    is_video = task_data.meta[\'task\'][\'mode\'] == \'interpolation\'\n\n    frame_number = None\n    if frame_number is None:\n        try:\n            frame_number = task_data.match_frame(item.id)\n        except Exception:\n            pass\n    if frame_number is None and item.has_image:\n        try:\n            frame_number = task_data.match_frame(item.image.filename)\n        except Exception:\n            pass\n    if frame_number is None:\n        try:\n            frame_number = int(item.id)\n        except Exception:\n            pass\n    if frame_number is None and is_video and item.id.startswith(\'frame_\'):\n        frame_number = int(item.id[len(\'frame_\'):])\n    if not frame_number in task_data.frame_info:\n        raise Exception(""Could not match item id: \'%s\' with any task frame"" %\n            item.id)\n    return frame_number\n\ndef import_dm_annotations(dm_dataset, task_data):\n    shapes = {\n        datumaro.AnnotationType.bbox: ShapeType.RECTANGLE,\n        datumaro.AnnotationType.polygon: ShapeType.POLYGON,\n        datumaro.AnnotationType.polyline: ShapeType.POLYLINE,\n        datumaro.AnnotationType.points: ShapeType.POINTS,\n    }\n\n    label_cat = dm_dataset.categories()[datumaro.AnnotationType.label]\n\n    for item in dm_dataset:\n        frame_number = match_frame(item, task_data)\n\n        # do not store one-item groups\n        group_map = {0: 0}\n        group_size = {0: 0}\n        for ann in item.annotations:\n            if ann.type in shapes:\n                group = group_map.get(ann.group)\n                if group is None:\n                    group = len(group_map)\n                    group_map[ann.group] = group\n                    group_size[ann.group] = 1\n                else:\n                    group_size[ann.group] += 1\n        group_map = {g: s for g, s in group_size.items()\n            if 1 < s and group_map[g]}\n        group_map = {g: i for i, g in enumerate([0] + sorted(group_map))}\n\n        for ann in item.annotations:\n            if ann.type in shapes:\n                task_data.add_shape(task_data.LabeledShape(\n                    type=shapes[ann.type],\n                    frame=frame_number,\n                    label=label_cat.items[ann.label].name,\n                    points=ann.points,\n                    occluded=ann.attributes.get(\'occluded\') == True,\n                    z_order=ann.z_order,\n                    group=group_map.get(ann.group, 0),\n                    attributes=[task_data.Attribute(name=n, value=str(v))\n                        for n, v in ann.attributes.items()],\n                ))\n            elif ann.type == datumaro.AnnotationType.label:\n                task_data.add_tag(task_data.Tag(\n                    frame=frame_number,\n                    label=label_cat.items[ann.label].name,\n                    group=group_map.get(ann.group, 0),\n                    attributes=[task_data.Attribute(name=n, value=str(v))\n                        for n, v in ann.attributes.items()],\n                ))\n'"
cvat/apps/dataset_manager/serializers.py,0,"b""# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom rest_framework import serializers\n\n\nclass DatasetFormatSerializer(serializers.Serializer):\n    name = serializers.CharField(max_length=64, source='DISPLAY_NAME')\n    ext = serializers.CharField(max_length=64, source='EXT')\n    version = serializers.CharField(max_length=64, source='VERSION')\n    enabled = serializers.BooleanField(source='ENABLED')\n\nclass DatasetFormatsSerializer(serializers.Serializer):\n    importers = DatasetFormatSerializer(many=True)\n    exporters = DatasetFormatSerializer(many=True)"""
cvat/apps/dataset_manager/task.py,0,"b'\n# Copyright (C) 2019-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nfrom enum import Enum\n\nfrom django.conf import settings\nfrom django.db import transaction\nfrom django.utils import timezone\n\nfrom cvat.apps.engine import models, serializers\nfrom cvat.apps.engine.plugins import plugin_decorator\nfrom cvat.apps.profiler import silk_profile\n\nfrom .annotation import AnnotationIR, AnnotationManager\nfrom .bindings import TaskData\nfrom .formats.registry import make_exporter, make_importer\n\n\nclass dotdict(OrderedDict):\n    """"""dot.notation access to dictionary attributes""""""\n    __getattr__ = OrderedDict.get\n    __setattr__ = OrderedDict.__setitem__\n    __delattr__ = OrderedDict.__delitem__\n    __eq__ = lambda self, other: self.id == other.id\n    __hash__ = lambda self: self.id\n\nclass PatchAction(str, Enum):\n    CREATE = ""create""\n    UPDATE = ""update""\n    DELETE = ""delete""\n\n    @classmethod\n    def values(cls):\n        return [item.value for item in cls]\n\n    def __str__(self):\n        return self.value\n\ndef bulk_create(db_model, objects, flt_param):\n    if objects:\n        if flt_param:\n            if \'postgresql\' in settings.DATABASES[""default""][""ENGINE""]:\n                return db_model.objects.bulk_create(objects)\n            else:\n                ids = list(db_model.objects.filter(**flt_param).values_list(\'id\', flat=True))\n                db_model.objects.bulk_create(objects)\n\n                return list(db_model.objects.exclude(id__in=ids).filter(**flt_param))\n        else:\n            return db_model.objects.bulk_create(objects)\n\n    return []\n\ndef _merge_table_rows(rows, keys_for_merge, field_id):\n    # It is necessary to keep a stable order of original rows\n    # (e.g. for tracked boxes). Otherwise prev_box.frame can be bigger\n    # than next_box.frame.\n    merged_rows = OrderedDict()\n\n    # Group all rows by field_id. In grouped rows replace fields in\n    # accordance with keys_for_merge structure.\n    for row in rows:\n        row_id = row[field_id]\n        if not row_id in merged_rows:\n            merged_rows[row_id] = dotdict(row)\n            for key in keys_for_merge:\n                merged_rows[row_id][key] = []\n\n        for key in keys_for_merge:\n            item = dotdict({v.split(\'__\', 1)[-1]:row[v] for v in keys_for_merge[key]})\n            if item.id is not None:\n                merged_rows[row_id][key].append(item)\n\n    # Remove redundant keys from final objects\n    redundant_keys = [item for values in keys_for_merge.values() for item in values]\n    for i in merged_rows:\n        for j in redundant_keys:\n            del merged_rows[i][j]\n\n    return list(merged_rows.values())\n\nclass JobAnnotation:\n    def __init__(self, pk):\n        self.db_job = models.Job.objects.select_related(\'segment__task\') \\\n            .select_for_update().get(id=pk)\n\n        db_segment = self.db_job.segment\n        self.start_frame = db_segment.start_frame\n        self.stop_frame = db_segment.stop_frame\n        self.ir_data = AnnotationIR()\n\n        self.db_labels = {db_label.id:db_label\n            for db_label in db_segment.task.label_set.all()}\n\n        self.db_attributes = {}\n        for db_label in self.db_labels.values():\n            self.db_attributes[db_label.id] = {\n                ""mutable"": OrderedDict(),\n                ""immutable"": OrderedDict(),\n                ""all"": OrderedDict(),\n            }\n            for db_attr in db_label.attributespec_set.all():\n                default_value = dotdict([\n                    (\'spec_id\', db_attr.id),\n                    (\'value\', db_attr.default_value),\n                ])\n                if db_attr.mutable:\n                    self.db_attributes[db_label.id][""mutable""][db_attr.id] = default_value\n                else:\n                    self.db_attributes[db_label.id][""immutable""][db_attr.id] = default_value\n\n                self.db_attributes[db_label.id][""all""][db_attr.id] = default_value\n\n    def reset(self):\n        self.ir_data.reset()\n\n    def _save_tracks_to_db(self, tracks):\n        db_tracks = []\n        db_track_attrvals = []\n        db_shapes = []\n        db_shape_attrvals = []\n\n        for track in tracks:\n            track_attributes = track.pop(""attributes"", [])\n            shapes = track.pop(""shapes"")\n            db_track = models.LabeledTrack(job=self.db_job, **track)\n            if db_track.label_id not in self.db_labels:\n                raise AttributeError(""label_id `{}` is invalid"".format(db_track.label_id))\n\n            for attr in track_attributes:\n                db_attrval = models.LabeledTrackAttributeVal(**attr)\n                if db_attrval.spec_id not in self.db_attributes[db_track.label_id][""immutable""]:\n                    raise AttributeError(""spec_id `{}` is invalid"".format(db_attrval.spec_id))\n                db_attrval.track_id = len(db_tracks)\n                db_track_attrvals.append(db_attrval)\n\n            for shape in shapes:\n                shape_attributes = shape.pop(""attributes"", [])\n                # FIXME: need to clamp points (be sure that all of them inside the image)\n                # Should we check here or implement a validator?\n                db_shape = models.TrackedShape(**shape)\n                db_shape.track_id = len(db_tracks)\n\n                for attr in shape_attributes:\n                    db_attrval = models.TrackedShapeAttributeVal(**attr)\n                    if db_attrval.spec_id not in self.db_attributes[db_track.label_id][""mutable""]:\n                        raise AttributeError(""spec_id `{}` is invalid"".format(db_attrval.spec_id))\n                    db_attrval.shape_id = len(db_shapes)\n                    db_shape_attrvals.append(db_attrval)\n\n                db_shapes.append(db_shape)\n                shape[""attributes""] = shape_attributes\n\n            db_tracks.append(db_track)\n            track[""attributes""] = track_attributes\n            track[""shapes""] = shapes\n\n        db_tracks = bulk_create(\n            db_model=models.LabeledTrack,\n            objects=db_tracks,\n            flt_param={""job_id"": self.db_job.id}\n        )\n\n        for db_attrval in db_track_attrvals:\n            db_attrval.track_id = db_tracks[db_attrval.track_id].id\n        bulk_create(\n            db_model=models.LabeledTrackAttributeVal,\n            objects=db_track_attrvals,\n            flt_param={}\n        )\n\n        for db_shape in db_shapes:\n            db_shape.track_id = db_tracks[db_shape.track_id].id\n\n        db_shapes = bulk_create(\n            db_model=models.TrackedShape,\n            objects=db_shapes,\n            flt_param={""track__job_id"": self.db_job.id}\n        )\n\n        for db_attrval in db_shape_attrvals:\n            db_attrval.shape_id = db_shapes[db_attrval.shape_id].id\n\n        bulk_create(\n            db_model=models.TrackedShapeAttributeVal,\n            objects=db_shape_attrvals,\n            flt_param={}\n        )\n\n        shape_idx = 0\n        for track, db_track in zip(tracks, db_tracks):\n            track[""id""] = db_track.id\n            for shape in track[""shapes""]:\n                shape[""id""] = db_shapes[shape_idx].id\n                shape_idx += 1\n\n        self.ir_data.tracks = tracks\n\n    def _save_shapes_to_db(self, shapes):\n        db_shapes = []\n        db_attrvals = []\n\n        for shape in shapes:\n            attributes = shape.pop(""attributes"", [])\n            # FIXME: need to clamp points (be sure that all of them inside the image)\n            # Should we check here or implement a validator?\n            db_shape = models.LabeledShape(job=self.db_job, **shape)\n            if db_shape.label_id not in self.db_labels:\n                raise AttributeError(""label_id `{}` is invalid"".format(db_shape.label_id))\n\n            for attr in attributes:\n                db_attrval = models.LabeledShapeAttributeVal(**attr)\n                if db_attrval.spec_id not in self.db_attributes[db_shape.label_id][""all""]:\n                    raise AttributeError(""spec_id `{}` is invalid"".format(db_attrval.spec_id))\n\n                db_attrval.shape_id = len(db_shapes)\n                db_attrvals.append(db_attrval)\n\n            db_shapes.append(db_shape)\n            shape[""attributes""] = attributes\n\n        db_shapes = bulk_create(\n            db_model=models.LabeledShape,\n            objects=db_shapes,\n            flt_param={""job_id"": self.db_job.id}\n        )\n\n        for db_attrval in db_attrvals:\n            db_attrval.shape_id = db_shapes[db_attrval.shape_id].id\n\n        bulk_create(\n            db_model=models.LabeledShapeAttributeVal,\n            objects=db_attrvals,\n            flt_param={}\n        )\n\n        for shape, db_shape in zip(shapes, db_shapes):\n            shape[""id""] = db_shape.id\n\n        self.ir_data.shapes = shapes\n\n    def _save_tags_to_db(self, tags):\n        db_tags = []\n        db_attrvals = []\n\n        for tag in tags:\n            attributes = tag.pop(""attributes"", [])\n            db_tag = models.LabeledImage(job=self.db_job, **tag)\n            if db_tag.label_id not in self.db_labels:\n                raise AttributeError(""label_id `{}` is invalid"".format(db_tag.label_id))\n\n            for attr in attributes:\n                db_attrval = models.LabeledImageAttributeVal(**attr)\n                if db_attrval.spec_id not in self.db_attributes[db_tag.label_id][""all""]:\n                    raise AttributeError(""spec_id `{}` is invalid"".format(db_attrval.spec_id))\n                db_attrval.tag_id = len(db_tags)\n                db_attrvals.append(db_attrval)\n\n            db_tags.append(db_tag)\n            tag[""attributes""] = attributes\n\n        db_tags = bulk_create(\n            db_model=models.LabeledImage,\n            objects=db_tags,\n            flt_param={""job_id"": self.db_job.id}\n        )\n\n        for db_attrval in db_attrvals:\n            db_attrval.image_id = db_tags[db_attrval.tag_id].id\n\n        bulk_create(\n            db_model=models.LabeledImageAttributeVal,\n            objects=db_attrvals,\n            flt_param={}\n        )\n\n        for tag, db_tag in zip(tags, db_tags):\n            tag[""id""] = db_tag.id\n\n        self.ir_data.tags = tags\n\n    def _commit(self):\n        db_prev_commit = self.db_job.commits.last()\n        db_curr_commit = models.JobCommit()\n        if db_prev_commit:\n            db_curr_commit.version = db_prev_commit.version + 1\n        else:\n            db_curr_commit.version = 1\n        db_curr_commit.job = self.db_job\n        db_curr_commit.message = ""Changes: tags - {}; shapes - {}; tracks - {}"".format(\n            len(self.ir_data.tags), len(self.ir_data.shapes), len(self.ir_data.tracks))\n        db_curr_commit.save()\n        self.ir_data.version = db_curr_commit.version\n\n    def _set_updated_date(self):\n        db_task = self.db_job.segment.task\n        db_task.updated_date = timezone.now()\n        db_task.save()\n\n    def _save_to_db(self, data):\n        self.reset()\n        self._save_tags_to_db(data[""tags""])\n        self._save_shapes_to_db(data[""shapes""])\n        self._save_tracks_to_db(data[""tracks""])\n\n        return self.ir_data.tags or self.ir_data.shapes or self.ir_data.tracks\n\n    def _create(self, data):\n        if self._save_to_db(data):\n            self._set_updated_date()\n            self.db_job.save()\n\n    def create(self, data):\n        self._create(data)\n        self._commit()\n\n    def put(self, data):\n        self._delete()\n        self._create(data)\n        self._commit()\n\n    def update(self, data):\n        self._delete(data)\n        self._create(data)\n        self._commit()\n\n    def _delete(self, data=None):\n        deleted_shapes = 0\n        if data is None:\n            deleted_shapes += self.db_job.labeledimage_set.all().delete()[0]\n            deleted_shapes += self.db_job.labeledshape_set.all().delete()[0]\n            deleted_shapes += self.db_job.labeledtrack_set.all().delete()[0]\n        else:\n            labeledimage_ids = [image[""id""] for image in data[""tags""]]\n            labeledshape_ids = [shape[""id""] for shape in data[""shapes""]]\n            labeledtrack_ids = [track[""id""] for track in data[""tracks""]]\n            labeledimage_set = self.db_job.labeledimage_set\n            labeledimage_set = labeledimage_set.filter(pk__in=labeledimage_ids)\n            labeledshape_set = self.db_job.labeledshape_set\n            labeledshape_set = labeledshape_set.filter(pk__in=labeledshape_ids)\n            labeledtrack_set = self.db_job.labeledtrack_set\n            labeledtrack_set = labeledtrack_set.filter(pk__in=labeledtrack_ids)\n\n            # It is not important for us that data had some ""invalid"" objects\n            # which were skipped (not acutally deleted). The main idea is to\n            # say that all requested objects are absent in DB after the method.\n            self.ir_data.tags = data[\'tags\']\n            self.ir_data.shapes = data[\'shapes\']\n            self.ir_data.tracks = data[\'tracks\']\n\n            deleted_shapes += labeledimage_set.delete()[0]\n            deleted_shapes += labeledshape_set.delete()[0]\n            deleted_shapes += labeledtrack_set.delete()[0]\n\n        if deleted_shapes:\n            self._set_updated_date()\n\n    def delete(self, data=None):\n        self._delete(data)\n        self._commit()\n\n    @staticmethod\n    def _extend_attributes(attributeval_set, default_attribute_values):\n        shape_attribute_specs_set = set(attr.spec_id for attr in attributeval_set)\n        for db_attr in default_attribute_values:\n            if db_attr.spec_id not in shape_attribute_specs_set:\n                attributeval_set.append(dotdict([\n                    (\'spec_id\', db_attr.spec_id),\n                    (\'value\', db_attr.value),\n                ]))\n\n    def _init_tags_from_db(self):\n        db_tags = self.db_job.labeledimage_set.prefetch_related(\n            ""label"",\n            ""labeledimageattributeval_set""\n        ).values(\n            \'id\',\n            \'frame\',\n            \'label_id\',\n            \'group\',\n            \'labeledimageattributeval__spec_id\',\n            \'labeledimageattributeval__value\',\n            \'labeledimageattributeval__id\',\n        ).order_by(\'frame\')\n\n        db_tags = _merge_table_rows(\n            rows=db_tags,\n            keys_for_merge={\n                ""labeledimageattributeval_set"": [\n                    \'labeledimageattributeval__spec_id\',\n                    \'labeledimageattributeval__value\',\n                    \'labeledimageattributeval__id\',\n                ],\n            },\n            field_id=\'id\',\n        )\n\n        for db_tag in db_tags:\n            self._extend_attributes(db_tag.labeledimageattributeval_set,\n                self.db_attributes[db_tag.label_id][""all""].values())\n\n        serializer = serializers.LabeledImageSerializer(db_tags, many=True)\n        self.ir_data.tags = serializer.data\n\n    def _init_shapes_from_db(self):\n        db_shapes = self.db_job.labeledshape_set.prefetch_related(\n            ""label"",\n            ""labeledshapeattributeval_set""\n        ).values(\n            \'id\',\n            \'label_id\',\n            \'type\',\n            \'frame\',\n            \'group\',\n            \'occluded\',\n            \'z_order\',\n            \'points\',\n            \'labeledshapeattributeval__spec_id\',\n            \'labeledshapeattributeval__value\',\n            \'labeledshapeattributeval__id\',\n            ).order_by(\'frame\')\n\n        db_shapes = _merge_table_rows(\n            rows=db_shapes,\n            keys_for_merge={\n                \'labeledshapeattributeval_set\': [\n                    \'labeledshapeattributeval__spec_id\',\n                    \'labeledshapeattributeval__value\',\n                    \'labeledshapeattributeval__id\',\n                ],\n            },\n            field_id=\'id\',\n        )\n        for db_shape in db_shapes:\n            self._extend_attributes(db_shape.labeledshapeattributeval_set,\n                self.db_attributes[db_shape.label_id][""all""].values())\n\n        serializer = serializers.LabeledShapeSerializer(db_shapes, many=True)\n        self.ir_data.shapes = serializer.data\n\n    def _init_tracks_from_db(self):\n        db_tracks = self.db_job.labeledtrack_set.prefetch_related(\n            ""label"",\n            ""labeledtrackattributeval_set"",\n            ""trackedshape_set__trackedshapeattributeval_set""\n        ).values(\n            ""id"",\n            ""frame"",\n            ""label_id"",\n            ""group"",\n            ""labeledtrackattributeval__spec_id"",\n            ""labeledtrackattributeval__value"",\n            ""labeledtrackattributeval__id"",\n            ""trackedshape__type"",\n            ""trackedshape__occluded"",\n            ""trackedshape__z_order"",\n            ""trackedshape__points"",\n            ""trackedshape__id"",\n            ""trackedshape__frame"",\n            ""trackedshape__outside"",\n            ""trackedshape__trackedshapeattributeval__spec_id"",\n            ""trackedshape__trackedshapeattributeval__value"",\n            ""trackedshape__trackedshapeattributeval__id"",\n        ).order_by(\'id\', \'trackedshape__frame\')\n\n        db_tracks = _merge_table_rows(\n            rows=db_tracks,\n            keys_for_merge={\n                ""labeledtrackattributeval_set"": [\n                    ""labeledtrackattributeval__spec_id"",\n                    ""labeledtrackattributeval__value"",\n                    ""labeledtrackattributeval__id"",\n                ],\n                ""trackedshape_set"":[\n                    ""trackedshape__type"",\n                    ""trackedshape__occluded"",\n                    ""trackedshape__z_order"",\n                    ""trackedshape__points"",\n                    ""trackedshape__id"",\n                    ""trackedshape__frame"",\n                    ""trackedshape__outside"",\n                    ""trackedshape__trackedshapeattributeval__spec_id"",\n                    ""trackedshape__trackedshapeattributeval__value"",\n                    ""trackedshape__trackedshapeattributeval__id"",\n                ],\n            },\n            field_id=""id"",\n        )\n\n        for db_track in db_tracks:\n            db_track[""trackedshape_set""] = _merge_table_rows(db_track[""trackedshape_set""], {\n                \'trackedshapeattributeval_set\': [\n                    \'trackedshapeattributeval__value\',\n                    \'trackedshapeattributeval__spec_id\',\n                    \'trackedshapeattributeval__id\',\n                ]\n            }, \'id\')\n\n            # A result table can consist many equal rows for track/shape attributes\n            # We need filter unique attributes manually\n            db_track[""labeledtrackattributeval_set""] = list(set(db_track[""labeledtrackattributeval_set""]))\n            self._extend_attributes(db_track.labeledtrackattributeval_set,\n                self.db_attributes[db_track.label_id][""immutable""].values())\n\n            default_attribute_values = self.db_attributes[db_track.label_id][""mutable""].values()\n            for db_shape in db_track[""trackedshape_set""]:\n                db_shape[""trackedshapeattributeval_set""] = list(\n                    set(db_shape[""trackedshapeattributeval_set""])\n                )\n                # in case of trackedshapes need to interpolate attriute values and extend it\n                # by previous shape attribute values (not default values)\n                self._extend_attributes(db_shape[""trackedshapeattributeval_set""], default_attribute_values)\n                default_attribute_values = db_shape[""trackedshapeattributeval_set""]\n\n\n        serializer = serializers.LabeledTrackSerializer(db_tracks, many=True)\n        self.ir_data.tracks = serializer.data\n\n    def _init_version_from_db(self):\n        db_commit = self.db_job.commits.last()\n        self.ir_data.version = db_commit.version if db_commit else 0\n\n    def init_from_db(self):\n        self._init_tags_from_db()\n        self._init_shapes_from_db()\n        self._init_tracks_from_db()\n        self._init_version_from_db()\n\n    @property\n    def data(self):\n        return self.ir_data.data\n\n    def import_annotations(self, src_file, importer):\n        task_data = TaskData(\n            annotation_ir=AnnotationIR(),\n            db_task=self.db_job.segment.task,\n            create_callback=self.create,\n        )\n        self.delete()\n\n        importer(src_file, task_data)\n\n        self.create(task_data.data.slice(self.start_frame, self.stop_frame).serialize())\n\nclass TaskAnnotation:\n    def __init__(self, pk):\n        self.db_task = models.Task.objects.prefetch_related(""data__images"").get(id=pk)\n\n        # Postgres doesn\'t guarantee an order by default without explicit order_by\n        self.db_jobs = models.Job.objects.select_related(""segment"").filter(segment__task_id=pk).order_by(\'id\')\n        self.ir_data = AnnotationIR()\n\n    def reset(self):\n        self.ir_data.reset()\n\n    def _patch_data(self, data, action):\n        _data = data if isinstance(data, AnnotationIR) else AnnotationIR(data)\n        splitted_data = {}\n        jobs = {}\n        for db_job in self.db_jobs:\n            jid = db_job.id\n            start = db_job.segment.start_frame\n            stop = db_job.segment.stop_frame\n            jobs[jid] = { ""start"": start, ""stop"": stop }\n            splitted_data[jid] = _data.slice(start, stop)\n\n        for jid, job_data in splitted_data.items():\n            _data = AnnotationIR()\n            if action is None:\n                _data.data = put_job_data(jid, job_data)\n            else:\n                _data.data = patch_job_data(jid, job_data, action)\n            if _data.version > self.ir_data.version:\n                self.ir_data.version = _data.version\n            self._merge_data(_data, jobs[jid][""start""], self.db_task.overlap)\n\n    def _merge_data(self, data, start_frame, overlap):\n        annotation_manager = AnnotationManager(self.ir_data)\n        annotation_manager.merge(data, start_frame, overlap)\n\n    def put(self, data):\n        self._patch_data(data, None)\n\n    def create(self, data):\n        self._patch_data(data, PatchAction.CREATE)\n\n    def update(self, data):\n        self._patch_data(data, PatchAction.UPDATE)\n\n    def delete(self, data=None):\n        if data:\n            self._patch_data(data, PatchAction.DELETE)\n        else:\n            for db_job in self.db_jobs:\n                delete_job_data(db_job.id)\n\n    def init_from_db(self):\n        self.reset()\n\n        for db_job in self.db_jobs:\n            annotation = JobAnnotation(db_job.id)\n            annotation.init_from_db()\n            if annotation.ir_data.version > self.ir_data.version:\n                self.ir_data.version = annotation.ir_data.version\n            db_segment = db_job.segment\n            start_frame = db_segment.start_frame\n            overlap = self.db_task.overlap\n            self._merge_data(annotation.ir_data, start_frame, overlap)\n\n    def export(self, dst_file, exporter, host=\'\', **options):\n        task_data = TaskData(\n            annotation_ir=self.ir_data,\n            db_task=self.db_task,\n            host=host,\n        )\n        exporter(dst_file, task_data, **options)\n\n    def import_annotations(self, src_file, importer, **options):\n        task_data = TaskData(\n            annotation_ir=AnnotationIR(),\n            db_task=self.db_task,\n            create_callback=self.create,\n        )\n        self.delete()\n\n        importer(src_file, task_data, **options)\n\n        self.create(task_data.data.serialize())\n\n    @property\n    def data(self):\n        return self.ir_data.data\n\n\n@silk_profile(name=""GET job data"")\n@transaction.atomic\ndef get_job_data(pk):\n    annotation = JobAnnotation(pk)\n    annotation.init_from_db()\n\n    return annotation.data\n\n@silk_profile(name=""POST job data"")\n@transaction.atomic\ndef put_job_data(pk, data):\n    annotation = JobAnnotation(pk)\n    annotation.put(data)\n\n    return annotation.data\n\n@silk_profile(name=""UPDATE job data"")\n@plugin_decorator\n@transaction.atomic\ndef patch_job_data(pk, data, action):\n    annotation = JobAnnotation(pk)\n    if action == PatchAction.CREATE:\n        annotation.create(data)\n    elif action == PatchAction.UPDATE:\n        annotation.update(data)\n    elif action == PatchAction.DELETE:\n        annotation.delete(data)\n\n    return annotation.data\n\n@silk_profile(name=""DELETE job data"")\n@transaction.atomic\ndef delete_job_data(pk):\n    annotation = JobAnnotation(pk)\n    annotation.delete()\n\n@silk_profile(name=""GET task data"")\n@transaction.atomic\ndef get_task_data(pk):\n    annotation = TaskAnnotation(pk)\n    annotation.init_from_db()\n\n    return annotation.data\n\n@silk_profile(name=""POST task data"")\n@transaction.atomic\ndef put_task_data(pk, data):\n    annotation = TaskAnnotation(pk)\n    annotation.put(data)\n\n    return annotation.data\n\n@silk_profile(name=""UPDATE task data"")\n@transaction.atomic\ndef patch_task_data(pk, data, action):\n    annotation = TaskAnnotation(pk)\n    if action == PatchAction.CREATE:\n        annotation.create(data)\n    elif action == PatchAction.UPDATE:\n        annotation.update(data)\n    elif action == PatchAction.DELETE:\n        annotation.delete(data)\n\n    return annotation.data\n\n@silk_profile(name=""DELETE task data"")\n@transaction.atomic\ndef delete_task_data(pk):\n    annotation = TaskAnnotation(pk)\n    annotation.delete()\n\ndef export_task(task_id, dst_file, format_name,\n        server_url=None, save_images=False):\n    # For big tasks dump function may run for a long time and\n    # we dont need to acquire lock after the task has been initialized from DB.\n    # But there is the bug with corrupted dump file in case 2 or\n    # more dump request received at the same time:\n    # https://github.com/opencv/cvat/issues/217\n    with transaction.atomic():\n        task = TaskAnnotation(task_id)\n        task.init_from_db()\n\n    exporter = make_exporter(format_name)\n    with open(dst_file, \'wb\') as f:\n        task.export(f, exporter, host=server_url,\n            save_images=save_images)\n\n@transaction.atomic\ndef import_task_annotations(task_id, src_file, format_name):\n    task = TaskAnnotation(task_id)\n    task.init_from_db()\n\n    importer = make_importer(format_name)\n    with open(src_file, \'rb\') as f:\n        task.import_annotations(f, importer)\n\n@transaction.atomic\ndef import_job_annotations(job_id, src_file, format_name):\n    job = JobAnnotation(job_id)\n    job.init_from_db()\n\n    importer = make_importer(format_name)\n    with open(src_file, \'rb\') as f:\n        job.import_annotations(f, importer)\n'"
cvat/apps/dataset_manager/util.py,0,"b""\n# Copyright (C) 2019-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport inspect\nimport os, os.path as osp\nimport zipfile\n\n\ndef current_function_name(depth=1):\n    return inspect.getouterframes(inspect.currentframe())[depth].function\n\n\ndef make_zip_archive(src_path, dst_path):\n    with zipfile.ZipFile(dst_path, 'w') as archive:\n        for (dirpath, _, filenames) in os.walk(src_path):\n            for name in filenames:\n                path = osp.join(dirpath, name)\n                archive.write(path, osp.relpath(path, src_path))\n"""
cvat/apps/dataset_manager/views.py,0,"b'# Copyright (C) 2019-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport os.path as osp\nimport tempfile\nfrom datetime import timedelta\n\nimport django_rq\nfrom django.utils import timezone\n\nimport cvat.apps.dataset_manager.task as task\nfrom cvat.apps.engine.log import slogger\nfrom cvat.apps.engine.models import Task\nfrom datumaro.cli.util import make_file_name\nfrom datumaro.util import to_snake_case\n\nfrom .formats.registry import EXPORT_FORMATS, IMPORT_FORMATS\nfrom .util import current_function_name\n\n\n_MODULE_NAME = __package__ + \'.\' + osp.splitext(osp.basename(__file__))[0]\ndef log_exception(logger=None, exc_info=True):\n    if logger is None:\n        logger = slogger\n    logger.exception(""[%s @ %s]: exception occurred"" % \\\n            (_MODULE_NAME, current_function_name(2)),\n        exc_info=exc_info)\n\n\ndef get_export_cache_dir(db_task):\n    task_dir = osp.abspath(db_task.get_task_dirname())\n    if osp.isdir(task_dir):\n        return osp.join(task_dir, \'export_cache\')\n    else:\n        raise Exception(\'Task dir {} does not exist\'.format(task_dir))\n\nDEFAULT_CACHE_TTL = timedelta(hours=10)\nCACHE_TTL = DEFAULT_CACHE_TTL\n\n\ndef export_task(task_id, dst_format, server_url=None, save_images=False):\n    try:\n        db_task = Task.objects.get(pk=task_id)\n\n        cache_dir = get_export_cache_dir(db_task)\n\n        exporter = EXPORT_FORMATS[dst_format]\n        output_base = \'%s_%s\' % (\'dataset\' if save_images else \'annotations\',\n            make_file_name(to_snake_case(dst_format)))\n        output_path = \'%s.%s\' % (output_base, exporter.EXT)\n        output_path = osp.join(cache_dir, output_path)\n\n        task_time = timezone.localtime(db_task.updated_date).timestamp()\n        if not (osp.exists(output_path) and \\\n                task_time <= osp.getmtime(output_path)):\n            os.makedirs(cache_dir, exist_ok=True)\n            with tempfile.TemporaryDirectory(dir=cache_dir) as temp_dir:\n                temp_file = osp.join(temp_dir, \'result\')\n                task.export_task(task_id, temp_file, dst_format,\n                    server_url=server_url, save_images=save_images)\n                os.replace(temp_file, output_path)\n\n            archive_ctime = osp.getctime(output_path)\n            scheduler = django_rq.get_scheduler()\n            cleaning_job = scheduler.enqueue_in(time_delta=CACHE_TTL,\n                func=clear_export_cache,\n                task_id=task_id,\n                file_path=output_path, file_ctime=archive_ctime)\n            slogger.task[task_id].info(\n                ""The task \'{}\' is exported as \'{}\' at \'{}\' ""\n                ""and available for downloading for the next {}. ""\n                ""Export cache cleaning job is enqueued, id \'{}\'"".format(\n                db_task.name, dst_format, output_path, CACHE_TTL,\n                cleaning_job.id))\n\n        return output_path\n    except Exception:\n        log_exception(slogger.task[task_id])\n        raise\n\ndef export_task_as_dataset(task_id, dst_format=None, server_url=None):\n    return export_task(task_id, dst_format, server_url=server_url, save_images=True)\n\ndef export_task_annotations(task_id, dst_format=None, server_url=None):\n    return export_task(task_id, dst_format, server_url=server_url, save_images=False)\n\ndef clear_export_cache(task_id, file_path, file_ctime):\n    try:\n        if osp.exists(file_path) and osp.getctime(file_path) == file_ctime:\n            os.remove(file_path)\n            slogger.task[task_id].info(\n                ""Export cache file \'{}\' successfully removed"" \\\n                .format(file_path))\n    except Exception:\n        log_exception(slogger.task[task_id])\n        raise\n\n\ndef get_export_formats():\n    return list(EXPORT_FORMATS.values())\n\ndef get_import_formats():\n    return list(IMPORT_FORMATS.values())\n\ndef get_all_formats():\n    return {\n        \'importers\': get_import_formats(),\n        \'exporters\': get_export_formats(),\n    }'"
cvat/apps/dextr_segmentation/__init__.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom cvat.settings.base import JS_3RDPARTY\n\nJS_3RDPARTY['engine'] = JS_3RDPARTY.get('engine', []) + ['dextr_segmentation/js/enginePlugin.js']\n"""
cvat/apps/dextr_segmentation/apps.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\nclass DextrSegmentationConfig(AppConfig):\n    name = 'dextr_segmentation'\n"""
cvat/apps/dextr_segmentation/dextr.py,0,"b'\n# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom cvat.apps.auto_annotation.inference_engine import make_plugin_or_core, make_network\nfrom cvat.apps.engine.frame_provider import FrameProvider\n\nimport os\nimport cv2\nimport PIL\nimport numpy as np\n\n_IE_CPU_EXTENSION = os.getenv(""IE_CPU_EXTENSION"", ""libcpu_extension_avx2.so"")\n_IE_PLUGINS_PATH = os.getenv(""IE_PLUGINS_PATH"", None)\n\n_DEXTR_MODEL_DIR = os.getenv(""DEXTR_MODEL_DIR"", None)\n_DEXTR_PADDING = 50\n_DEXTR_TRESHOLD = 0.9\n_DEXTR_SIZE = 512\n\nclass DEXTR_HANDLER:\n    def __init__(self):\n        self._plugin = None\n        self._network = None\n        self._exec_network = None\n        self._input_blob = None\n        self._output_blob = None\n        if not _DEXTR_MODEL_DIR:\n            raise Exception(""DEXTR_MODEL_DIR is not defined"")\n\n\n    def handle(self, db_data, frame, points):\n        # Lazy initialization\n        if not self._plugin:\n            self._plugin = make_plugin_or_core()\n            self._network = make_network(os.path.join(_DEXTR_MODEL_DIR, \'dextr.xml\'),\n                os.path.join(_DEXTR_MODEL_DIR, \'dextr.bin\'))\n            self._input_blob = next(iter(self._network.inputs))\n            self._output_blob = next(iter(self._network.outputs))\n            if getattr(self._plugin, \'load_network\', False):\n                self._exec_network = self._plugin.load_network(self._network, \'CPU\')\n            else:\n                self._exec_network = self._plugin.load(network=self._network)\n\n        frame_provider = FrameProvider(db_data)\n        image = frame_provider.get_frame(frame, frame_provider.Quality.ORIGINAL)\n        image = PIL.Image.open(image[0])\n        numpy_image = np.array(image)\n        points = np.asarray([[int(p[""x""]), int(p[""y""])] for p in points], dtype=int)\n\n        # Padding mustn\'t be more than the closest distance to an edge of an image\n        [height, width] = numpy_image.shape[:2]\n        x_values = points[:, 0]\n        y_values = points[:, 1]\n        [min_x, max_x] = [np.min(x_values), np.max(x_values)]\n        [min_y, max_y] = [np.min(y_values), np.max(y_values)]\n        padding = min(min_x, min_y, width - max_x, height - max_y, _DEXTR_PADDING)\n        bounding_box = (\n            max(min(points[:, 0]) - padding, 0),\n            max(min(points[:, 1]) - padding, 0),\n            min(max(points[:, 0]) + padding, width - 1),\n            min(max(points[:, 1]) + padding, height - 1)\n        )\n\n        # Prepare an image\n        numpy_cropped = np.array(image.crop(bounding_box))\n        resized = cv2.resize(numpy_cropped, (_DEXTR_SIZE, _DEXTR_SIZE),\n            interpolation = cv2.INTER_CUBIC).astype(np.float32)\n\n        # Make a heatmap\n        points = points - [min(points[:, 0]), min(points[:, 1])] + [padding, padding]\n        points = (points * [_DEXTR_SIZE / numpy_cropped.shape[1], _DEXTR_SIZE / numpy_cropped.shape[0]]).astype(int)\n        heatmap = np.zeros(shape=resized.shape[:2], dtype=np.float64)\n        for point in points:\n            gaussian_x_axis = np.arange(0, _DEXTR_SIZE, 1, float) - point[0]\n            gaussian_y_axis = np.arange(0, _DEXTR_SIZE, 1, float)[:, np.newaxis] - point[1]\n            gaussian = np.exp(-4 * np.log(2) * ((gaussian_x_axis ** 2 + gaussian_y_axis ** 2) / 100)).astype(np.float64)\n            heatmap = np.maximum(heatmap, gaussian)\n        cv2.normalize(heatmap,  heatmap, 0, 255, cv2.NORM_MINMAX)\n\n        # Concat an image and a heatmap\n        input_dextr = np.concatenate((resized, heatmap[:, :, np.newaxis].astype(resized.dtype)), axis=2)\n        input_dextr = input_dextr.transpose((2,0,1))\n\n        pred = self._exec_network.infer(inputs={self._input_blob: input_dextr[np.newaxis, ...]})[self._output_blob][0, 0, :, :]\n        pred = cv2.resize(pred, tuple(reversed(numpy_cropped.shape[:2])), interpolation = cv2.INTER_CUBIC)\n        result = np.zeros(numpy_image.shape[:2])\n        result[bounding_box[1]:bounding_box[1] + pred.shape[0], bounding_box[0]:bounding_box[0] + pred.shape[1]] = pred > _DEXTR_TRESHOLD\n\n        # Convert a mask to a polygon\n        result = np.array(result, dtype=np.uint8)\n        cv2.normalize(result,result,0,255,cv2.NORM_MINMAX)\n        contours = None\n        if int(cv2.__version__.split(\'.\')[0]) > 3:\n            contours = cv2.findContours(result, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)[0]\n        else:\n            contours = cv2.findContours(result, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)[1]\n\n        contours = max(contours, key=lambda arr: arr.size)\n        if contours.shape.count(1):\n            contours = np.squeeze(contours)\n        if contours.size < 3 * 2:\n            raise Exception(\'Less then three point have been detected. Can not build a polygon.\')\n\n        result = """"\n        for point in contours:\n            result += ""{},{} "".format(int(point[0]), int(point[1]))\n        result = result[:-1]\n\n        return result\n\n    def __del__(self):\n        if self._exec_network:\n            del self._exec_network\n        if self._network:\n            del self._network\n        if self._plugin:\n            del self._plugin\n'"
cvat/apps/dextr_segmentation/urls.py,0,"b""# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('create/<int:jid>', views.create),\n    path('cancel/<int:jid>', views.cancel),\n    path('check/<int:jid>', views.check),\n    path('enabled', views.enabled)\n]\n"""
cvat/apps/dextr_segmentation/views.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.http import HttpResponse, HttpResponseBadRequest, JsonResponse\nfrom cvat.apps.authentication.decorators import login_required\nfrom rules.contrib.views import permission_required, objectgetter\n\nfrom cvat.apps.engine.models import Job\nfrom cvat.apps.engine.log import slogger\nfrom cvat.apps.dextr_segmentation.dextr import DEXTR_HANDLER\n\nimport django_rq\nimport json\nimport rq\n\n__RQ_QUEUE_NAME = ""default""\n__DEXTR_HANDLER = DEXTR_HANDLER()\n\ndef _dextr_thread(db_data, frame, points):\n    job = rq.get_current_job()\n    job.meta[""result""] = __DEXTR_HANDLER.handle(db_data, frame, points)\n    job.save_meta()\n\n\n@login_required\n@permission_required(perm=[""engine.job.change""],\n    fn=objectgetter(Job, ""jid""), raise_exception=True)\ndef create(request, jid):\n    try:\n        data = json.loads(request.body.decode(""utf-8""))\n\n        points = data[""points""]\n        frame = int(data[""frame""])\n        username = request.user.username\n\n        slogger.job[jid].info(""create dextr request for the JOB: {} "".format(jid)\n            + ""by the USER: {} on the FRAME: {}"".format(username, frame))\n\n        db_data = Job.objects.select_related(""segment__task__data"").get(id=jid).segment.task.data\n\n        queue = django_rq.get_queue(__RQ_QUEUE_NAME)\n        rq_id = ""dextr.create/{}/{}"".format(jid, username)\n        job = queue.fetch_job(rq_id)\n\n        if job is not None and (job.is_started or job.is_queued):\n            if ""cancel"" not in job.meta:\n                raise Exception(""Segmentation process has been already run for the "" +\n                    ""JOB: {} and the USER: {}"".format(jid, username))\n            else:\n                job.delete()\n\n        queue.enqueue_call(func=_dextr_thread,\n            args=(db_data, frame, points),\n            job_id=rq_id,\n            timeout=15,\n            ttl=30)\n\n        return HttpResponse()\n    except Exception as ex:\n        slogger.job[jid].error(""can\'t create a dextr request for the job {}"".format(jid), exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\n\n@login_required\n@permission_required(perm=[""engine.job.change""],\n    fn=objectgetter(Job, ""jid""), raise_exception=True)\ndef cancel(request, jid):\n    try:\n        username = request.user.username\n        slogger.job[jid].info(""cancel dextr request for the JOB: {} "".format(jid)\n            + ""by the USER: {}"".format(username))\n\n        queue = django_rq.get_queue(__RQ_QUEUE_NAME)\n        rq_id = ""dextr.create/{}/{}"".format(jid, username)\n        job = queue.fetch_job(rq_id)\n\n        if job is None or job.is_finished or job.is_failed:\n            raise Exception(""Segmentation isn\'t running now"")\n        elif ""cancel"" not in job.meta:\n            job.meta[""cancel""] = True\n            job.save_meta()\n\n        return HttpResponse()\n    except Exception as ex:\n        slogger.job[jid].error(""can\'t cancel a dextr request for the job {}"".format(jid), exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\n\n@login_required\n@permission_required(perm=[""engine.job.change""],\n    fn=objectgetter(Job, ""jid""), raise_exception=True)\ndef check(request, jid):\n    try:\n        username = request.user.username\n        slogger.job[jid].info(""check dextr request for the JOB: {} "".format(jid)\n            + ""by the USER: {}"".format(username))\n\n        queue = django_rq.get_queue(__RQ_QUEUE_NAME)\n        rq_id = ""dextr.create/{}/{}"".format(jid, username)\n        job = queue.fetch_job(rq_id)\n        data = {}\n\n        if job is None:\n            data[""status""] = ""unknown""\n        else:\n            if ""cancel"" in job.meta:\n                data[""status""] = ""finished""\n            elif job.is_queued:\n                data[""status""] = ""queued""\n            elif job.is_started:\n                data[""status""] = ""started""\n            elif job.is_finished:\n                data[""status""] = ""finished""\n                data[""result""] = job.meta[""result""]\n                job.delete()\n            else:\n                data[""status""] = ""failed""\n                data[""stderr""] = job.exc_info\n                job.delete()\n\n        return JsonResponse(data)\n    except Exception as ex:\n        slogger.job[jid].error(""can\'t check a dextr request for the job {}"".format(jid), exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\ndef enabled(request):\n    return HttpResponse()\n'"
cvat/apps/documentation/__init__.py,0,b'\n# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/documentation/admin.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.contrib import admin\n\n# Register your models here.\n\n'
cvat/apps/documentation/apps.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\n\nclass DocumentationConfig(AppConfig):\n    name = 'cvat.apps.documentation'\n\n"""
cvat/apps/documentation/models.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.db import models\n\n# Create your models here.\n\n'
cvat/apps/documentation/tests.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.test import TestCase\n\n# Create your tests here.\n\n'
cvat/apps/documentation/urls.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('user_guide.html', views.UserGuideView),\n    path('xml_format.html', views.XmlFormatView),\n]\n\n"""
cvat/apps/documentation/views.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.shortcuts import render\nimport os\n\ndef UserGuideView(request):\n    module_dir = os.path.dirname(__file__)\n    doc_path = os.path.join(module_dir, \'user_guide.md\')\n\n    return render(request, \'documentation/user_guide.html\',\n        context={""user_guide"": open(doc_path, ""r"").read()})\n\ndef XmlFormatView(request):\n    module_dir = os.path.dirname(__file__)\n    doc_path = os.path.join(module_dir, \'xml_format.md\')\n\n    return render(request, \'documentation/xml_format.html\',\n        context={""xml_format"": open(doc_path, ""r"").read()})\n'"
cvat/apps/engine/__init__.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\ndefault_app_config = 'cvat.apps.engine.apps.EngineConfig'\n"""
cvat/apps/engine/admin.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.contrib import admin\nfrom .models import Task, Segment, Job, Label, AttributeSpec\n\nclass JobInline(admin.TabularInline):\n    model = Job\n    can_delete = False\n\n    # Don't show extra lines to add an object\n    def has_add_permission(self, request, object=None):\n        return False\n\nclass SegmentInline(admin.TabularInline):\n    model = Segment\n    show_change_link = True\n    readonly_fields = ('start_frame', 'stop_frame')\n    can_delete = False\n\n    # Don't show extra lines to add an object\n    def has_add_permission(self, request, object=None):\n        return False\n\n\nclass AttributeSpecInline(admin.TabularInline):\n    model = AttributeSpec\n    extra = 0\n    max_num = None\n\nclass LabelInline(admin.TabularInline):\n    model = Label\n    show_change_link = True\n    extra = 0\n    max_num = None\n\nclass LabelAdmin(admin.ModelAdmin):\n    # Don't show on admin index page\n    def has_module_permission(self, request):\n        return False\n\n    inlines = [\n        AttributeSpecInline\n    ]\n\nclass SegmentAdmin(admin.ModelAdmin):\n    # Don't show on admin index page\n    def has_module_permission(self, request):\n        return False\n\n    inlines = [\n        JobInline\n    ]\n\nclass TaskAdmin(admin.ModelAdmin):\n    date_hierarchy = 'updated_date'\n    readonly_fields = ('created_date', 'updated_date', 'overlap')\n    list_display = ('name', 'mode', 'owner', 'assignee', 'created_date', 'updated_date')\n    search_fields = ('name', 'mode', 'owner__username', 'owner__first_name',\n        'owner__last_name', 'owner__email', 'assignee__username', 'assignee__first_name',\n        'assignee__last_name')\n    inlines = [\n        SegmentInline,\n        LabelInline\n    ]\n\n    # Don't allow to add a task because it isn't trivial operation\n    def has_add_permission(self, request):\n        return False\n\n\nadmin.site.register(Task, TaskAdmin)\nadmin.site.register(Segment, SegmentAdmin)\nadmin.site.register(Label, LabelAdmin)\n"""
cvat/apps/engine/apps.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\nclass EngineConfig(AppConfig):\n    name = \'cvat.apps.engine\'\n\n    def ready(self):\n        from django.db.models.signals import post_save\n        from .signals import update_task_status\n\n        post_save.connect(update_task_status, sender=\'engine.Job\',\n            dispatch_uid=""update_task_status"")\n'"
cvat/apps/engine/frame_provider.py,0,"b""# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport math\nfrom enum import Enum\nfrom io import BytesIO\n\nimport numpy as np\nfrom PIL import Image\n\nfrom cvat.apps.engine.media_extractors import VideoReader, ZipReader\nfrom cvat.apps.engine.mime_types import mimetypes\nfrom cvat.apps.engine.models import DataChoice\n\n\nclass RandomAccessIterator:\n    def __init__(self, iterable):\n        self.iterable = iterable\n        self.iterator = None\n        self.pos = -1\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self[self.pos + 1]\n\n    def __getitem__(self, idx):\n        assert 0 <= idx\n        if self.iterator is None or idx <= self.pos:\n            self.reset()\n        v = None\n        while self.pos < idx:\n            # NOTE: don't keep the last item in self, it can be expensive\n            v = next(self.iterator)\n            self.pos += 1\n        return v\n\n    def reset(self):\n        self.iterator = iter(self.iterable)\n        self.pos = -1\n\nclass FrameProvider:\n    class Quality(Enum):\n        COMPRESSED = 0\n        ORIGINAL = 100\n\n    class Type(Enum):\n        BUFFER = 0\n        PIL = 1\n        NUMPY_ARRAY = 2\n\n    class ChunkLoader:\n        def __init__(self, reader_class, path_getter):\n            self.chunk_id = None\n            self.chunk_reader = None\n            self.reader_class = reader_class\n            self.get_chunk_path = path_getter\n\n        def load(self, chunk_id):\n            if self.chunk_id != chunk_id:\n                self.chunk_id = chunk_id\n                self.chunk_reader = RandomAccessIterator(\n                    self.reader_class([self.get_chunk_path(chunk_id)]))\n            return self.chunk_reader\n\n    def __init__(self, db_data):\n        self._db_data = db_data\n        self._loaders = {}\n\n        reader_class = {\n            DataChoice.IMAGESET: ZipReader,\n            DataChoice.VIDEO: VideoReader,\n        }\n        self._loaders[self.Quality.COMPRESSED] = self.ChunkLoader(\n            reader_class[db_data.compressed_chunk_type],\n            db_data.get_compressed_chunk_path)\n        self._loaders[self.Quality.ORIGINAL] = self.ChunkLoader(\n            reader_class[db_data.original_chunk_type],\n            db_data.get_original_chunk_path)\n\n    def __len__(self):\n        return self._db_data.size\n\n    def _validate_frame_number(self, frame_number):\n        frame_number_ = int(frame_number)\n        if frame_number_ < 0 or frame_number_ >= self._db_data.size:\n            raise Exception('Incorrect requested frame number: {}'.format(frame_number_))\n\n        chunk_number = frame_number_ // self._db_data.chunk_size\n        frame_offset = frame_number_ % self._db_data.chunk_size\n\n        return frame_number_, chunk_number, frame_offset\n\n    def _validate_chunk_number(self, chunk_number):\n        chunk_number_ = int(chunk_number)\n        if chunk_number_ < 0 or chunk_number_ >= math.ceil(self._db_data.size / self._db_data.chunk_size):\n            raise Exception('requested chunk does not exist')\n\n        return chunk_number_\n\n    @staticmethod\n    def _av_frame_to_png_bytes(av_frame):\n        pil_img = av_frame.to_image()\n        buf = BytesIO()\n        pil_img.save(buf, format='PNG')\n        buf.seek(0)\n        return buf\n\n    def _convert_frame(self, frame, reader_class, out_type):\n        if out_type == self.Type.BUFFER:\n            return self._av_frame_to_png_bytes(frame) if reader_class is VideoReader else frame\n        elif out_type == self.Type.PIL:\n            return frame.to_image() if reader_class is VideoReader else Image.open(frame)\n        elif out_type == self.Type.NUMPY_ARRAY:\n            if reader_class is VideoReader:\n                image = np.array(frame.to_image())\n            else:\n                image = np.array(Image.open(frame))\n            if len(image.shape) == 3 and image.shape[2] in {3, 4}:\n                image[:, :, :3] = image[:, :, 2::-1] # RGB to BGR\n            return image\n        else:\n            raise Exception('unsupported output type')\n\n    def get_preview(self):\n        return self._db_data.get_preview_path()\n\n    def get_chunk(self, chunk_number, quality=Quality.ORIGINAL):\n        chunk_number = self._validate_chunk_number(chunk_number)\n        return self._loaders[quality].get_chunk_path(chunk_number)\n\n    def get_frame(self, frame_number, quality=Quality.ORIGINAL,\n            out_type=Type.BUFFER):\n        _, chunk_number, frame_offset = self._validate_frame_number(frame_number)\n        loader = self._loaders[quality]\n        chunk_reader = loader.load(chunk_number)\n        frame, frame_name, _ = chunk_reader[frame_offset]\n\n        frame = self._convert_frame(frame, loader.reader_class, out_type)\n        if loader.reader_class is VideoReader:\n            return (frame, 'image/png')\n        return (frame, mimetypes.guess_type(frame_name))\n\n    def get_frames(self, quality=Quality.ORIGINAL, out_type=Type.BUFFER):\n        for idx in range(self._db_data.size):\n            yield self.get_frame(idx, quality=quality, out_type=out_type)\n"""
cvat/apps/engine/log.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport logging\nfrom cvat.settings.base import LOGGING\nfrom .models import Job, Task\n\ndef _get_task(tid):\n    try:\n        return Task.objects.get(pk=tid)\n    except Exception:\n        raise Exception(\'{} key must be a task identifier\'.format(tid))\n\ndef _get_job(jid):\n    try:\n        return Job.objects.select_related(""segment__task"").get(id=jid)\n    except Exception:\n        raise Exception(\'{} key must be a job identifier\'.format(jid))\n\nclass TaskLoggerStorage:\n    def __init__(self):\n        self._storage = dict()\n\n    def __getitem__(self, tid):\n        if tid not in self._storage:\n            self._storage[tid] = self._create_task_logger(tid)\n        return self._storage[tid]\n\n    def _create_task_logger(self, tid):\n        task = _get_task(tid)\n\n        logger = logging.getLogger(\'cvat.server.task_{}\'.format(tid))\n        server_file = logging.FileHandler(filename=task.get_log_path())\n        formatter = logging.Formatter(LOGGING[\'formatters\'][\'standard\'][\'format\'])\n        server_file.setFormatter(formatter)\n        logger.addHandler(server_file)\n\n        return logger\n\nclass JobLoggerStorage:\n    def __init__(self):\n        self._storage = dict()\n\n    def __getitem__(self, jid):\n        if jid not in self._storage:\n            self._storage[jid] = self._get_task_logger(jid)\n        return self._storage[jid]\n\n    def _get_task_logger(self, jid):\n        job = _get_job(jid)\n        return slogger.task[job.segment.task.id]\n\nclass TaskClientLoggerStorage:\n    def __init__(self):\n        self._storage = dict()\n\n    def __getitem__(self, tid):\n        if tid not in self._storage:\n            self._storage[tid] = self._create_client_logger(tid)\n        return self._storage[tid]\n\n    def _create_client_logger(self, tid):\n        task = _get_task(tid)\n        logger = logging.getLogger(\'cvat.client.task_{}\'.format(tid))\n        client_file = logging.FileHandler(filename=task.get_client_log_path())\n        logger.addHandler(client_file)\n\n        return logger\n\nclass JobClientLoggerStorage:\n    def __init__(self):\n        self._storage = dict()\n\n    def __getitem__(self, jid):\n        if jid not in self._storage:\n            self._storage[jid] = self._get_task_logger(jid)\n        return self._storage[jid]\n\n    def _get_task_logger(self, jid):\n        job = _get_job(jid)\n        return clogger.task[job.segment.task.id]\n\nclass dotdict(dict):\n    """"""dot.notation access to dictionary attributes""""""\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\nclogger = dotdict({\n    \'task\': TaskClientLoggerStorage(),\n    \'job\': JobClientLoggerStorage(),\n    \'glob\': logging.getLogger(\'cvat.client\'),\n})\n\nslogger = dotdict({\n    \'task\': TaskLoggerStorage(),\n    \'job\': JobLoggerStorage(),\n    \'glob\': logging.getLogger(\'cvat.server\'),\n})\n'"
cvat/apps/engine/media_extractors.py,0,"b'# Copyright (C) 2019-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport tempfile\nimport shutil\nimport zipfile\nimport io\nfrom abc import ABC, abstractmethod\n\nimport av\nimport av.datasets\nimport numpy as np\nfrom pyunpack import Archive\nfrom PIL import Image, ImageFile\n\n# fixes: ""OSError:broken data stream"" when executing line 72 while loading images downloaded from the web\n# see: https://stackoverflow.com/questions/42462431/oserror-broken-data-stream-when-reading-image-file\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom cvat.apps.engine.mime_types import mimetypes\n\ndef get_mime(name):\n    for type_name, type_def in MEDIA_TYPES.items():\n        if type_def[\'has_mime_type\'](name):\n            return type_name\n\n    return \'unknown\'\n\ndef create_tmp_dir():\n    return tempfile.mkdtemp(prefix=\'cvat-\', suffix=\'.data\')\n\ndef delete_tmp_dir(tmp_dir):\n    if tmp_dir:\n        shutil.rmtree(tmp_dir)\n\nclass IMediaReader(ABC):\n    def __init__(self, source_path, step, start, stop):\n        self._source_path = sorted(source_path)\n        self._step = step\n        self._start = start\n        self._stop = stop\n\n    @abstractmethod\n    def __iter__(self):\n        pass\n\n    @abstractmethod\n    def get_preview(self):\n        pass\n\n    @abstractmethod\n    def get_progress(self, pos):\n        pass\n\n    @staticmethod\n    def _get_preview(obj):\n        PREVIEW_SIZE = (256, 256)\n        if isinstance(obj, io.IOBase):\n            preview = Image.open(obj)\n        else:\n            preview = obj\n        preview.thumbnail(PREVIEW_SIZE)\n\n        return preview.convert(\'RGB\')\n\n    @abstractmethod\n    def get_image_size(self):\n        pass\n\nclass ImageListReader(IMediaReader):\n    def __init__(self, source_path, step=1, start=0, stop=None):\n        if not source_path:\n            raise Exception(\'No image found\')\n\n        if stop is None:\n            stop = len(source_path)\n        else:\n            stop = min(len(source_path), stop + 1)\n        step = max(step, 1)\n        assert stop > start\n\n        super().__init__(\n            source_path=source_path,\n            step=step,\n            start=start,\n            stop=stop,\n        )\n\n    def __iter__(self):\n        for i in range(self._start, self._stop, self._step):\n            yield (self.get_image(i), self.get_path(i), i)\n\n    def get_path(self, i):\n        return self._source_path[i]\n\n    def get_image(self, i):\n        return self._source_path[i]\n\n    def get_progress(self, pos):\n        return (pos - self._start + 1) / (self._stop - self._start)\n\n    def get_preview(self):\n        fp = open(self._source_path[0], ""rb"")\n        return self._get_preview(fp)\n\n    def get_image_size(self):\n        img = Image.open(self._source_path[0])\n        return img.width, img.height\n\nclass DirectoryReader(ImageListReader):\n    def __init__(self, source_path, step=1, start=0, stop=None):\n        image_paths = []\n        for source in source_path:\n            for root, _, files in os.walk(source):\n                paths = [os.path.join(root, f) for f in files]\n                paths = filter(lambda x: get_mime(x) == \'image\', paths)\n                image_paths.extend(paths)\n        super().__init__(\n            source_path=image_paths,\n            step=step,\n            start=start,\n            stop=stop,\n        )\n\nclass ArchiveReader(DirectoryReader):\n    def __init__(self, source_path, step=1, start=0, stop=None):\n        self._tmp_dir = create_tmp_dir()\n        self._archive_source = source_path[0]\n        Archive(self._archive_source).extractall(self._tmp_dir)\n        super().__init__(\n            source_path=[self._tmp_dir],\n            step=step,\n            start=start,\n            stop=stop,\n        )\n\n    def __del__(self):\n        delete_tmp_dir(self._tmp_dir)\n\n    def get_path(self, i):\n        base_dir = os.path.dirname(self._archive_source)\n        return os.path.join(base_dir, os.path.relpath(self._source_path[i], self._tmp_dir))\n\nclass PdfReader(DirectoryReader):\n    def __init__(self, source_path, step=1, start=0, stop=None):\n        if not source_path:\n            raise Exception(\'No PDF found\')\n\n        from pdf2image import convert_from_path\n        self._pdf_source = source_path[0]\n        self._tmp_dir = create_tmp_dir()\n        file_ = convert_from_path(self._pdf_source)\n        basename = os.path.splitext(os.path.basename(self._pdf_source))[0]\n        for page_num, page in enumerate(file_):\n            output = os.path.join(self._tmp_dir, \'{}{:09d}.jpeg\'.format(basename, page_num))\n            page.save(output, \'JPEG\')\n\n        super().__init__(\n            source_path=[self._tmp_dir],\n            step=step,\n            start=start,\n            stop=stop,\n        )\n\n    def __del__(self):\n        delete_tmp_dir(self._tmp_dir)\n\n    def get_path(self, i):\n        base_dir = os.path.dirname(self._pdf_source)\n        return os.path.join(base_dir, os.path.relpath(self._source_path[i], self._tmp_dir))\n\nclass ZipReader(ImageListReader):\n    def __init__(self, source_path, step=1, start=0, stop=None):\n        self._zip_source = zipfile.ZipFile(source_path[0], mode=\'r\')\n        file_list = [f for f in self._zip_source.namelist() if get_mime(f) == \'image\']\n        super().__init__(file_list, step, start, stop)\n\n    def __del__(self):\n        self._zip_source.close()\n\n    def get_preview(self):\n        io_image = io.BytesIO(self._zip_source.read(self._source_path[0]))\n        return self._get_preview(io_image)\n\n    def get_image_size(self):\n        img = Image.open(io.BytesIO(self._zip_source.read(self._source_path[0])))\n        return img.width, img.height\n\n    def get_image(self, i):\n        return io.BytesIO(self._zip_source.read(self._source_path[i]))\n\n    def get_path(self, i):\n        return os.path.join(os.path.dirname(self._zip_source.filename), self._source_path[i])\n\nclass VideoReader(IMediaReader):\n    def __init__(self, source_path, step=1, start=0, stop=None):\n        super().__init__(\n            source_path=source_path,\n            step=step,\n            start=start,\n            stop=stop + 1 if stop is not None else stop,\n        )\n\n    def _has_frame(self, i):\n        if i >= self._start:\n            if (i - self._start) % self._step == 0:\n                if self._stop is None or i < self._stop:\n                    return True\n\n        return False\n\n    def _decode(self, container):\n        frame_num = 0\n        for packet in container.demux():\n            if packet.stream.type == \'video\':\n                for image in packet.decode():\n                    frame_num += 1\n                    if self._has_frame(frame_num - 1):\n                        yield (image, self._source_path[0], image.pts)\n\n    def __iter__(self):\n        container = self._get_av_container()\n        source_video_stream = container.streams.video[0]\n        source_video_stream.thread_type = \'AUTO\'\n\n        return self._decode(container)\n\n    def get_progress(self, pos):\n        container = self._get_av_container()\n        # Not for all containers return real value\n        stream = container.streams.video[0]\n        return pos / stream.duration if stream.duration else None\n\n    def _get_av_container(self):\n        return av.open(av.datasets.curated(self._source_path[0]))\n\n    def get_preview(self):\n        container = self._get_av_container()\n        stream = container.streams.video[0]\n        preview = next(container.decode(stream))\n        return self._get_preview(preview.to_image())\n\n    def get_image_size(self):\n        image = (next(iter(self)))[0]\n        return image.width, image.height\n\nclass IChunkWriter(ABC):\n    def __init__(self, quality):\n        self._image_quality = quality\n\n    @staticmethod\n    def _compress_image(image_path, quality):\n        image = image_path.to_image() if isinstance(image_path, av.VideoFrame) else Image.open(image_path)\n        # Ensure image data fits into 8bit per pixel before RGB conversion as PIL clips values on conversion\n        if image.mode == ""I"":\n            # Image mode is 32bit integer pixels.\n            # Autoscale pixels by factor 2**8 / im_data.max() to fit into 8bit\n            im_data = np.array(image)\n            im_data = im_data * (2**8 / im_data.max())\n            image = Image.fromarray(im_data.astype(np.int32))\n        converted_image = image.convert(\'RGB\')\n        image.close()\n        buf = io.BytesIO()\n        converted_image.save(buf, format=\'JPEG\', quality=quality, optimize=True)\n        buf.seek(0)\n        width, height = converted_image.size\n        converted_image.close()\n        return width, height, buf\n\n    @abstractmethod\n    def save_as_chunk(self, images, chunk_path):\n        pass\n\nclass ZipChunkWriter(IChunkWriter):\n    def save_as_chunk(self, images, chunk_path):\n        with zipfile.ZipFile(chunk_path, \'x\') as zip_chunk:\n            for idx, (image, path, _) in enumerate(images):\n                arcname = \'{:06d}{}\'.format(idx, os.path.splitext(path)[1])\n                if isinstance(image, io.BytesIO):\n                    zip_chunk.writestr(arcname, image.getvalue())\n                else:\n                    zip_chunk.write(filename=image, arcname=arcname)\n        # return empty list because ZipChunkWriter write files as is\n        # and does not decode it to know img size.\n        return []\n\nclass ZipCompressedChunkWriter(IChunkWriter):\n    def save_as_chunk(self, images, chunk_path):\n        image_sizes = []\n        with zipfile.ZipFile(chunk_path, \'x\') as zip_chunk:\n            for idx, (image, _ , _) in enumerate(images):\n                w, h, image_buf = self._compress_image(image, self._image_quality)\n                image_sizes.append((w, h))\n                arcname = \'{:06d}.jpeg\'.format(idx)\n                zip_chunk.writestr(arcname, image_buf.getvalue())\n\n        return image_sizes\n\nclass Mpeg4ChunkWriter(IChunkWriter):\n    def __init__(self, _):\n        super().__init__(17)\n        self._output_fps = 25\n\n    @staticmethod\n    def _create_av_container(path, w, h, rate, options):\n            # x264 requires width and height must be divisible by 2 for yuv420p\n            if h % 2:\n                h += 1\n            if w % 2:\n                w += 1\n\n            container = av.open(path, \'w\')\n            video_stream = container.add_stream(\'libx264\', rate=rate)\n            video_stream.pix_fmt = ""yuv420p""\n            video_stream.width = w\n            video_stream.height = h\n            video_stream.options = options\n\n            return container, video_stream\n\n    def save_as_chunk(self, images, chunk_path):\n        if not images:\n            raise Exception(\'no images to save\')\n\n        input_w = images[0][0].width\n        input_h = images[0][0].height\n\n        output_container, output_v_stream = self._create_av_container(\n            path=chunk_path,\n            w=input_w,\n            h=input_h,\n            rate=self._output_fps,\n            options={\n                ""crf"": str(self._image_quality),\n                ""preset"": ""ultrafast"",\n            },\n        )\n\n        self._encode_images(images, output_container, output_v_stream)\n        output_container.close()\n        return [(input_w, input_h)]\n\n    @staticmethod\n    def _encode_images(images, container, stream):\n        for frame, _, _ in images:\n            # let libav set the correct pts and time_base\n            frame.pts = None\n            frame.time_base = None\n\n            for packet in stream.encode(frame):\n                container.mux(packet)\n\n        # Flush streams\n        for packet in stream.encode():\n            container.mux(packet)\n\nclass Mpeg4CompressedChunkWriter(Mpeg4ChunkWriter):\n    def __init__(self, quality):\n        # translate inversed range [1:100] to [0:51]\n        self._image_quality = round(51 * (100 - quality) / 99)\n        self._output_fps = 25\n\n\n    def save_as_chunk(self, images, chunk_path):\n        if not images:\n            raise Exception(\'no images to save\')\n\n        input_w = images[0][0].width\n        input_h = images[0][0].height\n\n        downscale_factor = 1\n        while input_h / downscale_factor >= 1080:\n            downscale_factor *= 2\n\n        output_h = input_h // downscale_factor\n        output_w = input_w // downscale_factor\n\n        output_container, output_v_stream = self._create_av_container(\n            path=chunk_path,\n            w=output_w,\n            h=output_h,\n            rate=self._output_fps,\n            options={\n                \'profile\': \'baseline\',\n                \'coder\': \'0\',\n                \'crf\': str(self._image_quality),\n                \'wpredp\': \'0\',\n                \'flags\': \'-loop\'\n            },\n        )\n\n        self._encode_images(images, output_container, output_v_stream)\n        output_container.close()\n        return [(input_w, input_h)]\n\ndef _is_archive(path):\n    mime = mimetypes.guess_type(path)\n    mime_type = mime[0]\n    encoding = mime[1]\n    supportedArchives = [\'application/x-rar-compressed\',\n        \'application/x-tar\', \'application/x-7z-compressed\', \'application/x-cpio\',\n        \'gzip\', \'bzip2\']\n    return mime_type in supportedArchives or encoding in supportedArchives\n\ndef _is_video(path):\n    mime = mimetypes.guess_type(path)\n    return mime[0] is not None and mime[0].startswith(\'video\')\n\ndef _is_image(path):\n    mime = mimetypes.guess_type(path)\n    # Exclude vector graphic images because Pillow cannot work with them\n    return mime[0] is not None and mime[0].startswith(\'image\') and \\\n        not mime[0].startswith(\'image/svg\')\n\ndef _is_dir(path):\n    return os.path.isdir(path)\n\ndef _is_pdf(path):\n    mime = mimetypes.guess_type(path)\n    return mime[0] == \'application/pdf\'\n\ndef _is_zip(path):\n    mime = mimetypes.guess_type(path)\n    mime_type = mime[0]\n    encoding = mime[1]\n    supportedArchives = [\'application/zip\']\n    return mime_type in supportedArchives or encoding in supportedArchives\n\n# \'has_mime_type\': function receives 1 argument - path to file.\n#                  Should return True if file has specified media type.\n# \'extractor\': class that extracts images from specified media.\n# \'mode\': \'annotation\' or \'interpolation\' - mode of task that should be created.\n# \'unique\': True or False - describes how the type can be combined with other.\n#           True - only one item of this type and no other is allowed\n#           False - this media types can be combined with other which have unique == False\n\nMEDIA_TYPES = {\n    \'image\': {\n        \'has_mime_type\': _is_image,\n        \'extractor\': ImageListReader,\n        \'mode\': \'annotation\',\n        \'unique\': False,\n    },\n    \'video\': {\n        \'has_mime_type\': _is_video,\n        \'extractor\': VideoReader,\n        \'mode\': \'interpolation\',\n        \'unique\': True,\n    },\n    \'archive\': {\n        \'has_mime_type\': _is_archive,\n        \'extractor\': ArchiveReader,\n        \'mode\': \'annotation\',\n        \'unique\': True,\n    },\n    \'directory\': {\n        \'has_mime_type\': _is_dir,\n        \'extractor\': DirectoryReader,\n        \'mode\': \'annotation\',\n        \'unique\': False,\n    },\n    \'pdf\': {\n        \'has_mime_type\': _is_pdf,\n        \'extractor\': PdfReader,\n        \'mode\': \'annotation\',\n        \'unique\': True,\n    },\n    \'zip\': {\n        \'has_mime_type\': _is_zip,\n        \'extractor\': ZipReader,\n        \'mode\': \'annotation\',\n        \'unique\': True,\n    }\n}\n'"
cvat/apps/engine/mime_types.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport mimetypes\n\n\n_SCRIPT_DIR = os.path.realpath(os.path.dirname(__file__))\nMEDIA_MIMETYPES_FILES = [\n    os.path.join(_SCRIPT_DIR, ""media.mimetypes""),\n]\nmimetypes.init(files=MEDIA_MIMETYPES_FILES)\n'"
cvat/apps/engine/models.py,0,"b'# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom enum import Enum\nimport re\nimport os\n\nfrom django.db import models\nfrom django.conf import settings\n\nfrom django.contrib.auth.models import User\nfrom django.core.files.storage import FileSystemStorage\n\nclass SafeCharField(models.CharField):\n    def get_prep_value(self, value):\n        value = super().get_prep_value(value)\n        if value:\n            return value[:self.max_length]\n        return value\n\nclass StatusChoice(str, Enum):\n    ANNOTATION = \'annotation\'\n    VALIDATION = \'validation\'\n    COMPLETED = \'completed\'\n\n    @classmethod\n    def choices(cls):\n        return tuple((x.value, x.name) for x in cls)\n\n    def __str__(self):\n        return self.value\n\nclass DataChoice(str, Enum):\n    VIDEO = \'video\'\n    IMAGESET = \'imageset\'\n    LIST = \'list\'\n\n    @classmethod\n    def choices(cls):\n        return tuple((x.value, x.name) for x in cls)\n\n    def __str__(self):\n        return self.value\n\nclass Data(models.Model):\n    chunk_size = models.PositiveIntegerField(null=True)\n    size = models.PositiveIntegerField(default=0)\n    image_quality = models.PositiveSmallIntegerField(default=50)\n    start_frame = models.PositiveIntegerField(default=0)\n    stop_frame = models.PositiveIntegerField(default=0)\n    frame_filter = models.CharField(max_length=256, default="""", blank=True)\n    compressed_chunk_type = models.CharField(max_length=32, choices=DataChoice.choices(),\n        default=DataChoice.IMAGESET)\n    original_chunk_type = models.CharField(max_length=32, choices=DataChoice.choices(),\n        default=DataChoice.IMAGESET)\n\n    class Meta:\n        default_permissions = ()\n\n    def get_frame_step(self):\n        match = re.search(""step\\s*=\\s*([1-9]\\d*)"", self.frame_filter)\n        return int(match.group(1)) if match else 1\n\n    def get_data_dirname(self):\n        return os.path.join(settings.MEDIA_DATA_ROOT, str(self.id))\n\n    def get_upload_dirname(self):\n        return os.path.join(self.get_data_dirname(), ""raw"")\n\n    def get_compressed_cache_dirname(self):\n        return os.path.join(self.get_data_dirname(), ""compressed"")\n\n    def get_original_cache_dirname(self):\n        return os.path.join(self.get_data_dirname(), ""original"")\n\n    @staticmethod\n    def _get_chunk_name(chunk_number, chunk_type):\n        if chunk_type == DataChoice.VIDEO:\n            ext = \'mp4\'\n        elif chunk_type == DataChoice.IMAGESET:\n            ext = \'zip\'\n        else:\n            ext = \'list\'\n\n        return \'{}.{}\'.format(chunk_number, ext)\n\n    def _get_compressed_chunk_name(self, chunk_number):\n        return self._get_chunk_name(chunk_number, self.compressed_chunk_type)\n\n    def _get_original_chunk_name(self, chunk_number):\n        return self._get_chunk_name(chunk_number, self.original_chunk_type)\n\n    def get_original_chunk_path(self, chunk_number):\n        return os.path.join(self.get_original_cache_dirname(),\n            self._get_original_chunk_name(chunk_number))\n\n    def get_compressed_chunk_path(self, chunk_number):\n        return os.path.join(self.get_compressed_cache_dirname(),\n            self._get_compressed_chunk_name(chunk_number))\n\n    def get_preview_path(self):\n        return os.path.join(self.get_data_dirname(), \'preview.jpeg\')\n\nclass Video(models.Model):\n    data = models.OneToOneField(Data, on_delete=models.CASCADE, related_name=""video"", null=True)\n    path = models.CharField(max_length=1024, default=\'\')\n    width = models.PositiveIntegerField()\n    height = models.PositiveIntegerField()\n\n    class Meta:\n        default_permissions = ()\n\nclass Image(models.Model):\n    data = models.ForeignKey(Data, on_delete=models.CASCADE, related_name=""images"", null=True)\n    path = models.CharField(max_length=1024, default=\'\')\n    frame = models.PositiveIntegerField()\n    width = models.PositiveIntegerField()\n    height = models.PositiveIntegerField()\n\n    class Meta:\n        default_permissions = ()\n\nclass Project(models.Model):\n    name = SafeCharField(max_length=256)\n    owner = models.ForeignKey(User, null=True, blank=True,\n        on_delete=models.SET_NULL, related_name=""+"")\n    assignee = models.ForeignKey(User, null=True,  blank=True,\n        on_delete=models.SET_NULL, related_name=""+"")\n    bug_tracker = models.CharField(max_length=2000, blank=True, default="""")\n    created_date = models.DateTimeField(auto_now_add=True)\n    updated_date = models.DateTimeField(auto_now_add=True)\n    status = models.CharField(max_length=32, choices=StatusChoice.choices(),\n        default=StatusChoice.ANNOTATION)\n\n    # Extend default permission model\n    class Meta:\n        default_permissions = ()\n\nclass Task(models.Model):\n    project = models.ForeignKey(Project, on_delete=models.CASCADE,\n        null=True, blank=True, related_name=""tasks"",\n        related_query_name=""task"")\n    name = SafeCharField(max_length=256)\n    mode = models.CharField(max_length=32)\n    owner = models.ForeignKey(User, null=True, blank=True,\n        on_delete=models.SET_NULL, related_name=""owners"")\n    assignee = models.ForeignKey(User, null=True,  blank=True,\n        on_delete=models.SET_NULL, related_name=""assignees"")\n    bug_tracker = models.CharField(max_length=2000, blank=True, default="""")\n    created_date = models.DateTimeField(auto_now_add=True)\n    updated_date = models.DateTimeField(auto_now_add=True)\n    overlap = models.PositiveIntegerField(null=True)\n    # Zero means that there are no limits (default)\n    segment_size = models.PositiveIntegerField(default=0)\n    z_order = models.BooleanField(default=False)\n    status = models.CharField(max_length=32, choices=StatusChoice.choices(),\n        default=StatusChoice.ANNOTATION)\n    data = models.ForeignKey(Data, on_delete=models.CASCADE, null=True, related_name=""tasks"")\n\n    # Extend default permission model\n    class Meta:\n        default_permissions = ()\n\n    def get_task_dirname(self):\n        return os.path.join(settings.TASKS_ROOT, str(self.id))\n\n    def get_task_logs_dirname(self):\n        return os.path.join(self.get_task_dirname(), \'logs\')\n\n    def get_client_log_path(self):\n        return os.path.join(self.get_task_logs_dirname(), ""client.log"")\n\n    def get_log_path(self):\n        return os.path.join(self.get_task_logs_dirname(), ""task.log"")\n\n    def get_task_artifacts_dirname(self):\n        return os.path.join(self.get_task_dirname(), \'artifacts\')\n\n    def __str__(self):\n        return self.name\n\n# Redefined a couple of operation for FileSystemStorage to avoid renaming\n# or other side effects.\nclass MyFileSystemStorage(FileSystemStorage):\n    def get_valid_name(self, name):\n        return name\n\n    def get_available_name(self, name, max_length=None):\n        if self.exists(name) or (max_length and len(name) > max_length):\n            raise IOError(\'`{}` file already exists or its name is too long\'.format(name))\n        return name\n\ndef upload_path_handler(instance, filename):\n    return os.path.join(instance.data.get_upload_dirname(), filename)\n\n# For client files which the user is uploaded\nclass ClientFile(models.Model):\n    data = models.ForeignKey(Data, on_delete=models.CASCADE, null=True, related_name=\'client_files\')\n    file = models.FileField(upload_to=upload_path_handler,\n        max_length=1024, storage=MyFileSystemStorage())\n\n    class Meta:\n        default_permissions = ()\n        unique_together = (""data"", ""file"")\n\n# For server files on the mounted share\nclass ServerFile(models.Model):\n    data = models.ForeignKey(Data, on_delete=models.CASCADE, null=True, related_name=\'server_files\')\n    file = models.CharField(max_length=1024)\n\n    class Meta:\n        default_permissions = ()\n\n# For URLs\nclass RemoteFile(models.Model):\n    data = models.ForeignKey(Data, on_delete=models.CASCADE, null=True, related_name=\'remote_files\')\n    file = models.CharField(max_length=1024)\n\n    class Meta:\n        default_permissions = ()\n\nclass Segment(models.Model):\n    task = models.ForeignKey(Task, on_delete=models.CASCADE)\n    start_frame = models.IntegerField()\n    stop_frame = models.IntegerField()\n\n    class Meta:\n        default_permissions = ()\n\nclass Job(models.Model):\n    segment = models.ForeignKey(Segment, on_delete=models.CASCADE)\n    assignee = models.ForeignKey(User, null=True, blank=True, on_delete=models.SET_NULL)\n    status = models.CharField(max_length=32, choices=StatusChoice.choices(),\n        default=StatusChoice.ANNOTATION)\n\n    class Meta:\n        default_permissions = ()\n\nclass Label(models.Model):\n    task = models.ForeignKey(Task, on_delete=models.CASCADE)\n    name = SafeCharField(max_length=64)\n\n    def __str__(self):\n        return self.name\n\n    class Meta:\n        default_permissions = ()\n        unique_together = (\'task\', \'name\')\n\nclass AttributeType(str, Enum):\n    CHECKBOX = \'checkbox\'\n    RADIO = \'radio\'\n    NUMBER = \'number\'\n    TEXT = \'text\'\n    SELECT = \'select\'\n\n    @classmethod\n    def choices(cls):\n        return tuple((x.value, x.name) for x in cls)\n\n    def __str__(self):\n        return self.value\n\nclass AttributeSpec(models.Model):\n    label = models.ForeignKey(Label, on_delete=models.CASCADE)\n    name = models.CharField(max_length=64)\n    mutable = models.BooleanField()\n    input_type = models.CharField(max_length=16,\n        choices=AttributeType.choices())\n    default_value = models.CharField(max_length=128)\n    values = models.CharField(max_length=4096)\n\n    class Meta:\n        default_permissions = ()\n        unique_together = (\'label\', \'name\')\n\n    def __str__(self):\n        return self.name\n\nclass AttributeVal(models.Model):\n    # TODO: add a validator here to be sure that it corresponds to self.label\n    id = models.BigAutoField(primary_key=True)\n    spec = models.ForeignKey(AttributeSpec, on_delete=models.CASCADE)\n    value = SafeCharField(max_length=4096)\n\n    class Meta:\n        abstract = True\n        default_permissions = ()\n\nclass ShapeType(str, Enum):\n    RECTANGLE = \'rectangle\' # (x0, y0, x1, y1)\n    POLYGON = \'polygon\'     # (x0, y0, ..., xn, yn)\n    POLYLINE = \'polyline\'   # (x0, y0, ..., xn, yn)\n    POINTS = \'points\'       # (x0, y0, ..., xn, yn)\n    CUBOID = \'cuboid\'\n\n    @classmethod\n    def choices(cls):\n        return tuple((x.value, x.name) for x in cls)\n\n    def __str__(self):\n        return self.value\n\nclass Annotation(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    job = models.ForeignKey(Job, on_delete=models.CASCADE)\n    label = models.ForeignKey(Label, on_delete=models.CASCADE)\n    frame = models.PositiveIntegerField()\n    group = models.PositiveIntegerField(null=True)\n\n    class Meta:\n        abstract = True\n        default_permissions = ()\n\nclass Commit(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    author = models.ForeignKey(User, null=True, blank=True, on_delete=models.SET_NULL)\n    version = models.PositiveIntegerField(default=0)\n    timestamp = models.DateTimeField(auto_now=True)\n    message = models.CharField(max_length=4096, default="""")\n\n    class Meta:\n        abstract = True\n        default_permissions = ()\n\nclass JobCommit(Commit):\n    job = models.ForeignKey(Job, on_delete=models.CASCADE, related_name=""commits"")\n\nclass FloatArrayField(models.TextField):\n    separator = "",""\n\n    def from_db_value(self, value, expression, connection):\n        if not value:\n            return value\n        return [float(v) for v in value.split(self.separator)]\n\n    def to_python(self, value):\n        if isinstance(value, list):\n            return value\n\n        return self.from_db_value(value, None, None)\n\n    def get_prep_value(self, value):\n        return self.separator.join(map(str, value))\n\nclass Shape(models.Model):\n    type = models.CharField(max_length=16, choices=ShapeType.choices())\n    occluded = models.BooleanField(default=False)\n    z_order = models.IntegerField(default=0)\n    points = FloatArrayField()\n\n    class Meta:\n        abstract = True\n        default_permissions = ()\n\nclass LabeledImage(Annotation):\n    pass\n\nclass LabeledImageAttributeVal(AttributeVal):\n    image = models.ForeignKey(LabeledImage, on_delete=models.CASCADE)\n\nclass LabeledShape(Annotation, Shape):\n    pass\n\nclass LabeledShapeAttributeVal(AttributeVal):\n    shape = models.ForeignKey(LabeledShape, on_delete=models.CASCADE)\n\nclass LabeledTrack(Annotation):\n    pass\n\nclass LabeledTrackAttributeVal(AttributeVal):\n    track = models.ForeignKey(LabeledTrack, on_delete=models.CASCADE)\n\nclass TrackedShape(Shape):\n    id = models.BigAutoField(primary_key=True)\n    track = models.ForeignKey(LabeledTrack, on_delete=models.CASCADE)\n    frame = models.PositiveIntegerField()\n    outside = models.BooleanField(default=False)\n\nclass TrackedShapeAttributeVal(AttributeVal):\n    shape = models.ForeignKey(TrackedShape, on_delete=models.CASCADE)\n\nclass Plugin(models.Model):\n    name = models.SlugField(max_length=32, primary_key=True)\n    description = SafeCharField(max_length=8192)\n    maintainer = models.ForeignKey(User, null=True, blank=True,\n        on_delete=models.SET_NULL, related_name=""maintainers"")\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now_add=True)\n\n    # Extend default permission model\n    class Meta:\n        default_permissions = ()\n\nclass PluginOption(models.Model):\n    plugin = models.ForeignKey(Plugin, on_delete=models.CASCADE)\n    name = SafeCharField(max_length=32)\n    value = SafeCharField(max_length=1024)\n'"
cvat/apps/engine/pagination.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport sys\nfrom rest_framework.pagination import PageNumberPagination\n\nclass CustomPagination(PageNumberPagination):\n    page_size_query_param = ""page_size""\n\n    def get_page_size(self, request):\n        page_size = 0\n        try:\n            value = request.query_params[self.page_size_query_param]\n            if value == ""all"":\n                page_size = sys.maxsize\n            else:\n                page_size = int(value)\n        except (KeyError, ValueError):\n            pass\n\n        return page_size if page_size > 0 else self.page_size\n'"
cvat/apps/engine/plugins.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom functools import update_wrapper\n\n__plugins = {}\n\n\ndef add_plugin(name, function, order, exc_ok = False):\n    if order not in [""before"", ""after""]:\n        raise Exception(""Order may be \'before\' or \'after\' only. Got {}."".format(order))\n\n    if not callable(function):\n        raise Exception(""\'function\' argument should be a callable element"")\n\n    if not isinstance(name, str):\n        raise Exception(""\'name\' argument should be a string. Got {}."".format(type(name)))\n\n    if name not in __plugins:\n        __plugins[name] = {\n            ""before"": [],\n            ""after"": []\n        }\n\n    if function in __plugins[name][order]:\n        raise Exception(""plugin has been attached already"")\n\n    __plugins[name][order].append(function)\n\n    function.exc_ok = exc_ok\n\n\ndef remove_plugin(name, function):\n    if name in __plugins:\n        if function in __plugins[name][""before""]:\n            __plugins[name][""before""].remove(function)\n            del function.exc_ok\n        if function in __plugins[name][""after""]:\n            __plugins[name][""after""].remove(function)\n            del function.exc_ok\n\n\ndef plugin_decorator(function_to_decorate):\n    name = function_to_decorate.__name__\n\n    def function_wrapper(*args, **kwargs):\n        if name in __plugins:\n            for wrapper in __plugins[name][""before""]:\n                try:\n                    wrapper(*args, **kwargs)\n                except Exception as ex:\n                    if not wrapper.exc_ok:\n                        raise ex\n\n        result = function_to_decorate(*args, **kwargs)\n\n        if name in __plugins:\n            for wrapper in __plugins[name][""after""]:\n                try:\n                    wrapper(*args, **kwargs)\n                except Exception as ex:\n                    if not wrapper.exc_ok:\n                        raise ex\n\n        return result\n\n    # Copy meta info about wrapped function to wrapper function\n    update_wrapper(function_wrapper, function_to_decorate)\n    return function_wrapper\n'"
cvat/apps/engine/serializers.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport re\nimport shutil\n\nfrom rest_framework import serializers\nfrom django.contrib.auth.models import User, Group\n\nfrom cvat.apps.engine import models\nfrom cvat.apps.engine.log import slogger\n\n\nclass AttributeSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.AttributeSpec\n        fields = (\'id\', \'name\', \'mutable\', \'input_type\', \'default_value\',\n            \'values\')\n\n    # pylint: disable=no-self-use\n    def to_internal_value(self, data):\n        attribute = data.copy()\n        attribute[\'values\'] = \'\\n\'.join(map(lambda x: x.strip(), data.get(\'values\', [])))\n        return attribute\n\n    def to_representation(self, instance):\n        if instance:\n            attribute = super().to_representation(instance)\n            attribute[\'values\'] = attribute[\'values\'].split(\'\\n\')\n        else:\n            attribute = instance\n\n        return attribute\n\nclass LabelSerializer(serializers.ModelSerializer):\n    attributes = AttributeSerializer(many=True, source=\'attributespec_set\',\n        default=[])\n    class Meta:\n        model = models.Label\n        fields = (\'id\', \'name\', \'attributes\')\n\nclass JobCommitSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.JobCommit\n        fields = (\'id\', \'version\', \'author\', \'message\', \'timestamp\')\n\nclass JobSerializer(serializers.ModelSerializer):\n    task_id = serializers.ReadOnlyField(source=""segment.task.id"")\n    start_frame = serializers.ReadOnlyField(source=""segment.start_frame"")\n    stop_frame = serializers.ReadOnlyField(source=""segment.stop_frame"")\n\n    class Meta:\n        model = models.Job\n        fields = (\'url\', \'id\', \'assignee\', \'status\', \'start_frame\',\n            \'stop_frame\', \'task_id\')\n\nclass SimpleJobSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.Job\n        fields = (\'url\', \'id\', \'assignee\', \'status\')\n\nclass SegmentSerializer(serializers.ModelSerializer):\n    jobs = SimpleJobSerializer(many=True, source=\'job_set\')\n\n    class Meta:\n        model = models.Segment\n        fields = (\'start_frame\', \'stop_frame\', \'jobs\')\n\nclass ClientFileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.ClientFile\n        fields = (\'file\', )\n\n    # pylint: disable=no-self-use\n    def to_internal_value(self, data):\n        return {\'file\': data}\n\n    # pylint: disable=no-self-use\n    def to_representation(self, instance):\n        if instance:\n            upload_dir = instance.data.get_upload_dirname()\n            return instance.file.path[len(upload_dir) + 1:]\n        else:\n            return instance\n\nclass ServerFileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.ServerFile\n        fields = (\'file\', )\n\n    # pylint: disable=no-self-use\n    def to_internal_value(self, data):\n        return {\'file\': data}\n\n    # pylint: disable=no-self-use\n    def to_representation(self, instance):\n        return instance.file if instance else instance\n\nclass RemoteFileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.RemoteFile\n        fields = (\'file\', )\n\n    # pylint: disable=no-self-use\n    def to_internal_value(self, data):\n        return {\'file\': data}\n\n    # pylint: disable=no-self-use\n    def to_representation(self, instance):\n        return instance.file if instance else instance\n\nclass RqStatusSerializer(serializers.Serializer):\n    state = serializers.ChoiceField(choices=[\n        ""Queued"", ""Started"", ""Finished"", ""Failed""])\n    message = serializers.CharField(allow_blank=True, default="""")\n\nclass WriteOnceMixin:\n    """"""Adds support for write once fields to serializers.\n\n    To use it, specify a list of fields as `write_once_fields` on the\n    serializer\'s Meta:\n    ```\n    class Meta:\n        model = SomeModel\n        fields = \'__all__\'\n        write_once_fields = (\'collection\', )\n    ```\n\n    Now the fields in `write_once_fields` can be set during POST (create),\n    but cannot be changed afterwards via PUT or PATCH (update).\n    Inspired by http://stackoverflow.com/a/37487134/627411.\n    """"""\n\n    def get_extra_kwargs(self):\n        extra_kwargs = super().get_extra_kwargs()\n\n        # We\'re only interested in PATCH/PUT.\n        if \'update\' in getattr(self.context.get(\'view\'), \'action\', \'\'):\n            return self._set_write_once_fields(extra_kwargs)\n\n        return extra_kwargs\n\n    def _set_write_once_fields(self, extra_kwargs):\n        """"""Set all fields in `Meta.write_once_fields` to read_only.""""""\n        write_once_fields = getattr(self.Meta, \'write_once_fields\', None)\n        if not write_once_fields:\n            return extra_kwargs\n\n        if not isinstance(write_once_fields, (list, tuple)):\n            raise TypeError(\n                \'The `write_once_fields` option must be a list or tuple. \'\n                \'Got {}.\'.format(type(write_once_fields).__name__)\n            )\n\n        for field_name in write_once_fields:\n            kwargs = extra_kwargs.get(field_name, {})\n            kwargs[\'read_only\'] = True\n            extra_kwargs[field_name] = kwargs\n\n        return extra_kwargs\n\nclass DataSerializer(serializers.ModelSerializer):\n    image_quality = serializers.IntegerField(min_value=0, max_value=100)\n    use_zip_chunks = serializers.BooleanField(default=False)\n    client_files = ClientFileSerializer(many=True, default=[])\n    server_files = ServerFileSerializer(many=True, default=[])\n    remote_files = RemoteFileSerializer(many=True, default=[])\n\n    class Meta:\n        model = models.Data\n        fields = (\'chunk_size\', \'size\', \'image_quality\', \'start_frame\', \'stop_frame\', \'frame_filter\',\n            \'compressed_chunk_type\', \'original_chunk_type\', \'client_files\', \'server_files\', \'remote_files\', \'use_zip_chunks\')\n\n    # pylint: disable=no-self-use\n    def validate_frame_filter(self, value):\n        match = re.search(""step\\s*=\\s*([1-9]\\d*)"", value)\n        if not match:\n            raise serializers.ValidationError(""Invalid frame filter expression"")\n        return value\n\n    # pylint: disable=no-self-use\n    def validate_chunk_size(self, value):\n        if not value > 0:\n            raise serializers.ValidationError(\'Chunk size must be a positive integer\')\n        return value\n\n    # pylint: disable=no-self-use\n    def validate(self, data):\n        if \'start_frame\' in data and \'stop_frame\' in data \\\n            and data[\'start_frame\'] > data[\'stop_frame\']:\n            raise serializers.ValidationError(\'Stop frame must be more or equal start frame\')\n        return data\n\n    # pylint: disable=no-self-use\n    def create(self, validated_data):\n        client_files = validated_data.pop(\'client_files\')\n        server_files = validated_data.pop(\'server_files\')\n        remote_files = validated_data.pop(\'remote_files\')\n        validated_data.pop(\'use_zip_chunks\')\n        db_data = models.Data.objects.create(**validated_data)\n\n        data_path = db_data.get_data_dirname()\n        if os.path.isdir(data_path):\n            shutil.rmtree(data_path)\n\n        os.makedirs(db_data.get_compressed_cache_dirname())\n        os.makedirs(db_data.get_original_cache_dirname())\n        os.makedirs(db_data.get_upload_dirname())\n\n        for f in client_files:\n            client_file = models.ClientFile(data=db_data, **f)\n            client_file.save()\n\n        for f in server_files:\n            server_file = models.ServerFile(data=db_data, **f)\n            server_file.save()\n\n        for f in remote_files:\n            remote_file = models.RemoteFile(data=db_data, **f)\n            remote_file.save()\n\n        db_data.save()\n        return db_data\n\nclass TaskSerializer(WriteOnceMixin, serializers.ModelSerializer):\n    labels = LabelSerializer(many=True, source=\'label_set\', partial=True)\n    segments = SegmentSerializer(many=True, source=\'segment_set\', read_only=True)\n    data_chunk_size = serializers.ReadOnlyField(source=\'data.chunk_size\')\n    data_compressed_chunk_type = serializers.ReadOnlyField(source=\'data.compressed_chunk_type\')\n    data_original_chunk_type = serializers.ReadOnlyField(source=\'data.original_chunk_type\')\n    size = serializers.ReadOnlyField(source=\'data.size\')\n    image_quality = serializers.ReadOnlyField(source=\'data.image_quality\')\n    data = serializers.ReadOnlyField(source=\'data.id\')\n\n    class Meta:\n        model = models.Task\n        fields = (\'url\', \'id\', \'name\', \'mode\', \'owner\', \'assignee\',\n            \'bug_tracker\', \'created_date\', \'updated_date\', \'overlap\',\n            \'segment_size\', \'z_order\', \'status\', \'labels\', \'segments\',\n            \'project\', \'data_chunk_size\', \'data_compressed_chunk_type\', \'data_original_chunk_type\', \'size\', \'image_quality\', \'data\')\n        read_only_fields = (\'mode\', \'created_date\', \'updated_date\', \'status\', \'data_chunk_size\',\n            \'data_compressed_chunk_type\', \'data_original_chunk_type\', \'size\', \'image_quality\', \'data\')\n        write_once_fields = (\'overlap\', \'segment_size\')\n        ordering = [\'-id\']\n\n    # pylint: disable=no-self-use\n    def create(self, validated_data):\n        labels = validated_data.pop(\'label_set\')\n        db_task = models.Task.objects.create(**validated_data)\n        for label in labels:\n            attributes = label.pop(\'attributespec_set\')\n            db_label = models.Label.objects.create(task=db_task, **label)\n            for attr in attributes:\n                models.AttributeSpec.objects.create(label=db_label, **attr)\n\n        task_path = db_task.get_task_dirname()\n        if os.path.isdir(task_path):\n            shutil.rmtree(task_path)\n\n        os.makedirs(db_task.get_task_logs_dirname())\n        os.makedirs(db_task.get_task_artifacts_dirname())\n\n        db_task.save()\n        return db_task\n\n    # pylint: disable=no-self-use\n    def update(self, instance, validated_data):\n        instance.name = validated_data.get(\'name\', instance.name)\n        instance.owner = validated_data.get(\'owner\', instance.owner)\n        instance.assignee = validated_data.get(\'assignee\', instance.assignee)\n        instance.bug_tracker = validated_data.get(\'bug_tracker\',\n            instance.bug_tracker)\n        instance.z_order = validated_data.get(\'z_order\', instance.z_order)\n        instance.project = validated_data.get(\'project\', instance.project)\n        labels = validated_data.get(\'label_set\', [])\n        for label in labels:\n            attributes = label.pop(\'attributespec_set\', [])\n            (db_label, created) = models.Label.objects.get_or_create(task=instance,\n                name=label[\'name\'])\n            if created:\n                slogger.task[instance.id].info(""New {} label was created""\n                    .format(db_label.name))\n            else:\n                slogger.task[instance.id].info(""{} label was updated""\n                    .format(db_label.name))\n            for attr in attributes:\n                (db_attr, created) = models.AttributeSpec.objects.get_or_create(\n                    label=db_label, name=attr[\'name\'], defaults=attr)\n                if created:\n                    slogger.task[instance.id].info(""New {} attribute for {} label was created""\n                        .format(db_attr.name, db_label.name))\n                else:\n                    slogger.task[instance.id].info(""{} attribute for {} label was updated""\n                        .format(db_attr.name, db_label.name))\n\n                    # FIXME: need to update only ""safe"" fields\n                    db_attr.default_value = attr.get(\'default_value\', db_attr.default_value)\n                    db_attr.mutable = attr.get(\'mutable\', db_attr.mutable)\n                    db_attr.input_type = attr.get(\'input_type\', db_attr.input_type)\n                    db_attr.values = attr.get(\'values\', db_attr.values)\n                    db_attr.save()\n\n        instance.save()\n        return instance\n\nclass ProjectSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.Project\n        fields = (\'url\', \'id\', \'name\', \'owner\', \'assignee\', \'bug_tracker\',\n            \'created_date\', \'updated_date\', \'status\')\n        read_only_fields = (\'created_date\', \'updated_date\', \'status\')\n        ordering = [\'-id\']\n\nclass BasicUserSerializer(serializers.ModelSerializer):\n    def validate(self, data):\n        if hasattr(self, \'initial_data\'):\n            unknown_keys = set(self.initial_data.keys()) - set(self.fields.keys())\n            if unknown_keys:\n                if set([\'is_staff\', \'is_superuser\', \'groups\']) & unknown_keys:\n                    message = \'You do not have permissions to access some of\' + \\\n                        \' these fields: {}\'.format(unknown_keys)\n                else:\n                    message = \'Got unknown fields: {}\'.format(unknown_keys)\n                raise serializers.ValidationError(message)\n        return data\n\n    class Meta:\n        model = User\n        fields = (\'url\', \'id\', \'username\', \'first_name\', \'last_name\')\n        ordering = [\'-id\']\n\nclass UserSerializer(serializers.ModelSerializer):\n    groups = serializers.SlugRelatedField(many=True,\n        slug_field=\'name\', queryset=Group.objects.all())\n\n    class Meta:\n        model = User\n        fields = (\'url\', \'id\', \'username\', \'first_name\', \'last_name\', \'email\',\n            \'groups\', \'is_staff\', \'is_superuser\', \'is_active\', \'last_login\',\n            \'date_joined\')\n        read_only_fields = (\'last_login\', \'date_joined\')\n        write_only_fields = (\'password\', )\n        ordering = [\'-id\']\n\nclass ExceptionSerializer(serializers.Serializer):\n    system = serializers.CharField(max_length=255)\n    client = serializers.CharField(max_length=255)\n    time = serializers.DateTimeField()\n\n    job_id = serializers.IntegerField(required=False)\n    task_id = serializers.IntegerField(required=False)\n    proj_id = serializers.IntegerField(required=False)\n    client_id = serializers.IntegerField()\n\n    message = serializers.CharField(max_length=4096)\n    filename = serializers.URLField()\n    line = serializers.IntegerField()\n    column = serializers.IntegerField()\n    stack = serializers.CharField(max_length=8192,\n        style={\'base_template\': \'textarea.html\'}, allow_blank=True)\n\nclass AboutSerializer(serializers.Serializer):\n    name = serializers.CharField(max_length=128)\n    description = serializers.CharField(max_length=2048)\n    version = serializers.CharField(max_length=64)\n\nclass FrameMetaSerializer(serializers.Serializer):\n    width = serializers.IntegerField()\n    height = serializers.IntegerField()\n    name = serializers.CharField(max_length=1024)\n\nclass DataMetaSerializer(serializers.ModelSerializer):\n    frames = FrameMetaSerializer(many=True, allow_null=True)\n    image_quality = serializers.IntegerField(min_value=0, max_value=100)\n\n    class Meta:\n        model = models.Data\n        fields = (\n            \'chunk_size\',\n            \'size\',\n            \'image_quality\',\n            \'start_frame\',\n            \'stop_frame\',\n            \'frame_filter\',\n            \'frames\',\n        )\n        read_only_fields = (\n            \'chunk_size\',\n            \'size\',\n            \'image_quality\',\n            \'start_frame\',\n            \'stop_frame\',\n            \'frame_filter\',\n            \'frames\',\n        )\n\nclass AttributeValSerializer(serializers.Serializer):\n    spec_id = serializers.IntegerField()\n    value = serializers.CharField(max_length=4096, allow_blank=True)\n\n    def to_internal_value(self, data):\n        data[\'value\'] = str(data[\'value\'])\n        return super().to_internal_value(data)\n\nclass AnnotationSerializer(serializers.Serializer):\n    id = serializers.IntegerField(default=None, allow_null=True)\n    frame = serializers.IntegerField(min_value=0)\n    label_id = serializers.IntegerField(min_value=0)\n    group = serializers.IntegerField(min_value=0, allow_null=True)\n\nclass LabeledImageSerializer(AnnotationSerializer):\n    attributes = AttributeValSerializer(many=True,\n        source=""labeledimageattributeval_set"")\n\nclass ShapeSerializer(serializers.Serializer):\n    type = serializers.ChoiceField(choices=models.ShapeType.choices())\n    occluded = serializers.BooleanField()\n    z_order = serializers.IntegerField(default=0)\n    points = serializers.ListField(\n        child=serializers.FloatField(),\n        allow_empty=False,\n    )\n\nclass LabeledShapeSerializer(ShapeSerializer, AnnotationSerializer):\n    attributes = AttributeValSerializer(many=True,\n        source=""labeledshapeattributeval_set"")\n\nclass TrackedShapeSerializer(ShapeSerializer):\n    id = serializers.IntegerField(default=None, allow_null=True)\n    frame = serializers.IntegerField(min_value=0)\n    outside = serializers.BooleanField()\n    attributes = AttributeValSerializer(many=True,\n        source=""trackedshapeattributeval_set"")\n\nclass LabeledTrackSerializer(AnnotationSerializer):\n    shapes = TrackedShapeSerializer(many=True, allow_empty=False,\n        source=""trackedshape_set"")\n    attributes = AttributeValSerializer(many=True,\n        source=""labeledtrackattributeval_set"")\n\nclass LabeledDataSerializer(serializers.Serializer):\n    version = serializers.IntegerField()\n    tags   = LabeledImageSerializer(many=True)\n    shapes = LabeledShapeSerializer(many=True)\n    tracks = LabeledTrackSerializer(many=True)\n\nclass FileInfoSerializer(serializers.Serializer):\n    name = serializers.CharField(max_length=1024)\n    type = serializers.ChoiceField(choices=[""REG"", ""DIR""])\n\nclass PluginSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = models.Plugin\n        fields = (\'name\', \'description\', \'maintainer\', \'created_at\',\n            \'updated_at\')\n\nclass LogEventSerializer(serializers.Serializer):\n    job_id = serializers.IntegerField(required=False)\n    task_id = serializers.IntegerField(required=False)\n    proj_id = serializers.IntegerField(required=False)\n    client_id = serializers.IntegerField()\n\n    name = serializers.CharField(max_length=64)\n    time = serializers.DateTimeField()\n    message = serializers.CharField(max_length=4096, required=False)\n    payload = serializers.DictField(required=False)\n    is_active = serializers.BooleanField()\n\nclass AnnotationFileSerializer(serializers.Serializer):\n    annotation_file = serializers.FileField()'"
cvat/apps/engine/signals.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom .models import Job, StatusChoice\n\ndef update_task_status(instance, **kwargs):\n    db_task = instance.segment.task\n    db_jobs = list(Job.objects.filter(segment__task_id=db_task.id))\n    status = StatusChoice.COMPLETED\n    if   list(filter(lambda x: x.status == StatusChoice.ANNOTATION, db_jobs)):\n        status = StatusChoice.ANNOTATION\n    elif list(filter(lambda x: x.status == StatusChoice.VALIDATION, db_jobs)):\n        status = StatusChoice.VALIDATION\n\n    if status != db_task.status:\n        db_task.status = status\n        db_task.save()\n\n'"
cvat/apps/engine/task.py,0,"b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport itertools\nimport os\nimport sys\nimport rq\nimport shutil\nfrom traceback import print_exception\nfrom urllib import error as urlerror\nfrom urllib import parse as urlparse\nfrom urllib import request as urlrequest\n\nfrom cvat.apps.engine.media_extractors import get_mime, MEDIA_TYPES, Mpeg4ChunkWriter, ZipChunkWriter, Mpeg4CompressedChunkWriter, ZipCompressedChunkWriter\nfrom cvat.apps.engine.models import DataChoice\n\nimport django_rq\nfrom django.conf import settings\nfrom django.db import transaction\nfrom distutils.dir_util import copy_tree\n\nfrom . import models\nfrom .log import slogger\n\n############################# Low Level server API\n\ndef create(tid, data):\n    """"""Schedule the task""""""\n    q = django_rq.get_queue(\'default\')\n    q.enqueue_call(func=_create_thread, args=(tid, data),\n        job_id=""/api/v1/tasks/{}"".format(tid))\n\n@transaction.atomic\ndef rq_handler(job, exc_type, exc_value, traceback):\n    splitted = job.id.split(\'/\')\n    tid = int(splitted[splitted.index(\'tasks\') + 1])\n    try:\n        db_task = models.Task.objects.select_for_update().get(pk=tid)\n        with open(db_task.get_log_path(), ""wt"") as log_file:\n            print_exception(exc_type, exc_value, traceback, file=log_file)\n    except models.Task.DoesNotExist:\n        pass # skip exceptions in the code\n\n    return False\n\n############################# Internal implementation for server API\n\ndef _copy_data_from_share(server_files, upload_dir):\n    job = rq.get_current_job()\n    job.meta[\'status\'] = \'Data are being copied from share..\'\n    job.save_meta()\n\n    for path in server_files:\n        source_path = os.path.join(settings.SHARE_ROOT, os.path.normpath(path))\n        target_path = os.path.join(upload_dir, path)\n        if os.path.isdir(source_path):\n            copy_tree(source_path, target_path)\n        else:\n            target_dir = os.path.dirname(target_path)\n            if not os.path.exists(target_dir):\n                os.makedirs(target_dir)\n            shutil.copyfile(source_path, target_path)\n\ndef _save_task_to_db(db_task):\n    job = rq.get_current_job()\n    job.meta[\'status\'] = \'Task is being saved in database\'\n    job.save_meta()\n\n    segment_size = db_task.segment_size\n    segment_step = segment_size\n    if segment_size == 0:\n        segment_size = db_task.data.size\n\n        # Segment step must be more than segment_size + overlap in single-segment tasks\n        # Otherwise a task contains an extra segment\n        segment_step = sys.maxsize\n\n    default_overlap = 5 if db_task.mode == \'interpolation\' else 0\n    if db_task.overlap is None:\n        db_task.overlap = default_overlap\n    db_task.overlap = min(db_task.overlap, segment_size  // 2)\n\n    segment_step -= db_task.overlap\n\n    for start_frame in range(0, db_task.data.size, segment_step):\n        stop_frame = min(start_frame + segment_size - 1, db_task.data.size - 1)\n\n        slogger.glob.info(""New segment for task #{}: start_frame = {}, \\\n            stop_frame = {}"".format(db_task.id, start_frame, stop_frame))\n\n        db_segment = models.Segment()\n        db_segment.task = db_task\n        db_segment.start_frame = start_frame\n        db_segment.stop_frame = stop_frame\n        db_segment.save()\n\n        db_job = models.Job()\n        db_job.segment = db_segment\n        db_job.save()\n\n    db_task.data.save()\n    db_task.save()\n\ndef _count_files(data):\n    share_root = settings.SHARE_ROOT\n    server_files = []\n\n    for path in data[""server_files""]:\n        path = os.path.normpath(path).lstrip(\'/\')\n        if \'..\' in path.split(os.path.sep):\n            raise ValueError(""Don\'t use \'..\' inside file paths"")\n        full_path = os.path.abspath(os.path.join(share_root, path))\n        if os.path.commonprefix([share_root, full_path]) != share_root:\n            raise ValueError(""Bad file path: "" + path)\n        server_files.append(path)\n\n    server_files.sort(reverse=True)\n    # The idea of the code is trivial. After sort we will have files in the\n    # following order: \'a/b/c/d/2.txt\', \'a/b/c/d/1.txt\', \'a/b/c/d\', \'a/b/c\'\n    # Let\'s keep all items which aren\'t substrings of the previous item. In\n    # the example above only 2.txt and 1.txt files will be in the final list.\n    # Also need to correctly handle \'a/b/c0\', \'a/b/c\' case.\n    data[\'server_files\'] = [v[1] for v in zip([""""] + server_files, server_files)\n        if not os.path.dirname(v[0]).startswith(v[1])]\n\n    def count_files(file_mapping, counter):\n        for rel_path, full_path in file_mapping.items():\n            mime = get_mime(full_path)\n            if mime in counter:\n                counter[mime].append(rel_path)\n            else:\n                slogger.glob.warn(""Skip \'{}\' file (its mime type doesn\'t ""\n                    ""correspond to a video or an image file)"".format(full_path))\n\n\n    counter = { media_type: [] for media_type in MEDIA_TYPES.keys() }\n\n    count_files(\n        file_mapping={ f:f for f in data[\'remote_files\'] or data[\'client_files\']},\n        counter=counter,\n    )\n\n    count_files(\n        file_mapping={ f:os.path.abspath(os.path.join(share_root, f)) for f in data[\'server_files\']},\n        counter=counter,\n    )\n\n    return counter\n\ndef _validate_data(counter):\n    unique_entries = 0\n    multiple_entries = 0\n    for media_type, media_config in MEDIA_TYPES.items():\n        if counter[media_type]:\n            if media_config[\'unique\']:\n                unique_entries += len(counter[media_type])\n            else:\n                multiple_entries += len(counter[media_type])\n\n    if unique_entries == 1 and multiple_entries > 0 or unique_entries > 1:\n        unique_types = \', \'.join([k for k, v in MEDIA_TYPES.items() if v[\'unique\']])\n        multiply_types = \', \'.join([k for k, v in MEDIA_TYPES.items() if not v[\'unique\']])\n        count = \', \'.join([\'{} {}(s)\'.format(len(v), k) for k, v in counter.items()])\n        raise ValueError(\'Only one {} or many {} can be used simultaneously, \\\n            but {} found.\'.format(unique_types, multiply_types, count))\n\n    if unique_entries == 0 and multiple_entries == 0:\n        raise ValueError(\'No media data found\')\n\n    task_modes = [MEDIA_TYPES[media_type][\'mode\'] for media_type, media_files in counter.items() if media_files]\n\n    if not all(mode == task_modes[0] for mode in task_modes):\n        raise Exception(\'Could not combine different task modes for data\')\n\n    return counter, task_modes[0]\n\ndef _download_data(urls, upload_dir):\n    job = rq.get_current_job()\n    local_files = {}\n    for url in urls:\n        name = os.path.basename(urlrequest.url2pathname(urlparse.urlparse(url).path))\n        if name in local_files:\n            raise Exception(""filename collision: {}"".format(name))\n        slogger.glob.info(""Downloading: {}"".format(url))\n        job.meta[\'status\'] = \'{} is being downloaded..\'.format(url)\n        job.save_meta()\n\n        req = urlrequest.Request(url, headers={\'User-Agent\': \'Mozilla/5.0\'})\n        try:\n            with urlrequest.urlopen(req) as fp, open(os.path.join(upload_dir, name), \'wb\') as tfp:\n                while True:\n                    block = fp.read(8192)\n                    if not block:\n                        break\n                    tfp.write(block)\n        except urlerror.HTTPError as err:\n            raise Exception(""Failed to download "" + url + "". "" + str(err.code) + \' - \' + err.reason)\n        except urlerror.URLError as err:\n            raise Exception(""Invalid URL: "" + url + "". "" + err.reason)\n\n        local_files[name] = True\n    return list(local_files.keys())\n\n@transaction.atomic\ndef _create_thread(tid, data):\n    slogger.glob.info(""create task #{}"".format(tid))\n\n    db_task = models.Task.objects.select_for_update().get(pk=tid)\n    db_data = db_task.data\n    if db_task.data.size != 0:\n        raise NotImplementedError(""Adding more data is not implemented"")\n\n    upload_dir = db_data.get_upload_dirname()\n\n    if data[\'remote_files\']:\n        data[\'remote_files\'] = _download_data(data[\'remote_files\'], upload_dir)\n\n    media = _count_files(data)\n    media, task_mode = _validate_data(media)\n\n    if data[\'server_files\']:\n        _copy_data_from_share(data[\'server_files\'], upload_dir)\n\n    job = rq.get_current_job()\n    job.meta[\'status\'] = \'Media files are being extracted...\'\n    job.save_meta()\n\n    db_images = []\n    extractor = None\n\n    for media_type, media_files in media.items():\n        if media_files:\n            if extractor is not None:\n                raise Exception(\'Combined data types are not supported\')\n            extractor = MEDIA_TYPES[media_type][\'extractor\'](\n                source_path=[os.path.join(upload_dir, f) for f in media_files],\n                step=db_data.get_frame_step(),\n                start=db_data.start_frame,\n                stop=data[\'stop_frame\'],\n            )\n    db_task.mode = task_mode\n    db_data.compressed_chunk_type = models.DataChoice.VIDEO if task_mode == \'interpolation\' and not data[\'use_zip_chunks\'] else models.DataChoice.IMAGESET\n    db_data.original_chunk_type = models.DataChoice.VIDEO if task_mode == \'interpolation\' else models.DataChoice.IMAGESET\n\n    def update_progress(progress):\n        progress_animation = \'|/-\\\\\'\n        if not hasattr(update_progress, \'call_counter\'):\n            update_progress.call_counter = 0\n\n        status_template = \'Images are being compressed {}\'\n        if progress:\n            current_progress = \'{}%\'.format(round(progress * 100))\n        else:\n            current_progress = \'{}\'.format(progress_animation[update_progress.call_counter])\n        job.meta[\'status\'] = status_template.format(current_progress)\n        job.save_meta()\n        update_progress.call_counter = (update_progress.call_counter + 1) % len(progress_animation)\n\n    compressed_chunk_writer_class = Mpeg4CompressedChunkWriter if db_data.compressed_chunk_type == DataChoice.VIDEO else ZipCompressedChunkWriter\n    original_chunk_writer_class = Mpeg4ChunkWriter if db_data.original_chunk_type == DataChoice.VIDEO else ZipChunkWriter\n\n    compressed_chunk_writer = compressed_chunk_writer_class(db_data.image_quality)\n    original_chunk_writer = original_chunk_writer_class(100)\n\n    # calculate chunk size if it isn\'t specified\n    if db_data.chunk_size is None:\n        if isinstance(compressed_chunk_writer, ZipCompressedChunkWriter):\n            w, h = extractor.get_image_size()\n            area = h * w\n            db_data.chunk_size = max(2, min(72, 36 * 1920 * 1080 // area))\n        else:\n            db_data.chunk_size = 36\n\n    video_path = """"\n    video_size = (0, 0)\n\n    counter = itertools.count()\n    generator = itertools.groupby(extractor, lambda x: next(counter) // db_data.chunk_size)\n    for chunk_idx, chunk_data in generator:\n        chunk_data = list(chunk_data)\n        original_chunk_path = db_data.get_original_chunk_path(chunk_idx)\n        original_chunk_writer.save_as_chunk(chunk_data, original_chunk_path)\n\n        compressed_chunk_path = db_data.get_compressed_chunk_path(chunk_idx)\n        img_sizes = compressed_chunk_writer.save_as_chunk(chunk_data, compressed_chunk_path)\n\n        if db_task.mode == \'annotation\':\n            db_images.extend([\n                models.Image(\n                    data=db_data,\n                    path=os.path.relpath(data[1], upload_dir),\n                    frame=data[2],\n                    width=size[0],\n                    height=size[1])\n\n                for data, size in zip(chunk_data, img_sizes)\n            ])\n        else:\n            video_size = img_sizes[0]\n            video_path = chunk_data[0][1]\n\n        db_data.size += len(chunk_data)\n        progress = extractor.get_progress(chunk_data[-1][2])\n        update_progress(progress)\n\n    if db_task.mode == \'annotation\':\n        models.Image.objects.bulk_create(db_images)\n        db_images = []\n    else:\n        models.Video.objects.create(\n            data=db_data,\n            path=os.path.relpath(video_path, upload_dir),\n            width=video_size[0], height=video_size[1])\n\n    if db_data.stop_frame == 0:\n        db_data.stop_frame = db_data.start_frame + (db_data.size - 1) * db_data.get_frame_step()\n\n    preview = extractor.get_preview()\n    preview.save(db_data.get_preview_path())\n\n    slogger.glob.info(""Founded frames {} for Data #{}"".format(db_data.size, db_data.id))\n    _save_task_to_db(db_task)\n'"
cvat/apps/engine/urls.py,0,"b'\n# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path, include\nfrom . import views\nfrom rest_framework import routers\nfrom rest_framework import permissions\nfrom drf_yasg.views import get_schema_view\nfrom drf_yasg import openapi\nfrom cvat.apps.restrictions.views import RestrictionsViewSet\n\nschema_view = get_schema_view(\n   openapi.Info(\n      title=""CVAT REST API"",\n      default_version=\'v1\',\n      description=""REST API for Computer Vision Annotation Tool (CVAT)"",\n      terms_of_service=""https://www.google.com/policies/terms/"",\n      contact=openapi.Contact(email=""nikita.manovich@intel.com""),\n      license=openapi.License(name=""MIT License""),\n   ),\n   public=True,\n   permission_classes=(permissions.IsAuthenticated,),\n)\n\nrouter = routers.DefaultRouter(trailing_slash=False)\nrouter.register(\'projects\', views.ProjectViewSet)\nrouter.register(\'tasks\', views.TaskViewSet)\nrouter.register(\'jobs\', views.JobViewSet)\nrouter.register(\'users\', views.UserViewSet)\nrouter.register(\'server\', views.ServerViewSet, basename=\'server\')\nrouter.register(\'plugins\', views.PluginViewSet)\nrouter.register(\'restrictions\', RestrictionsViewSet, basename=\'restrictions\')\n\nurlpatterns = [\n    # Entry point for a client\n    path(\'\', views.dispatch_request),\n    path(\'dashboard/\', views.dispatch_request),\n\n    # documentation for API\n    path(\'api/swagger<str:scheme>\', views.wrap_swagger(\n       schema_view.without_ui(cache_timeout=0)), name=\'schema-json\'),\n    path(\'api/swagger/\', views.wrap_swagger(\n       schema_view.with_ui(\'swagger\', cache_timeout=0)), name=\'schema-swagger-ui\'),\n    path(\'api/docs/\', views.wrap_swagger(\n       schema_view.with_ui(\'redoc\', cache_timeout=0)), name=\'schema-redoc\'),\n\n    # entry point for API\n    path(\'api/v1/auth/\', include(\'cvat.apps.authentication.api_urls\')),\n    path(\'api/v1/\', include((router.urls, \'cvat\'), namespace=\'v1\'))\n]\n'"
cvat/apps/engine/utils.py,0,"b'import ast\nfrom collections import namedtuple\nimport importlib\nimport sys\nimport traceback\n\nImport = namedtuple(""Import"", [""module"", ""name"", ""alias""])\n\ndef parse_imports(source_code: str):\n    root = ast.parse(source_code)\n\n    for node in ast.iter_child_nodes(root):\n        if isinstance(node, ast.Import):\n            module = []\n        elif isinstance(node, ast.ImportFrom):\n            module = node.module\n        else:\n            continue\n\n        for n in node.names:\n            yield Import(module, n.name, n.asname)\n\ndef import_modules(source_code: str):\n    results = {}\n    imports = parse_imports(source_code)\n    for import_ in imports:\n        module = import_.module if import_.module else import_.name\n        loaded_module = importlib.import_module(module)\n\n        if not import_.name == module:\n            loaded_module = getattr(loaded_module, import_.name)\n\n        if import_.alias:\n            results[import_.alias] = loaded_module\n        else:\n            results[import_.name] = loaded_module\n\n    return results\n\nclass InterpreterError(Exception):\n    pass\n\ndef execute_python_code(source_code, global_vars=None, local_vars=None):\n    try:\n        exec(source_code, global_vars, local_vars)\n    except SyntaxError as err:\n        error_class = err.__class__.__name__\n        details = err.args[0]\n        line_number = err.lineno\n        raise InterpreterError(""{} at line {}: {}"".format(error_class, line_number, details))\n    except AssertionError as err:\n        # AssertionError doesn\'t contain any args and line number\n        error_class = err.__class__.__name__\n        raise InterpreterError(""{}"".format(error_class))\n    except Exception as err:\n        error_class = err.__class__.__name__\n        details = err.args[0]\n        _, _, tb = sys.exc_info()\n        line_number = traceback.extract_tb(tb)[-1][1]\n        raise InterpreterError(""{} at line {}: {}"".format(error_class, line_number, details))\n'"
cvat/apps/engine/views.py,0,"b'# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport os.path as osp\nimport shutil\nimport traceback\nfrom datetime import datetime\nfrom tempfile import mkstemp\n\nimport django_rq\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\nfrom django.db import IntegrityError\nfrom django.http import HttpResponse, HttpResponseNotFound\nfrom django.shortcuts import render\nfrom django.utils import timezone\nfrom django.utils.decorators import method_decorator\nfrom django.views.generic import RedirectView\nfrom django_filters import rest_framework as filters\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom drf_yasg import openapi\nfrom drf_yasg.inspectors import CoreAPICompatInspector, NotHandled\nfrom drf_yasg.utils import swagger_auto_schema\nfrom rest_framework import mixins, serializers, status, viewsets\nfrom rest_framework.decorators import action\nfrom rest_framework.exceptions import APIException\nfrom rest_framework.permissions import SAFE_METHODS, IsAuthenticated\nfrom rest_framework.renderers import JSONRenderer\nfrom rest_framework.response import Response\nfrom sendfile import sendfile\n\nimport cvat.apps.dataset_manager as dm\nimport cvat.apps.dataset_manager.views # pylint: disable=unused-import\nfrom cvat.apps.authentication import auth\nfrom cvat.apps.authentication.decorators import login_required\nfrom cvat.apps.dataset_manager.serializers import DatasetFormatsSerializer\nfrom cvat.apps.engine.frame_provider import FrameProvider\nfrom cvat.apps.engine.models import Job, Plugin, StatusChoice, Task\nfrom cvat.apps.engine.serializers import (\n    AboutSerializer, AnnotationFileSerializer, BasicUserSerializer,\n    DataMetaSerializer, DataSerializer, ExceptionSerializer,\n    FileInfoSerializer, JobSerializer, LabeledDataSerializer,\n    LogEventSerializer, PluginSerializer, ProjectSerializer,\n    RqStatusSerializer, TaskSerializer, UserSerializer)\nfrom cvat.settings.base import CSS_3RDPARTY, JS_3RDPARTY\n\nfrom . import models, task\nfrom .log import clogger, slogger\n\n\n# drf-yasg component doesn\'t handle correctly URL_FORMAT_OVERRIDE and\n# send requests with ?format=openapi suffix instead of ?scheme=openapi.\n# We map the required paramater explicitly and add it into query arguments\n# on the server side.\ndef wrap_swagger(view):\n    @login_required\n    def _map_format_to_schema(request, scheme=None):\n        if \'format\' in request.GET:\n            request.GET = request.GET.copy()\n            format_alias = settings.REST_FRAMEWORK[\'URL_FORMAT_OVERRIDE\']\n            request.GET[format_alias] = request.GET[\'format\']\n\n        return view(request, format=scheme)\n\n    return _map_format_to_schema\n\n# Server REST API\n@login_required\ndef dispatch_request(request):\n    """"""An entry point to dispatch legacy requests""""""\n    if \'dashboard\' in request.path or (request.path == \'/\' and \'id\' not in request.GET):\n        return RedirectView.as_view(\n            url=settings.UI_URL,\n            permanent=True,\n            query_string=True\n        )(request)\n    elif request.method == \'GET\' and \'id\' in request.GET and request.path == \'/\':\n        return render(request, \'engine/annotation.html\', {\n            \'css_3rdparty\': CSS_3RDPARTY.get(\'engine\', []),\n            \'js_3rdparty\': JS_3RDPARTY.get(\'engine\', []),\n            \'status_list\': [str(i) for i in StatusChoice],\n            \'ui_url\': settings.UI_URL\n        })\n    else:\n        return HttpResponseNotFound()\n\n\nclass ServerViewSet(viewsets.ViewSet):\n    serializer_class = None\n\n    # To get nice documentation about ServerViewSet actions it is necessary\n    # to implement the method. By default, ViewSet doesn\'t provide it.\n    def get_serializer(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Method provides basic CVAT information\',\n        responses={\'200\': AboutSerializer})\n    @action(detail=False, methods=[\'GET\'], serializer_class=AboutSerializer)\n    def about(request):\n        from cvat import __version__ as cvat_version\n        about = {\n            ""name"": ""Computer Vision Annotation Tool"",\n            ""version"": cvat_version,\n            ""description"": ""CVAT is completely re-designed and re-implemented "" +\n                ""version of Video Annotation Tool from Irvine, California "" +\n                ""tool. It is free, online, interactive video and image annotation "" +\n                ""tool for computer vision. It is being used by our team to "" +\n                ""annotate million of objects with different properties. Many UI "" +\n                ""and UX decisions are based on feedbacks from professional data "" +\n                ""annotation team.""\n        }\n        serializer = AboutSerializer(data=about)\n        if serializer.is_valid(raise_exception=True):\n            return Response(data=serializer.data)\n\n    @staticmethod\n    @swagger_auto_schema(method=\'post\', request_body=ExceptionSerializer)\n    @action(detail=False, methods=[\'POST\'], serializer_class=ExceptionSerializer)\n    def exception(request):\n        """"""\n        Saves an exception from a client on the server\n\n        Sends logs to the ELK if it is connected\n        """"""\n        serializer = ExceptionSerializer(data=request.data)\n        if serializer.is_valid(raise_exception=True):\n            additional_info = {\n                ""username"": request.user.username,\n                ""name"": ""Send exception"",\n            }\n            message = JSONRenderer().render({**serializer.data, **additional_info}).decode(\'UTF-8\')\n            jid = serializer.data.get(""job_id"")\n            tid = serializer.data.get(""task_id"")\n            if jid:\n                clogger.job[jid].error(message)\n            elif tid:\n                clogger.task[tid].error(message)\n            else:\n                clogger.glob.error(message)\n\n            return Response(serializer.data, status=status.HTTP_201_CREATED)\n\n    @staticmethod\n    @swagger_auto_schema(method=\'post\', request_body=LogEventSerializer(many=True))\n    @action(detail=False, methods=[\'POST\'], serializer_class=LogEventSerializer)\n    def logs(request):\n        """"""\n        Saves logs from a client on the server\n\n        Sends logs to the ELK if it is connected\n        """"""\n        serializer = LogEventSerializer(many=True, data=request.data)\n        if serializer.is_valid(raise_exception=True):\n            user = { ""username"": request.user.username }\n            for event in serializer.data:\n                message = JSONRenderer().render({**event, **user}).decode(\'UTF-8\')\n                jid = event.get(""job_id"")\n                tid = event.get(""task_id"")\n                if jid:\n                    clogger.job[jid].info(message)\n                elif tid:\n                    clogger.task[tid].info(message)\n                else:\n                    clogger.glob.info(message)\n            return Response(serializer.data, status=status.HTTP_201_CREATED)\n\n    @staticmethod\n    @swagger_auto_schema(\n        method=\'get\', operation_summary=\'Returns all files and folders that are on the server along specified path\',\n        manual_parameters=[openapi.Parameter(\'directory\', openapi.IN_QUERY, type=openapi.TYPE_STRING, description=\'Directory to browse\')],\n        responses={\'200\' : FileInfoSerializer(many=True)}\n    )\n    @action(detail=False, methods=[\'GET\'], serializer_class=FileInfoSerializer)\n    def share(request):\n        param = request.query_params.get(\'directory\', \'/\')\n        if param.startswith(""/""):\n            param = param[1:]\n        directory = os.path.abspath(os.path.join(settings.SHARE_ROOT, param))\n\n        if directory.startswith(settings.SHARE_ROOT) and os.path.isdir(directory):\n            data = []\n            content = os.scandir(directory)\n            for entry in content:\n                entry_type = None\n                if entry.is_file():\n                    entry_type = ""REG""\n                elif entry.is_dir():\n                    entry_type = ""DIR""\n\n                if entry_type:\n                    data.append({""name"": entry.name, ""type"": entry_type})\n\n            serializer = FileInfoSerializer(many=True, data=data)\n            if serializer.is_valid(raise_exception=True):\n                return Response(serializer.data)\n        else:\n            return Response(""{} is an invalid directory"".format(param),\n                status=status.HTTP_400_BAD_REQUEST)\n\n    @staticmethod\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Method provides the list of supported annotations formats\',\n        responses={\'200\': DatasetFormatsSerializer()})\n    @action(detail=False, methods=[\'GET\'], url_path=\'annotation/formats\')\n    def annotation_formats(request):\n        data = dm.views.get_all_formats()\n        return Response(DatasetFormatsSerializer(data).data)\n\nclass ProjectFilter(filters.FilterSet):\n    name = filters.CharFilter(field_name=""name"", lookup_expr=""icontains"")\n    owner = filters.CharFilter(field_name=""owner__username"", lookup_expr=""icontains"")\n    status = filters.CharFilter(field_name=""status"", lookup_expr=""icontains"")\n    assignee = filters.CharFilter(field_name=""assignee__username"", lookup_expr=""icontains"")\n\n    class Meta:\n        model = models.Project\n        fields = (""id"", ""name"", ""owner"", ""status"", ""assignee"")\n\n@method_decorator(name=\'list\', decorator=swagger_auto_schema(\n    operation_summary=\'Returns a paginated list of projects according to query parameters (10 projects per page)\',\n    manual_parameters=[\n        openapi.Parameter(\'id\', openapi.IN_QUERY, description=""A unique number value identifying this project"",\n            type=openapi.TYPE_NUMBER),\n        openapi.Parameter(\'name\', openapi.IN_QUERY, description=""Find all projects where name contains a parameter value"",\n            type=openapi.TYPE_STRING),\n        openapi.Parameter(\'owner\', openapi.IN_QUERY, description=""Find all project where owner name contains a parameter value"",\n            type=openapi.TYPE_STRING),\n        openapi.Parameter(\'status\', openapi.IN_QUERY, description=""Find all projects with a specific status"",\n            type=openapi.TYPE_STRING, enum=[str(i) for i in StatusChoice]),\n        openapi.Parameter(\'assignee\', openapi.IN_QUERY, description=""Find all projects where assignee name contains a parameter value"",\n            type=openapi.TYPE_STRING)]))\n@method_decorator(name=\'create\', decorator=swagger_auto_schema(operation_summary=\'Method creates a new project\'))\n@method_decorator(name=\'retrieve\', decorator=swagger_auto_schema(operation_summary=\'Method returns details of a specific project\'))\n@method_decorator(name=\'destroy\', decorator=swagger_auto_schema(operation_summary=\'Method deletes a specific project\'))\n@method_decorator(name=\'partial_update\', decorator=swagger_auto_schema(operation_summary=\'Methods does a partial update of chosen fields in a project\'))\nclass ProjectViewSet(auth.ProjectGetQuerySetMixin, viewsets.ModelViewSet):\n    queryset = models.Project.objects.all().order_by(\'-id\')\n    serializer_class = ProjectSerializer\n    search_fields = (""name"", ""owner__username"", ""assignee__username"", ""status"")\n    filterset_class = ProjectFilter\n    ordering_fields = (""id"", ""name"", ""owner"", ""status"", ""assignee"")\n    http_method_names = [\'get\', \'post\', \'head\', \'patch\', \'delete\']\n\n    def get_permissions(self):\n        http_method = self.request.method\n        permissions = [IsAuthenticated]\n\n        if http_method in SAFE_METHODS:\n            permissions.append(auth.ProjectAccessPermission)\n        elif http_method in [""POST""]:\n            permissions.append(auth.ProjectCreatePermission)\n        elif http_method in [""PATCH""]:\n            permissions.append(auth.ProjectChangePermission)\n        elif http_method in [""DELETE""]:\n            permissions.append(auth.ProjectDeletePermission)\n        else:\n            permissions.append(auth.AdminRolePermission)\n\n        return [perm() for perm in permissions]\n\n    def perform_create(self, serializer):\n        if self.request.data.get(\'owner\', None):\n            serializer.save()\n        else:\n            serializer.save(owner=self.request.user)\n\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Returns information of the tasks of the project with the selected id\',\n        responses={\'200\': TaskSerializer(many=True)})\n    @action(detail=True, methods=[\'GET\'], serializer_class=TaskSerializer)\n    def tasks(self, request, pk):\n        self.get_object() # force to call check_object_permissions\n        queryset = Task.objects.filter(project_id=pk).order_by(\'-id\')\n        queryset = auth.filter_task_queryset(queryset, request.user)\n\n        page = self.paginate_queryset(queryset)\n        if page is not None:\n            serializer = self.get_serializer(page, many=True,\n                context={""request"": request})\n            return self.get_paginated_response(serializer.data)\n\n        serializer = self.get_serializer(queryset, many=True,\n            context={""request"": request})\n        return Response(serializer.data)\n\nclass TaskFilter(filters.FilterSet):\n    project = filters.CharFilter(field_name=""project__name"", lookup_expr=""icontains"")\n    name = filters.CharFilter(field_name=""name"", lookup_expr=""icontains"")\n    owner = filters.CharFilter(field_name=""owner__username"", lookup_expr=""icontains"")\n    mode = filters.CharFilter(field_name=""mode"", lookup_expr=""icontains"")\n    status = filters.CharFilter(field_name=""status"", lookup_expr=""icontains"")\n    assignee = filters.CharFilter(field_name=""assignee__username"", lookup_expr=""icontains"")\n\n    class Meta:\n        model = Task\n        fields = (""id"", ""project_id"", ""project"", ""name"", ""owner"", ""mode"", ""status"",\n            ""assignee"")\n\nclass DjangoFilterInspector(CoreAPICompatInspector):\n    def get_filter_parameters(self, filter_backend):\n        if isinstance(filter_backend, DjangoFilterBackend):\n            result = super(DjangoFilterInspector, self).get_filter_parameters(filter_backend)\n            res = result.copy()\n\n            for param in result:\n                if param.get(\'name\') == \'project_id\' or param.get(\'name\') == \'project\':\n                    res.remove(param)\n            return res\n\n        return NotHandled\n\n@method_decorator(name=\'list\', decorator=swagger_auto_schema(\n    operation_summary=\'Returns a paginated list of tasks according to query parameters (10 tasks per page)\',\n    manual_parameters=[\n            openapi.Parameter(\'id\',openapi.IN_QUERY,description=""A unique number value identifying this task"",type=openapi.TYPE_NUMBER),\n            openapi.Parameter(\'name\', openapi.IN_QUERY, description=""Find all tasks where name contains a parameter value"", type=openapi.TYPE_STRING),\n            openapi.Parameter(\'owner\', openapi.IN_QUERY, description=""Find all tasks where owner name contains a parameter value"", type=openapi.TYPE_STRING),\n            openapi.Parameter(\'mode\', openapi.IN_QUERY, description=""Find all tasks with a specific mode"", type=openapi.TYPE_STRING, enum=[\'annotation\', \'interpolation\']),\n            openapi.Parameter(\'status\', openapi.IN_QUERY, description=""Find all tasks with a specific status"", type=openapi.TYPE_STRING,enum=[\'annotation\',\'validation\',\'completed\']),\n            openapi.Parameter(\'assignee\', openapi.IN_QUERY, description=""Find all tasks where assignee name contains a parameter value"", type=openapi.TYPE_STRING)\n        ],\n    filter_inspectors=[DjangoFilterInspector]))\n@method_decorator(name=\'create\', decorator=swagger_auto_schema(operation_summary=\'Method creates a new task in a database without any attached images and videos\'))\n@method_decorator(name=\'retrieve\', decorator=swagger_auto_schema(operation_summary=\'Method returns details of a specific task\'))\n@method_decorator(name=\'update\', decorator=swagger_auto_schema(operation_summary=\'Method updates a task by id\'))\n@method_decorator(name=\'destroy\', decorator=swagger_auto_schema(operation_summary=\'Method deletes a specific task, all attached jobs, annotations, and data\'))\n@method_decorator(name=\'partial_update\', decorator=swagger_auto_schema(operation_summary=\'Methods does a partial update of chosen fields in a task\'))\nclass TaskViewSet(auth.TaskGetQuerySetMixin, viewsets.ModelViewSet):\n    queryset = Task.objects.all().prefetch_related(\n            ""label_set__attributespec_set"",\n            ""segment_set__job_set"",\n        ).order_by(\'-id\')\n    serializer_class = TaskSerializer\n    search_fields = (""name"", ""owner__username"", ""mode"", ""status"")\n    filterset_class = TaskFilter\n    ordering_fields = (""id"", ""name"", ""owner"", ""status"", ""assignee"")\n\n    def get_permissions(self):\n        http_method = self.request.method\n        permissions = [IsAuthenticated]\n\n        if http_method in SAFE_METHODS:\n            permissions.append(auth.TaskAccessPermission)\n        elif http_method in [""POST""]:\n            permissions.append(auth.TaskCreatePermission)\n        elif self.action == \'annotations\' or http_method in [""PATCH"", ""PUT""]:\n            permissions.append(auth.TaskChangePermission)\n        elif http_method in [""DELETE""]:\n            permissions.append(auth.TaskDeletePermission)\n        else:\n            permissions.append(auth.AdminRolePermission)\n\n        return [perm() for perm in permissions]\n\n    def perform_create(self, serializer):\n        def validate_task_limit(owner):\n            admin_perm = auth.AdminRolePermission()\n            is_admin = admin_perm.has_permission(self.request, self)\n            if not is_admin and settings.RESTRICTIONS[\'task_limit\'] is not None and \\\n                Task.objects.filter(owner=owner).count() >= settings.RESTRICTIONS[\'task_limit\']:\n                raise serializers.ValidationError(\'The user has the maximum number of tasks\')\n\n        owner = self.request.data.get(\'owner\', None)\n        if owner:\n            validate_task_limit(owner)\n            serializer.save()\n        else:\n            validate_task_limit(self.request.user)\n            serializer.save(owner=self.request.user)\n\n    def perform_destroy(self, instance):\n        task_dirname = instance.get_task_dirname()\n        super().perform_destroy(instance)\n        shutil.rmtree(task_dirname, ignore_errors=True)\n        if instance.data and not instance.data.tasks.all():\n            shutil.rmtree(instance.data.get_data_dirname(), ignore_errors=True)\n            instance.data.delete()\n\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Returns a list of jobs for a specific task\',\n        responses={\'200\': JobSerializer(many=True)})\n    @action(detail=True, methods=[\'GET\'], serializer_class=JobSerializer)\n    def jobs(self, request, pk):\n        self.get_object() # force to call check_object_permissions\n        queryset = Job.objects.filter(segment__task_id=pk)\n        serializer = JobSerializer(queryset, many=True,\n            context={""request"": request})\n\n        return Response(serializer.data)\n\n    @swagger_auto_schema(method=\'post\', operation_summary=\'Method permanently attaches images or video to a task\')\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Method returns data for a specific task\',\n        manual_parameters=[\n            openapi.Parameter(\'type\', in_=openapi.IN_QUERY, required=True, type=openapi.TYPE_STRING,\n                enum=[\'chunk\', \'frame\', \'preview\'],\n                description=""Specifies the type of the requested data""),\n            openapi.Parameter(\'quality\', in_=openapi.IN_QUERY, required=True, type=openapi.TYPE_STRING,\n                enum=[\'compressed\', \'original\'],\n                description=""Specifies the quality level of the requested data, doesn\'t matter for \'preview\' type""),\n            openapi.Parameter(\'number\', in_=openapi.IN_QUERY, required=True, type=openapi.TYPE_NUMBER,\n                description=""A unique number value identifying chunk or frame, doesn\'t matter for \'preview\' type""),\n            ]\n    )\n    @action(detail=True, methods=[\'POST\', \'GET\'])\n    def data(self, request, pk):\n        if request.method == \'POST\':\n            db_task = self.get_object() # call check_object_permissions as well\n            serializer = DataSerializer(data=request.data)\n            serializer.is_valid(raise_exception=True)\n            db_data = serializer.save()\n            db_task.data = db_data\n            db_task.save()\n            data = {k:v for k, v in serializer.data.items()}\n            data[\'use_zip_chunks\'] = serializer.validated_data[\'use_zip_chunks\']\n            # if the value of stop_frame is 0, then inside the function we cannot know\n            # the value specified by the user or it\'s default value from the database\n            if \'stop_frame\' not in serializer.validated_data:\n                data[\'stop_frame\'] = None\n            task.create(db_task.id, data)\n            return Response(serializer.data, status=status.HTTP_202_ACCEPTED)\n        else:\n            data_type = request.query_params.get(\'type\', None)\n            data_id = request.query_params.get(\'number\', None)\n            data_quality = request.query_params.get(\'quality\', \'compressed\')\n\n            possible_data_type_values = (\'chunk\', \'frame\', \'preview\')\n            possible_quality_values = (\'compressed\', \'original\')\n\n            if not data_type or data_type not in possible_data_type_values:\n                return Response(data=\'data type not specified or has wrong value\', status=status.HTTP_400_BAD_REQUEST)\n            elif data_type == \'chunk\' or data_type == \'frame\':\n                if not data_id:\n                    return Response(data=\'number not specified\', status=status.HTTP_400_BAD_REQUEST)\n                elif data_quality not in possible_quality_values:\n                    return Response(data=\'wrong quality value\', status=status.HTTP_400_BAD_REQUEST)\n\n            try:\n                db_task = self.get_object()\n                frame_provider = FrameProvider(db_task.data)\n\n                if data_type == \'chunk\':\n                    data_id = int(data_id)\n                    data_quality = FrameProvider.Quality.COMPRESSED \\\n                        if data_quality == \'compressed\' else FrameProvider.Quality.ORIGINAL\n                    path = os.path.realpath(frame_provider.get_chunk(data_id, data_quality))\n\n                    # Follow symbol links if the chunk is a link on a real image otherwise\n                    # mimetype detection inside sendfile will work incorrectly.\n                    return sendfile(request, path)\n\n                elif data_type == \'frame\':\n                    data_id = int(data_id)\n                    data_quality = FrameProvider.Quality.COMPRESSED \\\n                        if data_quality == \'compressed\' else FrameProvider.Quality.ORIGINAL\n                    buf, mime = frame_provider.get_frame(data_id, data_quality)\n\n                    return HttpResponse(buf.getvalue(), content_type=mime)\n\n                elif data_type == \'preview\':\n                    return sendfile(request, frame_provider.get_preview())\n                else:\n                    return Response(data=\'unknown data type {}.\'.format(data_type), status=status.HTTP_400_BAD_REQUEST)\n            except APIException as e:\n                return Response(data=e.default_detail, status=e.status_code)\n            except Exception as e:\n                msg = \'cannot get requested data type: {}, number: {}, quality: {}\'.format(data_type, data_id, data_quality)\n                slogger.task[pk].error(msg, exc_info=True)\n                return Response(data=msg + \'\\n\' + str(e), status=status.HTTP_400_BAD_REQUEST)\n\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Method allows to download task annotations\',\n        manual_parameters=[\n            openapi.Parameter(\'format\', openapi.IN_QUERY,\n                description=""Desired output format name\\nYou can get the list of supported formats at:\\n/server/annotation/formats"",\n                type=openapi.TYPE_STRING, required=False),\n            openapi.Parameter(\'filename\', openapi.IN_QUERY,\n                description=""Desired output file name"",\n                type=openapi.TYPE_STRING, required=False),\n            openapi.Parameter(\'action\', in_=openapi.IN_QUERY,\n                description=\'Used to start downloading process after annotation file had been created\',\n                type=openapi.TYPE_STRING, required=False, enum=[\'download\'])\n        ],\n        responses={\n            \'202\': openapi.Response(description=\'Dump of annotations has been started\'),\n            \'201\': openapi.Response(description=\'Annotations file is ready to download\'),\n            \'200\': openapi.Response(description=\'Download of file started\'),\n            \'405\': openapi.Response(description=\'Format is not available\'),\n        }\n    )\n    @swagger_auto_schema(method=\'put\', operation_summary=\'Method allows to upload task annotations\',\n        manual_parameters=[\n            openapi.Parameter(\'format\', openapi.IN_QUERY,\n                description=""Input format name\\nYou can get the list of supported formats at:\\n/server/annotation/formats"",\n                type=openapi.TYPE_STRING, required=False),\n        ],\n        responses={\n            \'202\': openapi.Response(description=\'Uploading has been started\'),\n            \'201\': openapi.Response(description=\'Uploading has finished\'),\n            \'405\': openapi.Response(description=\'Format is not available\'),\n        }\n    )\n    @swagger_auto_schema(method=\'patch\', operation_summary=\'Method performs a partial update of annotations in a specific task\',\n        manual_parameters=[openapi.Parameter(\'action\', in_=openapi.IN_QUERY, required=True, type=openapi.TYPE_STRING,\n            enum=[\'create\', \'update\', \'delete\'])])\n    @swagger_auto_schema(method=\'delete\', operation_summary=\'Method deletes all annotations for a specific task\')\n    @action(detail=True, methods=[\'GET\', \'DELETE\', \'PUT\', \'PATCH\'],\n        serializer_class=LabeledDataSerializer)\n    def annotations(self, request, pk):\n        db_task = self.get_object() # force to call check_object_permissions\n        if request.method == \'GET\':\n            format_name = request.query_params.get(\'format\')\n            if format_name:\n                return _export_annotations(db_task=db_task,\n                    rq_id=""/api/v1/tasks/{}/annotations/{}"".format(pk, format_name),\n                    request=request,\n                    action=request.query_params.get(""action"", """").lower(),\n                    callback=dm.views.export_task_annotations,\n                    format_name=format_name,\n                    filename=request.query_params.get(""filename"", """").lower(),\n                )\n            else:\n                data = dm.task.get_task_data(pk)\n                serializer = LabeledDataSerializer(data=data)\n                if serializer.is_valid(raise_exception=True):\n                    return Response(serializer.data)\n        elif request.method == \'PUT\':\n            format_name = request.query_params.get(\'format\')\n            if format_name:\n                return _import_annotations(\n                    request=request,\n                    rq_id=""{}@/api/v1/tasks/{}/annotations/upload"".format(request.user, pk),\n                    rq_func=dm.task.import_task_annotations,\n                    pk=pk,\n                    format_name=format_name,\n                )\n            else:\n                serializer = LabeledDataSerializer(data=request.data)\n                if serializer.is_valid(raise_exception=True):\n                    data = dm.task.put_task_data(pk, serializer.data)\n                    return Response(data)\n        elif request.method == \'DELETE\':\n            dm.task.delete_task_data(pk)\n            return Response(status=status.HTTP_204_NO_CONTENT)\n        elif request.method == \'PATCH\':\n            action = self.request.query_params.get(""action"", None)\n            if action not in dm.task.PatchAction.values():\n                raise serializers.ValidationError(\n                    ""Please specify a correct \'action\' for the request"")\n            serializer = LabeledDataSerializer(data=request.data)\n            if serializer.is_valid(raise_exception=True):\n                try:\n                    data = dm.task.patch_task_data(pk, serializer.data, action)\n                except (AttributeError, IntegrityError) as e:\n                    return Response(data=str(e), status=status.HTTP_400_BAD_REQUEST)\n                return Response(data)\n\n    @swagger_auto_schema(method=\'get\', operation_summary=\'When task is being created the method returns information about a status of the creation process\')\n    @action(detail=True, methods=[\'GET\'], serializer_class=RqStatusSerializer)\n    def status(self, request, pk):\n        self.get_object() # force to call check_object_permissions\n        response = self._get_rq_response(queue=""default"",\n            job_id=""/api/{}/tasks/{}"".format(request.version, pk))\n        serializer = RqStatusSerializer(data=response)\n\n        if serializer.is_valid(raise_exception=True):\n            return Response(serializer.data)\n\n    @staticmethod\n    def _get_rq_response(queue, job_id):\n        queue = django_rq.get_queue(queue)\n        job = queue.fetch_job(job_id)\n        response = {}\n        if job is None or job.is_finished:\n            response = { ""state"": ""Finished"" }\n        elif job.is_queued:\n            response = { ""state"": ""Queued"" }\n        elif job.is_failed:\n            response = { ""state"": ""Failed"", ""message"": job.exc_info }\n        else:\n            response = { ""state"": ""Started"" }\n            if \'status\' in job.meta:\n                response[\'message\'] = job.meta[\'status\']\n\n        return response\n\n    @staticmethod\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Method provides a meta information about media files which are related with the task\',\n        responses={\'200\': DataMetaSerializer()})\n    @action(detail=True, methods=[\'GET\'], serializer_class=DataMetaSerializer,\n        url_path=\'data/meta\')\n    def data_info(request, pk):\n        db_task = models.Task.objects.prefetch_related(\'data__images\').select_related(\'data__video\').get(pk=pk)\n\n        if hasattr(db_task.data, \'video\'):\n            media = [db_task.data.video]\n        else:\n            media = list(db_task.data.images.order_by(\'frame\'))\n\n        frame_meta = [{\n            \'width\': item.width,\n            \'height\': item.height,\n            \'name\': item.path,\n        } for item in media]\n\n        db_data = db_task.data\n        db_data.frames = frame_meta\n\n        serializer = DataMetaSerializer(db_data)\n        return Response(serializer.data)\n\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Export task as a dataset in a specific format\',\n        manual_parameters=[\n            openapi.Parameter(\'format\', openapi.IN_QUERY,\n                description=""Desired output format name\\nYou can get the list of supported formats at:\\n/server/annotation/formats"",\n                type=openapi.TYPE_STRING, required=True),\n            openapi.Parameter(\'filename\', openapi.IN_QUERY,\n                description=""Desired output file name"",\n                type=openapi.TYPE_STRING, required=False),\n            openapi.Parameter(\'action\', in_=openapi.IN_QUERY,\n                description=\'Used to start downloading process after annotation file had been created\',\n                type=openapi.TYPE_STRING, required=False, enum=[\'download\'])\n        ],\n        responses={\'202\': openapi.Response(description=\'Exporting has been started\'),\n            \'201\': openapi.Response(description=\'Output file is ready for downloading\'),\n            \'200\': openapi.Response(description=\'Download of file started\'),\n            \'405\': openapi.Response(description=\'Format is not available\'),\n        }\n    )\n    @action(detail=True, methods=[\'GET\'], serializer_class=None,\n        url_path=\'dataset\')\n    def dataset_export(self, request, pk):\n        db_task = self.get_object() # force to call check_object_permissions\n\n        format_name = request.query_params.get(""format"", """")\n        return _export_annotations(db_task=db_task,\n            rq_id=""/api/v1/tasks/{}/dataset/{}"".format(pk, format_name),\n            request=request,\n            action=request.query_params.get(""action"", """").lower(),\n            callback=dm.views.export_task_as_dataset,\n            format_name=format_name,\n            filename=request.query_params.get(""filename"", """").lower(),\n        )\n\n@method_decorator(name=\'retrieve\', decorator=swagger_auto_schema(operation_summary=\'Method returns details of a job\'))\n@method_decorator(name=\'update\', decorator=swagger_auto_schema(operation_summary=\'Method updates a job by id\'))\n@method_decorator(name=\'partial_update\', decorator=swagger_auto_schema(\n    operation_summary=\'Methods does a partial update of chosen fields in a job\'))\nclass JobViewSet(viewsets.GenericViewSet,\n    mixins.RetrieveModelMixin, mixins.UpdateModelMixin):\n    queryset = Job.objects.all().order_by(\'id\')\n    serializer_class = JobSerializer\n\n    def get_permissions(self):\n        http_method = self.request.method\n        permissions = [IsAuthenticated]\n\n        if http_method in SAFE_METHODS:\n            permissions.append(auth.JobAccessPermission)\n        elif http_method in [""PATCH"", ""PUT"", ""DELETE""]:\n            permissions.append(auth.JobChangePermission)\n        else:\n            permissions.append(auth.AdminRolePermission)\n\n        return [perm() for perm in permissions]\n\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Method returns annotations for a specific job\')\n    @swagger_auto_schema(method=\'put\', operation_summary=\'Method performs an update of all annotations in a specific job\')\n    @swagger_auto_schema(method=\'patch\', manual_parameters=[\n        openapi.Parameter(\'action\', in_=openapi.IN_QUERY, type=openapi.TYPE_STRING, required=True,\n            enum=[\'create\', \'update\', \'delete\'])],\n            operation_summary=\'Method performs a partial update of annotations in a specific job\')\n    @swagger_auto_schema(method=\'delete\', operation_summary=\'Method deletes all annotations for a specific job\')\n    @action(detail=True, methods=[\'GET\', \'DELETE\', \'PUT\', \'PATCH\'],\n        serializer_class=LabeledDataSerializer)\n    def annotations(self, request, pk):\n        self.get_object() # force to call check_object_permissions\n        if request.method == \'GET\':\n            data = dm.task.get_job_data(pk)\n            return Response(data)\n        elif request.method == \'PUT\':\n            format_name = request.query_params.get(""format"", """")\n            if format_name:\n                return _import_annotations(\n                    request=request,\n                    rq_id=""{}@/api/v1/jobs/{}/annotations/upload"".format(request.user, pk),\n                    rq_func=dm.task.import_job_annotations,\n                    pk=pk,\n                    format_name=format_name\n                )\n            else:\n                serializer = LabeledDataSerializer(data=request.data)\n                if serializer.is_valid(raise_exception=True):\n                    try:\n                        data = dm.task.put_job_data(pk, serializer.data)\n                    except (AttributeError, IntegrityError) as e:\n                        return Response(data=str(e), status=status.HTTP_400_BAD_REQUEST)\n                    return Response(data)\n        elif request.method == \'DELETE\':\n            dm.task.delete_job_data(pk)\n            return Response(status=status.HTTP_204_NO_CONTENT)\n        elif request.method == \'PATCH\':\n            action = self.request.query_params.get(""action"", None)\n            if action not in dm.task.PatchAction.values():\n                raise serializers.ValidationError(\n                    ""Please specify a correct \'action\' for the request"")\n            serializer = LabeledDataSerializer(data=request.data)\n            if serializer.is_valid(raise_exception=True):\n                try:\n                    data = dm.task.patch_job_data(pk, serializer.data, action)\n                except (AttributeError, IntegrityError) as e:\n                    return Response(data=str(e), status=status.HTTP_400_BAD_REQUEST)\n                return Response(data)\n\n@method_decorator(name=\'list\', decorator=swagger_auto_schema(\n    operation_summary=\'Method provides a paginated list of users registered on the server\'))\n@method_decorator(name=\'retrieve\', decorator=swagger_auto_schema(\n    operation_summary=\'Method provides information of a specific user\'))\n@method_decorator(name=\'partial_update\', decorator=swagger_auto_schema(\n    operation_summary=\'Method updates chosen fields of a user\'))\n@method_decorator(name=\'destroy\', decorator=swagger_auto_schema(\n    operation_summary=\'Method deletes a specific user from the server\'))\nclass UserViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,\n    mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin):\n    queryset = User.objects.all().order_by(\'id\')\n    http_method_names = [\'get\', \'post\', \'head\', \'patch\', \'delete\']\n\n    def get_serializer_class(self):\n        user = self.request.user\n        if user.is_staff:\n            return UserSerializer\n        else:\n            is_self = int(self.kwargs.get(""pk"", 0)) == user.id or \\\n                self.action == ""self""\n            if is_self and self.request.method in SAFE_METHODS:\n                return UserSerializer\n            else:\n                return BasicUserSerializer\n\n    def get_permissions(self):\n        permissions = [IsAuthenticated]\n        user = self.request.user\n\n        if not self.request.method in SAFE_METHODS:\n            is_self = int(self.kwargs.get(""pk"", 0)) == user.id\n            if not is_self:\n                permissions.append(auth.AdminRolePermission)\n\n        return [perm() for perm in permissions]\n\n    @swagger_auto_schema(method=\'get\', operation_summary=\'Method returns an instance of a user who is currently authorized\')\n    @action(detail=False, methods=[\'GET\'])\n    def self(self, request):\n        """"""\n        Method returns an instance of a user who is currently authorized\n        """"""\n        serializer_class = self.get_serializer_class()\n        serializer = serializer_class(request.user, context={ ""request"": request })\n        return Response(serializer.data)\n\nclass PluginViewSet(viewsets.ModelViewSet):\n    queryset = Plugin.objects.all()\n    serializer_class = PluginSerializer\n\n    # @action(detail=True, methods=[\'GET\', \'PATCH\', \'PUT\'], serializer_class=None)\n    # def config(self, request, name):\n    #     pass\n\n    # @action(detail=True, methods=[\'GET\', \'POST\'], serializer_class=None)\n    # def data(self, request, name):\n    #     pass\n\n    # @action(detail=True, methods=[\'GET\', \'DELETE\', \'PATCH\', \'PUT\'],\n    #     serializer_class=None, url_path=\'data/(?P<id>\\d+)\')\n    # def data_detail(self, request, name, id):\n    #     pass\n\n\n    @action(detail=True, methods=[\'GET\', \'POST\'], serializer_class=RqStatusSerializer)\n    def requests(self, request, name):\n        pass\n\n    @action(detail=True, methods=[\'GET\', \'DELETE\'],\n        serializer_class=RqStatusSerializer, url_path=\'requests/(?P<id>\\d+)\')\n    def request_detail(self, request, name, rq_id):\n        pass\n\ndef rq_handler(job, exc_type, exc_value, tb):\n    job.exc_info = """".join(\n        traceback.format_exception_only(exc_type, exc_value))\n    job.save()\n    if ""tasks"" in job.id.split(""/""):\n        return task.rq_handler(job, exc_type, exc_value, tb)\n\n    return True\n\n# TODO: Method should be reimplemented as a separated view\n# @swagger_auto_schema(method=\'put\', manual_parameters=[openapi.Parameter(\'format\', in_=openapi.IN_QUERY,\n#         description=\'A name of a loader\\nYou can get annotation loaders from this API:\\n/server/annotation/formats\',\n#         required=True, type=openapi.TYPE_STRING)],\n#     operation_summary=\'Method allows to upload annotations\',\n#     responses={\'202\': openapi.Response(description=\'Load of annotations has been started\'),\n#         \'201\': openapi.Response(description=\'Annotations have been uploaded\')},\n#     tags=[\'tasks\'])\n# @api_view([\'PUT\'])\ndef _import_annotations(request, rq_id, rq_func, pk, format_name):\n    format_desc = {f.DISPLAY_NAME: f\n        for f in dm.views.get_import_formats()}.get(format_name)\n    if format_desc is None:\n        raise serializers.ValidationError(\n            ""Unknown input format \'{}\'"".format(format_name))\n    elif not format_desc.ENABLED:\n        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n\n    queue = django_rq.get_queue(""default"")\n    rq_job = queue.fetch_job(rq_id)\n\n    if not rq_job:\n        serializer = AnnotationFileSerializer(data=request.data)\n        if serializer.is_valid(raise_exception=True):\n            anno_file = serializer.validated_data[\'annotation_file\']\n            fd, filename = mkstemp(prefix=\'cvat_{}\'.format(pk))\n            with open(filename, \'wb+\') as f:\n                for chunk in anno_file.chunks():\n                    f.write(chunk)\n            rq_job = queue.enqueue_call(\n                func=rq_func,\n                args=(pk, filename, format_name),\n                job_id=rq_id\n            )\n            rq_job.meta[\'tmp_file\'] = filename\n            rq_job.meta[\'tmp_file_descriptor\'] = fd\n            rq_job.save_meta()\n    else:\n        if rq_job.is_finished:\n            os.close(rq_job.meta[\'tmp_file_descriptor\'])\n            os.remove(rq_job.meta[\'tmp_file\'])\n            rq_job.delete()\n            return Response(status=status.HTTP_201_CREATED)\n        elif rq_job.is_failed:\n            os.close(rq_job.meta[\'tmp_file_descriptor\'])\n            os.remove(rq_job.meta[\'tmp_file\'])\n            exc_info = str(rq_job.exc_info)\n            rq_job.delete()\n            return Response(data=exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n    return Response(status=status.HTTP_202_ACCEPTED)\n\ndef _export_annotations(db_task, rq_id, request, format_name, action, callback, filename):\n    if action not in {"""", ""download""}:\n        raise serializers.ValidationError(\n            ""Unexpected action specified for the request"")\n\n    format_desc = {f.DISPLAY_NAME: f\n        for f in dm.views.get_export_formats()}.get(format_name)\n    if format_desc is None:\n        raise serializers.ValidationError(\n            ""Unknown format specified for the request"")\n    elif not format_desc.ENABLED:\n        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\n\n    queue = django_rq.get_queue(""default"")\n\n    rq_job = queue.fetch_job(rq_id)\n    if rq_job:\n        last_task_update_time = timezone.localtime(db_task.updated_date)\n        request_time = rq_job.meta.get(\'request_time\', None)\n        if request_time is None or request_time < last_task_update_time:\n            rq_job.cancel()\n            rq_job.delete()\n        else:\n            if rq_job.is_finished:\n                file_path = rq_job.return_value\n                if action == ""download"" and osp.exists(file_path):\n                    rq_job.delete()\n\n                    timestamp = datetime.strftime(last_task_update_time,\n                        ""%Y_%m_%d_%H_%M_%S"")\n                    filename = filename or \\\n                        ""task_{}-{}-{}{}"".format(\n                        db_task.name, timestamp,\n                        format_name, osp.splitext(file_path)[1])\n                    return sendfile(request, file_path, attachment=True,\n                        attachment_filename=filename.lower())\n                else:\n                    if osp.exists(file_path):\n                        return Response(status=status.HTTP_201_CREATED)\n            elif rq_job.is_failed:\n                exc_info = str(rq_job.exc_info)\n                rq_job.delete()\n                return Response(exc_info,\n                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n            else:\n                return Response(status=status.HTTP_202_ACCEPTED)\n\n    try:\n        if request.scheme:\n            server_address = request.scheme + \'://\'\n        server_address += request.get_host()\n    except Exception:\n        server_address = None\n\n    ttl = dm.views.CACHE_TTL.total_seconds()\n    queue.enqueue_call(func=callback,\n        args=(db_task.id, format_name, server_address), job_id=rq_id,\n        meta={ \'request_time\': timezone.localtime() },\n        result_ttl=ttl, failure_ttl=ttl)\n    return Response(status=status.HTTP_202_ACCEPTED)'"
cvat/apps/git/__init__.py,0,b'# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/git/admin.py,0,b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/git/apps.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\n\nclass GitConfig(AppConfig):\n    name = 'git'\n"""
cvat/apps/git/git.py,0,"b'# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport datetime\nimport json\nimport math\nimport os\nimport re\nimport shutil\nimport subprocess\nfrom glob import glob\nimport zipfile\n\nimport django_rq\nimport git\nfrom django.db import transaction\nfrom django.utils import timezone\n\nfrom cvat.apps.dataset_manager.task import export_task\nfrom cvat.apps.engine.log import slogger\nfrom cvat.apps.engine.models import Job, Task, User\nfrom cvat.apps.engine.plugins import add_plugin\nfrom cvat.apps.git.models import GitData, GitStatusChoice\n\n\ndef _have_no_access_exception(ex):\n    if \'Permission denied\' in ex.stderr or \'Could not read from remote repository\' in ex.stderr:\n        keys = subprocess.run([\'ssh-add -L\'], shell = True,\n            stdout = subprocess.PIPE).stdout.decode(\'utf-8\').split(\'\\n\')\n        keys = list(filter(len, list(map(lambda x: x.strip(), keys))))\n        raise Exception(\n            \'Could not connect to the remote repository. \' +\n            \'Please make sure you have the correct access rights and the repository exists. \' +\n            \'Available public keys are: \' + str(keys)\n        )\n    else:\n        raise ex\n\n\ndef _read_old_diffs(diff_dir, summary):\n    diff_files = list(map(lambda x: os.path.join(diff_dir, x), os.listdir(diff_dir)))\n    for diff_file in diff_files:\n        diff_file = open(diff_file, \'r\')\n        diff = json.loads(diff_file.read())\n\n        for action_key in diff:\n            summary[action_key] += sum([diff[action_key][key] for key in diff[action_key]])\n\n\n\nclass Git:\n    def __init__(self, db_git, db_task, user):\n        self._db_git = db_git\n        self._url = db_git.url\n        self._path = db_git.path\n        self._tid = db_task.id\n        self._user = {\n            ""name"": user.username,\n            ""email"": user.email or ""dummy@cvat.com""\n        }\n        self._cwd = os.path.join(db_task.get_task_artifacts_dirname(), ""repos"")\n        self._diffs_dir = os.path.join(db_task.get_task_artifacts_dirname(), ""repos_diffs_v2"")\n        self._task_mode = db_task.mode\n        self._task_name = re.sub(r\'[\\\\/*?:""<>|\\s]\', \'_\', db_task.name)[:100]\n        self._branch_name = \'cvat_{}_{}\'.format(db_task.id, self._task_name)\n        self._annotation_file = os.path.join(self._cwd, self._path)\n        self._sync_date = db_git.sync_date\n        self._lfs = db_git.lfs\n\n\n    # Method parses an got URL.\n    # SSH: git@github.com/proj/repos[.git]\n    # HTTP/HTTPS: [http://]github.com/proj/repos[.git]\n    def _parse_url(self):\n        try:\n            # Almost STD66 (RFC3986), but schema can include a leading digit\n            # Reference on URL formats accepted by Git:\n            # https://github.com/git/git/blob/77bd3ea9f54f1584147b594abc04c26ca516d987/url.c\n\n            host_pattern = r""((?:(?:(?:\\d{1,3}\\.){3}\\d{1,3})|(?:[a-zA-Z0-9._-]+.[a-zA-Z]+))(?::\\d+)?)""\n            http_pattern = r""(?:http[s]?://)?"" + host_pattern + r""((?:/[a-zA-Z0-9._-]+){2})""\n            ssh_pattern = r""([a-zA-Z0-9._-]+)@"" + host_pattern + r"":([a-zA-Z0-9._-]+)/([a-zA-Z0-9._-]+)""\n\n            http_match = re.match(http_pattern, self._url)\n            ssh_match = re.match(ssh_pattern, self._url)\n\n            user = ""git""\n            host = None\n            repos = None\n\n            if http_match:\n                host = http_match.group(1)\n                repos = http_match.group(2)[1:]\n            elif ssh_match:\n                user = ssh_match.group(1)\n                host = ssh_match.group(2)\n                repos = ""{}/{}"".format(ssh_match.group(3), ssh_match.group(4))\n            else:\n                raise Exception(""Git repository URL does not satisfy pattern"")\n\n            if not repos.endswith("".git""):\n                repos += "".git""\n\n            return user, host, repos\n        except Exception as ex:\n            slogger.glob.exception(\'URL parsing errors occurred\', exc_info = True)\n            raise ex\n\n\n    # Method creates the main branch if repostory doesn\'t have any branches\n    def _create_master_branch(self):\n        if len(self._rep.heads):\n            raise Exception(""Some heads already exists"")\n        readme_md_name = os.path.join(self._cwd, ""README.md"")\n        with open(readme_md_name, ""w""):\n            pass\n        self._rep.index.add([readme_md_name])\n        self._rep.index.commit(""CVAT Annotation. Initial commit by {} at {}"".format(self._user[""name""], timezone.now()))\n\n        self._rep.git.push(""origin"", ""master"")\n\n\n    # Method creates task branch for repository from current master\n    def _to_task_branch(self):\n        # Remove user branch from local repository if it exists\n        if self._branch_name not in list(map(lambda x: x.name, self._rep.heads)):\n            self._rep.create_head(self._branch_name)\n\n        self._rep.head.reference = self._rep.heads[self._branch_name]\n\n\n    # Method setups a config file for current user\n    def _update_config(self):\n        slogger.task[self._tid].info(""User config initialization.."")\n        with self._rep.config_writer() as cw:\n            if not cw.has_section(""user""):\n                cw.add_section(""user"")\n            cw.set(""user"", ""name"", self._user[""name""])\n            cw.set(""user"", ""email"", self._user[""email""])\n            cw.release()\n\n    # Method initializes repos. It setup configuration, creates master branch if need and checkouts to task branch\n    def _configurate(self):\n        self._update_config()\n        if not len(self._rep.heads):\n            self._create_master_branch()\n        self._to_task_branch()\n        os.makedirs(self._diffs_dir, exist_ok = True)\n\n\n    def _ssh_url(self):\n        user, host, repos = self._parse_url()\n        return ""{}@{}:{}"".format(user, host, repos)\n\n\n    # Method clones a remote repos to the local storage using SSH and initializes it\n    def _clone(self):\n        os.makedirs(self._cwd)\n        ssh_url = self._ssh_url()\n\n        # Cloning\n        slogger.task[self._tid].info(""Cloning remote repository from {}.."".format(ssh_url))\n        self._rep = git.Repo.clone_from(ssh_url, self._cwd)\n\n        # Intitialization\n        self._configurate()\n\n\n    # Method is some wrapper for clone\n    # It restores state if any errors have occured\n    # It useful if merge conflicts have occured during pull\n    def _reclone(self):\n        if os.path.exists(self._cwd):\n            if not os.path.isdir(self._cwd):\n                os.remove(self._cwd)\n            else:\n                # Rename current repository dir\n                tmp_repo = os.path.abspath(os.path.join(self._cwd, "".."", ""tmp_repo""))\n                os.rename(self._cwd, tmp_repo)\n\n                # Try clone repository\n                try:\n                    self._clone()\n                    shutil.rmtree(tmp_repo, True)\n                except Exception as ex:\n                    # Restore state if any errors have occured\n                    if os.path.isdir(self._cwd):\n                        shutil.rmtree(self._cwd, True)\n                    os.rename(tmp_repo, self._cwd)\n                    raise ex\n        else:\n            self._clone()\n\n\n    # Method checkouts to master branch and pulls it from remote repos\n    def _pull(self):\n        self._rep.head.reference = self._rep.heads[""master""]\n        try:\n            self._rep.git.pull(""origin"", ""master"")\n\n            if self._branch_name in list(map(lambda x: x.name, self._rep.heads)):\n                self._rep.head.reference = self._rep.heads[""master""]\n                self._rep.delete_head(self._branch_name, force=True)\n                self._rep.head.reset(""HEAD"", index=True, working_tree=True)\n\n            self._to_task_branch()\n        except git.exc.GitError:\n            # Merge conflicts\n            self._reclone()\n\n\n    # Method connects a local repository if it exists\n    # Otherwise it clones it before\n    def init_repos(self, wo_remote = False):\n        try:\n            # Try to use a local repos. It can throw GitError exception\n            self._rep = git.Repo(self._cwd)\n            self._configurate()\n\n            # Check if remote URL is actual\n            if self._ssh_url() != self._rep.git.remote(\'get-url\', \'--all\', \'origin\'):\n                slogger.task[self._tid].info(""Local repository URL is obsolete."")\n                # We need reinitialize repository if it\'s false\n                raise git.exc.GitError(""Actual and saved repository URLs aren\'t match"")\n        except git.exc.GitError:\n            if wo_remote:\n                raise Exception(\'Local repository is failed\')\n            slogger.task[self._tid].info(""Local repository initialization.."")\n            shutil.rmtree(self._cwd, True)\n            self._clone()\n\n\n    # Method prepares an annotation, merges diffs and pushes it to remote repository to user branch\n    def push(self, user, scheme, host, db_task, last_save):\n        # Update local repository\n        self._pull()\n\n        os.makedirs(os.path.join(self._cwd, os.path.dirname(self._annotation_file)), exist_ok = True)\n        # Remove old annotation file if it exists\n        if os.path.exists(self._annotation_file):\n            os.remove(self._annotation_file)\n\n        # Initialize LFS if need\n        if self._lfs:\n            updated = False\n            lfs_settings = [""*.xml\\tfilter=lfs diff=lfs merge=lfs -text\\n"", ""*.zip\\tfilter=lfs diff=lfs merge=lfs -text\\n""]\n            if not os.path.isfile(os.path.join(self._cwd, "".gitattributes"")):\n                with open(os.path.join(self._cwd, "".gitattributes""), ""w"") as gitattributes:\n                    gitattributes.writelines(lfs_settings)\n                    updated = True\n            else:\n                with open(os.path.join(self._cwd, "".gitattributes""), ""r+"") as gitattributes:\n                    lines = gitattributes.readlines()\n                    for setting in lfs_settings:\n                        if setting not in lines:\n                            updated = True\n                            lines.append(setting)\n                    gitattributes.seek(0)\n                    gitattributes.writelines(lines)\n                    gitattributes.truncate()\n\n            if updated:\n                self._rep.git.add([\'.gitattributes\'])\n\n        # Dump an annotation\n        timestamp = datetime.datetime.now().strftime(""%Y_%m_%d_%H_%M_%S"")\n        if self._task_mode == ""annotation"":\n            format_name = ""CVAT for images 1.1""\n        else:\n            format_name = ""CVAT for video 1.1""\n        dump_name = os.path.join(db_task.get_task_dirname(),\n            ""git_annotation_{}.zip"".format(timestamp))\n        export_task(\n            task_id=self._tid,\n            dst_file=dump_name,\n            format_name=format_name,\n            server_url=scheme + host,\n            save_images=False,\n        )\n\n        ext = os.path.splitext(self._path)[1]\n        if ext == \'.zip\':\n            shutil.move(dump_name, self._annotation_file)\n        elif ext == \'.xml\':\n            with zipfile.ZipFile(dump_name) as archive:\n                for f in archive.namelist():\n                    if f.endswith(\'.xml\'):\n                        with open(self._annotation_file, \'wb\') as output:\n                            output.write(archive.read(f))\n                        break\n            os.remove(dump_name)\n        else:\n            raise Exception(""Got unknown annotation file type"")\n\n        self._rep.git.add(self._annotation_file)\n\n        # Merge diffs\n        summary_diff = {\n            ""update"": 0,\n            ""create"": 0,\n            ""delete"": 0\n        }\n\n        old_diffs_dir = os.path.join(os.path.dirname(self._diffs_dir), \'repos_diffs\')\n        if (os.path.isdir(old_diffs_dir)):\n            _read_old_diffs(old_diffs_dir, summary_diff)\n\n        for diff_name in list(map(lambda x: os.path.join(self._diffs_dir, x), os.listdir(self._diffs_dir))):\n            with open(diff_name, \'r\') as f:\n                diff = json.loads(f.read())\n                for key in diff:\n                    summary_diff[key] += diff[key]\n\n        message = ""CVAT Annotation updated by {}. \\n"".format(self._user[""name""])\n        message += \'Task URL: {}://{}/dashboard?id={}\\n\'.format(scheme, host, db_task.id)\n        if db_task.bug_tracker:\n            message += \'Bug Tracker URL: {}\\n\'.format(db_task.bug_tracker)\n        message += ""Created: {}, updated: {}, deleted: {}\\n"".format(\n            summary_diff[""create""],\n            summary_diff[""update""],\n            summary_diff[""delete""]\n        )\n        message += ""Annotation time: {} hours\\n"".format(math.ceil((last_save - self._sync_date).total_seconds() / 3600))\n        message += ""Total annotation time: {} hours"".format(math.ceil((last_save - db_task.created_date).total_seconds() / 3600))\n\n        self._rep.index.commit(message)\n        self._rep.git.push(""origin"", self._branch_name, ""--force"")\n\n        shutil.rmtree(old_diffs_dir, True)\n        shutil.rmtree(self._diffs_dir, True)\n\n\n    # Method checks status of repository annotation\n    def remote_status(self, last_save):\n        # Check repository exists and archive exists\n        if not os.path.isfile(self._annotation_file) or last_save != self._sync_date:\n            return GitStatusChoice.NON_SYNCED\n        else:\n            self._rep.git.update_ref(\'-d\', \'refs/remotes/origin/{}\'.format(self._branch_name))\n            self._rep.git.remote(\'-v\', \'update\')\n\n            last_hash = self._rep.git.show_ref(\'refs/heads/{}\'.format(self._branch_name), \'--hash\')\n            merge_base_hash = self._rep.merge_base(\'refs/remotes/origin/master\', self._branch_name)[0].hexsha\n            if last_hash == merge_base_hash:\n                return GitStatusChoice.MERGED\n            else:\n                try:\n                    self._rep.git.show_ref(\'refs/remotes/origin/{}\'.format(self._branch_name), \'--hash\')\n                    return GitStatusChoice.SYNCED\n                except git.exc.GitCommandError:\n                    # Remote branch has been deleted w/o merge\n                    return GitStatusChoice.NON_SYNCED\n\n\ndef initial_create(tid, git_path, lfs, user):\n    try:\n        db_task = Task.objects.get(pk = tid)\n        path_pattern = r""\\[(.+)\\]""\n        path_search = re.search(path_pattern, git_path)\n        path = None\n\n        if path_search is not None:\n            path = path_search.group(1)\n            git_path = git_path[0:git_path.find(path) - 1].strip()\n            path = os.path.join(\'/\', path.strip())\n        else:\n            anno_file = re.sub(r\'[\\\\/*?:""<>|\\s]\', \'_\', db_task.name)[:100]\n            path = \'/annotation/{}.zip\'.format(anno_file)\n\n        path = path[1:]\n        _split = os.path.splitext(path)\n        if len(_split) < 2 or _split[1] not in ["".xml"", "".zip""]:\n            raise Exception(""Only .xml and .zip formats are supported"")\n\n        db_git = GitData()\n        db_git.url = git_path\n        db_git.path = path\n        db_git.task = db_task\n        db_git.lfs = lfs\n\n        try:\n            _git = Git(db_git, db_task, db_task.owner)\n            _git.init_repos()\n            db_git.save()\n        except git.exc.GitCommandError as ex:\n            _have_no_access_exception(ex)\n    except Exception as ex:\n        slogger.task[tid].exception(\'exception occured during git initial_create\', exc_info = True)\n        raise ex\n\n\n@transaction.atomic\ndef push(tid, user, scheme, host):\n    try:\n        db_task = Task.objects.get(pk = tid)\n        db_git = GitData.objects.select_for_update().get(pk = db_task)\n        try:\n            _git = Git(db_git, db_task, user)\n            _git.init_repos()\n            _git.push(user, scheme, host, db_task, db_task.updated_date)\n\n            # Update timestamp\n            db_git.sync_date = db_task.updated_date\n            db_git.status = GitStatusChoice.SYNCED\n            db_git.save()\n        except git.exc.GitCommandError as ex:\n            _have_no_access_exception(ex)\n    except Exception as ex:\n        slogger.task[tid].exception(\'push to remote repository errors occured\', exc_info = True)\n        raise ex\n\n\n@transaction.atomic\ndef get(tid, user):\n    response = {}\n    response[""url""] = {""value"": None}\n    response[""status""] = {""value"": None, ""error"": None}\n\n    db_task = Task.objects.get(pk = tid)\n    if GitData.objects.filter(pk = db_task).exists():\n        db_git = GitData.objects.select_for_update().get(pk = db_task)\n        response[\'url\'][\'value\'] = \'{} [{}]\'.format(db_git.url, db_git.path)\n        try:\n            rq_id = ""git.push.{}"".format(tid)\n            queue = django_rq.get_queue(\'default\')\n            rq_job = queue.fetch_job(rq_id)\n            if rq_job is not None and (rq_job.is_queued or rq_job.is_started):\n                db_git.status = GitStatusChoice.SYNCING\n                response[\'status\'][\'value\'] = str(db_git.status)\n            else:\n                try:\n                    _git = Git(db_git, db_task, user)\n                    _git.init_repos(True)\n                    db_git.status = _git.remote_status(db_task.updated_date)\n                    response[\'status\'][\'value\'] = str(db_git.status)\n                except git.exc.GitCommandError as ex:\n                    _have_no_access_exception(ex)\n            db_git.save()\n        except Exception as ex:\n            db_git.status = GitStatusChoice.NON_SYNCED\n            db_git.save()\n            response[\'status\'][\'error\'] = str(ex)\n\n    return response\n\ndef update_states():\n    db_git_records = GitData.objects.all()\n    db_user = User.objects.first()\n    if db_user is None:\n        # User hasn\'t been created yet\n        return\n\n    for db_git in db_git_records:\n        try:\n            get(db_git.task_id, db_user)\n        except Exception:\n            slogger.glob(""Exception occured during a status updating for db_git with tid: {}"".format(db_git.task_id))\n\n@transaction.atomic\ndef _onsave(jid, data, action):\n    db_task = Job.objects.select_related(\'segment__task\').get(pk = jid).segment.task\n    try:\n        db_git = GitData.objects.select_for_update().get(pk = db_task.id)\n        diff_dir = os.path.join(db_task.get_task_artifacts_dirname(), ""repos_diffs"")\n        diff_dir_v2 = os.path.join(db_task.get_task_artifacts_dirname(), ""repos_diffs_v2"")\n\n        summary = {\n            ""update"": 0,\n            ""create"": 0,\n            ""delete"": 0\n        }\n\n        if os.path.isdir(diff_dir):\n            _read_old_diffs(diff_dir, summary)\n            shutil.rmtree(diff_dir, True)\n\n        os.makedirs(diff_dir_v2, exist_ok = True)\n\n        summary[action] += sum([len(data[key]) for key in [\'shapes\', \'tracks\', \'tags\']])\n\n        if summary[""update""] or summary[""create""] or summary[""delete""]:\n            diff_files = list(map(lambda x: os.path.join(diff_dir_v2, x), os.listdir(diff_dir_v2)))\n            last_num = 0\n            for f in diff_files:\n                number = os.path.splitext(os.path.basename(f))[0]\n                number = int(number) if number.isdigit() else last_num\n                last_num = max(last_num, number)\n\n            with open(os.path.join(diff_dir_v2, ""{}.diff"".format(last_num + 1)), \'w\') as f:\n                f.write(json.dumps(summary))\n\n            db_git.status = GitStatusChoice.NON_SYNCED\n            db_git.save()\n\n    except GitData.DoesNotExist:\n        pass\n\nadd_plugin(""patch_job_data"", _onsave, ""after"", exc_ok = False)\n\n# TODO: Append git repository into dump file\n# def _ondump(task_id, dst_file, format_name,\n#         server_url=None, save_images=False, plugin_meta_data):\n#     db_task = Task.objects.get(pk = tid)\n#     try:\n#         db_git = GitData.objects.get(pk = db_task)\n#         plugin_meta_data[\'git\'] = OrderedDict({\n#             ""url"": db_git.url,\n#             ""path"": db_git.path,\n#         })\n#     except GitData.DoesNotExist:\n#         pass\n# add_plugin(""_dump"", _ondump, ""before"", exc_ok = False)\n'"
cvat/apps/git/models.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.db import models\nfrom cvat.apps.engine.models import Task\nfrom enum import Enum\n\nclass GitStatusChoice(Enum):\n    NON_SYNCED = '!sync'\n    SYNCING = 'syncing'\n    SYNCED = 'sync'\n    MERGED = 'merged'\n\n    def __str__(self):\n        return self.value\n\n\nclass GitData(models.Model):\n    task = models.OneToOneField(Task, on_delete = models.CASCADE, primary_key = True)\n    url = models.URLField(max_length = 2000)\n    path = models.CharField(max_length=256)\n    sync_date = models.DateTimeField(auto_now_add=True)\n    status = models.CharField(max_length=20, default=GitStatusChoice.NON_SYNCED)\n    lfs = models.BooleanField(default=True)\n"""
cvat/apps/git/tests.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom itertools import product\n\nfrom django.test import TestCase\n\n# Create your tests here.\n\nfrom cvat.apps.git.git import Git\n\n\nclass GitUrlTest(TestCase):\n    class FakeGit:\n        def __init__(self, url):\n            self._url = url\n\n    def _check_correct_urls(self, samples):\n        for i, (expected, url) in enumerate(samples):\n            git = GitUrlTest.FakeGit(url)\n            try:\n                actual = Git._parse_url(git)\n                self.assertEqual(expected, actual, ""URL #%s: \'%s\'"" % (i, url))\n            except Exception:\n                self.assertFalse(True, ""URL #%s: \'%s\'"" % (i, url))\n\n    def test_correct_urls_can_be_parsed(self):\n        hosts = [\'host.zone\', \'1.2.3.4\']\n        ports = [\'\', \':42\']\n        repo_groups = [\'repo\', \'r4p0\']\n        repo_repos = [\'nkjl23\', \'hewj\']\n        git_suffixes = [\'\', \'.git\']\n\n        samples = []\n\n        # http samples\n        protocols = [\'\', \'http://\', \'https://\']\n        for protocol, host, port, repo_group, repo, git in product(\n                protocols, hosts, ports, repo_groups, repo_repos, git_suffixes):\n            url = \'{protocol}{host}{port}/{repo_group}/{repo}{git}\'.format(\n                protocol=protocol, host=host, port=port,\n                repo_group=repo_group, repo=repo, git=git\n            )\n            expected = (\'git\', host + port, \'%s/%s.git\' % (repo_group, repo))\n            samples.append((expected, url))\n\n        # git samples\n        users = [\'user\', \'u123_.\']\n        for user, host, port, repo_group, repo, git in product(\n                users, hosts, ports, repo_groups, repo_repos, git_suffixes):\n            url = \'{user}@{host}{port}:{repo_group}/{repo}{git}\'.format(\n                user=user, host=host, port=port,\n                repo_group=repo_group, repo=repo, git=git\n            )\n            expected = (user, host + port, \'%s/%s.git\' % (repo_group, repo))\n            samples.append((expected, url))\n\n        self._check_correct_urls(samples)'"
cvat/apps/git/urls.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\nfrom django.urls import path\nfrom . import views\n\n\nurlpatterns = [\n    path('create/<int:tid>', views.create),\n    path('get/<int:tid>', views.get_repository),\n    path('push/<int:tid>', views.push_repository),\n    path('check/<str:rq_id>', views.check_process),\n    path('meta/get', views.get_meta_info),\n]\n"""
cvat/apps/git/views.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.http import HttpResponseBadRequest, JsonResponse\nfrom rules.contrib.views import permission_required, objectgetter\n\nfrom cvat.apps.authentication.decorators import login_required\nfrom cvat.apps.engine.log import slogger\nfrom cvat.apps.engine import models\nfrom cvat.apps.git.models import GitData\n\nimport cvat.apps.git.git as CVATGit\nimport django_rq\nimport json\n\n@login_required\ndef check_process(request, rq_id):\n    try:\n        queue = django_rq.get_queue(\'default\')\n        rq_job = queue.fetch_job(rq_id)\n\n        if rq_job is not None:\n            if rq_job.is_queued or rq_job.is_started:\n                return JsonResponse({""status"": rq_job.get_status()})\n            elif rq_job.is_finished:\n                return JsonResponse({""status"": rq_job.get_status()})\n            else:\n                return JsonResponse({""status"": rq_job.get_status(), ""stderr"": rq_job.exc_info})\n        else:\n            return JsonResponse({""status"": ""unknown""})\n    except Exception as ex:\n        slogger.glob.error(""error occured during checking repository request with rq id {}"".format(rq_id), exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\n\n@login_required\n@permission_required(perm=[\'engine.task.create\'],\n    fn=objectgetter(models.Task, \'tid\'), raise_exception=True)\ndef create(request, tid):\n    try:\n        slogger.task[tid].info(""create repository request"")\n\n        body = json.loads(request.body.decode(\'utf-8\'))\n        path = body[""path""]\n        lfs = body[""lfs""]\n        rq_id = ""git.create.{}"".format(tid)\n        queue = django_rq.get_queue(""default"")\n\n        queue.enqueue_call(func = CVATGit.initial_create, args = (tid, path, lfs, request.user), job_id = rq_id)\n        return JsonResponse({ ""rq_id"": rq_id })\n    except Exception as ex:\n        slogger.glob.error(""error occured during initial cloning repository request with rq id {}"".format(rq_id), exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\n\n@login_required\n@permission_required(perm=[\'engine.task.access\'],\n    fn=objectgetter(models.Task, \'tid\'), raise_exception=True)\ndef push_repository(request, tid):\n    try:\n        slogger.task[tid].info(""push repository request"")\n\n        rq_id = ""git.push.{}"".format(tid)\n        queue = django_rq.get_queue(\'default\')\n        queue.enqueue_call(func = CVATGit.push, args = (tid, request.user, request.scheme, request.get_host()), job_id = rq_id)\n\n        return JsonResponse({ ""rq_id"": rq_id })\n    except Exception as ex:\n        try:\n            slogger.task[tid].error(""error occured during pushing repository request"", exc_info=True)\n        except Exception:\n            pass\n        return HttpResponseBadRequest(str(ex))\n\n\n@login_required\n@permission_required(perm=[\'engine.task.access\'],\n    fn=objectgetter(models.Task, \'tid\'), raise_exception=True)\ndef get_repository(request, tid):\n    try:\n        slogger.task[tid].info(""get repository request"")\n        return JsonResponse(CVATGit.get(tid, request.user))\n    except Exception as ex:\n        try:\n            slogger.task[tid].error(""error occured during getting repository info request"", exc_info=True)\n        except Exception:\n            pass\n        return HttpResponseBadRequest(str(ex))\n\n\n@login_required\ndef get_meta_info(request):\n    try:\n        db_git_records = GitData.objects.all()\n        response = {}\n        for db_git in db_git_records:\n            response[db_git.task_id] = db_git.status\n\n        return JsonResponse(response, safe = False)\n    except Exception as ex:\n        slogger.glob.exception(""error occured during get meta request"", exc_info = True)\n        return HttpResponseBadRequest(str(ex))\n'"
cvat/apps/log_viewer/__init__.py,0,b'# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/log_viewer/apps.py,0,"b""from django.apps import AppConfig\n\n\nclass LogViewerConfig(AppConfig):\n    name = 'log_viewer'\n"""
cvat/apps/log_viewer/urls.py,0,"b""\n# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('<path:path>', views.LogViewerProxy.as_view())\n]\n"""
cvat/apps/log_viewer/views.py,0,"b""# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\n\nfrom revproxy.views import ProxyView\nfrom django.utils.decorators import method_decorator\nfrom django.conf import settings\nfrom rules.contrib.views import PermissionRequiredMixin\n\nfrom cvat.apps.authentication.decorators import login_required\n\n@method_decorator(login_required, name='dispatch')\nclass LogViewerProxy(PermissionRequiredMixin, ProxyView):\n    permission_required = settings.RESTRICTIONS['analytics_access']\n\n    upstream = 'http://{}:{}'.format(os.getenv('DJANGO_LOG_VIEWER_HOST'),\n        os.getenv('DJANGO_LOG_VIEWER_PORT'))\n    add_remote_user = True\n\n    def get_request_headers(self):\n        headers = super().get_request_headers()\n        headers['X-Forwarded-User'] = headers['REMOTE_USER']\n\n        return headers\n\n    # Returns True if the user has any of the specified permissions\n    def has_permission(self):\n        perms = self.get_permission_required()\n        return any(self.request.user.has_perm(perm) for perm in perms)\n"""
cvat/apps/reid/__init__.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom cvat.settings.base import JS_3RDPARTY\n\ndefault_app_config = 'cvat.apps.reid.apps.ReidConfig'\n\nJS_3RDPARTY['engine'] = JS_3RDPARTY.get('engine', []) + ['reid/js/enginePlugin.js']\n"""
cvat/apps/reid/apps.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\nclass ReidConfig(AppConfig):\n    name = 'cvat.apps.reid'\n"""
cvat/apps/reid/reid.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport rq\nimport cv2\nimport math\nimport numpy\nimport itertools\n\nfrom openvino.inference_engine import IENetwork, IEPlugin\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.spatial.distance import euclidean, cosine\n\nfrom cvat.apps.engine.models import Job\nfrom cvat.apps.engine.frame_provider import FrameProvider\n\n\nclass ReID:\n    __threshold = None\n    __max_distance = None\n    __frame_urls = None\n    __frame_boxes = None\n    __stop_frame = None\n    __plugin = None\n    __executable_network = None\n    __input_blob_name = None\n    __output_blob_name = None\n    __input_height = None\n    __input_width = None\n\n\n    def __init__(self, jid, data):\n        self.__threshold = data[""threshold""]\n        self.__max_distance = data[""maxDistance""]\n\n        self.__frame_boxes = {}\n\n        db_job = Job.objects.select_related(\'segment__task\').get(pk = jid)\n        db_segment = db_job.segment\n        db_task = db_segment.task\n        self.__frame_iter = itertools.islice(\n            FrameProvider(db_task.data).get_frames(FrameProvider.Quality.ORIGINAL),\n            db_segment.start_frame,\n            db_segment.stop_frame + 1,\n        )\n\n        self.__stop_frame = db_segment.stop_frame\n        for frame in range(db_segment.start_frame, db_segment.stop_frame + 1):\n            self.__frame_boxes[frame] = [box for box in data[""boxes""] if box[""frame""] == frame]\n\n        IE_PLUGINS_PATH = os.getenv(\'IE_PLUGINS_PATH\', None)\n        REID_MODEL_DIR = os.getenv(\'REID_MODEL_DIR\', None)\n\n        if not IE_PLUGINS_PATH:\n            raise Exception(""Environment variable \'IE_PLUGINS_PATH\' isn\'t defined"")\n        if not REID_MODEL_DIR:\n            raise Exception(""Environment variable \'REID_MODEL_DIR\' isn\'t defined"")\n\n        REID_XML = os.path.join(REID_MODEL_DIR, ""reid.xml"")\n        REID_BIN = os.path.join(REID_MODEL_DIR, ""reid.bin"")\n\n        self.__plugin = IEPlugin(device=""CPU"", plugin_dirs=[IE_PLUGINS_PATH])\n        network = IENetwork.from_ir(model=REID_XML, weights=REID_BIN)\n        self.__input_blob_name = next(iter(network.inputs))\n        self.__output_blob_name = next(iter(network.outputs))\n        self.__input_height, self.__input_width = network.inputs[self.__input_blob_name].shape[-2:]\n        self.__executable_network = self.__plugin.load(network=network)\n        del network\n\n\n    def __del__(self):\n        if self.__executable_network:\n            del self.__executable_network\n            self.__executable_network = None\n\n        if self.__plugin:\n            del self.__plugin\n            self.__plugin = None\n\n\n    def __boxes_are_compatible(self, cur_box, next_box):\n        cur_c_x = (cur_box[""points""][0] + cur_box[""points""][2]) / 2\n        cur_c_y = (cur_box[""points""][1] + cur_box[""points""][3]) / 2\n        next_c_x = (next_box[""points""][0] + next_box[""points""][2]) / 2\n        next_c_y = (next_box[""points""][1] + next_box[""points""][3]) / 2\n        compatible_distance = euclidean([cur_c_x, cur_c_y], [next_c_x, next_c_y]) <= self.__max_distance\n        compatible_label = cur_box[""label_id""] == next_box[""label_id""]\n        return compatible_distance and compatible_label and ""path_id"" not in next_box\n\n\n    def __compute_difference(self, image_1, image_2):\n        image_1 = cv2.resize(image_1, (self.__input_width, self.__input_height)).transpose((2,0,1))\n        image_2 = cv2.resize(image_2, (self.__input_width, self.__input_height)).transpose((2,0,1))\n\n        input_1 = {\n            self.__input_blob_name: image_1[numpy.newaxis, ...]\n        }\n\n        input_2 = {\n            self.__input_blob_name: image_2[numpy.newaxis, ...]\n        }\n\n        embedding_1 = self.__executable_network.infer(inputs = input_1)[self.__output_blob_name]\n        embedding_2 = self.__executable_network.infer(inputs = input_2)[self.__output_blob_name]\n\n        embedding_1 = embedding_1.reshape(embedding_1.size)\n        embedding_2 = embedding_2.reshape(embedding_2.size)\n\n        return cosine(embedding_1, embedding_2)\n\n\n    def __compute_difference_matrix(self, cur_boxes, next_boxes, cur_image, next_image):\n        def _int(number, upper):\n            return math.floor(numpy.clip(number, 0, upper - 1))\n\n        default_mat_value = 1000.0\n\n        matrix = numpy.full([len(cur_boxes), len(next_boxes)], default_mat_value, dtype=float)\n        for row, cur_box in enumerate(cur_boxes):\n            cur_width = cur_image.shape[1]\n            cur_height = cur_image.shape[0]\n            cur_xtl, cur_xbr, cur_ytl, cur_ybr = (\n                _int(cur_box[""points""][0], cur_width), _int(cur_box[""points""][2], cur_width),\n                _int(cur_box[""points""][1], cur_height), _int(cur_box[""points""][3], cur_height)\n            )\n\n            for col, next_box in enumerate(next_boxes):\n                next_box = next_boxes[col]\n                next_width = next_image.shape[1]\n                next_height = next_image.shape[0]\n                next_xtl, next_xbr, next_ytl, next_ybr = (\n                    _int(next_box[""points""][0], next_width), _int(next_box[""points""][2], next_width),\n                    _int(next_box[""points""][1], next_height), _int(next_box[""points""][3], next_height)\n                )\n\n                if not self.__boxes_are_compatible(cur_box, next_box):\n                    continue\n\n                crop_1 = cur_image[cur_ytl:cur_ybr, cur_xtl:cur_xbr]\n                crop_2 = next_image[next_ytl:next_ybr, next_xtl:next_xbr]\n                matrix[row][col] = self.__compute_difference(crop_1, crop_2)\n\n        return matrix\n\n\n    def __apply_matching(self):\n        frames = sorted(list(self.__frame_boxes.keys()))\n        job = rq.get_current_job()\n        box_tracks = {}\n\n        next_image = cv2.imdecode(numpy.fromstring((next(self.__frame_iter)[0]).read(), numpy.uint8), cv2.IMREAD_COLOR)\n        for idx, (cur_frame, next_frame) in enumerate(list(zip(frames[:-1], frames[1:]))):\n            job.refresh()\n            if ""cancel"" in job.meta:\n                return None\n\n            job.meta[""progress""] = idx * 100.0 / len(frames)\n            job.save_meta()\n\n            cur_boxes = self.__frame_boxes[cur_frame]\n            next_boxes = self.__frame_boxes[next_frame]\n\n            for box in cur_boxes:\n                if ""path_id"" not in box:\n                    path_id = len(box_tracks)\n                    box_tracks[path_id] = [box]\n                    box[""path_id""] = path_id\n\n            if not (len(cur_boxes) and len(next_boxes)):\n                continue\n\n            cur_image = next_image\n            next_image = cv2.imdecode(numpy.fromstring((next(self.__frame_iter)[0]).read(), numpy.uint8), cv2.IMREAD_COLOR)\n            difference_matrix = self.__compute_difference_matrix(cur_boxes, next_boxes, cur_image, next_image)\n            cur_idxs, next_idxs = linear_sum_assignment(difference_matrix)\n            for idx, cur_idx in enumerate(cur_idxs):\n                if (difference_matrix[cur_idx][next_idxs[idx]]) <= self.__threshold:\n                    cur_box = cur_boxes[cur_idx]\n                    next_box = next_boxes[next_idxs[idx]]\n                    next_box[""path_id""] = cur_box[""path_id""]\n                    box_tracks[cur_box[""path_id""]].append(next_box)\n\n        for box in self.__frame_boxes[frames[-1]]:\n            if ""path_id"" not in box:\n                path_id = len(box_tracks)\n                box[""path_id""] = path_id\n                box_tracks[path_id] = [box]\n\n        return box_tracks\n\n\n    def run(self):\n        box_tracks = self.__apply_matching()\n        output = []\n\n        # ReID process has been canceled\n        if box_tracks is None:\n            return\n\n        for path_id in box_tracks:\n            output.append({\n                ""label_id"": box_tracks[path_id][0][""label_id""],\n                ""group"": None,\n                ""attributes"": [],\n                ""frame"": box_tracks[path_id][0][""frame""],\n                ""shapes"": box_tracks[path_id]\n            })\n\n            for box in output[-1][""shapes""]:\n                if ""id"" in box:\n                    del box[""id""]\n                del box[""path_id""]\n                del box[""group""]\n                del box[""label_id""]\n                box[""outside""] = False\n                box[""attributes""] = []\n\n        for path in output:\n            if path[""shapes""][-1][""frame""] != self.__stop_frame:\n                copy = path[""shapes""][-1].copy()\n                copy[""outside""] = True\n                copy[""frame""] += 1\n                path[""shapes""].append(copy)\n\n        return output\n'"
cvat/apps/reid/urls.py,0,"b""# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('start/job/<int:jid>', views.start),\n    path('cancel/<int:jid>', views.cancel),\n    path('check/<int:jid>', views.check),\n    path('enabled', views.enabled),\n]\n"""
cvat/apps/reid/views.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.http import HttpResponse, HttpResponseBadRequest, JsonResponse\nfrom cvat.apps.authentication.decorators import login_required\nfrom rules.contrib.views import permission_required, objectgetter\n\nfrom cvat.apps.engine.models import Job\nfrom cvat.apps.reid.reid import ReID\n\nimport django_rq\nimport json\nimport rq\n\n\ndef _create_thread(jid, data):\n    job = rq.get_current_job()\n    reid_obj = ReID(jid, data)\n    job.meta[""result""] = json.dumps(reid_obj.run())\n    job.save_meta()\n\n\n@login_required\n@permission_required(perm=[""engine.job.change""],\n    fn=objectgetter(Job, \'jid\'), raise_exception=True)\ndef start(request, jid):\n    try:\n        data = json.loads(request.body.decode(\'utf-8\'))\n        queue = django_rq.get_queue(""low"")\n        job_id = ""reid.create.{}"".format(jid)\n        job = queue.fetch_job(job_id)\n        if job is not None and (job.is_started or job.is_queued):\n            raise Exception(\'ReID process has been already started\')\n        queue.enqueue_call(func=_create_thread, args=(jid, data), job_id=job_id, timeout=7200)\n        job = queue.fetch_job(job_id)\n        job.meta = {}\n        job.save_meta()\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n\n    return HttpResponse()\n\n\n@login_required\n@permission_required(perm=[""engine.job.change""],\n    fn=objectgetter(Job, \'jid\'), raise_exception=True)\ndef check(request, jid):\n    try:\n        queue = django_rq.get_queue(""low"")\n        rq_id = ""reid.create.{}"".format(jid)\n        job = queue.fetch_job(rq_id)\n        if job is not None and ""cancel"" in job.meta:\n            return JsonResponse({""status"": ""finished""})\n        data = {}\n        if job is None:\n            data[""status""] = ""unknown""\n        elif job.is_queued:\n            data[""status""] = ""queued""\n        elif job.is_started:\n            data[""status""] = ""started""\n            if ""progress"" in job.meta:\n                data[""progress""] = job.meta[""progress""]\n        elif job.is_finished:\n            data[""status""] = ""finished""\n            data[""result""] = job.meta[""result""]\n            job.delete()\n        else:\n            data[""status""] = ""failed""\n            data[""stderr""] = job.exc_info\n            job.delete()\n\n    except Exception as ex:\n        data[""stderr""] = str(ex)\n        data[""status""] = ""unknown""\n\n    return JsonResponse(data)\n\n\n@login_required\n@permission_required(perm=[""engine.job.change""],\n    fn=objectgetter(Job, \'jid\'), raise_exception=True)\ndef cancel(request, jid):\n    try:\n        queue = django_rq.get_queue(""low"")\n        rq_id = ""reid.create.{}"".format(jid)\n        job = queue.fetch_job(rq_id)\n        if job is None or job.is_finished or job.is_failed:\n            raise Exception(""Task is not being annotated currently"")\n        elif ""cancel"" not in job.meta:\n            job.meta[""cancel""] = True\n            job.save_meta()\n    except Exception as e:\n        return HttpResponseBadRequest(str(e))\n\n    return HttpResponse()\n\ndef enabled(request):\n    return HttpResponse()\n'"
cvat/apps/restrictions/__init__.py,0,b'# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/restrictions/apps.py,0,"b""# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\nfrom django.apps import AppConfig\n\n\nclass RestrictionsConfig(AppConfig):\n    name = 'cvat.apps.restrictions'\n"""
cvat/apps/restrictions/serializers.py,0,"b""# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\nfrom rest_framework import serializers\nfrom django.conf import settings\n\nfrom cvat.apps.authentication.serializers import RegisterSerializerEx\n\nclass UserAgreementSerializer(serializers.Serializer):\n    name = serializers.CharField(max_length=256)\n    display_text = serializers.CharField(max_length=2048, default='')\n    url = serializers.CharField(max_length=2048, default='')\n    required = serializers.BooleanField(default=False)\n    value = serializers.BooleanField(default=False)\n\n    # pylint: disable=no-self-use\n    def to_representation(self, instance):\n        instance_ = instance.copy()\n        instance_['displayText'] = instance_.pop('display_text')\n        return instance_\n\nclass RestrictedRegisterSerializer(RegisterSerializerEx):\n    confirmations = UserAgreementSerializer(many=True, required=False)\n\n    def validate(self, data):\n        validated_data = super().validate(data)\n        server_user_agreements = settings.RESTRICTIONS['user_agreements']\n        for server_agreement in server_user_agreements:\n            if server_agreement['required']:\n                is_confirmed = False\n                for confirmation in validated_data['confirmations']:\n                    if confirmation['name'] == server_agreement['name'] \\\n                        and confirmation['value']:\n                        is_confirmed = True\n\n                if not is_confirmed:\n                    raise serializers.ValidationError(\n                        'Agreement {} must be accepted'.format(server_agreement['name'])\n                    )\n\n        return validated_data\n"""
cvat/apps/restrictions/tests.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\nfrom rest_framework.test import APITestCase, APIClient\nfrom rest_framework import status\nfrom django.conf import settings\n\n\nclass UserAgreementsTest(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n        self.user_agreements = settings.RESTRICTIONS[\'user_agreements\']\n        settings.RESTRICTIONS[\'user_agreements\'] = [\n            {\n                \'name\': \'agreement_1\',\n                \'display_text\': \'some display text 1\',\n                \'url\': \'https://example.com\',\n                \'required\': True,\n            },\n            {\n                \'name\': \'agreement_2\',\n                \'display_text\': \'some display text 2\',\n                \'url\': \'https://example2.com\',\n                \'required\': True,\n            },\n            {\n                \'name\': \'agreement_3\',\n                \'display_text\': \'some display text 3\',\n                \'url\': \'https://example3.com\',\n                \'required\': False,\n            },\n        ]\n\n    def tearDown(self):\n        settings.RESTRICTIONS[\'user_agreements\'] = self.user_agreements\n\n    def _get_user_agreements(self):\n        response = self.client.get(\'/api/v1/restrictions/user-agreements\')\n        assert response.status_code == status.HTTP_200_OK\n        for agreements in response.data:\n            assert \'name\' in agreements, agreements[\'name\']\n            assert \'displayText\' in agreements\n            assert \'required\' in agreements\n        return response.data\n\n    def _register_user(self, data):\n        response = self.client.post(\'/api/v1/auth/register\', data=data, format=""json"")\n        return response\n\n\n    def test_user_agreements(self):\n        self._get_user_agreements()\n\n    def test_register_user_with_required_confirmations(self):\n        agreements = self._get_user_agreements()\n        data = {\n            \'username\': \'some_username1\',\n            \'first_name\': \'some first name 1\',\n            \'last_name\': \'some last name 1\',\n            \'email\': \'user1@example.com\',\n            \'password1\': \'FnvL4YdF24NAmnQ8\',\n            \'password2\': \'FnvL4YdF24NAmnQ8\',\n            \'confirmations\':[],\n        }\n        for agreement in agreements:\n            if agreement[\'required\']:\n                data[\'confirmations\'].append({\n                    \'name\': agreement[\'name\'],\n                    \'value\': True,\n                })\n        response = self._register_user(data)\n        assert response.status_code == status.HTTP_201_CREATED\n\n    def test_register_user_without_confirmations(self):\n        data = {\n            \'username\': \'some_username2\',\n            \'first_name\': \'some first name 2\',\n            \'last_name\': \'some last name 2\',\n            \'email\': \'user2@example.com\',\n            \'password1\': \'FnvL4YdF24NAmnQ8\',\n            \'password2\': \'FnvL4YdF24NAmnQ8\',\n            \'confirmations\':[],\n        }\n\n        response = self._register_user(data)\n        assert response.status_code == status.HTTP_400_BAD_REQUEST\n\n    def test_register_user_with_all_confirmations(self):\n        agreements = self._get_user_agreements()\n        data = {\n            \'username\': \'some_username3\',\n            \'first_name\': \'some first name 3\',\n            \'last_name\': \'some last name 3\',\n            \'email\': \'user3@example.com\',\n            \'password1\': \'FnvL4YdF24NAmnQ8\',\n            \'password2\': \'FnvL4YdF24NAmnQ8\',\n            \'confirmations\':[],\n        }\n\n        for agreement in agreements:\n            data[\'confirmations\'].append({\n                \'name\': agreement[\'name\'],\n                \'value\': True,\n            })\n\n        response = self._register_user(data)\n        assert response.status_code == status.HTTP_201_CREATED\n'"
cvat/apps/restrictions/urls.py,0,b''
cvat/apps/restrictions/views.py,0,"b""# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.conf import settings\nfrom rest_framework.response import Response\nfrom rest_framework.decorators import action\nfrom rest_framework import viewsets\nfrom rest_framework.permissions import AllowAny\nfrom rest_framework.renderers import TemplateHTMLRenderer\nfrom drf_yasg.utils import swagger_auto_schema\n\nfrom cvat.apps.restrictions.serializers import UserAgreementSerializer\n\nclass RestrictionsViewSet(viewsets.ViewSet):\n    serializer_class = None\n    permission_classes = [AllowAny]\n    authentication_classes = []\n\n    # To get nice documentation about ServerViewSet actions it is necessary\n    # to implement the method. By default, ViewSet doesn't provide it.\n    def get_serializer(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    @swagger_auto_schema(\n        method='get',\n        operation_summary='Method provides user agreements that the user must accept to register',\n        responses={'200': UserAgreementSerializer})\n    @action(detail=False, methods=['GET'], serializer_class=UserAgreementSerializer, url_path='user-agreements')\n    def user_agreements(request):\n        user_agreements = settings.RESTRICTIONS['user_agreements']\n        serializer = UserAgreementSerializer(data=user_agreements, many=True)\n        serializer.is_valid(raise_exception=True)\n        return Response(data=serializer.data)\n\n    @staticmethod\n    @action(detail=False, methods=['GET'], renderer_classes=(TemplateHTMLRenderer,),\n        url_path='terms-of-use')\n    def terms_of_use(request):\n        return Response(template_name='restrictions/terms_of_use.html')\n"""
cvat/apps/tf_annotation/__init__.py,0,b'\n# Copyright (C) 2018-2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/tf_annotation/admin.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.contrib import admin\n\n# Register your models here.\n\n'
cvat/apps/tf_annotation/apps.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.apps import AppConfig\n\n\nclass TFAnnotationConfig(AppConfig):\n    name = 'tf_annotation'\n\n"""
cvat/apps/tf_annotation/models.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.db import models\n\n# Create your models here.\n\n'
cvat/apps/tf_annotation/tests.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.test import TestCase\n\n# Create your tests here.\n\n'
cvat/apps/tf_annotation/urls.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('create/task/<int:tid>', views.create),\n    path('check/task/<int:tid>', views.check),\n    path('cancel/task/<int:tid>', views.cancel),\n    path('meta/get', views.get_meta_info),\n]\n"""
cvat/apps/tf_annotation/views.py,6,"b'\n# Copyright (C) 2018-2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.http import HttpResponse, JsonResponse, HttpResponseBadRequest\nfrom rest_framework.decorators import api_view\nfrom rules.contrib.views import permission_required, objectgetter\nfrom cvat.apps.authentication.decorators import login_required\nfrom cvat.apps.dataset_manager.task import put_task_data\nfrom cvat.apps.engine.models import Task as TaskModel\nfrom cvat.apps.engine.serializers import LabeledDataSerializer\nfrom cvat.apps.engine.frame_provider import FrameProvider\n\nimport django_rq\nimport os\nimport rq\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom PIL import Image\nfrom cvat.apps.engine.log import slogger\n\n\ndef load_image_into_numpy(image):\n    (im_width, im_height) = image.size\n    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n\n\ndef run_inference_engine_annotation(image_list, labels_mapping, treshold):\n    from cvat.apps.auto_annotation.inference_engine import make_plugin_or_core, make_network\n\n    def _normalize_box(box, w, h, dw, dh):\n        xmin = min(int(box[0] * dw * w), w)\n        ymin = min(int(box[1] * dh * h), h)\n        xmax = min(int(box[2] * dw * w), w)\n        ymax = min(int(box[3] * dh * h), h)\n        return xmin, ymin, xmax, ymax\n\n    result = {}\n    MODEL_PATH = os.environ.get(\'TF_ANNOTATION_MODEL_PATH\')\n    if MODEL_PATH is None:\n        raise OSError(\'Model path env not found in the system.\')\n\n    core_or_plugin = make_plugin_or_core()\n    network = make_network(\'{}.xml\'.format(MODEL_PATH), \'{}.bin\'.format(MODEL_PATH))\n    input_blob_name = next(iter(network.inputs))\n    output_blob_name = next(iter(network.outputs))\n    if getattr(core_or_plugin, \'load_network\', False):\n        executable_network = core_or_plugin.load_network(network, \'CPU\')\n    else:\n        executable_network = core_or_plugin.load(network=network)\n    job = rq.get_current_job()\n\n    del network\n\n    try:\n        for image_num, im_name in enumerate(image_list):\n\n            job.refresh()\n            if \'cancel\' in job.meta:\n                del job.meta[\'cancel\']\n                job.save()\n                return None\n            job.meta[\'progress\'] = image_num * 100 / len(image_list)\n            job.save_meta()\n\n            image = Image.open(im_name)\n            width, height = image.size\n            image.thumbnail((600, 600), Image.ANTIALIAS)\n            dwidth, dheight = 600 / image.size[0], 600 / image.size[1]\n            image = image.crop((0, 0, 600, 600))\n            image_np = load_image_into_numpy(image)\n            image_np = np.transpose(image_np, (2, 0, 1))\n            prediction = executable_network.infer(inputs={input_blob_name: image_np[np.newaxis, ...]})[output_blob_name][0][0]\n            for obj in prediction:\n                obj_class = int(obj[1])\n                obj_value = obj[2]\n                if obj_class and obj_class in labels_mapping and obj_value >= treshold:\n                    label = labels_mapping[obj_class]\n                    if label not in result:\n                        result[label] = []\n                    xmin, ymin, xmax, ymax = _normalize_box(obj[3:7], width, height, dwidth, dheight)\n                    result[label].append([image_num, xmin, ymin, xmax, ymax])\n    finally:\n        del executable_network\n        del plugin\n\n    return result\n\n\ndef run_tensorflow_annotation(frame_provider, labels_mapping, treshold):\n    def _normalize_box(box, w, h):\n        xmin = int(box[1] * w)\n        ymin = int(box[0] * h)\n        xmax = int(box[3] * w)\n        ymax = int(box[2] * h)\n        return xmin, ymin, xmax, ymax\n\n    result = {}\n    model_path = os.environ.get(\'TF_ANNOTATION_MODEL_PATH\')\n    if model_path is None:\n        raise OSError(\'Model path env not found in the system.\')\n    job = rq.get_current_job()\n\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(model_path + \'.pb\', \'rb\') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name=\'\')\n\n        try:\n            config = tf.ConfigProto()\n            config.gpu_options.allow_growth=True\n            sess = tf.Session(graph=detection_graph, config=config)\n            frames = frame_provider.get_frames(frame_provider.Quality.ORIGINAL)\n            for image_num, (image, _) in enumerate(frames):\n\n                job.refresh()\n                if \'cancel\' in job.meta:\n                    del job.meta[\'cancel\']\n                    job.save()\n                    return None\n                job.meta[\'progress\'] = image_num * 100 / len(frame_provider)\n                job.save_meta()\n\n                image = Image.open(image)\n                width, height = image.size\n                if width > 1920 or height > 1080:\n                    image = image.resize((width // 2, height // 2), Image.ANTIALIAS)\n                image_np = load_image_into_numpy(image)\n                image_np_expanded = np.expand_dims(image_np, axis=0)\n\n                image_tensor = detection_graph.get_tensor_by_name(\'image_tensor:0\')\n                boxes = detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n                scores = detection_graph.get_tensor_by_name(\'detection_scores:0\')\n                classes = detection_graph.get_tensor_by_name(\'detection_classes:0\')\n                num_detections = detection_graph.get_tensor_by_name(\'num_detections:0\')\n                (boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections], feed_dict={image_tensor: image_np_expanded})\n\n                for i in range(len(classes[0])):\n                    if classes[0][i] in labels_mapping.keys():\n                        if scores[0][i] >= treshold:\n                            xmin, ymin, xmax, ymax = _normalize_box(boxes[0][i], width, height)\n                            label = labels_mapping[classes[0][i]]\n                            if label not in result:\n                                result[label] = []\n                            result[label].append([image_num, xmin, ymin, xmax, ymax])\n        finally:\n            sess.close()\n            del sess\n    return result\n\ndef convert_to_cvat_format(data):\n    result = {\n        ""tracks"": [],\n        ""shapes"": [],\n        ""tags"": [],\n        ""version"": 0,\n    }\n\n    for label in data:\n        boxes = data[label]\n        for box in boxes:\n            result[\'shapes\'].append({\n                ""type"": ""rectangle"",\n                ""label_id"": label,\n                ""frame"": box[0],\n                ""points"": [box[1], box[2], box[3], box[4]],\n                ""z_order"": 0,\n                ""group"": None,\n                ""occluded"": False,\n                ""attributes"": [],\n            })\n\n    return result\n\ndef create_thread(tid, labels_mapping, user):\n    try:\n        TRESHOLD = 0.5\n        # Init rq job\n        job = rq.get_current_job()\n        job.meta[\'progress\'] = 0\n        job.save_meta()\n        # Get job indexes and segment length\n        db_task = TaskModel.objects.get(pk=tid)\n        # Get image list\n        image_list = FrameProvider(db_task.data)\n\n        # Run auto annotation by tf\n        result = None\n        slogger.glob.info(""tf annotation with tensorflow framework for task {}"".format(tid))\n        result = run_tensorflow_annotation(image_list, labels_mapping, TRESHOLD)\n\n        if result is None:\n            slogger.glob.info(\'tf annotation for task {} canceled by user\'.format(tid))\n            return\n\n        # Modify data format and save\n        result = convert_to_cvat_format(result)\n        serializer = LabeledDataSerializer(data = result)\n        if serializer.is_valid(raise_exception=True):\n            put_task_data(tid, result)\n        slogger.glob.info(\'tf annotation for task {} done\'.format(tid))\n    except Exception as ex:\n        try:\n            slogger.task[tid].exception(\'exception was occured during tf annotation of the task\', exc_info=True)\n        except:\n            slogger.glob.exception(\'exception was occured during tf annotation of the task {}\'.format(tid), exc_info=True)\n        raise ex\n\n@api_view([\'POST\'])\n@login_required\ndef get_meta_info(request):\n    try:\n        queue = django_rq.get_queue(\'low\')\n        tids = request.data\n        result = {}\n        for tid in tids:\n            job = queue.fetch_job(\'tf_annotation.create/{}\'.format(tid))\n            if job is not None:\n                result[tid] = {\n                    ""active"": job.is_queued or job.is_started,\n                    ""success"": not job.is_failed\n                }\n\n        return JsonResponse(result)\n    except Exception as ex:\n        slogger.glob.exception(\'exception was occured during tf meta request\', exc_info=True)\n        return HttpResponseBadRequest(str(ex))\n\n\n@login_required\n@permission_required(perm=[\'engine.task.change\'],\n    fn=objectgetter(TaskModel, \'tid\'), raise_exception=True)\ndef create(request, tid):\n    slogger.glob.info(\'tf annotation create request for task {}\'.format(tid))\n    try:\n        db_task = TaskModel.objects.get(pk=tid)\n        queue = django_rq.get_queue(\'low\')\n        job = queue.fetch_job(\'tf_annotation.create/{}\'.format(tid))\n        if job is not None and (job.is_started or job.is_queued):\n            raise Exception(""The process is already running"")\n\n        db_labels = db_task.label_set.prefetch_related(\'attributespec_set\').all()\n        db_labels = {db_label.id:db_label.name for db_label in db_labels}\n\n        tf_annotation_labels = {\n            ""person"": 1, ""bicycle"": 2, ""car"": 3, ""motorcycle"": 4, ""airplane"": 5,\n            ""bus"": 6, ""train"": 7, ""truck"": 8, ""boat"": 9, ""traffic_light"": 10,\n            ""fire_hydrant"": 11, ""stop_sign"": 13, ""parking_meter"": 14, ""bench"": 15,\n            ""bird"": 16, ""cat"": 17, ""dog"": 18, ""horse"": 19, ""sheep"": 20, ""cow"": 21,\n            ""elephant"": 22, ""bear"": 23, ""zebra"": 24, ""giraffe"": 25, ""backpack"": 27,\n            ""umbrella"": 28, ""handbag"": 31, ""tie"": 32, ""suitcase"": 33, ""frisbee"": 34,\n            ""skis"": 35, ""snowboard"": 36, ""sports_ball"": 37, ""kite"": 38, ""baseball_bat"": 39,\n            ""baseball_glove"": 40, ""skateboard"": 41, ""surfboard"": 42, ""tennis_racket"": 43,\n            ""bottle"": 44, ""wine_glass"": 46, ""cup"": 47, ""fork"": 48, ""knife"": 49, ""spoon"": 50,\n            ""bowl"": 51, ""banana"": 52, ""apple"": 53, ""sandwich"": 54, ""orange"": 55, ""broccoli"": 56,\n            ""carrot"": 57, ""hot_dog"": 58, ""pizza"": 59, ""donut"": 60, ""cake"": 61, ""chair"": 62,\n            ""couch"": 63, ""potted_plant"": 64, ""bed"": 65, ""dining_table"": 67, ""toilet"": 70,\n            ""tv"": 72, ""laptop"": 73, ""mouse"": 74, ""remote"": 75, ""keyboard"": 76, ""cell_phone"": 77,\n            ""microwave"": 78, ""oven"": 79, ""toaster"": 80, ""sink"": 81, ""refrigerator"": 83,\n            ""book"": 84, ""clock"": 85, ""vase"": 86, ""scissors"": 87, ""teddy_bear"": 88, ""hair_drier"": 89,\n            ""toothbrush"": 90\n            }\n\n        labels_mapping = {}\n        for key, labels in db_labels.items():\n            if labels in tf_annotation_labels.keys():\n                labels_mapping[tf_annotation_labels[labels]] = key\n\n        if not len(labels_mapping.values()):\n            raise Exception(\'No labels found for tf annotation\')\n\n        # Run tf annotation job\n        queue.enqueue_call(func=create_thread,\n            args=(tid, labels_mapping, request.user),\n            job_id=\'tf_annotation.create/{}\'.format(tid),\n            timeout=604800)     # 7 days\n\n        slogger.task[tid].info(\'tensorflow annotation job enqueued with labels {}\'.format(labels_mapping))\n\n    except Exception as ex:\n        try:\n            slogger.task[tid].exception(""exception was occured during tensorflow annotation request"", exc_info=True)\n        except:\n            pass\n        return HttpResponseBadRequest(str(ex))\n\n    return HttpResponse()\n\n@login_required\n@permission_required(perm=[\'engine.task.access\'],\n    fn=objectgetter(TaskModel, \'tid\'), raise_exception=True)\ndef check(request, tid):\n    try:\n        queue = django_rq.get_queue(\'low\')\n        job = queue.fetch_job(\'tf_annotation.create/{}\'.format(tid))\n        if job is not None and \'cancel\' in job.meta:\n            return JsonResponse({\'status\': \'finished\'})\n        data = {}\n        if job is None:\n            data[\'status\'] = \'unknown\'\n        elif job.is_queued:\n            data[\'status\'] = \'queued\'\n        elif job.is_started:\n            data[\'status\'] = \'started\'\n            data[\'progress\'] = job.meta[\'progress\']\n        elif job.is_finished:\n            data[\'status\'] = \'finished\'\n            job.delete()\n        else:\n            data[\'status\'] = \'failed\'\n            data[\'stderr\'] = job.exc_info\n            job.delete()\n\n    except Exception:\n        data[\'status\'] = \'unknown\'\n\n    return JsonResponse(data)\n\n\n@login_required\n@permission_required(perm=[\'engine.task.change\'],\n    fn=objectgetter(TaskModel, \'tid\'), raise_exception=True)\ndef cancel(request, tid):\n    try:\n        queue = django_rq.get_queue(\'low\')\n        job = queue.fetch_job(\'tf_annotation.create/{}\'.format(tid))\n        if job is None or job.is_finished or job.is_failed:\n            raise Exception(\'Task is not being annotated currently\')\n        elif \'cancel\' not in job.meta:\n            job.meta[\'cancel\'] = True\n            job.save()\n\n    except Exception as ex:\n        try:\n            slogger.task[tid].exception(""cannot cancel tensorflow annotation for task #{}"".format(tid), exc_info=True)\n        except:\n            pass\n        return HttpResponseBadRequest(str(ex))\n\n    return HttpResponse()\n'"
datumaro/datumaro/cli/__init__.py,0,b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
datumaro/datumaro/cli/__main__.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\nimport logging as log\nimport sys\n\nfrom . import contexts, commands\nfrom .util import CliException, add_subparser\nfrom ..version import VERSION\n\n\n_log_levels = {\n    \'debug\': log.DEBUG,\n    \'info\': log.INFO,\n    \'warning\': log.WARNING,\n    \'error\': log.ERROR,\n    \'critical\': log.CRITICAL\n}\n\ndef loglevel(name):\n    return _log_levels[name]\n\nclass _LogManager:\n    @classmethod\n    def init_logger(cls, args=None):\n        # Define minimalistic parser only to obtain loglevel\n        parser = argparse.ArgumentParser(add_help=False)\n        cls._define_loglevel_option(parser)\n        args, _ = parser.parse_known_args(args)\n\n        log.basicConfig(format=\'%(asctime)s %(levelname)s: %(message)s\',\n            level=args.loglevel)\n\n    @staticmethod\n    def _define_loglevel_option(parser):\n        parser.add_argument(\'--loglevel\', type=loglevel, default=\'info\',\n            help=""Logging level (options: %s; default: %s)"" % \\\n                (\', \'.join(_log_levels.keys()), ""%(default)s""))\n        return parser\n\n\ndef _make_subcommands_help(commands, help_line_start=0):\n    desc = """"\n    for command_name, _, command_help in commands:\n        desc += (""  %-"" + str(max(0, help_line_start - 2 - 1)) + ""s%s\\n"") % \\\n            (command_name, command_help)\n    return desc\n\ndef make_parser():\n    parser = argparse.ArgumentParser(prog=""datumaro"",\n        description=""Dataset Framework"",\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    parser.add_argument(\'--version\', action=\'version\', version=VERSION)\n    _LogManager._define_loglevel_option(parser)\n\n    known_contexts = [\n        (\'project\', contexts.project, ""Actions on projects (datasets)""),\n        (\'source\', contexts.source, ""Actions on data sources""),\n        (\'model\', contexts.model, ""Actions on models""),\n    ]\n    known_commands = [\n        (\'create\', commands.create, ""Create project""),\n        (\'add\', commands.add, ""Add source to project""),\n        (\'remove\', commands.remove, ""Remove source from project""),\n        (\'export\', commands.export, ""Export project""),\n        (\'explain\', commands.explain, ""Run Explainable AI algorithm for model""),\n    ]\n\n    # Argparse doesn\'t support subparser groups:\n    # https://stackoverflow.com/questions/32017020/grouping-argparse-subparser-arguments\n    help_line_start = max((len(e[0]) for e in known_contexts + known_commands),\n        default=0)\n    help_line_start = max((2 + help_line_start) // 4 + 1, 6) * 4 # align to tabs\n    subcommands_desc = """"\n    if known_contexts:\n        subcommands_desc += ""Contexts:\\n""\n        subcommands_desc += _make_subcommands_help(known_contexts,\n            help_line_start)\n    if known_commands:\n        if subcommands_desc:\n            subcommands_desc += ""\\n""\n        subcommands_desc += ""Commands:\\n""\n        subcommands_desc += _make_subcommands_help(known_commands,\n            help_line_start)\n    if subcommands_desc:\n        subcommands_desc += \\\n            ""\\nRun \'%s COMMAND --help\' for more information on a command."" % \\\n                parser.prog\n\n    subcommands = parser.add_subparsers(title=subcommands_desc,\n        description="""", help=argparse.SUPPRESS)\n    for command_name, command, _ in known_contexts + known_commands:\n        add_subparser(subcommands, command_name, command.build_parser)\n\n    return parser\n\n\ndef main(args=None):\n    _LogManager.init_logger(args)\n\n    parser = make_parser()\n    args = parser.parse_args(args)\n\n    if \'command\' not in args:\n        parser.print_help()\n        return 1\n\n    try:\n        return args.command(args)\n    except CliException as e:\n        log.error(e)\n        return 1\n    except Exception as e:\n        log.error(e)\n        raise\n\n\nif __name__ == \'__main__\':\n    sys.exit(main())'"
datumaro/datumaro/components/__init__.py,0,b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n'
datumaro/datumaro/components/cli_plugin.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\n\nfrom datumaro.cli.util import MultilineFormatter\nfrom datumaro.util import to_snake_case\n\n\nclass CliPlugin:\n    @staticmethod\n    def _get_name(cls):\n        return getattr(cls, \'NAME\',\n            remove_plugin_type(to_snake_case(cls.__name__)))\n\n    @staticmethod\n    def _get_doc(cls):\n        return getattr(cls, \'__doc__\', """")\n\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        args = {\n            \'prog\': cls._get_name(cls),\n            \'description\': cls._get_doc(cls),\n            \'formatter_class\': MultilineFormatter,\n        }\n        args.update(kwargs)\n\n        return argparse.ArgumentParser(**args)\n\n    @classmethod\n    def from_cmdline(cls, args=None):\n        if args and args[0] == \'--\':\n            args = args[1:]\n        parser = cls.build_cmdline_parser()\n        args = parser.parse_args(args)\n        return vars(args)\n\ndef remove_plugin_type(s):\n    for t in {\'transform\', \'extractor\', \'converter\', \'launcher\', \'importer\'}:\n        s = s.replace(\'_\' + t, \'\')\n    return s\n'"
datumaro/datumaro/components/comparator.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom itertools import zip_longest\nimport numpy as np\n\nfrom datumaro.components.extractor import AnnotationType, LabelCategories\n\n\nclass Comparator:\n    def __init__(self,\n            iou_threshold=0.5, conf_threshold=0.9):\n        self.iou_threshold = iou_threshold\n        self.conf_threshold = conf_threshold\n\n    @staticmethod\n    def iou(box_a, box_b):\n        return box_a.iou(box_b)\n\n    # pylint: disable=no-self-use\n    def compare_dataset_labels(self, extractor_a, extractor_b):\n        a_label_cat = extractor_a.categories().get(AnnotationType.label)\n        b_label_cat = extractor_b.categories().get(AnnotationType.label)\n        if not a_label_cat and not b_label_cat:\n            return None\n        if not a_label_cat:\n            a_label_cat = LabelCategories()\n        if not b_label_cat:\n            b_label_cat = LabelCategories()\n\n        mismatches = []\n        for a_label, b_label in zip_longest(a_label_cat.items, b_label_cat.items):\n            if a_label != b_label:\n                mismatches.append((a_label, b_label))\n        return mismatches\n    # pylint: enable=no-self-use\n\n    def compare_item_labels(self, item_a, item_b):\n        conf_threshold = self.conf_threshold\n\n        a_labels = set([ann.label for ann in item_a.annotations \\\n            if ann.type is AnnotationType.label and \\\n               conf_threshold < ann.attributes.get('score', 1)])\n        b_labels = set([ann.label for ann in item_b.annotations \\\n            if ann.type is AnnotationType.label and \\\n               conf_threshold < ann.attributes.get('score', 1)])\n\n        a_unmatched = a_labels - b_labels\n        b_unmatched = b_labels - a_labels\n        matches = a_labels & b_labels\n\n        return matches, a_unmatched, b_unmatched\n\n    def compare_item_bboxes(self, item_a, item_b):\n        iou_threshold = self.iou_threshold\n        conf_threshold = self.conf_threshold\n\n        a_boxes = [ann for ann in item_a.annotations \\\n            if ann.type is AnnotationType.bbox and \\\n               conf_threshold < ann.attributes.get('score', 1)]\n        b_boxes = [ann for ann in item_b.annotations \\\n            if ann.type is AnnotationType.bbox and \\\n               conf_threshold < ann.attributes.get('score', 1)]\n        a_boxes.sort(key=lambda ann: 1 - ann.attributes.get('score', 1))\n        b_boxes.sort(key=lambda ann: 1 - ann.attributes.get('score', 1))\n\n        # a_matches: indices of b_boxes matched to a bboxes\n        # b_matches: indices of a_boxes matched to b bboxes\n        a_matches = -np.ones(len(a_boxes), dtype=int)\n        b_matches = -np.ones(len(b_boxes), dtype=int)\n\n        iou_matrix = np.array([\n            [self.iou(a, b) for b in b_boxes] for a in a_boxes\n        ])\n\n        # matches: boxes we succeeded to match completely\n        # mispred: boxes we succeeded to match, having label mismatch\n        matches = []\n        mispred = []\n\n        for a_idx, a_bbox in enumerate(a_boxes):\n            if len(b_boxes) == 0:\n                break\n            matched_b = a_matches[a_idx]\n            iou_max = max(iou_matrix[a_idx, matched_b], iou_threshold)\n            for b_idx, b_bbox in enumerate(b_boxes):\n                if 0 <= b_matches[b_idx]: # assign a_bbox with max conf\n                    continue\n                iou = iou_matrix[a_idx, b_idx]\n                if iou < iou_max:\n                    continue\n                iou_max = iou\n                matched_b = b_idx\n\n            if matched_b < 0:\n                continue\n            a_matches[a_idx] = matched_b\n            b_matches[matched_b] = a_idx\n\n            b_bbox = b_boxes[matched_b]\n\n            if a_bbox.label == b_bbox.label:\n                matches.append( (a_bbox, b_bbox) )\n            else:\n                mispred.append( (a_bbox, b_bbox) )\n\n        # *_umatched: boxes of (*) we failed to match\n        a_unmatched = [a_boxes[i] for i, m in enumerate(a_matches) if m < 0]\n        b_unmatched = [b_boxes[i] for i, m in enumerate(b_matches) if m < 0]\n\n        return matches, mispred, a_unmatched, b_unmatched\n"""
datumaro/datumaro/components/config.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport yaml\n\n\nclass Schema:\n    class Item:\n        def __init__(self, ctor, internal=False):\n            self.ctor = ctor\n            self.internal = internal\n\n        def __call__(self, *args, **kwargs):\n            return self.ctor(*args, **kwargs)\n\n    def __init__(self, items=None, fallback=None):\n        self._items = {}\n        if items is not None:\n            self._items.update(items)\n        self._fallback = fallback\n\n    def _get_items(self, allow_fallback=True):\n        all_items = {}\n\n        if allow_fallback and self._fallback is not None:\n            all_items.update(self._fallback)\n        all_items.update(self._items)\n\n        return all_items\n\n    def items(self, allow_fallback=True):\n        return self._get_items(allow_fallback=allow_fallback).items()\n\n    def keys(self, allow_fallback=True):\n        return self._get_items(allow_fallback=allow_fallback).keys()\n\n    def values(self, allow_fallback=True):\n        return self._get_items(allow_fallback=allow_fallback).values()\n\n    def __contains__(self, key):\n        return key in self.keys()\n\n    def __len__(self):\n        return len(self._get_items())\n\n    def __iter__(self):\n        return iter(self._get_items())\n\n    def __getitem__(self, key):\n        default = object()\n        value = self.get(key, default=default)\n        if value is default:\n            raise KeyError(\'Key ""%s"" does not exist\' % (key))\n        return value\n\n    def get(self, key, default=None):\n        found = self._items.get(key, default)\n        if found is not default:\n            return found\n\n        if self._fallback is not None:\n            return self._fallback.get(key, default)\n\nclass SchemaBuilder:\n    def __init__(self):\n        self._items = {}\n\n    def add(self, name, ctor=str, internal=False):\n        if name in self._items:\n            raise KeyError(\'Key ""%s"" already exists\' % (name))\n\n        self._items[name] = Schema.Item(ctor, internal=internal)\n        return self\n\n    def build(self):\n        return Schema(self._items)\n\nclass Config:\n    def __init__(self, config=None, fallback=None, schema=None, mutable=True):\n        # schema should be established first\n        self.__dict__[\'_schema\'] = schema\n        self.__dict__[\'_mutable\'] = True\n\n        self.__dict__[\'_config\'] = {}\n        if fallback is not None:\n            for k, v in fallback.items(allow_fallback=False):\n                self.set(k, v)\n        if config is not None:\n            self.update(config)\n\n        self.__dict__[\'_mutable\'] = mutable\n\n    def _items(self, allow_fallback=True, allow_internal=True):\n        all_config = {}\n        if allow_fallback and self._schema is not None:\n            for key, item in self._schema.items():\n                all_config[key] = item()\n        all_config.update(self._config)\n\n        if not allow_internal and self._schema is not None:\n            for key, item in self._schema.items():\n                if item.internal:\n                    all_config.pop(key)\n        return all_config\n\n    def items(self, allow_fallback=True, allow_internal=True):\n        return self._items(\n                allow_fallback=allow_fallback,\n                allow_internal=allow_internal\n            ).items()\n\n    def keys(self, allow_fallback=True, allow_internal=True):\n        return self._items(\n                allow_fallback=allow_fallback,\n                allow_internal=allow_internal\n            ).keys()\n\n    def values(self, allow_fallback=True, allow_internal=True):\n        return self._items(\n                allow_fallback=allow_fallback,\n                allow_internal=allow_internal\n            ).values()\n\n    def __contains__(self, key):\n        return key in self.keys()\n\n    def __len__(self):\n        return len(self.items())\n\n    def __iter__(self):\n        return iter(zip(self.keys(), self.values()))\n\n    def __getitem__(self, key):\n        default = object()\n        value = self.get(key, default=default)\n        if value is default:\n            raise KeyError(\'Key ""%s"" does not exist\' % (key))\n        return value\n\n    def __setitem__(self, key, value):\n        return self.set(key, value)\n\n    def __getattr__(self, key):\n        return self.get(key)\n\n    def __setattr__(self, key, value):\n        return self.set(key, value)\n\n    def __eq__(self, other):\n        try:\n            for k, my_v in self.items(allow_internal=False):\n                other_v = other[k]\n                if my_v != other_v:\n                    return False\n            return True\n        except Exception:\n            return False\n\n    def update(self, other):\n        for k, v in other.items():\n            self.set(k, v)\n\n    def remove(self, key):\n        if not self._mutable:\n            raise Exception(""Cannot set value of immutable object"")\n\n        self._config.pop(key, None)\n\n    def get(self, key, default=None):\n        found = self._config.get(key, default)\n        if found is not default:\n            return found\n\n        if self._schema is not None:\n            found = self._schema.get(key, default)\n            if found is not default:\n                # ignore mutability\n                found = found()\n                self._config[key] = found\n                return found\n\n        return found\n\n    def set(self, key, value):\n        if not self._mutable:\n            raise Exception(""Cannot set value of immutable object"")\n\n        if self._schema is not None:\n            if key not in self._schema:\n                raise Exception(""Can not set key \'%s\' - schema mismatch"" % (key))\n\n            schema_entry = self._schema[key]\n            schema_entry_instance = schema_entry()\n            if not isinstance(value, type(schema_entry_instance)):\n                if isinstance(value, dict) and \\\n                        isinstance(schema_entry_instance, Config):\n                    schema_entry_instance.update(value)\n                    value = schema_entry_instance\n                else:\n                    raise Exception(""Can not set key \'%s\' - schema mismatch"" % (key))\n\n        self._config[key] = value\n        return value\n\n    @staticmethod\n    def parse(path):\n        with open(path, \'r\') as f:\n            return Config(yaml.safe_load(f))\n\n    @staticmethod\n    def yaml_representer(dumper, value):\n        return dumper.represent_data(\n            value._items(allow_internal=False, allow_fallback=False))\n\n    def dump(self, path):\n        with open(path, \'w+\') as f:\n            yaml.dump(self, f)\n\nyaml.add_multi_representer(Config, Config.yaml_representer)\n\n\nclass DefaultConfig(Config):\n    def __init__(self, default=None):\n        super().__init__()\n        self.__dict__[\'_default\'] = default\n\n    def set(self, key, value):\n        if key not in self.keys(allow_fallback=False):\n            value = self._default(value)\n            return super().set(key, value)\n        else:\n            return super().set(key, value)\n\n\nDEFAULT_FORMAT = \'datumaro\''"
datumaro/datumaro/components/config_model.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom datumaro.components.config import Config, \\\n    DefaultConfig as _DefaultConfig, \\\n    SchemaBuilder as _SchemaBuilder\n\n\nSOURCE_SCHEMA = _SchemaBuilder() \\\n    .add('url', str) \\\n    .add('format', str) \\\n    .add('options', dict) \\\n    .build()\n\nclass Source(Config):\n    def __init__(self, config=None):\n        super().__init__(config, schema=SOURCE_SCHEMA)\n\n\nMODEL_SCHEMA = _SchemaBuilder() \\\n    .add('launcher', str) \\\n    .add('model_dir', str, internal=True) \\\n    .add('options', dict) \\\n    .build()\n\nclass Model(Config):\n    def __init__(self, config=None):\n        super().__init__(config, schema=MODEL_SCHEMA)\n\n\nPROJECT_SCHEMA = _SchemaBuilder() \\\n    .add('project_name', str) \\\n    .add('format_version', int) \\\n    \\\n    .add('subsets', list) \\\n    .add('sources', lambda: _DefaultConfig(\n        lambda v=None: Source(v))) \\\n    .add('models', lambda: _DefaultConfig(\n        lambda v=None: Model(v))) \\\n    \\\n    .add('models_dir', str, internal=True) \\\n    .add('plugins_dir', str, internal=True) \\\n    .add('sources_dir', str, internal=True) \\\n    .add('dataset_dir', str, internal=True) \\\n    .add('project_filename', str, internal=True) \\\n    .add('project_dir', str, internal=True) \\\n    .add('env_dir', str, internal=True) \\\n    .build()\n\nPROJECT_DEFAULT_CONFIG = Config({\n    'project_name': 'undefined',\n    'format_version': 1,\n\n    'sources_dir': 'sources',\n    'dataset_dir': 'dataset',\n    'models_dir': 'models',\n    'plugins_dir': 'plugins',\n\n    'project_filename': 'config.yaml',\n    'project_dir': '',\n    'env_dir': '.datumaro',\n}, mutable=False, schema=PROJECT_SCHEMA)\n"""
datumaro/datumaro/components/converter.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nclass Converter:\n    def __init__(self, cmdline_args=None):\n        pass\n\n    def __call__(self, extractor, save_dir):\n        raise NotImplementedError()\n\n    def _parse_cmdline(self, cmdline):\n        parser = self.build_cmdline_parser()\n\n        if len(cmdline) != 0 and cmdline[0] == '--':\n            cmdline = cmdline[1:]\n        args = parser.parse_args(cmdline)\n        return vars(args)"""
datumaro/datumaro/components/dataset_filter.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport logging as log\nfrom lxml import etree as ET # NOTE: lxml has proper XPath implementation\nfrom datumaro.components.extractor import (Transform,\n    Annotation, AnnotationType,\n    Label, Mask, Points, Polygon, PolyLine, Bbox, Caption,\n)\n\n\nclass DatasetItemEncoder:\n    @classmethod\n    def encode(cls, item, categories=None):\n        item_elem = ET.Element(\'item\')\n        ET.SubElement(item_elem, \'id\').text = str(item.id)\n        ET.SubElement(item_elem, \'subset\').text = str(item.subset)\n        ET.SubElement(item_elem, \'path\').text = str(\'/\'.join(item.path))\n\n        image = item.image\n        if image is not None:\n            item_elem.append(cls.encode_image(image))\n\n        for ann in item.annotations:\n            item_elem.append(cls.encode_annotation(ann, categories))\n\n        return item_elem\n\n    @classmethod\n    def encode_image(cls, image):\n        image_elem = ET.Element(\'image\')\n\n        size = image.size\n        if size is not None:\n            h, w = size\n        else:\n            h = \'unknown\'\n            w = h\n        ET.SubElement(image_elem, \'width\').text = str(w)\n        ET.SubElement(image_elem, \'height\').text = str(h)\n\n        ET.SubElement(image_elem, \'has_data\').text = \'%d\' % int(image.has_data)\n        ET.SubElement(image_elem, \'path\').text = image.path\n\n        return image_elem\n\n    @classmethod\n    def encode_annotation_base(cls, annotation):\n        assert isinstance(annotation, Annotation)\n        ann_elem = ET.Element(\'annotation\')\n        ET.SubElement(ann_elem, \'id\').text = str(annotation.id)\n        ET.SubElement(ann_elem, \'type\').text = str(annotation.type.name)\n\n        for k, v in annotation.attributes.items():\n            ET.SubElement(ann_elem, k.replace(\' \', \'-\')).text = str(v)\n\n        ET.SubElement(ann_elem, \'group\').text = str(annotation.group)\n\n        return ann_elem\n\n    @staticmethod\n    def _get_label(label_id, categories):\n        label = \'\'\n        if label_id is None:\n            return \'\'\n        if categories is not None:\n            label_cat = categories.get(AnnotationType.label)\n            if label_cat is not None:\n                label = label_cat.items[label_id].name\n        return label\n\n    @classmethod\n    def encode_label_object(cls, obj, categories):\n        ann_elem = cls.encode_annotation_base(obj)\n\n        ET.SubElement(ann_elem, \'label\').text = \\\n            str(cls._get_label(obj.label, categories))\n        ET.SubElement(ann_elem, \'label_id\').text = str(obj.label)\n\n        return ann_elem\n\n    @classmethod\n    def encode_mask_object(cls, obj, categories):\n        ann_elem = cls.encode_annotation_base(obj)\n\n        ET.SubElement(ann_elem, \'label\').text = \\\n            str(cls._get_label(obj.label, categories))\n        ET.SubElement(ann_elem, \'label_id\').text = str(obj.label)\n\n        return ann_elem\n\n    @classmethod\n    def encode_bbox_object(cls, obj, categories):\n        ann_elem = cls.encode_annotation_base(obj)\n\n        ET.SubElement(ann_elem, \'label\').text = \\\n            str(cls._get_label(obj.label, categories))\n        ET.SubElement(ann_elem, \'label_id\').text = str(obj.label)\n        ET.SubElement(ann_elem, \'x\').text = str(obj.x)\n        ET.SubElement(ann_elem, \'y\').text = str(obj.y)\n        ET.SubElement(ann_elem, \'w\').text = str(obj.w)\n        ET.SubElement(ann_elem, \'h\').text = str(obj.h)\n        ET.SubElement(ann_elem, \'area\').text = str(obj.get_area())\n\n        return ann_elem\n\n    @classmethod\n    def encode_points_object(cls, obj, categories):\n        ann_elem = cls.encode_annotation_base(obj)\n\n        ET.SubElement(ann_elem, \'label\').text = \\\n            str(cls._get_label(obj.label, categories))\n        ET.SubElement(ann_elem, \'label_id\').text = str(obj.label)\n\n        x, y, w, h = obj.get_bbox()\n        area = w * h\n        bbox_elem = ET.SubElement(ann_elem, \'bbox\')\n        ET.SubElement(bbox_elem, \'x\').text = str(x)\n        ET.SubElement(bbox_elem, \'y\').text = str(y)\n        ET.SubElement(bbox_elem, \'w\').text = str(w)\n        ET.SubElement(bbox_elem, \'h\').text = str(h)\n        ET.SubElement(bbox_elem, \'area\').text = str(area)\n\n        points = obj.points\n        for i in range(0, len(points), 2):\n            point_elem = ET.SubElement(ann_elem, \'point\')\n            ET.SubElement(point_elem, \'x\').text = str(points[i])\n            ET.SubElement(point_elem, \'y\').text = str(points[i + 1])\n            ET.SubElement(point_elem, \'visible\').text = \\\n                str(obj.visibility[i // 2].name)\n\n        return ann_elem\n\n    @classmethod\n    def encode_polygon_object(cls, obj, categories):\n        ann_elem = cls.encode_annotation_base(obj)\n\n        ET.SubElement(ann_elem, \'label\').text = \\\n            str(cls._get_label(obj.label, categories))\n        ET.SubElement(ann_elem, \'label_id\').text = str(obj.label)\n\n        x, y, w, h = obj.get_bbox()\n        area = w * h\n        bbox_elem = ET.SubElement(ann_elem, \'bbox\')\n        ET.SubElement(bbox_elem, \'x\').text = str(x)\n        ET.SubElement(bbox_elem, \'y\').text = str(y)\n        ET.SubElement(bbox_elem, \'w\').text = str(w)\n        ET.SubElement(bbox_elem, \'h\').text = str(h)\n        ET.SubElement(bbox_elem, \'area\').text = str(area)\n\n        points = obj.points\n        for i in range(0, len(points), 2):\n            point_elem = ET.SubElement(ann_elem, \'point\')\n            ET.SubElement(point_elem, \'x\').text = str(points[i])\n            ET.SubElement(point_elem, \'y\').text = str(points[i + 1])\n\n        return ann_elem\n\n    @classmethod\n    def encode_polyline_object(cls, obj, categories):\n        ann_elem = cls.encode_annotation_base(obj)\n\n        ET.SubElement(ann_elem, \'label\').text = \\\n            str(cls._get_label(obj.label, categories))\n        ET.SubElement(ann_elem, \'label_id\').text = str(obj.label)\n\n        x, y, w, h = obj.get_bbox()\n        area = w * h\n        bbox_elem = ET.SubElement(ann_elem, \'bbox\')\n        ET.SubElement(bbox_elem, \'x\').text = str(x)\n        ET.SubElement(bbox_elem, \'y\').text = str(y)\n        ET.SubElement(bbox_elem, \'w\').text = str(w)\n        ET.SubElement(bbox_elem, \'h\').text = str(h)\n        ET.SubElement(bbox_elem, \'area\').text = str(area)\n\n        points = obj.points\n        for i in range(0, len(points), 2):\n            point_elem = ET.SubElement(ann_elem, \'point\')\n            ET.SubElement(point_elem, \'x\').text = str(points[i])\n            ET.SubElement(point_elem, \'y\').text = str(points[i + 1])\n\n        return ann_elem\n\n    @classmethod\n    def encode_caption_object(cls, obj):\n        ann_elem = cls.encode_annotation_base(obj)\n\n        ET.SubElement(ann_elem, \'caption\').text = str(obj.caption)\n\n        return ann_elem\n\n    @classmethod\n    def encode_annotation(cls, o, categories=None):\n        if isinstance(o, Label):\n            return cls.encode_label_object(o, categories)\n        if isinstance(o, Mask):\n            return cls.encode_mask_object(o, categories)\n        if isinstance(o, Bbox):\n            return cls.encode_bbox_object(o, categories)\n        if isinstance(o, Points):\n            return cls.encode_points_object(o, categories)\n        if isinstance(o, PolyLine):\n            return cls.encode_polyline_object(o, categories)\n        if isinstance(o, Polygon):\n            return cls.encode_polygon_object(o, categories)\n        if isinstance(o, Caption):\n            return cls.encode_caption_object(o)\n        raise NotImplementedError(""Unexpected annotation object passed: %s"" % o)\n\n    @staticmethod\n    def to_string(encoded_item):\n        return ET.tostring(encoded_item, encoding=\'unicode\', pretty_print=True)\n\ndef XPathDatasetFilter(extractor, xpath=None):\n    if xpath is None:\n        return extractor\n    try:\n        xpath = ET.XPath(xpath)\n    except Exception:\n        log.error(""Failed to create XPath from expression \'%s\'"", xpath)\n        raise\n    f = lambda item: bool(xpath(\n        DatasetItemEncoder.encode(item, extractor.categories())))\n    return extractor.select(f)\n\nclass XPathAnnotationsFilter(Transform):\n    def __init__(self, extractor, xpath=None, remove_empty=False):\n        super().__init__(extractor)\n\n        if xpath is not None:\n            try:\n                xpath = ET.XPath(xpath)\n            except Exception:\n                log.error(""Failed to create XPath from expression \'%s\'"", xpath)\n                raise\n        self._filter = xpath\n\n        self._remove_empty = remove_empty\n\n    def __iter__(self):\n        for item in self._extractor:\n            item = self.transform_item(item)\n            if item is not None:\n                yield item\n\n    def transform_item(self, item):\n        if self._filter is None:\n            return item\n\n        encoded = DatasetItemEncoder.encode(item, self._extractor.categories())\n        filtered = self._filter(encoded)\n        filtered = [elem for elem in filtered if elem.tag == \'annotation\']\n\n        encoded = encoded.findall(\'annotation\')\n        annotations = [item.annotations[encoded.index(e)] for e in filtered]\n\n        if self._remove_empty and len(annotations) == 0:\n            return None\n        return self.wrap_item(item, annotations=annotations)'"
datumaro/datumaro/components/extractor.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import namedtuple\nfrom enum import Enum\nimport numpy as np\n\nfrom datumaro.util.image import Image\n\nAnnotationType = Enum(\'AnnotationType\',\n    [\n        \'label\',\n        \'mask\',\n        \'points\',\n        \'polygon\',\n        \'polyline\',\n        \'bbox\',\n        \'caption\',\n    ])\n\nclass Annotation:\n    # pylint: disable=redefined-builtin\n    def __init__(self, id=None, type=None, attributes=None, group=None):\n        if id is not None:\n            id = int(id)\n        self.id = id\n\n        assert type in AnnotationType\n        self.type = type\n\n        if attributes is None:\n            attributes = {}\n        else:\n            attributes = dict(attributes)\n        self.attributes = attributes\n\n        if group is None:\n            group = 0\n        else:\n            group = int(group)\n        self.group = group\n    # pylint: enable=redefined-builtin\n\n    def __eq__(self, other):\n        if not isinstance(other, Annotation):\n            return False\n        return \\\n            (self.id == other.id) and \\\n            (self.type == other.type) and \\\n            (self.attributes == other.attributes) and \\\n            (self.group == other.group)\n\nclass Categories:\n    def __init__(self, attributes=None):\n        if attributes is None:\n            attributes = set()\n        else:\n            if not isinstance(attributes, set):\n                attributes = set(attributes)\n            for attr in attributes:\n                assert isinstance(attr, str)\n        self.attributes = attributes\n\n    def __eq__(self, other):\n        if not isinstance(other, Categories):\n            return False\n        return \\\n            (self.attributes == other.attributes)\n\nclass LabelCategories(Categories):\n    Category = namedtuple(\'Category\', [\'name\', \'parent\', \'attributes\'])\n\n    def __init__(self, items=None, attributes=None):\n        super().__init__(attributes=attributes)\n\n        if items is None:\n            items = []\n        self.items = items\n\n        self._indices = {}\n        self._reindex()\n\n    def _reindex(self):\n        indices = {}\n        for index, item in enumerate(self.items):\n            assert item.name not in self._indices\n            indices[item.name] = index\n        self._indices = indices\n\n    def add(self, name, parent=None, attributes=None):\n        assert name not in self._indices\n        if attributes is None:\n            attributes = set()\n        else:\n            if not isinstance(attributes, set):\n                attributes = set(attributes)\n            for attr in attributes:\n                assert isinstance(attr, str)\n        if parent is None:\n            parent = \'\'\n\n        index = len(self.items)\n        self.items.append(self.Category(name, parent, attributes))\n        self._indices[name] = index\n        return index\n\n    def find(self, name):\n        index = self._indices.get(name)\n        if index:\n            return index, self.items[index]\n        return index, None\n\n    def __eq__(self, other):\n        if not super().__eq__(other):\n            return False\n        return \\\n            (self.items == other.items)\n\nclass Label(Annotation):\n    # pylint: disable=redefined-builtin\n    def __init__(self, label=None,\n            id=None, attributes=None, group=None):\n        super().__init__(id=id, type=AnnotationType.label,\n            attributes=attributes, group=group)\n\n        if label is not None:\n            label = int(label)\n        self.label = label\n    # pylint: enable=redefined-builtin\n\n    def __eq__(self, other):\n        if not super().__eq__(other):\n            return False\n        return \\\n            (self.label == other.label)\n\nclass MaskCategories(Categories):\n    def __init__(self, colormap=None, inverse_colormap=None, attributes=None):\n        super().__init__(attributes=attributes)\n\n        # colormap: label id -> color\n        if colormap is None:\n            colormap = {}\n        self.colormap = colormap\n        self._inverse_colormap = inverse_colormap\n\n    @property\n    def inverse_colormap(self):\n        from datumaro.util.mask_tools import invert_colormap\n        if self._inverse_colormap is None:\n            if self.colormap is not None:\n                try:\n                    self._inverse_colormap = invert_colormap(self.colormap)\n                except Exception:\n                    pass\n        return self._inverse_colormap\n\n    def __eq__(self, other):\n        if not super().__eq__(other):\n            return False\n        for label_id, my_color in self.colormap.items():\n            other_color = other.colormap.get(label_id)\n            if not np.array_equal(my_color, other_color):\n                return False\n        return True\n\nclass Mask(Annotation):\n    # pylint: disable=redefined-builtin\n    def __init__(self, image=None, label=None, z_order=None,\n            id=None, attributes=None, group=None):\n        super().__init__(type=AnnotationType.mask,\n            id=id, attributes=attributes, group=group)\n\n        self._image = image\n\n        if label is not None:\n            label = int(label)\n        self._label = label\n\n        if z_order is None:\n            z_order = 0\n        else:\n            z_order = int(z_order)\n        self._z_order = z_order\n    # pylint: enable=redefined-builtin\n\n    @property\n    def image(self):\n        if callable(self._image):\n            return self._image()\n        return self._image\n\n    @property\n    def label(self):\n        return self._label\n\n    @property\n    def z_order(self):\n        return self._z_order\n\n    def as_class_mask(self, label_id=None):\n        if label_id is None:\n            label_id = self.label\n        return self.image * label_id\n\n    def as_instance_mask(self, instance_id):\n        return self.image * instance_id\n\n    def get_area(self):\n        return np.count_nonzero(self.image)\n\n    def get_bbox(self):\n        from datumaro.util.mask_tools import find_mask_bbox\n        return find_mask_bbox(self.image)\n\n    def paint(self, colormap):\n        from datumaro.util.mask_tools import paint_mask\n        return paint_mask(self.as_class_mask(), colormap)\n\n    def __eq__(self, other):\n        if not super().__eq__(other):\n            return False\n        return \\\n            (self.label == other.label) and \\\n            (self.z_order == other.z_order) and \\\n            (self.image is not None and other.image is not None and \\\n                np.array_equal(self.image, other.image))\n\nclass RleMask(Mask):\n    # pylint: disable=redefined-builtin\n    def __init__(self, rle=None, label=None, z_order=None,\n            id=None, attributes=None, group=None):\n        lazy_decode = self._lazy_decode(rle)\n        super().__init__(image=lazy_decode, label=label, z_order=z_order,\n            id=id, attributes=attributes, group=group)\n\n        self._rle = rle\n    # pylint: enable=redefined-builtin\n\n    @staticmethod\n    def _lazy_decode(rle):\n        from pycocotools import mask as mask_utils\n        return lambda: mask_utils.decode(rle).astype(np.bool)\n\n    def get_area(self):\n        from pycocotools import mask as mask_utils\n        return mask_utils.area(self._rle)\n\n    def get_bbox(self):\n        from pycocotools import mask as mask_utils\n        return mask_utils.toBbox(self._rle)\n\n    @property\n    def rle(self):\n        return self._rle\n\n    def __eq__(self, other):\n        if not isinstance(other, __class__):\n            return super().__eq__(other)\n        return self._rle == other._rle\n\nclass CompiledMask:\n    @staticmethod\n    def from_instance_masks(instance_masks,\n            instance_ids=None, instance_labels=None):\n        from datumaro.util.mask_tools import merge_masks\n\n        if instance_ids is not None:\n            assert len(instance_ids) == len(instance_masks)\n        else:\n            instance_ids = [None] * len(instance_masks)\n\n        if instance_labels is not None:\n            assert len(instance_labels) == len(instance_masks)\n        else:\n            instance_labels = [None] * len(instance_masks)\n\n        instance_masks = sorted(\n            zip(instance_masks, instance_ids, instance_labels),\n            key=lambda m: m[0].z_order)\n\n        instance_mask = [m.as_instance_mask(id if id is not None else 1 + idx)\n            for idx, (m, id, _) in enumerate(instance_masks)]\n        instance_mask = merge_masks(instance_mask)\n\n        cls_mask = [m.as_class_mask(c) for m, _, c in instance_masks]\n        cls_mask = merge_masks(cls_mask)\n        return __class__(class_mask=cls_mask, instance_mask=instance_mask)\n\n    def __init__(self, class_mask=None, instance_mask=None):\n        self._class_mask = class_mask\n        self._instance_mask = instance_mask\n\n    @staticmethod\n    def _get_image(image):\n        if callable(image):\n            return image()\n        return image\n\n    @property\n    def class_mask(self):\n        return self._get_image(self._class_mask)\n\n    @property\n    def instance_mask(self):\n        return self._get_image(self._instance_mask)\n\n    @property\n    def instance_count(self):\n        return int(self.instance_mask.max())\n\n    def get_instance_labels(self):\n        class_shift = 16\n        m = (self.class_mask.astype(np.uint32) << class_shift) \\\n            + self.instance_mask.astype(np.uint32)\n        keys = np.unique(m)\n        instance_labels = {k & ((1 << class_shift) - 1): k >> class_shift\n            for k in keys if k & ((1 << class_shift) - 1) != 0\n        }\n        return instance_labels\n\n    def extract(self, instance_id):\n        return self.instance_mask == instance_id\n\n    def lazy_extract(self, instance_id):\n        return lambda: self.extract(instance_id)\n\ndef compute_iou(bbox_a, bbox_b):\n    aX, aY, aW, aH = bbox_a\n    bX, bY, bW, bH = bbox_b\n    in_right = min(aX + aW, bX + bW)\n    in_left = max(aX, bX)\n    in_top = max(aY, bY)\n    in_bottom = min(aY + aH, bY + bH)\n\n    in_w = max(0, in_right - in_left)\n    in_h = max(0, in_bottom - in_top)\n    intersection = in_w * in_h\n\n    a_area = aW * aH\n    b_area = bW * bH\n    union = a_area + b_area - intersection\n\n    return intersection / max(1.0, union)\n\nclass _Shape(Annotation):\n    # pylint: disable=redefined-builtin\n    def __init__(self, type, points=None, label=None, z_order=None,\n            id=None, attributes=None, group=None):\n        super().__init__(id=id, type=type,\n            attributes=attributes, group=group)\n        self._points = points\n\n        if label is not None:\n            label = int(label)\n        self._label = label\n\n        if z_order is None:\n            z_order = 0\n        else:\n            z_order = int(z_order)\n        self._z_order = z_order\n    # pylint: enable=redefined-builtin\n\n    @property\n    def points(self):\n        return self._points\n\n    @property\n    def label(self):\n        return self._label\n\n    @property\n    def z_order(self):\n        return self._z_order\n\n    def get_area(self):\n        raise NotImplementedError()\n\n    def get_bbox(self):\n        points = self.points\n        if not points:\n            return None\n\n        xs = [p for p in points[0::2]]\n        ys = [p for p in points[1::2]]\n        x0 = min(xs)\n        x1 = max(xs)\n        y0 = min(ys)\n        y1 = max(ys)\n        return [x0, y0, x1 - x0, y1 - y0]\n\n    def __eq__(self, other):\n        if not super().__eq__(other):\n            return False\n        return \\\n            (np.array_equal(self.points, other.points)) and \\\n            (self.z_order == other.z_order) and \\\n            (self.label == other.label)\n\nclass PolyLine(_Shape):\n    # pylint: disable=redefined-builtin\n    def __init__(self, points=None, label=None, z_order=None,\n            id=None, attributes=None, group=None):\n        super().__init__(type=AnnotationType.polyline,\n            points=points, label=label, z_order=z_order,\n            id=id, attributes=attributes, group=group)\n    # pylint: enable=redefined-builtin\n\n    def as_polygon(self):\n        return self.points[:]\n\n    def get_area(self):\n        return 0\n\nclass Polygon(_Shape):\n    # pylint: disable=redefined-builtin\n    def __init__(self, points=None, label=None,\n            z_order=None, id=None, attributes=None, group=None):\n        if points is not None:\n            # keep the message on the single line to produce\n            # informative output\n            assert len(points) % 2 == 0 and 3 <= len(points) // 2, ""Wrong polygon points: %s"" % points\n        super().__init__(type=AnnotationType.polygon,\n            points=points, label=label, z_order=z_order,\n            id=id, attributes=attributes, group=group)\n    # pylint: enable=redefined-builtin\n\n    def get_area(self):\n        import pycocotools.mask as mask_utils\n\n        _, _, w, h = self.get_bbox()\n        rle = mask_utils.frPyObjects([self.points], h, w)\n        area = mask_utils.area(rle)[0]\n        return area\n\nclass Bbox(_Shape):\n    # pylint: disable=redefined-builtin\n    def __init__(self, x=0, y=0, w=0, h=0, label=None, z_order=None,\n            id=None, attributes=None, group=None):\n        super().__init__(type=AnnotationType.bbox,\n            points=[x, y, x + w, y + h], label=label, z_order=z_order,\n            id=id, attributes=attributes, group=group)\n    # pylint: enable=redefined-builtin\n\n    @property\n    def x(self):\n        return self.points[0]\n\n    @property\n    def y(self):\n        return self.points[1]\n\n    @property\n    def w(self):\n        return self.points[2] - self.points[0]\n\n    @property\n    def h(self):\n        return self.points[3] - self.points[1]\n\n    def get_area(self):\n        return self.w * self.h\n\n    def get_bbox(self):\n        return [self.x, self.y, self.w, self.h]\n\n    def as_polygon(self):\n        x, y, w, h = self.get_bbox()\n        return [\n            x, y,\n            x + w, y,\n            x + w, y + h,\n            x, y + h\n        ]\n\n    def iou(self, other):\n        return compute_iou(self.get_bbox(), other.get_bbox())\n\nclass PointsCategories(Categories):\n    Category = namedtuple(\'Category\', [\'labels\', \'joints\'])\n\n    def __init__(self, items=None, attributes=None):\n        super().__init__(attributes=attributes)\n\n        if items is None:\n            items = {}\n        self.items = items\n\n    def add(self, label_id, labels=None, joints=None):\n        if labels is None:\n            labels = []\n        if joints is None:\n            joints = []\n        joints = set(map(tuple, joints))\n        self.items[label_id] = self.Category(labels, joints)\n\n    def __eq__(self, other):\n        if not super().__eq__(other):\n            return False\n        return \\\n            (self.items == other.items)\n\nclass Points(_Shape):\n    Visibility = Enum(\'Visibility\', [\n        (\'absent\', 0),\n        (\'hidden\', 1),\n        (\'visible\', 2),\n    ])\n\n    # pylint: disable=redefined-builtin\n    def __init__(self, points=None, visibility=None, label=None, z_order=None,\n            id=None, attributes=None, group=None):\n        if points is not None:\n            assert len(points) % 2 == 0\n\n            if visibility is not None:\n                assert len(visibility) == len(points) // 2\n                for i, v in enumerate(visibility):\n                    if not isinstance(v, self.Visibility):\n                        visibility[i] = self.Visibility(v)\n            else:\n                visibility = []\n                for _ in range(len(points) // 2):\n                    visibility.append(self.Visibility.visible)\n\n        super().__init__(type=AnnotationType.points,\n            points=points, label=label, z_order=z_order,\n            id=id, attributes=attributes, group=group)\n\n        self.visibility = visibility\n    # pylint: enable=redefined-builtin\n\n    def get_area(self):\n        return 0\n\n    def get_bbox(self):\n        xs = [p for p, v in zip(self.points[0::2], self.visibility)\n            if v != __class__.Visibility.absent]\n        ys = [p for p, v in zip(self.points[1::2], self.visibility)\n            if v != __class__.Visibility.absent]\n        x0 = min(xs, default=0)\n        x1 = max(xs, default=0)\n        y0 = min(ys, default=0)\n        y1 = max(ys, default=0)\n        return [x0, y0, x1 - x0, y1 - y0]\n\n    def __eq__(self, other):\n        if not super().__eq__(other):\n            return False\n        return \\\n            (self.visibility == other.visibility)\n\nclass Caption(Annotation):\n    # pylint: disable=redefined-builtin\n    def __init__(self, caption=None,\n            id=None, attributes=None, group=None):\n        super().__init__(id=id, type=AnnotationType.caption,\n            attributes=attributes, group=group)\n\n        if caption is None:\n            caption = \'\'\n        else:\n            caption = str(caption)\n        self.caption = caption\n    # pylint: enable=redefined-builtin\n\n    def __eq__(self, other):\n        if not super().__eq__(other):\n            return False\n        return \\\n            (self.caption == other.caption)\n\nclass DatasetItem:\n    # pylint: disable=redefined-builtin\n    def __init__(self, id=None, annotations=None,\n            subset=None, path=None, image=None, attributes=None):\n        assert id is not None\n        self._id = str(id)\n\n        if subset is None:\n            subset = \'\'\n        else:\n            subset = str(subset)\n        self._subset = subset\n\n        if path is None:\n            path = []\n        else:\n            path = list(path)\n        self._path = path\n\n        if annotations is None:\n            annotations = []\n        else:\n            annotations = list(annotations)\n        self._annotations = annotations\n\n        if callable(image) or isinstance(image, np.ndarray):\n            image = Image(data=image)\n        elif isinstance(image, str):\n            image = Image(path=image)\n        assert image is None or isinstance(image, Image)\n        self._image = image\n\n        if attributes is None:\n            attributes = {}\n        else:\n            attributes = dict(attributes)\n        self._attributes = attributes\n    # pylint: enable=redefined-builtin\n\n    @property\n    def id(self):\n        return self._id\n\n    @property\n    def subset(self):\n        return self._subset\n\n    @property\n    def path(self):\n        return self._path\n\n    @property\n    def annotations(self):\n        return self._annotations\n\n    @property\n    def image(self):\n        return self._image\n\n    @property\n    def has_image(self):\n        return self._image is not None\n\n    @property\n    def attributes(self):\n        return self._attributes\n\n    def __eq__(self, other):\n        if not isinstance(other, __class__):\n            return False\n        return \\\n            (self.id == other.id) and \\\n            (self.subset == other.subset) and \\\n            (self.path == other.path) and \\\n            (self.annotations == other.annotations) and \\\n            (self.image == other.image) and \\\n            (self.attributes == other.attributes)\n\n    def wrap(item, **kwargs):\n        expected_args = {\'id\', \'annotations\', \'subset\',\n            \'path\', \'image\', \'attributes\'}\n        for k in expected_args:\n            if k not in kwargs:\n                kwargs[k] = getattr(item, k)\n        return DatasetItem(**kwargs)\n\nclass IExtractor:\n    def __iter__(self):\n        raise NotImplementedError()\n\n    def __len__(self):\n        raise NotImplementedError()\n\n    def subsets(self):\n        raise NotImplementedError()\n\n    def get_subset(self, name):\n        raise NotImplementedError()\n\n    def categories(self):\n        raise NotImplementedError()\n\n    def select(self, pred):\n        raise NotImplementedError()\n\nclass _DatasetFilter:\n    def __init__(self, iterable, predicate):\n        self.iterable = iterable\n        self.predicate = predicate\n\n    def __iter__(self):\n        return filter(self.predicate, self.iterable)\n\nclass _ExtractorBase(IExtractor):\n    def __init__(self, length=None, subsets=None):\n        self._length = length\n        self._subsets = subsets\n\n    def _init_cache(self):\n        subsets = set()\n        length = -1\n        for length, item in enumerate(self):\n            subsets.add(item.subset)\n        length += 1\n\n        if self._length is None:\n            self._length = length\n        if self._subsets is None:\n            self._subsets = subsets\n\n    def __len__(self):\n        if self._length is None:\n            self._init_cache()\n        return self._length\n\n    def subsets(self):\n        if self._subsets is None:\n            self._init_cache()\n        return list(self._subsets)\n\n    def get_subset(self, name):\n        if name in self.subsets():\n            return self.select(lambda item: item.subset == name)\n        else:\n            raise Exception(""Unknown subset \'%s\' requested"" % name)\n\n    def transform(self, method, *args, **kwargs):\n        return method(self, *args, **kwargs)\n\nclass DatasetIteratorWrapper(_ExtractorBase):\n    def __init__(self, iterable, categories, subsets=None):\n        super().__init__(length=None, subsets=subsets)\n        self._iterable = iterable\n        self._categories = categories\n\n    def __iter__(self):\n        return iter(self._iterable)\n\n    def categories(self):\n        return self._categories\n\n    def select(self, pred):\n        return DatasetIteratorWrapper(\n            _DatasetFilter(self, pred), self.categories(), self.subsets())\n\nclass Extractor(_ExtractorBase):\n    def __init__(self, length=None):\n        super().__init__(length=None)\n\n    def categories(self):\n        return {}\n\n    def select(self, pred):\n        return DatasetIteratorWrapper(\n            _DatasetFilter(self, pred), self.categories(), self.subsets())\n\nDEFAULT_SUBSET_NAME = \'default\'\n\n\nclass SourceExtractor(Extractor):\n    def __init__(self, length=None, subset=None):\n        super().__init__(length=length)\n\n        if subset == DEFAULT_SUBSET_NAME:\n            subset = None\n        self._subset = subset\n\n    def subsets(self):\n        return [self._subset]\n\n    def get_subset(self, name):\n        if name != self._subset:\n            return None\n        return self\n\nclass Importer:\n    @classmethod\n    def detect(cls, path):\n        raise NotImplementedError()\n\n    def __call__(self, path, **extra_params):\n        raise NotImplementedError()\n\nclass Transform(Extractor):\n    @staticmethod\n    def wrap_item(item, **kwargs):\n        return item.wrap(**kwargs)\n\n    def __init__(self, extractor):\n        super().__init__()\n\n        self._extractor = extractor\n\n    def __iter__(self):\n        for item in self._extractor:\n            yield self.transform_item(item)\n\n    def categories(self):\n        return self._extractor.categories()\n\n    def transform_item(self, item):\n        raise NotImplementedError()\n'"
datumaro/datumaro/components/launcher.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport numpy as np\n\nfrom datumaro.components.extractor import Transform\nfrom datumaro.util import take_by\n\n\n# pylint: disable=no-self-use\nclass Launcher:\n    def __init__(self, model_dir=None):\n        pass\n\n    def launch(self, inputs):\n        raise NotImplementedError()\n\n    def preferred_input_size(self):\n        return None\n\n    def categories(self):\n        return None\n# pylint: enable=no-self-use\n\nclass ModelTransform(Transform):\n    def __init__(self, extractor, launcher, batch_size=1):\n        super().__init__(extractor)\n        self._launcher = launcher\n        self._batch_size = batch_size\n\n    def __iter__(self):\n        for batch in take_by(self._extractor, self._batch_size):\n            inputs = np.array([item.image.data for item in batch])\n            inference = self._launcher.launch(inputs)\n\n            for item, annotations in zip(batch, inference):\n                yield self.wrap_item(item, annotations=annotations)\n\n    def get_subset(self, name):\n        subset = self._extractor.get_subset(name)\n        return __class__(subset, self._launcher, self._batch_size)\n\n    def categories(self):\n        launcher_override = self._launcher.categories()\n        if launcher_override is not None:\n            return launcher_override\n        return self._extractor.categories()\n\n    def transform_item(self, item):\n        inputs = np.expand_dims(item.image, axis=0)\n        annotations = self._launcher.launch(inputs)[0]\n        return self.wrap_item(item, annotations=annotations)'"
datumaro/datumaro/components/project.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict, defaultdict\nfrom functools import reduce\nimport git\nfrom glob import glob\nimport importlib\nimport inspect\nimport logging as log\nimport os\nimport os.path as osp\nimport shutil\nimport sys\n\nfrom datumaro.components.config import Config, DEFAULT_FORMAT\nfrom datumaro.components.config_model import (Model, Source,\n    PROJECT_DEFAULT_CONFIG, PROJECT_SCHEMA)\nfrom datumaro.components.extractor import Extractor\nfrom datumaro.components.launcher import ModelTransform\nfrom datumaro.components.dataset_filter import \\\n    XPathDatasetFilter, XPathAnnotationsFilter\n\n\ndef import_foreign_module(name, path, package=None):\n    module = None\n    default_path = sys.path.copy()\n    try:\n        sys.path = [ osp.abspath(path), ] + default_path\n        sys.modules.pop(name, None) # remove from cache\n        module = importlib.import_module(name, package=package)\n        sys.modules.pop(name) # remove from cache\n    except Exception:\n        raise\n    finally:\n        sys.path = default_path\n    return module\n\n\nclass Registry:\n    def __init__(self, config=None, item_type=None):\n        self.item_type = item_type\n\n        self.items = {}\n\n        if config is not None:\n            self.load(config)\n\n    def load(self, config):\n        pass\n\n    def register(self, name, value):\n        if self.item_type:\n            value = self.item_type(value)\n        self.items[name] = value\n        return value\n\n    def unregister(self, name):\n        return self.items.pop(name, None)\n\n    def get(self, key):\n        return self.items[key] # returns a class / ctor\n\n\nclass ModelRegistry(Registry):\n    def __init__(self, config=None):\n        super().__init__(config, item_type=Model)\n\n    def load(self, config):\n        # TODO: list default dir, insert values\n        if \'models\' in config:\n            for name, model in config.models.items():\n                self.register(name, model)\n\n\nclass SourceRegistry(Registry):\n    def __init__(self, config=None):\n        super().__init__(config, item_type=Source)\n\n    def load(self, config):\n        # TODO: list default dir, insert values\n        if \'sources\' in config:\n            for name, source in config.sources.items():\n                self.register(name, source)\n\nclass PluginRegistry(Registry):\n    def __init__(self, config=None, builtin=None, local=None):\n        super().__init__(config)\n\n        from datumaro.components.cli_plugin import CliPlugin\n\n        if builtin is not None:\n            for v in builtin:\n                k = CliPlugin._get_name(v)\n                self.register(k, v)\n        if local is not None:\n            for v in local:\n                k = CliPlugin._get_name(v)\n                self.register(k, v)\n\nclass GitWrapper:\n    def __init__(self, config=None):\n        self.repo = None\n\n        if config is not None and osp.isdir(config.project_dir):\n            self.init(config.project_dir)\n\n    @staticmethod\n    def _git_dir(base_path):\n        return osp.join(base_path, \'.git\')\n\n    @classmethod\n    def spawn(cls, path):\n        spawn = not osp.isdir(cls._git_dir(path))\n        repo = git.Repo.init(path=path)\n        if spawn:\n            author = git.Actor(""Nobody"", ""nobody@example.com"")\n            repo.index.commit(\'Initial commit\', author=author)\n        return repo\n\n    def init(self, path):\n        self.repo = self.spawn(path)\n        return self.repo\n\n    def is_initialized(self):\n        return self.repo is not None\n\n    def create_submodule(self, name, dst_dir, **kwargs):\n        self.repo.create_submodule(name, dst_dir, **kwargs)\n\n    def has_submodule(self, name):\n        return name in [submodule.name for submodule in self.repo.submodules]\n\n    def remove_submodule(self, name, **kwargs):\n        return self.repo.submodule(name).remove(**kwargs)\n\ndef load_project_as_dataset(url):\n    # symbol forward declaration\n    raise NotImplementedError()\n\nclass Environment:\n    _builtin_plugins = None\n    PROJECT_EXTRACTOR_NAME = \'datumaro_project\'\n\n    def __init__(self, config=None):\n        config = Config(config,\n            fallback=PROJECT_DEFAULT_CONFIG, schema=PROJECT_SCHEMA)\n\n        self.models = ModelRegistry(config)\n        self.sources = SourceRegistry(config)\n\n        self.git = GitWrapper(config)\n\n        env_dir = osp.join(config.project_dir, config.env_dir)\n        builtin = self._load_builtin_plugins()\n        custom = self._load_plugins2(osp.join(env_dir, config.plugins_dir))\n        select = lambda seq, t: [e for e in seq if issubclass(e, t)]\n        from datumaro.components.extractor import Transform\n        from datumaro.components.extractor import SourceExtractor\n        from datumaro.components.extractor import Importer\n        from datumaro.components.converter import Converter\n        from datumaro.components.launcher import Launcher\n        self.extractors = PluginRegistry(\n            builtin=select(builtin, SourceExtractor),\n            local=select(custom, SourceExtractor)\n        )\n        self.extractors.register(self.PROJECT_EXTRACTOR_NAME,\n            load_project_as_dataset)\n\n        self.importers = PluginRegistry(\n            builtin=select(builtin, Importer),\n            local=select(custom, Importer)\n        )\n        self.launchers = PluginRegistry(\n            builtin=select(builtin, Launcher),\n            local=select(custom, Launcher)\n        )\n        self.converters = PluginRegistry(\n            builtin=select(builtin, Converter),\n            local=select(custom, Converter)\n        )\n        self.transforms = PluginRegistry(\n            builtin=select(builtin, Transform),\n            local=select(custom, Transform)\n        )\n\n    @staticmethod\n    def _find_plugins(plugins_dir):\n        plugins = []\n        if not osp.exists(plugins_dir):\n            return plugins\n\n        for plugin_name in os.listdir(plugins_dir):\n            p = osp.join(plugins_dir, plugin_name)\n            if osp.isfile(p) and p.endswith(\'.py\'):\n                plugins.append((plugins_dir, plugin_name, None))\n            elif osp.isdir(p):\n                plugins += [(plugins_dir,\n                        osp.splitext(plugin_name)[0] + \'.\' + osp.basename(p),\n                        osp.splitext(plugin_name)[0]\n                    )\n                    for p in glob(osp.join(p, \'*.py\'))]\n        return plugins\n\n    @classmethod\n    def _import_module(cls, module_dir, module_name, types, package=None):\n        module = import_foreign_module(osp.splitext(module_name)[0], module_dir,\n            package=package)\n\n        exports = []\n        if hasattr(module, \'exports\'):\n            exports = module.exports\n        else:\n            for symbol in dir(module):\n                if symbol.startswith(\'_\'):\n                    continue\n                exports.append(getattr(module, symbol))\n\n        exports = [s for s in exports\n            if inspect.isclass(s) and issubclass(s, types) and not s in types]\n\n        return exports\n\n    @classmethod\n    def _load_plugins(cls, plugins_dir, types):\n        types = tuple(types)\n\n        plugins = cls._find_plugins(plugins_dir)\n\n        all_exports = []\n        for module_dir, module_name, package in plugins:\n            try:\n                exports = cls._import_module(module_dir, module_name, types,\n                    package)\n            except Exception as e:\n                module_search_error = ImportError\n                try:\n                    module_search_error = ModuleNotFoundError # python 3.6+\n                except NameError:\n                    pass\n\n                message = [""Failed to import module \'%s\': %s"", module_name, e]\n                if isinstance(e, module_search_error):\n                    log.debug(*message)\n                else:\n                    log.warning(*message)\n                continue\n\n            log.debug(""Imported the following symbols from %s: %s"" % \\\n                (\n                    module_name,\n                    \', \'.join(s.__name__ for s in exports)\n                )\n            )\n            all_exports.extend(exports)\n\n        return all_exports\n\n    @classmethod\n    def _load_builtin_plugins(cls):\n        if not cls._builtin_plugins:\n            plugins_dir = osp.join(\n                __file__[: __file__.rfind(osp.join(\'datumaro\', \'components\'))],\n                osp.join(\'datumaro\', \'plugins\')\n            )\n            assert osp.isdir(plugins_dir), plugins_dir\n            cls._builtin_plugins = cls._load_plugins2(plugins_dir)\n        return cls._builtin_plugins\n\n    @classmethod\n    def _load_plugins2(cls, plugins_dir):\n        from datumaro.components.extractor import Transform\n        from datumaro.components.extractor import SourceExtractor\n        from datumaro.components.extractor import Importer\n        from datumaro.components.converter import Converter\n        from datumaro.components.launcher import Launcher\n        types = [SourceExtractor, Converter, Importer, Launcher, Transform]\n\n        return cls._load_plugins(plugins_dir, types)\n\n    def make_extractor(self, name, *args, **kwargs):\n        return self.extractors.get(name)(*args, **kwargs)\n\n    def make_importer(self, name, *args, **kwargs):\n        return self.importers.get(name)(*args, **kwargs)\n\n    def make_launcher(self, name, *args, **kwargs):\n        return self.launchers.get(name)(*args, **kwargs)\n\n    def make_converter(self, name, *args, **kwargs):\n        return self.converters.get(name)(*args, **kwargs)\n\n    def register_model(self, name, model):\n        self.models.register(name, model)\n\n    def unregister_model(self, name):\n        self.models.unregister(name)\n\n\nclass Subset(Extractor):\n    def __init__(self, parent):\n        self._parent = parent\n        self.items = OrderedDict()\n\n    def __iter__(self):\n        for item in self.items.values():\n            yield item\n\n    def __len__(self):\n        return len(self.items)\n\n    def categories(self):\n        return self._parent.categories()\n\nclass Dataset(Extractor):\n    @classmethod\n    def from_extractors(cls, *sources):\n        # merge categories\n        # TODO: implement properly with merging and annotations remapping\n        categories = {}\n        for source in sources:\n            categories.update(source.categories())\n        for source in sources:\n            for cat_type, source_cat in source.categories().items():\n                if not categories[cat_type] == source_cat:\n                    raise NotImplementedError(\n                        ""Merging different categories is not implemented yet"")\n        dataset = Dataset(categories=categories)\n\n        # merge items\n        subsets = defaultdict(lambda: Subset(dataset))\n        for source in sources:\n            for item in source:\n                existing_item = subsets[item.subset].items.get(item.id)\n                if existing_item is not None:\n                    path = existing_item.path\n                    if item.path != path:\n                        path = None\n                    item = cls._merge_items(existing_item, item, path=path)\n\n                subsets[item.subset].items[item.id] = item\n\n        dataset._subsets = dict(subsets)\n        return dataset\n\n    def __init__(self, categories=None):\n        super().__init__()\n\n        self._subsets = {}\n\n        if not categories:\n            categories = {}\n        self._categories = categories\n\n    def __iter__(self):\n        for subset in self._subsets.values():\n            for item in subset:\n                yield item\n\n    def __len__(self):\n        if self._length is None:\n            self._length = reduce(lambda s, x: s + len(x),\n                self._subsets.values(), 0)\n        return self._length\n\n    def get_subset(self, name):\n        return self._subsets[name]\n\n    def subsets(self):\n        return list(self._subsets)\n\n    def categories(self):\n        return self._categories\n\n    def get(self, item_id, subset=None, path=None):\n        if path:\n            raise KeyError(""Requested dataset item path is not found"")\n        if subset is None:\n            subset = \'\'\n        return self._subsets[subset].items[item_id]\n\n    def put(self, item, item_id=None, subset=None, path=None):\n        if path:\n            raise KeyError(""Requested dataset item path is not found"")\n\n        if item_id is None:\n            item_id = item.id\n        if subset is None:\n            subset = item.subset\n\n        item = item.wrap(path=None, annotations=item.annotations)\n        if item.subset not in self._subsets:\n            self._subsets[item.subset] = Subset(self)\n        self._subsets[subset].items[item_id] = item\n        self._length = None\n\n        return item\n\n    def extract(self, filter_expr, filter_annotations=False, remove_empty=False):\n        if filter_annotations:\n            return self.transform(XPathAnnotationsFilter, filter_expr,\n                remove_empty)\n        else:\n            return self.transform(XPathDatasetFilter, filter_expr)\n\n    def update(self, items):\n        for item in items:\n            self.put(item)\n        return self\n\n    def define_categories(self, categories):\n        assert not self._categories\n        self._categories = categories\n\n    @staticmethod\n    def _lazy_image(item):\n        # NOTE: avoid https://docs.python.org/3/faq/programming.html#why-do-lambdas-defined-in-a-loop-with-different-values-all-return-the-same-result\n        return lambda: item.image\n\n    @classmethod\n    def _merge_items(cls, existing_item, current_item, path=None):\n        return existing_item.wrap(path=path,\n        image=cls._merge_images(existing_item, current_item),\n            annotations=cls._merge_anno(\n                existing_item.annotations, current_item.annotations))\n\n    @staticmethod\n    def _merge_images(existing_item, current_item):\n        image = None\n        if existing_item.has_image and current_item.has_image:\n            if existing_item.image.has_data:\n                image = existing_item.image\n            else:\n                image = current_item.image\n\n            if existing_item.image.path != current_item.image.path:\n                if not existing_item.image.path:\n                    image._path = current_item.image.path\n\n            if all([existing_item.image._size, current_item.image._size]):\n                assert existing_item.image._size == current_item.image._size, ""Image info differs for item \'%s\'"" % existing_item.id\n            elif existing_item.image._size:\n                image._size = existing_item.image._size\n            else:\n                image._size = current_item.image._size\n        elif existing_item.has_image:\n            image = existing_item.image\n        else:\n            image = current_item.image\n\n        return image\n\n    @staticmethod\n    def _merge_anno(a, b):\n        from itertools import chain\n        merged = []\n        for item in chain(a, b):\n            found = False\n            for elem in merged:\n                if elem == item:\n                    found = True\n                    break\n            if not found:\n                merged.append(item)\n\n        return merged\n\nclass ProjectDataset(Dataset):\n    def __init__(self, project):\n        super().__init__()\n\n        self._project = project\n        config = self.config\n        env = self.env\n\n        sources = {}\n        for s_name, source in config.sources.items():\n            s_format = source.format\n            if not s_format:\n                s_format = env.PROJECT_EXTRACTOR_NAME\n            options = {}\n            options.update(source.options)\n\n            url = source.url\n            if not source.url:\n                url = osp.join(config.project_dir, config.sources_dir, s_name)\n            sources[s_name] = env.make_extractor(s_format,\n                url, **options)\n        self._sources = sources\n\n        own_source = None\n        own_source_dir = osp.join(config.project_dir, config.dataset_dir)\n        if config.project_dir and osp.isdir(own_source_dir):\n            log.disable(log.INFO)\n            own_source = env.make_importer(DEFAULT_FORMAT)(own_source_dir) \\\n                .make_dataset()\n            log.disable(log.NOTSET)\n\n        # merge categories\n        # TODO: implement properly with merging and annotations remapping\n        categories = {}\n        for source in self._sources.values():\n            categories.update(source.categories())\n        for source in self._sources.values():\n            for cat_type, source_cat in source.categories().items():\n                if not categories[cat_type] == source_cat:\n                    raise NotImplementedError(\n                        ""Merging different categories is not implemented yet"")\n        if own_source is not None and (not categories or len(own_source) != 0):\n            categories.update(own_source.categories())\n        self._categories = categories\n\n        # merge items\n        subsets = defaultdict(lambda: Subset(self))\n        for source_name, source in self._sources.items():\n            log.debug(""Loading \'%s\' source contents..."" % source_name)\n            for item in source:\n                existing_item = subsets[item.subset].items.get(item.id)\n                if existing_item is not None:\n                    path = existing_item.path\n                    if item.path != path:\n                        path = None # NOTE: move to our own dataset\n                    item = self._merge_items(existing_item, item, path=path)\n                else:\n                    s_config = config.sources[source_name]\n                    if s_config and \\\n                            s_config.format != env.PROJECT_EXTRACTOR_NAME:\n                        # NOTE: consider imported sources as our own dataset\n                        path = None\n                    else:\n                        path = item.path\n                        if path is None:\n                            path = []\n                        path = [source_name] + path\n                    item = item.wrap(path=path, annotations=item.annotations)\n\n                subsets[item.subset].items[item.id] = item\n\n        # override with our items, fallback to existing images\n        if own_source is not None:\n            log.debug(""Loading own dataset..."")\n            for item in own_source:\n                existing_item = subsets[item.subset].items.get(item.id)\n                if existing_item is not None:\n                    item = item.wrap(path=None,\n                        image=self._merge_images(existing_item, item),\n                        annotations=item.annotations)\n\n                subsets[item.subset].items[item.id] = item\n\n        # TODO: implement subset remapping when needed\n        subsets_filter = config.subsets\n        if len(subsets_filter) != 0:\n            subsets = { k: v for k, v in subsets.items() if k in subsets_filter}\n        self._subsets = dict(subsets)\n\n        self._length = None\n\n    def iterate_own(self):\n        return self.select(lambda item: not item.path)\n\n    def get(self, item_id, subset=None, path=None):\n        if path:\n            source = path[0]\n            rest_path = path[1:]\n            return self._sources[source].get(\n                item_id=item_id, subset=subset, path=rest_path)\n        return self._subsets[subset].items[item_id]\n\n    def put(self, item, item_id=None, subset=None, path=None):\n        if path is None:\n            path = item.path\n        if path:\n            source = path[0]\n            rest_path = path[1:]\n            # TODO: reverse remapping\n            self._sources[source].put(item,\n                item_id=item_id, subset=subset, path=rest_path)\n\n        if item_id is None:\n            item_id = item.id\n        if subset is None:\n            subset = item.subset\n\n        item = item.wrap(path=path, annotations=item.annotations)\n        if item.subset not in self._subsets:\n            self._subsets[item.subset] = Subset(self)\n        self._subsets[subset].items[item_id] = item\n        self._length = None\n\n        return item\n\n    def save(self, save_dir=None, merge=False, recursive=True,\n            save_images=False):\n        if save_dir is None:\n            assert self.config.project_dir\n            save_dir = self.config.project_dir\n            project = self._project\n        else:\n            merge = True\n\n        if merge:\n            project = Project(Config(self.config))\n            project.config.remove(\'sources\')\n\n        save_dir = osp.abspath(save_dir)\n        os.makedirs(save_dir, exist_ok=True)\n\n        dataset_save_dir = osp.join(save_dir, project.config.dataset_dir)\n        os.makedirs(dataset_save_dir, exist_ok=True)\n\n        converter_kwargs = {\n            \'save_images\': save_images,\n        }\n\n        if merge:\n            # merge and save the resulting dataset\n            converter = self.env.make_converter(\n                DEFAULT_FORMAT, **converter_kwargs)\n            converter(self, dataset_save_dir)\n        else:\n            if recursive:\n                # children items should already be updated\n                # so we just save them recursively\n                for source in self._sources.values():\n                    if isinstance(source, ProjectDataset):\n                        source.save(**converter_kwargs)\n\n            converter = self.env.make_converter(\n                DEFAULT_FORMAT, **converter_kwargs)\n            converter(self.iterate_own(), dataset_save_dir)\n\n        project.save(save_dir)\n\n    @property\n    def env(self):\n        return self._project.env\n\n    @property\n    def config(self):\n        return self._project.config\n\n    @property\n    def sources(self):\n        return self._sources\n\n    def _save_branch_project(self, extractor, save_dir=None):\n        extractor = Dataset.from_extractors(extractor) # apply lazy transforms\n\n        # NOTE: probably this function should be in the ViewModel layer\n        save_dir = osp.abspath(save_dir)\n        if save_dir:\n            dst_project = Project()\n        else:\n            if not self.config.project_dir:\n                raise Exception(""Either a save directory or a project ""\n                    ""directory should be specified"")\n            save_dir = self.config.project_dir\n\n            dst_project = Project(Config(self.config))\n            dst_project.config.remove(\'project_dir\')\n            dst_project.config.remove(\'sources\')\n        dst_project.config.project_name = osp.basename(save_dir)\n\n        dst_dataset = dst_project.make_dataset()\n        dst_dataset.define_categories(extractor.categories())\n        dst_dataset.update(extractor)\n\n        dst_dataset.save(save_dir=save_dir, merge=True)\n\n    def transform_project(self, method, save_dir=None, **method_kwargs):\n        # NOTE: probably this function should be in the ViewModel layer\n        if isinstance(method, str):\n            method = self.env.make_transform(method)\n\n        transformed = self.transform(method, **method_kwargs)\n        self._save_branch_project(transformed, save_dir=save_dir)\n\n    def apply_model(self, model, save_dir=None, batch_size=1):\n        # NOTE: probably this function should be in the ViewModel layer\n        if isinstance(model, str):\n            launcher = self._project.make_executable_model(model)\n\n        self.transform_project(ModelTransform, launcher=launcher,\n            save_dir=save_dir, batch_size=batch_size)\n\n    def export_project(self, save_dir, converter,\n            filter_expr=None, filter_annotations=False, remove_empty=False):\n        # NOTE: probably this function should be in the ViewModel layer\n        dataset = self\n        if filter_expr:\n            dataset = dataset.extract(filter_expr,\n                filter_annotations=filter_annotations,\n                remove_empty=remove_empty)\n\n        save_dir = osp.abspath(save_dir)\n        save_dir_existed = osp.exists(save_dir)\n        try:\n            os.makedirs(save_dir, exist_ok=True)\n            converter(dataset, save_dir)\n        except Exception:\n            if not save_dir_existed:\n                shutil.rmtree(save_dir)\n            raise\n\n    def extract_project(self, filter_expr, filter_annotations=False,\n            save_dir=None, remove_empty=False):\n        # NOTE: probably this function should be in the ViewModel layer\n        filtered = self\n        if filter_expr:\n            filtered = self.extract(filter_expr,\n                filter_annotations=filter_annotations,\n                remove_empty=remove_empty)\n        self._save_branch_project(filtered, save_dir=save_dir)\n\nclass Project:\n    @classmethod\n    def load(cls, path):\n        path = osp.abspath(path)\n        config_path = osp.join(path, PROJECT_DEFAULT_CONFIG.env_dir,\n            PROJECT_DEFAULT_CONFIG.project_filename)\n        config = Config.parse(config_path)\n        config.project_dir = path\n        config.project_filename = osp.basename(config_path)\n        return Project(config)\n\n    def save(self, save_dir=None):\n        config = self.config\n\n        if save_dir is None:\n            assert config.project_dir\n            project_dir = config.project_dir\n        else:\n            project_dir = save_dir\n\n        env_dir = osp.join(project_dir, config.env_dir)\n        save_dir = osp.abspath(env_dir)\n\n        project_dir_existed = osp.exists(project_dir)\n        env_dir_existed = osp.exists(env_dir)\n        try:\n            os.makedirs(save_dir, exist_ok=True)\n\n            config_path = osp.join(save_dir, config.project_filename)\n            config.dump(config_path)\n        except Exception:\n            if not env_dir_existed:\n                shutil.rmtree(save_dir, ignore_errors=True)\n            if not project_dir_existed:\n                shutil.rmtree(project_dir, ignore_errors=True)\n            raise\n\n    @staticmethod\n    def generate(save_dir, config=None):\n        project = Project(config)\n        project.save(save_dir)\n        project.config.project_dir = save_dir\n        return project\n\n    @staticmethod\n    def import_from(path, dataset_format, env=None, **kwargs):\n        if env is None:\n            env = Environment()\n        importer = env.make_importer(dataset_format)\n        return importer(path, **kwargs)\n\n    def __init__(self, config=None):\n        self.config = Config(config,\n            fallback=PROJECT_DEFAULT_CONFIG, schema=PROJECT_SCHEMA)\n        self.env = Environment(self.config)\n\n    def make_dataset(self):\n        return ProjectDataset(self)\n\n    def add_source(self, name, value=None):\n        if value is None or isinstance(value, (dict, Config)):\n            value = Source(value)\n        self.config.sources[name] = value\n        self.env.sources.register(name, value)\n\n    def remove_source(self, name):\n        self.config.sources.remove(name)\n        self.env.sources.unregister(name)\n\n    def get_source(self, name):\n        try:\n            return self.config.sources[name]\n        except KeyError:\n            raise KeyError(""Source \'%s\' is not found"" % name)\n\n    def get_subsets(self):\n        return self.config.subsets\n\n    def set_subsets(self, value):\n        if not value:\n            self.config.remove(\'subsets\')\n        else:\n            self.config.subsets = value\n\n    def add_model(self, name, value=None):\n        if value is None or isinstance(value, (dict, Config)):\n            value = Model(value)\n        self.env.register_model(name, value)\n        self.config.models[name] = value\n\n    def get_model(self, name):\n        try:\n            return self.env.models.get(name)\n        except KeyError:\n            raise KeyError(""Model \'%s\' is not found"" % name)\n\n    def remove_model(self, name):\n        self.config.models.remove(name)\n        self.env.unregister_model(name)\n\n    def make_executable_model(self, name):\n        model = self.get_model(name)\n        model.model_dir = self.local_model_dir(name)\n        return self.env.make_launcher(model.launcher,\n            **model.options, model_dir=model.model_dir)\n\n    def make_source_project(self, name):\n        source = self.get_source(name)\n\n        config = Config(self.config)\n        config.remove(\'sources\')\n        config.remove(\'subsets\')\n        project = Project(config)\n        project.add_source(name, source)\n        return project\n\n    def local_model_dir(self, model_name):\n        return osp.join(\n            self.config.env_dir, self.config.models_dir, model_name)\n\n    def local_source_dir(self, source_name):\n        return osp.join(self.config.sources_dir, source_name)\n\n# pylint: disable=function-redefined\ndef load_project_as_dataset(url):\n    # implement the function declared above\n    return Project.load(url).make_dataset()\n# pylint: enable=function-redefined\n'"
datumaro/datumaro/plugins/__init__.py,0,b''
datumaro/datumaro/plugins/image_dir.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nimport os\nimport os.path as osp\n\nfrom datumaro.components.extractor import DatasetItem, SourceExtractor, Importer\nfrom datumaro.components.converter import Converter\nfrom datumaro.util.image import save_image\n\n\nclass ImageDirImporter(Importer):\n    EXTRACTOR_NAME = \'image_dir\'\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        if not osp.isdir(path):\n            raise Exception(""Can\'t find a directory at \'%s\'"" % path)\n\n        source_name = osp.basename(osp.normpath(path))\n        project.add_source(source_name, {\n            \'url\': source_name,\n            \'format\': self.EXTRACTOR_NAME,\n            \'options\': dict(extra_params),\n        })\n\n        return project\n\n\nclass ImageDirExtractor(SourceExtractor):\n    _SUPPORTED_FORMATS = [\'.png\', \'.jpg\']\n\n    def __init__(self, url):\n        super().__init__()\n\n        assert osp.isdir(url), url\n\n        items = []\n        for name in os.listdir(url):\n            path = osp.join(url, name)\n            if self._is_image(path):\n                item_id = osp.splitext(name)[0]\n                item = DatasetItem(id=item_id, image=path)\n                items.append((item.id, item))\n\n        items = sorted(items, key=lambda e: e[0])\n        items = OrderedDict(items)\n        self._items = items\n\n    def __iter__(self):\n        for item in self._items.values():\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n\n    def get(self, item_id, subset=None, path=None):\n        if path or subset:\n            raise KeyError()\n        return self._items[item_id]\n\n    def _is_image(self, path):\n        if not osp.isfile(path):\n            return False\n        for ext in self._SUPPORTED_FORMATS:\n            if path.endswith(ext):\n                return True\n        return False\n\n\nclass ImageDirConverter(Converter):\n    def __call__(self, extractor, save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n\n        for item in extractor:\n            if item.has_image and item.image.has_data:\n                filename = item.image.filename\n                if filename:\n                    filename = osp.splitext(filename)[0]\n                else:\n                    filename = item.id\n                filename += \'.jpg\'\n                save_image(osp.join(save_dir, filename), item.image.data,\n                    create_dir=True)'"
datumaro/datumaro/plugins/labelme_format.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import defaultdict\nfrom defusedxml import ElementTree\nimport logging as log\nimport numpy as np\nimport os\nimport os.path as osp\n\nfrom datumaro.components.extractor import (SourceExtractor, DEFAULT_SUBSET_NAME,\n    DatasetItem, AnnotationType, Mask, Bbox, Polygon, LabelCategories\n)\nfrom datumaro.components.extractor import Importer\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom datumaro.util.image import Image, save_image\nfrom datumaro.util.mask_tools import load_mask, find_mask_bbox\n\n\nclass LabelMePath:\n    MASKS_DIR = \'Masks\'\n    IMAGE_EXT = \'.jpg\'\n\nclass LabelMeExtractor(SourceExtractor):\n    def __init__(self, path, subset_name=None):\n        assert osp.isdir(path), path\n        super().__init__(subset=subset_name)\n\n        items, categories = self._parse(path)\n        self._categories = categories\n        self._items = items\n\n    def categories(self):\n        return self._categories\n\n    def __iter__(self):\n        for item in self._items:\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n\n    def _parse(self, path):\n        categories = {\n            AnnotationType.label: LabelCategories(attributes={\n                \'occluded\', \'username\'\n            })\n        }\n\n        items = []\n        for p in sorted(p for p in os.listdir(path) if p.endswith(\'.xml\')):\n            root = ElementTree.parse(osp.join(path, p))\n\n            image_path = osp.join(path, root.find(\'filename\').text)\n            image_size = None\n            imagesize_elem = root.find(\'imagesize\')\n            if imagesize_elem is not None:\n                width_elem = imagesize_elem.find(\'ncols\')\n                height_elem = imagesize_elem.find(\'nrows\')\n                image_size = (int(height_elem.text), int(width_elem.text))\n            image = Image(path=image_path, size=image_size)\n\n            annotations = self._parse_annotations(root, path, categories)\n\n            items.append(DatasetItem(id=osp.splitext(p)[0],\n                subset=self._subset, image=image, annotations=annotations))\n        return items, categories\n\n    @classmethod\n    def _parse_annotations(cls, xml_root, dataset_root, categories):\n        def parse_attributes(attr_str):\n            parsed = []\n            if not attr_str:\n                return parsed\n\n            for attr in [a.strip() for a in attr_str.split(\',\') if a.strip()]:\n                if \'=\' in attr:\n                    name, value = attr.split(\'=\', maxsplit=1)\n                    if value.lower() in {\'true\', \'false\'}:\n                        value = value.lower() == \'true\'\n                    else:\n                        try:\n                            value = float(value)\n                        except Exception:\n                            pass\n                    parsed.append((name, value))\n                else:\n                    parsed.append((attr, True))\n\n            return parsed\n\n        label_cat = categories[AnnotationType.label]\n        def get_label_id(label):\n            if not label:\n                return None\n            idx, _ = label_cat.find(label)\n            if idx is None:\n                idx = label_cat.add(label)\n            return idx\n\n        image_annotations = []\n\n        parsed_annotations = dict()\n        group_assignments = dict()\n        root_annotations = set()\n        for obj_elem in xml_root.iter(\'object\'):\n            obj_id = int(obj_elem.find(\'id\').text)\n\n            ann_items = []\n\n            label = get_label_id(obj_elem.find(\'name\').text)\n\n            attributes = []\n            attributes_elem = obj_elem.find(\'attributes\')\n            if attributes_elem is not None and attributes_elem.text:\n                attributes = parse_attributes(attributes_elem.text)\n\n            occluded = False\n            occluded_elem = obj_elem.find(\'occluded\')\n            if occluded_elem is not None and occluded_elem.text:\n                occluded = (occluded_elem.text == \'yes\')\n            attributes.append((\'occluded\', occluded))\n\n            deleted = False\n            deleted_elem = obj_elem.find(\'deleted\')\n            if deleted_elem is not None and deleted_elem.text:\n                deleted = bool(int(deleted_elem.text))\n\n            user = \'\'\n\n            poly_elem = obj_elem.find(\'polygon\')\n            segm_elem = obj_elem.find(\'segm\')\n            type_elem = obj_elem.find(\'type\') # the only value is \'bounding_box\'\n            if poly_elem is not None:\n                user_elem = poly_elem.find(\'username\')\n                if user_elem is not None and user_elem.text:\n                    user = user_elem.text\n                attributes.append((\'username\', user))\n\n                points = []\n                for point_elem in poly_elem.iter(\'pt\'):\n                    x = float(point_elem.find(\'x\').text)\n                    y = float(point_elem.find(\'y\').text)\n                    points.append(x)\n                    points.append(y)\n\n                if type_elem is not None and type_elem.text == \'bounding_box\':\n                    xmin = min(points[::2])\n                    xmax = max(points[::2])\n                    ymin = min(points[1::2])\n                    ymax = max(points[1::2])\n                    ann_items.append(Bbox(xmin, ymin, xmax - xmin, ymax - ymin,\n                        label=label, attributes=attributes, id=obj_id,\n                    ))\n                else:\n                    ann_items.append(Polygon(points,\n                        label=label, attributes=attributes, id=obj_id,\n                    ))\n            elif segm_elem is not None:\n                user_elem = segm_elem.find(\'username\')\n                if user_elem is not None and user_elem.text:\n                    user = user_elem.text\n                attributes.append((\'username\', user))\n\n                mask_path = osp.join(dataset_root, LabelMePath.MASKS_DIR,\n                    segm_elem.find(\'mask\').text)\n                if not osp.isfile(mask_path):\n                    raise Exception(""Can\'t find mask at \'%s\'"" % mask_path)\n                mask = load_mask(mask_path)\n                mask = np.any(mask, axis=2)\n                ann_items.append(Mask(image=mask, label=label, id=obj_id,\n                    attributes=attributes))\n\n            if not deleted:\n                parsed_annotations[obj_id] = ann_items\n\n            # Find parents and children\n            parts_elem = obj_elem.find(\'parts\')\n            if parts_elem is not None:\n                children_ids = []\n                hasparts_elem = parts_elem.find(\'hasparts\')\n                if hasparts_elem is not None and hasparts_elem.text:\n                    children_ids = [int(c) for c in hasparts_elem.text.split(\',\')]\n\n                parent_ids = []\n                ispartof_elem = parts_elem.find(\'ispartof\')\n                if ispartof_elem is not None and ispartof_elem.text:\n                    parent_ids = [int(c) for c in ispartof_elem.text.split(\',\')]\n\n                if children_ids and not parent_ids and hasparts_elem.text:\n                    root_annotations.add(obj_id)\n                group_assignments[obj_id] = [None, children_ids]\n\n        # assign single group to all grouped annotations\n        current_group_id = 0\n        annotations_to_visit = list(root_annotations)\n        while annotations_to_visit:\n            ann_id = annotations_to_visit.pop()\n            ann_assignment = group_assignments[ann_id]\n            group_id, children_ids = ann_assignment\n            if group_id:\n                continue\n\n            if ann_id in root_annotations:\n                current_group_id += 1 # start a new group\n\n            group_id = current_group_id\n            ann_assignment[0] = group_id\n\n            # continue with children\n            annotations_to_visit.extend(children_ids)\n\n        assert current_group_id == len(root_annotations)\n\n        for ann_id, ann_items in parsed_annotations.items():\n            group_id = 0\n            if ann_id in group_assignments:\n                ann_assignment = group_assignments[ann_id]\n                group_id = ann_assignment[0]\n\n            for ann_item in ann_items:\n                if group_id:\n                    ann_item.group = group_id\n\n                image_annotations.append(ann_item)\n\n        return image_annotations\n\n\nclass LabelMeImporter(Importer):\n    _EXTRACTOR_NAME = \'label_me\'\n\n    @classmethod\n    def detect(cls, path):\n        if not osp.isdir(path):\n            return False\n        return len(cls.find_subsets(path)) != 0\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        subset_paths = self.find_subsets(path)\n        if len(subset_paths) == 0:\n            raise Exception(""Failed to find \'label_me\' dataset at \'%s\'"" % path)\n\n        for subset_path, subset_name in subset_paths:\n            params = {}\n            if subset_name:\n                params[\'subset_name\'] = subset_name\n            params.update(extra_params)\n\n            source_name = osp.splitext(osp.basename(subset_path))[0]\n            project.add_source(source_name,\n            {\n                \'url\': subset_path,\n                \'format\': self._EXTRACTOR_NAME,\n                \'options\': params,\n            })\n\n        return project\n\n    @staticmethod\n    def find_subsets(path):\n        subset_paths = []\n        if not osp.isdir(path):\n            raise Exception(""Expected directory path, got \'%s\'"" % path)\n\n        path = osp.normpath(path)\n\n        def has_annotations(d):\n            return len([p for p in os.listdir(d) if p.endswith(\'.xml\')]) != 0\n\n        if has_annotations(path):\n            subset_paths = [(path, None)]\n        else:\n            for d in os.listdir(path):\n                subset = d\n                d = osp.join(path, d)\n                if osp.isdir(d) and has_annotations(d):\n                    subset_paths.append((d, subset))\n        return subset_paths\n\n\nclass LabelMeConverter(Converter, CliPlugin):\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        return parser\n\n    def __init__(self, save_images=False):\n        super().__init__()\n\n        self._save_images = save_images\n\n    def __call__(self, extractor, save_dir):\n        self._extractor = extractor\n\n        subsets = extractor.subsets()\n        if len(subsets) == 0:\n            subsets = [ None ]\n\n        for subset_name in subsets:\n            if subset_name:\n                subset = extractor.get_subset(subset_name)\n            else:\n                subset_name = DEFAULT_SUBSET_NAME\n                subset = extractor\n\n            subset_dir = osp.join(save_dir, subset_name)\n            os.makedirs(subset_dir, exist_ok=True)\n            os.makedirs(osp.join(subset_dir, LabelMePath.MASKS_DIR),\n                exist_ok=True)\n\n            for item in subset:\n                self._save_item(item, subset_dir)\n\n    def _get_label(self, label_id):\n        if label_id is None:\n            return \'\'\n        return self._extractor.categories()[AnnotationType.label] \\\n            .items[label_id].name\n\n    def _save_item(self, item, subset_dir):\n        from lxml import etree as ET\n\n        log.debug(""Converting item \'%s\'"", item.id)\n\n        image_filename = \'\'\n        if item.has_image:\n            image_filename = item.image.filename\n        if self._save_images:\n            if item.has_image and item.image.has_data:\n                if image_filename:\n                    image_filename = osp.splitext(image_filename)[0]\n                else:\n                    image_filename = item.id\n                image_filename += LabelMePath.IMAGE_EXT\n                save_image(osp.join(subset_dir, image_filename),\n                    item.image.data, create_dir=True)\n            else:\n                log.debug(""Item \'%s\' has no image"" % item.id)\n\n        root_elem = ET.Element(\'annotation\')\n        ET.SubElement(root_elem, \'filename\').text = image_filename\n        ET.SubElement(root_elem, \'folder\').text = \'\'\n\n        source_elem = ET.SubElement(root_elem, \'source\')\n        ET.SubElement(source_elem, \'sourceImage\').text = \'\'\n        ET.SubElement(source_elem, \'sourceAnnotation\').text = \'Datumaro\'\n\n        if item.has_image:\n            image_elem = ET.SubElement(root_elem, \'imagesize\')\n            image_size = item.image.size\n            ET.SubElement(image_elem, \'nrows\').text = str(image_size[0])\n            ET.SubElement(image_elem, \'ncols\').text = str(image_size[1])\n\n        groups = defaultdict(list)\n\n        obj_id = 0\n        for ann in item.annotations:\n            if not ann.type in { AnnotationType.polygon,\n                    AnnotationType.bbox, AnnotationType.mask }:\n                continue\n\n            obj_elem = ET.SubElement(root_elem, \'object\')\n            ET.SubElement(obj_elem, \'name\').text = self._get_label(ann.label)\n            ET.SubElement(obj_elem, \'deleted\').text = \'0\'\n            ET.SubElement(obj_elem, \'verified\').text = \'0\'\n            ET.SubElement(obj_elem, \'occluded\').text = \\\n                \'yes\' if ann.attributes.pop(\'occluded\', \'\') == True else \'no\'\n            ET.SubElement(obj_elem, \'date\').text = \'\'\n            ET.SubElement(obj_elem, \'id\').text = str(obj_id)\n\n            parts_elem = ET.SubElement(obj_elem, \'parts\')\n            if ann.group:\n                groups[ann.group].append((obj_id, parts_elem))\n            else:\n                ET.SubElement(parts_elem, \'hasparts\').text = \'\'\n                ET.SubElement(parts_elem, \'ispartof\').text = \'\'\n\n            if ann.type == AnnotationType.bbox:\n                ET.SubElement(obj_elem, \'type\').text = \'bounding_box\'\n\n                poly_elem = ET.SubElement(obj_elem, \'polygon\')\n                x0, y0, x1, y1 = ann.points\n                points = [ (x0, y0), (x1, y0), (x1, y1), (x0, y1) ]\n                for x, y in points:\n                    point_elem = ET.SubElement(poly_elem, \'pt\')\n                    ET.SubElement(point_elem, \'x\').text = \'%.2f\' % x\n                    ET.SubElement(point_elem, \'y\').text = \'%.2f\' % y\n\n                ET.SubElement(poly_elem, \'username\').text = \\\n                    str(ann.attributes.pop(\'username\', \'\'))\n            elif ann.type == AnnotationType.polygon:\n                poly_elem = ET.SubElement(obj_elem, \'polygon\')\n                for x, y in zip(ann.points[::2], ann.points[1::2]):\n                    point_elem = ET.SubElement(poly_elem, \'pt\')\n                    ET.SubElement(point_elem, \'x\').text = \'%.2f\' % x\n                    ET.SubElement(point_elem, \'y\').text = \'%.2f\' % y\n\n                ET.SubElement(poly_elem, \'username\').text = \\\n                    str(ann.attributes.pop(\'username\', \'\'))\n            elif ann.type == AnnotationType.mask:\n                mask_filename = \'%s_mask_%s.png\' % (item.id, obj_id)\n                save_image(osp.join(subset_dir, LabelMePath.MASKS_DIR,\n                        mask_filename),\n                    self._paint_mask(ann.image))\n\n                segm_elem = ET.SubElement(obj_elem, \'segm\')\n                ET.SubElement(segm_elem, \'mask\').text = mask_filename\n\n                bbox = find_mask_bbox(ann.image)\n                box_elem = ET.SubElement(segm_elem, \'box\')\n                ET.SubElement(box_elem, \'xmin\').text = \'%.2f\' % bbox[0]\n                ET.SubElement(box_elem, \'ymin\').text = \'%.2f\' % bbox[1]\n                ET.SubElement(box_elem, \'xmax\').text = \\\n                    \'%.2f\' % (bbox[0] + bbox[2])\n                ET.SubElement(box_elem, \'ymax\').text = \\\n                    \'%.2f\' % (bbox[1] + bbox[3])\n\n                ET.SubElement(segm_elem, \'username\').text = \\\n                    str(ann.attributes.pop(\'username\', \'\'))\n            else:\n                raise NotImplementedError(""Unknown shape type \'%s\'"" % ann.type)\n\n            attrs = []\n            for k, v in ann.attributes.items():\n                attrs.append(\'%s=%s\' % (k, v))\n            ET.SubElement(obj_elem, \'attributes\').text = \', \'.join(attrs)\n\n            obj_id += 1\n\n        for _, group in groups.items():\n            leader_id, leader_parts_elem = group[0]\n            leader_parts = [str(o_id) for o_id, _ in group[1:]]\n            ET.SubElement(leader_parts_elem, \'hasparts\').text = \\\n                \',\'.join(leader_parts)\n            ET.SubElement(leader_parts_elem, \'ispartof\').text = \'\'\n\n            for obj_id, parts_elem in group[1:]:\n                ET.SubElement(parts_elem, \'hasparts\').text = \'\'\n                ET.SubElement(parts_elem, \'ispartof\').text = str(leader_id)\n\n        xml_path = osp.join(subset_dir, \'%s.xml\' % item.id)\n        with open(xml_path, \'w\', encoding=\'utf-8\') as f:\n            xml_data = ET.tostring(root_elem, encoding=\'unicode\',\n                pretty_print=True)\n            f.write(xml_data)\n\n    @staticmethod\n    def _paint_mask(mask):\n        # TODO: check if mask colors are random\n        return np.array([[0, 0, 0, 0], [255, 203, 0, 153]],\n            dtype=np.uint8)[mask.astype(np.uint8)]'"
datumaro/datumaro/plugins/mot_format.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# The Multiple Object Tracking Benchmark challenge format support\n# Format description: https://arxiv.org/pdf/1906.04567.pdf\n# Another description: https://motchallenge.net/instructions\n\nfrom collections import OrderedDict\nimport csv\nfrom enum import Enum\nimport logging as log\nimport os\nimport os.path as osp\n\nfrom datumaro.components.extractor import (SourceExtractor,\n    DatasetItem, AnnotationType, Bbox, LabelCategories\n)\nfrom datumaro.components.extractor import Importer\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom datumaro.util import cast\nfrom datumaro.util.image import Image, save_image\n\n\nMotLabel = Enum(\'MotLabel\', [\n    (\'pedestrian\', 1),\n    (\'person on vehicle\', 2),\n    (\'car\', 3),\n    (\'bicycle\', 4),\n    (\'motorbike\', 5),\n    (\'non motorized vehicle\', 6),\n    (\'static person\', 7),\n    (\'distractor\', 8),\n    (\'occluder\', 9),\n    (\'occluder on the ground\', 10),\n    (\'occluder full\', 11),\n    (\'reflection\', 12),\n])\n\nclass MotPath:\n    IMAGE_DIR = \'img1\'\n    SEQINFO_FILE = \'seqinfo.ini\'\n    LABELS_FILE = \'labels.txt\'\n    GT_FILENAME = \'gt.txt\'\n    DET_FILENAME = \'det.txt\'\n\n    IMAGE_EXT = \'.jpg\'\n\n    FIELDS = [\n        \'frame_id\',\n        \'track_id\',\n        \'x\',\n        \'y\',\n        \'w\',\n        \'h\',\n        \'confidence\', # or \'not ignored\' flag for GT anns\n        \'class_id\',\n        \'visibility\'\n    ]\n\n\nclass MotSeqExtractor(SourceExtractor):\n    def __init__(self, path, labels=None, occlusion_threshold=0, is_gt=None):\n        super().__init__()\n\n        assert osp.isfile(path)\n        seq_root = osp.dirname(osp.dirname(path))\n        self._image_dir = \'\'\n        if osp.isdir(osp.join(seq_root, MotPath.IMAGE_DIR)):\n            self._image_dir = osp.join(seq_root, MotPath.IMAGE_DIR)\n\n        seq_info = osp.join(seq_root, MotPath.SEQINFO_FILE)\n        if osp.isfile(seq_info):\n            seq_info = self._parse_seq_info(seq_info)\n            self._image_dir = osp.join(seq_root, seq_info[\'imdir\'])\n        else:\n            seq_info = None\n        self._seq_info = seq_info\n\n        self._occlusion_threshold = float(occlusion_threshold)\n\n        assert is_gt in {None, True, False}\n        if is_gt is None:\n            if osp.basename(path) == MotPath.DET_FILENAME:\n                is_gt = False\n            else:\n                is_gt = True\n        self._is_gt = is_gt\n\n        if labels is None:\n            labels = osp.join(osp.dirname(path), MotPath.LABELS_FILE)\n            if not osp.isfile(labels):\n                labels = [lbl.name for lbl in MotLabel]\n        if isinstance(labels, str):\n            labels = self._parse_labels(labels)\n        elif isinstance(labels, list):\n            assert all(isinstance(lbl, str) for lbl in labels), labels\n        else:\n            raise TypeError(""Unexpected type of \'labels\' argument: %s"" % labels)\n        self._categories = self._load_categories(labels)\n        self._items = self._load_items(path)\n\n    def categories(self):\n        return self._categories\n\n    def __iter__(self):\n        for item in self._items.values():\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n\n    @staticmethod\n    def _parse_labels(path):\n        with open(path, encoding=\'utf-8\') as labels_file:\n            return [s.strip() for s in labels_file]\n\n    def _load_categories(self, labels):\n        attributes = [\'track_id\']\n        if self._is_gt:\n            attributes += [\'occluded\', \'visibility\', \'ignored\']\n        else:\n            attributes += [\'score\']\n        label_cat = LabelCategories(attributes=attributes)\n        for label in labels:\n            label_cat.add(label)\n\n        return { AnnotationType.label: label_cat }\n\n    def _load_items(self, path):\n        labels_count = len(self._categories[AnnotationType.label].items)\n        items = OrderedDict()\n\n        if self._seq_info:\n            for frame_id in range(self._seq_info[\'seqlength\']):\n                items[frame_id] = DatasetItem(\n                    id=frame_id,\n                    subset=self._subset,\n                    image=Image(\n                        path=osp.join(self._image_dir,\n                            \'%06d%s\' % (frame_id, self._seq_info[\'imext\'])),\n                        size=(self._seq_info[\'imheight\'], self._seq_info[\'imwidth\'])\n                    )\n                )\n        elif osp.isdir(self._image_dir):\n            for p in os.listdir(self._image_dir):\n                if p.endswith(MotPath.IMAGE_EXT):\n                    frame_id = int(osp.splitext(p)[0])\n                    items[frame_id] = DatasetItem(\n                        id=frame_id,\n                        subset=self._subset,\n                        image=osp.join(self._image_dir, p),\n                    )\n\n        with open(path, newline=\'\', encoding=\'utf-8\') as csv_file:\n            # NOTE: Different MOT files have different count of fields\n            # (7, 9 or 10). This is handled by reader:\n            # - all extra fields go to a separate field\n            # - all unmet fields have None values\n            for row in csv.DictReader(csv_file, fieldnames=MotPath.FIELDS):\n                frame_id = int(row[\'frame_id\'])\n                item = items.get(frame_id)\n                if item is None:\n                    item = DatasetItem(id=frame_id, subset=self._subset)\n                annotations = item.annotations\n\n                x, y = float(row[\'x\']), float(row[\'y\'])\n                w, h = float(row[\'w\']), float(row[\'h\'])\n                label_id = row.get(\'class_id\')\n                if label_id and label_id != \'-1\':\n                    label_id = int(label_id) - 1\n                    assert label_id < labels_count, label_id\n                else:\n                    label_id = None\n\n                attributes = {}\n\n                # Annotations for detection task are not related to any track\n                track_id = int(row[\'track_id\'])\n                if 0 < track_id:\n                    attributes[\'track_id\'] = track_id\n\n                confidence = cast(row.get(\'confidence\'), float, 1)\n                visibility = cast(row.get(\'visibility\'), float, 1)\n                if self._is_gt:\n                    attributes[\'visibility\'] = visibility\n                    attributes[\'occluded\'] = \\\n                        visibility <= self._occlusion_threshold\n                    attributes[\'ignored\'] = confidence == 0\n                else:\n                    attributes[\'score\'] = float(confidence)\n\n                annotations.append(Bbox(x, y, w, h, label=label_id,\n                    attributes=attributes))\n\n                items[frame_id] = item\n        return items\n\n    @classmethod\n    def _parse_seq_info(cls, path):\n        fields = {}\n        with open(path, encoding=\'utf-8\') as f:\n            for line in f:\n                entry = line.lower().strip().split(\'=\', maxsplit=1)\n                if len(entry) == 2:\n                    fields[entry[0]] = entry[1]\n        cls._check_seq_info(fields)\n        for k in { \'framerate\', \'seqlength\', \'imwidth\', \'imheight\' }:\n            fields[k] = int(fields[k])\n        return fields\n\n    @staticmethod\n    def _check_seq_info(seq_info):\n        assert set(seq_info) == {\'name\', \'imdir\', \'framerate\', \'seqlength\', \'imwidth\', \'imheight\', \'imext\'}, seq_info\n\nclass MotSeqImporter(Importer):\n    _EXTRACTOR_NAME = \'mot_seq\'\n\n    @classmethod\n    def detect(cls, path):\n        return len(cls.find_subsets(path)) != 0\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        subsets = self.find_subsets(path)\n        if len(subsets) == 0:\n            raise Exception(""Failed to find \'mot\' dataset at \'%s\'"" % path)\n\n        for ann_file in subsets:\n            log.info(""Found a dataset at \'%s\'"" % ann_file)\n\n            source_name = osp.splitext(osp.basename(ann_file))[0]\n            project.add_source(source_name, {\n                \'url\': ann_file,\n                \'format\': self._EXTRACTOR_NAME,\n                \'options\': extra_params,\n            })\n\n        return project\n\n    @staticmethod\n    def find_subsets(path):\n        subsets = []\n        if path.endswith(\'.txt\') and osp.isfile(path):\n            subsets = [path]\n        elif osp.isdir(path):\n            p = osp.join(path, \'gt\', MotPath.GT_FILENAME)\n            if osp.isfile(p):\n                subsets.append(p)\n        return subsets\n\nclass MotSeqGtConverter(Converter, CliPlugin):\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().__init__(**kwargs)\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        return parser\n\n    def __init__(self, save_images=False):\n        super().__init__()\n\n        self._save_images = save_images\n\n    def __call__(self, extractor, save_dir):\n        images_dir = osp.join(save_dir, MotPath.IMAGE_DIR)\n        os.makedirs(images_dir, exist_ok=True)\n        self._images_dir = images_dir\n\n        anno_dir = osp.join(save_dir, \'gt\')\n        os.makedirs(anno_dir, exist_ok=True)\n        anno_file = osp.join(anno_dir, MotPath.GT_FILENAME)\n        with open(anno_file, \'w\', encoding=""utf-8"") as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=MotPath.FIELDS)\n\n            track_id_mapping = {-1: -1}\n            for idx, item in enumerate(extractor):\n                log.debug(""Converting item \'%s\'"", item.id)\n\n                frame_id = cast(item.id, int, 1 + idx)\n\n                for anno in item.annotations:\n                    if anno.type != AnnotationType.bbox:\n                        continue\n\n                    track_id = int(anno.attributes.get(\'track_id\', -1))\n                    if track_id not in track_id_mapping:\n                        track_id_mapping[track_id] = len(track_id_mapping)\n                    track_id = track_id_mapping[track_id]\n                    writer.writerow({\n                        \'frame_id\': frame_id,\n                        \'track_id\': track_id,\n                        \'x\': anno.x,\n                        \'y\': anno.y,\n                        \'w\': anno.w,\n                        \'h\': anno.h,\n                        \'confidence\': int(anno.attributes.get(\'ignored\') != True),\n                        \'class_id\': 1 + cast(anno.label, int, -2),\n                        \'visibility\': float(\n                            anno.attributes.get(\'visibility\',\n                                1 - float(\n                                    anno.attributes.get(\'occluded\', False)\n                                )\n                            )\n                        )\n                    })\n\n                if self._save_images:\n                    if item.has_image and item.image.has_data:\n                        self._save_image(item, index=frame_id)\n                    else:\n                        log.debug(""Item \'%s\' has no image"" % item.id)\n\n        labels_file = osp.join(anno_dir, MotPath.LABELS_FILE)\n        with open(labels_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(\'\\n\'.join(l.name\n                for l in extractor.categories()[AnnotationType.label].items)\n            )\n\n    def _save_image(self, item, index):\n        if item.image.filename:\n            frame_id = osp.splitext(item.image.filename)[0]\n        else:\n            frame_id = item.id\n        frame_id = cast(frame_id, int, index)\n        image_filename = \'%06d%s\' % (frame_id, MotPath.IMAGE_EXT)\n        save_image(osp.join(self._images_dir, image_filename),\n            item.image.data)'"
datumaro/datumaro/plugins/openvino_launcher.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# pylint: disable=exec-used\n\nimport cv2\nimport logging as log\nimport numpy as np\nimport os.path as osp\nimport shutil\n\nfrom openvino.inference_engine import IECore\n\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom datumaro.components.launcher import Launcher\n\n\nclass OpenVinoImporter(CliPlugin):\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'-d\', \'--description\', required=True,\n            help=""Path to the model description file (.xml)"")\n        parser.add_argument(\'-w\', \'--weights\', required=True,\n            help=""Path to the model weights file (.bin)"")\n        parser.add_argument(\'-i\', \'--interpreter\', required=True,\n            help=""Path to the network output interprter script (.py)"")\n        parser.add_argument(\'--device\', default=\'CPU\',\n            help=""Target device (default: %(default)s)"")\n        return parser\n\n    @staticmethod\n    def copy_model(model_dir, model):\n        shutil.copy(model[\'description\'],\n            osp.join(model_dir, osp.basename(model[\'description\'])))\n        model[\'description\'] = osp.basename(model[\'description\'])\n\n        shutil.copy(model[\'weights\'],\n            osp.join(model_dir, osp.basename(model[\'weights\'])))\n        model[\'weights\'] = osp.basename(model[\'weights\'])\n\n        shutil.copy(model[\'interpreter\'],\n            osp.join(model_dir, osp.basename(model[\'interpreter\'])))\n        model[\'interpreter\'] = osp.basename(model[\'interpreter\'])\n\n\nclass InterpreterScript:\n    def __init__(self, path):\n        with open(path, \'r\') as f:\n            script = f.read()\n\n        context = {}\n        exec(script, context, context)\n\n        process_outputs = context.get(\'process_outputs\')\n        if not callable(process_outputs):\n            raise Exception(""Can\'t find \'process_outputs\' function in ""\n                ""the interpreter script"")\n        self.__dict__[\'process_outputs\'] = process_outputs\n\n        get_categories = context.get(\'get_categories\')\n        assert get_categories is None or callable(get_categories)\n        if get_categories:\n            self.__dict__[\'get_categories\'] = get_categories\n\n    @staticmethod\n    def get_categories():\n        return None\n\n    @staticmethod\n    def process_outputs(inputs, outputs):\n        raise NotImplementedError(\n            ""Function should be implemented in the interpreter script"")\n\n\nclass OpenVinoLauncher(Launcher):\n    cli_plugin = OpenVinoImporter\n\n    def __init__(self, description, weights, interpreter,\n            plugins_path=None, device=None, model_dir=None):\n        model_dir = model_dir or \'\'\n        if not osp.isfile(description):\n            description = osp.join(model_dir, description)\n        if not osp.isfile(description):\n            raise Exception(\'Failed to open model description file ""%s""\' % \\\n                (description))\n\n        if not osp.isfile(weights):\n            weights = osp.join(model_dir, weights)\n        if not osp.isfile(weights):\n            raise Exception(\'Failed to open model weights file ""%s""\' % \\\n                (weights))\n\n        if not osp.isfile(interpreter):\n            interpreter = osp.join(model_dir, interpreter)\n        if not osp.isfile(interpreter):\n            raise Exception(\'Failed to open model interpreter script file ""%s""\' % \\\n                (interpreter))\n\n        self._interpreter = InterpreterScript(interpreter)\n\n        self._device = device or \'CPU\'\n\n        self._ie = IECore()\n        if hasattr(self._ie, \'read_network\'):\n            self._network = self._ie.read_network(description, weights)\n        else: # backward compatibility\n            from openvino.inference_engine import IENetwork\n            self._network = IENetwork.from_ir(description, weights)\n        self._check_model_support(self._network, self._device)\n        self._load_executable_net()\n\n    def _check_model_support(self, net, device):\n        supported_layers = set(self._ie.query_network(net, device))\n        not_supported_layers = set(net.layers) - supported_layers\n        if len(not_supported_layers) != 0:\n            log.error(""The following layers are not supported "" \\\n                ""by the plugin for device \'%s\': %s."" % \\\n                (device, \', \'.join(not_supported_layers)))\n            raise NotImplementedError(\n                ""Some layers are not supported on the device"")\n\n    def _load_executable_net(self, batch_size=1):\n        network = self._network\n\n        iter_inputs = iter(network.inputs)\n        self._input_blob_name = next(iter_inputs)\n        self._output_blob_name = next(iter(network.outputs))\n\n        # NOTE: handling for the inclusion of `image_info` in OpenVino2019\n        self._require_image_info = \'image_info\' in network.inputs\n        if self._input_blob_name == \'image_info\':\n            self._input_blob_name = next(iter_inputs)\n\n        input_type = network.inputs[self._input_blob_name]\n        self._input_layout = input_type if isinstance(input_type, list) else input_type.shape\n\n        self._input_layout[0] = batch_size\n        network.reshape({self._input_blob_name: self._input_layout})\n        self._batch_size = batch_size\n\n        self._net = self._ie.load_network(network=network, num_requests=1,\n            device_name=self._device)\n\n    def infer(self, inputs):\n        assert len(inputs.shape) == 4, \\\n            ""Expected an input image in (N, H, W, C) format, got %s"" % \\\n            (inputs.shape)\n        assert inputs.shape[3] == 3, ""Expected BGR input, got %s"" % inputs.shape\n\n        n, c, h, w = self._input_layout\n        if inputs.shape[1:3] != (h, w):\n            resized_inputs = np.empty((n, h, w, c), dtype=inputs.dtype)\n            for inp, resized_input in zip(inputs, resized_inputs):\n                cv2.resize(inp, (w, h), resized_input)\n            inputs = resized_inputs\n        inputs = inputs.transpose((0, 3, 1, 2)) # NHWC to NCHW\n        inputs = {self._input_blob_name: inputs}\n        if self._require_image_info:\n            info = np.zeros([1, 3])\n            info[0, 0] = h\n            info[0, 1] = w\n            info[0, 2] = 1.0 # scale\n            inputs[\'image_info\'] = info\n\n        results = self._net.infer(inputs)\n        if len(results) == 1:\n            return results[self._output_blob_name]\n        else:\n            return results\n\n    def launch(self, inputs):\n        batch_size = len(inputs)\n        if self._batch_size < batch_size:\n            self._load_executable_net(batch_size)\n\n        outputs = self.infer(inputs)\n        results = self.process_outputs(inputs, outputs)\n        return results\n\n    def categories(self):\n        return self._interpreter.get_categories()\n\n    def process_outputs(self, inputs, outputs):\n        return self._interpreter.process_outputs(inputs, outputs)\n\n    def preferred_input_size(self):\n        _, _, h, w = self._input_layout\n        return (h, w)\n'"
datumaro/datumaro/plugins/transforms.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom enum import Enum\nimport logging as log\nimport os.path as osp\nimport random\n\nimport pycocotools.mask as mask_utils\n\nfrom datumaro.components.extractor import (Transform, AnnotationType,\n    RleMask, Polygon, Bbox,\n    LabelCategories, MaskCategories, PointsCategories\n)\nfrom datumaro.components.cli_plugin import CliPlugin\nimport datumaro.util.mask_tools as mask_tools\nfrom datumaro.util.annotation_tools import find_group_leader, find_instances\n\n\nclass CropCoveredSegments(Transform, CliPlugin):\n    def transform_item(self, item):\n        annotations = []\n        segments = []\n        for ann in item.annotations:\n            if ann.type in {AnnotationType.polygon, AnnotationType.mask}:\n                segments.append(ann)\n            else:\n                annotations.append(ann)\n        if not segments:\n            return item\n\n        if not item.has_image:\n            raise Exception(""Image info is required for this transform"")\n        h, w = item.image.size\n        segments = self.crop_segments(segments, w, h)\n\n        annotations += segments\n        return self.wrap_item(item, annotations=annotations)\n\n    @classmethod\n    def crop_segments(cls, segment_anns, img_width, img_height):\n        segment_anns = sorted(segment_anns, key=lambda x: x.z_order)\n\n        segments = []\n        for s in segment_anns:\n            if s.type == AnnotationType.polygon:\n                segments.append(s.points)\n            elif s.type == AnnotationType.mask:\n                if isinstance(s, RleMask):\n                    rle = s.rle\n                else:\n                    rle = mask_tools.mask_to_rle(s.image)\n                segments.append(rle)\n\n        segments = mask_tools.crop_covered_segments(\n            segments, img_width, img_height)\n\n        new_anns = []\n        for ann, new_segment in zip(segment_anns, segments):\n            fields = {\'z_order\': ann.z_order, \'label\': ann.label,\n                \'id\': ann.id, \'group\': ann.group, \'attributes\': ann.attributes\n            }\n            if ann.type == AnnotationType.polygon:\n                if fields[\'group\'] is None:\n                    fields[\'group\'] = cls._make_group_id(\n                        segment_anns + new_anns, fields[\'id\'])\n                for polygon in new_segment:\n                    new_anns.append(Polygon(points=polygon, **fields))\n            else:\n                rle = mask_tools.mask_to_rle(new_segment)\n                rle = mask_utils.frPyObjects(rle, *rle[\'size\'])\n                new_anns.append(RleMask(rle=rle, **fields))\n\n        return new_anns\n\n    @staticmethod\n    def _make_group_id(anns, ann_id):\n        if ann_id:\n            return ann_id\n        max_gid = max(anns, default=0, key=lambda x: x.group)\n        return max_gid + 1\n\nclass MergeInstanceSegments(Transform, CliPlugin):\n    """"""\n    Replaces instance masks and, optionally, polygons with a single mask.\n    """"""\n\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'--include-polygons\', action=\'store_true\',\n            help=""Include polygons"")\n        return parser\n\n    def __init__(self, extractor, include_polygons=False):\n        super().__init__(extractor)\n\n        self._include_polygons = include_polygons\n\n    def transform_item(self, item):\n        annotations = []\n        segments = []\n        for ann in item.annotations:\n            if ann.type in {AnnotationType.polygon, AnnotationType.mask}:\n                segments.append(ann)\n            else:\n                annotations.append(ann)\n        if not segments:\n            return item\n\n        if not item.has_image:\n            raise Exception(""Image info is required for this transform"")\n        h, w = item.image.size\n        instances = self.find_instances(segments)\n        segments = [self.merge_segments(i, w, h, self._include_polygons)\n            for i in instances]\n        segments = sum(segments, [])\n\n        annotations += segments\n        return self.wrap_item(item, annotations=annotations)\n\n    @classmethod\n    def merge_segments(cls, instance, img_width, img_height,\n            include_polygons=False):\n        polygons = [a for a in instance if a.type == AnnotationType.polygon]\n        masks = [a for a in instance if a.type == AnnotationType.mask]\n        if not polygons and not masks:\n            return []\n\n        leader = find_group_leader(polygons + masks)\n        instance = []\n\n        # Build the resulting mask\n        mask = None\n\n        if include_polygons and polygons:\n            polygons = [p.points for p in polygons]\n            mask = mask_tools.rles_to_mask(polygons, img_width, img_height)\n        else:\n            instance += polygons # keep unused polygons\n\n        if masks:\n            masks = [m.image for m in masks]\n            if mask is not None:\n                masks += [mask]\n            mask = mask_tools.merge_masks(masks)\n\n        if mask is None:\n            return instance\n\n        mask = mask_tools.mask_to_rle(mask)\n        mask = mask_utils.frPyObjects(mask, *mask[\'size\'])\n        instance.append(\n            RleMask(rle=mask, label=leader.label, z_order=leader.z_order,\n                id=leader.id, attributes=leader.attributes, group=leader.group\n            )\n        )\n        return instance\n\n    @staticmethod\n    def find_instances(annotations):\n        return find_instances(a for a in annotations\n            if a.type in {AnnotationType.polygon, AnnotationType.mask})\n\nclass PolygonsToMasks(Transform, CliPlugin):\n    def transform_item(self, item):\n        annotations = []\n        for ann in item.annotations:\n            if ann.type == AnnotationType.polygon:\n                if not item.has_image:\n                    raise Exception(""Image info is required for this transform"")\n                h, w = item.image.size\n                annotations.append(self.convert_polygon(ann, h, w))\n            else:\n                annotations.append(ann)\n\n        return self.wrap_item(item, annotations=annotations)\n\n    @staticmethod\n    def convert_polygon(polygon, img_h, img_w):\n        rle = mask_utils.frPyObjects([polygon.points], img_h, img_w)[0]\n\n        return RleMask(rle=rle, label=polygon.label, z_order=polygon.z_order,\n            id=polygon.id, attributes=polygon.attributes, group=polygon.group)\n\nclass BoxesToMasks(Transform, CliPlugin):\n    def transform_item(self, item):\n        annotations = []\n        for ann in item.annotations:\n            if ann.type == AnnotationType.bbox:\n                if not item.has_image:\n                    raise Exception(""Image info is required for this transform"")\n                h, w = item.image.size\n                annotations.append(self.convert_bbox(ann, h, w))\n            else:\n                annotations.append(ann)\n\n        return self.wrap_item(item, annotations=annotations)\n\n    @staticmethod\n    def convert_bbox(bbox, img_h, img_w):\n        rle = mask_utils.frPyObjects([bbox.as_polygon()], img_h, img_w)[0]\n\n        return RleMask(rle=rle, label=bbox.label, z_order=bbox.z_order,\n            id=bbox.id, attributes=bbox.attributes, group=bbox.group)\n\nclass MasksToPolygons(Transform, CliPlugin):\n    def transform_item(self, item):\n        annotations = []\n        for ann in item.annotations:\n            if ann.type == AnnotationType.mask:\n                polygons = self.convert_mask(ann)\n                if not polygons:\n                    log.debug(""[%s]: item %s: ""\n                        ""Mask conversion to polygons resulted in too ""\n                        ""small polygons, which were discarded"" % \\\n                        (self._get_name(__class__), item.id))\n                annotations.extend(polygons)\n            else:\n                annotations.append(ann)\n\n        return self.wrap_item(item, annotations=annotations)\n\n    @staticmethod\n    def convert_mask(mask):\n        polygons = mask_tools.mask_to_polygons(mask.image)\n\n        return [\n            Polygon(points=p, label=mask.label, z_order=mask.z_order,\n                id=mask.id, attributes=mask.attributes, group=mask.group)\n            for p in polygons\n        ]\n\nclass ShapesToBoxes(Transform, CliPlugin):\n    def transform_item(self, item):\n        annotations = []\n        for ann in item.annotations:\n            if ann.type in { AnnotationType.mask, AnnotationType.polygon,\n                AnnotationType.polyline, AnnotationType.points,\n            }:\n                annotations.append(self.convert_shape(ann))\n            else:\n                annotations.append(ann)\n\n        return self.wrap_item(item, annotations=annotations)\n\n    @staticmethod\n    def convert_shape(shape):\n        bbox = shape.get_bbox()\n        return Bbox(*bbox, label=shape.label, z_order=shape.z_order,\n            id=shape.id, attributes=shape.attributes, group=shape.group)\n\nclass Reindex(Transform, CliPlugin):\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'-s\', \'--start\', type=int, default=1,\n            help=""Start value for item ids"")\n        return parser\n\n    def __init__(self, extractor, start=1):\n        super().__init__(extractor)\n\n        self._start = start\n\n    def __iter__(self):\n        for i, item in enumerate(self._extractor):\n            yield self.wrap_item(item, id=i + self._start)\n\nclass MapSubsets(Transform, CliPlugin):\n    @staticmethod\n    def _mapping_arg(s):\n        parts = s.split(\':\')\n        if len(parts) != 2:\n            import argparse\n            raise argparse.ArgumentTypeError()\n        return parts\n\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'-s\', \'--subset\', action=\'append\',\n            type=cls._mapping_arg, dest=\'mapping\',\n            help=""Subset mapping of the form: \'src:dst\' (repeatable)"")\n        return parser\n\n    def __init__(self, extractor, mapping=None):\n        super().__init__(extractor)\n\n        if mapping is None:\n            mapping = {}\n        elif not isinstance(mapping, dict):\n            mapping = dict(tuple(m) for m in mapping)\n        self._mapping = mapping\n\n    def transform_item(self, item):\n        return self.wrap_item(item,\n            subset=self._mapping.get(item.subset, item.subset))\n\nclass RandomSplit(Transform, CliPlugin):\n    """"""\n    Joins all subsets into one and splits the result into few parts.\n    It is expected that item ids are unique and subset ratios sum up to 1.|n\n    |n\n    Example:|n\n    |s|s%(prog)s --subset train:.67 --subset test:.33\n    """"""\n\n    @staticmethod\n    def _split_arg(s):\n        parts = s.split(\':\')\n        if len(parts) != 2:\n            import argparse\n            raise argparse.ArgumentTypeError()\n        return (parts[0], float(parts[1]))\n\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'-s\', \'--subset\', action=\'append\',\n            type=cls._split_arg, dest=\'splits\',\n            help=""Subsets in the form of: \'<subset>:<ratio>\' (repeatable)"")\n        parser.add_argument(\'--seed\', type=int, help=""Random seed"")\n        return parser\n\n    def __init__(self, extractor, splits, seed=None):\n        super().__init__(extractor)\n\n        assert 0 < len(splits), ""Expected at least one split""\n        assert all(0.0 <= r and r <= 1.0 for _, r in splits), \\\n            ""Ratios are expected to be in the range [0; 1], but got %s"" % splits\n\n        total_ratio = sum(s[1] for s in splits)\n        if not abs(total_ratio - 1.0) <= 1e-7:\n            raise Exception(\n                ""Sum of ratios is expected to be 1, got %s, which is %s"" %\n                (splits, total_ratio))\n\n        dataset_size = len(extractor)\n        indices = list(range(dataset_size))\n\n        random.seed(seed)\n        random.shuffle(indices)\n        parts = []\n        s = 0\n        for subset, ratio in splits:\n            s += ratio\n            boundary = int(s * dataset_size)\n            parts.append((boundary, subset))\n\n        self._parts = parts\n\n    def _find_split(self, index):\n        for boundary, subset in self._parts:\n            if index < boundary:\n                return subset\n        return subset # all the possible remainder goes to the last split\n\n    def __iter__(self):\n        for i, item in enumerate(self._extractor):\n            yield self.wrap_item(item, subset=self._find_split(i))\n\nclass IdFromImageName(Transform, CliPlugin):\n    def transform_item(self, item):\n        name = item.id\n        if item.has_image and item.image.filename:\n            name = osp.splitext(item.image.filename)[0]\n        return self.wrap_item(item, id=name)\n\nclass RemapLabels(Transform, CliPlugin):\n    DefaultAction = Enum(\'DefaultAction\', [\'keep\', \'delete\'])\n\n    @staticmethod\n    def _split_arg(s):\n        parts = s.split(\':\')\n        if len(parts) != 2:\n            import argparse\n            raise argparse.ArgumentTypeError()\n        return (parts[0], parts[1])\n\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'-l\', \'--label\', action=\'append\',\n            type=cls._split_arg, dest=\'mapping\',\n            help=""Label in the form of: \'<src>:<dst>\' (repeatable)"")\n        parser.add_argument(\'--default\',\n            choices=[a.name for a in cls.DefaultAction],\n            default=cls.DefaultAction.keep.name,\n            help=""Action for unspecified labels"")\n        return parser\n\n    def __init__(self, extractor, mapping, default=None):\n        super().__init__(extractor)\n\n        assert isinstance(default, (str, self.DefaultAction))\n        if isinstance(default, str):\n            default = self.DefaultAction[default]\n\n        assert isinstance(mapping, (dict, list))\n        if isinstance(mapping, list):\n            mapping = dict(mapping)\n\n        self._categories = {}\n\n        src_label_cat = self._extractor.categories().get(AnnotationType.label)\n        if src_label_cat is not None:\n            self._make_label_id_map(src_label_cat, mapping, default)\n\n        src_mask_cat = self._extractor.categories().get(AnnotationType.mask)\n        if src_mask_cat is not None:\n            assert src_label_cat is not None\n            dst_mask_cat = MaskCategories(attributes=src_mask_cat.attributes)\n            dst_mask_cat.colormap = {\n                id: src_mask_cat.colormap[id]\n                for id, _ in enumerate(src_label_cat.items)\n                if self._map_id(id) or id == 0\n            }\n            self._categories[AnnotationType.mask] = dst_mask_cat\n\n        src_points_cat = self._extractor.categories().get(AnnotationType.points)\n        if src_points_cat is not None:\n            assert src_label_cat is not None\n            dst_points_cat = PointsCategories(attributes=src_points_cat.attributes)\n            dst_points_cat.items = {\n                id: src_points_cat.items[id]\n                for id, item in enumerate(src_label_cat.items)\n                if self._map_id(id) or id == 0\n            }\n            self._categories[AnnotationType.points] = dst_points_cat\n\n    def _make_label_id_map(self, src_label_cat, label_mapping, default_action):\n        dst_label_cat = LabelCategories(attributes=src_label_cat.attributes)\n        id_mapping = {}\n        for src_index, src_label in enumerate(src_label_cat.items):\n            dst_label = label_mapping.get(src_label.name)\n            if not dst_label and default_action == self.DefaultAction.keep:\n                dst_label = src_label.name # keep unspecified as is\n            if not dst_label:\n                continue\n\n            dst_index = dst_label_cat.find(dst_label)[0]\n            if dst_index is None:\n                dst_index = dst_label_cat.add(dst_label,\n                    src_label.parent, src_label.attributes)\n            id_mapping[src_index] = dst_index\n\n        if log.getLogger().isEnabledFor(log.DEBUG):\n            log.debug(""Label mapping:"")\n            for src_id, src_label in enumerate(src_label_cat.items):\n                if id_mapping.get(src_id):\n                    log.debug(""#%s \'%s\' -> #%s \'%s\'"",\n                        src_id, src_label.name, id_mapping[src_id],\n                        dst_label_cat.items[id_mapping[src_id]].name\n                    )\n                else:\n                    log.debug(""#%s \'%s\' -> <deleted>"", src_id, src_label.name)\n\n        self._map_id = lambda src_id: id_mapping.get(src_id, None)\n        self._categories[AnnotationType.label] = dst_label_cat\n\n    def categories(self):\n        return self._categories\n\n    def transform_item(self, item):\n        # TODO: provide non-inplace version\n        annotations = []\n        for ann in item.annotations:\n            if ann.type in { AnnotationType.label, AnnotationType.mask,\n                AnnotationType.points, AnnotationType.polygon,\n                AnnotationType.polyline, AnnotationType.bbox\n            } and ann.label is not None:\n                conv_label = self._map_id(ann.label)\n                if conv_label is not None:\n                    ann._label = conv_label\n                    annotations.append(ann)\n            else:\n                annotations.append(ann)\n        item._annotations = annotations\n        return item'"
datumaro/datumaro/util/__init__.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport os.path as osp\nfrom itertools import islice\n\n\ndef find(iterable, pred=lambda x: True, default=None):\n    return next((x for x in iterable if pred(x)), default)\n\ndef dir_items(path, ext, truncate_ext=False):\n    items = []\n    for f in os.listdir(path):\n        ext_pos = f.rfind(ext)\n        if ext_pos != -1:\n            if truncate_ext:\n                f = f[:ext_pos]\n            items.append(f)\n    return items\n\ndef split_path(path):\n    path = osp.normpath(path)\n    parts = []\n\n    while True:\n        path, part = osp.split(path)\n        if part:\n            parts.append(part)\n        else:\n            if path:\n                parts.append(path)\n            break\n    parts.reverse()\n\n    return parts\n\ndef cast(value, type_conv, default=None):\n    if value is None:\n        return default\n    try:\n        return type_conv(value)\n    except Exception:\n        return default\n\ndef to_snake_case(s):\n    if not s:\n        return \'\'\n\n    name = [s[0].lower()]\n    for idx, char in enumerate(s[1:]):\n        idx = idx + 1\n        if char.isalpha() and char.isupper():\n            prev_char = s[idx - 1]\n            if not (prev_char.isalpha() and prev_char.isupper()):\n                # avoid ""HTML"" -> ""h_t_m_l""\n                name.append(\'_\')\n            name.append(char.lower())\n        else:\n            name.append(char)\n    return \'\'.join(name)\n\ndef take_by(iterable, count):\n    """"""\n    Returns elements from the input iterable by batches of N items.\n    (\'abcdefg\', 3) -> [\'a\', \'b\', \'c\'], [\'d\', \'e\', \'f\'], [\'g\']\n    """"""\n\n    it = iter(iterable)\n    while True:\n        batch = list(islice(it, count))\n        if len(batch) == 0:\n            break\n\n        yield batch'"
datumaro/datumaro/util/annotation_tools.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom itertools import groupby\n\n\ndef find_instances(instance_anns):\n    instance_anns = sorted(instance_anns, key=lambda a: a.group)\n    ann_groups = []\n    for g_id, group in groupby(instance_anns, lambda a: a.group):\n        if not g_id:\n            ann_groups.extend(([a] for a in group))\n        else:\n            ann_groups.append(list(group))\n\n    return ann_groups\n\ndef find_group_leader(group):\n    return max(group, key=lambda x: x.get_area())\n\ndef compute_bbox(annotations):\n    boxes = [ann.get_bbox() for ann in annotations]\n    x0 = min((b[0] for b in boxes), default=0)\n    y0 = min((b[1] for b in boxes), default=0)\n    x1 = max((b[0] + b[2] for b in boxes), default=0)\n    y1 = max((b[1] + b[3] for b in boxes), default=0)\n    return [x0, y0, x1 - x0, y1 - y0]'"
datumaro/datumaro/util/command_targets.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\nfrom enum import Enum\n\nfrom datumaro.components.project import Project\nfrom datumaro.util.image import load_image\n\n\nTargetKinds = Enum('TargetKinds',\n    ['project', 'source', 'external_dataset', 'inference', 'image'])\n\ndef is_project_name(value, project):\n    return value == project.config.project_name\n\ndef is_project_path(value):\n    if value:\n        try:\n            Project.load(value)\n            return True\n        except Exception:\n            pass\n    return False\n\ndef is_project(value, project=None):\n    if is_project_path(value):\n        return True\n    elif project is not None:\n        return is_project_name(value, project)\n\n    return False\n\ndef is_source(value, project=None):\n    if project is not None:\n        try:\n            project.get_source(value)\n            return True\n        except KeyError:\n            pass\n\n    return False\n\ndef is_external_source(value):\n    return False\n\ndef is_inference_path(value):\n    return False\n\ndef is_image_path(value):\n    try:\n        return load_image(value) is not None\n    except Exception:\n        return False\n\n\nclass Target:\n    def __init__(self, kind, test, is_default=False, name=None):\n        self.kind = kind\n        self.test = test\n        self.is_default = is_default\n        self.name = name\n\n    def _get_fields(self):\n        return [self.kind, self.test, self.is_default, self.name]\n\n    def __str__(self):\n        return self.name or str(self.kind)\n\n    def __len__(self):\n        return len(self._get_fields())\n\n    def __iter__(self):\n        return iter(self._get_fields())\n\ndef ProjectTarget(kind=TargetKinds.project, test=None,\n        is_default=False, name='project name or path',\n        project=None):\n    if test is None:\n        test = lambda v: is_project(v, project=project)\n    return Target(kind, test, is_default, name)\n\ndef SourceTarget(kind=TargetKinds.source, test=None,\n        is_default=False, name='source name',\n        project=None):\n    if test is None:\n        test = lambda v: is_source(v, project=project)\n    return Target(kind, test, is_default, name)\n\ndef ExternalDatasetTarget(kind=TargetKinds.external_dataset,\n        test=is_external_source,\n        is_default=False, name='external dataset path'):\n    return Target(kind, test, is_default, name)\n\ndef InferenceTarget(kind=TargetKinds.inference, test=is_inference_path,\n        is_default=False, name='inference path'):\n    return Target(kind, test, is_default, name)\n\ndef ImageTarget(kind=TargetKinds.image, test=is_image_path,\n            is_default=False, name='image path'):\n    return Target(kind, test, is_default, name)\n\n\ndef target_selector(*targets):\n    def selector(value):\n        for (kind, test, is_default, _) in targets:\n            if (is_default and (value == '' or value is None)) or test(value):\n                return (kind, value)\n        raise argparse.ArgumentTypeError('Value should be one of: %s' \\\n            % (', '.join([str(t) for t in targets])))\n    return selector\n"""
datumaro/datumaro/util/image.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# pylint: disable=unused-import\n\nfrom enum import Enum\nfrom io import BytesIO\nimport numpy as np\nimport os\nimport os.path as osp\n\n_IMAGE_BACKENDS = Enum(\'_IMAGE_BACKENDS\', [\'cv2\', \'PIL\'])\n_IMAGE_BACKEND = None\ntry:\n    import cv2\n    _IMAGE_BACKEND = _IMAGE_BACKENDS.cv2\nexcept ImportError:\n    import PIL\n    _IMAGE_BACKEND = _IMAGE_BACKENDS.PIL\n\nfrom datumaro.util.image_cache import ImageCache as _ImageCache\n\n\ndef load_image(path):\n    """"""\n    Reads an image in the HWC Grayscale/BGR(A) float [0; 255] format.\n    """"""\n\n    if _IMAGE_BACKEND == _IMAGE_BACKENDS.cv2:\n        import cv2\n        image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        image = image.astype(np.float32)\n    elif _IMAGE_BACKEND == _IMAGE_BACKENDS.PIL:\n        from PIL import Image\n        image = Image.open(path)\n        image = np.asarray(image, dtype=np.float32)\n        if len(image.shape) == 3 and image.shape[2] in {3, 4}:\n            image[:, :, :3] = image[:, :, 2::-1] # RGB to BGR\n    else:\n        raise NotImplementedError()\n\n    assert len(image.shape) in {2, 3}\n    if len(image.shape) == 3:\n        assert image.shape[2] in {3, 4}\n    return image\n\ndef save_image(path, image, create_dir=False, **kwargs):\n    # NOTE: Check destination path for existence\n    # OpenCV silently fails if target directory does not exist\n    dst_dir = osp.dirname(path)\n    if dst_dir:\n        if create_dir:\n            os.makedirs(dst_dir, exist_ok=True)\n        elif not osp.isdir(dst_dir):\n            raise FileNotFoundError(""Directory does not exist: \'%s\'"" % dst_dir)\n\n    if not kwargs:\n        kwargs = {}\n\n    if _IMAGE_BACKEND == _IMAGE_BACKENDS.cv2:\n        import cv2\n\n        params = []\n\n        ext = path[-4:]\n        if ext.upper() == \'.JPG\':\n            params = [\n                int(cv2.IMWRITE_JPEG_QUALITY), kwargs.get(\'jpeg_quality\', 75)\n            ]\n\n        image = image.astype(np.uint8)\n        cv2.imwrite(path, image, params=params)\n    elif _IMAGE_BACKEND == _IMAGE_BACKENDS.PIL:\n        from PIL import Image\n\n        params = {}\n        params[\'quality\'] = kwargs.get(\'jpeg_quality\')\n        if kwargs.get(\'jpeg_quality\') == 100:\n            params[\'subsampling\'] = 0\n\n        image = image.astype(np.uint8)\n        if len(image.shape) == 3 and image.shape[2] in {3, 4}:\n            image[:, :, :3] = image[:, :, 2::-1] # BGR to RGB\n        image = Image.fromarray(image)\n        image.save(path, **params)\n    else:\n        raise NotImplementedError()\n\ndef encode_image(image, ext, **kwargs):\n    if not kwargs:\n        kwargs = {}\n\n    if _IMAGE_BACKEND == _IMAGE_BACKENDS.cv2:\n        import cv2\n\n        params = []\n\n        if not ext.startswith(\'.\'):\n            ext = \'.\' + ext\n\n        if ext.upper() == \'.JPG\':\n            params = [\n                int(cv2.IMWRITE_JPEG_QUALITY), kwargs.get(\'jpeg_quality\', 75)\n            ]\n\n        image = image.astype(np.uint8)\n        success, result = cv2.imencode(ext, image, params=params)\n        if not success:\n            raise Exception(""Failed to encode image to \'%s\' format"" % (ext))\n        return result.tobytes()\n    elif _IMAGE_BACKEND == _IMAGE_BACKENDS.PIL:\n        from PIL import Image\n\n        if ext.startswith(\'.\'):\n            ext = ext[1:]\n\n        params = {}\n        params[\'quality\'] = kwargs.get(\'jpeg_quality\')\n        if kwargs.get(\'jpeg_quality\') == 100:\n            params[\'subsampling\'] = 0\n\n        image = image.astype(np.uint8)\n        if len(image.shape) == 3 and image.shape[2] in {3, 4}:\n            image[:, :, :3] = image[:, :, 2::-1] # BGR to RGB\n        image = Image.fromarray(image)\n        with BytesIO() as buffer:\n            image.save(buffer, format=ext, **params)\n            return buffer.getvalue()\n    else:\n        raise NotImplementedError()\n\ndef decode_image(image_bytes):\n    if _IMAGE_BACKEND == _IMAGE_BACKENDS.cv2:\n        import cv2\n        image = np.frombuffer(image_bytes, dtype=np.uint8)\n        image = cv2.imdecode(image, cv2.IMREAD_UNCHANGED)\n        image = image.astype(np.float32)\n    elif _IMAGE_BACKEND == _IMAGE_BACKENDS.PIL:\n        from PIL import Image\n        image = Image.open(BytesIO(image_bytes))\n        image = np.asarray(image, dtype=np.float32)\n        if len(image.shape) == 3 and image.shape[2] in {3, 4}:\n            image[:, :, :3] = image[:, :, 2::-1] # RGB to BGR\n    else:\n        raise NotImplementedError()\n\n    assert len(image.shape) in {2, 3}\n    if len(image.shape) == 3:\n        assert image.shape[2] in {3, 4}\n    return image\n\n\nclass lazy_image:\n    def __init__(self, path, loader=None, cache=None):\n        if loader is None:\n            loader = load_image\n        self.path = path\n        self.loader = loader\n\n        # Cache:\n        # - False: do not cache\n        # - None: use the global cache\n        # - object: an object to be used as cache\n        assert cache in {None, False} or isinstance(cache, object)\n        self.cache = cache\n\n    def __call__(self):\n        image = None\n        image_id = hash(self) # path is not necessary hashable or a file path\n\n        cache = self._get_cache(self.cache)\n        if cache is not None:\n            image = cache.get(image_id)\n\n        if image is None:\n            image = self.loader(self.path)\n            if cache is not None:\n                cache.push(image_id, image)\n        return image\n\n    @staticmethod\n    def _get_cache(cache):\n        if cache is None:\n            cache = _ImageCache.get_instance()\n        elif cache == False:\n            return None\n        return cache\n\n    def __hash__(self):\n        return hash((id(self), self.path, self.loader))\n\nclass Image:\n    def __init__(self, data=None, path=None, loader=None, cache=None,\n            size=None):\n        assert size is None or len(size) == 2\n        if size is not None:\n            assert len(size) == 2 and 0 < size[0] and 0 < size[1], size\n            size = tuple(size)\n        self._size = size # (H, W)\n\n        assert path is None or isinstance(path, str)\n        if path is None:\n            path = \'\'\n        self._path = path\n\n        assert data is not None or path or loader, ""Image can not be empty""\n        if data is None and (path or loader):\n            if osp.isfile(path) or loader:\n                data = lazy_image(path, loader=loader, cache=cache)\n        self._data = data\n\n    @property\n    def path(self):\n        return self._path\n\n    @property\n    def filename(self):\n        return osp.basename(self._path)\n\n    @property\n    def data(self):\n        if callable(self._data):\n            return self._data()\n        return self._data\n\n    @property\n    def has_data(self):\n        return self._data is not None\n\n    @property\n    def size(self):\n        if self._size is None:\n            data = self.data\n            if data is not None:\n                self._size = data.shape[:2]\n        return self._size\n\n    def __eq__(self, other):\n        if isinstance(other, np.ndarray):\n            return self.has_data and np.array_equal(self.data, other)\n\n        if not isinstance(other, __class__):\n            return False\n        return \\\n            (np.array_equal(self.size, other.size)) and \\\n            (self.has_data == other.has_data) and \\\n            (self.has_data and np.array_equal(self.data, other.data) or \\\n                not self.has_data)'"
datumaro/datumaro/util/image_cache.py,0,"b'from collections import OrderedDict\n\n\n_instance = None\n\nDEFAULT_CAPACITY = 2\n\nclass ImageCache:\n    @staticmethod\n    def get_instance():\n        global _instance\n        if _instance is None:\n            _instance = ImageCache()\n        return _instance\n\n    def __init__(self, capacity=DEFAULT_CAPACITY):\n        self.capacity = int(capacity)\n        self.items = OrderedDict()\n\n    def push(self, item_id, image):\n        if self.capacity <= len(self.items):\n            self.items.popitem(last=True)\n        self.items[item_id] = image\n\n    def get(self, item_id):\n        default = object()\n        item = self.items.get(item_id, default)\n        if item is default:\n            return None\n\n        self.items.move_to_end(item_id, last=False) # naive splay tree\n        return item\n\n    def size(self):\n        return len(self.items)\n\n    def clear(self):\n        self.items.clear()'"
datumaro/datumaro/util/log_utils.py,0,b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom contextlib import contextmanager\nimport logging\n\n@contextmanager\ndef logging_disabled(max_level=logging.CRITICAL):\n    previous_level = logging.root.manager.disable\n    logging.disable(max_level)\n    try:\n        yield\n    finally:\n        logging.disable(previous_level)'
datumaro/datumaro/util/mask_tools.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport numpy as np\n\nfrom datumaro.util.image import lazy_image, load_image\n\n\ndef generate_colormap(length=256):\n    """"""\n    Generates colors using PASCAL VOC algorithm.\n\n    Returns index -> (R, G, B) mapping.\n    """"""\n\n    def get_bit(number, index):\n        return (number >> index) & 1\n\n    colormap = np.zeros((length, 3), dtype=int)\n    indices = np.arange(length, dtype=int)\n\n    for j in range(7, -1, -1):\n        for c in range(3):\n            colormap[:, c] |= get_bit(indices, c) << j\n        indices >>= 3\n\n    return {\n        id: tuple(color) for id, color in enumerate(colormap)\n    }\n\ndef invert_colormap(colormap):\n    return {\n        tuple(a): index for index, a in colormap.items()\n    }\n\ndef check_is_mask(mask):\n    assert len(mask.shape) in {2, 3}\n    if len(mask.shape) == 3:\n        assert mask.shape[2] == 1\n\n_default_colormap = generate_colormap()\n_default_unpaint_colormap = invert_colormap(_default_colormap)\n\ndef unpaint_mask(painted_mask, inverse_colormap=None):\n    # Covert color mask to index mask\n\n    # mask: HWC BGR [0; 255]\n    # colormap: (R, G, B) -> index\n    assert len(painted_mask.shape) == 3\n    if inverse_colormap is None:\n        inverse_colormap = _default_unpaint_colormap\n\n    if callable(inverse_colormap):\n        map_fn = lambda a: inverse_colormap(\n                (a >> 16) & 255, (a >> 8) & 255, a & 255\n            )\n    else:\n        map_fn = lambda a: inverse_colormap[(\n                (a >> 16) & 255, (a >> 8) & 255, a & 255\n            )]\n\n    painted_mask = painted_mask.astype(int)\n    painted_mask = painted_mask[:, :, 0] + \\\n                   (painted_mask[:, :, 1] << 8) + \\\n                   (painted_mask[:, :, 2] << 16)\n    uvals, unpainted_mask = np.unique(painted_mask, return_inverse=True)\n    palette = np.array([map_fn(v) for v in uvals], dtype=np.float32)\n    unpainted_mask = palette[unpainted_mask].reshape(painted_mask.shape[:2])\n\n    return unpainted_mask\n\ndef paint_mask(mask, colormap=None):\n    # Applies colormap to index mask\n\n    # mask: HW(C) [0; max_index] mask\n    # colormap: index -> (R, G, B)\n    check_is_mask(mask)\n\n    if colormap is None:\n        colormap = _default_colormap\n    if callable(colormap):\n        map_fn = colormap\n    else:\n        map_fn = lambda c: colormap.get(c, (-1, -1, -1))\n    palette = np.array([map_fn(c)[::-1] for c in range(256)], dtype=np.float32)\n\n    mask = mask.astype(np.uint8)\n    painted_mask = palette[mask].reshape((*mask.shape[:2], 3))\n    return painted_mask\n\ndef remap_mask(mask, map_fn):\n    # Changes mask elements from one colormap to another\n\n    # mask: HW(C) [0; max_index] mask\n    check_is_mask(mask)\n\n    return np.array([map_fn(c) for c in range(256)], dtype=np.uint8)[mask]\n\ndef make_index_mask(binary_mask, index):\n    return np.choose(binary_mask, np.array([0, index], dtype=np.uint8))\n\ndef make_binary_mask(mask):\n    return np.nonzero(mask)\n\n\ndef load_mask(path, inverse_colormap=None):\n    mask = load_image(path)\n    mask = mask.astype(np.uint8)\n    if inverse_colormap is not None:\n        if len(mask.shape) == 3 and mask.shape[2] != 1:\n            mask = unpaint_mask(mask, inverse_colormap)\n    return mask\n\ndef lazy_mask(path, inverse_colormap=None):\n    return lazy_image(path, lambda path: load_mask(path, inverse_colormap))\n\ndef mask_to_rle(binary_mask):\n    # walk in row-major order as COCO format specifies\n    bounded = binary_mask.ravel(order=\'F\')\n\n    # add borders to sequence\n    # find boundary positions for sequences and compute their lengths\n    difs = np.diff(bounded, prepend=[1 - bounded[0]], append=[1 - bounded[-1]])\n    counts, = np.where(difs != 0)\n\n    # start RLE encoding from 0 as COCO format specifies\n    if bounded[0] != 0:\n        counts = np.diff(counts, prepend=[0])\n    else:\n        counts = np.diff(counts)\n\n    return {\n        \'counts\': counts,\n        \'size\': list(binary_mask.shape)\n    }\n\ndef mask_to_polygons(mask, tolerance=1.0, area_threshold=1):\n    """"""\n    Convert an instance mask to polygons\n\n    Args:\n        mask: a 2d binary mask\n        tolerance: maximum distance from original points of\n            a polygon to the approximated ones\n        area_threshold: minimal area of generated polygons\n\n    Returns:\n        A list of polygons like [[x1,y1, x2,y2 ...], [...]]\n    """"""\n    from pycocotools import mask as mask_utils\n    from skimage import measure\n\n    polygons = []\n\n    # pad mask with 0 around borders\n    padded_mask = np.pad(mask, pad_width=1, mode=\'constant\', constant_values=0)\n    contours = measure.find_contours(padded_mask, 0.5)\n    # Fix coordinates after padding\n    contours = np.subtract(contours, 1)\n\n    for contour in contours:\n        if not np.array_equal(contour[0], contour[-1]):\n            contour = np.vstack((contour, contour[0])) # make polygon closed\n\n        contour = measure.approximate_polygon(contour, tolerance)\n        if len(contour) <= 2:\n            continue\n\n        contour = np.flip(contour, axis=1).flatten().clip(0) # [x0, y0, ...]\n\n        # Check if the polygon is big enough\n        rle = mask_utils.frPyObjects([contour], mask.shape[0], mask.shape[1])\n        area = sum(mask_utils.area(rle))\n        if area_threshold <= area:\n            polygons.append(contour)\n    return polygons\n\ndef crop_covered_segments(segments, width, height,\n        iou_threshold=0.0, ratio_tolerance=0.001, area_threshold=1,\n        return_masks=False):\n    """"""\n    Find all segments occluded by others and crop them to the visible part only.\n    Input segments are expected to be sorted from background to foreground.\n\n    Args:\n        segments: 1d list of segment RLEs (in COCO format)\n        width: width of the image\n        height: height of the image\n        iou_threshold: IoU threshold for objects to be counted as intersected\n            By default is set to 0 to process any intersected objects\n        ratio_tolerance: an IoU ""handicap"" value for a situation\n            when an object is (almost) fully covered by another one and we\n            don\'t want make a ""hole"" in the background object\n        area_threshold: minimal area of included segments\n\n    Returns:\n        A list of input segments\' parts (in the same order as input):\n            [\n                [[x1,y1, x2,y2 ...], ...], # input segment #0 parts\n                mask1, # input segment #1 mask (if source segment is mask)\n                [], # when source segment is too small\n                ...\n            ]\n    """"""\n    from pycocotools import mask as mask_utils\n\n    segments = [[s] for s in segments]\n    input_rles = [mask_utils.frPyObjects(s, height, width) for s in segments]\n\n    for i, rle_bottom in enumerate(input_rles):\n        area_bottom = sum(mask_utils.area(rle_bottom))\n        if area_bottom < area_threshold:\n            segments[i] = [] if not return_masks else None\n            continue\n\n        rles_top = []\n        for j in range(i + 1, len(input_rles)):\n            rle_top = input_rles[j]\n            iou = sum(mask_utils.iou(rle_bottom, rle_top, [0, 0]))[0]\n\n            if iou <= iou_threshold:\n                continue\n\n            area_top = sum(mask_utils.area(rle_top))\n            area_ratio = area_top / area_bottom\n\n            # If a segment is fully inside another one, skip this segment\n            if abs(area_ratio - iou) < ratio_tolerance:\n                continue\n\n            # Check if the bottom segment is fully covered by the top one.\n            # There is a mistake in the annotation, keep the background one\n            if abs(1 / area_ratio - iou) < ratio_tolerance:\n                rles_top = []\n                break\n\n            rles_top += rle_top\n\n        if not rles_top and not isinstance(segments[i][0], dict) \\\n                and not return_masks:\n            continue\n\n        rle_bottom = rle_bottom[0]\n        bottom_mask = mask_utils.decode(rle_bottom).astype(np.uint8)\n\n        if rles_top:\n            rle_top = mask_utils.merge(rles_top)\n            top_mask = mask_utils.decode(rle_top).astype(np.uint8)\n\n            bottom_mask -= top_mask\n            bottom_mask[bottom_mask != 1] = 0\n\n        if not return_masks and not isinstance(segments[i][0], dict):\n            segments[i] = mask_to_polygons(bottom_mask,\n                area_threshold=area_threshold)\n        else:\n            segments[i] = bottom_mask\n\n    return segments\n\ndef rles_to_mask(rles, width, height):\n    from pycocotools import mask as mask_utils\n\n    rles = mask_utils.frPyObjects(rles, height, width)\n    rles = mask_utils.merge(rles)\n    mask = mask_utils.decode(rles)\n    return mask\n\ndef find_mask_bbox(mask):\n    cols = np.any(mask, axis=0)\n    rows = np.any(mask, axis=1)\n    x0, x1 = np.where(cols)[0][[0, -1]]\n    y0, y1 = np.where(rows)[0][[0, -1]]\n    return [x0, y0, x1 - x0, y1 - y0]\n\ndef merge_masks(masks):\n    """"""\n        Merges masks into one, mask order is responsible for z order.\n    """"""\n    if not masks:\n        return None\n\n    merged_mask = masks[0]\n    for m in masks[1:]:\n        merged_mask = np.where(m != 0, m, merged_mask)\n\n    return merged_mask'"
datumaro/datumaro/util/os_util.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport subprocess\n\n\ndef check_instruction_set(instruction):\n    return instruction == str.strip(\n        # Let\'s ignore a warning from bandit about using shell=True.\n        # In this case it isn\'t a security issue and we use some\n        # shell features like pipes.\n        subprocess.check_output(\n            \'lscpu | grep -o ""%s"" | head -1\' % instruction,\n            shell=True).decode(\'utf-8\') # nosec\n    )'"
datumaro/datumaro/util/test_utils.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport inspect\nimport os\nimport os.path as osp\nimport shutil\nimport tempfile\n\nfrom datumaro.components.extractor import AnnotationType\nfrom datumaro.util import find\n\n\ndef current_function_name(depth=1):\n    return inspect.getouterframes(inspect.currentframe())[depth].function\n\nclass FileRemover:\n    def __init__(self, path, is_dir=False, ignore_errors=False):\n        self.path = path\n        self.is_dir = is_dir\n        self.ignore_errors = ignore_errors\n\n    def __enter__(self):\n        return self.path\n\n    # pylint: disable=redefined-builtin\n    def __exit__(self, type=None, value=None, traceback=None):\n        if self.is_dir:\n            shutil.rmtree(self.path, ignore_errors=self.ignore_errors)\n        else:\n            os.remove(self.path)\n    # pylint: enable=redefined-builtin\n\nclass TestDir(FileRemover):\n    def __init__(self, path=None, ignore_errors=False):\n        if path is None:\n            path = osp.abspath('temp_%s-' % current_function_name(2))\n            path = tempfile.mkdtemp(dir=os.getcwd(), prefix=path)\n        else:\n            os.makedirs(path, exist_ok=ignore_errors)\n\n        super().__init__(path, is_dir=True, ignore_errors=ignore_errors)\n\ndef ann_to_str(ann):\n    return vars(ann)\n\ndef item_to_str(item):\n    return '\\n'.join(\n        [\n            '%s' % vars(item)\n        ] + [\n            'ann[%s]: %s' % (i, ann_to_str(a))\n            for i, a in enumerate(item.annotations)\n        ]\n    )\n\ndef compare_categories(test, expected, actual):\n    test.assertEqual(\n        sorted(expected, key=lambda t: t.value),\n        sorted(actual, key=lambda t: t.value)\n    )\n\n    if AnnotationType.label in expected:\n        test.assertEqual(\n            expected[AnnotationType.label].items,\n            actual[AnnotationType.label].items,\n        )\n    if AnnotationType.mask in expected:\n        test.assertEqual(\n            expected[AnnotationType.mask].colormap,\n            actual[AnnotationType.mask].colormap,\n        )\n    if AnnotationType.points in expected:\n        test.assertEqual(\n            expected[AnnotationType.points].items,\n            actual[AnnotationType.points].items,\n        )\n\ndef compare_datasets(test, expected, actual):\n    compare_categories(test, expected.categories(), actual.categories())\n\n    test.assertEqual(sorted(expected.subsets()), sorted(actual.subsets()))\n    test.assertEqual(len(expected), len(actual))\n    for item_a in expected:\n        item_b = find(actual, lambda x: x.id == item_a.id and \\\n            x.subset == item_a.subset)\n        test.assertFalse(item_b is None, item_a.id)\n        test.assertEqual(len(item_a.annotations), len(item_b.annotations))\n        for ann_a in item_a.annotations:\n            # We might find few corresponding items, so check them all\n            ann_b_matches = [x for x in item_b.annotations\n                if x.id == ann_a.id and \\\n                    x.type == ann_a.type and x.group == ann_a.group]\n            test.assertFalse(len(ann_b_matches) == 0, 'ann id: %s' % ann_a.id)\n\n            ann_b = find(ann_b_matches, lambda x: x == ann_a)\n            test.assertEqual(ann_a, ann_b, 'ann: %s' % ann_to_str(ann_a))\n            item_b.annotations.remove(ann_b) # avoid repeats\n\ndef compare_datasets_strict(test, expected, actual):\n    # Compares datasets for strong equality\n\n    test.assertEqual(expected.categories(), actual.categories())\n\n    test.assertListEqual(sorted(expected.subsets()), sorted(actual.subsets()))\n    test.assertEqual(len(expected), len(actual))\n\n    for subset_name in expected.subsets():\n        e_subset = expected.get_subset(subset_name)\n        a_subset = actual.get_subset(subset_name)\n        test.assertEqual(len(e_subset), len(a_subset))\n        for idx, (item_a, item_b) in enumerate(zip(e_subset, a_subset)):\n            test.assertEqual(item_a, item_b,\n                '%s:\\n%s\\nvs.\\n%s\\n' % \\\n                (idx, item_to_str(item_a), item_to_str(item_b)))"""
datumaro/datumaro/util/tf_util.py,4,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\ndef check_import():\n    # Workaround for checking import availability:\n    # Official TF builds include AVX instructions. Once we try to import,\n    # the program crashes. We raise an exception instead.\n\n    import subprocess\n    import sys\n\n    from .os_util import check_instruction_set\n\n    result = subprocess.run([sys.executable, \'-c\', \'import tensorflow\'],\n        timeout=60,\n        universal_newlines=True, # use text mode for output stream\n        stdout=subprocess.PIPE, stderr=subprocess.PIPE) # capture output\n\n    if result.returncode != 0:\n        message = result.stderr\n        if not message:\n            message = ""Can\'t import tensorflow. "" \\\n                ""Test process exit code: %s."" % result.returncode\n            if not check_instruction_set(\'avx\'):\n                # The process has probably crashed for AVX unavalability\n                message += "" This is likely because your CPU does not "" \\\n                    ""support AVX instructions, "" \\\n                    ""which are required for tensorflow.""\n\n        raise ImportError(message)\n\ndef import_tf(check=True):\n    import sys\n\n    tf = sys.modules.get(\'tensorflow\', None)\n    if tf is not None:\n        return tf\n\n    # Reduce output noise, https://stackoverflow.com/questions/38073432/how-to-suppress-verbose-tensorflow-logging\n    import os\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n    if check:\n        check_import()\n\n    import tensorflow as tf\n\n    try:\n        tf.get_logger().setLevel(\'WARNING\')\n    except AttributeError:\n        pass\n    try:\n        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.WARN)\n    except AttributeError:\n        pass\n\n    # Enable eager execution in early versions to unlock dataset operations\n    eager_enabled = False\n    try:\n        tf.compat.v1.enable_eager_execution()\n        eager_enabled = True\n    except AttributeError:\n        pass\n    try:\n        if not eager_enabled:\n            tf.enable_eager_execution()\n    except AttributeError:\n        pass\n\n    return tf\n'"
utils/cli/core/__init__.py,0,"b'# SPDX-License-Identifier: MIT\nfrom .definition import parser, ResourceType  # noqa\nfrom .core import CLI, CVAT_API_V1  # noqa\n'"
utils/cli/core/core.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport json\nimport logging\nimport os\nimport requests\nfrom io import BytesIO\nimport mimetypes\n\nfrom PIL import Image\n\nfrom .definition import ResourceType\nlog = logging.getLogger(__name__)\n\n\nclass CLI():\n\n    def __init__(self, session, api, credentials):\n        self.api = api\n        self.session = session\n        self.login(credentials)\n\n    def tasks_data(self, task_id, resource_type, resources):\n        """""" Add local, remote, or shared files to an existing task. """"""\n        url = self.api.tasks_id_data(task_id)\n        data = {}\n        files = None\n        if resource_type == ResourceType.LOCAL:\n            files = {\'client_files[{}]\'.format(i): open(f, \'rb\') for i, f in enumerate(resources)}\n        elif resource_type == ResourceType.REMOTE:\n            data = {\'remote_files[{}]\'.format(i): f for i, f in enumerate(resources)}\n        elif resource_type == ResourceType.SHARE:\n            data = {\'server_files[{}]\'.format(i): f for i, f in enumerate(resources)}\n        data[\'image_quality\'] = 50\n        response = self.session.post(url, data=data, files=files)\n        response.raise_for_status()\n\n    def tasks_list(self, use_json_output, **kwargs):\n        """""" List all tasks in either basic or JSON format. """"""\n        url = self.api.tasks\n        response = self.session.get(url)\n        response.raise_for_status()\n        page = 1\n        while True:\n            response_json = response.json()\n            for r in response_json[\'results\']:\n                if use_json_output:\n                    log.info(json.dumps(r, indent=4))\n                else:\n                    log.info(\'{id},{name},{status}\'.format(**r))\n            if not response_json[\'next\']:\n                return\n            page += 1\n            url = self.api.tasks_page(page)\n            response = self.session.get(url)\n            response.raise_for_status()\n\n    def tasks_create(self, name, labels, bug, resource_type, resources, **kwargs):\n        """""" Create a new task with the given name and labels JSON and\n        add the files to it. """"""\n        url = self.api.tasks\n        data = {\'name\': name,\n                \'labels\': labels,\n                \'bug_tracker\': bug,\n        }\n        response = self.session.post(url, json=data)\n        response.raise_for_status()\n        response_json = response.json()\n        log.info(\'Created task ID: {id} NAME: {name}\'.format(**response_json))\n        self.tasks_data(response_json[\'id\'], resource_type, resources)\n\n    def tasks_delete(self, task_ids, **kwargs):\n        """""" Delete a list of tasks, ignoring those which don\'t exist. """"""\n        for task_id in task_ids:\n            url = self.api.tasks_id(task_id)\n            response = self.session.delete(url)\n            try:\n                response.raise_for_status()\n                log.info(\'Task ID {} deleted\'.format(task_id))\n            except requests.exceptions.HTTPError as e:\n                if response.status_code == 404:\n                    log.info(\'Task ID {} not found\'.format(task_id))\n                else:\n                    raise e\n\n    def tasks_frame(self, task_id, frame_ids, outdir=\'\', quality=\'original\', **kwargs):\n        """""" Download the requested frame numbers for a task and save images as\n        task_<ID>_frame_<FRAME>.jpg.""""""\n        for frame_id in frame_ids:\n            url = self.api.tasks_id_frame_id(task_id, frame_id, quality)\n            response = self.session.get(url)\n            response.raise_for_status()\n            im = Image.open(BytesIO(response.content))\n            mime_type = im.get_format_mimetype() or \'image/jpg\'\n            im_ext = mimetypes.guess_extension(mime_type)\n            # FIXME It is better to use meta information from the server\n            # to determine the extension\n            # replace \'.jpe\' or \'.jpeg\' with a more used \'.jpg\'\n            if im_ext == \'.jpe\' or \'.jpeg\' or None:\n                im_ext = \'.jpg\'\n\n            outfile = \'task_{}_frame_{:06d}{}\'.format(task_id, frame_id, im_ext)\n            im.save(os.path.join(outdir, outfile))\n\n    def tasks_dump(self, task_id, fileformat, filename, **kwargs):\n        """""" Download annotations for a task in the specified format\n        (e.g. \'YOLO ZIP 1.0\').""""""\n        url = self.api.tasks_id(task_id)\n        response = self.session.get(url)\n        response.raise_for_status()\n        response_json = response.json()\n\n        url = self.api.tasks_id_annotations_filename(task_id,\n                                                     response_json[\'name\'],\n                                                     fileformat)\n        while True:\n            response = self.session.get(url)\n            response.raise_for_status()\n            log.info(\'STATUS {}\'.format(response.status_code))\n            if response.status_code == 201:\n                break\n\n        response = self.session.get(url + \'&action=download\')\n        response.raise_for_status()\n\n        with open(filename, \'wb\') as fp:\n            fp.write(response.content)\n\n    def tasks_upload(self, task_id, fileformat, filename, **kwargs):\n        """""" Upload annotations for a task in the specified format\n        (e.g. \'YOLO ZIP 1.0\').""""""\n        url = self.api.tasks_id_annotations_format(task_id, fileformat)\n        while True:\n            response = self.session.put(\n                url,\n                files={\'annotation_file\':open(filename, \'rb\')}\n                )\n            response.raise_for_status()\n            if response.status_code == 201:\n                break\n\n        logger_string = ""Upload job for Task ID {} "".format(task_id) +\\\n            ""with annotation file {} finished"".format(filename)\n        log.info(logger_string)\n\n    def login(self, credentials):\n        url = self.api.login\n        auth = {\'username\': credentials[0], \'password\': credentials[1]}\n        response = self.session.post(url, auth)\n        response.raise_for_status()\n        if \'csrftoken\' in response.cookies:\n            self.session.headers[\'X-CSRFToken\'] = response.cookies[\'csrftoken\']\n\n\nclass CVAT_API_V1():\n    """""" Build parameterized API URLs """"""\n\n    def __init__(self, host):\n        self.base = \'http://{}/api/v1/\'.format(host)\n\n    @property\n    def tasks(self):\n        return self.base + \'tasks\'\n\n    def tasks_page(self, page_id):\n        return self.tasks + \'?page={}\'.format(page_id)\n\n    def tasks_id(self, task_id):\n        return self.tasks + \'/{}\'.format(task_id)\n\n    def tasks_id_data(self, task_id):\n        return self.tasks_id(task_id) + \'/data\'\n\n    def tasks_id_frame_id(self, task_id, frame_id, quality):\n        return self.tasks_id(task_id) + \'/data?type=frame&number={}&quality={}\'.format(frame_id, quality)\n\n    def tasks_id_annotations_format(self, task_id, fileformat):\n        return self.tasks_id(task_id) + \'/annotations?format={}\' \\\n            .format(fileformat)\n\n    def tasks_id_annotations_filename(self, task_id, name, fileformat):\n        return self.tasks_id(task_id) + \'/annotations?format={}&filename={}\' \\\n            .format(fileformat, name)\n\n    @property\n    def login(self):\n        return self.base + \'auth/login\'\n'"
utils/cli/core/definition.py,0,"b'# SPDX-License-Identifier: MIT\nimport argparse\nimport getpass\nimport json\nimport logging\nimport os\nfrom enum import Enum\n\n\ndef get_auth(s):\n    """""" Parse USER[:PASS] strings and prompt for password if none was\n    supplied. """"""\n    user, _, password = s.partition(\':\')\n    password = password or os.environ.get(\'PASS\') or getpass.getpass()\n    return user, password\n\n\ndef parse_label_arg(s):\n    """""" If s is a file load it as JSON, otherwise parse s as JSON.""""""\n    if os.path.exists(s):\n        fp = open(s, \'r\')\n        return json.load(fp)\n    else:\n        return json.loads(s)\n\n\nclass ResourceType(Enum):\n\n    LOCAL = 0\n    SHARE = 1\n    REMOTE = 2\n\n    def __str__(self):\n        return self.name.lower()\n\n    def __repr__(self):\n        return str(self)\n\n    @staticmethod\n    def argparse(s):\n        try:\n            return ResourceType[s.upper()]\n        except KeyError:\n            return s\n\n\n#######################################################################\n# Command line interface definition\n#######################################################################\n\nparser = argparse.ArgumentParser(\n    description=\'Perform common operations related to CVAT tasks.\\n\\n\'\n)\ntask_subparser = parser.add_subparsers(dest=\'action\')\n\n#######################################################################\n# Positional arguments\n#######################################################################\n\nparser.add_argument(\n    \'--auth\',\n    type=get_auth,\n    metavar=\'USER:[PASS]\',\n    default=getpass.getuser(),\n    help=\'\'\'defaults to the current user and supports the PASS\n            environment variable or password prompt\n            (default user: %(default)s).\'\'\'\n)\nparser.add_argument(\n    \'--server-host\',\n    type=str,\n    default=\'localhost\',\n    help=\'host (default: %(default)s)\'\n)\nparser.add_argument(\n    \'--server-port\',\n    type=int,\n    default=\'8080\',\n    help=\'port (default: %(default)s)\'\n)\nparser.add_argument(\n    \'--debug\',\n    action=\'store_const\',\n    dest=\'loglevel\',\n    const=logging.DEBUG,\n    default=logging.INFO,\n    help=\'show debug output\'\n)\n\n#######################################################################\n# Create\n#######################################################################\n\ntask_create_parser = task_subparser.add_parser(\n    \'create\',\n    description=\'Create a new CVAT task.\'\n)\ntask_create_parser.add_argument(\n    \'name\',\n    type=str,\n    help=\'name of the task\'\n)\ntask_create_parser.add_argument(\n    \'--labels\',\n    default=\'[]\',\n    type=parse_label_arg,\n    help=\'string or file containing JSON labels specification\'\n)\ntask_create_parser.add_argument(\n    \'--bug\',\n    default=\'\',\n    type=str,\n    help=\'bug tracker URL\'\n)\ntask_create_parser.add_argument(\n    \'resource_type\',\n    default=\'local\',\n    choices=list(ResourceType),\n    type=ResourceType.argparse,\n    help=\'type of files specified\'\n)\ntask_create_parser.add_argument(\n    \'resources\',\n    type=str,\n    help=\'list of paths or URLs\',\n    nargs=\'+\'\n)\n\n#######################################################################\n# Delete\n#######################################################################\n\ndelete_parser = task_subparser.add_parser(\n    \'delete\',\n    description=\'Delete a CVAT task.\'\n)\ndelete_parser.add_argument(\n    \'task_ids\',\n    type=int,\n    help=\'list of task IDs\',\n    nargs=\'+\'\n)\n\n#######################################################################\n# List\n#######################################################################\n\nls_parser = task_subparser.add_parser(\n    \'ls\',\n    description=\'List all CVAT tasks in simple or JSON format.\'\n)\nls_parser.add_argument(\n    \'--json\',\n    dest=\'use_json_output\',\n    default=False,\n    action=\'store_true\',\n    help=\'output JSON data\'\n)\n\n#######################################################################\n# Frames\n#######################################################################\n\nframes_parser = task_subparser.add_parser(\n    \'frames\',\n    description=\'Download all frame images for a CVAT task.\'\n)\nframes_parser.add_argument(\n    \'task_id\',\n    type=int,\n    help=\'task ID\'\n)\nframes_parser.add_argument(\n    \'frame_ids\',\n    type=int,\n    help=\'list of frame IDs to download\',\n    nargs=\'+\'\n)\nframes_parser.add_argument(\n    \'--outdir\',\n    type=str,\n    default=\'\',\n    help=\'directory to save images (default: CWD)\'\n)\nframes_parser.add_argument(\n    \'--quality\',\n    type=str,\n    choices=(\'original\', \'compressed\'),\n    default=\'original\',\n    help=\'choose quality of images (default: %(default)s)\'\n)\n\n#######################################################################\n# Dump\n#######################################################################\n\ndump_parser = task_subparser.add_parser(\n    \'dump\',\n    description=\'Download annotations for a CVAT task.\'\n)\ndump_parser.add_argument(\n    \'task_id\',\n    type=int,\n    help=\'task ID\'\n)\ndump_parser.add_argument(\n    \'filename\',\n    type=str,\n    help=\'output file\'\n)\ndump_parser.add_argument(\n    \'--format\',\n    dest=\'fileformat\',\n    type=str,\n    default=\'CVAT for images 1.1\',\n    help=\'annotation format (default: %(default)s)\'\n)\n\n#######################################################################\n# Upload Annotations\n#######################################################################\n\nupload_parser = task_subparser.add_parser(\n    \'upload\',\n    description=\'Upload annotations for a CVAT task.\'\n)\nupload_parser.add_argument(\n    \'task_id\',\n    type=int,\n    help=\'task ID\'\n)\nupload_parser.add_argument(\n    \'filename\',\n    type=str,\n    help=\'upload file\'\n)\nupload_parser.add_argument(\n    \'--format\',\n    dest=\'fileformat\',\n    type=str,\n    default=\'CVAT 1.1\',\n    help=\'annotation format (default: %(default)s)\'\n)\n'"
utils/cli/tests/__init__.py,0,b''
utils/cli/tests/_test_cli.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport io\nimport logging\nimport os\nimport sys\nimport unittest\n\nfrom django.conf import settings\nfrom PIL import Image\nfrom rest_framework.test import APITestCase, RequestsClient\n\nfrom cvat.apps.engine.tests._test_rest_api import (create_db_users,\n    generate_image_file)\nfrom utils.cli.core import CLI, CVAT_API_V1, ResourceType\n\n\nclass TestCLI(APITestCase):\n    @unittest.mock.patch(\'sys.stdout\', new_callable=io.StringIO)\n    def setUp(self, mock_stdout):\n        self.client = RequestsClient()\n        self.credentials = (\'admin\', \'admin\')\n        self.api = CVAT_API_V1(\'testserver\')\n        self.cli = CLI(self.client, self.api, self.credentials)\n        self.taskname = \'test_task\'\n        self.cli.tasks_create(self.taskname,\n                              [{\'name\' : \'car\'}, {\'name\': \'person\'}],\n                              \'\',\n                              ResourceType.LOCAL,\n                              [self.img_file])\n        # redirect logging to mocked stdout to test program output\n        self.mock_stdout = mock_stdout\n        log = logging.getLogger(\'utils.cli.core\')\n        log.setLevel(logging.INFO)\n        log.addHandler(logging.StreamHandler(sys.stdout))\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.img_file = os.path.join(settings.SHARE_ROOT, \'test_cli.jpg\')\n        _, data = generate_image_file(cls.img_file)\n        with open(cls.img_file, \'wb\') as image:\n            image.write(data.read())\n\n    @classmethod\n    def tearDownClass(cls):\n        super().tearDownClass()\n        os.remove(cls.img_file)\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n\n    def test_tasks_list(self):\n        self.cli.tasks_list(False)\n        self.assertRegex(self.mock_stdout.getvalue(), \'.*{}.*\'.format(self.taskname))\n\n    def test_tasks_delete(self):\n        self.cli.tasks_delete([1])\n        self.cli.tasks_list(False)\n        self.assertNotRegex(self.mock_stdout.getvalue(), \'.*{}.*\'.format(self.taskname))\n\n    def test_tasks_dump(self):\n        path = os.path.join(settings.SHARE_ROOT, \'test_cli.xml\')\n        self.cli.tasks_dump(1, \'CVAT for images 1.1\', path)\n        self.assertTrue(os.path.exists(path))\n        os.remove(path)\n\n    def test_tasks_frame_original(self):\n        path = os.path.join(settings.SHARE_ROOT, \'task_1_frame_000000.jpg\')\n        self.cli.tasks_frame(1, [0], outdir=settings.SHARE_ROOT, quality=\'original\')\n        self.assertTrue(os.path.exists(path))\n        os.remove(path)\n\n    def test_tasks_frame(self):\n        path = os.path.join(settings.SHARE_ROOT, \'task_1_frame_000000.jpg\')\n        self.cli.tasks_frame(1, [0], outdir=settings.SHARE_ROOT, quality=\'compressed\')\n        self.assertTrue(os.path.exists(path))\n        os.remove(path)\n\n    def test_tasks_upload(self):\n        test_image = Image.open(self.img_file)\n        width, height = test_image.size\n\n        # Using generate_coco_anno() from:\n        # https://github.com/opencv/cvat/blob/develop/cvat/apps/engine/tests/test_rest_api.py\n        def generate_coco_anno():\n            return b""""""{\n            ""categories"": [\n                {\n                ""id"": 1,\n                ""name"": ""car"",\n                ""supercategory"": """"\n                },\n                {\n                ""id"": 2,\n                ""name"": ""person"",\n                ""supercategory"": """"\n                }\n            ],\n            ""images"": [\n                {\n                ""coco_url"": """",\n                ""date_captured"": """",\n                ""flickr_url"": """",\n                ""license"": 0,\n                ""id"": 0,\n                ""file_name"": ""test_cli.jpg"",\n                ""height"": %d,\n                ""width"": %d\n                }\n            ],\n            ""annotations"": [\n                {\n                ""category_id"": 1,\n                ""id"": 1,\n                ""image_id"": 0,\n                ""iscrowd"": 0,\n                ""segmentation"": [\n                    []\n                ],\n                ""area"": 17702.0,\n                ""bbox"": [\n                    574.0,\n                    407.0,\n                    167.0,\n                    106.0\n                ]\n                }\n            ]\n            }""""""\n        content = generate_coco_anno() % (height, width)\n        path = os.path.join(settings.SHARE_ROOT, \'test_cli.json\')\n        with open(path, ""wb"") as coco:\n            coco.write(content)\n        self.cli.tasks_upload(1, \'COCO 1.0\', path)\n        self.assertRegex(self.mock_stdout.getvalue(), \'.*{}.*\'.format(""annotation file""))\n        os.remove(path)\n'"
utils/open_model_zoo/faster_rcnn_inception_v2_coco/interp.py,0,"b""threshold = .5\n\nfor detection in detections:\n    frame_number = detection['frame_id']\n    height = detection['frame_height']\n    width = detection['frame_width']\n    detection = detection['detections']\n\n    prediction = detection[0][0]\n    for obj in prediction:\n        obj_class = int(obj[1])\n        obj_value = obj[2]\n        if obj_value >= threshold:\n            x = obj[3] * width\n            y = obj[4] * height\n            right = obj[5] * width\n            bottom = obj[6] * height\n\n            results.add_box(x, y, right, bottom, obj_class, frame_number)\n"""
utils/open_model_zoo/mask_rcnn_inception_resnet_v2_atrous_coco/interp.py,0,"b""import numpy as np\nimport cv2\nfrom skimage.measure import approximate_polygon, find_contours\n\n\nMASK_THRESHOLD = .5\nPROBABILITY_THRESHOLD = 0.2\n\n\n# Ref: https://software.intel.com/en-us/forums/computer-vision/topic/804895\ndef segm_postprocess(box: list, raw_cls_mask, im_h, im_w, threshold):\n    ymin, xmin, ymax, xmax = box\n\n    width = int(abs(xmax - xmin))\n    height = int(abs(ymax - ymin))\n\n    result = np.zeros((im_h, im_w), dtype=np.uint8)\n    resized_mask = cv2.resize(raw_cls_mask, dsize=(height, width), interpolation=cv2.INTER_CUBIC)\n\n    # extract the ROI of the image\n    ymin = int(round(ymin))\n    xmin = int(round(xmin))\n    ymax = ymin + height\n    xmax = xmin + width\n    result[xmin:xmax, ymin:ymax] = (resized_mask>threshold).astype(np.uint8) * 255\n\n    return result\n\n\nfor detection in detections:\n    frame_number = detection['frame_id']\n    height = detection['frame_height']\n    width = detection['frame_width']\n    detection = detection['detections']\n\n    masks = detection['masks']\n    boxes = detection['reshape_do_2d']\n\n    for index, box in enumerate(boxes):\n        label = int(box[1])\n        obj_value = box[2]\n        if obj_value >= PROBABILITY_THRESHOLD:\n            x = box[3] * width\n            y = box[4] * height\n            right = box[5] * width\n            bottom = box[6] * height\n            mask = masks[index][label - 1]\n\n            mask = segm_postprocess((x, y, right, bottom),\n                                    mask,\n                                    height,\n                                    width,\n                                    MASK_THRESHOLD)\n\n            contours = find_contours(mask, MASK_THRESHOLD)\n            contour = contours[0]\n            contour = np.flip(contour, axis=1)\n            contour = approximate_polygon(contour, tolerance=2.5)\n            segmentation = contour.tolist()\n\n\n            # NOTE: if you want to see the boxes, uncomment next line\n            # results.add_box(x, y, right, bottom, label, frame_number)\n            results.add_polygon(segmentation, label, frame_number)\n"""
utils/open_model_zoo/yolov3/interp.py,0,"b'from math import exp\n\n\nclass Parser:\n    IOU_THRESHOLD = 0.4\n    PROB_THRESHOLD = 0.5\n\n    def __init__(self):\n        self.objects = []\n\n    def scale_bbox(self, x, y, h, w, class_id, confidence, h_scale, w_scale):\n        xmin = int((x - w / 2) * w_scale)\n        ymin = int((y - h / 2) * h_scale)\n        xmax = int(xmin + w * w_scale)\n        ymax = int(ymin + h * h_scale)\n\n        return dict(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, class_id=class_id, confidence=confidence)\n\n    def entry_index(self, side, coord, classes, location, entry):\n        side_power_2 = side ** 2\n        n = location // side_power_2\n        loc = location % side_power_2\n        return int(side_power_2 * (n * (coord + classes + 1) + entry) + loc)\n\n    def intersection_over_union(self, box_1, box_2):\n        width_of_overlap_area = min(box_1[\'xmax\'], box_2[\'xmax\']) - max(box_1[\'xmin\'], box_2[\'xmin\'])\n        height_of_overlap_area = min(box_1[\'ymax\'], box_2[\'ymax\']) - max(box_1[\'ymin\'], box_2[\'ymin\'])\n        if width_of_overlap_area < 0 or height_of_overlap_area < 0:\n            area_of_overlap = 0\n        else:\n            area_of_overlap = width_of_overlap_area * height_of_overlap_area\n        box_1_area = (box_1[\'ymax\'] - box_1[\'ymin\']) * (box_1[\'xmax\'] - box_1[\'xmin\'])\n        box_2_area = (box_2[\'ymax\'] - box_2[\'ymin\']) * (box_2[\'xmax\'] - box_2[\'xmin\'])\n        area_of_union = box_1_area + box_2_area - area_of_overlap\n        if area_of_union == 0:\n            return 0\n        return area_of_overlap / area_of_union\n\n\n    def sort_objects(self):\n        self.objects = sorted(self.objects, key=lambda obj : obj[\'confidence\'], reverse=True)\n\n        for i in range(len(self.objects)):\n            if self.objects[i][\'confidence\'] == 0:\n                continue\n            for j in range(i + 1, len(self.objects)):\n                if self.intersection_over_union(self.objects[i], self.objects[j]) > self.IOU_THRESHOLD:\n                    self.objects[j][\'confidence\'] = 0\n\n    def parse_yolo_region(self, blob: \'np.ndarray\', original_shape: list, params: dict) -> list:\n\n        # YOLO magic numbers\n        # See: https://github.com/opencv/open_model_zoo/blob/acf297c73db8cb3f68791ae1fad4a7cc4a6039e5/demos/python_demos/object_detection_demo_yolov3_async/object_detection_demo_yolov3_async.py#L61\n        num = 3\n        coords = 4\n        classes = 80\n        # -----------------\n\n        _, _, out_blob_h, out_blob_w = blob.shape\n        assert out_blob_w == out_blob_h, ""Invalid size of output blob. It sould be in NCHW layout and height should "" \\\n                                         ""be equal to width. Current height = {}, current width = {}"" \\\n                                         """".format(out_blob_h, out_blob_w)\n\n        # ------ Extracting layer parameters --\n        orig_im_h, orig_im_w = original_shape\n        predictions = blob.flatten()\n        side_square = params[\'side\'] * params[\'side\']\n\n        # ------ Parsing YOLO Region output --\n        for i in range(side_square):\n            row = i // params[\'side\']\n            col = i % params[\'side\']\n            for n in range(num):\n                # -----entry index calcs------\n                obj_index = self.entry_index(params[\'side\'], coords, classes, n * side_square + i, coords)\n                scale = predictions[obj_index]\n                if scale < self.PROB_THRESHOLD:\n                    continue\n                box_index = self.entry_index(params[\'side\'], coords, classes, n * side_square + i, 0)\n\n                # Network produces location predictions in absolute coordinates of feature maps.\n                # Scale it to relative coordinates.\n                x = (col + predictions[box_index + 0 * side_square]) / params[\'side\'] * 416\n                y = (row + predictions[box_index + 1 * side_square]) / params[\'side\'] * 416\n                # Value for exp is very big number in some cases so following construction is using here\n                try:\n                    h_exp = exp(predictions[box_index + 3 * side_square])\n                    w_exp = exp(predictions[box_index + 2 * side_square])\n                except OverflowError:\n                    continue\n\n                w = w_exp * params[\'anchors\'][2 * n]\n                h = h_exp * params[\'anchors\'][2 * n + 1]\n\n                for j in range(classes):\n                    class_index = self.entry_index(params[\'side\'], coords, classes, n * side_square + i,\n                                              coords + 1 + j)\n                    confidence = scale * predictions[class_index]\n                    if confidence < self.PROB_THRESHOLD:\n                        continue\n\n                    self.objects.append(self.scale_bbox(x=x,\n                                                        y=y,\n                                                        h=h,\n                                                        w=w,\n                                                        class_id=j,\n                                                        confidence=confidence,\n                                                        h_scale=(orig_im_h/416),\n                                                        w_scale=(orig_im_w/416)))\n\n\nfor detection in detections:\n    frame_number = detection[\'frame_id\']\n    height = detection[\'frame_height\']\n    width = detection[\'frame_width\']\n    detection = detection[\'detections\']\n\n    original_shape = (height, width)\n\n    # https://github.com/opencv/open_model_zoo/blob/master/demos/python_demos/object_detection_demo_yolov3_async/object_detection_demo_yolov3_async.py#L72\n    anchors = [10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326]\n    conv_6 = {\'side\': 13, \'mask\': [6,7,8]}\n    conv_14 = {\'side\': 26, \'mask\': [3,4,5]}\n    conv_22 = {\'side\': 52, \'mask\': [0,1,2]}\n\n    yolo_params = {\'detector/yolo-v3/Conv_6/BiasAdd/YoloRegion\': conv_6,\n                   \'detector/yolo-v3/Conv_14/BiasAdd/YoloRegion\': conv_14,\n                   \'detector/yolo-v3/Conv_22/BiasAdd/YoloRegion\': conv_22}\n\n    for conv_net in yolo_params.values():\n        mask = conv_net[\'mask\']\n        masked_anchors = []\n        for idx in mask:\n            masked_anchors += [anchors[idx * 2], anchors[idx * 2 + 1]]\n\n        conv_net[\'anchors\'] = masked_anchors\n\n    parser = Parser()\n\n    for name, blob in detection.items():\n        parser.parse_yolo_region(blob, original_shape, yolo_params[name])\n\n    parser.sort_objects()\n\n    objects = []\n    for obj in parser.objects:\n        if obj[\'confidence\'] >= parser.PROB_THRESHOLD:\n            label = obj[\'class_id\']\n            xmin = obj[\'xmin\']\n            xmax = obj[\'xmax\']\n            ymin = obj[\'ymin\']\n            ymax = obj[\'ymax\']\n\n            # Enforcing extra checks for bounding box coordinates\n            xmin = max(0,xmin)\n            ymin = max(0,ymin)\n            xmax = min(xmax,width)\n            ymax = min(ymax,height)\n\n            results.add_box(xmin, ymin, xmax, ymax, label, frame_number)\n'"
cvat/apps/authentication/migrations/__init__.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n'
cvat/apps/auto_annotation/migrations/0001_initial.py,0,"b""# Generated by Django 2.1.3 on 2019-01-24 14:05\n\nimport cvat.apps.auto_annotation.models\nfrom django.conf import settings\nimport django.core.files.storage\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='AnnotationModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', cvat.apps.auto_annotation.models.SafeCharField(max_length=256)),\n                ('created_date', models.DateTimeField(auto_now_add=True)),\n                ('updated_date', models.DateTimeField(auto_now_add=True)),\n                ('model_file', models.FileField(storage=django.core.files.storage.FileSystemStorage(), upload_to=cvat.apps.auto_annotation.models.upload_path_handler)),\n                ('weights_file', models.FileField(storage=django.core.files.storage.FileSystemStorage(), upload_to=cvat.apps.auto_annotation.models.upload_path_handler)),\n                ('labelmap_file', models.FileField(storage=django.core.files.storage.FileSystemStorage(), upload_to=cvat.apps.auto_annotation.models.upload_path_handler)),\n                ('interpretation_file', models.FileField(storage=django.core.files.storage.FileSystemStorage(), upload_to=cvat.apps.auto_annotation.models.upload_path_handler)),\n                ('shared', models.BooleanField(default=False)),\n                ('primary', models.BooleanField(default=False)),\n                ('framework', models.CharField(default=cvat.apps.auto_annotation.models.FrameworkChoice('openvino'), max_length=32)),\n                ('owner', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL)),\n            ],\n            options={\n                'default_permissions': (),\n            },\n        ),\n    ]\n"""
cvat/apps/auto_annotation/migrations/__init__.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n'
cvat/apps/auto_segmentation/migrations/__init__.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n'
cvat/apps/dataset_manager/formats/coco.py,0,"b""# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport zipfile\nfrom tempfile import TemporaryDirectory\n\nfrom datumaro.components.project import Dataset\nfrom cvat.apps.dataset_manager.bindings import CvatTaskDataExtractor, \\\n    import_dm_annotations\nfrom cvat.apps.dataset_manager.util import make_zip_archive\n\nfrom .registry import dm_env, exporter, importer\n\n\n@exporter(name='COCO', ext='ZIP', version='1.0')\ndef _export(dst_file, task_data, save_images=False):\n    extractor = CvatTaskDataExtractor(task_data, include_images=save_images)\n    extractor = Dataset.from_extractors(extractor) # apply lazy transforms\n    with TemporaryDirectory() as temp_dir:\n        converter = dm_env.make_converter('coco_instances',\n            save_images=save_images)\n        converter(extractor, save_dir=temp_dir)\n\n        make_zip_archive(temp_dir, dst_file)\n\n@importer(name='COCO', ext='JSON, ZIP', version='1.0')\ndef _import(src_file, task_data):\n    if zipfile.is_zipfile(src_file):\n        with TemporaryDirectory() as tmp_dir:\n            zipfile.ZipFile(src_file).extractall(tmp_dir)\n\n            dataset = dm_env.make_importer('coco')(tmp_dir).make_dataset()\n            import_dm_annotations(dataset, task_data)\n    else:\n        dataset = dm_env.make_extractor('coco_instances', src_file.name)\n        import_dm_annotations(dataset, task_data)"""
cvat/apps/dataset_manager/formats/cvat.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport os.path as osp\nimport zipfile\nfrom collections import OrderedDict\nfrom glob import glob\nfrom tempfile import TemporaryDirectory\n\nfrom cvat.apps.dataset_manager.util import make_zip_archive\nfrom cvat.apps.engine.frame_provider import FrameProvider\nfrom datumaro.util.image import save_image\n\nfrom .registry import exporter, importer\n\n\ndef pairwise(iterable):\n    a = iter(iterable)\n    return zip(a, a)\n\ndef create_xml_dumper(file_object):\n    from xml.sax.saxutils import XMLGenerator\n    class XmlAnnotationWriter:\n        def __init__(self, file):\n            self.version = ""1.1""\n            self.file = file\n            self.xmlgen = XMLGenerator(self.file, \'utf-8\')\n            self._level = 0\n\n        def _indent(self, newline = True):\n            if newline:\n                self.xmlgen.ignorableWhitespace(""\\n"")\n            self.xmlgen.ignorableWhitespace(""  "" * self._level)\n\n        def _add_version(self):\n            self._indent()\n            self.xmlgen.startElement(""version"", {})\n            self.xmlgen.characters(self.version)\n            self.xmlgen.endElement(""version"")\n\n        def open_root(self):\n            self.xmlgen.startDocument()\n            self.xmlgen.startElement(""annotations"", {})\n            self._level += 1\n            self._add_version()\n\n        def _add_meta(self, meta):\n            self._level += 1\n            for k, v in meta.items():\n                if isinstance(v, OrderedDict):\n                    self._indent()\n                    self.xmlgen.startElement(k, {})\n                    self._add_meta(v)\n                    self._indent()\n                    self.xmlgen.endElement(k)\n                elif isinstance(v, list):\n                    self._indent()\n                    self.xmlgen.startElement(k, {})\n                    for tup in v:\n                        self._add_meta(OrderedDict([tup]))\n                    self._indent()\n                    self.xmlgen.endElement(k)\n                else:\n                    self._indent()\n                    self.xmlgen.startElement(k, {})\n                    self.xmlgen.characters(v)\n                    self.xmlgen.endElement(k)\n            self._level -= 1\n\n        def add_meta(self, meta):\n            self._indent()\n            self.xmlgen.startElement(""meta"", {})\n            self._add_meta(meta)\n            self._indent()\n            self.xmlgen.endElement(""meta"")\n\n        def open_track(self, track):\n            self._indent()\n            self.xmlgen.startElement(""track"", track)\n            self._level += 1\n\n        def open_image(self, image):\n            self._indent()\n            self.xmlgen.startElement(""image"", image)\n            self._level += 1\n\n        def open_box(self, box):\n            self._indent()\n            self.xmlgen.startElement(""box"", box)\n            self._level += 1\n\n        def open_polygon(self, polygon):\n            self._indent()\n            self.xmlgen.startElement(""polygon"", polygon)\n            self._level += 1\n\n        def open_polyline(self, polyline):\n            self._indent()\n            self.xmlgen.startElement(""polyline"", polyline)\n            self._level += 1\n\n        def open_points(self, points):\n            self._indent()\n            self.xmlgen.startElement(""points"", points)\n            self._level += 1\n\n        def open_cuboid(self, cuboid):\n            self._indent()\n            self.xmlgen.startElement(""cuboid"", cuboid)\n            self._level += 1\n\n        def open_tag(self, tag):\n            self._indent()\n            self.xmlgen.startElement(""tag"", tag)\n            self._level += 1\n\n        def add_attribute(self, attribute):\n            self._indent()\n            self.xmlgen.startElement(""attribute"", {""name"": attribute[""name""]})\n            self.xmlgen.characters(attribute[""value""])\n            self.xmlgen.endElement(""attribute"")\n\n        def close_box(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""box"")\n\n        def close_polygon(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""polygon"")\n\n        def close_polyline(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""polyline"")\n\n        def close_points(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""points"")\n\n        def close_cuboid(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""cuboid"")\n\n        def close_tag(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""tag"")\n\n        def close_image(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""image"")\n\n        def close_track(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""track"")\n\n        def close_root(self):\n            self._level -= 1\n            self._indent()\n            self.xmlgen.endElement(""annotations"")\n            self.xmlgen.endDocument()\n\n    return XmlAnnotationWriter(file_object)\n\ndef dump_as_cvat_annotation(file_object, annotations):\n    dumper = create_xml_dumper(file_object)\n    dumper.open_root()\n    dumper.add_meta(annotations.meta)\n\n    for frame_annotation in annotations.group_by_frame(include_empty=True):\n        frame_id = frame_annotation.frame\n        dumper.open_image(OrderedDict([\n            (""id"", str(frame_id)),\n            (""name"", frame_annotation.name),\n            (""width"", str(frame_annotation.width)),\n            (""height"", str(frame_annotation.height))\n        ]))\n\n        for shape in frame_annotation.labeled_shapes:\n            dump_data = OrderedDict([\n                (""label"", shape.label),\n                (""occluded"", str(int(shape.occluded))),\n            ])\n\n            if shape.type == ""rectangle"":\n                dump_data.update(OrderedDict([\n                    (""xtl"", ""{:.2f}"".format(shape.points[0])),\n                    (""ytl"", ""{:.2f}"".format(shape.points[1])),\n                    (""xbr"", ""{:.2f}"".format(shape.points[2])),\n                    (""ybr"", ""{:.2f}"".format(shape.points[3]))\n                ]))\n            elif shape.type == ""cuboid"":\n                dump_data.update(OrderedDict([\n                    (""xtl1"", ""{:.2f}"".format(shape.points[0])),\n                    (""ytl1"", ""{:.2f}"".format(shape.points[1])),\n                    (""xbl1"", ""{:.2f}"".format(shape.points[2])),\n                    (""ybl1"", ""{:.2f}"".format(shape.points[3])),\n                    (""xtr1"", ""{:.2f}"".format(shape.points[4])),\n                    (""ytr1"", ""{:.2f}"".format(shape.points[5])),\n                    (""xbr1"", ""{:.2f}"".format(shape.points[6])),\n                    (""ybr1"", ""{:.2f}"".format(shape.points[7])),\n                    (""xtl2"", ""{:.2f}"".format(shape.points[8])),\n                    (""ytl2"", ""{:.2f}"".format(shape.points[9])),\n                    (""xbl2"", ""{:.2f}"".format(shape.points[10])),\n                    (""ybl2"", ""{:.2f}"".format(shape.points[11])),\n                    (""xtr2"", ""{:.2f}"".format(shape.points[12])),\n                    (""ytr2"", ""{:.2f}"".format(shape.points[13])),\n                    (""xbr2"", ""{:.2f}"".format(shape.points[14])),\n                    (""ybr2"", ""{:.2f}"".format(shape.points[15]))\n                ]))\n            else:\n                dump_data.update(OrderedDict([\n                    (""points"", \';\'.join((\n                        \',\'.join((\n                            ""{:.2f}"".format(x),\n                            ""{:.2f}"".format(y)\n                        )) for x, y in pairwise(shape.points))\n                    )),\n                ]))\n\n            if annotations.meta[""task""][""z_order""] != ""False"":\n                dump_data[\'z_order\'] = str(shape.z_order)\n            if shape.group:\n                dump_data[\'group_id\'] = str(shape.group)\n\n\n            if shape.type == ""rectangle"":\n                dumper.open_box(dump_data)\n            elif shape.type == ""polygon"":\n                dumper.open_polygon(dump_data)\n            elif shape.type == ""polyline"":\n                dumper.open_polyline(dump_data)\n            elif shape.type == ""points"":\n                dumper.open_points(dump_data)\n            elif shape.type == ""cuboid"":\n                dumper.open_cuboid(dump_data)\n            else:\n                raise NotImplementedError(""unknown shape type"")\n\n            for attr in shape.attributes:\n                dumper.add_attribute(OrderedDict([\n                    (""name"", attr.name),\n                    (""value"", attr.value)\n                ]))\n\n            if shape.type == ""rectangle"":\n                dumper.close_box()\n            elif shape.type == ""polygon"":\n                dumper.close_polygon()\n            elif shape.type == ""polyline"":\n                dumper.close_polyline()\n            elif shape.type == ""points"":\n                dumper.close_points()\n            elif shape.type == ""cuboid"":\n                dumper.close_cuboid()\n            else:\n                raise NotImplementedError(""unknown shape type"")\n\n        for tag in frame_annotation.tags:\n            tag_data = OrderedDict([\n                (""label"", tag.label),\n            ])\n            if tag.group:\n                tag_data[""group_id""] = str(tag.group)\n            dumper.open_tag(tag_data)\n\n            for attr in tag.attributes:\n                dumper.add_attribute(OrderedDict([\n                    (""name"", attr.name),\n                    (""value"", attr.value)\n                ]))\n\n            dumper.close_tag()\n\n        dumper.close_image()\n    dumper.close_root()\n\ndef dump_as_cvat_interpolation(file_object, annotations):\n    dumper = create_xml_dumper(file_object)\n    dumper.open_root()\n    dumper.add_meta(annotations.meta)\n    def dump_track(idx, track):\n        track_id = idx\n        dump_data = OrderedDict([\n            (""id"", str(track_id)),\n            (""label"", track.label),\n        ])\n\n        if track.group:\n            dump_data[\'group_id\'] = str(track.group)\n        dumper.open_track(dump_data)\n\n        for shape in track.shapes:\n            dump_data = OrderedDict([\n                (""frame"", str(shape.frame)),\n                (""outside"", str(int(shape.outside))),\n                (""occluded"", str(int(shape.occluded))),\n                (""keyframe"", str(int(shape.keyframe))),\n            ])\n\n            if shape.type == ""rectangle"":\n                dump_data.update(OrderedDict([\n                    (""xtl"", ""{:.2f}"".format(shape.points[0])),\n                    (""ytl"", ""{:.2f}"".format(shape.points[1])),\n                    (""xbr"", ""{:.2f}"".format(shape.points[2])),\n                    (""ybr"", ""{:.2f}"".format(shape.points[3])),\n                ]))\n            elif shape.type == ""cuboid"":\n                dump_data.update(OrderedDict([\n                    (""xtl1"", ""{:.2f}"".format(shape.points[0])),\n                    (""ytl1"", ""{:.2f}"".format(shape.points[1])),\n                    (""xbl1"", ""{:.2f}"".format(shape.points[2])),\n                    (""ybl1"", ""{:.2f}"".format(shape.points[3])),\n                    (""xtr1"", ""{:.2f}"".format(shape.points[4])),\n                    (""ytr1"", ""{:.2f}"".format(shape.points[5])),\n                    (""xbr1"", ""{:.2f}"".format(shape.points[6])),\n                    (""ybr1"", ""{:.2f}"".format(shape.points[7])),\n                    (""xtl2"", ""{:.2f}"".format(shape.points[8])),\n                    (""ytl2"", ""{:.2f}"".format(shape.points[9])),\n                    (""xbl2"", ""{:.2f}"".format(shape.points[10])),\n                    (""ybl2"", ""{:.2f}"".format(shape.points[11])),\n                    (""xtr2"", ""{:.2f}"".format(shape.points[12])),\n                    (""ytr2"", ""{:.2f}"".format(shape.points[13])),\n                    (""xbr2"", ""{:.2f}"".format(shape.points[14])),\n                    (""ybr2"", ""{:.2f}"".format(shape.points[15]))\n                ]))\n            else:\n                dump_data.update(OrderedDict([\n                    (""points"", \';\'.join([\'{:.2f},{:.2f}\'.format(x, y)\n                        for x,y in pairwise(shape.points)]))\n                ]))\n\n            if annotations.meta[""task""][""z_order""] != ""False"":\n                dump_data[""z_order""] = str(shape.z_order)\n\n            if shape.type == ""rectangle"":\n                dumper.open_box(dump_data)\n            elif shape.type == ""polygon"":\n                dumper.open_polygon(dump_data)\n            elif shape.type == ""polyline"":\n                dumper.open_polyline(dump_data)\n            elif shape.type == ""points"":\n                dumper.open_points(dump_data)\n            elif shape.type == ""cuboid"":\n                dumper.open_cuboid(dump_data)\n            else:\n                raise NotImplementedError(""unknown shape type"")\n\n            for attr in shape.attributes:\n                dumper.add_attribute(OrderedDict([\n                    (""name"", attr.name),\n                    (""value"", attr.value)\n                ]))\n\n            if shape.type == ""rectangle"":\n                dumper.close_box()\n            elif shape.type == ""polygon"":\n                dumper.close_polygon()\n            elif shape.type == ""polyline"":\n                dumper.close_polyline()\n            elif shape.type == ""points"":\n                dumper.close_points()\n            elif shape.type == ""cuboid"":\n                dumper.close_cuboid()\n            else:\n                raise NotImplementedError(""unknown shape type"")\n        dumper.close_track()\n\n    counter = 0\n    for track in annotations.tracks:\n        dump_track(counter, track)\n        counter += 1\n\n    for shape in annotations.shapes:\n        dump_track(counter, annotations.Track(\n            label=shape.label,\n            group=shape.group,\n            shapes=[annotations.TrackedShape(\n                type=shape.type,\n                points=shape.points,\n                occluded=shape.occluded,\n                outside=False,\n                keyframe=True,\n                z_order=shape.z_order,\n                frame=shape.frame,\n                attributes=shape.attributes,\n            ),\n            annotations.TrackedShape(\n                type=shape.type,\n                points=shape.points,\n                occluded=shape.occluded,\n                outside=True,\n                keyframe=True,\n                z_order=shape.z_order,\n                frame=shape.frame + annotations.frame_step,\n                attributes=shape.attributes,\n            ),\n            ],\n        ))\n        counter += 1\n\n    dumper.close_root()\n\ndef load(file_object, annotations):\n    from defusedxml import ElementTree\n    context = ElementTree.iterparse(file_object, events=(""start"", ""end""))\n    context = iter(context)\n    ev, _ = next(context)\n\n    supported_shapes = (\'box\', \'polygon\', \'polyline\', \'points\', \'cuboid\')\n\n    track = None\n    shape = None\n    tag = None\n    image_is_opened = False\n    attributes = None\n    for ev, el in context:\n        if ev == \'start\':\n            if el.tag == \'track\':\n                track = annotations.Track(\n                    label=el.attrib[\'label\'],\n                    group=int(el.attrib.get(\'group_id\', 0)),\n                    shapes=[],\n                )\n            elif el.tag == \'image\':\n                image_is_opened = True\n                frame_id = int(el.attrib[\'id\'])\n            elif el.tag in supported_shapes and (track is not None or image_is_opened):\n                attributes = []\n                shape = {\n                    \'attributes\': attributes,\n                    \'points\': [],\n                }\n            elif el.tag == \'tag\' and image_is_opened:\n                attributes = []\n                tag = {\n                    \'frame\': frame_id,\n                    \'label\': el.attrib[\'label\'],\n                    \'group\': int(el.attrib.get(\'group_id\', 0)),\n                    \'attributes\': attributes,\n                }\n        elif ev == \'end\':\n            if el.tag == \'attribute\' and attributes is not None:\n                attributes.append(annotations.Attribute(\n                    name=el.attrib[\'name\'],\n                    value=el.text or """",\n                ))\n            if el.tag in supported_shapes:\n                if track is not None:\n                    shape[\'frame\'] = el.attrib[\'frame\']\n                    shape[\'outside\'] = el.attrib[\'outside\'] == ""1""\n                    shape[\'keyframe\'] = el.attrib[\'keyframe\'] == ""1""\n                else:\n                    shape[\'frame\'] = frame_id\n                    shape[\'label\'] = el.attrib[\'label\']\n                    shape[\'group\'] = int(el.attrib.get(\'group_id\', 0))\n\n                shape[\'type\'] = \'rectangle\' if el.tag == \'box\' else el.tag\n                shape[\'occluded\'] = el.attrib[\'occluded\'] == \'1\'\n                shape[\'z_order\'] = int(el.attrib.get(\'z_order\', 0))\n\n                if el.tag == \'box\':\n                    shape[\'points\'].append(el.attrib[\'xtl\'])\n                    shape[\'points\'].append(el.attrib[\'ytl\'])\n                    shape[\'points\'].append(el.attrib[\'xbr\'])\n                    shape[\'points\'].append(el.attrib[\'ybr\'])\n                elif el.tag == \'cuboid\':\n                    shape[\'points\'].append(el.attrib[\'xtl1\'])\n                    shape[\'points\'].append(el.attrib[\'ytl1\'])\n                    shape[\'points\'].append(el.attrib[\'xbl1\'])\n                    shape[\'points\'].append(el.attrib[\'ybl1\'])\n                    shape[\'points\'].append(el.attrib[\'xtr1\'])\n                    shape[\'points\'].append(el.attrib[\'ytr1\'])\n                    shape[\'points\'].append(el.attrib[\'xbr1\'])\n                    shape[\'points\'].append(el.attrib[\'ybr1\'])\n\n                    shape[\'points\'].append(el.attrib[\'xtl2\'])\n                    shape[\'points\'].append(el.attrib[\'ytl2\'])\n                    shape[\'points\'].append(el.attrib[\'xbl2\'])\n                    shape[\'points\'].append(el.attrib[\'ybl2\'])\n                    shape[\'points\'].append(el.attrib[\'xtr2\'])\n                    shape[\'points\'].append(el.attrib[\'ytr2\'])\n                    shape[\'points\'].append(el.attrib[\'xbr2\'])\n                    shape[\'points\'].append(el.attrib[\'ybr2\'])\n                else:\n                    for pair in el.attrib[\'points\'].split(\';\'):\n                        shape[\'points\'].extend(map(float, pair.split(\',\')))\n\n                if track is not None:\n                    if shape[""keyframe""]:\n                        track.shapes.append(annotations.TrackedShape(**shape))\n                else:\n                    annotations.add_shape(annotations.LabeledShape(**shape))\n                shape = None\n\n            elif el.tag == \'track\':\n                annotations.add_track(track)\n                track = None\n            elif el.tag == \'image\':\n                image_is_opened = False\n            elif el.tag == \'tag\':\n                annotations.add_tag(annotations.Tag(**tag))\n                tag = None\n            el.clear()\n\ndef _export(dst_file, task_data, anno_callback, save_images=False):\n    with TemporaryDirectory() as temp_dir:\n        with open(osp.join(temp_dir, \'annotations.xml\'), \'wb\') as f:\n            anno_callback(f, task_data)\n\n        if save_images:\n            img_dir = osp.join(temp_dir, \'images\')\n            frame_provider = FrameProvider(task_data.db_task.data)\n            frames = frame_provider.get_frames(\n                frame_provider.Quality.ORIGINAL,\n                frame_provider.Type.NUMPY_ARRAY)\n            for frame_id, (frame_data, _) in enumerate(frames):\n                frame_name = task_data.frame_info[frame_id][\'path\']\n                if \'.\' in frame_name:\n                    save_image(osp.join(img_dir, frame_name),\n                        frame_data, jpeg_quality=100, create_dir=True)\n                else:\n                    save_image(osp.join(img_dir, frame_name + \'.png\'),\n                        frame_data, create_dir=True)\n\n        make_zip_archive(temp_dir, dst_file)\n\n@exporter(name=\'CVAT for video\', ext=\'ZIP\', version=\'1.1\')\ndef _export_video(dst_file, task_data, save_images=False):\n    _export(dst_file, task_data,\n        anno_callback=dump_as_cvat_interpolation, save_images=save_images)\n\n@exporter(name=\'CVAT for images\', ext=\'ZIP\', version=\'1.1\')\ndef _export_images(dst_file, task_data, save_images=False):\n    _export(dst_file, task_data,\n        anno_callback=dump_as_cvat_annotation, save_images=save_images)\n\n@importer(name=\'CVAT\', ext=\'XML, ZIP\', version=\'1.1\')\ndef _import(src_file, task_data):\n    is_zip = zipfile.is_zipfile(src_file)\n    src_file.seek(0)\n    if is_zip:\n        with TemporaryDirectory() as tmp_dir:\n            zipfile.ZipFile(src_file).extractall(tmp_dir)\n\n            anno_paths = glob(osp.join(tmp_dir, \'**\', \'*.xml\'), recursive=True)\n            for p in anno_paths:\n                load(p, task_data)\n    else:\n        load(src_file, task_data)'"
cvat/apps/dataset_manager/formats/labelme.py,0,"b""# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom tempfile import TemporaryDirectory\n\nfrom pyunpack import Archive\n\nfrom cvat.apps.dataset_manager.bindings import (CvatTaskDataExtractor,\n    import_dm_annotations)\nfrom cvat.apps.dataset_manager.util import make_zip_archive\nfrom datumaro.components.project import Dataset\n\nfrom .registry import dm_env, exporter, importer\n\n\n@exporter(name='LabelMe', ext='ZIP', version='3.0')\ndef _export(dst_file, task_data, save_images=False):\n    extractor = CvatTaskDataExtractor(task_data, include_images=save_images)\n    envt = dm_env.transforms\n    extractor = extractor.transform(envt.get('id_from_image_name'))\n    extractor = Dataset.from_extractors(extractor) # apply lazy transforms\n    with TemporaryDirectory() as temp_dir:\n        converter = dm_env.make_converter('label_me', save_images=save_images)\n        converter(extractor, save_dir=temp_dir)\n\n        make_zip_archive(temp_dir, dst_file)\n\n@importer(name='LabelMe', ext='ZIP', version='3.0')\ndef _import(src_file, task_data):\n    with TemporaryDirectory() as tmp_dir:\n        Archive(src_file.name).extractall(tmp_dir)\n\n        dataset = dm_env.make_importer('label_me')(tmp_dir).make_dataset()\n        masks_to_polygons = dm_env.transforms.get('masks_to_polygons')\n        dataset = dataset.transform(masks_to_polygons)\n        import_dm_annotations(dataset, task_data)\n"""
cvat/apps/dataset_manager/formats/mask.py,0,"b""# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os.path as osp\nfrom tempfile import TemporaryDirectory\n\nfrom pyunpack import Archive\n\nfrom cvat.apps.dataset_manager.bindings import (CvatTaskDataExtractor,\n    import_dm_annotations)\nfrom cvat.apps.dataset_manager.util import make_zip_archive\nfrom datumaro.cli.util import make_file_name\nfrom datumaro.components.project import Dataset\nfrom datumaro.util.mask_tools import generate_colormap\n\nfrom .registry import dm_env, exporter, importer\n\n\n@exporter(name='Segmentation mask', ext='ZIP', version='1.1')\ndef _export(dst_file, task_data, save_images=False):\n    extractor = CvatTaskDataExtractor(task_data, include_images=save_images)\n    envt = dm_env.transforms\n    extractor = extractor.transform(envt.get('polygons_to_masks'))\n    extractor = extractor.transform(envt.get('boxes_to_masks'))\n    extractor = extractor.transform(envt.get('merge_instance_segments'))\n    extractor = extractor.transform(envt.get('id_from_image_name'))\n    extractor = Dataset.from_extractors(extractor) # apply lazy transforms\n    with TemporaryDirectory() as temp_dir:\n        converter = dm_env.make_converter('voc_segmentation',\n            apply_colormap=True, label_map=make_colormap(task_data),\n            save_images=save_images)\n        converter(extractor, save_dir=temp_dir)\n\n        make_zip_archive(temp_dir, dst_file)\n\n@importer(name='Segmentation mask', ext='ZIP', version='1.1')\ndef _import(src_file, task_data):\n    with TemporaryDirectory() as tmp_dir:\n        Archive(src_file.name).extractall(tmp_dir)\n\n        dataset = dm_env.make_importer('voc')(tmp_dir).make_dataset()\n        masks_to_polygons = dm_env.transforms.get('masks_to_polygons')\n        dataset = dataset.transform(masks_to_polygons)\n        import_dm_annotations(dataset, task_data)\n\n\nDEFAULT_COLORMAP_CAPACITY = 2000\nDEFAULT_COLORMAP_PATH = osp.join(osp.dirname(__file__), 'predefined_colors.txt')\ndef parse_default_colors(file_path=None):\n    if file_path is None:\n        file_path = DEFAULT_COLORMAP_PATH\n\n    colors = {}\n    with open(file_path) as f:\n        for line in f:\n            line = line.strip()\n            if not line or line[0] == '#':\n                continue\n            _, label, color = line.split(':')\n            colors[label] = tuple(map(int, color.split(',')))\n    return colors\n\ndef normalize_label(label):\n    label = make_file_name(label) # basically, convert to ASCII lowercase\n    label = label.replace('-', '_')\n    return label\n\ndef make_colormap(task_data):\n    labels = sorted([label['name']\n        for _, label in task_data.meta['task']['labels']])\n    if 'background' not in labels:\n        labels.insert(0, 'background')\n\n    predefined = parse_default_colors()\n\n    # NOTE: using pop() to avoid collisions\n    colormap = {k: predefined.pop(normalize_label(k), None) for k in labels}\n\n    random_labels = [k for k in labels if not colormap[k]]\n    if random_labels:\n        colors = generate_colormap(DEFAULT_COLORMAP_CAPACITY + len(random_labels))\n        for i, label in enumerate(random_labels):\n            colormap[label] = colors[DEFAULT_COLORMAP_CAPACITY + i]\n    return {l: [c, [], []] for l, c in colormap.items()}"""
cvat/apps/dataset_manager/formats/mot.py,0,"b""# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom tempfile import TemporaryDirectory\n\nfrom pyunpack import Archive\n\nimport datumaro.components.extractor as datumaro\nfrom cvat.apps.dataset_manager.bindings import (CvatTaskDataExtractor,\n    match_frame)\nfrom cvat.apps.dataset_manager.util import make_zip_archive\nfrom datumaro.components.project import Dataset\n\nfrom .registry import dm_env, exporter, importer\n\n\n@exporter(name='MOT', ext='ZIP', version='1.1')\ndef _export(dst_file, task_data, save_images=False):\n    extractor = CvatTaskDataExtractor(task_data, include_images=save_images)\n    envt = dm_env.transforms\n    extractor = extractor.transform(envt.get('id_from_image_name'))\n    extractor = Dataset.from_extractors(extractor) # apply lazy transforms\n    with TemporaryDirectory() as temp_dir:\n        converter = dm_env.make_converter('mot_seq_gt',\n            save_images=save_images)\n        converter(extractor, save_dir=temp_dir)\n\n        make_zip_archive(temp_dir, dst_file)\n\n@importer(name='MOT', ext='ZIP', version='1.1')\ndef _import(src_file, task_data):\n    with TemporaryDirectory() as tmp_dir:\n        Archive(src_file.name).extractall(tmp_dir)\n\n        dataset = dm_env.make_importer('mot_seq')(tmp_dir).make_dataset()\n\n        tracks = {}\n        label_cat = dataset.categories()[datumaro.AnnotationType.label]\n\n        for item in dataset:\n            item = item.wrap(id=int(item.id) - 1) # NOTE: MOT frames start from 1\n            frame_id = match_frame(item, task_data)\n\n            for ann in item.annotations:\n                if ann.type != datumaro.AnnotationType.bbox:\n                    continue\n\n                track_id = ann.attributes.get('track_id')\n                if track_id is None:\n                    continue\n\n                shape = task_data.TrackedShape(\n                    type='rectangle',\n                    points=ann.points,\n                    occluded=ann.attributes.get('occluded') == True,\n                    outside=False,\n                    keyframe=False,\n                    z_order=ann.z_order,\n                    frame=frame_id,\n                    attributes=[],\n                )\n\n                # build trajectories as lists of shapes in track dict\n                if track_id not in tracks:\n                    tracks[track_id] = task_data.Track(\n                        label_cat.items[ann.label].name, 0, [])\n                tracks[track_id].shapes.append(shape)\n\n        for track in tracks.values():\n            # MOT annotations do not require frames to be ordered\n            track.shapes.sort(key=lambda t: t.frame)\n            # Set outside=True for the last shape in a track to finish the track\n            track.shapes[-1] = track.shapes[-1]._replace(outside=True)\n            task_data.add_track(track)\n"""
cvat/apps/dataset_manager/formats/pascal_voc.py,0,"b""# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\nimport os.path as osp\nimport shutil\nfrom glob import glob\n\nfrom tempfile import TemporaryDirectory\n\nfrom pyunpack import Archive\n\nfrom cvat.apps.dataset_manager.bindings import (CvatTaskDataExtractor,\n    import_dm_annotations)\nfrom cvat.apps.dataset_manager.util import make_zip_archive\nfrom datumaro.components.project import Dataset\n\nfrom .registry import dm_env, exporter, importer\n\n\n@exporter(name='PASCAL VOC', ext='ZIP', version='1.1')\ndef _export(dst_file, task_data, save_images=False):\n    extractor = CvatTaskDataExtractor(task_data, include_images=save_images)\n    envt = dm_env.transforms\n    extractor = extractor.transform(envt.get('id_from_image_name'))\n    extractor = Dataset.from_extractors(extractor) # apply lazy transforms\n    with TemporaryDirectory() as temp_dir:\n        converter = dm_env.make_converter('voc', label_map='source',\n            save_images=save_images)\n        converter(extractor, save_dir=temp_dir)\n\n        make_zip_archive(temp_dir, dst_file)\n\n@importer(name='PASCAL VOC', ext='ZIP', version='1.1')\ndef _import(src_file, task_data):\n    with TemporaryDirectory() as tmp_dir:\n        Archive(src_file.name).extractall(tmp_dir)\n\n        # put label map from the task if not present\n        labelmap_file = osp.join(tmp_dir, 'labelmap.txt')\n        if not osp.isfile(labelmap_file):\n            labels = (label['name'] + ':::'\n                for _, label in task_data.meta['task']['labels'])\n            with open(labelmap_file, 'w') as f:\n                f.write('\\n'.join(labels))\n\n        # support flat archive layout\n        anno_dir = osp.join(tmp_dir, 'Annotations')\n        if not osp.isdir(anno_dir):\n            anno_files = glob(osp.join(tmp_dir, '**', '*.xml'), recursive=True)\n            subsets_dir = osp.join(tmp_dir, 'ImageSets', 'Main')\n            os.makedirs(subsets_dir, exist_ok=True)\n            with open(osp.join(subsets_dir, 'train.txt'), 'w') as subset_file:\n                for f in anno_files:\n                    subset_file.write(osp.splitext(osp.basename(f))[0] + '\\n')\n\n            os.makedirs(anno_dir, exist_ok=True)\n            for f in anno_files:\n                shutil.move(f, anno_dir)\n\n        dataset = dm_env.make_importer('voc')(tmp_dir).make_dataset()\n        masks_to_polygons = dm_env.transforms.get('masks_to_polygons')\n        dataset = dataset.transform(masks_to_polygons)\n        import_dm_annotations(dataset, task_data)\n"""
cvat/apps/dataset_manager/formats/registry.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom datumaro.components.project import Environment\n\n\ndm_env = Environment()\n\nclass _Format:\n    NAME = \'\'\n    EXT = \'\'\n    VERSION = \'\'\n    DISPLAY_NAME = \'{NAME} {VERSION}\'\n    ENABLED = True\n\nclass Exporter(_Format):\n    def __call__(self, dst_file, task_data, **options):\n        raise NotImplementedError()\n\nclass Importer(_Format):\n    def __call__(self, src_file, task_data, **options):\n        raise NotImplementedError()\n\ndef _wrap_format(f_or_cls, klass, name, version, ext, display_name, enabled):\n    import inspect\n    assert inspect.isclass(f_or_cls) or inspect.isfunction(f_or_cls)\n    if inspect.isclass(f_or_cls):\n        assert hasattr(f_or_cls, \'__call__\')\n        target = f_or_cls\n    elif inspect.isfunction(f_or_cls):\n        class wrapper(klass):\n            # pylint: disable=arguments-differ\n            def __call__(self, *args, **kwargs):\n                f_or_cls(*args, **kwargs)\n\n        wrapper.__name__ = f_or_cls.__name__\n        wrapper.__module__ = f_or_cls.__module__\n        target = wrapper\n\n    target.NAME = name or klass.NAME or f_or_cls.__name__\n    target.VERSION = version or klass.VERSION\n    target.EXT = ext or klass.EXT\n    target.DISPLAY_NAME = (display_name or klass.DISPLAY_NAME).format(\n        NAME=name, VERSION=version, EXT=ext)\n    assert all([target.NAME, target.VERSION, target.EXT, target.DISPLAY_NAME])\n    target.ENABLED = enabled\n\n    return target\n\nEXPORT_FORMATS = {}\ndef exporter(name, version, ext, display_name=None, enabled=True):\n    assert name not in EXPORT_FORMATS, ""Export format \'%s\' already registered"" % name\n    def wrap_with_params(f_or_cls):\n        t = _wrap_format(f_or_cls, Exporter,\n            name=name, ext=ext, version=version, display_name=display_name,\n            enabled=enabled)\n        key = t.DISPLAY_NAME\n        assert key not in EXPORT_FORMATS, ""Export format \'%s\' already registered"" % name\n        EXPORT_FORMATS[key] = t\n        return t\n    return wrap_with_params\n\nIMPORT_FORMATS = {}\ndef importer(name, version, ext, display_name=None, enabled=True):\n    def wrap_with_params(f_or_cls):\n        t = _wrap_format(f_or_cls, Importer,\n            name=name, ext=ext, version=version, display_name=display_name,\n            enabled=enabled)\n        key = t.DISPLAY_NAME\n        assert key not in IMPORT_FORMATS, ""Import format \'%s\' already registered"" % name\n        IMPORT_FORMATS[key] = t\n        return t\n    return wrap_with_params\n\ndef make_importer(name):\n    return IMPORT_FORMATS[name]()\n\ndef make_exporter(name):\n    return EXPORT_FORMATS[name]()\n\n# pylint: disable=unused-import\nimport cvat.apps.dataset_manager.formats.coco\nimport cvat.apps.dataset_manager.formats.cvat\nimport cvat.apps.dataset_manager.formats.datumaro\nimport cvat.apps.dataset_manager.formats.labelme\nimport cvat.apps.dataset_manager.formats.mask\nimport cvat.apps.dataset_manager.formats.mot\nimport cvat.apps.dataset_manager.formats.pascal_voc\nimport cvat.apps.dataset_manager.formats.tfrecord\nimport cvat.apps.dataset_manager.formats.yolo'"
cvat/apps/dataset_manager/formats/tfrecord.py,0,"b""# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom tempfile import TemporaryDirectory\n\nfrom pyunpack import Archive\n\nfrom cvat.apps.dataset_manager.bindings import (CvatTaskDataExtractor,\n    import_dm_annotations)\nfrom cvat.apps.dataset_manager.util import make_zip_archive\nfrom datumaro.components.project import Dataset\n\nfrom .registry import dm_env, exporter, importer\n\n\nfrom datumaro.util.tf_util import import_tf\ntry:\n    import_tf()\n    tf_available = True\nexcept ImportError:\n    tf_available = False\n\n\n@exporter(name='TFRecord', ext='ZIP', version='1.0', enabled=tf_available)\ndef _export(dst_file, task_data, save_images=False):\n    extractor = CvatTaskDataExtractor(task_data, include_images=save_images)\n    extractor = Dataset.from_extractors(extractor) # apply lazy transforms\n    with TemporaryDirectory() as temp_dir:\n        converter = dm_env.make_converter('tf_detection_api',\n            save_images=save_images)\n        converter(extractor, save_dir=temp_dir)\n\n        make_zip_archive(temp_dir, dst_file)\n\n@importer(name='TFRecord', ext='ZIP', version='1.0', enabled=tf_available)\ndef _import(src_file, task_data):\n    with TemporaryDirectory() as tmp_dir:\n        Archive(src_file.name).extractall(tmp_dir)\n\n        dataset = dm_env.make_importer('tf_detection_api')(tmp_dir).make_dataset()\n        import_dm_annotations(dataset, task_data)\n"""
cvat/apps/dataset_manager/formats/yolo.py,0,"b""# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os.path as osp\nfrom glob import glob\nfrom tempfile import TemporaryDirectory\n\nfrom pyunpack import Archive\n\nfrom cvat.apps.dataset_manager.bindings import (CvatTaskDataExtractor,\n    import_dm_annotations, match_frame)\nfrom cvat.apps.dataset_manager.util import make_zip_archive\nfrom datumaro.components.extractor import DatasetItem\nfrom datumaro.components.project import Dataset\n\nfrom .registry import dm_env, exporter, importer\n\n\n@exporter(name='YOLO', ext='ZIP', version='1.1')\ndef _export(dst_file, task_data, save_images=False):\n    extractor = CvatTaskDataExtractor(task_data, include_images=save_images)\n    extractor = Dataset.from_extractors(extractor) # apply lazy transforms\n    with TemporaryDirectory() as temp_dir:\n        converter = dm_env.make_converter('yolo', save_images=save_images)\n        converter(extractor, save_dir=temp_dir)\n\n        make_zip_archive(temp_dir, dst_file)\n\n@importer(name='YOLO', ext='ZIP', version='1.1')\ndef _import(src_file, task_data):\n    with TemporaryDirectory() as tmp_dir:\n        Archive(src_file.name).extractall(tmp_dir)\n\n        image_info = {}\n        anno_files = glob(osp.join(tmp_dir, '**', '*.txt'), recursive=True)\n        for filename in anno_files:\n            filename = osp.splitext(osp.basename(filename))[0]\n            frame_info = None\n            try:\n                frame_id = match_frame(DatasetItem(id=filename), task_data)\n                frame_info = task_data.frame_info[frame_id]\n            except Exception:\n                pass\n            if frame_info is not None:\n                image_info[filename] = (frame_info['height'], frame_info['width'])\n\n        dataset = dm_env.make_importer('yolo')(tmp_dir, image_info=image_info) \\\n            .make_dataset()\n        import_dm_annotations(dataset, task_data)\n"""
cvat/apps/dataset_manager/tests/_test_formats.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# FIXME: Git application and package name clash in tests\nclass _GitImportFix:\n    import sys\n    former_path = sys.path[:]\n\n    @classmethod\n    def apply(cls):\n        # HACK: fix application and module name clash\n        # \'git\' app is found earlier than a library in the path.\n        # The clash is introduced by unittest discover\n        import sys\n        print(\'apply\')\n\n        apps_dir = __file__[:__file__.rfind(\'/dataset_manager/\')]\n        assert \'apps\' in apps_dir\n        try:\n            sys.path.remove(apps_dir)\n        except ValueError:\n            pass\n\n        for name in list(sys.modules):\n            if name.startswith(\'git.\') or name == \'git\':\n                m = sys.modules.pop(name, None)\n                del m\n\n        import git\n        assert apps_dir not in git.__file__\n\n    @classmethod\n    def restore(cls):\n        import sys\n        print(\'restore\')\n\n        for name in list(sys.modules):\n            if name.startswith(\'git.\') or name == \'git\':\n                m = sys.modules.pop(name)\n                del m\n\n        sys.path.insert(0, __file__[:__file__.rfind(\'/dataset_manager/\')])\n\n        import importlib\n        importlib.invalidate_caches()\n\ndef _setUpModule():\n    _GitImportFix.apply()\n    import cvat.apps.dataset_manager as dm\n    globals()[\'dm\'] = dm\n\n    import datumaro\n    globals()[\'datumaro\'] = datumaro\n\n    import sys\n    sys.path.insert(0, __file__[:__file__.rfind(\'/dataset_manager/\')])\n\n# def tearDownModule():\n    # _GitImportFix.restore()\n\nfrom io import BytesIO\nimport os.path as osp\nimport tempfile\nimport zipfile\n\nfrom PIL import Image\nfrom django.contrib.auth.models import User, Group\nfrom rest_framework.test import APITestCase, APIClient\nfrom rest_framework import status\n\n_setUpModule()\n\n\ndef generate_image_file(filename, size=(100, 50)):\n    f = BytesIO()\n    image = Image.new(\'RGB\', size=size)\n    image.save(f, \'jpeg\')\n    f.name = filename\n    f.seek(0)\n    return f\n\nclass ForceLogin:\n    def __init__(self, user, client):\n        self.user = user\n        self.client = client\n\n    def __enter__(self):\n        if self.user:\n            self.client.force_login(self.user,\n                backend=\'django.contrib.auth.backends.ModelBackend\')\n\n        return self\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        if self.user:\n            self.client.logout()\n\nclass _DbTestBase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        cls.create_db_users()\n\n    @classmethod\n    def create_db_users(cls):\n        group, _ = Group.objects.get_or_create(name=""adm"")\n\n        admin = User.objects.create_superuser(\n            username=""test"", password=""test"", email="""")\n        admin.groups.add(group)\n\n        cls.user = admin\n\n    def _put_api_v1_task_id_annotations(self, tid, data):\n        with ForceLogin(self.user, self.client):\n            response = self.client.put(""/api/v1/tasks/%s/annotations"" % tid,\n                data=data, format=""json"")\n\n        return response\n\n    def _create_task(self, data, image_data):\n        with ForceLogin(self.user, self.client):\n            response = self.client.post(\'/api/v1/tasks\', data=data, format=""json"")\n            assert response.status_code == status.HTTP_201_CREATED, response.status_code\n            tid = response.data[""id""]\n\n            response = self.client.post(""/api/v1/tasks/%s/data"" % tid,\n                data=image_data)\n            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code\n\n            response = self.client.get(""/api/v1/tasks/%s"" % tid)\n            task = response.data\n\n        return task\n\nclass TaskExportTest(_DbTestBase):\n    def _generate_annotations(self, task):\n        annotations = {\n            ""version"": 0,\n            ""tags"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": []\n                }\n            ],\n            ""shapes"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""default_value""]\n                        }\n                    ],\n                    ""points"": [1.0, 2.1, 100, 300.222],\n                    ""type"": ""rectangle"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""points"": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],\n                    ""type"": ""polygon"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": 1,\n                    ""attributes"": [],\n                    ""points"": [100, 300.222, 400, 500, 1, 3],\n                    ""type"": ""points"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": 1,\n                    ""attributes"": [],\n                    ""points"": [2.0, 2.1, 400, 500, 1, 3],\n                    ""type"": ""polyline"",\n                    ""occluded"": False\n                },\n            ],\n            ""tracks"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                    ],\n                    ""shapes"": [\n                        {\n                            ""frame"": 0,\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False,\n                            ""attributes"": [\n                                {\n                                    ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                    ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                                }\n                            ]\n                        },\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [2.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": True,\n                            ""outside"": True\n                        },\n                    ]\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False\n                        }\n                    ]\n                },\n            ]\n        }\n        self._put_api_v1_task_id_annotations(task[""id""], annotations)\n        return annotations\n\n    def _generate_task_images(self, count):\n        images = {\n            ""client_files[%d]"" % i: generate_image_file(""image_%d.jpg"" % i)\n            for i in range(count)\n        }\n        images[""image_quality""] = 75\n        return images\n\n    def _generate_task(self, images):\n        task = {\n            ""name"": ""my task #1"",\n            ""owner"": \'\',\n            ""assignee"": \'\',\n            ""overlap"": 0,\n            ""segment_size"": 100,\n            ""z_order"": False,\n            ""labels"": [\n                {\n                    ""name"": ""car"",\n                    ""attributes"": [\n                        {\n                            ""name"": ""model"",\n                            ""mutable"": False,\n                            ""input_type"": ""select"",\n                            ""default_value"": ""mazda"",\n                            ""values"": [""bmw"", ""mazda"", ""renault""]\n                        },\n                        {\n                            ""name"": ""parked"",\n                            ""mutable"": True,\n                            ""input_type"": ""checkbox"",\n                            ""default_value"": False\n                        },\n                    ]\n                },\n                {""name"": ""person""},\n            ]\n        }\n        return self._create_task(task, images)\n\n    @staticmethod\n    def _test_export(check, task, format_name, **export_args):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            file_path = osp.join(temp_dir, format_name)\n            dm.task.export_task(task[""id""], file_path,\n                format_name, **export_args)\n\n            check(file_path)\n\n    def test_export_formats_query(self):\n        formats = dm.views.get_export_formats()\n\n        self.assertEqual({f.DISPLAY_NAME for f in formats},\n        {\n            \'COCO 1.0\',\n            \'CVAT for images 1.1\',\n            \'CVAT for video 1.1\',\n            \'Datumaro 1.0\',\n            \'LabelMe 3.0\',\n            \'MOT 1.1\',\n            \'PASCAL VOC 1.1\',\n            \'Segmentation mask 1.1\',\n            \'TFRecord 1.0\',\n            \'YOLO 1.1\',\n        })\n\n    def test_import_formats_query(self):\n        formats = dm.views.get_import_formats()\n\n        self.assertEqual({f.DISPLAY_NAME for f in formats},\n        {\n            \'COCO 1.0\',\n            \'CVAT 1.1\',\n            \'LabelMe 3.0\',\n            \'MOT 1.1\',\n            \'PASCAL VOC 1.1\',\n            \'Segmentation mask 1.1\',\n            \'TFRecord 1.0\',\n            \'YOLO 1.1\',\n        })\n\n    def test_exports(self):\n        def check(file_path):\n            with open(file_path, \'rb\') as f:\n                self.assertTrue(len(f.read()) != 0)\n\n        for f in dm.views.get_export_formats():\n            if not f.ENABLED:\n                self.skipTest(""Format is disabled"")\n\n            format_name = f.DISPLAY_NAME\n            for save_images in { True, False }:\n                images = self._generate_task_images(3)\n                task = self._generate_task(images)\n                self._generate_annotations(task)\n                with self.subTest(format=format_name, save_images=save_images):\n                    self._test_export(check, task,\n                        format_name, save_images=save_images)\n\n    def test_empty_images_are_exported(self):\n        dm_env = dm.formats.registry.dm_env\n\n        for format_name, importer_name in [\n            (\'COCO 1.0\', \'coco\'),\n            (\'CVAT for images 1.1\', \'cvat\'),\n            # (\'CVAT for video 1.1\', \'cvat\'), # does not support\n            (\'Datumaro 1.0\', \'datumaro_project\'),\n            (\'LabelMe 3.0\', \'label_me\'),\n            # (\'MOT 1.1\', \'mot_seq\'), # does not support\n            (\'PASCAL VOC 1.1\', \'voc\'),\n            (\'Segmentation mask 1.1\', \'voc\'),\n            (\'TFRecord 1.0\', \'tf_detection_api\'),\n            (\'YOLO 1.1\', \'yolo\'),\n        ]:\n            with self.subTest(format=format_name):\n                if not dm.formats.registry.EXPORT_FORMATS[format_name].ENABLED:\n                    self.skipTest(""Format is disabled"")\n\n                images = self._generate_task_images(3)\n                task = self._generate_task(images)\n\n                def check(file_path):\n                    def load_dataset(src):\n                        if importer_name == \'datumaro_project\':\n                            project = datumaro.components.project. \\\n                                Project.load(src)\n\n                            # NOTE: can\'t import cvat.utils.cli\n                            # for whatever reason, so remove the dependency\n                            project.config.remove(\'sources\')\n\n                            return project.make_dataset()\n                        return dm_env.make_importer(importer_name)(src) \\\n                            .make_dataset()\n\n                    if zipfile.is_zipfile(file_path):\n                        with tempfile.TemporaryDirectory() as tmp_dir:\n                            zipfile.ZipFile(file_path).extractall(tmp_dir)\n                            dataset = load_dataset(tmp_dir)\n                    else:\n                        dataset = load_dataset(file_path)\n\n                    self.assertEqual(len(dataset), task[""size""])\n                self._test_export(check, task, format_name, save_images=False)\n\n'"
cvat/apps/dataset_manager/tests/test_annotation.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom cvat.apps.dataset_manager.annotation import TrackManager\n\nfrom unittest import TestCase\n\n\nclass TrackManagerTest(TestCase):\n    def test_single_point_interpolation(self):\n        track = {\n            ""frame"": 0,\n            ""label_id"": 0,\n            ""group"": None,\n            ""attributes"": [],\n            ""shapes"": [\n                {\n                    ""frame"": 0,\n                    ""points"": [1.0, 2.0],\n                    ""type"": ""points"",\n                    ""occluded"": False,\n                    ""outside"": False,\n                    ""attributes"": []\n                },\n                {\n                    ""frame"": 2,\n                    ""attributes"": [],\n                    ""points"": [3.0, 4.0, 5.0, 6.0],\n                    ""type"": ""points"",\n                    ""occluded"": False,\n                    ""outside"": True\n                },\n            ]\n        }\n\n        interpolated = TrackManager.get_interpolated_shapes(track, 0, 2)\n\n        self.assertEqual(len(interpolated), 3)'"
cvat/apps/documentation/migrations/__init__.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n'
cvat/apps/engine/migrations/0001_release_v0_1_0.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# Generated by Django 2.0.3 on 2018-05-23 11:51\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n    initial = True\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='AttributeSpec',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('text', models.CharField(max_length=1024)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Job',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Label',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=64)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='LabeledBox',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('frame', models.PositiveIntegerField()),\n                ('xtl', models.FloatField()),\n                ('ytl', models.FloatField()),\n                ('xbr', models.FloatField()),\n                ('ybr', models.FloatField()),\n                ('occluded', models.BooleanField(default=False)),\n                ('job', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Job')),\n                ('label', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Label')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='LabeledBoxAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('box', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.LabeledBox')),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='ObjectPath',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('frame', models.PositiveIntegerField()),\n                ('job', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Job')),\n                ('label', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Label')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='ObjectPathAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n                ('track', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.ObjectPath')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='Segment',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('start_frame', models.IntegerField()),\n                ('stop_frame', models.IntegerField()),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Task',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=256)),\n                ('size', models.PositiveIntegerField()),\n                ('path', models.CharField(max_length=256)),\n                ('mode', models.CharField(max_length=32)),\n                ('created_date', models.DateTimeField(auto_now_add=True)),\n                ('updated_date', models.DateTimeField(auto_now_add=True)),\n                ('status', models.CharField(default='annotate', max_length=32)),\n                ('bug_tracker', models.CharField(default='', max_length=2000)),\n                ('owner', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),\n                ('overlap', models.PositiveIntegerField(default=0)),\n            ],\n            options={\n                'permissions': (('view_task', 'Can see available tasks'), ('view_annotation', 'Can see annotation for the task'), ('change_annotation', 'Can modify annotation for the task')),\n            },\n        ),\n        migrations.CreateModel(\n            name='TrackedBox',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('xtl', models.FloatField()),\n                ('ytl', models.FloatField()),\n                ('xbr', models.FloatField()),\n                ('ybr', models.FloatField()),\n                ('occluded', models.BooleanField(default=False)),\n                ('frame', models.PositiveIntegerField()),\n                ('outside', models.BooleanField(default=False)),\n                ('track', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.ObjectPath')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='TrackedBoxAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('box', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.TrackedBox')),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.AddField(\n            model_name='segment',\n            name='task',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Task'),\n        ),\n        migrations.AddField(\n            model_name='label',\n            name='task',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Task'),\n        ),\n        migrations.AddField(\n            model_name='job',\n            name='segment',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Segment'),\n        ),\n        migrations.AddField(\n            model_name='attributespec',\n            name='label',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Label'),\n        ),\n        migrations.AlterField(\n            model_name='labeledbox',\n            name='id',\n            field=models.PositiveIntegerField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='objectpath',\n            name='id',\n            field=models.PositiveIntegerField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedbox',\n            name='id',\n            field=models.PositiveIntegerField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='labeledbox',\n            name='id',\n            field=models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID'),\n        ),\n        migrations.AlterField(\n            model_name='objectpath',\n            name='id',\n            field=models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID'),\n        ),\n        migrations.AlterField(\n            model_name='trackedbox',\n            name='id',\n            field=models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID'),\n        ),\n        migrations.AddField(\n            model_name='job',\n            name='annotator',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0002_labeledpoints_labeledpointsattributeval_labeledpolygon_labeledpolygonattributeval_labeledpolyline_la.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# Generated by Django 2.0.3 on 2018-05-30 09:53\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0001_release_v0_1_0'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='LabeledPoints',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('frame', models.PositiveIntegerField()),\n                ('occluded', models.BooleanField(default=False)),\n                ('points', models.TextField()),\n                ('job', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Job')),\n                ('label', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Label')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='LabeledPointsAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('points', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.LabeledPoints')),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='LabeledPolygon',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('frame', models.PositiveIntegerField()),\n                ('occluded', models.BooleanField(default=False)),\n                ('points', models.TextField()),\n                ('job', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Job')),\n                ('label', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Label')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='LabeledPolygonAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('polygon', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.LabeledPolygon')),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='LabeledPolyline',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('frame', models.PositiveIntegerField()),\n                ('occluded', models.BooleanField(default=False)),\n                ('points', models.TextField()),\n                ('job', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Job')),\n                ('label', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Label')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='LabeledPolylineAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('polyline', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.LabeledPolyline')),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='TrackedPoints',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('occluded', models.BooleanField(default=False)),\n                ('points', models.TextField()),\n                ('frame', models.PositiveIntegerField()),\n                ('outside', models.BooleanField(default=False)),\n                ('track', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.ObjectPath')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='TrackedPointsAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('points', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.TrackedPoints')),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='TrackedPolygon',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('occluded', models.BooleanField(default=False)),\n                ('points', models.TextField()),\n                ('frame', models.PositiveIntegerField()),\n                ('outside', models.BooleanField(default=False)),\n                ('track', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.ObjectPath')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='TrackedPolygonAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('polygon', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.TrackedPolygon')),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='TrackedPolyline',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('occluded', models.BooleanField(default=False)),\n                ('points', models.TextField()),\n                ('frame', models.PositiveIntegerField()),\n                ('outside', models.BooleanField(default=False)),\n                ('track', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.ObjectPath')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='TrackedPolylineAttributeVal',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('value', models.CharField(max_length=64)),\n                ('polyline', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.TrackedPolyline')),\n                ('spec', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.AttributeSpec')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0003_objectpath_shapes.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# Generated by Django 2.0.3 on 2018-06-04 11:21\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0002_labeledpoints_labeledpointsattributeval_labeledpolygon_labeledpolygonattributeval_labeledpolyline_la'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='objectpath',\n            name='shapes',\n            field=models.CharField(default='boxes', max_length=10),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0004_task_z_order.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# Generated by Django 2.0.3 on 2018-06-09 10:09\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0003_objectpath_shapes'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='task',\n            name='z_order',\n            field=models.BooleanField(default=False),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0005_auto_20180609_1512.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# Generated by Django 2.0.3 on 2018-06-09 12:12\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0004_task_z_order'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='labeledbox',\n            name='z_order',\n            field=models.IntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='labeledpoints',\n            name='z_order',\n            field=models.IntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='labeledpolygon',\n            name='z_order',\n            field=models.IntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='labeledpolyline',\n            name='z_order',\n            field=models.IntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='trackedbox',\n            name='z_order',\n            field=models.IntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='trackedpoints',\n            name='z_order',\n            field=models.IntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='trackedpolygon',\n            name='z_order',\n            field=models.IntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='trackedpolyline',\n            name='z_order',\n            field=models.IntegerField(default=0),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0006_auto_20180629_1501.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# Generated by Django 2.0.3 on 2018-06-29 12:01\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0005_auto_20180609_1512'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='labeledbox',\n            name='group_id',\n            field=models.PositiveIntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='labeledpoints',\n            name='group_id',\n            field=models.PositiveIntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='labeledpolygon',\n            name='group_id',\n            field=models.PositiveIntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='labeledpolyline',\n            name='group_id',\n            field=models.PositiveIntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='objectpath',\n            name='group_id',\n            field=models.PositiveIntegerField(default=0),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0007_task_flipped.py,0,"b""\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# Generated by Django 2.0.3 on 2018-07-16 08:19\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0006_auto_20180629_1501'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='task',\n            name='flipped',\n            field=models.BooleanField(default=False),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0008_auto_20180917_1424.py,0,"b""# Generated by Django 2.0.3 on 2018-09-17 11:24\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0007_task_flipped'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='task',\n            name='owner',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0009_auto_20180917_1424.py,0,"b""# Generated by Django 2.0.3 on 2018-09-17 11:24\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0008_auto_20180917_1424'),\n    ]\n\n\n\n    operations = [\n        migrations.AlterField(\n            model_name='labeledbox',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='labeledboxattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='labeledpoints',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='labeledpointsattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='labeledpolygon',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='labeledpolygonattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='labeledpolyline',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='labeledpolylineattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='objectpath',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='objectpathattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedbox',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedboxattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedpoints',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedpointsattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolygon',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolygonattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolyline',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolylineattributeval',\n            name='id',\n            field=models.BigAutoField(primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='objectpathattributeval',\n            name='track_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='objectpathattributeval',\n            name='track_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='trackedpoints',\n            name='track_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolygon',\n            name='track_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolyline',\n            name='track_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='trackedboxattributeval',\n            name='box_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='trackedpointsattributeval',\n            name='points_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolygonattributeval',\n            name='polygon_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolylineattributeval',\n            name='polyline_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='labeledboxattributeval',\n            name='box_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='labeledpointsattributeval',\n            name='points_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='labeledpolygonattributeval',\n            name='polygon_id',\n            field=models.BigIntegerField(),\n        ),\n        migrations.AlterField(\n            model_name='labeledpolylineattributeval',\n            name='polyline_id',\n            field=models.BigIntegerField(),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0010_auto_20181011_1517.py,0,"b""# Generated by Django 2.0.9 on 2018-10-11 12:17\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0009_auto_20180917_1424'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='labeledbox',\n            name='client_id',\n            field=models.BigIntegerField(default=-1),\n        ),\n        migrations.AddField(\n            model_name='labeledpoints',\n            name='client_id',\n            field=models.BigIntegerField(default=-1),\n        ),\n        migrations.AddField(\n            model_name='labeledpolygon',\n            name='client_id',\n            field=models.BigIntegerField(default=-1),\n        ),\n        migrations.AddField(\n            model_name='labeledpolyline',\n            name='client_id',\n            field=models.BigIntegerField(default=-1),\n        ),\n        migrations.AddField(\n            model_name='objectpath',\n            name='client_id',\n            field=models.BigIntegerField(default=-1),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0011_add_task_source_and_safecharfield.py,0,"b""# Generated by Django 2.0.9 on 2018-10-24 10:50\n\nimport cvat.apps.engine.models\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0010_auto_20181011_1517'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='task',\n            name='source',\n            field=cvat.apps.engine.models.SafeCharField(default='unknown', max_length=256),\n        ),\n        migrations.AlterField(\n            model_name='label',\n            name='name',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='labeledboxattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='labeledpointsattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='labeledpolygonattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='labeledpolylineattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='objectpathattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='name',\n            field=cvat.apps.engine.models.SafeCharField(max_length=256),\n        ),\n        migrations.AlterField(\n            model_name='trackedboxattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='trackedpointsattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolygonattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n        migrations.AlterField(\n            model_name='trackedpolylineattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=64),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0012_auto_20181025_1618.py,0,"b""# Generated by Django 2.0.9 on 2018-10-25 13:18\n\nimport cvat.apps.engine.models\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0011_add_task_source_and_safecharfield'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='job',\n            name='status',\n            field=models.CharField(default=cvat.apps.engine.models.StatusChoice('annotation'), max_length=32),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='status',\n            field=models.CharField(default=cvat.apps.engine.models.StatusChoice('annotation'), max_length=32),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0013_auth_no_default_permissions.py,0,"b""# Generated by Django 2.0.9 on 2018-11-07 12:25\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0012_auto_20181025_1618'),\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.AlterModelOptions(\n            name='attributespec',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='job',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='label',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='labeledboxattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='labeledpointsattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='labeledpolygonattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='labeledpolylineattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='objectpathattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='segment',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='task',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='trackedbox',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='trackedboxattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='trackedpoints',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='trackedpointsattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='trackedpolygon',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='trackedpolygonattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='trackedpolyline',\n            options={'default_permissions': ()},\n        ),\n        migrations.AlterModelOptions(\n            name='trackedpolylineattributeval',\n            options={'default_permissions': ()},\n        ),\n        migrations.RenameField(\n            model_name='job',\n            old_name='annotator',\n            new_name='assignee',\n        ),\n        migrations.AddField(\n            model_name='task',\n            name='assignee',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='assignees', to=settings.AUTH_USER_MODEL),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='owner',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='owners', to=settings.AUTH_USER_MODEL),\n        ),\n        migrations.AlterField(\n            model_name='job',\n            name='assignee',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='bug_tracker',\n            field=models.CharField(blank=True, default='', max_length=2000),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='owner',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='owners', to=settings.AUTH_USER_MODEL),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0014_job_max_shape_id.py,0,"b""# Generated by Django 2.1.3 on 2018-11-23 10:07\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0013_auth_no_default_permissions'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='job',\n            name='max_shape_id',\n            field=models.BigIntegerField(default=-1),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0015_db_redesign_20190217.py,0,"b""# Generated by Django 2.1.5 on 2019-02-17 19:32\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.migrations.operations.special\nimport django.db.models.deletion\nimport cvat.apps.engine.models\n\ndef set_segment_size(apps, schema_editor):\n    Task = apps.get_model('engine', 'Task')\n    for task in Task.objects.all():\n        segment = task.segment_set.first()\n        if segment:\n            task.segment_size = segment.stop_frame - segment.start_frame + 1\n            task.save()\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n        ('engine', '0014_job_max_shape_id'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='task',\n            name='segment_size',\n            field=models.PositiveIntegerField(null=True),\n        ),\n        migrations.RunPython(\n            code=set_segment_size,\n            reverse_code=django.db.migrations.operations.special.RunPython.noop,\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='segment_size',\n            field=models.PositiveIntegerField(),\n        ),\n        migrations.CreateModel(\n            name='ClientFile',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('file', models.FileField(max_length=1024, storage=cvat.apps.engine.models.MyFileSystemStorage(),\n                    upload_to=cvat.apps.engine.models.upload_path_handler)),\n                ('task', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Task')),\n            ],\n            options={\n                'default_permissions': (),\n            },\n        ),\n        migrations.CreateModel(\n            name='RemoteFile',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('file', models.CharField(max_length=1024)),\n                ('task', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Task')),\n            ],\n            options={\n                'default_permissions': (),\n            },\n        ),\n        migrations.CreateModel(\n            name='ServerFile',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('file', models.CharField(max_length=1024)),\n                ('task', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Task')),\n            ],\n            options={\n                'default_permissions': (),\n            },\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='status',\n            field=models.CharField(choices=[('ANNOTATION', 'annotation'), ('VALIDATION', 'validation'), ('COMPLETED', 'completed')], default=cvat.apps.engine.models.StatusChoice('annotation'), max_length=32),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='overlap',\n            field=models.PositiveIntegerField(null=True),\n        ),\n        migrations.RemoveField(\n            model_name='task',\n            name='path',\n        ),\n        migrations.AddField(\n            model_name='task',\n            name='image_quality',\n            field=models.PositiveSmallIntegerField(default=50),\n        ),\n        migrations.CreateModel(\n            name='Plugin',\n            fields=[\n                ('name', models.SlugField(max_length=32, primary_key=True, serialize=False)),\n                ('description', cvat.apps.engine.models.SafeCharField(max_length=8192)),\n                ('created_at', models.DateTimeField(auto_now_add=True)),\n                ('updated_at', models.DateTimeField(auto_now_add=True)),\n                ('maintainer', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='maintainers', to=settings.AUTH_USER_MODEL)),\n            ],\n            options={\n                'default_permissions': (),\n            },\n        ),\n        migrations.CreateModel(\n            name='PluginOption',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', cvat.apps.engine.models.SafeCharField(max_length=32)),\n                ('value', cvat.apps.engine.models.SafeCharField(max_length=1024)),\n                ('plugin', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Plugin')),\n            ],\n        ),\n        migrations.AlterUniqueTogether(\n            name='label',\n            unique_together={('task', 'name')},\n        ),\n        migrations.AlterUniqueTogether(\n            name='clientfile',\n            unique_together={('task', 'file')},\n        ),\n        migrations.AddField(\n            model_name='attributespec',\n            name='default_value',\n            field=models.CharField(default='', max_length=128),\n            preserve_default=False,\n        ),\n        migrations.AddField(\n            model_name='attributespec',\n            name='input_type',\n            field=models.CharField(choices=[('CHECKBOX', 'checkbox'), ('RADIO', 'radio'), ('NUMBER', 'number'), ('TEXT', 'text'), ('SELECT', 'select')], default='select', max_length=16),\n            preserve_default=False,\n        ),\n        migrations.AddField(\n            model_name='attributespec',\n            name='mutable',\n            field=models.BooleanField(default=True),\n            preserve_default=False,\n        ),\n        migrations.AddField(\n            model_name='attributespec',\n            name='name',\n            field=models.CharField(default='test', max_length=64),\n            preserve_default=False,\n        ),\n        migrations.AddField(\n            model_name='attributespec',\n            name='values',\n            field=models.CharField(default='', max_length=4096),\n            preserve_default=False,\n        ),\n        migrations.AlterField(\n            model_name='job',\n            name='status',\n            field=models.CharField(choices=[('ANNOTATION', 'annotation'), ('VALIDATION', 'validation'), ('COMPLETED', 'completed')], default=cvat.apps.engine.models.StatusChoice('annotation'), max_length=32),\n        ),\n        migrations.AlterField(\n            model_name='attributespec',\n            name='text',\n            field=models.CharField(default='', max_length=1024),\n        ),\n        migrations.AlterField(\n            model_name='attributespec',\n            name='input_type',\n            field=models.CharField(choices=[('checkbox', 'CHECKBOX'), ('radio', 'RADIO'), ('number', 'NUMBER'), ('text', 'TEXT'), ('select', 'SELECT')], max_length=16),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='segment_size',\n            field=models.PositiveIntegerField(default=0),\n        ),\n        migrations.AlterField(\n            model_name='job',\n            name='status',\n            field=models.CharField(choices=[('annotation', 'ANNOTATION'), ('validation', 'VALIDATION'), ('completed', 'COMPLETED')], default=cvat.apps.engine.models.StatusChoice('annotation'), max_length=32),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='status',\n            field=models.CharField(choices=[('annotation', 'ANNOTATION'), ('validation', 'VALIDATION'), ('completed', 'COMPLETED')], default=cvat.apps.engine.models.StatusChoice('annotation'), max_length=32),\n        ),\n        migrations.CreateModel(\n            name='Image',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('path', models.CharField(max_length=1024)),\n                ('frame', models.PositiveIntegerField()),\n                ('task', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='engine.Task')),\n                ('height', models.PositiveIntegerField()),\n                ('width', models.PositiveIntegerField()),\n            ],\n            options={\n                'default_permissions': (),\n            },\n        ),\n        migrations.CreateModel(\n            name='Video',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('path', models.CharField(max_length=1024)),\n                ('start_frame', models.PositiveIntegerField()),\n                ('stop_frame', models.PositiveIntegerField()),\n                ('step', models.PositiveIntegerField(default=1)),\n                ('task', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, to='engine.Task')),\n                ('height', models.PositiveIntegerField()),\n                ('width', models.PositiveIntegerField()),\n            ],\n            options={\n                'default_permissions': (),\n            },\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0016_attribute_spec_20190217.py,0,"b'import os\nimport re\nimport csv\nfrom io import StringIO\nfrom PIL import Image\nfrom django.db import migrations\nfrom django.conf import settings\nfrom cvat.apps.engine.media_extractors import get_mime\n\ndef parse_attribute(value):\n    match = re.match(r\'^([~@])(\\w+)=(\\w+):(.+)?$\', value)\n    if match:\n        prefix = match.group(1)\n        input_type = match.group(2)\n        name = match.group(3)\n        if match.group(4):\n            values = list(csv.reader(StringIO(match.group(4)),\n                quotechar=""\'""))[0]\n        else:\n            values = []\n\n        return {\'prefix\':prefix, \'type\':input_type, \'name\':name, \'values\':values}\n    else:\n        return None\n\ndef split_text_attribute(apps, schema_editor):\n    AttributeSpec = apps.get_model(\'engine\', \'AttributeSpec\')\n    for attribute in AttributeSpec.objects.all():\n        spec = parse_attribute(attribute.text)\n        if spec:\n            attribute.mutable = (spec[\'prefix\'] == \'~\')\n            attribute.input_type = spec[\'type\']\n            attribute.name = spec[\'name\']\n            attribute.default_value = spec[\'values\'][0] if spec[\'values\'] else \'\'\n            attribute.values = \'\\n\'.join(spec[\'values\'])\n            attribute.save()\n\ndef join_text_attribute(apps, schema_editor):\n    AttributeSpec = apps.get_model(\'engine\', \'AttributeSpec\')\n    for attribute in AttributeSpec.objects.all():\n        attribute.text = """"\n        if attribute.mutable:\n            attribute.text += ""~""\n        else:\n            attribute.text += ""@""\n\n        attribute.text += attribute.input_type\n        attribute.text += ""="" + attribute.name + "":""\n        attribute.text += "","".join(attribute.values.split(\'\\n\'))\n        attribute.save()\n\ndef _get_task_dirname(task_obj):\n    return os.path.join(settings.DATA_ROOT, str(task_obj.id))\n\ndef _get_upload_dirname(task_obj):\n    return os.path.join(_get_task_dirname(task_obj), "".upload"")\n\ndef _get_frame_path(task_obj, frame):\n    return os.path.join(\n        _get_task_dirname(task_obj),\n        ""data"",\n        str(int(frame) // 10000),\n        str(int(frame) // 100),\n        str(frame) + \'.jpg\',\n    )\n\ndef fill_task_meta_data_forward(apps, schema_editor):\n    db_alias = schema_editor.connection.alias\n    task_model = apps.get_model(\'engine\', \'Task\')\n    video_model = apps.get_model(\'engine\', ""Video"")\n    image_model = apps.get_model(\'engine\', \'Image\')\n\n    for db_task in task_model.objects.all():\n        if db_task.mode == \'interpolation\':\n            db_video = video_model()\n            db_video.task_id = db_task.id\n            db_video.start_frame = 0\n            db_video.stop_frame = db_task.size\n            db_video.step = 1\n\n            video = """"\n            for root, _, files in os.walk(_get_upload_dirname(db_task)):\n                fullnames = map(lambda f: os.path.join(root, f), files)\n                videos = list(filter(lambda x: get_mime(x) == \'video\', fullnames))\n                if len(videos):\n                    video = videos[0]\n                    break\n            db_video.path = video\n            try:\n                image = Image.open(_get_frame_path(db_task, 0))\n                db_video.width = image.width\n                db_video.height = image.height\n                image.close()\n            except FileNotFoundError:\n                db_video.width = 0\n                db_video.height = 0\n\n            db_video.save()\n        else:\n            filenames = []\n            for root, _, files in os.walk(_get_upload_dirname(db_task)):\n                fullnames = map(lambda f: os.path.join(root, f), files)\n                images = filter(lambda x: get_mime(x) == \'image\', fullnames)\n                filenames.extend(images)\n            filenames.sort()\n\n            db_images = []\n            for i, image_path in enumerate(filenames):\n                db_image = image_model()\n                db_image.task_id = db_task.id\n                db_image.path = image_path\n                db_image.frame = i\n                try:\n                    image = Image.open(image_path)\n                    db_image.width = image.width\n                    db_image.height = image.height\n                    image.close()\n                except FileNotFoundError:\n                    db_image.width = 0\n                    db_image.height = 0\n\n                db_images.append(db_image)\n            image_model.objects.using(db_alias).bulk_create(db_images)\n\ndef fill_task_meta_data_backward(apps, schema_editor):\n    task_model = apps.get_model(\'engine\', \'Task\')\n    video_model = apps.get_model(\'engine\', ""Video"")\n    image_model = apps.get_model(\'engine\', \'Image\')\n\n    for db_task in task_model.objects.all():\n        upload_dir = _get_upload_dirname(db_task)\n        if db_task.mode == \'interpolation\':\n            video = video_model.objects.get(task__id=db_task.id)\n            db_task.source = os.path.relpath(video.path, upload_dir)\n            video.delete()\n        else:\n            images = image_model.objects.filter(task__id=db_task.id)\n            db_task.source = \'{} images: {}, ...\'.format(\n                len(images),\n                "", "".join([os.path.relpath(x.path, upload_dir) for x in images[0:2]])\n            )\n            images.delete()\n        db_task.save()\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\'engine\', \'0015_db_redesign_20190217\'),\n    ]\n\n    operations = [\n        migrations.RunPython(\n            code=split_text_attribute,\n            reverse_code=join_text_attribute,\n        ),\n        migrations.RemoveField(\n            model_name=\'attributespec\',\n            name=\'text\',\n        ),\n        migrations.AlterUniqueTogether(\n            name=\'attributespec\',\n            unique_together={(\'label\', \'name\')},\n        ),\n        migrations.RunPython(\n            code=fill_task_meta_data_forward,\n            reverse_code=fill_task_meta_data_backward,\n        ),\n        migrations.RemoveField(\n            model_name=\'task\',\n            name=\'source\',\n        ),\n    ]\n'"
cvat/apps/engine/migrations/0017_db_redesign_20190221.py,0,"b'# Generated by Django 2.1.5 on 2019-02-21 12:25\n\nimport cvat.apps.engine.models\nfrom django.db import migrations, models\nimport django.db.models.deletion\nfrom django.conf import settings\nfrom cvat.apps.dataset_manager.task import _merge_table_rows\n\n# some modified functions to transer annotation\ndef _bulk_create(db_model, db_alias, objects, flt_param):\n    if objects:\n        if flt_param:\n            if \'postgresql\' in settings.DATABASES[""default""][""ENGINE""]:\n                return db_model.objects.using(db_alias).bulk_create(objects)\n            else:\n                ids = list(db_model.objects.using(db_alias).filter(**flt_param).values_list(\'id\', flat=True))\n                db_model.objects.using(db_alias).bulk_create(objects)\n\n                return list(db_model.objects.using(db_alias).exclude(id__in=ids).filter(**flt_param))\n        else:\n            return db_model.objects.using(db_alias).bulk_create(objects)\n\ndef get_old_db_shapes(shape_type, db_job):\n    def _get_shape_set(db_job, shape_type):\n        if shape_type == \'polygons\':\n            return db_job.labeledpolygon_set\n        elif shape_type == \'polylines\':\n            return db_job.labeledpolyline_set\n        elif shape_type == \'boxes\':\n            return db_job.labeledbox_set\n        elif shape_type == \'points\':\n            return db_job.labeledpoints_set\n\n    def get_values(shape_type):\n        if shape_type == \'polygons\':\n            return [\n                (\'id\', \'frame\', \'points\', \'label_id\', \'group_id\', \'occluded\', \'z_order\', \'client_id\',\n                \'labeledpolygonattributeval__value\', \'labeledpolygonattributeval__spec_id\',\n                \'labeledpolygonattributeval__id\'), {\n                    \'attributes\': [\n                        \'labeledpolygonattributeval__value\',\n                        \'labeledpolygonattributeval__spec_id\',\n                        \'labeledpolygonattributeval__id\'\n                    ]\n                }, \'labeledpolygonattributeval_set\'\n            ]\n        elif shape_type == \'polylines\':\n            return [\n                (\'id\', \'frame\', \'points\', \'label_id\', \'group_id\', \'occluded\', \'z_order\', \'client_id\',\n                \'labeledpolylineattributeval__value\', \'labeledpolylineattributeval__spec_id\',\n                \'labeledpolylineattributeval__id\'), {\n                    \'attributes\': [\n                        \'labeledpolylineattributeval__value\',\n                        \'labeledpolylineattributeval__spec_id\',\n                        \'labeledpolylineattributeval__id\'\n                    ]\n                }, \'labeledpolylineattributeval_set\'\n            ]\n        elif shape_type == \'boxes\':\n            return [\n                (\'id\', \'frame\', \'xtl\', \'ytl\', \'xbr\', \'ybr\', \'label_id\', \'group_id\', \'occluded\', \'z_order\', \'client_id\',\n                \'labeledboxattributeval__value\', \'labeledboxattributeval__spec_id\',\n                \'labeledboxattributeval__id\'), {\n                    \'attributes\': [\n                        \'labeledboxattributeval__value\',\n                        \'labeledboxattributeval__spec_id\',\n                        \'labeledboxattributeval__id\'\n                    ]\n                }, \'labeledboxattributeval_set\'\n            ]\n        elif shape_type == \'points\':\n            return [\n                (\'id\', \'frame\', \'points\', \'label_id\', \'group_id\', \'occluded\', \'z_order\', \'client_id\',\n                \'labeledpointsattributeval__value\', \'labeledpointsattributeval__spec_id\',\n                \'labeledpointsattributeval__id\'), {\n                    \'attributes\': [\n                        \'labeledpointsattributeval__value\',\n                        \'labeledpointsattributeval__spec_id\',\n                        \'labeledpointsattributeval__id\'\n                    ]\n                }, \'labeledpointsattributeval_set\'\n            ]\n    (values, merge_keys, prefetch) = get_values(shape_type)\n    db_shapes = list(_get_shape_set(db_job, shape_type).prefetch_related(prefetch).values(*values).order_by(\'frame\'))\n    return _merge_table_rows(db_shapes, merge_keys, \'id\')\n\ndef get_old_db_paths(db_job):\n    db_paths = db_job.objectpath_set\n    for shape in [\'trackedpoints_set\', \'trackedbox_set\', \'trackedpolyline_set\', \'trackedpolygon_set\']:\n        db_paths.prefetch_related(shape)\n    for shape_attr in [\'trackedpoints_set__trackedpointsattributeval_set\', \'trackedbox_set__trackedboxattributeval_set\',\n        \'trackedpolygon_set__trackedpolygonattributeval_set\', \'trackedpolyline_set__trackedpolylineattributeval_set\']:\n        db_paths.prefetch_related(shape_attr)\n    db_paths.prefetch_related(\'objectpathattributeval_set\')\n    db_paths = list (db_paths.values(\'id\', \'frame\', \'group_id\', \'shapes\', \'client_id\', \'objectpathattributeval__spec_id\',\n        \'objectpathattributeval__id\', \'objectpathattributeval__value\',\n        \'trackedbox\', \'trackedpolygon\', \'trackedpolyline\', \'trackedpoints\',\n        \'trackedbox__id\', \'label_id\', \'trackedbox__xtl\', \'trackedbox__ytl\',\n        \'trackedbox__xbr\', \'trackedbox__ybr\', \'trackedbox__frame\', \'trackedbox__occluded\',\n        \'trackedbox__z_order\',\'trackedbox__outside\', \'trackedbox__trackedboxattributeval__spec_id\',\n        \'trackedbox__trackedboxattributeval__value\', \'trackedbox__trackedboxattributeval__id\',\n        \'trackedpolygon__id\' ,\'trackedpolygon__points\', \'trackedpolygon__frame\', \'trackedpolygon__occluded\',\n        \'trackedpolygon__z_order\', \'trackedpolygon__outside\', \'trackedpolygon__trackedpolygonattributeval__spec_id\',\n        \'trackedpolygon__trackedpolygonattributeval__value\', \'trackedpolygon__trackedpolygonattributeval__id\',\n        \'trackedpolyline__id\', \'trackedpolyline__points\', \'trackedpolyline__frame\', \'trackedpolyline__occluded\',\n        \'trackedpolyline__z_order\', \'trackedpolyline__outside\', \'trackedpolyline__trackedpolylineattributeval__spec_id\',\n        \'trackedpolyline__trackedpolylineattributeval__value\', \'trackedpolyline__trackedpolylineattributeval__id\',\n        \'trackedpoints__id\', \'trackedpoints__points\', \'trackedpoints__frame\', \'trackedpoints__occluded\',\n        \'trackedpoints__z_order\', \'trackedpoints__outside\', \'trackedpoints__trackedpointsattributeval__spec_id\',\n        \'trackedpoints__trackedpointsattributeval__value\', \'trackedpoints__trackedpointsattributeval__id\')\n        .order_by(\'id\', \'trackedbox__frame\', \'trackedpolygon__frame\', \'trackedpolyline__frame\', \'trackedpoints__frame\'))\n\n    db_box_paths = list(filter(lambda path: path[\'shapes\'] == \'boxes\', db_paths ))\n    db_polygon_paths = list(filter(lambda path: path[\'shapes\'] == \'polygons\', db_paths ))\n    db_polyline_paths = list(filter(lambda path: path[\'shapes\'] == \'polylines\', db_paths ))\n    db_points_paths = list(filter(lambda path: path[\'shapes\'] == \'points\', db_paths ))\n\n    object_path_attr_merge_key = [\n        \'objectpathattributeval__value\',\n        \'objectpathattributeval__spec_id\',\n        \'objectpathattributeval__id\'\n    ]\n\n    db_box_paths = _merge_table_rows(db_box_paths, {\n        \'attributes\': object_path_attr_merge_key,\n        \'shapes\': [\n            \'trackedbox__id\', \'trackedbox__xtl\', \'trackedbox__ytl\',\n            \'trackedbox__xbr\', \'trackedbox__ybr\', \'trackedbox__frame\',\n            \'trackedbox__occluded\', \'trackedbox__z_order\', \'trackedbox__outside\',\n            \'trackedbox__trackedboxattributeval__value\',\n            \'trackedbox__trackedboxattributeval__spec_id\',\n            \'trackedbox__trackedboxattributeval__id\'\n        ],\n    }, \'id\')\n\n    db_polygon_paths = _merge_table_rows(db_polygon_paths, {\n        \'attributes\': object_path_attr_merge_key,\n        \'shapes\': [\n            \'trackedpolygon__id\', \'trackedpolygon__points\', \'trackedpolygon__frame\',\n            \'trackedpolygon__occluded\', \'trackedpolygon__z_order\', \'trackedpolygon__outside\',\n            \'trackedpolygon__trackedpolygonattributeval__value\',\n            \'trackedpolygon__trackedpolygonattributeval__spec_id\',\n            \'trackedpolygon__trackedpolygonattributeval__id\'\n        ]\n    }, \'id\')\n\n    db_polyline_paths = _merge_table_rows(db_polyline_paths, {\n        \'attributes\': object_path_attr_merge_key,\n        \'shapes\': [\n            \'trackedpolyline__id\', \'trackedpolyline__points\', \'trackedpolyline__frame\',\n            \'trackedpolyline__occluded\', \'trackedpolyline__z_order\', \'trackedpolyline__outside\',\n            \'trackedpolyline__trackedpolylineattributeval__value\',\n            \'trackedpolyline__trackedpolylineattributeval__spec_id\',\n            \'trackedpolyline__trackedpolylineattributeval__id\'\n        ],\n    }, \'id\')\n\n    db_points_paths = _merge_table_rows(db_points_paths, {\n        \'attributes\': object_path_attr_merge_key,\n        \'shapes\': [\n            \'trackedpoints__id\', \'trackedpoints__points\', \'trackedpoints__frame\',\n            \'trackedpoints__occluded\', \'trackedpoints__z_order\', \'trackedpoints__outside\',\n            \'trackedpoints__trackedpointsattributeval__value\',\n            \'trackedpoints__trackedpointsattributeval__spec_id\',\n            \'trackedpoints__trackedpointsattributeval__id\'\n        ]\n    }, \'id\')\n\n    for db_box_path in db_box_paths:\n        db_box_path.attributes = list(set(db_box_path.attributes))\n        db_box_path.type = \'box_path\'\n        db_box_path.shapes = _merge_table_rows(db_box_path.shapes, {\n            \'attributes\': [\n                \'trackedboxattributeval__value\',\n                \'trackedboxattributeval__spec_id\',\n                \'trackedboxattributeval__id\'\n            ]\n        }, \'id\')\n\n    for db_polygon_path in db_polygon_paths:\n        db_polygon_path.attributes = list(set(db_polygon_path.attributes))\n        db_polygon_path.type = \'poligon_path\'\n        db_polygon_path.shapes = _merge_table_rows(db_polygon_path.shapes, {\n            \'attributes\': [\n                \'trackedpolygonattributeval__value\',\n                \'trackedpolygonattributeval__spec_id\',\n                \'trackedpolygonattributeval__id\'\n            ]\n        }, \'id\')\n\n    for db_polyline_path in db_polyline_paths:\n        db_polyline_path.attributes = list(set(db_polyline_path.attributes))\n        db_polyline_path.type = \'polyline_path\'\n        db_polyline_path.shapes = _merge_table_rows(db_polyline_path.shapes, {\n            \'attributes\': [\n                \'trackedpolylineattributeval__value\',\n                \'trackedpolylineattributeval__spec_id\',\n                \'trackedpolylineattributeval__id\'\n            ]\n        }, \'id\')\n\n    for db_points_path in db_points_paths:\n        db_points_path.attributes = list(set(db_points_path.attributes))\n        db_points_path.type = \'points_path\'\n        db_points_path.shapes = _merge_table_rows(db_points_path.shapes, {\n            \'attributes\': [\n                \'trackedpointsattributeval__value\',\n                \'trackedpointsattributeval__spec_id\',\n                \'trackedpointsattributeval__id\'\n            ]\n        }, \'id\')\n    return db_box_paths + db_polygon_paths + db_polyline_paths + db_points_paths\n\ndef process_shapes(db_job, apps, db_labels, db_attributes, db_alias):\n    LabeledShape = apps.get_model(\'engine\', \'LabeledShape\')\n    LabeledShapeAttributeVal = apps.get_model(\'engine\', \'LabeledShapeAttributeVal\')\n    new_db_shapes = []\n    new_db_attrvals = []\n    for shape_type in [\'boxes\', \'points\', \'polygons\', \'polylines\']:\n        for shape in get_old_db_shapes(shape_type, db_job):\n            new_db_shape = LabeledShape()\n            new_db_shape.job = db_job\n            new_db_shape.label = db_labels[shape.label_id]\n            new_db_shape.group = shape.group_id\n\n            if shape_type == \'boxes\':\n                new_db_shape.type = cvat.apps.engine.models.ShapeType.RECTANGLE\n                new_db_shape.points = [shape.xtl, shape.ytl, shape.xbr, shape.ybr]\n            else:\n                new_db_shape.points = shape.points.replace(\',\', \' \').split()\n                if shape_type == \'points\':\n                    new_db_shape.type = cvat.apps.engine.models.ShapeType.POINTS\n                elif shape_type == \'polygons\':\n                    new_db_shape.type = cvat.apps.engine.models.ShapeType.POLYGON\n                elif shape_type == \'polylines\':\n                    new_db_shape.type = cvat.apps.engine.models.ShapeType.POLYLINE\n\n            new_db_shape.frame = shape.frame\n            new_db_shape.occluded = shape.occluded\n            new_db_shape.z_order = shape.z_order\n\n            for attr in shape.attributes:\n                db_attrval = LabeledShapeAttributeVal()\n                db_attrval.shape_id = len(new_db_shapes)\n                db_attrval.spec = db_attributes[attr.spec_id]\n                db_attrval.value = attr.value\n                new_db_attrvals.append(db_attrval)\n\n            new_db_shapes.append(new_db_shape)\n\n    new_db_shapes = _bulk_create(LabeledShape, db_alias, new_db_shapes, {""job_id"": db_job.id})\n    for db_attrval in new_db_attrvals:\n        db_attrval.shape_id = new_db_shapes[db_attrval.shape_id].id\n\n    _bulk_create(LabeledShapeAttributeVal, db_alias, new_db_attrvals, {})\n\ndef process_paths(db_job, apps, db_labels, db_attributes, db_alias):\n    TrackedShape = apps.get_model(\'engine\', \'TrackedShape\')\n    LabeledTrack = apps.get_model(\'engine\', \'LabeledTrack\')\n    LabeledTrackAttributeVal = apps.get_model(\'engine\', \'LabeledTrackAttributeVal\')\n    TrackedShapeAttributeVal = apps.get_model(\'engine\', \'TrackedShapeAttributeVal\')\n    tracks = get_old_db_paths(db_job)\n\n    new_db_tracks = []\n    new_db_track_attrvals = []\n    new_db_shapes = []\n    new_db_shape_attrvals = []\n\n    for track in tracks:\n        db_track = LabeledTrack()\n        db_track.job = db_job\n        db_track.label = db_labels[track.label_id]\n        db_track.frame = track.frame\n        db_track.group = track.group_id\n\n        for attr in track.attributes:\n            db_attrspec = db_attributes[attr.spec_id]\n            db_attrval = LabeledTrackAttributeVal()\n            db_attrval.track_id = len(new_db_tracks)\n            db_attrval.spec = db_attrspec\n            db_attrval.value = attr.value\n            new_db_track_attrvals.append(db_attrval)\n\n        for shape in track.shapes:\n            db_shape = TrackedShape()\n            db_shape.track_id = len(new_db_tracks)\n            db_shape.frame = shape.frame\n            db_shape.occluded = shape.occluded\n            db_shape.z_order = shape.z_order\n            db_shape.outside = shape.outside\n            if track.type == \'box_path\':\n                db_shape.type = cvat.apps.engine.models.ShapeType.RECTANGLE\n                db_shape.points = [shape.xtl, shape.ytl, shape.xbr, shape.ybr]\n            else:\n                db_shape.points = shape.points.replace(\',\', \' \').split()\n                if track.type == \'points_path\':\n                    db_shape.type = cvat.apps.engine.models.ShapeType.POINTS\n                elif track.type == \'polygon_path\':\n                    db_shape.type = cvat.apps.engine.models.ShapeType.POLYGON\n                elif track.type == \'polyline_path\':\n                    db_shape.type = cvat.apps.engine.models.ShapeType.POLYLINE\n\n            for attr in shape.attributes:\n                db_attrspec = db_attributes[attr.spec_id]\n                db_attrval = TrackedShapeAttributeVal()\n                db_attrval.shape_id = len(new_db_shapes)\n                db_attrval.spec = db_attrspec\n                db_attrval.value = attr.value\n                new_db_shape_attrvals.append(db_attrval)\n\n            new_db_shapes.append(db_shape)\n        new_db_tracks.append(db_track)\n\n    new_db_tracks = _bulk_create(LabeledTrack, db_alias, new_db_tracks, {""job_id"": db_job.id})\n\n    for db_attrval in new_db_track_attrvals:\n        db_attrval.track_id = new_db_tracks[db_attrval.track_id].id\n    _bulk_create(LabeledTrackAttributeVal, db_alias, new_db_track_attrvals, {})\n\n    for db_shape in new_db_shapes:\n        db_shape.track_id = new_db_tracks[db_shape.track_id].id\n\n    new_db_shapes = _bulk_create(TrackedShape, db_alias, new_db_shapes, {""track__job_id"": db_job.id})\n\n    for db_attrval in new_db_shape_attrvals:\n        db_attrval.shape_id = new_db_shapes[db_attrval.shape_id].id\n\n    _bulk_create(TrackedShapeAttributeVal, db_alias, new_db_shape_attrvals, {})\n\ndef copy_annotations_forward(apps, schema_editor):\n    db_alias = schema_editor.connection.alias\n    Task = apps.get_model(\'engine\', \'Task\')\n    AttributeSpec = apps.get_model(\'engine\', \'AttributeSpec\')\n\n\n    for task in Task.objects.all():\n        print(""run anno migration for the task {}"".format(task.id))\n        db_labels = {db_label.id:db_label for db_label in task.label_set.all()}\n        db_attributes = {db_attr.id:db_attr for db_attr in AttributeSpec.objects.filter(label__task__id=task.id)}\n        for segment in task.segment_set.prefetch_related(\'job_set\').all():\n            db_job = segment.job_set.first()\n            print(""run anno migration for the job {}"".format(db_job.id))\n            process_shapes(db_job, apps, db_labels, db_attributes, db_alias)\n            process_paths(db_job, apps, db_labels, db_attributes, db_alias)\n\ndef _save_old_shapes_to_db(apps, db_shapes, db_attributes, db_alias, db_job):\n    def _get_shape_class(shape_type):\n        if shape_type == \'polygons\':\n            return apps.get_model(\'engine\', \'LabeledPolygon\')\n        elif shape_type == \'polylines\':\n            return apps.get_model(\'engine\', \'LabeledPolyline\')\n        elif shape_type == \'boxes\':\n            return apps.get_model(\'engine\', \'LabeledBox\')\n        elif shape_type == \'points\':\n            return apps.get_model(\'engine\', \'LabeledPoints\')\n\n    def _get_shape_attr_class(shape_type):\n        if shape_type == \'polygons\':\n            return apps.get_model(\'engine\', \'LabeledPolygonAttributeVal\')\n        elif shape_type == \'polylines\':\n            return apps.get_model(\'engine\', \'LabeledPolylineAttributeVal\')\n        elif shape_type == \'boxes\':\n            return apps.get_model(\'engine\', \'LabeledBoxAttributeVal\')\n        elif shape_type == \'points\':\n            return apps.get_model(\'engine\', \'LabeledPointsAttributeVal\')\n\n    shapes = [\n        list(filter(lambda s: s.type == cvat.apps.engine.models.ShapeType.RECTANGLE, db_shapes)),\n        list(filter(lambda s: s.type == cvat.apps.engine.models.ShapeType.POLYLINE, db_shapes)),\n        list(filter(lambda s: s.type == cvat.apps.engine.models.ShapeType.POLYGON, db_shapes)),\n        list(filter(lambda s: s.type == cvat.apps.engine.models.ShapeType.POINTS, db_shapes)),\n    ]\n    for i, shape_type in enumerate([\'boxes\', \'polylines\', \'polygons\', \'points\']):\n        new_db_shapes = []\n        new_db_attrvals = []\n        for shape in shapes[i]:\n            db_shape = _get_shape_class(shape_type)()\n            db_shape.job = shape.job\n            db_shape.label = shape.label\n            db_shape.group_id = shape.group\n            if shape.type == cvat.apps.engine.models.ShapeType.RECTANGLE:\n                db_shape.xtl = shape.points[0]\n                db_shape.ytl = shape.points[1]\n                db_shape.xbr = shape.points[2]\n                db_shape.ybr = shape.points[3]\n            else:\n                point_iterator = iter(shape.points)\n                db_shape.points =  \' \'.join([\'{},{}\'.format(point, next(point_iterator)) for point in point_iterator])\n            db_shape.frame = shape.frame\n            db_shape.occluded = shape.occluded\n            db_shape.z_order = shape.z_order\n\n            for attr in list(shape.labeledshapeattributeval_set.all()):\n                db_attrval = _get_shape_attr_class(shape_type)()\n                if shape.type == cvat.apps.engine.models.ShapeType.POLYGON:\n                    db_attrval.polygon_id = len(new_db_shapes)\n                elif shape.type == cvat.apps.engine.models.ShapeType.POLYLINE:\n                    db_attrval.polyline_id = len(new_db_shapes)\n                elif shape.type == cvat.apps.engine.models.ShapeType.RECTANGLE:\n                    db_attrval.box_id = len(new_db_shapes)\n                else:\n                    db_attrval.points_id = len(new_db_shapes)\n\n                db_attrval.spec = db_attributes[attr.spec_id]\n                db_attrval.value = attr.value\n                new_db_attrvals.append(db_attrval)\n\n            new_db_shapes.append(db_shape)\n\n        new_db_shapes = _bulk_create(_get_shape_class(shape_type), db_alias, new_db_shapes, {""job_id"": db_job.id})\n\n        for db_attrval in new_db_attrvals:\n            if shape_type == \'polygons\':\n                db_attrval.polygon_id = new_db_shapes[db_attrval.polygon_id].id\n            elif shape_type == \'polylines\':\n                db_attrval.polyline_id = new_db_shapes[db_attrval.polyline_id].id\n            elif shape_type == \'boxes\':\n                db_attrval.box_id = new_db_shapes[db_attrval.box_id].id\n            else:\n                db_attrval.points_id = new_db_shapes[db_attrval.points_id].id\n\n        _bulk_create(_get_shape_attr_class(shape_type), db_alias, new_db_attrvals, {})\n\ndef _save_old_tracks_to_db(apps, db_shapes, db_attributes, db_alias, db_job):\n    def _get_shape_class(shape_type):\n        if shape_type == \'polygon_paths\':\n            return apps.get_model(\'engine\', \'TrackedPolygon\')\n        elif shape_type == \'polyline_paths\':\n            return apps.get_model(\'engine\', \'TrackedPolyline\')\n        elif shape_type == \'box_paths\':\n            return apps.get_model(\'engine\', \'TrackedBox\')\n        elif shape_type == \'points_paths\':\n            return apps.get_model(\'engine\', \'TrackedPoints\')\n\n    def _get_shape_attr_class(shape_type):\n        if shape_type == \'polygon_paths\':\n            return apps.get_model(\'engine\', \'TrackedPolygonAttributeVal\')\n        elif shape_type == \'polyline_paths\':\n            return apps.get_model(\'engine\', \'TrackedPolylineAttributeVal\')\n        elif shape_type == \'box_paths\':\n            return apps.get_model(\'engine\', \'TrackedBoxAttributeVal\')\n        elif shape_type == \'points_paths\':\n            return apps.get_model(\'engine\', \'TrackedPointsAttributeVal\')\n\n    tracks = [\n        list(filter(lambda t: t.trackedshape_set.first().type == cvat.apps.engine.models.ShapeType.RECTANGLE, db_shapes)),\n        list(filter(lambda t: t.trackedshape_set.first().type == cvat.apps.engine.models.ShapeType.POLYLINE, db_shapes)),\n        list(filter(lambda t: t.trackedshape_set.first().type == cvat.apps.engine.models.ShapeType.POLYGON, db_shapes)),\n        list(filter(lambda t: t.trackedshape_set.first().type == cvat.apps.engine.models.ShapeType.POINTS, db_shapes)),\n    ]\n\n    ObjectPath = apps.get_model(\'engine\', \'ObjectPath\')\n    ObjectPathAttributeVal = apps.get_model(\'engine\', \'ObjectPathAttributeVal\')\n\n    for i, shape_type in enumerate([\'box_paths\', \'polyline_paths\', \'polygon_paths\', \'points_paths\', ]):\n        new_db_paths = []\n        new_db_path_attrvals = []\n        new_db_shapes = []\n        new_db_shape_attrvals = []\n\n        for path in tracks[i]:\n            db_path = ObjectPath()\n            db_path.job = db_job\n            db_path.label = path.label\n            db_path.frame = path.frame\n            db_path.group_id = path.group\n            # db_path.client_id = path.client_id\n            if shape_type == \'polygon_paths\':\n                db_path.shapes = \'polygons\'\n            elif shape_type == \'polyline_paths\':\n                db_path.shapes = \'polylines\'\n            elif shape_type == \'box_paths\':\n                db_path.shapes = \'boxes\'\n            elif shape_type == \'points_paths\':\n                db_path.shapes = \'points\'\n\n            for attr in list(path.labeledtrackattributeval_set.all()):\n                db_attrspec = db_attributes[attr.spec_id]\n                db_attrval = ObjectPathAttributeVal()\n                db_attrval.track_id = len(new_db_paths)\n                db_attrval.spec = db_attrspec\n                db_attrval.value = attr.value\n                new_db_path_attrvals.append(db_attrval)\n\n            for shape in list(path.trackedshape_set.all()):\n                db_shape = _get_shape_class(shape_type)()\n                db_shape.track_id = len(new_db_paths)\n                if shape_type == \'box_paths\':\n                    db_shape.xtl = shape.points[0]\n                    db_shape.ytl = shape.points[1]\n                    db_shape.xbr = shape.points[2]\n                    db_shape.ybr = shape.points[3]\n                else:\n                    point_iterator = iter(shape.points)\n                    db_shape.points =  \' \'.join([\'{},{}\'.format(point, next(point_iterator)) for point in point_iterator])\n\n                db_shape.frame = shape.frame\n                db_shape.occluded = shape.occluded\n                db_shape.z_order = shape.z_order\n                db_shape.outside = shape.outside\n\n                for attr in list(shape.trackedshapeattributeval_set.all()):\n                    db_attrspec = db_attributes[attr.spec_id]\n                    db_attrval = _get_shape_attr_class(shape_type)()\n                    if shape_type == \'polygon_paths\':\n                        db_attrval.polygon_id = len(new_db_shapes)\n                    elif shape_type == \'polyline_paths\':\n                        db_attrval.polyline_id = len(new_db_shapes)\n                    elif shape_type == \'box_paths\':\n                        db_attrval.box_id = len(new_db_shapes)\n                    elif shape_type == \'points_paths\':\n                        db_attrval.points_id = len(new_db_shapes)\n                    db_attrval.spec = db_attrspec\n                    db_attrval.value = attr.value\n                    new_db_shape_attrvals.append(db_attrval)\n\n                new_db_shapes.append(db_shape)\n            new_db_paths.append(db_path)\n\n        new_db_paths = _bulk_create(ObjectPath, db_alias, new_db_paths, {""job_id"": db_job.id})\n\n        for db_attrval in new_db_path_attrvals:\n            db_attrval.track_id = new_db_paths[db_attrval.track_id].id\n        _bulk_create(ObjectPathAttributeVal, db_alias, new_db_path_attrvals, {})\n\n        for db_shape in new_db_shapes:\n            db_shape.track_id = new_db_paths[db_shape.track_id].id\n\n        db_shapes = _bulk_create(_get_shape_class(shape_type), db_alias, new_db_shapes, {""track__job_id"": db_job.id})\n\n        for db_attrval in new_db_shape_attrvals:\n            if shape_type == \'polygon_paths\':\n                db_attrval.polygon_id = db_shapes[db_attrval.polygon_id].id\n            elif shape_type == \'polyline_paths\':\n                db_attrval.polyline_id = db_shapes[db_attrval.polyline_id].id\n            elif shape_type == \'box_paths\':\n                db_attrval.box_id = db_shapes[db_attrval.box_id].id\n            elif shape_type == \'points_paths\':\n                db_attrval.points_id = db_shapes[db_attrval.points_id].id\n\n        _bulk_create(_get_shape_attr_class(shape_type), db_alias, new_db_shape_attrvals, {})\n\ndef copy_annotations_backward(apps, schema_editor):\n    Task = apps.get_model(\'engine\', \'Task\')\n    AttributeSpec = apps.get_model(\'engine\', \'AttributeSpec\')\n    db_alias = schema_editor.connection.alias\n\n    for task in Task.objects.all():\n        db_attributes = {db_attr.id:db_attr for db_attr in AttributeSpec.objects.filter(label__task__id=task.id)}\n        for segment in task.segment_set.prefetch_related(\'job_set\').all():\n            db_job = segment.job_set.first()\n\n            db_shapes = list(db_job.labeledshape_set\n                    .prefetch_related(""label"")\n                    .prefetch_related(""labeledshapeattributeval_set""))\n            _save_old_shapes_to_db(apps, db_shapes, db_attributes, db_alias, db_job)\n\n            db_tracks = list(db_job.labeledtrack_set\n                .select_related(""label"")\n                .prefetch_related(""labeledtrackattributeval_set"")\n                .prefetch_related(""trackedshape_set__trackedshapeattributeval_set""))\n            _save_old_tracks_to_db(apps, db_tracks, db_attributes, db_alias, db_job)\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\'engine\', \'0016_attribute_spec_20190217\'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name=\'LabeledImageAttributeVal\',\n            fields=[\n                (\'id\', models.BigAutoField(primary_key=True, serialize=False)),\n                (\'value\', cvat.apps.engine.models.SafeCharField(max_length=64)),\n                (\'spec\', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.AttributeSpec\')),\n            ],\n            options={\n                \'abstract\': False,\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.CreateModel(\n            name=\'LabeledShapeAttributeVal\',\n            fields=[\n                (\'id\', models.BigAutoField(primary_key=True, serialize=False)),\n                (\'value\', cvat.apps.engine.models.SafeCharField(max_length=64)),\n                (\'spec\', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.AttributeSpec\')),\n            ],\n            options={\n                \'abstract\': False,\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.CreateModel(\n            name=\'LabeledTrackAttributeVal\',\n            fields=[\n                (\'id\', models.BigAutoField(primary_key=True, serialize=False)),\n                (\'value\', cvat.apps.engine.models.SafeCharField(max_length=64)),\n                (\'spec\', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.AttributeSpec\')),\n            ],\n            options={\n                \'abstract\': False,\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.CreateModel(\n            name=\'TrackedShape\',\n            fields=[\n                (\'type\', models.CharField(choices=[(\'rectangle\', \'RECTANGLE\'), (\'polygon\', \'POLYGON\'), (\'polyline\', \'POLYLINE\'), (\'points\', \'POINTS\')], max_length=16)),\n                (\'occluded\', models.BooleanField(default=False)),\n                (\'z_order\', models.IntegerField(default=0)),\n                (\'points\', cvat.apps.engine.models.FloatArrayField()),\n                (\'id\', models.BigAutoField(primary_key=True, serialize=False)),\n                (\'frame\', models.PositiveIntegerField()),\n                (\'outside\', models.BooleanField(default=False)),\n            ],\n            options={\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.CreateModel(\n            name=\'TrackedShapeAttributeVal\',\n            fields=[\n                (\'id\', models.BigAutoField(primary_key=True, serialize=False)),\n                (\'value\', cvat.apps.engine.models.SafeCharField(max_length=64)),\n                (\'shape\', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.TrackedShape\')),\n                (\'spec\', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.AttributeSpec\')),\n            ],\n            options={\n                \'abstract\': False,\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.CreateModel(\n            name=\'LabeledImage\',\n            fields=[\n                (\'id\', models.BigAutoField(primary_key=True, serialize=False)),\n                (\'frame\', models.PositiveIntegerField()),\n                (\'group\', models.PositiveIntegerField(null=True)),\n            ],\n            options={\n                \'abstract\': False,\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.CreateModel(\n            name=\'LabeledShape\',\n            fields=[\n                (\'id\', models.BigAutoField(primary_key=True, serialize=False)),\n                (\'frame\', models.PositiveIntegerField()),\n                (\'group\', models.PositiveIntegerField(null=True)),\n                (\'type\', models.CharField(choices=[(\'rectangle\', \'RECTANGLE\'), (\'polygon\', \'POLYGON\'), (\'polyline\', \'POLYLINE\'), (\'points\', \'POINTS\')], max_length=16)),\n                (\'occluded\', models.BooleanField(default=False)),\n                (\'z_order\', models.IntegerField(default=0)),\n                (\'points\', cvat.apps.engine.models.FloatArrayField()),\n            ],\n            options={\n                \'abstract\': False,\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.CreateModel(\n            name=\'LabeledTrack\',\n            fields=[\n                (\'id\', models.BigAutoField(primary_key=True, serialize=False)),\n                (\'frame\', models.PositiveIntegerField()),\n                (\'group\', models.PositiveIntegerField(null=True)),\n            ],\n            options={\n                \'abstract\': False,\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.AddField(\n            model_name=\'labeledimage\',\n            name=\'job\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.Job\'),\n        ),\n        migrations.AddField(\n            model_name=\'labeledtrack\',\n            name=\'job\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.Job\'),\n        ),\n        migrations.AddField(\n            model_name=\'labeledshape\',\n            name=\'job\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.Job\'),\n        ),\n        migrations.AddField(\n            model_name=\'labeledimage\',\n            name=\'label\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.Label\'),\n        ),\n        migrations.AddField(\n            model_name=\'labeledshape\',\n            name=\'label\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.Label\'),\n        ),\n        migrations.AddField(\n            model_name=\'labeledtrack\',\n            name=\'label\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.Label\'),\n        ),\n        migrations.AddField(\n            model_name=\'trackedshape\',\n            name=\'track\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.LabeledTrack\'),\n        ),\n        migrations.AddField(\n            model_name=\'labeledtrackattributeval\',\n            name=\'track\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.LabeledTrack\'),\n        ),\n        migrations.AddField(\n            model_name=\'labeledshapeattributeval\',\n            name=\'shape\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.LabeledShape\'),\n        ),\n        migrations.AddField(\n            model_name=\'labeledimageattributeval\',\n            name=\'image\',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=\'engine.LabeledImage\'),\n        ),\n        migrations.RunPython(\n            code=copy_annotations_forward,\n            reverse_code=copy_annotations_backward,\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledbox\',\n            name=\'job\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledbox\',\n            name=\'label\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledboxattributeval\',\n            name=\'box\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledboxattributeval\',\n            name=\'spec\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpoints\',\n            name=\'job\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpoints\',\n            name=\'label\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpointsattributeval\',\n            name=\'points\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpointsattributeval\',\n            name=\'spec\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpolygon\',\n            name=\'job\',\n        ),\n        migrations.RemoveField(\n            model_name=\'job\',\n            name=\'max_shape_id\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpolygon\',\n            name=\'label\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpolygonattributeval\',\n            name=\'polygon\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpolygonattributeval\',\n            name=\'spec\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpolyline\',\n            name=\'job\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpolyline\',\n            name=\'label\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpolylineattributeval\',\n            name=\'polyline\',\n        ),\n        migrations.RemoveField(\n            model_name=\'labeledpolylineattributeval\',\n            name=\'spec\',\n        ),\n        migrations.RemoveField(\n            model_name=\'objectpath\',\n            name=\'job\',\n        ),\n        migrations.RemoveField(\n            model_name=\'objectpath\',\n            name=\'label\',\n        ),\n        migrations.RemoveField(\n            model_name=\'objectpathattributeval\',\n            name=\'spec\',\n        ),\n        migrations.RemoveField(\n            model_name=\'objectpathattributeval\',\n            name=\'track\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedbox\',\n            name=\'track\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedboxattributeval\',\n            name=\'box\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedboxattributeval\',\n            name=\'spec\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpoints\',\n            name=\'track\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpointsattributeval\',\n            name=\'points\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpointsattributeval\',\n            name=\'spec\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpolygon\',\n            name=\'track\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpolygonattributeval\',\n            name=\'polygon\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpolygonattributeval\',\n            name=\'spec\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpolyline\',\n            name=\'track\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpolylineattributeval\',\n            name=\'polyline\',\n        ),\n        migrations.RemoveField(\n            model_name=\'trackedpolylineattributeval\',\n            name=\'spec\',\n        ),\n        migrations.DeleteModel(\n            name=\'LabeledBox\',\n        ),\n        migrations.DeleteModel(\n            name=\'LabeledBoxAttributeVal\',\n        ),\n        migrations.DeleteModel(\n            name=\'LabeledPoints\',\n        ),\n        migrations.DeleteModel(\n            name=\'LabeledPointsAttributeVal\',\n        ),\n        migrations.DeleteModel(\n            name=\'LabeledPolygon\',\n        ),\n        migrations.DeleteModel(\n            name=\'LabeledPolygonAttributeVal\',\n        ),\n        migrations.DeleteModel(\n            name=\'LabeledPolyline\',\n        ),\n        migrations.DeleteModel(\n            name=\'LabeledPolylineAttributeVal\',\n        ),\n        migrations.DeleteModel(\n            name=\'ObjectPath\',\n        ),\n        migrations.DeleteModel(\n            name=\'ObjectPathAttributeVal\',\n        ),\n        migrations.DeleteModel(\n            name=\'TrackedBox\',\n        ),\n        migrations.DeleteModel(\n            name=\'TrackedBoxAttributeVal\',\n        ),\n        migrations.DeleteModel(\n            name=\'TrackedPoints\',\n        ),\n        migrations.DeleteModel(\n            name=\'TrackedPointsAttributeVal\',\n        ),\n        migrations.DeleteModel(\n            name=\'TrackedPolygon\',\n        ),\n        migrations.DeleteModel(\n            name=\'TrackedPolygonAttributeVal\',\n        ),\n        migrations.DeleteModel(\n            name=\'TrackedPolyline\',\n        ),\n        migrations.DeleteModel(\n            name=\'TrackedPolylineAttributeVal\',\n        ),\n    ]\n'"
cvat/apps/engine/migrations/0018_jobcommit.py,0,"b""# Generated by Django 2.1.7 on 2019-04-17 09:25\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n        ('engine', '0017_db_redesign_20190221'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='JobCommit',\n            fields=[\n                ('id', models.BigAutoField(primary_key=True, serialize=False)),\n                ('version', models.PositiveIntegerField(default=0)),\n                ('timestamp', models.DateTimeField(auto_now=True)),\n                ('message', models.CharField(default='', max_length=4096)),\n                ('author', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL)),\n                ('job', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='commits', to='engine.Job')),\n            ],\n            options={\n                'abstract': False,\n                'default_permissions': (),\n            },\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0019_frame_selection.py,0,"b""# Generated by Django 2.1.7 on 2019-05-10 08:23\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0018_jobcommit'),\n    ]\n\n    operations = [\n        migrations.RemoveField(\n            model_name='video',\n            name='start_frame',\n        ),\n        migrations.RemoveField(\n            model_name='video',\n            name='step',\n        ),\n        migrations.RemoveField(\n            model_name='video',\n            name='stop_frame',\n        ),\n        migrations.AddField(\n            model_name='task',\n            name='frame_filter',\n            field=models.CharField(default='', max_length=256),\n        ),\n        migrations.AddField(\n            model_name='task',\n            name='start_frame',\n            field=models.PositiveIntegerField(default=0),\n        ),\n        migrations.AddField(\n            model_name='task',\n            name='stop_frame',\n            field=models.PositiveIntegerField(default=0),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0020_remove_task_flipped.py,0,"b'# Generated by Django 2.1.7 on 2019-06-18 11:08\n\nfrom django.db import migrations\nfrom django.conf import settings\n\nfrom cvat.apps.engine.models import Job, ShapeType\nfrom cvat.apps.engine.media_extractors import get_mime\n\nfrom PIL import Image\nfrom ast import literal_eval\nimport os\n\ndef make_image_meta_cache(db_task):\n    with open(db_task.get_image_meta_cache_path(), \'w\') as meta_file:\n        cache = {\n            \'original_size\': []\n        }\n\n        if db_task.mode == \'interpolation\':\n            image = Image.open(db_task.get_frame_path(0))\n            cache[\'original_size\'].append({\n                \'width\': image.size[0],\n                \'height\': image.size[1]\n            })\n            image.close()\n        else:\n            filenames = []\n            for root, _, files in os.walk(db_task.get_upload_dirname()):\n                fullnames = map(lambda f: os.path.join(root, f), files)\n                images = filter(lambda x: get_mime(x) == \'image\', fullnames)\n                filenames.extend(images)\n            filenames.sort()\n\n            for image_path in filenames:\n                image = Image.open(image_path)\n                cache[\'original_size\'].append({\n                    \'width\': image.size[0],\n                    \'height\': image.size[1]\n                })\n                image.close()\n\n        meta_file.write(str(cache))\n\n\ndef get_image_meta_cache(db_task):\n    try:\n        with open(db_task.get_image_meta_cache_path()) as meta_cache_file:\n            return literal_eval(meta_cache_file.read())\n    except Exception:\n        make_image_meta_cache(db_task)\n        with open(db_task.get_image_meta_cache_path()) as meta_cache_file:\n            return literal_eval(meta_cache_file.read())\n\n\ndef _flip_shape(shape, size):\n    if shape.type == ShapeType.RECTANGLE:\n        shape.points = [\n            shape.points[2], # xbr -> xtl\n            shape.points[3], # ybr -> ytl\n            shape.points[0], # xtl -> xbr\n            shape.points[1]  # ytl -> ybr\n        ]\n\n    for x in range(0, len(shape.points), 2):\n        y = x + 1\n        shape.points[x] = size[\'width\'] - shape.points[x]\n        shape.points[y] = size[\'height\'] - shape.points[y]\n\n\ndef frame_path(db_task, frame):\n    task_dirname = os.path.join(settings.DATA_ROOT, str(db_task.id))\n    d1 = str(int(frame) // 10000)\n    d2 = str(int(frame) // 100)\n    path = os.path.join(task_dirname, \'data\', d1, d2, str(frame) + \'.jpg\')\n    return path\n\n\ndef _get_image_meta_cache_path(self):\n    task_dirname = os.path.join(settings.DATA_ROOT, str(self.id))\n    return os.path.join(task_dirname, ""image_meta.cache"")\n\n\ndef forwards_func(apps, schema_editor):\n    Task = apps.get_model(\'engine\', \'Task\')\n\n    # class methods unavailable in the class which got via get_model()\n    # nevertheless it is needed for us to use the function get_image_meta_cache()\n    setattr(Task, \'get_image_meta_cache_path\', _get_image_meta_cache_path)\n\n    print(\'Getting flipped tasks...\')\n    db_flipped_tasks = Task.objects.prefetch_related(\n        \'image_set\',\n    ).filter(flipped=True).all()\n\n    print(\'Conversion started...\')\n    for db_task in db_flipped_tasks:\n        print(\'Processing task {}...\'.format(db_task.id))\n        db_image_by_frame = {}\n        if db_task.mode == \'annotation\':\n            db_image_by_frame = {db_image.frame: {\'width\': db_image.width, \'height\': db_image.height}\n                for db_image in db_task.image_set.all()}\n        else:\n            im_meta_data = get_image_meta_cache(db_task)[\'original_size\']\n            db_image_by_frame = {\n                0: {\n                    \'width\': im_meta_data[0][\'width\'],\n                    \'height\': im_meta_data[0][\'height\']\n                }\n            }\n\n\n        def get_size(frame):\n            if frame in db_image_by_frame:\n                return db_image_by_frame[frame]\n            else:\n                return db_image_by_frame[0]\n\n        db_jobs = Job.objects.select_related(\'segment\').prefetch_related(\n            \'labeledshape_set\',\n            \'labeledtrack_set\',\n            \'labeledtrack_set__trackedshape_set\').filter(segment__task_id=db_task.id).all()\n\n        for db_job in db_jobs:\n            db_shapes = db_job.labeledshape_set.all()\n            db_tracks = db_job.labeledtrack_set.all()\n            for db_shape in db_shapes:\n                _flip_shape(db_shape, get_size(db_shape.frame))\n                db_shape.save()\n\n            for db_track in db_tracks:\n                db_shapes = db_track.trackedshape_set.all()\n                for db_shape in db_shapes:\n                    _flip_shape(db_shape, get_size(db_shape.frame))\n                    db_shape.save()\n\n    for db_task in db_flipped_tasks:\n        for frame in range(db_task.size):\n            path = frame_path(db_task, frame)\n            if os.path.islink(path):\n                path = os.path.realpath(path)\n\n            try:\n                image = Image.open(path)\n                image = image.transpose(Image.ROTATE_180)\n                image.save(path)\n            except IOError as ex:\n                print(\'Error of handling the frame {}\'.format(frame))\n                print(ex)\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\'engine\', \'0019_frame_selection\'),\n    ]\n\n    operations = [\n        migrations.RunPython(\n            forwards_func\n        ),\n\n        migrations.RemoveField(\n            model_name=\'task\',\n            name=\'flipped\',\n        ),\n    ]\n'"
cvat/apps/engine/migrations/0021_auto_20190826_1827.py,0,"b""# Generated by Django 2.2.4 on 2019-08-26 15:27\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0020_remove_task_flipped'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='task',\n            name='frame_filter',\n            field=models.CharField(blank=True, default='', max_length=256),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0022_auto_20191004_0817.py,0,"b""# Generated by Django 2.2.3 on 2019-10-04 08:17\n\nimport cvat.apps.engine.models\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n        ('engine', '0021_auto_20190826_1827'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Project',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', cvat.apps.engine.models.SafeCharField(max_length=256)),\n                ('bug_tracker', models.CharField(blank=True, default='', max_length=2000)),\n                ('created_date', models.DateTimeField(auto_now_add=True)),\n                ('updated_date', models.DateTimeField(auto_now_add=True)),\n                ('status', models.CharField(choices=[('annotation', 'ANNOTATION'), ('validation', 'VALIDATION'), ('completed', 'COMPLETED')], default=cvat.apps.engine.models.StatusChoice('annotation'), max_length=32)),\n                ('assignee', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='+', to=settings.AUTH_USER_MODEL)),\n                ('owner', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='+', to=settings.AUTH_USER_MODEL)),\n            ],\n            options={\n                'default_permissions': (),\n            },\n        ),\n        migrations.AddField(\n            model_name='task',\n            name='project',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, related_name='tasks', related_query_name='task', to='engine.Project'),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0023_auto_20200113_1323.py,0,"b""# Generated by Django 2.2.8 on 2020-01-13 13:23\n\nimport cvat.apps.engine.models\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0022_auto_20191004_0817'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='labeledimageattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=4096),\n        ),\n        migrations.AlterField(\n            model_name='labeledshapeattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=4096),\n        ),\n        migrations.AlterField(\n            model_name='labeledtrackattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=4096),\n        ),\n        migrations.AlterField(\n            model_name='trackedshapeattributeval',\n            name='value',\n            field=cvat.apps.engine.models.SafeCharField(max_length=4096),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/0024_auto_20191023_1025.py,0,"b'# Generated by Django 2.2.4 on 2019-10-23 10:25\n\nimport os\nimport re\nimport shutil\nimport glob\nimport logging\nimport sys\nimport traceback\nimport itertools\nimport multiprocessing\nimport time\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\nfrom django.conf import settings\n\nfrom cvat.apps.engine.media_extractors import (VideoReader, ArchiveReader, ZipReader,\n    PdfReader , ImageListReader, Mpeg4ChunkWriter,\n    ZipChunkWriter, ZipCompressedChunkWriter, get_mime)\nfrom cvat.apps.engine.models import DataChoice\n\nMIGRATION_THREAD_COUNT = 2\n\ndef fix_path(path):\n        ind = path.find(\'.upload\')\n        if ind != -1:\n            path = path[ind + len(\'.upload\') + 1:]\n        return path\n\ndef get_frame_step(frame_filter):\n    match = re.search(""step\\s*=\\s*([1-9]\\d*)"", frame_filter)\n    return int(match.group(1)) if match else 1\n\ndef get_task_on_disk():\n    folders = [os.path.relpath(f, settings.DATA_ROOT)\n        for f in glob.glob(os.path.join(settings.DATA_ROOT, \'*\'), recursive=False)]\n\n    return set(int(f) for f in folders if f.isdigit())\n\ndef get_frame_path(task_data_dir, frame):\n    d1 = str(int(frame) // 10000)\n    d2 = str(int(frame) // 100)\n    path = os.path.join(task_data_dir, d1, d2,\n        str(frame) + \'.jpg\')\n\n    return path\n\ndef slice_by_size(frames, size):\n    it = itertools.islice(frames, 0, None)\n    frames = list(itertools.islice(it, 0, size , 1))\n    while frames:\n        yield frames\n        frames = list(itertools.islice(it, 0, size, 1))\n\ndef migrate_task_data(db_task_id, db_data_id, original_video, original_images, size, start_frame,\n    stop_frame, frame_filter, image_quality, chunk_size, return_dict):\n    try:\n        db_data_dir = os.path.join(settings.MEDIA_DATA_ROOT, str(db_data_id))\n        compressed_cache_dir = os.path.join(db_data_dir, \'compressed\')\n        original_cache_dir = os.path.join(db_data_dir, \'original\')\n        old_db_task_dir = os.path.join(settings.DATA_ROOT, str(db_task_id))\n        old_task_data_dir = os.path.join(old_db_task_dir, \'data\')\n        if os.path.exists(old_task_data_dir) and size != 0:\n            if original_video:\n                if os.path.exists(original_video):\n                    _stop_frame = stop_frame if stop_frame else None\n                    reader = VideoReader([original_video], get_frame_step(frame_filter), start_frame, _stop_frame)\n                    original_chunk_writer = Mpeg4ChunkWriter(100)\n                    compressed_chunk_writer = ZipCompressedChunkWriter(image_quality)\n\n                    counter = itertools.count()\n                    generator = itertools.groupby(reader, lambda x: next(counter) // chunk_size)\n                    for chunk_idx, chunk_images in generator:\n                        chunk_images = list(chunk_images)\n                        original_chunk_path = os.path.join(original_cache_dir, \'{}.mp4\'.format(chunk_idx))\n                        original_chunk_writer.save_as_chunk(chunk_images, original_chunk_path)\n\n                        compressed_chunk_path = os.path.join(compressed_cache_dir, \'{}.zip\'.format(chunk_idx))\n                        compressed_chunk_writer.save_as_chunk(chunk_images, compressed_chunk_path)\n\n                    preview = reader.get_preview()\n                    preview.save(os.path.join(db_data_dir, \'preview.jpeg\'))\n                else:\n                    original_chunk_writer = ZipChunkWriter(100)\n                    for chunk_idx, chunk_image_ids in enumerate(slice_by_size(range(size), chunk_size)):\n                        chunk_images = []\n                        for image_id in chunk_image_ids:\n                            image_path = get_frame_path(old_task_data_dir, image_id)\n                            chunk_images.append((image_path, image_path))\n\n                        original_chunk_path = os.path.join(original_cache_dir, \'{}.zip\'.format(chunk_idx))\n                        original_chunk_writer.save_as_chunk(chunk_images, original_chunk_path)\n\n                        compressed_chunk_path = os.path.join(compressed_cache_dir, \'{}.zip\'.format(chunk_idx))\n                        os.symlink(original_chunk_path, compressed_chunk_path)\n                        shutil.copyfile(get_frame_path(old_task_data_dir, image_id), os.path.join(db_data_dir, \'preview.jpeg\'))\n            else:\n                reader = None\n                if os.path.exists(original_images[0]): # task created from images\n                    reader = ImageListReader(original_images)\n                else: # task created from archive or pdf\n                    archives = []\n                    pdfs = []\n                    zips = []\n                    for p in glob.iglob(os.path.join(db_data_dir, \'raw\', \'**\', \'*\'), recursive=True):\n                        mime_type = get_mime(p)\n                        if mime_type == \'archive\':\n                            archives.append(p)\n                        elif mime_type == \'pdf\':\n                            pdfs.append(p)\n                        elif mime_type == \'zip\':\n                            zips.append(p)\n                    if archives:\n                        reader = ArchiveReader(archives)\n                    elif zips:\n                        reader = ZipReader(archives)\n                    elif pdfs:\n                        reader = PdfReader(pdfs)\n\n                if not reader:\n                    original_chunk_writer = ZipChunkWriter(100)\n                    for chunk_idx, chunk_image_ids in enumerate(slice_by_size(range(size), chunk_size)):\n                        chunk_images = []\n                        for image_id in chunk_image_ids:\n                            image_path = get_frame_path(old_task_data_dir, image_id)\n                            chunk_images.append((image_path, image_path))\n\n                        original_chunk_path = os.path.join(original_cache_dir, \'{}.zip\'.format(chunk_idx))\n                        original_chunk_writer.save_as_chunk(chunk_images, original_chunk_path)\n\n                        compressed_chunk_path = os.path.join(compressed_cache_dir, \'{}.zip\'.format(chunk_idx))\n                        os.symlink(original_chunk_path, compressed_chunk_path)\n                        shutil.copyfile(get_frame_path(old_task_data_dir, image_id), os.path.join(db_data_dir, \'preview.jpeg\'))\n                else:\n                    original_chunk_writer = ZipChunkWriter(100)\n                    compressed_chunk_writer = ZipCompressedChunkWriter(image_quality)\n\n                    counter = itertools.count()\n                    generator = itertools.groupby(reader, lambda x: next(counter) // chunk_size)\n                    for chunk_idx, chunk_images in generator:\n                        chunk_images = list(chunk_images)\n                        compressed_chunk_path = os.path.join(compressed_cache_dir, \'{}.zip\'.format(chunk_idx))\n                        compressed_chunk_writer.save_as_chunk(chunk_images, compressed_chunk_path)\n\n                        original_chunk_path = os.path.join(original_cache_dir, \'{}.zip\'.format(chunk_idx))\n                        original_chunk_writer.save_as_chunk(chunk_images, original_chunk_path)\n\n                    preview = reader.get_preview()\n                    preview.save(os.path.join(db_data_dir, \'preview.jpeg\'))\n            shutil.rmtree(old_db_task_dir)\n        return_dict[db_task_id] = (True, \'\')\n    except Exception as e:\n        traceback.print_exc(file=sys.stderr)\n        return_dict[db_task_id] = (False, str(e))\n    return 0\n\ndef migrate_task_schema(db_task, Data, log):\n    log.info(\'Start schema migration of task ID {}.\'.format(db_task.id))\n    try:\n        # create folders\n        new_task_dir = os.path.join(settings.TASKS_ROOT, str(db_task.id))\n        os.makedirs(new_task_dir, exist_ok=True)\n        os.makedirs(os.path.join(new_task_dir, \'artifacts\'),  exist_ok=True)\n        new_task_logs_dir = os.path.join(new_task_dir, \'logs\')\n        os.makedirs(new_task_logs_dir,  exist_ok=True)\n\n        # create Data object\n        db_data = Data.objects.create(\n            size=db_task.size,\n            image_quality=db_task.image_quality,\n            start_frame=db_task.start_frame,\n            stop_frame=db_task.stop_frame,\n            frame_filter=db_task.frame_filter,\n            compressed_chunk_type = DataChoice.IMAGESET,\n            original_chunk_type = DataChoice.VIDEO if db_task.mode == \'interpolation\' else DataChoice.IMAGESET,\n        )\n        db_data.save()\n\n        db_task.data = db_data\n\n        db_data_dir = os.path.join(settings.MEDIA_DATA_ROOT, str(db_data.id))\n        os.makedirs(db_data_dir, exist_ok=True)\n        compressed_cache_dir = os.path.join(db_data_dir, \'compressed\')\n        os.makedirs(compressed_cache_dir, exist_ok=True)\n\n        original_cache_dir = os.path.join(db_data_dir, \'original\')\n        os.makedirs(original_cache_dir, exist_ok=True)\n\n        old_db_task_dir = os.path.join(settings.DATA_ROOT, str(db_task.id))\n\n        # move logs\n        for log_file in (\'task.log\', \'client.log\'):\n            task_log_file = os.path.join(old_db_task_dir, log_file)\n            if os.path.isfile(task_log_file):\n                shutil.move(task_log_file, new_task_logs_dir)\n\n        if hasattr(db_task, \'video\'):\n            db_task.video.data = db_data\n            db_task.video.path = fix_path(db_task.video.path)\n            db_task.video.save()\n\n        for db_image in db_task.image_set.all():\n            db_image.data = db_data\n            db_image.path = fix_path(db_image.path)\n            db_image.save()\n\n        old_raw_dir = os.path.join(old_db_task_dir, \'.upload\')\n        new_raw_dir = os.path.join(db_data_dir, \'raw\')\n\n        for client_file in db_task.clientfile_set.all():\n            client_file.file = client_file.file.path.replace(old_raw_dir, new_raw_dir)\n            client_file.save()\n\n        for server_file in db_task.serverfile_set.all():\n            server_file.file = server_file.file.replace(old_raw_dir, new_raw_dir)\n            server_file.save()\n\n        for remote_file in db_task.remotefile_set.all():\n            remote_file.file = remote_file.file.replace(old_raw_dir, new_raw_dir)\n            remote_file.save()\n\n        db_task.save()\n\n        #move old raw data\n        if os.path.exists(old_raw_dir):\n            shutil.move(old_raw_dir, new_raw_dir)\n\n        return (db_task.id, db_data.id)\n\n    except Exception as e:\n        log.error(\'Cannot migrate schema for the task: {}\'.format(db_task.id))\n        log.error(str(e))\n        traceback.print_exc(file=sys.stderr)\n\ndef create_data_objects(apps, schema_editor):\n    migration_name = os.path.splitext(os.path.basename(__file__))[0]\n    migration_log_file = \'{}.log\'.format(migration_name)\n    stdout = sys.stdout\n    stderr = sys.stderr\n    # redirect all stdout to the file\n    log_file_object = open(os.path.join(settings.MIGRATIONS_LOGS_ROOT, migration_log_file), \'w\')\n    sys.stdout = log_file_object\n    sys.stderr = log_file_object\n\n    log = logging.getLogger(migration_name)\n    log.addHandler(logging.StreamHandler(stdout))\n    log.addHandler(logging.StreamHandler(log_file_object))\n    log.setLevel(logging.INFO)\n\n    disk_tasks = get_task_on_disk()\n\n    Task = apps.get_model(\'engine\', \'Task\')\n    Data = apps.get_model(\'engine\', \'Data\')\n\n    db_tasks = Task.objects\n    task_count = db_tasks.count()\n    log.info(\'\\nStart schema migration...\')\n    migrated_db_tasks = []\n    for counter, db_task in enumerate(db_tasks.all().iterator()):\n        res = migrate_task_schema(db_task, Data, log)\n        log.info(\'Schema migration for the task {} completed. Progress {}/{}\'.format(db_task.id, counter+1, task_count))\n        if res:\n            migrated_db_tasks.append(res)\n\n    log.info(\'\\nSchema migration is finished...\')\n    log.info(\'\\nStart data migration...\')\n\n    manager = multiprocessing.Manager()\n    return_dict = manager.dict()\n\n    def create_process(db_task_id, db_data_id):\n        db_data = Data.objects.get(pk=db_data_id)\n        db_data_dir = os.path.join(settings.MEDIA_DATA_ROOT, str(db_data_id))\n        new_raw_dir = os.path.join(db_data_dir, \'raw\')\n\n        original_video = None\n        original_images = None\n        if hasattr(db_data, \'video\'):\n            original_video = os.path.join(new_raw_dir, db_data.video.path)\n        else:\n            original_images = [os.path.realpath(os.path.join(new_raw_dir, db_image.path)) for db_image in db_data.images.all()]\n\n        args = (db_task_id, db_data_id, original_video, original_images, db_data.size,\n            db_data.start_frame, db_data.stop_frame, db_data.frame_filter, db_data.image_quality, db_data.chunk_size, return_dict)\n\n        return multiprocessing.Process(target=migrate_task_data, args=args)\n\n    results = {}\n    task_idx = 0\n    while True:\n        for res_idx in list(results.keys()):\n            res = results[res_idx]\n            if not res.is_alive():\n                del results[res_idx]\n                if res.exitcode == 0:\n                    ret_code, message = return_dict[res_idx]\n                    if ret_code:\n                        counter = (task_idx - len(results))\n                        progress = (100 * counter) / task_count\n                        log.info(\'Data migration for the task {} completed. Progress: {:.02f}% | {}/{}.\'.format(res_idx, progress, counter, task_count))\n                    else:\n                        log.error(\'Cannot migrate data for the task: {}\'.format(res_idx))\n                        log.error(str(message))\n                    if res_idx in disk_tasks:\n                        disk_tasks.remove(res_idx)\n                else:\n                    log.error(\'#Cannot migrate data for the task: {}\'.format(res_idx))\n\n        while task_idx < len(migrated_db_tasks) and len(results) < MIGRATION_THREAD_COUNT:\n            log.info(\'Start data migration for the task {}, data ID {}\'.format(migrated_db_tasks[task_idx][0], migrated_db_tasks[task_idx][1]))\n            results[migrated_db_tasks[task_idx][0]] = create_process(*migrated_db_tasks[task_idx])\n            results[migrated_db_tasks[task_idx][0]].start()\n            task_idx += 1\n\n        if len(results) == 0:\n            break\n\n        time.sleep(5)\n\n    if disk_tasks:\n        suspicious_tasks_dir = os.path.join(settings.DATA_ROOT, \'suspicious_tasks\')\n        os.makedirs(suspicious_tasks_dir, exist_ok=True)\n        for tid in disk_tasks:\n            suspicious_task_path = os.path.join(settings.DATA_ROOT, str(tid))\n            try:\n                shutil.move(suspicious_task_path, suspicious_tasks_dir)\n            except Exception as e:\n                log.error(\'Cannot move data for the suspicious task {}, \\\n                    that is not represented in the database.\'.format(suspicious_task_path))\n                log.error(str(e))\n\n    # DL models migration\n    if apps.is_installed(\'auto_annotation\'):\n        DLModel = apps.get_model(\'auto_annotation\', \'AnnotationModel\')\n\n        for db_model in DLModel.objects.all():\n            try:\n                old_location = os.path.join(settings.BASE_DIR, \'models\', str(db_model.id))\n                new_location = os.path.join(settings.BASE_DIR, \'data\', \'models\', str(db_model.id))\n\n                if os.path.isdir(old_location):\n                    shutil.move(old_location, new_location)\n\n                    db_model.model_file.name = db_model.model_file.name.replace(old_location, new_location)\n                    db_model.weights_file.name = db_model.weights_file.name.replace(old_location, new_location)\n                    db_model.labelmap_file.name = db_model.labelmap_file.name.replace(old_location, new_location)\n                    db_model.interpretation_file.name = db_model.interpretation_file.name.replace(old_location, new_location)\n\n                    db_model.save()\n            except Exception as e:\n                log.error(\'Cannot migrate data for the DL model: {}\'.format(db_model.id))\n                log.error(str(e))\n\n    log_file_object.close()\n    sys.stdout = stdout\n    sys.stderr = stderr\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\'engine\', \'0023_auto_20200113_1323\'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name=\'Data\',\n            fields=[\n                (\'id\', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=\'ID\')),\n                (\'chunk_size\', models.PositiveIntegerField(default=36)),\n                (\'size\', models.PositiveIntegerField(default=0)),\n                (\'image_quality\', models.PositiveSmallIntegerField(default=50)),\n                (\'start_frame\', models.PositiveIntegerField(default=0)),\n                (\'stop_frame\', models.PositiveIntegerField(default=0)),\n                (\'frame_filter\', models.CharField(blank=True, default=\'\', max_length=256)),\n                (\'compressed_chunk_type\', models.CharField(choices=[(\'video\', \'VIDEO\'), (\'imageset\', \'IMAGESET\'), (\'list\', \'LIST\')], default=DataChoice(\'imageset\'), max_length=32)),\n                (\'original_chunk_type\', models.CharField(choices=[(\'video\', \'VIDEO\'), (\'imageset\', \'IMAGESET\'), (\'list\', \'LIST\')], default=DataChoice(\'imageset\'), max_length=32)),\n            ],\n            options={\n                \'default_permissions\': (),\n            },\n        ),\n        migrations.AddField(\n            model_name=\'task\',\n            name=\'data\',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name=\'tasks\', to=\'engine.Data\'),\n        ),\n        migrations.AddField(\n            model_name=\'image\',\n            name=\'data\',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name=\'images\', to=\'engine.Data\'),\n        ),\n        migrations.AddField(\n            model_name=\'video\',\n            name=\'data\',\n            field=models.OneToOneField(null=True, on_delete=django.db.models.deletion.CASCADE, related_name=\'video\', to=\'engine.Data\'),\n        ),\n        migrations.AddField(\n            model_name=\'clientfile\',\n            name=\'data\',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name=\'client_files\', to=\'engine.Data\'),\n        ),\n        migrations.AddField(\n            model_name=\'remotefile\',\n            name=\'data\',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name=\'remote_files\', to=\'engine.Data\'),\n        ),\n        migrations.AddField(\n            model_name=\'serverfile\',\n            name=\'data\',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name=\'server_files\', to=\'engine.Data\'),\n        ),\n        migrations.RunPython(\n            code=create_data_objects\n        ),\n        migrations.RemoveField(\n            model_name=\'image\',\n            name=\'task\',\n        ),\n        migrations.RemoveField(\n            model_name=\'remotefile\',\n            name=\'task\',\n        ),\n        migrations.RemoveField(\n            model_name=\'serverfile\',\n            name=\'task\',\n        ),\n        migrations.RemoveField(\n            model_name=\'task\',\n            name=\'frame_filter\',\n        ),\n        migrations.RemoveField(\n            model_name=\'task\',\n            name=\'image_quality\',\n        ),\n        migrations.RemoveField(\n            model_name=\'task\',\n            name=\'size\',\n        ),\n        migrations.RemoveField(\n            model_name=\'task\',\n            name=\'start_frame\',\n        ),\n        migrations.RemoveField(\n            model_name=\'task\',\n            name=\'stop_frame\',\n        ),\n        migrations.RemoveField(\n            model_name=\'video\',\n            name=\'task\',\n        ),\n        migrations.AlterField(\n            model_name=\'image\',\n            name=\'path\',\n            field=models.CharField(default=\'\', max_length=1024),\n        ),\n        migrations.AlterField(\n            model_name=\'video\',\n            name=\'path\',\n            field=models.CharField(default=\'\', max_length=1024),\n        ),\n        migrations.AlterUniqueTogether(\n            name=\'clientfile\',\n            unique_together={(\'data\', \'file\')},\n        ),\n        migrations.RemoveField(\n            model_name=\'clientfile\',\n            name=\'task\',\n        ),\n    ]\n'"
cvat/apps/engine/migrations/0025_auto_20200324_1222.py,0,"b""# Generated by Django 2.2.10 on 2020-03-24 12:22\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('engine', '0024_auto_20191023_1025'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='data',\n            name='chunk_size',\n            field=models.PositiveIntegerField(null=True),\n        ),\n    ]\n"""
cvat/apps/engine/migrations/__init__.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n'
cvat/apps/engine/tests/__init__.py,0,b''
cvat/apps/engine/tests/_test_rest_api.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# FIXME: Git application and package name clash in tests\nclass _GitImportFix:\n    import sys\n    former_path = sys.path[:]\n\n    @classmethod\n    def apply(cls):\n        # HACK: fix application and module name clash\n        # \'git\' app is found earlier than a library in the path.\n        # The clash is introduced by unittest discover\n        import sys\n        print(\'apply\')\n\n        apps_dir = __file__[:__file__.rfind(\'/engine/\')]\n        assert \'apps\' in apps_dir\n        try:\n            sys.path.remove(apps_dir)\n        except ValueError:\n            pass\n\n        for name in list(sys.modules):\n            if name.startswith(\'git.\') or name == \'git\':\n                m = sys.modules.pop(name, None)\n                del m\n\n        import git\n        assert apps_dir not in git.__file__\n\n    @classmethod\n    def restore(cls):\n        import sys\n        print(\'restore\')\n\n        for name in list(sys.modules):\n            if name.startswith(\'git.\') or name == \'git\':\n                m = sys.modules.pop(name)\n                del m\n\n        sys.path.insert(0, __file__[:__file__.rfind(\'/engine/\')])\n\n        import importlib\n        importlib.invalidate_caches()\n\ndef _setUpModule():\n    _GitImportFix.apply()\n\n    import sys\n    sys.path.insert(0, __file__[:__file__.rfind(\'/engine/\')])\n\n# def tearDownModule():\n    # _GitImportFix.restore()\n\nimport io\nimport os\nimport os.path as osp\nimport random\nimport shutil\nimport tempfile\nimport xml.etree.ElementTree as ET\nimport zipfile\nfrom collections import defaultdict\nfrom enum import Enum\nfrom glob import glob\nfrom io import BytesIO\nfrom unittest import mock\n\nimport av\nimport numpy as np\nfrom django.conf import settings\nfrom django.contrib.auth.models import Group, User\nfrom PIL import Image\nfrom pycocotools import coco as coco_loader\nfrom rest_framework import status\nfrom rest_framework.test import APIClient, APITestCase\n\nfrom cvat.apps.engine.models import (AttributeType, Data, Job, Project,\n    Segment, StatusChoice, Task)\n\n_setUpModule()\n\ndef create_db_users(cls):\n    (group_admin, _) = Group.objects.get_or_create(name=""admin"")\n    (group_user, _) = Group.objects.get_or_create(name=""user"")\n    (group_annotator, _) = Group.objects.get_or_create(name=""annotator"")\n    (group_observer, _) = Group.objects.get_or_create(name=""observer"")\n\n    user_admin = User.objects.create_superuser(username=""admin"", email="""",\n        password=""admin"")\n    user_admin.groups.add(group_admin)\n    user_owner = User.objects.create_user(username=""user1"", password=""user1"")\n    user_owner.groups.add(group_user)\n    user_assignee = User.objects.create_user(username=""user2"", password=""user2"")\n    user_assignee.groups.add(group_annotator)\n    user_annotator = User.objects.create_user(username=""user3"", password=""user3"")\n    user_annotator.groups.add(group_annotator)\n    user_observer = User.objects.create_user(username=""user4"", password=""user4"")\n    user_observer.groups.add(group_observer)\n    user_dummy = User.objects.create_user(username=""user5"", password=""user5"")\n    user_dummy.groups.add(group_user)\n\n    cls.admin = user_admin\n    cls.owner = cls.user1 = user_owner\n    cls.assignee = cls.user2 = user_assignee\n    cls.annotator = cls.user3 = user_annotator\n    cls.observer = cls.user4 = user_observer\n    cls.user = cls.user5 = user_dummy\n\ndef create_db_task(data):\n    data_settings = {\n        ""size"": data.pop(""size""),\n        ""image_quality"": data.pop(""image_quality""),\n    }\n\n    db_data = Data.objects.create(**data_settings)\n    shutil.rmtree(db_data.get_data_dirname(), ignore_errors=True)\n    os.makedirs(db_data.get_data_dirname())\n    os.makedirs(db_data.get_upload_dirname())\n\n    db_task = Task.objects.create(**data)\n    shutil.rmtree(db_task.get_task_dirname(), ignore_errors=True)\n    os.makedirs(db_task.get_task_dirname())\n    os.makedirs(db_task.get_task_logs_dirname())\n    os.makedirs(db_task.get_task_artifacts_dirname())\n    db_task.data = db_data\n    db_task.save()\n\n    for x in range(0, db_task.data.size, db_task.segment_size):\n        start_frame = x\n        stop_frame = min(x + db_task.segment_size - 1, db_task.data.size - 1)\n\n        db_segment = Segment()\n        db_segment.task = db_task\n        db_segment.start_frame = start_frame\n        db_segment.stop_frame = stop_frame\n        db_segment.save()\n\n        db_job = Job()\n        db_job.segment = db_segment\n        db_job.save()\n\n    return db_task\n\ndef create_dummy_db_tasks(obj, project=None):\n    tasks = []\n\n    data = {\n        ""name"": ""my task #1"",\n        ""owner"": obj.owner,\n        ""assignee"": obj.assignee,\n        ""overlap"": 0,\n        ""segment_size"": 100,\n        ""z_order"": False,\n        ""image_quality"": 75,\n        ""size"": 100,\n        ""project"": project\n    }\n    db_task = create_db_task(data)\n    tasks.append(db_task)\n\n    data = {\n        ""name"": ""my multijob task"",\n        ""owner"": obj.user,\n        ""overlap"": 0,\n        ""segment_size"": 100,\n        ""z_order"": True,\n        ""image_quality"": 50,\n        ""size"": 200,\n        ""project"": project\n    }\n    db_task = create_db_task(data)\n    tasks.append(db_task)\n\n    data = {\n        ""name"": ""my task #2"",\n        ""owner"": obj.owner,\n        ""assignee"": obj.assignee,\n        ""overlap"": 0,\n        ""segment_size"": 100,\n        ""z_order"": False,\n        ""image_quality"": 75,\n        ""size"": 100,\n        ""project"": project\n    }\n    db_task = create_db_task(data)\n    tasks.append(db_task)\n\n    data = {\n        ""name"": ""super task"",\n        ""owner"": obj.admin,\n        ""overlap"": 0,\n        ""segment_size"": 50,\n        ""z_order"": False,\n        ""image_quality"": 95,\n        ""size"": 50,\n        ""project"": project\n    }\n    db_task = create_db_task(data)\n    tasks.append(db_task)\n\n    return tasks\n\ndef create_dummy_db_projects(obj):\n    projects = []\n\n    data = {\n        ""name"": ""my empty project"",\n        ""owner"": obj.owner,\n        ""assignee"": obj.assignee,\n    }\n    db_project = Project.objects.create(**data)\n    projects.append(db_project)\n\n    data = {\n        ""name"": ""my project without assignee"",\n        ""owner"": obj.user,\n    }\n    db_project = Project.objects.create(**data)\n    create_dummy_db_tasks(obj, db_project)\n    projects.append(db_project)\n\n    data = {\n        ""name"": ""my big project"",\n        ""owner"": obj.owner,\n        ""assignee"": obj.assignee,\n    }\n    db_project = Project.objects.create(**data)\n    create_dummy_db_tasks(obj, db_project)\n    projects.append(db_project)\n\n    data = {\n        ""name"": ""public project"",\n    }\n    db_project = Project.objects.create(**data)\n    create_dummy_db_tasks(obj, db_project)\n    projects.append(db_project)\n\n    data = {\n        ""name"": ""super project"",\n        ""owner"": obj.admin,\n        ""assignee"": obj.assignee,\n    }\n    db_project = Project.objects.create(**data)\n    create_dummy_db_tasks(obj, db_project)\n    projects.append(db_project)\n\n    return projects\n\n\nclass ForceLogin:\n    def __init__(self, user, client):\n        self.user = user\n        self.client = client\n\n    def __enter__(self):\n        if self.user:\n            self.client.force_login(self.user, backend=\'django.contrib.auth.backends.ModelBackend\')\n\n        return self\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        if self.user:\n            self.client.logout()\n\n\nclass JobGetAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.task = create_dummy_db_tasks(cls)[0]\n        cls.job = Job.objects.filter(segment__task_id=cls.task.id).first()\n        cls.job.assignee = cls.annotator\n        cls.job.save()\n\n    def _run_api_v1_jobs_id(self, jid, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/jobs/{}\'.format(jid))\n\n        return response\n\n    def _check_request(self, response):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertEqual(response.data[""id""], self.job.id)\n        self.assertEqual(response.data[""status""], StatusChoice.ANNOTATION)\n        self.assertEqual(response.data[""start_frame""], self.job.segment.start_frame)\n        self.assertEqual(response.data[""stop_frame""], self.job.segment.stop_frame)\n\n    def test_api_v1_jobs_id_admin(self):\n        response = self._run_api_v1_jobs_id(self.job.id, self.admin)\n        self._check_request(response)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.admin)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_owner(self):\n        response = self._run_api_v1_jobs_id(self.job.id, self.owner)\n        self._check_request(response)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.owner)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_annotator(self):\n        response = self._run_api_v1_jobs_id(self.job.id, self.annotator)\n        self._check_request(response)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.annotator)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_observer(self):\n        response = self._run_api_v1_jobs_id(self.job.id, self.observer)\n        self._check_request(response)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.observer)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_user(self):\n        response = self._run_api_v1_jobs_id(self.job.id, self.user)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.user)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_no_auth(self):\n        response = self._run_api_v1_jobs_id(self.job.id, None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n\nclass JobUpdateAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n        self.task = create_dummy_db_tasks(self)[0]\n        self.job = Job.objects.filter(segment__task_id=self.task.id).first()\n        self.job.assignee = self.annotator\n        self.job.save()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n\n    def _run_api_v1_jobs_id(self, jid, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.put(\'/api/v1/jobs/{}\'.format(jid), data=data, format=\'json\')\n\n        return response\n\n    def _check_request(self, response, data):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertEqual(response.data[""id""], self.job.id)\n        self.assertEqual(response.data[""status""], data.get(\'status\', self.job.status))\n        assignee = self.job.assignee.id if self.job.assignee else None\n        self.assertEqual(response.data[""assignee""], data.get(\'assignee\', assignee))\n        self.assertEqual(response.data[""start_frame""], self.job.segment.start_frame)\n        self.assertEqual(response.data[""stop_frame""], self.job.segment.stop_frame)\n\n    def test_api_v1_jobs_id_admin(self):\n        data = {""status"": StatusChoice.COMPLETED, ""assignee"": self.owner.id}\n        response = self._run_api_v1_jobs_id(self.job.id, self.admin, data)\n        self._check_request(response, data)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.admin, data)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_owner(self):\n        data = {""status"": StatusChoice.VALIDATION, ""assignee"": self.annotator.id}\n        response = self._run_api_v1_jobs_id(self.job.id, self.owner, data)\n        self._check_request(response, data)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.owner, data)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_annotator(self):\n        data = {""status"": StatusChoice.ANNOTATION, ""assignee"": self.user.id}\n        response = self._run_api_v1_jobs_id(self.job.id, self.annotator, data)\n        self._check_request(response, data)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.annotator, data)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_observer(self):\n        data = {""status"": StatusChoice.ANNOTATION, ""assignee"": self.admin.id}\n        response = self._run_api_v1_jobs_id(self.job.id, self.observer, data)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.observer, data)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_user(self):\n        data = {""status"": StatusChoice.ANNOTATION, ""assignee"": self.user.id}\n        response = self._run_api_v1_jobs_id(self.job.id, self.user, data)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, self.user, data)\n        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)\n\n    def test_api_v1_jobs_id_no_auth(self):\n        data = {""status"": StatusChoice.ANNOTATION, ""assignee"": self.user.id}\n        response = self._run_api_v1_jobs_id(self.job.id, None, data)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n        response = self._run_api_v1_jobs_id(self.job.id + 10, None, data)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass JobPartialUpdateAPITestCase(JobUpdateAPITestCase):\n    def _run_api_v1_jobs_id(self, jid, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.patch(\'/api/v1/jobs/{}\'.format(jid), data=data, format=\'json\')\n\n        return response\n\n    def test_api_v1_jobs_id_annotator_partial(self):\n        data = {""status"": StatusChoice.VALIDATION}\n        response = self._run_api_v1_jobs_id(self.job.id, self.owner, data)\n        self._check_request(response, data)\n\n    def test_api_v1_jobs_id_admin_partial(self):\n        data = {""assignee"": self.user.id}\n        response = self._run_api_v1_jobs_id(self.job.id, self.owner, data)\n        self._check_request(response, data)\n\nclass ServerAboutAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n\n    def _run_api_v1_server_about(self, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/server/about\')\n\n        return response\n\n    def _check_request(self, response):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertIsNotNone(response.data.get(""name"", None))\n        self.assertIsNotNone(response.data.get(""description"", None))\n        self.assertIsNotNone(response.data.get(""version"", None))\n\n    def test_api_v1_server_about_admin(self):\n        response = self._run_api_v1_server_about(self.admin)\n        self._check_request(response)\n\n    def test_api_v1_server_about_user(self):\n        response = self._run_api_v1_server_about(self.user)\n        self._check_request(response)\n\n    def test_api_v1_server_about_no_auth(self):\n        response = self._run_api_v1_server_about(None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass ServerExceptionAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.data = {\n            ""system"": ""Linux"",\n            ""client"": ""rest_framework.APIClient"",\n            ""time"": ""2019-01-29T12:34:56.000000Z"",\n            ""task_id"": 1,\n            ""job_id"": 1,\n            ""proj_id"": 2,\n            ""client_id"": 12321235123,\n            ""message"": ""just test message"",\n            ""filename"": ""http://localhost/my_file.js"",\n            ""line"": 1,\n            ""column"": 1,\n            ""stack"": """"\n        }\n\n    def _run_api_v1_server_exception(self, user):\n        with ForceLogin(user, self.client):\n            #pylint: disable=unused-variable\n            with mock.patch(""cvat.apps.engine.views.clogger"") as clogger:\n                response = self.client.post(\'/api/v1/server/exception\',\n                    self.data, format=\'json\')\n\n        return response\n\n    def test_api_v1_server_exception_admin(self):\n        response = self._run_api_v1_server_exception(self.admin)\n        self.assertEqual(response.status_code, status.HTTP_201_CREATED)\n\n    def test_api_v1_server_exception_user(self):\n        response = self._run_api_v1_server_exception(self.user)\n        self.assertEqual(response.status_code, status.HTTP_201_CREATED)\n\n    def test_api_v1_server_exception_no_auth(self):\n        response = self._run_api_v1_server_exception(None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n\nclass ServerLogsAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.data = [\n        {\n            ""time"": ""2019-01-29T12:34:56.000000Z"",\n            ""task_id"": 1,\n            ""job_id"": 1,\n            ""proj_id"": 2,\n            ""client_id"": 12321235123,\n            ""message"": ""just test message"",\n            ""name"": ""add point"",\n            ""is_active"": True,\n            ""payload"": {""count"": 1}\n        },\n        {\n            ""time"": ""2019-02-24T12:34:56.000000Z"",\n            ""client_id"": 12321235123,\n            ""name"": ""add point"",\n            ""is_active"": True,\n        }]\n\n    def _run_api_v1_server_logs(self, user):\n        with ForceLogin(user, self.client):\n            #pylint: disable=unused-variable\n            with mock.patch(""cvat.apps.engine.views.clogger"") as clogger:\n                response = self.client.post(\'/api/v1/server/logs\',\n                    self.data, format=\'json\')\n\n        return response\n\n    def test_api_v1_server_logs_admin(self):\n        response = self._run_api_v1_server_logs(self.admin)\n        self.assertEqual(response.status_code, status.HTTP_201_CREATED)\n\n    def test_api_v1_server_logs_user(self):\n        response = self._run_api_v1_server_logs(self.user)\n        self.assertEqual(response.status_code, status.HTTP_201_CREATED)\n\n    def test_api_v1_server_logs_no_auth(self):\n        response = self._run_api_v1_server_logs(None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n\nclass UserAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n        create_db_users(self)\n\n    def _check_response(self, user, response, is_full=True):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self._check_data(user, response.data, is_full)\n\n    def _check_data(self, user, data, is_full):\n        self.assertEqual(data[""id""], user.id)\n        self.assertEqual(data[""username""], user.username)\n        self.assertEqual(data[""first_name""], user.first_name)\n        self.assertEqual(data[""last_name""], user.last_name)\n        extra_check = self.assertIn if is_full else self.assertNotIn\n        extra_check(""email"", data)\n        extra_check(""groups"", data)\n        extra_check(""is_staff"", data)\n        extra_check(""is_superuser"", data)\n        extra_check(""is_active"", data)\n        extra_check(""last_login"", data)\n        extra_check(""date_joined"", data)\n\nclass UserListAPITestCase(UserAPITestCase):\n    def _run_api_v1_users(self, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/users\')\n\n        return response\n\n    def _check_response(self, user, response, is_full):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        for user_info in response.data[\'results\']:\n            db_user = getattr(self, user_info[\'username\'])\n            self._check_data(db_user, user_info, is_full)\n\n    def test_api_v1_users_admin(self):\n        response = self._run_api_v1_users(self.admin)\n        self._check_response(self.admin, response, True)\n\n    def test_api_v1_users_user(self):\n        response = self._run_api_v1_users(self.user)\n        self._check_response(self.user, response, False)\n\n    def test_api_v1_users_annotator(self):\n        response = self._run_api_v1_users(self.annotator)\n        self._check_response(self.annotator, response, False)\n\n    def test_api_v1_users_observer(self):\n        response = self._run_api_v1_users(self.observer)\n        self._check_response(self.observer, response, False)\n\n    def test_api_v1_users_no_auth(self):\n        response = self._run_api_v1_users(None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass UserSelfAPITestCase(UserAPITestCase):\n    def _run_api_v1_users_self(self, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/users/self\')\n\n        return response\n\n    def test_api_v1_users_self_admin(self):\n        response = self._run_api_v1_users_self(self.admin)\n        self._check_response(self.admin, response)\n\n    def test_api_v1_users_self_user(self):\n        response = self._run_api_v1_users_self(self.user)\n        self._check_response(self.user, response)\n\n    def test_api_v1_users_self_annotator(self):\n        response = self._run_api_v1_users_self(self.annotator)\n        self._check_response(self.annotator, response)\n\n    def test_api_v1_users_self_observer(self):\n        response = self._run_api_v1_users_self(self.observer)\n        self._check_response(self.observer, response)\n\n    def test_api_v1_users_self_no_auth(self):\n        response = self._run_api_v1_users_self(None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass UserGetAPITestCase(UserAPITestCase):\n    def _run_api_v1_users_id(self, user, user_id):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/users/{}\'.format(user_id))\n\n        return response\n\n    def test_api_v1_users_id_admin(self):\n        response = self._run_api_v1_users_id(self.admin, self.user.id)\n        self._check_response(self.user, response, True)\n\n        response = self._run_api_v1_users_id(self.admin, self.admin.id)\n        self._check_response(self.admin, response, True)\n\n        response = self._run_api_v1_users_id(self.admin, self.owner.id)\n        self._check_response(self.owner, response, True)\n\n    def test_api_v1_users_id_user(self):\n        response = self._run_api_v1_users_id(self.user, self.user.id)\n        self._check_response(self.user, response, True)\n\n        response = self._run_api_v1_users_id(self.user, self.owner.id)\n        self._check_response(self.owner, response, False)\n\n    def test_api_v1_users_id_annotator(self):\n        response = self._run_api_v1_users_id(self.annotator, self.annotator.id)\n        self._check_response(self.annotator, response, True)\n\n        response = self._run_api_v1_users_id(self.annotator, self.user.id)\n        self._check_response(self.user, response, False)\n\n    def test_api_v1_users_id_observer(self):\n        response = self._run_api_v1_users_id(self.observer, self.observer.id)\n        self._check_response(self.observer, response, True)\n\n        response = self._run_api_v1_users_id(self.observer, self.user.id)\n        self._check_response(self.user, response, False)\n\n    def test_api_v1_users_id_no_auth(self):\n        response = self._run_api_v1_users_id(None, self.user.id)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass UserPartialUpdateAPITestCase(UserAPITestCase):\n    def _run_api_v1_users_id(self, user, user_id, data):\n        with ForceLogin(user, self.client):\n            response = self.client.patch(\'/api/v1/users/{}\'.format(user_id), data=data)\n\n        return response\n\n    def _check_response_with_data(self, user, response, data, is_full):\n        # refresh information about the user from DB\n        user = User.objects.get(id=user.id)\n        for k,v in data.items():\n            self.assertEqual(response.data[k], v)\n        self._check_response(user, response, is_full)\n\n    def test_api_v1_users_id_admin_partial(self):\n        data = {""username"": ""user09"", ""last_name"": ""my last name""}\n        response = self._run_api_v1_users_id(self.admin, self.user.id, data)\n\n        self._check_response_with_data(self.user, response, data, True)\n\n    def test_api_v1_users_id_user_partial(self):\n        data = {""username"": ""user10"", ""first_name"": ""my name""}\n        response = self._run_api_v1_users_id(self.user, self.user.id, data)\n        self._check_response_with_data(self.user, response, data, False)\n\n        data = {""is_staff"": True}\n        response = self._run_api_v1_users_id(self.user, self.user.id, data)\n        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)\n\n        data = {""username"": ""admin"", ""is_superuser"": True}\n        response = self._run_api_v1_users_id(self.user, self.user.id, data)\n        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)\n\n        data = {""username"": ""non_active"", ""is_active"": False}\n        response = self._run_api_v1_users_id(self.user, self.user.id, data)\n        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)\n\n        data = {""username"": ""annotator01"", ""first_name"": ""slave""}\n        response = self._run_api_v1_users_id(self.user, self.annotator.id, data)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n\n    def test_api_v1_users_id_no_auth_partial(self):\n        data = {""username"": ""user12""}\n        response = self._run_api_v1_users_id(None, self.user.id, data)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass UserDeleteAPITestCase(UserAPITestCase):\n    def _run_api_v1_users_id(self, user, user_id):\n        with ForceLogin(user, self.client):\n            response = self.client.delete(\'/api/v1/users/{}\'.format(user_id))\n\n        return response\n\n    def test_api_v1_users_id_admin(self):\n        response = self._run_api_v1_users_id(self.admin, self.user.id)\n        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)\n\n        response = self._run_api_v1_users_id(self.admin, self.admin.id)\n        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)\n\n    def test_api_v1_users_id_user(self):\n        response = self._run_api_v1_users_id(self.user, self.owner.id)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n\n        response = self._run_api_v1_users_id(self.user, self.user.id)\n        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)\n\n    def test_api_v1_users_id_annotator(self):\n        response = self._run_api_v1_users_id(self.annotator, self.user.id)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n\n        response = self._run_api_v1_users_id(self.annotator, self.annotator.id)\n        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)\n\n    def test_api_v1_users_id_observer(self):\n        response = self._run_api_v1_users_id(self.observer, self.user.id)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n\n        response = self._run_api_v1_users_id(self.observer, self.observer.id)\n        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)\n\n    def test_api_v1_users_id_no_auth(self):\n        response = self._run_api_v1_users_id(None, self.user.id)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass ProjectListAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.projects = create_dummy_db_projects(cls)\n\n    def _run_api_v1_projects(self, user, params=""""):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/projects{}\'.format(params))\n\n        return response\n\n    def test_api_v1_projects_admin(self):\n        response = self._run_api_v1_projects(self.admin)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([project.name for project in self.projects]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_projects_user(self):\n        response = self._run_api_v1_projects(self.user)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([project.name for project in self.projects\n                if \'my empty project\' != project.name]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_projects_observer(self):\n        response = self._run_api_v1_projects(self.observer)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([project.name for project in self.projects]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_projects_no_auth(self):\n        response = self._run_api_v1_projects(None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass ProjectGetAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.projects = create_dummy_db_projects(cls)\n\n    def _run_api_v1_projects_id(self, pid, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/projects/{}\'.format(pid))\n\n        return response\n\n    def _check_response(self, response, db_project):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertEqual(response.data[""name""], db_project.name)\n        owner = db_project.owner.id if db_project.owner else None\n        self.assertEqual(response.data[""owner""], owner)\n        assignee = db_project.assignee.id if db_project.assignee else None\n        self.assertEqual(response.data[""assignee""], assignee)\n        self.assertEqual(response.data[""status""], db_project.status)\n\n    def _check_api_v1_projects_id(self, user):\n        for db_project in self.projects:\n            response = self._run_api_v1_projects_id(db_project.id, user)\n            if user and user.has_perm(""engine.project.access"", db_project):\n                self._check_response(response, db_project)\n            elif user:\n                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n            else:\n                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n    def test_api_v1_projects_id_admin(self):\n        self._check_api_v1_projects_id(self.admin)\n\n    def test_api_v1_projects_id_user(self):\n        self._check_api_v1_projects_id(self.user)\n\n    def test_api_v1_projects_id_observer(self):\n        self._check_api_v1_projects_id(self.observer)\n\n    def test_api_v1_projects_id_no_auth(self):\n        self._check_api_v1_projects_id(None)\n\nclass ProjectDeleteAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.projects = create_dummy_db_projects(cls)\n\n    def _run_api_v1_projects_id(self, pid, user):\n        with ForceLogin(user, self.client):\n            response = self.client.delete(\'/api/v1/projects/{}\'.format(pid), format=""json"")\n\n        return response\n\n    def _check_api_v1_projects_id(self, user):\n        for db_project in self.projects:\n            response = self._run_api_v1_projects_id(db_project.id, user)\n            if user and user.has_perm(""engine.project.delete"", db_project):\n                self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)\n            elif user:\n                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n            else:\n                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n    def test_api_v1_projects_id_admin(self):\n        self._check_api_v1_projects_id(self.admin)\n\n    def test_api_v1_projects_id_user(self):\n        self._check_api_v1_projects_id(self.user)\n\n    def test_api_v1_projects_id_observer(self):\n        self._check_api_v1_projects_id(self.observer)\n\n    def test_api_v1_projects_id_no_auth(self):\n        self._check_api_v1_projects_id(None)\n\nclass ProjectCreateAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n\n    def _run_api_v1_projects(self, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.post(\'/api/v1/projects\', data=data, format=""json"")\n\n        return response\n\n    def _check_response(self, response, user, data):\n        self.assertEqual(response.status_code, status.HTTP_201_CREATED)\n        self.assertEqual(response.data[""name""], data[""name""])\n        self.assertEqual(response.data[""owner""], data.get(""owner"", user.id))\n        self.assertEqual(response.data[""assignee""], data.get(""assignee""))\n        self.assertEqual(response.data[""bug_tracker""], data.get(""bug_tracker"", """"))\n        self.assertEqual(response.data[""status""], StatusChoice.ANNOTATION)\n\n    def _check_api_v1_projects(self, user, data):\n        response = self._run_api_v1_projects(user, data)\n        if user and user.has_perm(""engine.project.create""):\n            self._check_response(response, user, data)\n        elif user:\n            self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n        else:\n            self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n    def test_api_v1_projects_admin(self):\n        data = {\n            ""name"": ""new name for the project"",\n            ""bug_tracker"": ""http://example.com""\n        }\n        self._check_api_v1_projects(self.admin, data)\n\n        data = {\n            ""owner"": self.owner.id,\n            ""assignee"": self.assignee.id,\n            ""name"": ""new name for the project""\n        }\n        self._check_api_v1_projects(self.admin, data)\n\n        data = {\n            ""owner"": self.admin.id,\n            ""name"": ""2""\n        }\n        self._check_api_v1_projects(self.admin, data)\n\n\n    def test_api_v1_projects_user(self):\n        data = {\n            ""name"": ""Dummy name"",\n            ""bug_tracker"": ""it is just text""\n        }\n        self._check_api_v1_projects(self.user, data)\n\n        data = {\n            ""owner"": self.owner.id,\n            ""assignee"": self.assignee.id,\n            ""name"": ""My import project with data""\n        }\n        self._check_api_v1_projects(self.user, data)\n\n\n    def test_api_v1_projects_observer(self):\n        data = {\n            ""name"": ""My Project #1"",\n            ""owner"": self.owner.id,\n            ""assignee"": self.assignee.id\n        }\n        self._check_api_v1_projects(self.observer, data)\n\n    def test_api_v1_projects_no_auth(self):\n        data = {\n            ""name"": ""My Project #2"",\n            ""owner"": self.admin.id,\n        }\n        self._check_api_v1_projects(None, data)\n\nclass ProjectPartialUpdateAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.projects = create_dummy_db_projects(cls)\n\n    def _run_api_v1_projects_id(self, pid, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.patch(\'/api/v1/projects/{}\'.format(pid),\n                data=data, format=""json"")\n\n        return response\n\n    def _check_response(self, response, db_project, data):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        name = data.get(""name"", db_project.name)\n        self.assertEqual(response.data[""name""], name)\n        owner = db_project.owner.id if db_project.owner else None\n        owner = data.get(""owner"", owner)\n        self.assertEqual(response.data[""owner""], owner)\n        assignee = db_project.assignee.id if db_project.assignee else None\n        assignee = data.get(""assignee"", assignee)\n        self.assertEqual(response.data[""assignee""], assignee)\n        self.assertEqual(response.data[""status""], db_project.status)\n\n    def _check_api_v1_projects_id(self, user, data):\n        for db_project in self.projects:\n            response = self._run_api_v1_projects_id(db_project.id, user, data)\n            if user and user.has_perm(""engine.project.change"", db_project):\n                self._check_response(response, db_project, data)\n            elif user:\n                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n            else:\n                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n    def test_api_v1_projects_id_admin(self):\n        data = {\n            ""name"": ""new name for the project"",\n            ""owner"": self.owner.id,\n        }\n        self._check_api_v1_projects_id(self.admin, data)\n\n    def test_api_v1_projects_id_user(self):\n        data = {\n            ""name"": ""new name for the project"",\n            ""owner"": self.assignee.id,\n        }\n        self._check_api_v1_projects_id(self.user, data)\n\n    def test_api_v1_projects_id_observer(self):\n        data = {\n            ""name"": ""new name for the project"",\n        }\n        self._check_api_v1_projects_id(self.observer, data)\n\n    def test_api_v1_projects_id_no_auth(self):\n        data = {\n            ""name"": ""new name for the project"",\n        }\n        self._check_api_v1_projects_id(None, data)\n\nclass ProjectListOfTasksAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.projects = create_dummy_db_projects(cls)\n\n    def _run_api_v1_projects_id_tasks(self, user, pid):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/projects/{}/tasks\'.format(pid))\n\n        return response\n\n    def test_api_v1_projects_id_tasks_admin(self):\n        project = self.projects[1]\n        response = self._run_api_v1_projects_id_tasks(self.admin, project.id)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([task.name for task in project.tasks.all()]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_projects_id_tasks_user(self):\n        project = self.projects[1]\n        response = self._run_api_v1_projects_id_tasks(self.user, project.id)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([task.name for task in project.tasks.all()\n                if  task.owner in [None, self.user] or\n                    task.assignee in [None, self.user]]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_projects_id_tasks_observer(self):\n        project = self.projects[1]\n        response = self._run_api_v1_projects_id_tasks(self.observer, project.id)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([task.name for task in project.tasks.all()]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_projects_id_tasks_no_auth(self):\n        project = self.projects[1]\n        response = self._run_api_v1_projects_id_tasks(None, project.id)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n\nclass TaskListAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.tasks = create_dummy_db_tasks(cls)\n\n    def _run_api_v1_tasks(self, user, params=""""):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/tasks{}\'.format(params))\n\n        return response\n\n    def test_api_v1_tasks_admin(self):\n        response = self._run_api_v1_tasks(self.admin)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([task.name for task in self.tasks]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_tasks_user(self):\n        response = self._run_api_v1_tasks(self.user)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([task.name for task in self.tasks\n                if (task.owner == self.user or task.assignee == None)]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_tasks_observer(self):\n        response = self._run_api_v1_tasks(self.observer)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertListEqual(\n            sorted([task.name for task in self.tasks]),\n            sorted([res[""name""] for res in response.data[""results""]]))\n\n    def test_api_v1_tasks_no_auth(self):\n        response = self._run_api_v1_tasks(None)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\nclass TaskGetAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.tasks = create_dummy_db_tasks(cls)\n\n    def _run_api_v1_tasks_id(self, tid, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\'/api/v1/tasks/{}\'.format(tid))\n\n        return response\n\n    def _check_response(self, response, db_task):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        self.assertEqual(response.data[""name""], db_task.name)\n        self.assertEqual(response.data[""size""], db_task.data.size)\n        self.assertEqual(response.data[""mode""], db_task.mode)\n        owner = db_task.owner.id if db_task.owner else None\n        self.assertEqual(response.data[""owner""], owner)\n        assignee = db_task.assignee.id if db_task.assignee else None\n        self.assertEqual(response.data[""assignee""], assignee)\n        self.assertEqual(response.data[""overlap""], db_task.overlap)\n        self.assertEqual(response.data[""segment_size""], db_task.segment_size)\n        self.assertEqual(response.data[""z_order""], db_task.z_order)\n        self.assertEqual(response.data[""image_quality""], db_task.data.image_quality)\n        self.assertEqual(response.data[""status""], db_task.status)\n        self.assertListEqual(\n            [label.name for label in db_task.label_set.all()],\n            [label[""name""] for label in response.data[""labels""]]\n        )\n\n    def _check_api_v1_tasks_id(self, user):\n        for db_task in self.tasks:\n            response = self._run_api_v1_tasks_id(db_task.id, user)\n            if user and user.has_perm(""engine.task.access"", db_task):\n                self._check_response(response, db_task)\n            elif user:\n                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n            else:\n                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n    def test_api_v1_tasks_id_admin(self):\n        self._check_api_v1_tasks_id(self.admin)\n\n    def test_api_v1_tasks_id_user(self):\n        self._check_api_v1_tasks_id(self.user)\n\n    def test_api_v1_tasks_id_observer(self):\n        self._check_api_v1_tasks_id(self.observer)\n\n    def test_api_v1_tasks_id_no_auth(self):\n        self._check_api_v1_tasks_id(None)\n\nclass TaskDeleteAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.tasks = create_dummy_db_tasks(cls)\n\n    def _run_api_v1_tasks_id(self, tid, user):\n        with ForceLogin(user, self.client):\n            response = self.client.delete(\'/api/v1/tasks/{}\'.format(tid), format=""json"")\n\n        return response\n\n    def _check_api_v1_tasks_id(self, user):\n        for db_task in self.tasks:\n            response = self._run_api_v1_tasks_id(db_task.id, user)\n            if user and user.has_perm(""engine.task.delete"", db_task):\n                self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)\n            elif user:\n                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n            else:\n                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n    def test_api_v1_tasks_id_admin(self):\n        self._check_api_v1_tasks_id(self.admin)\n\n    def test_api_v1_tasks_id_user(self):\n        self._check_api_v1_tasks_id(self.user)\n\n    def test_api_v1_tasks_id_observer(self):\n        self._check_api_v1_tasks_id(self.observer)\n\n    def test_api_v1_tasks_id_no_auth(self):\n        self._check_api_v1_tasks_id(None)\n\nclass TaskUpdateAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n        cls.tasks = create_dummy_db_tasks(cls)\n\n    def _run_api_v1_tasks_id(self, tid, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.put(\'/api/v1/tasks/{}\'.format(tid),\n                data=data, format=""json"")\n\n        return response\n\n    def _check_response(self, response, db_task, data):\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        name = data.get(""name"", db_task.name)\n        self.assertEqual(response.data[""name""], name)\n        self.assertEqual(response.data[""size""], db_task.data.size)\n        mode = data.get(""mode"", db_task.mode)\n        self.assertEqual(response.data[""mode""], mode)\n        owner = db_task.owner.id if db_task.owner else None\n        owner = data.get(""owner"", owner)\n        self.assertEqual(response.data[""owner""], owner)\n        assignee = db_task.assignee.id if db_task.assignee else None\n        assignee = data.get(""assignee"", assignee)\n        self.assertEqual(response.data[""assignee""], assignee)\n        self.assertEqual(response.data[""overlap""], db_task.overlap)\n        self.assertEqual(response.data[""segment_size""], db_task.segment_size)\n        z_order = data.get(""z_order"", db_task.z_order)\n        self.assertEqual(response.data[""z_order""], z_order)\n        image_quality = data.get(""image_quality"", db_task.data.image_quality)\n        self.assertEqual(response.data[""image_quality""], image_quality)\n        self.assertEqual(response.data[""status""], db_task.status)\n        if data.get(""labels""):\n            self.assertListEqual(\n                [label[""name""] for label in data.get(""labels"")],\n                [label[""name""] for label in response.data[""labels""]]\n            )\n        else:\n            self.assertListEqual(\n                [label.name for label in db_task.label_set.all()],\n                [label[""name""] for label in response.data[""labels""]]\n            )\n\n    def _check_api_v1_tasks_id(self, user, data):\n        for db_task in self.tasks:\n            response = self._run_api_v1_tasks_id(db_task.id, user, data)\n            if user and user.has_perm(""engine.task.change"", db_task):\n                self._check_response(response, db_task, data)\n            elif user:\n                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n            else:\n                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n    def test_api_v1_tasks_id_admin(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""owner"": self.owner.id,\n            ""labels"": [{\n                ""name"": ""non-vehicle"",\n                ""attributes"": [{\n                    ""name"": ""my_attribute"",\n                    ""mutable"": True,\n                    ""input_type"": AttributeType.CHECKBOX,\n                    ""default_value"": ""true""\n                }]\n            }]\n        }\n        self._check_api_v1_tasks_id(self.admin, data)\n\n    def test_api_v1_tasks_id_user(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""owner"": self.assignee.id,\n            ""labels"": [{\n                ""name"": ""car"",\n                ""attributes"": [{\n                    ""name"": ""color"",\n                    ""mutable"": False,\n                    ""input_type"": AttributeType.SELECT,\n                    ""default_value"": ""white"",\n                    ""values"": [""white"", ""yellow"", ""green"", ""red""]\n                }]\n            }]\n        }\n        self._check_api_v1_tasks_id(self.user, data)\n\n    def test_api_v1_tasks_id_observer(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""labels"": [{\n                ""name"": ""test"",\n            }]\n        }\n        self._check_api_v1_tasks_id(self.observer, data)\n\n    def test_api_v1_tasks_id_no_auth(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""labels"": [{\n                ""name"": ""test"",\n            }]\n        }\n        self._check_api_v1_tasks_id(None, data)\n\nclass TaskPartialUpdateAPITestCase(TaskUpdateAPITestCase):\n    def _run_api_v1_tasks_id(self, tid, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.patch(\'/api/v1/tasks/{}\'.format(tid),\n                data=data, format=""json"")\n\n        return response\n\n    def test_api_v1_tasks_id_admin_partial(self):\n        data = {\n            ""name"": ""new name for the task #2"",\n        }\n        self._check_api_v1_tasks_id(self.admin, data)\n\n        data = {\n            ""name"": ""new name for the task"",\n            ""owner"": self.owner.id\n        }\n        self._check_api_v1_tasks_id(self.admin, data)\n        # Now owner is updated, but self.db_tasks are obsolete\n        # We can\'t do any tests without owner in data below\n\n\n    def test_api_v1_tasks_id_user_partial(self):\n        data = {\n            ""labels"": [{\n                ""name"": ""car"",\n                ""attributes"": [{\n                    ""name"": ""color"",\n                    ""mutable"": False,\n                    ""input_type"": AttributeType.SELECT,\n                    ""default_value"": ""white"",\n                    ""values"": [""white"", ""yellow"", ""green"", ""red""]\n                }]\n            }]\n        }\n        self._check_api_v1_tasks_id(self.user, data)\n\n        data = {\n            ""owner"": self.observer.id,\n            ""assignee"": self.annotator.id\n        }\n        self._check_api_v1_tasks_id(self.user, data)\n\n\n    def test_api_v1_tasks_id_observer(self):\n        data = {\n            ""name"": ""my task #3""\n        }\n        self._check_api_v1_tasks_id(self.observer, data)\n\n    def test_api_v1_tasks_id_no_auth(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""labels"": [{\n                ""name"": ""test"",\n            }]\n        }\n        self._check_api_v1_tasks_id(None, data)\n\nclass TaskCreateAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n\n    def _run_api_v1_tasks(self, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.post(\'/api/v1/tasks\', data=data, format=""json"")\n\n        return response\n\n    def _check_response(self, response, user, data):\n        self.assertEqual(response.status_code, status.HTTP_201_CREATED)\n        self.assertEqual(response.data[""name""], data[""name""])\n        self.assertEqual(response.data[""mode""], """")\n        self.assertEqual(response.data[""owner""], data.get(""owner"", user.id))\n        self.assertEqual(response.data[""assignee""], data.get(""assignee""))\n        self.assertEqual(response.data[""bug_tracker""], data.get(""bug_tracker"", """"))\n        self.assertEqual(response.data[""overlap""], data.get(""overlap"", None))\n        self.assertEqual(response.data[""segment_size""], data.get(""segment_size"", 0))\n        self.assertEqual(response.data[""z_order""], data.get(""z_order"", False))\n        self.assertEqual(response.data[""status""], StatusChoice.ANNOTATION)\n        self.assertListEqual(\n            [label[""name""] for label in data.get(""labels"")],\n            [label[""name""] for label in response.data[""labels""]]\n        )\n\n    def _check_api_v1_tasks(self, user, data):\n        response = self._run_api_v1_tasks(user, data)\n        if user and user.has_perm(""engine.task.create""):\n            self._check_response(response, user, data)\n        elif user:\n            self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n        else:\n            self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\n    def test_api_v1_tasks_admin(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""labels"": [{\n                ""name"": ""non-vehicle"",\n                ""attributes"": [{\n                    ""name"": ""my_attribute"",\n                    ""mutable"": True,\n                    ""input_type"": AttributeType.CHECKBOX,\n                    ""default_value"": ""true""\n                }]\n            }]\n        }\n        self._check_api_v1_tasks(self.admin, data)\n\n    def test_api_v1_tasks_user(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""owner"": self.assignee.id,\n            ""labels"": [{\n                ""name"": ""car"",\n                ""attributes"": [{\n                    ""name"": ""color"",\n                    ""mutable"": False,\n                    ""input_type"": AttributeType.SELECT,\n                    ""default_value"": ""white"",\n                    ""values"": [""white"", ""yellow"", ""green"", ""red""]\n                }]\n            }]\n        }\n        self._check_api_v1_tasks(self.user, data)\n\n    def test_api_v1_tasks_observer(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""labels"": [{\n                ""name"": ""test"",\n            }]\n        }\n        self._check_api_v1_tasks(self.observer, data)\n\n    def test_api_v1_tasks_no_auth(self):\n        data = {\n            ""name"": ""new name for the task"",\n            ""labels"": [{\n                ""name"": ""test"",\n            }]\n        }\n        self._check_api_v1_tasks(None, data)\n\ndef generate_image_file(filename):\n    f = BytesIO()\n    gen = random.SystemRandom()\n    width = gen.randint(100, 800)\n    height = gen.randint(100, 800)\n    image = Image.new(\'RGB\', size=(width, height))\n    image.save(f, \'jpeg\')\n    f.name = filename\n    f.seek(0)\n\n    return (width, height), f\n\ndef generate_image_files(*args):\n    images = []\n    image_sizes = []\n    for image_name in args:\n        img_size, image = generate_image_file(image_name)\n        image_sizes.append(img_size)\n        images.append(image)\n\n    return image_sizes, images\n\ndef generate_video_file(filename, width=1920, height=1080, duration=1, fps=25):\n    f = BytesIO()\n    total_frames = duration * fps\n    container = av.open(f, mode=\'w\', format=\'mp4\')\n\n    stream = container.add_stream(\'mpeg4\', rate=fps)\n    stream.width = width\n    stream.height = height\n    stream.pix_fmt = \'yuv420p\'\n\n    for frame_i in range(total_frames):\n        img = np.empty((stream.width, stream.height, 3))\n        img[:, :, 0] = 0.5 + 0.5 * np.sin(2 * np.pi * (0 / 3 + frame_i / total_frames))\n        img[:, :, 1] = 0.5 + 0.5 * np.sin(2 * np.pi * (1 / 3 + frame_i / total_frames))\n        img[:, :, 2] = 0.5 + 0.5 * np.sin(2 * np.pi * (2 / 3 + frame_i / total_frames))\n\n        img = np.round(255 * img).astype(np.uint8)\n        img = np.clip(img, 0, 255)\n\n        frame = av.VideoFrame.from_ndarray(img, format=\'rgb24\')\n        for packet in stream.encode(frame):\n            container.mux(packet)\n\n    # Flush stream\n    for packet in stream.encode():\n        container.mux(packet)\n\n    # Close the file\n    container.close()\n    f.name = filename\n    f.seek(0)\n\n    return [(width, height)] * total_frames, f\n\ndef generate_zip_archive_file(filename, count):\n    image_sizes = []\n    zip_buf = BytesIO()\n    with zipfile.ZipFile(zip_buf, \'w\') as zip_chunk:\n        for idx in range(count):\n            image_name = ""image_{:6d}.jpg"".format(idx)\n            size, image_buf = generate_image_file(image_name)\n            image_sizes.append(size)\n            zip_chunk.writestr(image_name, image_buf.getvalue())\n\n    zip_buf.name = filename\n    zip_buf.seek(0)\n    return image_sizes, zip_buf\n\nclass TaskDataAPITestCase(APITestCase):\n    _image_sizes = {}\n\n    class ChunkType(str, Enum):\n        IMAGESET = \'imageset\'\n        VIDEO = \'video\'\n\n        def __str__(self):\n            return self.value\n\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        filename = ""test_1.jpg""\n        path = os.path.join(settings.SHARE_ROOT, filename)\n        img_size, data = generate_image_file(filename)\n        with open(path, ""wb"") as image:\n            image.write(data.read())\n        cls._image_sizes[filename] = img_size\n\n        filename = ""test_2.jpg""\n        path = os.path.join(settings.SHARE_ROOT, filename)\n        img_size, data = generate_image_file(filename)\n        with open(path, ""wb"") as image:\n            image.write(data.read())\n        cls._image_sizes[filename] = img_size\n\n        filename = ""test_3.jpg""\n        path = os.path.join(settings.SHARE_ROOT, filename)\n        img_size, data = generate_image_file(filename)\n        with open(path, ""wb"") as image:\n            image.write(data.read())\n        cls._image_sizes[filename] = img_size\n\n        filename = os.path.join(""data"", ""test_3.jpg"")\n        path = os.path.join(settings.SHARE_ROOT, filename)\n        os.makedirs(os.path.dirname(path))\n        img_size, data = generate_image_file(filename)\n        with open(path, ""wb"") as image:\n            image.write(data.read())\n        cls._image_sizes[filename] = img_size\n\n        filename = ""test_video_1.mp4""\n        path = os.path.join(settings.SHARE_ROOT, filename)\n        img_sizes, data = generate_video_file(filename, width=1280, height=720)\n        with open(path, ""wb"") as video:\n            video.write(data.read())\n        cls._image_sizes[filename] = img_sizes\n\n        filename = os.path.join(""videos"", ""test_video_1.mp4"")\n        path = os.path.join(settings.SHARE_ROOT, filename)\n        os.makedirs(os.path.dirname(path))\n        img_sizes, data = generate_video_file(filename, width=1280, height=720)\n        with open(path, ""wb"") as video:\n            video.write(data.read())\n        cls._image_sizes[filename] = img_sizes\n\n        filename = os.path.join(""test_archive_1.zip"")\n        path = os.path.join(settings.SHARE_ROOT, filename)\n        img_sizes, data = generate_zip_archive_file(filename, count=5)\n        with open(path, ""wb"") as zip_archive:\n            zip_archive.write(data.read())\n        cls._image_sizes[filename] = img_sizes\n\n    @classmethod\n    def tearDownClass(cls):\n        super().tearDownClass()\n        path = os.path.join(settings.SHARE_ROOT, ""test_1.jpg"")\n        os.remove(path)\n\n        path = os.path.join(settings.SHARE_ROOT, ""test_2.jpg"")\n        os.remove(path)\n\n        path = os.path.join(settings.SHARE_ROOT, ""test_3.jpg"")\n        os.remove(path)\n\n        path = os.path.join(settings.SHARE_ROOT, ""data"", ""test_3.jpg"")\n        os.remove(path)\n\n        path = os.path.join(settings.SHARE_ROOT, ""test_video_1.mp4"")\n        os.remove(path)\n\n        path = os.path.join(settings.SHARE_ROOT, ""videos"", ""test_video_1.mp4"")\n        os.remove(path)\n\n\n    def _run_api_v1_tasks_id_data_post(self, tid, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.post(\'/api/v1/tasks/{}/data\'.format(tid),\n                data=data)\n\n        return response\n\n    def _create_task(self, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.post(\'/api/v1/tasks\', data=data, format=""json"")\n        return response\n\n    def _get_task(self, user, tid):\n        with ForceLogin(user, self.client):\n            return self.client.get(""/api/v1/tasks/{}"".format(tid))\n\n    def _run_api_v1_task_id_data_get(self, tid, user, data_type, data_quality=None, data_number=None):\n        url = \'/api/v1/tasks/{}/data?type={}\'.format(tid, data_type)\n        if data_quality is not None:\n            url += \'&quality={}\'.format(data_quality)\n        if data_number is not None:\n            url += \'&number={}\'.format(data_number)\n        with ForceLogin(user, self.client):\n            return self.client.get(url)\n\n    def _get_preview(self, tid, user):\n        return self._run_api_v1_task_id_data_get(tid, user, ""preview"")\n\n    def _get_compressed_chunk(self, tid, user, number):\n        return self._run_api_v1_task_id_data_get(tid, user, ""chunk"", ""compressed"", number)\n\n    def _get_original_chunk(self, tid, user, number):\n        return self._run_api_v1_task_id_data_get(tid, user, ""chunk"", ""original"", number)\n\n    def _get_compressed_frame(self, tid, user, number):\n        return self._run_api_v1_task_id_data_get(tid, user, ""frame"", ""compressed"", number)\n\n    def _get_original_frame(self, tid, user, number):\n        return self._run_api_v1_task_id_data_get(tid, user, ""frame"", ""original"", number)\n\n    @staticmethod\n    def _extract_zip_chunk(chunk_buffer):\n        chunk = zipfile.ZipFile(chunk_buffer, mode=\'r\')\n        return [Image.open(BytesIO(chunk.read(f))) for f in sorted(chunk.namelist())]\n\n    @staticmethod\n    def _extract_video_chunk(chunk_buffer):\n        container = av.open(chunk_buffer)\n        stream = container.streams.video[0]\n        return [f.to_image() for f in container.decode(stream)]\n\n    def _test_api_v1_tasks_id_data_spec(self, user, spec, data, expected_compressed_type, expected_original_type, image_sizes):\n        # create task\n        response = self._create_task(user, spec)\n        self.assertEqual(response.status_code, status.HTTP_201_CREATED)\n\n        task_id = response.data[""id""]\n\n        # post data for the task\n        response = self._run_api_v1_tasks_id_data_post(task_id, user, data)\n        self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)\n\n        response = self._get_task(user, task_id)\n\n        expected_status_code = status.HTTP_200_OK\n        if user == self.user and ""owner"" in spec and spec[""owner""] != user.id and \\\n           ""assignee"" in spec and spec[""assignee""] != user.id:\n            expected_status_code = status.HTTP_403_FORBIDDEN\n        self.assertEqual(response.status_code, expected_status_code)\n\n        if expected_status_code == status.HTTP_200_OK:\n            task = response.json()\n            self.assertEqual(expected_compressed_type, task[""data_compressed_chunk_type""])\n            self.assertEqual(expected_original_type, task[""data_original_chunk_type""])\n            self.assertEqual(len(image_sizes), task[""size""])\n\n        # check preview\n        response = self._get_preview(task_id, user)\n        self.assertEqual(response.status_code, expected_status_code)\n        if expected_status_code == status.HTTP_200_OK:\n            preview = Image.open(io.BytesIO(b"""".join(response.streaming_content)))\n            self.assertLessEqual(preview.size, image_sizes[0])\n\n        # check compressed chunk\n        response = self._get_compressed_chunk(task_id, user, 0)\n        self.assertEqual(response.status_code, expected_status_code)\n        if expected_status_code == status.HTTP_200_OK:\n            compressed_chunk = io.BytesIO(b"""".join(response.streaming_content))\n            if task[""data_compressed_chunk_type""] == self.ChunkType.IMAGESET:\n                images = self._extract_zip_chunk(compressed_chunk)\n            else:\n                images = self._extract_video_chunk(compressed_chunk)\n\n            self.assertEqual(len(images), min(task[""data_chunk_size""], len(image_sizes)))\n\n            for image_idx, image in enumerate(images):\n                self.assertEqual(image.size, image_sizes[image_idx])\n\n        # check original chunk\n        response = self._get_original_chunk(task_id, user, 0)\n        self.assertEqual(response.status_code, expected_status_code)\n        if expected_status_code == status.HTTP_200_OK:\n            original_chunk  = io.BytesIO(b"""".join(response.streaming_content))\n            if task[""data_original_chunk_type""] == self.ChunkType.IMAGESET:\n                images = self._extract_zip_chunk(original_chunk)\n            else:\n                images = self._extract_video_chunk(original_chunk)\n\n            for image_idx, image in enumerate(images):\n                self.assertEqual(image.size, image_sizes[image_idx])\n\n            self.assertEqual(len(images), min(task[""data_chunk_size""], len(image_sizes)))\n\n            if task[""data_original_chunk_type""] == self.ChunkType.IMAGESET:\n                server_files = [img for key, img in data.items() if key.startswith(""server_files"")]\n                client_files = [img for key, img in data.items() if key.startswith(""client_files"")]\n\n                if server_files:\n                    source_files = [os.path.join(settings.SHARE_ROOT, f) for f in sorted(server_files)]\n                else:\n                    source_files = [f for f in sorted(client_files, key=lambda e: e.name)]\n\n                source_images = []\n                for f in source_files:\n                    if zipfile.is_zipfile(f):\n                        source_images.extend(self._extract_zip_chunk(f))\n                    else:\n                        source_images.append(Image.open(f))\n\n                for img_idx, image in enumerate(images):\n                    server_image = np.array(image)\n                    source_image = np.array(source_images[img_idx])\n                    self.assertTrue(np.array_equal(source_image, server_image))\n\n    def _test_api_v1_tasks_id_data(self, user):\n        task_spec = {\n            ""name"": ""my task #1"",\n            ""owner"": self.owner.id,\n            ""assignee"": self.assignee.id,\n            ""overlap"": 0,\n            ""segment_size"": 100,\n            ""z_order"": False,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n\n        image_sizes, images = generate_image_files(""test_1.jpg"", ""test_2.jpg"", ""test_3.jpg"")\n        task_data = {\n            ""client_files[0]"": images[0],\n            ""client_files[1]"": images[1],\n            ""client_files[2]"": images[2],\n            ""image_quality"": 75,\n        }\n\n        self._test_api_v1_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes)\n\n        task_spec = {\n            ""name"": ""my task #2"",\n            ""overlap"": 0,\n            ""segment_size"": 0,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n\n        task_data = {\n            ""server_files[0]"": ""test_1.jpg"",\n            ""server_files[1]"": ""test_2.jpg"",\n            ""server_files[2]"": ""test_3.jpg"",\n            ""server_files[3]"": os.path.join(""data"", ""test_3.jpg""),\n            ""image_quality"": 75,\n        }\n        image_sizes = [\n            self._image_sizes[task_data[""server_files[3]""]],\n            self._image_sizes[task_data[""server_files[0]""]],\n            self._image_sizes[task_data[""server_files[1]""]],\n            self._image_sizes[task_data[""server_files[2]""]],\n        ]\n\n        self._test_api_v1_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes)\n\n        task_spec = {\n            ""name"": ""my video task #1"",\n            ""overlap"": 0,\n            ""segment_size"": 100,\n            ""z_order"": False,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n        image_sizes, video = generate_video_file(filename=""test_video_1.mp4"", width=1280, height=720)\n        task_data = {\n            ""client_files[0]"": video,\n            ""image_quality"": 43,\n        }\n\n        self._test_api_v1_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO, image_sizes)\n\n        task_spec = {\n            ""name"": ""my video task #2"",\n            ""overlap"": 0,\n            ""segment_size"": 5,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n\n        task_data = {\n            ""server_files[0]"": ""test_video_1.mp4"",\n            ""image_quality"": 57,\n        }\n        image_sizes = self._image_sizes[task_data[""server_files[0]""]]\n\n        self._test_api_v1_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO, image_sizes)\n\n        task_spec = {\n            ""name"": ""my video task #3"",\n            ""overlap"": 0,\n            ""segment_size"": 0,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n        task_data = {\n            ""server_files[0]"": os.path.join(""videos"", ""test_video_1.mp4""),\n            ""image_quality"": 57,\n        }\n        image_sizes = self._image_sizes[task_data[""server_files[0]""]]\n\n        self._test_api_v1_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO, image_sizes)\n\n        task_spec = {\n            ""name"": ""my video task #4"",\n            ""overlap"": 0,\n            ""segment_size"": 5,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n\n        task_data = {\n            ""server_files[0]"": ""test_video_1.mp4"",\n            ""image_quality"": 12,\n            ""use_zip_chunks"": True,\n        }\n        image_sizes = self._image_sizes[task_data[""server_files[0]""]]\n\n        self._test_api_v1_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.VIDEO, image_sizes)\n\n        task_spec = {\n            ""name"": ""my archive task #6"",\n            ""overlap"": 0,\n            ""segment_size"": 0,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n        task_data = {\n            ""server_files[0]"": ""test_archive_1.zip"",\n            ""image_quality"": 88,\n        }\n        image_sizes = self._image_sizes[task_data[""server_files[0]""]]\n\n        self._test_api_v1_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes)\n\n        task_spec = {\n            ""name"": ""my archive task #7"",\n            ""overlap"": 0,\n            ""segment_size"": 0,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n        image_sizes, archive = generate_zip_archive_file(""test_archive_2.zip"", 7)\n        task_data = {\n            ""client_files[0]"": archive,\n            ""image_quality"": 100,\n        }\n\n        self._test_api_v1_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes)\n\n    def test_api_v1_tasks_id_data_admin(self):\n        self._test_api_v1_tasks_id_data(self.admin)\n\n    def test_api_v1_tasks_id_data_owner(self):\n        self._test_api_v1_tasks_id_data(self.owner)\n\n    def test_api_v1_tasks_id_data_user(self):\n        self._test_api_v1_tasks_id_data(self.user)\n\n    def test_api_v1_tasks_id_data_no_auth(self):\n        data = {\n            ""name"": ""my task #3"",\n            ""owner"": self.owner.id,\n            ""assignee"": self.assignee.id,\n            ""overlap"": 0,\n            ""segment_size"": 100,\n            ""z_order"": False,\n            ""labels"": [\n                {""name"": ""car""},\n                {""name"": ""person""},\n            ]\n        }\n        response = self._create_task(None, data)\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n\ndef compare_objects(self, obj1, obj2, ignore_keys, fp_tolerance=.001):\n    if isinstance(obj1, dict):\n        self.assertTrue(isinstance(obj2, dict), ""{} != {}"".format(obj1, obj2))\n        for k, v1 in obj1.items():\n            if k in ignore_keys:\n                continue\n            v2 = obj2[k]\n            if k == \'attributes\':\n                key = lambda a: a[\'spec_id\']\n                v1.sort(key=key)\n                v2.sort(key=key)\n            compare_objects(self, v1, v2, ignore_keys)\n    elif isinstance(obj1, list):\n        self.assertTrue(isinstance(obj2, list), ""{} != {}"".format(obj1, obj2))\n        self.assertEqual(len(obj1), len(obj2), ""{} != {}"".format(obj1, obj2))\n        for v1, v2 in zip(obj1, obj2):\n            compare_objects(self, v1, v2, ignore_keys)\n    else:\n        if isinstance(obj1, float) or isinstance(obj2, float):\n            self.assertAlmostEqual(obj1, obj2, delta=fp_tolerance)\n        else:\n            self.assertEqual(obj1, obj2)\n\nclass JobAnnotationAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n\n    def _create_task(self, owner, assignee):\n        data = {\n            ""name"": ""my task #1"",\n            ""owner"": owner.id,\n            ""assignee"": assignee.id,\n            ""overlap"": 0,\n            ""segment_size"": 100,\n            ""z_order"": False,\n            ""labels"": [\n                {\n                    ""name"": ""car"",\n                    ""attributes"": [\n                        {\n                            ""name"": ""model"",\n                            ""mutable"": False,\n                            ""input_type"": ""select"",\n                            ""default_value"": ""mazda"",\n                            ""values"": [""bmw"", ""mazda"", ""renault""]\n                        },\n                        {\n                            ""name"": ""parked"",\n                            ""mutable"": True,\n                            ""input_type"": ""checkbox"",\n                            ""default_value"": False\n                        },\n                    ]\n                },\n                {""name"": ""person""},\n            ]\n        }\n\n        with ForceLogin(owner, self.client):\n            response = self.client.post(\'/api/v1/tasks\', data=data, format=""json"")\n            assert response.status_code == status.HTTP_201_CREATED\n            tid = response.data[""id""]\n\n            images = {\n                ""client_files[0]"": generate_image_file(""test_1.jpg"")[1],\n                ""client_files[1]"": generate_image_file(""test_2.jpg"")[1],\n                ""client_files[2]"": generate_image_file(""test_3.jpg"")[1],\n                ""image_quality"": 75,\n            }\n            response = self.client.post(""/api/v1/tasks/{}/data"".format(tid), data=images)\n            assert response.status_code == status.HTTP_202_ACCEPTED\n\n            response = self.client.get(""/api/v1/tasks/{}"".format(tid))\n            task = response.data\n\n            response = self.client.get(""/api/v1/tasks/{}/jobs"".format(tid))\n            jobs = response.data\n\n        return (task, jobs)\n\n    @staticmethod\n    def _get_default_attr_values(task):\n        default_attr_values = {}\n        for label in task[""labels""]:\n            default_attr_values[label[""id""]] = {\n                ""mutable"": [],\n                ""immutable"": [],\n                ""all"": [],\n            }\n            for attr in label[""attributes""]:\n                default_value = {\n                    ""spec_id"": attr[""id""],\n                    ""value"": attr[""default_value""],\n                }\n                if attr[""mutable""]:\n                    default_attr_values[label[""id""]][""mutable""].append(default_value)\n                else:\n                    default_attr_values[label[""id""]][""immutable""].append(default_value)\n                default_attr_values[label[""id""]][""all""].append(default_value)\n        return default_attr_values\n\n    def _put_api_v1_jobs_id_data(self, jid, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.put(""/api/v1/jobs/{}/annotations"".format(jid),\n                data=data, format=""json"")\n\n        return response\n\n    def _get_api_v1_jobs_id_data(self, jid, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(""/api/v1/jobs/{}/annotations"".format(jid))\n\n        return response\n\n    def _delete_api_v1_jobs_id_data(self, jid, user):\n        with ForceLogin(user, self.client):\n            response = self.client.delete(""/api/v1/jobs/{}/annotations"".format(jid),\n            format=""json"")\n\n        return response\n\n    def _patch_api_v1_jobs_id_data(self, jid, user, action, data):\n        with ForceLogin(user, self.client):\n            response = self.client.patch(\n                ""/api/v1/jobs/{}/annotations?action={}"".format(jid, action),\n                data=data, format=""json"")\n\n        return response\n\n    def _check_response(self, response, data):\n        if not response.status_code in [\n            status.HTTP_403_FORBIDDEN, status.HTTP_401_UNAUTHORIZED]:\n            compare_objects(self, data, response.data, ignore_keys=[""id""])\n\n    def _run_api_v1_jobs_id_annotations(self, owner, assignee, annotator):\n        task, jobs = self._create_task(owner, assignee)\n        if annotator:\n            HTTP_200_OK = status.HTTP_200_OK\n            HTTP_204_NO_CONTENT = status.HTTP_204_NO_CONTENT\n            HTTP_400_BAD_REQUEST = status.HTTP_400_BAD_REQUEST\n        else:\n            HTTP_200_OK = status.HTTP_401_UNAUTHORIZED\n            HTTP_204_NO_CONTENT = status.HTTP_401_UNAUTHORIZED\n            HTTP_400_BAD_REQUEST = status.HTTP_401_UNAUTHORIZED\n\n        job = jobs[0]\n        data = {\n            ""version"": 0,\n            ""tags"": [],\n            ""shapes"": [],\n            ""tracks"": []\n        }\n        response = self._put_api_v1_jobs_id_data(job[""id""], annotator, data)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n\n        data = {\n            ""version"": 1,\n            ""tags"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": []\n                }\n            ],\n            ""shapes"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                            ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                        }\n                    ],\n                    ""points"": [1.0, 2.1, 100, 300.222],\n                    ""type"": ""rectangle"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""points"": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],\n                    ""type"": ""polygon"",\n                    ""occluded"": False\n                },\n            ],\n            ""tracks"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                    ],\n                    ""shapes"": [\n                        {\n                            ""frame"": 0,\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False,\n                            ""attributes"": [\n                                {\n                                    ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                    ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                                },\n                            ]\n                        },\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [2.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": True,\n                            ""outside"": True\n                        },\n                    ]\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False\n                        }\n                    ]\n                },\n            ]\n        }\n\n        default_attr_values = self._get_default_attr_values(task)\n        response = self._put_api_v1_jobs_id_data(job[""id""], annotator, data)\n        data[""version""] += 1 # need to update the version\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        response = self._get_api_v1_jobs_id_data(job[""id""], annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        # server should add default attribute values if puted data doesn\'t contain it\n        data[""tags""][0][""attributes""] = default_attr_values[data[""tags""][0][""label_id""]][""all""]\n        data[""tracks""][0][""shapes""][1][""attributes""] = default_attr_values[data[""tracks""][0][""label_id""]][""mutable""]\n        self._check_response(response, data)\n\n        response = self._delete_api_v1_jobs_id_data(job[""id""], annotator)\n        data[""version""] += 1 # need to update the version\n        self.assertEqual(response.status_code, HTTP_204_NO_CONTENT)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [],\n            ""shapes"": [],\n            ""tracks"": []\n        }\n        response = self._get_api_v1_jobs_id_data(job[""id""], annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": []\n                }\n            ],\n            ""shapes"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                            ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                        }\n                    ],\n                    ""points"": [1.0, 2.1, 100, 300.222],\n                    ""type"": ""rectangle"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""points"": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],\n                    ""type"": ""polygon"",\n                    ""occluded"": False\n                },\n            ],\n            ""tracks"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                    ],\n                    ""shapes"": [\n                        {\n                            ""frame"": 0,\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False,\n                            ""attributes"": [\n                                {\n                                    ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                    ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                                },\n                            ]\n                        },\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [2.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": True,\n                            ""outside"": True\n                        },\n                    ]\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False\n                        }\n                    ]\n                },\n            ]\n        }\n        response = self._patch_api_v1_jobs_id_data(job[""id""], annotator,\n            ""create"", data)\n        data[""version""] += 1\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        response = self._get_api_v1_jobs_id_data(job[""id""], annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        # server should add default attribute values if puted data doesn\'t contain it\n        data[""tags""][0][""attributes""] = default_attr_values[data[""tags""][0][""label_id""]][""all""]\n        data[""tracks""][0][""shapes""][1][""attributes""] = default_attr_values[data[""tracks""][0][""label_id""]][""mutable""]\n        self._check_response(response, data)\n\n        data = response.data\n        if not response.status_code in [\n            status.HTTP_403_FORBIDDEN, status.HTTP_401_UNAUTHORIZED]:\n            data[""tags""][0][""label_id""] = task[""labels""][0][""id""]\n            data[""shapes""][0][""points""] = [1, 2, 3.0, 100, 120, 1, 2, 4.0]\n            data[""shapes""][0][""type""] = ""polygon""\n            data[""tracks""][0][""group""] = 10\n            data[""tracks""][0][""shapes""][0][""outside""] = False\n            data[""tracks""][0][""shapes""][0][""occluded""] = False\n\n        response = self._patch_api_v1_jobs_id_data(job[""id""], annotator,\n            ""update"", data)\n        data[""version""] = data.get(""version"", 0) + 1 # need to update the version\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        response = self._get_api_v1_jobs_id_data(job[""id""], annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        response = self._patch_api_v1_jobs_id_data(job[""id""], annotator,\n            ""delete"", data)\n        data[""version""] += 1 # need to update the version\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [],\n            ""shapes"": [],\n            ""tracks"": []\n        }\n        response = self._get_api_v1_jobs_id_data(job[""id""], annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": 11010101,\n                    ""group"": None,\n                    ""attributes"": []\n                }\n            ],\n            ""shapes"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": 32234234,\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""default_value""]\n                        }\n                    ],\n                    ""points"": [1.0, 2.1, 100, 300.222],\n                    ""type"": ""rectangle"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": 1212121,\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""points"": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],\n                    ""type"": ""polygon"",\n                    ""occluded"": False\n                },\n            ],\n            ""tracks"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": 0,\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 0,\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False,\n                            ""attributes"": [\n                                {\n                                    ""spec_id"": 10000,\n                                    ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                                },\n                                {\n                                    ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                    ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                                }\n                            ]\n                        },\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [2.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": True,\n                            ""outside"": True\n                        },\n                    ]\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False\n                        }\n                    ]\n                },\n            ]\n        }\n        response = self._patch_api_v1_jobs_id_data(job[""id""], annotator,\n            ""create"", data)\n        self.assertEqual(response.status_code, HTTP_400_BAD_REQUEST)\n\n    def test_api_v1_jobs_id_annotations_admin(self):\n        self._run_api_v1_jobs_id_annotations(self.admin, self.assignee,\n            self.assignee)\n\n    def test_api_v1_jobs_id_annotations_user(self):\n        self._run_api_v1_jobs_id_annotations(self.user, self.assignee,\n            self.assignee)\n\n    def test_api_v1_jobs_id_annotations_observer(self):\n        _, jobs = self._create_task(self.user, self.assignee)\n        job = jobs[0]\n        data = {\n            ""version"": 0,\n            ""tags"": [],\n            ""shapes"": [],\n            ""tracks"": []\n        }\n\n        response = self._get_api_v1_jobs_id_data(job[""id""], self.observer)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n\n        response = self._put_api_v1_jobs_id_data(job[""id""], self.observer, data)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n\n        response = self._patch_api_v1_jobs_id_data(job[""id""], self.observer, ""create"", data)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n\n        response = self._delete_api_v1_jobs_id_data(job[""id""], self.observer)\n        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)\n\n\n    def test_api_v1_jobs_id_annotations_no_auth(self):\n        self._run_api_v1_jobs_id_annotations(self.user, self.assignee, None)\n\nclass TaskAnnotationAPITestCase(JobAnnotationAPITestCase):\n    def _put_api_v1_tasks_id_annotations(self, pk, user, data):\n        with ForceLogin(user, self.client):\n            response = self.client.put(""/api/v1/tasks/{}/annotations"".format(pk),\n                data=data, format=""json"")\n\n        return response\n\n    def _get_api_v1_tasks_id_annotations(self, pk, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(""/api/v1/tasks/{}/annotations"".format(pk))\n\n        return response\n\n    def _delete_api_v1_tasks_id_annotations(self, pk, user):\n        with ForceLogin(user, self.client):\n            response = self.client.delete(""/api/v1/tasks/{}/annotations"".format(pk),\n            format=""json"")\n\n        return response\n\n    def _dump_api_v1_tasks_id_annotations(self, pk, user, query_params=""""):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\n                ""/api/v1/tasks/{0}/annotations{1}"".format(pk, query_params))\n\n        return response\n\n    def _patch_api_v1_tasks_id_annotations(self, pk, user, action, data):\n        with ForceLogin(user, self.client):\n            response = self.client.patch(\n                ""/api/v1/tasks/{}/annotations?action={}"".format(pk, action),\n                data=data, format=""json"")\n\n        return response\n\n    def _upload_api_v1_tasks_id_annotations(self, pk, user, data, query_params=""""):\n        with ForceLogin(user, self.client):\n            response = self.client.put(\n                path=""/api/v1/tasks/{0}/annotations?{1}"".format(pk, query_params),\n                data=data,\n                format=""multipart"",\n                )\n\n        return response\n\n    def _get_formats(self, user):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\n                path=""/api/v1/server/annotation/formats""\n            )\n        return response\n\n    def _check_response(self, response, data):\n        if not response.status_code in [\n            status.HTTP_401_UNAUTHORIZED, status.HTTP_403_FORBIDDEN]:\n            try:\n                compare_objects(self, data, response.data, ignore_keys=[""id""])\n            except AssertionError as e:\n                print(""Objects are not equal: "", data, response.data)\n                print(e)\n                raise\n\n    def _run_api_v1_tasks_id_annotations(self, owner, assignee, annotator):\n        task, _ = self._create_task(owner, assignee)\n        if annotator:\n            HTTP_200_OK = status.HTTP_200_OK\n            HTTP_204_NO_CONTENT = status.HTTP_204_NO_CONTENT\n            HTTP_400_BAD_REQUEST = status.HTTP_400_BAD_REQUEST\n        else:\n            HTTP_200_OK = status.HTTP_401_UNAUTHORIZED\n            HTTP_204_NO_CONTENT = status.HTTP_401_UNAUTHORIZED\n            HTTP_400_BAD_REQUEST = status.HTTP_401_UNAUTHORIZED\n\n        data = {\n            ""version"": 0,\n            ""tags"": [],\n            ""shapes"": [],\n            ""tracks"": []\n        }\n        response = self._put_api_v1_tasks_id_annotations(task[""id""], annotator, data)\n        data[""version""] += 1\n        self.assertEqual(response.status_code, HTTP_200_OK)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": []\n                }\n            ],\n            ""shapes"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""default_value""]\n                        }\n                    ],\n                    ""points"": [1.0, 2.1, 100, 300.222],\n                    ""type"": ""rectangle"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""points"": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],\n                    ""type"": ""polygon"",\n                    ""occluded"": False\n                },\n            ],\n            ""tracks"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                    ],\n                    ""shapes"": [\n                        {\n                            ""frame"": 0,\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False,\n                            ""attributes"": [\n                                {\n                                    ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                    ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                                }\n                            ]\n                        },\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [2.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": True,\n                            ""outside"": True\n                        },\n                    ]\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False\n                        }\n                    ]\n                },\n            ]\n        }\n        response = self._put_api_v1_tasks_id_annotations(task[""id""], annotator, data)\n        data[""version""] += 1\n\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        default_attr_values = self._get_default_attr_values(task)\n        response = self._get_api_v1_tasks_id_annotations(task[""id""], annotator)\n        # server should add default attribute values if puted data doesn\'t contain it\n        data[""tags""][0][""attributes""] = default_attr_values[data[""tags""][0][""label_id""]][""all""]\n        data[""tracks""][0][""shapes""][1][""attributes""] = default_attr_values[data[""tracks""][0][""label_id""]][""mutable""]\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        response = self._delete_api_v1_tasks_id_annotations(task[""id""], annotator)\n        data[""version""] += 1\n        self.assertEqual(response.status_code, HTTP_204_NO_CONTENT)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [],\n            ""shapes"": [],\n            ""tracks"": []\n        }\n        response = self._get_api_v1_tasks_id_annotations(task[""id""], annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": []\n                }\n            ],\n            ""shapes"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""default_value""]\n                        }\n                    ],\n                    ""points"": [1.0, 2.1, 100, 300.222],\n                    ""type"": ""rectangle"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""points"": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],\n                    ""type"": ""polygon"",\n                    ""occluded"": False\n                },\n            ],\n            ""tracks"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                    ],\n                    ""shapes"": [\n                        {\n                            ""frame"": 0,\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False,\n                            ""attributes"": [\n                                {\n                                    ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                    ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                                }\n                            ]\n                        },\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [2.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": True,\n                            ""outside"": True\n                        },\n                    ]\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False\n                        }\n                    ]\n                },\n            ]\n        }\n        response = self._patch_api_v1_tasks_id_annotations(task[""id""], annotator,\n            ""create"", data)\n        data[""version""] += 1\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        response = self._get_api_v1_tasks_id_annotations(task[""id""], annotator)\n        # server should add default attribute values if puted data doesn\'t contain it\n        data[""tags""][0][""attributes""] = default_attr_values[data[""tags""][0][""label_id""]][""all""]\n        data[""tracks""][0][""shapes""][1][""attributes""] = default_attr_values[data[""tracks""][0][""label_id""]][""mutable""]\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        data = response.data\n        if not response.status_code in [\n            status.HTTP_403_FORBIDDEN, status.HTTP_401_UNAUTHORIZED]:\n            data[""tags""][0][""label_id""] = task[""labels""][0][""id""]\n            data[""shapes""][0][""points""] = [1, 2, 3.0, 100, 120, 1, 2, 4.0]\n            data[""shapes""][0][""type""] = ""polygon""\n            data[""tracks""][0][""group""] = 10\n            data[""tracks""][0][""shapes""][0][""outside""] = False\n            data[""tracks""][0][""shapes""][0][""occluded""] = False\n\n        response = self._patch_api_v1_tasks_id_annotations(task[""id""], annotator,\n            ""update"", data)\n        data[""version""] = data.get(""version"", 0) + 1\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        response = self._get_api_v1_tasks_id_annotations(task[""id""], annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        response = self._patch_api_v1_tasks_id_annotations(task[""id""], annotator,\n            ""delete"", data)\n        data[""version""] += 1\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [],\n            ""shapes"": [],\n            ""tracks"": []\n        }\n        response = self._get_api_v1_tasks_id_annotations(task[""id""], annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        self._check_response(response, data)\n\n        data = {\n            ""version"": data[""version""],\n            ""tags"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": 11010101,\n                    ""group"": None,\n                    ""attributes"": []\n                }\n            ],\n            ""shapes"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": task[""labels""][0][""id""],\n                    ""group"": None,\n                    ""attributes"": [\n                        {\n                            ""spec_id"": 32234234,\n                            ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                        },\n                        {\n                            ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                            ""value"": task[""labels""][0][""attributes""][0][""default_value""]\n                        }\n                    ],\n                    ""points"": [1.0, 2.1, 100, 300.222],\n                    ""type"": ""rectangle"",\n                    ""occluded"": False\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": 1212121,\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""points"": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],\n                    ""type"": ""polygon"",\n                    ""occluded"": False\n                },\n            ],\n            ""tracks"": [\n                {\n                    ""frame"": 0,\n                    ""label_id"": 0,\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 0,\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False,\n                            ""attributes"": [\n                                {\n                                    ""spec_id"": 10000,\n                                    ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                                },\n                                {\n                                    ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                    ""value"": task[""labels""][0][""attributes""][0][""default_value""]\n                                }\n                            ]\n                        },\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [2.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": True,\n                            ""outside"": True\n                        },\n                    ]\n                },\n                {\n                    ""frame"": 1,\n                    ""label_id"": task[""labels""][1][""id""],\n                    ""group"": None,\n                    ""attributes"": [],\n                    ""shapes"": [\n                        {\n                            ""frame"": 1,\n                            ""attributes"": [],\n                            ""points"": [1.0, 2.1, 100, 300.222],\n                            ""type"": ""rectangle"",\n                            ""occluded"": False,\n                            ""outside"": False\n                        }\n                    ]\n                },\n            ]\n        }\n        response = self._patch_api_v1_tasks_id_annotations(task[""id""], annotator,\n            ""create"", data)\n        self.assertEqual(response.status_code, HTTP_400_BAD_REQUEST)\n\n    def _run_api_v1_tasks_id_annotations_dump_load(self, owner, assignee, annotator):\n        if annotator:\n            HTTP_200_OK = status.HTTP_200_OK\n            HTTP_204_NO_CONTENT = status.HTTP_204_NO_CONTENT\n            HTTP_202_ACCEPTED = status.HTTP_202_ACCEPTED\n            HTTP_201_CREATED = status.HTTP_201_CREATED\n        else:\n            HTTP_200_OK = status.HTTP_401_UNAUTHORIZED\n            HTTP_204_NO_CONTENT = status.HTTP_401_UNAUTHORIZED\n            HTTP_202_ACCEPTED = status.HTTP_401_UNAUTHORIZED\n            HTTP_201_CREATED = status.HTTP_401_UNAUTHORIZED\n\n        def _get_initial_annotation(annotation_format):\n            rectangle_tracks_with_attrs = [{\n                ""frame"": 0,\n                ""label_id"": task[""labels""][0][""id""],\n                ""group"": 0,\n                ""attributes"": [\n                    {\n                        ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                        ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                    },\n                ],\n                ""shapes"": [\n                    {\n                        ""frame"": 0,\n                        ""points"": [1.0, 2.1, 50.1, 30.22],\n                        ""type"": ""rectangle"",\n                        ""occluded"": False,\n                        ""outside"": False,\n                        ""attributes"": [\n                            {\n                                ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                            }\n                        ]\n                    },\n                    {\n                        ""frame"": 1,\n                        ""points"": [2.0, 2.1, 77.2, 36.22],\n                        ""type"": ""rectangle"",\n                        ""occluded"": True,\n                        ""outside"": True,\n                        ""attributes"": [\n                            {\n                                ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                                ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                            }\n                        ]\n                    },\n                ]\n            }]\n            rectangle_tracks_wo_attrs = [{\n                ""frame"": 1,\n                ""label_id"": task[""labels""][1][""id""],\n                ""group"": 0,\n                ""attributes"": [],\n                ""shapes"": [\n                    {\n                        ""frame"": 1,\n                        ""attributes"": [],\n                        ""points"": [1.0, 2.1, 50.2, 36.6],\n                        ""type"": ""rectangle"",\n                        ""occluded"": False,\n                        ""outside"": False\n                    },\n                    {\n                        ""frame"": 2,\n                        ""attributes"": [],\n                        ""points"": [1.0, 2.1, 51, 36.6],\n                        ""type"": ""rectangle"",\n                        ""occluded"": False,\n                        ""outside"": True\n                    }\n                ]\n            }]\n\n            rectangle_shapes_with_attrs = [{\n                ""frame"": 0,\n                ""label_id"": task[""labels""][0][""id""],\n                ""group"": 0,\n                ""attributes"": [\n                    {\n                        ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                        ""value"": task[""labels""][0][""attributes""][0][""values""][0]\n                    },\n                    {\n                        ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                        ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                    }\n                ],\n                ""points"": [1.0, 2.1, 10.6, 53.22],\n                ""type"": ""rectangle"",\n                ""occluded"": False\n            }]\n\n            rectangle_shapes_wo_attrs = [{\n                ""frame"": 1,\n                ""label_id"": task[""labels""][1][""id""],\n                ""group"": 0,\n                ""attributes"": [],\n                ""points"": [2.0, 2.1, 40, 50.7],\n                ""type"": ""rectangle"",\n                ""occluded"": False\n            }]\n\n            polygon_shapes_wo_attrs = [{\n                ""frame"": 1,\n                ""label_id"": task[""labels""][1][""id""],\n                ""group"": 0,\n                ""attributes"": [],\n                ""points"": [2.0, 2.1, 100, 30.22, 40, 77, 1, 3],\n                ""type"": ""polygon"",\n                ""occluded"": False\n            }]\n\n            polygon_shapes_with_attrs = [{\n                ""frame"": 2,\n                ""label_id"": task[""labels""][0][""id""],\n                ""group"": 1,\n                ""attributes"": [\n                    {\n                        ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                        ""value"": task[""labels""][0][""attributes""][0][""values""][1]\n                    },\n                    {\n                        ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                        ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                    }\n                ],\n                ""points"": [20.0, 0.1, 10, 3.22, 4, 7, 10, 30, 1, 2, 4.44, 5.55],\n                ""type"": ""polygon"",\n                ""occluded"": True\n            },\n            {\n                ""frame"": 2,\n                ""label_id"": task[""labels""][1][""id""],\n                ""group"": 1,\n                ""attributes"": [],\n                ""points"": [4, 7, 10, 30, 4, 5.55],\n                ""type"": ""polygon"",\n                ""occluded"": False\n            }]\n\n            tags_wo_attrs = [{\n                ""frame"": 2,\n                ""label_id"": task[""labels""][1][""id""],\n                ""group"": 0,\n                ""attributes"": []\n            }]\n            tags_with_attrs = [{\n                ""frame"": 1,\n                ""label_id"": task[""labels""][0][""id""],\n                ""group"": 3,\n                ""attributes"": [\n                    {\n                        ""spec_id"": task[""labels""][0][""attributes""][0][""id""],\n                        ""value"": task[""labels""][0][""attributes""][0][""values""][1]\n                    },\n                    {\n                        ""spec_id"": task[""labels""][0][""attributes""][1][""id""],\n                        ""value"": task[""labels""][0][""attributes""][1][""default_value""]\n                    }\n                ],\n            }]\n\n            annotations = {\n                    ""version"": 0,\n                    ""tags"": [],\n                    ""shapes"": [],\n                    ""tracks"": [],\n                }\n            if annotation_format == ""CVAT for video 1.1"":\n                annotations[""tracks""] = rectangle_tracks_with_attrs + rectangle_tracks_wo_attrs\n\n            elif annotation_format == ""CVAT for images 1.1"":\n                annotations[""shapes""] = rectangle_shapes_with_attrs + rectangle_shapes_wo_attrs \\\n                    + polygon_shapes_wo_attrs + polygon_shapes_with_attrs\n                annotations[""tags""] = tags_with_attrs + tags_wo_attrs\n\n            elif annotation_format == ""PASCAL VOC 1.1"":\n                annotations[""shapes""] = rectangle_shapes_wo_attrs\n                annotations[""tags""] = tags_wo_attrs\n\n            elif annotation_format == ""YOLO 1.1"" or \\\n                 annotation_format == ""TFRecord 1.0"":\n                annotations[""shapes""] = rectangle_shapes_wo_attrs\n\n            elif annotation_format == ""COCO 1.0"":\n                annotations[""shapes""] = polygon_shapes_wo_attrs\n\n            elif annotation_format == ""Segmentation mask 1.1"":\n                annotations[""shapes""] = rectangle_shapes_wo_attrs + polygon_shapes_wo_attrs\n                annotations[""tracks""] = rectangle_tracks_wo_attrs\n\n            elif annotation_format == ""MOT 1.1"":\n                annotations[""tracks""] = rectangle_tracks_wo_attrs\n\n            elif annotation_format == ""LabelMe 3.0"":\n                annotations[""shapes""] = rectangle_shapes_with_attrs + \\\n                                        rectangle_shapes_wo_attrs + \\\n                                        polygon_shapes_wo_attrs + \\\n                                        polygon_shapes_with_attrs\n\n            elif annotation_format == ""Datumaro 1.0"":\n                annotations[""shapes""] = rectangle_shapes_with_attrs + \\\n                                        rectangle_shapes_wo_attrs + \\\n                                        polygon_shapes_wo_attrs + \\\n                                        polygon_shapes_with_attrs\n                annotations[""tags""] = tags_with_attrs + tags_wo_attrs\n\n            else:\n                raise Exception(""Unknown format {}"".format(annotation_format))\n\n            return annotations\n\n        response = self._get_formats(annotator)\n        self.assertEqual(response.status_code, HTTP_200_OK)\n        if annotator is not None:\n            data = response.data\n        else:\n            data = self._get_formats(owner).data\n        import_formats = data[\'importers\']\n        export_formats = data[\'exporters\']\n        self.assertTrue(isinstance(import_formats, list) and import_formats)\n        self.assertTrue(isinstance(export_formats, list) and export_formats)\n        import_formats = { v[\'name\']: v for v in import_formats }\n        export_formats = { v[\'name\']: v for v in export_formats }\n\n        formats = { exp: exp if exp in import_formats else None\n            for exp in export_formats }\n        if \'CVAT 1.1\' in import_formats:\n            if \'CVAT for video 1.1\' in export_formats:\n                formats[\'CVAT for video 1.1\'] = \'CVAT 1.1\'\n            if \'CVAT for images 1.1\' in export_formats:\n                formats[\'CVAT for images 1.1\'] = \'CVAT 1.1\'\n        if set(import_formats) ^ set(export_formats):\n            # NOTE: this may not be an error, so we should not fail\n            print(""The following import formats have no pair:"",\n                set(import_formats) - set(export_formats))\n            print(""The following export formats have no pair:"",\n                set(export_formats) - set(import_formats))\n\n        for export_format, import_format in formats.items():\n            with self.subTest(export_format=export_format,\n                    import_format=import_format):\n                # 1. create task\n                task, jobs = self._create_task(owner, assignee)\n\n                # 2. add annotation\n                data = _get_initial_annotation(export_format)\n                response = self._put_api_v1_tasks_id_annotations(task[""id""], annotator, data)\n                data[""version""] += 1\n\n                self.assertEqual(response.status_code, HTTP_200_OK)\n                self._check_response(response, data)\n\n                # 3. download annotation\n                response = self._dump_api_v1_tasks_id_annotations(task[""id""], annotator,\n                    ""?format={}"".format(export_format))\n                if annotator and not export_formats[export_format][\'enabled\']:\n                    self.assertEqual(response.status_code,\n                        status.HTTP_405_METHOD_NOT_ALLOWED)\n                    continue\n                else:\n                    self.assertEqual(response.status_code, HTTP_202_ACCEPTED)\n\n                response = self._dump_api_v1_tasks_id_annotations(task[""id""], annotator,\n                    ""?format={}"".format(export_format))\n                self.assertEqual(response.status_code, HTTP_201_CREATED)\n\n                response = self._dump_api_v1_tasks_id_annotations(task[""id""], annotator,\n                    ""?format={}&action=download"".format(export_format))\n                self.assertEqual(response.status_code, HTTP_200_OK)\n\n                # 4. check downloaded data\n                if annotator is not None:\n                    self.assertTrue(response.streaming)\n                    content = io.BytesIO(b"""".join(response.streaming_content))\n                    self._check_dump_content(content, task, jobs, data, export_format)\n                    content.seek(0)\n                else:\n                    content = io.BytesIO()\n\n                # 5. remove annotation form the task\n                response = self._delete_api_v1_tasks_id_annotations(task[""id""], annotator)\n                data[""version""] += 1\n                self.assertEqual(response.status_code, HTTP_204_NO_CONTENT)\n\n                # 6. upload annotation\n                if not import_format:\n                    continue\n\n                uploaded_data = {\n                    ""annotation_file"": content,\n                }\n                response = self._upload_api_v1_tasks_id_annotations(\n                    task[""id""], annotator, uploaded_data,\n                    ""format={}"".format(import_format))\n                self.assertEqual(response.status_code, HTTP_202_ACCEPTED)\n\n                response = self._upload_api_v1_tasks_id_annotations(\n                    task[""id""], annotator, {},\n                    ""format={}"".format(import_format))\n                self.assertEqual(response.status_code, HTTP_201_CREATED)\n\n                # 7. check annotation\n                if import_format == ""Segmentation mask 1.1"":\n                    continue # can\'t really predict the result to check\n                response = self._get_api_v1_tasks_id_annotations(task[""id""], annotator)\n                self.assertEqual(response.status_code, HTTP_200_OK)\n\n                if annotator is None:\n                    continue\n                data[""version""] += 2 # upload is delete + put\n                self._check_response(response, data)\n\n    def _check_dump_content(self, content, task, jobs, data, format_name):\n        def etree_to_dict(t):\n            d = {t.tag: {} if t.attrib else None}\n            children = list(t)\n            if children:\n                dd = defaultdict(list)\n                for dc in map(etree_to_dict, children):\n                    for k, v in dc.items():\n                        dd[k].append(v)\n                d = {t.tag: {k: v[0] if len(v) == 1 else v\n                    for k, v in dd.items()}}\n            if t.attrib:\n                d[t.tag].update((\'@\' + k, v) for k, v in t.attrib.items())\n            if t.text:\n                text = t.text.strip()\n                if not (children or t.attrib):\n                    d[t.tag] = text\n            return d\n\n        if format_name in {""CVAT for video 1.1"", ""CVAT for images 1.1""}:\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                zipfile.ZipFile(content).extractall(tmp_dir)\n                xmls = glob(osp.join(tmp_dir, \'**\', \'*.xml\'), recursive=True)\n                self.assertTrue(xmls)\n                for xml in xmls:\n                    xmlroot = ET.parse(xml).getroot()\n                    self.assertEqual(xmlroot.tag, ""annotations"")\n                    tags = xmlroot.findall(""./meta"")\n                    self.assertEqual(len(tags), 1)\n                    meta = etree_to_dict(tags[0])[""meta""]\n                    self.assertEqual(meta[""task""][""name""], task[""name""])\n        elif format_name == ""PASCAL VOC 1.1"":\n            self.assertTrue(zipfile.is_zipfile(content))\n        elif format_name == ""YOLO 1.1"":\n            self.assertTrue(zipfile.is_zipfile(content))\n        elif format_name == ""COCO 1.0"":\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                zipfile.ZipFile(content).extractall(tmp_dir)\n                jsons = glob(osp.join(tmp_dir, \'**\', \'*.json\'), recursive=True)\n                self.assertTrue(jsons)\n                for json in jsons:\n                    coco = coco_loader.COCO(json)\n                    self.assertTrue(coco.getAnnIds())\n        elif format_name == ""TFRecord 1.0"":\n            self.assertTrue(zipfile.is_zipfile(content))\n        elif format_name == ""Segmentation mask 1.1"":\n            self.assertTrue(zipfile.is_zipfile(content))\n\n\n    def _run_coco_annotation_upload_test(self, user):\n        def generate_coco_anno():\n            return b""""""{\n            ""categories"": [\n                {\n                ""id"": 1,\n                ""name"": ""car"",\n                ""supercategory"": """"\n                },\n                {\n                ""id"": 2,\n                ""name"": ""person"",\n                ""supercategory"": """"\n                }\n            ],\n            ""images"": [\n                {\n                ""coco_url"": """",\n                ""date_captured"": """",\n                ""flickr_url"": """",\n                ""license"": 0,\n                ""id"": 0,\n                ""file_name"": ""test_1.jpg"",\n                ""height"": 720,\n                ""width"": 1280\n                }\n            ],\n            ""annotations"": [\n                {\n                ""category_id"": 1,\n                ""id"": 1,\n                ""image_id"": 0,\n                ""iscrowd"": 0,\n                ""segmentation"": [\n                    []\n                ],\n                ""area"": 17702.0,\n                ""bbox"": [\n                    574.0,\n                    407.0,\n                    167.0,\n                    106.0\n                ]\n                }\n            ]\n            }""""""\n\n        task, _ = self._create_task(user, user)\n\n        content = io.BytesIO(generate_coco_anno())\n        content.seek(0)\n\n        format_name = ""COCO 1.0""\n        uploaded_data = {\n            ""annotation_file"": content,\n        }\n        response = self._upload_api_v1_tasks_id_annotations(\n            task[""id""], user, uploaded_data,\n            ""format={}"".format(format_name))\n        self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)\n\n        response = self._upload_api_v1_tasks_id_annotations(\n            task[""id""], user, {}, ""format={}"".format(format_name))\n        self.assertEqual(response.status_code, status.HTTP_201_CREATED)\n\n        response = self._get_api_v1_tasks_id_annotations(task[""id""], user)\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n\n    def test_api_v1_tasks_id_annotations_admin(self):\n        self._run_api_v1_tasks_id_annotations(self.admin, self.assignee,\n            self.assignee)\n\n    def test_api_v1_tasks_id_annotations_user(self):\n        self._run_api_v1_tasks_id_annotations(self.user, self.assignee,\n            self.assignee)\n\n    def test_api_v1_tasks_id_annotations_no_auth(self):\n        self._run_api_v1_tasks_id_annotations(self.user, self.assignee, None)\n\n    def test_api_v1_tasks_id_annotations_dump_load_admin(self):\n        self._run_api_v1_tasks_id_annotations_dump_load(self.admin, self.assignee,\n            self.assignee)\n\n    def test_api_v1_tasks_id_annotations_dump_load_user(self):\n        self._run_api_v1_tasks_id_annotations_dump_load(self.user, self.assignee,\n            self.assignee)\n\n    def test_api_v1_tasks_id_annotations_dump_load_no_auth(self):\n        self._run_api_v1_tasks_id_annotations_dump_load(self.user, self.assignee, None)\n\n    def test_api_v1_tasks_id_annotations_upload_coco_user(self):\n        self._run_coco_annotation_upload_test(self.user)\n\nclass ServerShareAPITestCase(APITestCase):\n    def setUp(self):\n        self.client = APIClient()\n\n    @classmethod\n    def setUpTestData(cls):\n        create_db_users(cls)\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        path = os.path.join(settings.SHARE_ROOT, ""file0.txt"")\n        open(path, ""w"").write(""test string"")\n        path = os.path.join(settings.SHARE_ROOT, ""test1"")\n        os.makedirs(path)\n        path = os.path.join(path, ""file1.txt"")\n        open(path, ""w"").write(""test string"")\n        directory = os.path.join(settings.SHARE_ROOT, ""test1"", ""test3"")\n        os.makedirs(directory)\n        path = os.path.join(settings.SHARE_ROOT, ""test2"")\n        os.makedirs(path)\n        path = os.path.join(path, ""file2.txt"")\n        open(path, ""w"").write(""test string"")\n\n    @classmethod\n    def tearDownClass(cls):\n        super().tearDownClass()\n        path = os.path.join(settings.SHARE_ROOT, ""file0.txt"")\n        os.remove(path)\n        path = os.path.join(settings.SHARE_ROOT, ""test1"")\n        shutil.rmtree(path)\n        path = os.path.join(settings.SHARE_ROOT, ""test2"")\n        shutil.rmtree(path)\n\n    def _run_api_v1_server_share(self, user, directory):\n        with ForceLogin(user, self.client):\n            response = self.client.get(\n                \'/api/v1/server/share?directory={}\'.format(directory))\n\n        return response\n\n    def _test_api_v1_server_share(self, user):\n        data = [\n            {""name"": ""test1"", ""type"": ""DIR""},\n            {""name"": ""test2"", ""type"": ""DIR""},\n            {""name"": ""file0.txt"", ""type"": ""REG""},\n        ]\n\n        response = self._run_api_v1_server_share(user, ""/"")\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        compare_objects(\n            self=self,\n            obj1=sorted(data, key=lambda d: d[""name""]),\n            obj2=sorted(response.data, key=lambda d: d[""name""]),\n            ignore_keys=[]\n        )\n\n        data = [\n            {""name"": ""file1.txt"", ""type"": ""REG""},\n            {""name"": ""test3"", ""type"": ""DIR""},\n        ]\n        response = self._run_api_v1_server_share(user, ""/test1"")\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        compare_objects(\n            self=self,\n            obj1=sorted(data, key=lambda d: d[""name""]),\n            obj2=sorted(response.data, key=lambda d: d[""name""]),\n            ignore_keys=[]\n        )\n\n        data = []\n        response = self._run_api_v1_server_share(user, ""/test1/test3"")\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        compare_objects(\n            self=self,\n            obj1=sorted(data, key=lambda d: d[""name""]),\n            obj2=sorted(response.data, key=lambda d: d[""name""]),\n            ignore_keys=[]\n        )\n\n        data = [\n            {""name"": ""file2.txt"", ""type"": ""REG""},\n        ]\n        response = self._run_api_v1_server_share(user, ""/test2"")\n        self.assertEqual(response.status_code, status.HTTP_200_OK)\n        compare_objects(\n            self=self,\n            obj1=sorted(data, key=lambda d: d[""name""]),\n            obj2=sorted(response.data, key=lambda d: d[""name""]),\n            ignore_keys=[]\n        )\n\n        response = self._run_api_v1_server_share(user, ""/test4"")\n        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)\n\n    def test_api_v1_server_share_admin(self):\n        self._test_api_v1_server_share(self.admin)\n\n    def test_api_v1_server_share_owner(self):\n        self._test_api_v1_server_share(self.owner)\n\n    def test_api_v1_server_share_assignee(self):\n        self._test_api_v1_server_share(self.assignee)\n\n    def test_api_v1_server_share_user(self):\n        self._test_api_v1_server_share(self.user)\n\n    def test_api_v1_server_share_annotator(self):\n        self._test_api_v1_server_share(self.annotator)\n\n    def test_api_v1_server_share_observer(self):\n        self._test_api_v1_server_share(self.observer)\n\n    def test_api_v1_server_share_no_auth(self):\n        response = self._run_api_v1_server_share(None, ""/"")\n        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)\n'"
cvat/apps/git/management/__init__.py,0,b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT'
cvat/apps/git/migrations/0001_initial.py,0,"b""# Generated by Django 2.1.3 on 2018-12-05 13:24\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n        ('engine', '0014_job_max_shape_id'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='GitData',\n            fields=[\n                ('task', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, primary_key=True, serialize=False, to='engine.Task')),\n                ('url', models.URLField(max_length=2000)),\n                ('path', models.CharField(max_length=256)),\n                ('sync_date', models.DateTimeField(auto_now_add=True)),\n                ('status', models.CharField(default='!sync', max_length=20)),\n            ],\n        ),\n    ]\n"""
cvat/apps/git/migrations/0002_auto_20190123_1305.py,0,"b""# Generated by Django 2.1.3 on 2019-01-23 10:05\n\nimport cvat.apps.git.models\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('git', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='gitdata',\n            name='status',\n            field=models.CharField(default=cvat.apps.git.models.GitStatusChoice('!sync'), max_length=20),\n        ),\n    ]\n"""
cvat/apps/git/migrations/0003_gitdata_lfs.py,0,"b""# Generated by Django 2.1.3 on 2019-02-05 17:08\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('git', '0002_auto_20190123_1305'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='gitdata',\n            name='lfs',\n            field=models.BooleanField(default=True),\n        ),\n    ]\n"""
cvat/apps/git/migrations/__init__.py,0,b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/log_viewer/migrations/__init__.py,0,b''
cvat/apps/restrictions/migrations/__init__.py,0,b'# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n'
cvat/apps/tf_annotation/migrations/__init__.py,0,b'\n# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n'
datumaro/datumaro/cli/commands/__init__.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom . import add, create, explain, export, remove'"
datumaro/datumaro/cli/commands/add.py,0,b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# pylint: disable=unused-import\n\nfrom ..contexts.source import build_add_parser as build_parser\n'
datumaro/datumaro/cli/commands/create.py,0,b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# pylint: disable=unused-import\n\nfrom ..contexts.project import build_create_parser as build_parser'
datumaro/datumaro/cli/commands/explain.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\nimport logging as log\nimport os\nimport os.path as osp\n\nfrom datumaro.components.project import Project\nfrom datumaro.util.command_targets import (TargetKinds, target_selector,\n    ProjectTarget, SourceTarget, ImageTarget, is_project_path)\nfrom datumaro.util.image import load_image, save_image\nfrom ..util import MultilineFormatter\nfrom ..util.project import load_project\n\n\ndef build_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(help=""Run Explainable AI algorithm"",\n        description=""Runs an explainable AI algorithm for a model."")\n\n    parser.add_argument(\'-m\', \'--model\', required=True,\n        help=""Model to use for inference"")\n    parser.add_argument(\'-t\', \'--target\', default=None,\n        help=""Inference target - image, source, project ""\n             ""(default: current dir)"")\n    parser.add_argument(\'-o\', \'--output-dir\', dest=\'save_dir\', default=None,\n        help=""Directory to save output (default: display only)"")\n\n    method_sp = parser.add_subparsers(dest=\'algorithm\')\n\n    rise_parser = method_sp.add_parser(\'rise\',\n        description=""""""\n        RISE: Randomized Input Sampling for\n        Explanation of Black-box Models algorithm|n\n        |n\n        See explanations at: https://arxiv.org/pdf/1806.07421.pdf\n        """""",\n        formatter_class=MultilineFormatter)\n    rise_parser.add_argument(\'-s\', \'--max-samples\', default=None, type=int,\n        help=""Number of algorithm iterations (default: mask size ^ 2)"")\n    rise_parser.add_argument(\'--mw\', \'--mask-width\',\n        dest=\'mask_width\', default=7, type=int,\n        help=""Mask width (default: %(default)s)"")\n    rise_parser.add_argument(\'--mh\', \'--mask-height\',\n        dest=\'mask_height\', default=7, type=int,\n        help=""Mask height (default: %(default)s)"")\n    rise_parser.add_argument(\'--prob\', default=0.5, type=float,\n        help=""Mask pixel inclusion probability (default: %(default)s)"")\n    rise_parser.add_argument(\'--iou\', \'--iou-thresh\',\n        dest=\'iou_thresh\', default=0.9, type=float,\n        help=""IoU match threshold for detections (default: %(default)s)"")\n    rise_parser.add_argument(\'--nms\', \'--nms-iou-thresh\',\n        dest=\'nms_iou_thresh\', default=0.0, type=float,\n        help=""IoU match threshold in Non-maxima suppression (default: no NMS)"")\n    rise_parser.add_argument(\'--conf\', \'--det-conf-thresh\',\n        dest=\'det_conf_thresh\', default=0.0, type=float,\n        help=""Confidence threshold for detections (default: include all)"")\n    rise_parser.add_argument(\'-b\', \'--batch-size\', default=1, type=int,\n        help=""Inference batch size (default: %(default)s)"")\n    rise_parser.add_argument(\'--progressive\', action=\'store_true\',\n        help=""Visualize results during computations"")\n\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.set_defaults(command=explain_command)\n\n    return parser\n\ndef explain_command(args):\n    project_path = args.project_dir\n    if is_project_path(project_path):\n        project = Project.load(project_path)\n    else:\n        project = None\n    args.target = target_selector(\n        ProjectTarget(is_default=True, project=project),\n        SourceTarget(project=project),\n        ImageTarget()\n    )(args.target)\n    if args.target[0] == TargetKinds.project:\n        if is_project_path(args.target[1]):\n            args.project_dir = osp.dirname(osp.abspath(args.target[1]))\n\n\n    import cv2\n    from matplotlib import cm\n\n    project = load_project(args.project_dir)\n\n    model = project.make_executable_model(args.model)\n\n    if str(args.algorithm).lower() != \'rise\':\n        raise NotImplementedError()\n\n    from datumaro.components.algorithms.rise import RISE\n    rise = RISE(model,\n        max_samples=args.max_samples,\n        mask_width=args.mask_width,\n        mask_height=args.mask_height,\n        prob=args.prob,\n        iou_thresh=args.iou_thresh,\n        nms_thresh=args.nms_iou_thresh,\n        det_conf_thresh=args.det_conf_thresh,\n        batch_size=args.batch_size)\n\n    if args.target[0] == TargetKinds.image:\n        image_path = args.target[1]\n        image = load_image(image_path)\n        if model.preferred_input_size() is not None:\n            h, w = model.preferred_input_size()\n            image = cv2.resize(image, (w, h))\n\n        log.info(""Running inference explanation for \'%s\'"" % image_path)\n        heatmap_iter = rise.apply(image, progressive=args.progressive)\n\n        image = image / 255.0\n        file_name = osp.splitext(osp.basename(image_path))[0]\n        if args.progressive:\n            for i, heatmaps in enumerate(heatmap_iter):\n                for j, heatmap in enumerate(heatmaps):\n                    hm_painted = cm.jet(heatmap)[:, :, 2::-1]\n                    disp = (image + hm_painted) / 2\n                    cv2.imshow(\'heatmap-%s\' % j, hm_painted)\n                    cv2.imshow(file_name + \'-heatmap-%s\' % j, disp)\n                cv2.waitKey(10)\n                print(""Iter"", i, ""of"", args.max_samples, end=\'\\r\')\n        else:\n            heatmaps = next(heatmap_iter)\n\n        if args.save_dir is not None:\n            log.info(""Saving inference heatmaps at \'%s\'"" % args.save_dir)\n            os.makedirs(args.save_dir, exist_ok=True)\n\n            for j, heatmap in enumerate(heatmaps):\n                save_path = osp.join(args.save_dir,\n                    file_name + \'-heatmap-%s.png\' % j)\n                save_image(save_path, heatmap * 255.0)\n        else:\n            for j, heatmap in enumerate(heatmaps):\n                disp = (image + cm.jet(heatmap)[:, :, 2::-1]) / 2\n                cv2.imshow(file_name + \'-heatmap-%s\' % j, disp)\n            cv2.waitKey(0)\n    elif args.target[0] == TargetKinds.source or \\\n         args.target[0] == TargetKinds.project:\n        if args.target[0] == TargetKinds.source:\n            source_name = args.target[1]\n            dataset = project.make_source_project(source_name).make_dataset()\n            log.info(""Running inference explanation for \'%s\'"" % source_name)\n        else:\n            project_name = project.config.project_name\n            dataset = project.make_dataset()\n            log.info(""Running inference explanation for \'%s\'"" % project_name)\n\n        for item in dataset:\n            image = item.image\n            if image is None:\n                log.warn(\n                    ""Dataset item %s does not have image data. Skipping."" % \\\n                    (item.id))\n                continue\n\n            if model.preferred_input_size() is not None:\n                h, w = model.preferred_input_size()\n                image = cv2.resize(image, (w, h))\n            heatmap_iter = rise.apply(image)\n\n            image = image / 255.0\n            file_name = osp.splitext(osp.basename(image_path))[0]\n            heatmaps = next(heatmap_iter)\n\n            if args.save_dir is not None:\n                log.info(""Saving inference heatmaps at \'%s\'"" % args.save_dir)\n                os.makedirs(args.save_dir, exist_ok=True)\n\n                for j, heatmap in enumerate(heatmaps):\n                    save_path = osp.join(args.save_dir,\n                        file_name + \'-heatmap-%s.png\' % j)\n                    save_image(save_path, heatmap * 255.0)\n\n            if args.progressive:\n                for j, heatmap in enumerate(heatmaps):\n                    disp = (image + cm.jet(heatmap)[:, :, 2::-1]) / 2\n                    cv2.imshow(file_name + \'-heatmap-%s\' % j, disp)\n                cv2.waitKey(0)\n    else:\n        raise NotImplementedError()\n\n    return 0\n'"
datumaro/datumaro/cli/commands/export.py,0,b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# pylint: disable=unused-import\n\nfrom ..contexts.project import build_export_parser as build_parser'
datumaro/datumaro/cli/commands/remove.py,0,b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# pylint: disable=unused-import\n\nfrom ..contexts.source import build_remove_parser as build_parser'
datumaro/datumaro/cli/contexts/__init__.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom . import project, source, model, item'"
datumaro/datumaro/cli/util/__init__.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\nimport textwrap\n\n\nclass CliException(Exception): pass\n\ndef add_subparser(subparsers, name, builder):\n    return builder(lambda **kwargs: subparsers.add_parser(name, **kwargs))\n\nclass MultilineFormatter(argparse.HelpFormatter):\n    """"""\n    Keeps line breaks introduced with \'|n\' separator\n    and spaces introduced with \'|s\'.\n    """"""\n\n    def __init__(self, keep_natural=False, **kwargs):\n        super().__init__(**kwargs)\n        self._keep_natural = keep_natural\n\n    def _fill_text(self, text, width, indent):\n        text = self._whitespace_matcher.sub(\' \', text).strip()\n        text = text.replace(\'|s\', \' \')\n\n        paragraphs = text.split(\'|n \')\n        if self._keep_natural:\n            paragraphs = sum((p.split(\'\\n \') for p in paragraphs), [])\n\n        multiline_text = \'\'\n        for paragraph in paragraphs:\n            formatted_paragraph = textwrap.fill(paragraph, width,\n                initial_indent=indent, subsequent_indent=indent) + \'\\n\'\n            multiline_text += formatted_paragraph\n        return multiline_text\n\ndef make_file_name(s):\n    # adapted from\n    # https://docs.djangoproject.com/en/2.1/_modules/django/utils/text/#slugify\n    """"""\n    Normalizes string, converts to lowercase, removes non-alpha characters,\n    and converts spaces to hyphens.\n    """"""\n    import unicodedata, re\n    s = unicodedata.normalize(\'NFKD\', s).encode(\'ascii\', \'ignore\')\n    s = s.decode()\n    s = re.sub(r\'[^\\w\\s-]\', \'\', s).strip().lower()\n    s = re.sub(r\'[-\\s]+\', \'-\', s)\n    return s'"
datumaro/datumaro/cli/util/project.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport os\n\nfrom datumaro.components.project import Project\n\n\ndef load_project(project_dir):\n    return Project.load(project_dir)\n\ndef generate_next_dir_name(dirname, basedir=\'.\', sep=\'.\'):\n    """"""\n    If basedir does not contain dirname, returns dirname itself,\n    else generates a dirname by appending separator to the dirname\n    and the number, next to the last used number in the basedir for\n    files with dirname prefix.\n    """"""\n\n    def _to_int(s):\n        try:\n            return int(s)\n        except Exception:\n            return 0\n    sep_count = dirname.count(sep) + 2\n\n    files = [e for e in os.listdir(basedir) if e.startswith(dirname)]\n    if files:\n        files = [e.split(sep) for e in files]\n        files = [_to_int(e[-1]) for e in files if len(e) == sep_count]\n        dirname += \'%s%s\' % (sep, max(files, default=0) + 1)\n    return dirname'"
datumaro/datumaro/components/algorithms/__init__.py,0,b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n'
datumaro/datumaro/components/algorithms/rise.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# pylint: disable=unused-variable\n\nimport numpy as np\nfrom math import ceil\n\nfrom datumaro.components.extractor import AnnotationType\n\n\ndef flatmatvec(mat):\n    return np.reshape(mat, (len(mat), -1))\n\ndef expand(array, axis=None):\n    if axis is None:\n        axis = len(array.shape)\n    return np.expand_dims(array, axis=axis)\n\nclass RISE:\n    """"""\n    Implements RISE: Randomized Input Sampling for\n    Explanation of Black-box Models algorithm\n    See explanations at: https://arxiv.org/pdf/1806.07421.pdf\n    """"""\n\n    def __init__(self, model,\n            max_samples=None, mask_width=7, mask_height=7, prob=0.5,\n            iou_thresh=0.9, nms_thresh=0.0, det_conf_thresh=0.0,\n            batch_size=1):\n        self.model = model\n        self.max_samples = max_samples\n        self.mask_height = mask_height\n        self.mask_width = mask_width\n        self.prob = prob\n        self.iou_thresh = iou_thresh\n        self.nms_thresh = nms_thresh\n        self.det_conf_thresh = det_conf_thresh\n        self.batch_size = batch_size\n\n    @staticmethod\n    def split_outputs(annotations):\n        labels = []\n        bboxes = []\n        for r in annotations:\n            if r.type is AnnotationType.label:\n                labels.append(r)\n            elif r.type is AnnotationType.bbox:\n                bboxes.append(r)\n        return labels, bboxes\n\n    @staticmethod\n    def nms(boxes, iou_thresh=0.5):\n        indices = np.argsort([b.attributes[\'score\'] for b in boxes])\n        ious = np.array([[a.iou(b) for b in boxes] for a in boxes])\n\n        predictions = []\n        while len(indices) != 0:\n            i = len(indices) - 1\n            pred_idx = indices[i]\n            to_remove = [i]\n            predictions.append(boxes[pred_idx])\n            for i, box_idx in enumerate(indices[:i]):\n                if iou_thresh < ious[pred_idx, box_idx]:\n                    to_remove.append(i)\n            indices = np.delete(indices, to_remove)\n\n        return predictions\n\n    def normalize_hmaps(self, heatmaps, counts):\n        eps = np.finfo(heatmaps.dtype).eps\n        mhmaps = flatmatvec(heatmaps)\n        mhmaps /= expand(counts * self.prob + eps)\n        mhmaps -= expand(np.min(mhmaps, axis=1))\n        mhmaps /= expand(np.max(mhmaps, axis=1) + eps)\n        return np.reshape(mhmaps, heatmaps.shape)\n\n    def apply(self, image, progressive=False):\n        import cv2\n\n        assert len(image.shape) in [2, 3], \\\n            ""Expected an input image in (H, W, C) format""\n        if len(image.shape) == 3:\n            assert image.shape[2] in [3, 4], ""Expected BGR or BGRA input""\n        image = image[:, :, :3].astype(np.float32)\n\n        model = self.model\n        iou_thresh = self.iou_thresh\n\n        image_size = np.array((image.shape[:2]))\n        mask_size = np.array((self.mask_height, self.mask_width))\n        cell_size = np.ceil(image_size / mask_size)\n        upsampled_size = np.ceil((mask_size + 1) * cell_size)\n\n        rng = lambda shape=None: np.random.rand(*shape)\n        samples = np.prod(image_size)\n        if self.max_samples is not None:\n            samples = min(self.max_samples, samples)\n        batch_size = self.batch_size\n\n        result = next(iter(model.launch(expand(image, 0))))\n        result_labels, result_bboxes = self.split_outputs(result)\n        if 0 < self.det_conf_thresh:\n            result_bboxes = [b for b in result_bboxes \\\n                if self.det_conf_thresh <= b.attributes[\'score\']]\n        if 0 < self.nms_thresh:\n            result_bboxes = self.nms(result_bboxes, self.nms_thresh)\n\n        predicted_labels = set()\n        if len(result_labels) != 0:\n            predicted_label = max(result_labels,\n                key=lambda r: r.attributes[\'score\']).label\n            predicted_labels.add(predicted_label)\n        if len(result_bboxes) != 0:\n            for bbox in result_bboxes:\n                predicted_labels.add(bbox.label)\n        predicted_labels = { label: idx \\\n            for idx, label in enumerate(predicted_labels) }\n\n        predicted_bboxes = result_bboxes\n\n        heatmaps_count = len(predicted_labels) + len(predicted_bboxes)\n        heatmaps = np.zeros((heatmaps_count, *image_size), dtype=np.float32)\n        total_counts = np.zeros(heatmaps_count, dtype=np.int32)\n        confs = np.zeros(heatmaps_count, dtype=np.float32)\n\n        heatmap_id = 0\n\n        label_heatmaps = None\n        label_total_counts = None\n        label_confs = None\n        if len(predicted_labels) != 0:\n            step = len(predicted_labels)\n            label_heatmaps = heatmaps[heatmap_id : heatmap_id + step]\n            label_total_counts = total_counts[heatmap_id : heatmap_id + step]\n            label_confs = confs[heatmap_id : heatmap_id + step]\n            heatmap_id += step\n\n        bbox_heatmaps = None\n        bbox_total_counts = None\n        bbox_confs = None\n        if len(predicted_bboxes) != 0:\n            step = len(predicted_bboxes)\n            bbox_heatmaps = heatmaps[heatmap_id : heatmap_id + step]\n            bbox_total_counts = total_counts[heatmap_id : heatmap_id + step]\n            bbox_confs = confs[heatmap_id : heatmap_id + step]\n            heatmap_id += step\n\n        ups_mask = np.empty(upsampled_size.astype(int), dtype=np.float32)\n        masks = np.empty((batch_size, *image_size), dtype=np.float32)\n\n        full_batch_inputs = np.empty((batch_size, *image.shape), dtype=np.float32)\n        current_heatmaps = np.empty_like(heatmaps)\n        for b in range(ceil(samples / batch_size)):\n            batch_pos = b * batch_size\n            current_batch_size = min(samples - batch_pos, batch_size)\n\n            batch_masks = masks[: current_batch_size]\n            for i in range(current_batch_size):\n                mask = (rng(mask_size) < self.prob).astype(np.float32)\n                cv2.resize(mask, (int(upsampled_size[1]), int(upsampled_size[0])),\n                    ups_mask)\n\n                offsets = np.round(rng((2,)) * cell_size)\n                mask = ups_mask[\n                    int(offsets[0]):int(image_size[0] + offsets[0]),\n                    int(offsets[1]):int(image_size[1] + offsets[1]) ]\n                batch_masks[i] = mask\n\n            batch_inputs = full_batch_inputs[:current_batch_size]\n            np.multiply(expand(batch_masks), expand(image, 0), out=batch_inputs)\n\n            results = model.launch(batch_inputs)\n            for mask, result in zip(batch_masks, results):\n                result_labels, result_bboxes = self.split_outputs(result)\n\n                confs.fill(0)\n                if len(predicted_labels) != 0:\n                    for r in result_labels:\n                        idx = predicted_labels.get(r.label, None)\n                        if idx is not None:\n                            label_total_counts[idx] += 1\n                            label_confs[idx] += r.attributes[\'score\']\n                    for r in result_bboxes:\n                        idx = predicted_labels.get(r.label, None)\n                        if idx is not None:\n                            label_total_counts[idx] += 1\n                            label_confs[idx] += r.attributes[\'score\']\n\n                if len(predicted_bboxes) != 0 and len(result_bboxes) != 0:\n                    if 0 < self.det_conf_thresh:\n                        result_bboxes = [b for b in result_bboxes \\\n                            if self.det_conf_thresh <= b.attributes[\'score\']]\n                    if 0 < self.nms_thresh:\n                        result_bboxes = self.nms(result_bboxes, self.nms_thresh)\n\n                    for detection in result_bboxes:\n                        for pred_idx, pred in enumerate(predicted_bboxes):\n                            if pred.label != detection.label:\n                                continue\n\n                            iou = pred.iou(detection)\n                            assert 0 <= iou and iou <= 1\n                            if iou < iou_thresh:\n                                continue\n\n                            bbox_total_counts[pred_idx] += 1\n\n                            conf = detection.attributes[\'score\']\n                            bbox_confs[pred_idx] += conf\n\n                np.multiply.outer(confs, mask, out=current_heatmaps)\n                heatmaps += current_heatmaps\n\n                if progressive:\n                    yield self.normalize_hmaps(heatmaps.copy(), total_counts)\n\n        yield self.normalize_hmaps(heatmaps, total_counts)'"
datumaro/datumaro/plugins/coco_format/__init__.py,0,b''
datumaro/datumaro/plugins/coco_format/converter.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom enum import Enum\nfrom itertools import groupby\nimport json\nimport logging as log\nimport os\nimport os.path as osp\n\nimport pycocotools.mask as mask_utils\n\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.extractor import (DEFAULT_SUBSET_NAME,\n    AnnotationType, Points\n)\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom datumaro.util import find, cast\nfrom datumaro.util.image import save_image\nimport datumaro.util.mask_tools as mask_tools\nimport datumaro.util.annotation_tools as anno_tools\n\nfrom .format import CocoTask, CocoPath\n\n\nSegmentationMode = Enum(\'SegmentationMode\', [\'guess\', \'polygons\', \'mask\'])\n\nclass _TaskConverter:\n    def __init__(self, context):\n        self._min_ann_id = 1\n        self._context = context\n\n        data = {\n            \'licenses\': [],\n            \'info\': {},\n            \'categories\': [],\n            \'images\': [],\n            \'annotations\': []\n            }\n\n        data[\'licenses\'].append({\n            \'name\': \'\',\n            \'id\': 0,\n            \'url\': \'\'\n        })\n\n        data[\'info\'] = {\n            \'contributor\': \'\',\n            \'date_created\': \'\',\n            \'description\': \'\',\n            \'url\': \'\',\n            \'version\': \'\',\n            \'year\': \'\'\n        }\n        self._data = data\n\n    def is_empty(self):\n        return len(self._data[\'annotations\']) == 0\n\n    def _get_image_id(self, item):\n        return self._context._get_image_id(item)\n\n    def save_image_info(self, item, filename):\n        if item.has_image:\n            h, w = item.image.size\n        else:\n            h = 0\n            w = 0\n\n        self._data[\'images\'].append({\n            \'id\': self._get_image_id(item),\n            \'width\': int(w),\n            \'height\': int(h),\n            \'file_name\': cast(filename, str, \'\'),\n            \'license\': 0,\n            \'flickr_url\': \'\',\n            \'coco_url\': \'\',\n            \'date_captured\': 0,\n        })\n\n    def save_categories(self, dataset):\n        raise NotImplementedError()\n\n    def save_annotations(self, item):\n        raise NotImplementedError()\n\n    def write(self, path):\n        next_id = self._min_ann_id\n        for ann in self.annotations:\n            if ann[\'id\'] is None:\n                ann[\'id\'] = next_id\n                next_id += 1\n\n        with open(path, \'w\') as outfile:\n            json.dump(self._data, outfile)\n\n    @property\n    def annotations(self):\n        return self._data[\'annotations\']\n\n    @property\n    def categories(self):\n        return self._data[\'categories\']\n\n    def _get_ann_id(self, annotation):\n        ann_id = annotation.id\n        if ann_id:\n            self._min_ann_id = max(ann_id, self._min_ann_id)\n        return ann_id\n\nclass _ImageInfoConverter(_TaskConverter):\n    def is_empty(self):\n        return len(self._data[\'images\']) == 0\n\n    def save_categories(self, dataset):\n        pass\n\n    def save_annotations(self, item):\n        pass\n\nclass _CaptionsConverter(_TaskConverter):\n    def save_categories(self, dataset):\n        pass\n\n    def save_annotations(self, item):\n        for ann_idx, ann in enumerate(item.annotations):\n            if ann.type != AnnotationType.caption:\n                continue\n\n            elem = {\n                \'id\': self._get_ann_id(ann),\n                \'image_id\': self._get_image_id(item),\n                \'category_id\': 0, # NOTE: workaround for a bug in cocoapi\n                \'caption\': ann.caption,\n            }\n            if \'score\' in ann.attributes:\n                try:\n                    elem[\'score\'] = float(ann.attributes[\'score\'])\n                except Exception as e:\n                    log.warning(""Item \'%s\', ann #%s: failed to convert ""\n                        ""attribute \'score\': %e"" % (item.id, ann_idx, e))\n\n            self.annotations.append(elem)\n\nclass _InstancesConverter(_TaskConverter):\n    def save_categories(self, dataset):\n        label_categories = dataset.categories().get(AnnotationType.label)\n        if label_categories is None:\n            return\n\n        for idx, cat in enumerate(label_categories.items):\n            self.categories.append({\n                \'id\': 1 + idx,\n                \'name\': cast(cat.name, str, \'\'),\n                \'supercategory\': cast(cat.parent, str, \'\'),\n            })\n\n    @classmethod\n    def crop_segments(cls, instances, img_width, img_height):\n        instances = sorted(instances, key=lambda x: x[0].z_order)\n\n        segment_map = []\n        segments = []\n        for inst_idx, (_, polygons, mask, _) in enumerate(instances):\n            if polygons:\n                segment_map.extend(inst_idx for p in polygons)\n                segments.extend(polygons)\n            elif mask is not None:\n                segment_map.append(inst_idx)\n                segments.append(mask)\n\n        segments = mask_tools.crop_covered_segments(\n            segments, img_width, img_height)\n\n        for inst_idx, inst in enumerate(instances):\n            new_segments = [s for si_id, s in zip(segment_map, segments)\n                if si_id == inst_idx]\n\n            if not new_segments:\n                inst[1] = []\n                inst[2] = None\n                continue\n\n            if inst[1]:\n                inst[1] = sum(new_segments, [])\n            else:\n                mask = mask_tools.merge_masks(new_segments)\n                inst[2] = mask_tools.mask_to_rle(mask)\n\n        return instances\n\n    def find_instance_parts(self, group, img_width, img_height):\n        boxes = [a for a in group if a.type == AnnotationType.bbox]\n        polygons = [a for a in group if a.type == AnnotationType.polygon]\n        masks = [a for a in group if a.type == AnnotationType.mask]\n\n        anns = boxes + polygons + masks\n        leader = anno_tools.find_group_leader(anns)\n        bbox = anno_tools.compute_bbox(anns)\n        mask = None\n        polygons = [p.points for p in polygons]\n\n        if self._context._segmentation_mode == SegmentationMode.guess:\n            use_masks = True == leader.attributes.get(\'is_crowd\',\n                find(masks, lambda x: x.label == leader.label) is not None)\n        elif self._context._segmentation_mode == SegmentationMode.polygons:\n            use_masks = False\n        elif self._context._segmentation_mode == SegmentationMode.mask:\n            use_masks = True\n        else:\n            raise NotImplementedError(""Unexpected segmentation mode \'%s\'"" % \\\n                self._context._segmentation_mode)\n\n        if use_masks:\n            if polygons:\n                mask = mask_tools.rles_to_mask(polygons, img_width, img_height)\n\n            if masks:\n                if mask is not None:\n                    masks += [mask]\n                mask = mask_tools.merge_masks([m.image for m in masks])\n\n            if mask is not None:\n                mask = mask_tools.mask_to_rle(mask)\n            polygons = []\n        else:\n            if masks:\n                mask = mask_tools.merge_masks([m.image for m in masks])\n                polygons += mask_tools.mask_to_polygons(mask)\n            mask = None\n\n        return [leader, polygons, mask, bbox]\n\n    @staticmethod\n    def find_instance_anns(annotations):\n        return [a for a in annotations\n            if a.type in { AnnotationType.bbox,\n                AnnotationType.polygon, AnnotationType.mask }\n        ]\n\n    @classmethod\n    def find_instances(cls, annotations):\n        return anno_tools.find_instances(cls.find_instance_anns(annotations))\n\n    def save_annotations(self, item):\n        instances = self.find_instances(item.annotations)\n        if not instances:\n            return\n\n        if not item.has_image:\n            log.warn(""Item \'%s\': skipping writing instances ""\n                ""since no image info available"" % item.id)\n            return\n        h, w = item.image.size\n        instances = [self.find_instance_parts(i, w, h) for i in instances]\n\n        if self._context._crop_covered:\n            instances = self.crop_segments(instances, w, h)\n\n        for instance in instances:\n            elem = self.convert_instance(instance, item)\n            if elem:\n                self.annotations.append(elem)\n\n    def convert_instance(self, instance, item):\n        ann, polygons, mask, bbox = instance\n\n        is_crowd = mask is not None\n        if is_crowd:\n            segmentation = {\n                \'counts\': list(int(c) for c in mask[\'counts\']),\n                \'size\': list(int(c) for c in mask[\'size\'])\n            }\n        else:\n            segmentation = [list(map(float, p)) for p in polygons]\n\n        area = 0\n        if segmentation:\n            if item.has_image:\n                h, w = item.image.size\n            else:\n                # NOTE: here we can guess the image size as\n                # it is only needed for the area computation\n                w = bbox[0] + bbox[2]\n                h = bbox[1] + bbox[3]\n\n            rles = mask_utils.frPyObjects(segmentation, h, w)\n            if is_crowd:\n                rles = [rles]\n            else:\n                rles = mask_utils.merge(rles)\n            area = mask_utils.area(rles)\n        else:\n            x, y, w, h = bbox\n            segmentation = [[x, y, x + w, y, x + w, y + h, x, y + h]]\n            area = w * h\n\n        elem = {\n            \'id\': self._get_ann_id(ann),\n            \'image_id\': self._get_image_id(item),\n            \'category_id\': cast(ann.label, int, -1) + 1,\n            \'segmentation\': segmentation,\n            \'area\': float(area),\n            \'bbox\': list(map(float, bbox)),\n            \'iscrowd\': int(is_crowd),\n        }\n        if \'score\' in ann.attributes:\n            try:\n                elem[\'score\'] = float(ann.attributes[\'score\'])\n            except Exception as e:\n                log.warning(""Item \'%s\': failed to convert attribute ""\n                    ""\'score\': %e"" % (item.id, e))\n\n        return elem\n\nclass _KeypointsConverter(_InstancesConverter):\n    def save_categories(self, dataset):\n        label_categories = dataset.categories().get(AnnotationType.label)\n        if label_categories is None:\n            return\n        point_categories = dataset.categories().get(AnnotationType.points)\n\n        for idx, label_cat in enumerate(label_categories.items):\n            cat = {\n                \'id\': 1 + idx,\n                \'name\': cast(label_cat.name, str, \'\'),\n                \'supercategory\': cast(label_cat.parent, str, \'\'),\n                \'keypoints\': [],\n                \'skeleton\': [],\n\n            }\n\n            if point_categories is not None:\n                kp_cat = point_categories.items.get(idx)\n                if kp_cat is not None:\n                    cat.update({\n                        \'keypoints\': [str(l) for l in kp_cat.labels],\n                        \'skeleton\': [list(map(int, j)) for j in kp_cat.joints],\n                    })\n            self.categories.append(cat)\n\n    def save_annotations(self, item):\n        point_annotations = [a for a in item.annotations\n            if a.type == AnnotationType.points]\n        if not point_annotations:\n            return\n\n        # Create annotations for solitary keypoints annotations\n        for points in self.find_solitary_points(item.annotations):\n            instance = [points, [], None, points.get_bbox()]\n            elem = super().convert_instance(instance, item)\n            elem.update(self.convert_points_object(points))\n            self.annotations.append(elem)\n\n        # Create annotations for complete instance + keypoints annotations\n        super().save_annotations(item)\n\n    @classmethod\n    def find_solitary_points(cls, annotations):\n        annotations = sorted(annotations, key=lambda a: a.group)\n        solitary_points = []\n\n        for g_id, group in groupby(annotations, lambda a: a.group):\n            if not g_id or g_id and not cls.find_instance_anns(group):\n                group = [a for a in group if a.type == AnnotationType.points]\n                solitary_points.extend(group)\n\n        return solitary_points\n\n    @staticmethod\n    def convert_points_object(ann):\n        keypoints = []\n        points = ann.points\n        visibility = ann.visibility\n        for index in range(0, len(points), 2):\n            kp = points[index : index + 2]\n            state = visibility[index // 2].value\n            keypoints.extend([*kp, state])\n\n        num_annotated = len([v for v in visibility \\\n            if v != Points.Visibility.absent])\n\n        return {\n            \'keypoints\': keypoints,\n            \'num_keypoints\': num_annotated,\n        }\n\n    def convert_instance(self, instance, item):\n        points_ann = find(item.annotations, lambda x: \\\n            x.type == AnnotationType.points and \\\n            instance[0].group and x.group == instance[0].group)\n        if not points_ann:\n            return None\n\n        elem = super().convert_instance(instance, item)\n        elem.update(self.convert_points_object(points_ann))\n\n        return elem\n\nclass _LabelsConverter(_TaskConverter):\n    def save_categories(self, dataset):\n        label_categories = dataset.categories().get(AnnotationType.label)\n        if label_categories is None:\n            return\n\n        for idx, cat in enumerate(label_categories.items):\n            self.categories.append({\n                \'id\': 1 + idx,\n                \'name\': cast(cat.name, str, \'\'),\n                \'supercategory\': cast(cat.parent, str, \'\'),\n            })\n\n    def save_annotations(self, item):\n        for ann in item.annotations:\n            if ann.type != AnnotationType.label:\n                continue\n\n            elem = {\n                \'id\': self._get_ann_id(ann),\n                \'image_id\': self._get_image_id(item),\n                \'category_id\': int(ann.label) + 1,\n            }\n            if \'score\' in ann.attributes:\n                try:\n                    elem[\'score\'] = float(ann.attributes[\'score\'])\n                except Exception as e:\n                    log.warning(""Item \'%s\': failed to convert attribute ""\n                        ""\'score\': %e"" % (item.id, e))\n\n            self.annotations.append(elem)\n\nclass _Converter:\n    _TASK_CONVERTER = {\n        CocoTask.image_info: _ImageInfoConverter,\n        CocoTask.instances: _InstancesConverter,\n        CocoTask.person_keypoints: _KeypointsConverter,\n        CocoTask.captions: _CaptionsConverter,\n        CocoTask.labels: _LabelsConverter,\n    }\n\n    def __init__(self, extractor, save_dir,\n            tasks=None, save_images=False, segmentation_mode=None,\n            crop_covered=False):\n        assert tasks is None or isinstance(tasks, (CocoTask, list, str))\n        if tasks is None:\n            tasks = list(self._TASK_CONVERTER)\n        elif isinstance(tasks, CocoTask):\n            tasks = [tasks]\n        elif isinstance(tasks, str):\n            tasks = [CocoTask[tasks]]\n        else:\n            for i, t in enumerate(tasks):\n                if isinstance(t, str):\n                    tasks[i] = CocoTask[t]\n                else:\n                    assert t in CocoTask, t\n        self._tasks = tasks\n\n        self._extractor = extractor\n        self._save_dir = save_dir\n\n        self._save_images = save_images\n\n        assert segmentation_mode is None or \\\n            isinstance(segmentation_mode, str) or \\\n            segmentation_mode in SegmentationMode\n        if segmentation_mode is None:\n            segmentation_mode = SegmentationMode.guess\n        if isinstance(segmentation_mode, str):\n            segmentation_mode = SegmentationMode[segmentation_mode]\n        self._segmentation_mode = segmentation_mode\n\n        self._crop_covered = crop_covered\n\n        self._image_ids = {}\n\n    def _make_dirs(self):\n        self._images_dir = osp.join(self._save_dir, CocoPath.IMAGES_DIR)\n        os.makedirs(self._images_dir, exist_ok=True)\n\n        self._ann_dir = osp.join(self._save_dir, CocoPath.ANNOTATIONS_DIR)\n        os.makedirs(self._ann_dir, exist_ok=True)\n\n    def _make_task_converter(self, task):\n        if task not in self._TASK_CONVERTER:\n            raise NotImplementedError()\n        return self._TASK_CONVERTER[task](self)\n\n    def _make_task_converters(self):\n        return {\n            task: self._make_task_converter(task) for task in self._tasks\n        }\n\n    def _get_image_id(self, item):\n        image_id = self._image_ids.get(item.id)\n        if image_id is None:\n            image_id = cast(item.id, int, len(self._image_ids) + 1)\n            self._image_ids[item.id] = image_id\n        return image_id\n\n    def _save_image(self, item):\n        image = item.image.data\n        if image is None:\n            log.warning(""Item \'%s\' has no image"" % item.id)\n            return \'\'\n\n        filename = item.image.filename\n        if filename:\n            filename = osp.splitext(filename)[0]\n        else:\n            filename = item.id\n        filename += CocoPath.IMAGE_EXT\n        path = osp.join(self._images_dir, filename)\n        save_image(path, image)\n        return path\n\n    def convert(self):\n        self._make_dirs()\n\n        subsets = self._extractor.subsets()\n        if len(subsets) == 0:\n            subsets = [ None ]\n\n        for subset_name in subsets:\n            if subset_name:\n                subset = self._extractor.get_subset(subset_name)\n            else:\n                subset_name = DEFAULT_SUBSET_NAME\n                subset = self._extractor\n\n            task_converters = self._make_task_converters()\n            for task_conv in task_converters.values():\n                task_conv.save_categories(subset)\n            for item in subset:\n                filename = \'\'\n                if item.has_image:\n                    filename = item.image.path\n                if self._save_images:\n                    if item.has_image:\n                        filename = self._save_image(item)\n                    else:\n                        log.debug(""Item \'%s\' has no image info"" % item.id)\n                for task_conv in task_converters.values():\n                    task_conv.save_image_info(item, filename)\n                    task_conv.save_annotations(item)\n\n            for task, task_conv in task_converters.items():\n                task_conv.write(osp.join(self._ann_dir,\n                    \'%s_%s.json\' % (task.name, subset_name)))\n\nclass CocoConverter(Converter, CliPlugin):\n    @staticmethod\n    def _split_tasks_string(s):\n        return [CocoTask[i.strip()] for i in s.split(\',\')]\n\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        parser.add_argument(\'--segmentation-mode\',\n            choices=[m.name for m in SegmentationMode],\n            default=SegmentationMode.guess.name,\n            help=""""""\n                Save mode for instance segmentation:|n\n                - \'{sm.guess.name}\': guess the mode for each instance,|n\n                |s|suse \'is_crowd\' attribute as hint|n\n                - \'{sm.polygons.name}\': save polygons,|n\n                |s|smerge and convert masks, prefer polygons|n\n                - \'{sm.mask.name}\': save masks,|n\n                |s|smerge and convert polygons, prefer masks|n\n                Default: %(default)s.\n                """""".format(sm=SegmentationMode))\n        parser.add_argument(\'--crop-covered\', action=\'store_true\',\n            help=""Crop covered segments so that background objects\' ""\n                ""segmentation was more accurate (default: %(default)s)"")\n        parser.add_argument(\'--tasks\', type=cls._split_tasks_string,\n            default=None,\n            help=""COCO task filter, comma-separated list of {%s} ""\n                ""(default: all)"" % \', \'.join([t.name for t in CocoTask]))\n        return parser\n\n    def __init__(self,\n            tasks=None, save_images=False, segmentation_mode=None,\n            crop_covered=False):\n        super().__init__()\n\n        self._options = {\n            \'tasks\': tasks,\n            \'save_images\': save_images,\n            \'segmentation_mode\': segmentation_mode,\n            \'crop_covered\': crop_covered,\n        }\n\n    def __call__(self, extractor, save_dir):\n        converter = _Converter(extractor, save_dir, **self._options)\n        converter.convert()\n\nclass CocoInstancesConverter(CocoConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = CocoTask.instances\n        super().__init__(**kwargs)\n\nclass CocoImageInfoConverter(CocoConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = CocoTask.image_info\n        super().__init__(**kwargs)\n\nclass CocoPersonKeypointsConverter(CocoConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = CocoTask.person_keypoints\n        super().__init__(**kwargs)\n\nclass CocoCaptionsConverter(CocoConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = CocoTask.captions\n        super().__init__(**kwargs)\n\nclass CocoLabelsConverter(CocoConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = CocoTask.labels\n        super().__init__(**kwargs)\n'"
datumaro/datumaro/plugins/coco_format/extractor.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nimport logging as log\nimport os.path as osp\n\nfrom pycocotools.coco import COCO\nimport pycocotools.mask as mask_utils\n\nfrom datumaro.components.extractor import (SourceExtractor,\n    DEFAULT_SUBSET_NAME, DatasetItem,\n    AnnotationType, Label, RleMask, Points, Polygon, Bbox, Caption,\n    LabelCategories, PointsCategories\n)\nfrom datumaro.util.image import Image\n\nfrom .format import CocoTask, CocoPath\n\n\nclass _CocoExtractor(SourceExtractor):\n    def __init__(self, path, task, merge_instance_polygons=False):\n        assert osp.isfile(path), path\n\n        subset = osp.splitext(osp.basename(path))[0].rsplit(\'_\', maxsplit=1)[1]\n        super().__init__(subset=subset)\n\n        rootpath = \'\'\n        if path.endswith(osp.join(CocoPath.ANNOTATIONS_DIR, osp.basename(path))):\n            rootpath = path.rsplit(CocoPath.ANNOTATIONS_DIR, maxsplit=1)[0]\n        images_dir = \'\'\n        if rootpath and osp.isdir(osp.join(rootpath, CocoPath.IMAGES_DIR)):\n            images_dir = osp.join(rootpath, CocoPath.IMAGES_DIR)\n            if osp.isdir(osp.join(images_dir, subset or DEFAULT_SUBSET_NAME)):\n                images_dir = osp.join(images_dir, subset or DEFAULT_SUBSET_NAME)\n        self._images_dir = images_dir\n        self._task = task\n\n        self._merge_instance_polygons = merge_instance_polygons\n\n        loader = self._make_subset_loader(path)\n        self._load_categories(loader)\n        self._items = self._load_items(loader)\n\n    def categories(self):\n        return self._categories\n\n    def __iter__(self):\n        for item in self._items.values():\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n\n    @staticmethod\n    def _make_subset_loader(path):\n        # COCO API has an \'unclosed file\' warning\n        coco_api = COCO()\n        with open(path, \'r\') as f:\n            import json\n            dataset = json.load(f)\n\n        coco_api.dataset = dataset\n        coco_api.createIndex()\n        return coco_api\n\n    def _load_categories(self, loader):\n        self._categories = {}\n\n        if self._task in [CocoTask.instances, CocoTask.labels,\n                CocoTask.person_keypoints,\n                # TODO: Task.stuff, CocoTask.panoptic\n                ]:\n            label_categories, label_map = self._load_label_categories(loader)\n            self._categories[AnnotationType.label] = label_categories\n            self._label_map = label_map\n\n        if self._task == CocoTask.person_keypoints:\n            person_kp_categories = self._load_person_kp_categories(loader)\n            self._categories[AnnotationType.points] = person_kp_categories\n\n    # pylint: disable=no-self-use\n    def _load_label_categories(self, loader):\n        catIds = loader.getCatIds()\n        cats = loader.loadCats(catIds)\n\n        categories = LabelCategories()\n        label_map = {}\n        for idx, cat in enumerate(cats):\n            label_map[cat[\'id\']] = idx\n            categories.add(name=cat[\'name\'], parent=cat[\'supercategory\'])\n\n        return categories, label_map\n    # pylint: enable=no-self-use\n\n    def _load_person_kp_categories(self, loader):\n        catIds = loader.getCatIds()\n        cats = loader.loadCats(catIds)\n\n        categories = PointsCategories()\n        for cat in cats:\n            label_id = self._label_map[cat[\'id\']]\n            categories.add(label_id=label_id,\n                labels=cat[\'keypoints\'], joints=cat[\'skeleton\']\n            )\n\n        return categories\n\n    def _load_items(self, loader):\n        items = OrderedDict()\n\n        for img_id in loader.getImgIds():\n            image_info = loader.loadImgs(img_id)[0]\n            image_path = osp.join(self._images_dir, image_info[\'file_name\'])\n            image_size = (image_info.get(\'height\'), image_info.get(\'width\'))\n            if all(image_size):\n                image_size = (int(image_size[0]), int(image_size[1]))\n            else:\n                image_size = None\n            image = Image(path=image_path, size=image_size)\n\n            anns = loader.getAnnIds(imgIds=img_id)\n            anns = loader.loadAnns(anns)\n            anns = sum((self._load_annotations(a, image_info) for a in anns), [])\n\n            items[img_id] = DatasetItem(id=img_id, subset=self._subset,\n                image=image, annotations=anns)\n\n        return items\n\n    def _get_label_id(self, ann):\n        cat_id = ann.get(\'category_id\')\n        if cat_id in [0, None]:\n            return None\n        return self._label_map[cat_id]\n\n    def _load_annotations(self, ann, image_info=None):\n        parsed_annotations = []\n\n        ann_id = ann.get(\'id\')\n\n        attributes = {}\n        if \'score\' in ann:\n            attributes[\'score\'] = ann[\'score\']\n\n        group = ann_id # make sure all tasks\' annotations are merged\n\n        if self._task in [CocoTask.instances, CocoTask.person_keypoints]:\n            x, y, w, h = ann[\'bbox\']\n            label_id = self._get_label_id(ann)\n\n            is_crowd = bool(ann[\'iscrowd\'])\n            attributes[\'is_crowd\'] = is_crowd\n\n            if self._task is CocoTask.person_keypoints:\n                keypoints = ann[\'keypoints\']\n                points = [p for i, p in enumerate(keypoints) if i % 3 != 2]\n                visibility = keypoints[2::3]\n                parsed_annotations.append(\n                    Points(points, visibility, label=label_id,\n                        id=ann_id, attributes=attributes, group=group)\n                )\n\n            segmentation = ann.get(\'segmentation\')\n            if segmentation and segmentation != [[]]:\n                rle = None\n\n                if isinstance(segmentation, list):\n                    if not self._merge_instance_polygons:\n                        # polygon - a single object can consist of multiple parts\n                        for polygon_points in segmentation:\n                            parsed_annotations.append(Polygon(\n                                points=polygon_points, label=label_id,\n                                id=ann_id, attributes=attributes, group=group\n                            ))\n                    else:\n                        # merge all parts into a single mask RLE\n                        img_h = image_info[\'height\']\n                        img_w = image_info[\'width\']\n                        rles = mask_utils.frPyObjects(segmentation, img_h, img_w)\n                        rle = mask_utils.merge(rles)\n                elif isinstance(segmentation[\'counts\'], list):\n                    # uncompressed RLE\n                    img_h = image_info[\'height\']\n                    img_w = image_info[\'width\']\n                    mask_h, mask_w = segmentation[\'size\']\n                    if img_h == mask_h and img_w == mask_w:\n                        rle = mask_utils.frPyObjects(\n                            [segmentation], mask_h, mask_w)[0]\n                    else:\n                        log.warning(""item #%s: mask #%s ""\n                            ""does not match image size: %s vs. %s. ""\n                            ""Skipping this annotation."",\n                            image_info[\'id\'], ann_id,\n                            (mask_h, mask_w), (img_h, img_w)\n                        )\n                else:\n                    # compressed RLE\n                    rle = segmentation\n\n                if rle is not None:\n                    parsed_annotations.append(RleMask(rle=rle, label=label_id,\n                        id=ann_id, attributes=attributes, group=group\n                    ))\n            else:\n                parsed_annotations.append(\n                    Bbox(x, y, w, h, label=label_id,\n                        id=ann_id, attributes=attributes, group=group)\n                )\n        elif self._task is CocoTask.labels:\n            label_id = self._get_label_id(ann)\n            parsed_annotations.append(\n                Label(label=label_id,\n                    id=ann_id, attributes=attributes, group=group)\n            )\n        elif self._task is CocoTask.captions:\n            caption = ann[\'caption\']\n            parsed_annotations.append(\n                Caption(caption,\n                    id=ann_id, attributes=attributes, group=group)\n            )\n        else:\n            raise NotImplementedError()\n\n        return parsed_annotations\n\nclass CocoImageInfoExtractor(_CocoExtractor):\n    def __init__(self, path, **kwargs):\n        kwargs[\'task\'] = CocoTask.image_info\n        super().__init__(path, **kwargs)\n\nclass CocoCaptionsExtractor(_CocoExtractor):\n    def __init__(self, path, **kwargs):\n        kwargs[\'task\'] = CocoTask.captions\n        super().__init__(path, **kwargs)\n\nclass CocoInstancesExtractor(_CocoExtractor):\n    def __init__(self, path, **kwargs):\n        kwargs[\'task\'] = CocoTask.instances\n        super().__init__(path, **kwargs)\n\nclass CocoPersonKeypointsExtractor(_CocoExtractor):\n    def __init__(self, path, **kwargs):\n        kwargs[\'task\'] = CocoTask.person_keypoints\n        super().__init__(path, **kwargs)\n\nclass CocoLabelsExtractor(_CocoExtractor):\n    def __init__(self, path, **kwargs):\n        kwargs[\'task\'] = CocoTask.labels\n        super().__init__(path, **kwargs)\n'"
datumaro/datumaro/plugins/coco_format/format.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom enum import Enum\n\n\nCocoTask = Enum('CocoTask', [\n    'instances',\n    'person_keypoints',\n    'captions',\n    'labels', # extension, does not exist in the original COCO format\n    'image_info',\n    # 'panoptic',\n    # 'stuff',\n])\n\nclass CocoPath:\n    IMAGES_DIR = 'images'\n    ANNOTATIONS_DIR = 'annotations'\n\n    IMAGE_EXT = '.jpg'\n"""
datumaro/datumaro/plugins/coco_format/importer.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import defaultdict\nfrom glob import glob\nimport logging as log\nimport os.path as osp\n\nfrom datumaro.components.extractor import Importer\nfrom datumaro.util.log_utils import logging_disabled\n\nfrom .format import CocoTask\n\n\nclass CocoImporter(Importer):\n    _COCO_EXTRACTORS = {\n        CocoTask.instances: \'coco_instances\',\n        CocoTask.person_keypoints: \'coco_person_keypoints\',\n        CocoTask.captions: \'coco_captions\',\n        CocoTask.labels: \'coco_labels\',\n        CocoTask.image_info: \'coco_image_info\',\n    }\n\n    @classmethod\n    def detect(cls, path):\n        with logging_disabled(log.WARN):\n            return len(cls.find_subsets(path)) != 0\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        subsets = self.find_subsets(path)\n\n        if len(subsets) == 0:\n            raise Exception(""Failed to find \'coco\' dataset at \'%s\'"" % path)\n\n        # TODO: should be removed when proper label merging is implemented\n        conflicting_types = {CocoTask.instances,\n            CocoTask.person_keypoints, CocoTask.labels}\n        ann_types = set(t for s in subsets.values() for t in s) \\\n            & conflicting_types\n        if 1 <= len(ann_types):\n            selected_ann_type = sorted(ann_types, key=lambda x: x.name)[0]\n        if 1 < len(ann_types):\n            log.warning(""Not implemented: ""\n                ""Found potentially conflicting source types with labels: %s. ""\n                ""Only one type will be used: %s"" \\\n                % ("", "".join(t.name for t in ann_types), selected_ann_type.name))\n\n        for ann_files in subsets.values():\n            for ann_type, ann_file in ann_files.items():\n                if ann_type in conflicting_types:\n                    if ann_type is not selected_ann_type:\n                        log.warning(""Not implemented: ""\n                            ""conflicting source \'%s\' is skipped."" % ann_file)\n                        continue\n                log.info(""Found a dataset at \'%s\'"" % ann_file)\n\n                source_name = osp.splitext(osp.basename(ann_file))[0]\n                project.add_source(source_name, {\n                    \'url\': ann_file,\n                    \'format\': self._COCO_EXTRACTORS[ann_type],\n                    \'options\': dict(extra_params),\n                })\n\n        return project\n\n    @staticmethod\n    def find_subsets(path):\n        if path.endswith(\'.json\') and osp.isfile(path):\n            subset_paths = [path]\n        else:\n            subset_paths = glob(osp.join(path, \'**\', \'*_*.json\'),\n                recursive=True)\n\n        subsets = defaultdict(dict)\n        for subset_path in subset_paths:\n            name_parts = osp.splitext(osp.basename(subset_path))[0] \\\n                .rsplit(\'_\', maxsplit=1)\n\n            ann_type = name_parts[0]\n            try:\n                ann_type = CocoTask[ann_type]\n            except KeyError:\n                log.warn(""Skipping \'%s\': unknown subset ""\n                    ""type \'%s\', the only known are: %s"" % \\\n                    (subset_path, ann_type,\n                        \', \'.join([e.name for e in CocoTask])))\n                continue\n            subset_name = name_parts[1]\n            subsets[subset_name][ann_type] = subset_path\n        return dict(subsets)\n'"
datumaro/datumaro/plugins/cvat_format/__init__.py,0,b''
datumaro/datumaro/plugins/cvat_format/converter.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nimport logging as log\nimport os\nimport os.path as osp\nfrom xml.sax.saxutils import XMLGenerator\n\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.extractor import DEFAULT_SUBSET_NAME, AnnotationType\nfrom datumaro.util import cast\nfrom datumaro.util.image import save_image\n\nfrom .format import CvatPath\n\n\ndef pairwise(iterable):\n    a = iter(iterable)\n    return zip(a, a)\n\nclass XmlAnnotationWriter:\n    VERSION = \'1.1\'\n\n    def __init__(self, f):\n        self.xmlgen = XMLGenerator(f, \'utf-8\')\n        self._level = 0\n\n    def _indent(self, newline = True):\n        if newline:\n            self.xmlgen.ignorableWhitespace(\'\\n\')\n        self.xmlgen.ignorableWhitespace(\'  \' * self._level)\n\n    def _add_version(self):\n        self._indent()\n        self.xmlgen.startElement(\'version\', {})\n        self.xmlgen.characters(self.VERSION)\n        self.xmlgen.endElement(\'version\')\n\n    def open_root(self):\n        self.xmlgen.startDocument()\n        self.xmlgen.startElement(\'annotations\', {})\n        self._level += 1\n        self._add_version()\n\n    def _add_meta(self, meta):\n        self._level += 1\n        for k, v in meta.items():\n            if isinstance(v, OrderedDict):\n                self._indent()\n                self.xmlgen.startElement(k, {})\n                self._add_meta(v)\n                self._indent()\n                self.xmlgen.endElement(k)\n            elif isinstance(v, list):\n                self._indent()\n                self.xmlgen.startElement(k, {})\n                for tup in v:\n                    self._add_meta(OrderedDict([tup]))\n                self._indent()\n                self.xmlgen.endElement(k)\n            else:\n                self._indent()\n                self.xmlgen.startElement(k, {})\n                self.xmlgen.characters(v)\n                self.xmlgen.endElement(k)\n        self._level -= 1\n\n    def write_meta(self, meta):\n        self._indent()\n        self.xmlgen.startElement(\'meta\', {})\n        self._add_meta(meta)\n        self._indent()\n        self.xmlgen.endElement(\'meta\')\n\n    def open_track(self, track):\n        self._indent()\n        self.xmlgen.startElement(\'track\', track)\n        self._level += 1\n\n    def open_image(self, image):\n        self._indent()\n        self.xmlgen.startElement(\'image\', image)\n        self._level += 1\n\n    def open_box(self, box):\n        self._indent()\n        self.xmlgen.startElement(\'box\', box)\n        self._level += 1\n\n    def open_polygon(self, polygon):\n        self._indent()\n        self.xmlgen.startElement(\'polygon\', polygon)\n        self._level += 1\n\n    def open_polyline(self, polyline):\n        self._indent()\n        self.xmlgen.startElement(\'polyline\', polyline)\n        self._level += 1\n\n    def open_points(self, points):\n        self._indent()\n        self.xmlgen.startElement(\'points\', points)\n        self._level += 1\n\n    def open_tag(self, tag):\n        self._indent()\n        self.xmlgen.startElement(""tag"", tag)\n        self._level += 1\n\n    def add_attribute(self, attribute):\n        self._indent()\n        self.xmlgen.startElement(\'attribute\', {\'name\': attribute[\'name\']})\n        self.xmlgen.characters(attribute[\'value\'])\n        self.xmlgen.endElement(\'attribute\')\n\n    def _close_element(self, element):\n        self._level -= 1\n        self._indent()\n        self.xmlgen.endElement(element)\n\n    def close_box(self):\n        self._close_element(\'box\')\n\n    def close_polygon(self):\n        self._close_element(\'polygon\')\n\n    def close_polyline(self):\n        self._close_element(\'polyline\')\n\n    def close_points(self):\n        self._close_element(\'points\')\n\n    def close_tag(self):\n        self._close_element(\'tag\')\n\n    def close_image(self):\n        self._close_element(\'image\')\n\n    def close_track(self):\n        self._close_element(\'track\')\n\n    def close_root(self):\n        self._close_element(\'annotations\')\n        self.xmlgen.endDocument()\n\nclass _SubsetWriter:\n    def __init__(self, file, name, extractor, context):\n        self._writer = XmlAnnotationWriter(file)\n        self._name = name\n        self._extractor = extractor\n        self._context = context\n\n    def write(self):\n        self._writer.open_root()\n        self._write_meta()\n\n        for index, item in enumerate(self._extractor):\n            self._write_item(item, index)\n\n        self._writer.close_root()\n\n    def _save_image(self, item):\n        image = item.image.data\n        if image is None:\n            log.warning(""Item \'%s\' has no image"" % item.id)\n            return \'\'\n\n        filename = item.image.filename\n        if filename:\n            filename = osp.splitext(filename)[0]\n        else:\n            filename = item.id\n        filename += CvatPath.IMAGE_EXT\n        image_path = osp.join(self._context._images_dir, filename)\n        save_image(image_path, image)\n        return filename\n\n    def _write_item(self, item, index):\n        image_info = OrderedDict([\n            (""id"", str(cast(item.id, int, index))),\n        ])\n        if item.has_image:\n            size = item.image.size\n            if size:\n                h, w = size\n                image_info[""width""] = str(w)\n                image_info[""height""] = str(h)\n\n            filename = item.image.filename\n            if self._context._save_images:\n                filename = self._save_image(item)\n            image_info[""name""] = filename\n        else:\n            log.debug(""Item \'%s\' has no image info"" % item.id)\n        self._writer.open_image(image_info)\n\n        for ann in item.annotations:\n            if ann.type in {AnnotationType.points, AnnotationType.polyline,\n                    AnnotationType.polygon, AnnotationType.bbox}:\n                self._write_shape(ann)\n            elif ann.type == AnnotationType.label:\n                self._write_tag(ann)\n            else:\n                continue\n\n        self._writer.close_image()\n\n    def _write_meta(self):\n        label_cat = self._extractor.categories()[AnnotationType.label]\n        meta = OrderedDict([\n            (""task"", OrderedDict([\n                (""id"", """"),\n                (""name"", self._name),\n                (""size"", str(len(self._extractor))),\n                (""mode"", ""annotation""),\n                (""overlap"", """"),\n                (""start_frame"", ""0""),\n                (""stop_frame"", str(len(self._extractor))),\n                (""frame_filter"", """"),\n                (""z_order"", ""True""),\n\n                (""labels"", [\n                    (""label"", OrderedDict([\n                        (""name"", label.name),\n                        (""attributes"", [\n                            (""attribute"", OrderedDict([\n                                (""name"", attr),\n                                (""mutable"", ""True""),\n                                (""input_type"", ""text""),\n                                (""default_value"", """"),\n                                (""values"", """"),\n                            ])) for attr in label.attributes\n                        ])\n                    ])) for label in label_cat.items\n                ]),\n            ])),\n        ])\n        self._writer.write_meta(meta)\n\n    def _get_label(self, label_id):\n        label_cat = self._extractor.categories()[AnnotationType.label]\n        return label_cat.items[label_id]\n\n    def _write_shape(self, shape):\n        if shape.label is None:\n            return\n\n        shape_data = OrderedDict([\n            (""label"", self._get_label(shape.label).name),\n            (""occluded"", str(int(shape.attributes.get(\'occluded\', False)))),\n        ])\n\n        if shape.type == AnnotationType.bbox:\n            shape_data.update(OrderedDict([\n                (""xtl"", ""{:.2f}"".format(shape.points[0])),\n                (""ytl"", ""{:.2f}"".format(shape.points[1])),\n                (""xbr"", ""{:.2f}"".format(shape.points[2])),\n                (""ybr"", ""{:.2f}"".format(shape.points[3]))\n            ]))\n        else:\n            shape_data.update(OrderedDict([\n                (""points"", \';\'.join((\n                    \',\'.join((\n                        ""{:.2f}"".format(x),\n                        ""{:.2f}"".format(y)\n                    )) for x, y in pairwise(shape.points))\n                )),\n            ]))\n\n        shape_data[\'z_order\'] = str(int(shape.z_order))\n        if shape.group:\n            shape_data[\'group_id\'] = str(shape.group)\n\n        if shape.type == AnnotationType.bbox:\n            self._writer.open_box(shape_data)\n        elif shape.type == AnnotationType.polygon:\n            self._writer.open_polygon(shape_data)\n        elif shape.type == AnnotationType.polyline:\n            self._writer.open_polyline(shape_data)\n        elif shape.type == AnnotationType.points:\n            self._writer.open_points(shape_data)\n        else:\n            raise NotImplementedError(""unknown shape type"")\n\n        for attr_name, attr_value in shape.attributes.items():\n            if isinstance(attr_value, bool):\n                attr_value = \'true\' if attr_value else \'false\'\n            if attr_name in self._get_label(shape.label).attributes:\n                self._writer.add_attribute(OrderedDict([\n                    (""name"", str(attr_name)),\n                    (""value"", str(attr_value)),\n                ]))\n\n        if shape.type == AnnotationType.bbox:\n            self._writer.close_box()\n        elif shape.type == AnnotationType.polygon:\n            self._writer.close_polygon()\n        elif shape.type == AnnotationType.polyline:\n            self._writer.close_polyline()\n        elif shape.type == AnnotationType.points:\n            self._writer.close_points()\n        else:\n            raise NotImplementedError(""unknown shape type"")\n\n    def _write_tag(self, label):\n        if label.label is None:\n            return\n\n        tag_data = OrderedDict([\n            (\'label\', self._get_label(label.label).name),\n        ])\n        if label.group:\n            tag_data[\'group_id\'] = str(label.group)\n        self._writer.open_tag(tag_data)\n\n        for attr_name, attr_value in label.attributes.items():\n            if isinstance(attr_value, bool):\n                attr_value = \'true\' if attr_value else \'false\'\n            if attr_name in self._get_label(label.label).attributes:\n                self._writer.add_attribute(OrderedDict([\n                    (""name"", str(attr_name)),\n                    (""value"", str(attr_value)),\n                ]))\n\n        self._writer.close_tag()\n\nclass _Converter:\n    def __init__(self, extractor, save_dir, save_images=False):\n        self._extractor = extractor\n        self._save_dir = save_dir\n        self._save_images = save_images\n\n    def convert(self):\n        os.makedirs(self._save_dir, exist_ok=True)\n\n        images_dir = osp.join(self._save_dir, CvatPath.IMAGES_DIR)\n        os.makedirs(images_dir, exist_ok=True)\n        self._images_dir = images_dir\n\n        subsets = self._extractor.subsets()\n        if len(subsets) == 0:\n            subsets = [ None ]\n\n        for subset_name in subsets:\n            if subset_name:\n                subset = self._extractor.get_subset(subset_name)\n            else:\n                subset_name = DEFAULT_SUBSET_NAME\n                subset = self._extractor\n\n            with open(osp.join(self._save_dir, \'%s.xml\' % subset_name), \'w\') as f:\n                writer = _SubsetWriter(f, subset_name, subset, self)\n                writer.write()\n\nclass CvatConverter(Converter, CliPlugin):\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        return parser\n\n    def __init__(self, save_images=False):\n        super().__init__()\n\n        self._options = {\n            \'save_images\': save_images,\n        }\n\n    def __call__(self, extractor, save_dir):\n        converter = _Converter(extractor, save_dir, **self._options)\n        converter.convert()'"
datumaro/datumaro/plugins/cvat_format/extractor.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nimport os.path as osp\nfrom defusedxml import ElementTree\n\nfrom datumaro.components.extractor import (SourceExtractor, DatasetItem,\n    AnnotationType, Points, Polygon, PolyLine, Bbox, Label,\n    LabelCategories\n)\nfrom datumaro.util.image import Image\n\nfrom .format import CvatPath\n\n\nclass CvatExtractor(SourceExtractor):\n    _SUPPORTED_SHAPES = (\'box\', \'polygon\', \'polyline\', \'points\')\n\n    def __init__(self, path):\n        assert osp.isfile(path), path\n        rootpath = osp.dirname(path)\n        images_dir = \'\'\n        if osp.isdir(osp.join(rootpath, CvatPath.IMAGES_DIR)):\n            images_dir = osp.join(rootpath, CvatPath.IMAGES_DIR)\n        self._images_dir = images_dir\n        self._path = path\n\n        super().__init__(subset=osp.splitext(osp.basename(path))[0])\n\n        items, categories = self._parse(path)\n        self._items = self._load_items(items)\n        self._categories = categories\n\n    def categories(self):\n        return self._categories\n\n    def __iter__(self):\n        for item in self._items.values():\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n\n    @classmethod\n    def _parse(cls, path):\n        context = ElementTree.iterparse(path, events=(""start"", ""end""))\n        context = iter(context)\n\n        categories, frame_size = cls._parse_meta(context)\n\n        items = OrderedDict()\n\n        track = None\n        shape = None\n        tag = None\n        attributes = None\n        image = None\n        for ev, el in context:\n            if ev == \'start\':\n                if el.tag == \'track\':\n                    track = {\n                        \'id\': el.attrib[\'id\'],\n                        \'label\': el.attrib.get(\'label\'),\n                        \'group\': int(el.attrib.get(\'group_id\', 0)),\n                        \'height\': frame_size[0],\n                        \'width\': frame_size[1],\n                    }\n                elif el.tag == \'image\':\n                    image = {\n                        \'name\': el.attrib.get(\'name\'),\n                        \'frame\': el.attrib[\'id\'],\n                        \'width\': el.attrib.get(\'width\'),\n                        \'height\': el.attrib.get(\'height\'),\n                    }\n                elif el.tag in cls._SUPPORTED_SHAPES and (track or image):\n                    attributes = {}\n                    shape = {\n                        \'type\': None,\n                        \'attributes\': attributes,\n                    }\n                    if track:\n                        shape.update(track)\n                        shape[\'track_id\'] = int(track[\'id\'])\n                    if image:\n                        shape.update(image)\n                elif el.tag == \'tag\' and image:\n                    attributes = {}\n                    tag = {\n                        \'frame\': image[\'frame\'],\n                        \'attributes\': attributes,\n                        \'group\': int(el.attrib.get(\'group_id\', 0)),\n                        \'label\': el.attrib[\'label\'],\n                    }\n            elif ev == \'end\':\n                if el.tag == \'attribute\' and attributes is not None:\n                    attr_value = el.text\n                    if el.text in [\'true\', \'false\']:\n                        attr_value = attr_value == \'true\'\n                    else:\n                        try:\n                            attr_value = float(attr_value)\n                        except Exception:\n                            pass\n                    attributes[el.attrib[\'name\']] = attr_value\n                elif el.tag in cls._SUPPORTED_SHAPES:\n                    if track is not None:\n                        shape[\'frame\'] = el.attrib[\'frame\']\n                        shape[\'outside\'] = (el.attrib.get(\'outside\') == \'1\')\n                        shape[\'keyframe\'] = (el.attrib.get(\'keyframe\') == \'1\')\n                    if image is not None:\n                        shape[\'label\'] = el.attrib.get(\'label\')\n                        shape[\'group\'] = int(el.attrib.get(\'group_id\', 0))\n\n                    shape[\'type\'] = el.tag\n                    shape[\'occluded\'] = (el.attrib.get(\'occluded\') == \'1\')\n                    shape[\'z_order\'] = int(el.attrib.get(\'z_order\', 0))\n\n                    if el.tag == \'box\':\n                        shape[\'points\'] = list(map(float, [\n                            el.attrib[\'xtl\'], el.attrib[\'ytl\'],\n                            el.attrib[\'xbr\'], el.attrib[\'ybr\'],\n                        ]))\n                    else:\n                        shape[\'points\'] = []\n                        for pair in el.attrib[\'points\'].split(\';\'):\n                            shape[\'points\'].extend(map(float, pair.split(\',\')))\n\n                    frame_desc = items.get(shape[\'frame\'], {\'annotations\': []})\n                    frame_desc[\'annotations\'].append(\n                        cls._parse_shape_ann(shape, categories))\n                    items[shape[\'frame\']] = frame_desc\n                    shape = None\n\n                elif el.tag == \'tag\':\n                    frame_desc = items.get(tag[\'frame\'], {\'annotations\': []})\n                    frame_desc[\'annotations\'].append(\n                        cls._parse_tag_ann(tag, categories))\n                    items[tag[\'frame\']] = frame_desc\n                    tag = None\n                elif el.tag == \'track\':\n                    track = None\n                elif el.tag == \'image\':\n                    frame_desc = items.get(image[\'frame\'], {\'annotations\': []})\n                    frame_desc.update({\n                        \'name\': image.get(\'name\'),\n                        \'height\': image.get(\'height\'),\n                        \'width\': image.get(\'width\'),\n                    })\n                    items[image[\'frame\']] = frame_desc\n                    image = None\n                el.clear()\n\n        return items, categories\n\n    @staticmethod\n    def _parse_meta(context):\n        ev, el = next(context)\n        if not (ev == \'start\' and el.tag == \'annotations\'):\n            raise Exception(""Unexpected token "")\n\n        categories = {}\n\n        frame_size = None\n        mode = None\n        labels = OrderedDict()\n        label = None\n\n        # Recursive descent parser\n        el = None\n        states = [\'annotations\']\n        def accepted(expected_state, tag, next_state=None):\n            state = states[-1]\n            if state == expected_state and el is not None and el.tag == tag:\n                if not next_state:\n                    next_state = tag\n                states.append(next_state)\n                return True\n            return False\n        def consumed(expected_state, tag):\n            state = states[-1]\n            if state == expected_state and el is not None and el.tag == tag:\n                states.pop()\n                return True\n            return False\n\n        for ev, el in context:\n            if ev == \'start\':\n                if accepted(\'annotations\', \'meta\'): pass\n                elif accepted(\'meta\', \'task\'): pass\n                elif accepted(\'task\', \'mode\'): pass\n                elif accepted(\'task\', \'original_size\'):\n                    frame_size = [None, None]\n                elif accepted(\'original_size\', \'height\', next_state=\'frame_height\'): pass\n                elif accepted(\'original_size\', \'width\', next_state=\'frame_width\'): pass\n                elif accepted(\'task\', \'labels\'): pass\n                elif accepted(\'labels\', \'label\'):\n                    label = { \'name\': None, \'attributes\': set() }\n                elif accepted(\'label\', \'name\', next_state=\'label_name\'): pass\n                elif accepted(\'label\', \'attributes\'): pass\n                elif accepted(\'attributes\', \'attribute\'): pass\n                elif accepted(\'attribute\', \'name\', next_state=\'attr_name\'): pass\n                elif accepted(\'annotations\', \'image\') or \\\n                     accepted(\'annotations\', \'track\') or \\\n                     accepted(\'annotations\', \'tag\'):\n                    break\n                else:\n                    pass\n            elif ev == \'end\':\n                if consumed(\'meta\', \'meta\'):\n                    break\n                elif consumed(\'task\', \'task\'): pass\n                elif consumed(\'mode\', \'mode\'):\n                    mode = el.text\n                elif consumed(\'original_size\', \'original_size\'): pass\n                elif consumed(\'frame_height\', \'height\'):\n                    frame_size[0] = int(el.text)\n                elif consumed(\'frame_width\', \'width\'):\n                    frame_size[1] = int(el.text)\n                elif consumed(\'label_name\', \'name\'):\n                    label[\'name\'] = el.text\n                elif consumed(\'attr_name\', \'name\'):\n                    label[\'attributes\'].add(el.text)\n                elif consumed(\'attribute\', \'attribute\'): pass\n                elif consumed(\'attributes\', \'attributes\'): pass\n                elif consumed(\'label\', \'label\'):\n                    labels[label[\'name\']] = label[\'attributes\']\n                    label = None\n                elif consumed(\'labels\', \'labels\'): pass\n                else:\n                    pass\n\n        assert len(states) == 1 and states[0] == \'annotations\', \\\n            ""Expected \'meta\' section in the annotation file, path: %s"" % states\n\n        common_attrs = [\'occluded\']\n        if mode == \'interpolation\':\n            common_attrs.append(\'keyframe\')\n            common_attrs.append(\'outside\')\n            common_attrs.append(\'track_id\')\n\n        label_cat = LabelCategories(attributes=common_attrs)\n        for label, attrs in labels.items():\n            label_cat.add(label, attributes=attrs)\n\n        categories[AnnotationType.label] = label_cat\n\n        return categories, frame_size\n\n    @classmethod\n    def _parse_shape_ann(cls, ann, categories):\n        ann_id = ann.get(\'id\')\n        ann_type = ann[\'type\']\n\n        attributes = ann.get(\'attributes\') or {}\n        if \'occluded\' in categories[AnnotationType.label].attributes:\n            attributes[\'occluded\'] = ann.get(\'occluded\', False)\n        if \'outside\' in ann:\n            attributes[\'outside\'] = ann[\'outside\']\n        if \'keyframe\' in ann:\n            attributes[\'keyframe\'] = ann[\'keyframe\']\n        if \'track_id\' in ann:\n            attributes[\'track_id\'] = ann[\'track_id\']\n\n        group = ann.get(\'group\')\n\n        label = ann.get(\'label\')\n        label_id = categories[AnnotationType.label].find(label)[0]\n\n        z_order = ann.get(\'z_order\', 0)\n        points = ann.get(\'points\', [])\n\n        if ann_type == \'polyline\':\n            return PolyLine(points, label=label_id, z_order=z_order,\n                id=ann_id, attributes=attributes, group=group)\n\n        elif ann_type == \'polygon\':\n            return Polygon(points, label=label_id, z_order=z_order,\n                id=ann_id, attributes=attributes, group=group)\n\n        elif ann_type == \'points\':\n            return Points(points, label=label_id, z_order=z_order,\n                id=ann_id, attributes=attributes, group=group)\n\n        elif ann_type == \'box\':\n            x, y = points[0], points[1]\n            w, h = points[2] - x, points[3] - y\n            return Bbox(x, y, w, h, label=label_id, z_order=z_order,\n                id=ann_id, attributes=attributes, group=group)\n\n        else:\n            raise NotImplementedError(""Unknown annotation type \'%s\'"" % ann_type)\n\n    @classmethod\n    def _parse_tag_ann(cls, ann, categories):\n        label = ann.get(\'label\')\n        label_id = categories[AnnotationType.label].find(label)[0]\n        group = ann.get(\'group\')\n        attributes = ann.get(\'attributes\')\n        return Label(label_id, attributes=attributes, group=group)\n\n    def _load_items(self, parsed):\n        for frame_id, item_desc in parsed.items():\n            path = item_desc.get(\'name\', \'frame_%06d.png\' % int(frame_id))\n            image_size = (item_desc.get(\'height\'), item_desc.get(\'width\'))\n            if all(image_size):\n                image_size = (int(image_size[0]), int(image_size[1]))\n            else:\n                image_size = None\n            image = None\n            if path:\n                image = Image(path=osp.join(self._images_dir, path),\n                    size=image_size)\n\n            parsed[frame_id] = DatasetItem(id=frame_id, subset=self._subset,\n                image=image, annotations=item_desc.get(\'annotations\'))\n        return parsed\n'"
datumaro/datumaro/plugins/cvat_format/format.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nclass CvatPath:\n    IMAGES_DIR = 'images'\n\n    IMAGE_EXT = '.jpg'\n"""
datumaro/datumaro/plugins/cvat_format/importer.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom glob import glob\nimport logging as log\nimport os.path as osp\n\nfrom datumaro.components.extractor import Importer\n\n\nclass CvatImporter(Importer):\n    EXTRACTOR_NAME = \'cvat\'\n\n    @classmethod\n    def detect(cls, path):\n        return len(cls.find_subsets(path)) != 0\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        subset_paths = self.find_subsets(path)\n\n        if len(subset_paths) == 0:\n            raise Exception(""Failed to find \'cvat\' dataset at \'%s\'"" % path)\n\n        for subset_path in subset_paths:\n            if not osp.isfile(subset_path):\n                continue\n\n            log.info(""Found a dataset at \'%s\'"" % subset_path)\n\n            subset_name = osp.splitext(osp.basename(subset_path))[0]\n\n            project.add_source(subset_name, {\n                \'url\': subset_path,\n                \'format\': self.EXTRACTOR_NAME,\n                \'options\': dict(extra_params),\n            })\n\n        return project\n\n    @staticmethod\n    def find_subsets(path):\n        if path.endswith(\'.xml\') and osp.isfile(path):\n            subset_paths = [path]\n        else:\n            subset_paths = glob(osp.join(path, \'**\', \'*.xml\'), recursive=True)\n        return subset_paths'"
datumaro/datumaro/plugins/datumaro_format/__init__.py,0,b''
datumaro/datumaro/plugins/datumaro_format/converter.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n# pylint: disable=no-self-use\n\nimport json\nimport numpy as np\nimport os\nimport os.path as osp\n\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.extractor import (\n    DEFAULT_SUBSET_NAME, Annotation, _Shape,\n    Label, Mask, RleMask, Points, Polygon, PolyLine, Bbox, Caption,\n    LabelCategories, MaskCategories, PointsCategories\n)\nfrom datumaro.util import cast\nfrom datumaro.util.image import save_image\nimport pycocotools.mask as mask_utils\nfrom datumaro.components.cli_plugin import CliPlugin\n\nfrom .format import DatumaroPath\n\n\nclass _SubsetWriter:\n    def __init__(self, name, context):\n        self._name = name\n        self._context = context\n\n        self._data = {\n            \'info\': {},\n            \'categories\': {},\n            \'items\': [],\n        }\n\n    @property\n    def categories(self):\n        return self._data[\'categories\']\n\n    @property\n    def items(self):\n        return self._data[\'items\']\n\n    def write_item(self, item):\n        annotations = []\n        item_desc = {\n            \'id\': item.id,\n            \'annotations\': annotations,\n        }\n        if item.attributes:\n            item_desc[\'attr\'] = item.attributes\n        if item.path:\n            item_desc[\'path\'] = item.path\n        if item.has_image:\n            path = item.image.path\n            if self._context._save_images:\n                path = self._context._save_image(item)\n\n            item_desc[\'image\'] = {\n                \'size\': item.image.size,\n                \'path\': path,\n            }\n        self.items.append(item_desc)\n\n        for ann in item.annotations:\n            if isinstance(ann, Label):\n                converted_ann = self._convert_label_object(ann)\n            elif isinstance(ann, Mask):\n                converted_ann = self._convert_mask_object(ann)\n            elif isinstance(ann, Points):\n                converted_ann = self._convert_points_object(ann)\n            elif isinstance(ann, PolyLine):\n                converted_ann = self._convert_polyline_object(ann)\n            elif isinstance(ann, Polygon):\n                converted_ann = self._convert_polygon_object(ann)\n            elif isinstance(ann, Bbox):\n                converted_ann = self._convert_bbox_object(ann)\n            elif isinstance(ann, Caption):\n                converted_ann = self._convert_caption_object(ann)\n            else:\n                raise NotImplementedError()\n            annotations.append(converted_ann)\n\n    def write_categories(self, categories):\n        for ann_type, desc in categories.items():\n            if isinstance(desc, LabelCategories):\n                converted_desc = self._convert_label_categories(desc)\n            elif isinstance(desc, MaskCategories):\n                converted_desc = self._convert_mask_categories(desc)\n            elif isinstance(desc, PointsCategories):\n                converted_desc = self._convert_points_categories(desc)\n            else:\n                raise NotImplementedError()\n            self.categories[ann_type.name] = converted_desc\n\n    def write(self, save_dir):\n        with open(osp.join(save_dir, \'%s.json\' % (self._name)), \'w\') as f:\n            json.dump(self._data, f)\n\n    def _convert_annotation(self, obj):\n        assert isinstance(obj, Annotation)\n\n        ann_json = {\n            \'id\': cast(obj.id, int),\n            \'type\': cast(obj.type.name, str),\n            \'attributes\': obj.attributes,\n            \'group\': cast(obj.group, int, 0),\n        }\n        return ann_json\n\n    def _convert_label_object(self, obj):\n        converted = self._convert_annotation(obj)\n\n        converted.update({\n            \'label_id\': cast(obj.label, int),\n        })\n        return converted\n\n    def _convert_mask_object(self, obj):\n        converted = self._convert_annotation(obj)\n\n        if isinstance(obj, RleMask):\n            rle = obj.rle\n        else:\n            rle = mask_utils.encode(\n                np.require(obj.image, dtype=np.uint8, requirements=\'F\'))\n\n        converted.update({\n            \'label_id\': cast(obj.label, int),\n            \'rle\': {\n                # serialize as compressed COCO mask\n                \'counts\': rle[\'counts\'].decode(\'ascii\'),\n                \'size\': list(int(c) for c in rle[\'size\']),\n            },\n            \'z_order\': obj.z_order,\n        })\n        return converted\n\n    def _convert_shape_object(self, obj):\n        assert isinstance(obj, _Shape)\n        converted = self._convert_annotation(obj)\n\n        converted.update({\n            \'label_id\': cast(obj.label, int),\n            \'points\': [float(p) for p in obj.points],\n            \'z_order\': obj.z_order,\n        })\n        return converted\n\n    def _convert_polyline_object(self, obj):\n        return self._convert_shape_object(obj)\n\n    def _convert_polygon_object(self, obj):\n        return self._convert_shape_object(obj)\n\n    def _convert_bbox_object(self, obj):\n        converted = self._convert_shape_object(obj)\n        converted.pop(\'points\', None)\n        converted[\'bbox\'] = [float(p) for p in obj.get_bbox()]\n        return converted\n\n    def _convert_points_object(self, obj):\n        converted = self._convert_shape_object(obj)\n\n        converted.update({\n            \'visibility\': [int(v.value) for v in obj.visibility],\n        })\n        return converted\n\n    def _convert_caption_object(self, obj):\n        converted = self._convert_annotation(obj)\n\n        converted.update({\n            \'caption\': cast(obj.caption, str),\n        })\n        return converted\n\n    def _convert_label_categories(self, obj):\n        converted = {\n            \'labels\': [],\n        }\n        for label in obj.items:\n            converted[\'labels\'].append({\n                \'name\': cast(label.name, str),\n                \'parent\': cast(label.parent, str),\n            })\n        return converted\n\n    def _convert_mask_categories(self, obj):\n        converted = {\n            \'colormap\': [],\n        }\n        for label_id, color in obj.colormap.items():\n            converted[\'colormap\'].append({\n                \'label_id\': int(label_id),\n                \'r\': int(color[0]),\n                \'g\': int(color[1]),\n                \'b\': int(color[2]),\n            })\n        return converted\n\n    def _convert_points_categories(self, obj):\n        converted = {\n            \'items\': [],\n        }\n        for label_id, item in obj.items.items():\n            converted[\'items\'].append({\n                \'label_id\': int(label_id),\n                \'labels\': [cast(label, str) for label in item.labels],\n                \'joints\': [list(map(int, j)) for j in item.joints],\n            })\n        return converted\n\nclass _Converter:\n    def __init__(self, extractor, save_dir, save_images=False):\n        self._extractor = extractor\n        self._save_dir = save_dir\n        self._save_images = save_images\n\n    def convert(self):\n        os.makedirs(self._save_dir, exist_ok=True)\n\n        images_dir = osp.join(self._save_dir, DatumaroPath.IMAGES_DIR)\n        os.makedirs(images_dir, exist_ok=True)\n        self._images_dir = images_dir\n\n        annotations_dir = osp.join(self._save_dir, DatumaroPath.ANNOTATIONS_DIR)\n        os.makedirs(annotations_dir, exist_ok=True)\n        self._annotations_dir = annotations_dir\n\n        subsets = self._extractor.subsets()\n        if len(subsets) == 0:\n            subsets = [ None ]\n        subsets = [n or DEFAULT_SUBSET_NAME for n in subsets]\n        subsets = { name: _SubsetWriter(name, self) for name in subsets }\n\n        for subset, writer in subsets.items():\n            writer.write_categories(self._extractor.categories())\n\n        for item in self._extractor:\n            subset = item.subset or DEFAULT_SUBSET_NAME\n            writer = subsets[subset]\n\n            writer.write_item(item)\n\n        for subset, writer in subsets.items():\n            writer.write(annotations_dir)\n\n    def _save_image(self, item):\n        image = item.image.data\n        if image is None:\n            return \'\'\n\n        filename = item.image.filename\n        if filename:\n            filename = osp.splitext(filename)[0]\n        else:\n            filename = item.id\n        filename += DatumaroPath.IMAGE_EXT\n        image_path = osp.join(self._images_dir, filename)\n        save_image(image_path, image, create_dir=True)\n        return filename\n\nclass DatumaroConverter(Converter, CliPlugin):\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        return parser\n\n    def __init__(self, save_images=False):\n        super().__init__()\n\n        self._options = {\n            \'save_images\': save_images,\n        }\n\n    def __call__(self, extractor, save_dir):\n        converter = _Converter(extractor, save_dir, **self._options)\n        converter.convert()\n\n\nclass DatumaroProjectConverter(Converter):\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        return parser\n\n    def __init__(self, config=None, save_images=False):\n        self._config = config\n        self._save_images = save_images\n\n    def __call__(self, extractor, save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n\n        from datumaro.components.project import Project\n        project = Project.generate(save_dir, config=self._config)\n\n        converter = project.env.make_converter(\'datumaro\',\n            save_images=self._save_images)\n        converter(extractor, save_dir=osp.join(\n            project.config.project_dir, project.config.dataset_dir))'"
datumaro/datumaro/plugins/datumaro_format/extractor.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport json\nimport os.path as osp\n\nfrom datumaro.components.extractor import (SourceExtractor, DatasetItem,\n    AnnotationType, Label, RleMask, Points, Polygon, PolyLine, Bbox, Caption,\n    LabelCategories, MaskCategories, PointsCategories\n)\nfrom datumaro.util.image import Image\n\nfrom .format import DatumaroPath\n\n\nclass DatumaroExtractor(SourceExtractor):\n    def __init__(self, path):\n        assert osp.isfile(path), path\n        rootpath = ''\n        if path.endswith(osp.join(DatumaroPath.ANNOTATIONS_DIR, osp.basename(path))):\n            rootpath = path.rsplit(DatumaroPath.ANNOTATIONS_DIR, maxsplit=1)[0]\n        images_dir = ''\n        if rootpath and osp.isdir(osp.join(rootpath, DatumaroPath.IMAGES_DIR)):\n            images_dir = osp.join(rootpath, DatumaroPath.IMAGES_DIR)\n        self._images_dir = images_dir\n\n        super().__init__(subset=osp.splitext(osp.basename(path))[0])\n\n        with open(path, 'r') as f:\n            parsed_anns = json.load(f)\n        self._categories = self._load_categories(parsed_anns)\n        self._items = self._load_items(parsed_anns)\n\n    def categories(self):\n        return self._categories\n\n    def __iter__(self):\n        for item in self._items:\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n\n    @staticmethod\n    def _load_categories(parsed):\n        categories = {}\n\n        parsed_label_cat = parsed['categories'].get(AnnotationType.label.name)\n        if parsed_label_cat:\n            label_categories = LabelCategories()\n            for item in parsed_label_cat['labels']:\n                label_categories.add(item['name'], parent=item['parent'])\n\n            categories[AnnotationType.label] = label_categories\n\n        parsed_mask_cat = parsed['categories'].get(AnnotationType.mask.name)\n        if parsed_mask_cat:\n            colormap = {}\n            for item in parsed_mask_cat['colormap']:\n                colormap[int(item['label_id'])] = \\\n                    (item['r'], item['g'], item['b'])\n\n            mask_categories = MaskCategories(colormap=colormap)\n            categories[AnnotationType.mask] = mask_categories\n\n        parsed_points_cat = parsed['categories'].get(AnnotationType.points.name)\n        if parsed_points_cat:\n            point_categories = PointsCategories()\n            for item in parsed_points_cat['items']:\n                point_categories.add(int(item['label_id']),\n                    item['labels'], joints=item['joints'])\n\n            categories[AnnotationType.points] = point_categories\n\n        return categories\n\n    def _load_items(self, parsed):\n        items = []\n        for item_desc in parsed['items']:\n            item_id = item_desc['id']\n\n            image = None\n            image_info = item_desc.get('image', {})\n            if image_info:\n                image_path = osp.join(self._images_dir,\n                    image_info.get('path', '')) # relative or absolute fits\n                image = Image(path=image_path, size=image_info.get('size'))\n\n            annotations = self._load_annotations(item_desc)\n\n            item = DatasetItem(id=item_id, subset=self._subset,\n                annotations=annotations, image=image,\n                attributes=item_desc.get('attr'))\n\n            items.append(item)\n\n        return items\n\n    @staticmethod\n    def _load_annotations(item):\n        parsed = item['annotations']\n        loaded = []\n\n        for ann in parsed:\n            ann_id = ann.get('id')\n            ann_type = AnnotationType[ann['type']]\n            attributes = ann.get('attributes')\n            group = ann.get('group')\n\n            label_id = ann.get('label_id')\n            z_order = ann.get('z_order')\n            points = ann.get('points')\n\n            if ann_type == AnnotationType.label:\n                loaded.append(Label(label=label_id,\n                    id=ann_id, attributes=attributes, group=group))\n\n            elif ann_type == AnnotationType.mask:\n                rle = ann['rle']\n                rle['counts'] = rle['counts'].encode('ascii')\n                loaded.append(RleMask(rle=rle, label=label_id,\n                    id=ann_id, attributes=attributes, group=group,\n                    z_order=z_order))\n\n            elif ann_type == AnnotationType.polyline:\n                loaded.append(PolyLine(points, label=label_id,\n                    id=ann_id, attributes=attributes, group=group,\n                    z_order=z_order))\n\n            elif ann_type == AnnotationType.polygon:\n                loaded.append(Polygon(points, label=label_id,\n                    id=ann_id, attributes=attributes, group=group,\n                    z_order=z_order))\n\n            elif ann_type == AnnotationType.bbox:\n                x, y, w, h = ann['bbox']\n                loaded.append(Bbox(x, y, w, h, label=label_id,\n                    id=ann_id, attributes=attributes, group=group,\n                    z_order=z_order))\n\n            elif ann_type == AnnotationType.points:\n                loaded.append(Points(points, label=label_id,\n                    id=ann_id, attributes=attributes, group=group,\n                    z_order=z_order))\n\n            elif ann_type == AnnotationType.caption:\n                caption = ann.get('caption')\n                loaded.append(Caption(caption,\n                    id=ann_id, attributes=attributes, group=group))\n\n            else:\n                raise NotImplementedError()\n\n        return loaded\n"""
datumaro/datumaro/plugins/datumaro_format/format.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nclass DatumaroPath:\n    IMAGES_DIR = 'images'\n    ANNOTATIONS_DIR = 'annotations'\n    MASKS_DIR = 'masks'\n\n    IMAGE_EXT = '.jpg'\n    MASK_EXT = '.png'\n"""
datumaro/datumaro/plugins/datumaro_format/importer.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom glob import glob\nimport logging as log\nimport os.path as osp\n\nfrom datumaro.components.extractor import Importer\n\nfrom .format import DatumaroPath\n\n\nclass DatumaroImporter(Importer):\n    EXTRACTOR_NAME = \'datumaro\'\n\n    @classmethod\n    def detect(cls, path):\n        return len(cls.find_subsets(path)) != 0\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        subset_paths = self.find_subsets(path)\n        if len(subset_paths) == 0:\n            raise Exception(""Failed to find \'datumaro\' dataset at \'%s\'"" % path)\n\n        for subset_path in subset_paths:\n            if not osp.isfile(subset_path):\n                continue\n\n            log.info(""Found a dataset at \'%s\'"" % subset_path)\n\n            subset_name = osp.splitext(osp.basename(subset_path))[0]\n\n            project.add_source(subset_name, {\n                \'url\': subset_path,\n                \'format\': self.EXTRACTOR_NAME,\n                \'options\': dict(extra_params),\n            })\n\n        return project\n\n    @staticmethod\n    def find_subsets(path):\n        if path.endswith(\'.json\') and osp.isfile(path):\n            subset_paths = [path]\n        else:\n            subset_paths = glob(osp.join(path, \'*.json\'))\n\n            if osp.basename(osp.normpath(path)) != DatumaroPath.ANNOTATIONS_DIR:\n                path = osp.join(path, DatumaroPath.ANNOTATIONS_DIR)\n                subset_paths += glob(osp.join(path, \'*.json\'))\n        return subset_paths'"
datumaro/datumaro/plugins/tf_detection_api_format/__init__.py,0,b''
datumaro/datumaro/plugins/tf_detection_api_format/converter.py,8,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport codecs\nfrom collections import OrderedDict\nimport logging as log\nimport os\nimport os.path as osp\nimport string\n\nfrom datumaro.components.extractor import (AnnotationType, DEFAULT_SUBSET_NAME,\n    LabelCategories\n)\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom datumaro.util.image import encode_image\nfrom datumaro.util.mask_tools import merge_masks\nfrom datumaro.util.annotation_tools import (compute_bbox,\n    find_group_leader, find_instances)\nfrom datumaro.util.tf_util import import_tf as _import_tf\n\nfrom .format import DetectionApiPath\ntf = _import_tf()\n\n\n# filter out non-ASCII characters, otherwise training will crash\n_printable = set(string.printable)\ndef _make_printable(s):\n    return \'\'.join(filter(lambda x: x in _printable, s))\n\ndef int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef int64_list_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\ndef bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef bytes_list_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\ndef float_list_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\nclass TfDetectionApiConverter(Converter, CliPlugin):\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        parser.add_argument(\'--save-masks\', action=\'store_true\',\n            help=""Include instance masks (default: %(default)s)"")\n        return parser\n\n    def __init__(self, save_images=False, save_masks=False):\n        super().__init__()\n\n        self._save_images = save_images\n        self._save_masks = save_masks\n\n    def __call__(self, extractor, save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n\n        label_categories = extractor.categories().get(AnnotationType.label,\n            LabelCategories())\n        get_label = lambda label_id: label_categories.items[label_id].name \\\n            if label_id is not None else \'\'\n        label_ids = OrderedDict((label.name, 1 + idx)\n            for idx, label in enumerate(label_categories.items))\n        map_label_id = lambda label_id: label_ids.get(get_label(label_id), 0)\n        self._get_label = get_label\n        self._get_label_id = map_label_id\n\n        subsets = extractor.subsets()\n        if len(subsets) == 0:\n            subsets = [ None ]\n\n        for subset_name in subsets:\n            if subset_name:\n                subset = extractor.get_subset(subset_name)\n            else:\n                subset_name = DEFAULT_SUBSET_NAME\n                subset = extractor\n\n            labelmap_path = osp.join(save_dir, DetectionApiPath.LABELMAP_FILE)\n            with codecs.open(labelmap_path, \'w\', encoding=\'utf8\') as f:\n                for label, idx in label_ids.items():\n                    f.write(\n                        \'item {\\n\' +\n                        (\'\\tid: %s\\n\' % (idx)) +\n                        (""\\tname: \'%s\'\\n"" % (label)) +\n                        \'}\\n\\n\'\n                    )\n\n            anno_path = osp.join(save_dir, \'%s.tfrecord\' % (subset_name))\n            with tf.io.TFRecordWriter(anno_path) as writer:\n                for item in subset:\n                    tf_example = self._make_tf_example(item)\n                    writer.write(tf_example.SerializeToString())\n\n    @staticmethod\n    def _find_instances(annotations):\n        return find_instances(a for a in annotations\n            if a.type in { AnnotationType.bbox, AnnotationType.mask })\n\n    def _find_instance_parts(self, group, img_width, img_height):\n        boxes = [a for a in group if a.type == AnnotationType.bbox]\n        masks = [a for a in group if a.type == AnnotationType.mask]\n\n        anns = boxes + masks\n        leader = find_group_leader(anns)\n        bbox = compute_bbox(anns)\n\n        mask = None\n        if self._save_masks:\n            mask = merge_masks([m.image for m in masks])\n\n        return [leader, mask, bbox]\n\n    def _export_instances(self, instances, width, height):\n        xmins = [] # List of normalized left x coordinates of bounding boxes (1 per box)\n        xmaxs = [] # List of normalized right x coordinates of bounding boxes (1 per box)\n        ymins = [] # List of normalized top y coordinates of bounding boxes (1 per box)\n        ymaxs = [] # List of normalized bottom y coordinates of bounding boxes (1 per box)\n        classes_text = [] # List of class names of bounding boxes (1 per box)\n        classes = [] # List of class ids of bounding boxes (1 per box)\n        masks = [] # List of PNG-encoded instance masks (1 per box)\n\n        for leader, mask, box in instances:\n            label = _make_printable(self._get_label(leader.label))\n            classes_text.append(label.encode(\'utf-8\'))\n            classes.append(self._get_label_id(leader.label))\n\n            xmins.append(box[0] / width)\n            xmaxs.append((box[0] + box[2]) / width)\n            ymins.append(box[1] / height)\n            ymaxs.append((box[1] + box[3]) / height)\n\n            if self._save_masks:\n                if mask is not None:\n                    mask = encode_image(mask, \'.png\')\n                else:\n                    mask = b\'\'\n                masks.append(mask)\n\n        result = {}\n        if classes:\n            result = {\n                \'image/object/bbox/xmin\': float_list_feature(xmins),\n                \'image/object/bbox/xmax\': float_list_feature(xmaxs),\n                \'image/object/bbox/ymin\': float_list_feature(ymins),\n                \'image/object/bbox/ymax\': float_list_feature(ymaxs),\n                \'image/object/class/text\': bytes_list_feature(classes_text),\n                \'image/object/class/label\': int64_list_feature(classes),\n            }\n            if masks:\n                result[\'image/object/mask\'] = bytes_list_feature(masks)\n        return result\n\n    def _make_tf_example(self, item):\n        features = {\n            \'image/source_id\': bytes_feature(str(item.id).encode(\'utf-8\')),\n        }\n\n        filename = \'\'\n        if item.has_image:\n            filename = item.image.filename\n        if not filename:\n            filename = item.id + DetectionApiPath.IMAGE_EXT\n        features[\'image/filename\'] = bytes_feature(filename.encode(\'utf-8\'))\n\n        if not item.has_image:\n            raise Exception(""Failed to export dataset item \'%s\': ""\n                ""item has no image info"" % item.id)\n        height, width = item.image.size\n\n        features.update({\n            \'image/height\': int64_feature(height),\n            \'image/width\': int64_feature(width),\n        })\n\n        features.update({\n            \'image/encoded\': bytes_feature(b\'\'),\n            \'image/format\': bytes_feature(b\'\')\n        })\n        if self._save_images:\n            if item.has_image and item.image.has_data:\n                fmt = DetectionApiPath.IMAGE_FORMAT\n                buffer = encode_image(item.image.data, DetectionApiPath.IMAGE_EXT)\n\n                features.update({\n                    \'image/encoded\': bytes_feature(buffer),\n                    \'image/format\': bytes_feature(fmt.encode(\'utf-8\')),\n                })\n            else:\n                log.warning(""Item \'%s\' has no image"" % item.id)\n\n        instances = self._find_instances(item.annotations)\n        instances = [self._find_instance_parts(i, width, height) for i in instances]\n        features.update(self._export_instances(instances, width, height))\n\n        tf_example = tf.train.Example(\n            features=tf.train.Features(feature=features))\n\n        return tf_example\n'"
datumaro/datumaro/plugins/tf_detection_api_format/extractor.py,26,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nimport numpy as np\nimport os.path as osp\nimport re\n\nfrom datumaro.components.extractor import (SourceExtractor, DatasetItem,\n    AnnotationType, Bbox, Mask, LabelCategories\n)\nfrom datumaro.util.image import Image, decode_image, lazy_image\nfrom datumaro.util.tf_util import import_tf as _import_tf\n\nfrom .format import DetectionApiPath\ntf = _import_tf()\n\n\ndef clamp(value, _min, _max):\n    return max(min(_max, value), _min)\n\nclass TfDetectionApiExtractor(SourceExtractor):\n    def __init__(self, path):\n        assert osp.isfile(path), path\n        images_dir = \'\'\n        root_dir = osp.dirname(osp.abspath(path))\n        if osp.basename(root_dir) == DetectionApiPath.ANNOTATIONS_DIR:\n            root_dir = osp.dirname(root_dir)\n            images_dir = osp.join(root_dir, DetectionApiPath.IMAGES_DIR)\n            if not osp.isdir(images_dir):\n                images_dir = \'\'\n\n        super().__init__(subset=osp.splitext(osp.basename(path))[0])\n\n        items, labels = self._parse_tfrecord_file(path, self._subset, images_dir)\n        self._items = items\n        self._categories = self._load_categories(labels)\n\n    def categories(self):\n        return self._categories\n\n    def __iter__(self):\n        for item in self._items:\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n\n    @staticmethod\n    def _load_categories(labels):\n        label_categories = LabelCategories()\n        labels = sorted(labels.items(), key=lambda item: item[1])\n        for label, _ in labels:\n            label_categories.add(label)\n        return {\n            AnnotationType.label: label_categories\n        }\n\n    @classmethod\n    def _parse_labelmap(cls, text):\n        id_pattern = r\'(?:id\\s*:\\s*(?P<id>\\d+))\'\n        name_pattern = r\'(?:name\\s*:\\s*[\\\'\\""](?P<name>.*?)[\\\'\\""])\'\n        entry_pattern = r\'(\\{(?:[\\s\\n]*(?:%(id)s|%(name)s)[\\s\\n]*){2}\\})+\' % \\\n            {\'id\': id_pattern, \'name\': name_pattern}\n        matches = re.finditer(entry_pattern, text)\n\n        labelmap = {}\n        for match in matches:\n            label_id = match.group(\'id\')\n            label_name = match.group(\'name\')\n            if label_id is not None and label_name is not None:\n                labelmap[label_name] = int(label_id)\n\n        return labelmap\n\n    @classmethod\n    def _parse_tfrecord_file(cls, filepath, subset, images_dir):\n        dataset = tf.data.TFRecordDataset(filepath)\n        features = {\n            \'image/filename\': tf.io.FixedLenFeature([], tf.string),\n            \'image/source_id\': tf.io.FixedLenFeature([], tf.string),\n            \'image/height\': tf.io.FixedLenFeature([], tf.int64),\n            \'image/width\': tf.io.FixedLenFeature([], tf.int64),\n            \'image/encoded\': tf.io.FixedLenFeature([], tf.string),\n            \'image/format\': tf.io.FixedLenFeature([], tf.string),\n            # Object boxes and classes.\n            \'image/object/bbox/xmin\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/xmax\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/ymin\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/ymax\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/class/label\': tf.io.VarLenFeature(tf.int64),\n            \'image/object/class/text\': tf.io.VarLenFeature(tf.string),\n            \'image/object/mask\': tf.io.VarLenFeature(tf.string),\n        }\n\n        dataset_labels = OrderedDict()\n        labelmap_path = osp.join(osp.dirname(filepath),\n            DetectionApiPath.LABELMAP_FILE)\n        if osp.exists(labelmap_path):\n            with open(labelmap_path, \'r\', encoding=\'utf-8\') as f:\n                labelmap_text = f.read()\n            dataset_labels.update({ label: id - 1\n                for label, id in cls._parse_labelmap(labelmap_text).items()\n            })\n\n        dataset_items = []\n\n        for record in dataset:\n            parsed_record = tf.io.parse_single_example(record, features)\n            frame_id = parsed_record[\'image/source_id\'].numpy().decode(\'utf-8\')\n            frame_filename = \\\n                parsed_record[\'image/filename\'].numpy().decode(\'utf-8\')\n            frame_height = tf.cast(\n                parsed_record[\'image/height\'], tf.int64).numpy().item()\n            frame_width = tf.cast(\n                parsed_record[\'image/width\'], tf.int64).numpy().item()\n            frame_image = parsed_record[\'image/encoded\'].numpy()\n            frame_format = parsed_record[\'image/format\'].numpy().decode(\'utf-8\')\n            xmins = tf.sparse.to_dense(\n                parsed_record[\'image/object/bbox/xmin\']).numpy()\n            ymins = tf.sparse.to_dense(\n                parsed_record[\'image/object/bbox/ymin\']).numpy()\n            xmaxs = tf.sparse.to_dense(\n                parsed_record[\'image/object/bbox/xmax\']).numpy()\n            ymaxs = tf.sparse.to_dense(\n                parsed_record[\'image/object/bbox/ymax\']).numpy()\n            label_ids = tf.sparse.to_dense(\n                parsed_record[\'image/object/class/label\']).numpy()\n            labels = tf.sparse.to_dense(\n                parsed_record[\'image/object/class/text\'],\n                default_value=b\'\').numpy()\n            masks = tf.sparse.to_dense(\n                parsed_record[\'image/object/mask\'],\n                default_value=b\'\').numpy()\n\n            for label, label_id in zip(labels, label_ids):\n                label = label.decode(\'utf-8\')\n                if not label:\n                    continue\n                if label_id <= 0:\n                    continue\n                if label in dataset_labels:\n                    continue\n                dataset_labels[label] = label_id - 1\n\n            item_id = frame_id\n            if not item_id:\n                item_id = osp.splitext(frame_filename)[0]\n\n            annotations = []\n            for shape_id, shape in enumerate(\n                    np.dstack((labels, xmins, ymins, xmaxs, ymaxs))[0]):\n                label = shape[0].decode(\'utf-8\')\n\n                mask = None\n                if len(masks) != 0:\n                    mask = masks[shape_id]\n\n                if mask is not None:\n                    if isinstance(mask, bytes):\n                        mask = lazy_image(mask, decode_image)\n                    annotations.append(Mask(image=mask,\n                        label=dataset_labels.get(label)\n                    ))\n                else:\n                    x = clamp(shape[1] * frame_width, 0, frame_width)\n                    y = clamp(shape[2] * frame_height, 0, frame_height)\n                    w = clamp(shape[3] * frame_width, 0, frame_width) - x\n                    h = clamp(shape[4] * frame_height, 0, frame_height) - y\n                    annotations.append(Bbox(x, y, w, h,\n                        label=dataset_labels.get(label)\n                    ))\n\n            image_size = None\n            if frame_height and frame_width:\n                image_size = (frame_height, frame_width)\n\n            image_params = {}\n            if frame_image and frame_format:\n                image_params[\'data\'] = lazy_image(frame_image, decode_image)\n            if frame_filename:\n                image_params[\'path\'] = osp.join(images_dir, frame_filename)\n\n            image = None\n            if image_params:\n                image = Image(**image_params, size=image_size)\n\n            dataset_items.append(DatasetItem(id=item_id, subset=subset,\n                image=image, annotations=annotations))\n\n        return dataset_items, dataset_labels\n'"
datumaro/datumaro/plugins/tf_detection_api_format/format.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nclass DetectionApiPath:\n    IMAGES_DIR = 'images'\n    ANNOTATIONS_DIR = 'annotations'\n\n    IMAGE_EXT = '.jpg'\n    IMAGE_FORMAT = 'jpeg'\n\n    LABELMAP_FILE = 'label_map.pbtxt'"""
datumaro/datumaro/plugins/tf_detection_api_format/importer.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom glob import glob\nimport logging as log\nimport os.path as osp\n\nfrom datumaro.components.extractor import Importer\n\n\nclass TfDetectionApiImporter(Importer):\n    EXTRACTOR_NAME = \'tf_detection_api\'\n\n    @classmethod\n    def detect(cls, path):\n        return len(cls.find_subsets(path)) != 0\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        subset_paths = self.find_subsets(path)\n        if len(subset_paths) == 0:\n            raise Exception(\n                ""Failed to find \'tf_detection_api\' dataset at \'%s\'"" % path)\n\n        for subset_path in subset_paths:\n            if not osp.isfile(subset_path):\n                continue\n\n            log.info(""Found a dataset at \'%s\'"" % subset_path)\n\n            subset_name = osp.splitext(osp.basename(subset_path))[0]\n\n            project.add_source(subset_name, {\n                \'url\': subset_path,\n                \'format\': self.EXTRACTOR_NAME,\n                \'options\': dict(extra_params),\n            })\n\n        return project\n\n    @staticmethod\n    def find_subsets(path):\n        if path.endswith(\'.tfrecord\') and osp.isfile(path):\n            subset_paths = [path]\n        else:\n            subset_paths = glob(osp.join(path, \'**\', \'*.tfrecord\'),\n                recursive=True)\n        return subset_paths'"
datumaro/datumaro/plugins/voc_format/__init__.py,0,b''
datumaro/datumaro/plugins/voc_format/converter.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict, defaultdict\nfrom enum import Enum\nfrom itertools import chain\nimport logging as log\nfrom lxml import etree as ET\nimport os\nimport os.path as osp\n\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.extractor import (DEFAULT_SUBSET_NAME, AnnotationType,\n    LabelCategories, CompiledMask,\n)\nfrom datumaro.util.image import save_image\nfrom datumaro.util.mask_tools import paint_mask, remap_mask\n\nfrom .format import (VocTask, VocPath,\n    VocInstColormap, VocPose,\n    parse_label_map, make_voc_label_map, make_voc_categories, write_label_map\n)\n\n\ndef _convert_attr(name, attributes, type_conv, default=None, warn=True):\n    d = object()\n    value = attributes.get(name, d)\n    if value is d:\n        return default\n\n    try:\n        return type_conv(value)\n    except Exception as e:\n        log.warning(""Failed to convert attribute \'%s\'=\'%s\': %s"" % \\\n            (name, value, e))\n        return default\n\ndef _write_xml_bbox(bbox, parent_elem):\n    x, y, w, h = bbox\n    bbox_elem = ET.SubElement(parent_elem, \'bndbox\')\n    ET.SubElement(bbox_elem, \'xmin\').text = str(x)\n    ET.SubElement(bbox_elem, \'ymin\').text = str(y)\n    ET.SubElement(bbox_elem, \'xmax\').text = str(x + w)\n    ET.SubElement(bbox_elem, \'ymax\').text = str(y + h)\n    return bbox_elem\n\n\nLabelmapType = Enum(\'LabelmapType\', [\'voc\', \'source\', \'guess\'])\n\nclass _Converter:\n    def __init__(self, extractor, save_dir,\n            tasks=None, apply_colormap=True, save_images=False, label_map=None):\n        assert tasks is None or isinstance(tasks, (VocTask, list, set))\n        if tasks is None:\n            tasks = set(VocTask)\n        elif isinstance(tasks, VocTask):\n            tasks = {tasks}\n        else:\n            tasks = set(t if t in VocTask else VocTask[t] for t in tasks)\n        self._tasks = tasks\n\n        self._extractor = extractor\n        self._save_dir = save_dir\n        self._apply_colormap = apply_colormap\n        self._save_images = save_images\n\n        self._load_categories(label_map)\n\n    def convert(self):\n        self.init_dirs()\n        self.save_subsets()\n        self.save_label_map()\n\n    def init_dirs(self):\n        save_dir = self._save_dir\n        subsets_dir = osp.join(save_dir, VocPath.SUBSETS_DIR)\n        cls_subsets_dir = osp.join(subsets_dir,\n            VocPath.TASK_DIR[VocTask.classification])\n        action_subsets_dir = osp.join(subsets_dir,\n            VocPath.TASK_DIR[VocTask.action_classification])\n        layout_subsets_dir = osp.join(subsets_dir,\n            VocPath.TASK_DIR[VocTask.person_layout])\n        segm_subsets_dir = osp.join(subsets_dir,\n            VocPath.TASK_DIR[VocTask.segmentation])\n        ann_dir = osp.join(save_dir, VocPath.ANNOTATIONS_DIR)\n        img_dir = osp.join(save_dir, VocPath.IMAGES_DIR)\n        segm_dir = osp.join(save_dir, VocPath.SEGMENTATION_DIR)\n        inst_dir = osp.join(save_dir, VocPath.INSTANCES_DIR)\n        images_dir = osp.join(save_dir, VocPath.IMAGES_DIR)\n\n        os.makedirs(subsets_dir, exist_ok=True)\n        os.makedirs(ann_dir, exist_ok=True)\n        os.makedirs(img_dir, exist_ok=True)\n        os.makedirs(segm_dir, exist_ok=True)\n        os.makedirs(inst_dir, exist_ok=True)\n        os.makedirs(images_dir, exist_ok=True)\n\n        self._subsets_dir = subsets_dir\n        self._cls_subsets_dir = cls_subsets_dir\n        self._action_subsets_dir = action_subsets_dir\n        self._layout_subsets_dir = layout_subsets_dir\n        self._segm_subsets_dir = segm_subsets_dir\n        self._ann_dir = ann_dir\n        self._img_dir = img_dir\n        self._segm_dir = segm_dir\n        self._inst_dir = inst_dir\n        self._images_dir = images_dir\n\n    def get_label(self, label_id):\n        return self._extractor. \\\n            categories()[AnnotationType.label].items[label_id].name\n\n    def save_subsets(self):\n        subsets = self._extractor.subsets()\n        if len(subsets) == 0:\n            subsets = [ None ]\n\n        for subset_name in subsets:\n            if subset_name:\n                subset = self._extractor.get_subset(subset_name)\n            else:\n                subset_name = DEFAULT_SUBSET_NAME\n                subset = self._extractor\n\n            class_lists = OrderedDict()\n            clsdet_list = OrderedDict()\n            action_list = OrderedDict()\n            layout_list = OrderedDict()\n            segm_list = OrderedDict()\n\n            for item in subset:\n                log.debug(""Converting item \'%s\'"", item.id)\n\n                image_filename = \'\'\n                if item.has_image:\n                    image_filename = item.image.filename\n                if self._save_images:\n                    if item.has_image and item.image.has_data:\n                        if image_filename:\n                            image_filename = osp.splitext(image_filename)[0]\n                        else:\n                            image_filename = item.id\n                        image_filename += VocPath.IMAGE_EXT\n                        save_image(osp.join(self._images_dir, image_filename),\n                            item.image.data, create_dir=True)\n                    else:\n                        log.debug(""Item \'%s\' has no image"" % item.id)\n\n                labels = []\n                bboxes = []\n                masks = []\n                for a in item.annotations:\n                    if a.type == AnnotationType.label:\n                        labels.append(a)\n                    elif a.type == AnnotationType.bbox:\n                        bboxes.append(a)\n                    elif a.type == AnnotationType.mask:\n                        masks.append(a)\n\n                if len(bboxes) != 0:\n                    root_elem = ET.Element(\'annotation\')\n                    if \'_\' in item.id:\n                        folder = item.id[ : item.id.find(\'_\')]\n                    else:\n                        folder = \'\'\n                    ET.SubElement(root_elem, \'folder\').text = folder\n                    ET.SubElement(root_elem, \'filename\').text = image_filename\n\n                    source_elem = ET.SubElement(root_elem, \'source\')\n                    ET.SubElement(source_elem, \'database\').text = \'Unknown\'\n                    ET.SubElement(source_elem, \'annotation\').text = \'Unknown\'\n                    ET.SubElement(source_elem, \'image\').text = \'Unknown\'\n\n                    if item.has_image:\n                        h, w = item.image.size\n                        if item.image.has_data:\n                            image_shape = item.image.data.shape\n                            c = 1 if len(image_shape) == 2 else image_shape[2]\n                        else:\n                            c = 3\n                        size_elem = ET.SubElement(root_elem, \'size\')\n                        ET.SubElement(size_elem, \'width\').text = str(w)\n                        ET.SubElement(size_elem, \'height\').text = str(h)\n                        ET.SubElement(size_elem, \'depth\').text = str(c)\n\n                    item_segmented = 0 < len(masks)\n                    ET.SubElement(root_elem, \'segmented\').text = \\\n                        str(int(item_segmented))\n\n                    objects_with_parts = []\n                    objects_with_actions = defaultdict(dict)\n\n                    main_bboxes = []\n                    layout_bboxes = []\n                    for bbox in bboxes:\n                        label = self.get_label(bbox.label)\n                        if self._is_part(label):\n                            layout_bboxes.append(bbox)\n                        elif self._is_label(label):\n                            main_bboxes.append(bbox)\n\n                    for new_obj_id, obj in enumerate(main_bboxes):\n                        attr = obj.attributes\n\n                        obj_elem = ET.SubElement(root_elem, \'object\')\n\n                        obj_label = self.get_label(obj.label)\n                        ET.SubElement(obj_elem, \'name\').text = obj_label\n\n                        if \'pose\' in attr:\n                            pose = _convert_attr(\'pose\', attr,\n                                lambda v: VocPose[v], VocPose.Unspecified)\n                            ET.SubElement(obj_elem, \'pose\').text = pose.name\n\n                        if \'truncated\' in attr:\n                            truncated = _convert_attr(\'truncated\', attr, int, 0)\n                            ET.SubElement(obj_elem, \'truncated\').text = \\\n                                \'%d\' % truncated\n\n                        if \'difficult\' in attr:\n                            difficult = _convert_attr(\'difficult\', attr, int, 0)\n                            ET.SubElement(obj_elem, \'difficult\').text = \\\n                                \'%d\' % difficult\n\n                        if \'occluded\' in attr:\n                            occluded = _convert_attr(\'occluded\', attr, int, 0)\n                            ET.SubElement(obj_elem, \'occluded\').text = \\\n                                \'%d\' % occluded\n\n                        bbox = obj.get_bbox()\n                        if bbox is not None:\n                            _write_xml_bbox(bbox, obj_elem)\n\n                        for part_bbox in filter(\n                                lambda x: obj.group and obj.group == x.group,\n                                layout_bboxes):\n                            part_elem = ET.SubElement(obj_elem, \'part\')\n                            ET.SubElement(part_elem, \'name\').text = \\\n                                self.get_label(part_bbox.label)\n                            _write_xml_bbox(part_bbox.get_bbox(), part_elem)\n\n                            objects_with_parts.append(new_obj_id)\n\n                        label_actions = self._get_actions(obj_label)\n                        actions_elem = ET.Element(\'actions\')\n                        for action in label_actions:\n                            present = 0\n                            if action in attr:\n                                present = _convert_attr(action, attr,\n                                    lambda v: int(v == True), 0)\n                                ET.SubElement(actions_elem, action).text = \\\n                                    \'%d\' % present\n\n                            objects_with_actions[new_obj_id][action] = present\n                        if len(actions_elem) != 0:\n                            obj_elem.append(actions_elem)\n\n                    if self._tasks & {None,\n                            VocTask.detection,\n                            VocTask.person_layout,\n                            VocTask.action_classification}:\n                        ann_path = osp.join(self._ann_dir, item.id + \'.xml\')\n                        os.makedirs(osp.dirname(ann_path), exist_ok=True)\n                        with open(ann_path, \'w\') as f:\n                            f.write(ET.tostring(root_elem,\n                                encoding=\'unicode\', pretty_print=True))\n\n                    clsdet_list[item.id] = True\n                    layout_list[item.id] = objects_with_parts\n                    action_list[item.id] = objects_with_actions\n\n                for label_ann in labels:\n                    label = self.get_label(label_ann.label)\n                    if not self._is_label(label):\n                        continue\n                    class_list = class_lists.get(item.id, set())\n                    class_list.add(label_ann.label)\n                    class_lists[item.id] = class_list\n\n                    clsdet_list[item.id] = True\n\n                if masks:\n                    compiled_mask = CompiledMask.from_instance_masks(masks,\n                        instance_labels=[self._label_id_mapping(m.label)\n                            for m in masks])\n\n                    self.save_segm(\n                        osp.join(self._segm_dir, item.id + VocPath.SEGM_EXT),\n                        compiled_mask.class_mask)\n                    self.save_segm(\n                        osp.join(self._inst_dir, item.id + VocPath.SEGM_EXT),\n                        compiled_mask.instance_mask,\n                        colormap=VocInstColormap)\n\n                    segm_list[item.id] = True\n\n                if len(item.annotations) == 0:\n                    clsdet_list[item.id] = None\n                    layout_list[item.id] = None\n                    action_list[item.id] = None\n                    segm_list[item.id] = None\n\n                if self._tasks & {None,\n                        VocTask.classification,\n                        VocTask.detection,\n                        VocTask.action_classification,\n                        VocTask.person_layout}:\n                    self.save_clsdet_lists(subset_name, clsdet_list)\n                    if self._tasks & {None, VocTask.classification}:\n                        self.save_class_lists(subset_name, class_lists)\n                if self._tasks & {None, VocTask.action_classification}:\n                    self.save_action_lists(subset_name, action_list)\n                if self._tasks & {None, VocTask.person_layout}:\n                    self.save_layout_lists(subset_name, layout_list)\n                if self._tasks & {None, VocTask.segmentation}:\n                    self.save_segm_lists(subset_name, segm_list)\n\n    def save_action_lists(self, subset_name, action_list):\n        if not action_list:\n            return\n\n        os.makedirs(self._action_subsets_dir, exist_ok=True)\n\n        ann_file = osp.join(self._action_subsets_dir, subset_name + \'.txt\')\n        with open(ann_file, \'w\') as f:\n            for item in action_list:\n                f.write(\'%s\\n\' % item)\n\n        if len(action_list) == 0:\n            return\n\n        all_actions = set(chain(*(self._get_actions(l)\n            for l in self._label_map)))\n        for action in all_actions:\n            ann_file = osp.join(self._action_subsets_dir,\n                \'%s_%s.txt\' % (action, subset_name))\n            with open(ann_file, \'w\') as f:\n                for item, objs in action_list.items():\n                    if not objs:\n                        continue\n                    for obj_id, obj_actions in objs.items():\n                        presented = obj_actions[action]\n                        f.write(\'%s %s % d\\n\' % \\\n                            (item, 1 + obj_id, 1 if presented else -1))\n\n    def save_class_lists(self, subset_name, class_lists):\n        if not class_lists:\n            return\n\n        os.makedirs(self._cls_subsets_dir, exist_ok=True)\n\n        for label in self._label_map:\n            ann_file = osp.join(self._cls_subsets_dir,\n                \'%s_%s.txt\' % (label, subset_name))\n            with open(ann_file, \'w\') as f:\n                for item, item_labels in class_lists.items():\n                    if not item_labels:\n                        continue\n                    item_labels = [self.get_label(l) for l in item_labels]\n                    presented = label in item_labels\n                    f.write(\'%s % d\\n\' % (item, 1 if presented else -1))\n\n    def save_clsdet_lists(self, subset_name, clsdet_list):\n        if not clsdet_list:\n            return\n\n        os.makedirs(self._cls_subsets_dir, exist_ok=True)\n\n        ann_file = osp.join(self._cls_subsets_dir, subset_name + \'.txt\')\n        with open(ann_file, \'w\') as f:\n            for item in clsdet_list:\n                f.write(\'%s\\n\' % item)\n\n    def save_segm_lists(self, subset_name, segm_list):\n        if not segm_list:\n            return\n\n        os.makedirs(self._segm_subsets_dir, exist_ok=True)\n\n        ann_file = osp.join(self._segm_subsets_dir, subset_name + \'.txt\')\n        with open(ann_file, \'w\') as f:\n            for item in segm_list:\n                f.write(\'%s\\n\' % item)\n\n    def save_layout_lists(self, subset_name, layout_list):\n        if not layout_list:\n            return\n\n        os.makedirs(self._layout_subsets_dir, exist_ok=True)\n\n        ann_file = osp.join(self._layout_subsets_dir, subset_name + \'.txt\')\n        with open(ann_file, \'w\') as f:\n            for item, item_layouts in layout_list.items():\n                if item_layouts:\n                    for obj_id in item_layouts:\n                        f.write(\'%s % d\\n\' % (item, 1 + obj_id))\n                else:\n                    f.write(\'%s\\n\' % (item))\n\n    def save_segm(self, path, mask, colormap=None):\n        if self._apply_colormap:\n            if colormap is None:\n                colormap = self._categories[AnnotationType.mask].colormap\n            mask = paint_mask(mask, colormap)\n        save_image(path, mask, create_dir=True)\n\n    def save_label_map(self):\n        path = osp.join(self._save_dir, VocPath.LABELMAP_FILE)\n        write_label_map(path, self._label_map)\n\n    def _load_categories(self, label_map_source=None):\n        if label_map_source == LabelmapType.voc.name:\n            # use the default VOC colormap\n            label_map = make_voc_label_map()\n\n        elif label_map_source == LabelmapType.source.name and \\\n                AnnotationType.mask not in self._extractor.categories():\n            # generate colormap for input labels\n            labels = self._extractor.categories() \\\n                .get(AnnotationType.label, LabelCategories())\n            label_map = OrderedDict()\n            label_map[\'background\'] = [None, [], []]\n            for item in labels.items:\n                label_map[item.name] = [None, [], []]\n\n        elif label_map_source == LabelmapType.source.name and \\\n                AnnotationType.mask in self._extractor.categories():\n            # use source colormap\n            labels = self._extractor.categories()[AnnotationType.label]\n            colors = self._extractor.categories()[AnnotationType.mask]\n            label_map = OrderedDict()\n            has_black = False\n            for idx, item in enumerate(labels.items):\n                color = colors.colormap.get(idx)\n                if idx is not None:\n                    if color == (0, 0, 0):\n                        has_black = True\n                    label_map[item.name] = [color, [], []]\n            if not has_black and \'background\' not in label_map:\n                label_map[\'background\'] = [(0, 0, 0), [], []]\n                label_map.move_to_end(\'background\', last=False)\n\n        elif label_map_source in [LabelmapType.guess.name, None]:\n            # generate colormap for union of VOC and input dataset labels\n            label_map = make_voc_label_map()\n\n            rebuild_colormap = False\n            source_labels = self._extractor.categories() \\\n                .get(AnnotationType.label, LabelCategories())\n            for label in source_labels.items:\n                if label.name not in label_map:\n                    rebuild_colormap = True\n                if label.attributes or label.name not in label_map:\n                    label_map[label.name] = [None, [], label.attributes]\n\n            if rebuild_colormap:\n                for item in label_map.values():\n                    item[0] = None\n\n        elif isinstance(label_map_source, dict):\n            label_map = label_map_source\n\n        elif isinstance(label_map_source, str) and osp.isfile(label_map_source):\n            label_map = parse_label_map(label_map_source)\n\n        else:\n            raise Exception(""Wrong labelmap specified, ""\n                ""expected one of %s or a file path"" % \\\n                \', \'.join(t.name for t in LabelmapType))\n\n        self._categories = make_voc_categories(label_map)\n\n        self._label_map = label_map\n        colormap = self._categories[AnnotationType.mask].colormap\n        for label_id, color in colormap.items():\n            label_desc = label_map[\n                self._categories[AnnotationType.label].items[label_id].name]\n            label_desc[0] = color\n\n        self._label_id_mapping = self._make_label_id_map()\n\n    def _is_label(self, s):\n        return self._label_map.get(s) is not None\n\n    def _is_part(self, s):\n        for label_desc in self._label_map.values():\n            if s in label_desc[1]:\n                return True\n        return False\n\n    def _is_action(self, label, s):\n        return s in self._get_actions(label)\n\n    def _get_actions(self, label):\n        label_desc = self._label_map.get(label)\n        if not label_desc:\n            return []\n        return label_desc[2]\n\n    def _make_label_id_map(self):\n        source_labels = {\n            id: label.name for id, label in\n            enumerate(self._extractor.categories().get(\n                AnnotationType.label, LabelCategories()).items)\n        }\n        target_labels = {\n            label.name: id for id, label in\n            enumerate(self._categories[AnnotationType.label].items)\n        }\n        id_mapping = {\n            src_id: target_labels.get(src_label, 0)\n            for src_id, src_label in source_labels.items()\n        }\n\n        void_labels = [src_label for src_id, src_label in source_labels.items()\n            if src_label not in target_labels]\n        if void_labels:\n            log.warning(""The following labels are remapped to background: %s"" %\n                \', \'.join(void_labels))\n        log.debug(""Saving segmentations with the following label mapping: \\n%s"" %\n            \'\\n\'.join([""#%s \'%s\' -> #%s \'%s\'"" %\n                (\n                    src_id, src_label, id_mapping[src_id],\n                    self._categories[AnnotationType.label] \\\n                        .items[id_mapping[src_id]].name\n                )\n                for src_id, src_label in source_labels.items()\n            ])\n        )\n\n        def map_id(src_id):\n            return id_mapping.get(src_id, 0)\n        return map_id\n\n    def _remap_mask(self, mask):\n        return remap_mask(mask, self._label_id_mapping)\n\nclass VocConverter(Converter, CliPlugin):\n    @staticmethod\n    def _split_tasks_string(s):\n        return [VocTask[i.strip()] for i in s.split(\',\')]\n\n    @staticmethod\n    def _get_labelmap(s):\n        if osp.isfile(s):\n            return s\n        try:\n            return LabelmapType[s].name\n        except KeyError:\n            import argparse\n            raise argparse.ArgumentTypeError()\n\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        parser.add_argument(\'--apply-colormap\', type=bool, default=True,\n            help=""Use colormap for class and instance masks ""\n                ""(default: %(default)s)"")\n        parser.add_argument(\'--label-map\', type=cls._get_labelmap, default=None,\n            help=""Labelmap file path or one of %s"" % \\\n                \', \'.join(t.name for t in LabelmapType))\n        parser.add_argument(\'--tasks\', type=cls._split_tasks_string,\n            default=None,\n            help=""VOC task filter, comma-separated list of {%s} ""\n                ""(default: all)"" % \', \'.join([t.name for t in VocTask]))\n\n        return parser\n\n    def __init__(self, tasks=None, save_images=False,\n            apply_colormap=False, label_map=None):\n        super().__init__()\n\n        self._options = {\n            \'tasks\': tasks,\n            \'save_images\': save_images,\n            \'apply_colormap\': apply_colormap,\n            \'label_map\': label_map,\n        }\n\n    def __call__(self, extractor, save_dir):\n        converter = _Converter(extractor, save_dir, **self._options)\n        converter.convert()\n\nclass VocClassificationConverter(VocConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = VocTask.classification\n        super().__init__(**kwargs)\n\nclass VocDetectionConverter(VocConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = VocTask.detection\n        super().__init__(**kwargs)\n\nclass VocLayoutConverter(VocConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = VocTask.person_layout\n        super().__init__(**kwargs)\n\nclass VocActionConverter(VocConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = VocTask.action_classification\n        super().__init__(**kwargs)\n\nclass VocSegmentationConverter(VocConverter):\n    def __init__(self, **kwargs):\n        kwargs[\'tasks\'] = VocTask.segmentation\n        super().__init__(**kwargs)\n'"
datumaro/datumaro/plugins/voc_format/extractor.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import defaultdict\nimport logging as log\nimport numpy as np\nimport os.path as osp\nfrom defusedxml import ElementTree\n\nfrom datumaro.components.extractor import (SourceExtractor, DatasetItem,\n    AnnotationType, Label, Mask, Bbox, CompiledMask\n)\nfrom datumaro.util import dir_items\nfrom datumaro.util.image import Image\nfrom datumaro.util.mask_tools import lazy_mask, invert_colormap\n\nfrom .format import (\n    VocTask, VocPath, VocInstColormap, parse_label_map, make_voc_categories\n)\n\n\n_inverse_inst_colormap = invert_colormap(VocInstColormap)\n\nclass _VocExtractor(SourceExtractor):\n    def __init__(self, path):\n        assert osp.isfile(path), path\n        self._path = path\n        self._dataset_dir = osp.dirname(osp.dirname(osp.dirname(path)))\n\n        super().__init__(subset=osp.splitext(osp.basename(path))[0])\n\n        self._categories = self._load_categories(self._dataset_dir)\n\n        label_color = lambda label_idx: \\\n            self._categories[AnnotationType.mask].colormap.get(label_idx, None)\n        log.debug(""Loaded labels: %s"" % \', \'.join(\n            ""\'%s\' %s"" % (l.name, (\'(%s, %s, %s)\' % c) if c else \'\')\n            for i, l, c in ((i, l, label_color(i)) for i, l in enumerate(\n                self._categories[AnnotationType.label].items\n            ))\n        ))\n        self._items = self._load_subset_list(path)\n\n    def categories(self):\n        return self._categories\n\n    def __len__(self):\n        return len(self._items)\n\n    def _get_label_id(self, label):\n        label_id, _ = self._categories[AnnotationType.label].find(label)\n        assert label_id is not None, label\n        return label_id\n\n    @staticmethod\n    def _load_categories(dataset_path):\n        label_map = None\n        label_map_path = osp.join(dataset_path, VocPath.LABELMAP_FILE)\n        if osp.isfile(label_map_path):\n            label_map = parse_label_map(label_map_path)\n        return make_voc_categories(label_map)\n\n    @staticmethod\n    def _load_subset_list(subset_path):\n        with open(subset_path) as f:\n            return [line.split()[0] for line in f]\n\nclass VocClassificationExtractor(_VocExtractor):\n    def __iter__(self):\n        raw_anns = self._load_annotations()\n        for item_id in self._items:\n            log.debug(""Reading item \'%s\'"" % item_id)\n            image = osp.join(self._dataset_dir, VocPath.IMAGES_DIR,\n                item_id + VocPath.IMAGE_EXT)\n            anns = self._parse_annotations(raw_anns, item_id)\n            yield DatasetItem(id=item_id, subset=self._subset,\n                image=image, annotations=anns)\n\n    def _load_annotations(self):\n        annotations = defaultdict(list)\n        task_dir = osp.dirname(self._path)\n        anno_files = [s for s in dir_items(task_dir, \'.txt\')\n            if s.endswith(\'_\' + osp.basename(self._path))]\n        for ann_filename in anno_files:\n            with open(osp.join(task_dir, ann_filename)) as f:\n                label = ann_filename[:ann_filename.rfind(\'_\')]\n                label_id = self._get_label_id(label)\n                for line in f:\n                    item, present = line.split()\n                    if present == \'1\':\n                        annotations[item].append(label_id)\n\n        return dict(annotations)\n\n    @staticmethod\n    def _parse_annotations(raw_anns, item_id):\n        return [Label(label_id) for label_id in raw_anns.get(item_id, [])]\n\nclass _VocXmlExtractor(_VocExtractor):\n    def __init__(self, path, task):\n        super().__init__(path)\n        self._task = task\n\n    def __iter__(self):\n        anno_dir = osp.join(self._dataset_dir, VocPath.ANNOTATIONS_DIR)\n\n        for item_id in self._items:\n            log.debug(""Reading item \'%s\'"" % item_id)\n            image = osp.join(self._dataset_dir, VocPath.IMAGES_DIR,\n                item_id + VocPath.IMAGE_EXT)\n\n            anns = []\n            ann_file = osp.join(anno_dir, item_id + \'.xml\')\n            if osp.isfile(ann_file):\n                root_elem = ElementTree.parse(ann_file)\n                height = root_elem.find(\'size/height\')\n                if height is not None:\n                    height = int(height.text)\n                width = root_elem.find(\'size/width\')\n                if width is not None:\n                    width = int(width.text)\n                if height and width:\n                    image = Image(path=image, size=(height, width))\n\n                anns = self._parse_annotations(root_elem)\n\n            yield DatasetItem(id=item_id, subset=self._subset,\n                image=image, annotations=anns)\n\n    def _parse_annotations(self, root_elem):\n        item_annotations = []\n\n        for obj_id, object_elem in enumerate(root_elem.findall(\'object\')):\n            obj_id += 1\n            attributes = {}\n            group = obj_id\n\n            obj_label_id = None\n            label_elem = object_elem.find(\'name\')\n            if label_elem is not None:\n                obj_label_id = self._get_label_id(label_elem.text)\n\n            obj_bbox = self._parse_bbox(object_elem)\n\n            if obj_label_id is None or obj_bbox is None:\n                continue\n\n            difficult_elem = object_elem.find(\'difficult\')\n            attributes[\'difficult\'] = difficult_elem is not None and \\\n                difficult_elem.text == \'1\'\n\n            truncated_elem = object_elem.find(\'truncated\')\n            attributes[\'truncated\'] = truncated_elem is not None and \\\n                truncated_elem.text == \'1\'\n\n            occluded_elem = object_elem.find(\'occluded\')\n            attributes[\'occluded\'] = occluded_elem is not None and \\\n                occluded_elem.text == \'1\'\n\n            pose_elem = object_elem.find(\'pose\')\n            if pose_elem is not None:\n                attributes[\'pose\'] = pose_elem.text\n\n            point_elem = object_elem.find(\'point\')\n            if point_elem is not None:\n                point_x = point_elem.find(\'x\')\n                point_y = point_elem.find(\'y\')\n                point = [float(point_x.text), float(point_y.text)]\n                attributes[\'point\'] = point\n\n            actions_elem = object_elem.find(\'actions\')\n            actions = {a: False\n                for a in self._categories[AnnotationType.label] \\\n                    .items[obj_label_id].attributes}\n            if actions_elem is not None:\n                for action_elem in actions_elem:\n                    actions[action_elem.tag] = (action_elem.text == \'1\')\n            for action, present in actions.items():\n                attributes[action] = present\n\n            has_parts = False\n            for part_elem in object_elem.findall(\'part\'):\n                part = part_elem.find(\'name\').text\n                part_label_id = self._get_label_id(part)\n                part_bbox = self._parse_bbox(part_elem)\n\n                if self._task is not VocTask.person_layout:\n                    break\n                if part_bbox is None:\n                    continue\n                has_parts = True\n                item_annotations.append(Bbox(*part_bbox, label=part_label_id,\n                    group=group))\n\n            if self._task is VocTask.person_layout and not has_parts:\n                continue\n            if self._task is VocTask.action_classification and not actions:\n                continue\n\n            item_annotations.append(Bbox(*obj_bbox, label=obj_label_id,\n                attributes=attributes, id=obj_id, group=group))\n\n        return item_annotations\n\n    @staticmethod\n    def _parse_bbox(object_elem):\n        bbox_elem = object_elem.find(\'bndbox\')\n        xmin = float(bbox_elem.find(\'xmin\').text)\n        xmax = float(bbox_elem.find(\'xmax\').text)\n        ymin = float(bbox_elem.find(\'ymin\').text)\n        ymax = float(bbox_elem.find(\'ymax\').text)\n        return [xmin, ymin, xmax - xmin, ymax - ymin]\n\nclass VocDetectionExtractor(_VocXmlExtractor):\n    def __init__(self, path):\n        super().__init__(path, task=VocTask.detection)\n\nclass VocLayoutExtractor(_VocXmlExtractor):\n    def __init__(self, path):\n        super().__init__(path, task=VocTask.person_layout)\n\nclass VocActionExtractor(_VocXmlExtractor):\n    def __init__(self, path):\n        super().__init__(path, task=VocTask.action_classification)\n\nclass VocSegmentationExtractor(_VocExtractor):\n    def __iter__(self):\n        for item_id in self._items:\n            log.debug(""Reading item \'%s\'"" % item_id)\n            image = osp.join(self._dataset_dir, VocPath.IMAGES_DIR,\n                item_id + VocPath.IMAGE_EXT)\n            anns = self._load_annotations(item_id)\n            yield DatasetItem(id=item_id, subset=self._subset,\n                image=image, annotations=anns)\n\n    @staticmethod\n    def _lazy_extract_mask(mask, c):\n        return lambda: mask == c\n\n    def _load_annotations(self, item_id):\n        item_annotations = []\n\n        class_mask = None\n        segm_path = osp.join(self._dataset_dir, VocPath.SEGMENTATION_DIR,\n            item_id + VocPath.SEGM_EXT)\n        if osp.isfile(segm_path):\n            inverse_cls_colormap = \\\n                self._categories[AnnotationType.mask].inverse_colormap\n            class_mask = lazy_mask(segm_path, inverse_cls_colormap)\n\n        instances_mask = None\n        inst_path = osp.join(self._dataset_dir, VocPath.INSTANCES_DIR,\n            item_id + VocPath.SEGM_EXT)\n        if osp.isfile(inst_path):\n            instances_mask = lazy_mask(inst_path, _inverse_inst_colormap)\n\n        if instances_mask is not None:\n            compiled_mask = CompiledMask(class_mask, instances_mask)\n\n            if class_mask is not None:\n                label_cat = self._categories[AnnotationType.label]\n                instance_labels = compiled_mask.get_instance_labels()\n            else:\n                instance_labels = {i: None\n                    for i in range(compiled_mask.instance_count)}\n\n            for instance_id, label_id in instance_labels.items():\n                image = compiled_mask.lazy_extract(instance_id)\n\n                attributes = {}\n                if label_id is not None:\n                    actions = {a: False\n                        for a in label_cat.items[label_id].attributes\n                    }\n                    attributes.update(actions)\n\n                item_annotations.append(Mask(\n                    image=image, label=label_id,\n                    attributes=attributes, group=instance_id\n                ))\n        elif class_mask is not None:\n            log.warn(""item \'%s\': has only class segmentation, ""\n                ""instance masks will not be available"" % item_id)\n            class_mask = class_mask()\n            classes = np.unique(class_mask)\n            for label_id in classes:\n                image = self._lazy_extract_mask(class_mask, label_id)\n                item_annotations.append(Mask(image=image, label=label_id))\n\n        return item_annotations\n'"
datumaro/datumaro/plugins/voc_format/format.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nfrom enum import Enum\nfrom itertools import chain\nimport numpy as np\n\nfrom datumaro.components.extractor import (AnnotationType,\n    LabelCategories, MaskCategories\n)\n\n\nVocTask = Enum(\'VocTask\', [\n    \'classification\',\n    \'detection\',\n    \'segmentation\',\n    \'action_classification\',\n    \'person_layout\',\n])\n\nVocLabel = Enum(\'VocLabel\', [\n    (\'background\', 0),\n    (\'aeroplane\', 1),\n    (\'bicycle\', 2),\n    (\'bird\', 3),\n    (\'boat\', 4),\n    (\'bottle\', 5),\n    (\'bus\', 6),\n    (\'car\', 7),\n    (\'cat\', 8),\n    (\'chair\', 9),\n    (\'cow\', 10),\n    (\'diningtable\', 11),\n    (\'dog\', 12),\n    (\'horse\', 13),\n    (\'motorbike\', 14),\n    (\'person\', 15),\n    (\'pottedplant\', 16),\n    (\'sheep\', 17),\n    (\'sofa\', 18),\n    (\'train\', 19),\n    (\'tvmonitor\', 20),\n    (\'ignored\', 255),\n])\n\nVocPose = Enum(\'VocPose\', [\n    \'Unspecified\',\n    \'Left\',\n    \'Right\',\n    \'Frontal\',\n    \'Rear\',\n])\n\nVocBodyPart = Enum(\'VocBodyPart\', [\n    \'head\',\n    \'hand\',\n    \'foot\',\n])\n\nVocAction = Enum(\'VocAction\', [\n    \'other\',\n    \'jumping\',\n    \'phoning\',\n    \'playinginstrument\',\n    \'reading\',\n    \'ridingbike\',\n    \'ridinghorse\',\n    \'running\',\n    \'takingphoto\',\n    \'usingcomputer\',\n    \'walking\',\n])\n\ndef generate_colormap(length=256):\n    def get_bit(number, index):\n        return (number >> index) & 1\n\n    colormap = np.zeros((length, 3), dtype=int)\n    indices = np.arange(length, dtype=int)\n\n    for j in range(7, -1, -1):\n        for c in range(3):\n            colormap[:, c] |= get_bit(indices, c) << j\n        indices >>= 3\n\n    return OrderedDict(\n        (id, tuple(color)) for id, color in enumerate(colormap)\n    )\n\nVocColormap = {id: color for id, color in generate_colormap(256).items()\n    if id in [l.value for l in VocLabel]}\nVocInstColormap = generate_colormap(256)\n\nclass VocPath:\n    IMAGES_DIR = \'JPEGImages\'\n    ANNOTATIONS_DIR = \'Annotations\'\n    SEGMENTATION_DIR = \'SegmentationClass\'\n    INSTANCES_DIR = \'SegmentationObject\'\n    SUBSETS_DIR = \'ImageSets\'\n    IMAGE_EXT = \'.jpg\'\n    SEGM_EXT = \'.png\'\n    LABELMAP_FILE = \'labelmap.txt\'\n\n    TASK_DIR = {\n        VocTask.classification: \'Main\',\n        VocTask.detection: \'Main\',\n        VocTask.segmentation: \'Segmentation\',\n        VocTask.action_classification: \'Action\',\n        VocTask.person_layout: \'Layout\',\n    }\n\n\ndef make_voc_label_map():\n    labels = sorted(VocLabel, key=lambda l: l.value)\n    label_map = OrderedDict(\n        (label.name, [VocColormap[label.value], [], []]) for label in labels)\n    label_map[VocLabel.person.name][1] = [p.name for p in VocBodyPart]\n    label_map[VocLabel.person.name][2] = [a.name for a in VocAction]\n    return label_map\n\ndef parse_label_map(path):\n    if not path:\n        return None\n\n    label_map = OrderedDict()\n    with open(path, \'r\') as f:\n        for line in f:\n            # skip empty and commented lines\n            line = line.strip()\n            if not line or line and line[0] == \'#\':\n                continue\n\n            # name, color, parts, actions\n            label_desc = line.strip().split(\':\')\n            name = label_desc[0]\n\n            if 1 < len(label_desc) and len(label_desc[1]) != 0:\n                color = label_desc[1].split(\',\')\n                assert len(color) == 3, \\\n                    ""Label \'%s\' has wrong color, expected \'r,g,b\', got \'%s\'"" % \\\n                    (name, color)\n                color = tuple([int(c) for c in color])\n            else:\n                color = None\n\n            if 2 < len(label_desc) and len(label_desc[2]) != 0:\n                parts = label_desc[2].split(\',\')\n            else:\n                parts = []\n\n            if 3 < len(label_desc) and len(label_desc[3]) != 0:\n                actions = label_desc[3].split(\',\')\n            else:\n                actions = []\n\n            label_map[name] = [color, parts, actions]\n    return label_map\n\ndef write_label_map(path, label_map):\n    with open(path, \'w\') as f:\n        f.write(\'# label:color_rgb:parts:actions\\n\')\n        for label_name, label_desc in label_map.items():\n            if label_desc[0]:\n                color_rgb = \',\'.join(str(c) for c in label_desc[0])\n            else:\n                color_rgb = \'\'\n\n            parts = \',\'.join(str(p) for p in label_desc[1])\n            actions = \',\'.join(str(a) for a in label_desc[2])\n\n            f.write(\'%s\\n\' % \':\'.join([label_name, color_rgb, parts, actions]))\n\n# pylint: disable=pointless-statement\ndef make_voc_categories(label_map=None):\n    if label_map is None:\n        label_map = make_voc_label_map()\n\n    categories = {}\n\n    label_categories = LabelCategories()\n    label_categories.attributes.update([\'difficult\', \'truncated\', \'occluded\'])\n\n    for label, desc in label_map.items():\n        label_categories.add(label, attributes=desc[2])\n    for part in OrderedDict((k, None) for k in chain(\n            *(desc[1] for desc in label_map.values()))):\n        label_categories.add(part)\n    categories[AnnotationType.label] = label_categories\n\n    has_colors = sum(v[0] is not None for v in label_map.values())\n    if not has_colors:\n        colormap = generate_colormap(len(label_map))\n    else:\n        label_id = lambda label: label_categories.find(label)[0]\n        colormap = { label_id(name): desc[0]\n            for name, desc in label_map.items() }\n    mask_categories = MaskCategories(colormap)\n    mask_categories.inverse_colormap # force init\n    categories[AnnotationType.mask] = mask_categories\n\n    return categories\n# pylint: enable=pointless-statement'"
datumaro/datumaro/plugins/voc_format/importer.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom glob import glob\nimport os.path as osp\n\nfrom datumaro.components.extractor import Importer\n\nfrom .format import VocTask, VocPath\n\n\nclass VocImporter(Importer):\n    _TASKS = [\n        (VocTask.classification, \'voc_classification\', \'Main\'),\n        (VocTask.detection, \'voc_detection\', \'Main\'),\n        (VocTask.segmentation, \'voc_segmentation\', \'Segmentation\'),\n        (VocTask.person_layout, \'voc_layout\', \'Layout\'),\n        (VocTask.action_classification, \'voc_action\', \'Action\'),\n    ]\n\n    @classmethod\n    def detect(cls, path):\n        return len(cls.find_subsets(path)) != 0\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        subset_paths = self.find_subsets(path)\n        if len(subset_paths) == 0:\n            raise Exception(""Failed to find \'voc\' dataset at \'%s\'"" % path)\n\n        for task, extractor_type, subset_path in subset_paths:\n            project.add_source(\'%s-%s\' %\n                (task.name, osp.splitext(osp.basename(subset_path))[0]),\n            {\n                \'url\': subset_path,\n                \'format\': extractor_type,\n                \'options\': dict(extra_params),\n            })\n\n        return project\n\n    @staticmethod\n    def find_subsets(path):\n        subset_paths = []\n        for task, extractor_type, task_dir in __class__._TASKS:\n            task_dir = osp.join(path, VocPath.SUBSETS_DIR, task_dir)\n            if not osp.isdir(task_dir):\n                continue\n            task_subsets = [p for p in glob(osp.join(task_dir, \'*.txt\'))\n                if \'_\' not in osp.basename(p)]\n            subset_paths += [(task, extractor_type, p) for p in task_subsets]\n        return subset_paths\n'"
datumaro/datumaro/plugins/yolo_format/__init__.py,0,b''
datumaro/datumaro/plugins/yolo_format/converter.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nimport logging as log\nimport os\nimport os.path as osp\n\nfrom datumaro.components.converter import Converter\nfrom datumaro.components.extractor import AnnotationType\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom datumaro.util.image import save_image\n\nfrom .format import YoloPath\n\n\ndef _make_yolo_bbox(img_size, box):\n    # https://github.com/pjreddie/darknet/blob/master/scripts/voc_label.py\n    # <x> <y> <width> <height> - values relative to width and height of image\n    # <x> <y> - are center of rectangle\n    x = (box[0] + box[2]) / 2 / img_size[0]\n    y = (box[1] + box[3]) / 2 / img_size[1]\n    w = (box[2] - box[0]) / img_size[0]\n    h = (box[3] - box[1]) / img_size[1]\n    return x, y, w, h\n\nclass YoloConverter(Converter, CliPlugin):\n    # https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects\n\n    @classmethod\n    def build_cmdline_parser(cls, **kwargs):\n        parser = super().build_cmdline_parser(**kwargs)\n        parser.add_argument(\'--save-images\', action=\'store_true\',\n            help=""Save images (default: %(default)s)"")\n        return parser\n\n    def __init__(self, save_images=False):\n        super().__init__()\n        self._save_images = save_images\n\n    def __call__(self, extractor, save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n\n        label_categories = extractor.categories()[AnnotationType.label]\n        label_ids = {label.name: idx\n            for idx, label in enumerate(label_categories.items)}\n        with open(osp.join(save_dir, \'obj.names\'), \'w\') as f:\n            f.writelines(\'%s\\n\' % l[0]\n                for l in sorted(label_ids.items(), key=lambda x: x[1]))\n\n        subsets = extractor.subsets()\n        if len(subsets) == 0:\n            subsets = [ None ]\n\n        subset_lists = OrderedDict()\n\n        for subset_name in subsets:\n            if subset_name and subset_name in YoloPath.SUBSET_NAMES:\n                subset = extractor.get_subset(subset_name)\n            elif not subset_name:\n                subset_name = YoloPath.DEFAULT_SUBSET_NAME\n                subset = extractor\n            else:\n                log.warn(""Skipping subset export \'%s\'. ""\n                    ""If specified, the only valid names are %s"" % \\\n                    (subset_name, \', \'.join(\n                        ""\'%s\'"" % s for s in YoloPath.SUBSET_NAMES)))\n                continue\n\n            subset_dir = osp.join(save_dir, \'obj_%s_data\' % subset_name)\n            os.makedirs(subset_dir, exist_ok=True)\n\n            image_paths = OrderedDict()\n\n            for item in subset:\n                if not item.has_image:\n                    raise Exception(""Failed to export item \'%s\': ""\n                        ""item has no image info"" % item.id)\n                height, width = item.image.size\n\n                image_name = item.image.filename\n                item_name = osp.splitext(item.image.filename)[0]\n                if self._save_images:\n                    if item.has_image and item.image.has_data:\n                        if not item_name:\n                            item_name = item.id\n                        image_name = item_name + \'.jpg\'\n                        save_image(osp.join(subset_dir, image_name),\n                            item.image.data, create_dir=True)\n                    else:\n                        log.warning(""Item \'%s\' has no image"" % item.id)\n                image_paths[item.id] = osp.join(\'data\',\n                    osp.basename(subset_dir), image_name)\n\n                yolo_annotation = \'\'\n                for bbox in item.annotations:\n                    if bbox.type is not AnnotationType.bbox:\n                        continue\n                    if bbox.label is None:\n                        continue\n\n                    yolo_bb = _make_yolo_bbox((width, height), bbox.points)\n                    yolo_bb = \' \'.join(\'%.6f\' % p for p in yolo_bb)\n                    yolo_annotation += \'%s %s\\n\' % (bbox.label, yolo_bb)\n\n                annotation_path = osp.join(subset_dir, \'%s.txt\' % item_name)\n                with open(annotation_path, \'w\') as f:\n                    f.write(yolo_annotation)\n\n            subset_list_name = \'%s.txt\' % subset_name\n            subset_lists[subset_name] = subset_list_name\n            with open(osp.join(save_dir, subset_list_name), \'w\') as f:\n                f.writelines(\'%s\\n\' % s for s in image_paths.values())\n\n        with open(osp.join(save_dir, \'obj.data\'), \'w\') as f:\n            f.write(\'classes = %s\\n\' % len(label_ids))\n\n            for subset_name, subset_list_name in subset_lists.items():\n                f.write(\'%s = %s\\n\' % (subset_name,\n                    osp.join(\'data\', subset_list_name)))\n\n            f.write(\'names = %s\\n\' % osp.join(\'data\', \'obj.names\'))\n            f.write(\'backup = backup/\\n\')'"
datumaro/datumaro/plugins/yolo_format/extractor.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import OrderedDict\nimport os.path as osp\nimport re\n\nfrom datumaro.components.extractor import (SourceExtractor, Extractor,\n    DatasetItem, AnnotationType, Bbox, LabelCategories\n)\nfrom datumaro.util.image import Image\n\nfrom .format import YoloPath\n\n\nclass YoloExtractor(SourceExtractor):\n    class Subset(Extractor):\n        def __init__(self, name, parent):\n            super().__init__()\n            self._name = name\n            self._parent = parent\n            self.items = OrderedDict()\n\n        def __iter__(self):\n            for item_id in self.items:\n                yield self._parent._get(item_id, self._name)\n\n        def __len__(self):\n            return len(self.items)\n\n        def categories(self):\n            return self._parent.categories()\n\n    def __init__(self, config_path, image_info=None):\n        super().__init__()\n\n        if not osp.isfile(config_path):\n            raise Exception(""Can\'t read dataset descriptor file \'%s\'"" %\n                config_path)\n\n        rootpath = osp.dirname(config_path)\n        self._path = rootpath\n\n        assert image_info is None or isinstance(image_info, (str, dict))\n        if image_info is None:\n            image_info = osp.join(rootpath, YoloPath.IMAGE_META_FILE)\n            if not osp.isfile(image_info):\n                image_info = {}\n        if isinstance(image_info, str):\n            if not osp.isfile(image_info):\n                raise Exception(""Can\'t read image meta file \'%s\'"" % image_info)\n            with open(image_info) as f:\n                image_info = {}\n                for line in f:\n                    image_name, h, w = line.strip().split()\n                    image_info[image_name] = (int(h), int(w))\n        self._image_info = image_info\n\n        with open(config_path, \'r\') as f:\n            config_lines = f.readlines()\n\n        subsets = OrderedDict()\n        names_path = None\n\n        for line in config_lines:\n            match = re.match(r\'(\\w+)\\s*=\\s*(.+)$\', line)\n            if not match:\n                continue\n\n            key = match.group(1)\n            value = match.group(2)\n            if key == \'names\':\n                names_path = value\n            elif key in YoloPath.SUBSET_NAMES:\n                subsets[key] = value\n            else:\n                continue\n\n        if not names_path:\n            raise Exception(""Failed to parse labels path from \'%s\'"" % \\\n                config_path)\n\n        for subset_name, list_path in subsets.items():\n            list_path = self._make_local_path(list_path)\n            if not osp.isfile(list_path):\n                raise Exception(""Not found \'%s\' subset list file"" % subset_name)\n\n            subset = YoloExtractor.Subset(subset_name, self)\n            with open(list_path, \'r\') as f:\n                subset.items = OrderedDict(\n                    (osp.splitext(osp.basename(p.strip()))[0], p.strip())\n                    for p in f\n                )\n            subsets[subset_name] = subset\n\n        self._subsets = subsets\n\n        self._categories = {\n            AnnotationType.label:\n                self._load_categories(self._make_local_path(names_path))\n        }\n\n    def _make_local_path(self, path):\n        default_base = osp.join(\'data\', \'\')\n        if path.startswith(default_base): # default path\n            path = path[len(default_base) : ]\n        return osp.join(self._path, path) # relative or absolute path\n\n    def _get(self, item_id, subset_name):\n        subset = self._subsets[subset_name]\n        item = subset.items[item_id]\n\n        if isinstance(item, str):\n            image_path = self._make_local_path(item)\n            image_size = self._image_info.get(item_id)\n            image = Image(path=image_path, size=image_size)\n\n            anno_path = osp.splitext(image_path)[0] + \'.txt\'\n            annotations = self._parse_annotations(anno_path, image)\n\n            item = DatasetItem(id=item_id, subset=subset_name,\n                image=image, annotations=annotations)\n            subset.items[item_id] = item\n\n        return item\n\n    @staticmethod\n    def _parse_annotations(anno_path, image):\n        lines = []\n        with open(anno_path, \'r\') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    lines.append(line)\n\n        annotations = []\n        if lines:\n            # use image info as late as possible\n            image_height, image_width = image.size\n        for line in lines:\n            label_id, xc, yc, w, h = line.split()\n            label_id = int(label_id)\n            w = float(w)\n            h = float(h)\n            x = float(xc) - w * 0.5\n            y = float(yc) - h * 0.5\n            annotations.append(Bbox(\n                round(x * image_width, 1), round(y * image_height, 1),\n                round(w * image_width, 1), round(h * image_height, 1),\n                label=label_id\n            ))\n\n        return annotations\n\n    @staticmethod\n    def _load_categories(names_path):\n        label_categories = LabelCategories()\n\n        with open(names_path, \'r\') as f:\n            for label in f:\n                label_categories.add(label.strip())\n\n        return label_categories\n\n    def categories(self):\n        return self._categories\n\n    def __iter__(self):\n        for subset in self._subsets.values():\n            for item in subset:\n                yield item\n\n    def __len__(self):\n        length = 0\n        for subset in self._subsets.values():\n            length += len(subset)\n        return length\n\n    def subsets(self):\n        return list(self._subsets)\n\n    def get_subset(self, name):\n        return self._subsets[name]'"
datumaro/datumaro/plugins/yolo_format/format.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\n\nclass YoloPath:\n    DEFAULT_SUBSET_NAME = 'train'\n    SUBSET_NAMES = ['train', 'valid']\n\n    IMAGE_META_FILE = 'images.meta'"""
datumaro/datumaro/plugins/yolo_format/importer.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom glob import glob\nimport logging as log\nimport os.path as osp\n\nfrom datumaro.components.extractor import Importer\n\n\nclass YoloImporter(Importer):\n    @classmethod\n    def detect(cls, path):\n        return len(cls.find_configs(path)) != 0\n\n    def __call__(self, path, **extra_params):\n        from datumaro.components.project import Project # cyclic import\n        project = Project()\n\n        config_paths = self.find_configs(path)\n        if len(config_paths) == 0:\n            raise Exception(""Failed to find \'yolo\' dataset at \'%s\'"" % path)\n\n        for config_path in config_paths:\n            log.info(""Found a dataset at \'%s\'"" % config_path)\n\n            source_name = \'%s_%s\' % (\n                osp.basename(osp.dirname(config_path)),\n                osp.splitext(osp.basename(config_path))[0])\n            project.add_source(source_name, {\n                \'url\': config_path,\n                \'format\': \'yolo\',\n                \'options\': dict(extra_params),\n            })\n\n        return project\n\n    @staticmethod\n    def find_configs(path):\n        if path.endswith(\'.data\') and osp.isfile(path):\n            config_paths = [path]\n        else:\n            config_paths = glob(osp.join(path, \'**\', \'*.data\'), recursive=True)\n        return config_paths'"
utils/open_model_zoo/Transportation/semantic-segmentation-adas/interp.py,0,"b""import numpy as np\nfrom skimage.measure import approximate_polygon, find_contours\n\nimport cv2\n\n\nfor frame_results in detections:\n    frame_height = frame_results['frame_height']\n    frame_width = frame_results['frame_width']\n    frame_number = frame_results['frame_id']\n    detection = frame_results['detections']\n    detection = detection[0, 0, :, :]\n    width, height = detection.shape\n\n    for i in range(21):\n        zero = np.zeros((width,height),dtype=np.uint8)\n\n        f = float(i)\n        zero = ((detection == f) * 255).astype(np.float32)\n        zero = cv2.resize(zero, dsize=(frame_width, frame_height), interpolation=cv2.INTER_CUBIC)\n\n        contours = find_contours(zero, 0.8)\n\n        for contour in contours:\n            contour = np.flip(contour, axis=1)\n            contour = approximate_polygon(contour, tolerance=2.5)\n            segmentation = contour.tolist()\n            if len(segmentation) < 3:\n                continue\n\n            results.add_polygon(segmentation, i, frame_number)\n"""
cvat/apps/dataset_manager/formats/datumaro/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport json\nimport os\nimport os.path as osp\nimport shutil\nfrom tempfile import TemporaryDirectory\n\nfrom cvat.apps.dataset_manager.bindings import (CvatTaskDataExtractor,\n    import_dm_annotations)\nfrom cvat.apps.dataset_manager.util import make_zip_archive\nfrom cvat.settings.base import BASE_DIR, DATUMARO_PATH\nfrom datumaro.components.project import Project\n\nfrom ..registry import dm_env, exporter\n\n\n@exporter(name=""Datumaro"", ext=""ZIP"", version=""1.0"")\nclass DatumaroProjectExporter:\n    _REMOTE_IMAGES_EXTRACTOR = \'cvat_rest_api_task_images\'\n    _TEMPLATES_DIR = osp.join(osp.dirname(__file__), \'export_templates\')\n\n    @staticmethod\n    def _save_image_info(save_dir, task_data):\n        os.makedirs(save_dir, exist_ok=True)\n\n        config = {\n            \'server_url\': task_data._host or \'localhost\',\n            \'task_id\': task_data.db_task.id,\n        }\n\n        images = []\n        images_meta = { \'images\': images, }\n        for frame_id, frame in task_data.frame_info.items():\n            images.append({\n                \'id\': frame_id,\n                \'name\': osp.basename(frame[\'path\']),\n                \'width\': frame[\'width\'],\n                \'height\': frame[\'height\'],\n            })\n\n        with open(osp.join(save_dir, \'config.json\'), \'w\') as config_file:\n            json.dump(config, config_file)\n        with open(osp.join(save_dir, \'images_meta.json\'), \'w\') as images_file:\n            json.dump(images_meta, images_file)\n\n    def _export(self, task_data, save_dir, save_images=False):\n        dataset = CvatTaskDataExtractor(task_data, include_images=save_images)\n        converter = dm_env.make_converter(\'datumaro_project\',\n            save_images=save_images,\n            config={ \'project_name\': task_data.db_task.name, }\n        )\n        converter(dataset, save_dir=save_dir)\n\n        project = Project.load(save_dir)\n        target_dir = project.config.project_dir\n        os.makedirs(target_dir, exist_ok=True)\n        shutil.copyfile(\n            osp.join(self._TEMPLATES_DIR, \'README.md\'),\n            osp.join(target_dir, \'README.md\'))\n\n        if not save_images:\n            # add remote links to images\n            source_name = \'task_%s_images\' % task_data.db_task.id\n            project.add_source(source_name, {\n                \'format\': self._REMOTE_IMAGES_EXTRACTOR,\n            })\n            self._save_image_info(\n                osp.join(save_dir, project.local_source_dir(source_name)),\n                task_data)\n            project.save()\n\n            templates_dir = osp.join(self._TEMPLATES_DIR, \'plugins\')\n            target_dir = osp.join(project.config.project_dir,\n                project.config.env_dir, project.config.plugins_dir)\n            os.makedirs(target_dir, exist_ok=True)\n            shutil.copyfile(\n                osp.join(templates_dir, self._REMOTE_IMAGES_EXTRACTOR + \'.py\'),\n                osp.join(target_dir, self._REMOTE_IMAGES_EXTRACTOR + \'.py\'))\n\n        # Make Datumaro and CVAT CLI modules available to the user\n        shutil.copytree(DATUMARO_PATH, osp.join(save_dir, \'datumaro\'),\n            ignore=lambda src, names: [\'__pycache__\'] + [\n                n for n in names\n                if sum([int(n.endswith(ext)) for ext in\n                        [\'.pyx\', \'.pyo\', \'.pyd\', \'.pyc\']])\n            ])\n\n        cvat_utils_dst_dir = osp.join(save_dir, \'cvat\', \'utils\')\n        os.makedirs(cvat_utils_dst_dir)\n        shutil.copytree(osp.join(BASE_DIR, \'utils\', \'cli\'),\n            osp.join(cvat_utils_dst_dir, \'cli\'))\n\n    def __call__(self, dst_file, task_data, save_images=False):\n        with TemporaryDirectory() as temp_dir:\n            self._export(task_data, save_dir=temp_dir, save_images=save_images)\n            make_zip_archive(temp_dir, dst_file)\n'"
cvat/apps/git/management/commands/__init__.py,0,b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT'
cvat/apps/git/management/commands/update_git_states.py,0,"b'# Copyright (C) 2018 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom django.core.management.base import BaseCommand\nfrom cvat.apps.git.git import update_states\nimport time\n\nINTERVAL_SEC = 600\n\nclass Command(BaseCommand):\n    help = \'Run a regular updating for git status\'\n\n    def handle(self, *args, **options):\n        while True:\n            try:\n                update_states()\n            except Exception as ex:\n                print(""An error occured during update task statuses: {}"".format(str(ex)))\n            time.sleep(INTERVAL_SEC)\n\n'"
datumaro/datumaro/cli/contexts/item/__init__.py,0,"b""\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\n\nfrom ...util import add_subparser\n\n\ndef build_export_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor()\n    return parser\n\ndef build_stats_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor()\n    return parser\n\ndef build_diff_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor()\n    return parser\n\ndef build_edit_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor()\n    return parser\n\ndef build_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor()\n\n    subparsers = parser.add_subparsers()\n    add_subparser(subparsers, 'export', build_export_parser)\n    add_subparser(subparsers, 'stats', build_stats_parser)\n    add_subparser(subparsers, 'diff', build_diff_parser)\n    add_subparser(subparsers, 'edit', build_edit_parser)\n\n    return parser\n"""
datumaro/datumaro/cli/contexts/model/__init__.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\nimport logging as log\nimport os\nimport os.path as osp\nimport re\n\nfrom datumaro.components.config import DEFAULT_FORMAT\nfrom datumaro.components.project import Environment\nfrom ...util import add_subparser, MultilineFormatter\nfrom ...util.project import load_project\n\n\ndef build_add_parser(parser_ctor=argparse.ArgumentParser):\n    builtins = sorted(Environment().launchers.items)\n\n    parser = parser_ctor(help=""Add model to project"",\n        description=""""""\n            Registers an executable model into a project. A model requires\n            a launcher to be executed. Each launcher has its own options, which\n            are passed after \'--\' separator, pass \'-- -h\' for more info.\n            |n\n            List of builtin launchers: %s\n        """""" % \', \'.join(builtins),\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'-l\', \'--launcher\', required=True,\n        help=""Model launcher"")\n    parser.add_argument(\'extra_args\', nargs=argparse.REMAINDER, default=None,\n        help=""Additional arguments for converter (pass \'-- -h\' for help)"")\n    parser.add_argument(\'--copy\', action=\'store_true\',\n        help=""Copy the model to the project"")\n    parser.add_argument(\'-n\', \'--name\', default=None,\n        help=""Name of the model to be added (default: generate automatically)"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n        help=""Overwrite if exists"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.set_defaults(command=add_command)\n\n    return parser\n\ndef add_command(args):\n    project = load_project(args.project_dir)\n\n    if args.name:\n        if not args.overwrite and args.name in project.config.models:\n            raise CliException(""Model \'%s\' already exists ""\n                ""(pass --overwrite to overwrite)"" % args.name)\n    else:\n        existing_ids = [int(n.split(\'-\')[1]) for n in project.config.models\n            if re.match(r\'model-\\d+\', n)]\n        max_idx = max(existing_ids, default=len(project.config.models))\n        args.name = \'model-%d\' % (max_idx + 1)\n        assert args.name not in project.config.models, args.name\n\n    try:\n        launcher = project.env.launchers.get(args.launcher)\n    except KeyError:\n        raise CliException(""Launcher \'%s\' is not found"" % args.launcher)\n\n    cli_plugin = launcher.cli_plugin\n    model_args = cli_plugin.from_cmdline(args.extra_args)\n\n    if args.copy:\n        try:\n            log.info(""Copying model data"")\n\n            model_dir = project.local_model_dir(args.name)\n            os.makedirs(model_dir, exist_ok=False)\n            cli_plugin.copy_model(model_dir, model_args)\n        except NotImplementedError:\n            log.error(""Can\'t copy: copying is not available for \'%s\' models"" % \\\n                (args.launcher))\n\n    log.info(""Adding the model"")\n    project.add_model(args.name, {\n        \'launcher\': args.launcher,\n        \'options\': model_args,\n    })\n\n    log.info(""Checking the model"")\n    project.make_executable_model(args.name)\n\n    project.save()\n\n    log.info(""Model \'%s\' with launcher \'%s\' has been added to project \'%s\'"" % \\\n        (args.name, args.launcher, project.config.project_name))\n\n    return 0\n\ndef build_remove_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor()\n\n    parser.add_argument(\'name\',\n        help=""Name of the model to be removed"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.set_defaults(command=remove_command)\n\n    return parser\n\ndef remove_command(args):\n    project = load_project(args.project_dir)\n\n    project.remove_model(args.name)\n    project.save()\n\n    return 0\n\ndef build_run_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor()\n\n    parser.add_argument(\'-o\', \'--output-dir\', dest=\'dst_dir\', required=True,\n        help=""Directory to save output"")\n    parser.add_argument(\'-m\', \'--model\', dest=\'model_name\', required=True,\n        help=""Model to apply to the project"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.set_defaults(command=run_command)\n\n    return parser\n\ndef run_command(args):\n    project = load_project(args.project_dir)\n\n    dst_dir = osp.abspath(args.dst_dir)\n    os.makedirs(dst_dir, exist_ok=False)\n    project.make_dataset().apply_model(\n        save_dir=dst_dir,\n        model=args.model_name)\n\n    log.info(""Inference results have been saved to \'%s\'"" % dst_dir)\n\n    return 0\n\n\ndef build_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor()\n\n    subparsers = parser.add_subparsers()\n    add_subparser(subparsers, \'add\', build_add_parser)\n    add_subparser(subparsers, \'remove\', build_remove_parser)\n    add_subparser(subparsers, \'run\', build_run_parser)\n\n    return parser\n'"
datumaro/datumaro/cli/contexts/project/__init__.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\nfrom enum import Enum\nimport logging as log\nimport os\nimport os.path as osp\nimport shutil\n\nfrom datumaro.components.project import Project, Environment, \\\n    PROJECT_DEFAULT_CONFIG as DEFAULT_CONFIG\nfrom datumaro.components.comparator import Comparator\nfrom datumaro.components.dataset_filter import DatasetItemEncoder\nfrom datumaro.components.extractor import AnnotationType\nfrom datumaro.components.cli_plugin import CliPlugin\nfrom .diff import DiffVisualizer\nfrom ...util import add_subparser, CliException, MultilineFormatter, \\\n    make_file_name\nfrom ...util.project import load_project, generate_next_dir_name\n\n\ndef build_create_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(help=""Create empty project"",\n        description=""""""\n            Create a new empty project.|n\n            |n\n            Examples:|n\n            - Create a project in the current directory:|n\n            |s|screate -n myproject|n\n            |n\n            - Create a project in other directory:|n\n            |s|screate -o path/I/like/\n        """""",\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'-o\', \'--output-dir\', default=\'.\', dest=\'dst_dir\',\n        help=""Save directory for the new project (default: current dir"")\n    parser.add_argument(\'-n\', \'--name\', default=None,\n        help=""Name of the new project (default: same as project dir)"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n        help=""Overwrite existing files in the save directory"")\n    parser.set_defaults(command=create_command)\n\n    return parser\n\ndef create_command(args):\n    project_dir = osp.abspath(args.dst_dir)\n\n    project_env_dir = osp.join(project_dir, DEFAULT_CONFIG.env_dir)\n    if osp.isdir(project_env_dir) and os.listdir(project_env_dir):\n        if not args.overwrite:\n            raise CliException(""Directory \'%s\' already exists ""\n                ""(pass --overwrite to force creation)"" % project_env_dir)\n        else:\n            shutil.rmtree(project_env_dir, ignore_errors=True)\n\n    own_dataset_dir = osp.join(project_dir, DEFAULT_CONFIG.dataset_dir)\n    if osp.isdir(own_dataset_dir) and os.listdir(own_dataset_dir):\n        if not args.overwrite:\n            raise CliException(""Directory \'%s\' already exists ""\n                ""(pass --overwrite to force creation)"" % own_dataset_dir)\n        else:\n            # NOTE: remove the dir to avoid using data from previous project\n            shutil.rmtree(own_dataset_dir)\n\n    project_name = args.name\n    if project_name is None:\n        project_name = osp.basename(project_dir)\n\n    log.info(""Creating project at \'%s\'"" % project_dir)\n\n    Project.generate(project_dir, {\n        \'project_name\': project_name,\n    })\n\n    log.info(""Project has been created at \'%s\'"" % project_dir)\n\n    return 0\n\ndef build_import_parser(parser_ctor=argparse.ArgumentParser):\n    builtins = sorted(Environment().importers.items)\n\n    parser = parser_ctor(help=""Create project from existing dataset"",\n        description=""""""\n            Creates a project from an existing dataset. The source can be:|n\n            - a dataset in a supported format (check \'formats\' section below)|n\n            - a Datumaro project|n\n            |n\n            Formats:|n\n            Datasets come in a wide variety of formats. Each dataset\n            format defines its own data structure and rules on how to\n            interpret the data. For example, the following data structure\n            is used in COCO format:|n\n            /dataset/|n\n            - /images/<id>.jpg|n\n            - /annotations/|n\n            |n\n            In Datumaro dataset formats are supported by\n            Extractor-s and Importer-s.\n            An Extractor produces a list of dataset items corresponding\n            to the dataset. An Importer creates a project from the\n            data source location.\n            It is possible to add a custom Extractor and Importer.\n            To do this, you need to put an Extractor and\n            Importer implementation scripts to\n            <project_dir>/.datumaro/extractors\n            and <project_dir>/.datumaro/importers.|n\n            |n\n            List of builtin dataset formats: %s|n\n            |n\n            Examples:|n\n            - Create a project from VOC dataset in the current directory:|n\n            |s|simport -f voc -i path/to/voc|n\n            |n\n            - Create a project from COCO dataset in other directory:|n\n            |s|simport -f coco -i path/to/coco -o path/I/like/\n        """""" % \', \'.join(builtins),\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'-o\', \'--output-dir\', default=\'.\', dest=\'dst_dir\',\n        help=""Directory to save the new project to (default: current dir)"")\n    parser.add_argument(\'-n\', \'--name\', default=None,\n        help=""Name of the new project (default: same as project dir)"")\n    parser.add_argument(\'--copy\', action=\'store_true\',\n        help=""Copy the dataset instead of saving source links"")\n    parser.add_argument(\'--skip-check\', action=\'store_true\',\n        help=""Skip source checking"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n        help=""Overwrite existing files in the save directory"")\n    parser.add_argument(\'-i\', \'--input-path\', required=True, dest=\'source\',\n        help=""Path to import project from"")\n    parser.add_argument(\'-f\', \'--format\',\n        help=""Source project format. Will try to detect, if not specified."")\n    parser.add_argument(\'extra_args\', nargs=argparse.REMAINDER,\n        help=""Additional arguments for importer (pass \'-- -h\' for help)"")\n    parser.set_defaults(command=import_command)\n\n    return parser\n\ndef import_command(args):\n    project_dir = osp.abspath(args.dst_dir)\n\n    project_env_dir = osp.join(project_dir, DEFAULT_CONFIG.env_dir)\n    if osp.isdir(project_env_dir) and os.listdir(project_env_dir):\n        if not args.overwrite:\n            raise CliException(""Directory \'%s\' already exists ""\n                ""(pass --overwrite to force creation)"" % project_env_dir)\n        else:\n            shutil.rmtree(project_env_dir, ignore_errors=True)\n\n    own_dataset_dir = osp.join(project_dir, DEFAULT_CONFIG.dataset_dir)\n    if osp.isdir(own_dataset_dir) and os.listdir(own_dataset_dir):\n        if not args.overwrite:\n            raise CliException(""Directory \'%s\' already exists ""\n                ""(pass --overwrite to force creation)"" % own_dataset_dir)\n        else:\n            # NOTE: remove the dir to avoid using data from previous project\n            shutil.rmtree(own_dataset_dir)\n\n    project_name = args.name\n    if project_name is None:\n        project_name = osp.basename(project_dir)\n\n    env = Environment()\n    log.info(""Importing project from \'%s\'"" % args.source)\n\n    extra_args = {}\n    if not args.format:\n        if args.extra_args:\n            raise CliException(""Extra args can not be used without format"")\n\n        log.info(""Trying to detect dataset format..."")\n\n        matches = []\n        for format_name in env.importers.items:\n            log.debug(""Checking \'%s\' format..."", format_name)\n            importer = env.make_importer(format_name)\n            try:\n                match = importer.detect(args.source)\n                if match:\n                    log.debug(""format matched"")\n                    matches.append((format_name, importer))\n            except NotImplementedError:\n                log.debug(""Format \'%s\' does not support auto detection."",\n                    format_name)\n\n        if len(matches) == 0:\n            log.error(""Failed to detect dataset format automatically. ""\n                ""Try to specify format with \'-f/--format\' parameter."")\n            return 1\n        elif len(matches) != 1:\n            log.error(""Multiple formats match the dataset: %s. ""\n                ""Try to specify format with \'-f/--format\' parameter."",\n                \', \'.join(m[0] for m in matches))\n            return 2\n\n        format_name, importer = matches[0]\n        args.format = format_name\n    else:\n        try:\n            importer = env.make_importer(args.format)\n            if hasattr(importer, \'from_cmdline\'):\n                extra_args = importer.from_cmdline(args.extra_args)\n        except KeyError:\n            raise CliException(""Importer for format \'%s\' is not found"" % \\\n                args.format)\n\n    log.info(""Importing project as \'%s\'"" % args.format)\n\n    source = osp.abspath(args.source)\n    project = importer(source, **extra_args)\n    project.config.project_name = project_name\n    project.config.project_dir = project_dir\n\n    if not args.skip_check or args.copy:\n        log.info(""Checking the dataset..."")\n        dataset = project.make_dataset()\n    if args.copy:\n        log.info(""Cloning data..."")\n        dataset.save(merge=True, save_images=True)\n    else:\n        project.save()\n\n    log.info(""Project has been created at \'%s\'"" % project_dir)\n\n    return 0\n\n\nclass FilterModes(Enum):\n    # primary\n    items = 1\n    annotations = 2\n    items_annotations = 3\n\n    # shortcuts\n    i = 1\n    a = 2\n    i_a = 3\n    a_i = 3\n    annotations_items = 3\n\n    @staticmethod\n    def parse(s):\n        s = s.lower()\n        s = s.replace(\'+\', \'_\')\n        return FilterModes[s]\n\n    @classmethod\n    def make_filter_args(cls, mode):\n        if mode == cls.items:\n            return {}\n        elif mode == cls.annotations:\n            return {\n                \'filter_annotations\': True\n            }\n        elif mode == cls.items_annotations:\n            return {\n                \'filter_annotations\': True,\n                \'remove_empty\': True,\n            }\n        else:\n            raise NotImplementedError()\n\n    @classmethod\n    def list_options(cls):\n        return [m.name.replace(\'_\', \'+\') for m in cls]\n\ndef build_export_parser(parser_ctor=argparse.ArgumentParser):\n    builtins = sorted(Environment().converters.items)\n\n    parser = parser_ctor(help=""Export project"",\n        description=""""""\n            Exports the project dataset in some format. Optionally, a filter\n            can be passed, check \'extract\' command description for more info.\n            Each dataset format has its own options, which\n            are passed after \'--\' separator (see examples), pass \'-- -h\'\n            for more info. If not stated otherwise, by default\n            only annotations are exported, to include images pass\n            \'--save-images\' parameter.|n\n            |n\n            Formats:|n\n            In Datumaro dataset formats are supported by Converter-s.\n            A Converter produces a dataset of a specific format\n            from dataset items. It is possible to add a custom Converter.\n            To do this, you need to put a Converter\n            definition script to <project_dir>/.datumaro/converters.|n\n            |n\n            List of builtin dataset formats: %s|n\n            |n\n            Examples:|n\n            - Export project as a VOC-like dataset, include images:|n\n            |s|sexport -f voc -- --save-images|n\n            |n\n            - Export project as a COCO-like dataset in other directory:|n\n            |s|sexport -f coco -o path/I/like/\n        """""" % \', \'.join(builtins),\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'-e\', \'--filter\', default=None,\n        help=""Filter expression for dataset items"")\n    parser.add_argument(\'--filter-mode\', default=FilterModes.i.name,\n        type=FilterModes.parse,\n        help=""Filter mode (options: %s; default: %s)"" % \\\n            (\', \'.join(FilterModes.list_options()) , \'%(default)s\'))\n    parser.add_argument(\'-o\', \'--output-dir\', dest=\'dst_dir\', default=None,\n        help=""Directory to save output (default: a subdir in the current one)"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n        help=""Overwrite existing files in the save directory"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.add_argument(\'-f\', \'--format\', required=True,\n        help=""Output format"")\n    parser.add_argument(\'extra_args\', nargs=argparse.REMAINDER, default=None,\n        help=""Additional arguments for converter (pass \'-- -h\' for help)"")\n    parser.set_defaults(command=export_command)\n\n    return parser\n\ndef export_command(args):\n    project = load_project(args.project_dir)\n\n    dst_dir = args.dst_dir\n    if dst_dir:\n        if not args.overwrite and osp.isdir(dst_dir) and os.listdir(dst_dir):\n            raise CliException(""Directory \'%s\' already exists ""\n                ""(pass --overwrite to force creation)"" % dst_dir)\n    else:\n        dst_dir = generate_next_dir_name(\'%s-%s\' % \\\n            (project.config.project_name, make_file_name(args.format)))\n    dst_dir = osp.abspath(dst_dir)\n\n    try:\n        converter = project.env.converters.get(args.format)\n    except KeyError:\n        raise CliException(""Converter for format \'%s\' is not found"" % \\\n            args.format)\n\n    if hasattr(converter, \'from_cmdline\'):\n        extra_args = converter.from_cmdline(args.extra_args)\n        converter = converter(**extra_args)\n\n    filter_args = FilterModes.make_filter_args(args.filter_mode)\n\n    log.info(""Loading the project..."")\n    dataset = project.make_dataset()\n\n    log.info(""Exporting the project..."")\n    dataset.export_project(\n        save_dir=dst_dir,\n        converter=converter,\n        filter_expr=args.filter,\n        **filter_args)\n    log.info(""Project exported to \'%s\' as \'%s\'"" % \\\n        (dst_dir, args.format))\n\n    return 0\n\ndef build_extract_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(help=""Extract subproject"",\n        description=""""""\n            Extracts a subproject that contains only items matching filter.\n            A filter is an XPath expression, which is applied to XML\n            representation of a dataset item. Check \'--dry-run\' parameter\n            to see XML representations of the dataset items.|n\n            |n\n            To filter annotations use the mode (\'-m\') parameter.|n\n            Supported modes:|n\n            - \'i\', \'items\'|n\n            - \'a\', \'annotations\'|n\n            - \'i+a\', \'a+i\', \'items+annotations\', \'annotations+items\'|n\n            When filtering annotations, use the \'items+annotations\'\n            mode to point that annotation-less dataset items should be\n            removed. To select an annotation, write an XPath that\n            returns \'annotation\' elements (see examples).|n\n            |n\n            Examples:|n\n            - Filter images with width < height:|n\n            |s|sextract -e \'/item[image/width < image/height]\'|n\n            |n\n            - Filter images with large-area bboxes:|n\n            |s|sextract -e \'/item[annotation/type=""bbox"" and\n                annotation/area>2000]\'|n\n            |n\n            - Filter out all irrelevant annotations from items:|n\n            |s|sextract -m a -e \'/item/annotation[label = ""person""]\'|n\n            |n\n            - Filter out all irrelevant annotations from items:|n\n            |s|sextract -m a -e \'/item/annotation[label=""cat"" and\n            area > 99.5]\'|n\n            |n\n            - Filter occluded annotations and items, if no annotations left:|n\n            |s|sextract -m i+a -e \'/item/annotation[occluded=""True""]\'\n        """""",\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'-e\', \'--filter\', default=None,\n        help=""XML XPath filter expression for dataset items"")\n    parser.add_argument(\'-m\', \'--mode\', default=FilterModes.i.name,\n        type=FilterModes.parse,\n        help=""Filter mode (options: %s; default: %s)"" % \\\n            (\', \'.join(FilterModes.list_options()) , \'%(default)s\'))\n    parser.add_argument(\'--dry-run\', action=\'store_true\',\n        help=""Print XML representations to be filtered and exit"")\n    parser.add_argument(\'-o\', \'--output-dir\', dest=\'dst_dir\', default=None,\n        help=""Output directory (default: update current project)"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n        help=""Overwrite existing files in the save directory"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.set_defaults(command=extract_command)\n\n    return parser\n\ndef extract_command(args):\n    project = load_project(args.project_dir)\n\n    if not args.dry_run:\n        dst_dir = args.dst_dir\n        if dst_dir:\n            if not args.overwrite and osp.isdir(dst_dir) and os.listdir(dst_dir):\n                raise CliException(""Directory \'%s\' already exists ""\n                    ""(pass --overwrite to force creation)"" % dst_dir)\n        else:\n            dst_dir = generate_next_dir_name(\'%s-filter\' % \\\n                project.config.project_name)\n        dst_dir = osp.abspath(dst_dir)\n\n    dataset = project.make_dataset()\n\n    filter_args = FilterModes.make_filter_args(args.mode)\n\n    if args.dry_run:\n        dataset = dataset.extract(filter_expr=args.filter, **filter_args)\n        for item in dataset:\n            encoded_item = DatasetItemEncoder.encode(item, dataset.categories())\n            xml_item = DatasetItemEncoder.to_string(encoded_item)\n            print(xml_item)\n        return 0\n\n    if not args.filter:\n        raise CliException(""Expected a filter expression (\'-e\' argument)"")\n\n    dataset.extract_project(save_dir=dst_dir, filter_expr=args.filter,\n        **filter_args)\n\n    log.info(""Subproject has been extracted to \'%s\'"" % dst_dir)\n\n    return 0\n\ndef build_merge_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(help=""Merge projects"",\n        description=""""""\n            Updates items of the current project with items\n            from the other project.|n\n            |n\n            Examples:|n\n            - Update a project with items from other project:|n\n            |s|smerge -p path/to/first/project path/to/other/project\n        """""",\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'other_project_dir\',\n        help=""Directory of the project to get data updates from"")\n    parser.add_argument(\'-o\', \'--output-dir\', dest=\'dst_dir\', default=None,\n        help=""Output directory (default: current project\'s dir)"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n        help=""Overwrite existing files in the save directory"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.set_defaults(command=merge_command)\n\n    return parser\n\ndef merge_command(args):\n    first_project = load_project(args.project_dir)\n    second_project = load_project(args.other_project_dir)\n\n    dst_dir = args.dst_dir\n    if dst_dir:\n        if not args.overwrite and osp.isdir(dst_dir) and os.listdir(dst_dir):\n            raise CliException(""Directory \'%s\' already exists ""\n                ""(pass --overwrite to force creation)"" % dst_dir)\n\n    first_dataset = first_project.make_dataset()\n    first_dataset.update(second_project.make_dataset())\n\n    first_dataset.save(save_dir=dst_dir)\n\n    if dst_dir is None:\n        dst_dir = first_project.config.project_dir\n    dst_dir = osp.abspath(dst_dir)\n    log.info(""Merge results have been saved to \'%s\'"" % dst_dir)\n\n    return 0\n\ndef build_diff_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(help=""Compare projects"",\n        description=""""""\n        Compares two projects.|n\n        |n\n        Examples:|n\n        - Compare two projects, consider bboxes matching if their IoU > 0.7,|n\n        |s|s|s|sprint results to Tensorboard:\n        |s|sdiff path/to/other/project -o diff/ -f tensorboard --iou-thresh 0.7\n        """""",\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'other_project_dir\',\n        help=""Directory of the second project to be compared"")\n    parser.add_argument(\'-o\', \'--output-dir\', dest=\'dst_dir\', default=None,\n        help=""Directory to save comparison results (default: do not save)"")\n    parser.add_argument(\'-f\', \'--format\',\n        default=DiffVisualizer.DEFAULT_FORMAT,\n        choices=[f.name for f in DiffVisualizer.Format],\n        help=""Output format (default: %(default)s)"")\n    parser.add_argument(\'--iou-thresh\', default=0.5, type=float,\n        help=""IoU match threshold for detections (default: %(default)s)"")\n    parser.add_argument(\'--conf-thresh\', default=0.5, type=float,\n        help=""Confidence threshold for detections (default: %(default)s)"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n        help=""Overwrite existing files in the save directory"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the first project to be compared (default: current dir)"")\n    parser.set_defaults(command=diff_command)\n\n    return parser\n\ndef diff_command(args):\n    first_project = load_project(args.project_dir)\n    second_project = load_project(args.other_project_dir)\n\n    comparator = Comparator(\n        iou_threshold=args.iou_thresh,\n        conf_threshold=args.conf_thresh)\n\n    dst_dir = args.dst_dir\n    if dst_dir:\n        if not args.overwrite and osp.isdir(dst_dir) and os.listdir(dst_dir):\n            raise CliException(""Directory \'%s\' already exists ""\n                ""(pass --overwrite to force creation)"" % dst_dir)\n    else:\n        dst_dir = generate_next_dir_name(\'%s-%s-diff\' % (\n            first_project.config.project_name,\n            second_project.config.project_name)\n        )\n    dst_dir = osp.abspath(dst_dir)\n    log.info(""Saving diff to \'%s\'"" % dst_dir)\n\n    visualizer = DiffVisualizer(save_dir=dst_dir, comparator=comparator,\n        output_format=args.format)\n    visualizer.save_dataset_diff(\n        first_project.make_dataset(),\n        second_project.make_dataset())\n\n    return 0\n\ndef build_transform_parser(parser_ctor=argparse.ArgumentParser):\n    builtins = sorted(Environment().transforms.items)\n\n    parser = parser_ctor(help=""Transform project"",\n        description=""""""\n            Applies some operation to dataset items in the project\n            and produces a new project.|n\n            |n\n            Builtin transforms: %s|n\n            |n\n            Examples:|n\n            - Convert instance polygons to masks:|n\n            |s|stransform -n polygons_to_masks\n        """""" % \', \'.join(builtins),\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'-t\', \'--transform\', required=True,\n        help=""Transform to apply to the project"")\n    parser.add_argument(\'-o\', \'--output-dir\', dest=\'dst_dir\', default=None,\n        help=""Directory to save output (default: current dir)"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n        help=""Overwrite existing files in the save directory"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.add_argument(\'extra_args\', nargs=argparse.REMAINDER, default=None,\n        help=""Additional arguments for transformation (pass \'-- -h\' for help)"")\n    parser.set_defaults(command=transform_command)\n\n    return parser\n\ndef transform_command(args):\n    project = load_project(args.project_dir)\n\n    dst_dir = args.dst_dir\n    if dst_dir:\n        if not args.overwrite and osp.isdir(dst_dir) and os.listdir(dst_dir):\n            raise CliException(""Directory \'%s\' already exists ""\n                ""(pass --overwrite to force creation)"" % dst_dir)\n    else:\n        dst_dir = generate_next_dir_name(\'%s-%s\' % \\\n            (project.config.project_name, make_file_name(args.transform)))\n    dst_dir = osp.abspath(dst_dir)\n\n    try:\n        transform = project.env.transforms.get(args.transform)\n    except KeyError:\n        raise CliException(""Transform \'%s\' is not found"" % args.transform)\n\n    extra_args = {}\n    if hasattr(transform, \'from_cmdline\'):\n        extra_args = transform.from_cmdline(args.extra_args)\n\n    log.info(""Loading the project..."")\n    dataset = project.make_dataset()\n\n    log.info(""Transforming the project..."")\n    dataset.transform_project(\n        method=transform,\n        save_dir=dst_dir,\n        **extra_args\n    )\n\n    log.info(""Transform results have been saved to \'%s\'"" % dst_dir)\n\n    return 0\n\ndef build_info_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(help=""Get project info"",\n        description=""""""\n            Outputs project info.\n        """""",\n        formatter_class=MultilineFormatter)\n\n    parser.add_argument(\'--all\', action=\'store_true\',\n        help=""Print all information"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.set_defaults(command=info_command)\n\n    return parser\n\ndef info_command(args):\n    project = load_project(args.project_dir)\n    config = project.config\n    env = project.env\n    dataset = project.make_dataset()\n\n    print(""Project:"")\n    print(""  name:"", config.project_name)\n    print(""  location:"", config.project_dir)\n    print(""Plugins:"")\n    print(""  importers:"", \', \'.join(env.importers.items))\n    print(""  extractors:"", \', \'.join(env.extractors.items))\n    print(""  converters:"", \', \'.join(env.converters.items))\n    print(""  launchers:"", \', \'.join(env.launchers.items))\n\n    print(""Sources:"")\n    for source_name, source in config.sources.items():\n        print(""  source \'%s\':"" % source_name)\n        print(""    format:"", source.format)\n        print(""    url:"", source.url)\n        print(""    location:"", project.local_source_dir(source_name))\n\n    def print_extractor_info(extractor, indent=\'\'):\n        print(""%slength:"" % indent, len(extractor))\n\n        categories = extractor.categories()\n        print(""%scategories:"" % indent, \', \'.join(c.name for c in categories))\n\n        for cat_type, cat in categories.items():\n            print(""%s  %s:"" % (indent, cat_type.name))\n            if cat_type == AnnotationType.label:\n                print(""%s    count:"" % indent, len(cat.items))\n\n                count_threshold = 10\n                if args.all:\n                    count_threshold = len(cat.items)\n                labels = \', \'.join(c.name for c in cat.items[:count_threshold])\n                if count_threshold < len(cat.items):\n                    labels += "" (and %s more)"" % (\n                        len(cat.items) - count_threshold)\n                print(""%s    labels:"" % indent, labels)\n\n    print(""Dataset:"")\n    print_extractor_info(dataset, indent=""  "")\n\n    subsets = dataset.subsets()\n    print(""  subsets:"", \', \'.join(subsets))\n    for subset_name in subsets:\n        subset = dataset.get_subset(subset_name)\n        print(""    subset \'%s\':"" % subset_name)\n        print_extractor_info(subset, indent=""      "")\n\n    print(""Models:"")\n    for model_name, model in config.models.items():\n        print(""  model \'%s\':"" % model_name)\n        print(""    type:"", model.launcher)\n\n    return 0\n\n\ndef build_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(\n        description=""""""\n            Manipulate projects.|n\n            |n\n            By default, the project to be operated on is searched for\n            in the current directory. An additional \'-p\' argument can be\n            passed to specify project location.\n        """""",\n        formatter_class=MultilineFormatter)\n\n    subparsers = parser.add_subparsers()\n    add_subparser(subparsers, \'create\', build_create_parser)\n    add_subparser(subparsers, \'import\', build_import_parser)\n    add_subparser(subparsers, \'export\', build_export_parser)\n    add_subparser(subparsers, \'extract\', build_extract_parser)\n    add_subparser(subparsers, \'merge\', build_merge_parser)\n    add_subparser(subparsers, \'diff\', build_diff_parser)\n    add_subparser(subparsers, \'transform\', build_transform_parser)\n    add_subparser(subparsers, \'info\', build_info_parser)\n\n    return parser\n'"
datumaro/datumaro/cli/contexts/project/diff.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nfrom collections import Counter\nfrom enum import Enum\nimport numpy as np\nimport os\nimport os.path as osp\n\n_formats = [\'simple\']\n\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(""ignore"")\n    import tensorboardX as tb\n    _formats.append(\'tensorboard\')\n\nfrom datumaro.components.extractor import AnnotationType\nfrom datumaro.util.image import save_image\n\n\nFormat = Enum(\'Formats\', _formats)\n\nclass DiffVisualizer:\n    Format = Format\n    DEFAULT_FORMAT = Format.simple\n\n    _UNMATCHED_LABEL = -1\n\n\n    def __init__(self, comparator, save_dir, output_format=DEFAULT_FORMAT):\n        self.comparator = comparator\n\n        if isinstance(output_format, str):\n            output_format = Format[output_format]\n        assert output_format in Format\n        self.output_format = output_format\n\n        self.save_dir = save_dir\n        if output_format is Format.tensorboard:\n            logdir = osp.join(self.save_dir, \'logs\', \'diff\')\n            self.file_writer = tb.SummaryWriter(logdir)\n        if output_format is Format.simple:\n            self.label_diff_writer = None\n\n        self.categories = {}\n\n        self.label_confusion_matrix = Counter()\n        self.bbox_confusion_matrix = Counter()\n\n    def save_dataset_diff(self, extractor_a, extractor_b):\n        if self.save_dir:\n            os.makedirs(self.save_dir, exist_ok=True)\n\n        if len(extractor_a) != len(extractor_b):\n            print(""Datasets have different lengths: %s vs %s"" % \\\n                (len(extractor_a), len(extractor_b)))\n\n        self.categories = {}\n\n        label_mismatch = self.comparator. \\\n            compare_dataset_labels(extractor_a, extractor_b)\n        if label_mismatch is None:\n            print(""Datasets have no label information"")\n        elif len(label_mismatch) != 0:\n            print(""Datasets have mismatching labels:"")\n            for a_label, b_label in label_mismatch:\n                if a_label is None:\n                    print(""  > %s"" % b_label.name)\n                elif b_label is None:\n                    print(""  < %s"" % a_label.name)\n                else:\n                    print(""  %s != %s"" % (a_label.name, b_label.name))\n        else:\n            self.categories.update(extractor_a.categories())\n            self.categories.update(extractor_b.categories())\n\n        self.label_confusion_matrix = Counter()\n        self.bbox_confusion_matrix = Counter()\n\n        if self.output_format is Format.tensorboard:\n            self.file_writer.reopen()\n\n        for i, (item_a, item_b) in enumerate(zip(extractor_a, extractor_b)):\n            if item_a.id != item_b.id or not item_a.id or not item_b.id:\n                print(""Dataset items #%s \'%s\' \'%s\' do not match"" % \\\n                    (i + 1, item_a.id, item_b.id))\n                continue\n\n            label_diff = self.comparator.compare_item_labels(item_a, item_b)\n            self.update_label_confusion(label_diff)\n\n            bbox_diff = self.comparator.compare_item_bboxes(item_a, item_b)\n            self.update_bbox_confusion(bbox_diff)\n\n            self.save_item_label_diff(item_a, item_b, label_diff)\n            self.save_item_bbox_diff(item_a, item_b, bbox_diff)\n\n        if len(self.label_confusion_matrix) != 0:\n            self.save_conf_matrix(self.label_confusion_matrix,\n                \'labels_confusion.png\')\n        if len(self.bbox_confusion_matrix) != 0:\n            self.save_conf_matrix(self.bbox_confusion_matrix,\n                \'bbox_confusion.png\')\n\n        if self.output_format is Format.tensorboard:\n            self.file_writer.flush()\n            self.file_writer.close()\n        elif self.output_format is Format.simple:\n            if self.label_diff_writer:\n                self.label_diff_writer.flush()\n                self.label_diff_writer.close()\n\n    def update_label_confusion(self, label_diff):\n        matches, a_unmatched, b_unmatched = label_diff\n        for label in matches:\n            self.label_confusion_matrix[(label, label)] += 1\n        for a_label in a_unmatched:\n            self.label_confusion_matrix[(a_label, self._UNMATCHED_LABEL)] += 1\n        for b_label in b_unmatched:\n            self.label_confusion_matrix[(self._UNMATCHED_LABEL, b_label)] += 1\n\n    def update_bbox_confusion(self, bbox_diff):\n        matches, mispred, a_unmatched, b_unmatched = bbox_diff\n        for a_bbox, b_bbox in matches:\n            self.bbox_confusion_matrix[(a_bbox.label, b_bbox.label)] += 1\n        for a_bbox, b_bbox in mispred:\n            self.bbox_confusion_matrix[(a_bbox.label, b_bbox.label)] += 1\n        for a_bbox in a_unmatched:\n            self.bbox_confusion_matrix[(a_bbox.label, self._UNMATCHED_LABEL)] += 1\n        for b_bbox in b_unmatched:\n            self.bbox_confusion_matrix[(self._UNMATCHED_LABEL, b_bbox.label)] += 1\n\n    @classmethod\n    def draw_text_with_background(cls, frame, text, origin,\n            font=None, scale=1.0,\n            color=(0, 0, 0), thickness=1, bgcolor=(1, 1, 1)):\n        import cv2\n\n        if not font:\n            font = cv2.FONT_HERSHEY_SIMPLEX\n\n        text_size, baseline = cv2.getTextSize(text, font, scale, thickness)\n        cv2.rectangle(frame,\n            tuple((origin + (0, baseline)).astype(int)),\n            tuple((origin + (text_size[0], -text_size[1])).astype(int)),\n            bgcolor, cv2.FILLED)\n        cv2.putText(frame, text,\n            tuple(origin.astype(int)),\n            font, scale, color, thickness)\n        return text_size, baseline\n\n    def draw_detection_roi(self, frame, x, y, w, h, label, conf, color):\n        import cv2\n\n        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n\n        text = \'%s %.2f%%\' % (label, 100.0 * conf)\n        text_scale = 0.5\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        text_size = cv2.getTextSize(text, font, text_scale, 1)\n        line_height = np.array([0, text_size[0][1]])\n        self.draw_text_with_background(frame, text,\n            np.array([x, y]) - line_height * 0.5,\n            font, scale=text_scale, color=[255 - c for c in color])\n\n    def get_label(self, label_id):\n        cat = self.categories.get(AnnotationType.label)\n        if cat is None:\n            return str(label_id)\n        return cat.items[label_id].name\n\n    def draw_bbox(self, img, shape, color):\n        x, y, w, h = shape.get_bbox()\n        self.draw_detection_roi(img, int(x), int(y), int(w), int(h),\n            self.get_label(shape.label), shape.attributes.get(\'score\', 1),\n            color)\n\n    def get_label_diff_file(self):\n        if self.label_diff_writer is None:\n            self.label_diff_writer = \\\n                open(osp.join(self.save_dir, \'label_diff.txt\'), \'w\')\n        return self.label_diff_writer\n\n    def save_item_label_diff(self, item_a, item_b, diff):\n        _, a_unmatched, b_unmatched = diff\n\n        if 0 < len(a_unmatched) + len(b_unmatched):\n            if self.output_format is Format.simple:\n                f = self.get_label_diff_file()\n                f.write(item_a.id + \'\\n\')\n                for a_label in a_unmatched:\n                    f.write(\'  >%s\\n\' % self.get_label(a_label))\n                for b_label in b_unmatched:\n                    f.write(\'  <%s\\n\' % self.get_label(b_label))\n            elif self.output_format is Format.tensorboard:\n                tag = item_a.id\n                for a_label in a_unmatched:\n                    self.file_writer.add_text(tag,\n                        \'>%s\\n\' % self.get_label(a_label))\n                for b_label in b_unmatched:\n                    self.file_writer.add_text(tag,\n                        \'<%s\\n\' % self.get_label(b_label))\n\n    def save_item_bbox_diff(self, item_a, item_b, diff):\n        _, mispred, a_unmatched, b_unmatched = diff\n\n        if 0 < len(a_unmatched) + len(b_unmatched) + len(mispred):\n            img_a = item_a.image.copy()\n            img_b = img_a.copy()\n            for a_bbox, b_bbox in mispred:\n                self.draw_bbox(img_a, a_bbox, (0, 255, 0))\n                self.draw_bbox(img_b, b_bbox, (0, 0, 255))\n            for a_bbox in a_unmatched:\n                self.draw_bbox(img_a, a_bbox, (255, 255, 0))\n            for b_bbox in b_unmatched:\n                self.draw_bbox(img_b, b_bbox, (255, 255, 0))\n\n            img = np.hstack([img_a, img_b])\n\n            path = osp.join(self.save_dir, item_a.id)\n\n            if self.output_format is Format.simple:\n                save_image(path + \'.png\', img, create_dir=True)\n            elif self.output_format is Format.tensorboard:\n                self.save_as_tensorboard(img, path)\n\n    def save_as_tensorboard(self, img, name):\n        img = img[:, :, ::-1] # to RGB\n        img = np.transpose(img, (2, 0, 1)) # to (C, H, W)\n        img = img.astype(dtype=np.uint8)\n        self.file_writer.add_image(name, img)\n\n    def save_conf_matrix(self, conf_matrix, filename):\n        import matplotlib.pyplot as plt\n\n        classes = None\n        label_categories = self.categories.get(AnnotationType.label)\n        if label_categories is not None:\n            classes = { id: c.name for id, c in enumerate(label_categories.items) }\n        if classes is None:\n            classes = { c: \'label_%s\' % c for c, _ in conf_matrix }\n        classes[self._UNMATCHED_LABEL] = \'unmatched\'\n\n        class_idx = { id: i for i, id in enumerate(classes.keys()) }\n        matrix = np.zeros((len(classes), len(classes)), dtype=int)\n        for idx_pair in conf_matrix:\n            index = (class_idx[idx_pair[0]], class_idx[idx_pair[1]])\n            matrix[index] = conf_matrix[idx_pair]\n\n        labels = [label for id, label in classes.items()]\n\n        fig = plt.figure()\n        fig.add_subplot(111)\n        table = plt.table(\n            cellText=matrix,\n            colLabels=labels,\n            rowLabels=labels,\n            loc =\'center\')\n        table.auto_set_font_size(False)\n        table.set_fontsize(8)\n        table.scale(3, 3)\n        # Removing ticks and spines enables you to get the figure only with table\n        plt.tick_params(axis=\'x\', which=\'both\', bottom=False, top=False, labelbottom=False)\n        plt.tick_params(axis=\'y\', which=\'both\', right=False, left=False, labelleft=False)\n        for pos in [\'right\',\'top\',\'bottom\',\'left\']:\n            plt.gca().spines[pos].set_visible(False)\n\n        for idx_pair in conf_matrix:\n            i = class_idx[idx_pair[0]]\n            j = class_idx[idx_pair[1]]\n            if conf_matrix[idx_pair] != 0:\n                if i != j:\n                    table._cells[(i + 1, j)].set_facecolor(\'#FF0000\')\n                else:\n                    table._cells[(i + 1, j)].set_facecolor(\'#00FF00\')\n\n        plt.savefig(osp.join(self.save_dir, filename),\n            bbox_inches=\'tight\', pad_inches=0.05)\n'"
datumaro/datumaro/cli/contexts/source/__init__.py,0,"b'\n# Copyright (C) 2019 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport argparse\nimport logging as log\nimport os\nimport os.path as osp\nimport shutil\n\nfrom datumaro.components.project import Environment\nfrom ...util import add_subparser, CliException, MultilineFormatter\nfrom ...util.project import load_project\n\n\ndef build_add_parser(parser_ctor=argparse.ArgumentParser):\n    builtins = sorted(Environment().extractors.items)\n\n    base_parser = argparse.ArgumentParser(add_help=False)\n    base_parser.add_argument(\'-n\', \'--name\', default=None,\n        help=""Name of the new source"")\n    base_parser.add_argument(\'-f\', \'--format\', required=True,\n        help=""Source dataset format"")\n    base_parser.add_argument(\'--skip-check\', action=\'store_true\',\n        help=""Skip source checking"")\n    base_parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n\n    parser = parser_ctor(help=""Add data source to project"",\n        description=""""""\n            Adds a data source to a project. The source can be:|n\n            - a dataset in a supported format (check \'formats\' section below)|n\n            - a Datumaro project|n\n            |n\n            The source can be either a local directory or a remote\n            git repository. Each source type has its own parameters, which can\n            be checked by:|n\n            \'%s\'.|n\n            |n\n            Formats:|n\n            Datasets come in a wide variety of formats. Each dataset\n            format defines its own data structure and rules on how to\n            interpret the data. For example, the following data structure\n            is used in COCO format:|n\n            /dataset/|n\n            - /images/<id>.jpg|n\n            - /annotations/|n\n            |n\n            In Datumaro dataset formats are supported by Extractor-s.\n            An Extractor produces a list of dataset items corresponding\n            to the dataset. It is possible to add a custom Extractor.\n            To do this, you need to put an Extractor\n            definition script to <project_dir>/.datumaro/extractors.|n\n            |n\n            List of builtin source formats: %s|n\n            |n\n            Examples:|n\n            - Add a local directory with VOC-like dataset:|n\n            |s|sadd path path/to/voc -f voc_detection|n\n            - Add a local file with CVAT annotations, call it \'mysource\'|n\n            |s|s|s|sto the project somewhere else:|n\n            |s|sadd path path/to/cvat.xml -f cvat -n mysource -p somewhere/else/\n        """""" % (\'%(prog)s SOURCE_TYPE --help\', \', \'.join(builtins)),\n        formatter_class=MultilineFormatter,\n        add_help=False)\n    parser.set_defaults(command=add_command)\n\n    sp = parser.add_subparsers(dest=\'source_type\', metavar=\'SOURCE_TYPE\',\n        help=""The type of the data source ""\n            ""(call \'%s SOURCE_TYPE --help\' for more info)"" % parser.prog)\n\n    dir_parser = sp.add_parser(\'path\', help=""Add local path as source"",\n        parents=[base_parser])\n    dir_parser.add_argument(\'url\',\n        help=""Path to the source"")\n    dir_parser.add_argument(\'--copy\', action=\'store_true\',\n        help=""Copy the dataset instead of saving source links"")\n\n    repo_parser = sp.add_parser(\'git\', help=""Add git repository as source"",\n        parents=[base_parser])\n    repo_parser.add_argument(\'url\',\n        help=""URL of the source git repository"")\n    repo_parser.add_argument(\'-b\', \'--branch\', default=\'master\',\n        help=""Branch of the source repository (default: %(default)s)"")\n    repo_parser.add_argument(\'--checkout\', action=\'store_true\',\n        help=""Do branch checkout"")\n\n    # NOTE: add common parameters to the parent help output\n    # the other way could be to use parse_known_args()\n    display_parser = argparse.ArgumentParser(\n        parents=[base_parser, parser],\n        prog=parser.prog, usage=""%(prog)s [-h] SOURCE_TYPE ..."",\n        description=parser.description, formatter_class=MultilineFormatter)\n    class HelpAction(argparse._HelpAction):\n        def __call__(self, parser, namespace, values, option_string=None):\n            display_parser.print_help()\n            parser.exit()\n\n    parser.add_argument(\'-h\', \'--help\', action=HelpAction,\n        help=\'show this help message and exit\')\n\n    # TODO: needed distinction on how to add an extractor or a remote source\n\n    return parser\n\ndef add_command(args):\n    project = load_project(args.project_dir)\n\n    if args.source_type == \'git\':\n        name = args.name\n        if name is None:\n            name = osp.splitext(osp.basename(args.url))[0]\n\n        if project.env.git.has_submodule(name):\n            raise CliException(""Git submodule \'%s\' already exists"" % name)\n\n        try:\n            project.get_source(name)\n            raise CliException(""Source \'%s\' already exists"" % name)\n        except KeyError:\n            pass\n\n        rel_local_dir = project.local_source_dir(name)\n        local_dir = osp.join(project.config.project_dir, rel_local_dir)\n        url = args.url\n        project.env.git.create_submodule(name, local_dir,\n            url=url, branch=args.branch, no_checkout=not args.checkout)\n    elif args.source_type == \'path\':\n        url = osp.abspath(args.url)\n        if not osp.exists(url):\n            raise CliException(""Source path \'%s\' does not exist"" % url)\n\n        name = args.name\n        if name is None:\n            name = osp.splitext(osp.basename(url))[0]\n\n        if project.env.git.has_submodule(name):\n            raise CliException(""Git submodule \'%s\' already exists"" % name)\n\n        try:\n            project.get_source(name)\n            raise CliException(""Source \'%s\' already exists"" % name)\n        except KeyError:\n            pass\n\n        rel_local_dir = project.local_source_dir(name)\n        local_dir = osp.join(project.config.project_dir, rel_local_dir)\n\n        if args.copy:\n            log.info(""Copying from \'%s\' to \'%s\'"" % (url, local_dir))\n            if osp.isdir(url):\n                # copytree requires destination dir not to exist\n                shutil.copytree(url, local_dir)\n                url = rel_local_dir\n            elif osp.isfile(url):\n                os.makedirs(local_dir)\n                shutil.copy2(url, local_dir)\n                url = osp.join(rel_local_dir, osp.basename(url))\n            else:\n                raise Exception(""Expected file or directory"")\n        else:\n            os.makedirs(local_dir)\n\n    project.add_source(name, { \'url\': url, \'format\': args.format })\n\n    if not args.skip_check:\n        log.info(""Checking the source..."")\n        try:\n            project.make_source_project(name).make_dataset()\n        except Exception:\n            shutil.rmtree(local_dir, ignore_errors=True)\n            raise\n\n    project.save()\n\n    log.info(""Source \'%s\' has been added to the project, location: \'%s\'"" \\\n        % (name, rel_local_dir))\n\n    return 0\n\ndef build_remove_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(help=""Remove source from project"",\n        description=""Remove a source from a project."")\n\n    parser.add_argument(\'-n\', \'--name\', required=True,\n        help=""Name of the source to be removed"")\n    parser.add_argument(\'--force\', action=\'store_true\',\n        help=""Ignore possible errors during removal"")\n    parser.add_argument(\'--keep-data\', action=\'store_true\',\n        help=""Do not remove source data"")\n    parser.add_argument(\'-p\', \'--project\', dest=\'project_dir\', default=\'.\',\n        help=""Directory of the project to operate on (default: current dir)"")\n    parser.set_defaults(command=remove_command)\n\n    return parser\n\ndef remove_command(args):\n    project = load_project(args.project_dir)\n\n    name = args.name\n    if not name:\n        raise CliException(""Expected source name"")\n    try:\n        project.get_source(name)\n    except KeyError:\n        if not args.force:\n            raise CliException(""Source \'%s\' does not exist"" % name)\n\n    if project.env.git.has_submodule(name):\n        if args.force:\n            log.warning(""Forcefully removing the \'%s\' source..."" % name)\n\n        project.env.git.remove_submodule(name, force=args.force)\n\n    source_dir = osp.join(project.config.project_dir,\n        project.local_source_dir(name))\n    project.remove_source(name)\n    project.save()\n\n    if not args.keep_data:\n        shutil.rmtree(source_dir, ignore_errors=True)\n\n    log.info(""Source \'%s\' has been removed from the project"" % name)\n\n    return 0\n\ndef build_parser(parser_ctor=argparse.ArgumentParser):\n    parser = parser_ctor(description=""""""\n            Manipulate data sources inside of a project.|n\n            |n\n            A data source is a source of data for a project.\n            The project combines multiple data sources into one dataset.\n            The role of a data source is to provide dataset items - images\n            and/or annotations.|n\n            |n\n            By default, the project to be operated on is searched for\n            in the current directory. An additional \'-p\' argument can be\n            passed to specify project location.\n        """""",\n        formatter_class=MultilineFormatter)\n\n    subparsers = parser.add_subparsers()\n    add_subparser(subparsers, \'add\', build_add_parser)\n    add_subparser(subparsers, \'remove\', build_remove_parser)\n\n    return parser\n'"
cvat/apps/dataset_manager/formats/datumaro/export_templates/plugins/cvat_rest_api_task_images.py,0,"b'\n# Copyright (C) 2020 Intel Corporation\n#\n# SPDX-License-Identifier: MIT\n\nimport getpass\nimport json\nimport os\nimport os.path as osp\nfrom collections import OrderedDict\n\nimport requests\n\nfrom cvat.utils.cli.core import CLI as CVAT_CLI\nfrom cvat.utils.cli.core import CVAT_API_V1\nfrom datumaro.components.config import Config, SchemaBuilder\nfrom datumaro.components.extractor import SourceExtractor, DatasetItem\nfrom datumaro.util.image import Image, lazy_image, load_image\n\nCONFIG_SCHEMA = SchemaBuilder() \\\n    .add(\'task_id\', int) \\\n    .add(\'server_url\', str) \\\n    .build()\n\nclass cvat_rest_api_task_images(SourceExtractor):\n    def _image_local_path(self, item_id):\n        task_id = self._config.task_id\n        return osp.join(self._cache_dir,\n            \'task_{}_frame_{:06d}.jpg\'.format(task_id, int(item_id)))\n\n    def _make_image_loader(self, item_id):\n        return lazy_image(item_id,\n            lambda item_id: self._image_loader(item_id, self))\n\n    def _is_image_cached(self, item_id):\n        return osp.isfile(self._image_local_path(item_id))\n\n    def _download_image(self, item_id):\n        self._connect()\n        os.makedirs(self._cache_dir, exist_ok=True)\n        self._cvat_cli.tasks_frame(task_id=self._config.task_id,\n            frame_ids=[item_id], outdir=self._cache_dir, quality=\'original\')\n\n    def _connect(self):\n        if self._session is not None:\n            return\n\n        session = None\n        try:\n            print(""Enter credentials for \'%s\' to read task data:"" % \\\n                (self._config.server_url))\n            username = input(\'User: \')\n            password = getpass.getpass()\n\n            session = requests.Session()\n            session.auth = (username, password)\n\n            api = CVAT_API_V1(self._config.server_url)\n            cli = CVAT_CLI(session, api)\n\n            self._session = session\n            self._cvat_cli = cli\n        except Exception:\n            if session is not None:\n                session.close()\n\n    def __del__(self):\n        if hasattr(self, \'_session\'):\n            if self._session is not None:\n                self._session.close()\n\n    @staticmethod\n    def _image_loader(item_id, extractor):\n        if not extractor._is_image_cached(item_id):\n            extractor._download_image(item_id)\n        local_path = extractor._image_local_path(item_id)\n        return load_image(local_path)\n\n    def __init__(self, url):\n        super().__init__()\n\n        local_dir = url\n        self._local_dir = local_dir\n        self._cache_dir = osp.join(local_dir, \'images\')\n\n        with open(osp.join(url, \'config.json\'), \'r\') as config_file:\n            config = json.load(config_file)\n            config = Config(config, schema=CONFIG_SCHEMA)\n        self._config = config\n\n        with open(osp.join(url, \'images_meta.json\'), \'r\') as images_file:\n            images_meta = json.load(images_file)\n            image_list = images_meta[\'images\']\n\n        items = []\n        for entry in image_list:\n            item_id = entry[\'id\']\n            item_filename = entry.get(\'name\', str(item_id))\n            size = None\n            if entry.get(\'height\') and entry.get(\'width\'):\n                size = (entry[\'height\'], entry[\'width\'])\n            image = Image(data=self._make_image_loader(item_id),\n                path=item_filename, size=size)\n            item = DatasetItem(id=item_id, image=image)\n            items.append((item.id, item))\n\n        items = sorted(items, key=lambda e: int(e[0]))\n        items = OrderedDict(items)\n        self._items = items\n\n        self._cvat_cli = None\n        self._session = None\n\n    def __iter__(self):\n        for item in self._items.values():\n            yield item\n\n    def __len__(self):\n        return len(self._items)\n'"
utils/open_model_zoo/Retail/object_detection/text/pixel_link_mobilenet_v2/0001/pixel_link_mobilenet_v2.py,0,"b'import cv2\nimport numpy as np\n\n\nclass PixelLinkDecoder():\n    def __init__(self):\n        four_neighbours = False\n        if four_neighbours:\n            self._get_neighbours = self._get_neighbours_4\n        else:\n            self._get_neighbours = self._get_neighbours_8\n        self.pixel_conf_threshold = 0.8 \n        self.link_conf_threshold = 0.8 \n\n    def decode(self, height, width, detections: dict):\n        self.image_height = height\n        self.image_width = width\n        self.pixel_scores = self._set_pixel_scores(detections[\'pixel_cls/add_2\'])\n        self.link_scores = self._set_link_scores(detections[\'pixel_link/add_2\'])\n\n        self.pixel_mask = self.pixel_scores >= self.pixel_conf_threshold\n        self.link_mask = self.link_scores >= self.link_conf_threshold\n        self.points = list(zip(*np.where(self.pixel_mask)))\n        self.h, self.w = np.shape(self.pixel_mask)\n        self.group_mask = dict.fromkeys(self.points, -1)\n        self.bboxes = None\n        self.root_map = None\n        self.mask = None\n\n        self._decode()\n\n    def _softmax(self, x, axis=None):\n        return np.exp(x - self._logsumexp(x, axis=axis, keepdims=True))\n\n    def _logsumexp(self, a, axis=None, b=None, keepdims=False, return_sign=False):\n        if b is not None:\n            a, b = np.broadcast_arrays(a, b)\n            if np.any(b == 0):\n                a = a + 0.  # promote to at least float\n                a[b == 0] = -np.inf\n\n        a_max = np.amax(a, axis=axis, keepdims=True)\n\n        if a_max.ndim > 0:\n            a_max[~np.isfinite(a_max)] = 0\n        elif not np.isfinite(a_max):\n            a_max = 0\n\n        if b is not None:\n            b = np.asarray(b)\n            tmp = b * np.exp(a - a_max)\n        else:\n            tmp = np.exp(a - a_max)\n\n        # suppress warnings about log of zero\n        with np.errstate(divide=\'ignore\'):\n            s = np.sum(tmp, axis=axis, keepdims=keepdims)\n            if return_sign:\n                sgn = np.sign(s)\n                s *= sgn  # /= makes more sense but we need zero -> zero\n            out = np.log(s)\n\n        if not keepdims:\n            a_max = np.squeeze(a_max, axis=axis)\n        out += a_max\n\n        if return_sign:\n            return out, sgn\n        else:\n            return out\n\n    def _set_pixel_scores(self, pixel_scores):\n        ""get softmaxed properly shaped pixel scores""\n        tmp = np.transpose(pixel_scores, (0, 2, 3, 1))\n        return self._softmax(tmp, axis=-1)[0, :, :, 1]\n\n    def _set_link_scores(self, link_scores):\n        ""get softmaxed properly shaped links scores""\n        tmp = np.transpose(link_scores, (0, 2, 3, 1))\n        tmp_reshaped = tmp.reshape(tmp.shape[:-1] + (8, 2))\n        return self._softmax(tmp_reshaped, axis=-1)[0, :, :, :, 1]\n\n    def _find_root(self, point):\n        root = point\n        update_parent = False\n        tmp = self.group_mask[root]\n        while tmp is not -1:\n            root = tmp\n            tmp = self.group_mask[root]\n            update_parent = True\n        if update_parent:\n            self.group_mask[point] = root\n        return root\n\n    def _join(self, p1, p2):\n        root1 = self._find_root(p1)\n        root2 = self._find_root(p2)\n        if root1 != root2:\n            self.group_mask[root2] = root1\n\n    def _get_index(self, root):\n        if root not in self.root_map:\n            self.root_map[root] = len(self.root_map) + 1\n        return self.root_map[root]\n\n    def _get_all(self):\n        self.root_map = {}\n        self.mask = np.zeros_like(self.pixel_mask, dtype=np.int32)\n\n        for point in self.points:\n            point_root = self._find_root(point)\n            bbox_idx = self._get_index(point_root)\n            self.mask[point] = bbox_idx\n\n    def _get_neighbours_8(self, x, y):\n        w, h = self.w, self.h\n        tmp = [(0, x - 1, y - 1), (1, x, y - 1),\n               (2, x + 1, y - 1), (3, x - 1, y),\n               (4, x + 1, y), (5, x - 1, y + 1),\n               (6, x, y + 1), (7, x + 1, y + 1)]\n\n        return [i for i in tmp if i[1] >= 0 and i[1] < w and i[2] >= 0 and i[2] < h]\n\n    def _get_neighbours_4(self, x, y):\n        w, h = self.w, self.h\n        tmp = [(1, x, y - 1),\n               (3, x - 1, y),\n               (4, x + 1, y),\n               (6, x, y + 1)]\n\n        return [i for i in tmp if i[1] >= 0 and i[1] < w and i[2] >= 0 and i[2] < h]\n\n    def _mask_to_bboxes(self, min_area=300, min_height=10):\n        self.bboxes = []\n        max_bbox_idx = self.mask.max()\n        mask_tmp = cv2.resize(self.mask, (self.image_width, self.image_height), interpolation=cv2.INTER_NEAREST)\n\n        for bbox_idx in range(1, max_bbox_idx + 1):\n            bbox_mask = mask_tmp == bbox_idx\n            cnts, _ = cv2.findContours(bbox_mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            if len(cnts) == 0:\n                continue\n            cnt = cnts[0]\n            rect, w, h = self._min_area_rect(cnt)\n            if min(w, h) < min_height:\n                continue\n            if w * h < min_area:\n                continue\n            self.bboxes.append(self._order_points(rect))\n\n    def _min_area_rect(self, cnt):\n        rect = cv2.minAreaRect(cnt)\n        w, h = rect[1]\n        box = cv2.boxPoints(rect)\n        box = np.int0(box)\n        return box, w, h\n\n    def _order_points(self, rect):\n        """""" (x, y)\n            Order: TL, TR, BR, BL\n        """"""\n        tmp = np.zeros_like(rect)\n        sums = rect.sum(axis=1)\n        tmp[0] = rect[np.argmin(sums)]\n        tmp[2] = rect[np.argmax(sums)]\n        diff = np.diff(rect, axis=1)\n        tmp[1] = rect[np.argmin(diff)]\n        tmp[3] = rect[np.argmax(diff)]\n        return tmp\n\n    def _decode(self):\n        for point in self.points:\n            y, x = point\n            neighbours = self._get_neighbours(x, y)\n            for n_idx, nx, ny in neighbours:\n                link_value = self.link_mask[y, x, n_idx]\n                pixel_cls = self.pixel_mask[ny, nx]\n                if link_value and pixel_cls:\n                    self._join(point, (ny, nx))\n\n        self._get_all()\n        self._mask_to_bboxes()\n\n\nlabel = 1\npcd = PixelLinkDecoder()\nfor detection in detections:\n    frame = detection[\'frame_id\']\n    pcd.decode(detection[\'frame_height\'], detection[\'frame_width\'], detection[\'detections\'])\n    for box in pcd.bboxes:\n        box = [[int(b[0]), int(b[1])] for b in box]\n        results.add_polygon(box, label, frame)\n'"
utils/open_model_zoo/Retail/object_detection/text/pixel_link_mobilenet_v2/0004/pixel_link_mobilenet_v2.py,0,"b'# SPDX-License-Identifier: MIT`\n\nimport cv2\nimport numpy as np\n\n\nclass PixelLinkDecoder():\n    def __init__(self):\n        four_neighbours = False\n        if four_neighbours:\n            self._get_neighbours = self._get_neighbours_4\n        else:\n            self._get_neighbours = self._get_neighbours_8\n        self.pixel_conf_threshold = 0.8\n        self.link_conf_threshold = 0.8\n\n    def decode(self, height, width, detections: dict):\n        self.image_height = height\n        self.image_width = width\n        self.pixel_scores = self._set_pixel_scores(detections[\'model/segm_logits/add\'])\n        self.link_scores = self._set_link_scores(detections[\'model/link_logits_/add\'])\n\n        self.pixel_mask = self.pixel_scores >= self.pixel_conf_threshold\n        self.link_mask = self.link_scores >= self.link_conf_threshold\n        self.points = list(zip(*np.where(self.pixel_mask)))\n        self.h, self.w = np.shape(self.pixel_mask)\n        self.group_mask = dict.fromkeys(self.points, -1)\n        self.bboxes = None\n        self.root_map = None\n        self.mask = None\n\n        self._decode()\n\n    def _softmax(self, x, axis=None):\n        return np.exp(x - self._logsumexp(x, axis=axis, keepdims=True))\n\n    # pylint: disable=no-self-use\n    def _logsumexp(self, a, axis=None, b=None, keepdims=False, return_sign=False):\n        if b is not None:\n            a, b = np.broadcast_arrays(a, b)\n            if np.any(b == 0):\n                a = a + 0.  # promote to at least float\n                a[b == 0] = -np.inf\n\n        a_max = np.amax(a, axis=axis, keepdims=True)\n\n        if a_max.ndim > 0:\n            a_max[~np.isfinite(a_max)] = 0\n        elif not np.isfinite(a_max):\n            a_max = 0\n\n        if b is not None:\n            b = np.asarray(b)\n            tmp = b * np.exp(a - a_max)\n        else:\n            tmp = np.exp(a - a_max)\n\n        # suppress warnings about log of zero\n        with np.errstate(divide=\'ignore\'):\n            s = np.sum(tmp, axis=axis, keepdims=keepdims)\n            if return_sign:\n                sgn = np.sign(s)\n                s *= sgn  # /= makes more sense but we need zero -> zero\n            out = np.log(s)\n\n        if not keepdims:\n            a_max = np.squeeze(a_max, axis=axis)\n        out += a_max\n\n        if return_sign:\n            return out, sgn\n        else:\n            return out\n\n    def _set_pixel_scores(self, pixel_scores):\n        ""get softmaxed properly shaped pixel scores""\n        tmp = np.transpose(pixel_scores, (0, 2, 3, 1))\n        return self._softmax(tmp, axis=-1)[0, :, :, 1]\n\n    def _set_link_scores(self, link_scores):\n        ""get softmaxed properly shaped links scores""\n        tmp = np.transpose(link_scores, (0, 2, 3, 1))\n        tmp_reshaped = tmp.reshape(tmp.shape[:-1] + (8, 2))\n        return self._softmax(tmp_reshaped, axis=-1)[0, :, :, :, 1]\n\n    def _find_root(self, point):\n        root = point\n        update_parent = False\n        tmp = self.group_mask[root]\n        while tmp is not -1:\n            root = tmp\n            tmp = self.group_mask[root]\n            update_parent = True\n        if update_parent:\n            self.group_mask[point] = root\n        return root\n\n    def _join(self, p1, p2):\n        root1 = self._find_root(p1)\n        root2 = self._find_root(p2)\n        if root1 != root2:\n            self.group_mask[root2] = root1\n\n    def _get_index(self, root):\n        if root not in self.root_map:\n            self.root_map[root] = len(self.root_map) + 1\n        return self.root_map[root]\n\n    def _get_all(self):\n        self.root_map = {}\n        self.mask = np.zeros_like(self.pixel_mask, dtype=np.int32)\n\n        for point in self.points:\n            point_root = self._find_root(point)\n            bbox_idx = self._get_index(point_root)\n            self.mask[point] = bbox_idx\n\n    def _get_neighbours_8(self, x, y):\n        w, h = self.w, self.h\n        tmp = [(0, x - 1, y - 1), (1, x, y - 1),\n               (2, x + 1, y - 1), (3, x - 1, y),\n               (4, x + 1, y), (5, x - 1, y + 1),\n               (6, x, y + 1), (7, x + 1, y + 1)]\n\n        return [i for i in tmp if i[1] >= 0 and i[1] < w and i[2] >= 0 and i[2] < h]\n\n    def _get_neighbours_4(self, x, y):\n        w, h = self.w, self.h\n        tmp = [(1, x, y - 1),\n               (3, x - 1, y),\n               (4, x + 1, y),\n               (6, x, y + 1)]\n\n        return [i for i in tmp if i[1] >= 0 and i[1] < w and i[2] >= 0 and i[2] < h]\n\n    def _mask_to_bboxes(self, min_area=300, min_height=10):\n        self.bboxes = []\n        max_bbox_idx = self.mask.max()\n        mask_tmp = cv2.resize(self.mask, (self.image_width, self.image_height), interpolation=cv2.INTER_NEAREST)\n\n        for bbox_idx in range(1, max_bbox_idx + 1):\n            bbox_mask = mask_tmp == bbox_idx\n            cnts, _ = cv2.findContours(bbox_mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            if len(cnts) == 0:\n                continue\n            cnt = cnts[0]\n            rect, w, h = self._min_area_rect(cnt)\n            if min(w, h) < min_height:\n                continue\n            if w * h < min_area:\n                continue\n            self.bboxes.append(self._order_points(rect))\n\n    # pylint: disable=no-self-use\n    def _min_area_rect(self, cnt):\n        rect = cv2.minAreaRect(cnt)\n        w, h = rect[1]\n        box = cv2.boxPoints(rect)\n        box = np.int0(box)\n        return box, w, h\n\n    # pylint: disable=no-self-use\n    def _order_points(self, rect):\n        """""" (x, y)\n            Order: TL, TR, BR, BL\n        """"""\n        tmp = np.zeros_like(rect)\n        sums = rect.sum(axis=1)\n        tmp[0] = rect[np.argmin(sums)]\n        tmp[2] = rect[np.argmax(sums)]\n        diff = np.diff(rect, axis=1)\n        tmp[1] = rect[np.argmin(diff)]\n        tmp[3] = rect[np.argmax(diff)]\n        return tmp\n\n    def _decode(self):\n        for point in self.points:\n            y, x = point\n            neighbours = self._get_neighbours(x, y)\n            for n_idx, nx, ny in neighbours:\n                link_value = self.link_mask[y, x, n_idx]\n                pixel_cls = self.pixel_mask[ny, nx]\n                if link_value and pixel_cls:\n                    self._join(point, (ny, nx))\n\n        self._get_all()\n        self._mask_to_bboxes()\n\n\nlabel = 1\npcd = PixelLinkDecoder()\nfor detection in detections:\n    frame = detection[\'frame_id\']\n    pcd.decode(detection[\'frame_height\'], detection[\'frame_width\'], detection[\'detections\'])\n    for box in pcd.bboxes:\n        box = [[int(b[0]), int(b[1])] for b in box]\n        results.add_polygon(box, label, frame)\n'"
