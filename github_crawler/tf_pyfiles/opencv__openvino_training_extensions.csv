file_path,api_count,code
tools/downscale_images.py,0,"b'""""""\n  Script for downscaling images. Can be used for improving training speed.\n""""""\n\nimport argparse\nimport os\n\nimport cv2\n\n\ndef parse_args():\n  """"""\n    Parse command arguments.\n  """"""\n  parser = argparse.ArgumentParser(description=\'Downscale images inplace\')\n  parser.add_argument(\'path\', help=\'Path to a directory with images\')\n  parser.add_argument(\'-target_size\', type=int, default=450,\n                      help=\'A minimum size of a image side (weight or height)\')\n  return parser.parse_args()\n\n\ndef downscale(paths: list, min_size: int, save_aspect_ratio: bool):\n  """""" Downscale images inplace\n\n  Args:\n    paths: Full paths to images\n    min_size: A minimum size of a image side (weight or height)\n    save_aspect_ratio: Save aspect ratio of an origin image\n  """"""\n\n  def _resize_and_save(path, img, target_width, target_height):\n    img = cv2.resize(img, (target_width, target_height), interpolation=cv2.INTER_AREA)\n    cv2.imwrite(path, img)\n\n  downscaled_count = 0\n  skipped_count = 0\n  smaller_count = 0\n  for id_, key in enumerate(paths):\n    path = key\n    img = cv2.imread(path)\n    if img is None:\n      print(\'Skip: {}\'.format(path))\n      continue\n    height, width = img.shape[:2]\n    if save_aspect_ratio:\n      if min_size in (width, height):\n        print(""{0:04d}: {1}    ({2}) == ({3}, {4})"".format(id_, path, min_size, width, height))\n        skipped_count += 1\n      elif height > min_size and width > min_size:\n        target_width = int(width * min_size / height)\n        target_height = int(height * min_size / width)\n\n        if target_height >= target_width:\n          target_width = min_size\n        else:\n          target_height = min_size\n\n        _resize_and_save(path, img, target_width, target_height)\n        print(""{0:04d}: {1}    ({2}, {3}) -> ({4}, {5})"".format(id_, path, width, height, target_width, target_height))\n        downscaled_count += 1\n      else:\n        print(""{0:04d}: {1}    ({2}) < max({3}, {4})"".format(id_, path, min_size, width, height))\n        smaller_count += 1\n    else:\n      _resize_and_save(path, img, min_size, min_size)\n      print(""{0:04d}: {1}    ({2}, {3}) -> ({4}, {5})"".format(id_, path, width, height, min_size, min_size))\n      downscaled_count += 1\n\n  print(\'Summary (min_size = {0}, save_aspect_ratio = {1}):\'.format(min_size, save_aspect_ratio))\n  print(\'  downscaled images     = {0}\'.format(downscaled_count))\n  print(\'  equal min_size images = {0}\'.format(skipped_count))\n  print(\'  small images          = {0}\'.format(smaller_count))\n\n\ndef main():\n  args = parse_args()\n  root_dir = args.path\n\n  for (dirpath, _, filenames) in os.walk(root_dir):\n    print(\'Process {0}\'.format(dirpath))\n    filenames = sorted([os.path.join(dirpath, file) for file in filenames])\n    downscale(filenames, min_size=args.target_size, save_aspect_ratio=True)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
data/coco/add_full_image_path.py,0,"b""import argparse\nimport json\nimport os\n\nfrom tqdm import tqdm\n\n\ndef parse_args():\n  example_text = '''Usage example:\n\n   python add_full_image_path.py instances_train2017.json ~/coco/train2017 instances_train2017_full_paths.json\n   '''\n\n  parser = argparse.ArgumentParser(description='Add full image paths to an annotation in COCO format',\n                                   epilog=example_text,\n                                   formatter_class=argparse.RawDescriptionHelpFormatter)\n  parser.add_argument('annotation_path', help='Path to an annotation in COCO format')\n  parser.add_argument('images_dir', help='Path to a directory with images')\n  parser.add_argument('output_path', help='Path to an output file')\n  return parser.parse_args()\n\n\ndef add_full_path(annotation_path, images_dir, output_path):\n  with open(annotation_path) as f:\n    data = json.load(f)\n\n  for im in tqdm(data['images'], total=len(data['images'])):\n    im['image'] = os.path.join(images_dir, im['file_name'])\n\n  with open(output_path, 'w') as f:\n    json.dump(data, f)\n\n\ndef main():\n  args = parse_args()\n  add_full_path(args.annotation_path, args.images_dir, args.output_path)\n\n\nif __name__ == '__main__':\n  main()"""
data/synthetic_chinese_license_plates/make_train_val_split.py,0,"b""import os\nfrom random import shuffle, seed\nimport argparse\n\nSEED = 33\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description='Make train/val split of original annotation.')\n  parser.add_argument('path_to_annotation', help='Path to Synthetic Chinese License Plates annotation file.')\n  return parser.parse_args()\n\ndef main():\n  seed(SEED)\n  args = parse_args()\n\n  annotation_dir = os.path.dirname(os.path.realpath(args.path_to_annotation))\n  with open(args.path_to_annotation) as f:\n    annotations = [line.split() for line in f]\n\n  shuffle(annotations)\n  train_len = int(len(annotations) * 0.99)\n  val_len = len(annotations) - train_len\n\n  with open(os.path.join(annotation_dir, 'train'), 'w') as f:\n    for line in annotations[:train_len]:\n      f.write(os.path.join(annotation_dir, line[0]) + ' ' + line[1] + '\\n')\n\n  with open(os.path.join(annotation_dir, 'val'), 'w') as f:\n    for line in annotations[train_len:]:\n      f.write(os.path.join(annotation_dir, line[0]) + ' ' + line[1] + '\\n')\n\n  with open(os.path.join(annotation_dir, 'test_infer'), 'w') as f:\n    for line in annotations[train_len:]:\n      f.write(os.path.join(annotation_dir, line[0]) + '\\n')\n\nif __name__ == '__main__':\n  main()\n"""
pytorch_toolkit/action_recognition/main.py,0,"b'import json\nimport re\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom pprint import pprint\n\nimport torch\nfrom tensorboardX import SummaryWriter\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import WeightedRandomSampler, RandomSampler\n\nfrom action_recognition.dataset import make_dataset\nfrom action_recognition.logging import TrainingLogger, StreamHandler, TensorboardHandler, CSVHandler\nfrom action_recognition.loss import create_criterion\nfrom action_recognition.model import create_model\nfrom action_recognition.options import parse_arguments\nfrom action_recognition.spatial_transforms import (\n    MEAN_STATISTICS, STD_STATISTICS, CenterCrop, Compose, CornerCrop,\n    GaussCrop, HorizontalFlip, MultiScaleCrop, Normalize, RandomHorizontalFlip,\n    Scale, ToTensor\n)\nfrom action_recognition.target_transforms import ClassLabel\nfrom action_recognition.temporal_transforms import (\n    LoopPadding, TemporalRandomCrop, TemporalStride)\nfrom action_recognition.test import test\nfrom action_recognition.train import train\nfrom action_recognition.utils import (\n    TeedStream, json_serialize, load_state,\n    create_code_snapshot, mkdir_if_not_exists, print_git_revision)\nfrom action_recognition.validation import validate\n\n\ndef export_onnx(args, model, export_name):\n    model = model.module if args.cuda else model\n    model.eval()\n\n    if hasattr(model, ""export_onnx""):\n        model.export_onnx(export_name)\n        return\n\n    param = next(model.parameters())\n    x = param.new_zeros(1, args.sample_duration, 3, args.sample_size, args.sample_size)\n\n    with torch.no_grad():\n        torch.onnx.export(model, (x,), export_name, verbose=True)\n    print(""Done"")\n\n\ndef make_normalization(args):\n    if not args.mean_norm and not args.std_norm:\n        norm_method = Normalize([0, 0, 0], [1, 1, 1])\n    elif not args.std_norm:\n        norm_method = Normalize(MEAN_STATISTICS[args.mean_dataset], [1, 1, 1])\n    else:\n        norm_method = Normalize(MEAN_STATISTICS[args.mean_dataset], STD_STATISTICS[args.mean_dataset])\n    return norm_method\n\n\ndef setup_dataset(args, train=True, val=True, test=True):\n    temporal_stride_size = args.temporal_stride\n    sample_duration = args.sample_duration * temporal_stride_size\n\n    normalization = make_normalization(args)\n\n    train_spatial_transform = [Compose([\n        Scale(args.sample_size * 8 // 7),\n        MultiScaleCrop((args.sample_size, args.sample_size), args.scales),\n        # PhotometricDistort(),\n        ToTensor(args.norm_value),\n        normalization,\n    ])]\n    if args.hflip:\n        train_spatial_transform[0].transforms.insert(1, RandomHorizontalFlip())\n\n    temporal_stride = TemporalStride(temporal_stride_size)\n    train_temporal_transform = Compose(\n        [temporal_stride, TemporalRandomCrop(sample_duration / temporal_stride_size)])\n    train_target_transform = ClassLabel()\n    train_data = make_dataset(args, \'training\', train_spatial_transform, train_temporal_transform,\n                              train_target_transform) if train else None\n\n    # validation data\n    val_spatial_transform = [Compose([\n        Scale(args.sample_size),\n        CenterCrop(args.sample_size),\n        ToTensor(args.norm_value),\n        normalization\n    ])]\n    val_temporal_transform = Compose([temporal_stride, LoopPadding(sample_duration / temporal_stride_size)])\n    val_target_transform = ClassLabel()\n    val_data = make_dataset(args, \'validation\', val_spatial_transform, val_temporal_transform,\n                            val_target_transform) if val else None\n\n    # test data\n    test_spatial_transform = [Compose([\n        Scale(int(args.sample_size / args.scale_in_test)),\n        CornerCrop(args.sample_size, args.crop_position_in_test),\n        ToTensor(args.norm_value), normalization\n    ])]\n\n    if args.tta:\n        test_spatial_transform = []\n\n        for i in range(5):\n            test_spatial_transform.append(Compose([\n                Scale(int(args.sample_size / args.scale_in_test)),\n                GaussCrop(args.sample_size),\n                ToTensor(args.norm_value), normalization\n            ]))\n            test_spatial_transform.append(Compose([\n                Scale(int(args.sample_size / args.scale_in_test)),\n                GaussCrop(args.sample_size),\n                HorizontalFlip(),\n                ToTensor(args.norm_value), normalization\n            ]))\n\n    test_temporal_transform = Compose([temporal_stride, LoopPadding(sample_duration / temporal_stride_size)])\n    test_target_transform = ClassLabel()\n\n    test_data = make_dataset(args, \'testing\', test_spatial_transform, test_temporal_transform,\n                             test_target_transform) if test else None\n\n    return train_data, val_data, test_data\n\n\ndef setup_solver(args, parameters):\n    if args.optimizer == \'adam\':\n        optimizer = optim.Adam(parameters, lr=args.learning_rate, weight_decay=args.weight_decay,\n                               eps=1e-3 if args.fp16 else 1e-8)\n    else:  # args.optimizer == \'sgd\'\n        optimizer = optim.SGD(parameters, lr=args.learning_rate, weight_decay=args.weight_decay,\n                              momentum=0.9, nesterov=args.nesterov)\n\n    if args.scheduler == \'plateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=args.lr_patience, threshold=0.01, cooldown=0,\n                                                   threshold_mode=\'abs\', mode=\'max\', factor=args.gamma)\n    else:  # args.scheduler == \'step\':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.gamma)\n\n    return optimizer, scheduler\n\n\ndef setup_logging(args):\n    logger = TrainingLogger()\n\n    # Create handlers\n    logger.register_handler(""val_batch"", StreamHandler(prefix=\'Val: \', scope=\'batch\'))\n    logger.register_handler(""val_epoch"", StreamHandler(fmt=""* {epoch} epochs done:  {values}"", scope=\'epoch\',\n                                                       display_instant=False))\n\n    logger.register_handler(""test_std"", StreamHandler(fmt=""Testing: [{step}/{total}]\\t{values}"",\n                                                      scope=\'batch\'))\n    logger.register_handler(""test_end"", StreamHandler(fmt=""* Testing results: {values}"", scope=\'epoch\',\n                                                      display_instant=False))\n    logger.register_handler(""train_epoch"", StreamHandler(fmt=""* Train epoch {epoch} done:  {values}"", scope=\'epoch\'))\n    logger.register_handler(""train_batch"", StreamHandler(prefix=\'Train: \', scope=\'batch\'))\n\n    logger.register_handler(""tb"", TensorboardHandler(scope=\'epoch\', summary_writer=args.writer))\n    logger.register_handler(""tb_global"", TensorboardHandler(scope=\'global\', summary_writer=args.writer))\n\n    logger.register_handler(""val_csv"", CSVHandler(scope=\'epoch\', csv_path=(args.result_path / \'val.csv\'),\n                                                  index_col=\'epoch\'))\n    logger.register_handler(""train_csv"", CSVHandler(scope=\'epoch\', csv_path=(args.result_path / \'train.csv\'),\n                                                    index_col=\'epoch\'))\n\n    # Create logged values\n    logger.register_value(""train/acc"", [\'train_batch\', \'tb_global\'], average=True, display_name=\'clip\')\n    logger.register_value(""train/loss"", [\'train_batch\', \'tb_global\'], average=True, display_name=\'loss\')\n    logger.register_value(""train/kd_loss"", [\'train_batch\', \'tb_global\'], average=True, display_name=\'loss\')\n    logger.register_value(""train/epoch_acc"", [\'train_epoch\', \'tb\', \'train_csv\'], display_name=\'clip\')\n    logger.register_value(""train/epoch_loss"", [\'train_epoch\', \'tb\', \'train_csv\'], display_name=\'loss\')\n    logger.register_value_group(""lr/.*"", [\'tb\'])\n\n    logger.register_value(""time/train_data"", [\'train_batch\'], average=True, display_name=\'data time\')\n    logger.register_value(""time/train_step"", [\'train_batch\'], average=True, display_name=\'time\')\n    logger.register_value(""time/train_epoch"", [\'train_epoch\'], display_name=\'Train epoch time\')\n\n    logger.register_value(""val/acc"", [\'val_batch\', \'val_epoch\', \'tb\', \'val_csv\'], average=True, display_name=\'clip\')\n    logger.register_value(""val/video"", [\'val_batch\', \'val_epoch\', \'tb\', \'val_csv\'], average=False, display_name=\'video\')\n    logger.register_value(""val/loss"", [\'val_batch\', \'tb\', \'val_csv\'], average=True, display_name=\'loss\')\n    logger.register_value(""val/generalization_error"", [\'val_epoch\', \'tb\', \'val_csv\'],\n                          display_name=\'Train Val accuracy gap\')\n\n    logger.register_value(""time/val_data"", [\'val_batch\'], average=True, display_name=\'data time\')\n    logger.register_value(""time/val_step"", [\'val_batch\'], average=True, display_name=\'time\')\n    logger.register_value(""time/val_epoch"", [\'val_epoch\'], average=False, display_name=\'Validation time\')\n\n    logger.register_value(""test/acc"", [\'test_std\', \'test_end\', \'tb\'], average=True, display_name=\'clip\')\n    logger.register_value(""test/video"", [\'test_std\', \'test_end\', \'tb\'], average=False, display_name=\'video\')\n\n    return logger\n\n\ndef log_configuration(args):\n    print(""ARGV:"", ""-"" * 80, sep=\'\\n\')\n    pprint(sys.argv)\n    print()\n    print(""CONFIG: "", ""-"" * 80, sep=\'\\n\')\n    pprint(vars(args))\n    print_git_revision()\n    print()\n\n\ndef log_training_setup(model, train_data, val_data, optimizer, scheduler):\n    print(""Model:"", model)\n    print(""Train spatial transforms: "", train_data.spatial_transform)\n    print(""Train temporal transforms: "", train_data.temporal_transform)\n    print(""Val spatial transforms: "", val_data.spatial_transform)\n    print(""Val temporal transforms: "", val_data.temporal_transform)\n    print(""Optimizer: "", optimizer)\n    print(""Scheduler: "", scheduler)\n    print(""-"" * 89)\n\n\ndef find_latest_checkpoint(result_path):\n    latest_found = -1\n    latest_path = None\n    for ckpt_path in (result_path / \'checkpoints\').iterdir():\n        ckpt_name = ckpt_path.name\n\n        match = re.match(r"".*_(\\d+)\\..*"", ckpt_name)\n        if match and int(match.group(1)) > latest_found:\n            latest_found = int(match.group(1))\n            latest_path = ckpt_path\n\n    return latest_path\n\n\ndef prepare_result_dir(result_path):\n    result_path = Path(result_path)\n    mkdir_if_not_exists(result_path)\n\n    # find first directory suffix\n    # if directory already ends-up with numeric suffix, use it as result path.\n    if not re.match(r\'\\d+\', result_path.parts[-1]):\n\n        files = [str(f.name) for f in result_path.iterdir()]\n        i = 1\n        while str(i) in files:\n            i += 1\n        result_path = result_path / str(i)\n        result_path.mkdir()\n    # create aux dirs\n    mkdir_if_not_exists(result_path / \'tb\')\n    mkdir_if_not_exists(result_path / \'checkpoints\')\n\n    return result_path\n\n\ndef configure_paths(args):\n    relative_paths = (\'video_path\', \'annotation_path\', \'video_path\', \'flow_path\', \'resume_path\', \'pretrain_path\')\n    if args.root_path:\n        # make paths relative to the args.root_path\n        for path in relative_paths:\n            arg_path = getattr(args, path, None)\n            if arg_path:\n                setattr(args, path, args.root_path / arg_path)\n\n    # create directory for storing results (checkpoints, logs, etc.)\n    args.result_path = prepare_result_dir(args.result_path)\n    # try resume training from latest checkpoint in a result dir\n    if args.try_resume and not args.resume_path:\n        args.resume_path = find_latest_checkpoint(args.result_path)\n\n\ndef configure_dataset(args):\n    dataset = args.dataset\n    if dataset in (\'hmdb51\', \'ucf101\'):\n        dataset += \'_\' + str(getattr(args, \'split\', 1))\n\n    config_path = Path(__file__).parent / \'datasets\' / ""{}.json"".format(dataset)\n    if args.dataset_config:\n        config_path = Path(args.dataset_config)\n    with config_path.open() as fp:\n        dataset_config = json.load(fp)\n    if \'flow_path\' not in dataset_config:\n        args.flow_path = None\n\n    # copy options from dataset config\n    for k, v in dataset_config.items():\n        if not hasattr(args, k) or not getattr(args, k):\n            setattr(args, k, v)\n\n\ndef configure():\n    args = parse_arguments()\n\n    configure_dataset(args)\n    configure_paths(args)\n\n    args.scales = [args.initial_scale]\n    for i in range(1, args.n_scales):\n        args.scales.append(args.scales[-1] * args.scale_step)\n    args.arch = args.model\n\n    with (args.result_path / \'opts.json\').open(\'w\') as opt_file:\n        json.dump(vars(args), opt_file, default=json_serialize)\n\n    args.tee = TeedStream(args.result_path / ""output.log"")\n\n    args.time_suffix = datetime.now().strftime(""%d%m%H%M"")\n    tb_path = args.result_path / ""tb""\n\n    args.writer = SummaryWriter(tb_path.as_posix())\n    args.device = torch.device(""cuda"" if args.cuda else ""cpu"")\n\n    create_code_snapshot(Path(__file__).parent, args.result_path / ""snapshot.tgz"")\n    torch.manual_seed(args.manual_seed)\n\n    args.logger = setup_logging(args)\n    return args\n\n\ndef main():\n    args = configure()\n    log_configuration(args)\n\n    model, parameters = create_model(args, args.model, pretrain_path=args.pretrain_path)\n    optimizer, scheduler = setup_solver(args, parameters)\n\n    if args.resume_path:\n        print(\'loading checkpoint {}\'.format(args.resume_path))\n        checkpoint = torch.load(str(args.resume_path))\n        load_state(model, checkpoint[\'state_dict\'])\n\n        if args.resume_train:\n            args.begin_epoch = checkpoint[\'epoch\']\n            if not args.train and checkpoint.get(\'optimizer\') is not None:\n                optimizer.load_state_dict(checkpoint[\'optimizer\'])\n\n    if args.onnx:\n        export_onnx(args, model, args.onnx)\n        return\n\n    criterion = create_criterion(args)\n    train_data, val_data, test_data = setup_dataset(args, args.train, args.val, args.test)\n\n    if args.train or args.val:\n        val_loader = torch.utils.data.DataLoader(\n            val_data,\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=args.n_threads,\n            pin_memory=True,\n            drop_last=False\n        )\n\n    if args.train:\n        if args.weighted_sampling:\n            class_weights = getattr(args, \'class_weights\', None)\n            sampler = WeightedRandomSampler(train_data.get_sample_weights(class_weights), len(train_data))\n        else:\n            if len(train_data) < args.batch_size:\n                sampler = RandomSampler(train_data, replacement=True, num_samples=args.batch_size)\n            else:\n                sampler = RandomSampler(train_data, replacement=False)\n\n        train_loader = torch.utils.data.DataLoader(\n            train_data,\n            batch_size=args.batch_size,\n            sampler=sampler,\n            num_workers=args.n_threads,\n            pin_memory=True,\n            drop_last=args.sync_bn\n        )\n\n        log_training_setup(model, train_data, val_data, optimizer, scheduler)\n\n        train(args, model, train_loader, val_loader, criterion, optimizer, scheduler, args.logger)\n\n    if not args.train and args.val:\n        with torch.no_grad():\n            validate(args, args.begin_epoch, val_loader, model, criterion, args.logger)\n\n    if args.test:\n        test_loader = torch.utils.data.DataLoader(\n            test_data,\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=args.n_threads,\n            pin_memory=True\n        )\n\n        with torch.no_grad():\n            with args.logger.scope():\n                test(args, test_loader, model, args.logger)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/action_recognition/model_summary.py,0,"b'from collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\n\nfrom action_recognition.model import create_model\nfrom action_recognition.models.modules.self_attention import (\n    MultiHeadAttention, ScaledDotProductAttention\n)\nfrom action_recognition.options import parse_arguments\n\n\nclass LayerStatistic(object):\n    def __init__(self):\n        self.flops = 0\n        self.count = 0\n\n\nclass FlopsCounter(object):\n    def __init__(self, types, count_fn):\n        self.types = types\n        self.count_fn = count_fn\n\n\nclass Layer:\n    def __init__(self):\n        self.flops = 0\n        self.params = 0\n        self.module = None\n        self.inputs = None\n        self.outputs = None\n        self.type = None\n\n        self.name = None\n        self.out_edges = []\n\n\ncounter_fns = {}\n\n\ndef count_flops_impl(layer_types, key=None):\n    if key is None:\n        type_ = layer_types[0] if isinstance(layer_types, tuple) else layer_types\n        key = type_.__name__\n\n    def decorator(counter):\n        counter_fns[key] = FlopsCounter(layer_types, counter)\n\n        def wrap(module, input, output):\n            counter(module, input, output)\n\n        return wrap\n\n    return decorator\n\n\n@count_flops_impl(torch.nn.Conv2d)\ndef count_flops_conv2d(module, input, output):\n    _, _, output_h, output_w = output.size()\n    output_c, input_c, kernel_w, kernel_h = module.weight.data.size()\n\n    flops = (kernel_w * kernel_h * input_c + 1) * output_c * output_h * output_w / module.groups\n\n    params = module.weight.numel()\n    if module.bias is not None:\n        params += module.bias.numel()\n    return flops, params\n\n\n@count_flops_impl(torch.nn.Conv3d)\ndef count_flops_conv3d(module, input, output):\n    _, _, output_t, output_h, output_w = output.size()\n    output_c, input_c, kernel_t, kernel_w, kernel_h = module.weight.data.size()\n\n    flops = (kernel_t * kernel_w * kernel_h * input_c + 1) * output_c * output_h * output_w * output_t\n\n    params = module.weight.numel()\n    if module.bias is not None:\n        params += module.bias.numel()\n    return flops, params\n\n\n@count_flops_impl(torch.nn.BatchNorm2d)\ndef count_flops_bn2d(module, input, output):\n    _, output_c, output_h, output_w = output.size()\n\n    flops = output_c * output_h * output_w\n\n    params = module.weight.numel()\n    if module.bias is not None:\n        params += module.bias.numel()\n    return flops, params\n\n\n@count_flops_impl(torch.nn.BatchNorm3d)\ndef count_flops_bn3d(module, input, output):\n    _, output_t, output_c, output_h, output_w = output.size()\n\n    flops = output_c * output_h * output_w * output_t\n\n    params = module.weight.numel()\n    if module.bias is not None:\n        params += module.bias.numel()\n    return flops, params\n\n\n@count_flops_impl(torch.nn.MaxPool2d)\ndef count_flops_maxpool2d(module, input, output):\n    _, output_c, output_h, output_w = output.size()\n    kernel_w, kernel_h = module.kernel_size, module.kernel_size\n\n    flops = kernel_w * kernel_h * output_c * output_h * output_w\n\n    return flops, 0\n\n\n@count_flops_impl(torch.nn.Conv1d)\ndef count_flops_conv1d(module, input, output):\n    _, _, output_w = output.size()\n    output_c, input_c, kernel_w = module.weight.data.size()\n\n    flops = (kernel_w * input_c + 1) * output_c * output_w\n\n    params = module.weight.numel()\n    if module.bias is not None:\n        params += module.bias.numel()\n    return flops, params\n\n\n@count_flops_impl(torch.nn.Linear)\ndef count_flops_fc(module, inputs, output):\n    flops = (module.in_features + 1) * module.out_features * np.prod(inputs[0].shape[:-1])\n    params = module.weight.data.numel() + module.bias.data.numel()\n\n    return flops, params\n\n\n@count_flops_impl(torch.nn.ReLU)\ndef count_flops_relu(module, input, output):\n    flops = np.prod(output.shape[1:])\n    return flops, 0\n\n\n@count_flops_impl(ScaledDotProductAttention)\ndef count_flops_attention(module, inputs, output):\n    q, k, v = inputs\n    # does not count softmax\n    # 2 x bmm only\n    w_flops = q.size(0) * q.size(1) * q.size(2) * k.size(1)\n    attend_flops = 2 * q.size(0) * q.size(1) * k.size(1) * v.size(1)\n    flops = w_flops + attend_flops\n    return flops, 0\n\n\n@count_flops_impl(MultiHeadAttention)\ndef count_flops_mh_attention(module, inputs, output):\n    q, k, v = inputs\n\n    # does not count inner attention layer\n    q_t_flops = module.w_qs.size(0) * module.w_qs.size(1) * q.size(1) * module.n_head\n    k_t_flops = module.w_ks.size(0) * module.w_ks.size(1) * k.size(1) * module.n_head\n    v_t_flops = module.w_vs.size(0) * module.w_vs.size(1) * v.size(1) * module.n_head\n\n    flops = q_t_flops + k_t_flops + v_t_flops\n    params = module.w_qs.numel() + module.w_ks.numel() + module.w_vs.numel()\n    return flops, params\n\n\n@count_flops_impl(torch.nn.LSTM)\ndef count_flops_LSTM(module, input, output):\n    num_layers = module.num_layers\n    input_size = module.input_size\n    hidden_size = module.hidden_size\n    sigmoid_flops = 1\n    tanh_flops = 1\n    i_t_flops = (hidden_size * input_size) + hidden_size + (hidden_size * hidden_size) + hidden_size + (\n            sigmoid_flops * hidden_size)\n    f_t_flops = i_t_flops\n    o_t_flops = i_t_flops\n    g_t_flops = (hidden_size * input_size) + hidden_size + (hidden_size * hidden_size) + hidden_size + (\n            tanh_flops * hidden_size)\n    c_t_flops = 2 * hidden_size\n    h_t_flops = hidden_size * tanh_flops + hidden_size\n\n    flops = i_t_flops + f_t_flops + g_t_flops + o_t_flops + c_t_flops + h_t_flops\n    flops *= num_layers\n\n    params = 0\n    for k, v in module._parameters.items():\n        params += np.prod(v.shape)\n\n    return flops, params\n\n\nlayer_summary = defaultdict(LayerStatistic)\n\nall_layers = []\nnamed_layers = {}\n\nmodule_to_layer = {}\n\n\ndef compute_flops(module, inputs, outputs):\n    for key, counter in counter_fns.items():\n        if isinstance(module, counter.types):\n            return counter.count_fn(module, inputs, outputs)\n    return 0, 0\n\n\ndef compute_layer_statistics_hook(module, input, output):\n    new_layer = Layer()\n    new_layer.module = module\n    new_layer.inputs = input\n    new_layer.outputs = output\n    flops, params = compute_flops(module, input, output)\n    new_layer.flops = flops\n    new_layer.params = params\n\n    if isinstance(module, nn.Conv2d):\n        new_layer.type = ""Conv2d ({})"".format(""x"".join(str(i) for i in module.kernel_size))\n    else:\n        new_layer.type = type(module).__name__\n\n    if new_layer.type not in (\'Identity\', \'Dropout\', \'BasicBlock\', \'Sequential\'):\n        all_layers.append(new_layer)\n\n\ndef restore_module_names(model: torch.nn.Module):\n    for name, module in model.named_modules():\n        for layer in all_layers:\n            if module is layer.module:\n                layer.name = name\n                named_layers[name] = layer\n\n                module_to_layer[module] = layer\n\n    # connect edges\n    for module, layer in module_to_layer.items():\n        for name, children in module.named_children():\n            if children not in module_to_layer:\n                continue\n\n            if module_to_layer[children].inputs is layer.outputs:\n                layer.out_edges.append(module_to_layer[children])\n\n\ndef human_readable_fmt(value):\n    if value > 1e9:\n        return ""{:.2f}G"".format((value / 1e9))\n    if value > 1e6:\n        return ""{:.2f}M"".format((value / 1e6))\n    return ""{:.2f}M"".format((value / 1e6))\n\n\ndef print_statistics():\n    layers_table = []\n\n    for layer in all_layers:\n        layers_table.append({\n            \'name\': layer.name,\n            \'type\': layer.type,\n            \'flops\': layer.flops,\n            \'input_shape\': "", "".join(str(tuple(in_.shape)) for in_ in layer.inputs if isinstance(in_, torch.Tensor)),\n            \'params\': layer.params\n        })\n\n    data_frame = pd.DataFrame(layers_table)\n\n    total_flops = data_frame.flops.sum()\n    total_params = data_frame.params.sum()\n\n    data_frame[\'flops_\'] = data_frame.flops.transform(human_readable_fmt)\n    data_frame[\'params_\'] = data_frame.params.transform(human_readable_fmt)\n    data_frame[\'flops%\'] = data_frame.flops / total_flops * 100\n    data_frame_ = data_frame[[\'name\', \'type\', \'flops_\', \'flops%\', \'params_\', \'input_shape\']]\n\n    with pd.option_context(\'display.max_rows\', None, \'display.max_columns\', 10, \'display.max_colwidth\', 60,\n                           \'display.width\', 320, \'display.float_format\', lambda f: ""{:.2f}"".format(f)):\n        print(data_frame_)\n\n    print(""Total Flops: "", human_readable_fmt(total_flops))\n    print(""Total params: "", human_readable_fmt(total_params))\n\n\ndef main():\n    args = parse_arguments()\n    net, _ = create_model(args, args.model)\n    net = net.module\n    net.cpu()\n\n    if net is None:\n        return\n    net.eval()\n\n    h, w = args.sample_size, args.sample_size\n    var = torch.randn(1, args.sample_duration, 3, h, w).to(\'cpu\')\n\n    net.apply(lambda m: m.register_forward_hook(compute_layer_statistics_hook))\n\n    out = net(var)\n\n    restore_module_names(net)\n    print_statistics()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/action_recognition/vtn_demo.py,0,"b'import sys\nimport time\nfrom argparse import ArgumentParser\nfrom collections import deque\nfrom copy import deepcopy\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom action_recognition.model import create_model\nfrom action_recognition.options import add_input_args\nfrom action_recognition.spatial_transforms import (CenterCrop, Compose,\n                                                   Normalize, Scale, ToTensor, MEAN_STATISTICS, STD_STATISTICS)\nfrom action_recognition.utils import load_state, generate_args\n\nTEXT_COLOR = (255, 255, 255)\nTEXT_FONT_FACE = cv2.FONT_HERSHEY_DUPLEX\nTEXT_FONT_SIZE = 1\nTEXT_VERTICAL_INTERVAL = 45\nNUM_LABELS_TO_DISPLAY = 2\n\n\nclass TorchActionRecognition:\n    def __init__(self, encoder, checkpoint_path, num_classes=400, **kwargs):\n        model_type = ""{}_vtn"".format(encoder)\n        args, _ = generate_args(model=model_type, n_classes=num_classes, layer_norm=False, **kwargs)\n        self.args = args\n        self.model, _ = create_model(args, model_type)\n\n        self.model = self.model.module\n        self.model.eval()\n        self.model.cuda()\n\n        checkpoint = torch.load(str(checkpoint_path))\n        load_state(self.model, checkpoint[\'state_dict\'])\n\n        self.preprocessing = make_preprocessing(args)\n        self.embeds = deque(maxlen=(args.sample_duration * args.temporal_stride))\n\n    def preprocess_frame(self, frame):\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        return self.preprocessing(frame)\n\n    def infer_frame(self, frame):\n        embedding = self._infer_embed(self.preprocess_frame(frame))\n        self.embeds.append(embedding)\n        sequence = self.get_seq()\n        return self._infer_logits(sequence)\n\n    def _infer_embed(self, frame):\n        with torch.no_grad():\n            frame_tensor = frame.unsqueeze(0).to(\'cuda\')\n            tensor = self.model.resnet(frame_tensor)\n            tensor = self.model.reduce_conv(tensor)\n            embed = F.avg_pool2d(tensor, 7)\n        return embed.squeeze(-1).squeeze(-1)\n\n    def _infer_logits(self, embeddings):\n        with torch.no_grad():\n            ys = self.model.self_attention_decoder(embeddings)\n            ys = self.model.fc(ys)\n            ys = ys.mean(1)\n        return ys.cpu()\n\n    def _infer_seq(self, frame):\n        with torch.no_grad():\n            result = self.model(frame.view(1, self.args.sample_duration, 3,\n                                           self.args.sample_size, self.args.sample_size).to(\'cuda\'))\n        return result.cpu()\n\n    def get_seq(self):\n        sequence = torch.stack(tuple(self.embeds), 1)\n        if self.args.temporal_stride > 1:\n            sequence = sequence[:, ::self.args.temporal_stride, :]\n\n        n = self.args.sample_duration\n        if sequence.size(1) < n:\n            num_repeats = (n - 1) // sequence.size(1) + 1\n            sequence = sequence.repeat(1, num_repeats, 1)[:, :n, :]\n\n        return sequence\n\n\ndef make_preprocessing(args):\n    return Compose([\n        Scale(args.sample_size),\n        CenterCrop(args.sample_size),\n        ToTensor(args.norm_value),\n        Normalize(MEAN_STATISTICS[args.mean_dataset], STD_STATISTICS[args.mean_dataset])\n    ])\n\n\ndef draw_rect(image, bottom_left, top_right, color=(0, 0, 0), alpha=1.):\n    xmin, ymin = bottom_left\n    xmax, ymax = top_right\n\n    image[ymin:ymax, xmin:xmax, :] = image[ymin:ymax, xmin:xmax, :] * (1 - alpha) + np.asarray(color) * alpha\n    return image\n\n\ndef render_frame(frame, probs, labels):\n    order = probs.argsort(descending=True)\n\n    status_bar_coordinates = (\n        (0, 0),  # top left\n        (650, 25 + TEXT_VERTICAL_INTERVAL * NUM_LABELS_TO_DISPLAY)  # bottom right\n    )\n\n    draw_rect(frame, status_bar_coordinates[0], status_bar_coordinates[1], alpha=0.5)\n\n    for i, imax in enumerate(order[:NUM_LABELS_TO_DISPLAY]):\n        text = \'{} - {:.1f}%\'.format(labels[imax], probs[imax] * 100)\n        text = text.upper().replace(""_"", "" "")\n        cv2.putText(frame, text, (15, TEXT_VERTICAL_INTERVAL * (i + 1)), TEXT_FONT_SIZE,\n                    TEXT_FONT_FACE, TEXT_COLOR)\n\n    return frame\n\n\ndef run_demo(model, video_cap, labels):\n    fps = float(video_cap.get(cv2.CAP_PROP_FPS))\n    print(\'fps: {}\'.format(fps))\n    if not fps:\n        fps = 30.0\n    delay = max(1, int(1000 / fps))\n    tick = time.time()\n\n    while video_cap.isOpened():\n        ok, frame = video_cap.read()\n\n        if not ok:\n            break\n\n        logits = model.infer_frame(frame)\n        probs = F.softmax(logits[0], dim=0)\n        frame = render_frame(frame, probs, labels)\n\n        tock = time.time()\n        expected_time = tick + 1 / fps\n        if tock < expected_time:\n            delay = max(1, int((expected_time - tock) * 1000))\n        tick = tock\n\n        cv2.imshow(""demo"", frame)\n        key = cv2.waitKey(delay)\n        if key == 27 or key == ord(\'q\'):\n            break\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(""--encoder"", help=""What encoder to use "", default=\'resnet34\')\n    parser.add_argument(""--checkpoint"", help=""Path to pretrained model (.pth) file"", required=True)\n    parser.add_argument(""--input-video"", type=str, help=""Path to input video"", required=True)\n    parser.add_argument(""--labels"", help=""Path to labels file (new-line separated file with label names)"", type=str,\n                        required=True)\n    add_input_args(parser)\n    args = parser.parse_args()\n\n    with open(args.labels) as fd:\n        labels = fd.read().strip().split(\'\\n\')\n\n    extra_args = deepcopy(vars(args))\n    input_data_params = set(x.dest for x in parser._action_groups[-1]._group_actions)\n    for name in list(extra_args.keys()):\n        if name not in input_data_params:\n            del extra_args[name]\n\n    model = TorchActionRecognition(args.encoder, args.checkpoint, num_classes=len(labels), **extra_args)\n    cap = cv2.VideoCapture(args.input_video)\n    run_demo(model, cap, labels)\n\n\nif __name__ == \'__main__\':\n    sys.exit(main())\n'"
pytorch_toolkit/face_recognition/__init__.py,0,b''
pytorch_toolkit/face_recognition/dump_features.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nimport argparse\nimport os\nimport os.path as osp\n\nfrom tqdm import tqdm\nimport numpy as np\nimport glog as log\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as t\n\nfrom scripts.matio import save_mat\nfrom model.common import models_backbones\nfrom datasets.megaface import MegaFace\nfrom datasets.trillion_pairs import TrillionPairs\nfrom utils.utils import load_model_state\nfrom utils.augmentation import ResizeNumpy, NumpyToTensor\n\n\ndef clean_megaface(filenames, features, noises_list_path):\n    """"""Filters megaface from outliers""""""\n    with open(noises_list_path, \'r\') as f:\n        noises_list = f.readlines()\n        noises_list = [line.strip() for line in noises_list]\n    clean_features = np.zeros((features.shape[0], features.shape[1] + 1), dtype=np.float32)\n\n    for i, filename in enumerate(tqdm(filenames)):\n        clean_features[i, 0: features.shape[1]] = features[i, :]\n        for line in noises_list:\n            if line in filename:\n                clean_features[i, features.shape[1]] = 100.0\n                break\n\n    return clean_features\n\n\ndef clean_facescrub(filenames, features, noises_list_path):\n    """"""Replaces wrong instances of identities from the Facescrub with the centroids of these identities""""""\n    clean_feature_size = features.shape[1] + 1\n    with open(noises_list_path, \'r\') as f:\n        noises_list = f.readlines()\n        noises_list = [osp.splitext(line.strip())[0] for line in noises_list]\n    clean_features = np.zeros((features.shape[0], clean_feature_size), dtype=np.float32)\n\n    centroids = {}\n    for i, filename in enumerate(tqdm(filenames)):\n        clean_features[i, 0: features.shape[1]] = features[i, :]\n        id_name = osp.basename(filename).split(\'_\')[0]\n        if not id_name in centroids:\n            centroids[id_name] = np.zeros(clean_feature_size, dtype=np.float32)\n        centroids[id_name] += clean_features[i, :]\n\n    for i, file_path in enumerate(tqdm(filenames)):\n        filename = osp.basename(file_path)\n        for line in noises_list:\n            if line in filename.replace(\' \', \'_\'):\n                id_name = filename.split(\'_\')[0]\n                clean_features[i, :] = centroids[id_name] + np.random.uniform(-0.001, 0.001, clean_feature_size)\n                clean_features[i, :] /= np.linalg.norm(clean_features[i, :])\n                break\n\n    return clean_features\n\n\n@torch.no_grad()\ndef main(args):\n    input_filenames = []\n    output_filenames = []\n    input_dir = os.path.abspath(args.input_dir)\n    output_dir = os.path.abspath(args.output_dir)\n\n    if not args.trillion_format:\n        log.info(\'Reading info...\')\n        with open(os.path.join(args.input_dir, os.path.basename(args.input_list)), \'r\') as f:\n            lines = f.readlines()\n\n            for line in tqdm(lines):\n                info = line.strip().split(\'|\')\n                file = info[0].strip()\n                filename = os.path.join(input_dir, file)\n\n                path, _ = osp.split(filename)\n                out_folder = path.replace(input_dir, output_dir)\n                if not osp.isdir(out_folder):\n                    os.makedirs(out_folder)\n\n                landmarks = None\n                bbox = None\n\n                if len(info) > 2:\n                    landmarks = info[1].strip().split(\' \')\n                    landmarks = [float(x) for x in landmarks]\n                    bbox = info[2].strip().split(\' \')\n                    bbox = [int(float(x)) for x in bbox]\n                outname = filename.replace(input_dir, output_dir) + args.file_ending\n                input_filenames.append({\'path\': filename, \'landmarks\': landmarks, \'bbox\': bbox})\n                output_filenames += [outname]\n\n        nrof_images = len(input_filenames)\n        log.info(""Total number of images: "", nrof_images)\n        dataset = MegaFace(input_filenames)\n    else:\n        dataset = TrillionPairs(args.input_dir, osp.join(args.input_dir, \'testdata_lmk.txt\'), test_mode=True)\n        nrof_images = len(dataset)\n\n    emb_array = np.zeros((nrof_images, args.embedding_size), dtype=np.float32)\n\n    dataset.transform = t.Compose([ResizeNumpy(models_backbones[args.model]().get_input_res()),\n                                   NumpyToTensor(switch_rb=True)])\n    val_loader = DataLoader(dataset, batch_size=args.batch_size, num_workers=5, shuffle=False)\n\n    model = models_backbones[args.model](embedding_size=args.embedding_size, feature=True)\n    assert args.snap is not None\n    log.info(\'Snapshot \' + args.snap + \' ...\')\n    log.info(\'Extracting embeddings ...\')\n    model = load_model_state(model, args.snap, args.devices[0], eval_state=True)\n    model = torch.nn.DataParallel(model, device_ids=args.devices, output_device=args.devices[0])\n\n    f_output_filenames = []\n\n    with torch.cuda.device(args.devices[0]):\n        for i, data in enumerate(tqdm(val_loader), 0):\n            idxs, imgs = data[\'idx\'], data[\'img\']\n            batch_embeddings = F.normalize(model(imgs), p=2, dim=1).data.cpu().numpy()\n            batch_embeddings = batch_embeddings.reshape(batch_embeddings.shape[0], -1)\n            path_indices = idxs.data.cpu().numpy()\n\n            start_index = i*args.batch_size\n            end_index = min((i+1)*args.batch_size, nrof_images)\n            assert start_index == path_indices[0]\n            assert end_index == path_indices[-1] + 1\n            assert emb_array[start_index:end_index, :].shape == batch_embeddings.shape\n            emb_array[start_index:end_index, :] = batch_embeddings\n\n            if not args.trillion_format:\n                for index in path_indices:\n                    f_output_filenames.append(output_filenames[index])\n\n    assert len(output_filenames) == len(output_filenames)\n\n    log.info(\'Extracting features Done.\')\n\n    if args.trillion_format:\n        save_mat(args.file_ending, emb_array)\n    else:\n        if \'megaface_noises.txt\' in args.noises_list:\n            log.info(\'Cleaning Megaface features\')\n            emb_array = clean_megaface(f_output_filenames, emb_array, args.noises_list)\n        elif \'facescrub_noises.txt\' in args.noises_list:\n            log.info(\'Cleaning Facescrub features\')\n            emb_array = clean_facescrub(f_output_filenames, emb_array, args.noises_list)\n        else:\n            log.info(\'Megaface features are not cleaned up.\')\n\n        log.info(\'Saving features to files...\')\n        for i in tqdm(range(len(f_output_filenames))):\n            save_mat(f_output_filenames[i], emb_array[i, :])\n\n\ndef parse_argument(argv):\n    parser = argparse.ArgumentParser(description=\'Save embeddings to MegaFace features files\')\n    parser.add_argument(\'--model\', choices=models_backbones.keys(), type=str, default=\'rmnet\', help=\'Model type.\')\n    parser.add_argument(\'input_dir\', help=\'Path to MegaFace Features\')\n    parser.add_argument(\'output_dir\', help=\'Path to FaceScrub Features\')\n    parser.add_argument(\'--input_list\', default=\'list.txt\', type=str, required=False)\n    parser.add_argument(\'--batch_size\', type=int, default=128)\n    parser.add_argument(\'--embedding_size\', type=int, default=128)\n    parser.add_argument(\'--devices\', type=int, nargs=\'+\', default=[0], help=\'CUDA devices to use.\')\n    parser.add_argument(\'--snap\', type=str, required=True, help=\'Snapshot to evaluate.\')\n    parser.add_argument(\'--noises_list\', type=str, default=\'\', required=False, help=\'A list of the Megaface or Facescrub noises produced by insightface. \\\n                                                                        See https://github.com/deepinsight/insightface/blob/master/src/megaface/README.md\')\n    parser.add_argument(\'--file_ending\', help=\'Ending appended to original photo files. i.e.\\\n                        11084833664_0.jpg_LBP_100x100.bin => _LBP_100x100.bin\', default=\'_rmnet.bin\')\n    parser.add_argument(\'--trillion_format\', action=\'store_true\')\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_argument(sys.argv[1:]))\n'"
pytorch_toolkit/face_recognition/evaluate_landmarks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nfrom tqdm import tqdm\n\nimport glog as log\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms as t\n\nfrom nncf.config import Config\nfrom nncf.dynamic_graph import patch_torch_operators\nfrom nncf.algo_selector import create_compression_algorithm\nfrom datasets import VGGFace2, CelebA, NDG\n\nfrom model.common import models_landmarks\nfrom utils.landmarks_augmentation import Rescale, ToTensor\nfrom utils.utils import load_model_state\n\n\ndef evaluate(val_loader, model):\n    """"""Calculates average error""""""\n    total_loss = 0.\n    total_pp_error = 0.\n    failures_num = 0\n    items_num = 0\n    for _, data in enumerate(tqdm(val_loader), 0):\n        data, gt_landmarks = data[\'img\'].cuda(), data[\'landmarks\'].cuda()\n        predicted_landmarks = model(data)\n        loss = predicted_landmarks - gt_landmarks\n        items_num += loss.shape[0]\n        n_points = loss.shape[1] // 2\n        per_point_error = loss.data.view(-1, n_points, 2)\n        per_point_error = torch.norm(per_point_error, p=2, dim=2)\n        avg_error = torch.sum(per_point_error, 1) / n_points\n        eyes_dist = torch.norm(gt_landmarks[:, 0:2] - gt_landmarks[:, 2:4], p=2, dim=1).reshape(-1)\n\n        per_point_error = torch.div(per_point_error, eyes_dist.view(-1, 1))\n        total_pp_error += torch.sum(per_point_error, 0)\n\n        avg_error = torch.div(avg_error, eyes_dist)\n        failures_num += torch.nonzero(avg_error > 0.1).shape[0]\n        total_loss += torch.sum(avg_error)\n\n    return total_loss / items_num, (total_pp_error / items_num).data.cpu().numpy(), float(failures_num) / items_num\n\n\ndef start_evaluation(args):\n    """"""Launches the evaluation process""""""\n\n    if args.dataset == \'vgg\':\n        dataset = VGGFace2(args.val, args.v_list, args.v_land, landmarks_training=True)\n    elif args.dataset == \'celeb\':\n        dataset = CelebA(args.val, args.v_land, test=True)\n    else:\n        dataset = NDG(args.val, args.v_land)\n\n    if dataset.have_landmarks:\n        log.info(\'Use alignment for the train data\')\n        dataset.transform = t.Compose([Rescale((48, 48)), ToTensor(switch_rb=True)])\n    else:\n        exit()\n\n    val_loader = DataLoader(dataset, batch_size=args.val_batch_size, num_workers=4, shuffle=False, pin_memory=True)\n\n    model = models_landmarks[\'landnet\']()\n\n    assert args.snapshot is not None\n    if args.compr_config:\n        config = Config.from_json(args.compr_config)\n        compression_algo = create_compression_algorithm(model, config)\n        model = compression_algo.model\n\n    log.info(\'Testing snapshot \' + args.snapshot + \' ...\')\n    model = load_model_state(model, args.snapshot, args.device, eval_state=True)\n    model.eval()\n    cudnn.benchmark = True\n    model = torch.nn.DataParallel(model, device_ids=[args.device], )\n\n    log.info(\'Face landmarks model:\')\n    log.info(model)\n\n    avg_err, per_point_avg_err, failures_rate = evaluate(val_loader, model)\n    log.info(\'Avg RMSE error: {}\'.format(avg_err))\n    log.info(\'Per landmark RMSE error: {}\'.format(per_point_avg_err))\n    log.info(\'Failure rate: {}\'.format(failures_rate))\n    if args.compr_config and ""sparsity_level"" in compression_algo.statistics():\n        log.info(""Sparsity level: {0:.2f}"".format(compression_algo.statistics()[\'sparsity_rate_for_sparsified_modules\']))\n\n\ndef main():\n    """"""Creates a cl parser""""""\n    parser = argparse.ArgumentParser(description=\'Evaluation script for landmarks detection network\')\n    parser.add_argument(\'--device\', \'-d\', default=0, type=int)\n    parser.add_argument(\'--val_data_root\', dest=\'val\', required=True, type=str, help=\'Path to val data.\')\n    parser.add_argument(\'--val_list\', dest=\'v_list\', required=False, type=str, help=\'Path to test data image list.\')\n    parser.add_argument(\'--val_landmarks\', dest=\'v_land\', default=\'\', required=False, type=str,\n                        help=\'Path to landmarks for test images.\')\n    parser.add_argument(\'--val_batch_size\', type=int, default=1, help=\'Validation batch size.\')\n    parser.add_argument(\'--snapshot\', type=str, default=None, help=\'Snapshot to evaluate.\')\n    parser.add_argument(\'--dataset\', choices=[\'vgg\', \'celeb\', \'ngd\'], type=str, default=\'vgg\', help=\'Dataset.\')\n    parser.add_argument(\'-c\', \'--compr_config\', help=\'Path to a file with compression parameters\', required=False)\n    args = parser.parse_args()\n\n    if args.compr_config:\n        patch_torch_operators()\n\n    with torch.cuda.device(args.device):\n        start_evaluation(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/evaluate_lfw.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport datetime\nfrom functools import partial\n\nimport cv2 as cv\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as t\n\nfrom scipy.spatial.distance import cosine\nimport glog as log\nfrom tqdm import tqdm\nimport numpy as np\nfrom tensorboardX import SummaryWriter\n\nfrom nncf.config import Config\nfrom nncf.dynamic_graph import patch_torch_operators\nfrom nncf.algo_selector import create_compression_algorithm\n\nfrom datasets.lfw import LFW\nfrom utils.utils import load_model_state, get_model_parameters_number, flip_tensor\nfrom utils.augmentation import ResizeNumpy, CenterCropNumpy, NumpyToTensor\nfrom utils.face_align import FivePointsAligner\nfrom model.common import models_backbones\n\n\ndef get_subset(container, subset_bounds):\n    """"""Returns a subset of the given list with respect to the list of bounds""""""\n    subset = []\n    for bound in subset_bounds:\n        subset += container[bound[0]: bound[1]]\n    return subset\n\n\ndef get_roc(scores_with_gt, n_threshs=400):\n    """"""Computes a ROC cureve on the LFW dataset""""""\n    thresholds = np.linspace(0., 4., n_threshs)\n\n    fp_rates = []\n    tp_rates = []\n\n    for threshold in thresholds:\n        fp = 0\n        tp = 0\n        for score_with_gt in scores_with_gt:\n            predict_same = score_with_gt[\'score\'] < threshold\n            actual_same = score_with_gt[\'is_same\']\n\n            if predict_same and actual_same:\n                tp += 1\n            elif predict_same and not actual_same:\n                fp += 1\n\n        fp_rates.append(float(fp) / len(scores_with_gt) * 2)\n        tp_rates.append(float(tp) / len(scores_with_gt) * 2)\n\n    return np.array(fp_rates), np.array(tp_rates)\n\n\ndef get_auc(fprs, tprs):\n    """"""Computes AUC under a ROC curve""""""\n    sorted_fprs, sorted_tprs = zip(*sorted(zip(*(fprs, tprs))))\n    sorted_fprs = list(sorted_fprs)\n    sorted_tprs = list(sorted_tprs)\n    if sorted_fprs[-1] != 1.0:\n        sorted_fprs.append(1.0)\n        sorted_tprs.append(sorted_tprs[-1])\n    return np.trapz(sorted_tprs, sorted_fprs)\n\n\ndef save_roc(fp_rates, tp_rates, fname):\n    assert fp_rates.shape[0] == tp_rates.shape[0]\n    with open(fname + \'.txt\', \'w\') as f:\n        for i in range(fp_rates.shape[0]):\n            f.write(\'{} {}\\n\'.format(fp_rates[i], tp_rates[i]))\n\n\n@torch.no_grad()\ndef compute_embeddings_lfw(args, dataset, model, batch_size, dump_embeddings=False,\n                           pdist=lambda x, y: 1. - F.cosine_similarity(x, y), flipped_embeddings=False):\n    """"""Computes embeddings of all images from the LFW dataset using PyTorch""""""\n    val_loader = DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n    scores_with_gt = []\n    embeddings = []\n    ids = []\n\n    for batch_idx, data in enumerate(tqdm(val_loader, \'Computing embeddings\')):\n        images_1 = data[\'img1\']\n        images_2 = data[\'img2\']\n        is_same = data[\'is_same\']\n        if torch.cuda.is_available() and args.devices[0] != -1:\n            images_1 = images_1.cuda()\n            images_2 = images_2.cuda()\n        emb_1 = model(images_1)\n        emb_2 = model(images_2)\n        if flipped_embeddings:\n            images_1_flipped = flip_tensor(images_1, 3)\n            images_2_flipped = flip_tensor(images_2, 3)\n            emb_1_flipped = model(images_1_flipped)\n            emb_2_flipped = model(images_2_flipped)\n            emb_1 = (emb_1 + emb_1_flipped)*.5\n            emb_2 = (emb_2 + emb_2_flipped)*.5\n        scores = pdist(emb_1, emb_2).data.cpu().numpy()\n\n        for i, _ in enumerate(scores):\n            scores_with_gt.append({\'score\': scores[i], \'is_same\': is_same[i], \'idx\': batch_idx*batch_size + i})\n\n        if dump_embeddings:\n            id0 = data[\'id0\']\n            id1 = data[\'id1\']\n            ids.append(id0)\n            ids.append(id1)\n            to_dump_1 = emb_1.data.cpu()\n            to_dump_2 = emb_2.data.cpu()\n            embeddings.append(to_dump_1)\n            embeddings.append(to_dump_2)\n\n    if dump_embeddings:\n        total_emb = np.concatenate(embeddings, axis=0)\n        total_ids = np.concatenate(ids, axis=0)\n        log_path = \'./logs/{:%Y_%m_%d_%H_%M}\'.format(datetime.datetime.now())\n        writer = SummaryWriter(log_path)\n        writer.add_embedding(torch.from_numpy(total_emb), total_ids)\n\n    return scores_with_gt\n\n\ndef compute_embeddings_lfw_ie(args, dataset, model, batch_size=1, dump_embeddings=False,\n                              pdist=cosine, flipped_embeddings=False, lm_model=None):\n    """"""Computes embeddings of all images from the LFW dataset using Inference Engine""""""\n    assert batch_size == 1\n    scores_with_gt = []\n\n    for batch_idx, data in enumerate(tqdm(dataset, \'Computing embeddings\')):\n        images_1 = data[\'img1\']\n        images_2 = data[\'img2\']\n        if lm_model:\n            lm_input_size = tuple(lm_model.get_input_shape()[2:])\n            landmarks_1 = lm_model.forward(cv.resize(images_1, lm_input_size)).reshape(-1)\n            images_1 = FivePointsAligner.align(images_1, landmarks_1, *images_1.shape[:2], normalize=False, show=False)\n\n            landmarks_2 = lm_model.forward(cv.resize(images_2, lm_input_size)).reshape(-1)\n            images_2 = FivePointsAligner.align(images_2, landmarks_2, *images_2.shape[:2], normalize=False)\n\n        is_same = data[\'is_same\']\n        emb_1 = model.forward(images_1).reshape(-1)\n        emb_2 = model.forward(images_2).reshape(-1)\n        score = pdist(emb_1, emb_2)\n        scores_with_gt.append({\'score\': score, \'is_same\': is_same, \'idx\': batch_idx * batch_size})\n\n    return scores_with_gt\n\n\ndef compute_optimal_thresh(scores_with_gt):\n    """"""Computes an optimal threshold for pairwise face verification""""""\n    pos_scores = []\n    neg_scores = []\n    for score_with_gt in scores_with_gt:\n        if score_with_gt[\'is_same\']:\n            pos_scores.append(score_with_gt[\'score\'])\n        else:\n            neg_scores.append(score_with_gt[\'score\'])\n\n    hist_pos, bins = np.histogram(np.array(pos_scores), 60)\n    hist_neg, _ = np.histogram(np.array(neg_scores), bins)\n\n    intersection_bins = []\n\n    for i in range(1, len(hist_neg)):\n        if hist_pos[i - 1] >= hist_neg[i - 1] and 0.05 < hist_pos[i] <= hist_neg[i]:\n            intersection_bins.append(bins[i])\n\n    if not intersection_bins:\n        intersection_bins.append(0.5)\n\n    return np.mean(intersection_bins)\n\n\ndef evaluate(args, dataset, model, compute_embeddings_fun, val_batch_size=16,\n             dump_embeddings=False, roc_fname=\'\', snap_name=\'\', verbose=True, show_failed=False):\n    """"""Computes the LFW score of given model""""""\n    if verbose and isinstance(model, torch.nn.Module):\n        log.info(\'Face recognition model config:\')\n        log.info(model)\n        log.info(\'Number of parameters: {}\'.format(get_model_parameters_number(model)))\n\n    scores_with_gt = compute_embeddings_fun(args, dataset, model, val_batch_size, dump_embeddings)\n    num_pairs = len(scores_with_gt)\n\n    subsets = []\n    for i in range(10):\n        lower_bnd = i * num_pairs // 10\n        upper_bnd = (i + 1) * num_pairs // 10\n        subset_test = [(lower_bnd, upper_bnd)]\n        subset_train = [(0, lower_bnd), (upper_bnd, num_pairs)]\n        subsets.append({\'test\': subset_test, \'train\': subset_train})\n\n    same_scores = []\n    diff_scores = []\n    val_scores = []\n    threshs = []\n    mean_fpr = np.zeros(400)\n    mean_tpr = np.zeros(400)\n    failed_pairs = []\n\n    for subset in tqdm(subsets, \'{} evaluation\'.format(snap_name), disable=not verbose):\n        train_list = get_subset(scores_with_gt, subset[\'train\'])\n        optimal_thresh = compute_optimal_thresh(train_list)\n        threshs.append(optimal_thresh)\n\n        test_list = get_subset(scores_with_gt, subset[\'test\'])\n        same_correct = 0\n        diff_correct = 0\n        pos_pairs_num = neg_pairs_num = len(test_list) // 2\n\n        for score_with_gt in test_list:\n            if score_with_gt[\'score\'] < optimal_thresh and score_with_gt[\'is_same\']:\n                same_correct += 1\n            elif score_with_gt[\'score\'] >= optimal_thresh and not score_with_gt[\'is_same\']:\n                diff_correct += 1\n\n            if score_with_gt[\'score\'] >= optimal_thresh and score_with_gt[\'is_same\']:\n                failed_pairs.append(score_with_gt[\'idx\'])\n            if score_with_gt[\'score\'] < optimal_thresh and not score_with_gt[\'is_same\']:\n                failed_pairs.append(score_with_gt[\'idx\'])\n\n        same_scores.append(float(same_correct) / pos_pairs_num)\n        diff_scores.append(float(diff_correct) / neg_pairs_num)\n        val_scores.append(0.5*(same_scores[-1] + diff_scores[-1]))\n\n        fprs, tprs = get_roc(test_list, mean_fpr.shape[0])\n        mean_fpr = mean_fpr + fprs\n        mean_tpr = mean_tpr + tprs\n\n    mean_fpr /= 10\n    mean_tpr /= 10\n\n    if roc_fname:\n        save_roc(mean_tpr, mean_fpr, roc_fname)\n\n    same_acc = np.mean(same_scores)\n    diff_acc = np.mean(diff_scores)\n    overall_acc = np.mean(val_scores)\n    auc = get_auc(mean_fpr, mean_tpr)\n\n    if show_failed:\n        log.info(\'Number of misclassified pairs: {}\'.format(len(failed_pairs)))\n        for pair in failed_pairs:\n            dataset.show_item(pair)\n\n    if verbose:\n        log.info(\'Accuracy/Val_same_accuracy mean: {0:.4f}\'.format(same_acc))\n        log.info(\'Accuracy/Val_diff_accuracy mean: {0:.4f}\'.format(diff_acc))\n        log.info(\'Accuracy/Val_accuracy mean: {0:.4f}\'.format(overall_acc))\n        log.info(\'Accuracy/Val_accuracy std dev: {0:.4f}\'.format(np.std(val_scores)))\n        log.info(\'AUC: {0:.4f}\'.format(auc))\n        log.info(\'Estimated threshold: {0:.4f}\'.format(np.mean(threshs)))\n        if args.compr_config and ""sparsity_level"" in compression_algo.statistics():\n            log.info(\n                ""Sparsity level: {0:.2f}"".format(compression_algo.statistics()[\'sparsity_rate_for_sparsified_modules\']))\n    return same_acc, diff_acc, overall_acc, auc\n\n\ndef load_test_dataset(arguments):\n    """"""Loads and configures the LFW dataset""""""\n    input_size = models_backbones[arguments.model]().get_input_res()\n    lfw = LFW(arguments.val, arguments.v_list, arguments.v_land)\n    assert lfw.use_landmarks\n    log.info(\'Using landmarks for the LFW images.\')\n    transform = t.Compose([ResizeNumpy(input_size),\n                           NumpyToTensor(switch_rb=True)])\n    lfw.transform = transform\n    return lfw, partial(compute_embeddings_lfw, flipped_embeddings=arguments.flipped_emb)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Evaluation script for Face Recognition in PyTorch\')\n    parser.add_argument(\'--devices\', type=int, nargs=\'+\', default=[0], help=\'CUDA devices to use.\')\n    parser.add_argument(\'--embed_size\', type=int, default=128, help=\'Size of the face embedding.\')\n    parser.add_argument(\'--val_data_root\', dest=\'val\', required=True, type=str, help=\'Path to validation data.\')\n    parser.add_argument(\'--val_list\', dest=\'v_list\', required=True, type=str, help=\'Path to train data image list.\')\n    parser.add_argument(\'--val_landmarks\', dest=\'v_land\', default=\'\', required=False, type=str,\n                        help=\'Path to landmarks for the test images.\')\n    parser.add_argument(\'--val_batch_size\', type=int, default=8, help=\'Validation batch size.\')\n    parser.add_argument(\'--snap\', type=str, required=False, help=\'Snapshot to evaluate.\')\n    parser.add_argument(\'--roc_fname\', type=str, default=\'\', help=\'ROC file.\')\n    parser.add_argument(\'--dump_embeddings\', action=\'store_true\', help=\'Dump embeddings to summary writer.\')\n    parser.add_argument(\'--dist\', choices=[\'l2\', \'cos\'], type=str, default=\'cos\', help=\'Distance.\')\n    parser.add_argument(\'--flipped_emb\', action=\'store_true\', help=\'Flipped embedding concatenation trick.\')\n    parser.add_argument(\'--show_failed\', action=\'store_true\', help=\'Show misclassified pairs.\')\n    parser.add_argument(\'--model\', choices=models_backbones.keys(), type=str, default=\'rmnet\', help=\'Model type.\')\n    parser.add_argument(\'--engine\', choices=[\'pt\', \'ie\'], type=str, default=\'pt\', help=\'Framework to use for eval.\')\n\n    # IE-related options\n    parser.add_argument(\'--fr_model\', type=str, required=False)\n    parser.add_argument(\'--lm_model\', type=str, required=False)\n    parser.add_argument(\'-pp\', \'--plugin_dir\', type=str, default=None, help=\'Path to a plugin folder\')\n    parser.add_argument(\'-c\', \'--compr_config\', help=\'Path to a file with compression parameters\', required=False)\n    args = parser.parse_args()\n\n    if args.engine == \'pt\':\n        assert args.snap is not None, \'To evaluate PyTorch snapshot, please, specify --snap option.\'\n\n        if args.compr_config:\n            patch_torch_operators()\n\n        with torch.cuda.device(args.devices[0]):\n            data, embeddings_fun = load_test_dataset(args)\n            model = models_backbones[args.model](embedding_size=args.embed_size, feature=True)\n\n            if args.compr_config:\n                config = Config.from_json(args.compr_config)\n                compression_algo = create_compression_algorithm(model, config)\n                model = compression_algo.model\n\n            model = load_model_state(model, args.snap, args.devices[0])\n            evaluate(args, data, model, embeddings_fun, args.val_batch_size, args.dump_embeddings,\n                     args.roc_fname, args.snap, True, args.show_failed)\n\n            if args.compr_config and ""sparsity_level"" in compression_algo.statistics():\n                log.info(""Sparsity level: {0:.2f}"".format(\n                    compression_algo.statistics()[\'sparsity_rate_for_sparsified_modules\']))\n    else:\n        from utils.ie_tools import load_ie_model\n\n        assert args.fr_model is not None, \'To evaluate IE model, please, specify --fr_model option.\'\n        fr_model = load_ie_model(args.fr_model, \'CPU\', args.plugin_dir)\n        lm_model = None\n        if args.lm_model:\n            lm_model = load_ie_model(args.lm_model, \'CPU\', args.plugin_dir)\n        input_size = tuple(fr_model.get_input_shape()[2:])\n\n        lfw = LFW(args.val, args.v_list, args.v_land)\n        if not lfw.use_landmarks or lm_model:\n            lfw.transform = t.Compose([ResizeNumpy(220), CenterCropNumpy(input_size)])\n            lfw.use_landmarks = False\n        else:\n            log.info(\'Using landmarks for the LFW images.\')\n            lfw.transform = t.Compose([ResizeNumpy(input_size)])\n\n        evaluate(args, lfw, fr_model, partial(compute_embeddings_lfw_ie, lm_model=lm_model), val_batch_size=1,\n                 dump_embeddings=False, roc_fname=\'\', snap_name=\'\', verbose=True, show_failed=False)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/train.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport datetime\nimport os.path as osp\nimport os\nfrom pprint import pformat\n\nimport glog as log\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms as t\nfrom tensorboardX import SummaryWriter\n\nfrom nncf.config import Config\nfrom nncf.dynamic_graph import patch_torch_operators\nfrom nncf.algo_selector import create_compression_algorithm\nfrom datasets import LFW, VGGFace2, MSCeleb1M, IMDBFace, TrillionPairs\n\nfrom losses.am_softmax import AMSoftmaxLoss\nfrom losses.metric_losses import MetricLosses\nfrom evaluate_lfw import evaluate, compute_embeddings_lfw\n\nfrom utils.utils import load_model_state, save_model_cpu\nimport utils.augmentation as augm\nfrom utils.parser_yaml import ArgumentParserWithYaml\nfrom model.common import models_backbones\n\n\ndef train(args):\n    """"""Performs training of a face recognition network""""""\n    input_size = models_backbones[args.model]().get_input_res()\n    if args.train_dataset == \'vgg\':\n        assert args.t_list\n        dataset = VGGFace2(args.train, args.t_list, args.t_land)\n    elif args.train_dataset == \'imdbface\':\n        dataset = IMDBFace(args.train, args.t_list)\n    elif args.train_dataset == \'trp\':\n        dataset = TrillionPairs(args.train, args.t_list)\n    else:\n        dataset = MSCeleb1M(args.train, args.t_list)\n\n    if dataset.have_landmarks:\n        log.info(\'Use alignment for the train data\')\n        dataset.transform = t.Compose([augm.HorizontalFlipNumpy(p=.5),\n                                       augm.CutOutWithPrior(p=0.05, max_area=0.1),\n                                       augm.RandomRotationNumpy(10, p=.95),\n                                       augm.ResizeNumpy(input_size),\n                                       augm.BlurNumpy(k=5, p=.2),\n                                       augm.NumpyToTensor(switch_rb=True)])\n    else:\n        dataset.transform = t.Compose([augm.ResizeNumpy(input_size),\n                                       augm.HorizontalFlipNumpy(),\n                                       augm.RandomRotationNumpy(10),\n                                       augm.NumpyToTensor(switch_rb=True)])\n\n    if args.weighted:\n        train_weights = dataset.get_weights()\n        train_weights = torch.DoubleTensor(train_weights)\n        sampler = torch.utils.data.sampler.WeightedRandomSampler(train_weights, len(train_weights))\n        train_loader = torch.utils.data.DataLoader(dataset, batch_size=args.train_batch_size,\n                                                   sampler=sampler, num_workers=3, pin_memory=False)\n    else:\n        train_loader = DataLoader(dataset, batch_size=args.train_batch_size, num_workers=4, shuffle=True)\n\n    lfw = LFW(args.val, args.v_list, args.v_land)\n    if lfw.use_landmarks:\n        log.info(\'Use alignment for the test data\')\n        lfw.transform = t.Compose([augm.ResizeNumpy(input_size),\n                                   augm.NumpyToTensor(switch_rb=True)])\n    else:\n        lfw.transform = t.Compose([augm.ResizeNumpy((160, 160)),\n                                   augm.CenterCropNumpy(input_size),\n                                   augm.NumpyToTensor(switch_rb=True)])\n\n    log_path = \'./logs/{:%Y_%m_%d_%H_%M}_{}\'.format(datetime.datetime.now(), args.snap_prefix)\n    writer = SummaryWriter(log_path)\n\n    if not osp.exists(args.snap_folder):\n        os.mkdir(args.snap_folder)\n\n    model = models_backbones[args.model](embedding_size=args.embed_size,\n                                         num_classes=dataset.get_num_classes(), feature=False)\n\n    set_dropout_fn = model.set_dropout_ratio\n\n    compression_algo = None\n    if args.snap_to_resume is not None:\n        if args.compr_config:\n            config = Config.from_json(args.compr_config)\n            compression_algo = create_compression_algorithm(model, config)\n            model = compression_algo.model\n\n        log.info(\'Resuming snapshot \' + args.snap_to_resume + \' ...\')\n        model = load_model_state(model, args.snap_to_resume, args.devices[0], eval_state=False)\n        model = torch.nn.DataParallel(model, device_ids=args.devices)\n    else:\n        model = torch.nn.DataParallel(model, device_ids=args.devices, output_device=args.devices[0])\n        model.cuda()\n        model.train()\n        cudnn.benchmark = True\n\n    if args.to_onnx is not None:\n        if args.compr_config:\n            compression_algo.export_model(args.to_onnx)\n        else:\n            model = model.eval().cpu()\n            input_shape = tuple([1, 3] + list(input_size))\n            with torch.no_grad():\n                torch.onnx.export(model.module, torch.randn(input_shape), args.to_onnx, verbose=True)\n\n        print(""Saved to"", args.to_onnx)\n        return\n\n    log.info(\'Face Recognition model:\')\n    log.info(model)\n\n    if args.mining_type == \'focal\':\n        softmax_criterion = AMSoftmaxLoss(gamma=args.gamma, m=args.m, margin_type=args.margin_type, s=args.s)\n    else:\n        softmax_criterion = AMSoftmaxLoss(t=args.t, m=0.35, margin_type=args.margin_type, s=args.s)\n    aux_losses = MetricLosses(dataset.get_num_classes(), args.embed_size, writer)\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    if args.compr_config:\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [0, 2, 4, 6, 8])\n    else:\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [3, 6, 9, 13])\n\n    log.info(\'Epoch length: %d\' % len(train_loader))\n    for epoch_num in range(args.epoch_total_num):\n        log.info(\'Epoch: %d\' % epoch_num)\n        scheduler.step()\n\n        if epoch_num > 6 or args.compr_config:\n            set_dropout_fn(0.)\n\n        classification_correct = 0\n        classification_total = 0\n\n        for i, data in enumerate(train_loader, 0):\n            iteration = epoch_num * len(train_loader) + i\n\n            if iteration % args.val_step == 0:\n                snapshot_name = osp.join(args.snap_folder, args.snap_prefix + \'_{0}.pt\'.format(iteration))\n                if iteration > 0:\n                    log.info(\'Saving Snapshot: \' + snapshot_name)\n                    save_model_cpu(model, optimizer, snapshot_name, epoch_num)\n\n                log.info(\'Evaluating Snapshot: \' + snapshot_name)\n                model.eval()\n                same_acc, diff_acc, all_acc, auc = evaluate(args, lfw, model, compute_embeddings_lfw,\n                                                            args.val_batch_size, verbose=False)\n\n                model.train()\n\n                log.info(\'Validation accuracy: {0:.4f}, {1:.4f}\'.format(same_acc, diff_acc))\n                log.info(\'Validation accuracy mean: {0:.4f}\'.format(all_acc))\n                log.info(\'Validation AUC: {0:.4f}\'.format(auc))\n                writer.add_scalar(\'Epoch\', epoch_num, iteration)\n                writer.add_scalar(\'Accuracy/Val_same_accuracy\', same_acc, iteration)\n                writer.add_scalar(\'Accuracy/Val_diff_accuracy\', diff_acc, iteration)\n                writer.add_scalar(\'Accuracy/Val_accuracy\', all_acc, iteration)\n                writer.add_scalar(\'Accuracy/AUC\', auc, iteration)\n\n            data, label = data[\'img\'], data[\'label\'].cuda()\n            features, sm_outputs = model(data)\n\n            optimizer.zero_grad()\n            aux_losses.init_iteration()\n            aux_loss, aux_log = aux_losses(features, label, epoch_num, iteration)\n            loss_sm = softmax_criterion(sm_outputs, label)\n            compr_loss = compression_algo.loss() if args.compr_config else 0\n            loss = loss_sm + aux_loss + compr_loss\n            loss.backward()\n            aux_losses.end_iteration()\n            optimizer.step()\n\n            _, predicted = torch.max(sm_outputs.data, 1)\n            classification_total += int(label.size(0))\n            classification_correct += int(torch.sum(predicted.eq(label)))\n            train_acc = float(classification_correct) / classification_total\n\n            if i % 10 == 0:\n                log.info(\'Iteration %d, Softmax loss: %.4f, Total loss: %.4f\' % (iteration, loss_sm, loss) + aux_log)\n                log.info(\'Learning rate: %f\' % scheduler.get_lr()[0])\n                writer.add_scalar(\'Loss/train_loss\', loss, iteration)\n                writer.add_scalar(\'Loss/softmax_loss\', loss_sm, iteration)\n                writer.add_scalar(\'Learning_rate\', scheduler.get_lr()[0], iteration)\n                writer.add_scalar(\'Accuracy/classification\', train_acc, iteration)\n                if args.compr_config and ""sparsity_level"" in compression_algo.statistics():\n                    log.info(\'Sparsity_level: %.4f\' % compression_algo.statistics()[""sparsity_level""])\n                    writer.add_scalar(\'Sparsity_level\', compression_algo.statistics()[""sparsity_level""], iteration)\n\n            if args.compr_config:\n                compression_algo.scheduler.step()\n\n        if args.compr_config:\n            compression_algo.scheduler.epoch_step()\n\n\ndef main():\n    """"""Creates a command line parser and starts training""""""\n    parser = ArgumentParserWithYaml(description=\'Training Face Recognition in PyTorch\',\n                                    fromfile_prefix_chars=\'@\',\n                                    epilog=""Please, note that you can parse parameters from a yaml file if \\\n                                    you add @<path_to_yaml_file> to command line"")\n\n    #datasets configuration\n    parser.add_argument(\'--train_dataset\', choices=[\'vgg\', \'ms1m\', \'trp\', \'imdbface\'],\n                        type=str, default=\'vgg\', help=\'Name of the train dataset.\')\n    parser.add_argument(\'--train_data_root\', dest=\'train\', required=True, type=str, help=\'Path to train data.\')\n    parser.add_argument(\'--train_list\', dest=\'t_list\', required=False, type=str, help=\'Path to train data image list.\')\n    parser.add_argument(\'--train_landmarks\', default=\'\', dest=\'t_land\', required=False, type=str,\n                        help=\'Path to landmarks for the train images.\')\n\n    parser.add_argument(\'--val_data_root\', dest=\'val\', required=True, type=str, help=\'Path to val data.\')\n    parser.add_argument(\'--val_step\', type=int, default=1000, help=\'Evaluate model each val_step during each epoch.\')\n    parser.add_argument(\'--val_list\', dest=\'v_list\', required=True, type=str, help=\'Path to test data image list.\')\n    parser.add_argument(\'--val_landmarks\', dest=\'v_land\', default=\'\', required=False, type=str,\n                        help=\'Path to landmarks for test images.\')\n\n    #model configuration\n    parser.add_argument(\'--model\', choices=models_backbones.keys(), type=str, default=\'mobilenet\', help=\'Model type.\')\n    parser.add_argument(\'--embed_size\', type=int, default=256, help=\'Size of the face embedding.\')\n\n    #optimizer configuration\n    parser.add_argument(\'--train_batch_size\', type=int, default=170, help=\'Train batch size.\')\n    parser.add_argument(\'--epoch_total_num\', type=int, default=30, help=\'Number of epochs to train.\')\n    parser.add_argument(\'--lr\', type=float, default=0.4, help=\'Learning rate.\')\n    parser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\n    parser.add_argument(\'--weight_decay\', type=float, default=0.0001, help=\'Weight decay.\')\n\n    #loss configuration\n    parser.add_argument(\'--mining_type\', choices=[\'focal\', \'sv\'],\n                        type=str, default=\'sv\', help=\'Hard mining method in loss.\')\n    parser.add_argument(\'--t\', type=float, default=1.1, help=\'t in support vector softmax. See https://arxiv.org/abs/1812.11317 for details\')\n    parser.add_argument(\'--gamma\', type=float, default=2., help=\'Gamma in focal loss. See https://arxiv.org/abs/1708.02002 for details\')\n    parser.add_argument(\'--m\', type=float, default=0.35, help=\'Margin size for AMSoftmax.\')\n    parser.add_argument(\'--s\', type=float, default=30., help=\'Scale for AMSoftmax.\')\n    parser.add_argument(\'--margin_type\', choices=[\'cos\', \'arc\'],\n                        type=str, default=\'cos\', help=\'Margin type for AMSoftmax loss.\')\n\n    #other parameters\n    parser.add_argument(\'--devices\', type=int, nargs=\'+\', default=[0], help=\'CUDA devices to use.\')\n    parser.add_argument(\'--val_batch_size\', type=int, default=20, help=\'Validation batch size.\')\n    parser.add_argument(\'--snap_folder\', type=str, default=\'./snapshots/\', help=\'Folder to save snapshots.\')\n    parser.add_argument(\'--snap_prefix\', type=str, default=\'FaceReidNet\', help=\'Prefix for snapshots.\')\n    parser.add_argument(\'--snap_to_resume\', type=str, default=None, help=\'Snapshot to resume.\')\n    parser.add_argument(\'--weighted\', action=\'store_true\')\n    parser.add_argument(\'-c\', \'--compr_config\', help=\'Path to a file with compression parameters\', required=False)\n    parser.add_argument(\'--to-onnx\', type=str, metavar=\'PATH\', default=None, help=\'Export to ONNX model by given path\')\n\n    args = parser.parse_args()\n    log.info(\'Arguments:\\n\' + pformat(args.__dict__))\n\n    if args.compr_config:\n        patch_torch_operators()\n\n    with torch.cuda.device(args.devices[0]):\n        train(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/train_landmarks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport datetime\nimport os.path as osp\n\nimport numpy as np\nimport glog as log\nfrom tensorboardX import SummaryWriter\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms\n\nfrom nncf.config import Config\nfrom nncf.dynamic_graph import patch_torch_operators\nfrom nncf.algo_selector import create_compression_algorithm\nfrom datasets import VGGFace2, CelebA, NDG\n\nfrom model.common import models_landmarks\nfrom utils import landmarks_augmentation\nfrom utils.utils import save_model_cpu, load_model_state\nfrom losses.alignment import AlignmentLoss\nfrom evaluate_landmarks import evaluate\n\n\ndef train(args):\n    """"""Launches training of landmark regression model""""""\n    input_size = models_landmarks[\'landnet\']().get_input_res()\n    if args.dataset == \'vgg\':\n        drops_schedule = [1, 6, 9, 13]\n        dataset = VGGFace2(args.train, args.t_list, args.t_land, landmarks_training=True)\n    elif args.dataset == \'celeba\':\n        drops_schedule = [10, 20]\n        dataset = CelebA(args.train, args.t_land)\n    else:\n        drops_schedule = [90, 140, 200]\n        dataset = NDG(args.train, args.t_land)\n\n    if dataset.have_landmarks:\n        log.info(\'Use alignment for the train data\')\n        dataset.transform = transforms.Compose([landmarks_augmentation.Rescale((56, 56)),\n                                                landmarks_augmentation.Blur(k=3, p=.2),\n                                                landmarks_augmentation.HorizontalFlip(p=.5),\n                                                landmarks_augmentation.RandomRotate(50),\n                                                landmarks_augmentation.RandomScale(.8, .9, p=.4),\n                                                landmarks_augmentation.RandomCrop(48),\n                                                landmarks_augmentation.ToTensor(switch_rb=True)])\n    else:\n        log.info(\'Error: training dataset has no landmarks data\')\n        exit()\n\n    train_loader = DataLoader(dataset, batch_size=args.train_batch_size, num_workers=4, shuffle=True)\n    writer = SummaryWriter(\'./logs_landm/{:%Y_%m_%d_%H_%M}_\'.format(datetime.datetime.now()) + args.snap_prefix)\n    model = models_landmarks[\'landnet\']()\n\n    set_dropout_fn = model.set_dropout_ratio\n\n    compression_algo = None\n    if args.snap_to_resume is not None:\n            config = Config.from_json(args.compr_config)\n            compression_algo = create_compression_algorithm(model, config)\n            model = compression_algo.model\n\n        log.info(\'Resuming snapshot \' + args.snap_to_resume + \' ...\')\n        model = load_model_state(model, args.snap_to_resume, args.device, eval_state=False)\n        model = torch.nn.DataParallel(model, device_ids=[args.device])\n    else:\n        model = torch.nn.DataParallel(model, device_ids=[args.device])\n        model.cuda()\n        model.train()\n        cudnn.enabled = True\n        cudnn.benchmark = True\n\n    if args.to_onnx is not None:\n        if args.compr_config:\n            compression_algo.export_model(args.to_onnx)\n        else:\n            model = model.eval().cpu()\n            input_shape = tuple([1, 3] + list(input_size))\n            with torch.no_grad():\n                torch.onnx.export(model.module, torch.randn(input_shape), args.to_onnx, verbose=True)\n\n        print(""Saved to"", args.to_onnx)\n        return\n\n    log.info(\'Face landmarks model:\')\n    log.info(model)\n\n    criterion = AlignmentLoss(\'wing\')\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, drops_schedule)\n\n    log.info(\'Epoch length: %d\' % len(train_loader))\n    for epoch_num in range(args.epoch_total_num):\n        log.info(\'Epoch: %d\' % epoch_num)\n\n        scheduler.step()\n        if epoch_num > 5 or args.compr_config:\n            set_dropout_fn(0.)\n\n        for i, data in enumerate(train_loader, 0):\n            iteration = epoch_num * len(train_loader) + i\n\n            if iteration % args.val_step == 0:\n                snapshot_name = osp.join(args.snap_folder,\n                                         args.snap_prefix + \'_{0}.pt\'.format(iteration))\n                log.info(\'Saving Snapshot: \' + snapshot_name)\n                save_model_cpu(model, optimizer, snapshot_name, epoch_num)\n\n                model.eval()\n                log.info(\'Evaluating Snapshot: \' + snapshot_name)\n                avg_err, per_point_avg_err, failures_rate = evaluate(train_loader, model)\n                weights = per_point_avg_err / np.sum(per_point_avg_err)\n                criterion.set_weights(weights)\n                log.info(str(weights))\n                log.info(\'Avg train error: {}\'.format(avg_err))\n                log.info(\'Train failure rate: {}\'.format(failures_rate))\n                writer.add_scalar(\'Quality/Avg_error\', avg_err, iteration)\n                writer.add_scalar(\'Quality/Failure_rate\', failures_rate, iteration)\n                writer.add_scalar(\'Epoch\', epoch_num, iteration)\n                model.train()\n\n            data, gt_landmarks = data[\'img\'].cuda(), data[\'landmarks\'].cuda()\n            predicted_landmarks = model(data)\n\n            optimizer.zero_grad()\n            compr_loss = compression_algo.loss() if args.compr_config else 0\n            loss = criterion(predicted_landmarks, gt_landmarks) + compr_loss\n            loss.backward()\n            optimizer.step()\n            if args.compr_config:\n                compression_algo.scheduler.step()\n\n            if i % 10 == 0:\n                log.info(\'Iteration %d, Loss: %.4f\' % (iteration, loss))\n                log.info(\'Learning rate: %f\' % scheduler.get_lr()[0])\n                writer.add_scalar(\'Loss/train_loss\', loss.item(), iteration)\n                writer.add_scalar(\'Learning_rate\', scheduler.get_lr()[0], iteration)\n                if args.compr_config and ""sparsity_level"" in compression_algo.statistics():\n                    log.info(\'Sparsity_level: %.4f\' % compression_algo.statistics()[""sparsity_level""])\n                    writer.add_scalar(\'Sparsity_level\', compression_algo.statistics()[""sparsity_level""], iteration)\n\n        if args.compr_config:\n            compression_algo.scheduler.epoch_step()\n\n\ndef main():\n    """"""Creates a command line parser""""""\n    parser = argparse.ArgumentParser(description=\'Training Landmarks detector in PyTorch\')\n    parser.add_argument(\'--train_data_root\', dest=\'train\', required=True, type=str, help=\'Path to train data.\')\n    parser.add_argument(\'--train_list\', dest=\'t_list\', required=False, type=str, help=\'Path to train data image list.\')\n    parser.add_argument(\'--train_landmarks\', default=\'\', dest=\'t_land\', required=False, type=str,\n                        help=\'Path to landmarks for the train images.\')\n    parser.add_argument(\'--train_batch_size\', type=int, default=170, help=\'Train batch size.\')\n    parser.add_argument(\'--epoch_total_num\', type=int, default=30, help=\'Number of epochs to train.\')\n    parser.add_argument(\'--lr\', type=float, default=0.4, help=\'Learning rate.\')\n    parser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\n    parser.add_argument(\'--val_step\', type=int, default=2000, help=\'Evaluate model each val_step during each epoch.\')\n    parser.add_argument(\'--weight_decay\', type=float, default=0.0001, help=\'Weight decay.\')\n    parser.add_argument(\'--device\', \'-d\', default=0, type=int)\n    parser.add_argument(\'--snap_folder\', type=str, default=\'./snapshots/\', help=\'Folder to save snapshots.\')\n    parser.add_argument(\'--snap_prefix\', type=str, default=\'LandmarksNet\', help=\'Prefix for snapshots.\')\n    parser.add_argument(\'--snap_to_resume\', type=str, default=None, help=\'Snapshot to resume.\')\n    parser.add_argument(\'--dataset\', choices=[\'vgg\', \'celeb\', \'ngd\'], type=str, default=\'vgg\', help=\'Dataset.\')\n    parser.add_argument(\'-c\', \'--compr_config\', help=\'Path to a file with compression parameters\', required=False)\n    parser.add_argument(\'--to-onnx\', type=str, metavar=\'PATH\', default=None, help=\'Export to ONNX model by given path\')\n    arguments = parser.parse_args()\n\n    if args.compr_config:\n        patch_torch_operators()\n\n    with torch.cuda.device(arguments.device):\n        train(arguments)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/human_pose_estimation/demo.py,0,"b""import argparse\n\nimport cv2\nimport numpy as np\nimport torch\n\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom modules.keypoints import extract_keypoints, group_keypoints\nfrom modules.load_state import load_state\nfrom modules.pose import Pose, propagate_ids\nfrom val import normalize, pad_width\n\n\nclass ImageReader(object):\n    def __init__(self, file_names):\n        self.file_names = file_names\n        self.max_idx = len(file_names)\n\n    def __iter__(self):\n        self.idx = 0\n        return self\n\n    def __next__(self):\n        if self.idx == self.max_idx:\n            raise StopIteration\n        img = cv2.imread(self.file_names[self.idx], cv2.IMREAD_COLOR)\n        if img.size == 0:\n            raise IOError('Image {} cannot be read'.format(self.file_names[self.idx]))\n        self.idx = self.idx + 1\n        return img\n\n\nclass VideoReader(object):\n    def __init__(self, file_name):\n        self.file_name = file_name\n        try:  # OpenCV needs int to read from webcam\n            self.file_name = int(file_name)\n        except ValueError:\n            pass\n\n    def __iter__(self):\n        self.cap = cv2.VideoCapture(self.file_name)\n        if not self.cap.isOpened():\n            raise IOError('Video {} cannot be opened'.format(self.file_name))\n        return self\n\n    def __next__(self):\n        was_read, img = self.cap.read()\n        if not was_read:\n            raise StopIteration\n        return img\n\n\ndef infer_fast(net, img, net_input_height_size, stride, upsample_ratio, cpu,\n               pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n    height, width, _ = img.shape\n    scale = net_input_height_size / height\n\n    scaled_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n    scaled_img = normalize(scaled_img, img_mean, img_scale)\n    min_dims = [net_input_height_size, max(scaled_img.shape[1], net_input_height_size)]\n    padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n\n    tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float()\n    if not cpu:\n        tensor_img = tensor_img.cuda()\n\n    stages_output = net(tensor_img)\n\n    stage2_heatmaps = stages_output[-2]\n    heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n    heatmaps = cv2.resize(heatmaps, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n\n    stage2_pafs = stages_output[-1]\n    pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n    pafs = cv2.resize(pafs, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n\n    return heatmaps, pafs, scale, pad\n\n\ndef run_demo(net, image_provider, height_size, cpu, track_ids):\n    net = net.eval()\n    if not cpu:\n        net = net.cuda()\n\n    stride = 8\n    upsample_ratio = 4\n    num_keypoints = Pose.num_kpts\n    previous_poses = []\n    for img in image_provider:\n        orig_img = img.copy()\n        heatmaps, pafs, scale, pad = infer_fast(net, img, height_size, stride, upsample_ratio, cpu)\n\n        total_keypoints_num = 0\n        all_keypoints_by_type = []\n        for kpt_idx in range(num_keypoints):  # 19th for bg\n            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n\n        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, demo=True)\n        for kpt_id in range(all_keypoints.shape[0]):\n            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n        current_poses = []\n        for n in range(len(pose_entries)):\n            if len(pose_entries[n]) == 0:\n                continue\n            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n            for kpt_id in range(num_keypoints):\n                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n            pose = Pose(pose_keypoints, pose_entries[n][18])\n            current_poses.append(pose)\n            pose.draw(img)\n\n        img = cv2.addWeighted(orig_img, 0.6, img, 0.4, 0)\n        if track_ids:\n            propagate_ids(previous_poses, current_poses)\n            previous_poses = current_poses\n            for pose in current_poses:\n                cv2.rectangle(img, (pose.bbox[0], pose.bbox[1]),\n                              (pose.bbox[0] + pose.bbox[2], pose.bbox[1] + pose.bbox[3]), (0, 255, 0))\n                cv2.putText(img, 'id: {}'.format(pose.id), (pose.bbox[0], pose.bbox[1] - 16),\n                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n        cv2.imshow('Lightweight Human Pose Estimation Python Demo', img)\n        key = cv2.waitKey(33)\n        if key == 27:  # esc\n            return\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='''Lightweight human pose estimation python demo.\n                       This is just for quick results preview.\n                       Please, consider c++ demo for the best performance.''')\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint')\n    parser.add_argument('--height-size', type=int, default=256, help='network input layer height size')\n    parser.add_argument('--video', type=str, default='', help='path to video file or camera id')\n    parser.add_argument('--images', nargs='+', default='', help='path to input image(s)')\n    parser.add_argument('--cpu', action='store_true', help='run network inference on cpu')\n    parser.add_argument('--track-ids', action='store_true', help='track poses ids')\n    args = parser.parse_args()\n\n    if args.video == '' and args.images == '':\n        raise ValueError('Either --video or --image has to be provided')\n\n    net = PoseEstimationWithMobileNet()\n    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')\n    load_state(net, checkpoint)\n\n    frame_provider = ImageReader(args.images)\n    if args.video != '':\n        frame_provider = VideoReader(args.video)\n\n    run_demo(net, frame_provider, args.height_size, args.cpu, args.track_ids)\n"""
pytorch_toolkit/human_pose_estimation/train.py,0,"b""import argparse\nimport cv2\nimport os\n\nimport torch\nfrom torch.nn import DataParallel\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom datasets.coco import CocoTrainDataset\nfrom datasets.transformations import ConvertKeypoints, Scale, Rotate, CropPad, Flip\nfrom modules.get_parameters import get_parameters_conv, get_parameters_bn, get_parameters_conv_depthwise\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom modules.loss import l2_loss\nfrom modules.load_state import load_state, load_from_mobilenet\nfrom val import evaluate\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)  # To prevent freeze of DataLoader\n\n\ndef train(prepared_train_labels, train_images_folder, num_refinement_stages, base_lr, batch_size, batches_per_iter,\n          num_workers, checkpoint_path, weights_only, from_mobilenet, checkpoints_folder, log_after,\n          val_labels, val_images_folder, val_output_name, checkpoint_after, val_after):\n    net = PoseEstimationWithMobileNet(num_refinement_stages)\n\n    stride = 8\n    sigma = 7\n    path_thickness = 1\n    dataset = CocoTrainDataset(prepared_train_labels, train_images_folder,\n                               stride, sigma, path_thickness,\n                               transform=transforms.Compose([\n                                   ConvertKeypoints(),\n                                   Scale(),\n                                   Rotate(pad=(128, 128, 128)),\n                                   CropPad(pad=(128, 128, 128)),\n                                   Flip()]))\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n    optimizer = optim.Adam([\n        {'params': get_parameters_conv(net.model, 'weight')},\n        {'params': get_parameters_conv_depthwise(net.model, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.model, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.model, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv(net.cpm, 'weight'), 'lr': base_lr},\n        {'params': get_parameters_conv(net.cpm, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv_depthwise(net.cpm, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_conv(net.initial_stage, 'weight'), 'lr': base_lr},\n        {'params': get_parameters_conv(net.initial_stage, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv(net.refinement_stages, 'weight'), 'lr': base_lr * 4},\n        {'params': get_parameters_conv(net.refinement_stages, 'bias'), 'lr': base_lr * 8, 'weight_decay': 0},\n        {'params': get_parameters_bn(net.refinement_stages, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.refinement_stages, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n    ], lr=base_lr, weight_decay=5e-4)\n\n    num_iter = 0\n    current_epoch = 0\n    drop_after_epoch = [100, 200, 260]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=drop_after_epoch, gamma=0.333)\n    if checkpoint_path:\n        checkpoint = torch.load(checkpoint_path)\n\n        if from_mobilenet:\n            load_from_mobilenet(net, checkpoint)\n        else:\n            load_state(net, checkpoint)\n            if not weights_only:\n                optimizer.load_state_dict(checkpoint['optimizer'])\n                scheduler.load_state_dict(checkpoint['scheduler'])\n                num_iter = checkpoint['iter']\n                current_epoch = checkpoint['current_epoch']\n\n    net = DataParallel(net).cuda()\n    net.train()\n    for epochId in range(current_epoch, 280):\n        scheduler.step()\n        total_losses = [0, 0] * (num_refinement_stages + 1)  # heatmaps loss, paf loss per stage\n        batch_per_iter_idx = 0\n        for batch_data in train_loader:\n            if batch_per_iter_idx == 0:\n                optimizer.zero_grad()\n\n            images = batch_data['image'].cuda()\n            keypoint_masks = batch_data['keypoint_mask'].cuda()\n            paf_masks = batch_data['paf_mask'].cuda()\n            keypoint_maps = batch_data['keypoint_maps'].cuda()\n            paf_maps = batch_data['paf_maps'].cuda()\n\n            stages_output = net(images)\n\n            losses = []\n            for loss_idx in range(len(total_losses) // 2):\n                losses.append(l2_loss(stages_output[loss_idx * 2], keypoint_maps, images.shape[0], keypoint_masks))\n                losses.append(l2_loss(stages_output[loss_idx * 2 + 1], paf_maps, images.shape[0], paf_masks))\n                total_losses[loss_idx * 2] += losses[-2].item() / batches_per_iter\n                total_losses[loss_idx * 2 + 1] += losses[-1].item() / batches_per_iter\n\n            loss = losses[0]\n            for loss_idx in range(1, len(losses)):\n                loss += losses[loss_idx]\n            loss /= batches_per_iter\n            loss.backward()\n            batch_per_iter_idx += 1\n            if batch_per_iter_idx == batches_per_iter:\n                optimizer.step()\n                batch_per_iter_idx = 0\n                num_iter += 1\n            else:\n                continue\n\n            if num_iter % log_after == 0:\n                print('Iter: {}'.format(num_iter))\n                for loss_idx in range(len(total_losses) // 2):\n                    print('\\n'.join(['stage{}_pafs_loss:     {}', 'stage{}_heatmaps_loss: {}']).format(\n                        loss_idx + 1, total_losses[loss_idx * 2 + 1] / log_after,\n                        loss_idx + 1, total_losses[loss_idx * 2] / log_after))\n                for loss_idx in range(len(total_losses)):\n                    total_losses[loss_idx] = 0\n            if num_iter % checkpoint_after == 0:\n                snapshot_name = '{}/checkpoint_iter_{}.pth'.format(checkpoints_folder, num_iter)\n                torch.save({'state_dict': net.module.state_dict(),\n                            'optimizer': optimizer.state_dict(),\n                            'scheduler': scheduler.state_dict(),\n                            'iter': num_iter,\n                            'current_epoch': epochId},\n                           snapshot_name)\n            if num_iter % val_after == 0:\n                print('Validation...')\n                evaluate(val_labels, val_output_name, val_images_folder, net)\n                net.train()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--prepared-train-labels', type=str, required=True,\n                        help='path to the file with prepared annotations')\n    parser.add_argument('--train-images-folder', type=str, required=True, help='path to COCO train images folder')\n    parser.add_argument('--num-refinement-stages', type=int, default=1, help='number of refinement stages')\n    parser.add_argument('--base-lr', type=float, default=4e-5, help='initial learning rate')\n    parser.add_argument('--batch-size', type=int, default=80, help='batch size')\n    parser.add_argument('--batches-per-iter', type=int, default=1, help='number of batches to accumulate gradient from')\n    parser.add_argument('--num-workers', type=int, default=8, help='number of workers')\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint to continue training from')\n    parser.add_argument('--from-mobilenet', action='store_true',\n                        help='load weights from mobilenet feature extractor')\n    parser.add_argument('--weights-only', action='store_true',\n                        help='just initialize layers with pretrained weights and start training from the beginning')\n    parser.add_argument('--experiment-name', type=str, default='default',\n                        help='experiment name to create folder for checkpoints')\n    parser.add_argument('--log-after', type=int, default=100, help='number of iterations to print train loss')\n\n    parser.add_argument('--val-labels', type=str, required=True, help='path to json with keypoints val labels')\n    parser.add_argument('--val-images-folder', type=str, required=True, help='path to COCO val images folder')\n    parser.add_argument('--val-output-name', type=str, default='detections.json',\n                        help='name of output json file with detected keypoints')\n    parser.add_argument('--checkpoint-after', type=int, default=5000,\n                        help='number of iterations to save checkpoint')\n    parser.add_argument('--val-after', type=int, default=5000,\n                        help='number of iterations to run validation')\n    args = parser.parse_args()\n\n    checkpoints_folder = '{}_checkpoints'.format(args.experiment_name)\n    if not os.path.exists(checkpoints_folder):\n        os.makedirs(checkpoints_folder)\n\n    train(args.prepared_train_labels, args.train_images_folder, args.num_refinement_stages, args.base_lr, args.batch_size,\n          args.batches_per_iter, args.num_workers, args.checkpoint_path, args.weights_only, args.from_mobilenet,\n          checkpoints_folder, args.log_after, args.val_labels, args.val_images_folder, args.val_output_name,\n          args.checkpoint_after, args.val_after)\n"""
pytorch_toolkit/human_pose_estimation/train_single.py,0,"b""import argparse\nimport cv2\nimport os\n\nimport torch\nfrom torch.nn import DataParallel\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom datasets.lip import LipTrainDataset, LipValDataset\nfrom datasets.transformations import SinglePersonRotate, SinglePersonCropPad, SinglePersonFlip, SinglePersonBodyMasking,\\\n    ChannelPermutation\nfrom modules.calc_pckh import calc_pckh\nfrom modules.get_parameters import get_parameters_conv, get_parameters_bn, get_parameters_conv_depthwise\nfrom models.single_person_pose_with_mobilenet import SinglePersonPoseEstimationWithMobileNet\nfrom modules.loss import l2_loss\nfrom modules.load_state import load_state, load_from_mobilenet\nfrom val_single import evaluate\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)  # To prevent freeze of DataLoader\n\n\ndef train(images_folder, num_refinement_stages, base_lr, batch_size, batches_per_iter,\n          num_workers, checkpoint_path, weights_only, from_mobilenet, checkpoints_folder,\n          log_after, checkpoint_after):\n    net = SinglePersonPoseEstimationWithMobileNet(num_refinement_stages).cuda()\n    stride = 8\n    sigma = 7\n    dataset = LipTrainDataset(images_folder, stride, sigma,\n                              transform=transforms.Compose([\n                                   SinglePersonBodyMasking(),\n                                   ChannelPermutation(),\n                                   SinglePersonRotate(pad=(128, 128, 128), max_rotate_degree=40),\n                                   SinglePersonCropPad(pad=(128, 128, 128), crop_x=256, crop_y=256),\n                                   SinglePersonFlip(left_keypoints_indice=LipTrainDataset.left_keypoints_indice,\n                                                    right_keypoints_indice=LipTrainDataset.right_keypoints_indice)\n                              ]))\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n    optimizer = optim.Adam([\n        {'params': get_parameters_conv(net.model, 'weight')},\n        {'params': get_parameters_conv_depthwise(net.model, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.model, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.model, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv(net.cpm, 'weight'), 'lr': base_lr},\n        {'params': get_parameters_conv(net.cpm, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv_depthwise(net.cpm, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_conv(net.initial_stage, 'weight'), 'lr': base_lr},\n        {'params': get_parameters_conv(net.initial_stage, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_bn(net.initial_stage, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.initial_stage, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv(net.refinement_stages, 'weight'), 'lr': base_lr * 4},\n        {'params': get_parameters_conv(net.refinement_stages, 'bias'), 'lr': base_lr * 8, 'weight_decay': 0},\n        {'params': get_parameters_bn(net.refinement_stages, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.refinement_stages, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n    ], lr=base_lr, weight_decay=5e-4)\n\n    num_iter = 0\n    current_epoch = 0\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, threshold=1e-2, verbose=True)\n    if checkpoint_path:\n        checkpoint = torch.load(checkpoint_path)\n\n        if from_mobilenet:\n            load_from_mobilenet(net, checkpoint)\n        else:\n            load_state(net, checkpoint)\n            if not weights_only:\n                optimizer.load_state_dict(checkpoint['optimizer'])\n                scheduler.load_state_dict(checkpoint['scheduler'])\n                num_iter = checkpoint['iter']\n                current_epoch = checkpoint['current_epoch']+1\n\n    net = DataParallel(net)\n    net.train()\n    for epochId in range(current_epoch, 100):\n        print('Epoch: {}'.format(epochId))\n        net.train()\n        total_losses = [0] * (num_refinement_stages + 1)  # heatmaps loss per stage\n        batch_per_iter_idx = 0\n        for batch_data in train_loader:\n            if batch_per_iter_idx == 0:\n                optimizer.zero_grad()\n\n            images = batch_data['image'].cuda()\n            keypoint_maps = batch_data['keypoint_maps'].cuda()\n\n            stages_output = net(images)\n\n            losses = []\n            for loss_idx in range(len(total_losses)):\n                losses.append(l2_loss(stages_output[loss_idx], keypoint_maps, images.shape[0]))\n                total_losses[loss_idx] += losses[-1].item() / batches_per_iter\n\n            loss = losses[0]\n            for loss_idx in range(1, len(losses)):\n                loss += losses[loss_idx]\n            loss /= batches_per_iter\n            loss.backward()\n            batch_per_iter_idx += 1\n            if batch_per_iter_idx == batches_per_iter:\n                optimizer.step()\n                batch_per_iter_idx = 0\n                num_iter += 1\n            else:\n                continue\n\n            if num_iter % log_after == 0:\n                print('Iter: {}'.format(num_iter))\n                for loss_idx in range(len(total_losses)):\n                    print('\\n'.join(['stage{}_heatmaps_loss: {}']).format(\n                        loss_idx + 1, total_losses[loss_idx] / log_after))\n                for loss_idx in range(len(total_losses)):\n                    total_losses[loss_idx] = 0\n\n        snapshot_name = '{}/checkpoint_last_epoch.pth'.format(checkpoints_folder)\n        torch.save({'state_dict': net.module.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'scheduler': scheduler.state_dict(),\n                    'iter': num_iter,\n                    'current_epoch': epochId},\n                   snapshot_name)\n        if (epochId + 1) % checkpoint_after == 0:\n            snapshot_name = '{}/checkpoint_epoch_{}.pth'.format(checkpoints_folder, epochId)\n            torch.save({'state_dict': net.module.state_dict(),\n                        'optimizer': optimizer.state_dict(),\n                        'scheduler': scheduler.state_dict(),\n                        'iter': num_iter,\n                        'current_epoch': epochId},\n                       snapshot_name)\n        print('Validation...')\n        net.eval()\n        eval_num = 1000\n        val_dataset = LipValDataset(images_folder, eval_num)\n        predictions_name = '{}/val_results.csv'.format(checkpoints_folder)\n        evaluate(val_dataset, predictions_name, net)\n        pck = calc_pckh(val_dataset.labels_file_path, predictions_name, eval_num=eval_num)\n\n        val_loss = 100 - pck[-1][-1]  # 100 - avg_pckh\n        print('Val loss: {}'.format(val_loss))\n        scheduler.step(val_loss, epochId)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset-folder', type=str, required=True, help='path to dataset folder')\n    parser.add_argument('--num-refinement-stages', type=int, default=5, help='number of refinement stages')\n    parser.add_argument('--base-lr', type=float, default=4e-5, help='initial learning rate')\n    parser.add_argument('--batch-size', type=int, default=32, help='batch size')\n    parser.add_argument('--batches-per-iter', type=int, default=1, help='number of batches to accumulate gradient from')\n    parser.add_argument('--num-workers', type=int, default=8, help='number of workers')\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint to continue training from')\n    parser.add_argument('--from-mobilenet', action='store_true',\n                        help='load weights from mobilenet feature extractor')\n    parser.add_argument('--weights-only', action='store_true',\n                        help='just initialize layers with pretrained weights and start training from the beginning')\n    parser.add_argument('--experiment-name', type=str, default='default',\n                        help='experiment name to create folder for checkpoints')\n    parser.add_argument('--log-after', type=int, default=100, help='number of iterations to print train loss')\n    parser.add_argument('--checkpoint-after', type=int, default=10,\n                        help='number of epochs to save checkpoint')\n    args = parser.parse_args()\n\n    checkpoints_folder = '{}_checkpoints'.format(args.experiment_name)\n    if not os.path.exists(checkpoints_folder):\n        os.makedirs(checkpoints_folder)\n\n    train(args.dataset_folder, args.num_refinement_stages, args.base_lr, args.batch_size,\n          args.batches_per_iter, args.num_workers, args.checkpoint_path, args.weights_only, args.from_mobilenet,\n          checkpoints_folder, args.log_after, args.checkpoint_after)\n"""
pytorch_toolkit/human_pose_estimation/train_single_coco.py,0,"b""import argparse\nimport os\n\nimport cv2\nimport torch\nfrom torch.nn import DataParallel\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom datasets.coco_single import CocoSingleTrainDataset, CocoSingleValDataset\nfrom datasets.transformations import SinglePersonFlip,\\\n    SinglePersonBodyMasking, ChannelPermutation, SinglePersonRandomAffineTransform, RandomScaleRotate,\\\n    HalfBodyTransform, Normalization\nfrom models.single_person_pose_with_mobilenet import SinglePersonPoseEstimationWithMobileNet\nfrom modules.loss import mse_loss\nfrom modules.load_state import load_state, load_from_mobilenet\nfrom val_single import val\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)  # To prevent freeze of DataLoader\n\n\ndef train(images_folder, num_refinement_stages, base_lr, batch_size, batches_per_iter,\n          num_workers, checkpoint_path, weights_only, from_mobilenet, checkpoints_folder,\n          log_after, checkpoint_after):\n\n    dataset = CocoSingleTrainDataset(images_folder,\n                                     transform=transforms.Compose([\n                                         HalfBodyTransform(),\n                                         RandomScaleRotate(),\n                                         SinglePersonFlip(left_keypoints_indice=\n                                                          CocoSingleTrainDataset.left_keypoints_indice,\n                                                          right_keypoints_indice=\n                                                          CocoSingleTrainDataset.right_keypoints_indice),\n                                         SinglePersonRandomAffineTransform(),\n                                         SinglePersonBodyMasking(),\n                                         Normalization(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                         ChannelPermutation()\n                                         ]))\n    net = SinglePersonPoseEstimationWithMobileNet(num_refinement_stages, num_heatmaps=dataset._num_keypoints,\n                                                  mode='nearest').cuda()\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n    optimizer = optim.Adam(net.parameters(), lr=base_lr)\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [170, 200], 0.1)\n\n    num_iter = 0\n    current_epoch = 0\n    if checkpoint_path:\n        checkpoint = torch.load(checkpoint_path)\n        if from_mobilenet:\n            load_from_mobilenet(net, checkpoint)\n        else:\n            load_state(net, checkpoint)\n            if not weights_only:\n                optimizer.load_state_dict(checkpoint['optimizer'])\n                scheduler.load_state_dict(checkpoint['scheduler'])\n                num_iter = checkpoint['iter']\n                current_epoch = checkpoint['current_epoch']+1\n\n    net = DataParallel(net)\n    net.train()\n    for epochId in range(current_epoch, 210):\n        print('Epoch: {}'.format(epochId))\n        net.train()\n        total_losses = [0] * (num_refinement_stages + 1)  # heatmaps loss per stage\n        batch_per_iter_idx = 0\n        for batch_data in train_loader:\n            if batch_per_iter_idx == 0:\n                optimizer.zero_grad()\n\n            images = batch_data['image'].float().cuda()\n            keypoint_maps = batch_data['keypoint_maps']\n            stages_output = net(images)\n\n            losses = []\n            for loss_idx in range(len(total_losses)):\n                losses.append(mse_loss(stages_output[loss_idx], keypoint_maps,\n                                       batch_data['keypoints'][:, 2::3].view(batch_data['keypoints'].shape[0], -1, 1)))\n                total_losses[loss_idx] += losses[-1].item() / batches_per_iter\n\n            loss = 0\n            for loss_idx in range(len(losses)):\n                loss += losses[loss_idx]\n            loss /= batches_per_iter\n            loss.backward()\n            batch_per_iter_idx += 1\n            if batch_per_iter_idx == batches_per_iter:\n                optimizer.step()\n                batch_per_iter_idx = 0\n                num_iter += 1\n            else:\n                continue\n            if num_iter % log_after == 0:\n                print('Iter: {}'.format(num_iter))\n                for loss_idx in range(len(total_losses)):\n                    print('\\n'.join(['stage{}_heatmaps_loss: {}']).format(\n                        loss_idx + 1, total_losses[loss_idx] / log_after))\n                for loss_idx in range(len(total_losses)):\n                    total_losses[loss_idx] = 0\n\n        snapshot_name = '{}/checkpoint_last_epoch.pth'.format(checkpoints_folder)\n        torch.save({'state_dict': net.module.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'scheduler': scheduler.state_dict(),\n                    'iter': num_iter,\n                    'current_epoch': epochId},\n                   snapshot_name)\n\n        if (epochId + 1) % checkpoint_after == 0:\n            snapshot_name = '{}/checkpoint_epoch_{}.pth'.format(checkpoints_folder, epochId)\n            torch.save({'state_dict': net.module.state_dict(),\n                        'optimizer': optimizer.state_dict(),\n                        'scheduler': scheduler.state_dict(),\n                        'iter': num_iter,\n                        'current_epoch': epochId},\n                       snapshot_name)\n        print('Validation...')\n        net.eval()\n        val_dataset = CocoSingleValDataset(images_folder, transform=transforms.Compose([\n                                         SinglePersonRandomAffineTransform(mode='val'),\n                                         Normalization(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n        predictions_name = '{}/val_results2.json'.format(checkpoints_folder)\n        val_loss = val(net, val_dataset, predictions_name, 'CocoSingle')\n        print('Val loss: {}'.format(val_loss))\n        scheduler.step()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset-folder', type=str, required=True, help='path to dataset folder')\n    parser.add_argument('--num-refinement-stages', type=int, default=5, help='number of refinement stages')\n    parser.add_argument('--base-lr', type=float, default=0.001, help='initial learning rate')\n    parser.add_argument('--batch-size', type=int, default=32, help='batch size')\n    parser.add_argument('--batches-per-iter', type=int, default=1, help='number of batches to accumulate gradient from')\n    parser.add_argument('--num-workers', type=int, default=16, help='number of workers')\n    parser.add_argument('--checkpoint-path', type=str, required=True,\n                        help='path to the checkpoint to continue training from')\n    parser.add_argument('--from-mobilenet', action='store_true',\n                        help='load weights from mobilenet feature extractor')\n    parser.add_argument('--weights-only', action='store_true',\n                        help='just initialize layers with pretrained weights and start training from the beginning')\n    parser.add_argument('--experiment-name', type=str, default='default',\n                        help='experiment name to create folder for checkpoints')\n    parser.add_argument('--log-after', type=int, default=100, help='number of iterations to print train loss')\n    parser.add_argument('--checkpoint-after', type=int, default=1,\n                        help='number of epochs to save checkpoint')\n    args = parser.parse_args()\n\n    checkpoints_folder = '{}_checkpoints'.format(args.experiment_name)\n    if not os.path.exists(checkpoints_folder):\n        os.makedirs(checkpoints_folder)\n\n    train(args.dataset_folder, args.num_refinement_stages, args.base_lr, args.batch_size,\n          args.batches_per_iter, args.num_workers, args.checkpoint_path, args.weights_only, args.from_mobilenet,\n          checkpoints_folder, args.log_after, args.checkpoint_after)\n"""
pytorch_toolkit/human_pose_estimation/val.py,0,"b""import argparse\nimport cv2\nimport json\nimport math\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport torch\n\nfrom datasets.coco import CocoValDataset\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom modules.keypoints import extract_keypoints, group_keypoints\nfrom modules.load_state import load_state\n\n\ndef run_coco_eval(gt_file_path, dt_file_path):\n    annotation_type = 'keypoints'\n    print('Running test for {} results.'.format(annotation_type))\n\n    coco_gt = COCO(gt_file_path)\n    coco_dt = coco_gt.loadRes(dt_file_path)\n\n    result = COCOeval(coco_gt, coco_dt, annotation_type)\n    result.evaluate()\n    result.accumulate()\n    result.summarize()\n\n    return result.stats\n\n\ndef normalize(img, img_mean, img_scale):\n    img = np.array(img, dtype=np.float32)\n    img = (img - img_mean) * img_scale\n    return img\n\n\ndef pad_width(img, stride, pad_value, min_dims):\n    h, w, _ = img.shape\n    h = min(min_dims[0], h)\n    min_dims[0] = math.ceil(min_dims[0] / float(stride)) * stride\n    min_dims[1] = max(min_dims[1], w)\n    min_dims[1] = math.ceil(min_dims[1] / float(stride)) * stride\n    pad = []\n    pad.append(int(math.floor((min_dims[0] - h) / 2.0)))\n    pad.append(int(math.floor((min_dims[1] - w) / 2.0)))\n    pad.append(int(min_dims[0] - h - pad[0]))\n    pad.append(int(min_dims[1] - w - pad[1]))\n    padded_img = cv2.copyMakeBorder(img, pad[0], pad[2], pad[1], pad[3],\n                                    cv2.BORDER_CONSTANT, value=pad_value)\n    return padded_img, pad\n\n\ndef convert_to_coco_format(pose_entries, all_keypoints):\n    coco_keypoints = []\n    scores = []\n    for n in range(len(pose_entries)):\n        if len(pose_entries[n]) == 0:\n            continue\n        keypoints = [0] * 17 * 3\n        to_coco_map = [0, -1, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n        person_score = pose_entries[n][-2]\n        position_id = -1\n        for keypoint_id in pose_entries[n][:-2]:\n            position_id += 1\n            if position_id == 1:  # no 'neck' in COCO\n                continue\n\n            cx, cy, score, visibility = 0, 0, 0, 0  # keypoint not found\n            if keypoint_id != -1:\n                cx, cy, score = all_keypoints[int(keypoint_id), 0:3]\n                cx = cx + 0.5\n                cy = cy + 0.5\n                visibility = 1\n            keypoints[to_coco_map[position_id] * 3 + 0] = cx\n            keypoints[to_coco_map[position_id] * 3 + 1] = cy\n            keypoints[to_coco_map[position_id] * 3 + 2] = visibility\n        coco_keypoints.append(keypoints)\n        scores.append(person_score * max(0, (pose_entries[n][-1] - 1)))  # -1 for 'neck'\n    return coco_keypoints, scores\n\n\ndef infer(net, img, scales, base_height, stride, pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n    normed_img = normalize(img, img_mean, img_scale)\n    height, width, _ = normed_img.shape\n    scales_ratios = [scale * base_height / float(height) for scale in scales]\n    avg_heatmaps = np.zeros((height, width, 19), dtype=np.float32)\n    avg_pafs = np.zeros((height, width, 38), dtype=np.float32)\n\n    for ratio in scales_ratios:\n        scaled_img = cv2.resize(normed_img, (0, 0), fx=ratio, fy=ratio, interpolation=cv2.INTER_CUBIC)\n        min_dims = [base_height, max(scaled_img.shape[1], base_height)]\n        padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n\n        tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float().cuda()\n        stages_output = net(tensor_img)\n\n        stage2_heatmaps = stages_output[-2]\n        heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n        heatmaps = cv2.resize(heatmaps, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n        heatmaps = heatmaps[pad[0]:heatmaps.shape[0] - pad[2], pad[1]:heatmaps.shape[1] - pad[3]:, :]\n        heatmaps = cv2.resize(heatmaps, (width, height), interpolation=cv2.INTER_CUBIC)\n        avg_heatmaps = avg_heatmaps + heatmaps / len(scales_ratios)\n\n        stage2_pafs = stages_output[-1]\n        pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n        pafs = cv2.resize(pafs, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n        pafs = pafs[pad[0]:pafs.shape[0] - pad[2], pad[1]:pafs.shape[1] - pad[3], :]\n        pafs = cv2.resize(pafs, (width, height), interpolation=cv2.INTER_CUBIC)\n        avg_pafs = avg_pafs + pafs / len(scales_ratios)\n\n    return avg_heatmaps, avg_pafs\n\n\ndef evaluate(labels, output_name, images_folder, net, multiscale=False, visualize=False):\n    net = net.cuda().eval()\n    base_height = 368\n    scales = [1]\n    if multiscale:\n        scales = [0.5, 1.0, 1.5, 2.0]\n    stride = 8\n\n    dataset = CocoValDataset(labels, images_folder)\n    coco_result = []\n    for sample in dataset:\n        file_name = sample['file_name']\n        img = sample['img']\n\n        avg_heatmaps, avg_pafs = infer(net, img, scales, base_height, stride)\n\n        total_keypoints_num = 0\n        all_keypoints_by_type = []\n        for kpt_idx in range(18):  # 19th for bg\n            total_keypoints_num += extract_keypoints(avg_heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n\n        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, avg_pafs)\n\n        coco_keypoints, scores = convert_to_coco_format(pose_entries, all_keypoints)\n\n        image_id = int(file_name[0:file_name.rfind('.')])\n        for idx in range(len(coco_keypoints)):\n            coco_result.append({\n                'image_id': image_id,\n                'category_id': 1,  # person\n                'keypoints': coco_keypoints[idx],\n                'score': scores[idx]\n            })\n\n        if visualize:\n            for keypoints in coco_keypoints:\n                for idx in range(len(keypoints) // 3):\n                    cv2.circle(img, (int(keypoints[idx * 3]), int(keypoints[idx * 3 + 1])),\n                               3, (255, 0, 255), -1)\n            cv2.imshow('keypoints', img)\n            key = cv2.waitKey()\n            if key == 27:  # esc\n                return\n\n    with open(output_name, 'w') as f:\n        json.dump(coco_result, f, indent=4)\n\n    run_coco_eval(labels, output_name)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--labels', type=str, required=True, help='path to json with keypoints val labels')\n    parser.add_argument('--output-name', type=str, default='detections.json',\n                        help='name of output json file with detected keypoints')\n    parser.add_argument('--images-folder', type=str, required=True, help='path to COCO val images folder')\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint')\n    parser.add_argument('--multiscale', action='store_true', help='average inference results over multiple scales')\n    parser.add_argument('--visualize', action='store_true', help='show keypoints')\n    args = parser.parse_args()\n\n    net = PoseEstimationWithMobileNet()\n    checkpoint = torch.load(args.checkpoint_path)\n    load_state(net, checkpoint)\n\n    evaluate(args.labels, args.output_name, args.images_folder, net, args.multiscale, args.visualize)\n"""
pytorch_toolkit/human_pose_estimation/val_single.py,0,"b'import argparse\nimport json\nimport os\nimport numpy as np\n\nimport cv2\nimport torch\nfrom torchvision import transforms\n\nfrom datasets.lip import LipValDataset\nfrom datasets.coco_single import CocoSingleValDataset\nfrom models.single_person_pose_with_mobilenet import SinglePersonPoseEstimationWithMobileNet\nfrom modules.calc_pckh import calc_pckh\nfrom modules.load_state import load_state\nfrom val import run_coco_eval\nfrom datasets.transformations import SinglePersonRandomAffineTransform, Normalization\n\n\ndef extract_keypoints(heatmap, min_confidence=-100):\n    ind = np.unravel_index(np.argmax(heatmap, axis=None), heatmap.shape)\n    if heatmap[ind] < min_confidence:\n        ind = (-1, -1)\n    else:\n        ind = (int(ind[1]), int(ind[0]))\n    return heatmap[ind[1]][ind[0]], ind\n\n\ndef normalize(img, img_mean, img_scale):\n    img = np.array(img, dtype=np.float32)\n    img = (img - img_mean) * img_scale\n    return img\n\n\ndef infer(net, img, scales, base_height, stride, img_mean=[128, 128, 128], img_scale=1/256):\n    height, width, _ = img.shape\n    scales_ratios = [scale * base_height / max(height, width) for scale in scales]\n    avg_heatmaps = np.zeros((height, width, 17), dtype=np.float32)\n\n    for ratio in scales_ratios:\n        resized_img = cv2.resize(img, dsize=None, fx=ratio, fy=ratio, interpolation=cv2.INTER_LINEAR)\n        max_side = max(resized_img.shape[0], resized_img.shape[1])\n\n        padded_img = np.ones((max_side, max_side, 3), dtype=np.uint8) * img_mean\n        x_offset = (padded_img.shape[1] - resized_img.shape[1]) // 2\n        y_offset = (padded_img.shape[0] - resized_img.shape[0]) // 2\n        padded_img[y_offset:y_offset + resized_img.shape[0], x_offset:x_offset + resized_img.shape[1], :] = resized_img\n        padded_img = normalize(padded_img, img_mean, img_scale)\n        pad = [y_offset, x_offset,\n               padded_img.shape[0] - resized_img.shape[0] - y_offset,\n               padded_img.shape[1] - resized_img.shape[1] - x_offset]\n\n        tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float().cuda()\n        stages_output = net(tensor_img)\n\n        heatmaps = np.transpose(stages_output[-1].squeeze().cpu().data.numpy(), (1, 2, 0))\n        heatmaps = cv2.resize(heatmaps, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n        heatmaps = heatmaps[pad[0]:heatmaps.shape[0] - pad[2], pad[1]:heatmaps.shape[1] - pad[3]:, :]\n        heatmaps = cv2.resize(heatmaps, (width, height), interpolation=cv2.INTER_CUBIC)\n        avg_heatmaps = avg_heatmaps + heatmaps / len(scales_ratios)\n\n    return avg_heatmaps\n\n\ndef evaluate(dataset, output_name, net, multiscale=False, visualize=False):\n    net = net.cuda().eval()\n    base_height = 256\n    scales = [1]\n    if multiscale:\n        scales = [0.75, 1.0, 1.25]\n    stride = 8\n\n    res_file = open(output_name, \'w\')\n    for sample_id in range(len(dataset)):\n        sample = dataset[sample_id]\n        file_name = sample[\'file_name\']\n        img = sample[\'image\']\n\n        avg_heatmaps = infer(net, img, scales, base_height, stride)\n\n        flip = False\n        if flip:\n            flipped_img = cv2.flip(img, 1)\n            flipped_avg_heatmaps = infer(net, flipped_img, scales, base_height, stride)\n            orig_order = [0, 1, 2, 10, 11, 12]\n            flip_order = [5, 4, 3, 15, 14, 13]\n            for r, l in zip(orig_order, flip_order):\n                flipped_avg_heatmaps[:, :, r], flipped_avg_heatmaps[:, :, l] =\\\n                    flipped_avg_heatmaps[:, :, l].copy(), flipped_avg_heatmaps[:, :, r].copy()\n            avg_heatmaps = (avg_heatmaps + flipped_avg_heatmaps[:, ::-1]) / 2\n\n        all_keypoints = []\n        for kpt_idx in range(dataset._num_keypoints):\n            all_keypoints.append(extract_keypoints(avg_heatmaps[:, :, kpt_idx]))\n\n        res_file.write(\'{}\'.format(file_name))\n        for id in range(dataset._num_keypoints):\n            score, ind = all_keypoints[id]\n            val = [int(ind[0]), int(ind[1])]\n            if val[0] == -1:\n                val[0], val[1] = \'nan\', \'nan\'\n            res_file.write(\',{},{}\'.format(val[0], val[1]))\n        res_file.write(\'\\n\')\n\n        if visualize:\n            kpt_names = [\'r_ank\', \'r_kne\', \'r_hip\', \'l_hip\', \'l_kne\', \'l_ank\', \'pel\', \'spi\', \'nec\', \'hea\',\n                         \'r_wri\', \'r_elb\', \'r_sho\', \'l_sho\', \'l_elb\', \'l_wri\']\n            colors = [(255, 0, 0), (255, 0, 0), (255, 0, 0), (0, 0, 255), (0, 0, 255), (0, 0, 255), (0, 255, 0),\n                      (0, 255, 0), (0, 255, 0), (0, 255, 0),\n                      (255, 0, 0), (255, 0, 0), (255, 0, 0), (0, 0, 255), (0, 0, 255), (0, 0, 255)]\n            for id in range(len(all_keypoints)):\n                score, keypoint = all_keypoints[id]\n                if keypoint[0] != -1:\n                    radius = 3\n                    if colors[id] == (255, 0, 0):\n                        cv2.circle(img, (int(keypoint[0]), int(keypoint[1])),\n                                   radius + 2, (255, 0, 0), -1)\n                    else:\n                        cv2.circle(img, (int(keypoint[0]), int(keypoint[1])),\n                                   radius, colors[id], -1)\n            cv2.imshow(\'keypoints\', img)\n            key = cv2.waitKey()\n            if key == 27:  # esc\n                break\n\n    res_file.close()\n\n\ndef affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.])\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef coco_evaluate(dataset, output_name, net, visualize=False):\n    net = net.cuda().eval()\n\n    coco_result = []\n    with torch.no_grad():\n        for sample_id in range(len(dataset)):\n            sample = dataset[sample_id]\n            img = sample[\'image\']\n            tensor_img = torch.from_numpy(img[None, ]).float().cuda()\n            stages_output = net(tensor_img)\n            heatmaps = np.transpose(stages_output[-1].squeeze().cpu().data.numpy(), (1, 2, 0))\n\n            sum_score = 0\n            all_keypoints = []\n            scores = []\n            sum_score_thr = 0\n            num_kp_thr = 0\n            for kpt_idx in range(dataset._num_keypoints):\n                score, coord = extract_keypoints(heatmaps[:, :, kpt_idx])\n                scores.append(score)\n                all_keypoints.append(affine_transform(coord, sample[\'rev_trans\']))\n                sum_score += score\n                if score > 0.2:\n                    sum_score_thr += score\n                    num_kp_thr += 1\n            if num_kp_thr > 0:\n                pose_score = sum_score_thr / num_kp_thr\n            else:\n                pose_score = sum_score / dataset._num_keypoints\n\n            coco_format_keypoints = []\n            for ind in range(dataset._num_keypoints):\n                    coco_format_keypoints.append(all_keypoints[ind][0])\n                    coco_format_keypoints.append(all_keypoints[ind][1])\n                    coco_format_keypoints.append(1)\n\n            coco_result.append({\n                \'image_id\': sample[\'image_id\'],\n                \'category_id\': 1,  # person\n                \'keypoints\': coco_format_keypoints,\n                \'score\': pose_score\n            })\n\n            if visualize:\n                kpt_names = [\'nose\', \'left_eye\', \'right_eye\', \'left_ear\', \'right_ear\', \'left_shoulder\',\n                             \'right_shoulder\', \'left_elbow\', \'right_elbow\', \'left_wrist\', \'right_wrist\',\n                             \'left_hip\', \'right_hip\', \'left_knee\', \'right_knee\', \'left_ankle\', \'right_ankle\']\n                colors = [(0, 0, 255),\n                          (255, 0, 0), (0, 255, 0), (255, 0, 0), (0, 255, 0),\n                          (255, 0, 0), (0, 255, 0), (255, 0, 0), (0, 255, 0),\n                          (255, 0, 0), (0, 255, 0), (255, 0, 0), (0, 255, 0),\n                          (255, 0, 0), (0, 255, 0), (255, 0, 0), (0, 255, 0)]\n\n                for id in range(len(all_keypoints)):\n                    keypoint = all_keypoints[id]\n                    if keypoint[0] != -1:\n                        radius = 3\n                        if colors[id] == (255, 0, 0):\n                            cv2.circle(sample[\'input\'], (int(keypoint[0]), int(keypoint[1])),\n                                       radius + 2, (255, 0, 0), -1)\n                        else:\n                            cv2.circle(sample[\'input\'], (int(keypoint[0]), int(keypoint[1])),\n                                       radius, colors[id], -1)\n                cv2.imshow(\'keypoints\', sample[\'input\'])\n                key = cv2.waitKey()\n\n        with open(output_name, \'w\') as res_file:\n            json.dump(coco_result, res_file, indent=4)\n\n\ndef val(net, val_dataset, predictions_name, name_dataset):\n    if name_dataset == ""Lip"":\n        evaluate(val_dataset, predictions_name, net)\n        pck = calc_pckh(val_dataset.labels_file_path, predictions_name)\n        val_loss = 100 - pck[-1][-1]\n    elif name_dataset == ""CocoSingle"":\n        coco_evaluate(val_dataset, predictions_name, net)\n        ap_metric = run_coco_eval(os.path.join(val_dataset._dataset_folder, \'annotations\', \'person_keypoints_val2017.json\'),\n                                    predictions_name)\n        val_loss = 100 - ap_metric[0] * 100\n    else:\n        raise RuntimeError(""Unknown dataset."")\n\n    return val_loss\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dataset-folder\', type=str, required=True, help=\'path to dataset folder\')\n    parser.add_argument(\'--output-name\', type=str, default=\'detections.csv\',\n                        help=\'name of output file with detected keypoints\')\n    parser.add_argument(\'--checkpoint-path\', type=str, required=True, help=\'path to the checkpoint\')\n    parser.add_argument(\'--multiscale\', action=\'store_true\', help=\'average inference results over multiple scales\')\n    parser.add_argument(\'--visualize\', action=\'store_true\', help=\'show keypoints\')\n    parser.add_argument(\'--name-dataset\', type=str, required=True, choices=[\'CocoSingle\', \'Lip\'],\n                        help=\'name dataset for validation: <Lip> or <CocoSingle>\')\n    args = parser.parse_args()\n\n    if args.name_dataset == \'CocoSingle\':\n        val_dataset = CocoSingleValDataset(args.dataset_folder, transform=transforms.Compose([\n                                         SinglePersonRandomAffineTransform(mode=\'val\'),\n                                         Normalization(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n        num_heatmaps = val_dataset._num_keypoints\n    elif args.name_dataset == ""Lip"":\n        val_dataset = LipValDataset(args.dataset_folder)\n        num_heatmaps = val_dataset._num_keypoints + 1\n    else:\n        raise RuntimeError(""Unknown dataset."")\n\n    net = SinglePersonPoseEstimationWithMobileNet(num_refinement_stages=5, num_heatmaps=num_heatmaps)\n    checkpoint = torch.load(args.checkpoint_path)\n    load_state(net, checkpoint)\n\n    val_loss = val(net, val_dataset, args.output_name, args.name_dataset)\n\n    print(\'Val loss: {}\'.format(val_loss))\n'"
pytorch_toolkit/instance_segmentation/setup.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\n\nfrom setuptools import setup, find_packages\nimport torch\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension, CUDA_HOME\n\n\nthis_dir = osp.dirname(osp.abspath(__file__))\nextensions_dir = osp.join(this_dir, \'segmentoly\', \'extensions\')\nsources = [\n    osp.join(extensions_dir, \'extensions.cpp\'),\n    osp.join(extensions_dir, \'nms\', \'nms.cpp\'),\n    osp.join(extensions_dir, \'nms\', \'cpu\', \'nms_kernel.cpp\'),\n    osp.join(extensions_dir, \'roi_align\', \'roi_align.cpp\'),\n    osp.join(extensions_dir, \'roi_align\', \'cpu\', \'roi_align_kernel.cpp\'),\n]\n\nextension = CppExtension\nmacroses = []\n\nif torch.cuda.is_available() and CUDA_HOME is not None:\n    sources_gpu = [\n        osp.join(extensions_dir, \'nms\', \'gpu\', \'nms_kernel.cu\'),\n        osp.join(extensions_dir, \'roi_align\', \'gpu\', \'roi_align_kernel.cu\'),\n        osp.join(extensions_dir, \'deformable_conv\', \'deform_conv_kernel.cu\'),\n    ]\n    extension = CUDAExtension\n    sources += sources_gpu\n    macroses += [(""WITH_CUDA"", None)]\n\nsetup(name=\'segmentoly\',\n      ext_modules=[extension(name=\'segmentoly.extensions._EXTRA\',\n                             sources=sources,\n                             include_dirs=[extensions_dir],\n                             define_macros=macroses)\n                   ],\n      cmdclass={\'build_ext\': BuildExtension},\n      packages=find_packages()\n      )\n'"
pytorch_toolkit/nncf/setup.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport re\nimport sys\nimport codecs\nimport setuptools\nimport glob\nimport sysconfig\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\n\ndef read(*parts):\n    with codecs.open(os.path.join(here, *parts), \'r\') as fp:\n        return fp.read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"",\n                              version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n\nINSTALL_REQUIRES = [""ninja"",\n                    ""addict"",\n                    ""pillow==6.2.1"",\n                    ""texttable"",\n                    ""scipy==1.3.2"",\n                    ""pyyaml"",\n                    ""networkx"",\n                    ""graphviz"",\n                    ""jsonschema"",\n                    ""pydot"",\n                    ""tensorboardX"",\n                    ""jstyleson"",\n                    ""matplotlib==3.0.3"",\n                    ""numpy"",\n                    ""tqdm"",\n                    ""onnx"",\n                    ""opencv-python"",\n                    ""pytest-mock"",\n                    ""prettytable"",\n                    ""mdutils"",\n                    ""yattag"",\n                    ""jsonschema"",\n                    ""wheel""]\n\nDEPENDENCY_LINKS = []\nif ""--cpu-only"" in sys.argv:\n    INSTALL_REQUIRES.extend([""torch"", ""torchvision""])\n    if sys.version_info[:2] == (3, 5):\n        DEPENDENCY_LINKS = [\n            \'https://download.pytorch.org/whl/cpu/torch-1.3.1%2Bcpu-cp35-cp35m-linux_x86_64.whl\',\n            \'https://download.pytorch.org/whl/cpu/torchvision-0.4.2%2Bcpu-cp35-cp35m-linux_x86_64.whl\']\n    elif sys.version_info[:2] == (3, 6):\n        DEPENDENCY_LINKS = [\n            \'https://download.pytorch.org/whl/cpu/torch-1.3.1%2Bcpu-cp36-cp36m-linux_x86_64.whl\',\n            \'https://download.pytorch.org/whl/cpu/torchvision-0.4.2%2Bcpu-cp36-cp36m-linux_x86_64.whl\']\n    elif sys.version_info[:2] >= (3, 7):\n        DEPENDENCY_LINKS = [\n            \'https://download.pytorch.org/whl/cpu/torch-1.3.1%2Bcpu-cp37-cp37m-linux_x86_64.whl\',\n            \'https://download.pytorch.org/whl/cpu/torchvision-0.4.2%2Bcpu-cp37-cp37m-linux_x86_64.whl\']\n    else:\n        print(""Only Python > 3.5 is supported"")\n        sys.exit(0)\n    KEY = [""CPU""]\n    sys.argv.remove(""--cpu-only"")\nelse:\n    INSTALL_REQUIRES.extend([""torch==1.3.1"", ""torchvision==0.4.2""])\n    KEY = [""GPU""]\n\n\nEXTRAS_REQUIRE = {\n    ""tests"": [\n        ""pytest""],\n    ""docs"": []\n}\n\npackage_data = {\'nncf\': [\'quantization/cpu/functions_cpu.cpp\',\n                         \'quantization/cuda/functions_cuda.cpp\',\n                         \'quantization/cuda/functions_cuda_kernel.cu\',\n                         \'binarization/cpu/functions_cpu.cpp\',\n                         \'binarization/cuda/functions_cuda.cpp\',\n                         \'binarization/cuda/functions_cuda_kernel.cu\']}\n\n\nsetuptools.setup(\n    name=""nncf"",\n    version=find_version(os.path.join(here, ""nncf/version.py"")),\n    author=""Intel"",\n    author_email=""alexander.kozlov@intel.com"",\n    description=""Neural Networks Compression Framework"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/opencv/openvino-training-extensions"",\n    packages=setuptools.find_packages(),\n    dependency_links=DEPENDENCY_LINKS,\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: OS Independent"",\n    ],\n    install_requires=INSTALL_REQUIRES,\n    extras_require=EXTRAS_REQUIRE,\n    package_data=package_data,\n    keywords=KEY\n)\n\npath_to_ninja = glob.glob(str(sysconfig.get_paths()[""purelib""]+""/ninja*/ninja/data/bin/""))\nif path_to_ninja:\n    path_to_ninja = str(path_to_ninja[0]+""ninja"")\n    if not os.access(path_to_ninja, os.X_OK):\n        os.chmod(path_to_ninja, 755)\n'"
pytorch_toolkit/open_closed_eye/demo.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport argparse\nimport cv2 as cv\nfrom utils.ie_tools import load_ie_model\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Eye state classifier\')\n    parser.add_argument(\'-m\', \'--model\', required=True, help=\'Path to model xml file\')\n    parser.add_argument(\'-d\', \'--data_root\', required=True, help=\'Directory with validation images\')\n    args = parser.parse_args()\n    return args\n\n\ndef load_eye_db(root_dir):\n    data = []\n    for subdir, dirs, files in os.walk(root_dir):\n        for i, file in enumerate(files):                \n            full_path = os.path.join(subdir, file)\n            state = 1 if file[0] == \'o\' else 0\n            data.append({\'filename\': full_path, \'label\': state})\n    return data\n\n\ndef main():\n    args = parse_args()\n\n    test_db = load_eye_db(args.data_root)\n    net = load_ie_model(args.model, \'CPU\', None)\n    _, _, height, width = net.get_input_shape().shape\n\n    for sample in test_db:\n        img = cv.imread(sample[\'filename\'])\n        assert not img is None\n        h, w, _ = img.shape\n        out = net.forward(cv.resize(img, (width, height)))\n        is_open = out[0][0][0][0] < out[0][1][0][0]\n        if is_open:\n            cv.rectangle(img, (1, 1), (w-1, h-1), (0, 255, 0), 2)\n        else:\n            cv.rectangle(img, (1, 1), (w-1, h-1), (0, 0, 255), 2)\n        cv.imshow(""Eye"", img)\n        cv.waitKey(0)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/open_closed_eye/train.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.dataset_eye import EyeDataset\nimport torch.onnx\nimport os.path\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=3)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n        self.conv3 = nn.Conv2d(20, 50, kernel_size=3)\n        self.conv4 = nn.Conv2d(50, 2, kernel_size=1, bias=False, padding=0, stride=1)\n        self.max_pool2d = nn.MaxPool2d((4, 4))\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.max_pool2d(x)\n        x = self.softmax(x)\n        return x\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Eye state classifier\')\n    parser.add_argument(\'-d\', \'--data_root\', required=True, help=\'Path to dataset\')\n    parser.add_argument(\'-e\', \'--epoch\', default=20, type=int, help=\'Number of epochs\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=124, type=int)\n    parser.add_argument(\'-l\', \'--learning_rate\', default=0.1, type=float, help=\'Star learning rate\')\n    parser.add_argument(\'-p\', \'--pretrained\', help=\'Path to pretrained weights\')\n    parser.add_argument(\'-m\', \'--model_dir\', default=\'model\', help=\'Path to save snapshots\')\n    args = parser.parse_args()\n    return args\n\n\ndef train(net, loader, device, optimizer):\n    running_loss = 0.0\n    criterion = nn.CrossEntropyLoss()\n    for i, (inputs, labels, _) in enumerate(loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs).view(len(labels), 2)\n       \n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        running_loss += loss\n        # if i % 1 == 9:    # print every 20 mini-batches\n        print(\'iter: %5d loss: %.3f\' % (i + 1, running_loss / 10))\n        running_loss = 0.0\n\n\ndef test(net, loader, device):\n    corrected = 0.\n    total = 0.\n    with torch.no_grad():\n        for i, (inputs, labels, fname) in enumerate(loader):           \n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = net(inputs)\n            _, predicted = torch.max(outputs, 1)\n            \n            for label, pred in zip(labels, predicted):\n                if label == pred[0][0]:\n                    corrected += 1 \n                total += 1\n\n    print(""Test accuracy: {}"".format(corrected / total))\n\n\ndef main():\n    args = parse_args()\n    torch.manual_seed(100)\n    device = torch.device(""cuda"")\n    net = Net().to(device)\n    if args.pretrained:\n        net.load_state_dict(torch.load(args.pretrained))\n\n    lr = args.learning_rate\n    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n\n    train_transforms = transforms.Compose([\n                        transforms.Resize((32, 32)),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.ToTensor(),\n                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[1, 1, 1], inplace=False),\n                        ])\n    test_transforms = transforms.Compose([\n                        transforms.Resize((32, 32)),\n                        transforms.ToTensor(),\n                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[1, 1, 1], inplace=False),\n                        ])\n\n    train_db = EyeDataset(args.data_root, \'train\', train_transforms)\n    test_db = EyeDataset(args.data_root, \'val\', test_transforms)\n\n    train_db_loader = torch.utils.data.DataLoader(train_db, batch_size=args.batch_size, shuffle=True, num_workers=5)\n    test_db_loader = torch.utils.data.DataLoader(test_db, batch_size=1, num_workers=1)\n\n    if not os.path.exists(args.model_dir):\n        os.mkdir(args.model_dir)\n\n    for epoch in range(args.epoch):         \n        if epoch in [10, 15, 25]:\n            lr = 0.1 * lr\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n        print (\'epoch={}, lr={}\'.format(epoch, lr))\n        train(net, train_db_loader, device, optimizer)\n        test(net, test_db_loader, device)\n\n        snap_path = os.path.join(args.model_dir, ""open_closed_eye_epoch_{}.pth"".format(epoch))\n        onnx_path = os.path.join(args.model_dir, ""open_closed_eye_epoch_{}.onnx"".format(epoch))\n        torch.save(net.state_dict(), snap_path)\n        dummy_input = torch.randn(1, 3, 32, 32, requires_grad=False).to(device)\n        torch.onnx.export(net, dummy_input, onnx_path, export_params=True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/person_reidentification/__init__.py,0,b''
pytorch_toolkit/person_reidentification/convert_to_onnx.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom config.default_config import get_default_config\n\nfrom data.transforms import build_transforms\nfrom torchreid.utils import load_pretrained_weights\n\nfrom models.builder import build_model\n\n\ndef main():\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--config-file\', type=str, default=\'\', help=\'path to config file\')\n    parser.add_argument(\'--output-name\', type=str, default=\'model\')\n    parser.add_argument(\'--verbose\', default=False, action=\'store_true\',\n                        help=\'Verbose mode for onnx.export\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER,\n                        help=\'Modify config options using the command-line\')\n    args = parser.parse_args()\n\n    cfg = get_default_config()\n    if args.config_file:\n        cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    model = build_model(\n            name=cfg.model.name,\n            num_classes=1041,  # Does not matter in conversion\n            loss=cfg.loss.name,\n            pretrained=False,\n            use_gpu=True,\n            feature_dim=cfg.model.feature_dim,\n            fpn_cfg=cfg.model.fpn,\n            pooling_type=cfg.model.pooling_type,\n            input_size=(cfg.data.height, cfg.data.width),\n            dropout_cfg=cfg.model.dropout,\n            IN_first=cfg.model.IN_first,\n            extra_blocks=cfg.model.extra_blocks\n        )\n\n    load_pretrained_weights(model, cfg.model.load_weights)\n    model.eval()\n\n    _, transform = build_transforms(\n        cfg.data.height, cfg.data.width,\n        transforms=cfg.data.transforms,\n        norm_mean=cfg.data.norm_mean,\n        norm_std=cfg.data.norm_std,\n        apply_masks_to_test=False\n    )\n\n    input_size = (cfg.data.height, cfg.data.width, 3)\n    img = np.random.rand(*input_size).astype(np.float32)\n    img = np.uint8(img * 255)\n    im = Image.fromarray(img)\n    blob = transform(im).unsqueeze(0)\n\n    torch.onnx.export(model, blob, args.output_name + \'.onnx\',\n                      verbose=False, export_params=True,\n                      input_names=[\'data\'], output_names=[\'reid_embedding\'],\n                      opset_version=9)  # 9th version resolves nearest upsample issue\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/person_reidentification/main.py,0,"b'""""""\nMIT License\n\nCopyright (c) 2018 Kaiyang Zhou\n""""""\n""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nimport os\nimport os.path as osp\nimport time\nimport argparse\n\nimport torch\nimport torch.nn as nn\n\nfrom config.default_config import (\n    get_default_config, imagedata_kwargs, videodata_kwargs,\n    optimizer_kwargs, lr_scheduler_kwargs, engine_run_kwargs\n)\n\nimport torchreid\nfrom torchreid.utils import (\n    Logger, set_random_seed, check_isfile, resume_from_checkpoint,\n    load_pretrained_weights, compute_model_complexity, collect_env_info\n)\n\nfrom data.datamanager import ImageDataManagerWithTransforms\nfrom engine.builder import build_engine\nfrom engine.schedulers.lr_scheduler import build_lr_scheduler\nfrom models.builder import build_model\n\n\ndef build_datamanager(cfg):\n    if cfg.data.type == \'image\':\n        return ImageDataManagerWithTransforms(**imagedata_kwargs(cfg))\n    else:\n        return torchreid.data.VideoDataManager(**videodata_kwargs(cfg))\n\n\ndef reset_config(cfg, args):\n    if args.root:\n        cfg.data.root = args.root\n    if args.sources:\n        cfg.data.sources = args.sources\n    if args.targets:\n        cfg.data.targets = args.targets\n\n\ndef main():\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--config-file\', type=str, default=\'\', help=\'path to config file\')\n    parser.add_argument(\'-s\', \'--sources\', type=str, nargs=\'+\', help=\'source datasets (delimited by space)\')\n    parser.add_argument(\'-t\', \'--targets\', type=str, nargs=\'+\', help=\'target datasets (delimited by space)\')\n    parser.add_argument(\'--root\', type=str, default=\'\', help=\'path to data root\')\n    parser.add_argument(\'opts\', default=None, nargs=argparse.REMAINDER,\n                        help=\'Modify config options using the command-line\')\n    args = parser.parse_args()\n\n    cfg = get_default_config()\n    cfg.use_gpu = torch.cuda.is_available()\n    if args.config_file:\n        cfg.merge_from_file(args.config_file)\n    reset_config(cfg, args)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n    set_random_seed(cfg.train.seed)\n\n    log_name = \'test.log\' if cfg.test.evaluate else \'train.log\'\n    log_name += time.strftime(\'-%Y-%m-%d-%H-%M-%S\')\n    sys.stdout = Logger(osp.join(cfg.data.save_dir, log_name))\n\n    print(\'Show configuration\\n{}\\n\'.format(cfg))\n    print(\'Collecting env info ...\')\n    print(\'** System info **\\n{}\\n\'.format(collect_env_info()))\n\n    if cfg.use_gpu:\n        torch.backends.cudnn.benchmark = True\n\n    datamanager = build_datamanager(cfg)\n\n    print(\'Building model: {}\'.format(cfg.model.name))\n    model = build_model(\n        name=cfg.model.name,\n        num_classes=datamanager.num_train_pids,\n        loss=cfg.loss.name,\n        pretrained=cfg.model.pretrained,\n        use_gpu=cfg.use_gpu,\n        dropout_cfg=cfg.model.dropout,\n        feature_dim=cfg.model.feature_dim,\n        fpn_cfg=cfg.model.fpn,\n        pooling_type=cfg.model.pooling_type,\n        input_size=(cfg.data.height, cfg.data.width),\n        IN_first=cfg.model.IN_first,\n        extra_blocks=cfg.model.extra_blocks\n    )\n    num_params, flops = compute_model_complexity(model, (1, 3, cfg.data.height, cfg.data.width))\n    print(\'Model complexity: params={:,} flops={:,}\'.format(num_params, flops))\n\n    if cfg.model.load_weights and check_isfile(cfg.model.load_weights):\n        load_pretrained_weights(model, cfg.model.load_weights)\n\n    if cfg.use_gpu:\n        model = nn.DataParallel(model).cuda()\n\n    optimizer = torchreid.optim.build_optimizer(model, **optimizer_kwargs(cfg))\n    scheduler = build_lr_scheduler(optimizer, **lr_scheduler_kwargs(cfg))\n\n    if cfg.model.resume and check_isfile(cfg.model.resume):\n        args.start_epoch = resume_from_checkpoint(cfg.model.resume, model, optimizer=optimizer)\n\n    if len(cfg.model.openvino.name):\n        from models.openvino_wrapper import OpenVINOModel\n        openvino_model = OpenVINOModel(cfg.model.openvino.name, cfg.model.openvino.cpu_extension)\n    else:\n        openvino_model = None\n\n    print(\'Building {}-engine for {}-reid\'.format(cfg.loss.name, cfg.data.type))\n    engine = build_engine(cfg, datamanager, model, optimizer, scheduler, openvino_model=openvino_model)\n    engine.run(**engine_run_kwargs(cfg))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/segthor/setup.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n    REQUIRED = f.read().splitlines()\n\n# Add Model Optimizer requirements\nrequirements_onnx = os.path.join(os.environ.get(\'INTEL_OPENVINO_DIR\', \'/opt/intel/openvino\'),\n                                 \'./deployment_tools/model_optimizer/requirements_onnx.txt\')\nif os.path.isfile(requirements_onnx):\n    with open(requirements_onnx) as f:\n        REQUIRED += f.read().splitlines()\n\nsetup(\n    name=\'segthor\',\n    install_requires=REQUIRED,\n)\n'"
pytorch_toolkit/super_resolution/setup.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n    REQUIRED = f.read().splitlines()\n\n# Add Model Optimizer requirements\nrequirements_onnx = os.path.join(os.environ.get(\'INTEL_OPENVINO_DIR\', \'/opt/intel/openvino\'),\n                                 \'./deployment_tools/model_optimizer/requirements_onnx.txt\')\nif os.path.isfile(requirements_onnx):\n    with open(requirements_onnx) as f:\n        REQUIRED += f.read().splitlines()\n\nsetup(\n    name=\'sr\',\n    install_requires=REQUIRED,\n)\n'"
pytorch_toolkit/text_spotting/setup.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom setuptools import setup, find_packages\n\nsetup(name=\'text_spotting\',\n      version=\'1.0\',\n      packages=find_packages())\n'"
pytorch_toolkit/utils/pytorch_to_onnx.py,0,"b'import argparse\nimport sys\n\nimport torch.onnx\n\n\nTORCHVISION_MODELS = [\n    \'resnet-v1-50\',\n    \'inception-v3\'\n]\n\nPUBLIC_PYTORCH_MODELS = [\n    \'mobilenet-v2\'\n]\n\nSUPPORTED_MODELS = TORCHVISION_MODELS + PUBLIC_PYTORCH_MODELS\n\ndef positive_int_arg(value):\n    """"""Check positive integer type for input argument""""""\n    try:\n        ivalue = int(value)\n        if ivalue > 0:\n            return ivalue\n        raise argparse.ArgumentTypeError(\'Argument must be a positive integer\')\n    except Exception as exc:\n        print(exc)\n        sys.exit(\'Invalid value for input argument: {!r}, a positive integer is expected\'.format(value))\n\ndef parse_args():\n    """"""Parse input arguments""""""\n\n    parser = argparse.ArgumentParser(description=\'Conversion script for pretrained models from PyTorch to ONNX\')\n    parser.add_argument(\'--model-name\', type=str, required=True,\n                        help=\'Model name to convert.\'\n                             \' One of the \' + \', \'.join(SUPPORTED_MODELS) + \' is supported\')\n    parser.add_argument(\'--weights\', type=str, required=True,\n                        help=\'Path to the PyTorch pretrained weights\')\n    parser.add_argument(\'--input-shape\', type=positive_int_arg, nargs=4,\n                        metavar=\'INPUT_DIM\', required=True,\n                        help=\'Shape of the input blob\')\n    parser.add_argument(\'--output-file\', type=str, required=True,\n                        help=\'Path to the output ONNX model\')\n\n    parser.add_argument(\'--model-path\', type=str,\n                        help=\'Path to python model description if model is not a standard torchvision model\')\n    parser.add_argument(\'--input-names\', type=str, nargs=\'+\',\n                        help=\'Space separated names of the input layers\')\n    parser.add_argument(\'--output-names\', type=str, nargs=\'+\',\n                        help=\'Space separated names of the output layers\')\n    return parser.parse_args()\n\ndef load_model(model_name, weights, model_path=None):\n    """"""Import model and load pretrained weights""""""\n\n    if model_name not in SUPPORTED_MODELS:\n        sys.exit(\'Only \' + \', \'.join(SUPPORTED_MODELS) + \' are available for conversion\')\n\n    if model_name in TORCHVISION_MODELS:\n        try:\n            import torchvision\n            if model_name == \'resnet-v1-50\':\n                model = torchvision.models.resnet50()\n            elif model_name == \'inception-v3\':\n                model = torchvision.models.inception_v3()\n        except ImportError as exc:\n            print(exc)\n            sys.exit(\'The torchvision package was not found.\'\n                     \'Please install it to default location or \'\n                     \'update PYTHONPATH environment variable \'\n                     \'with the path to the installed torchvision package.\')\n    else:\n        sys.path.append(model_path)\n        try:\n            from MobileNetV2 import MobileNetV2\n            model = MobileNetV2(n_class=1000)\n        except ImportError as exc:\n            print(exc)\n            sys.exit(\'MobileNetV2 can not be imported. \' +\n                     \'Please provide valid path to python model description with use of --model-path argument\')\n\n    model.load_state_dict(torch.load(weights, map_location=\'cpu\'))\n    return model\n\ndef convert_to_onnx(pytorch_model, input_shape, output_file, input_names, output_names):\n    """"""Convert PyTorch model to ONNX and check the resulting onnx model""""""\n\n    pytorch_model.eval()\n    onnx_input_shape = torch.randn(input_shape)\n    torch.onnx.export(pytorch_model, onnx_input_shape, output_file,\n                      verbose=True, input_names=input_names, output_names=output_names)\n\n    # Model check after conversion\n    import onnx\n    model_from_onnx = onnx.load(output_file)\n    try:\n        onnx.checker.check_model(model_from_onnx)\n        print(\'ONNX check passed successfully.\')\n    except onnx.onnx_cpp2py_export.checker.ValidationError as exc:\n        sys.exit(\'ONNX check failed with error: \' + str(exc))\n\ndef main():\n    args = parse_args()\n    model = load_model(args.model_name, args.weights, args.model_path)\n    convert_to_onnx(model, args.input_shape, args.output_file, args.input_names, args.output_names)\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/setup.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n    REQUIRED = f.read().splitlines()\n\nsetup(\n    name=\'action-detection\',\n    install_requires=REQUIRED\n)\n'"
tensorflow_toolkit/image_retrieval/setup.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n    REQUIRED = f.read().splitlines()\n\nsetup(\n    name=\'image_retrieval\',\n    install_requires=REQUIRED,\n)\n'"
tensorflow_toolkit/lpr/setup.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n  REQUIRED = f.read().splitlines()\n\nif os.environ.get(\'CPU_ONLY\', None) == \'true\':\n  REQUIRED = [p.replace(\'-gpu\', \'\') if p.startswith(\'tensorflow\') else p for p in REQUIRED]\n\nsetup(\n  name=\'lpr\',\n  install_requires=REQUIRED\n)\n'"
tensorflow_toolkit/ssd_detector/setup.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n  REQUIRED = f.read().splitlines()\n\n# Under the Travis-CI we have to use a CPU version\nif os.environ.get(\'CPU_ONLY\', None) == \'true\':\n  REQUIRED = [p.replace(\'-gpu\', \'\') if p.startswith(\'tensorflow\') else p for p in REQUIRED]\n\nsetup(\n  name=\'ssd_detector\',\n  version=\'0.2.2.dev0\',\n  install_requires=REQUIRED\n)\n'"
tensorflow_toolkit/text_detection/setup.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n    REQUIRED = f.read().splitlines()\n\nsetup(\n    name=\'text_detection\',\n    install_requires=REQUIRED\n)\n'"
tensorflow_toolkit/text_recognition/setup.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n    REQUIRED = f.read().splitlines()\n\nsetup(\n    name=\'text-recognizer\',\n    install_requires=REQUIRED\n)\n'"
tensorflow_toolkit/utils/__init__.py,0,b''
tensorflow_toolkit/utils/setup.py,1,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n  REQUIRED = f.read().splitlines()\n\n# Add requirements for model optimizer\nmo_requirements = os.path.join(os.environ.get(\'INTEL_OPENVINO_DIR\', \'\'),\n                               \'deployment_tools/model_optimizer/requirements_tf.txt\')\nif os.path.isfile(mo_requirements):\n  with open(mo_requirements) as f:\n    REQUIRED += [x for x in f.read().splitlines() if not x.startswith(\'tensorflow\')]\n\nsetup(\n  name=\'tfutils\',\n  install_requires=REQUIRED\n)\n'"
tensorflow_toolkit/vehicle_attributes/setup.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom setuptools import setup\n\nwith open(\'./requirements.txt\') as f:\n  REQUIRED = f.read().splitlines()\n\nsetup(\n  name=\'vehicle_attributes\',\n  install_requires=REQUIRED\n)\n'"
pytorch_toolkit/action_recognition/action_recognition/__init__.py,0,b''
pytorch_toolkit/action_recognition/action_recognition/annotation.py,0,"b'import cv2\nimport numpy as np\n\nfrom .utils import load_json, load_value_file\n\n\ndef get_video_names_and_annotations(data, subset):\n    """"""Selects clips of a given subset from the parsed json annotation""""""\n    video_names = []\n    annotations = []\n\n    for key, value in data[\'database\'].items():\n        this_subset = value[\'subset\']\n        if this_subset == subset:\n            video_name = key\n            label = value[\'annotations\'].get(\'label\', \'\')\n            if label:\n                video_name = label + \'/\' + video_name\n            video_names.append(video_name)\n            annotations.append(value)\n\n    return video_names, annotations\n\n\ndef get_video_props(video_path, video_format, annotation):\n    """"""Tries to read video properties (total number of frames and FPS) from annotation\n    file or read it from file otherwise""""""\n\n    n_frames = annotation.get(\'n_frames\')\n    fps = annotation.get(\'fps\')\n    if n_frames and fps:\n        return n_frames, fps\n\n    if video_format == \'frames\':\n        if not video_path.exists():\n            return 0, 0\n        n_frames = int(load_value_file(video_path / \'n_frames\'))\n        fps = 30\n    else:\n        cap = cv2.VideoCapture(video_path.as_posix())\n        n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n    return n_frames, fps\n\n\ndef load_json_annotation(root_path, annotation_path, subset, flow_path=None, video_format=\'frames\'):\n    """"""Load annotation in ActivityNet-like format""""""\n    data = load_json(annotation_path)\n    video_names, annotations = get_video_names_and_annotations(data, subset)\n\n    idx_to_class = dict(enumerate(data[\'labels\']))\n    class_to_idx = {v: k for k, v in idx_to_class.items()}\n\n    videos = []\n    for i, (video_name, annotation) in enumerate(zip(video_names, annotations)):\n        if i % 1000 == 0:\n            print(\'dataset loading [{}/{}]\'.format(i, len(video_names)))\n\n        if video_format == \'video\' and not video_name.lower().endswith(\'.mp4\'):\n            video_name += \'.mp4\'\n\n        video_path = root_path / video_name\n\n        n_frames, fps = get_video_props(video_path, video_format, annotation)\n\n        if n_frames == 0:\n            continue\n\n        flow_full_path = flow_path\n        if flow_path is not None:\n            flow_full_path = (flow_path / video_name).as_posix()\n\n        try:\n            video_id = video_name.split(\'/\')[1]\n        except IndexError:\n            video_id = video_name\n\n        def add_sample(begin_frame, end_frame, label):\n            sample = {\n                \'video\': video_path.as_posix(),\n                \'flow\': flow_full_path,\n                \'segment\': [begin_frame, end_frame],\n                \'n_frames\': n_frames,\n                \'fps\': fps,\n                \'video_id\': video_id,\n                \'label\': class_to_idx[label]\n            }\n            videos.append(sample)\n\n        video_annotation = annotation[\'annotations\']\n        events_annotation = video_annotation.get(\'events\', None)\n        if events_annotation is not None:\n            for event in events_annotation:\n                begin_time = float(event[\'start\'])\n                end_time = float(event[\'stop\'])\n                label = event[\'event\']\n                assert label in class_to_idx\n                # From time to frame number.\n                timestamps = video_annotation[\'timestamps\']\n                begin_frame, end_frame = np.searchsorted(timestamps, [begin_time, end_time])\n                end_frame -= 1\n                assert begin_frame < end_frame\n                add_sample(begin_frame, end_frame, label)\n        else:\n            begin_frame = 1\n            end_frame = n_frames\n            add_sample(begin_frame, end_frame, annotation[\'annotations\'][\'label\'])\n\n    return videos, idx_to_class\n'"
pytorch_toolkit/action_recognition/action_recognition/dataset.py,0,"b'import copy\nfrom collections import Counter\n\nimport numpy as np\nimport torch\nfrom torch.utils import data\n\nfrom action_recognition.utils import cached\nfrom action_recognition.video_reader import make_video_reader, read_flow\nfrom .annotation import load_json_annotation\n\n\n@cached()\ndef load_annotation(annotation_path, flow_path, root_path, subset, video_format):\n    return load_json_annotation(root_path, annotation_path, subset, flow_path, video_format)\n\n\ndef make_dataset(args, subset, spatial_transform, temporal_transform, target_transform):\n    """"""Constructs VideoDataset instance for specified subset""""""\n    assert subset in [\'training\', \'validation\', \'testing\']\n\n    if subset == \'testing\':\n        num_samples_per_video = args.n_test_clips\n    elif subset == \'validation\':\n        num_samples_per_video = args.n_val_clips\n    else:  # train\n        num_samples_per_video = 1\n\n    if subset == \'testing\':\n        if args.test_subset == \'val\':\n            subset = \'validation\'\n        elif args.test_subset == \'test\':\n            subset = \'testing\'\n\n    return_flow = False\n    return_rgb = True\n    if ""flow"" in args.model:\n        if ""two_stream"" not in args.model:\n            return_rgb = False\n        return_flow = True\n\n    return VideoDataset(\n        args.video_path,\n        args.annotation_path,\n        subset,\n        num_samples_per_video,\n        spatial_transform,\n        temporal_transform,\n        target_transform,\n        sample_duration=(args.sample_duration * args.temporal_stride),\n        flow_path=args.flow_path,\n        return_rgb=return_rgb,\n        return_flow=return_flow,\n        video_format=getattr(args, \'video_format\', None),\n        image_reader=getattr(args, \'image_reader\', ""opencv"")\n    )\n\n\ndef sample_clips(videos, num_samples_per_video, sample_duration):\n    """"""Extracts clips with given length from each video.\n    Args:\n        videos: List of video samples\n        num_samples_per_video: Number of clips sampled for each video. If num_samples_per_video = 1 ,\n            then all frames are returned for each video. If num_samples_per_video <= 0, then all sequential\n            clips are sampled from video. If num_samples_per_video > 1, then clips are sampled uniformly.\n        sample_duration: Number of frames in sampled clips. Actual value may be smaller for short clips.\n\n    Returns: List of clip samples\n\n    """"""\n    videos = sorted(videos, key=lambda v: v[\'video\'].split(\'/\')[-1])\n    clips = []\n    for sample in videos:\n        segment_start, segment_end = sample[\'segment\']\n        n_frames = segment_end - segment_start + 1\n\n        if num_samples_per_video == 1:\n            # use all frames from video\n            sample[\'frame_indices\'] = list(range(segment_start, segment_end + 1))\n            clips.append(sample)\n\n        else:\n\n            num_samples = num_samples_per_video\n            if num_samples_per_video <= 0:\n                # Use all clips of sample_duration or the whole video if it\'s short enough.\n                step = 1\n                num_samples = max(1, n_frames - sample_duration + 1)\n            else:\n                step = max(1, (n_frames - sample_duration) // (num_samples_per_video - 1))\n\n            for clip_start in range(segment_start, segment_start + step * num_samples, step):\n                sampled_clip = copy.deepcopy(sample)\n                clip_end = min(segment_end + 1, clip_start + sample_duration)\n                sampled_clip[\'frame_indices\'] = list(range(clip_start, clip_end))\n                if sampled_clip[\'frame_indices\']:\n                    clips.append(sampled_clip)\n\n    return clips\n\n\nclass VideoDataset(data.Dataset):\n    """"""Generic video dataset.\n\n    Args:\n        video_path (Path): Directory with video files. Will be used by annotation_loader to resolve real paths.\n        annotation_path (Path): Path to annotation file.\n        subset (str): Which subset of dataset to use (validation/training/testing)\n        n_samples_for_each_video (int): How many clips should be sampled from every video per epoch.\n        spatial_transform (callable): A function/transform that takes in a clip (list of frames) and returns\n            transformed version\n        temporal_transform (callable): A function/transform that takes a list of clip frames and returns transformed\n            version\n        target_transform (callable): A function/transform that takes in the annotation object and returns transformed\n            version\n        sample_duration (int): Number of frames in sampled clips\n        flow_path (Path): Path to a optical flow directory\n        return_rgb (bool): Whether RGB frame should be returned\n        return_flow (bool): Whether Optical flow should be returned\n        video_reader (callable): Callable that takes in a path to video and transformed frame indices and returns\n            list of frames. If None, then object will be created according to the video_format.\n        video_format (str): Type of video_loader to be instantiated. If ""video"", then created video_loader will\n            attempt to read frames from .mp4 file. If ""frames"", then it will try to read from directory with images\n        image_reader (str): Backend for reading image files (pil, opencv, accimage)\n    """"""\n\n    def __init__(\n            self,\n            video_path,\n            annotation_path,\n            subset,\n            n_samples_for_each_video=1,\n            spatial_transform=None,\n            temporal_transform=None,\n            target_transform=None,\n            sample_duration=16,\n            flow_path=None,\n            return_rgb=True,\n            return_flow=False,\n            video_reader=None,\n            video_format=\'frames\',\n            image_reader=\'opencv\'\n    ):\n        if not video_reader:\n            self.video_loader = make_video_reader(video_format, image_reader)\n        else:\n            self.video_loader = video_reader\n\n        self.data, self.class_names = load_annotation(annotation_path, flow_path, video_path, subset, video_format)\n\n        if not self.data:\n            raise ValueError(""No videos found in {!s} directory. Please check correctness of provided paths""\n                             .format(video_path))\n\n        self.data = sample_clips(self.data, n_samples_for_each_video, sample_duration)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n        self.return_rgb = return_rgb\n        self.return_flow = return_flow\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            clips (dict): Dictionary where keys are input modalities and values are corresponding tensors\n            targets (dict): Dictionary with annotation data (label, video_id, etc)\n        """"""\n        clip_index = index // len(self.spatial_transform)\n        spatial_transform_index = index % len(self.spatial_transform)\n\n        self.spatial_transform[spatial_transform_index].randomize_parameters()\n\n        frame_indices = self.data[clip_index][\'frame_indices\']\n        if self.temporal_transform is not None:\n            frame_indices = self.temporal_transform(frame_indices)\n\n        clips = {\n            **self._read_rgb(clip_index, frame_indices, spatial_transform_index),\n            **self._read_flow(clip_index, frame_indices, spatial_transform_index)\n        }\n\n        target = self.data[clip_index]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return clips, target\n\n    def _read_rgb(self, clip_index, frames, spatial_transform_index):\n        if not self.return_rgb:\n            return {}\n\n        video_path = self.data[clip_index][\'video\']\n        clip = self.video_loader(str(video_path), frames)\n\n        clip = [self.spatial_transform[spatial_transform_index](frame) for frame in clip]\n\n        return {\'rgb_clip\': torch.stack(clip, 0)}\n\n    def _read_flow(self, clip_index, frames, spatial_transform_index):\n        if not self.return_flow:\n            return {}\n\n        flow_path = self.data[clip_index][\'flow\']\n        clip = read_flow(str(flow_path), frames)\n\n        clip = [self.spatial_transform[spatial_transform_index](frame) for frame in clip]\n\n        clip = torch.stack(clip, 0)\n        N, _, H, W = clip.shape\n        clip = clip.view((N // 2, 2, H, W))\n        return {\'flow_clip\': clip}\n\n    def __len__(self):\n        return len(self.data) * len(self.spatial_transform)\n\n    def get_sample_weights(self, class_weights):\n        """"""Transforms per-class sampling probability to per-clip sampling probability.\n        Used with torch.utils.data.WeightedRandomSampler in order to balance classes""""""\n        if class_weights is not None:\n            class_weights = np.asarray(class_weights)\n            class_weights /= np.sum(class_weights)\n        else:\n            num_labels = len(self.class_names)\n            sample_count = Counter(data_elem[\'label\'] for data_elem in self.data)\n            class_weights = [(1 / sample_count[label]) / num_labels for label in range(num_labels)]\n\n        return [class_weights[data_elem[\'label\']] for data_elem in self.data]\n'"
pytorch_toolkit/action_recognition/action_recognition/logging.py,0,"b'import csv\nimport re\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\n\nfrom .utils import AverageMeter\n\n\nclass Handler:\n    """"""Observes events from TrainingLogger and dispatches them to a specific destination.""""""\n\n    def write(self, value):\n        """"""Write value to handler.\n\n        Args:\n            value (Value): currently logged value.\n        """"""\n        pass\n\n    def start(self, scope, step, total):\n        """"""Indicate that scope is started""""""\n        pass\n\n    def end(self, scope):\n        """"""Indicate that scope is finished""""""\n        pass\n\n\nclass PeriodicHandler(Handler):\n    """"""Handler that flushes written values when scope ends (e.g. epoch ends).""""""\n\n    def __init__(self, scope):\n        super().__init__()\n        self.values = OrderedDict()\n        self.total = 0\n        self.step = 0\n        self.epoch = 0\n        self.scope = scope\n\n    def write(self, value):\n        self.values[value.tag] = value\n\n        if self.scope == \'global\':\n            self.end(self.scope)\n            self.start(self.scope, self.step + 1, None)\n\n    def end(self, scope):\n        if scope == self.scope:\n            self.flush()\n\n    def start(self, scope, step, total, epoch=0):\n        if scope != self.scope:\n            return\n        if step is None:\n            step = self.step + 1\n        self.step = step\n        self.epoch = epoch\n        self.total = total\n        self.values.clear()\n\n    def flush(self):\n        """"""Writes saved values""""""\n        raise NotImplementedError\n\n\nclass StreamHandler(PeriodicHandler):\n    """"""Writes logged values either to stream or python logger. """"""\n\n    def __init__(self, scope, prefix=\'\', fmt=""{prefix}[{epoch}][{step}/{total}]\\t{values}"", display_instant=True,\n                 display_frequency=1, stream=None, logger=None):\n        super().__init__(scope)\n        if stream is None:\n            stream = sys.stdout\n        self.logger = logger\n        self.stream = stream\n        self.display_frequency = display_frequency\n        self.display_instant = display_instant\n        self.fmt = fmt\n        self.prefix = prefix\n\n        self._display_count = 0\n\n    def end(self, scope):\n        if scope == self.scope:\n            self._display_count += 1\n            if self._display_count == self.display_frequency:\n                if self.values:\n                    self.flush()\n                self._display_count = 0\n\n    def flush(self):\n        msg = self.format_msg()\n        if self.logger:\n            self.logger.info(msg)\n        else:\n            self.stream.write(msg + \'\\n\')\n            self.stream.flush()\n\n    def format_value(self, value):\n        if value.instant_value is not None and self.display_instant:\n            return ""{name} {ival:.4f} ({val:.4f})"".format(name=value.display_name, val=value.value,\n                                                          ival=value.instant_value)\n        else:\n            return ""{name} {val:.4f}"".format(name=value.display_name, val=value.value)\n\n    def format_msg(self):\n        values_fmt = [self.format_value(v) for v in self.values.values()]\n        values_s = ""\\t"".join(values_fmt)\n        msg = self.fmt.format(prefix=self.prefix, epoch=self.epoch, step=self.step, total=self.total, values=values_s)\n        return msg\n\n\nclass TensorboardHandler(PeriodicHandler):\n    """"""Writes logged values to tensorboard.""""""\n\n    def __init__(self, scope, summary_writer):\n        super().__init__(scope)\n        self.summary_writer = summary_writer\n\n    def flush(self):\n        for tag, value in self.values.items():\n            step = self.step if self.scope != \'global\' else value.step\n            self.summary_writer.add_scalar(tag, value.value, step)\n\n\nclass CSVHandler(PeriodicHandler):\n    """"""Writes logged values to CSV file""""""\n\n    def __init__(self, scope, csv_path, index_col=\'step\'):\n        super().__init__(scope)\n        self.index_col = index_col\n        self.csv_path = str(csv_path)\n        self._csv_file = open(self.csv_path, \'w\')\n        self.csv_writer = csv.writer(self._csv_file, delimiter=\'\\t\')\n        self._header = None\n\n    def flush(self):\n        if not self.values:\n            return\n        if self._header is None:\n            self._header = [self.index_col] + [value.display_name for value in self.values.values()]\n            self.csv_writer.writerow(self._header)\n\n        new_row = [self.step if self.scope != \'global\' else next(iter(self.values.values())).step]\n\n        for value_obj in self.values.values():\n            new_row.append(value_obj.value)\n\n        self.csv_writer.writerow(new_row)\n        self._csv_file.flush()\n\n\nclass Value:\n    """"""Wraps logged value and maintain accumulated version.""""""\n\n    def __init__(self, tag, handlers, aggregator=None, save_instant=True, display_name=None):\n        if display_name is None:\n            display_name = tag\n\n        self.tag = tag\n        self.value = None\n        self.instant_value = None\n        self.step = 0\n        self.save_instant = save_instant\n        self.handlers = handlers\n        self.aggregator = aggregator\n        self.display_name = display_name\n\n    def reset(self):\n        """"""Reset aggregation and step counter""""""\n        if self.aggregator:\n            self.aggregator.reset()\n        self.step = 0\n\n    def update(self, value, num_averaged=1, step=None):\n        if step is None:\n            step = self.step + 1\n        self.step = step\n        if self.save_instant:\n            self.instant_value = value\n        self.value = value\n        if self.aggregator:\n            self.aggregator.update(value, num_averaged)\n            self.value = self.aggregator.avg\n\n\nclass TrainingLogger:\n    """"""Logging utility that accumulates all logged values within some scope (e.g. batch / epoch / etc),\n    and dispatches them to possibly multiple handlers.\n\n\n    Logged values should be registered with register_value or register_value_group method. Values should be attached\n    to handlers that correspond to a specific output stream (e.g. console, csv, TensorBoard, etc), which should be\n    registered in advance. After handlers and values are registered, logging can be accomplished by calling log_value\n    method. In order to indicate when handler flush logged values (e.g. to format all values, logged during epoch in\n    one line), all log_value calls should occur between start scope and end scope.\n    """"""\n\n    def __init__(self):\n        self._handlers = OrderedDict()\n        self._values = OrderedDict()\n        self._value_groups = OrderedDict()\n\n    def log_value(self, tag, value, num_averaged=1, step=None):\n        """"""Dispatch value to its handlers\n        Args:\n            tag (str): Name of the logged value. Value should be registered with the same name before logging.\n            value (float): The actual value (e.g. current batch loss)\n            num_averaged: If the value is registered with average=True and num_averaged > 1, then accumulated value\n            will be adjusted correctly. Use it in case if the actual value is the average of multiple value.\n            step (int): Order of the value (e.g. number of batch)\n        """"""\n        value_obj = self._resolve_tag(tag)\n        value_obj.update(value, num_averaged, step)\n        for handler in value_obj.handlers:\n            handler.write(value_obj)\n\n    def register_value(self, tag, handlers, average=False, display_instant=None, display_name=None):\n        """"""Register value and attach it to handlers.\n\n        Args:\n            tag (str): Name of the value.\n            handlers (List[str]): List of handler names that this value will be attached to.\n            average (bool): Whether value should be averaged, i.e. cumulative value will be passed to the handler.\n            display_instant (bool): Whether instant (non-cumulative) must be flushed by the handler as well (not every\n            handler will interpret this option)\n            display_name (str): Readable name of the value\n        """"""\n        if display_instant is None:\n            display_instant = average\n        handlers = [self._handlers[handler] for handler in handlers]\n        if average:\n            aggregator = AverageMeter()\n        else:\n            aggregator = None\n        value = Value(tag, handlers, aggregator=aggregator, save_instant=display_instant, display_name=display_name)\n        self._values[tag] = value\n\n    def register_value_group(self, group_pattern, handlers, average=False, display_instant=None):\n        """"""Register group of values and attach it to the same handler. New values will be registered on the fly.\n        Args:\n            group_pattern (str): Regular expression of the group name.\n            handlers (List[str]): List of handler names that this values will be attached to.\n            average (bool): Whether value should be averaged, i.e. cumulative value will be passed to handler.\n            display_instant (bool): Whether instant (non cumulative) must be flushed by the handler as well (not every\n            handler will interpret this option)\n        """"""\n        self._value_groups[group_pattern] = (re.compile(group_pattern), handlers, average, display_instant)\n\n    def register_handler(self, name, handler):\n        """"""Register value handler with a given name""""""\n        self._handlers[name] = handler\n\n    def start_scope(self, scope, step=None, total=None, epoch=0):\n        """"""Indicate start of the scope to registered handlers\n\n        Args:\n            scope (str): Started scope number\n            step (int): Started scope number\n            total (int): How many scopes with given name there will be in a row. Used by certain handlers to display\n            current progress\n            epoch (int): Current epoch number\n\n        """"""\n        for handler in self._handlers.values():\n            handler.start(scope, step, total, epoch=epoch)\n\n    def end_scope(self, scope):\n        """"""Indicate end of the scope to registered handlers""""""\n        for handler in self._handlers.values():\n            handler.end(scope)\n\n    def start_epoch(self, epoch, total=None):\n        """"""Start ""epoch"" scope""""""\n        self.start_scope(\'epoch\', step=epoch, total=total, epoch=epoch)\n\n    def start_batch(self, step=None, total=None, epoch=0):\n        """"""Start ""batch"" scope""""""\n        self.start_scope(\'batch\', step=step, total=total, epoch=epoch)\n\n    def end_batch(self):\n        """"""End ""batch"" scope""""""\n        self.end_scope(scope=\'batch\')\n\n    def end_epoch(self):\n        """"""End ""epoch"" scope""""""\n        self.end_scope(scope=\'epoch\')\n\n    def get_value(self, tag):\n        """"""Returns current cumulative value""""""\n        return self._values[tag].value\n\n    @contextmanager\n    def scope(self, epoch=0, scope=\'epoch\', total=None):\n        """"""Context manager that begins and ends scope with name ""scope"" """"""\n        self.start_scope(scope, epoch, total, epoch)\n        yield\n        self.end_scope(scope)\n\n    def reset_values(self, values_pattern):\n        """"""Resets accumulated value of values with given pattern""""""\n        rx = re.compile(values_pattern)\n        for value_name, value in self._values.items():\n            if rx.search(value_name):\n                value.reset()\n\n    def scope_enumerate(self, iterable, epoch=0, scope=\'batch\', total_time=None, fetch_time=None, body_time=None):\n        """"""The same as enumerate() but wraps every iteration with begin and end scope. Optionally can log cpu time\n        for certain code sections\n\n        Args:\n            iterable: Object that will be used for iteration\n            epoch (int): Current epoch number\n            scope (str): Name of the scope that will wrap every iteration.\n            total_time (str): Name of the value that will be used for logging total iteration time (if set to any value)\n            fetch_time (str): Name of the value that will be used for logging time that iterable takes to produce new\n                              item (if set to any value).\n            body_time (str): Name of the value that will be used for logging time of executing loop body (if set to\n            any value).\n\n        """"""\n        start_time = time.time()\n\n        t0 = time.time()\n        total = len(iterable)\n        for i, data in enumerate(iterable):\n            t1 = time.time()\n            self.start_scope(scope, i + 1, total=total, epoch=epoch)\n            yield i, data\n            t2 = time.time()\n\n            if fetch_time:\n                self.log_value(fetch_time, t1 - t0)\n            if body_time:\n                self.log_value(body_time, t2 - t1)\n            self.end_scope(scope)\n            t0 = t2\n        if total_time:\n            self.log_value(total_time, time.time() - start_time)\n\n    def _resolve_tag(self, tag):\n        if tag in self._values:\n            return self._values[tag]\n        for p, (rx, handlers, average, display_instant) in self._value_groups.items():\n            if rx.match(tag):\n                self.register_value(tag, handlers, average, display_instant, tag)\n        return self._values[tag]\n'"
pytorch_toolkit/action_recognition/action_recognition/loss.py,0,"b'from torch.nn import functional as F\nfrom torch import nn\n\nimport torch\n\nfrom .model import create_model\nfrom .utils import load_state\n\n\nclass LogitKLDivLoss(nn.Module):\n    """"""Kullback\xe2\x80\x93Leibler divergence loss. Inputs predicted and ground truth logits.\n\n    Args:\n        T (float): Softmax temperature.\n    """"""\n\n    def __init__(self, T=1):\n        super().__init__()\n        self.T = T\n\n    def forward(self, p_logits, q_logits, **kwargs):\n        log_p = F.log_softmax(p_logits / self.T, dim=1)\n        q = F.softmax(q_logits / self.T, dim=1)\n        return F.kl_div(log_p, q, reduction=\'batchmean\') * self.T ** 2\n\n\nclass DistillationLoss(nn.Module):\n    """"""Knowledge distillation loss.\n\n    Args:\n        teacher_model (torch.nn.Module): Model that will be used for supervision.\n        T (float): Softmax temperature.\n    """"""\n\n    def __init__(self, teacher_model, T=1):\n        super().__init__()\n        self.teacher_model = teacher_model\n        self.kl_div = LogitKLDivLoss(T)\n\n    def forward(self, outputs, inputs, **kwargs):\n        """"""\n        Args:\n            outputs: Predicted student model logits\n            inputs: Inputs that have been used to produce outputs.\n        """"""\n        with torch.no_grad():\n            teacher_logits = self.teacher_model(*inputs)\n        return self.kl_div(outputs, teacher_logits)\n\n\nclass SoftmaxLoss(nn.Module):\n    """"""Classification loss""""""\n\n    def forward(self, outputs, targets, **kwargs):\n        return F.cross_entropy(outputs, targets)\n\n\nclass WeightedSumLoss(nn.Module):\n    """"""Aggregate multiple loss functions in one weighted sum.""""""\n\n    def __init__(self, normalize=False):\n        super().__init__()\n        self.normalize = normalize\n        self.losses = nn.ModuleDict()\n        self.weights = {}\n        self.values = {}\n\n    def forward(self, outputs, **kwargs):\n        total_loss = outputs.new(1).zero_()\n        for loss in self.losses:\n            loss_val = self.losses[loss](outputs=outputs, **kwargs)\n            total_loss += self.weights[loss] * loss_val\n            self.values[loss] = loss_val\n\n        if self.normalize:\n            total_loss /= sum(self.weights.values())\n\n        return total_loss\n\n    def add_loss(self, name, loss, weight=1.0):\n        self.weights[name] = weight\n        self.losses.add_module(name, loss)\n\n\ndef create_criterion(args):\n    criterion = WeightedSumLoss()\n    softmax = SoftmaxLoss()\n    criterion.add_loss(\'softmax\', softmax)\n\n    if args.teacher_model:\n        teacher_model, _ = create_model(args, args.teacher_model)\n\n        checkpoint = torch.load(str(args.teacher_checkpoint))\n        load_state(teacher_model, checkpoint[\'state_dict\'])\n        teacher_model.eval()\n\n        distillation_loss = DistillationLoss(teacher_model, T=8)\n        criterion.add_loss(distillation_loss, 0.4)\n\n    return criterion\n'"
pytorch_toolkit/action_recognition/action_recognition/model.py,0,"b'import torch\nfrom torch import nn\n\nfrom .models import (densenet_3d, inception_i3d, lstm_attention,\n                     multi_frame_baseline, video_transformer, vtn_motion,\n                     vtn_two_stream)\nfrom .models.modules.sync_batchnorm import (DataParallelWithCallback,\n                                            SynchronizedBatchNorm2d)\nfrom .models.r3d import R3D_MODELS\nfrom .utils import load_state\n\nMODEL_REGISTRY = {\n    \'vtn\': lambda args, encoder: video_transformer.VideoTransformer(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        layer_norm=args.layer_norm,\n    ),\n    \'lstm\': lambda args, encoder: lstm_attention.VisualAttentionLSTM(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        use_attention=False,\n        bidirectional=args.bidirectional_lstm\n    ),\n    \'attn_lstm\': lambda args, encoder: lstm_attention.VisualAttentionLSTM(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        use_attention=True,\n    ),\n\n    \'vtn_rgbdiff\': lambda args, encoder: vtn_motion.VideoTransformerMotion(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        mode=\'rgbdiff\',\n        layer_norm=args.layer_norm,\n    ),\n\n    \'vtn_flow\': lambda args, encoder: vtn_motion.VideoTransformerMotion(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        mode=\'flow\',\n        layer_norm=args.layer_norm,\n    ),\n    \'vtn_encoder\': lambda args, encoder: video_transformer.VideoTransformerEncoder(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        layer_norm=args.layer_norm,\n    ),\n    \'vtn_decoder\': lambda args, encoder: video_transformer.VideoTransformerDecoder(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        layer_norm=args.layer_norm,\n    ),\n    \'vtn_two_stream\': lambda args, encoder: vtn_two_stream.VideoTransformerTwoStream(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        motion_path=args.motion_path,\n        rgb_path=args.rgb_path,\n        mode=\'rgbdiff\',\n        layer_norm=args.layer_norm,\n    ),\n\n    \'vtn_two_stream_flow\': lambda args, encoder: vtn_two_stream.VideoTransformerTwoStream(\n        args.hidden_size,\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        motion_path=args.motion_path,\n        rgb_path=args.rgb_path,\n        mode=\'flow\'\n    ),\n\n    \'baseline\': lambda args, encoder: multi_frame_baseline.MultiFrameBaseline(\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True\n    ),\n    \'baseline_encoder\': lambda args, encoder: multi_frame_baseline.MultiFrameBaselineEncoder(\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True\n    ),\n    \'baseline_decoder\': lambda args, encoder: multi_frame_baseline.MultiFrameBaselineDecoder(\n        args.sample_duration,\n        encoder,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True\n    ),\n\n    \'resnet34_attn_single\': lambda args, encoder: lstm_attention.ResnetAttSingleInput(\n        args.hidden_size,\n        args.sample_duration,\n        args.n_classes,\n        args.sample_size,\n        False if args.pretrain_path or args.resume_path else True,\n        resnet_size=34\n    ),\n\n    \'inception_i3d\': lambda args, encoder: inception_i3d.InceptionI3D(\n        num_classes=args.n_classes\n    ),\n    \'densenet201\': lambda args, encoder: densenet_3d.densenet201(\n        sample_size=args.sample_size,\n        sample_duration=args.sample_duration,\n        num_classes=400\n    )\n    ,\n    **R3D_MODELS,\n}\n\n\ndef make_bn_synchronized(bn_module):\n    """"""Convert all BatchNorm modules to SynchronizedBatchNorm""""""\n    new_module = SynchronizedBatchNorm2d(bn_module.num_features, eps=bn_module.eps, momentum=bn_module.momentum,\n                                         affine=bn_module.affine)\n\n    if new_module.track_running_stats:\n        new_module.running_mean = bn_module.running_mean\n        new_module.running_var = bn_module.running_var\n        new_module.num_batches_tracked = bn_module.num_batches_tracked\n    new_module.weight = bn_module.weight\n    new_module.bias = bn_module.bias\n    return new_module\n\n\ndef _replace_bns(model: nn.Module, memo=None):\n    if memo is None:\n        memo = set()\n    if model not in memo:\n        memo.add(model)\n        for name, module in model._modules.items():\n            if module is None:\n                continue\n            if isinstance(module, nn.BatchNorm2d):\n                if isinstance(model, nn.Sequential):\n                    model._modules[name] = make_bn_synchronized(module)\n                else:\n                    setattr(model, name, make_bn_synchronized(module))\n            _replace_bns(module, memo)\n    return model\n\n\ndef create_model(args, model, pretrain_path=None):\n    """"""Construct model with a given name and args.\n\n    Args:\n        args (Namespace): Options for model construction\n        model (str): Name of the model in ENCODER_DECODER or DECODER format.\n        pretrain_path (Path): Path to a checkpoint with the pretrained model.\n    """"""\n    model = model.replace(""self_attn"", ""vtn"")\n    if len(model.split(\'_\')) > 1:\n        encoder_name, model_type = model.split(\'_\', 1)\n    else:\n        encoder_name = args.encoder\n        model_type = model\n    encoder_name = encoder_name.replace(\'-\', \'_\')\n\n    if model in MODEL_REGISTRY:\n        # if model with exactly same name is known\n        model = MODEL_REGISTRY[model](args, encoder_name)\n    else:\n        model = MODEL_REGISTRY[model_type](args, encoder_name)\n\n    # load pretrained model\n    if pretrain_path:\n        print(\'loading pretrained model {}\'.format(args.pretrain_path))\n        pretrain = torch.load(str(args.pretrain_path))\n\n        if hasattr(model, \'load_checkpoint\'):\n            model.load_checkpoint(pretrain[\'state_dict\'])\n        else:\n            load_state(model, pretrain[\'state_dict\'])\n\n    if args.cuda:\n        model = model.cuda()\n        if args.sync_bn:\n            model = _replace_bns(model)\n            wrapped_model = DataParallelWithCallback(model)\n        else:\n            wrapped_model = nn.DataParallel(model)\n    else:\n        wrapped_model = model\n\n    if args.fp16:\n        model = model.half()\n\n        # do not train batchnorms in FP16 precision\n        def _float_bns(layer):\n            if isinstance(layer, (nn.BatchNorm2d,)):\n                layer.float()\n\n        model.apply(_float_bns)\n\n    parameters = model.trainable_parameters()\n    return wrapped_model, parameters\n'"
pytorch_toolkit/action_recognition/action_recognition/options.py,0,"b'import argparse\nfrom pathlib import Path\n\nfrom . import spatial_transforms\n\n\nclass BoolFlagAction(argparse.Action):\n    """"""Action that stores bool flag depending on whether --option or --no-option is passed""""""\n\n    def __init__(self,\n                 option_strings,\n                 dest,\n                 default=False,\n                 required=False,\n                 help=None):\n        option_strings = option_strings + [s.replace(\'--\', \'--no-\') for s in option_strings]\n        super().__init__(\n            option_strings=option_strings,\n            dest=dest,\n            nargs=0,\n            default=default,\n            required=required,\n            help=help\n        )\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        val = not option_string.startswith(\'--no\')\n        setattr(namespace, self.dest, val)\n\n\ndef get_argument_parser():\n    parser = argparse.ArgumentParser(""Action recognition"")\n\n    add_common_args(parser)\n    add_path_args(parser)\n    add_model_args(parser)\n    add_dataset_args(parser)\n    add_input_args(parser)\n    add_training_args(parser)\n\n    return parser\n\n\ndef add_common_args(parser):\n    parser.add_argument(\n        \'--train\',\n        action=BoolFlagAction,\n        default=True,\n        help=\'Whether training should be performed\'\n    )\n    parser.add_argument(\n        \'--val\',\n        action=BoolFlagAction,\n        default=True,\n        help=\'Whether validation should be performed\'\n    )\n    parser.add_argument(\n        \'--validate\',\n        default=5,\n        type=int,\n        help=\'Validation is run every `validate` epochs\'\n    )\n    parser.add_argument(\n        \'--test\',\n        action=BoolFlagAction,\n        default=True,\n        help=\'Whether testing should be performed\'\n    )\n    parser.add_argument(\n        \'--onnx\', type=str, metavar=\'OUTPUT_ONNX_FILE\',\n        help=\'Export to ONNX model with a given name and exit\'\n    )\n    parser.add_argument(\n        \'--sync-bn\',\n        action=BoolFlagAction,\n        help=\'Replace all batchnorm operations with synchronized batchnorm\'\n    )\n    parser.add_argument(\n        \'--fp16\',\n        action=BoolFlagAction,\n        help=\'Enable training and validation in half precision\'\n    )\n    parser.add_argument(\n        \'--tta\',\n        action=BoolFlagAction,\n        help=\'Enable test time augmentations. Testing may take longer\'\n    )\n    parser.add_argument(\n        \'--manual-seed\',\n        default=1,\n        type=int,\n        help=\'Manually set random seed\'\n    )\n    parser.add_argument(\n        \'--cuda\',\n        action=BoolFlagAction,\n        default=True,\n        help=\'Whether cuda should be used\'\n    )\n    parser.add_argument(\n        \'-j\', \'--n-threads\',\n        default=4,\n        type=int,\n        help=\'Number of threads for multi-thread loading\'\n    )\n    parser.add_argument(\n        \'--softmax-in-test\',\n        action=BoolFlagAction,\n        help=\'Normalize outputs in testing before argmax\'\n    )\n\n    parser.add_argument(\n        \'--checkpoint\',\n        default=5,\n        type=int,\n        help=\'Number of epochs to train before new checkpoint is saved\'\n    )\n    parser.add_argument(\n        \'--resume-train\',\n        action=BoolFlagAction,\n        default=True,\n        help=\'Restore optimizer state and start epoch when loading checkpoint\'\n    )\n\n    parser.add_argument(\n        \'--try-resume\',\n        action=BoolFlagAction,\n        default=True,\n        help=\'Attempts to find latest checkpoint in RESULT_PATH\'\n    )\n\n\ndef add_model_args(parser):\n    group = parser.add_argument_group(""Model"")\n    group.add_argument(\n        \'--model\',\n        default=\'resnet34_vtn\',\n        type=str,\n        help=\'Either full model name in form <encoder>_<type> or model type (in this case encoder option should be \'\n             \'provided). For example: resnet34_vtn, mobilenetv2_vtn, resnet34_vtn_two_stream\'\n    )\n    group.add_argument(\n        \'--encoder\',\n        default=\'resnet34\',\n        type=str,\n        help=\'Encoder used in model\'\n    )\n    group.add_argument(\n        \'--model-depth\',\n        default=18,\n        type=int,\n        choices=[10, 18, 34, 50, 101],\n        help=\'Depth of 3D-ResNet model\'\n    )\n    group.add_argument(\n        \'--resnet-shortcut\',\n        default=\'B\',\n        choices=[\'A\', \'B\'],\n        type=str,\n        help=\'Shortcut type of resnet\'\n    )\n    group.add_argument(\n        \'--wide-resnet-k\',\n        default=2,\n        type=int,\n        help=\'Wide resnet k\'\n    )\n    group.add_argument(\n        \'--resnext-cardinality\',\n        default=32,\n        type=int,\n        help=\'ResNeXt cardinality\'\n    )\n    group.add_argument(\n        \'--hidden-size\',\n        default=512,\n        type=int,\n        help=\'Size of hidden state and cell state of lstm\'\n    )\n    group.add_argument(\n        \'--bidirectional-lstm\',\n        action=BoolFlagAction,\n        help=\'Make LSTM layers bidirectional\'\n    )\n    group.add_argument(\n        \'--layer-norm\',\n        action=BoolFlagAction,\n        help=\'Use LayerNormalization in VTN\',\n        default=True\n    )\n\n\ndef add_input_args(parser):\n    group = parser.add_argument_group(""Input"")\n    group.add_argument(\n        \'--sample-size\',\n        default=224,\n        type=int,\n        help=\'Height and width of inputs\'\n    )\n    group.add_argument(\n        \'--sample-duration\', \'--seq\', \'--clip-size\',\n        default=16,\n        type=int,\n        help=\'Temporal duration of input clips\'\n    )\n    group.add_argument(\n        \'--temporal-stride\', \'--st\',\n        default=1, type=int,\n        help=\'Frame skip rate of sampled clips\'\n    )\n    group.add_argument(\n        \'--initial-scale\',\n        default=1.0,\n        type=float,\n        help=\'Initial scale for multiscale cropping\'\n    )\n    group.add_argument(\n        \'--n-scales\',\n        default=4,\n        type=int,\n        help=\'Number of scales for multiscale cropping\'\n    )\n    group.add_argument(\n        \'--scale-step\',\n        default=2 ** (-1 / 4),\n        type=float,\n        help=\'Scale step for multiscale cropping\'\n    )\n    group.add_argument(\n        \'--mean-dataset\',\n        default=\'imagenet\',\n        choices=list(spatial_transforms.MEAN_STATISTICS.keys()),\n        type=str,\n        help=\'Which dataset to use for mean subtraction\'\n    )\n    group.add_argument(\n        \'--mean-norm\',\n        default=True,\n        action=BoolFlagAction,\n        help=\'Should inputs be normalizaed by mean\'\n    )\n    group.add_argument(\n        \'--std-norm\',\n        default=True,\n        action=BoolFlagAction,\n        help=\'Should inputs be normalized by std\'\n    )\n    group.add_argument(\n        \'--hflip\',\n        default=True,\n        action=BoolFlagAction,\n        help=\'Should horizontal flipping be performed for augmentation\'\n    )\n    group.add_argument(\n        \'--drop-last\',\n        default=True,\n        action=BoolFlagAction,\n        help=\'Drop the last batch if it is incomplete.\'\n    )\n    group.add_argument(\n        \'--norm-value\',\n        default=255,\n        type=int,\n        help=\'If 1, range of inputs is [0-255]. If 255, range of inputs is [0-1].\'\n    )\n    group.add_argument(\n        \'--scale-in-test\',\n        default=1.0,\n        type=float,\n        help=\'Spatial scale in test\'\n    )\n    group.add_argument(\n        \'--crop-position-in-test\',\n        default=\'c\',\n        choices=[\'c\', \'tl\', \'tr\', \'bl\', \'br\'],\n        type=str,\n        help=\'Cropping position in test\'\n    )\n\n\ndef add_path_args(parser):\n    group = parser.add_argument_group(""Path"")\n    group.add_argument(\n        \'--root-path\',\n        type=Path,\n        help=\'Root directory path of data. Will be used as base directory for video path, annotation path, etc.\'\n    )\n    group.add_argument(\n        \'--video-path\',\n        type=Path,\n        help=\'Path to directory with video files\'\n    )\n    group.add_argument(\n        \'--annotation-path\',\n        type=Path,\n        help=\'Annotation file path\'\n    )\n    group.add_argument(\n        \'--result-path\',\n        metavar=\'RESULT_PATH\',\n        default=\'results\',\n        type=Path,\n        help=\'Where to store training results (logs, checkpoints, etc.)\'\n    )\n    group.add_argument(\n        \'--rgb-path\',\n        type=Path,\n        help=\'Path to RGB model weights (fo two stream models)\'\n    )\n    group.add_argument(\n        \'--motion-path\',\n        type=Path,\n        help=\'Path to motion model weights (for two stream models)\'\n    )\n    group.add_argument(\n        \'--resume-path\',\n        type=Path,\n        help=\'Path to checkpoint to resume training\'\n    )\n    group.add_argument(\n        \'--pretrain-path\',\n        type=Path,\n        help=\'Path to pretrained model (.pth)\'\n    )\n\n\ndef add_dataset_args(parser):\n    group = parser.add_argument_group(""Data"")\n    group.add_argument(\n        \'--dataset\',\n        default=\'kinetics\',\n        type=str,\n        help=\'Dataset name, possibly with split number (e.g. kinetics, ucf101_1)\'\n    )\n    group.add_argument(\n        \'--dataset-config\',\n        type=str,\n        help=\'Path to dataset configuration file\'\n    )\n    group.add_argument(\n        \'--split\',\n        default=1,\n        type=int,\n        help=\'Dataset split number\'\n    )\n    group.add_argument(\n        \'--n-classes\',\n        type=int,\n        help=\'Number of classes (for example: kinetics: 400, ucf101: 101, hmdb51: 51)\'\n    )\n    group.add_argument(\n        \'--n-finetune-classes\',\n        type=int,\n        help=\'Number of classes for fine-tuning. n_classes is set to the number when pretraining.\'\n    )\n    group.add_argument(\n        \'--test-subset\',\n        default=\'val\',\n        choices=(\'val\', \'test\'),\n        help=\'Used subset in test (val | test)\'\n    )\n    group.add_argument(\n        \'--n-val-clips\',\n        default=3,\n        type=int,\n        help=\'Number of clips to be sampled from video in validation\'\n    )\n    group.add_argument(\n        \'--n-test-clips\',\n        default=10,\n        type=int,\n        help=\'Number of clips to be sampled from video in testing\'\n    )\n    group.add_argument(\n        \'--video-format\',\n        default=\'frames\',\n        choices=[\'video\', \'frames\'],\n        help=\'In what format dataset is stored\'\n    )\n    group.add_argument(\n        \'--weighted-sampling\',\n        action=BoolFlagAction,\n        help=\'when this option is set, clips will be sampled with probability proportional to its label \'\n             \'""class_weights"" from dataset config if it is provided or with class balanced probability otherwise\'\n    )\n\n\ndef add_training_args(parser):\n    group = parser.add_argument_group(""Training"")\n    group.add_argument(\n        \'-b\', \'--batch-size\', \'--batch\',\n        default=128,\n        type=int,\n        metavar=\'BATCH_SIZE\',\n        help=\'Batch Size\'\n    )\n    group.add_argument(\n        \'--iter-size\',\n        default=1,\n        type=int,\n        metavar=\'ITER_SIZE\',\n        help=\'How many batches will be forwarded before parameter update (i.e. effective batch size will be \'\n             \'BATCH_SIZE * ITER_SIZE\'\n    )\n    group.add_argument(\n        \'--optimizer\', \'--solver\',\n        default=\'adam\',\n        type=str.lower,\n        choices=[\'sgd\', \'adam\'],\n        help=\'Which optimizer to use.\'\n    )\n    group.add_argument(\n        \'--learning-rate\', \'--lr\',\n        default=0.1,\n        type=float,\n        help=\'Initial learning rate\'\n    )\n    group.add_argument(\n        \'--momentum\',\n        default=0.9,\n        type=float,\n        help=\'Momentum\'\n    )\n    group.add_argument(\n        \'--dampening\',\n        default=0.9,\n        type=float,\n        help=\'dampening of SGD\'\n    )\n    group.add_argument(\n        \'--weight-decay\',\n        default=1e-4,\n        type=float,\n        help=\'Weight Decay\'\n    )\n    group.add_argument(\n        \'--nesterov\',\n        default=True,\n        action=BoolFlagAction,\n        help=\'Use Nesterov momentum\'\n    )\n    group.add_argument(\n        \'--scheduler\', \'--lr-policy\',\n        default=\'plateau\',\n        choices=[\'step\', \'plateau\'],\n        help=\'Learning rate decay policy.\'\n    )\n    group.add_argument(\n        \'--gamma\',\n        default=0.1,\n        type=float,\n        help=\'Factor by which learning rate will be decayed\'\n    )\n    group.add_argument(\n        \'--lr-patience\',\n        default=10,\n        type=int,\n        help=\'Patience of LR scheduler. See documentation of ReduceLROnPlateau.\'\n    )\n    group.add_argument(\n        \'--lr-step-size\',\n        default=10,\n        type=int,\n        help=\'Period of learning rate decay\'\n    )\n    group.add_argument(\n        \'--n-epochs\',\n        default=200,\n        type=int,\n        help=\'Total number of epochs to train\'\n    )\n    group.add_argument(\n        \'--begin-epoch\',\n        default=1,\n        type=int,\n        help=\'Training begins at this epoch. Previous trained model indicated by resume_path is loaded.\'\n    )\n    group.add_argument(\n        \'--teacher-model\',\n        type=str,\n        help=\'Model (in the same format as in --model) that will be used as teacher in knowledge distillation (if \'\n             \'option is passed)\'\n    )\n    group.add_argument(\n        \'--teacher-checkpoint\',\n        type=Path,\n        help=\'Path to teacher model checkpoint\'\n    )\n    group.add_argument(\n        \'--gradient-clipping\',\n        type=float,\n        help=\'Gradients will be clipped to this value (if passed)\'\n    )\n\n\ndef parse_arguments():\n    parser = get_argument_parser()\n    args = parser.parse_args()\n\n    return args\n'"
pytorch_toolkit/action_recognition/action_recognition/spatial_transforms.py,0,"b'import collections\nimport math\nimport numbers\nimport random\n\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image, ImageEnhance\n\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\nMEAN_STATISTICS = {\n    \'imagenet\': [0.485, 0.456, 0.406],\n    \'kinetics\': [0.434, 0.405, 0.378],\n    \'activitynet\': [0.450, 0.422, 0.390],\n    \'none\': [0.0, 0.0, 0.0]\n}\n\nSTD_STATISTICS = {\n    \'imagenet\': [0.229, 0.224, 0.225],\n    \'kinetics\': [0.152, 0.148, 0.157],\n    \'none\': [1.0, 1.0, 1.0]\n}\n\n\ndef resize(img, size):\n    if not isinstance(size, (list, tuple)):\n        size = (size, size)\n    if isinstance(img, np.ndarray):\n        return cv2.resize(img, size)\n    return img.resize(size, Image.LINEAR)\n\n\ndef crop(img, position):\n    x1, y1, x2, y2 = position\n    if isinstance(img, np.ndarray):\n        return img[y1:y2, x1:x2]\n    return img.crop(position)\n\n\ndef flip(img, horizontal=False):\n    if isinstance(img, np.ndarray):\n        if horizontal:\n            return img[:, ::-1, :]\n        return img[::-1, ...]\n    if horizontal:\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n    return img.transpose(Image.FLIP_TOP_BOTTOM)\n\n\ndef size(img):\n    if isinstance(img, np.ndarray):\n        h, w, c = img.shape\n        return w, h\n    w, h = img.size\n    return w, h\n\n\ndef _repr_params(**kwargs):\n    params = [\'{}={}\'.format(k, str(v)) for k, v in kwargs.items()]\n    return \'({})\'.format(\', \'.join(params))\n\n\nclass VideoSpatialTransform:\n    def randomize_parameters(self):\n        pass\n\n    def __repr__(self):\n        visible_params = {k: getattr(self, k) for k in dir(self) if\n                          not k.startswith(\'_\') and k != \'randomize_parameters\'}\n        return self.__class__.__name__ + _repr_params(**visible_params)\n\n\nclass Compose(VideoSpatialTransform):\n    """"""Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'(\'\n        for t in self.transforms:\n            format_string += \'\\n\'\n            format_string += \'    {0}\'.format(t)\n        format_string += \'\\n)\'\n        return format_string\n\n    def randomize_parameters(self):\n        for t in self.transforms:\n            t.randomize_parameters()\n\n\nclass ToTensor(VideoSpatialTransform):\n    """"""Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n\n    def __init__(self, norm_value=255):\n        self.norm_value = norm_value\n\n    def __call__(self, pic):\n        """"""\n        Args:\n            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        """"""\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            # backward compatibility\n            return img.float().div(self.norm_value)\n\n        if accimage is not None and isinstance(pic, accimage.Image):\n            nppic = np.zeros(\n                [pic.channels, pic.height, pic.width], dtype=np.float32)\n            pic.copyto(nppic)\n            return torch.from_numpy(nppic)\n\n        # handle PIL Image\n        if pic.mode == \'I\':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == \'I;16\':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == \'YCbCr\':\n            nchannel = 3\n        elif pic.mode == \'I;16\':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        # put it from HWC to CHW format\n        # yikes, this transpose takes 80% of the loading time/CPU\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(self.norm_value)\n        else:\n            return img\n\n\nclass Normalize(VideoSpatialTransform):\n    """"""Normalize an tensor image with mean and standard deviation.\n    Given mean: (R, G, B) and std: (R, G, B),\n    will normalize each channel of the torch.*Tensor, i.e.\n    channel = (channel - mean) / std\n    Args:\n        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n        std (sequence): Sequence of standard deviations for R, G, B channels\n            respecitvely.\n    """"""\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        """"""\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        """"""\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return tensor\n\n\nclass Scale(VideoSpatialTransform):\n    """"""Rescale the input image to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (w, h), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n    """"""\n\n    def __init__(self, size):\n        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and\n                                         len(size) == 2)\n        self.size = size\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image or np.ndarray): Image to be scaled.\n        Returns:\n            (PIL.Image or np.ndarray): Rescaled image.\n        """"""\n        if isinstance(self.size, int):\n            w, h = size(img)\n            if (w <= h and w == self.size) or (h <= w and h == self.size):\n                return img\n            if w < h:\n                ow = self.size\n                oh = int(self.size * h / w)\n                return resize(img, (ow, oh))\n            else:\n                oh = self.size\n                ow = int(self.size * w / h)\n                return resize(img, (ow, oh))\n        else:\n            return resize(img, self.size)\n\n\nclass CenterCrop(VideoSpatialTransform):\n    """"""Crops the given image at the center.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be cropped.\n        Returns:\n            PIL.Image: Cropped image.\n        """"""\n        w, h = size(img)\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return crop(img, (x1, y1, x1 + tw, y1 + th))\n\n\nclass CornerCrop(VideoSpatialTransform):\n    """"""Crops the given image at the corners""""""\n\n    def __init__(self, size, crop_position=None):\n        self.size = size\n        if crop_position is None:\n            self.randomize = True\n        else:\n            self.randomize = False\n        self.crop_position = crop_position\n        self.crop_positions = [\'c\', \'tl\', \'tr\', \'bl\', \'br\']\n\n    def __call__(self, img):\n        image_width = size(img)[0]\n        image_height = size(img)[1]\n\n        if self.crop_position == \'c\':\n            th, tw = (self.size, self.size)\n            x1 = int(round((image_width - tw) / 2.))\n            y1 = int(round((image_height - th) / 2.))\n            x2 = x1 + tw\n            y2 = y1 + th\n        elif self.crop_position == \'tl\':\n            x1 = 0\n            y1 = 0\n            x2 = self.size\n            y2 = self.size\n        elif self.crop_position == \'tr\':\n            x1 = image_width - self.size\n            y1 = 0\n            x2 = image_width\n            y2 = self.size\n        elif self.crop_position == \'bl\':\n            x1 = 0\n            y1 = image_height - self.size\n            x2 = self.size\n            y2 = image_height\n        elif self.crop_position == \'br\':\n            x1 = image_width - self.size\n            y1 = image_height - self.size\n            x2 = image_width\n            y2 = image_height\n\n        img = crop(img, (x1, y1, x2, y2))\n\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(size=self.size, crop_position=self.crop_position)\n\n    def randomize_parameters(self):\n        if self.randomize:\n            self.crop_position = self.crop_positions[random.randint(\n                0,\n                len(self.crop_positions) - 1)]\n\n\nclass GaussCrop(VideoSpatialTransform):\n    """"""Crop image at the random point with the standard normal distribution from the center""""""\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img):\n        w, h = size(img)\n        x_c = w // 2\n        sigma_x = max(0, x_c - self.size // 2)\n        y_c = h // 2\n        sigma_y = max(0, y_c - self.size // 2)\n        x_c = round(random.gauss(x_c, 3 * math.sqrt(sigma_x)))\n        y_c = round(random.gauss(y_c, 3 * math.sqrt(sigma_y)))\n\n        x1 = max(0, x_c - self.size // 2)\n        x2 = x1 + self.size\n        y1 = max(0, y_c - self.size // 2)\n        y2 = y1 + self.size\n        img = crop(img, (x1, y1, x2, y2))\n        return img\n\n\nclass RandomHorizontalFlip(VideoSpatialTransform):\n    """"""Horizontally flip the given PIL.Image randomly with a probability of 0.5.""""""\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be flipped.\n        Returns:\n            PIL.Image: Randomly flipped image.\n        """"""\n        if self._rand < 0.5:\n            return flip(img, horizontal=True)\n        return img\n\n    def randomize_parameters(self):\n        self._rand = random.random()\n\n\nclass HorizontalFlip(VideoSpatialTransform):\n    """"""Horizontally flip the given PIL.Image randomly with a probability of 0.5.""""""\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be flipped.\n        Returns:\n            PIL.Image: Flipped image.\n        """"""\n        return flip(img)\n\n\nclass MultiScaleCrop(VideoSpatialTransform):\n    """"""\n    Description: Corner cropping and multi-scale cropping. Two data augmentation techniques introduced in:\n        Towards Good Practices for Very Deep Two-Stream ConvNets,\n        http://arxiv.org/abs/1507.02159\n        Limin Wang, Yuanjun Xiong, Zhe Wang and Yu Qiao\n\n    Parameters:\n        size: height and width required by network input, e.g., (224, 224)\n        scale_ratios: efficient scale jittering, e.g., [1.0, 0.875, 0.75, 0.66]\n        fix_crop: use corner cropping or not. Default: True\n        more_fix_crop: use more corners or not. Default: True\n        max_distort: maximum distortion. Default: 1\n        interpolation: Default: cv2.INTER_LINEAR\n    """"""\n\n    def __init__(self, size, scale_ratios, fix_crop=True, more_fix_crop=True, max_distort=1,\n                 interpolation=Image.LINEAR):\n        self.height = size[0]\n        self.width = size[1]\n        self.scale_ratios = scale_ratios\n        self.fix_crop = fix_crop\n        self.more_fix_crop = more_fix_crop\n        self.max_distort = max_distort\n        self.interpolation = interpolation\n\n        self._crop_scale = None\n        self._crop_offset = None\n        self._num_scales = len(scale_ratios)\n        self._num_offsets = 5 if not more_fix_crop else 13\n\n    def fillFixOffset(self, datum_height, datum_width):\n        h_off = int((datum_height - self.height) / 4)\n        w_off = int((datum_width - self.width) / 4)\n\n        offsets = []\n        offsets.append((0, 0))  # upper left\n        offsets.append((0, 4 * w_off))  # upper right\n        offsets.append((4 * h_off, 0))  # lower left\n        offsets.append((4 * h_off, 4 * w_off))  # lower right\n        offsets.append((2 * h_off, 2 * w_off))  # center\n\n        if self.more_fix_crop:\n            offsets.append((0, 2 * w_off))  # top center\n            offsets.append((4 * h_off, 2 * w_off))  # bottom center\n            offsets.append((2 * h_off, 0))  # left center\n            offsets.append((2 * h_off, 4 * w_off))  # right center\n\n            offsets.append((1 * h_off, 1 * w_off))  # upper left quarter\n            offsets.append((1 * h_off, 3 * w_off))  # upper right quarter\n            offsets.append((3 * h_off, 1 * w_off))  # lower left quarter\n            offsets.append((3 * h_off, 3 * w_off))  # lower right quarter\n\n        return offsets\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(h=self.height, w=self.width, scales=self.scale_ratios,\n                                                      fix_crop=self.fix_crop, max_distort=self.max_distort)\n\n    def fillCropSize(self, input_height, input_width):\n        crop_sizes = []\n        base_size = np.min((input_height, input_width))\n        scale_rates = self.scale_ratios\n        for h in range(len(scale_rates)):\n            crop_h = int(base_size * scale_rates[h])\n            for w in range(len(scale_rates)):\n                crop_w = int(base_size * scale_rates[w])\n                # append this cropping size into the list\n                if np.absolute(h - w) <= self.max_distort:\n                    crop_sizes.append((crop_h, crop_w))\n\n        return crop_sizes\n\n    def __call__(self, image):\n        w, h = size(image)\n\n        crop_size_pairs = self.fillCropSize(h, w)\n        crop_height = crop_size_pairs[self._crop_scale][0]\n        crop_width = crop_size_pairs[self._crop_scale][1]\n\n        if self.fix_crop:\n            offsets = self.fillFixOffset(h, w)\n            h_off = offsets[self._crop_offset][0]\n            w_off = offsets[self._crop_offset][1]\n        else:\n            pass\n            # h_off = random.randint(0, h - self.height)\n            # w_off = random.randint(0, w - self.width)\n\n        x1, y1, x2, y2 = w_off, h_off, w_off + crop_width, h_off + crop_height\n\n        image = crop(image, (x1, y1, x2, y2))\n        return resize(image, (self.width, self.height))\n\n    def randomize_parameters(self):\n        self._crop_scale = np.random.choice(self._num_scales)\n        self._crop_offset = np.random.choice(self._num_offsets)\n\n\nclass RandomSaturation(VideoSpatialTransform):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image):\n        if self.rnd:\n            im2arr = np.array(image).astype(np.float32)\n            im2arr[:, :, 1] *= self.coef\n            im2arr[:, :, 1][im2arr[:, :, 1] > 100] = 100\n            image = Image.fromarray(im2arr.astype(np.uint8), mode=\'HSV\')\n        return image\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(lower=self.lower, upper=self.upper)\n\n    def randomize_parameters(self):\n        self.rnd = random.randint(0, 1)\n        self.scale = random.uniform(self.lower, self.upper)\n        self.coef = random.uniform(self.lower, self.upper)\n\n\nclass RandomHue(VideoSpatialTransform):\n    def __init__(self, delta=14.0):\n        assert 0.0 <= delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image):\n        if self.rnd:\n            im2arr = np.array(image).astype(np.float32)  # im2arr.shape: height x width x channel\n            im2arr[:, :, 0] += self.delta_rnd\n            im2arr[:, :, 0][im2arr[:, :, 0] > 360.0] = 360.0\n            im2arr[:, :, 0][im2arr[:, :, 0] < 0.0] = 0\n            image = Image.fromarray(im2arr.astype(np.uint8), mode=\'HSV\')\n        return image\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(delta=self.delta)\n\n    def randomize_parameters(self):\n        self.rnd = random.randint(0, 1)\n        self.delta_rnd = random.randint(-self.delta, self.delta)\n\n\nclass RandomLightingNoise(VideoSpatialTransform):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image):\n        if self.rnd:\n            shuffle = SwapChannels(self.perm)  # shuffle channels\n            image = shuffle(image)\n        return image\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(perms=self.perms)\n\n    def randomize_parameters(self):\n        self.rnd = random.randint(0, 1)\n        self.perm = self.perms[random.randint(0, len(self.perms) - 1)]\n\n\nclass ConvertColor(VideoSpatialTransform):\n    def __init__(self, current=\'RGB\', transform=\'HSV\'):\n        self.transform = transform\n        self.current = current\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(current=self.current, transform=self.transform)\n\n    def __call__(self, image):\n        if self.current == \'RGB\' and self.transform == \'HSV\':\n            image = image.convert(\'HSV\')\n        elif self.current == \'HSV\' and self.transform == \'RGB\':\n            image = image.convert(\'RGB\')\n        else:\n            raise NotImplementedError\n        return image\n\n\nclass RandomContrast(VideoSpatialTransform):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(lower=self.lower, upper=self.upper)\n\n    # expects float image\n    def __call__(self, image):\n        if self.rnd:\n            enhancer = ImageEnhance.Contrast(image)\n            image = enhancer.enhance(self.factor)\n        return image\n\n    def randomize_parameters(self):\n        self.rnd = random.randint(0, 1)\n        self.factor = random.uniform(self.lower, self.upper)\n\n\nclass RandomBrightness(VideoSpatialTransform):\n    def __init__(self, delta=0.15):\n        assert delta > 0.0\n        assert delta < 1.0\n        self.delta = delta\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(delta=self.delta)\n\n    def __call__(self, image):\n        if self.rnd:\n            enhancer = ImageEnhance.Brightness(image)\n            image = enhancer.enhance(self.factor)\n        return image\n\n    def randomize_parameters(self):\n        self.rnd = random.randint(0, 1)\n        self.factor = random.uniform(1.0 - self.delta, 1.0 + self.delta)\n\n\nclass RandomSharpness(VideoSpatialTransform):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(lower=self.lower, upper=self.upper)\n\n    # expects float image\n    def __call__(self, image):\n        if self.rnd:\n            enhancer = ImageEnhance.Sharpness(image)\n            image = enhancer.enhance(self.factor)\n        return image\n\n    def randomize_parameters(self):\n        self.rnd = random.randint(0, 1)\n        self.factor = random.uniform(self.lower, self.upper)\n\n\nclass SwapChannels(VideoSpatialTransform):\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        im2arr = np.array(image)\n        im2arr = im2arr[:, :, self.swaps]\n        image = Image.fromarray(im2arr)\n\n        return image\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(swaps=self.swaps)\n\n\nclass PhotometricDistort(VideoSpatialTransform):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),\n            ConvertColor(transform=\'HSV\'),\n            RandomSaturation(),\n            RandomHue(),\n            ConvertColor(current=\'HSV\', transform=\'RGB\'),\n            RandomContrast()\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image):\n        if isinstance(image, np.ndarray):\n            im = Image.fromarray(image)\n        else:\n            im = image.copy()\n\n        im = self.rand_brightness(im)\n        if self.rnd1:\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im = distort(im)\n\n        im = self.rand_light_noise(im)\n        if isinstance(image, np.ndarray):\n            return np.asarray(im)\n        return im\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params()\n\n    def randomize_parameters(self):\n        self.rnd1 = random.randint(0, 1)\n        self.rand_brightness.randomize_parameters()\n        self.rand_light_noise.randomize_parameters()\n        for aug in self.pd:\n            aug.randomize_parameters()\n'"
pytorch_toolkit/action_recognition/action_recognition/target_transforms.py,0,"b'class Compose(object):\n    """"""Compose multiple target transforms""""""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, target):\n        dst = []\n        for t in self.transforms:\n            dst.append(t(target))\n        return dst\n\n\nclass ClassLabel(object):\n    """"""Returns video label and name. Used for training and validation.""""""\n\n    def __call__(self, target):\n        return {\n            \'label\': target[\'label\'],\n            \'video\': target[\'video\']\n        }\n\n\nclass VideoID(object):\n    """"""Returns video name. Used for video prediction.""""""\n\n    def __call__(self, target):\n        return {\n            \'label\': target[\'label\'],\n            \'video_id\': target[\'video_id\']\n        }\n'"
pytorch_toolkit/action_recognition/action_recognition/temporal_transforms.py,0,"b'import random\n\nfrom .spatial_transforms import _repr_params\n\n\nclass LoopPadding(object):\n    """"""Extend short clip to a given size""""""\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        out = frame_indices\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(size=self.size)\n\n\nclass TemporalStride(object):\n    """"""Skips frames with a given step. Increases effective temporal receptive field.""""""\n\n    def __init__(self, stride=1):\n        self.stride = stride\n\n    def __call__(self, frame_indices):\n        return frame_indices[::self.stride]\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(stride=self.stride)\n\n\nclass TemporalBeginCrop(object):\n    """"""Temporally crop the given frame indices at a beginning.\n\n    If the number of frames is less than the size,\n    loop the indices as many times as necessary to satisfy the size.\n\n    Args:\n        size (int): Desired output size of the crop.\n    """"""\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        out = frame_indices[:self.size]\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(size=self.size)\n\n\nclass TemporalCenterCrop(object):\n    """"""Temporally crop the given frame indices at a center.\n\n    If the number of frames is less than the size,\n    loop the indices as many times as necessary to satisfy the size.\n\n    Args:\n        size (int): Desired output size of the crop.\n    """"""\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        """"""\n        Args:\n            frame_indices (list): frame indices to be cropped.\n        Returns:\n            list: Cropped frame indices.\n        """"""\n\n        center_index = len(frame_indices) // 2\n        begin_index = max(0, center_index - (self.size // 2))\n        end_index = min(begin_index + self.size, len(frame_indices))\n\n        out = frame_indices[begin_index:end_index]\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(size=self.size)\n\n\nclass TemporalRandomCrop(object):\n    """"""Temporally crop the given frame indices at a random location.\n\n    If the number of frames is less than the size,\n    loop the indices as many times as necessary to satisfy the size.\n\n    Args:\n        size (int): Desired output size of the crop.\n    """"""\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        """"""\n        Args:\n            frame_indices (list): frame indices to be cropped.\n        Returns:\n            list: Cropped frame indices.\n        """"""\n\n        rand_end = max(0, len(frame_indices) - self.size - 1)\n        begin_index = random.randint(0, rand_end)\n        end_index = min(begin_index + self.size, len(frame_indices))\n\n        out = frame_indices[int(begin_index):int(end_index)]\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n    def __repr__(self):\n        return self.__class__.__name__ + _repr_params(size=self.size)\n'"
pytorch_toolkit/action_recognition/action_recognition/test.py,0,"b'import torch\nimport torch.nn.functional as F\n\nfrom .utils import AverageMeter, prepare_batch\nfrom .utils import calculate_accuracy\n\n\ndef test(args, data_loader, model, logger):\n    print(\'test\')\n    model.eval()\n\n    video_acc = AverageMeter()\n\n    output_buffer = []\n    previous_video_id = None\n    for i, (inputs, targets) in logger.scope_enumerate(data_loader):\n        video_ids = targets[\'video\']\n        batch_size, inputs, labels = prepare_batch(args, inputs, targets)\n\n        outputs = model(*inputs)\n\n        if args.softmax_in_test:\n            outputs = F.softmax(outputs)\n\n        for j in range(outputs.size(0)):\n\n            if video_ids[j] != previous_video_id and not (i == 0 and j == 0):\n                # Computed all segments for current video\n                video_outputs = torch.stack(output_buffer)\n                video_result = torch.mean(video_outputs, dim=0)\n                probs, preds = torch.topk(video_result, k=1)\n\n                is_correct_match = (video_gt.cpu() == preds).item()\n                video_acc.update(is_correct_match)\n\n                output_buffer.clear()\n\n            output_buffer.append(outputs[j].data.cpu())\n            previous_video_id = video_ids[j]\n            video_gt = labels[j]\n\n        clip_acc = calculate_accuracy(outputs, labels)\n\n        logger.log_value(""test/acc"", clip_acc, batch_size)\n        logger.log_value(""test/video"", video_acc.avg)\n\n    return logger.get_value(""test/video""), logger.get_value(""test/acc"")\n'"
pytorch_toolkit/action_recognition/action_recognition/train.py,0,"b'import torch\nfrom torch.optim import lr_scheduler\n\nfrom .utils import (calculate_accuracy, prepare_batch,\n                    save_checkpoint)\nfrom .validation import validate\n\n\ndef train_epoch(args, epoch, data_loader, model, criterion, optimizer, logger):\n    model.train()\n\n    for i, (inputs_dict, targets) in logger.scope_enumerate(data_loader, epoch, total_time=\'time/train_epoch\',\n                                                            fetch_time=\'time/train_data\', body_time=\'time/train_step\'):\n        batch_size, inputs, labels = prepare_batch(args, inputs_dict, targets)\n        outputs = model(*inputs)\n\n        loss = criterion(outputs=outputs, inputs=inputs, targets=labels)\n        acc = calculate_accuracy(outputs, labels)\n\n        if i % args.iter_size == 0:\n            optimizer.zero_grad()\n\n        loss.backward()\n\n        if args.gradient_clipping:\n            torch.nn.utils.clip_grad_norm(model.parameters(), args.gradient_clipping)\n\n        if args.iter_size > 1 and (i + 1) % args.iter_size == 0:\n            for p in model.parameters():\n                p.grad.data.mul_(1 / args.iter_size)\n\n        optimizer.step()\n\n        logger.log_value(\'train/loss\', loss.item(), batch_size, epoch * len(data_loader) + i)\n        logger.log_value(\'train/acc\', acc, batch_size, epoch * len(data_loader) + i)\n        if \'kd\' in criterion.values:\n            logger.log_value(""train/kd_loss"", criterion.values[\'kd\'].item())\n\n    logger.log_value(""train/epoch_loss"", logger.get_value(""train/loss""))\n    logger.log_value(""train/epoch_acc"", logger.get_value(""train/acc""))\n\n    return logger.get_value(""train/acc""), logger.get_value(""train/loss"")\n\n\ndef train(args, model, train_loader, val_loader, criterion, optimizer, scheduler, logger):\n    for epoch in range(args.begin_epoch, args.n_epochs + 1):\n        with logger.scope(epoch):\n            for i, group in enumerate(optimizer.param_groups):\n                group_name = group.get(\'group_name\', i)\n                logger.log_value(""lr/{}"".format(group_name), group[\'lr\'])\n\n        with logger.scope(epoch):\n            train_acc, loss = train_epoch(args, epoch, train_loader, model, criterion, optimizer, logger)\n\n        if epoch % args.checkpoint == 0:\n            checkpoint_name = \'save_{}.pth\'.format(epoch)\n            save_checkpoint(checkpoint_name, model, optimizer, epoch, args)\n\n        if epoch % args.validate == 0:\n            with logger.scope(epoch):\n                val_acc = validate(args, epoch, val_loader, model, criterion, logger)\n                logger.log_value(""val/generalization_error"", val_acc - train_acc)\n\n        if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(val_acc)\n        else:\n            scheduler.step()\n\n        logger.reset_values(\'train\')\n        logger.reset_values(\'val\')\n'"
pytorch_toolkit/action_recognition/action_recognition/utils.py,0,"b'import json\nimport operator\nimport pickle\nimport re\nimport subprocess\nimport sys\nimport tarfile\nfrom collections import OrderedDict\nfrom hashlib import md5\nfrom itertools import islice, tee\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\n\nfrom action_recognition.options import get_argument_parser\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef load_value_file(file_path):\n    with open(str(file_path), \'r\') as input_file:\n        value = float(input_file.read().rstrip(\'\\n\\r\'))\n\n    return value\n\n\ndef load_json(data_file_path):\n    with open(str(data_file_path), \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef calculate_accuracy(outputs, targets):\n    batch_size = targets.size(0)\n\n    _, pred = outputs.topk(1, 1, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1))\n    n_correct_elems = correct.float().sum().item()\n\n    return n_correct_elems / batch_size\n\n\ndef mkdir_if_not_exists(path):\n    Path(path).mkdir(exist_ok=True, parents=True)\n\n\nclass TeedStream(object):\n    """"""Copy stdout to the file""""""\n\n    def __init__(self, fname, mode=\'w\'):\n        self.file = open(str(fname), mode)\n        self.stdout = sys.stdout\n        sys.stdout = self\n\n    def __del__(self):\n        sys.stdout = self.stdout\n        self.file.close()\n\n    def write(self, data):\n        self.file.write(data)\n        self.stdout.write(data)\n\n    def flush(self):\n        self.file.flush()\n\n\ndef get_nested_attr(obj, attr_name):\n    """"""Same as getattr(obj, attr_name), but allows attr_name to be nested\n    e.g. get_nested_attr(obj, ""foo.bar"") is equivalent to obj.foo.bar""""""\n    for name_part in attr_name.split(\'.\'):\n        if hasattr(obj, name_part):\n            obj = getattr(obj, name_part)\n        else:\n            raise AttributeError(""module has no "" + name_part + "" attribute"")\n\n    return obj\n\n\ndef load_state(module, state_dict, classifier_layer_name=None, remaps=None):\n    """"""\n    Robust checkpoint loading routine.\n\n    Args:\n        module: Loaded model\n        state_dict: dict containing parameters and buffers\n        classifier_layer_name (str): name of the classifier layer of the model\n        remaps (dict): mapping from checkpoint names to module names\n    """"""\n    if isinstance(module, nn.DataParallel):\n        module = module.module\n\n    if remaps is None:\n        remaps = {}\n\n    # unwrap module in state dict\n    unwrapped_state = OrderedDict()\n    strip_line = \'module.\'\n    for k, v in state_dict.items():\n        if k.startswith(strip_line):\n            k = k[len(strip_line):]\n\n        if k in remaps:\n            k = remaps[k]\n\n        unwrapped_state[k] = v\n\n    if classifier_layer_name is None:\n        return module.load_state_dict(unwrapped_state, strict=False)\n\n    module_classes = get_nested_attr(module, classifier_layer_name).out_features\n    checkpoint_classes = unwrapped_state[\'{}.weight\'.format(classifier_layer_name)].size(0)\n\n    if module_classes != checkpoint_classes:\n        print(""Number of classes in model and checkpoint vary ({} vs {}). Do not loading last FC weights"".format(\n            module_classes, checkpoint_classes))\n        del unwrapped_state[\'{}.weight\'.format(classifier_layer_name)]\n        del unwrapped_state[\'{}.bias\'.format(classifier_layer_name)]\n\n    return module.load_state_dict(unwrapped_state, strict=False)\n\n\ndef save_checkpoint(checkpoint_name, model, optimizer, epoch_no, args):\n    save_file_path = args.result_path / \'checkpoints\' / checkpoint_name\n    states = {\n        \'epoch\': epoch_no + 1,\n        \'state_dict\': model.state_dict(),\n        \'optimizer\': optimizer.state_dict(),\n    }\n    torch.save(states, save_file_path.as_posix())\n\n\ndef create_code_snapshot(root, dst_path, extensions=("".py"", "".json"")):\n    """"""Creates tarball with the source code""""""\n    with tarfile.open(str(dst_path), ""w:gz"") as tar:\n        for path in Path(root).rglob(""*""):\n            if \'.git\' in path.parts:\n                continue\n            if path.suffix.lower() in extensions:\n                tar.add(path.as_posix(), arcname=path.relative_to(root).as_posix(), recursive=True)\n\n\ndef print_git_revision():\n    try:\n        rev = subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\']).decode().strip()\n        branch = subprocess.check_output([\'git\', \'rev-parse\', \'--abbrev-ref\', \'HEAD\']).decode().strip()\n        print(""Git branch: {}"".format(branch))\n        print(""Git rev: {}"".format(rev))\n    except subprocess.CalledProcessError:\n        print(""No git repo found"")\n\n\ndef get_fine_tuning_parameters(model, param_groups=None):\n    """"""Returns parameter groups in optimizer format. Allows to select per-layer learning rates.""""""\n    if param_groups is None:\n        param_groups = [(\'trainable\', {\'re\': r\'\'})]\n\n    for param_name, param in model.named_parameters():\n        group = None\n        for group_name, group in param_groups:\n            # find first matched group for a given param\n            if re.search(group.get(\'re\', \'\'), param_name):\n                break\n\n        print(""{} -> {}"".format(param_name, group_name))\n        group.setdefault(\'params\', []).append(param)\n\n    # save group names for plotting lrs\n    # and init params key for optimizer\n    for param_name, param in param_groups:\n        param[\'group_name\'] = param_name\n        param.setdefault(\'params\', [])\n\n    return [group for name, group in param_groups]\n\n\ndef drop_last(iterable, n=1):\n    """"""Drops the last item of iterable""""""\n    t1, t2 = tee(iterable)\n    return map(operator.itemgetter(0), zip(t1, islice(t2, n, None)))\n\n\ndef json_serialize(obj):\n    """"""Serialization function for json.dump""""""\n    if isinstance(obj, Path):\n        return str(obj)\n    return obj.__dict__\n\n\ndef prepare_batch(args, inputs_dict, targets):\n    """"""Converts dict returned from data loader to tuple of tensors and converts targets to tensors""""""\n    labels = targets[\'label\']\n    labels = labels.to(args.device)\n    for key in inputs_dict:\n        inputs_dict[key] = inputs_dict[key].to(args.device)\n        if args.fp16:\n            inputs_dict[key] = inputs_dict[key].to(torch.half)\n        batch_size = inputs_dict[key].size(0)\n    inputs = tuple(inputs_dict[k] for k in (\'rgb_clip\', \'flow_clip\') if k in inputs_dict)\n    return batch_size, inputs, labels\n\n\ndef md5_hash(obj):\n    obj_serialize = json.dumps(obj, default=json_serialize)\n    digest = md5(obj_serialize.encode())\n    return digest.hexdigest()\n\n\ndef cached(file=None):\n    """"""Cache returned value of a wrapped function to disk. Next call with the same arguments will result in loading\n    the saved values.""""""\n\n    def decorator(fn):\n        nonlocal file\n        if file is None:\n            file = ""{}.cache"".format(fn.__name__)\n\n        def wrapped(*args, **kwargs):\n            data = {\'args\': None, \'kwargs\': None, \'ret\': None}\n            args_hex = md5_hash((args, kwargs))[-8:]\n            file_hex = Path(""{!s}.{}"".format(file, args_hex))\n            if file_hex.exists():\n                with file_hex.open(\'rb\') as f:\n                    data = pickle.load(f)\n\n            if data[\'args\'] != args or data[\'kwargs\'] != kwargs:\n                data[\'args\'] = args\n                data[\'kwargs\'] = kwargs\n                data[\'ret\'] = fn(*args, **kwargs)\n\n                with file_hex.open(\'wb\') as f:\n                    pickle.dump(data, f)\n\n            return data[\'ret\']\n\n        return wrapped\n\n    return decorator\n\n\ndef generate_args(*args, **kwargs):\n    argparser = get_argument_parser()\n    argv = list(args)\n    for k, v in kwargs.items():\n        key_format = ""--{:s}"".format(k.replace(\'_\', \'-\'))\n        no_key_format = ""--no-{:s}"".format(k.replace(\'_\', \'-\'))\n        if isinstance(v, bool):\n            if v:\n                argv.append(key_format)\n            else:\n                argv.append(no_key_format)\n        else:\n            argv.append(key_format)\n            argv.append(str(v))\n\n    return argparser.parse_known_args(argv)\n'"
pytorch_toolkit/action_recognition/action_recognition/validation.py,0,"b'import torch\n\nfrom .utils import AverageMeter, calculate_accuracy, prepare_batch\n\n\ndef validate(args, epoch, data_loader, model, criterion, logger):\n    model.eval()\n\n    video_acc = AverageMeter()\n    clip_logits = []\n    previous_video_id = None\n    for i, (inputs, targets) in logger.scope_enumerate(data_loader, epoch, total_time=\'time/val_epoch\',\n                                                       fetch_time=\'time/val_data\', body_time=\'time/val_step\'):\n        video_ids = targets[\'video\']\n        batch_size, inputs, labels = prepare_batch(args, inputs, targets)\n        with torch.no_grad():\n            outputs = model(*inputs)\n\n        # compute video acc\n        for j in range(outputs.size(0)):\n            if video_ids[j] != previous_video_id and previous_video_id is not None:\n                clip_logits = torch.stack(clip_logits)\n                video_logits = torch.mean(clip_logits, dim=0)\n                probs, preds = torch.topk(video_logits, k=1)\n\n                video_acc.update((previous_video_gt == preds).item())\n                clip_logits = []\n\n            clip_logits.append(outputs[j].data.cpu())\n            previous_video_id = video_ids[j]\n            previous_video_gt = labels[j].cpu()\n\n        loss = criterion(outputs=outputs, targets=labels, inputs=inputs)\n        acc = calculate_accuracy(outputs, labels)\n\n        logger.log_value(""val/loss"", loss.item(), batch_size)\n        logger.log_value(""val/acc"", acc, batch_size)\n    logger.log_value(""val/video"", video_acc.avg)\n\n    return logger.get_value(""val/acc"")\n'"
pytorch_toolkit/action_recognition/action_recognition/video_reader.py,0,"b'import os\nimport numpy as np\n\nimport cv2\nfrom PIL import Image\n\n\nclass VideoReader:\n    """"""This class reads specified frame numbers from video""""""\n\n    def read(self, video_path, frame_indices):\n        raise NotImplementedError\n\n    def __call__(self, video_path, frame_indices):\n        return self.read(video_path, frame_indices)\n\n\nclass ImageDirReader(VideoReader):\n    """"""Reads clip from directory with frames with a given file name pattern\n\n    For example:\n        video1/frame_00001.jpg\n        video1/frame_00002.jpg\n        ....\n\n    """"""\n\n    def __init__(self, read_image_fn, frame_name_pattern):\n        self.read_image_fn = read_image_fn\n        self.frame_name_pattern = frame_name_pattern\n\n    def read(self, video_path, frame_indices):\n        video = []\n        for i in frame_indices:\n            image_path = os.path.join(video_path, self.frame_name_pattern % i)\n            if os.path.exists(image_path):\n                video.append(self.read_image_fn(image_path))\n            else:\n                return video\n\n        return video\n\n\nclass VideoFileReader(VideoReader):\n    """"""Reads clip from video file.""""""\n\n    def read(self, video_path, frame_indices):\n        video = []\n        cap = cv2.VideoCapture(video_path)\n\n        unique_indices = sorted(set(frame_indices))\n        current_frame_idx = 0\n        frame_map = {}\n        for next_frame_idx in unique_indices:\n            while current_frame_idx != next_frame_idx:\n                status, frame = cap.read()\n                current_frame_idx += 1\n                if not status:\n                    cap.release()\n                    return video\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_map[next_frame_idx] = frame\n        cap.release()\n        video = [frame_map[i] for i in frame_indices]\n        return video\n\n\ndef pil_read_image(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        with Image.open(f) as img:\n            img = img.convert(\'RGB\')\n            return np.asarray(img)\n\n\ndef accimage_read_image(path):\n    try:\n        import accimage\n        return np.asarray(accimage.Image(path))\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_read_image(path)\n\n\ndef opencv_read_image(path):\n    image_bgr = cv2.imread(path)\n    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    return image_rgb\n\n\ndef make_video_reader(video_format=\'frames\', image_reader=\'opencv\'):\n    if image_reader == \'opencv\':\n        image_read_fn = opencv_read_image\n    elif image_reader == \'pil\':\n        image_read_fn = pil_read_image\n    elif image_reader == \'accimage\':\n        image_read_fn = accimage_read_image\n    else:\n        raise ValueError(""Unknown image reading function"")\n\n    if video_format and video_format.lower() == \'video\':\n        return VideoFileReader()\n    return ImageDirReader(image_read_fn, \'image_%05d.jpg\')\n\n\ndef read_flow(path, frame_indices):\n    video = []\n    for i in frame_indices[:-1]:\n        flow_x_path = os.path.join(path, \'flow_x_{:05d}.jpg\'.format(i))\n        flow_y_path = os.path.join(path, \'flow_y_{:05d}.jpg\'.format(i))\n        if os.path.exists(flow_x_path) and os.path.exists(flow_y_path):\n            with open(flow_x_path, \'rb\') as f:\n                with Image.open(f) as img:\n                    flow_x = img.convert(\'L\')\n            with open(flow_y_path, \'rb\') as f:\n                with Image.open(f) as img:\n                    flow_y = img.convert(\'L\')\n            video.append(flow_x)\n            video.append(flow_y)\n        else:\n            raise Exception(flow_x_path + "" does not exist"")\n    return video\n'"
pytorch_toolkit/action_recognition/tests/__init__.py,0,b''
pytorch_toolkit/action_recognition/tests/conftest.py,0,"b""from pathlib import Path\n\nimport numpy as np\nimport pytest\n\nTEST_DIR = Path(__file__).parent\nPROJECT_DIR = Path(__file__).parent\n\n\n@pytest.fixture\ndef data_path():\n    return TEST_DIR / 'test_data'\n\n\n@pytest.fixture\ndef rand():\n    return np.random.RandomState(seed=1)\n"""
pytorch_toolkit/action_recognition/tests/test_attention.py,0,"b'import torch\nimport numpy as np\n\nfrom action_recognition.models.modules.self_attention import ScaledDotProductAttention, MultiHeadAttention\n\n\nclass TestScaledDotProductAttention:\n    def test_shapes(self):\n        layer = ScaledDotProductAttention(16, attn_dropout=0)\n        q = torch.zeros(4, 8, 16)\n        k = torch.zeros(4, 4, 16)\n        v = torch.zeros(4, 4, 16)\n\n        with torch.no_grad():\n            outputs, attns = layer(q, k, v)\n\n        assert outputs.size() == q.size()\n        assert attns.size(0) == v.size(0)\n        assert attns.size(1) == q.size(1)\n        assert attns.size(2) == k.size(1)\n\n    def test_atten_range(self, rand):\n        layer = ScaledDotProductAttention(16, attn_dropout=0)\n        q = torch.from_numpy(rand.rand(2, 4, 16))\n        k = torch.from_numpy(rand.rand(2, 4, 16))\n        v = torch.from_numpy(rand.rand(2, 4, 16))\n\n        with torch.no_grad():\n            outputs, attns = layer(q, k, v)\n        attns = attns.numpy()\n\n        assert np.alltrue(attns >= 0)\n        assert np.alltrue(attns <= 1)\n        assert np.allclose(attns.sum(2), np.ones((2, 4)))\n\n\nclass TestMultiHeadAttention:\n    def test_shapes(self):\n        layer = MultiHeadAttention(\n            n_head=2,\n            input_size=16,\n            output_size=16,\n            d_k=8,\n            d_v=8,\n            dropout=0,\n            use_proj=False,\n            layer_norm=False\n        )\n        q = torch.zeros(2, 8, 16)\n        k = torch.zeros(2, 4, 16)\n        v = torch.zeros(2, 4, 16)\n\n        with torch.no_grad():\n            outputs, attns = layer(q, k, v)\n\n        assert outputs.size() == q.size()\n\n    def test_shapes_with_use_proj_set(self):\n        layer = MultiHeadAttention(\n            n_head=2,\n            input_size=16,\n            output_size=16,\n            d_k=4,\n            d_v=4,\n            dropout=0,\n            use_proj=True,\n            layer_norm=False\n        )\n        q = torch.zeros(2, 8, 16)\n        k = torch.zeros(2, 4, 16)\n        v = torch.zeros(2, 4, 16)\n\n        with torch.no_grad():\n            outputs, attns = layer(q, k, v)\n\n        assert outputs.size() == q.size()\n'"
pytorch_toolkit/action_recognition/tests/test_datasets.py,0,"b""from action_recognition.dataset import sample_clips\n\n\ndef _make_video(length):\n    return {\n        'segment': (1, length),\n        'frame_indices': list(range(1, 1 + length)),\n        'video': '1'\n    }\n\n\ndef _check_video_segment(video, start, expected_len):\n    assert video['frame_indices'] == list(range(start, start + expected_len))\n\n\nclass TestSampleClips:\n    def test_step_one(self):\n        videos = [_make_video(5)]\n\n        clips = sample_clips(videos, 3, 3)\n\n        assert len(clips) == 3\n        _check_video_segment(clips[0], 1, 3)\n        _check_video_segment(clips[1], 2, 3)\n        _check_video_segment(clips[2], 3, 3)\n\n    def test_step_two(self):\n        videos = [_make_video(7)]\n\n        clips = sample_clips(videos, 3, 3)\n\n        assert len(clips) == 3\n        _check_video_segment(clips[0], 1, 3)\n        _check_video_segment(clips[1], 3, 3)\n        _check_video_segment(clips[2], 5, 3)\n\n    def test_one_sample(self):\n        videos = [_make_video(16)]\n\n        clips = sample_clips(videos, 1, 1)\n\n        assert len(clips) == 1\n        assert clips[0]['frame_indices'] == videos[0]['frame_indices']\n\n    def test_short_clip(self):\n        videos = [_make_video(3)]\n\n        clips = sample_clips(videos, 4, 3)\n\n        assert len(clips) == 3\n        _check_video_segment(clips[0], 1, 3)\n        _check_video_segment(clips[1], 2, 2)\n        _check_video_segment(clips[2], 3, 1)\n"""
pytorch_toolkit/action_recognition/tests/test_logger.py,0,"b""import pytest\n\nfrom action_recognition.logging import TrainingLogger, PeriodicHandler, StreamHandler\n\n\nclass TestTrainingLogger:\n    def test_dispatches_to_handler(self, mocker):\n        logger = TrainingLogger()\n        handler_mock = mocker.Mock()\n        logger.register_handler('handler', handler_mock)\n        logger.register_value('value', ['handler'])\n\n        logger.log_value('value', 1)\n        assert handler_mock.write.call_count == 1\n\n    def test_raises_with_unknown_handler(self):\n        logger = TrainingLogger()\n\n        with pytest.raises(Exception):\n            logger.register_value('value', ['handler'])\n\n    def test_raises_with_unknown_value(self):\n        logger = TrainingLogger()\n\n        with pytest.raises(Exception):\n            logger.log_value('value', 1)\n\n    def test_dispatches_to_group_handler(self, mocker):\n        logger = TrainingLogger()\n        handler_mock = mocker.Mock()\n        logger.register_handler('handler', handler_mock)\n\n        logger.register_value_group(r'good_group/.*', ['handler'])\n\n        logger.log_value('good_group/value', 1)\n        assert handler_mock.write.call_count == 1\n\n        with pytest.raises(Exception):\n            logger.log_value('bad_group/value', 1)\n\n    def test_dispatches_to_multiple_handlers(self, mocker):\n        logger = TrainingLogger()\n        handler1_mock = mocker.Mock()\n        handler2_mock = mocker.Mock()\n        logger.register_handler('handler1', handler1_mock)\n        logger.register_handler('handler2', handler2_mock)\n        logger.register_value('value', ['handler1', 'handler2'])\n\n        logger.log_value('value', 1)\n        assert handler1_mock.write.call_count == 1\n        assert handler2_mock.write.call_count == 1\n\n    def test_value_averaged_within_scope(self, mocker):\n        logger = TrainingLogger()\n        handler_mock = mocker.Mock()\n        logger.register_handler('handler', handler_mock)\n        logger.register_value('value', ['handler'], average=True)\n\n        logger.log_value('value', 1)\n        assert logger.get_value('value') == 1\n        logger.log_value('value', 2)\n        assert logger.get_value('value') == 1.5\n        logger.log_value('value', 3)\n        assert logger.get_value('value') == 2\n\n    def test_saves_instant_values(self, mocker):\n        logger = TrainingLogger()\n        handler_mock = mocker.Mock()\n        logger.register_handler('handler', handler_mock)\n        logger.register_value('value', ['handler'], average=True)\n\n        logger.log_value('value', 1)\n        assert logger.get_value('value') == 1\n        assert handler_mock.write.call_args_list[0][0][0].instant_value == 1\n        logger.log_value('value', 2)\n        assert logger.get_value('value') == 1.5\n        assert handler_mock.write.call_args_list[1][0][0].instant_value == 2\n        logger.log_value('value', 3)\n        assert logger.get_value('value') == 2\n        assert handler_mock.write.call_args_list[2][0][0].instant_value == 3\n\n\n@pytest.fixture\ndef periodic_handler_mock(mocker):\n    periodic_handler = PeriodicHandler(scope='test_scope')\n    periodic_handler.flush = mocker.Mock()\n    return periodic_handler\n\n\nclass TestPeriodicHandler:\n    def test_value_resets(self, periodic_handler_mock):\n        logger = TrainingLogger()\n        logger.register_handler('handler', periodic_handler_mock)\n        logger.register_value('scope/value', ['handler'], average=True)\n\n        with logger.scope(scope='test_scope'):\n            logger.log_value('scope/value', 1)\n            logger.log_value('scope/value', 2)\n            logger.log_value('scope/value', 3)\n        assert logger.get_value('scope/value') == 2\n\n        logger.reset_values('scope')\n\n        with logger.scope(scope='test_scope'):\n            logger.log_value('scope/value', 5)\n            logger.log_value('scope/value', 10)\n            logger.log_value('scope/value', 15)\n        assert logger.get_value('scope/value') == 10\n\n    def test_correct_scope_handled(self, periodic_handler_mock):\n        logger = TrainingLogger()\n        logger.register_handler('handler', periodic_handler_mock)\n        logger.register_value('value', ['handler'])\n\n        with logger.scope(scope='wrong_scope'):\n            pass\n\n        periodic_handler_mock.flush.assert_not_called()\n\n        with logger.scope(scope='test_scope'):\n            pass\n\n        assert periodic_handler_mock.flush.call_count == 1\n\n\nclass TestStreamHandler:\n    def test_flush_writes_to_stream(self, mocker):\n        logger = TrainingLogger()\n        stream = mocker.Mock()\n        stream_handler = StreamHandler('epoch', stream=stream, fmt='* {values}')\n        logger.register_handler('handler', stream_handler)\n        logger.register_value('value', ['handler'])\n\n        with logger.scope():\n            logger.log_value('value', 1)\n            logger.log_value('value', 0.1)\n\n        stream.write.assert_called_once_with('* value 0.1000\\n')\n        assert stream.flush.call_count == 1\n\n    def test_instant_values_displayed(self, mocker):\n        logger = TrainingLogger()\n        stream = mocker.Mock()\n        stream_handler = StreamHandler('batch', stream=stream, fmt='* {values}')\n        logger.register_handler('handler', stream_handler)\n        logger.register_value('value', ['handler'], average=True)\n\n        for i, x in logger.scope_enumerate(range(1, 4)):\n            logger.log_value('value', x)\n\n        assert stream.write.call_count == 3\n        stream.write.assert_has_calls([\n            mocker.call('* value 1.0000 (1.0000)\\n'),\n            mocker.call('* value 2.0000 (1.5000)\\n'),\n            mocker.call('* value 3.0000 (2.0000)\\n'),\n        ])\n"""
pytorch_toolkit/action_recognition/tests/test_loss.py,0,"b""import numpy as np\nimport torch\n\nfrom action_recognition.loss import SoftmaxLoss, DistillationLoss, WeightedSumLoss\n\n\ndef _softmax(z, dim=1):\n    return np.exp(z) / np.sum(np.exp(z), axis=dim)[:, np.newaxis]\n\n\ndef _one_hot(x, n):\n    z = np.zeros((len(x), n), dtype=np.float)\n    z[np.arange(z.shape[0]), x] = 1\n    return z\n\n\ndef _log_softmax(z):\n    s = np.sum(np.exp(z), axis=1)\n    return (z.T - np.log(s)).T\n\n\ndef _kl_div(p, q):\n    return np.sum(np.where((p > 0) & (q > 0), p * np.log(p / q), 0)) / p.shape[0]\n\n\ndef _cross_entropy(p, q):\n    return - np.sum(np.where(q > 0, p * np.log(q), 0)) / p.shape[0]\n\n\nclass TestSoftMaxLoss:\n    def test_same(self):\n        loss = SoftmaxLoss()\n        p = torch.tensor([\n            [0, 10, 0, 0],\n            [0, 0, 10, 0],\n            [0, 0, 0, 10],\n        ], dtype=torch.float)\n\n        q = torch.tensor([1, 2, 3])\n\n        ref = _cross_entropy(_one_hot(q.numpy(), 4), _softmax(p.numpy()))\n        val = loss(outputs=p, targets=q).item()\n\n        assert np.abs(ref - val) < 1e-4\n\n    def test_different(self):\n        loss = SoftmaxLoss()\n        p = torch.tensor([\n            [0, 10, 0, 0],\n            [0, 0, 10, 0],\n            [0, 0, 0, 10],\n        ], dtype=torch.float)\n\n        q = torch.tensor([1, 3, 2])\n\n        ref = _cross_entropy(_one_hot(q.numpy(), 4), _softmax(p.numpy()))\n        val = loss(outputs=p, targets=q).item()\n\n        assert np.abs(ref - val) < 1e-4\n\n\nclass TestDistilationLoss:\n    def test_different_output(self, mocker):\n        p = torch.tensor([\n            [0, 10, 3, 0],\n            [1, 2, 10, 6],\n            [0, 3, 4, 10]\n        ], dtype=torch.float)\n\n        q = torch.tensor([\n            [0, 10, 0, 0],\n            [0, 0, 10, 0],\n            [0, 0, 10, 0]\n        ], dtype=torch.float)\n\n        teacher_mock = mocker.Mock(return_value=p)\n        loss = DistillationLoss(teacher_mock)\n\n        ref = _kl_div(_softmax(p.numpy()), _softmax(q.numpy()))\n        loss_val = loss(outputs=q, inputs=[1, 1, 1]).item()\n\n        assert np.abs(ref - loss_val) < 1e-4\n\n    def test_same_output(self, mocker):\n        p = torch.tensor([\n            [0, 10, 0, 0],\n            [0, 0, 10, 0],\n            [0, 0, 0, 10]\n        ], dtype=torch.float)\n        teacher_mock = mocker.Mock(return_value=p)\n        loss = DistillationLoss(teacher_mock)\n\n        ref = _kl_div(_softmax(p.numpy()), _softmax(p.numpy()))\n        loss_val = loss(outputs=p, inputs=[1, 1, 1])\n\n        assert np.abs(ref - loss_val) < 1e-4\n\n\nclass TestWeightedSumLoss:\n    def make_mock_loss(self, return_value):\n        class _Loss(torch.nn.Module):\n            def forward(self, outputs, **input):\n                return return_value\n\n        return _Loss()\n\n    def test_sum(self):\n        outputs = torch.zeros(1)\n        l1 = self.make_mock_loss(return_value=2)\n        l2 = self.make_mock_loss(return_value=3)\n        loss = WeightedSumLoss()\n        loss.add_loss('1', l1, 0.3)\n        loss.add_loss('2', l2, 0.7)\n\n        loss_val = loss(outputs)\n\n        assert np.isclose(0.3 * 2 + 0.7 * 3, loss_val.item())\n\n    def test_normazlize(self):\n        outputs = torch.zeros(1)\n        l1 = self.make_mock_loss(return_value=2)\n        l2 = self.make_mock_loss(return_value=3)\n        loss = WeightedSumLoss(normalize=True)\n        loss.add_loss('1', l1, 2)\n        loss.add_loss('2', l2, 1)\n\n        loss_val = loss(outputs)\n\n        assert np.isclose((2 / 3) * 2 + (1 / 3) * 3, loss_val.item())\n"""
pytorch_toolkit/action_recognition/tests/test_models.py,0,"b""from argparse import Namespace\n\nfrom torch import nn\n\nfrom action_recognition.model import create_model\nfrom action_recognition.models.backbone.mobilenetv2 import InvertedResidual\nfrom action_recognition.models.video_transformer import VideoTransformer\n\n\ndef _make_args(**kwargs):\n    kwargs.setdefault('fp16', False)\n    kwargs.setdefault('cuda', False)\n    kwargs.setdefault('hidden_size', 512)\n    kwargs.setdefault('sample_duration', 16)\n    kwargs.setdefault('n_classes', 10)\n    kwargs.setdefault('sample_size', 224)\n    kwargs.setdefault('pretrain_path', False)\n    kwargs.setdefault('layer_norm', False)\n    kwargs.setdefault('resume_path', None)\n    return Namespace(**kwargs)\n\n\nclass TestCreateModel:\n    def test_create_resnet34_vtn(self):\n        args = _make_args()\n        model, _ = create_model(args, 'resnet34_vtn')\n        num_convs = len([l for l in model.resnet.modules() if isinstance(l, nn.Conv2d)])\n\n        assert isinstance(model, VideoTransformer)\n        assert 36 == num_convs\n\n    def test_create_mobilenetv2_vtn(self):\n        args = _make_args()\n        model, _ = create_model(args, 'mobilenetv2_vtn')\n        num_convs = len([l for l in model.resnet.modules() if isinstance(l, nn.Conv2d)])\n        num_mobnet_blocks = len([l for l in model.resnet.modules() if isinstance(l, InvertedResidual)])\n\n        assert isinstance(model, VideoTransformer)\n        assert 52 == num_convs\n        assert 17 == num_mobnet_blocks\n\n    def test_select_encoder_from_args(self):\n        args = _make_args(encoder='mobilenetv2')\n        model, _ = create_model(args, 'vtn')\n        num_convs = len([l for l in model.resnet.modules() if isinstance(l, nn.Conv2d)])\n        num_mobnet_blocks = len([l for l in model.resnet.modules() if isinstance(l, InvertedResidual)])\n\n        assert isinstance(model, VideoTransformer)\n        assert 52 == num_convs\n        assert 17 == num_mobnet_blocks\n"""
pytorch_toolkit/action_recognition/tests/test_options.py,0,"b'from argparse import ArgumentParser, Namespace\n\nfrom action_recognition.options import get_argument_parser, BoolFlagAction\nfrom action_recognition.utils import generate_args\n\n\nclass TestBoolFlag:\n    def test_default_value(self):\n        argparse = ArgumentParser()\n        argparse.add_argument(""--opt"", action=BoolFlagAction)\n        args = argparse.parse_args([])\n        assert args.opt is False\n\n        argparse = ArgumentParser()\n        argparse.add_argument(""--opt"", default=True, action=BoolFlagAction)\n        args = argparse.parse_args([])\n        assert args.opt is True\n\n    def test_no_prefix(self):\n        argparse = ArgumentParser()\n        argparse.add_argument(""--opt"", action=BoolFlagAction)\n\n        args = argparse.parse_args([\'--opt\'])\n        assert args.opt is True\n        args = argparse.parse_args([\'--no-opt\'])\n        assert args.opt is False\n\n        argparse = ArgumentParser()\n        argparse.add_argument(""--opt"", default=True, action=BoolFlagAction)\n        args = argparse.parse_args([\'--opt\'])\n        assert args.opt is True\n        args = argparse.parse_args([\'--no-opt\'])\n        assert args.opt is False\n\n\nclass TestGenerateArgs:\n    def test_returns_namespace(self):\n        args, _ = generate_args()\n\n        assert isinstance(args, Namespace)\n\n    def test_kwargs_is_setting_args(self):\n        args, _ = generate_args(encoder=\'test\')\n\n        assert args.encoder == \'test\'\n\n    def test_bool_flags(self):\n        args, _ = generate_args(\'--no-val\')\n\n        assert args.val is False\n'"
pytorch_toolkit/action_recognition/tests/test_video_reader.py,0,"b'from action_recognition.video_reader import make_video_reader, ImageDirReader, opencv_read_image, pil_read_image, \\\n    accimage_read_image, VideoFileReader\nimport numpy as np\n\n\nclass TestMakeVideoReader:\n    def test_returns_frame_reader_by_default(self):\n        video_reader = make_video_reader()\n        assert isinstance(video_reader, ImageDirReader)\n\n    def test_returns_opencv_by_default(self):\n        video_reader = make_video_reader()\n        assert video_reader.read_image_fn is opencv_read_image\n\n\nclass TestImageDirReader:\n    def test_reads_image(self, mocker):\n        mocker.patch(\'os.path.exists\')\n        read_img_mock = mocker.Mock()\n        img_dir_reader = ImageDirReader(read_img_mock, ""/foo/image_%05d.jpg"")\n\n        ret = img_dir_reader.read(\'/foo/\', [1, 2, 3])\n\n        assert read_img_mock.call_count == 3\n        assert len(ret) == 3\n        read_img_mock.assert_has_calls([\n            mocker.call(\'/foo/image_00001.jpg\'),\n            mocker.call(\'/foo/image_00002.jpg\'),\n            mocker.call(\'/foo/image_00003.jpg\'),\n        ])\n\n\nclass _MockVideoCapture:\n    def __init__(self, *args, **kwargs):\n        self._i = 0\n        self._num_reads = 10\n\n    def read(self):\n        if self._i < self._num_reads:\n            self._i += 1\n            return True, self._i\n        return False, None\n\n    def release(self):\n        pass\n\n\nclass TestVideoFileReader:\n    def test_reads_frames(self, mocker):\n        mocker.patch(\'cv2.VideoCapture\', _MockVideoCapture)\n        mocker.patch(\'cv2.cvtColor\', lambda x, _: x)\n        video_reader = VideoFileReader()\n\n        frames = video_reader.read(\'/path\', [1, 2, 3])\n\n        assert [1, 2, 3] == frames\n\n    def test_skips_frames(self, mocker):\n        mocker.patch(\'cv2.VideoCapture\', _MockVideoCapture)\n        mocker.patch(\'cv2.cvtColor\', lambda x, _: x)\n        video_reader = VideoFileReader()\n\n        frames = video_reader.read(\'/path\', [2, 4, 6])\n\n        assert [2, 4, 6] == frames\n\n\ndef test_opencv_reader_returns_rgb(data_path):\n    img = opencv_read_image(str(data_path / \'rgb_test.png\'))\n    assert np.alltrue(img[:4, :4] == (255, 0, 0))\n    assert np.alltrue(img[4:8, 4:8] == (0, 255, 0))\n    assert np.alltrue(img[8:, 8:] == (0, 0, 255))\n\n\ndef test_pil_reader_returns_rgb(data_path):\n    img = pil_read_image(str(data_path / \'rgb_test.png\'))\n    img = np.asarray(img)\n    assert np.alltrue(img[:4, :4] == (255, 0, 0))\n    assert np.alltrue(img[4:8, 4:8] == (0, 255, 0))\n    assert np.alltrue(img[8:, 8:] == (0, 0, 255))\n'"
pytorch_toolkit/action_recognition/utils/hmdb51_json.py,0,"b""from __future__ import division, print_function\n\nimport json\nimport os\nimport sys\n\nimport pandas as pd\n\n\ndef convert_csv_to_dict(csv_dir_path, split_index):\n    database = {}\n    for filename in os.listdir(csv_dir_path):\n        if 'split{}'.format(split_index) not in filename:\n            continue\n\n        data = pd.read_csv(os.path.join(csv_dir_path, filename),\n                           delimiter=' ', header=None)\n        keys = []\n        subsets = []\n        for i in range(data.shape[0]):\n            row = data.ix[i, :]\n            if row[1] == 0:\n                continue\n            elif row[1] == 1:\n                subset = 'training'\n            elif row[1] == 2:\n                subset = 'validation'\n\n            keys.append(row[0].split('.')[0])\n            subsets.append(subset)\n\n        for i in range(len(keys)):\n            key = keys[i]\n            database[key] = {}\n            database[key]['subset'] = subsets[i]\n            label = '_'.join(filename.split('_')[:-2])\n            database[key]['annotations'] = {'label': label}\n\n    return database\n\n\ndef get_labels(csv_dir_path):\n    labels = []\n    for name in os.listdir(csv_dir_path):\n        labels.append('_'.join(name.split('_')[:-2]))\n    return sorted(list(set(labels)))\n\n\ndef convert_hmdb51_csv_to_activitynet_json(csv_dir_path, split_index, dst_json_path):\n    labels = get_labels(csv_dir_path)\n    database = convert_csv_to_dict(csv_dir_path, split_index)\n\n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(database)\n\n    with open(dst_json_path, 'w') as dst_file:\n        json.dump(dst_data, dst_file)\n\n\nif __name__ == '__main__':\n    csv_dir_path = sys.argv[1]\n\n    for split_index in range(1, 4):\n        dst_json_path = os.path.join(csv_dir_path, 'hmdb51_{}.json'.format(split_index))\n        convert_hmdb51_csv_to_activitynet_json(csv_dir_path, split_index, dst_json_path)\n"""
pytorch_toolkit/action_recognition/utils/kinetics_json.py,0,"b'from __future__ import division, print_function\n\nimport json\nimport os\nimport sys\n\nimport pandas as pd\n\n\ndef convert_csv_to_dict(csv_path, subset):\n    data = pd.read_csv(csv_path)\n    keys = []\n    key_labels = []\n    for i in range(data.shape[0]):\n        row = data.ix[i, :]\n        basename = \'%s_%s_%s\' % (row[\'youtube_id\'],\n                                 \'%06d\' % row[\'time_start\'],\n                                 \'%06d\' % row[\'time_end\'])\n        keys.append(basename)\n        if subset != \'testing\':\n            key_labels.append(row[\'label\'])\n\n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]\n        database[key] = {}\n        database[key][\'subset\'] = subset\n        if subset != \'testing\':\n            label = key_labels[i]\n            database[key][\'annotations\'] = {\'label\': label}\n        else:\n            database[key][\'annotations\'] = {}\n\n    return database\n\n\ndef load_labels(train_csv_path):\n    data = pd.read_csv(train_csv_path)\n    return data[\'label\'].unique().tolist()\n\n\ndef convert_kinetics_csv_to_activitynet_json(train_csv_path, val_csv_path, test_csv_path, dst_json_path):\n    labels = load_labels(train_csv_path)\n    train_database = convert_csv_to_dict(train_csv_path, \'training\')\n    val_database = convert_csv_to_dict(val_csv_path, \'validation\')\n    test_database = convert_csv_to_dict(test_csv_path, \'testing\')\n\n    dst_data = {}\n    dst_data[\'labels\'] = labels\n    dst_data[\'database\'] = {}\n    dst_data[\'database\'].update(train_database)\n    dst_data[\'database\'].update(val_database)\n    dst_data[\'database\'].update(test_database)\n\n    with open(dst_json_path, \'w\') as dst_file:\n        json.dump(dst_data, dst_file)\n\n\nif __name__ == ""__main__"":\n    train_csv_path = sys.argv[1]\n    val_csv_path = sys.argv[2]\n    test_csv_path = sys.argv[3]\n    dst_json_path = sys.argv[4]\n\n    convert_kinetics_csv_to_activitynet_json(\n        train_csv_path, val_csv_path, test_csv_path, dst_json_path)\n'"
pytorch_toolkit/action_recognition/utils/preprocess_videos.py,0,"b'from __future__ import division, print_function\n\nimport json\nimport os\nimport subprocess\nfrom argparse import ArgumentParser\nfrom functools import partial\nfrom multiprocessing import Pool\nfrom pathlib import Path\n\nimport cv2\nfrom tqdm import tqdm\n\nFFMPEG_CMD = ""ffmpeg -i \\""{input!s}\\"" -q:v {q} {filters} -an -threads 1 -y -loglevel panic \\""{output!s}\\""""\nSCALE_OPT = ""-vf \\""scale=trunc(((iw*{scale})/\'min(iw,ih)\')/2)*2:trunc(((ih*{scale})/\'min(iw,ih)\')/2)*2\\""""\n\n\ndef get_video_properties(video_path):\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        # video file exists but contains invalid video stream\n        return None, None\n\n    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    return n_frames, fps\n\n\ndef convert_video(video, output, scale, quality):\n    if scale:\n        filters_opt = SCALE_OPT.format(scale=scale)\n    else:\n        filters_opt = """"\n\n    cmd_format = FFMPEG_CMD.format(input=video, output=output, filters=filters_opt, q=quality)\n    subprocess.call(cmd_format, shell=True)\n\n\ndef callback(data, orig_path, destination_path, scale=None, quality=1, video_format=\'frames\', convert=True):\n    video_name, annotation = data\n\n    if annotation[\'subset\'] == \'testing\':\n        label = \'test\'\n    else:\n        label = annotation[\'annotations\'][\'label\']\n\n    class_path = os.path.join(destination_path, label)\n    Path(class_path).mkdir(exist_ok=True)\n\n    for ext in (\'.mp4\', \'.avi\'):\n        orig_video_path = os.path.join(orig_path, label, video_name) + ext\n        if os.path.exists(orig_video_path):\n            break\n    else:\n        return None\n\n    base_video_name = os.path.join(destination_path, label, video_name)\n    if video_format == \'video\':\n        destination_video_path = base_video_name + "".mp4""\n    else:  # video_format == \'frames\'\n        Path(base_video_name).mkdir(exist_ok=True)\n        destination_video_path = os.path.join(base_video_name, ""image_%05d.jpg"")\n\n    if convert:\n        convert_video(orig_video_path, destination_video_path, scale=scale, quality=quality)\n    else:\n        destination_video_path = orig_video_path\n\n    n_frames, fps = get_video_properties(destination_video_path)\n    if not n_frames:\n        return None\n    annotation[\'fps\'] = fps\n    annotation[\'n_frames\'] = n_frames\n    return video_name, annotation\n\n\ndef convert_videos(annotation_path, raw_path, destination_path, video_size, video_format=\'frames\',\n                   video_quality=1, subset=None, convert=True, n_jobs=8):\n    with open(annotation_path) as f:\n        annotation = json.load(f)\n\n    if subset is None:\n        data = annotation[\'database\']\n    else:\n        data = {k: v for k, v in annotation[\'database\'].items() if v.get(\'subset\') == subset}\n\n    updated_videos = []\n    # Pre-process video files\n    with Pool(processes=n_jobs) as p:\n        cb = partial(callback, orig_path=raw_path, destination_path=destination_path, scale=video_size,\n                     video_format=video_format, quality=video_quality, convert=convert)\n        for updated in tqdm(p.imap_unordered(cb, data.items()), total=len(data)):\n            if updated:\n                updated_videos.append(updated)\n\n    # update annotation file (add n_frames and fps)\n    annotation[\'database\'].update(updated_videos)\n    with open(os.path.join(destination_path, os.path.basename(annotation_path)), \'w\') as f:\n        json.dump(annotation, f)\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser(""Pre-process videos and update annotation file"")\n    parser.add_argument(""-a"", ""--annotation_file"",\n                        help=""Annotation file in dataset format"")\n    parser.add_argument(""-r"", ""--raw_dir"", help=""Directory where raw videos are stored"")\n    parser.add_argument(""-d"", ""--destination_dir"", help=""Directory where converted videos should be saved"")\n    parser.add_argument(""--video-size"", type=int, help=""Size of the smallest size of converted video"")\n    parser.add_argument(""--video-format"", default=\'video\', choices=[\'frames\', \'video\'],\n                        help=\'In what format preprocessed videos should be stored\')\n    parser.add_argument(""-q"", ""--video-quality"", type=int,\n                        help=""Quality of output videos. Lower values will likely correspond to a better accuracy, ""\n                             ""but will require more disk space. Optimal value is 1 for videos and 4 for frames"")\n    parser.add_argument(""--subset"", type=str, help=""Dataset subset"")\n    parser.add_argument(""--no-convert"", action=""store_true"", help=""Do not convert videos, only update annotation file"")\n    parser.add_argument(""-j"", ""--threads"", default=8, type=int, help=""Number of video conversion threads"")\n    args = parser.parse_args()\n\n    if not args.video_quality:\n        args.video_quality = 1 if args.video_format == \'video\' else 4\n\n    if args.no_convert:\n        args.destination_dir = args.raw_dir\n\n    if not os.path.exists(args.destination_dir):\n        os.mkdir(args.destination_dir)\n\n    convert_videos(args.annotation_file, args.raw_dir, args.destination_dir, video_size=args.video_size,\n                   n_jobs=args.threads, subset=args.subset, video_format=args.video_format,\n                   video_quality=args.video_quality, convert=(not args.no_convert))\n'"
pytorch_toolkit/action_recognition/utils/ucf101_json.py,0,"b""from __future__ import division, print_function\n\nimport json\nimport os\nimport sys\n\nimport pandas as pd\n\n\ndef convert_csv_to_dict(csv_path, subset):\n    data = pd.read_csv(csv_path, delimiter=' ', header=None)\n    keys = []\n    key_labels = []\n    for i in range(data.shape[0]):\n        row = data.ix[i, :]\n        slash_rows = data.ix[i, 0].split('/')\n        class_name = slash_rows[0]\n        basename = slash_rows[1].split('.')[0]\n\n        keys.append(basename)\n        key_labels.append(class_name)\n\n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]\n        database[key] = {}\n        database[key]['subset'] = subset\n        label = key_labels[i]\n        database[key]['annotations'] = {'label': label}\n\n    return database\n\n\ndef load_labels(label_csv_path):\n    data = pd.read_csv(label_csv_path, delimiter=' ', header=None)\n    labels = []\n    for i in range(data.shape[0]):\n        labels.append(data.ix[i, 1])\n    return labels\n\n\ndef convert_ucf101_csv_to_activitynet_json(label_csv_path, train_csv_path,\n                                           val_csv_path, dst_json_path):\n    labels = load_labels(label_csv_path)\n    train_database = convert_csv_to_dict(train_csv_path, 'training')\n    val_database = convert_csv_to_dict(val_csv_path, 'validation')\n\n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(train_database)\n    dst_data['database'].update(val_database)\n\n    with open(dst_json_path, 'w') as dst_file:\n        json.dump(dst_data, dst_file)\n\n\nif __name__ == '__main__':\n    csv_dir_path = sys.argv[1]\n\n    for split_index in range(1, 4):\n        label_csv_path = os.path.join(csv_dir_path, 'classInd.txt')\n        train_csv_path = os.path.join(csv_dir_path, 'trainlist0{}.txt'.format(split_index))\n        val_csv_path = os.path.join(csv_dir_path, 'testlist0{}.txt'.format(split_index))\n        dst_json_path = os.path.join(csv_dir_path, 'ucf101_0{}.json'.format(split_index))\n\n        convert_ucf101_csv_to_activitynet_json(label_csv_path, train_csv_path,\n                                               val_csv_path, dst_json_path)\n"""
pytorch_toolkit/asl_recognition/configs/s3d_rgb_mobilenetv3_stream.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n# global parameters\nnum_test_classes = 100\nnum_train_classes = 1000\n\nsubdir_name = \'msasl\'\nframes_type = \'global_crops\'\nroot_dir = \'data\'\nmixup_images_file = \'imagenet_train_list.txt\'\nmixup_images_root = \'imagenet/train\'\ndata_root_dir = \'{}/{}\'.format(root_dir, subdir_name)\nwork_dir = None\nload_from = None\nresume_from = None\n\n# model settings\nmodel_partial_init = False\nmodel = dict(\n    type=\'ASLNet3D\',\n    backbone=dict(\n        type=\'MobileNetV3_S3D\',\n        num_input_layers=3,\n        mode=\'large\',\n        pretrained=None,\n        pretrained2d=False,\n        width_mult=1.0,\n        pool1_stride_t=1,\n        # block ids:      0  1  2  3  4  5  6  7  8  9  10 11 12 13 14\n        temporal_strides=(1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1),\n        temporal_kernels=(5, 3, 3, 3, 3, 5, 5, 3, 3, 5, 3, 3, 3, 3, 3),\n        use_st_att=      (0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0),\n        use_temporal_avg_pool=True,\n        input_bn=False,\n        out_conv=True,\n        out_attention=False,\n        weight_norm=\'none\',\n        dropout_cfg=dict(\n            p=0.1,\n            mu=0.1,\n            sigma=0.03,\n            dist=\'gaussian\'\n        ),\n    ),\n    spatial_temporal_module=dict(\n        type=\'AggregatorSpatialTemporalModule\',\n        modules=[\n            dict(type=\'AverageSpatialTemporalModule\',\n                 temporal_size=4,\n                 spatial_size=7),\n        ],\n    ),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_size=1,\n        spatial_size=1,\n        dropout_ratio=None,\n        in_channels=960,\n        num_classes=num_train_classes,\n        embedding=True,\n        embd_size=256,\n        num_centers=1,\n        st_scale=10.0,\n        reg_weight=1.0,\n        reg_threshold=0.1,\n        angle_std=None,\n        class_counts=None,\n        main_loss_cfg=dict(\n            type=\'AMSoftmaxLoss\',\n            scale_cfg=dict(\n                type=\'PolyScheduler\',\n                start_scale=30.0,\n                end_scale=5.0,\n                power=1.2,\n                num_epochs=41.276,\n            ),\n            pr_product=True,\n            margin_type=\'cos\',\n            margin=0.35,\n            gamma=0.0,\n            t=1.0,\n            conf_penalty_weight=0.085,\n            filter_type=\'positives\',\n            top_k=None,\n        ),\n        extra_losses_cfg=dict(\n            loss_lpush=dict(\n                type=\'LocalPushLoss\',\n                margin=0.1,\n                weight=1.0,\n                smart_margin=True,\n            ),\n        ),\n    ),\n    masked_num=None,\n    grad_reg_weight=None,\n    bn_eval=False,\n)\ntrain_cfg = None\ntest_cfg = None\n\n# dataset settings\ntrain_dataset_type = \'StreamDataset\'\ntest_dataset_type = \'StreamDataset\'\nimages_dir = \'{}/{}\'.format(data_root_dir, frames_type)\nimg_norm_cfg = dict(\n    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n    std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n    to_rgb=True)\n\ndata = dict(\n    videos_per_gpu=14,\n    workers_per_gpu=3,\n    num_test_classes=num_test_classes,\n    train=dict(\n        type=train_dataset_type,\n        ann_file=\'{}/train{}.txt\'.format(data_root_dir, num_train_classes),\n        img_prefix=images_dir,\n        img_norm_cfg=img_norm_cfg,\n        out_length=16,\n        out_fps=15,\n        num_segments=1,\n        temporal_jitter=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=(224, 224),\n        min_intersection=0.6,\n        div_255=False,\n        flip_ratio=0.5,\n        rotate_delta=10.0,\n        resize_keep_ratio=True,\n        random_crop=True,\n        scale_limits=[1.0, 0.875],\n        extra_augm=dict(\n            brightness_range=(65, 190),\n            contrast_range=(0.6, 1.4),\n            saturation_range=(0.7, 1.3),\n            hue_delta=18,\n            noise_sigma=None),\n        dropout_prob=0.1,\n        dropout_scale=0.2,\n        mixup_alpha=0.2,\n        mixup_images_file=\'{}/{}\'.format(root_dir, mixup_images_file),\n        mixup_images_root=\'{}/{}\'.format(root_dir, mixup_images_root),\n        test_mode=False),\n    val=dict(\n        type=test_dataset_type,\n        ann_file=\'{}/val{}.txt\'.format(data_root_dir, num_test_classes),\n        img_prefix=images_dir,\n        img_norm_cfg=img_norm_cfg,\n        out_length=16,\n        out_fps=15,\n        num_segments=1,\n        temporal_jitter=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=(224, 224),\n        div_255=False,\n        flip_ratio=None,\n        rotate_delta=None,\n        resize_keep_ratio=True,\n        random_crop=False,\n        test_mode=True),\n    test=dict(\n        type=test_dataset_type,\n        ann_file=\'{}/test{}.txt\'.format(data_root_dir, num_test_classes),\n        img_prefix=images_dir,\n        img_norm_cfg=img_norm_cfg,\n        out_length=16,\n        out_fps=15,\n        num_segments=1,\n        temporal_jitter=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=(224, 224),\n        div_255=False,\n        flip_ratio=None,\n        rotate_delta=None,\n        resize_keep_ratio=True,\n        random_crop=False,\n        test_mode=True)\n)\n\n# optimizer\noptimizer = dict(type=\'SGD\', lr=1e-2, momentum=0.9, weight_decay=1e-4)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\ncudnn_benchmark = True\n\n# learning policy\nlr_config = dict(\n    warmup=\'linear\',\n    warmup_iters=5,\n    warmup_ratio=1e-2,\n    policy=\'step\',\n    step=[25, 50],\n)\ncheckpoint_config = dict(interval=1)\nworkflow = [(\'train\', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=10,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 80\neval_epoch = 1\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\n'"
pytorch_toolkit/asl_recognition/tools/download_msasl_videos.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport json\nimport subprocess\nfrom os import makedirs, listdir\nfrom os.path import exists, join, isfile\nfrom argparse import ArgumentParser\n\nfrom joblib import delayed, Parallel\nimport cv2\n\n\nclass VideoDownloader(object):\n    def __init__(self, num_jobs, num_attempts=5):\n        self.num_jobs = num_jobs\n        assert self.num_jobs > 0\n\n        self.num_attempts = num_attempts\n        assert self.num_attempts > 0\n\n    @staticmethod\n    def _log(message_tuple):\n        output_filename, status, msg = message_tuple\n        str_template = \'   - {}: {}\' if status else \'   - {}: Error: {}\'\n        print(str_template.format(output_filename, msg))\n\n    @staticmethod\n    def _get_number_of_frames(video_path):\n        # Get number of frame that can be read\n        # cap.get(cv2.CAP_PROP_FRAME_COUNT) can return incorrect value\n        cap = cv2.VideoCapture(video_path)\n        n = 0\n        s = True\n        while True:\n            s, _ = cap.read()\n            if not s:\n                break\n            n = n + 1\n        return n\n\n    def _download_video(self, video_data, output_filename, num_attempts=5):\n        status = False\n\n        command = [\'youtube-dl\',\n                   \'--quiet\', \'--no-warnings\',\n                   \'-f\', \'mp4\',\n                   \'-o\', \'""%s""\' % output_filename,\n                   \'""%s""\' % video_data[\'url\']]\n        command = \' \'.join(command)\n\n        attempts = 0\n        while True:\n            try:\n                _ = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n                num_frame = self._get_number_of_frames(output_filename)\n                if num_frame < video_data[\'end_frame\']:\n                    os.remove(output_filename)\n                    if attempts == num_attempts:\n                        return video_data[\'url\'], status, \'Corrupted file or incorrect annotation\'\n                    attempts += 1\n                    continue\n            except subprocess.CalledProcessError as err:\n                attempts += 1\n                if attempts == num_attempts:\n                    return video_data[\'url\'], status, err.output\n            else:\n                break\n\n        status = exists(output_filename)\n\n        message_tuple = video_data[\'url\'], status, \'Downloaded\'\n        self._log(message_tuple)\n\n        return message_tuple\n\n    def __call__(self, tasks):\n        if len(tasks) == 0:\n            return []\n\n        if self.num_jobs == 1:\n            status_lst = []\n            for data, out_video_path in tasks:\n                status_lst.append(self._download_video(data, out_video_path))\n        else:\n            status_lst = Parallel(n_jobs=self.num_jobs)(\n                delayed(self._download_video)(data, out_video_path)\n                for data, out_video_path in tasks\n            )\n\n        return status_lst\n\n\ndef ensure_dir_exists(dir_path):\n    if not exists(dir_path):\n        makedirs(dir_path)\n\n\ndef get_valid_sources(all_sources):\n    return [s for s in all_sources if exists(s)]\n\n\ndef print_data_sources_stat(data_sources):\n    print(\'Specified {} valid data sources:\'.format(len(data_sources)))\n    for data_source in data_sources:\n        print(\'   - {}\'.format(data_source))\n\n\ndef collect_videos(data_sources):\n    out_videos = dict()\n    for data_source in data_sources:\n        with open(data_source) as input_stream:\n            data = json.load(input_stream)\n\n        for record in data:\n            url = record[\'url\']\n            end_frame = record[\'end\']\n            video_name = url.split(\'?v=\')[-1]\n            if video_name not in out_videos:\n                out_videos[video_name] = {\'url\': url, \'end_frame\': end_frame}\n            else:\n                assert out_videos[video_name][\'url\'] == url\n\n    return out_videos\n\n\ndef prepare_tasks(video_sources, videos_dir, extension):\n    downloaded_videos = [join(videos_dir, f) for f in listdir(videos_dir)\n                         if isfile(join(videos_dir, f)) and f.endswith(extension)]\n\n    out_tasks = []\n    for video_name, video_data in video_sources.items():\n        video_path = join(videos_dir, \'{}.{}\'.format(video_name, extension))\n\n        if video_path not in downloaded_videos:\n            out_tasks.append((video_data, video_path))\n\n    return out_tasks\n\n\ndef print_status(status_lst):\n    if len(status_lst) == 0:\n        return\n\n    print(\'Status:\')\n    for status in status_lst:\n        str_template = \'   - {}: {}\' if status[1] else \'   - {}: Error: {}\'\n        print(str_template.format(status[0], status[2]))\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\'--sources\', \'-s\', nargs=\'+\', type=str, required=True)\n    parser.add_argument(\'--output_dir\', \'-o\', type=str, required=True)\n    parser.add_argument(\'--extension\', \'-e\', type=str, required=False, default=\'mp4\')\n    parser.add_argument(\'--num_jobs\', \'-n\', type=int, required=False, default=24)\n    args = parser.parse_args()\n\n    ensure_dir_exists(args.output_dir)\n\n    data_sources = get_valid_sources(args.sources)\n    print_data_sources_stat(data_sources)\n    assert len(data_sources) > 0\n\n    all_videos = collect_videos(data_sources)\n    print(\'Found {} unique videos.\'.format(len(all_videos)))\n\n    tasks = prepare_tasks(all_videos, args.output_dir, args.extension)\n    print(\'Prepared {} tasks for downloading.\'.format(len(tasks)))\n\n    downloader = VideoDownloader(args.num_jobs)\n    status_lst = downloader(tasks)\n    print_status(status_lst)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/asl_recognition/tools/extract_msasl_frames.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport json\nfrom os import makedirs, listdir\nfrom os.path import exists, join, isfile, basename\nfrom argparse import ArgumentParser\n\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\n\n\ndef ensure_dir_exists(dir_path):\n    if not exists(dir_path):\n        makedirs(dir_path)\n\n\ndef get_valid_sources(all_sources):\n    return [s for s in all_sources if exists(s)]\n\n\ndef print_data_sources_stat(data_sources):\n    print(\'Specified {} valid data sources:\'.format(len(data_sources)))\n    for data_source in data_sources:\n        print(\'   - {}\'.format(data_source))\n\n\ndef load_sign_durations(data_path):\n    with open(data_path) as input_stream:\n        data = json.load(input_stream)\n\n    return {int(k): float(v) for k, v in data.items()}\n\n\ndef get_video_names(videos_dir, extension):\n    return [f.split(\'.{}\'.format(extension))[0]\n            for f in listdir(videos_dir) if isfile(join(videos_dir, f)) and f.endswith(extension)]\n\n\ndef collect_records(data_sources, valid_video_names, mean_sign_duration=0.8, speed_factor=1.1, preparation_time=0.8):\n    num_skipped_records = 0\n    out_records = list()\n    for data_source in data_sources:\n        with open(data_source) as input_stream:\n            data = json.load(input_stream)\n\n        data_type = basename(data_source).split(\'.\')[0].split(\'_\')[-1]\n\n        for record in data:\n            url = record[\'url\']\n            video_name = url.split(\'?v=\')[-1]\n\n            if video_name not in valid_video_names:\n                num_skipped_records += 1\n                continue\n\n            bbox = record[\'box\']\n            start_frame = int(record[\'start\'])\n            end_frame = int(record[\'end\'])\n            assert start_frame >= 0 and end_frame >= 0\n            if end_frame - start_frame <= 1:\n                num_skipped_records += 1\n                continue\n\n            label = record[\'label\']\n            expected_sign_duration = speed_factor * mean_sign_duration * float(record[\'fps\'])\n            expected_clip_duration = expected_sign_duration + preparation_time * float(record[\'fps\'])\n            real_clip_duration = float(end_frame - start_frame)\n\n            if real_clip_duration > expected_clip_duration:\n                left_duration = real_clip_duration - preparation_time * float(record[\'fps\'])\n\n                ratio = left_duration / expected_sign_duration\n                num_repeats = int(np.round(ratio))\n\n                num_segments = num_repeats + 1 if num_repeats > 0 else 2\n                segment_length = real_clip_duration / float(num_segments)\n\n                center_frame = start_frame + segment_length  # get the first most probable position\n                start_frame = int(center_frame - 0.5 * expected_sign_duration)\n                end_frame = int(center_frame + 0.5 * expected_sign_duration)\n                out_limits = [(start_frame, end_frame)]\n            else:\n                center_frame = 0.5 * (start_frame + end_frame)\n                trg_sign_duration = np.minimum(real_clip_duration, expected_sign_duration)\n                start_frame = int(center_frame - 0.5 * trg_sign_duration)\n                end_frame = int(center_frame + 0.5 * trg_sign_duration)\n                out_limits = [(start_frame, end_frame)]\n\n            for fixed_start_frame, fixed_end_frame in out_limits:\n                out_records.append(dict(label=label,\n                                        signer_id=record[\'signer_id\'],\n                                        start=fixed_start_frame,\n                                        end=fixed_end_frame,\n                                        bbox=dict(xmin=bbox[1], ymin=bbox[0], xmax=bbox[3], ymax=bbox[2]),\n                                        video_name=video_name,\n                                        fps=float(record[\'fps\']),\n                                        type=data_type))\n\n    if num_skipped_records > 0:\n        print(\'Warning. Skipped {} records.\'.format(num_skipped_records))\n    else:\n        print(\'All records are parsed successfully.\')\n\n    return out_records\n\n\ndef order_by_video_name(records):\n    out_records = dict()\n    for record in records:\n        video_name = record[\'video_name\']\n        if video_name not in out_records:\n            out_records[video_name] = []\n\n        out_records[video_name].append(record)\n\n    return out_records\n\n\ndef validate_and_sort_records(records):\n    out_records = dict()\n    for video_name in records:\n        video_records = records[video_name]\n        video_records.sort(key=lambda r: r[\'start\'])\n\n        out_video_records = list()\n        for record_id in range(len(video_records) - 1):\n            cur_record = video_records[record_id]\n            next_record = video_records[record_id + 1]\n\n            if cur_record[\'end\'] > next_record[\'start\']:\n                cur_record[\'end\'] = next_record[\'start\']\n\n            if cur_record[\'start\'] < cur_record[\'end\']:\n                out_video_records.append(cur_record)\n\n        if len(video_records) > 0:\n            out_video_records.append(video_records[-1])\n\n        out_records[video_name] = out_video_records\n\n    return out_records\n\n\ndef print_records_stat(records):\n    num_records = np.sum([len(video_records) for video_records in records.values()])\n    clip_lengths = [record[\'end\'] - record[\'start\'] for video_records in records.values() for record in video_records]\n\n    print(\'Stat for {} records:\'.format(num_records))\n    print(\'   - min: {} max: {}\'.format(np.min(clip_lengths),\n                                        np.max(clip_lengths)))\n    print(\'   - p@5: {} p@50: {} p@95: {}\'.format(np.percentile(clip_lengths, 5.0),\n                                                  np.percentile(clip_lengths, 50.0),\n                                                  np.percentile(clip_lengths, 95.0)))\n\n\ndef crop_image(image, bbox, trg_size, scale):\n    def _fix_bbox():\n        frame_height, frame_width = image.shape[:2]\n        x_min, y_min, x_max, y_max = bbox[\'xmin\'], bbox[\'ymin\'], bbox[\'xmax\'], bbox[\'ymax\']\n\n        center_x = 0.5 * (x_min + x_max) * frame_width\n        center_y = 0.5 * (y_min + y_max) * frame_height\n\n        scaled_width = scale * (x_max - x_min) * frame_width\n        scaled_height = scale * (y_max - y_min) * frame_height\n\n        src_ar = float(scaled_height) / float(scaled_width)\n        trg_ar = float(trg_size[0]) / float(trg_size[1])\n\n        if src_ar < trg_ar:\n            out_width = scaled_width\n            out_height = trg_ar * out_width\n        else:\n            out_height = scaled_height\n            out_width = out_height / trg_ar\n\n        out_x_min = np.maximum(0, int(center_x - 0.5 * out_width))\n        out_y_min = np.maximum(0, int(center_y - 0.5 * out_height))\n        out_x_max = np.minimum(frame_width, int(center_x + 0.5 * out_width))\n        out_y_max = np.minimum(frame_height, int(center_y + 0.5 * out_height))\n\n        return out_x_min, out_y_min, out_x_max, out_y_max\n\n    roi = _fix_bbox()\n\n    cropped_image = image[roi[1]:roi[3], roi[0]:roi[2]]\n    resized_image = cv2.resize(cropped_image, (trg_size[1], trg_size[0]))\n\n    return resized_image\n\n\ndef extract_frames(records, videos_dir, video_name_template, out_dir, image_name_template,\n                   target_num_frames, trg_size, scale, trg_fps=30.):\n    pbar = tqdm(total=len(records), desc=\'Dumping\')\n\n    video_names = list(records)\n    for video_name in video_names:\n        video_records = records[video_name]\n        video_path = join(videos_dir, video_name_template.format(video_name))\n        video_capture = cv2.VideoCapture(video_path)\n        num_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        frame_ids_map = dict()\n        for record_id in range(len(video_records)):\n            record = video_records[record_id]\n\n            left_limit = video_records[record_id - 1][\'end\'] if record_id > 0 else 0\n            right_limit = video_records[record_id + 1][\'start\'] if record_id + 1 < len(video_records) else num_frames\n\n            fps_factor = trg_fps / record[\'fps\']\n            time_delta = np.maximum(int(target_num_frames / fps_factor), record[\'end\'] - record[\'start\'])\n            record[\'video_start\'] = np.maximum(left_limit, record[\'end\'] - time_delta)\n            record[\'video_end\'] = np.minimum(right_limit, record[\'start\'] + time_delta)\n            for i in range(record[\'video_start\'], record[\'video_end\']):\n                if i not in frame_ids_map:\n                    frame_ids_map[i] = []\n                frame_ids_map[i].append(record_id)\n\n            record[\'rel_images_dir\'] = join(video_name, \'clip_{:04}\'.format(record_id))\n            abs_images_dir = join(out_dir, record[\'rel_images_dir\'])\n            ensure_dir_exists(abs_images_dir)\n\n        success = True\n        read_frame_id = -1\n        while success:\n            success, frame = video_capture.read()\n            read_frame_id += 1\n\n            if not success or read_frame_id not in frame_ids_map:\n                continue\n\n            for record_id in frame_ids_map[read_frame_id]:\n                record = video_records[record_id]\n\n                cropped_frame = crop_image(frame, record[\'bbox\'], trg_size, scale)\n\n                images_dir = join(out_dir, record[\'rel_images_dir\'])\n                out_image_path = join(images_dir, image_name_template.format(read_frame_id - record[\'video_start\'] + 1))\n                cv2.imwrite(out_image_path, cropped_frame)\n\n        video_capture.release()\n        pbar.update(1)\n\n    pbar.close()\n\n    return records\n\n\ndef split_data(records):\n    out_data = dict()\n    for video_records in records.values():\n        for record in video_records:\n            record_type = record[\'type\']\n            if record_type not in out_data:\n                out_data[record_type] = []\n\n            out_data[record_type].append(record)\n\n    return out_data\n\n\ndef dump_paths(data, out_dir):\n    for data_type in data:\n        records = data[data_type]\n\n        out_path = join(out_dir, \'{}.txt\'.format(data_type))\n        with open(out_path, \'w\') as out_stream:\n            record_ids = list(range(len(records)))\n            if data_type == \'train\':\n                np.random.shuffle(record_ids)\n\n            for record_id in record_ids:\n                record = records[record_id]\n\n                converted_record = (\n                    record[\'rel_images_dir\'],\n                    str(record[\'label\']),\n                    str(record[\'start\'] - record[\'video_start\']),\n                    str(record[\'end\'] - record[\'video_start\']),\n                    str(0),\n                    str(record[\'video_end\'] - record[\'video_start\']),\n                    str(record[\'fps\'])\n                )\n                out_stream.write(\'{}\\n\'.format(\' \'.join(converted_record)))\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\'--sources\', \'-s\', nargs=\'+\', type=str, required=True)\n    parser.add_argument(\'--videos_dir\', \'-v\', type=str, required=True)\n    parser.add_argument(\'--output_dir\', \'-o\', type=str, required=True)\n    parser.add_argument(\'--video_extension\', \'-ve\', type=str, required=False, default=\'mp4\')\n    parser.add_argument(\'--image_extension\', \'-ie\', type=str, required=False, default=\'jpg\')\n    parser.add_argument(\'--target_num_frames\', \'-l\', type=int, required=False, default=64)\n    parser.add_argument(\'--trg_image_size\', type=str, required=False, default=\'300,256\')\n    parser.add_argument(\'--scale\', type=float, required=False, default=1.2)\n    args = parser.parse_args()\n\n    assert exists(args.videos_dir)\n\n    images_out_dir = join(args.output_dir, \'global_crops\')\n    ensure_dir_exists(images_out_dir)\n\n    data_sources = get_valid_sources(args.sources)\n    print_data_sources_stat(data_sources)\n    assert len(data_sources) > 0\n\n    available_video_names = get_video_names(args.videos_dir, args.video_extension)\n    records = order_by_video_name(collect_records(data_sources, available_video_names))\n    valid_records = validate_and_sort_records(records)\n    print_records_stat(valid_records)\n\n    trg_size = [int(v) for v in args.trg_image_size.split(\',\')]\n    video_name_template = \'{}\' + \'.{}\'.format(args.video_extension)\n    image_name_template = \'img_{:05}\' + \'.{}\'.format(args.image_extension)\n    extended_records = extract_frames(valid_records, args.videos_dir, video_name_template,\n                                      images_out_dir, image_name_template, args.target_num_frames,\n                                      trg_size, args.scale)\n\n    data_splits = split_data(extended_records)\n    dump_paths(data_splits, args.output_dir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/asl_recognition/tools/get_imagenet_paths.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom os import listdir\nfrom os.path import exists, join, isfile, isdir\nfrom argparse import ArgumentParser\n\nfrom tqdm import tqdm\n\n\nVALID_EXTENSIONS = [\'jpg\', \'jpeg\', \'png\']\n\n\ndef parse_image_paths(root_dir):\n    image_paths = []\n    for class_dir in tqdm(listdir(root_dir)):\n        class_dir_path = join(root_dir, class_dir)\n        if isdir(class_dir_path):\n            for file_name in listdir(class_dir_path):\n                file_path = join(class_dir_path, file_name)\n                file_extension = file_name.split(\'.\')[-1].lower()\n                if isfile(file_path) and file_extension in VALID_EXTENSIONS:\n                    image_paths.append(join(class_dir, file_name))\n\n    return image_paths\n\n\ndef dump_image_paths(paths, out_file_path):\n    with open(out_file_path, \'w\') as output_stream:\n        for file_path in paths:\n            output_stream.write(\'{}\\n\'.format(file_path))\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\'root_dir\', type=str)\n    parser.add_argument(\'output_file\', type=str)\n    args = parser.parse_args()\n\n    assert exists(args.root_dir)\n\n    image_paths = parse_image_paths(args.root_dir)\n    print(\'Found {} images\'.format(len(image_paths)))\n\n    dump_image_paths(image_paths, args.output_file)\n    print(\'Stored at: {}\'.format(args.output_file))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/asl_recognition/tools/split_msasl_annotation.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom os.path import exists, abspath\nfrom argparse import ArgumentParser\n\n\nclass RawFramesSegmentedRecord(object):\n    def __init__(self, row):\n        self._data = row\n\n        assert self.label >= 0\n\n    @property\n    def label(self):\n        return int(self._data[1])\n\n    @property\n    def data(self):\n        return self._data\n\n\ndef load_records(ann_file):\n    return [RawFramesSegmentedRecord(x.strip().split(\' \')) for x in open(ann_file)]\n\n\ndef filter_records(records, k):\n    return [record for record in records if record.label < k]\n\n\ndef dump_records(records, out_file_path):\n    with open(out_file_path, \'w\') as out_stream:\n        for record in records:\n            out_stream.write(\'{}\\n\'.format(\' \'.join(record.data)))\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\'--annot\', \'-a\', nargs=\'+\', type=str, required=True)\n    parser.add_argument(\'--topk\', \'-k\', nargs=\'+\', type=int, required=True)\n    args = parser.parse_args()\n\n    records = dict()\n    for annot_path in args.annot:\n        assert exists(annot_path)\n\n        records[annot_path] = load_records(annot_path)\n\n    for k in args.topk:\n        for annot_path in records:\n            filtered_records = filter_records(records[annot_path], k)\n\n            out_path = \'{}{}.txt\'.format(annot_path[:-len(\'.txt\')], k)\n            dump_records(filtered_records, out_path)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/datasets/__init__.py,0,"b'from .lfw import LFW\nfrom .vggface2 import VGGFace2\nfrom .ms_celeb1m import MSCeleb1M\nfrom .trillion_pairs import TrillionPairs\nfrom .imdbface import IMDBFace\n\nfrom .celeba import CelebA\nfrom .ndg import NDG\n\n__all__ = [LFW, VGGFace2, MSCeleb1M, TrillionPairs, IMDBFace, CelebA, NDG]\n'"
pytorch_toolkit/face_recognition/datasets/casia.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\n\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nimport cv2 as cv\n\nfrom utils.face_align import FivePointsAligner\n\nclass CASIA(Dataset):\n    """"""CASIA Dataset compatible with PyTorch DataLoader.""""""\n    def __init__(self, images_root_path, image_list_path, transform, use_landmarks=True):\n        self.image_list_path = image_list_path\n        self.images_root_path = images_root_path\n        self.identities = {}\n        self.use_landmarks = use_landmarks\n        self.samples_info = self._read_samples_info()\n        self.transform = transform\n\n    def _read_samples_info(self):\n        """"""Reads annotation of the dataset""""""\n        samples = []\n        with open(self.image_list_path, \'r\') as f:\n            for line in tqdm(f.readlines(), \'Preparing CASIA dataset\'):\n                sample = line.split()\n                sample_id = sample[1]\n                landmarks = [[sample[i], sample[i+1]] for i in range(2, 12, 2)]\n                self.identities[sample_id] = [1]\n                samples.append((osp.join(self.images_root_path, sample[0]), sample_id, landmarks))\n\n        return samples\n\n    def get_num_classes(self):\n        """"""Returns total number of identities""""""\n        return len(self.identities)\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.samples_info)\n\n    def __getitem__(self, idx):\n        img = cv.imread(self.samples_info[idx][0])\n        if self.use_landmarks:\n            img = FivePointsAligner.align(img, self.samples_info[idx][2],\n                                          d_size=(200, 200), normalized=True, show=False)\n\n        if self.transform:\n            img = self.transform(img)\n        return {\'img\': img, \'label\': int(self.samples_info[idx][1])}\n'"
pytorch_toolkit/face_recognition/datasets/celeba.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nimport cv2 as cv\n\n\nclass CelebA(Dataset):\n    """"""CelebA Dataset compatible with PyTorch DataLoader.""""""\n    def __init__(self, images_root_path, landmarks_folder_path, transform=None, test=False):\n        self.test = test\n        self.have_landmarks = True\n        self.images_root_path = images_root_path\n        bb_file_name = \'list_bbox_celeba.txt\'\n        landmarks_file_name = \'list_landmarks_celeba.txt\'\n        self.detections_file = open(osp.join(landmarks_folder_path, bb_file_name), \'r\')\n        self.landmarks_file = open(osp.join(landmarks_folder_path, landmarks_file_name), \'r\')\n        self.samples_info = self._read_samples_info()\n        self.transform = transform\n\n    def _read_samples_info(self):\n        """"""Reads annotation of the dataset""""""\n        samples = []\n\n        detections_file_lines = self.detections_file.readlines()[2:]\n        landmarks_file_lines = self.landmarks_file.readlines()[2:]\n        assert len(detections_file_lines) == len(landmarks_file_lines)\n\n        if self.test:\n            images_range = range(182638, len(landmarks_file_lines))\n        else:\n            images_range = range(182637)\n\n        for i in tqdm(images_range):\n            line = detections_file_lines[i].strip()\n            img_name = line.split(\' \')[0]\n            img_path = osp.join(self.images_root_path, img_name)\n\n            bbox = list(filter(bool, line.split(\' \')[1:]))\n            bbox = [int(coord) for coord in bbox]\n            if bbox[2] == 0 or bbox[3] == 0:\n                continue\n\n            line_landmarks = landmarks_file_lines[i].strip().split(\' \')[1:]\n            landmarks = list(filter(bool, line_landmarks))\n            landmarks = [float(coord) for coord in landmarks]\n            samples.append((img_path, bbox, landmarks))\n\n        return samples\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.samples_info)\n\n    def __getitem__(self, idx):\n        """"""Returns sample (image, landmarks) by index""""""\n        img = cv.imread(self.samples_info[idx][0], cv.IMREAD_COLOR)\n        bbox = self.samples_info[idx][1]\n        landmarks = self.samples_info[idx][2]\n\n        img = img[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n        landmarks = np.array([(float(landmarks[2*i]-bbox[0]) / bbox[2],\n                               float(landmarks[2*i + 1]-bbox[1])/ bbox[3]) \\\n                               for i in range(len(landmarks)//2)]).reshape(-1)\n        data = {\'img\': img, \'landmarks\': landmarks}\n        if self.transform:\n            data = self.transform(data)\n        return data\n'"
pytorch_toolkit/face_recognition/datasets/imdbface.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\n\nimport cv2 as cv\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\n\nfrom utils.face_align import FivePointsAligner\n\n\nclass IMDBFace(Dataset):\n    """"""IMDBFace Dataset compatible with PyTorch DataLoader.""""""\n    def __init__(self, images_root_path, image_list_path, transform=None):\n        self.image_list_path = image_list_path\n        self.images_root_path = images_root_path\n        self.identities = {}\n\n        assert osp.isfile(image_list_path)\n        self.have_landmarks = True\n\n        self.all_samples_info = self._read_samples_info()\n        self.samples_info = self.all_samples_info\n        self.transform = transform\n\n    def _read_samples_info(self):\n        """"""Reads annotation of the dataset""""""\n        samples = []\n\n        with open(self.image_list_path, \'r\') as f:\n            images_file_lines = f.readlines()\n            last_class_id = -1\n\n            for i in tqdm(range(len(images_file_lines))):\n                line = images_file_lines[i]\n                terms = line.split(\'|\')\n                if len(terms) < 3:\n                    continue # FD has failed on this imsage\n                path, landmarks, _ = terms\n                image_id, _ = path.rsplit(\'/\', 1)\n\n                if image_id in self.identities:\n                    self.identities[image_id].append(len(samples))\n                else:\n                    last_class_id += 1\n                    self.identities[image_id] = [len(samples)]\n\n                landmarks = [float(coord) for coord in landmarks.strip().split(\' \')]\n                assert len(landmarks) == 10\n                samples.append((osp.join(self.images_root_path, path).strip(), last_class_id, image_id, landmarks))\n\n        return samples\n\n    def get_weights(self):\n        """"""Computes weights of the each identity in dataset according to frequency of it\'s occurance""""""\n        weights = [0.]*len(self.all_samples_info)\n        for i, sample in enumerate(self.all_samples_info):\n            weights[i] = float(len(self.all_samples_info)) / len(self.identities[sample[2]])\n        return weights\n\n    def get_num_classes(self):\n        """"""Returns total number of identities""""""\n        return len(self.identities)\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.samples_info)\n\n    def __getitem__(self, idx):\n        """"""Returns sample (image, class id, image id) by index""""""\n        img = cv.imread(self.samples_info[idx][0], cv.IMREAD_COLOR)\n        landmarks = self.samples_info[idx][-1]\n        img = FivePointsAligner.align(img, landmarks, d_size=(200, 200), normalized=True, show=False)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return {\'img\': img, \'label\': self.samples_info[idx][1], \'instance\': self.samples_info[idx][2]}\n'"
pytorch_toolkit/face_recognition/datasets/lfw.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\nimport cv2 as cv\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nfrom utils.face_align import FivePointsAligner\n\n\nclass LFW(Dataset):\n    """"""LFW Dataset compatible with PyTorch DataLoader.""""""\n    def __init__(self, images_root_path, pairs_path, landmark_file_path=\'\', transform=None):\n        self.pairs_path = pairs_path\n        self.images_root_path = images_root_path\n        self.landmark_file_path = landmark_file_path\n        self.use_landmarks = len(self.landmark_file_path) > 0\n        if self.use_landmarks:\n            self.landmarks = self._read_landmarks()\n        self.pairs = self._read_pairs()\n        self.transform = transform\n\n    def _read_landmarks(self):\n        """"""Reads landmarks of the dataset""""""\n        landmarks = {}\n        with open(self.landmark_file_path, \'r\') as f:\n            for line in f.readlines():\n                sp = line.split()\n                key = sp[0][sp[0].rfind(\'/\')+1:]\n                landmarks[key] = [[int(sp[i]), int(sp[i+1])] for i in range(1, 11, 2)]\n\n        return landmarks\n\n    def _read_pairs(self):\n        """"""Reads annotation of the dataset""""""\n        pairs = []\n        with open(self.pairs_path, \'r\') as f:\n            for line in f.readlines()[1:]:  # skip header\n                pair = line.strip().split()\n                pairs.append(pair)\n\n        file_ext = \'jpg\'\n        lfw_dir = self.images_root_path\n        path_list = []\n\n        for pair in pairs:\n            if len(pair) == 3:\n                path0 = osp.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1]) + \'.\' + file_ext)\n                id0 = pair[0]\n                path1 = osp.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[2]) + \'.\' + file_ext)\n                id1 = pair[0]\n                issame = True\n            elif len(pair) == 4:\n                path0 = osp.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1]) + \'.\' + file_ext)\n                id0 = pair[0]\n                path1 = osp.join(lfw_dir, pair[2], pair[2] + \'_\' + \'%04d\' % int(pair[3]) + \'.\' + file_ext)\n                id1 = pair[0]\n                issame = False\n\n            path_list.append((path0, path1, issame, id0, id1))\n\n        return path_list\n\n    def _load_img(self, img_path):\n        """"""Loads an image from dist, then performs face alignment and applies transform""""""\n        img = cv.imread(img_path, cv.IMREAD_COLOR)\n\n        if self.use_landmarks:\n            landmarks = np.array(self.landmarks[img_path[img_path.rfind(\'/\')+1:]]).reshape(-1)\n            img = FivePointsAligner.align(img, landmarks, show=False)\n\n        if self.transform is None:\n            return img\n\n        return self.transform(img)\n\n    def show_item(self, index):\n        """"""Saves a pair with a given index to disk""""""\n        path_1, path_2, _, _, _ = self.pairs[index]\n        img1 = cv.imread(path_1)\n        img2 = cv.imread(path_2)\n        if self.use_landmarks:\n            landmarks1 = np.array(self.landmarks[path_1[path_1.rfind(\'/\')+1:]]).reshape(-1)\n            landmarks2 = np.array(self.landmarks[path_2[path_2.rfind(\'/\')+1:]]).reshape(-1)\n            img1 = FivePointsAligner.align(img1, landmarks1)\n            img2 = FivePointsAligner.align(img2, landmarks2)\n        else:\n            img1 = cv.resize(img1, (400, 400))\n            img2 = cv.resize(img2, (400, 400))\n        cv.imwrite(\'misclassified_{}.jpg\'.format(index), np.hstack([img1, img2]))\n\n    def __getitem__(self, index):\n        """"""Returns a pair of images and similarity flag by index""""""\n        (path_1, path_2, is_same, id0, id1) = self.pairs[index]\n        img1, img2 = self._load_img(path_1), self._load_img(path_2)\n\n        return {\'img1\': img1, \'img2\': img2, \'is_same\': is_same, \'id0\': id0, \'id1\': id1}\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.pairs)\n'"
pytorch_toolkit/face_recognition/datasets/megaface.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport cv2 as cv\n\nfrom utils.face_align import FivePointsAligner\n\n\nclass MegaFace(Dataset):\n    """"""MegaFace Dataset compatible with PyTorch DataLoader.""""""\n    def __init__(self, images_lsit, transform=None):\n        self.samples_info = images_lsit\n        self.transform = transform\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.samples_info)\n\n    def __getitem__(self, idx):\n        """"""Returns sample (image, index)""""""\n        img = None\n        try:\n            img = cv.imread(self.samples_info[idx][\'path\'], cv.IMREAD_COLOR)\n            bbox = self.samples_info[idx][\'bbox\']\n            landmarks = self.samples_info[idx][\'landmarks\']\n\n            if bbox is not None or landmarks is not None:\n                if landmarks is not None:\n                    landmarks = np.array(landmarks).reshape(5, -1)\n                    landmarks[:,0] = landmarks[:,0]*bbox[2] + bbox[0]\n                    landmarks[:,1] = landmarks[:,1]*bbox[3] + bbox[1]\n                    img = FivePointsAligner.align(img, landmarks.reshape(-1), d_size=(bbox[2], bbox[3]),\n                                                  normalized=False, show=False)\n                if bbox is not None and landmarks is None:\n                    img = img[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n        except BaseException:\n            print(\'Corrupted image!\', self.samples_info[idx])\n            img = np.zeros((128, 128, 3), dtype=\'uint8\')\n\n        if self.transform:\n            img = self.transform(img)\n\n        return {\'img\': img, \'idx\': idx}\n'"
pytorch_toolkit/face_recognition/datasets/ms_celeb1m.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\n\nimport cv2 as cv\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\n\nfrom utils.face_align import FivePointsAligner\n\n\nclass MSCeleb1M(Dataset):\n    """"""MSCeleb1M Dataset compatible with PyTorch DataLoader.""""""\n    def __init__(self, images_root_path, image_list_path, transform=None):\n        self.image_list_path = image_list_path\n        self.images_root_path = images_root_path\n        self.identities = {}\n\n        assert osp.isfile(image_list_path)\n        self.have_landmarks = True\n\n        self.all_samples_info = self._read_samples_info()\n        self.samples_info = self.all_samples_info\n        self.transform = transform\n\n    def _read_samples_info(self):\n        """"""Reads annotation of the dataset""""""\n        samples = []\n\n        with open(self.image_list_path, \'r\') as f:\n            images_file_lines = f.readlines()\n            last_class_id = -1\n\n            for i in tqdm(range(len(images_file_lines))):\n                line = images_file_lines[i]\n                terms = line.split(\'|\')\n                if len(terms) < 3:\n                    continue # FD has failed on this imsage\n                path, landmarks, bbox = terms\n                image_id, _ = path.split(\'/\')\n\n                if image_id in self.identities:\n                    self.identities[image_id].append(len(samples))\n                else:\n                    last_class_id += 1\n                    self.identities[image_id] = [len(samples)]\n\n                bbox = [max(int(coord), 0) for coord in bbox.strip().split(\' \')]\n                landmarks = [float(coord) for coord in landmarks.strip().split(\' \')]\n                assert len(bbox) == 4\n                assert len(landmarks) == 10\n                samples.append((osp.join(self.images_root_path, path).strip(),\n                                last_class_id, image_id, bbox, landmarks))\n\n        return samples\n\n    def get_weights(self):\n        """"""Computes weights of the each identity in dataset according to frequency of it\'s occurance""""""\n        weights = [0.]*len(self.all_samples_info)\n        for i, sample in enumerate(self.all_samples_info):\n            weights[i] = float(len(self.all_samples_info)) / len(self.identities[sample[2]])\n        return weights\n\n    def get_num_classes(self):\n        """"""Returns total number of identities""""""\n        return len(self.identities)\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.samples_info)\n\n    def __getitem__(self, idx):\n        """"""Returns sample (image, class id, image id) by index""""""\n        img = cv.imread(self.samples_info[idx][0], cv.IMREAD_COLOR)\n        bbox = self.samples_info[idx][-2]\n        landmarks = self.samples_info[idx][-1]\n\n        img = img[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n        img = FivePointsAligner.align(img, landmarks, d_size=(200, 200), normalized=True, show=False)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return {\'img\': img, \'label\': self.samples_info[idx][1], \'instance\': self.samples_info[idx][2]}\n'"
pytorch_toolkit/face_recognition/datasets/ndg.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\nimport json\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nimport cv2 as cv\n\n\nclass NDG(Dataset):\n    """"""NDG Dataset compatible with PyTorch DataLoader.""""""\n    def __init__(self, images_root_path, annotation_list, transform=None, test=False):\n        self.test = test\n        self.have_landmarks = True\n        self.images_root_path = images_root_path\n        self.landmarks_file = open(annotation_list, \'r\')\n        self.samples_info = self._read_samples_info()\n        self.transform = transform\n\n    def _read_samples_info(self):\n        """"""Reads annotation of the dataset""""""\n        samples = []\n        data = json.load(self.landmarks_file)\n\n        for image_info in tqdm(data):\n            img_name = image_info[\'path\']\n            img_path = osp.join(self.images_root_path, img_name)\n            landmarks = image_info[\'lm\']\n            samples.append((img_path, landmarks))\n\n        return samples\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.samples_info)\n\n    def __getitem__(self, idx):\n        """"""Returns sample (image, landmarks) by index""""""\n        img = cv.imread(self.samples_info[idx][0], cv.IMREAD_COLOR)\n        landmarks = self.samples_info[idx][1]\n        width, height = img.shape[1], img.shape[0]\n        landmarks = np.array([(float(landmarks[i][0]) / width,\n                               float(landmarks[i][1]) / height) for i in range(len(landmarks))]).reshape(-1)\n        data = {\'img\': img, \'landmarks\': landmarks}\n        if self.transform:\n            data = self.transform(data)\n        return data\n'"
pytorch_toolkit/face_recognition/datasets/trillion_pairs.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\n\nimport cv2 as cv\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\n\nfrom utils.face_align import FivePointsAligner\n\n\nclass TrillionPairs(Dataset):\n    """"""TrillionPairs Dataset compatible with PyTorch DataLoader. For details visit http://trillionpairs.deepglint.com/data""""""\n    def __init__(self, images_root_path, image_list_path, test_mode=False, transform=None):\n        self.image_list_path = image_list_path\n        self.images_root_path = images_root_path\n        self.test_mode = test_mode\n        self.identities = {}\n\n        assert osp.isfile(image_list_path)\n        self.have_landmarks = True\n\n        self.all_samples_info = self._read_samples_info()\n        self.samples_info = self.all_samples_info\n        self.transform = transform\n\n    def _read_samples_info(self):\n        """"""Reads annotation of the dataset""""""\n        samples = []\n\n        with open(self.image_list_path, \'r\') as f:\n            images_file_lines = f.readlines()\n\n            for i in tqdm(range(len(images_file_lines))):\n                line = images_file_lines[i].strip()\n                terms = line.split(\' \')\n                path = terms[0]\n                if not self.test_mode:\n                    label = int(terms[1])\n                    landmarks = terms[2:]\n                    if label in self.identities:\n                        self.identities[label].append(len(samples))\n                    else:\n                        self.identities[label] = [len(samples)]\n                else:\n                    label = 0\n                    landmarks = terms[1:]\n\n                landmarks = [float(coord) for coord in landmarks]\n                assert(len(landmarks) == 10)\n                samples.append((osp.join(self.images_root_path, path).strip(),\n                                label, landmarks))\n\n        return samples\n\n    def get_weights(self):\n        """"""Computes weights of the each identity in dataset according to frequency of it\'s occurance""""""\n        weights = [0.]*len(self.all_samples_info)\n        for i, sample in enumerate(self.all_samples_info):\n            weights[i] = float(len(self.all_samples_info)) / len(self.identities[sample[1]])\n        return weights\n\n    def get_num_classes(self):\n        """"""Returns total number of identities""""""\n        return len(self.identities)\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.samples_info)\n\n    def __getitem__(self, idx):\n        """"""Returns sample (image, class id, image id) by index""""""\n        img = cv.imread(self.samples_info[idx][0], cv.IMREAD_COLOR)\n        landmarks = self.samples_info[idx][-1]\n\n        img = FivePointsAligner.align(img, landmarks, d_size=(200, 200), normalized=False, show=False)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return {\'img\': img, \'label\': self.samples_info[idx][1], \'idx\': idx}\n'"
pytorch_toolkit/face_recognition/datasets/vggface2.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\nimport cv2 as cv\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nfrom utils.face_align import FivePointsAligner\n\n\nclass VGGFace2(Dataset):\n    """"""VGGFace2 Dataset compatible with PyTorch DataLoader.""""""\n    def __init__(self, images_root_path, image_list_path, landmarks_folder_path=\'\',\n                 transform=None, landmarks_training=False):\n        self.image_list_path = image_list_path\n        self.images_root_path = images_root_path\n        self.identities = {}\n\n        self.landmarks_file = None\n        self.detections_file = None\n        if osp.isdir(landmarks_folder_path):\n            if \'train\' in image_list_path:\n                bb_file_name = \'loose_landmark_train.csv\'\n                landmarks_file_name = \'loose_bb_train.csv\'\n            elif \'test\' in image_list_path:\n                bb_file_name = \'loose_landmark_test.csv\'\n                landmarks_file_name = \'loose_bb_test.csv\'\n            else:\n                bb_file_name = \'loose_landmark_all.csv\'\n                landmarks_file_name = \'loose_bb_all.csv\'\n            self.landmarks_file = open(osp.join(landmarks_folder_path, bb_file_name), \'r\')\n            self.detections_file = open(osp.join(landmarks_folder_path, landmarks_file_name), \'r\')\n        self.have_landmarks = not self.landmarks_file is None\n        self.landmarks_training = landmarks_training\n        if self.landmarks_training:\n            assert self.have_landmarks is True\n\n        self.samples_info = self._read_samples_info()\n\n        self.transform = transform\n\n    def _read_samples_info(self):\n        """"""Reads annotation of the dataset""""""\n        samples = []\n\n        with open(self.image_list_path, \'r\') as f:\n            last_class_id = -1\n            images_file_lines = f.readlines()\n\n            if self.have_landmarks:\n                detections_file_lines = self.detections_file.readlines()[1:]\n                landmarks_file_lines = self.landmarks_file.readlines()[1:]\n                assert len(detections_file_lines) == len(landmarks_file_lines)\n                assert len(images_file_lines) == len(detections_file_lines)\n\n            for i in tqdm(range(len(images_file_lines))):\n                sample = images_file_lines[i].strip()\n                sample_id = int(sample.split(\'/\')[0][1:])\n                frame_id = int(sample.split(\'/\')[1].split(\'_\')[0])\n                if sample_id in self.identities:\n                    self.identities[sample_id].append(len(samples))\n                else:\n                    last_class_id += 1\n                    self.identities[sample_id] = [len(samples)]\n                if not self.have_landmarks:\n                    samples.append((osp.join(self.images_root_path, sample), last_class_id, frame_id))\n                else:\n                    _, bbox = detections_file_lines[i].split(\'"",\')\n                    bbox = [max(int(coord), 0) for coord in bbox.split(\',\')]\n                    _, landmarks = landmarks_file_lines[i].split(\'"",\')\n                    landmarks = [float(coord) for coord in landmarks.split(\',\')]\n                    samples.append((osp.join(self.images_root_path, sample), last_class_id, sample_id, bbox, landmarks))\n\n        return samples\n\n    def get_weights(self):\n        """"""Computes weights of the each identity in dataset according to frequency of it\'s occurance""""""\n        weights = [0.]*len(self.samples_info)\n        for i, sample in enumerate(self.samples_info):\n            weights[i] = len(self.samples_info) / float(len(self.identities[sample[2]]))\n\n        return weights\n\n    def get_num_classes(self):\n        """"""Returns total number of identities""""""\n        return len(self.identities)\n\n    def __len__(self):\n        """"""Returns total number of samples""""""\n        return len(self.samples_info)\n\n    def __getitem__(self, idx):\n        """"""Returns sample (image, class id, image id) by index""""""\n        img = cv.imread(self.samples_info[idx][0], cv.IMREAD_COLOR)\n        if self.landmarks_training:\n            landmarks = self.samples_info[idx][-1]\n            bbox = self.samples_info[idx][-2]\n            img = img[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n            landmarks = [(float(landmarks[2*i]-bbox[0]) / bbox[2],\n                          float(landmarks[2*i + 1]-bbox[1])/ bbox[3]) for i in range(len(landmarks)//2)]\n            data = {\'img\': img, \'landmarks\': np.array(landmarks)}\n            if self.transform:\n                data = self.transform(data)\n            return data\n\n        if self.have_landmarks:\n            landmarks = self.samples_info[idx][-1]\n            img = FivePointsAligner.align(img, landmarks, d_size=(200, 200), normalized=False)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return {\'img\': img, \'label\': self.samples_info[idx][1], \'instance\': self.samples_info[idx][2]}\n'"
pytorch_toolkit/face_recognition/demo/run_demo.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport os\nimport os.path as osp\n\nimport glog as log\nimport cv2 as cv\nimport numpy as np\nfrom scipy.spatial.distance import cosine\n\nfrom utils import face_align\nfrom utils.ie_tools import load_ie_model\n\n\nclass FaceDetector:\n    """"""Wrapper class for face detector""""""\n    def __init__(self, model_path, conf=.6, device=\'CPU\', ext_path=\'\'):\n        self.net = load_ie_model(model_path, device, None, ext_path)\n        self.confidence = conf\n        self.expand_ratio = (1.1, 1.05)\n\n    def get_detections(self, frame):\n        """"""Returns all detections on frame""""""\n        _, _, h, w = self.net.get_input_shape().shape\n        out = self.net.forward(cv.resize(frame, (w, h)))\n        detections = self.__decode_detections(out, frame.shape)\n        return detections\n\n    def __decode_detections(self, out, frame_shape):\n        """"""Decodes raw SSD output""""""\n        detections = []\n\n        for detection in out[0, 0]:\n            confidence = detection[2]\n            if confidence > self.confidence:\n                left = int(max(detection[3], 0) * frame_shape[1])\n                top = int(max(detection[4], 0) * frame_shape[0])\n                right = int(max(detection[5], 0) * frame_shape[1])\n                bottom = int(max(detection[6], 0) * frame_shape[0])\n                if self.expand_ratio != (1., 1.):\n                    w = (right - left)\n                    h = (bottom - top)\n                    dw = w * (self.expand_ratio[0] - 1.) / 2\n                    dh = h * (self.expand_ratio[1] - 1.) / 2\n                    left = max(int(left - dw), 0)\n                    right = int(right + dw)\n                    top = max(int(top - dh), 0)\n                    bottom = int(bottom + dh)\n\n                detections.append(((left, top, right, bottom), confidence))\n\n        if len(detections) > 1:\n            detections.sort(key=lambda x: x[1], reverse=True)\n\n        return detections\n\n\nclass VectorCNN:\n    """"""Wrapper class for a nework returning a vector""""""\n    def __init__(self, model_path, device=\'CPU\'):\n        self.net = load_ie_model(model_path, device, None)\n\n    def forward(self, batch):\n        """"""Performs forward of the underlying network on a given batch""""""\n        _, _, h, w = self.net.get_input_shape().shape\n        outputs = [self.net.forward(cv.resize(frame, (w, h))) for frame in batch]\n        return outputs\n\n\ndef get_embeddings(frame, detections, face_reid, landmarks_predictor):\n    """"""Get embeddings for all detected faces on the frame""""""\n    rois = []\n    embeddings = []\n    for rect, _ in detections:\n        left, top, right, bottom = rect\n        rois.append(frame[top:bottom, left:right])\n\n    if rois:\n        landmarks = landmarks_predictor.forward(rois)\n        assert len(landmarks) == len(rois)\n\n        for i, _ in enumerate(rois):\n            roi_keypoints = landmarks[i].reshape(-1)\n            rois[i] = face_align.FivePointsAligner.align(rois[i], roi_keypoints,\n                                                         d_size=(rois[i].shape[1], rois[i].shape[0]),\n                                                         normalized=True, show=False)\n        embeddings = face_reid.forward(rois)\n        assert len(rois) == len(embeddings)\n\n    return embeddings\n\n\ndef find_nearest(x, gallery, thr):\n    """"""Finds the nearest to a given embedding in the gallery""""""\n    if gallery:\n        diffs = np.array([cosine(x, y) for y in gallery.values()])\n        min_pos = diffs.argmin()\n        min_dist = diffs[min_pos]\n        if min_dist < thr:\n            return min_pos, list(gallery.keys())[min_pos]\n    return None, None\n\n\ndef match_embeddings(embeds, gallery, thr):\n    """"""Matches input embeddings with ones in the gallery""""""\n    indexes = []\n    for emb in embeds:\n        _, name = find_nearest(emb, gallery, thr)\n        if name is not None:\n            indexes.append(name)\n        else:\n            indexes.append(\'Unknown\')\n\n    return indexes, gallery\n\n\ndef draw_detections(frame, detections, indexes):\n    """"""Draws detections and labels""""""\n    for i, rect in enumerate(detections):\n        left, top, right, bottom = rect[0]\n        cv.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), thickness=2)\n        label = str(indexes[i])\n        label_size, base_line = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 1, 1)\n        top = max(top, label_size[1])\n        cv.rectangle(frame, (left, top - label_size[1]), (left + label_size[0], top + base_line),\n                     (255, 255, 255), cv.FILLED)\n        cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n\n    return frame\n\n\ndef load_gallery(path_to_gallery, face_det, landmarks_detector, face_recognizer):\n    """"""Computes embeddings for gallery""""""\n    gallery = {}\n    files = os.listdir(path_to_gallery)\n    files = [file for file in files if file.endswith(\'.png\') or file.endswith(\'.jpg\')]\n    for file in files:\n        img = cv.imread(osp.join(path_to_gallery, file))\n        detections = face_det.get_detections(img)\n\n        if not detections:\n            detections = [[0, 0, img.shape[0], img.shape[1]], 0]\n            log.warn(\'Warning: failed to detect face on the image \' + file)\n\n        embed = get_embeddings(img, detections, face_recognizer, landmarks_detector)\n        gallery[file.replace(\'.png\', \'\').replace(\'.jpg\', \'\')] = embed[0]\n    return gallery\n\n\ndef run(params, capture, face_det, face_recognizer, landmarks_detector):\n    """"""Starts the face recognition demo""""""\n    win_name = \'Deep Face Recognition\'\n    gallery = load_gallery(params.path_to_gallery, face_det, landmarks_detector, face_recognizer)\n\n    while cv.waitKey(1) != 27:\n        has_frame, frame = capture.read()\n        if not has_frame:\n            return\n\n        detections = face_det.get_detections(frame)\n        embeds = get_embeddings(frame, detections, face_recognizer, landmarks_detector)\n        ids, gallery = match_embeddings(embeds, gallery, params.fr_thresh)\n        frame = draw_detections(frame, detections, ids)\n        cv.imshow(win_name, frame)\n\ndef main():\n    """"""Prepares data for the face recognition demo""""""\n    parser = argparse.ArgumentParser(description=\'Face recognition live demo script\')\n    parser.add_argument(\'--video\', type=str, default=None, help=\'Input video\')\n    parser.add_argument(\'--cam_id\', type=int, default=-1, help=\'Input cam\')\n\n    parser.add_argument(\'--fd_model\', type=str, required=True)\n    parser.add_argument(\'--fd_thresh\', type=float, default=0.6, help=\'Threshold for FD\')\n\n    parser.add_argument(\'--fr_model\', type=str, required=True)\n    parser.add_argument(\'--fr_thresh\', type=float, default=0.6, help=\'Threshold for FR\')\n\n    parser.add_argument(\'--path_to_gallery\', type=str, required=True, help=\'Path to gallery with subjects\')\n\n    parser.add_argument(\'--ld_model\', type=str, default=\'\', help=\'Path to a snapshots with landmarks detection model\')\n\n    parser.add_argument(\'--device\', type=str, default=\'CPU\')\n    parser.add_argument(\'-l\', \'--cpu_extension\',\n                        help=\'MKLDNN (CPU)-targeted custom layers.Absolute path to a shared library with the kernels \'\n                             \'impl.\', type=str, default=None)\n\n    args = parser.parse_args()\n\n    if args.cam_id >= 0:\n        log.info(\'Reading from cam {}\'.format(args.cam_id))\n        cap = cv.VideoCapture(args.cam_id)\n        cap.set(cv.CAP_PROP_FRAME_WIDTH, 1280)\n        cap.set(cv.CAP_PROP_FRAME_HEIGHT, 720)\n        cap.set(cv.CAP_PROP_FOURCC, cv.VideoWriter_fourcc(\'M\', \'J\', \'P\', \'G\'))\n    else:\n        assert args.video\n        log.info(\'Reading from {}\'.format(args.video))\n        cap = cv.VideoCapture(args.video)\n    assert cap.isOpened()\n\n    face_detector = FaceDetector(args.fd_model, args.fd_thresh, args.device, args.cpu_extension)\n    face_recognizer = VectorCNN(args.fr_model, args.device)\n    landmarks_detector = VectorCNN(args.ld_model, args.device)\n    run(args, cap, face_detector, face_recognizer, landmarks_detector)\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/losses/__init__.py,0,b''
pytorch_toolkit/face_recognition/losses/alignment.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\nimport torch\nimport torch.nn as nn\n\nVALID_CORE_FUNC_TYPES = [\'l1\', \'l2\', \'wing\']\n\n\ndef wing_core(abs_x, w, eps):\n    """"""Calculates the wing function from https://arxiv.org/pdf/1711.06753.pdf""""""\n    return w*math.log(1. + abs_x / eps)\n\nclass AlignmentLoss(nn.Module):\n    """"""Regression loss to train landmarks model""""""\n    def __init__(self, loss_type=\'l2\'):\n        super(AlignmentLoss, self).__init__()\n        assert loss_type in VALID_CORE_FUNC_TYPES\n        self.uniform_weights = True\n        self.weights = None\n        self.core_func_type = loss_type\n        self.eps = 0.031\n        self.w = 0.156\n\n    def set_weights(self, weights):\n        """"""Set weights for the each landmark point in loss""""""\n        self.uniform_weights = False\n        self.weights = torch.FloatTensor(weights).cuda()\n\n    def forward(self, input_values, target):\n        bs = input_values.shape[0]\n        loss = input_values - target\n        n_points = loss.shape[1] // 2\n        loss = loss.view(-1, n_points, 2)\n\n        if self.core_func_type == \'l2\':\n            loss = torch.norm(loss, p=2, dim=2)\n            loss = loss.pow(2)\n            eyes_dist = (torch.norm(target[:, 0:2] - target[:, 2:4], p=2, dim=1).reshape(-1)).pow_(2)\n        elif self.core_func_type == \'l1\':\n            loss = torch.norm(loss, p=1, dim=2)\n            eyes_dist = (torch.norm(target[:, 0:2] - target[:, 2:4], p=1, dim=1).reshape(-1))\n        elif self.core_func_type == \'wing\':\n            wing_const = self.w - wing_core(self.w, self.w, self.eps)\n            loss = torch.abs(loss)\n            loss[loss < wing_const] = self.w*torch.log(1. + loss[loss < wing_const] / self.eps)\n            loss[loss >= wing_const] -= wing_const\n            loss = torch.sum(loss, 2)\n            eyes_dist = (torch.norm(target[:, 0:2] - target[:, 2:4], p=1, dim=1).reshape(-1))\n\n        if self.uniform_weights:\n            loss = torch.sum(loss, 1)\n            loss /= n_points\n        else:\n            assert self.weights.shape[0] == loss.shape[1]\n            loss = torch.mul(loss, self.weights)\n            loss = torch.sum(loss, 1)\n\n        loss = torch.div(loss, eyes_dist)\n        loss = torch.sum(loss)\n        return loss / (2.*bs)\n'"
pytorch_toolkit/face_recognition/losses/am_softmax.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\n\nclass AngleSimpleLinear(nn.Module):\n    """"""Computes cos of angles between input vectors and weights vectors""""""\n    def __init__(self, in_features, out_features):\n        super(AngleSimpleLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n\n    def forward(self, x):\n        cos_theta = F.normalize(x, dim=1).mm(F.normalize(self.weight, dim=0))\n        return cos_theta.clamp(-1, 1)\n\n\ndef focal_loss(input_values, gamma):\n    """"""Computes the focal loss""""""\n    p = torch.exp(-input_values)\n    loss = (1 - p) ** gamma * input_values\n    return loss.mean()\n\n\nclass AMSoftmaxLoss(nn.Module):\n    """"""Computes the AM-Softmax loss with cos or arc margin""""""\n    margin_types = [\'cos\', \'arc\']\n\n    def __init__(self, margin_type=\'cos\', gamma=0., m=0.5, s=30, t=1.):\n        super(AMSoftmaxLoss, self).__init__()\n        assert margin_type in AMSoftmaxLoss.margin_types\n        self.margin_type = margin_type\n        assert gamma >= 0\n        self.gamma = gamma\n        assert m > 0\n        self.m = m\n        assert s > 0\n        self.s = s\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        assert t >= 1\n        self.t = t\n\n    def forward(self, cos_theta, target):\n        if self.margin_type == \'cos\':\n            phi_theta = cos_theta - self.m\n        else:\n            sine = torch.sqrt(1.0 - torch.pow(cos_theta, 2))\n            phi_theta = cos_theta * self.cos_m - sine * self.sin_m #cos(theta+m)\n            phi_theta = torch.where(cos_theta > self.th, phi_theta, cos_theta - self.sin_m * self.m)\n\n        index = torch.zeros_like(cos_theta, dtype=torch.uint8)\n        index.scatter_(1, target.data.view(-1, 1), 1)\n        output = torch.where(index, phi_theta, cos_theta)\n\n        if self.gamma == 0 and self.t == 1.:\n            return F.cross_entropy(self.s*output, target)\n\n        if self.t > 1:\n            h_theta = self.t - 1 + self.t*cos_theta\n            support_vecs_mask = (1 - index) * \\\n                torch.lt(torch.masked_select(phi_theta, index).view(-1, 1).repeat(1, h_theta.shape[1]) - cos_theta, 0)\n            output = torch.where(support_vecs_mask, h_theta, output)\n            return F.cross_entropy(self.s*output, target)\n\n        return focal_loss(F.cross_entropy(self.s*output, target, reduction=\'none\'), self.gamma)\n'"
pytorch_toolkit/face_recognition/losses/centroid_based.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass CenterLoss(nn.Module):\n    """"""Implements the Center loss from https://ydwen.github.io/papers/WenECCV16.pdf""""""\n    def __init__(self, num_classes, embed_size, cos_dist=True):\n        super().__init__()\n        self.cos_dist = cos_dist\n        self.num_classes = num_classes\n        self.centers = nn.Parameter(torch.randn(self.num_classes, embed_size).cuda())\n        self.embed_size = embed_size\n        self.mse = nn.MSELoss(reduction=\'elementwise_mean\')\n\n    def get_centers(self):\n        """"""Returns estimated centers""""""\n        return self.centers\n\n    def forward(self, features, labels):\n        features = F.normalize(features)\n        batch_size = labels.size(0)\n        features_dim = features.size(1)\n        assert features_dim == self.embed_size\n\n        if self.cos_dist:\n            self.centers.data = F.normalize(self.centers.data, p=2, dim=1)\n\n        centers_batch = self.centers[labels, :]\n\n        if self.cos_dist:\n            cos_sim = nn.CosineSimilarity()\n            cos_diff = 1. - cos_sim(features, centers_batch)\n            center_loss = torch.sum(cos_diff) / batch_size\n        else:\n            center_loss = self.mse(centers_batch, features)\n\n        return center_loss\n\n\nclass MinimumMargin(nn.Module):\n    """"""Implements the Minimum margin loss from https://arxiv.org/abs/1805.06741""""""\n    def __init__(self, margin=.6):\n        super().__init__()\n        self.margin = margin\n\n    def forward(self, centers, labels):\n        loss_value = 0\n\n        batch_centers = centers[labels, :]\n        labels = labels.cpu().data.numpy()\n\n        all_pairs = labels.reshape([-1, 1]) != labels.reshape([1, -1])\n        valid_pairs = (all_pairs * np.tri(*all_pairs.shape, k=-1, dtype=np.bool)).astype(np.float32)\n        losses = 1. - torch.mm(batch_centers, torch.t(batch_centers)) - self.margin\n\n        valid_pairs *= (losses.cpu().data.numpy() > 0.0)\n        num_valid = float(np.sum(valid_pairs))\n\n        if num_valid > 0:\n            loss_value = torch.sum(losses * torch.from_numpy(valid_pairs).cuda())\n        else:\n            return loss_value\n\n        return loss_value / num_valid\n\n\nclass GlobalPushPlus(nn.Module):\n    """"""Implements the Global Push Plus loss""""""\n    def __init__(self, margin=.6):\n        super().__init__()\n        self.min_margin = 0.15\n        self.max_margin = margin\n        self.num_calls = 0\n\n    def forward(self, features, centers, labels):\n        self.num_calls += 1\n        features = F.normalize(features)\n        loss_value = 0\n        batch_centers = centers[labels, :]\n        labels = labels.cpu().data.numpy()\n        assert len(labels.shape) == 1\n\n        center_ids = np.arange(centers.shape[0], dtype=np.int32)\n        different_class_pairs = labels.reshape([-1, 1]) != center_ids.reshape([1, -1])\n\n        pos_distances = 1.0 - torch.sum(features * batch_centers, dim=1)\n        neg_distances = 1.0 - torch.mm(features, torch.t(centers))\n\n        margin = self.min_margin + float(self.num_calls) / float(40000) * (self.max_margin - self.min_margin)\n        margin = min(margin, self.max_margin)\n\n        losses = margin + pos_distances.view(-1, 1) - neg_distances\n\n        valid_pairs = (different_class_pairs * (losses.cpu().data.numpy() > 0.0)).astype(np.float32)\n        num_valid = float(np.sum(valid_pairs))\n\n        if num_valid > 0:\n            loss_value = torch.sum(losses * torch.from_numpy(valid_pairs).cuda())\n        else:\n            return loss_value\n\n        return loss_value / num_valid\n\n\nclass PushPlusLoss(nn.Module):\n    """"""Implements the Push Plus loss""""""\n    def __init__(self, margin=.7):\n        super().__init__()\n        self.margin = margin\n\n    def forward(self, features, centers, labels):\n        features = F.normalize(features)\n        loss_value = 0\n        batch_centers = centers[labels, :]\n        labels = labels.cpu().data.numpy()\n        assert len(labels.shape) == 1\n\n        all_pairs = labels.reshape([-1, 1]) != labels.reshape([1, -1])\n        pos_distances = 1.0 - torch.sum(features * batch_centers, dim=1)\n        neg_distances = 1.0 - torch.mm(features, torch.t(features))\n\n        losses = self.margin + pos_distances.view(-1, 1) - neg_distances\n        valid_pairs = (all_pairs * (losses.cpu().data.numpy() > 0.0)).astype(np.float32)\n        num_valid = float(np.sum(valid_pairs))\n\n        if num_valid > 0:\n            loss_value = torch.sum(losses * torch.from_numpy(valid_pairs).cuda())\n        else:\n            return loss_value\n\n        return loss_value / num_valid\n\n\nclass PushLoss(nn.Module):\n    """"""Implements the Push loss""""""\n    def __init__(self, soft=True, margin=0.5):\n        super().__init__()\n        self.soft = soft\n        self.margin = margin\n\n    def forward(self, features, labels):\n        features = F.normalize(features)\n        loss_value = 0\n        labels = labels.cpu().data.numpy()\n        assert len(labels.shape) == 1\n\n        all_pairs = labels.reshape([-1, 1]) != labels.reshape([1, -1])\n        valid_pairs = (all_pairs * np.tri(*all_pairs.shape, k=-1, dtype=np.bool)).astype(np.float32)\n\n        if self.soft:\n            losses = torch.log(1. + torch.exp(torch.mm(features, torch.t(features)) - 1))\n            num_valid = float(np.sum(valid_pairs))\n        else:\n            losses = self.margin - (1. - torch.mm(features, torch.t(features)))\n            valid_pairs *= (losses.cpu().data.numpy() > 0.0)\n            num_valid = float(np.sum(valid_pairs))\n\n        if num_valid > 0:\n            loss_value = torch.sum(losses * torch.from_numpy(valid_pairs).cuda())\n        else:\n            return loss_value\n\n        return loss_value / num_valid\n'"
pytorch_toolkit/face_recognition/losses/metric_losses.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom losses.centroid_based import CenterLoss, PushLoss, MinimumMargin, PushPlusLoss, GlobalPushPlus\n\n\nclass MetricLosses:\n    """"""Class-aggregator for all metric-learning losses""""""\n    def __init__(self, classes_num, embed_size, writer):\n        self.writer = writer\n        self.center_loss = CenterLoss(classes_num, embed_size, cos_dist=True)\n        self.optimizer_centloss = torch.optim.SGD(self.center_loss.parameters(), lr=0.5)\n        self.center_coeff = 0.0\n\n        self.push_loss = PushLoss(soft=False, margin=0.7)\n        self.push_loss_coeff = 0.0\n\n        self.push_plus_loss = PushPlusLoss(margin=0.7)\n        self.push_plus_loss_coeff = 0.0\n\n        self.glob_push_plus_loss = GlobalPushPlus(margin=0.7)\n        self.glob_push_plus_loss_coeff = 0.0\n\n        self.min_margin_loss = MinimumMargin(margin=.7)\n        self.min_margin_loss_coeff = 0.0\n\n    def __call__(self, features, labels, epoch_num, iteration):\n        log_string = \'\'\n\n        center_loss_val = 0\n        if self.center_coeff > 0.:\n            center_loss_val = self.center_loss(features, labels)\n            self.writer.add_scalar(\'Loss/center_loss\', center_loss_val, iteration)\n            log_string += \' Center loss: %.4f\' % center_loss_val\n\n        push_loss_val = 0\n        if self.push_loss_coeff > 0.0:\n            push_loss_val = self.push_loss(features, labels)\n            self.writer.add_scalar(\'Loss/push_loss\', push_loss_val, iteration)\n            log_string += \' Push loss: %.4f\' % push_loss_val\n\n        push_plus_loss_val = 0\n        if self.push_plus_loss_coeff > 0.0 and self.center_coeff > 0.0:\n            push_plus_loss_val = self.push_plus_loss(features, self.center_loss.get_centers(), labels)\n            self.writer.add_scalar(\'Loss/push_plus_loss\', push_plus_loss_val, iteration)\n            log_string += \' Push Plus loss: %.4f\' % push_plus_loss_val\n\n        glob_push_plus_loss_val = 0\n        if self.glob_push_plus_loss_coeff > 0.0 and self.center_coeff > 0.0:\n            glob_push_plus_loss_val = self.glob_push_plus_loss(features, self.center_loss.get_centers(), labels)\n            self.writer.add_scalar(\'Loss/global_push_plus_loss\', glob_push_plus_loss_val, iteration)\n            log_string += \' Global Push Plus loss: %.4f\' % glob_push_plus_loss_val\n\n        min_margin_loss_val = 0\n        if self.min_margin_loss_coeff > 0.0 and self.center_coeff > 0.0:\n            min_margin_loss_val = self.min_margin_loss(self.center_loss.get_centers(), labels)\n            self.writer.add_scalar(\'Loss/min_margin_loss\', min_margin_loss_val, iteration)\n            log_string += \' Min margin loss: %.4f\' % min_margin_loss_val\n\n        loss_value = self.center_coeff * center_loss_val + self.push_loss_coeff * push_loss_val + \\\n                     self.push_plus_loss_coeff * push_plus_loss_val + self.min_margin_loss_coeff * min_margin_loss_val \\\n                     + self.glob_push_plus_loss_coeff * glob_push_plus_loss_val\n\n        if self.min_margin_loss_coeff + self.center_coeff + self.push_loss_coeff + self.push_plus_loss_coeff > 0.:\n            self.writer.add_scalar(\'Loss/AUX_losses\', loss_value, iteration)\n\n        return loss_value, log_string\n\n    def init_iteration(self):\n        """"""Initializes a training iteration""""""\n        if self.center_coeff > 0.:\n            self.optimizer_centloss.zero_grad()\n\n    def end_iteration(self):\n        """"""Finalizes a training iteration""""""\n        if self.center_coeff > 0.:\n            for param in self.center_loss.parameters():\n                param.grad.data *= (1. / self.center_coeff)\n            self.optimizer_centloss.step()\n'"
pytorch_toolkit/face_recognition/losses/regularizer.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef l2_reg_ortho(mdl):\n    """"""\n        Function used for Orthogonal Regularization.\n    """"""\n    l2_reg = None\n    for w in mdl.parameters():\n        if w.ndimension() < 2:\n            continue\n        else:\n            cols = w[0].numel()\n            w1 = w.view(-1, cols)\n            wt = torch.transpose(w1, 0, 1)\n            m = torch.matmul(wt, w1)\n            ident = torch.eye(cols, cols).cuda()\n\n            w_tmp = (m - ident)\n            height = w_tmp.size(0)\n            u = F.normalize(w_tmp.new_empty(height).normal_(0, 1), dim=0, eps=1e-12)\n            v = F.normalize(torch.matmul(w_tmp.t(), u), dim=0, eps=1e-12)\n            u = F.normalize(torch.matmul(w_tmp, v), dim=0, eps=1e-12)\n            sigma = torch.dot(u, torch.matmul(w_tmp, v))\n\n            if l2_reg is None:\n                l2_reg = (torch.norm(sigma, 2))**2\n            else:\n                l2_reg += (torch.norm(sigma, 2))**2\n    return l2_reg\n\n\nclass ODecayScheduler():\n    """"""Scheduler for the decay of the orthogonal regularizer""""""\n    def __init__(self, schedule, initial_decay, mult_factor):\n        assert len(schedule) > 1\n        self.schedule = schedule\n        self.epoch_num = 0\n        self.mult_factor = mult_factor\n        self.decay = initial_decay\n\n    def step(self):\n        """"""Switches to the next step""""""\n        self.epoch_num += 1\n        if self.epoch_num in self.schedule:\n            self.decay *= self.mult_factor\n        if self.epoch_num == self.schedule[-1]:\n            self.decay = 0.0\n\n    def get_decay(self):\n        """"""Returns the current value of decay according to th schedule""""""\n        return self.decay\n'"
pytorch_toolkit/face_recognition/model/__init__.py,0,b''
pytorch_toolkit/face_recognition/model/common.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom abc import abstractmethod\nfrom functools import partial\nimport torch.nn as nn\n\n\nclass ModelInterface(nn.Module):\n    """"""Abstract class for models""""""\n\n    @abstractmethod\n    def set_dropout_ratio(self, ratio):\n        """"""Sets dropout ratio of the model""""""\n\n    @abstractmethod\n    def get_input_res(self):\n        """"""Returns input resolution""""""\n\n\nfrom .rmnet_angular import RMNetAngular\nfrom .mobilefacenet import MobileFaceNet\nfrom .landnet import LandmarksNet\nfrom .se_resnet_angular import SEResNetAngular\nfrom .shufflenet_v2_angular import ShuffleNetV2Angular\nfrom .backbones.se_resnet import se_resnet50, se_resnet101, se_resnet152\nfrom .backbones.resnet import resnet50\nfrom .backbones.se_resnext import se_resnext50, se_resnext101, se_resnext152\n\n\nmodels_backbones = {\'rmnet\': RMNetAngular,\n                    \'mobilenetv2\': MobileFaceNet,\n                    \'mobilenetv2_2x\': partial(MobileFaceNet, width_multiplier=2.0),\n                    \'mobilenetv2_1_5x\': partial(MobileFaceNet, width_multiplier=1.5),\n                    \'resnet50\': partial(SEResNetAngular, base=resnet50),\n                    \'se_resnet50\': partial(SEResNetAngular, base=se_resnet50),\n                    \'se_resnet101\': partial(SEResNetAngular, base=se_resnet101),\n                    \'se_resnet152\': partial(SEResNetAngular, base=se_resnet152),\n                    \'se_resnext50\': partial(SEResNetAngular, base=se_resnext50),\n                    \'se_resnext101\': partial(SEResNetAngular, base=se_resnext101),\n                    \'se_resnext152\': partial(SEResNetAngular, base=se_resnext152),\n                    \'shufflenetv2\': ShuffleNetV2Angular}\n\nmodels_landmarks = {\'landnet\': LandmarksNet}\n'"
pytorch_toolkit/face_recognition/model/landnet.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom .common import ModelInterface\n\n\nclass LandmarksNet(ModelInterface):\n    """"""Facial landmarks localization network""""""\n    def __init__(self):\n        super(LandmarksNet, self).__init__()\n        self.bn_first = nn.BatchNorm2d(3)\n        activation = nn.PReLU\n        self.landnet = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            activation(),\n            nn.MaxPool2d(2, stride=2),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            activation(),\n            nn.MaxPool2d(2, stride=2),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            activation(),\n            nn.MaxPool2d(2, stride=2),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            activation(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            activation(),\n            nn.BatchNorm2d(128)\n        )\n        # dw pooling\n        self.bottleneck_size = 256\n        self.pool = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=6, padding=0, groups=128),\n            activation(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, self.bottleneck_size, kernel_size=1, padding=0),\n            activation(),\n            nn.BatchNorm2d(self.bottleneck_size),\n        )\n        # Regressor for 5 landmarks (10 coordinates)\n        self.fc_loc = nn.Sequential(\n            nn.Conv2d(self.bottleneck_size, 64, kernel_size=1),\n            activation(),\n            nn.Conv2d(64, 10, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        xs = self.landnet(self.bn_first(x))\n        xs = self.pool(xs)\n        xs = self.fc_loc(xs)\n        return xs\n\n    def get_input_res(self):\n        return 48, 48\n\n    def set_dropout_ratio(self, ratio):\n        pass\n'"
pytorch_toolkit/face_recognition/model/mobilefacenet.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\nimport torch.nn as nn\n\nfrom losses.am_softmax import AngleSimpleLinear\nfrom model.blocks.mobilenet_v2_blocks import InvertedResidual\nfrom model.blocks.shared_blocks import make_activation\nfrom .common import ModelInterface\n\n\ndef init_block(in_channels, out_channels, stride, activation=nn.PReLU):\n    """"""Builds the first block of the MobileFaceNet""""""\n    return nn.Sequential(\n        nn.BatchNorm2d(3),\n        nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(out_channels),\n        make_activation(activation)\n    )\n\n\nclass MobileFaceNet(ModelInterface):\n    """"""Implements modified MobileFaceNet from https://arxiv.org/abs/1804.07573""""""\n    def __init__(self, embedding_size=128, num_classes=1, width_multiplier=1., feature=True):\n        super(MobileFaceNet, self).__init__()\n        assert embedding_size > 0\n        assert num_classes > 0\n        assert width_multiplier > 0\n        self.feature = feature\n\n        # Set up of inverted residual blocks\n        inverted_residual_setting = [\n            # t, c, n, s\n            [2, 64, 5, 2],\n            [4, 128, 1, 2],\n            [2, 128, 6, 1],\n            [4, 128, 1, 2],\n            [2, 128, 2, 1]\n        ]\n\n        first_channel_num = 64\n        last_channel_num = 512\n        self.features = [init_block(3, first_channel_num, 2)]\n\n        self.features.append(nn.Conv2d(first_channel_num, first_channel_num, 3, 1, 1,\n                                       groups=first_channel_num, bias=False))\n        self.features.append(nn.BatchNorm2d(64))\n        self.features.append(nn.PReLU())\n\n        # Inverted Residual Blocks\n        in_channel_num = first_channel_num\n        size_h, size_w = MobileFaceNet.get_input_res()\n        size_h, size_w = size_h // 2, size_w // 2\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = int(c * width_multiplier)\n            for i in range(n):\n                if i == 0:\n                    size_h, size_w = size_h // s, size_w // s\n                    self.features.append(InvertedResidual(in_channel_num, output_channel,\n                                                          s, t, outp_size=(size_h, size_w)))\n                else:\n                    self.features.append(InvertedResidual(in_channel_num, output_channel,\n                                                          1, t, outp_size=(size_h, size_w)))\n                in_channel_num = output_channel\n\n        # 1x1 expand block\n        self.features.append(nn.Sequential(nn.Conv2d(in_channel_num, last_channel_num, 1, 1, 0, bias=False),\n                                           nn.BatchNorm2d(last_channel_num),\n                                           nn.PReLU()))\n        self.features = nn.Sequential(*self.features)\n\n        # Depth-wise pooling\n        k_size = (MobileFaceNet.get_input_res()[0] // 16, MobileFaceNet.get_input_res()[1] // 16)\n        self.dw_pool = nn.Conv2d(last_channel_num, last_channel_num, k_size,\n                                 groups=last_channel_num, bias=False)\n        self.dw_bn = nn.BatchNorm2d(last_channel_num)\n        self.conv1_extra = nn.Conv2d(last_channel_num, embedding_size, 1, stride=1, padding=0, bias=False)\n\n        if not self.feature:\n            self.fc_angular = AngleSimpleLinear(embedding_size, num_classes)\n\n        self.init_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.dw_bn(self.dw_pool(x))\n        x = self.conv1_extra(x)\n\n        if self.feature or not self.training:\n            return x\n\n        x = x.view(x.size(0), -1)\n        y = self.fc_angular(x)\n\n        return x, y\n\n    @staticmethod\n    def get_input_res():\n        return 128, 128\n\n    def set_dropout_ratio(self, ratio):\n        assert 0 <= ratio < 1.\n\n    def init_weights(self):\n        """"""Initializes weights of the model before training""""""\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n'"
pytorch_toolkit/face_recognition/model/rmnet_angular.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom losses.am_softmax import AngleSimpleLinear\nfrom model.backbones.rmnet import RMNetBody\nfrom model.blocks.rmnet_blocks import RMBlock\nfrom .common import ModelInterface\n\n\nclass RMNetAngular(ModelInterface):\n    """"""Face reid head for the ResMobNet architecture. See https://arxiv.org/pdf/1812.02465.pdf for details\n    about the ResMobNet backbone.""""""\n    def __init__(self, embedding_size, num_classes=0, feature=True, body=RMNetBody):\n        super(RMNetAngular, self).__init__()\n        self.feature = feature\n        self.backbone = body()\n        self.global_pooling = nn.MaxPool2d((8, 8))\n        self.conv1_extra = nn.Conv2d(256, embedding_size, 1, stride=1, padding=0, bias=False)\n        if not feature:\n            self.fc_angular = AngleSimpleLinear(embedding_size, num_classes)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.global_pooling(x)\n        x = self.conv1_extra(x)\n\n        if self.feature or not self.training:\n            return x\n\n        x = x.view(x.size(0), -1)\n        y = self.fc_angular(x)\n\n        return x, y\n\n    def set_dropout_ratio(self, ratio):\n        assert 0 <= ratio < 1.\n\n        for m in self.backbone.modules():\n            if isinstance(m, RMBlock):\n                m.dropout_ratio = ratio\n\n    @staticmethod\n    def get_input_res():\n        return 128, 128\n'"
pytorch_toolkit/face_recognition/model/se_resnet_angular.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom losses.am_softmax import AngleSimpleLinear\nfrom model.backbones.se_resnet import se_resnet50\nfrom .common import ModelInterface\n\n\nclass SEResNetAngular(ModelInterface):\n    """"""Face reid head for the SE ResNet architecture""""""\n    def __init__(self, embedding_size=128, num_classes=0, feature=True, loss=None, base=se_resnet50):\n        super(SEResNetAngular, self).__init__()\n\n        self.bn_first = nn.BatchNorm2d(3)\n        self.feature = feature\n        self.model = base(num_classes=embedding_size, activation=nn.PReLU)\n        self.feat_bn = nn.BatchNorm2d(self.model.get_output_channels())\n        self.conv1_extra = nn.Conv2d(self.model.get_output_channels(), embedding_size,\n                                     1, stride=1, padding=0, bias=False)\n        self.emb_bn = nn.BatchNorm2d(embedding_size)\n        self.embedding_size = embedding_size\n        self.dropout_ratio = 0.4\n        self.loss = loss\n        if not self.feature:\n            self.fc_angular = AngleSimpleLinear(self.embedding_size, num_classes)\n\n    def forward(self, x, target=None):\n        assert self.loss is not None or not self.training\n        x = self.bn_first(x)\n        x = self.feat_bn(self.model(x))\n        if self.dropout_ratio > 0:\n            x = F.dropout(x, p=self.dropout_ratio, training=self.training, inplace=True)\n        x = self.emb_bn(self.conv1_extra(x))\n\n        if self.feature or not self.training:\n            return x.view(x.shape[0], x.shape[1], 1, 1)\n\n        x = x.view(x.size(0), -1)\n        y = self.fc_angular(x)\n\n        return x, y, self.loss(y, target)\n\n    @staticmethod\n    def get_input_res():\n        return 112, 112\n\n    def set_dropout_ratio(self, ratio):\n        assert 0 <= ratio < 1.\n        self.dropout_ratio = ratio\n'"
pytorch_toolkit/face_recognition/model/shufflenet_v2_angular.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom losses.am_softmax import AngleSimpleLinear\nfrom model.backbones.shufflenet_v2 import ShuffleNetV2Body\nfrom .common import ModelInterface\n\n\nclass ShuffleNetV2Angular(ModelInterface):\n    """"""Face reid head for the ShuffleNetV2 architecture""""""\n    def __init__(self, embedding_size, num_classes=0, feature=True, body=ShuffleNetV2Body, **kwargs):\n        super(ShuffleNetV2Angular, self).__init__()\n        self.feature = feature\n        kwargs[\'input_size\'] = ShuffleNetV2Angular.get_input_res()[0]\n        kwargs[\'width_mult\'] = 1.\n        self.backbone = body(**kwargs)\n        k_size = int(kwargs[\'input_size\'] / self.backbone.get_downscale_factor())\n        self.global_pool = nn.Conv2d(self.backbone.stage_out_channels[-1], self.backbone.stage_out_channels[-1],\n                                     (k_size, k_size), groups=self.backbone.stage_out_channels[-1], bias=False)\n        self.conv1_extra = nn.Conv2d(self.backbone.get_num_output_channels(), embedding_size, 1, padding=0, bias=False)\n        if not feature:\n            self.fc_angular = AngleSimpleLinear(embedding_size, num_classes)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.global_pool(x)\n        x = self.conv1_extra(x)\n\n        if self.feature or not self.training:\n            return x\n\n        x = x.view(x.size(0), -1)\n        y = self.fc_angular(x)\n\n        return x, y\n\n    def set_dropout_ratio(self, ratio):\n        assert 0 <= ratio < 1.\n\n    @staticmethod\n    def get_input_res():\n        res = 128\n        return res, res\n'"
pytorch_toolkit/face_recognition/scripts/__init__.py,0,b''
pytorch_toolkit/face_recognition/scripts/accuracy_check.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport glog as log\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nimport cv2 as cv\n\nfrom utils.utils import load_model_state\nfrom utils.ie_tools import load_ie_model\nfrom model.common import models_backbones, models_landmarks\n\ndef main():\n    """"""Runs the accuracy check""""""\n    parser = argparse.ArgumentParser(description=\'Accuracy check script (pt vs caffe)\')\n    parser.add_argument(\'--embed_size\', type=int, default=128, help=\'Size of the face embedding.\')\n    parser.add_argument(\'--snap\', type=str, required=True, help=\'Snapshot to convert.\')\n    parser.add_argument(\'--device\', \'-d\', default=0, type=int, help=\'Device for model placement.\')\n    parser.add_argument(\'--model\', choices=list(models_backbones.keys()) + list(models_landmarks.keys()), type=str,\n                        default=\'rmnet\')\n\n    # IE-related options\n    parser.add_argument(\'--ie_model\', type=str, required=True)\n    parser.add_argument(""-l"", ""--cpu_extension"",\n                        help=""MKLDNN (CPU)-targeted custom layers.Absolute path to a shared library with the kernels ""\n                             ""impl."", type=str, default=None)\n    parser.add_argument(""-pp"", ""--plugin_dir"", help=""Path to a plugin folder"", type=str, default=None)\n    parser.add_argument(""-d_ie"", ""--device_ie"",\n                        help=""Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. Sample ""\n                             ""will look for a suitable plugin for device specified (CPU by default)"", default=""CPU"",\n                        type=str)\n\n    args = parser.parse_args()\n\n    max_err = 0.\n    with torch.cuda.device(args.device):\n        if args.model in models_landmarks.keys():\n            pt_model = models_landmarks[args.model]\n        else:\n            pt_model = models_backbones[args.model](embedding_size=args.embed_size, feature=True)\n        pt_model = load_model_state(pt_model, args.snap, args.device)\n\n        ie_model = load_ie_model(args.ie_model, args.device_ie, args.plugin_dir, args.cpu_extension)\n        np.random.seed(0)\n\n        for _ in tqdm(range(100)):\n            input_img = np.random.randint(0, high=255, size=(*pt_model.get_input_res(), 3), dtype=np.uint8)\n            input_bgr = cv.cvtColor(input_img, cv.COLOR_BGR2RGB)\n\n            input_pt = torch.unsqueeze(torch.from_numpy(input_img.transpose(2, 0, 1).astype(\'float32\') / 255.).cuda(),\n                                       dim=0)\n            pt_output = (pt_model(input_pt)).data.cpu().numpy().reshape(1, -1)\n            ie_output = ie_model.forward(input_bgr).reshape(1, -1)\n\n            max_err = max(np.linalg.norm(pt_output - ie_output, np.inf), max_err)\n\n    log.info(\'Max l_inf error: %e\', max_err)\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/scripts/align_images.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport os\nimport os.path as osp\nimport json\n\nimport cv2 as cv\nimport torch\nfrom tqdm import tqdm\nfrom torchvision.transforms import transforms\n\nfrom model import landnet\nfrom utils import utils\nfrom utils import augmentation\nfrom utils.face_align import FivePointsAligner\n\nclass LandnetPT:\n    """"""Wrapper for landmarks regression model""""""\n    def __init__(self, model):\n        self.net = model\n        self.transformer = transforms.Compose(\n            [augmentation.ResizeNumpy((48, 48)), augmentation.NumpyToTensor(switch_rb=True)])\n\n    def get_landmarks(self, batch):\n        converted_batch = []\n        for item in batch:\n            converted_batch.append(self.transformer(item))\n        pt_blob = torch.stack(converted_batch).cuda()\n        landmarks = self.net(pt_blob)\n        return landmarks.data.cpu().numpy()\n\n\nclass FaceDetector:\n    """"""Wrapper class for face detector""""""\n    def __init__(self, proto, model, conf=.6, expand_ratio=(1.1, 1.05), size=(300, 300)):\n        self.net = cv.dnn.readNetFromCaffe(proto, model)\n        self.net.setPreferableBackend(cv.dnn.DNN_BACKEND_DEFAULT)\n        self.net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n        last_layer_id = self.net.getLayerId(self.net.getLayerNames()[-1])\n        last_layer = self.net.getLayer(last_layer_id)\n        assert last_layer.type == \'DetectionOutput\'\n\n        self.confidence = conf\n        self.expand_ratio = expand_ratio\n        self.det_res = size\n\n    def __decode_detections(self, out, frame_shape):\n        """"""Decodes raw SSD output""""""\n        frame_height = frame_shape[0]\n        frame_width = frame_shape[1]\n        detections = []\n\n        for detection in out[0, 0]:\n            confidence = detection[2]\n            if confidence > self.confidence:\n                left = int(max(detection[3], 0) * frame_width)\n                top = int(max(detection[4], 0) * frame_height)\n                right = int(max(detection[5], 0) * frame_width)\n                bottom = int(max(detection[6], 0) * frame_height)\n                if self.expand_ratio != (1., 1.):\n                    w = (right - left)\n                    h = (bottom - top)\n                    dw = w * (self.expand_ratio[0] - 1.) / 2\n                    dh = h * (self.expand_ratio[1] - 1.) / 2\n                    left = max(int(left - dw), 0)\n                    right = int(right + dw)\n                    top = max(int(top - dh), 0)\n                    bottom = int(bottom + dh)\n\n                # classId = int(detection[1]) - 1  # Skip background label\n                detections.append(((left, top, right, bottom), confidence))\n\n        if len(detections) > 1:\n            detections.sort(key=lambda x: x[1], reverse=True)\n\n        return detections\n\n    def get_detections(self, frame):\n        """"""Returns all detections on frame""""""\n        blob = cv.dnn.blobFromImage(frame, 1., (self.det_res[0], self.det_res[1]), crop=False)\n        self.net.setInput(blob)\n        out = self.net.forward()\n        detections = self.__decode_detections(out, frame.shape)\n        return detections\n\n\ndef draw_detections(frame, detections, landmarks):\n    """"""Draw detections and landmarks on a frame""""""\n    for _, rect in enumerate(detections):\n        left, top, right, bottom = rect\n        cv.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), thickness=2)\n        for point in landmarks.reshape(-1, 2):\n            point = (int(left + point[0] * (right - left)), int(top + point[1] * (bottom - top)))\n            cv.circle(frame, point, 5, (255, 0, 0), -1)\n\n    return frame\n\n\ndef run_dumping(images_list, face_det, landmarks_regressor, vis_flag):\n    """"""Dumps detections and landmarks from images""""""\n    detected_num = 0\n    data = []\n    for path in tqdm(images_list, \'Dumping data\'):\n        image = cv.imread(path, cv.IMREAD_COLOR)\n        if image is None:\n            continue\n\n        detections = face_det.get_detections(image)\n        landmarks = None\n        if detections:\n            left, top, right, bottom = detections[0][0]\n            roi = image[top:bottom, left:right]\n            landmarks = landmarks_regressor.get_landmarks([roi]).reshape(-1)\n            data.append({\'path\': path, \'bbox\': detections[0][0], \'landmarks\': landmarks})\n            detected_num += 1\n            if vis_flag:\n                FivePointsAligner.align(roi, landmarks,\n                                        d_size=(200,200), normalize=False, show=True)\n        else:\n            data.append({\'path\': path, \'bbox\': None, \'landmarks\': None})\n\n    print(\'Detection ratio: \', float(detected_num) / len(data))\n\n    return data\n\n\ndef create_images_list(images_root, imgs_list):\n    input_filenames = []\n    input_dir = os.path.abspath(images_root)\n\n    if imgs_list is None:\n        stop = False\n        for path, _, files in os.walk(input_dir):\n            if stop:\n                break\n            for name in files:\n                if name.lower().endswith(\'.jpg\') or name.lower().endswith(\'.png\') \\\n                        or name.lower().endswith(\'.jpeg\') or name.lower().endswith(\'.gif\') \\\n                        or not \'.\' in name:\n                    filename = os.path.join(path, name)\n                    input_filenames.append(filename)\n    else:\n        with open(imgs_list) as f:\n            data = json.load(f)\n            for path in data[\'path\']:\n                filename = osp.join(images_root, path)\n                input_filenames.append(filename)\n\n    return input_filenames\n\n\ndef save_data(data, filename, root_dir):\n    print(\'Saving data...\')\n    with open(filename, \'w\') as f:\n        for instance in data:\n            line = osp.relpath(instance[\'path\'], start=root_dir) + \' | \'\n            if instance[\'bbox\'] is not None:\n                for x in instance[\'landmarks\']:\n                    line += str(x) + \' \'\n                line += \' | \'\n                left, top, right, bottom = instance[\'bbox\']\n                line += str(left) + \' \' + str(top) + \' \' + str(right - left) + \' \' + str(bottom - top)\n\n            f.write(line.strip() + \'\\n\')\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'--images_root\', type=str, default=None, required=True)\n    parser.add_argument(\'--images_list\', type=str, default=None, required=False)\n    parser.add_argument(\'--fd_proto\', type=str, default=\'../demo/face_detector/deploy_fd.prototxt\', help=\'\')\n    parser.add_argument(\'--fd_model\', type=str, default=\'../demo/face_detector/sq_300x300_iter_120000.caffemodel\',\n                        help=\'\')\n    parser.add_argument(\'--fr_thresh\', type=float, default=0.1)\n    parser.add_argument(\'--det_res\', type=int, nargs=2, default=[300, 300], help=\'Detection net input resolution.\')\n    parser.add_argument(\'--landnet_model\', type=str)\n    parser.add_argument(\'--device\', type=int, default=0)\n    parser.add_argument(\'--visualize\', action=\'store_true\')\n    args = parser.parse_args()\n\n    face_detector = FaceDetector(args.fd_proto, args.fd_model, conf=args.fr_thresh, size=args.det_res)\n\n    with torch.cuda.device(args.device):\n        landmarks_regressor = utils.load_model_state(landnet.LandmarksNet(), args.landnet_model, args.device)\n        data = run_dumping(create_images_list(args.images_root, args.images_list), face_detector,\n                           LandnetPT(landmarks_regressor), args.visualize)\n        save_data(data, osp.join(args.images_root, \'list.txt\'), args.images_root)\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/scripts/count_flops.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\n\nimport torch\nimport numpy as np\n\nfrom model.common import models_backbones, models_landmarks\nfrom utils.utils import get_model_parameters_number\n\n\ndef add_flops_counting_methods(net_main_module):\n    """"""Adds flops counting hooks to the specified module""""""\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module) # pylint: disable=E1111, E1120\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module) # pylint: disable=E1111, E1120\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module) # pylint: disable=E1111, E1120\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module) # pylint: disable=E1111, E1120\n\n    net_main_module.reset_flops_count()\n\n    # Adding variables necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    """"""\n\n    batches_count = self.__batch_counter__\n    flops_sum = 0\n    for module in self.modules():\n        if is_supported_instance(module):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    """"""\n    add_batch_counter_hook_function(self)\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    """"""\n    remove_batch_counter_hook_function(self)\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    """"""\n    add_batch_counter_variables_or_reset(self)\n    self.apply(add_flops_counter_variable_or_reset)\n\n\n# ---- Internal functions\ndef is_supported_instance(module):\n    """"""Internal auxiliary function""""""\n    if isinstance(module, (torch.nn.Conv2d, torch.nn.ReLU, torch.nn.PReLU, torch.nn.ELU, \\\n                           torch.nn.LeakyReLU, torch.nn.ReLU6, torch.nn.Linear, torch.nn.MaxPool2d, \\\n                           torch.nn.AvgPool2d, torch.nn.BatchNorm2d)):\n        return True\n\n    return False\n\n\ndef empty_flops_counter_hook(module, _, __):\n    """"""Internal auxiliary function""""""\n    module.__flops__ += 0\n\n\ndef relu_flops_counter_hook(module, input_, _):\n    """"""Counts flops in activations""""""\n    input_ = input_[0]\n    batch_size = input_.shape[0]\n    active_elements_count = batch_size\n    for val in input_.shape[1:]:\n        active_elements_count *= val\n\n    module.__flops__ += active_elements_count\n\n\ndef linear_flops_counter_hook(module, input_, output):\n    """"""Counts flops in linear layers""""""\n    input_ = input_[0]\n    batch_size = input_.shape[0]\n    module.__flops__ += batch_size * input_.shape[1] * output.shape[1]\n\n\ndef pool_flops_counter_hook(module, input_, _):\n    """"""Counts flops in max ind avg pooling layers""""""\n    input_ = input_[0]\n    module.__flops__ += np.prod(input_.shape)\n\n\ndef bn_flops_counter_hook(module, input_, _):\n    """"""Counts flops in batch normalization layers""""""\n    input_ = input_[0]\n\n    batch_flops = np.prod(input_.shape)\n    if module.affine:\n        batch_flops *= 2\n    module.__flops__ += batch_flops\n\n\ndef conv_flops_counter_hook(conv_module, input_, output):\n    """"""Counts flops in convolution layers""""""\n    # Can have multiple inputs, getting the first one\n    input_ = input_[0]\n\n    batch_size = input_.shape[0]\n    output_height, output_width = output.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = kernel_height * kernel_width * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * output_height * output_width\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += overall_flops\n\n\ndef batch_counter_hook(module, input_, _):\n    """"""Internal auxiliary function""""""\n    # Can have multiple inputs, getting the first one\n    input_ = input_[0]\n    batch_size = input_.shape[0]\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n    """"""Internal auxiliary function""""""\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    """"""Internal auxiliary function""""""\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    """"""Internal auxiliary function""""""\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    """"""Internal auxiliary function""""""\n    if is_supported_instance(module):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    """"""Internal auxiliary function""""""\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            return\n\n        if isinstance(module, torch.nn.Conv2d):\n            handle = module.register_forward_hook(conv_flops_counter_hook)\n        elif isinstance(module, (torch.nn.ReLU, torch.nn.PReLU, torch.nn.ELU,\n                                 torch.nn.LeakyReLU, torch.nn.ReLU6, torch.nn.Sigmoid)):\n            handle = module.register_forward_hook(relu_flops_counter_hook)\n        elif isinstance(module, torch.nn.Linear):\n            handle = module.register_forward_hook(linear_flops_counter_hook)\n        elif isinstance(module, (torch.nn.AvgPool2d, torch.nn.MaxPool2d)):\n            handle = module.register_forward_hook(pool_flops_counter_hook)\n        elif isinstance(module, torch.nn.BatchNorm2d):\n            handle = module.register_forward_hook(bn_flops_counter_hook)\n        else:\n            handle = module.register_forward_hook(empty_flops_counter_hook)\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    """"""Internal auxiliary function""""""\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n            del module.__flops_handle__\n\n\ndef add_flops_mask_variable_or_reset(module):\n    """"""Internal auxiliary function""""""\n    if is_supported_instance(module):\n        module.__mask__ = None\n\n\ndef flops_to_string(flops):\n    """"""Converts flops count to a human-readable form""""""\n    flops_str = \'\'\n    if flops // 10 ** 9 > 0:\n        flops_str = str(round(flops / 10. ** 9, 2)) + \'GMac\'\n    elif flops // 10 ** 6 > 0:\n        flops_str = str(round(flops / 10. ** 6, 2)) + \'MMac\'\n    elif flops // 10 ** 3 > 0:\n        flops_str = str(round(flops / 10. ** 3, 2)) + \'KMac\'\n    else:\n        flops_str = str(flops) + \'Mac\'\n    return flops_str\n\n\ndef main():\n    """"""Runs flops counter""""""\n    parser = argparse.ArgumentParser(description=\'Evaluation script for Face Recognition in PyTorch\')\n    parser.add_argument(\'--embed_size\', type=int, default=128, help=\'Size of the face embedding.\')\n    parser.add_argument(\'--device\', type=int, default=0, help=\'Device to store the model.\')\n    parser.add_argument(\'--model\', choices=list(models_backbones.keys()) + list(models_landmarks.keys()), type=str,\n                        default=\'rmnet\')\n    args = parser.parse_args()\n\n    with torch.cuda.device(args.device), torch.no_grad():\n        bs = 1\n        if args.model in models_landmarks.keys():\n            model = add_flops_counting_methods(models_landmarks[args.model]())\n            batch = torch.Tensor(bs, 3, *model.get_input_res())\n        else:\n            net = models_backbones[args.model](embedding_size=args.embed_size, feature=True)\n            batch = torch.Tensor(bs, 3, *net.get_input_res())\n            model = add_flops_counting_methods(net)\n\n        model.cuda().eval().start_flops_count()\n        output = model(batch.cuda())\n\n        print(model)\n        print(\'Output shape: {}\'.format(list(output.shape)))\n        print(\'Flops:  {}\'.format(flops_to_string(model.compute_average_flops_cost())))\n        print(\'Params: \' + get_model_parameters_number(model))\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/scripts/matio.py,1,"b'# pylint: skip-file\nimport struct\nimport numpy as np\n\ncv_type_to_dtype = {\n\t5 : np.dtype(\'float32\'),\n\t6 : np.dtype(\'float64\')\n}\n\ndtype_to_cv_type = {v : k for k,v in cv_type_to_dtype.items()}\n\ndef write_mat(f, m):\n    """"""Write mat m to file f""""""\n    if len(m.shape) == 1:\n        rows = m.shape[0]\n        cols = 1\n    else:\n        rows, cols = m.shape\n    header = struct.pack(\'iiii\', rows, cols, cols * 4, dtype_to_cv_type[m.dtype])\n    f.write(header)\n    f.write(m.data)\n\n\ndef read_mat(f):\n\t""""""\n\tReads an OpenCV mat from the given file opened in binary mode\n\t""""""\n\trows, cols, stride, type_ = struct.unpack(\'iiii\', f.read(4*4))\n\tmat = np.fromstring(f.read(rows*stride),dtype=cv_type_to_dtype[type_])\n\treturn mat.reshape(rows,cols)\n\ndef read_mkl_vec(f):\n\t""""""\n\tReads an OpenCV mat from the given file opened in binary mode\n\t""""""\n\t# Read past the header information\n\tf.read(4*4)\n\n\tlength, stride, type_ = struct.unpack(\'iii\', f.read(3*4))\n\tmat = np.fromstring(f.read(length*4),dtype=np.float32)\n\treturn mat\n\ndef load_mkl_vec(filename):\n\t""""""\n\tReads a OpenCV Mat from the given filename\n\t""""""\n\treturn read_mkl_vec(open(filename,\'rb\'))\n\ndef load_mat(filename):\n\t""""""\n\tReads a OpenCV Mat from the given filename\n\t""""""\n\treturn read_mat(open(filename,\'rb\'))\n\ndef save_mat(filename, m):\n    """"""Saves mat m to the given filename""""""\n    return write_mat(open(filename,\'wb\'), m)\n\ndef main():\n\tf = open(\'1_to_0.bin\',\'rb\')\n\tvx = read_mat(f)\n\tvy = read_mat(f)\n\nif __name__ == \'__main__\':\n\tmain()\n'"
pytorch_toolkit/face_recognition/scripts/plot_roc_curves_lfw.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\n\nimport matplotlib.pyplot as plt\nfrom evaluate_lfw import get_auc\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'rocs\', metavar=\'ROCs\', type=str, nargs=\'+\',\n                        help=\'paths to roc curves\')\n\n    args = parser.parse_args()\n\n    plt.xlabel(""False Positive Rate"")\n    plt.ylabel(""True Positive Rate"")\n    plt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\n    plt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'-\', alpha=0.2)\n    plt.minorticks_on()\n\n    for curve_file in args.rocs:\n        fprs = []\n        tprs = []\n        with open(curve_file, \'r\') as f:\n            for line in f.readlines():\n                values = line.strip().split()\n                fprs.append(float(values[1]))\n                tprs.append(float(values[0]))\n\n        curve_name = curve_file.split(\'/\')[-1].split(\'.\')[0]\n        plt.plot(fprs, tprs, label=curve_name)\n        plt.legend(loc=\'best\', fontsize=10)\n\n        print(\'AUC for {}: {}\'.format(curve_name, get_auc(fprs, tprs)))\n\n    plt.show()\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/scripts/pytorch2onnx.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport torch\n\nfrom utils.utils import load_model_state\nfrom model.common import models_backbones, models_landmarks\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Conversion script for FR models from PyTorch to ONNX\')\n    parser.add_argument(\'--embed_size\', type=int, default=128, help=\'Size of the face embedding.\')\n    parser.add_argument(\'--snap\', type=str, required=True, help=\'Snapshot to convert.\')\n    parser.add_argument(\'--device\', \'-d\', default=-1, type=int, help=\'Device for model placement.\')\n    parser.add_argument(\'--output_dir\', default=\'./\', type=str, help=\'Output directory.\')\n    parser.add_argument(\'--model\', choices=list(models_backbones.keys()) + list(models_landmarks.keys()),\n                        type=str, default=\'rmnet\')\n\n    args = parser.parse_args()\n\n    if args.model in models_landmarks.keys():\n        model = models_landmarks[args.model]()\n    else:\n        model = models_backbones[args.model](embedding_size=args.embed_size, feature=True)\n\n    model = load_model_state(model, args.snap, args.device, eval_state=True)\n    input_var = torch.rand(1, 3, *model.get_input_res())\n    dump_name = args.snap[args.snap.rfind(\'/\') + 1:-3]\n\n    torch.onnx.export(model, input_var, dump_name + \'.onnx\', verbose=True, export_params=True)\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/face_recognition/tests/__init__.py,0,b''
pytorch_toolkit/face_recognition/tests/test_alignment.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport unittest\nimport cv2 as cv\nimport numpy as np\n\nfrom utils.face_align import FivePointsAligner\nfrom utils.landmarks_augmentation import RandomRotate\n\n\nclass FaceAlignmentTests(unittest.TestCase):\n    """"""Tests for alignment methods""""""\n    def test_align_image(self):\n        """"""Synthetic test for alignment function""""""\n        image = np.zeros((128, 128, 3), dtype=np.float32)\n        for point in FivePointsAligner.ref_landmarks:\n            point_scaled = point * [128, 128]\n            cv.circle(image, tuple(point_scaled.astype(np.int)), 5, (255, 255, 255), cv.FILLED)\n\n        transform = RandomRotate(40., p=1.)\n        rotated_data = transform({\'img\': image, \'landmarks\': FivePointsAligner.ref_landmarks})\n        aligned_image = FivePointsAligner.align(rotated_data[\'img\'], \\\n                                                rotated_data[\'landmarks\'].reshape(-1),\n                                                d_size=(128, 128), normalized=True)\n\n        for point in FivePointsAligner.ref_landmarks:\n            point_scaled = (point * [128, 128]).astype(np.int)\n            check_sum = np.mean(aligned_image[point_scaled[1] - 3 : point_scaled[1] + 3,\n                                              point_scaled[0] - 3 : point_scaled[0] + 3])\n            self.assertGreaterEqual(check_sum, 220)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
pytorch_toolkit/face_recognition/tests/test_models.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport unittest\nimport os\nimport torch\n\nfrom model.common import models_backbones, models_landmarks\nfrom utils.utils import save_model_cpu, load_model_state\n\n\nclass BackbonesTests(unittest.TestCase):\n    """"""Tests for backbones""""""\n    def test_output_shape(self):\n        """"""Checks output shape""""""\n        embed_size = 256\n        for model_type in models_backbones.values():\n            model = model_type(embedding_size=embed_size, feature=True).eval()\n            batch = torch.Tensor(1, 3, *model.get_input_res()).uniform_()\n            output = model(batch)\n            self.assertEqual(list(output.shape), list((1, embed_size, 1, 1)))\n\n    def test_save_load_snap(self):\n        """"""Checks an ability to save and load model correctly""""""\n        embed_size = 256\n        snap_name = os.path.join(os.getcwd(), \'test_snap.pt\')\n        for model_type in models_backbones.values():\n            model = model_type(embedding_size=embed_size, feature=True).eval()\n            batch = torch.Tensor(1, 3, *model.get_input_res()).uniform_()\n            output = model(batch)\n            save_model_cpu(model, None, snap_name, 0, write_solverstate=False)\n\n            model_loaded = model_type(embedding_size=embed_size, feature=True)\n            load_model_state(model_loaded, snap_name, -1, eval_state=True)\n\n            output_loaded = model_loaded(batch)\n\n            self.assertEqual(torch.norm(output - output_loaded), 0)\n\n\nclass LandnetTests(unittest.TestCase):\n    """"""Tests for landmark regressor""""""\n    def test_output_shape(self):\n        """"""Checks output shape""""""\n        model = models_landmarks[\'landnet\']().eval()\n        batch = torch.Tensor(1, 3, *model.get_input_res())\n        output = model(batch)\n        self.assertEqual(list(output.shape), list((1, 10, 1, 1)))\n\n    def test_save_load_snap(self):\n        """"""Checks an ability to save and load model correctly""""""\n        snap_name = os.path.join(os.getcwd(), \'test_snap.pt\')\n        model = models_landmarks[\'landnet\']().eval()\n        batch = torch.Tensor(1, 3, *model.get_input_res()).uniform_()\n        output = model(batch)\n        save_model_cpu(model, None, snap_name, 0, write_solverstate=False)\n\n        model_loaded = models_landmarks[\'landnet\']()\n        load_model_state(model_loaded, snap_name, -1, eval_state=True)\n\n        output_loaded = model_loaded(batch)\n\n        self.assertEqual(torch.norm(output - output_loaded), 0)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
pytorch_toolkit/face_recognition/tests/test_utils.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport unittest\n\nimport torch\nfrom utils.utils import get_model_parameters_number\n\n\nclass UtilsTests(unittest.TestCase):\n    """"""Tests for utils""""""\n    def test_parameters_counter(self):\n        """"""Checks output of get_model_parameters_number""""""\n        class ParamsHolder(torch.nn.Module):\n            """"""Dummy parameters holder""""""\n            def __init__(self, n_params):\n                super(ParamsHolder, self).__init__()\n                self.p1 = torch.nn.Parameter(torch.Tensor(n_params // 2))\n                self.p2 = torch.nn.Parameter(torch.Tensor(n_params // 2))\n                self.dummy = -1\n\n        params_num = 1000\n        module = ParamsHolder(params_num)\n        estimated_params = get_model_parameters_number(module, as_string=False)\n        self.assertEqual(estimated_params, params_num)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
pytorch_toolkit/face_recognition/utils/__init__.py,0,b''
pytorch_toolkit/face_recognition/utils/augmentation.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\nimport torch\nimport numpy as np\nimport cv2 as cv\n\ntry:\n    from .face_align import FivePointsAligner\nexcept (ImportError, SystemError) as exp:\n    from face_align import FivePointsAligner\n\n\nclass HorizontalFlipNumpy:\n    """"""Horizontal flip augmentation with probability p""""""\n    def __init__(self, p=.5):\n        assert 0 <= p <= 1.\n        self.p = p\n\n    def __call__(self, img):\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            return cv.flip(img, 1)\n        return img\n\n\nclass ShowTransform:\n    """"""Show image using opencv""""""\n    def __call__(self, sample):\n        img = np.array(sample)\n        cv.imshow(\'image\', img)\n        cv.waitKey()\n        return sample\n\n\nclass NumpyToTensor:\n    """"""Converts a numpy array to torch.Tensor with optionally swapping R and B channels""""""\n    def __init__(self, switch_rb=False):\n        self.switch_rb = switch_rb\n\n    def __call__(self, image):\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        if self.switch_rb:\n            image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n        image = image.transpose((2, 0, 1))\n        return torch.from_numpy(image).type(torch.FloatTensor) / 255.\n\n\nclass RandomShiftNumpy:\n    """"""Shifts an image by a randomly generated offset along x and y axes""""""\n    def __init__(self, max_rel_shift, p=.5):\n        self.p = p\n        self.max_rel_shift = max_rel_shift\n\n    def __call__(self, image):\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            rel_shift = 2 * (torch.FloatTensor(1).uniform_() - .5) * self.max_rel_shift\n            h, w = image.shape[:2]\n            shift_w = w * rel_shift\n            shift_h = h * rel_shift\n            transl_mat = np.array([[1., 0., shift_w], [0., 1., shift_h]])\n            image = cv.warpAffine(image, transl_mat, (w, h))\n\n        return image\n\n\nclass RandomRotationNumpy:\n    """"""Rotates an image around it\'s center by a randomly generated angle""""""\n    def __init__(self, max_angle, p=.5):\n        self.max_angle = max_angle\n        self.p = p\n\n    def __call__(self, image):\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            angle = 2 * (torch.FloatTensor(1).uniform_() - .5) * self.max_angle\n            h, w = image.shape[:2]\n            rot_mat = cv.getRotationMatrix2D((w * 0.5, h * 0.5), angle, 1.)\n            image = cv.warpAffine(image, rot_mat, (w, h), flags=cv.INTER_LANCZOS4)\n\n        return image\n\n\nclass ResizeNumpy:\n    """"""Resizes an image in numpy format""""""\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, image):\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n        img = cv.resize(image, (new_h, new_w))\n        return img\n\n\nclass CenterCropNumpy:\n    """"""Performs a center crop of an images""""""\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, image):\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            new_h, new_w = self.output_size, self.output_size\n        else:\n            new_h, new_w = self.output_size\n\n        s_h = int(h / 2 - new_h / 2)\n        s_w = int(w / 2 - new_w / 2)\n        image = image[s_h: s_h + new_h, s_w: s_w + new_w]\n        return image\n\n\nclass BlurNumpy:\n    """"""Blurs an image with the given sigma and probability""""""\n    def __init__(self, p, k):\n        self.p = p\n        assert k % 2 == 1\n        self.k = k\n\n    def __call__(self, img):\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            img = cv.blur(img, (self.k, self.k))\n        return img\n\n\nclass CutOutWithPrior:\n    """"""Cuts rectangular patches from an image around pre-defined landmark locations""""""\n    def __init__(self, p, max_area):\n        self.p = p\n        self.max_area = max_area\n\n    # use after resize transform\n    def __call__(self, img):\n        height, width = img.shape[:2]\n        keypoints_ref = np.zeros((5, 2), dtype=np.float32)\n        keypoints_ref[:, 0] = FivePointsAligner.ref_landmarks[:, 0] * width\n        keypoints_ref[:, 1] = FivePointsAligner.ref_landmarks[:, 1] * height\n\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            erase_num = torch.LongTensor(1).random_(1, 4)\n            erase_ratio = torch.FloatTensor(1).uniform_(self.max_area / 2, self.max_area)\n            erase_h = math.sqrt(erase_ratio) / float(erase_num) * height\n            erase_w = math.sqrt(erase_ratio) / float(erase_num) * width\n\n            erased_idx = []\n            for _ in range(erase_num):\n                erase_pos = int(torch.LongTensor(1).random_(0, 5))\n                while erase_pos in erased_idx:\n                    erase_pos = int(torch.LongTensor(1).random_(0, 5))\n\n                left_corner = (\n                    int(keypoints_ref[erase_pos][0] - erase_h / 2), int(keypoints_ref[erase_pos][1] - erase_w / 2))\n                right_corner = (\n                    int(keypoints_ref[erase_pos][0] + erase_h / 2), int(keypoints_ref[erase_pos][1] + erase_w / 2))\n\n                cv.rectangle(img, tuple(left_corner), tuple(right_corner), (0, 0, 0), thickness=-1)\n                erased_idx.append(erase_pos)\n\n        return img\n'"
pytorch_toolkit/face_recognition/utils/face_align.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport cv2 as cv\nimport numpy as np\n\n\nclass FivePointsAligner():\n    """"""This class performs face alignmet by five reference points""""""\n    ref_landmarks = np.array([30.2946 / 96, 51.6963 / 112,\n                              65.5318 / 96, 51.5014 / 112,\n                              48.0252 / 96, 71.7366 / 112,\n                              33.5493 / 96, 92.3655 / 112,\n                              62.7299 / 96, 92.2041 / 112], dtype=np.float64).reshape(5, 2)\n    @staticmethod\n    def align(img, landmarks, d_size=(400, 400), normalized=False, show=False):\n        """"""Transforms given image in such a way that landmarks are located near ref_landmarks after transformation""""""\n        assert len(landmarks) == 10\n        assert isinstance(img, np.ndarray)\n        landmarks = np.array(landmarks).reshape(5, 2)\n        dw, dh = d_size\n\n        keypoints = landmarks.copy().astype(np.float64)\n        if normalized:\n            keypoints[:, 0] *= img.shape[1]\n            keypoints[:, 1] *= img.shape[0]\n\n        keypoints_ref = np.zeros((5, 2), dtype=np.float64)\n        keypoints_ref[:, 0] = FivePointsAligner.ref_landmarks[:, 0] * dw\n        keypoints_ref[:, 1] = FivePointsAligner.ref_landmarks[:, 1] * dh\n\n        transform_matrix = transformation_from_points(keypoints_ref, keypoints)\n        output_im = cv.warpAffine(img, transform_matrix, d_size, flags=cv.WARP_INVERSE_MAP)\n\n        if show:\n            tmp_output = output_im.copy()\n            for point in keypoints_ref:\n                cv.circle(tmp_output, (int(point[0]), int(point[1])), 5, (255, 0, 0), -1)\n            for point in keypoints:\n                cv.circle(img, (int(point[0]), int(point[1])), 5, (255, 0, 0), -1)\n            img = cv.resize(img, d_size)\n            cv.imshow(\'source/warped\', np.hstack((img, tmp_output)))\n            cv.waitKey()\n\n        return output_im\n\n\ndef transformation_from_points(points1, points2):\n    """"""Builds an affine transformation matrix form points1 to points2""""""\n    points1 = points1.astype(np.float64)\n    points2 = points2.astype(np.float64)\n\n    c1 = np.mean(points1, axis=0)\n    c2 = np.mean(points2, axis=0)\n    points1 -= c1\n    points2 -= c2\n\n    s1 = np.std(points1)\n    s2 = np.std(points2)\n    points1 /= s1\n    points2 /= s2\n\n    u, _, vt = np.linalg.svd(np.matmul(points1.T, points2))\n    r = np.matmul(u, vt).T\n\n    return np.hstack(((s2 / s1) * r, (c2.T - (s2 / s1) * np.matmul(r, c1.T)).reshape(2, -1)))\n'"
pytorch_toolkit/face_recognition/utils/ie_tools.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nimport os\n\nimport glog as log\nimport numpy as np\nfrom openvino.inference_engine import IENetwork, IEPlugin # pylint: disable=import-error,E0611\n\nclass IEModel:\n    """"""Class for inference of models in the Inference Engine format""""""\n    def __init__(self, exec_net, inputs_info, input_key, output_key):\n        self.net = exec_net\n        self.inputs_info = inputs_info\n        self.input_key = input_key\n        self.output_key = output_key\n\n    def forward(self, img):\n        """"""Performs forward pass of the wrapped IE model""""""\n        res = self.net.infer(inputs={self.input_key: np.expand_dims(img.transpose(2, 0, 1), axis=0)})\n        return np.copy(res[self.output_key])\n\n    def get_input_shape(self):\n        """"""Returns an input shape of the wrapped IE model""""""\n        return self.inputs_info[self.input_key]\n\n\ndef load_ie_model(model_xml, device, plugin_dir, cpu_extension=\'\'):\n    """"""Loads a model in the Inference Engine format""""""\n    model_bin = os.path.splitext(model_xml)[0] + "".bin""\n    # Plugin initialization for specified device and load extensions library if specified\n    plugin = IEPlugin(device=device, plugin_dirs=plugin_dir)\n    if cpu_extension and \'CPU\' in device:\n        plugin.add_cpu_extension(cpu_extension)\n    # Read IR\n    log.info(""Loading network files:\\n\\t%s\\n\\t%s"", model_xml, model_bin)\n    net = IENetwork(model=model_xml, weights=model_bin)\n\n    if ""CPU"" in plugin.device:\n        supported_layers = plugin.get_supported_layers(net)\n        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n        if not_supported_layers:\n            log.error(""Following layers are not supported by the plugin for specified device %s:\\n %s"",\n                      plugin.device, \', \'.join(not_supported_layers))\n            log.error(""Please try to specify cpu extensions library path in sample\'s command line parameters using -l ""\n                      ""or --cpu_extension command line argument"")\n            sys.exit(1)\n\n    assert len(net.inputs.keys()) == 1, ""Checker supports only single input topologies""\n    assert len(net.outputs) == 1, ""Checker supports only single output topologies""\n\n    log.info(""Preparing input blobs"")\n    input_blob = next(iter(net.inputs))\n    out_blob = next(iter(net.outputs))\n    net.batch_size = 1\n\n    # Loading model to the plugin\n    log.info(""Loading model to the plugin"")\n    exec_net = plugin.load(network=net)\n    model = IEModel(exec_net, net.inputs, input_blob, out_blob)\n    del net\n    return model\n'"
pytorch_toolkit/face_recognition/utils/landmarks_augmentation.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport cv2 as cv\nimport numpy as np\nimport torch\n\n\nclass Rescale:\n    """"""Resizes an image and corresponding landmarks""""""\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample[\'img\'], sample[\'landmarks\']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if w > h:\n                new_h, new_w = self.output_size, self.output_size * w / h\n            else:\n                new_h, new_w = self.output_size * h / w, self.output_size\n        else:\n            new_h, new_w = self.output_size\n        new_h, new_w = int(new_h), int(new_w)\n        img = cv.resize(image, (new_h, new_w))\n        return {\'img\': img, \'landmarks\': landmarks}\n\n\nclass RandomCrop:\n    """"""Makes a random crop from the source image with corresponding transformation of landmarks""""""\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample[\'img\'], sample[\'landmarks\'].reshape(-1, 2)\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        landmarks = landmarks - [left / float(w), top / float(h)]\n        for point in landmarks:\n            point[0] *= float(h) / new_h\n            point[1] *= float(w) / new_w\n\n        return {\'img\': image, \'landmarks\': landmarks}\n\n\nclass HorizontalFlip:\n    """"""Flips an input image and landmarks horizontally with a given probability""""""\n    def __init__(self, p=.5):\n        self.p = p\n\n    def __call__(self, sample):\n        image, landmarks = sample[\'img\'], sample[\'landmarks\'].reshape(-1, 2)\n\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            image = cv.flip(image, 1)\n            landmarks = landmarks.reshape(5, 2)\n            landmarks[:, 0] = 1. - landmarks[:, 0]\n            tmp = np.copy(landmarks[0])\n            landmarks[0] = landmarks[1]\n            landmarks[1] = tmp\n\n            tmp = np.copy(landmarks[3])\n            landmarks[3] = landmarks[4]\n            landmarks[4] = tmp\n\n        return {\'img\': image, \'landmarks\': landmarks}\n\n\nclass Blur:\n    """"""Blurs an image with the given sigma and probability""""""\n    def __init__(self, p, k):\n        self.p = p\n        assert k % 2 == 1\n        self.k = k\n\n    def __call__(self, sample):\n        image, landmarks = sample[\'img\'], sample[\'landmarks\']\n\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            image = cv.blur(image, (self.k, self.k))\n\n        return {\'img\': image, \'landmarks\': landmarks}\n\n\nclass Show:\n    """"""Show image using opencv""""""\n    def __call__(self, sample):\n        image, landmarks = sample[\'img\'].copy(), sample[\'landmarks\'].reshape(-1, 2)\n        h, w = image.shape[:2]\n        for point in landmarks:\n            cv.circle(image, (int(point[0]*w), int(point[1]*h)), 3, (255, 0, 0), -1)\n        cv.imshow(\'image\', image)\n        cv.waitKey()\n        return sample\n\n\nclass RandomRotate:\n    """"""\n        Rotates an image around it\'s center by a randomly generated angle.\n        Also performs the same transformation with landmark points.\n    """"""\n    def __init__(self, max_angle, p=.5):\n        self.max_angle = max_angle\n        self.p = p\n\n    def __call__(self, sample):\n        image, landmarks = sample[\'img\'], sample[\'landmarks\']\n\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            angle = 2*(torch.FloatTensor(1).uniform_() - .5)*self.max_angle\n            h, w = image.shape[:2]\n            rot_mat = cv.getRotationMatrix2D((w*0.5, h*0.5), angle, 1.)\n            image = cv.warpAffine(image, rot_mat, (w, h), flags=cv.INTER_LANCZOS4)\n            rot_mat_l = cv.getRotationMatrix2D((0.5, 0.5), angle, 1.)\n            landmarks = cv.transform(landmarks.reshape(1, 5, 2), rot_mat_l).reshape(5, 2)\n\n        return {\'img\': image, \'landmarks\': landmarks}\n\n\nclass ToTensor:\n    """"""Convert ndarrays in sample to Tensors.""""""\n    def __init__(self, switch_rb=False):\n        self.switch_rb = switch_rb\n\n    def __call__(self, sample):\n        image, landmarks = sample[\'img\'], sample[\'landmarks\']\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        if self.switch_rb:\n            image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n        image = image.transpose((2, 0, 1))\n        return {\'img\': torch.from_numpy(image).type(torch.FloatTensor) / 255,\n                \'landmarks\': torch.from_numpy(landmarks).type(torch.FloatTensor).view(-1, 1, 1)}\n\n\nclass RandomScale:\n    """"""Performs uniform scale with a random magnitude""""""\n    def __init__(self, max_scale, min_scale, p=.5):\n        self.max_scale = max_scale\n        self.min_scale = min_scale\n        self.p = p\n\n    def __call__(self, sample):\n        image, landmarks = sample[\'img\'], sample[\'landmarks\']\n\n        if float(torch.FloatTensor(1).uniform_()) < self.p:\n            scale = self.min_scale + torch.FloatTensor(1).uniform_()*(self.max_scale - self.min_scale)\n            h, w = image.shape[:2]\n            rot_mat = cv.getRotationMatrix2D((w*0.5, h*0.5), 0, scale)\n            image = cv.warpAffine(image, rot_mat, (w, h), flags=cv.INTER_LANCZOS4)\n            rot_mat_l = cv.getRotationMatrix2D((0.5, 0.5), 0, scale)\n            landmarks = cv.transform(landmarks.reshape(1, 5, 2), rot_mat_l).reshape(5, 2)\n\n        return {\'img\': image, \'landmarks\': landmarks}\n'"
pytorch_toolkit/face_recognition/utils/parser_yaml.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom argparse import ArgumentParser\nimport yaml\n\nclass ArgumentParserWithYaml(ArgumentParser):\n    """"""\n    Attention, this will work with simple yaml files only, and if there is no action=store_false\n    """"""\n    @staticmethod\n    def _check_arg_line_repr_None(arg_line, k, v):\n        """""" The method is required, since by default python prints None value as None, whereas yaml waiths for null """"""\n        s = arg_line.strip()\n        prefixes = [k, ""\'"" + k + ""\'"", \'""\' + k + \'""\']\n        is_ok = False\n        for prefix in prefixes:\n            if s.startswith(prefix):\n                s = s[len(prefix):]\n                is_ok = True\n                break\n        if not is_ok:\n            raise RuntimeError(""Unknown prefix in line \'{}\', k = \'{}\', v = \'{}\'"".format(arg_line, k, v))\n        s = s.strip()\n        assert s.startswith(\':\'), ""Bad format of line \'{}\', k = \'{}\', v = \'{}\'"".format(arg_line, k, v)\n        s = s[1:]\n        s = s.strip()\n        #print(""arg line \'{}\' repr None = {}, s = \'{}\'"".format(arg_line, s == ""None"", s))\n\n        return s == ""None"" #note that \'None\' will be a string, whereas just None will be None\n\n    def convert_arg_line_to_args(self, arg_line):\n        arg_line = arg_line.strip()\n        if not arg_line:\n            return []\n        if arg_line.endswith(\',\'):\n            arg_line = arg_line[:-1]\n\n        data = yaml.load(arg_line)\n        if data is None:\n            return []\n        assert type(data) is dict\n        assert len(data) == 1\n\n        res = []\n        for k, v in data.items():\n            if v == \'None\': # default value is None -- skipping\n                if self._check_arg_line_repr_None(arg_line, k, v): #additional check that somebody passed string ""None""\n                    continue\n                else:\n                    print(""WARNING: DURING PARSING ARGUMENTS FILE: possible error in the argument line \'{}\' -- probably None value is missed"".format(arg_line))\n\n            if type(v) is list:\n                res.append(\'--\' + str(k))\n                [res.append(str(item)) for item in v]\n                continue\n\n            if type(v) is bool: # special case, action=store_true, do not use store_false!\n                if v:\n                    res.append(\'--\' + str(k))\n                continue\n\n            # attention, there may be small issue with converting float -> string -> float -> string\n            res.extend([\'--\' + str(k), str(v)])\n\n        return res\n'"
pytorch_toolkit/face_recognition/utils/utils.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\ndef save_model_cpu(net, optim, ckpt_fname, epoch, write_solverstate=False):\n    """"""Saves model weights and optimizer state (optionally) to a file""""""\n    state_dict = net.state_dict()\n    for key in state_dict.keys():\n        state_dict[key] = state_dict[key].cpu()\n    snapshot_dict = {\n        \'epoch\': epoch,\n        \'state_dict\': state_dict}\n\n    if write_solverstate:\n        snapshot_dict[\'optimizer\'] = optim\n\n    torch.save(snapshot_dict, ckpt_fname)\n\n\ndef get_model_parameters_number(model, as_string=True):\n    """"""Returns a total number of trainable parameters in a specified model""""""\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if not as_string:\n        return params_num\n\n    if params_num // 10 ** 6 > 0:\n        flops_str = str(round(params_num / 10. ** 6, 2)) + \'M\'\n    elif params_num // 10 ** 3 > 0:\n        flops_str = str(round(params_num / 10. ** 3, 2)) + \'k\'\n    else:\n        flops_str = str(params_num)\n    return flops_str\n\n\ndef load_model_state(model, snap, device_id, eval_state=True):\n    """"""Loads model weight from a file produced by save_model_cpu""""""\n    if device_id != -1:\n        location = \'cuda:\' + str(device_id)\n    else:\n        location = \'cpu\'\n    state_dict = torch.load(snap, map_location=location)[\'state_dict\']\n\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        head = k[:7]\n        if head == \'module.\':\n            name = k[7:]  # remove `module.`\n        else:\n            name = k\n        new_state_dict[name] = v\n\n    model.load_state_dict(new_state_dict, strict=False)\n\n    if device_id != -1:\n        model.cuda(device_id)\n        cudnn.benchmark = True\n\n    if eval_state:\n        model.eval()\n    else:\n        model.train()\n\n    return model\n\n\ndef flip_tensor(x, dim):\n    """"""Flips a tensor along the specified axis""""""\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1) - 1, -1, -1),\n                                                    (\'cpu\', \'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n'"
pytorch_toolkit/human_pose_estimation/datasets/__init__.py,0,b''
pytorch_toolkit/human_pose_estimation/datasets/coco.py,0,"b""import copy\nimport json\nimport math\nimport os\nimport pickle\n\nimport cv2\nimport numpy as np\nimport pycocotools\n\nfrom torch.utils.data.dataset import Dataset\n\nBODY_PARTS_KPT_IDS = [[1, 8], [8, 9], [9, 10], [1, 11], [11, 12], [12, 13], [1, 2], [2, 3], [3, 4], [2, 16],\n                      [1, 5], [5, 6], [6, 7], [5, 17], [1, 0], [0, 14], [0, 15], [14, 16], [15, 17]]\n\n\ndef get_mask(segmentations, mask):\n    for segmentation in segmentations:\n        rle = pycocotools.mask.frPyObjects(segmentation, mask.shape[0], mask.shape[1])\n        mask[pycocotools.mask.decode(rle) > 0.5] = 0\n    return mask\n\n\nclass CocoTrainDataset(Dataset):\n    def __init__(self, labels, images_folder, stride, sigma, paf_thickness, transform=None):\n        super().__init__()\n        self._images_folder = images_folder\n        self._stride = stride\n        self._sigma = sigma\n        self._paf_thickness = paf_thickness\n        self._transform = transform\n        with open(labels, 'rb') as f:\n            self._labels = pickle.load(f)\n\n    def __getitem__(self, idx):\n        label = copy.deepcopy(self._labels[idx])  # label modified in transform\n        image = cv2.imread(os.path.join(self._images_folder, label['img_paths']), cv2.IMREAD_COLOR)\n        mask = np.ones(shape=(label['img_height'], label['img_width']), dtype=np.float32)\n        mask = get_mask(label['segmentations'], mask)\n        sample = {\n            'label': label,\n            'image': image,\n            'mask': mask\n        }\n        if self._transform:\n            sample = self._transform(sample)\n\n        mask = cv2.resize(sample['mask'], dsize=None, fx=1/self._stride, fy=1/self._stride, interpolation=cv2.INTER_AREA)\n        keypoint_maps = self.generate_keypoint_maps(sample)\n        sample['keypoint_maps'] = keypoint_maps\n        keypoint_mask = np.zeros(shape=keypoint_maps.shape, dtype=np.float32)\n        for idx in range(keypoint_mask.shape[0]):\n            keypoint_mask[idx] = mask\n        sample['keypoint_mask'] = keypoint_mask\n\n        paf_maps = self.generate_paf_maps(sample)\n        sample['paf_maps'] = paf_maps\n        paf_mask = np.zeros(shape=paf_maps.shape, dtype=np.float32)\n        for idx in range(paf_mask.shape[0]):\n            paf_mask[idx] = mask\n        sample['paf_mask'] = paf_mask\n\n        image = sample['image'].astype(np.float32)\n        image = (image - 128) / 256\n        sample['image'] = image.transpose((2, 0, 1))\n        return sample\n\n    def __len__(self):\n        return len(self._labels)\n\n    def generate_keypoint_maps(self, sample):\n        n_keypoints = 18\n        n_rows, n_cols, _ = sample['image'].shape\n        keypoint_maps = np.zeros(shape=(n_keypoints + 1,\n                                        n_rows // self._stride, n_cols // self._stride), dtype=np.float32)  # +1 for bg\n\n        label = sample['label']\n        for keypoint_idx in range(n_keypoints):\n            keypoint = label['keypoints'][keypoint_idx]\n            if keypoint[2] <= 1:\n                CocoTrainDataset.add_gaussian(keypoint_maps[keypoint_idx], keypoint[0], keypoint[1], self._stride, self._sigma)\n            for another_annotation in label['processed_other_annotations']:\n                keypoint = another_annotation['keypoints'][keypoint_idx]\n                if keypoint[2] <= 1:\n                    CocoTrainDataset.add_gaussian(keypoint_maps[keypoint_idx], keypoint[0], keypoint[1], self._stride, self._sigma)\n        keypoint_maps[-1] = 1 - keypoint_maps.max(axis=0)\n        return keypoint_maps\n\n    @staticmethod\n    def add_gaussian(keypoint_map, x, y, stride, sigma):\n        n_sigma = 4\n        tl = [int(x - n_sigma * sigma), int(y - n_sigma * sigma)]\n        tl[0] = max(tl[0], 0)\n        tl[1] = max(tl[1], 0)\n\n        br = [int(x + n_sigma * sigma), int(y + n_sigma * sigma)]\n        map_h, map_w = keypoint_map.shape\n        br[0] = min(br[0], map_w * stride)\n        br[1] = min(br[1], map_h * stride)\n\n        shift = stride / 2 - 0.5\n        for map_y in range(tl[1] // stride, br[1] // stride):\n            for map_x in range(tl[0] // stride, br[0] // stride):\n                d2 = (map_x * stride + shift - x) * (map_x * stride + shift - x) + \\\n                    (map_y * stride + shift - y) * (map_y * stride + shift - y)\n                exponent = d2 / 2 / sigma / sigma\n                if exponent > 4.6052:  # threshold, ln(100), ~0.01\n                    continue\n                keypoint_map[map_y, map_x] += math.exp(-exponent)\n                if keypoint_map[map_y, map_x] > 1:\n                    keypoint_map[map_y, map_x] = 1\n\n    def generate_paf_maps(self, sample):\n        n_pafs = len(BODY_PARTS_KPT_IDS)\n        n_rows, n_cols, _ = sample['image'].shape\n        paf_maps = np.zeros(shape=(n_pafs * 2, n_rows // self._stride, n_cols // self._stride), dtype=np.float32)\n\n        label = sample['label']\n        for paf_idx in range(n_pafs):\n            keypoint_a = label['keypoints'][BODY_PARTS_KPT_IDS[paf_idx][0]]\n            keypoint_b = label['keypoints'][BODY_PARTS_KPT_IDS[paf_idx][1]]\n            if keypoint_a[2] <= 1 and keypoint_b[2] <= 1:\n                CocoTrainDataset.set_paf(paf_maps[paf_idx * 2:paf_idx * 2 + 2],\n                                         keypoint_a[0], keypoint_a[1], keypoint_b[0], keypoint_b[1],\n                                         self._stride, self._paf_thickness)\n            for another_annotation in label['processed_other_annotations']:\n                keypoint_a = another_annotation['keypoints'][BODY_PARTS_KPT_IDS[paf_idx][0]]\n                keypoint_b = another_annotation['keypoints'][BODY_PARTS_KPT_IDS[paf_idx][1]]\n                if keypoint_a[2] <= 1 and keypoint_b[2] <= 1:\n                    CocoTrainDataset.set_paf(paf_maps[paf_idx * 2:paf_idx * 2 + 2],\n                                             keypoint_a[0], keypoint_a[1], keypoint_b[0], keypoint_b[1],\n                                             self._stride, self._paf_thickness)\n        return paf_maps\n\n    @staticmethod\n    def set_paf(paf_map, x_a, y_a, x_b, y_b, stride, thickness):\n        x_a /= stride\n        y_a /= stride\n        x_b /= stride\n        y_b /= stride\n        x_ba = x_b - x_a\n        y_ba = y_b - y_a\n        _, h_map, w_map = paf_map.shape\n        x_min = int(max(min(x_a, x_b) - thickness, 0))\n        x_max = int(min(max(x_a, x_b) + thickness, w_map))\n        y_min = int(max(min(y_a, y_b) - thickness, 0))\n        y_max = int(min(max(y_a, y_b) + thickness, h_map))\n        norm_ba = (x_ba * x_ba + y_ba * y_ba) ** 0.5\n        if norm_ba < 1e-7:  # Same points, no paf\n            return\n        x_ba /= norm_ba\n        y_ba /= norm_ba\n\n        for y in range(y_min, y_max):\n            for x in range(x_min, x_max):\n                x_ca = x - x_a\n                y_ca = y - y_a\n                d = math.fabs(x_ca * y_ba - y_ca * x_ba)\n                if d <= thickness:\n                    paf_map[0, y, x] = x_ba\n                    paf_map[1, y, x] = y_ba\n\n\nclass CocoValDataset(Dataset):\n    def __init__(self, labels, images_folder):\n        super().__init__()\n        with open(labels, 'r') as f:\n            self._labels = json.load(f)\n        self._images_folder = images_folder\n\n    def __getitem__(self, idx):\n        file_name = self._labels['images'][idx]['file_name']\n        img = cv2.imread(os.path.join(self._images_folder, file_name), cv2.IMREAD_COLOR)\n        return {\n            'img': img,\n            'file_name': file_name\n        }\n\n    def __len__(self):\n        return len(self._labels['images'])\n"""
pytorch_toolkit/human_pose_estimation/datasets/coco_single.py,0,"b""import os\nimport json\nimport copy\n\nimport cv2\nimport numpy as np\nfrom torch.utils.data.dataset import Dataset\n\ndef preprocess_bbox(bbox, image):\n    aspect_ratio = 0.75\n    bbox[0] = np.max((0, bbox[0]))\n    bbox[1] = np.max((0, bbox[1]))\n    x2 = np.min((image.shape[1] - 1, bbox[0] + np.max((0, bbox[2] - 1))))\n    y2 = np.min((image.shape[0] - 1, bbox[1] + np.max((0, bbox[3] - 1))))\n\n    if x2 >= bbox[0] and y2 >= bbox[1]:\n        bbox = [bbox[0], bbox[1], x2 - bbox[0], y2 - bbox[1]]\n\n    cx_bbox = bbox[0] + bbox[2] * 0.5\n    cy_bbox = bbox[1] + bbox[3] * 0.5\n    center = np.array([np.float32(cx_bbox), np.float32(cy_bbox)])\n\n    if bbox[2] > aspect_ratio * bbox[3]:\n        bbox[3] = bbox[2] * 1.0 / aspect_ratio\n    elif bbox[2] < aspect_ratio * bbox[3]:\n        bbox[2] = bbox[3] * aspect_ratio\n\n    s = np.array([bbox[2] / 200., bbox[3] / 200.], np.float32)\n    scale = s * 1.25\n\n    return center, scale\n\nclass CocoSingleTrainDataset(Dataset):\n\n    right_keypoints_indice = [6, 7, 8, 12, 13, 14, 18, 19, 20, 24, 25, 26, 30,\n                                    31, 32, 36, 37, 38, 42, 43, 44, 48, 49, 50]\n    left_keypoints_indice = [3, 4, 5, 9, 10, 11, 15, 16, 17, 21, 22, 23, 27,\n                                   28, 29, 33, 34, 35, 39, 40, 41, 45, 46, 47]\n\n    def __init__(self, dataset_folder, input_size_img=(288, 384), stride=(8, 8), sigma=3, transform=None):\n        super().__init__()\n        self._num_keypoints = 17\n        self._dataset_folder = dataset_folder\n        self._input_size_img = input_size_img\n        self._stride = stride\n        self._sigma = sigma\n        self._transform = transform\n        self._aspect_ratio = self._input_size_img[0] / self._input_size_img[1]\n        self._heatmap_size = [self._input_size_img[0] // self._stride[0], self._input_size_img[1] // self._stride[1]]\n        with open(os.path.join(self._dataset_folder, 'annotations', 'person_keypoints_train2017_converted.json')) as f:\n            self._labels = json.load(f)\n\n    def __getitem__(self, idx):\n        image_path = self._labels['annotations'][idx]['image_path']\n        image = cv2.imread(os.path.join(self._dataset_folder, 'train2017', image_path), cv2.IMREAD_COLOR)\n        tokens = self._labels['annotations'][idx]['keypoints']\n        bbox = copy.deepcopy(self._labels['annotations'][idx]['bbox'])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        center, scale = preprocess_bbox(bbox, image)\n        rotate = 0\n\n        keypoints = np.zeros(self._num_keypoints * 3, dtype=np.float32)\n\n        for id in range(self._num_keypoints):\n            if tokens[id * 3] != 0:\n                keypoints[id * 3] = int(tokens[id*3])          # x\n                keypoints[id * 3 + 1] = int(tokens[id*3 + 1])  # y\n                if tokens[id * 3 + 2] > 0:\n                    keypoints[id * 3 + 2] = 1  # visible == 1, not visible == 0\n\n        sample = {\n            'keypoints': keypoints,\n            'image': image,\n            'center': center,\n            'scale': scale,\n            'rotate': rotate\n        }\n\n        if self._transform:\n            sample = self._transform(sample)\n\n        keypoint_maps = self._generate_keypoint_maps(sample)\n        sample['keypoint_maps'] = keypoint_maps\n\n        sample['image'] = sample['image'].transpose(2, 0, 1)\n\n        return sample\n\n    def __len__(self):\n        return len(self._labels['annotations'])\n\n    def _generate_keypoint_maps(self, sample):\n        keypoints = sample['keypoints']\n        target = np.zeros((len(keypoints) // 3, self._heatmap_size[1], self._heatmap_size[0]), dtype=np.float32)\n        eps = self._sigma * self._sigma\n        for i in range(self._num_keypoints):\n            if keypoints[i * 3 + 2] > 0:  # visible\n                x = int(keypoints[i * 3] / self._stride[0] + 0.5)\n                y = int(keypoints[i * 3 + 1] / self._stride[1] + 0.5)\n                lt = [x - eps, y - eps]\n                rb = [x + eps + 1, y + eps + 1]\n                if lt[0] >= self._heatmap_size[0] or lt[1] >= self._heatmap_size[1] \\\n                        or rb[0] < 0 or rb[1] < 0:\n                    keypoints[i * 3 + 2] = 0\n                    continue\n                grid_x = np.arange(0, 2 * eps + 1, 1, np.float32)\n                grid_y = grid_x[:, None]\n                x0 = (2 * eps + 1) // 2\n                y0 = x0\n                gaussian = np.exp(- ((grid_x - x0) ** 2 + (grid_y - y0) ** 2) / (2 * self._sigma ** 2))\n                lt_heatmap = [max(0, lt[0]), max(0, lt[1])]\n                rb_heatmap = [min(rb[0], self._heatmap_size[0]), min(rb[1], self._heatmap_size[1])]\n\n                lt_gaussian = [max(0, -lt[0]), max(0, -lt[1])]\n                rb_gaussian = [min(rb[0], self._heatmap_size[0]) - lt[0], min(rb[1], self._heatmap_size[1]) - lt[1]]\n\n                if keypoints[i * 3 + 1] > 0.5:\n                    target[i][lt_heatmap[1]:rb_heatmap[1], lt_heatmap[0]:rb_heatmap[0]] = \\\n                        gaussian[lt_gaussian[1]:rb_gaussian[1], lt_gaussian[0]:rb_gaussian[0]]\n\n        return target\n\nclass CocoSingleValDataset(Dataset):\n    def __init__(self, dataset_folder, num_images=None, transform=None):\n        super().__init__()\n        self._num_keypoints = 17\n        self._dataset_folder = dataset_folder\n        self._transform = transform\n        with open(os.path.join(self._dataset_folder, 'annotations', 'person_keypoints_val2017.json')) as f:\n            data = json.load(f)\n\n        self._annotations = data['annotations']\n\n        if num_images is not None:\n            self._annotations = data['annotations'][:num_images]\n\n        self._images = data['images']\n\n    def __getitem__(self, idx):\n        image_path = next(it['file_name'] for it in self._images if it['id'] == self._annotations[idx]['image_id'])\n        image = cv2.imread(os.path.join(self._dataset_folder, 'val2017', image_path), cv2.IMREAD_COLOR)\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        bbox = self._annotations[idx]['bbox']\n        center, scale = preprocess_bbox(bbox, image)\n        rotate = 0\n\n        sample = {\n            'image': image,\n            'image_id': self._annotations[idx]['image_id'],\n            'bbox': bbox,\n            'center': center,\n            'scale': scale,\n            'rotate': rotate\n        }\n\n        if self._transform:\n            sample = self._transform(sample)\n\n        sample['image'] = sample['image'].transpose(2, 0, 1)\n        return sample\n\n    def __len__(self):\n        return len(self._annotations)\n"""
pytorch_toolkit/human_pose_estimation/datasets/lip.py,0,"b""import math\nimport os\nimport random\n\nimport cv2\nimport numpy as np\n\nfrom torch.utils.data.dataset import Dataset\n\nfrom datasets.coco import CocoTrainDataset\n\n\nclass LipTrainDataset(Dataset):\n\n    right_keypoints_indice = [0, 1, 2, 3, 4, 5, 6, 7, 8, 30, 31, 32, 33, 34, 35, 36, 37, 38]\n    left_keypoints_indice = [15, 16, 17, 12, 13, 14, 9, 10, 11, 45, 46, 47, 42, 43, 44, 39, 40, 41]\n\n    def __init__(self, dataset_folder, stride, sigma, transform=None):\n        super().__init__()\n        self._num_keypoints = 16\n        self._dataset_folder = dataset_folder\n        self._stride = stride\n        self._sigma = sigma\n        self._transform = transform\n        self._labels = [line.rstrip('\\n') for line in\n                        open(os.path.join(self._dataset_folder, 'TrainVal_pose_annotations', 'lip_train_set.csv'), 'r')]\n\n    def __getitem__(self, idx):\n        tokens = self._labels[idx].split(',')\n        image = cv2.imread(os.path.join(self._dataset_folder, 'TrainVal_images', 'train_images', tokens[0]), cv2.IMREAD_COLOR)\n        keypoints = np.ones(self._num_keypoints*3, dtype=np.float32) * -1\n        for id in range(keypoints.shape[0]//3):\n            if tokens[1 + id*3] != 'nan':\n                keypoints[id * 3] = int(tokens[1 + id*3])          # x\n                keypoints[id * 3 + 1] = int(tokens[1 + id*3 + 1])  # y\n                keypoints[id * 3 + 2] = 1                          # visible == 1, not visible == 0\n                if int(tokens[1 + id*3 + 2]) == 1:\n                    keypoints[id * 3 + 2] = 0\n\n        sample = {\n            'keypoints': keypoints,\n            'image': image,\n        }\n        if self._transform:\n            sample = self._transform(sample)\n\n        keypoint_maps = self._generate_keypoint_maps(sample)\n        sample['keypoint_maps'] = keypoint_maps\n\n        image = sample['image'].astype(np.float32)\n        image = (image - 128) / 256\n        sample['image'] = image.transpose((2, 0, 1))\n        return sample\n\n    def __len__(self):\n        return len(self._labels)\n\n    def _generate_keypoint_maps(self, sample):\n        n_rows, n_cols, _ = sample['image'].shape\n        keypoint_maps = np.zeros(shape=(self._num_keypoints + 1,\n                                        n_rows // self._stride, n_cols // self._stride), dtype=np.float32)  # +1 for bg\n\n        keypoints = sample['keypoints']\n        for id in range(len(keypoints) // 3):\n            if keypoints[id * 3] == -1:\n                continue\n            CocoTrainDataset.add_gaussian(keypoint_maps[id], keypoints[id * 3], keypoints[id * 3 + 1], self._stride, self._sigma)\n        keypoint_maps[-1] = 1 - keypoint_maps.max(axis=0)\n\n        return keypoint_maps\n\n\nclass LipValDataset(Dataset):\n    def __init__(self, dataset_folder, num_images=-1):\n        super().__init__()\n        self._num_keypoints = 16\n        self._dataset_folder = dataset_folder\n        self.labels_file_path = os.path.join(self._dataset_folder, 'TrainVal_pose_annotations', 'lip_val_set.csv')\n        self._labels = [line.rstrip('\\n') for line in open(self.labels_file_path, 'r')]\n        if num_images > 0:\n            self._labels = self._labels[:num_images]\n\n    def __getitem__(self, id):\n        tokens = self._labels[id].split(',')\n        image = cv2.imread(os.path.join(self._dataset_folder, 'TrainVal_images', 'val_images', tokens[0]), cv2.IMREAD_COLOR)\n        sample = {\n            'image': image,\n            'file_name': tokens[0]\n        }\n        return sample\n\n    def __len__(self):\n        return len(self._labels)\n\n\nclass LipTestDataset(Dataset):\n    def __init__(self, dataset_folder):\n        super().__init__()\n        self._num_keypoints = 16\n        self._dataset_folder = dataset_folder\n        self._names = [line.rstrip('\\n') for line in open(os.path.join(self._dataset_folder, 'Testing_images', 'test_id.txt'), 'r')]\n\n    def __getitem__(self, id):\n        name = '{}.jpg'.format(self._names[id])\n        img = cv2.imread(os.path.join(self._dataset_folder, 'Testing_images', 'testing_images', name))\n        sample = {\n            'image': img,\n            'file_name': name\n        }\n        return sample\n\n    def __len__(self):\n        return len(self._names)\n"""
pytorch_toolkit/human_pose_estimation/datasets/transformations.py,0,"b""import random\nimport copy\n\nimport cv2\nimport numpy as np\n\n\nclass ConvertKeypoints(object):\n    def __call__(self, sample):\n        label = sample['label']\n        h, w, _ = sample['image'].shape\n        keypoints = label['keypoints']\n        for keypoint in keypoints:  # keypoint[2] == 0: occluded, == 1: visible, == 2: not in image\n            if keypoint[0] == keypoint[1] == 0:\n                keypoint[2] = 2\n            if (keypoint[0] < 0\n                    or keypoint[0] >= w\n                    or keypoint[1] < 0\n                    or keypoint[1] >= h):\n                keypoint[2] = 2\n        for other_label in label['processed_other_annotations']:\n            keypoints = other_label['keypoints']\n            for keypoint in keypoints:\n                if keypoint[0] == keypoint[1] == 0:\n                    keypoint[2] = 2\n                if (keypoint[0] < 0\n                        or keypoint[0] >= w\n                        or keypoint[1] < 0\n                        or keypoint[1] >= h):\n                    keypoint[2] = 2\n        label['keypoints'] = self._convert(label['keypoints'], w, h)\n\n        for other_label in label['processed_other_annotations']:\n            other_label['keypoints'] = self._convert(other_label['keypoints'], w, h)\n        return sample\n\n    def _convert(self, keypoints, w, h):\n        # Nose, Neck, R hand, L hand, R leg, L leg, Eyes, Ears\n        reorder_map = [1, 7, 9, 11, 6, 8, 10, 13, 15, 17, 12, 14, 16, 3, 2, 5, 4]\n        converted_keypoints = list(keypoints[i - 1] for i in reorder_map)\n        converted_keypoints.insert(1, [(keypoints[5][0] + keypoints[6][0]) / 2,\n                                       (keypoints[5][1] + keypoints[6][1]) / 2, 0])  # Add neck as a mean of shoulders\n        if keypoints[5][2] == 2 and keypoints[6][2] == 2:\n            converted_keypoints[1][2] = 2\n        elif keypoints[5][2] == 3 and keypoints[6][2] == 3:\n            converted_keypoints[1][2] = 3\n        elif keypoints[5][2] == 1 and keypoints[6][2] == 1:\n            converted_keypoints[1][2] = 1\n        if (converted_keypoints[1][0] < 0\n                or converted_keypoints[1][0] >= w\n                or converted_keypoints[1][1] < 0\n                or converted_keypoints[1][1] >= h):\n            converted_keypoints[1][2] = 2\n        return converted_keypoints\n\n\nclass Scale(object):\n    def __init__(self, prob=1, min_scale=0.5, max_scale=1.1, target_dist=0.6):\n        self._prob = prob\n        self._min_scale = min_scale\n        self._max_scale = max_scale\n        self._target_dist = target_dist\n\n    def __call__(self, sample):\n        prob = random.random()\n        scale_multiplier = 1\n        if prob <= self._prob:\n            prob = random.random()\n            scale_multiplier = (self._max_scale - self._min_scale) * prob + self._min_scale\n        label = sample['label']\n        scale_abs = self._target_dist / label['scale_provided']\n        scale = scale_abs * scale_multiplier\n        sample['image'] = cv2.resize(sample['image'], dsize=(0, 0), fx=scale, fy=scale)\n        label['img_height'], label['img_width'], _ = sample['image'].shape\n        sample['mask'] = cv2.resize(sample['mask'], dsize=(0, 0), fx=scale, fy=scale)\n\n        label['objpos'][0] *= scale\n        label['objpos'][1] *= scale\n        for keypoint in sample['label']['keypoints']:\n            keypoint[0] *= scale\n            keypoint[1] *= scale\n        for other_annotation in sample['label']['processed_other_annotations']:\n            other_annotation['objpos'][0] *= scale\n            other_annotation['objpos'][1] *= scale\n            for keypoint in other_annotation['keypoints']:\n                keypoint[0] *= scale\n                keypoint[1] *= scale\n        return sample\n\n\nclass Rotate(object):\n    def __init__(self, pad, max_rotate_degree=40):\n        self._pad = pad\n        self._max_rotate_degree = max_rotate_degree\n\n    def __call__(self, sample):\n        prob = random.random()\n        degree = (prob - 0.5) * 2 * self._max_rotate_degree\n        h, w, _ = sample['image'].shape\n        img_center = (w / 2, h / 2)\n        R = cv2.getRotationMatrix2D(img_center, degree, 1)\n\n        abs_cos = abs(R[0, 0])\n        abs_sin = abs(R[0, 1])\n\n        bound_w = int(h * abs_sin + w * abs_cos)\n        bound_h = int(h * abs_cos + w * abs_sin)\n        dsize = (bound_w, bound_h)\n\n        R[0, 2] += dsize[0] / 2 - img_center[0]\n        R[1, 2] += dsize[1] / 2 - img_center[1]\n        sample['image'] = cv2.warpAffine(sample['image'], R, dsize=dsize,\n                                         borderMode=cv2.BORDER_CONSTANT, borderValue=self._pad)\n        sample['label']['img_height'], sample['label']['img_width'], _ = sample['image'].shape\n        sample['mask'] = cv2.warpAffine(sample['mask'], R, dsize=dsize,\n                                        borderMode=cv2.BORDER_CONSTANT, borderValue=(1, 1, 1))  # border is ok\n        label = sample['label']\n        label['objpos'] = self._rotate(label['objpos'], R)\n        for keypoint in label['keypoints']:\n            point = [keypoint[0], keypoint[1]]\n            point = self._rotate(point, R)\n            keypoint[0], keypoint[1] = point[0], point[1]\n        for other_annotation in label['processed_other_annotations']:\n            for keypoint in other_annotation['keypoints']:\n                point = [keypoint[0], keypoint[1]]\n                point = self._rotate(point, R)\n                keypoint[0], keypoint[1] = point[0], point[1]\n        return sample\n\n    def _rotate(self, point, R):\n        return [R[0, 0] * point[0] + R[0, 1] * point[1] + R[0, 2],\n                R[1, 0] * point[0] + R[1, 1] * point[1] + R[1, 2]]\n\n\nclass CropPad(object):\n    def __init__(self, pad, center_perterb_max=40, crop_x=368, crop_y=368):\n        self._pad = pad\n        self._center_perterb_max = center_perterb_max\n        self._crop_x = crop_x\n        self._crop_y = crop_y\n\n    def __call__(self, sample):\n        prob_x = random.random()\n        prob_y = random.random()\n\n        offset_x = int((prob_x - 0.5) * 2 * self._center_perterb_max)\n        offset_y = int((prob_y - 0.5) * 2 * self._center_perterb_max)\n        label = sample['label']\n        shifted_center = (label['objpos'][0] + offset_x, label['objpos'][1] + offset_y)\n        offset_left = -int(shifted_center[0] - self._crop_x / 2)\n        offset_up = -int(shifted_center[1] - self._crop_y / 2)\n\n        cropped_image = np.empty(shape=(self._crop_y, self._crop_x, 3), dtype=np.uint8)\n        for i in range(3):\n            cropped_image[:, :, i].fill(self._pad[i])\n        cropped_mask = np.empty(shape=(self._crop_y, self._crop_x), dtype=np.uint8)\n        cropped_mask.fill(1)\n\n        image_x_start = int(shifted_center[0] - self._crop_x / 2)\n        image_y_start = int(shifted_center[1] - self._crop_y / 2)\n        image_x_finish = image_x_start + self._crop_x\n        image_y_finish = image_y_start + self._crop_y\n        crop_x_start = 0\n        crop_y_start = 0\n        crop_x_finish = self._crop_x\n        crop_y_finish = self._crop_y\n\n        w, h = label['img_width'], label['img_height']\n        should_crop = True\n        if image_x_start < 0:  # Adjust crop area\n            crop_x_start -= image_x_start\n            image_x_start = 0\n        if image_x_start >= w:\n            should_crop = False\n\n        if image_y_start < 0:\n            crop_y_start -= image_y_start\n            image_y_start = 0\n        if image_y_start >= w:\n            should_crop = False\n\n        if image_x_finish > w:\n            diff = image_x_finish - w\n            image_x_finish -= diff\n            crop_x_finish -= diff\n        if image_x_finish < 0:\n            should_crop = False\n\n        if image_y_finish > h:\n            diff = image_y_finish - h\n            image_y_finish -= diff\n            crop_y_finish -= diff\n        if image_y_finish < 0:\n            should_crop = False\n\n        if should_crop:\n            cropped_image[crop_y_start:crop_y_finish, crop_x_start:crop_x_finish, :] =\\\n                sample['image'][image_y_start:image_y_finish, image_x_start:image_x_finish, :]\n            cropped_mask[crop_y_start:crop_y_finish, crop_x_start:crop_x_finish] =\\\n                sample['mask'][image_y_start:image_y_finish, image_x_start:image_x_finish]\n\n        sample['image'] = cropped_image\n        sample['mask'] = cropped_mask\n        label['img_width'] = self._crop_x\n        label['img_height'] = self._crop_y\n\n        label['objpos'][0] += offset_left\n        label['objpos'][1] += offset_up\n        for keypoint in label['keypoints']:\n            keypoint[0] += offset_left\n            keypoint[1] += offset_up\n        for other_annotation in label['processed_other_annotations']:\n            for keypoint in other_annotation['keypoints']:\n                keypoint[0] += offset_left\n                keypoint[1] += offset_up\n\n        return sample\n\n    def _inside(self, point, width, height):\n        if point[0] < 0 or point[1] < 0:\n            return False\n        if point[0] >= width or point[1] >= height:\n            return False\n        return True\n\n\nclass Flip(object):\n    def __init__(self, prob=0.5):\n        self._prob = prob\n\n    def __call__(self, sample):\n        prob = random.random()\n        do_flip = prob <= self._prob\n        if not do_flip:\n            return sample\n\n        sample['image'] = cv2.flip(sample['image'], 1)\n        sample['mask'] = cv2.flip(sample['mask'], 1)\n\n        label = sample['label']\n        w, h = label['img_width'], label['img_height']\n        label['objpos'][0] = w - 1 - label['objpos'][0]\n        for keypoint in label['keypoints']:\n            keypoint[0] = w - 1 - keypoint[0]\n        label['keypoints'] = self._swap_left_right(label['keypoints'])\n\n        for other_annotation in label['processed_other_annotations']:\n            other_annotation['objpos'][0] = w - 1 - other_annotation['objpos'][0]\n            for keypoint in other_annotation['keypoints']:\n                keypoint[0] = w - 1 - keypoint[0]\n            other_annotation['keypoints'] = self._swap_left_right(other_annotation['keypoints'])\n\n        return sample\n\n    def _swap_left_right(self, keypoints):\n        right = [2, 3, 4, 8, 9, 10, 14, 16]\n        left = [5, 6, 7, 11, 12, 13, 15, 17]\n        for r, l in zip(right, left):\n            keypoints[r], keypoints[l] = keypoints[l], keypoints[r]\n        return keypoints\n\n\nclass SinglePersonBodyMasking(object):\n    def __init__(self, prob=0.5, percentage=0.3, mask_color=(128, 128, 128)):\n        self._prob = prob\n        self._percentage = percentage\n        self._mask_color = mask_color\n\n    def __call__(self, sample):\n        image = sample['image']\n        h, w, c = image.shape\n        if random.random() > self._prob:\n            center_x = random.randint(w // 3, w - 1 - w // 3)\n            center_y = random.randint(h // 3, h - 1 - h // 3)\n            kpt = [\n                [center_x - random.randint(1, int(w * self._percentage)), center_y - random.randint(1, int(h * self._percentage))],\n                [center_x + random.randint(1, int(w * self._percentage)), center_y - random.randint(1, int(h * self._percentage))],\n                [center_x + random.randint(1, int(w * self._percentage)), center_y + random.randint(1, int(h * self._percentage))],\n                [center_x - random.randint(1, int(w * self._percentage)), center_y + random.randint(1, int(h * self._percentage))]\n            ]\n            cv2.fillConvexPoly(image, np.array(kpt, dtype=np.int32), (128, 128, 128))\n        return sample\n\n\nclass SinglePersonFlip(object):\n    def __init__(self, left_keypoints_indice, right_keypoints_indice, prob=0.5):\n        self._left_keypoints_indice = left_keypoints_indice\n        self._right_keypoints_indice = right_keypoints_indice\n        self._prob = prob\n\n    def __call__(self, sample):\n        rand = random.random()\n        do_flip = rand <= self._prob\n        if not do_flip:\n            return sample\n\n        sample['image'] = cv2.flip(sample['image'], 1)\n\n        w, h = sample['image'].shape[1], sample['image'].shape[0]\n        for id in range(len(sample['keypoints']) // 3):\n            if sample['keypoints'][id * 3] == -1:\n                continue\n            sample['keypoints'][id * 3] = w - 1 - sample['keypoints'][id * 3]\n        self._swap_left_right(sample['keypoints'])\n\n        return sample\n\n    def _swap_left_right(self, keypoints):\n        keypoints[self._right_keypoints_indice + self._left_keypoints_indice] =\\\n            keypoints[self._left_keypoints_indice + self._right_keypoints_indice]\n\n\nclass ChannelPermutation(object):\n    def __init__(self, prob=0.5):\n        self._prob = prob\n\n    def __call__(self, sample):\n        rand = random.random()\n        do_cp = rand < self._prob\n        if not do_cp:\n            return sample\n\n        new_order = np.random.permutation(3)\n        image = sample['image']\n        image[:, :, 0], image[:, :, 1], image[:, :, 2] =\\\n            image[:, :, new_order[0]], image[:, :, new_order[1]], image[:, :, new_order[2]]\n        sample['image'] = image\n\n        return sample\n\n\nclass SinglePersonRotate(object):\n    def __init__(self, pad=(128, 128, 128), max_rotate_degree=40):\n        self._pad = pad\n        self._max_rotate_degree = max_rotate_degree\n\n    def __call__(self, sample):\n        prob = random.random()\n        degree = (prob - 0.5) * 2 * self._max_rotate_degree\n        h, w, _ = sample['image'].shape\n        img_center = (w / 2, h / 2)\n        R = cv2.getRotationMatrix2D(img_center, degree, 1)\n\n        abs_cos = abs(R[0, 0])\n        abs_sin = abs(R[0, 1])\n\n        bound_w = int(h * abs_sin + w * abs_cos)\n        bound_h = int(h * abs_cos + w * abs_sin)\n        dsize = (bound_w, bound_h)\n\n        R[0, 2] += dsize[0] / 2 - img_center[0]\n        R[1, 2] += dsize[1] / 2 - img_center[1]\n        sample['image'] = cv2.warpAffine(sample['image'], R, dsize=dsize,\n                                         borderMode=cv2.BORDER_CONSTANT, borderValue=self._pad)\n\n        for id in range(len(sample['keypoints']) // 3):\n            if sample['keypoints'][id * 3] == -1:\n                continue\n            point = (sample['keypoints'][id * 3], sample['keypoints'][id * 3 + 1])\n            point = self._rotate(point, R)\n            sample['keypoints'][id * 3], sample['keypoints'][id * 3 + 1] = point\n        return sample\n\n    def _rotate(self, point, R):\n        return (R[0, 0] * point[0] + R[0, 1] * point[1] + R[0, 2],\n                R[1, 0] * point[0] + R[1, 1] * point[1] + R[1, 2])\n\n\nclass SinglePersonCropPad(object):\n    def __init__(self, pad, crop_x=256, crop_y=256):\n        self._pad = pad\n        self._crop_x = crop_x\n        self._crop_y = crop_y\n\n    def __call__(self, sample):\n        img = sample['image']\n        rnd_scale = 1\n        rnd_offset_x = 0\n        rnd_offset_y = 0\n\n        if random.random() > 0.5:\n            rnd_scale = random.random() * 0.7 + 0.8\n            h, w, _ = img.shape\n            scaled_img = cv2.resize(img, dsize=None, fx=rnd_scale, fy=rnd_scale, interpolation=cv2.INTER_CUBIC)\n            sh, sw, _ = scaled_img.shape\n            if rnd_scale >= 1:  # random crop from upsampled image\n                rnd_offset_x = (sw - w) // 2\n                rnd_offset_y = (sh - h) // 2\n                img = scaled_img[rnd_offset_y:rnd_offset_y + h, rnd_offset_x:rnd_offset_x + w]\n                rnd_offset_x *= -1\n                rnd_offset_y *= -1\n            else:  # pad to original size\n                rnd_offset_x = (w - sw) // 2\n                rnd_offset_y = (h - sh) // 2\n                b_border = h - sh - rnd_offset_y\n                r_border = w - sw - rnd_offset_x\n                img = cv2.copyMakeBorder(scaled_img, rnd_offset_y, b_border, rnd_offset_x, r_border,\n                                         borderType=cv2.BORDER_CONSTANT, value=self._pad)\n\n        scale = self._crop_x / max(img.shape[0], img.shape[1])\n        img = cv2.resize(img, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n\n        offset_x = (self._crop_x - img.shape[1]) // 2\n        offset_y = (self._crop_y - img.shape[0]) // 2\n\n        padded_img = np.ones((self._crop_y, self._crop_x, 3), dtype=np.uint8) * self._pad\n        padded_img[offset_y:offset_y+img.shape[0], offset_x:offset_x+img.shape[1], :] = img\n        sample['image'] = padded_img\n\n        for id in range(len(sample['keypoints']) // 3):\n            if sample['keypoints'][id * 3] == -1:\n                continue\n            sample['keypoints'][id * 3] = (sample['keypoints'][id * 3] * rnd_scale + rnd_offset_x) * scale + offset_x\n            sample['keypoints'][id * 3 + 1] = (sample['keypoints'][id * 3 + 1] * rnd_scale + rnd_offset_y) * scale + offset_y\n\n        return sample\n\n\nclass RandomScaleRotate(object):\n    def __init__(self, scale=0.35, rotate=45):\n        self._scale = scale\n        self._rotate = rotate\n\n    def __call__(self, sample):\n        scale = sample['scale']\n        rotate = sample['rotate']\n\n        scale = scale * np.clip(np.random.normal(loc=1, scale=scale) + 1, 1 - scale, 1 + scale)\n        rotate = np.clip(np.random.normal(loc=1, scale=rotate), -rotate * 2, rotate * 2)\\\n              if np.random.uniform() <= 0.6 else 0\n\n        sample['scale'] = scale\n        sample['rotate'] = rotate\n\n        return sample\n\n\nclass SinglePersonRandomAffineTransform(object):\n    def __init__(self, scale=0.35, rotate=45, mode='train', input_width=288, input_height=384, stride=8, num_keypoints=17):\n        self._mode = mode\n        self._scale = scale\n        self._rotate = rotate\n        self._width = input_width\n        self._height = input_height\n        self._stride = stride\n        self._num_keypoints = num_keypoints\n\n    def __call__(self, sample):\n        scale = sample['scale']\n        center = sample['center']\n        rotate = sample['rotate']\n\n        trans, _ = self._get_transformation_matrix(center, scale, rotate, [self._width, self._height])\n        input = cv2.warpAffine(sample['image'], trans, (self._width, self._height), flags=cv2.INTER_LINEAR)\n        sample['trans'] = trans\n        if self._mode == 'train':\n            for id in range(self._num_keypoints):\n                sample['keypoints'][3 * id: 3 * id + 2] = self._affine_transform(sample['keypoints'][3 * id: 3 * id + 2], trans)\n        else:\n            sample['rev_trans'] = self._get_transformation_matrix(center, scale, rotate,\n                                                                  [self._width // self._stride, self._height // self._stride])[1]\n        sample['image'] = input\n\n        return sample\n\n    def _get_transformation_matrix(self, center, scale, rotate, output_size):\n        w, _ = scale * 200\n        shift_y = self._rotation([0, -w * 0.5], rotate)\n        shift_x = self._rotation([-w * 0.5, 0], rotate)\n\n        points = np.array([center, center + shift_x, center + shift_y], dtype=np.float32)\n        transformed_points = np.array([[output_size[0] * 0.5, output_size[1] * 0.5],\n                                       [0, output_size[1] * 0.5],\n                                       [output_size[0] * 0.5, output_size[1] * 0.5 - output_size[0] * 0.5]],\n                                       dtype=np.float32)\n\n        rev_trans = cv2.getAffineTransform(np.float32(transformed_points), np.float32(points))\n\n        trans = cv2.getAffineTransform(np.float32(points), np.float32(transformed_points))\n\n        return trans, rev_trans\n\n    @staticmethod\n    def _affine_transform(pt, t):\n        new_pt = np.array([pt[0], pt[1], 1.])\n        new_pt = np.dot(t, new_pt)\n        return new_pt[:2]\n\n    @staticmethod\n    def _rotation(point, r):\n        r = np.pi * r / 180\n        return [point[0] * np.cos(r) - point[1] * np.sin(r), point[0] * np.sin(r) + point[1] * np.cos(r)]\n\n\nclass HalfBodyTransform(object):\n    def __init__(self, aspect_ratio=0.75, prob=0.3, num_keypoints=17):\n        self._prob = prob\n        self.aspect_ratio = aspect_ratio\n        self._num_keypoints = num_keypoints\n    def __call__(self, sample):\n        rand = np.random.uniform()\n        do_body_transform = rand <= self._prob and np.sum(sample['keypoints'][2:][::3]) > self._num_keypoints\n        if not do_body_transform:\n            return sample\n\n        upper_points = []\n        lower_points = []\n        for idx in range(len(sample['keypoints']) // 3):\n            if sample['keypoints'][idx * 3 + 2] > 0:\n                if idx < 11:\n                    upper_points.append([sample['keypoints'][idx * 3], sample['keypoints'][idx * 3 + 1]])\n                else:\n                    lower_points.append([sample['keypoints'][idx * 3], sample['keypoints'][idx * 3 + 1]])\n\n        if np.random.uniform() < 0.5 and len(upper_points) > 2 or len(lower_points) < 2:\n            target_points = upper_points\n        else:\n            target_points = lower_points\n\n        if len(target_points) < 2:\n            center, scale = None, None\n        else:\n            target_points = np.array(target_points, dtype=np.float32)\n            center = np.array([target_points[:, 0].sum() / len(target_points),\n                               target_points[:, 1].sum() / len(target_points)], dtype=np.float32)\n\n            w = target_points[:, 0].max() - target_points[:, 0].min()\n            h = target_points[:, 1].max() - target_points[:, 1].min()\n\n            if w > self.aspect_ratio * h:\n                h = w / self.aspect_ratio\n            elif w < self.aspect_ratio * h:\n                w = h * self.aspect_ratio\n\n            scale = np.array([w / 200, h / 200], dtype=np.float32)\n            scale *= 1.5\n\n        if center is not None:\n            sample['center'] = center\n            sample['scale'] = scale\n\n        return sample\n\n\nclass Normalization(object):\n    def __init__(self, mean=0.5, std=1):\n        self._mean = mean\n        self._std = std\n\n    def __call__(self, sample):\n        sample['image'] = sample['image'] / 255\n        sample['image'] = (sample['image'] - self._mean) / self._std\n\n        return sample\n"""
pytorch_toolkit/human_pose_estimation/models/__init__.py,0,b''
pytorch_toolkit/human_pose_estimation/models/single_person_pose_with_mobilenet.py,0,"b""import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom models.with_mobilenet import RefinementStageBlock\nfrom modules.conv import conv, conv_dw\n\n\nclass InitialStage(nn.Module):\n    def __init__(self, num_channels, num_heatmaps):\n        super().__init__()\n        self.trunk = nn.Sequential(\n            conv(num_channels, num_channels),\n            conv(num_channels, num_channels),\n            conv(num_channels, num_channels)\n        )\n        self.heatmaps = nn.Sequential(\n            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n            conv(512, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        heatmaps = self.heatmaps(trunk_features)\n        return [heatmaps]\n\n\nclass UShapedContextBlock(nn.Module):\n    def __init__(self, in_channels, mode='bilinear'):\n        super().__init__()\n        self.mode_interpolation = mode\n        self.encoder1 = nn.Sequential(\n            conv(in_channels, in_channels*2, stride=2),\n            conv(in_channels*2, in_channels*2),\n        )\n        self.encoder2 = nn.Sequential(\n            conv(in_channels*2, in_channels*2, stride=2),\n            conv(in_channels*2, in_channels*2),\n        )\n        self.decoder2 = nn.Sequential(\n            conv(in_channels*2 + in_channels*2, in_channels*2),\n            conv(in_channels*2, in_channels*2),\n        )\n        self.decoder1 = nn.Sequential(\n            conv(in_channels*3, in_channels*2),\n            conv(in_channels*2, in_channels)\n        )\n\n    def forward(self, x):\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        d2 = self.decoder2(torch.cat([e1, F.interpolate(e2, size=(int(e1.shape[2]), int(e1.shape[3])),\n                                                        mode=self.mode_interpolation)], 1))\n        d1 = self.decoder1(torch.cat([x, F.interpolate(d2, size=(int(x.shape[2]), int(x.shape[3])),\n                                                       mode=self.mode_interpolation)], 1))\n        return d1\n\n\nclass RefinementStage(nn.Module):\n    def __init__(self, in_channels, out_channels, num_heatmaps, mode='bilinear'):\n        super().__init__()\n\n        self.trunk = nn.Sequential(\n            UShapedContextBlock(in_channels, mode),\n            RefinementStageBlock(in_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n        )\n        self.heatmaps = nn.Sequential(\n            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n            conv(out_channels, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        heatmaps = self.heatmaps(trunk_features)\n        return [heatmaps]\n\n\nclass SinglePersonPoseEstimationWithMobileNet(nn.Module):\n    def __init__(self, num_refinement_stages=1, num_channels=128, num_heatmaps=17, mode='bilinear'):\n        super().__init__()\n        self.model = nn.Sequential(\n            conv(     3,  32, stride=2, bias=False),\n            conv_dw( 32,  64),\n            conv_dw( 64, 128, stride=2),\n            conv_dw(128, 128),\n            conv_dw(128, 256, stride=2),\n            conv_dw(256, 256),\n            conv_dw(256, 512),  # conv4_2\n            conv_dw(512, 512, dilation=2, padding=2),\n            conv_dw(512, 512),\n            conv_dw(512, 512),\n            conv_dw(512, 512),\n            conv_dw(512, 512),  # conv5_5\n        )\n        self.cpm = nn.Sequential(\n            conv(512, 256),\n            conv(256, 128),\n        )\n\n        self.initial_stage = InitialStage(num_channels, num_heatmaps)\n        self.refinement_stages = nn.ModuleList()\n        for idx in range(num_refinement_stages):\n            self.refinement_stages.append(RefinementStage(num_channels + num_heatmaps, num_channels, num_heatmaps, mode))\n\n    def forward(self, x):\n        backbone_features = self.model(x)\n        backbone_features = self.cpm(backbone_features)\n\n        stages_output = self.initial_stage(backbone_features)\n        for refinement_stage in self.refinement_stages:\n            stages_output.extend(\n                refinement_stage(torch.cat([backbone_features, stages_output[-1]], dim=1)))\n\n        return stages_output\n"""
pytorch_toolkit/human_pose_estimation/models/with_mobilenet.py,0,"b'import torch\nfrom torch import nn\n\nfrom modules.conv import conv, conv_dw, conv_dw_no_bn\n\n\nclass Cpm(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.align = conv(in_channels, out_channels, kernel_size=1, padding=0, bn=False)\n        self.trunk = nn.Sequential(\n            conv_dw_no_bn(out_channels, out_channels),\n            conv_dw_no_bn(out_channels, out_channels),\n            conv_dw_no_bn(out_channels, out_channels)\n        )\n        self.conv = conv(out_channels, out_channels, bn=False)\n\n    def forward(self, x):\n        x = self.align(x)\n        x = self.conv(x + self.trunk(x))\n        return x\n\n\nclass InitialStage(nn.Module):\n    def __init__(self, num_channels, num_heatmaps, num_pafs):\n        super().__init__()\n        self.trunk = nn.Sequential(\n            conv(num_channels, num_channels, bn=False),\n            conv(num_channels, num_channels, bn=False),\n            conv(num_channels, num_channels, bn=False)\n        )\n        self.heatmaps = nn.Sequential(\n            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n            conv(512, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n        self.pafs = nn.Sequential(\n            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n            conv(512, num_pafs, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        heatmaps = self.heatmaps(trunk_features)\n        pafs = self.pafs(trunk_features)\n        return [heatmaps, pafs]\n\n\nclass RefinementStageBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.initial = conv(in_channels, out_channels, kernel_size=1, padding=0, bn=False)\n        self.trunk = nn.Sequential(\n            conv(out_channels, out_channels),\n            conv(out_channels, out_channels, dilation=2, padding=2)\n        )\n\n    def forward(self, x):\n        initial_features = self.initial(x)\n        trunk_features = self.trunk(initial_features)\n        return initial_features + trunk_features\n\n\nclass RefinementStage(nn.Module):\n    def __init__(self, in_channels, out_channels, num_heatmaps, num_pafs):\n        super().__init__()\n        self.trunk = nn.Sequential(\n            RefinementStageBlock(in_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels)\n        )\n        self.heatmaps = nn.Sequential(\n            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n            conv(out_channels, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n        self.pafs = nn.Sequential(\n            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n            conv(out_channels, num_pafs, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        heatmaps = self.heatmaps(trunk_features)\n        pafs = self.pafs(trunk_features)\n        return [heatmaps, pafs]\n\n\nclass PoseEstimationWithMobileNet(nn.Module):\n    def __init__(self, num_refinement_stages=1, num_channels=128, num_heatmaps=19, num_pafs=38):\n        super().__init__()\n        self.model = nn.Sequential(\n            conv(     3,  32, stride=2, bias=False),\n            conv_dw( 32,  64),\n            conv_dw( 64, 128, stride=2),\n            conv_dw(128, 128),\n            conv_dw(128, 256, stride=2),\n            conv_dw(256, 256),\n            conv_dw(256, 512),  # conv4_2\n            conv_dw(512, 512, dilation=2, padding=2),\n            conv_dw(512, 512),\n            conv_dw(512, 512),\n            conv_dw(512, 512),\n            conv_dw(512, 512)   # conv5_5\n        )\n        self.cpm = Cpm(512, num_channels)\n\n        self.initial_stage = InitialStage(num_channels, num_heatmaps, num_pafs)\n        self.refinement_stages = nn.ModuleList()\n        for idx in range(num_refinement_stages):\n            self.refinement_stages.append(RefinementStage(num_channels + num_heatmaps + num_pafs, num_channels,\n                                                          num_heatmaps, num_pafs))\n\n    def forward(self, x):\n        backbone_features = self.model(x)\n        backbone_features = self.cpm(backbone_features)\n\n        stages_output = self.initial_stage(backbone_features)\n        for refinement_stage in self.refinement_stages:\n            stages_output.extend(\n                refinement_stage(torch.cat([backbone_features, stages_output[-2], stages_output[-1]], dim=1)))\n\n        return stages_output\n'"
pytorch_toolkit/human_pose_estimation/modules/__init__.py,0,b''
pytorch_toolkit/human_pose_estimation/modules/calc_pckh.py,0,"b""import numpy as np\nimport csv\n\n\ndef read_data(file_name, has_visibility):\n    all_keypoints_coords = []\n    with open(file_name, 'r') as f:\n        reader = csv.reader(f, delimiter=',')\n        for row in reader:\n            keypoint_coords = row[1:]\n            for i in range(len(keypoint_coords)):\n                if keypoint_coords[i] == 'nan':\n                    keypoint_coords[i] = '-1'\n                keypoint_coords[i] = float(keypoint_coords[i])\n            all_keypoints_coords.append(keypoint_coords)\n\n    data = np.array(all_keypoints_coords)\n    num_dims = 2\n    if has_visibility:\n        num_dims = 3\n    data = np.reshape(data, [data.shape[0], int(data.shape[1] / num_dims), num_dims])\n\n    vis_label = np.zeros((data.shape[0], data.shape[1]))\n\n    if has_visibility:\n        vis_label[:, :] = data[:, :, 2]\n        data = data[:, :, 0:2]\n    else:\n        vis_label = vis_label + 1\n        data[data < 0] = 1\n\n    return data, vis_label\n\n\ndef get_head_size(gt):\n    head_size = np.linalg.norm(gt[:, 9, :] - gt[:, 8, :], axis=1)\n    for n in range(gt.shape[0]):\n        if gt[n, 8, 0] < 0 or gt[n, 9, 0] < 0:\n            head_size[n] = 0\n\n    return head_size\n\n\ndef get_normalized_distance(gt, prediction, head_size):\n    num_images = prediction.shape[0]\n    num_keypoints = prediction.shape[1]\n    distances = np.zeros([num_images, num_keypoints])\n    for img_id in range(num_images):\n        current_head_size = head_size[img_id]\n        if current_head_size == 0:\n            distances[img_id, :] = -1\n        else:\n            distances[img_id, :] = np.linalg.norm(gt[img_id, :, :] - prediction[img_id, :, :], axis=1) / current_head_size\n            for kpt_id in range(num_keypoints):\n                if gt[img_id, kpt_id, 0] < 0 or gt[img_id, kpt_id, 1] < 0:\n                    distances[img_id, kpt_id] = -1\n    return distances\n\n\ndef compute_pckh(distances, pckh_threshold_range):\n    num_keypoints = distances.shape[1]\n    pckh = np.zeros([len(pckh_threshold_range), num_keypoints + 2])\n\n    for kpt_id in range(num_keypoints):\n        for threshold_id in range(len(pckh_threshold_range)):\n            threshold = pckh_threshold_range[threshold_id]\n            joint_distance = distances[:, kpt_id]\n            pckh[threshold_id, kpt_id] = np.mean(joint_distance[np.where(joint_distance >= 0)] <= threshold) * 100\n\n    for threshold_id in range(len(pckh_threshold_range)):\n        threshold = pckh_threshold_range[threshold_id]\n        joint_distance = distances[:, 8:16]\n        pckh[threshold_id, num_keypoints] = np.mean(joint_distance[np.where(joint_distance >= 0)] <= threshold) * 100\n\n    for threshold_id in range(len(pckh_threshold_range)):\n        threshold = pckh_threshold_range[threshold_id]\n        joints_index = list(range(0, 6)) + list(range(8, 16))\n        joint_distance = distances[:, joints_index]\n        pckh[threshold_id, num_keypoints + 1] = np.mean(joint_distance[np.where(joint_distance >= 0)] <= threshold) * 100\n\n    return pckh\n\n\ndef print_output(pckh, method_name):\n    template = '{0:10} & {1:6} & {2:6} & {3:6} & {4:6} & {5:6} & {6:6} & {7:6} & {8:6} & {9:6}'\n    header = template.format('PCKh@0.5', 'Head', 'Sho.', 'Elb.', 'Wri.', 'Hip', 'Knee', 'Ank.', 'U.Body', 'Avg.')\n    pckh = template.format(method_name, '%1.2f' % ((pckh[8]  + pckh[9])  / 2),\n                                        '%1.2f' % ((pckh[12] + pckh[13]) / 2),\n                                        '%1.2f' % ((pckh[11] + pckh[14]) / 2),\n                                        '%1.2f' % ((pckh[10] + pckh[15]) / 2),\n                                        '%1.2f' % ((pckh[2]  + pckh[3])  / 2),\n                                        '%1.2f' % ((pckh[1]  + pckh[4])  / 2),\n                                        '%1.2f' % ((pckh[0]  + pckh[5])  / 2),\n                                        '%1.2f' % (pckh[-2]),\n                                        '%1.2f' % (pckh[-1]))\n    print(header)\n    print(pckh)\n\n\ndef calc_pckh(gt_path, prediction_path, method_name='gccpm', eval_num=10000):\n    threshold_range = np.array([0.5])\n\n    prediction, _ = read_data(prediction_path, False)\n    prediction = prediction[:eval_num, :, :]\n    gt, _ = read_data(gt_path, True)\n    gt = gt[:eval_num, :, :]\n    assert gt.shape[0] == prediction.shape[0], 'number of images not matched'\n    assert gt.shape[1] == prediction.shape[1], 'number of joints not matched'\n    assert gt.shape[2] == prediction.shape[2], 'keypoint dims not matched'\n\n    head_size = get_head_size(gt)\n    normalized_distance = get_normalized_distance(gt, prediction, head_size)\n    pckh = compute_pckh(normalized_distance, threshold_range)\n    print_output(pckh[-1], method_name)\n\n    return pckh\n"""
pytorch_toolkit/human_pose_estimation/modules/conv.py,0,"b'from torch import nn\n\n\ndef conv(in_channels, out_channels, kernel_size=3, padding=1, bn=True, dilation=1, stride=1, relu=True, bias=True):\n    modules = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)]\n    if bn:\n        modules.append(nn.BatchNorm2d(out_channels))\n    if relu:\n        modules.append(nn.ReLU(inplace=True))\n    return nn.Sequential(*modules)\n\n\ndef conv_dw(in_channels, out_channels, kernel_size=3, padding=1, stride=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation=dilation, groups=in_channels, bias=False),\n        nn.BatchNorm2d(in_channels),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n    )\n\n\ndef conv_dw_no_bn(in_channels, out_channels, kernel_size=3, padding=1, stride=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation=dilation, groups=in_channels, bias=False),\n        nn.ELU(inplace=True),\n\n        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n        nn.ELU(inplace=True),\n    )\n'"
pytorch_toolkit/human_pose_estimation/modules/get_parameters.py,0,"b'from torch import nn\n\n\ndef get_parameters(model, predicate):\n    for module in model.modules():\n        for param_name, param in module.named_parameters():\n            if predicate(module, param_name):\n                yield param\n\n\ndef get_parameters_conv(model, name):\n    return get_parameters(model, lambda m, p: isinstance(m, nn.Conv2d) and m.groups == 1 and p == name)\n\n\ndef get_parameters_conv_depthwise(model, name):\n    return get_parameters(model, lambda m, p: isinstance(m, nn.Conv2d)\n                                              and m.groups == m.in_channels\n                                              and m.in_channels == m.out_channels\n                                              and p == name)\n\n\ndef get_parameters_bn(model, name):\n    return get_parameters(model, lambda m, p: isinstance(m, nn.BatchNorm2d) and p == name)\n'"
pytorch_toolkit/human_pose_estimation/modules/keypoints.py,0,"b""import math\nimport numpy as np\nfrom operator import itemgetter\n\nBODY_PARTS_KPT_IDS = [[1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], [8, 9], [9, 10], [1, 11],\n                      [11, 12], [12, 13], [1, 0], [0, 14], [14, 16], [0, 15], [15, 17], [2, 16], [5, 17]]\nBODY_PARTS_PAF_IDS = ([12, 13], [20, 21], [14, 15], [16, 17], [22, 23], [24, 25], [0, 1], [2, 3], [4, 5],\n                      [6, 7], [8, 9], [10, 11], [28, 29], [30, 31], [34, 35], [32, 33], [36, 37], [18, 19], [26, 27])\n\n\ndef linspace2d(start, stop, n=10):\n    points = 1 / (n - 1) * (stop - start)\n    return points[:, None] * np.arange(n) + start[:, None]\n\n\ndef extract_keypoints(heatmap, all_keypoints, total_keypoint_num):\n    heatmap[heatmap < 0.1] = 0\n    heatmap_with_borders = np.pad(heatmap, [(2, 2), (2, 2)], mode='constant')\n    heatmap_center = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 1:heatmap_with_borders.shape[1]-1]\n    heatmap_left = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 2:heatmap_with_borders.shape[1]]\n    heatmap_right = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 0:heatmap_with_borders.shape[1]-2]\n    heatmap_up = heatmap_with_borders[2:heatmap_with_borders.shape[0], 1:heatmap_with_borders.shape[1]-1]\n    heatmap_down = heatmap_with_borders[0:heatmap_with_borders.shape[0]-2, 1:heatmap_with_borders.shape[1]-1]\n\n    heatmap_peaks = (heatmap_center > heatmap_left) &\\\n                    (heatmap_center > heatmap_right) &\\\n                    (heatmap_center > heatmap_up) &\\\n                    (heatmap_center > heatmap_down)\n    heatmap_peaks = heatmap_peaks[1:heatmap_center.shape[0]-1, 1:heatmap_center.shape[1]-1]\n    keypoints = list(zip(np.nonzero(heatmap_peaks)[1], np.nonzero(heatmap_peaks)[0]))  # (w, h)\n    keypoints = sorted(keypoints, key=itemgetter(0))\n\n    suppressed = np.zeros(len(keypoints), np.uint8)\n    keypoints_with_score_and_id = []\n    keypoint_num = 0\n    for i in range(len(keypoints)):\n        if suppressed[i]:\n            continue\n        for j in range(i+1, len(keypoints)):\n            if math.sqrt((keypoints[i][0] - keypoints[j][0]) ** 2 +\n                         (keypoints[i][1] - keypoints[j][1]) ** 2) < 6:\n                suppressed[j] = 1\n        keypoint_with_score_and_id = (keypoints[i][0], keypoints[i][1], heatmap[keypoints[i][1], keypoints[i][0]],\n                                      total_keypoint_num + keypoint_num)\n        keypoints_with_score_and_id.append(keypoint_with_score_and_id)\n        keypoint_num += 1\n    all_keypoints.append(keypoints_with_score_and_id)\n    return keypoint_num\n\n\ndef group_keypoints(all_keypoints_by_type, pafs, pose_entry_size=20, min_paf_score=0.05, demo=False):\n    pose_entries = []\n    all_keypoints = np.array([item for sublist in all_keypoints_by_type for item in sublist])\n    for part_id in range(len(BODY_PARTS_PAF_IDS)):\n        part_pafs = pafs[:, :, BODY_PARTS_PAF_IDS[part_id]]\n        kpts_a = all_keypoints_by_type[BODY_PARTS_KPT_IDS[part_id][0]]\n        kpts_b = all_keypoints_by_type[BODY_PARTS_KPT_IDS[part_id][1]]\n        num_kpts_a = len(kpts_a)\n        num_kpts_b = len(kpts_b)\n        kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n        kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n\n        if num_kpts_a == 0 and num_kpts_b == 0:  # no keypoints for such body part\n            continue\n        elif num_kpts_a == 0:  # body part has just 'b' keypoints\n            for i in range(num_kpts_b):\n                num = 0\n                for j in range(len(pose_entries)):  # check if already in some pose, was added by another body part\n                    if pose_entries[j][kpt_b_id] == kpts_b[i][3]:\n                        num += 1\n                        continue\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_b_id] = kpts_b[i][3]  # keypoint idx\n                    pose_entry[-1] = 1                   # num keypoints in pose\n                    pose_entry[-2] = kpts_b[i][2]        # pose score\n                    pose_entries.append(pose_entry)\n            continue\n        elif num_kpts_b == 0:  # body part has just 'a' keypoints\n            for i in range(num_kpts_a):\n                num = 0\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == kpts_a[i][3]:\n                        num += 1\n                        continue\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_a_id] = kpts_a[i][3]\n                    pose_entry[-1] = 1\n                    pose_entry[-2] = kpts_a[i][2]\n                    pose_entries.append(pose_entry)\n            continue\n\n        connections = []\n        for i in range(num_kpts_a):\n            kpt_a = np.array(kpts_a[i][0:2])\n            for j in range(num_kpts_b):\n                kpt_b = np.array(kpts_b[j][0:2])\n                mid_point = [(), ()]\n                mid_point[0] = (int(round((kpt_a[0] + kpt_b[0]) * 0.5)),\n                                int(round((kpt_a[1] + kpt_b[1]) * 0.5)))\n                mid_point[1] = mid_point[0]\n\n                vec = [kpt_b[0] - kpt_a[0], kpt_b[1] - kpt_a[1]]\n                vec_norm = math.sqrt(vec[0] ** 2 + vec[1] ** 2)\n                if vec_norm == 0:\n                    continue\n                vec[0] /= vec_norm\n                vec[1] /= vec_norm\n                cur_point_score = (vec[0] * part_pafs[mid_point[0][1], mid_point[0][0], 0] +\n                                   vec[1] * part_pafs[mid_point[1][1], mid_point[1][0], 1])\n\n                height_n = pafs.shape[0] // 2\n                success_ratio = 0\n                point_num = 10  # number of points to integration over paf\n                if cur_point_score > -100:\n                    passed_point_score = 0\n                    passed_point_num = 0\n                    x, y = linspace2d(kpt_a, kpt_b)\n                    for point_idx in range(point_num):\n                        if not demo:\n                            px = int(round(x[point_idx]))\n                            py = int(round(y[point_idx]))\n                        else:\n                            px = int(x[point_idx])\n                            py = int(y[point_idx])\n                        paf = part_pafs[py, px, 0:2]\n                        cur_point_score = vec[0] * paf[0] + vec[1] * paf[1]\n                        if cur_point_score > min_paf_score:\n                            passed_point_score += cur_point_score\n                            passed_point_num += 1\n                    success_ratio = passed_point_num / point_num\n                    ratio = 0\n                    if passed_point_num > 0:\n                        ratio = passed_point_score / passed_point_num\n                    ratio += min(height_n / vec_norm - 1, 0)\n                if ratio > 0 and success_ratio > 0.8:\n                    score_all = ratio + kpts_a[i][2] + kpts_b[j][2]\n                    connections.append([i, j, ratio, score_all])\n        if len(connections) > 0:\n            connections = sorted(connections, key=itemgetter(2), reverse=True)\n\n        num_connections = min(num_kpts_a, num_kpts_b)\n        has_kpt_a = np.zeros(num_kpts_a, dtype=np.int32)\n        has_kpt_b = np.zeros(num_kpts_b, dtype=np.int32)\n        filtered_connections = []\n        for row in range(len(connections)):\n            if len(filtered_connections) == num_connections:\n                break\n            i, j, cur_point_score = connections[row][0:3]\n            if not has_kpt_a[i] and not has_kpt_b[j]:\n                filtered_connections.append([kpts_a[i][3], kpts_b[j][3], cur_point_score])\n                has_kpt_a[i] = 1\n                has_kpt_b[j] = 1\n        connections = filtered_connections\n        if len(connections) == 0:\n            continue\n\n        if part_id == 0:\n            pose_entries = [np.ones(pose_entry_size) * -1 for _ in range(len(connections))]\n            for i in range(len(connections)):\n                pose_entries[i][BODY_PARTS_KPT_IDS[0][0]] = connections[i][0]\n                pose_entries[i][BODY_PARTS_KPT_IDS[0][1]] = connections[i][1]\n                pose_entries[i][-1] = 2\n                pose_entries[i][-2] = np.sum(all_keypoints[connections[i][0:2], 2]) + connections[i][2]\n        elif part_id == 17 or part_id == 18:\n            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n            for i in range(len(connections)):\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == connections[i][0] and pose_entries[j][kpt_b_id] == -1:\n                        pose_entries[j][kpt_b_id] = connections[i][1]\n                    elif pose_entries[j][kpt_b_id] == connections[i][1] and pose_entries[j][kpt_a_id] == -1:\n                        pose_entries[j][kpt_a_id] = connections[i][0]\n            continue\n        else:\n            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n            for i in range(len(connections)):\n                num = 0\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == connections[i][0]:\n                        pose_entries[j][kpt_b_id] = connections[i][1]\n                        num += 1\n                        pose_entries[j][-1] += 1\n                        pose_entries[j][-2] += all_keypoints[connections[i][1], 2] + connections[i][2]\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_a_id] = connections[i][0]\n                    pose_entry[kpt_b_id] = connections[i][1]\n                    pose_entry[-1] = 2\n                    pose_entry[-2] = np.sum(all_keypoints[connections[i][0:2], 2]) + connections[i][2]\n                    pose_entries.append(pose_entry)\n\n    filtered_entries = []\n    for i in range(len(pose_entries)):\n        if pose_entries[i][-1] < 3 or (pose_entries[i][-2] / pose_entries[i][-1] < 0.2):\n            continue\n        filtered_entries.append(pose_entries[i])\n    pose_entries = np.asarray(filtered_entries)\n    return pose_entries, all_keypoints\n"""
pytorch_toolkit/human_pose_estimation/modules/load_state.py,0,"b""import collections\n\n\ndef load_state(net, checkpoint):\n    source_state = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n    target_state = net.state_dict()\n    new_target_state = collections.OrderedDict()\n    for target_key, target_value in target_state.items():\n        if target_key in source_state and source_state[target_key].size() == target_state[target_key].size():\n            new_target_state[target_key] = source_state[target_key]\n        else:\n            new_target_state[target_key] = target_state[target_key]\n            print('[WARNING] Not found pretrained parameters for {}'.format(target_key))\n\n    net.load_state_dict(new_target_state)\n\n\ndef load_from_mobilenet(net, checkpoint):\n    source_state = checkpoint['state_dict']\n    target_state = net.state_dict()\n    new_target_state = collections.OrderedDict()\n    for target_key, target_value in target_state.items():\n        k = target_key\n        if k.find('model') != -1:\n            k = k.replace('model', 'module.model')\n        if k in source_state and source_state[k].size() == target_state[target_key].size():\n            new_target_state[target_key] = source_state[k]\n        else:\n            new_target_state[target_key] = target_state[target_key]\n            print('[WARNING] Not found pretrained parameters for {}'.format(target_key))\n\n    net.load_state_dict(new_target_state)\n"""
pytorch_toolkit/human_pose_estimation/modules/loss.py,0,"b'import torch.nn as nn\n\ndef l2_loss(input, target, batch_size, mask=None):\n    if mask is not None:\n        loss = (input - target) * mask\n    else:\n        loss = (input - target)\n    loss = (loss * loss) / 2 / batch_size\n\n    return loss.sum()\n\ndef mse_loss(output, target, mask):\n    mse = nn.MSELoss()\n    batch_size = output.size(0)\n    num_keypoints = output.size(1)\n    heatmaps_target = target.reshape((batch_size, num_keypoints, -1)).split(1, 1)\n    heatmaps_pred = output.reshape((batch_size, num_keypoints, -1)).split(1, 1)\n    loss = 0\n    for idx in range(num_keypoints):\n        heatmap_pred = heatmaps_pred[idx].squeeze()\n        heatmap_target = heatmaps_target[idx].squeeze()\n        loss += 0.5 * mse(heatmap_pred.mul(mask.cuda()[:, idx]),\n                          heatmap_target.mul(mask[:, idx]).cuda())\n\n    return loss / num_keypoints\n'"
pytorch_toolkit/human_pose_estimation/modules/pose.py,0,"b'import cv2\nimport numpy as np\n\nfrom modules.keypoints import BODY_PARTS_KPT_IDS, BODY_PARTS_PAF_IDS\n\n\nclass Pose(object):\n    num_kpts = 18\n    kpt_names = [\'nose\', \'neck\',\n                 \'r_sho\', \'r_elb\', \'r_wri\', \'l_sho\', \'l_elb\', \'l_wri\',\n                 \'r_hip\', \'r_knee\', \'r_ank\', \'l_hip\', \'l_knee\', \'l_ank\',\n                 \'r_eye\', \'l_eye\',\n                 \'r_ear\', \'l_ear\']\n    sigmas = np.array([.26, .79, .79, .72, .62, .79, .72, .62, 1.07, .87, .89, 1.07, .87, .89, .25, .25, .35, .35],\n                      dtype=np.float32) / 10.0\n    vars = (sigmas * 2) ** 2\n    last_id = -1\n    color = [0, 224, 255]\n\n    def __init__(self, keypoints, confidence):\n        super().__init__()\n        self.keypoints = keypoints\n        self.confidence = confidence\n        found_keypoints = np.zeros((np.count_nonzero(keypoints[:, 0] != -1), 2), dtype=np.int32)\n        found_kpt_id = 0\n        for kpt_id in range(keypoints.shape[0]):\n            if keypoints[kpt_id, 0] == -1:\n                continue\n            found_keypoints[found_kpt_id] = keypoints[kpt_id]\n            found_kpt_id += 1\n        self.bbox = cv2.boundingRect(found_keypoints)\n        self.id = None\n\n    def update_id(self, id=None):\n        self.id = id\n        if self.id is None:\n            self.id = Pose.last_id + 1\n            Pose.last_id += 1\n\n    def draw(self, img):\n        assert self.keypoints.shape == (Pose.num_kpts, 2)\n\n        for part_id in range(len(BODY_PARTS_PAF_IDS) - 2):\n            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n            global_kpt_a_id = self.keypoints[kpt_a_id, 0]\n            if global_kpt_a_id != -1:\n                x_a, y_a = self.keypoints[kpt_a_id]\n                cv2.circle(img, (int(x_a), int(y_a)), 3, Pose.color, -1)\n            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n            global_kpt_b_id = self.keypoints[kpt_b_id, 0]\n            if global_kpt_b_id != -1:\n                x_b, y_b = self.keypoints[kpt_b_id]\n                cv2.circle(img, (int(x_b), int(y_b)), 3, Pose.color, -1)\n            if global_kpt_a_id != -1 and global_kpt_b_id != -1:\n                cv2.line(img, (int(x_a), int(y_a)), (int(x_b), int(y_b)), Pose.color, 2)\n\n\ndef get_similarity(a, b, threshold=0.5):\n    num_similar_kpt = 0\n    for kpt_id in range(Pose.num_kpts):\n        if a.keypoints[kpt_id, 0] != -1 and b.keypoints[kpt_id, 0] != -1:\n            distance = np.sum((a.keypoints[kpt_id] - b.keypoints[kpt_id]) ** 2)\n            area = max(a.bbox[2] * a.bbox[3], b.bbox[2] * b.bbox[3])\n            similarity = np.exp(-distance / (2 * (area + np.spacing(1)) * Pose.vars[kpt_id]))\n            if similarity > threshold:\n                num_similar_kpt += 1\n    return num_similar_kpt\n\n\ndef propagate_ids(previous_poses, current_poses, threshold=3):\n    """"""Propagate poses ids from previous frame results. Id is propagated,\n    if there are at least `threshold` similar keypoints between pose from previous frame and current.\n\n    :param previous_poses: poses from previous frame with ids\n    :param current_poses: poses from current frame to assign ids\n    :param threshold: minimal number of similar keypoints between poses\n    :return: None\n    """"""\n    current_poses = sorted(current_poses, key=lambda pose: pose.confidence, reverse=True)  # match confident poses first\n    mask = np.ones(len(previous_poses), dtype=np.int32)\n    for current_pose_id in range(len(current_poses)):\n        best_matched_id = None\n        best_matched_pose_id = None\n        best_matched_iou = 0\n        for previous_pose_id in range(len(previous_poses)):\n            if not mask[previous_pose_id]:\n                continue\n            iou = get_similarity(current_poses[current_pose_id], previous_poses[previous_pose_id])\n            if iou > best_matched_iou:\n                best_matched_iou = iou\n                best_matched_pose_id = previous_poses[previous_pose_id].id\n                best_matched_id = previous_pose_id\n        if best_matched_iou >= threshold:\n            mask[best_matched_id] = 0\n        else:  # pose not similar to any previous\n            best_matched_pose_id = None\n        current_poses[current_pose_id].update_id(best_matched_pose_id)\n'"
pytorch_toolkit/human_pose_estimation/scripts/convert_coco_labels.py,0,"b""import argparse\nimport json\n\n\ndef convert(labels_path, output_name):\n    with open(labels_path, 'r') as f:\n        labels = json.load(f)\n\n    converted_labels = {'annotations': []}\n    for annotation in labels['annotations']:\n        if annotation['num_keypoints'] == 0:\n            continue\n        if annotation['iscrowd']:\n            continue\n        converted_annotation = {\n            'bbox': annotation['bbox'],\n            'keypoints': annotation['keypoints'],\n            'image_path': '{:012}.jpg'.format(int(annotation['image_id']))\n        }\n        converted_labels['annotations'].append(converted_annotation)\n\n    with open(output_name, 'w') as f:\n        json.dump(converted_labels, f, indent=4)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--labels-path', type=str, required=True, help='path to MS COCO labels file')\n    parser.add_argument('--output-name', type=str, required=True, help='name of converted file')\n    args = parser.parse_args()\n\n    convert(args.labels_path, args.output_name)\n\n"""
pytorch_toolkit/human_pose_estimation/scripts/convert_to_onnx.py,0,"b""import argparse\n\nimport torch\n\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom models.single_person_pose_with_mobilenet import SinglePersonPoseEstimationWithMobileNet\nfrom modules.load_state import load_state\n\n\ndef convert_to_onnx(net, output_name, single_person, input_size):\n    input = torch.randn(1, 3, input_size[0], input_size[1])\n    input_layer_names = ['data']\n    output_layer_names = ['stage_0_output_1_heatmaps', 'stage_0_output_0_pafs',\n                          'stage_1_output_1_heatmaps', 'stage_1_output_0_pafs']\n    if single_person:\n        input = torch.randn(1, 3, input_size[0], input_size[1])\n        output_layer_names = ['stage_{}_output_1_heatmaps'.format(i) for i in range(len(net.refinement_stages) + 1)]\n\n    torch.onnx.export(net, input, output_name, verbose=True, input_names=input_layer_names,\n                      output_names=output_layer_names)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint')\n    parser.add_argument('--output-name', type=str, default='human-pose-estimation.onnx',\n                        help='name of output model in ONNX format')\n    parser.add_argument('--single-person', action='store_true', help='convert model for single-person pose estimation')\n    parser.add_argument('--input-size', nargs='+', type=int,  required=True,\n                        help='Size of input image in format: height width')\n    parser.add_argument('--mode-interpolation', type=str, required=False, default='bilinear',\n                        help='type interpolation <bilinear> or <nearest>')\n    parser.add_argument('--num-refinement-stages', type=int, default=1, help='number of refinement stages')\n\n    args = parser.parse_args()\n\n    net = PoseEstimationWithMobileNet()\n    if args.single_person:\n        net = SinglePersonPoseEstimationWithMobileNet(mode=args.mode_interpolation, num_refinement_stages=args.num_refinement_stages)\n    checkpoint = torch.load(args.checkpoint_path)\n    load_state(net, checkpoint)\n\n    convert_to_onnx(net, args.output_name, args.single_person, args.input_size)\n"""
pytorch_toolkit/human_pose_estimation/scripts/make_val_subset.py,0,"b""import argparse\nimport json\nimport random\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--labels', type=str, required=True, help='path to json with keypoints val labels')\n    parser.add_argument('--output-name', type=str, default='val_subset.json',\n                        help='name of output file with subset of val labels')\n    parser.add_argument('--num-images', type=int, default=250, help='number of images in subset')\n    args = parser.parse_args()\n\n    with open(args.labels, 'r') as f:\n        data = json.load(f)\n\n    random.seed(0)\n    total_val_images = 5000\n    idxs = list(range(total_val_images))\n    random.shuffle(idxs)\n\n    images_by_id = {}\n    for idx in idxs[:args.num_images]:\n        images_by_id[data['images'][idx]['id']] = data['images'][idx]\n\n    annotations_by_image_id = {}\n    for annotation in data['annotations']:\n        if annotation['image_id'] in images_by_id:\n            if not annotation['image_id'] in annotations_by_image_id:\n                annotations_by_image_id[annotation['image_id']] = []\n            annotations_by_image_id[annotation['image_id']].append(annotation)\n\n    subset = {\n        'info': data['info'],\n        'licenses': data['licenses'],\n        'images': [],\n        'annotations': [],\n        'categories': data['categories']\n    }\n    for image_id, image in images_by_id.items():\n        subset['images'].append(image)\n        if image_id in annotations_by_image_id:  # image has at least 1 annotation\n            subset['annotations'].extend(annotations_by_image_id[image_id])\n\n    with open(args.output_name, 'w') as f:\n        json.dump(subset, f, indent=4)\n\n"""
pytorch_toolkit/human_pose_estimation/scripts/prepare_train_labels.py,0,"b'import argparse\nimport json\nimport pickle\n\n\ndef prepare_annotations(annotations_per_image, images_info, net_input_size):\n    """"""Prepare labels for training. For each annotated person calculates center\n    to perform crop around it during the training. Also converts data to the internal format.\n\n    :param annotations_per_image: all annotations for specified image id\n    :param images_info: auxiliary information about all images\n    :param net_input_size: network input size during training\n    :return: list of prepared annotations\n    """"""\n    prepared_annotations = []\n    for _, annotations in annotations_per_image.items():\n        previous_centers = []\n        for annotation in annotations[0]:\n            if (annotation[\'num_keypoints\'] < 5\n                    or annotation[\'area\'] < 32 * 32):\n                continue\n            person_center = [annotation[\'bbox\'][0] + annotation[\'bbox\'][2] / 2,\n                             annotation[\'bbox\'][1] + annotation[\'bbox\'][3] / 2]\n            is_close = False\n            for previous_center in previous_centers:\n                distance_to_previous = ((person_center[0] - previous_center[0]) ** 2\n                                        + (person_center[1] - previous_center[1]) ** 2) ** 0.5\n                if distance_to_previous < previous_center[2] * 0.3:\n                    is_close = True\n                    break\n            if is_close:\n                continue\n\n            prepared_annotation = {\n                \'img_paths\': images_info[annotation[\'image_id\']][\'file_name\'],\n                \'img_width\': images_info[annotation[\'image_id\']][\'width\'],\n                \'img_height\': images_info[annotation[\'image_id\']][\'height\'],\n                \'objpos\': person_center,\n                \'image_id\': annotation[\'image_id\'],\n                \'bbox\': annotation[\'bbox\'],\n                \'segment_area\': annotation[\'area\'],\n                \'scale_provided\': annotation[\'bbox\'][3] / net_input_size,\n                \'num_keypoints\': annotation[\'num_keypoints\'],\n                \'segmentations\': annotations[1]\n            }\n\n            keypoints = []\n            for i in range(len(annotation[\'keypoints\']) // 3):\n                keypoint = [annotation[\'keypoints\'][i * 3], annotation[\'keypoints\'][i * 3 + 1], 2]\n                if annotation[\'keypoints\'][i * 3 + 2] == 1:\n                    keypoint[2] = 0\n                elif annotation[\'keypoints\'][i * 3 + 2] == 2:\n                    keypoint[2] = 1\n                keypoints.append(keypoint)\n            prepared_annotation[\'keypoints\'] = keypoints\n\n            prepared_other_annotations = []\n            for other_annotation in annotations[0]:\n                if other_annotation == annotation:\n                    continue\n\n                prepared_other_annotation = {\n                    \'objpos\': [other_annotation[\'bbox\'][0] + other_annotation[\'bbox\'][2] / 2,\n                               other_annotation[\'bbox\'][1] + other_annotation[\'bbox\'][3] / 2],\n                    \'bbox\': other_annotation[\'bbox\'],\n                    \'segment_area\': other_annotation[\'area\'],\n                    \'scale_provided\': other_annotation[\'bbox\'][3] / net_input_size,\n                    \'num_keypoints\': other_annotation[\'num_keypoints\']\n                }\n\n                keypoints = []\n                for i in range(len(other_annotation[\'keypoints\']) // 3):\n                    keypoint = [other_annotation[\'keypoints\'][i * 3], other_annotation[\'keypoints\'][i * 3 + 1], 2]\n                    if other_annotation[\'keypoints\'][i * 3 + 2] == 1:\n                        keypoint[2] = 0\n                    elif other_annotation[\'keypoints\'][i * 3 + 2] == 2:\n                        keypoint[2] = 1\n                    keypoints.append(keypoint)\n                prepared_other_annotation[\'keypoints\'] = keypoints\n                prepared_other_annotations.append(prepared_other_annotation)\n\n            prepared_annotation[\'processed_other_annotations\'] = prepared_other_annotations\n            prepared_annotations.append(prepared_annotation)\n\n            previous_centers.append((person_center[0], person_center[1], annotation[\'bbox\'][2], annotation[\'bbox\'][3]))\n    return prepared_annotations\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--labels\', type=str, required=True, help=\'path to json with keypoints train labels\')\n    parser.add_argument(\'--output-name\', type=str, default=\'prepared_train_annotation.pkl\',\n                        help=\'name of output file with prepared keypoints annotation\')\n    parser.add_argument(\'--net-input-size\', type=int, default=368, help=\'network input size\')\n    args = parser.parse_args()\n    with open(args.labels, \'r\') as f:\n        data = json.load(f)\n\n    annotations_per_image_mapping = {}\n    for annotation in data[\'annotations\']:\n        if annotation[\'num_keypoints\'] != 0 and not annotation[\'iscrowd\']:\n            if annotation[\'image_id\'] not in annotations_per_image_mapping:\n                annotations_per_image_mapping[annotation[\'image_id\']] = [[], []]\n            annotations_per_image_mapping[annotation[\'image_id\']][0].append(annotation)\n\n    crowd_segmentations_per_image_mapping = {}\n    for annotation in data[\'annotations\']:\n        if annotation[\'iscrowd\']:\n            if annotation[\'image_id\'] not in crowd_segmentations_per_image_mapping:\n                crowd_segmentations_per_image_mapping[annotation[\'image_id\']] = []\n            crowd_segmentations_per_image_mapping[annotation[\'image_id\']].append(annotation[\'segmentation\'])\n\n    for image_id, crowd_segmentations in crowd_segmentations_per_image_mapping.items():\n        if image_id in annotations_per_image_mapping:\n            annotations_per_image_mapping[image_id][1] = crowd_segmentations\n\n    images_info = {}\n    for image_info in data[\'images\']:\n        images_info[image_info[\'id\']] = image_info\n\n    prepared_annotations = prepare_annotations(annotations_per_image_mapping, images_info, args.net_input_size)\n\n    with open(args.output_name, \'wb\') as f:\n        pickle.dump(prepared_annotations, f)\n\n'"
pytorch_toolkit/instance_segmentation/segmentoly/__init__.py,0,b''
pytorch_toolkit/instance_segmentation/tools/convert_to_onnx.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport logging\nfrom pydoc import locate\n\nfrom segmentoly.datasets.factory import get_dataset\nfrom segmentoly.utils.logging import setup_logging\nfrom segmentoly.utils.onnx import onnx_export\nfrom segmentoly.utils.stats import add_flops_counting_methods, flops_to_string, print_model_with_flops\nfrom segmentoly.utils.weights import load_checkpoint\n\n\ndef parse_args():\n    """"""Parse input arguments""""""\n    parser = argparse.ArgumentParser(description=\'Export Mask R-CNN network to ONNX\')\n\n    parser.add_argument(\'--model\', type=str, required=True,\n                        help=\'Path to module and class implementing Mask R-CNN instance segmentation model.\')\n    parser.add_argument(\'--ckpt\', type=str, required=True,\n                        help=\'Checkpoint file path to load network weights from.\')\n    parser.add_argument(\'--input_size\', type=int, nargs=2, required=True,\n                        help=\'Input resolution.\')\n    parser.add_argument(\'--output_file\', type=str, required=True,\n                        help=\'Path to the output ONNX file.\')\n\n    dataset_group = parser.add_mutually_exclusive_group(required=True)\n    dataset_group.add_argument(\'--dataset\', type=str, default=None,\n                               help=\'Dataset name.\')\n    dataset_group.add_argument(\'-nc\', \'--classes_num\', type=int, default=None,\n                               help=\'Number of supported object classes.\')\n\n    parser.add_argument(\'--verbose\', action=\'store_true\',\n                        help=\'Run export in verbose mode.\')\n    parser.add_argument(\'--check\', action=\'store_true\',\n                        help=\'Run ONNX model checker after export.\')\n    parser.add_argument(\'--show_flops\', action=\'store_true\',\n                        help=\'Estimate computational complexity of the network.\')\n    return parser.parse_args()\n\n\ndef main(args):\n\n    if args.dataset:\n        dataset = get_dataset(args.dataset, False, False, None)\n        classes_num = dataset.classes_num\n    else:\n        classes_num = args.classes_num\n\n    net = locate(args.model)(classes_num, fc_detection_head=False)\n    load_checkpoint(net, args.ckpt)\n\n    if args.show_flops:\n        net = add_flops_counting_methods(net)\n        net.reset_flops_count()\n        net.start_flops_count()\n\n    printable_graph = onnx_export(net, args.input_size, args.output_file, check=args.check, verbose=args.verbose)\n\n    if args.verbose:\n        logging.info(printable_graph)\n\n    if args.show_flops:\n        net.stop_flops_count()\n        logging.info(\'Computational complexity: {}\'.format(flops_to_string(net.compute_average_flops_cost())))\n        if args.verbose:\n            print_model_with_flops(net)\n\n\nif __name__ == \'__main__\':\n    setup_logging()\n    args = parse_args()\n    main(args)\n'"
pytorch_toolkit/instance_segmentation/tools/demo.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport logging\nimport sys\nfrom pydoc import locate\n\nfrom tqdm import tqdm\n\nfrom segmentoly.data.dataparallel import collate\nfrom segmentoly.data.transforms import *\nfrom segmentoly.datasets.factory import get_dataset\nfrom segmentoly.datasets.images import ImagesDataset\nfrom segmentoly.datasets.video import VideoDataset\nfrom segmentoly.rcnn.openvino_net import MaskRCNNOpenVINO\nfrom segmentoly.utils.logging import setup_logging\nfrom segmentoly.utils.postprocess import postprocess\nfrom segmentoly.utils.profile import Timer\nfrom segmentoly.utils.stats import add_flops_counting_methods, flops_to_string, print_model_with_flops\nfrom segmentoly.utils.tracker import StaticIOUTracker\nfrom segmentoly.utils.visualizer import Visualizer\nfrom segmentoly.utils.weights import load_checkpoint\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Run instance segmentation live\')\n    subparsers = parser.add_subparsers(help=\'Backend\', dest=\'backend\')\n\n    parser.add_argument(\'--dataset\', help=\'Training dataset\')\n    parser.add_argument(\'--ckpt\', dest=\'checkpoint_file_path\', required=True, help=\'File with models weights\')\n    parser.add_argument(\'--mean_pixel\', default=(0.0, 0.0, 0.0), type=float, nargs=3,\n                        metavar=\'<num>\',\n                        help=\'Mean pixel value to subtract from image.\')\n    parser.add_argument(\'--rgb\', action=\'store_true\',\n                        help=\'Use RGB instead of BGR.\')\n    image_resize_group = parser.add_mutually_exclusive_group(required=True)\n    image_resize_group.add_argument(\'--fit_max\', dest=\'fit_max_image_size\', default=None, type=int, nargs=2,\n                                    metavar=\'<num>\',\n                                    help=\'Max processed image size in a format \'\n                                         \'(max short side, max long side).\')\n    image_resize_group.add_argument(\'--fit_window\', dest=\'fit_window_size\', default=None, type=int, nargs=2,\n                                    metavar=\'<num>\',\n                                    help=\'Max processed image size in a format (max height, max width).\')\n    image_resize_group.add_argument(\'--size\', dest=\'size\', default=None, type=int, nargs=2,\n                                    metavar=\'<num>\',\n                                    help=\'Precise input size (height, width).\')\n\n    data_source_parser = parser.add_mutually_exclusive_group(required=True)\n    data_source_parser.add_argument(\'-v\', \'--video\', default=None, type=str,\n                                    help=\'Path to a video file or numeric camera ID.\')\n    data_source_parser.add_argument(\'-i\', \'--images\', default=None, type=str,\n                                    help=\'Path to an image or a folder with images.\')\n\n    parser.add_argument(\'--show_scores\', action=\'store_true\',\n                        help=\'Show detection scores.\')\n    parser.add_argument(\'--show_boxes\', action=\'store_true\',\n                        help=\'Show bounding boxes.\')\n    parser.add_argument(\'--show_fps\', action=\'store_true\',\n                        help=\'Show FPS.\')\n    parser.add_argument(\'-pt\', \'--prob_threshold\', default=0.5, type=float,\n                        help=\'Probability threshold for detections filtering.\')\n    parser.add_argument(\'--delay\', default=0, type=int)\n\n    openvino_parser = subparsers.add_parser(\'openvino\')\n    openvino_parser.add_argument(\'--model\', dest=\'openvino_model_path\', default=None, type=str, required=True,\n                                 help=\'XML file with model description (OpenVINO format)\')\n    openvino_parser.add_argument(\'-d\', \'--device\',\n                                 help=\'Optional. Specify the target device to infer on: CPU, GPU, FPGA or MYRIAD. \'\n                                      \'The demo will look for a suitable plugin for device specified \'\n                                      \'(by default, it is CPU).\',\n                                 default=\'CPU\', type=str, metavar=\'""<device>""\')\n    openvino_parser.add_argument(\'-l\', \'--cpu_extension\',\n                                 help=\'Required for CPU custom layers. \'\n                                      \'Absolute path to a shared library with the kernels implementation.\',\n                                 default=None, type=str, metavar=\'""<absolute_path>""\')\n    openvino_parser.add_argument(\'-pp\', \'--plugin_dir\',\n                                 help=\'Optional. Path to a plugin folder.\',\n                                 default=None, type=str, metavar=\'""<absolute_path>""\')\n    openvino_parser.add_argument(\'--show_pc\', \'--show_performance_counters\', dest=\'show_performance_counters\',\n                                 help=\'Show OpenVINO performance counters.\',\n                                 action=\'store_true\')\n\n    pytorch_parser = subparsers.add_parser(\'pytorch\')\n    pytorch_parser.add_argument(\'--model\', dest=\'pytorch_model_class\', type=str, required=True,\n                                help=\'Path to module and class implementing Mask R-CNN instance segmentation model.\')\n    pytorch_parser.add_argument(\'--show_flops\',\n                                help=\'Show FLOPs.\',\n                                action=\'store_true\')\n    pytorch_parser.add_argument(\'--show_layers_flops\',\n                                help=\'Show FLOPs for all modules.\',\n                                action=\'store_true\')\n\n    return parser.parse_args()\n\n\ndef main(args):\n    transforms = Compose(\n        [\n            Resize(max_size=args.fit_max_image_size, window_size=args.fit_window_size, size=args.size),\n            ToTensor(),\n            Normalize(mean=args.mean_pixel, std=[1., 1., 1.], rgb=args.rgb),\n        ]\n    )\n    dataset = get_dataset(args.dataset, False, False, transforms)\n    logging.info(dataset)\n    batch_size = 1\n\n    logging.info(\'Using {} backend\'.format(args.backend))\n\n    logging.info(\'Loading network...\')\n    if args.backend == \'pytorch\':\n        net = locate(args.pytorch_model_class)(dataset.classes_num)\n        net.eval()\n        load_checkpoint(net, args.checkpoint_file_path)\n        if torch.cuda.is_available():\n            net = net.cuda()\n        net = add_flops_counting_methods(net)\n        net.reset_flops_count()\n        net.start_flops_count()\n    elif args.backend == \'openvino\':\n        net = MaskRCNNOpenVINO(args.openvino_model_path, args.checkpoint_file_path,\n                               device=args.device, plugin_dir=args.plugin_dir,\n                               cpu_extension_lib_path=args.cpu_extension,\n                               collect_perf_counters=args.show_performance_counters)\n    else:\n        raise ValueError(\'Unknown backend ""{}""\'.format(args.backend))\n\n    viz = Visualizer(dataset.classes, confidence_threshold=args.prob_threshold,\n                     show_boxes=args.show_boxes, show_scores=args.show_scores)\n\n    inference_timer = Timer(cuda_sync=True, warmup=1)\n    timer = Timer(cuda_sync=False, warmup=1)\n    timer.tic()\n\n    logging.info(\'Configuring data source...\')\n    if args.video:\n        try:\n            args.video = int(args.video)\n        except ValueError:\n            pass\n        demo_dataset = VideoDataset(args.video, labels=dataset.classes, transforms=transforms)\n        num_workers = 0\n        tracker = StaticIOUTracker()\n    else:\n        demo_dataset = ImagesDataset(args.images, labels=dataset.classes, transforms=transforms)\n        num_workers = 1\n        tracker = None\n\n    data_loader = torch.utils.data.DataLoader(\n        demo_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=collate\n    )\n\n    logging.info(\'Processing data...\')\n    frames_num = len(demo_dataset)\n    for data_batch in tqdm(iter(data_loader), total=frames_num if frames_num != sys.maxsize else 0):\n        im_data = data_batch[\'im_data\']\n        im_info = data_batch[\'im_info\']\n        if torch.cuda.is_available():\n            im_data = [i.cuda() for i in im_data]\n            im_info = [i.cuda() for i in im_info]\n        with torch.no_grad(), inference_timer:\n            boxes, classes, scores, _, masks = net(im_data, im_info)\n\n        meta = data_batch[\'meta\'][0]\n        scores, classes, boxes, masks = postprocess(scores, classes, boxes, masks,\n                                                    im_h=meta[\'original_size\'][0],\n                                                    im_w=meta[\'original_size\'][1],\n                                                    im_scale_y=meta[\'processed_size\'][0] / meta[\'original_size\'][0],\n                                                    im_scale_x=meta[\'processed_size\'][1] / meta[\'original_size\'][1],\n                                                    full_image_masks=True, encode_masks=False,\n                                                    confidence_threshold=args.prob_threshold)\n\n        masks_ids = tracker(masks, classes) if tracker is not None else None\n        image = data_batch[\'original_image\'][0]\n        visualization = viz(image, boxes, classes, scores, segms=masks, ids=masks_ids)\n        fps = 1 / timer.toc()\n        if args.show_fps:\n            visualization = cv2.putText(visualization, \'FPS: {:>2.2f}\'.format(fps),\n                                        (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n        cv2.imshow(\'result\', visualization)\n        key = cv2.waitKey(args.delay)\n        if key == 27:\n            break\n        timer.tic()\n\n    if inference_timer.average_time > 0:\n        logging.info(\'Average inference FPS: {:3.2f}\'.format(1 / inference_timer.average_time))\n\n    if args.backend == \'pytorch\':\n        net.stop_flops_count()\n        if args.show_flops:\n            logging.info(\'Average FLOPs:  {}\'.format(flops_to_string(net.compute_average_flops_cost())))\n        if args.show_layers_flops:\n            logging.info(\'Thorough computational complexity statistics:\')\n            print_model_with_flops(net)\n        if torch.cuda.is_available():\n            logging.info(\'GPU memory footprint:\')\n            logging.info(\'\\tMax allocated: {:.2f} MiB\'.format(torch.cuda.max_memory_allocated() / 1024 ** 2))\n            logging.info(\'\\tMax cached:    {:.2f} MiB\'.format(torch.cuda.max_memory_cached() / 1024 ** 2))\n    else:\n        if args.show_performance_counters:\n            net.print_performance_counters()\n\n    cv2.destroyAllWindows()\n    del net\n\n\nif __name__ == \'__main__\':\n    setup_logging()\n    args = parse_args()\n    main(args)\n'"
pytorch_toolkit/instance_segmentation/tools/download_pretrained_weights.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport logging\nimport os\nimport os.path as osp\nfrom pydoc import locate\nfrom subprocess import call\n\nimport requests\nimport yaml\nfrom termcolor import colored\nfrom tqdm import tqdm\n\nfrom segmentoly.utils.logging import setup_logging\nfrom segmentoly.utils.onnx import onnx_export\nfrom segmentoly.utils.weights import load_checkpoint\n\n\ndef download_file_from_web(url, destination):\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    save_response_content(response, destination)\n\n\ndef download_file_from_google_drive(id, destination):\n    def get_confirm_token(response):\n        for key, value in response.cookies.items():\n            if key.startswith(\'download_warning\'):\n                return value\n\n        return None\n\n    URL = \'https://docs.google.com/uc?export=download\'\n    session = requests.Session()\n    response = session.get(URL, params={\'id\': id}, stream=True)\n    response.raise_for_status()\n    token = get_confirm_token(response)\n    if token:\n        params = {\'id\': id, \'confirm\': token}\n        response = session.get(URL, params=params, stream=True)\n        response.raise_for_status()\n    save_response_content(response, destination)\n\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32 * 1024\n\n    file_size = None\n    if \'Content-Length\' in response.headers:\n        file_size = int(response.headers[\'Content-Length\'])\n\n    with open(destination, \'wb\') as f, \\\n            tqdm(desc=\'Downloading...\', total=file_size, unit=\'B\', unit_scale=True,\n                 unit_divisor=1024, leave=True) as pbar:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk:  # filter out keep-alive new chunks\n                pbar.update(len(chunk))\n                f.write(chunk)\n\n\ndef mkdir_for_file(file_path):\n    dir_path = osp.dirname(file_path)\n    if not osp.exists(dir_path):\n        os.makedirs(dir_path)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--model_zoo\', help=\'File with models to download and convert.\',\n                        default=osp.join(osp.dirname(osp.abspath(__file__)), \'model_zoo.yaml\'))\n    parser.add_argument(\'--output_dir\', help=\'Directory to save downloaded weights files.\',\n                        default=os.path.join(\'data\', \'pretrained_models\'))\n    parser.add_argument(\'-mo\', \'--model_optimizer\', dest=\'model_optimizer\',\n                        help=\'Model optimizer executable.\', default=\'mo.py\')\n    return parser.parse_args()\n\n\ndef main(args):\n    args.output_dir = osp.abspath(args.output_dir)\n\n    downloaders = dict(web=download_file_from_web,\n                       google_drive=download_file_from_google_drive)\n\n    with open(args.model_zoo, \'rt\') as f:\n        model_zoo = yaml.load(f)[\'models\']\n\n    logging.info(\'Models to be fetched:\')\n    for target in model_zoo:\n        logging.info(\'\\t{} ({}, {})\'.format(colored(target[\'name\'], \'green\'),\n                                           target[\'dataset\'], target[\'framework\']))\n    logging.info(\'\')\n\n    for target in model_zoo:\n        output_file = osp.join(args.output_dir, \'raw\', target[\'dst_file\'])\n        mkdir_for_file(output_file)\n        logging.info(\'Fetching {} ({}, {})\'.format(colored(target[\'name\'], \'green\'),\n                                                  target[\'dataset\'], target[\'framework\']))\n        try:\n            # Download weights.\n            if target[\'storage_type\'] in downloaders:\n                downloaders[target[\'storage_type\']](target[\'url\'], output_file)\n            else:\n                logging.warning(\'No downloaders available for storage {}\'.format(target[\'storage_type\']))\n                continue\n\n            # Convert weights.\n            logging.info(\'Downloaded to {}\'.format(output_file))\n            if target.get(\'weights_converter\', None):\n                logging.info(\'Converting weights...\')\n                converter = locate(target[\'weights_converter\'])\n                if converter is None:\n                    logging.warning(\'Invalid weights converter {}\'.format(target[\'weights_converter\']))\n                    continue\n                output_converted_file = osp.join(args.output_dir, \'converted\', target[\'dst_file\'])\n                output_converted_file = osp.splitext(output_converted_file)[0] + \'.pth\'\n                mkdir_for_file(output_converted_file)\n                try:\n                    converter(output_file, output_converted_file)\n                    logging.info(\'Converted weights file saved to {}\'.format(output_converted_file))\n                except Exception as ex:\n                    logging.warning(\'Failed to convert weights.\')\n                    logging.warning(ex)\n                    continue\n\n                if target.get(\'convert_to_ir\', False):\n                    # Convert to ONNX.\n                    logging.info(\'Exporting to ONNX...\')\n                    output_onnx_file = osp.join(args.output_dir, \'onnx\', target[\'dst_file\'])\n                    output_onnx_file = osp.splitext(output_onnx_file)[0] + \'.onnx\'\n                    mkdir_for_file(output_onnx_file)\n                    net = locate(target[\'model\'])(81, fc_detection_head=False)\n                    load_checkpoint(net, output_converted_file, verbose=False)\n                    onnx_export(net, target[\'input_size\'], output_onnx_file)\n                    logging.info(\'ONNX file is saved to {}\'.format(output_onnx_file))\n\n                    # Convert to IR.\n                    logging.info(\'Converting to IR...\')\n                    output_ir_dir = osp.join(args.output_dir, \'ir\', target[\'dst_file\'])\n                    mkdir_for_file(output_ir_dir)\n                    output_ir_dir = osp.dirname(output_ir_dir)\n                    status = call([args.model_optimizer,\n                                   \'--framework\', \'onnx\',\n                                   \'--input_model\', output_onnx_file,\n                                   \'--output_dir\', output_ir_dir,\n                                   \'--input\', \'im_data,im_info\',\n                                   \'--output\', \'boxes,scores,classes,batch_ids,raw_masks\',\n                                   \'--mean_values\',\n                                   \'im_data{},im_info[0,0,0]\'.format(str(target[\'mean_pixel\']).replace(\' \', \'\'))\n                                   ])\n                    if status:\n                        logging.warning(\'Failed to convert model to IR.\')\n                    else:\n                        logging.info(\'IR files saved to {}\'.format(output_ir_dir))\n\n        except Exception as ex:\n            logging.warning(repr(ex))\n\n\nif __name__ == \'__main__\':\n    setup_logging()\n    args = parse_args()\n    main(args)\n'"
pytorch_toolkit/instance_segmentation/tools/finetune_0050.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\nimport os.path as osp\nimport resource\n\nfrom segmentoly.data.dataparallel import collate\nfrom segmentoly.data.transforms import *\nfrom segmentoly.datasets.factory import get_dataset\nfrom segmentoly.rcnn.model_zoo.instance_segmentation_security_0050 import InstanceSegmentationSecurity0050 as Model\nfrom segmentoly.utils.logging import setup_logging, TextLogger, TensorboardLogger\nfrom segmentoly.utils.lr_scheduler import MultiStepLRWithWarmUp\nfrom segmentoly.utils.training_engine import DefaultMaskRCNNTrainingEngine\nfrom segmentoly.utils.weights import load_checkpoint\n\nlogger = logging.getLogger(__name__)\n\n\nclass Trainer(DefaultMaskRCNNTrainingEngine):\n    def __init__(self):\n        super().__init__()\n        self.identifier = \'instance-segmentation-security-0050\'\n        self.description = \'Fine-tuning of instance-segmentation-security-0050\'\n        self.root_directory = osp.join(osp.dirname(osp.abspath(__file__)), \'..\')\n        self.run_directory = self.create_run_directory(osp.join(self.root_directory, \'outputs\'))\n\n        setup_logging(file_path=osp.join(self.run_directory, \'log.txt\'))\n\n        logger.info(\'Running {}\'.format(self.identifier))\n        logger.info(self.description)\n        logger.info(\'Working directory ""{}""\'.format(self.run_directory))\n\n        self.batch_size = 32\n        self.virtual_iter_size = 1\n\n        # Training dataset.\n        training_transforms = Compose(\n            [\n                RandomResize(mode=\'size\', heights=(416, 448, 480, 512, 544), widths=(416, 448, 480, 512, 544)),\n                RandomHorizontalFlip(prob=0.5),\n                ToTensor(),\n                Normalize(mean=[102.9801, 115.9465, 122.7717], std=[1., 1., 1.], rgb=False),\n            ],\n        )\n        training_dataset_name = \'coco_2017_train\'\n        logger.info(\'Training dataset {}\'.format(training_dataset_name))\n        training_dataset = get_dataset(training_dataset_name, True, True, training_transforms)\n        logger.info(training_dataset)\n        self.training_data_loader = torch.utils.data.DataLoader(\n            training_dataset, batch_size=self.batch_size, num_workers=0,\n            shuffle=True, drop_last=True, collate_fn=collate\n        )\n\n        # Validation datasets.\n        validation_transforms = Compose(\n            [\n                Resize(size=[480, 480]),\n                ToTensor(),\n                Normalize(mean=[102.9801, 115.9465, 122.7717], std=[1., 1., 1.], rgb=False),\n            ]\n        )\n        validation_datasets = []\n        validation_dataset_name = \'coco_2017_val\'\n        logger.info(\'Validation dataset #{}: {}\'.format(len(validation_datasets) + 1, validation_dataset_name))\n        validation_datasets.append(get_dataset(validation_dataset_name, False, False, validation_transforms))\n        logger.info(validation_datasets[-1])\n\n        self.validation_data_loaders = []\n        for validation_dataset in validation_datasets:\n            self.validation_data_loaders.append(torch.utils.data.DataLoader(\n                validation_dataset,\n                batch_size=1, num_workers=8,\n                shuffle=False, drop_last=False, collate_fn=collate)\n            )\n        self.validate_every = 500\n\n        for validation_dataset in validation_datasets:\n            assert training_dataset.classes_num == validation_dataset.classes_num\n\n        # Model and optimizer.\n        logger.info(\'Model:\')\n        self.model = Model(training_dataset.classes_num)\n        logger.info(self.model)\n\n        self.training_iterations_num = 500\n        lr_scheduler_milestones = [500]\n        base_lr = 0.001\n        weight_decay = 0.0001\n        logger.info(\'Optimizer:\')\n        self.optimizer = torch.optim.SGD(self.setup_optimizer(self.model, base_lr, weight_decay),\n                                         lr=base_lr, weight_decay=weight_decay, momentum=0.9)\n        logger.info(self.optimizer)\n        logger.info(\'Learning Rate scheduler:\')\n        self.lr_scheduler = MultiStepLRWithWarmUp(\n            self.optimizer,\n            milestones=lr_scheduler_milestones,\n            warmup_iters=100,\n            warmup_method=\'linear\',\n            warmup_factor_base=0.333,\n            gamma=0.1,\n            last_epoch=0\n        )\n        logger.info(self.lr_scheduler)\n\n        self.start_step = 0\n        checkpoint_file_path = osp.join(self.root_directory, \'data\', \'pretrained_models\',\n                                        \'converted\', \'coco\', \'ote\', \'instance_segmentation_security_0050.pth\')\n        if not osp.exists(checkpoint_file_path):\n            raise IOError(\'Initial checkpoint file ""{}"" does not exist. \'\n                          \'Please fetch pretrained networks using \'\n                          \'tools/download_pretrained_weights.py script first.\'.format(checkpoint_file_path))\n        logger.info(\'Loading weights from ""{}""\'.format(checkpoint_file_path))\n        load_checkpoint(self.model, checkpoint_file_path)\n\n        # Loggers and misc. stuff.\n        self.loggers = [TextLogger(logger),\n                        TensorboardLogger(self.run_directory)]\n        self.log_every = 50\n\n        self.checkpoint_every = 500\n\n\nif __name__ == \'__main__\':\n    # RuntimeError: received 0 items of ancdata. Issue: pytorch/pytorch#973\n    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n    resource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n\n    experiment = Trainer()\n    torch.backends.cudnn.benchmark = False\n    experiment.run(experiment.start_step)\n    logger.info(\'Done.\')\n'"
pytorch_toolkit/instance_segmentation/tools/test.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport logging\nfrom pydoc import locate\n\nfrom tqdm import tqdm\n\nfrom segmentoly.data.dataparallel import collate, ShallowDataParallel\nfrom segmentoly.data.transforms import *\nfrom segmentoly.datasets.factory import get_dataset\nfrom segmentoly.rcnn.openvino_net import MaskRCNNOpenVINO\nfrom segmentoly.utils.logging import setup_logging\nfrom segmentoly.utils.postprocess import postprocess_batch\nfrom segmentoly.utils.stats import add_flops_counting_methods, flops_to_string, print_model_with_flops\nfrom segmentoly.utils.weights import load_checkpoint\nfrom segmentoly.utils.profile import Timer\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Evaluate instance segmentation\')\n    subparsers = parser.add_subparsers(help=\'Backend\', dest=\'backend\')\n\n    parser.add_argument(\'--dataset\', help=\'Dataset name.\', metavar=\'""<name>""\',)\n    parser.add_argument(\'--ckpt\', dest=\'checkpoint_file_path\', required=True, metavar=\'""<path>""\',\n                        help=\'File with models weights.\')\n    parser.add_argument(\'--nw\', dest=\'num_workers\', default=8, type=int, metavar=\'<num>\',\n                        help=\'Number of data loading workers.\')\n    parser.add_argument(\'--mean_pixel\', default=(0.0, 0.0, 0.0), type=float, nargs=3,\n                        metavar=\'<num>\',\n                        help=\'Mean pixel value to subtract from image.\')\n    parser.add_argument(\'--rgb\', action=\'store_true\',\n                        help=\'Use RGB instead of BGR.\')\n    image_resize_group = parser.add_mutually_exclusive_group(required=True)\n    image_resize_group.add_argument(\'--fit_max\', dest=\'fit_max_image_size\', default=None, type=int, nargs=2,\n                                    metavar=\'<num>\',\n                                    help=\'Max processed image size in a (max short side, max long side) format.\')\n    image_resize_group.add_argument(\'--fit_window\', dest=\'fit_window_size\', default=None, type=int, nargs=2,\n                                    metavar=\'<num>\',\n                                    help=\'Max processed image size in a (max height, max width) format.\')\n    image_resize_group.add_argument(\'--size\', dest=\'size\', default=None, type=int, nargs=2,\n                                    metavar=\'<num>\',\n                                    help=\'Input resolution in a (height, width) format.\')\n\n    openvino_parser = subparsers.add_parser(\'openvino\')\n    openvino_parser.add_argument(\'--model\', dest=\'openvino_model_path\', type=str, required=True, metavar=\'""<path>""\',\n                                 help=\'XML file with model description (OpenVINO format)\')\n    openvino_parser.add_argument(\'-d\', \'--device\',\n                                 help=\'Optional. Specify the target device to infer on: CPU, GPU, FPGA or MYRIAD. \'\n                                      \'The demo will look for a suitable plugin for device specified \'\n                                      \'(by default, it is CPU).\',\n                                 default=\'CPU\', type=str, metavar=\'""<device>""\')\n    openvino_parser.add_argument(\'-l\', \'--cpu_extension\',\n                                 help=\'Required for CPU custom layers. \'\n                                      \'Absolute path to a shared library with the kernels implementation.\',\n                                 default=None, type=str, metavar=\'""<absolute_path>""\')\n    openvino_parser.add_argument(\'-pp\', \'--plugin_dir\',\n                                 help=\'Optional. Path to a plugin folder.\',\n                                 default=None, type=str, metavar=\'""<absolute_path>""\')\n    openvino_parser.add_argument(\'--show_pc\', \'--show_performance_counters\', dest=\'show_performance_counters\',\n                                 help=\'Show OpenVINO performance counters.\',\n                                 action=\'store_true\')\n\n    pytorch_parser = subparsers.add_parser(\'pytorch\')\n    pytorch_parser.add_argument(\'--model\', dest=\'pytorch_model_class\', type=str, required=True, metavar=\'""<path>""\',\n                                help=\'Path to module and class implementing Mask R-CNN instance segmentation model.\')\n    pytorch_parser.add_argument(\'--batch_size\', default=1, type=int, metavar=\'<num>\',\n                                help=\'Batch size for all GPUs.\')\n    pytorch_parser.add_argument(\'--show_flops\',\n                                help=\'Show FLOPs.\',\n                                action=\'store_true\')\n    pytorch_parser.add_argument(\'--show_layers_flops\',\n                                help=\'Show FLOPs for all modules.\',\n                                action=\'store_true\')\n\n    return parser.parse_args()\n\n\ndef main(args):\n    transforms = Compose(\n        [\n            Resize(max_size=args.fit_max_image_size, window_size=args.fit_window_size, size=args.size),\n            ToTensor(),\n            Normalize(mean=args.mean_pixel, std=[1., 1., 1.], rgb=args.rgb),\n        ]\n    )\n    dataset = get_dataset(args.dataset, False, False, transforms)\n    logging.info(dataset)\n    num_workers = args.num_workers\n\n    inference_timer = Timer()\n\n    logging.info(\'Using {} backend\'.format(args.backend))\n\n    logging.info(\'Loading network...\')\n    if args.backend == \'pytorch\':\n        net = locate(args.pytorch_model_class)(dataset.classes_num, force_max_output_size=False)\n        net.eval()\n        load_checkpoint(net, args.checkpoint_file_path, verbose=True)\n        net = add_flops_counting_methods(net)\n        net.reset_flops_count()\n        net.start_flops_count()\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = True\n            net = net.cuda()\n            net = ShallowDataParallel(net)\n        batch_size = args.batch_size\n    elif args.backend == \'openvino\':\n        net = MaskRCNNOpenVINO(args.openvino_model_path, args.checkpoint_file_path,\n                               device=args.device, plugin_dir=args.plugin_dir,\n                               cpu_extension_lib_path=args.cpu_extension,\n                               collect_perf_counters=args.show_performance_counters)\n        batch_size = 1\n    else:\n        raise ValueError(\'Unknown backend ""{}""\'.format(args.backend))\n\n    logging.info(\'Using batch size {}\'.format(batch_size))\n    logging.info(\'Number of prefetching processes {}\'.format(num_workers))\n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=collate\n    )\n\n    logging.info(\'Processing dataset...\')\n    boxes_all = []\n    masks_all = []\n    classes_all = []\n    scores_all = []\n    for data_batch in tqdm(iter(data_loader)):\n        batch_meta = data_batch[\'meta\']\n        actual_batch_size = len(batch_meta)\n        with torch.no_grad(), inference_timer:\n            boxes, classes, scores, batch_ids, masks = net(**data_batch)\n\n        im_heights = [meta[\'original_size\'][0] for meta in batch_meta]\n        im_widths = [meta[\'original_size\'][1] for meta in batch_meta]\n        im_scale_y = [meta[\'processed_size\'][0] / meta[\'original_size\'][0] for meta in batch_meta]\n        im_scale_x = [meta[\'processed_size\'][1] / meta[\'original_size\'][1] for meta in batch_meta]\n        scores, classes, boxes, masks = postprocess_batch(batch_ids, scores, classes, boxes, masks, actual_batch_size,\n                                                          im_h=im_heights,\n                                                          im_w=im_widths,\n                                                          im_scale_y=im_scale_y,\n                                                          im_scale_x=im_scale_x,\n                                                          full_image_masks=True, encode_masks=True)\n        boxes_all.extend(boxes)\n        masks_all.extend(masks)\n        classes_all.extend(classes)\n        scores_all.extend(scores)\n\n    try:\n        del data_loader\n    except ConnectionResetError:\n        pass\n\n    logging.info(\'Evaluating results...\')\n    evaluation_results = dataset.evaluate(scores_all, classes_all, boxes_all, masks_all)\n    logging.info(evaluation_results)\n\n    logging.info(\'Average inference time {}\'.format(inference_timer.average_time))\n\n    if args.backend == \'pytorch\':\n        if torch.cuda.is_available():\n            net = net.module\n        net.stop_flops_count()\n        if args.show_flops:\n            logging.info(\'Average FLOPs:  {}\'.format(flops_to_string(net.compute_average_flops_cost())))\n        if args.show_layers_flops:\n            logging.info(\'Thorough computational complexity statistics:\')\n            print_model_with_flops(net)\n    else:\n        if args.show_performance_counters:\n            net.print_performance_counters()\n\n    del net\n\n\nif __name__ == \'__main__\':\n    setup_logging()\n    args = parse_args()\n    main(args)\n'"
pytorch_toolkit/instance_segmentation/tools/train.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport logging\nimport os.path as osp\nfrom pydoc import locate\nimport yaml\n\nimport torch\n\nfrom segmentoly.data.dataparallel import collate\nfrom segmentoly.data.transforms import *\nfrom segmentoly.datasets.factory import get_dataset\nfrom segmentoly.utils.lr_scheduler import MultiStepLRWithWarmUp\nfrom segmentoly.utils.logging import setup_logging, TextLogger, TensorboardLogger\nfrom segmentoly.utils.training_engine import DefaultMaskRCNNTrainingEngine\n\nsetup_logging()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Train Mask R-CNN\',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # Main parameters\n    parser.add_argument(\'--model\', default=\'segmentoly.rcnn.model_zoo.resnet_fpn_mask_rcnn.ResNet50FPNMaskRCNN\',\n                        help=\'Path to model\')\n    parser.add_argument(\'--dataset\', default=\'coco2017\',\n                        help=\'Dataset name\')\n    parser.add_argument(\'--load_cfg\',\n                        help=\'Path to loading a configuration file\')\n    parser.add_argument(\'--display_interval\', type=int, default=20,\n                        help=\'Display training info every N iterations\')\n    parser.add_argument(\'--nw\', type=int, default=4, dest=\'num_workers\',\n                        help=\'Number of workers to load data\')\n    parser.add_argument(\'--bs\', type=int, default=16, dest=\'batch_size\',\n                        help=\'Total batch size\')\n    parser.add_argument(\'--bs_per_gpu\', type=int, default=2,  dest=\'batch_size_per_gpu\',\n                        help=\'Number of images per GPU\')\n    parser.add_argument(\'--resume\', action=\'store_true\',\n                        help=\'Resume training from a checkpoint\')\n    parser.add_argument(\'--output_dir\', default=osp.join(osp.dirname(osp.realpath(__file__)), \'../outputs\'),\n                        help=\'Directory with output data\')\n    parser.add_argument(\'--load_ckpt\',\n                        help=\'Checkpoint path to load\')\n    parser.add_argument(\'--load_backbone\',\n                        help=\'Path to backbone weights file\')\n    parser.add_argument(\'--save_cfg\',\n                        help=\'Save current configuration to file\')\n    parser.add_argument(\'--checkpoint_interval\', type=int, default=10000,\n                        help=\'Save checkpoints every N iterations\')\n    parser.add_argument(\'--test_interval\', type=int, default=10000,\n                        help=\'Test net every N iterations\')\n    # Optimizer parameters\n    parser.add_argument(\'--lr\', default=0.02, type=float,\n                        help=\'Initial learning rate\')\n    parser.add_argument(\'--weight_decay\', default=0.0001, type=float,\n                        help=\'Weight decay\')\n    parser.add_argument(\'--momentum\', default=0.9, type=float,\n                        help=\'Momentum\')\n    # Warmup parameters\n    parser.add_argument(\'--warmup_iters\', default=1000, type=int,\n                        help=\'Number of iterations for warm up\')\n    parser.add_argument(\'--warmup_factor\', default=0.33, type=float,\n                        help=\'Warm up factor\')\n    parser.add_argument(\'--warmup_method\', default=\'linear\', choices=[\'linear\', \'constant\'],\n                        help=\'Warm up method\')\n    # Schedule parameters\n    parser.add_argument(\'--max_iter\', default=90000, type=int,\n                        help=\'Maximum number of iterations\')\n    parser.add_argument(\'--drop_lr\', default=(60000, 80000), nargs=\'+\', type=int, metavar=\'<num>\',\n                        help=\'Milestones to drop learning rate\')\n    parser.add_argument(\'--gamma\', default=0.1, type=float,\n                        help=\'Gamma for learning rate decay\')\n    # Input image parameters\n    parser.add_argument(\'--max_image_size\', default=(800, 1333), type=int, nargs=2, metavar=\'<num>\',\n                        help=\'Max processed image size in a format (max short side, max long side)\')\n    parser.add_argument(\'--mean_pixel\', default=(102.9801, 115.9465, 122.7717), type=float, nargs=3, metavar=\'<num>\',\n                        help=\'Mean pixel value to subtract from image\')\n\n    args = parser.parse_args()\n\n    # Load config\n    if args.load_cfg:\n        logging.info(\'Loading configuration file ""{}""...\'.format(args.load_cfg))\n        with open(args.load_cfg) as f:\n            config = yaml.load(f)\n        for k, v in config.items():\n            v = None if v == \'None\' else v\n            setattr(args, k, v)\n\n    # Save config\n    if args.save_cfg and not args.load_cfg:\n        with open(args.save_cfg, \'w\') as f:\n            config = {}\n            for arg in vars(args):\n                config[arg] = getattr(args, arg)\n            yaml.dump(config, f)\n        logging.info(\'Configuration file saved to ""{}""\'.format(args.save_cfg))\n\n    args_line = \'\'\n    for arg in vars(args):\n        args_line += \'\\n -\' + str(arg) + \': \' + str(getattr(args, arg))\n    logging.info(\'Called with args: {}\'.format(args_line))\n\n    return args\n\n\ndef main(args):\n    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n\n    train_tool = DefaultMaskRCNNTrainingEngine()\n\n    train_tool.identifier = \'MaskRCNN\'\n    train_tool.description = \'\'\n\n    train_tool.set_random_seeds()\n\n    train_tool.root_directory = osp.join(args.output_dir, train_tool.identifier)\n    train_tool.run_directory = train_tool.create_run_directory(train_tool.root_directory)\n\n    train_tool.batch_size = args.batch_size\n    train_tool.virtual_iter_size = \\\n        train_tool.adjust_virtual_iteration_size(num_gpus, args.batch_size, args.batch_size_per_gpu)\n\n    transforms = Compose(\n        [\n            Resize(max_size=args.max_image_size),\n            ToTensor(),\n            Normalize(mean=args.mean_pixel, std=[1., 1., 1.], rgb=False),\n        ]\n    )\n\n    if args.dataset == \'coco2017\':\n        train_dataset_name = \'coco_2017_train\'\n        val_dataset_name = \'coco_2017_val\'\n    else:\n        raise ValueError(\'Invalid dataset name ""{}""\'.format(args.dataset))\n\n    train_dataset = get_dataset(train_dataset_name, True, True, transforms)\n    val_dataset = get_dataset(val_dataset_name, False, False, transforms)\n    assert train_dataset.classes_num == val_dataset.classes_num\n\n    train_tool.training_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=args.batch_size // train_tool.virtual_iter_size,\n        num_workers=args.num_workers,\n        shuffle=True,\n        collate_fn=collate\n    )\n\n    train_tool.validation_data_loaders = [\n        torch.utils.data.DataLoader(\n            val_dataset,\n            batch_size=1,\n            num_workers=args.num_workers,\n            shuffle=False,\n            collate_fn=collate\n        )\n    ]\n\n    train_tool.validate_every = args.test_interval\n\n    train_tool.model = locate(args.model)(train_dataset.classes_num)\n\n    train_tool.training_iterations_num = args.max_iter\n    train_tool.lr_scheduler_milestones = args.drop_lr\n\n    params = train_tool.setup_optimizer(train_tool.model, args.lr, args.weight_decay)\n    train_tool.optimizer = torch.optim.SGD(params, momentum=args.momentum)\n\n    start_step = 0\n    if args.load_ckpt or args.load_backbone:\n        start_step = train_tool.load_checkpoint(train_tool.model, train_tool.optimizer,\n                                                args.load_ckpt, args.load_backbone, args.resume)\n\n    train_tool.lr_scheduler = MultiStepLRWithWarmUp(\n        train_tool.optimizer,\n        milestones=args.drop_lr,\n        warmup_iters=args.warmup_iters,\n        warmup_method=args.warmup_method,\n        warmup_factor_base=args.warmup_factor,\n        gamma=args.gamma,\n        last_epoch=start_step\n    )\n\n    text_log = TextLogger(logging.getLogger(__name__))\n    tensorboard_log = TensorboardLogger(train_tool.run_directory)\n    train_tool.loggers = [text_log, tensorboard_log]\n\n    train_tool.log_every = args.display_interval\n\n    train_tool.checkpoint_every = args.checkpoint_interval\n\n    # Begin training\n    train_tool.run(start_step)\n\n\nif __name__ == \'__main__\':\n    setup_logging()\n    args = parse_args()\n    main(args)\n'"
pytorch_toolkit/instance_segmentation/tools/train_0050.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\nimport os.path as osp\nimport resource\n\nfrom segmentoly.data.dataparallel import collate\nfrom segmentoly.data.transforms import *\nfrom segmentoly.datasets.factory import get_dataset\nfrom segmentoly.rcnn.model_zoo.instance_segmentation_security_0050 import InstanceSegmentationSecurity0050 as Model\nfrom segmentoly.utils.logging import setup_logging, TextLogger, TensorboardLogger\nfrom segmentoly.utils.lr_scheduler import MultiStepLRWithWarmUp\nfrom segmentoly.utils.training_engine import DefaultMaskRCNNTrainingEngine\nfrom segmentoly.utils.weights import load_checkpoint\n\nlogger = logging.getLogger(__name__)\n\n\nclass Trainer(DefaultMaskRCNNTrainingEngine):\n    def __init__(self):\n        super().__init__()\n        self.identifier = \'instance-segmentation-security-0050\'\n        self.description = \'Training of instance-segmentation-security-0050\'\n        self.root_directory = osp.join(osp.dirname(osp.abspath(__file__)), \'..\')\n        self.run_directory = self.create_run_directory(osp.join(self.root_directory, \'outputs\'))\n\n        setup_logging(file_path=osp.join(self.run_directory, \'log.txt\'))\n\n        logger.info(\'Running {}\'.format(self.identifier))\n        logger.info(self.description)\n        logger.info(\'Working directory ""{}""\'.format(self.run_directory))\n\n        self.batch_size = 32\n        self.virtual_iter_size = 1\n\n        # Training dataset.\n        training_transforms = Compose(\n            [\n                RandomResize(mode=\'size\', heights=(416, 448, 480, 512, 544), widths=(416, 448, 480, 512, 544)),\n                RandomHorizontalFlip(prob=0.5),\n                ToTensor(),\n                Normalize(mean=[102.9801, 115.9465, 122.7717], std=[1., 1., 1.], rgb=False),\n            ],\n        )\n        training_dataset_name = \'coco_2017_train\'\n        logger.info(\'Training dataset {}\'.format(training_dataset_name))\n        training_dataset = get_dataset(training_dataset_name, True, True, training_transforms)\n        logger.info(training_dataset)\n        self.training_data_loader = torch.utils.data.DataLoader(\n            training_dataset, batch_size=self.batch_size, num_workers=0,\n            shuffle=True, drop_last=True, collate_fn=collate\n        )\n\n        # Validation datasets.\n        validation_transforms = Compose(\n            [\n                Resize(size=[480, 480]),\n                ToTensor(),\n                Normalize(mean=[102.9801, 115.9465, 122.7717], std=[1., 1., 1.], rgb=False),\n            ]\n        )\n        validation_datasets = []\n        validation_dataset_name = \'coco_2017_val\'\n        logger.info(\'Validation dataset #{}: {}\'.format(len(validation_datasets) + 1, validation_dataset_name))\n        validation_datasets.append(get_dataset(validation_dataset_name, False, False, validation_transforms))\n        logger.info(validation_datasets[-1])\n\n        self.validation_data_loaders = []\n        for validation_dataset in validation_datasets:\n            self.validation_data_loaders.append(torch.utils.data.DataLoader(\n                validation_dataset,\n                batch_size=1, num_workers=8,\n                shuffle=False, drop_last=False, collate_fn=collate)\n            )\n        self.validate_every = 10000\n\n        for validation_dataset in validation_datasets:\n            assert training_dataset.classes_num == validation_dataset.classes_num\n\n        # Model and optimizer.\n        logger.info(\'Model:\')\n        self.model = Model(training_dataset.classes_num)\n        logger.info(self.model)\n\n        self.training_iterations_num = 270000\n        lr_scheduler_milestones = [220000, 250000]\n        base_lr = 0.02\n        weight_decay = 0.0001\n        logger.info(\'Optimizer:\')\n        self.optimizer = torch.optim.SGD(self.setup_optimizer(self.model, base_lr, weight_decay),\n                                         lr=base_lr, weight_decay=weight_decay, momentum=0.9)\n        logger.info(self.optimizer)\n        logger.info(\'Learning Rate scheduler:\')\n        self.lr_scheduler = MultiStepLRWithWarmUp(\n            self.optimizer,\n            milestones=lr_scheduler_milestones,\n            warmup_iters=1000,\n            warmup_method=\'linear\',\n            warmup_factor_base=0.333,\n            gamma=0.1,\n            last_epoch=0\n        )\n        logger.info(self.lr_scheduler)\n\n        self.start_step = 0\n        checkpoint_file_path = osp.join(self.root_directory, \'data\', \'pretrained_models\',\n                                        \'converted\', \'imagenet\', \'detectron\', \'resnet50.pth\')\n        if not osp.exists(checkpoint_file_path):\n            raise IOError(\'Initial checkpoint file ""{}"" does not exist. \'\n                          \'Please fetch pretrained backbone networks using \'\n                          \'tools/download_pretrained_weights.py script first.\'.format(checkpoint_file_path))\n        logger.info(\'Loading weights from ""{}""\'.format(checkpoint_file_path))\n        load_checkpoint(self.model.backbone, checkpoint_file_path)\n\n        # Loggers and misc. stuff.\n        self.loggers = [TextLogger(logger),\n                        TensorboardLogger(self.run_directory)]\n        self.log_every = 50\n\n        self.checkpoint_every = 10000\n\n\nif __name__ == \'__main__\':\n    # RuntimeError: received 0 items of ancdata. Issue: pytorch/pytorch#973\n    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n    resource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n\n    experiment = Trainer()\n    torch.backends.cudnn.benchmark = False\n    experiment.run(experiment.start_step)\n    logger.info(\'Done.\')\n'"
pytorch_toolkit/nncf/examples/__init__.py,0,b''
pytorch_toolkit/nncf/nncf/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom .version import __version__\n\n# Required for correct COMPRESSION_ALGORITHMS registry functioning\nfrom .binarization import algo as binarization_algo\nfrom .quantization import algo as quantization_algo\nfrom .sparsity.const import algo as const_sparsity_algo\nfrom .sparsity.magnitude import algo as magnitude_sparsity_algo\nfrom .sparsity.rb import algo as rb_sparsity_algo\nfrom .pruning.filter_pruning import algo as filter_pruning_algo\n\n# Functions most commonly used in integrating NNCF into training pipelines are\n# listed below for importing convenience\n\nfrom .model_creation import create_compressed_model\nfrom .checkpoint_loading import load_state\nfrom .config import Config\nfrom .nncf_logger import disable_logging\nfrom .nncf_logger import set_log_level\n\n# NNCF relies on tracing PyTorch operations. Each code that uses NNCF\n# should be executed with PyTorch operators wrapped via a call to ""patch_torch_operators"",\n# so this call is moved to package __init__ to ensure this.\nfrom .dynamic_graph.patch_pytorch import patch_torch_operators\npatch_torch_operators()\n'"
pytorch_toolkit/nncf/nncf/algo_selector.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom copy import copy\nfrom typing import List\n\nfrom nncf.hw_config import HWConfigType\nfrom .compression_method_api import CompressionAlgorithmBuilder, CompressionAlgorithmController\nfrom .registry import Registry\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\nCOMPRESSION_ALGORITHMS = Registry(\'compression algorithm\')\n\n\n@COMPRESSION_ALGORITHMS.register(\'NoCompressionAlgorithmBuilder\')\nclass NoCompressionAlgorithmBuilder(CompressionAlgorithmBuilder):\n    pass\n\n\nclass NoCompressionAlgorithmController(CompressionAlgorithmController):\n    pass\n\n\ndef get_compression_algorithm(config):\n    algorithm_key = config.get(\'algorithm\', \'NoCompressionAlgorithmBuilder\')\n    nncf_logger.info(""Creating compression algorithm: {}"".format(algorithm_key))\n    return COMPRESSION_ALGORITHMS.get(algorithm_key)\n\n\ndef remove_key(d, key):\n    sd = copy(d)\n    del sd[key]\n    return sd\n\n\ndef create_compression_algorithm_builders(config) -> List[CompressionAlgorithmBuilder]:\n    compression_config = config.get(\'compression\', {})\n\n    hw_config_type = None\n    hw_config_type_str = config.get(""hw_config_type"")\n    if hw_config_type_str is not None:\n        hw_config_type = HWConfigType.from_str(config.get(""hw_config_type""))\n    if isinstance(compression_config, dict):\n        compression_config[""hw_config_type""] = hw_config_type\n        return [get_compression_algorithm(compression_config)(compression_config), ]\n    retval = []\n    for algo_config in compression_config:\n        algo_config[""hw_config_type""] = hw_config_type\n        retval.append(get_compression_algorithm(algo_config)(algo_config))\n    return retval\n'"
pytorch_toolkit/nncf/nncf/checkpoint_loading.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport re\n\nimport torch\n\nfrom nncf.nncf_network import MODEL_WRAPPED_BY_NNCF_ATTR_NAME\nfrom nncf.nncf_logger import logger as nncf_logger\n\n\ndef load_state(model: torch.nn.Module, saved_state_dict: dict, is_resume: bool = False) -> int:\n    """"""\n    Used to load a checkpoint containing a compressed model into an NNCFNetwork object, but can\n    be used for any PyTorch module as well. Will do matching of saved_state_dict parameters to\n    the model\'s state_dict parameters while discarding irrelevant prefixes added during wrapping\n    in NNCFNetwork or DataParallel/DistributedDataParallel objects, and load the matched parameters\n    from the saved_state_dict into the model\'s state dict.\n    :param model: The target module for the saved_state_dict to be loaded to.\n    :param saved_state_dict: A state dict containing the parameters to be loaded into the model.\n    :param is_resume: Determines the behavior when the function cannot do a successful parameter match\n    when loading. If True, the function will raise an exception if it cannot match the saved_state_dict\n    parameters to the model\'s parameters (i.e. if some parameters required by model are missing in\n    saved_state_dict, or if saved_state_dict has parameters that could not be matched to model parameters,\n    or if the shape of parameters is not matching). If False, the exception won\'t be raised.\n    Usually is_resume is specified as False when loading uncompressed model\'s weights into the model with\n    compression algorithms already applied, and as True when loading a compressed model\'s weights into the model\n    with compression algorithms applied to evaluate the model.\n    :return: The number of saved_state_dict entries successfully matched and loaded into model.\n    """"""\n    def key_normalizer(key):\n        new_key = key\n        match = re.search(\'(pre_ops|post_ops)\\\\.(\\\\d+?)\\\\.op\', key)\n        return new_key if not match else new_key.replace(match.group(), \'operation\')\n\n    if \'state_dict\' in saved_state_dict:\n        saved_state_dict = saved_state_dict[\'state_dict\']\n    state_dict = model.state_dict()\n\n    new_dict, num_loaded_layers, problematic_keys = match_keys(is_resume, saved_state_dict, state_dict, key_normalizer)\n    num_saved_layers = len(saved_state_dict.items())\n    process_problematic_keys(is_resume, problematic_keys, num_loaded_layers == num_saved_layers)\n    nncf_logger.info(""Loaded {}/{} layers"".format(num_loaded_layers, len(state_dict.items())))\n\n    model.load_state_dict(new_dict, strict=False)\n    return num_loaded_layers\n\n\ndef process_problematic_keys(is_resume, issues, is_all_saved_loaded):\n    error_msgs = []\n\n    def add_error_msg(name, keys):\n        error_msgs.insert(\n            0, \'{} key(s):\\n{}. \'.format(name,\n                                         \',\\n\'.join(\'\\t\\t""{}""\'.format(k) for k in keys)))\n\n    for name, keys in issues.items():\n        is_missing = name == \'Missing\'\n        if keys and (not is_missing or is_missing and (is_resume or not is_all_saved_loaded)):\n            add_error_msg(name, keys)\n    if error_msgs:\n        error_msg = \'Error(s) when loading model parameters:\\n\\t{}\'.format(""\\n\\t"".join(error_msgs))\n        if is_resume:\n            raise RuntimeError(error_msg)\n        nncf_logger.warning(error_msg)\n\n\ndef match_keys(is_resume, saved_state_dict, state_dict, key_normalizer):\n    skipped_keys = []\n    num_loaded_layers = 0\n    new_dict = {}\n\n    def check_parameter_size(key, saved_value, num_loaded_layers):\n        saved_size = saved_value.size()\n        size = state_dict[key].size()\n        if saved_size == size:\n            new_dict[key] = saved_value\n            return num_loaded_layers + 1\n        nncf_logger.warning(""Different size of value of \'{}\' in dictionary ({}) and in resuming model ({})""\n                            .format(key, saved_size, size, ))\n        skipped_keys.append(key)\n        return num_loaded_layers\n\n    clip_patterns = [MODEL_WRAPPED_BY_NNCF_ATTR_NAME + \'.\',\n                     \'module.\']\n\n    clipped_keys = list(state_dict.keys())\n    for pattern in clip_patterns:\n        for i, _ in enumerate(clipped_keys):\n            clipped_keys[i] = clipped_keys[i].replace(pattern, \'\')\n\n    clipped_key_to_model_key_dict = dict(zip(clipped_keys, state_dict.keys()))\n\n    norm_clipped_keys = {}\n    collisions = []\n    for clipped_key, orig_key in clipped_key_to_model_key_dict.items():\n        normalized_key = key_normalizer(clipped_key)\n        if normalized_key in norm_clipped_keys:\n            collisions.append(clipped_key)\n        norm_clipped_keys[normalized_key] = orig_key\n\n    unexpected_keys = []\n\n    for (saved_key, saved_value) in saved_state_dict.items():\n        clipped_saved_key = saved_key\n        for pattern in clip_patterns:\n            clipped_saved_key = clipped_saved_key.replace(pattern, \'\')\n\n        if clipped_saved_key in clipped_key_to_model_key_dict:\n            key = clipped_key_to_model_key_dict[clipped_saved_key]\n            num_loaded_layers = check_parameter_size(key, saved_value, num_loaded_layers)\n        else:\n            norm_clipped_saved_key = key_normalizer(clipped_saved_key)\n            if norm_clipped_saved_key in norm_clipped_keys and clipped_saved_key not in collisions and not is_resume:\n                key = norm_clipped_keys[norm_clipped_saved_key]\n                num_loaded_layers = check_parameter_size(key, saved_value, num_loaded_layers)\n            else:\n                unexpected_keys.append(saved_key)\n    missing_keys = [k for k in state_dict.keys() if k not in new_dict and k not in skipped_keys]\n    problematic_keys = {\'Missing\': missing_keys,\n                        \'Unexpected\': unexpected_keys,\n                        \'Skipped\': skipped_keys}\n    return new_dict, num_loaded_layers, problematic_keys\n'"
pytorch_toolkit/nncf/nncf/composite_compression.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom typing import List\n\nimport torch.nn\nfrom torch.nn.modules.loss import _Loss\nfrom torch.utils.data import DataLoader\n\nfrom nncf.compression_method_api import CompressionLoss, CompressionScheduler, \\\n    CompressionAlgorithmController\nfrom nncf.nncf_network import NNCFNetwork\n\n\nclass CompositeCompressionLoss(CompressionLoss):\n    def __init__(self):\n        super().__init__()\n        self._child_losses = torch.nn.ModuleList()\n\n    @property\n    def child_losses(self):\n        return self._child_losses\n\n    def add(self, child_loss):\n        self._child_losses.append(child_loss)\n\n    def forward(self):\n        result_loss = 0\n        for loss in self._child_losses:\n            result_loss += loss()\n        return result_loss\n\n    def statistics(self):\n        stats = {}\n        for loss in self._child_losses:\n            stats.update(loss.statistics())\n        return stats\n\n\nclass CompositeCompressionScheduler(CompressionScheduler):\n    def __init__(self):\n        super().__init__()\n        self._child_schedulers = []\n\n    @property\n    def child_schedulers(self):\n        return self._child_schedulers\n\n    def add(self, child_scheduler):\n        self._child_schedulers.append(child_scheduler)\n\n    def step(self, last=None):\n        super().step(last)\n        for scheduler in self._child_schedulers:\n            scheduler.step(last)\n\n    def epoch_step(self, last=None):\n        super().epoch_step(last)\n        for scheduler in self._child_schedulers:\n            scheduler.epoch_step(last)\n\n    def state_dict(self):\n        result = {}\n        for child_scheduler in self._child_schedulers:\n            result.update(child_scheduler.state_dict())\n        return result\n\n    def load_state_dict(self, state_dict):\n        for child_scheduler in self._child_schedulers:\n            child_scheduler.load_state_dict(state_dict)\n\n    def initialize(self):\n        for child_scheduler in self._child_schedulers:\n            child_scheduler.initialize()\n\n\nclass CompositeCompressionAlgorithmController(CompressionAlgorithmController):\n    def __init__(self, target_model: NNCFNetwork):\n        super().__init__(target_model)\n        self._child_algos = []  # type: List[CompressionAlgorithmController]\n        self._loss = CompositeCompressionLoss()\n        self._scheduler = CompositeCompressionScheduler()\n\n    @property\n    def child_algos(self):\n        return self._child_algos\n\n    def add(self, child_algo: CompressionAlgorithmController):\n        # pylint: disable=protected-access\n        assert child_algo._model is self._model, ""Cannot create a composite controller "" \\\n                                                 ""from controllers belonging to different models!""\n        self.child_algos.append(child_algo)\n        self._loss.add(child_algo.loss)\n        self._scheduler.add(child_algo.scheduler)\n        self._model = child_algo._model\n\n    def distributed(self):\n        for algo in self.child_algos:\n            algo.distributed()\n\n    def initialize(self, data_loader: DataLoader = None, criterion: _Loss = None):\n        for algo in self.child_algos:\n            algo.initialize(data_loader, criterion)\n\n    def statistics(self):\n        stats = {}\n        for algo in self.child_algos:\n            stats.update(algo.statistics())\n        return stats\n\n    def export_model(self, filename):\n        self.child_algos[-1].export_model(filename)\n\n    def apply_to(self, target_model: NNCFNetwork) -> NNCFNetwork:\n        for algo in self.child_algos:\n            target_model = algo.apply_to(target_model)\n        return target_model\n'"
pytorch_toolkit/nncf/nncf/compression_method_api.py,0,"b'#\n#  Copyright (c) 2019-2020 Intel Corporation\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#       http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\n""""""\n@package docstring\nThis package defines the API for the NNCF compression methods, so that the user could\nextend the existing algorithms.\n""""""\n\nimport torch\nfrom copy import copy\nfrom functools import partial\nfrom torch import nn\nfrom torch.nn.modules.loss import _Loss\nfrom torch.utils.data import DataLoader\n\nfrom nncf.config import Config\nfrom nncf.dynamic_graph.graph_builder import create_mock_tensor\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.utils import in_scope_list\n\n\nclass CompressionLoss(nn.Module):\n    """"""\n    Used to calculate additional loss to be added to the base loss during the\n    training process. It uses the model graph to measure variables and activations\n    values of the layers during the loss construction. For example, the $L_0$-based\n    sparsity algorithm calculates the number of non-zero weights in convolutional\n    and fully-connected layers to construct the loss function.\n    """"""\n\n    def forward(self):\n        """"""\n        Returns the compression loss value.\n        """"""\n        return 0\n\n    def statistics(self):\n        """"""\n        Returns a dictionary of printable statistics.\n        """"""\n        return {}\n\n\nclass CompressionScheduler:\n    """"""\n    Implements the logic of compression method control during the training process.\n    May change the method hyperparameters in regards to the current training step or\n    epoch. For example, the sparsity method can smoothly increase the sparsity rate\n    over several epochs.\n    """"""\n\n    def __init__(self):\n        self.last_epoch = 0\n        self.last_step = 0\n\n    def step(self, last=None):\n        """"""\n        Should be called after each optimizer step during training.\n        Arguments:\n            `last` - specifies the initial ""previous"" step\n        """"""\n        if last is None:\n            last = self.last_step + 1\n        self.last_step = last\n\n    def epoch_step(self, last=None):\n        """"""\n        Should be called after each training epoch.\n        Arguments:\n            `last` - specifies the initial ""previous"" epoch\n        """"""\n        if last is None:\n            last = self.last_epoch + 1\n        self.last_epoch = last\n\n    def load_state_dict(self, state_dict):\n        self.__dict__.update(state_dict)\n\n    def state_dict(self):\n        default_keys = {\'last_step\', \'last_epoch\'}\n        return {key: val for key, val in self.__dict__.items() if key in default_keys}\n\n    def initialize(self):\n        pass\n\n\nclass CompressionAlgorithmController:\n    """"""Serves as a handle to the additional modules, parameters and hooks inserted\n    into the original uncompressed model in order to enable algorithm-specific compression.\n    Hosts entities that are to be used during the training process, such as compression scheduler and\n    compression loss.""""""\n    def __init__(self, target_model: NNCFNetwork):\n        self._model = target_model\n        self._loss = CompressionLoss()\n        self._scheduler = CompressionScheduler()\n\n    @property\n    def loss(self):\n        return self._loss\n\n    @property\n    def scheduler(self):\n        return self._scheduler\n\n    def distributed(self):\n        """"""\n        Should be called when distributed training with multiple training processes\n        is going to be used (i.e. after the model is wrapped with DistributedDataParallel).\n        Any special preparations for the algorithm to properly support distributed training\n        should be made inside this function.\n        """"""\n\n    def statistics(self):\n        """"""\n        Returns a dictionary of printable statistics.\n        """"""\n        stats = self._loss.statistics()\n        if hasattr(self._model, \'statistics\'):\n            stats.update(self._model.statistics())\n        return stats\n\n    def export_model(self, filename, *args, **kwargs):\n        """"""\n        Used to export the compressed model for inference into the ONNX format.\n        Makes method-specific preparations of the model graph,\n        (e.g. removing auxiliary layers that were used for the model compression),\n        then exports the model and dumps it into the output file.\n        Parameters:\n            `filename` - a path to the file for the exported model to be saved into.\n            *args, **kwargs - if the model\'s `forward` requires additional parameters\n            during export, specify these here.\n        """"""\n        model = self._model.eval().cpu()\n        input_tensor_list = []\n        for info in self._model.input_infos:\n            single_batch_info = copy(info)\n            input_shape = tuple([1] + list(info.shape)[1:])\n            single_batch_info.shape = input_shape\n            input_tensor_list.append(create_mock_tensor(single_batch_info, ""cpu""))\n        original_forward = model.forward\n        model.forward = partial(model.forward, *args, **kwargs)\n        with torch.no_grad():\n            torch.onnx.export(model, tuple(input_tensor_list),\n                              filename, verbose=True)\n        model.forward = original_forward\n\n    def initialize(self, data_loader: DataLoader = None, criterion: _Loss = None):\n        """"""\n        Configures certain parameters of the algorithm that could not be set during __init__\n        and require access to the dataset that the model was originally trained on (for example,\n        in order to do range initialization for activation quantizers) or to the\n        loss function to be used during fine-tuning (for example, to determine\n        quantizer precision bitwidth using HAWQ).\n        """"""\n\n\nclass CompressionAlgorithmBuilder:\n    """"""\n    Determines which modifications should be made to the original FP32 model in\n    order to enable algorithm-specific compression during fine-tuning. Operates\n    on an NNCFNetwork object wrapping a target PyTorch model (torch.nn.Module).\n    """"""\n\n    def __init__(self, config: Config):\n        """"""\n        Arguments:\n          `config` - a dictionary that contains parameters of compression method\n        """"""\n        self.config = config\n        if not isinstance(self.config, list):\n            self.ignored_scopes = self.config.get(\'ignored_scopes\')\n            self.target_scopes = self.config.get(\'target_scopes\')\n\n    def apply_to(self, target_model: NNCFNetwork) -> NNCFNetwork:\n        """"""\n        Applies algorithm-specific modifications to the model. Hooks to be executed during model\n        forward operation may be registered using NNCFNetwork command insertion methods. Additional\n        compression modules that are expected to be saved along with the network via torch.save should also be\n        registered and added to the model here.\n        :param target_model: An instance of NNCFNetwork for the algorithm to be applied to.\n        :return: NNCFNetwork with algorithm-specific modifications applied\n        """"""\n        self._model = target_model  # type: NNCFNetwork\n        return target_model\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n        """"""\n        Should be called once the compressed model target_model is fully constructed (i.e. hooks are applied and\n        modules are in place. Returns a CompressionAlgorithmController object containing information\n        and references to the compressed model or specific modules thereof required for the corresponding compression\n        scheduler operation or compression loss calculation.\n        :param target_model: An instance of NNCFNetwork with current algorithm already applied\n        :return: A CompressionAlgorithmController object.\n        """"""\n\n    def _should_consider_scope(self, scope_str: str) -> bool:\n        return (self.target_scopes is None or in_scope_list(scope_str, self.target_scopes)) \\\n               and not in_scope_list(scope_str, self.ignored_scopes)\n'"
pytorch_toolkit/nncf/nncf/config.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport itertools\nimport os\nimport jsonschema\nimport logging\n\nfrom nncf.config_schema import ROOT_NNCF_CONFIG_SCHEMA, validate_single_compression_algo_schema\n\ntry:\n    import jstyleson as json\nexcept ImportError:\n    import json\n\nfrom addict import Dict\n\n\n_DEFAULT_KEY_TO_ENV = {\n    ""world_size"": ""WORLD_SIZE"",\n}\n\nlogger = logging.getLogger(\'nncf\')\n\nclass ActionWrapper(argparse.Action):\n    def __init__(self, action):\n        self._action = action\n        super().__init__(action.option_strings, action.dest, nargs=action.nargs, const=action.const,\n                         default=action.default, type=action.type, choices=action.choices, required=action.required,\n                         help=action.help, metavar=action.metavar)\n        self._action = action\n\n    def __getattr__(self, item):\n        return getattr(self._action, item)\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        parser.seen_actions.add(self._action.dest)\n        return self._action(parser, namespace, values, option_string)\n\n\n# pylint: disable=protected-access\nclass CustomArgumentGroup(argparse._ArgumentGroup):\n    def _add_action(self, action):\n        super()._add_action(ActionWrapper(action))\n\n\n# pylint: disable=protected-access\nclass CustomActionContainer(argparse._ActionsContainer):\n    def add_argument_group(self, *args, **kwargs):\n        group = CustomArgumentGroup(self, *args, **kwargs)\n        self._action_groups.append(group)\n        return group\n\n\nclass CustomArgumentParser(CustomActionContainer, argparse.ArgumentParser):\n    """"""ArgumentParser that saves which arguments are provided""""""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, *kwargs)\n        self.seen_actions = set()\n\n    def parse_known_args(self, args=None, namespace=None):\n        self.seen_actions.clear()\n        return super().parse_known_args(args, namespace)\n\n\nclass Config(Dict):\n    """"""A regular dictionary object extended with some utility functions.""""""\n    def __getattr__(self, item):\n        if item not in self:\n            raise KeyError(""Key {} not found in config"".format(item))\n        return super().__getattr__(item)\n\n    @classmethod\n    def from_json(cls, path):\n        with open(path) as f:\n            loaded_json = cls(json.load(f))\n        try:\n            jsonschema.validate(loaded_json, schema=ROOT_NNCF_CONFIG_SCHEMA)\n\n            compression_section = loaded_json.get(""compression"")\n            if compression_section is None:\n                # No compression specified\n                return loaded_json\n\n            if isinstance(compression_section, dict):\n                validate_single_compression_algo_schema(compression_section)\n            else:\n                # Passed a list of dicts\n                for compression_algo_dict in compression_section:\n                    validate_single_compression_algo_schema(compression_algo_dict)\n\n        except jsonschema.ValidationError:\n            logger.error(""Invalid NNCF config supplied!"")\n            raise\n        return loaded_json\n\n    def update_from_args(self, args, argparser=None):\n        if argparser is not None:\n            if isinstance(argparser, CustomArgumentParser):\n                default_args = {arg for arg in vars(args) if arg not in argparser.seen_actions}\n            else:\n                # this will fail if we explicitly provide default argument in CLI\n                known_args = argparser.parse_known_args()\n                default_args = {k for k, v in vars(args).items() if known_args[k] == v}\n        else:\n            default_args = {k for k, v in vars(args).items() if v is None}\n\n        for key, value in vars(args).items():\n            if key not in default_args or key not in self:\n                self[key] = value\n\n    def update_from_env(self, key_to_env_dict=None):\n        if key_to_env_dict is None:\n            key_to_env_dict = _DEFAULT_KEY_TO_ENV\n        for k, v in key_to_env_dict:\n            if v in os.environ:\n                self[k] = int(os.environ[v])\n\n\ndef product_dict(d):\n    keys = d.keys()\n    vals = d.values()\n    for instance in itertools.product(*vals):\n        yield dict(zip(keys, instance))\n'"
pytorch_toolkit/nncf/nncf/config_schema.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport logging\nfrom typing import Dict\n\nimport jsonschema\n\nlogger = logging.getLogger(\'nncf\')\n\n\ndef make_string_or_array_of_strings_schema(addtl_dict_entries: Dict = None) -> Dict:\n    if addtl_dict_entries is None:\n        addtl_dict_entries = {}\n    retval = {\n        ""type"": [""array"", ""string""],\n        ""items"": {\n            ""type"": ""string""\n        }\n    }\n    retval.update(addtl_dict_entries)\n    return retval\n\n\ndef make_object_or_array_of_objects_schema(single_object_schema: Dict = None) -> Dict:\n    retval = {\n        ""oneOf"": [\n            {\n                ""type"": ""array"",\n                ""items"": single_object_schema\n            },\n            single_object_schema\n        ]\n    }\n    return retval\n\n\ndef with_attributes(schema: Dict, **kwargs) -> Dict:\n    retval = {**schema, **kwargs}\n    return retval\n\n\n_NUMBER = {\n    ""type"": ""number""\n}\n\n_STRING = {\n    ""type"": ""string""\n}\n\n_BOOLEAN = {\n    ""type"": ""boolean""\n}\n\n_ARRAY_OF_NUMBERS = {\n    ""type"": ""array"",\n    ""items"": _NUMBER\n}\n\n_ARRAY_OF_STRINGS = {\n    ""type"": ""array"",\n    ""items"": _STRING\n}\n\nSINGLE_INPUT_INFO_SCHEMA = {\n    ""type"": ""object"",\n    ""properties"": {\n        ""sample_size"": with_attributes(_ARRAY_OF_NUMBERS,\n                                       description=""Shape of the tensor expected as input to the model."",\n                                       examples=[[1, 3, 224, 224]]),\n        ""type"": with_attributes(_STRING,\n                                description=""Data type of the model input tensor.""),\n        ""filler"": with_attributes(_STRING,\n                                  description=""Determines what the tensor will be filled with when passed to the model""\n                                              "" during tracing and exporting.""),\n        ""keyword"": with_attributes(_STRING,\n                                   description=""Keyword to be used when passing the tensor to the model\'s ""\n                                               ""\'forward\' method."")\n    },\n    ""additionalProperties"": False\n}\n\nQUANTIZER_CONFIG_PROPERTIES = {\n    ""mode"": with_attributes(_STRING,\n                            description=""Mode of quantization""),\n    ""bits"": with_attributes(_NUMBER,\n                            description=""Bitwidth to quantize to.""),\n    ""signed"": with_attributes(_BOOLEAN,\n                              description=""Whether to use signed or unsigned input/output values for quantization.""\n                                          "" If specified as unsigned and the input values during initialization have ""\n                                          ""differing signs, will reset to performing signed quantization instead.""),\n    ""per_channel"": with_attributes(_BOOLEAN,\n                                   description=""Whether to quantize inputs per channel (i.e. per 0-th dimension for ""\n                                               ""weight quantization, and per 1-st dimension for activation ""\n                                               ""quantization)"")\n}\n\nIGNORED_SCOPES_DESCRIPTION = ""A list of model control flow graph node scopes to be ignored for this "" \\\n                             ""operation - functions as a \'blacklist\'. Optional.""\nTARGET_SCOPES_DESCRIPTION = ""A list of model control flow graph node scopes to be considered for this operation"" \\\n                            "" - functions as a \'whitelist\'. Optional.""\n\nQUANTIZER_GROUP_SCHEMA = {\n    ""type"": ""object"",\n    ""properties"": {\n        **QUANTIZER_CONFIG_PROPERTIES,\n        ""ignored_scopes"": with_attributes(make_object_or_array_of_objects_schema(_STRING),\n                                          description=IGNORED_SCOPES_DESCRIPTION),\n        ""target_scopes"": with_attributes(make_object_or_array_of_objects_schema(_STRING),\n                                         description=TARGET_SCOPES_DESCRIPTION)\n    },\n    ""additionalProperties"": False\n}\n\nINITIALIZER_SCHEMA = {\n    ""type"": ""object"",\n    ""properties"": {\n        ""range"":\n            {\n                ""type"": ""object"",\n                ""properties"": {\n                    ""num_init_steps"": with_attributes(_NUMBER,\n                                                      description=""Number of batches from the training dataset to ""\n                                                                  ""consume as sample model inputs for purposes of ""\n                                                                  ""setting initial minimum and maximum quantization ""\n                                                                  ""ranges""),\n                    ""type"": with_attributes(_STRING, description=""Type of the initializer - determines which ""\n                                                                 ""statistics gathered during initialization will be ""\n                                                                 ""used to initialize the quantization ranges"")\n                },\n                ""additionalProperties"": False,\n            },\n        ""precision"":\n            {\n                ""type"": ""object"",\n                ""properties"": {\n                    ""type"": with_attributes(_STRING,\n                                            description=""Type of precision initialization.""),\n                    ""bits"": with_attributes(_ARRAY_OF_NUMBERS,\n                                            description=""A list of bitwidth to choose from when ""\n                                                        ""performing precision initialization."",\n                                            examples=[[4, 8]]),\n                    ""num_data_points"": with_attributes(_NUMBER,\n                                                       description=""Number of data points to iteratively estimate ""\n                                                                   ""Hessian trace, 200 by default.""),\n                    ""iter_number"": with_attributes(_NUMBER,\n                                                   description=""Maximum number of iterations of Hutchinson algorithm ""\n                                                               ""to Estimate Hessian trace, 200 by default""),\n                    ""tolerance"": with_attributes(_NUMBER,\n                                                 description=""Minimum relative tolerance for stopping the Hutchinson ""\n                                                             ""algorithm. It\'s calculated  between mean average trace ""\n                                                             ""from previous iteration and current one. 1e-5 by default""\n                                                             ""bitwidth_per_scope""),\n                    ""bitwidth_per_scope"": {\n                        ""type"": ""array"",\n                        ""items"": {\n                            ""type"": ""array"",\n                            ""items"":\n                                [\n                                    _NUMBER,\n                                    _STRING\n                                ],\n                            ""description"": ""A tuple of a bitwidth and a scope of the quantizer to assign the ""\n                                           ""bitwidth to.""\n                        },\n                        ""description"": ""Manual settings for the quantizer bitwidths. Scopes are used to identify ""\n                                       ""the quantizers.""\n                    }\n                },\n                ""additionalProperties"": False,\n            }\n    },\n    ""additionalProperties"": False,\n}\n\nCOMMON_COMPRESSION_ALGORITHM_PROPERTIES = {\n    ""ignored_scopes"": with_attributes(make_string_or_array_of_strings_schema(),\n                                      description=IGNORED_SCOPES_DESCRIPTION),\n    ""target_scopes"": with_attributes(make_string_or_array_of_strings_schema(),\n                                     description=TARGET_SCOPES_DESCRIPTION)\n}\n\nBASIC_COMPRESSION_ALGO_SCHEMA = {\n    ""type"": ""object"",\n    ""required"": [""algorithm""]\n}\n\nQUANTIZATION_ALGO_NAME_IN_CONFIG = ""quantization""\nQUANTIZATION_SCHEMA = {\n    **BASIC_COMPRESSION_ALGO_SCHEMA,\n    ""properties"": {\n        ""algorithm"": {\n            ""const"": QUANTIZATION_ALGO_NAME_IN_CONFIG\n        },\n        ""initializer"": INITIALIZER_SCHEMA,\n        ""weights"": with_attributes(QUANTIZER_GROUP_SCHEMA,\n                                   description=""Constraints to be applied to model weights quantization only. ""\n                                               ""Overrides higher-level settings.""),\n        ""activations"": with_attributes(QUANTIZER_GROUP_SCHEMA,\n                                       description=""Constraints to be applied to model activations quantization only. ""\n                                                   ""Overrides higher-level settings.""),\n        ""quantize_inputs"": with_attributes(_BOOLEAN,\n                                           description=""Whether the model inputs should be immediately quantized prior ""\n                                                       ""to any other model operations."",\n                                           default=True),\n        ""quantizable_subgraph_patterns"": {\n            ""type"": ""array"",\n            ""items"": make_string_or_array_of_strings_schema(),\n            ""description"": ""Each sub-list in this list will correspond to a sequence of operations in the ""\n                           ""model control flow graph that will have a quantizer appended at the end of the ""\n                           ""sequence"",\n            ""examples"": [[""cat"", ""batch_norm""], ""h_swish""]\n        },\n        ""scope_overrides"": {\n            ""type"": ""object"",\n            ""patternProperties"": {\n                "".*"": {\n                    ""type"": ""object"",\n                    ""properties"": QUANTIZER_CONFIG_PROPERTIES,\n                    ""additionalProperties"": False\n                },\n            },\n            ""description"": ""This option is used to specify overriding quantization constraints for specific scope,""\n                           ""e.g. in case you need to quantize a single operation differently than the rest of the ""\n                           ""model.""\n        },\n        **COMMON_COMPRESSION_ALGORITHM_PROPERTIES,\n    },\n    ""additionalProperties"": False\n}\n\nBINARIZATION_ALGO_NAME_IN_CONFIG = ""binarization""\nBINARIZATION_SCHEMA = {\n    **BASIC_COMPRESSION_ALGO_SCHEMA,\n    ""properties"": {\n        ""algorithm"": {\n            ""const"": BINARIZATION_ALGO_NAME_IN_CONFIG\n        },\n        ""initializer"": INITIALIZER_SCHEMA,\n        ""mode"": with_attributes(_STRING,\n                                description=""Selects the mode of binarization - either \'xnor\' for XNOR binarization,""\n                                            ""or \'dorefa\' for DoReFa binarization""),\n        ""params"": {\n            ""type"": ""object"",\n            ""properties"": {\n                ""batch_multiplier"": with_attributes(_NUMBER,\n                                                    description=""Gradients will be accumulated for this number of ""\n                                                                ""batches before doing a \'backward\' call. Increasing ""\n                                                                ""this may improve training quality, since binarized ""\n                                                                ""networks exhibit noisy gradients requiring larger ""\n                                                                ""batch sizes than could be accomodated by GPUs""),\n                ""activations_bin_start_epoch"": with_attributes(_NUMBER,\n                                                               description=""Epoch to start binarizing activations""),\n                ""weights_bin_start_epoch"": with_attributes(_NUMBER,\n                                                           description=""Epoch to start binarizing weights""),\n                ""lr_poly_drop_start_epoch"": with_attributes(_NUMBER,\n                                                            description=""Epoch to start dropping the learning rate""),\n                ""lr_poly_drop_duration_epochs"": with_attributes(_NUMBER,\n                                                                description=""Duration, in epochs, of the learning ""\n                                                                            ""rate dropping process.""),\n                ""disable_wd_start_epoch"": with_attributes(_NUMBER,\n                                                          description=""Epoch to disable weight decay in the optimizer"")\n            },\n            ""additionalProperties"": False\n        },\n        **COMMON_COMPRESSION_ALGORITHM_PROPERTIES\n    },\n    ""additionalProperties"": False\n}\n\nCONST_SPARSITY_ALGO_NAME_IN_CONFIG = ""const_sparsity""\nCONST_SPARSITY_SCHEMA = {\n    **BASIC_COMPRESSION_ALGO_SCHEMA,\n    ""properties"": {\n        ""algorithm"": {\n            ""const"": CONST_SPARSITY_ALGO_NAME_IN_CONFIG\n        },\n        **COMMON_COMPRESSION_ALGORITHM_PROPERTIES,\n    },\n    ""additionalProperties"": False,\n    ""description"": ""This algorithm takes no additional parameters and is used when you want to load ""\n                   ""a checkpoint trained with another sparsity algorithm and do other compression without ""\n                   ""changing the sparsity mask.""\n}\n\nCOMMON_SPARSITY_PARAM_PROPERTIES = {\n    ""schedule"": with_attributes(_STRING,\n                                description=""The type of scheduling to use for adjusting the target""\n                                            ""sparsity level""),\n    ""patience"": with_attributes(_NUMBER,\n                                description=""A regular patience parameter for the scheduler, ""\n                                            ""as for any other standard scheduler. Specified in units ""\n                                            ""of scheduler steps.""),\n    ""sparsity_init"": with_attributes(_NUMBER,\n                                     description=""Initial value of the sparsity level applied to the ""\n                                                 ""model""),\n    ""sparsity_target"": with_attributes(_NUMBER,\n                                       description=""Target value of the sparsity level for the model""),\n    ""sparsity_steps"": with_attributes(_NUMBER,\n                                      description=""The default scheduler will do this many ""\n                                                  ""proportional target sparsity level adjustments, ""\n                                                  ""distributed evenly across ""\n                                                  ""\'sparsity_training_steps\'.""),\n    ""sparsity_training_steps"": with_attributes(_NUMBER,\n                                               description=""The number of steps after which the ""\n                                                           ""sparsity mask will be frozen and no ""\n                                                           ""longer trained""),\n    ""multistep_steps"": with_attributes(_ARRAY_OF_NUMBERS,\n                                       description=""A list of scheduler steps at which to transition ""\n                                                   ""to the next scheduled sparsity level (multistep ""\n                                                   ""scheduler only).""),\n    ""multistep_sparsity_levels"": with_attributes(_ARRAY_OF_NUMBERS,\n                                                 description=""Levels of sparsity to use at each step ""\n                                                             ""of the scheduler as specified in the ""\n                                                             ""\'multistep_steps\' attribute. The first""\n                                                             ""sparsity level will be applied ""\n                                                             ""immediately, so the length of this list ""\n                                                             ""should be larger than the length of the ""\n                                                             ""\'steps\' by one."")\n}\n\nMAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG = ""magnitude_sparsity""\nMAGNITUDE_SPARSITY_SCHEMA = {\n    **BASIC_COMPRESSION_ALGO_SCHEMA,\n    ""properties"": {\n        ""algorithm"": {\n            ""const"": MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG\n        },\n        ""params"":\n            {\n                ""type"": ""object"",\n                ""properties"": COMMON_SPARSITY_PARAM_PROPERTIES,\n                ""additionalProperties"": False\n            },\n        **COMMON_COMPRESSION_ALGORITHM_PROPERTIES\n    },\n    ""additionalProperties"": False\n}\n\nRB_SPARSITY_ALGO_NAME_IN_CONFIG = ""rb_sparsity""\nRB_SPARSITY_SCHEMA = {\n    **BASIC_COMPRESSION_ALGO_SCHEMA,\n    ""properties"": {\n        ""algorithm"": {\n            ""const"": RB_SPARSITY_ALGO_NAME_IN_CONFIG\n        },\n        # TODO: properly search for the initialize in the compression algo list and\n        # remove the necessity to piggyback on the RB sparsity algo entry in the config\n        ""initializer"": INITIALIZER_SCHEMA,\n        ""params"":\n            {\n                ""type"": ""object"",\n                ""properties"": COMMON_SPARSITY_PARAM_PROPERTIES,\n                ""additionalProperties"": False\n            },\n        **COMMON_COMPRESSION_ALGORITHM_PROPERTIES\n    },\n    ""additionalProperties"": False\n}\n\nFILTER_PRUNING_ALGO_NAME_IN_CONFIG = \'filter_pruning\'\nFILTER_PRUNING_SCHEMA = {\n    **BASIC_COMPRESSION_ALGO_SCHEMA,\n    ""properties"": {\n        ""algorithm"": {\n            ""const"": FILTER_PRUNING_ALGO_NAME_IN_CONFIG\n        },\n        ""params"":\n            {\n                ""type"": ""object"",\n                ""properties"": {\n                    ""schedule"": with_attributes(_STRING,\n                                                description=""The type of scheduling to use for adjusting the target""\n                                                            "" pruning level. Either `exponential`, `exponential_with""\n                                                            ""_bias`,  or `baseline`, by default it is `baseline`""),\n                    ""pruning_init"": with_attributes(_NUMBER,\n                                                    description=""Initial value of the pruning level applied to the""\n                                                                "" model. 0.0 by default.""),\n                    ""pruning_target"": with_attributes(_NUMBER,\n                                                      description=""Target value of the pruning level for the model.""\n                                                                  "" 0.5 by default.""),\n                    ""num_init_steps"": with_attributes(_NUMBER,\n                                                      description=""Number of epochs for model pretraining before""\n                                                                  "" starting filter pruning. 0 by default.""),\n                    ""pruning_steps"": with_attributes(_NUMBER,\n                                                     description=""Number of epochs during which the pruning rate is""\n                                                                 "" increased from `pruning_init` to `pruning_target`""\n                                                                 "" value.""),\n                    ""weight_importance"": with_attributes(_STRING,\n                                                         description=""The type of filter importance metric. Can be""\n                                                                     "" one of `L1`, `L2`, `geometric_median`.""\n                                                                     "" `L2` by default.""),\n                    ""all_weights"": with_attributes(_BOOLEAN,\n                                                   description=""Whether to prune layers independently (choose filters""\n                                                               "" with the smallest importance in each layer separately)""\n                                                               "" or not. `False` by default."",\n                                                   default=False),\n                    ""prune_first_conv"": with_attributes(_BOOLEAN,\n                                                        description=""Whether to prune first Convolutional layers or""\n                                                                    "" not. First means that it is a convolutional layer""\n                                                                    "" such that there is a path from model input to ""\n                                                                    ""this layer such that there are no other ""\n                                                                    ""convolution operations on it. `False` by default."",\n                                                        default=False\n                                                        ),\n                    ""prune_last_conv"": with_attributes(_BOOLEAN,\n                                                       description=""whether to prune last Convolutional layers or not.""\n                                                                   ""  Last means that it is a Convolutional layer such""\n                                                                   "" that there is a path from this layer to the model""\n                                                                   "" output such that there are no other convolution""\n                                                                   "" operations on it. `False` by default. "",\n                                                       default=False\n                                                       ),\n                    ""prune_downsample_convs"": with_attributes(_BOOLEAN,\n                                                              description=""whether to prune downsample Convolutional""\n                                                                          "" layers (with stride > 1) or not. `False`""\n                                                                          "" by default."",\n                                                              default=False\n                                                              ),\n                    ""prune_batch_norms"": with_attributes(_BOOLEAN,\n                                                         description=""whether to nullifies parameters of Batch Norm""\n                                                                     "" layer corresponds to zeroed filters of""\n                                                                     "" convolution corresponding to this Batch Norm.""\n                                                                     "" `False` by default."",\n                                                         default=False\n                                                         ),\n                    ""zero_grad"": with_attributes(_BOOLEAN,\n                                                 description=""Whether to setting gradients corresponding to zeroed""\n                                                             "" filters to zero during training, `True` by default."",\n                                                 default=True),\n\n                },\n                ""additionalProperties"": False,\n            },\n        **COMMON_COMPRESSION_ALGORITHM_PROPERTIES\n    },\n    ""additionalProperties"": False\n}\n\nALL_SUPPORTED_ALGO_SCHEMAE = [BINARIZATION_SCHEMA,\n                              QUANTIZATION_SCHEMA,\n                              CONST_SPARSITY_SCHEMA,\n                              MAGNITUDE_SPARSITY_SCHEMA,\n                              RB_SPARSITY_SCHEMA,\n                              FILTER_PRUNING_SCHEMA]\n\nREF_VS_ALGO_SCHEMA = {BINARIZATION_ALGO_NAME_IN_CONFIG: BINARIZATION_SCHEMA,\n                      QUANTIZATION_ALGO_NAME_IN_CONFIG: QUANTIZATION_SCHEMA,\n                      CONST_SPARSITY_ALGO_NAME_IN_CONFIG: CONST_SPARSITY_SCHEMA,\n                      MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG: MAGNITUDE_SPARSITY_SCHEMA,\n                      RB_SPARSITY_ALGO_NAME_IN_CONFIG: RB_SPARSITY_SCHEMA,\n                      FILTER_PRUNING_ALGO_NAME_IN_CONFIG: FILTER_PRUNING_SCHEMA}\n\nROOT_NNCF_CONFIG_SCHEMA = {\n    ""$schema"": ""http://json-schema.org/draft/2019-09/schema#"",\n    ""type"": ""object"",\n    ""properties"": {\n        ""input_info"": with_attributes(make_object_or_array_of_objects_schema(SINGLE_INPUT_INFO_SCHEMA),\n                                      description=""Required - describe the specifics of your model inputs here.""\n                                                  ""This information is used to build the internal graph representation""\n                                                  ""that is leveraged for proper compression functioning, and for ""\n                                                  ""exporting the compressed model to ONNX.""),\n        ""disable_shape_matching"": with_attributes(_BOOLEAN,\n                                                  description=""Whether to enable strict input tensor""\n                                                              ""shape matching when building the internal graph""\n                                                              ""representation of the model. Set this to false if your""\n                                                              ""model inputs have any variable dimension other than ""\n                                                              ""the 0-th (batch) dimension, or if any non-batch ""\n                                                              ""dimension of the intermediate tensors in your model ""\n                                                              ""execution flow depends on the input dimension,""\n                                                              ""otherwise the compression will most likely fail.""),\n        # Validation of each separate compression description schema occurs in a separate step.\n        # This is required for better user feedback, since holistic schema validation is uninformative\n        # if there is an error in one of the compression configs.\n        ""compression"": make_object_or_array_of_objects_schema(BASIC_COMPRESSION_ALGO_SCHEMA),\n        ""hw_config_type"": with_attributes(_STRING,\n                                          description=""If specified, the compression algorithms will use parameter ""\n                                                      ""presets that are more likely to result in best performance on ""\n                                                      ""a given HW type."")\n    },\n    ""required"": [""input_info""],\n    ""definitions"": REF_VS_ALGO_SCHEMA\n}\n\n\ndef validate_single_compression_algo_schema(single_compression_algo_dict: Dict):\n    """"""single_compression_algo_dict must conform to BASIC_COMPRESSION_ALGO_SCHEMA (and possibly has other\n    algo-specific properties""""""\n    algo_name = single_compression_algo_dict[""algorithm""]\n    if algo_name not in REF_VS_ALGO_SCHEMA:\n        raise jsonschema.ValidationError(\n            ""Incorrect algorithm name - must be one of ({})"".format("", "".join(REF_VS_ALGO_SCHEMA.keys())))\n    try:\n        jsonschema.validate(single_compression_algo_dict, schema=REF_VS_ALGO_SCHEMA[algo_name])\n    except Exception as e:\n        import sys\n        raise type(e)(""For algorithm: \'{}\'\\n"".format(algo_name) + str(e)).with_traceback(sys.exc_info()[2])\n'"
pytorch_toolkit/nncf/nncf/debug.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport logging\nimport warnings\nfrom typing import List, Dict\n\nfrom torch.nn import Module\nfrom nncf.nncf_logger import logger as nncf_logger\n\nDEBUG_LOG_DIR = ""/tmp""\n\n\ndef is_debug():\n    return nncf_logger.getEffectiveLevel() == logging.DEBUG\n\n\ndef set_debug_log_dir(dir_: str):\n    global DEBUG_LOG_DIR\n    DEBUG_LOG_DIR = dir_\n\n\n\nclass CallCountTracker:\n    def __init__(self, name):\n        self.name = name\n        self.call_counts = {}\n\n    def init_with_key_list(self, key_list: List):\n        self.call_counts = {key: 0 for key in key_list}\n        nncf_logger.debug(""{} tracker: registered {} entries"".format(self.name, len(self.call_counts)))\n\n    def register_call(self, key, counts=None):\n        if key not in self.call_counts:\n            warnings.warn(""DEBUG: {} tracker: called an unregistered module: {}"".format(self.name, key))\n            return\n        if counts is None:\n            self.call_counts[key] += 1\n        else:\n            self.call_counts[key] = counts\n\n    def get_never_called_keys(self) -> List[str]:\n        return [k for k, v in self.call_counts.items() if v == 0]\n\n    def get_overcalled_keys_with_call_counts(self) -> Dict[str, int]:\n        return {k: v for k, v in self.call_counts.items() if v > 1}\n\n    def get_total_call_count(self) -> int:\n        if self.call_counts:\n            return sum(self.call_counts.values())\n        return 0\n\n    def reset(self):\n        for key in self.call_counts:\n            self.call_counts[key] = 0\n\n\nclass DebugInterface:\n    def pre_forward_actions(self, module: Module):\n        raise NotImplementedError\n\n    def post_forward_actions(self, module: Module):\n        raise NotImplementedError\n\n    def init_actual(self, owner_model):\n        raise NotImplementedError\n\n\ndef debuggable_forward(forward_func):\n    def decorated(self, *args, **kwargs):\n        if self.debug_interface is not None:\n            self.debug_interface.pre_forward_actions(module=self)\n        retval = forward_func(self, *args, **kwargs)\n        if self.debug_interface is not None:\n            self.debug_interface.post_forward_actions(module=self)\n        return retval\n\n    return decorated\n\n\nclass CombinedDebugInterface(DebugInterface):\n    def __init__(self):\n        self._interfaces = []  # type: List[DebugInterface]\n\n    def add_interface(self, interface: \'DebugInterface\'):\n        self._interfaces.append(interface)\n\n    def init_actual(self, owner_model: \'NNCFNetwork\'):\n        for interface in self._interfaces:\n            interface.init_actual(owner_model)\n\n    def pre_forward_actions(self, module: Module):\n        for interface in self._interfaces:\n            interface.pre_forward_actions(module)\n\n    def post_forward_actions(self, module: Module):\n        for interface in self._interfaces:\n            interface.post_forward_actions(module)\n'"
pytorch_toolkit/nncf/nncf/definitions.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport pkg_resources\nimport email\nimport os\nNNCF_PACKAGE_ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nHW_CONFIG_RELATIVE_DIR = ""hw_configs""\n\n\ndef get_install_type():\n    try:\n        d = pkg_resources.get_distribution(\'nncf\').get_metadata(\'PKG-INFO\')\n    except pkg_resources.DistributionNotFound:\n        # Working with NNCF while not installed as a package\n        return ""GPU""\n    install_type = email.message_from_string(d)[\'Keywords\']\n    return install_type\n'"
pytorch_toolkit/nncf/nncf/functions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\n\ndef clamp(x, low, high):\n    return torch.max(torch.min(x, high), low)\n\n\ndef logit(x):\n    return torch.log(x / (1 - x))\n\n\nclass STRound(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, input_, inplace=False):\n        return g.op(""STRound"", input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        output = input_.round()\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\n\nclass STThreshold(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_):\n        output = (input_ > 0.5).type(input_.dtype)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n'"
pytorch_toolkit/nncf/nncf/hw_config.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import OrderedDict\nfrom enum import Enum\nfrom typing import Type, List, Dict\n\nimport addict as ad\nimport jstyleson as json\nimport warnings\n\nfrom nncf.config import product_dict\nfrom nncf.definitions import NNCF_PACKAGE_ROOT_DIR, HW_CONFIG_RELATIVE_DIR\nfrom nncf.dynamic_graph.operator_metatypes import OPERATOR_METATYPES\nfrom nncf.hw_config_op_names import HWConfigOpName\nfrom nncf.quantization.layers import QuantizerConfig, QuantizationMode\n\n\nclass HWConfigType(Enum):\n    CPU = ""cpu""\n    GPU = ""gpu""\n    VPU = ""vpu""\n\n    @staticmethod\n    def from_str(config_value: str) -> \'HWConfigType\':\n        if config_value == HWConfigType.CPU.value:\n            return HWConfigType.CPU\n        if config_value == HWConfigType.GPU.value:\n            return HWConfigType.GPU\n        if config_value == HWConfigType.VPU.value:\n            return HWConfigType.VPU\n        raise RuntimeError(""Unknown HW config type string"")\n\n\ndef get_metatypes_by_hw_config_name(hw_config_name: HWConfigOpName) -> List[\'OperatorMetatype\']:\n    retval = []\n    for op_meta in OPERATOR_METATYPES.registry_dict.values():  # type: OperatorMetatype\n        if hw_config_name in op_meta.hw_config_names:\n            retval.append(op_meta)\n    return retval\n\n\nclass HWConfig(List):\n    QUANTIZATION_ALGORITHM_NAME = ""quantization""\n\n    TYPE_TO_CONF_NAME_DICT = {\n        HWConfigType.CPU: ""cpu.json"",\n        HWConfigType.VPU: ""vpu.json"",\n        HWConfigType.GPU: ""gpu.json""\n    }\n\n    def __init__(self):\n        super().__init__()\n        self.registered_algorithm_configs = {}\n        self.target_device = None\n\n    @staticmethod\n    def get_path_to_hw_config(hw_config_type: HWConfigType):\n        return \'/\'.join([NNCF_PACKAGE_ROOT_DIR, HW_CONFIG_RELATIVE_DIR,\n                         HWConfig.TYPE_TO_CONF_NAME_DICT[hw_config_type]])\n\n    @classmethod\n    def from_json(cls, path):\n        # pylint:disable=too-many-nested-blocks,too-many-branches\n        with open(path) as f:\n            json_config = json.load(f, object_pairs_hook=OrderedDict)\n            hw_config = cls()\n            hw_config.target_device = json_config[\'target_device\']\n\n            for algorithm_name, algorithm_configs in json_config.get(\'config\', {}).items():\n                hw_config.registered_algorithm_configs[algorithm_name] = {}\n                for algo_config_alias, algo_config in algorithm_configs.items():\n                    for key, val in algo_config.items():\n                        if not isinstance(val, list):\n                            algo_config[key] = [val]\n\n                    hw_config.registered_algorithm_configs[algorithm_name][algo_config_alias] = list(\n                        product_dict(algo_config))\n\n            for op_dict in json_config.get(\'operations\', []):\n                for algorithm_name in op_dict:\n                    if algorithm_name not in hw_config.registered_algorithm_configs:\n                        continue\n                    tmp_config = {}\n                    for algo_and_op_specific_field_name, algorithm_configs in op_dict[algorithm_name].items():\n                        if not isinstance(algorithm_configs, list):\n                            algorithm_configs = [algorithm_configs]\n\n                        tmp_config[algo_and_op_specific_field_name] = []\n                        for algorithm_config in algorithm_configs:\n                            if isinstance(algorithm_config, str):  # Alias was supplied\n                                tmp_config[algo_and_op_specific_field_name].extend(\n                                    hw_config.registered_algorithm_configs[algorithm_name][algorithm_config])\n                            else:\n                                for key, val in algorithm_config.items():\n                                    if not isinstance(val, list):\n                                        algorithm_config[key] = [val]\n\n                                tmp_config[algo_and_op_specific_field_name].extend(list(product_dict(algorithm_config)))\n\n                    op_dict[algorithm_name] = tmp_config\n\n                hw_config.append(ad.Dict(op_dict))\n\n            return hw_config\n\n    @staticmethod\n    def get_quantization_mode_from_config_value(str_val: str):\n        if str_val == ""symmetric"":\n            return QuantizationMode.SYMMETRIC\n        if str_val == ""asymmetric"":\n            return QuantizationMode.ASYMMETRIC\n        raise RuntimeError(""Invalid quantization type specified in HW config"")\n\n    @staticmethod\n    def get_is_per_channel_from_config_value(str_val: str):\n        if str_val == ""perchannel"":\n            return True\n        if str_val == ""pertensor"":\n            return False\n        raise RuntimeError(""Invalid quantization granularity specified in HW config"")\n\n    @staticmethod\n    def get_qconf_from_hw_config_subdict(quantization_subdict: Dict):\n        bits = quantization_subdict[""bits""]\n        mode = HWConfig.get_quantization_mode_from_config_value(quantization_subdict[""mode""])\n        is_per_channel = HWConfig.get_is_per_channel_from_config_value(quantization_subdict[""granularity""])\n        return QuantizerConfig(bits=bits,\n                               mode=mode,\n                               per_channel=is_per_channel)\n\n    def get_metatype_vs_quantizer_configs_map(self, for_weights=False) -> Dict[Type[\'OperatorMetatype\'],\n                                                                               List[QuantizerConfig]]:\n        # \'None\' for marking ops as quantization agnostic by default if not specified otherwise by HW config\n        retval = {k: None for k in OPERATOR_METATYPES.registry_dict.values()}\n        config_key = ""weights"" if for_weights else ""activations""\n        for op_dict in self:\n            hw_config_op_name = op_dict.type  # type: HWConfigOpName\n\n            metatypes = get_metatypes_by_hw_config_name(hw_config_op_name)\n            if not metatypes:\n                warnings.warn(""Operation name {} in HW config is not registered in NNCF under any supported operation ""\n                              ""metatype - will be ignored"".format(hw_config_op_name))\n\n            if self.QUANTIZATION_ALGORITHM_NAME in op_dict:\n                allowed_qconfs = op_dict[self.QUANTIZATION_ALGORITHM_NAME][config_key]\n            else:\n                # TODO: Ops without specified quantization configs actually have to be associated\n                # with a special ""wildcard"" quantizer that can be merged with any non-wildcard quantizer\n                # or, if no merge occured during propagation, use any quantizer configuration. This is\n                # to ensure that as many ops in the model control flow graph as possible are executed in\n                # low precision to conserve memory.\n                allowed_qconfs = None\n\n            if allowed_qconfs is not None:\n                qconf_list_with_possible_duplicates = []\n                for hw_config_qconf_dict in allowed_qconfs:\n                    qconf_list_with_possible_duplicates.append(\n                        self.get_qconf_from_hw_config_subdict(hw_config_qconf_dict))\n\n                qconf_list = list(OrderedDict.fromkeys(qconf_list_with_possible_duplicates))\n            else:\n                qconf_list = None\n\n            for meta in metatypes:\n                retval[meta] = qconf_list\n\n        return retval\n'"
pytorch_toolkit/nncf/nncf/hw_config_op_names.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\nclass HWConfigOpName:\n    CONVOLUTION = ""Convolution""\n    DEPTHWISECONVOLUTION = ""DepthWiseConvolution""\n    MATMUL = ""MatMul""\n    ADD = ""Add""\n    MULTIPLY = ""Multiply""\n    MAXIMUM = ""Maximum""\n    LESS = ""Less""\n    LESSEQUAL = ""LessEqual""\n    GREATER = ""Greater""\n    GREATEREQUAL = ""GreaterEqual""\n    DIVIDE = ""Divide""\n    MINIMUM = ""Minimum""\n    EQUAL = ""Equal""\n    SUBTRACT = ""Subtract""\n    NOTEQUAL = ""NotEqual""\n    FLOORMOD = ""FloorMod""\n    LOGICALOR = ""LogicalOr""\n    LOGICALXOR = ""LogicalXor""\n    LOGICALAND = ""LogicalAnd""\n    LOGICALNOT = ""LogicalNot""\n    POWER = ""Power""\n    AVGPOOL = ""AvgPool""\n    REDUCEMEAN = ""ReduceMean""\n    MAXPOOL = ""MaxPool""\n    REDUCEMAX = ""ReduceMax""\n    INTERPOLATE = ""Interpolate""\n    MVN = ""MVN""\n    RESHAPE = ""Reshape""\n    CONCAT = ""Concat""\n    FLATTEN = ""Flatten""\n    SQUEEZE = ""Squeeze""\n    UNSQUEEZE = ""Unsqueeze""\n    SPLIT = ""Split""\n    CROP = ""Crop""\n    TRANSPOSE = ""Transpose""\n    TILE = ""Tile""\n'"
pytorch_toolkit/nncf/nncf/initialization.py,0,"b'import logging\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Dict, Tuple\n\nimport torch\nfrom tqdm import tqdm\n\nfrom nncf.utils import objwalk\nfrom nncf.quantization.init_range import MinMaxInitializer, ThreeSigmaInitializer, MeanMinMaxInitializer\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\n\ndef wrap_data_loader(data_loader, default_wrapper_cls, kwargs=None, device=None):\n    # pylint: disable=unidiomatic-typecheck\n    if type(data_loader) == torch.utils.data.DataLoader:\n        wrapped_data_loader = default_wrapper_cls(data_loader=data_loader,\n                                                  kwargs=kwargs,\n                                                  device=device)\n    else:\n        wrapped_data_loader = data_loader\n    return wrapped_data_loader\n\n\nclass RangeInitializerFactory:\n    @staticmethod\n    def create(init_type: str):\n        if init_type == ""min_max"":\n            return MinMaxInitializer\n        if init_type == ""threesigma"":\n            return ThreeSigmaInitializer\n        if init_type == ""mean_min_max"":\n            return MeanMinMaxInitializer\n        raise NotImplementedError\n\n\nclass InitializingDataLoader:\n    """"""\n    This class wraps the torch.utils.data.DataLoader class,\n    enabling to return a tuple containing the input tensor\n    and additional kwargs required for the forward pass\n    when used as an iterator.\n    Custom logic to prepare the input tensor might be\n    added to the __next__ method.\n    """"""\n\n    def __init__(self, data_loader, kwargs, device):\n        self.data_loader = data_loader\n        self.kwargs = kwargs\n        self.device = device\n\n    def __iter__(self):\n        self.data_loader_iter = iter(self.data_loader)\n        return self\n\n    def __next__(self):\n        batch_input, batch_target = next(self.data_loader_iter)\n        tup = (batch_input, batch_target)\n\n        def is_tensor(obj):\n            return isinstance(obj, torch.Tensor)\n\n        to_device_fn = partial(torch.Tensor.to, device=self.device)\n        batch_input, batch_target = objwalk(tup, is_tensor, to_device_fn)\n        return batch_input, batch_target, self.kwargs\n\n    @property\n    def num_workers(self):\n        return self.data_loader.num_workers\n\n    @num_workers.setter\n    def num_workers(self, num_workers):\n        self.data_loader.num_workers = num_workers\n\n\nclass DataLoaderInitializeRunner:\n    def __init__(self, model, modules_to_init: Dict[str, Tuple[torch.nn.Module, str]]):\n        super().__init__()\n        self.model = model\n        self.modules_to_init = modules_to_init\n\n    def run(self, data_loader, num_init_steps, is_distributed, *args, **kwargs):\n        class TQDMStream:\n            @classmethod\n            def write(cls, msg):\n                tqdm.write(msg, end=\'\')\n\n        stream_handler = logging.StreamHandler(TQDMStream)\n        nncf_logger.addHandler(stream_handler)\n        device = next(self.model.parameters()).device\n        wrapped_data_loader = wrap_data_loader(data_loader, InitializingDataLoader, kwargs, device)\n\n        initializers = OrderedDict()\n        hook_handles = []\n        for name, data in self.modules_to_init.items():\n            module, init_type = data\n            initializers[name] = RangeInitializerFactory.create(init_type)(module,\n                                                                           is_distributed,\n                                                                           log_module_name=name)\n            hook_handles.append(module.register_forward_hook(initializers[name].forward_hook))\n            module.init_stage = True\n        with torch.no_grad():\n            bar_format = \'{l_bar}{bar} |{n_fmt}/{total_fmt} [{elapsed}<{remaining}]\'\n            bar_desc = \'Algorithm initialization\'\n            for i, (input_, _, dataloader_kwargs) in tqdm(enumerate(wrapped_data_loader), total=num_init_steps,\n                                                          desc=bar_desc, bar_format=bar_format):\n                if num_init_steps is not None and i >= num_init_steps:\n                    break\n                self.model(input_, **dataloader_kwargs)\n            nncf_logger.removeHandler(stream_handler)\n            for handle in hook_handles:\n                handle.remove()\n            for initializer in initializers.values():\n                initializer.apply_init()\n\n        for module, _ in self.modules_to_init.values():\n            module.init_stage = False\n'"
pytorch_toolkit/nncf/nncf/layer_utils.py,0,"b""import torch.nn as nn\nfrom .registry import Registry\n\n\nCOMPRESSION_MODULES = Registry('compression modules')\n\n\nclass ProxyModule:\n    def __init__(self, module):\n        self._module = module\n\n    def __getattr__(self, name):\n        return getattr(self._module, name)\n\n\nclass _NNCFModuleMixin:\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pre_ops = nn.ModuleDict()\n        self.post_ops = nn.ModuleDict()\n\n    def get_pre_op(self, key):\n        return self.pre_ops[key]\n\n    def get_post_op(self, key):\n        return self.post_ops[key]\n\n    def register_pre_forward_operation(self, op):\n        key = str(len(self.pre_ops))\n        self.pre_ops[key] = op\n        return key\n\n    def remove_pre_forward_operation(self, key):\n        return self.pre_ops.pop(key)\n\n    def register_post_forward_operation(self, op):\n        key = str(len(self.post_ops))\n        self.post_ops[key] = op\n        return key\n\n    def remove_post_forward_operation(self, key):\n        return self.post_ops.pop(key)\n\n    def forward(self, *args):\n        proxy_module = ProxyModule(self)\n        for op in self.pre_ops.values():\n            op_args = op(proxy_module, args)\n            if op_args is not None:\n                if not isinstance(op_args, tuple):\n                    op_args = tuple([op_args])\n                args = op_args\n        results = super().forward.__func__(proxy_module, *args)\n        for op in self.post_ops.values():\n            op_results = op(proxy_module, results)\n            if op_results is not None:\n                results = op_results\n        return results\n"""
pytorch_toolkit/nncf/nncf/layers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport math\nimport numbers\nfrom typing import Tuple, Optional\n\nimport torch\nimport torch.nn.functional as F\nimport warnings\nfrom torch import nn\nfrom torch.nn import init\nfrom torch.nn.utils.rnn import PackedSequence\n\nfrom nncf.registry import Registry\nfrom .layer_utils import _NNCFModuleMixin\n\n\ndef dict_update(src, dst, recursive=True):\n    for name, value in dst.items():\n        if recursive and name in src and isinstance(value, dict):\n            dict_update(src[name], value, recursive)\n        else:\n            src[name] = value\n\n\nclass NNCFConv1d(_NNCFModuleMixin, nn.Conv1d):\n    @staticmethod\n    def from_module(module):\n        assert module.__class__.__name__ == nn.Conv1d.__name__\n        nncf_conv = NNCFConv1d(\n            module.in_channels, module.out_channels, module.kernel_size, module.stride,\n            module.padding, module.dilation, module.groups, hasattr(module, \'bias\')\n        )\n        dict_update(nncf_conv.__dict__, module.__dict__)\n        return nncf_conv\n\n\nclass NNCFConv2d(_NNCFModuleMixin, nn.Conv2d):\n    @staticmethod\n    def from_module(module):\n        assert module.__class__.__name__ == nn.Conv2d.__name__\n        nncf_conv = NNCFConv2d(\n            module.in_channels, module.out_channels, module.kernel_size, module.stride,\n            module.padding, module.dilation, module.groups, hasattr(module, \'bias\')\n        )\n        dict_update(nncf_conv.__dict__, module.__dict__)\n        return nncf_conv\n\n\nclass NNCFLinear(_NNCFModuleMixin, nn.Linear):\n    @staticmethod\n    def from_module(module):\n        assert module.__class__.__name__ == nn.Linear.__name__\n\n        nncf_linear = NNCFLinear(module.in_features, module.out_features, hasattr(module, \'bias\'))\n        dict_update(nncf_linear.__dict__, module.__dict__)\n        return nncf_linear\n\n\nclass NNCFConvTranspose2d(_NNCFModuleMixin, nn.ConvTranspose2d):\n    @staticmethod\n    def from_module(module):\n        assert module.__class__.__name__ == nn.ConvTranspose2d.__name__\n        args = [module.in_channels, module.out_channels, module.kernel_size, module.stride,\n                module.padding, module.output_padding, module.groups, hasattr(module, \'bias\'),\n                module.dilation]\n        if hasattr(module, \'padding_mode\'):\n            args.append(module.padding_mode)\n        nncf_conv_transpose2d = NNCFConvTranspose2d(*args)\n        dict_update(nncf_conv_transpose2d.__dict__, module.__dict__)\n        return nncf_conv_transpose2d\n\n\nclass NNCFConv3d(_NNCFModuleMixin, nn.Conv3d):\n    @staticmethod\n    def from_module(module):\n        assert module.__class__.__name__ == nn.Conv3d.__name__\n\n        nncf_conv3d = NNCFConv3d(\n            module.in_channels, module.out_channels, module.kernel_size, module.stride,\n            module.padding, module.dilation, module.groups, hasattr(module, \'bias\')\n        )\n        dict_update(nncf_conv3d.__dict__, module.__dict__)\n        return nncf_conv3d\n\n\nclass NNCFConvTranspose3d(_NNCFModuleMixin, nn.ConvTranspose3d):\n    @staticmethod\n    def from_module(module):\n        assert module.__class__.__name__ == nn.ConvTranspose3d.__name__\n        args = [module.in_channels, module.out_channels, module.kernel_size, module.stride,\n                module.padding, module.output_padding, module.groups, hasattr(module, \'bias\'),\n                module.dilation]\n        if hasattr(module, \'padding_mode\'):\n            args.append(module.padding_mode)\n        nncf_conv_transpose3d = NNCFConvTranspose3d(*args)\n        dict_update(nncf_conv_transpose3d.__dict__, module.__dict__)\n        return nncf_conv_transpose3d\n\n\nNNCF_MODULES_DICT = {\n    NNCFConv1d: nn.Conv1d,\n    NNCFConv2d: nn.Conv2d,\n    NNCFConv3d: nn.Conv3d,\n    NNCFLinear: nn.Linear,\n    NNCFConvTranspose2d: nn.ConvTranspose2d,\n    NNCFConvTranspose3d: nn.ConvTranspose3d,\n}\n\nNNCF_MODULES_MAP = {k.__name__: v.__name__ for k, v in NNCF_MODULES_DICT.items()}\nNNCF_MODULES = list(NNCF_MODULES_MAP.keys())\n\n\nNNCF_CONV_MODULES_DICT = {\n    NNCFConv1d: nn.Conv1d,\n    NNCFConv2d: nn.Conv2d,\n    NNCFConv3d: nn.Conv3d,\n}\nNNCF_DECONV_MODULES_DICT = {\n    NNCFConvTranspose2d: nn.ConvTranspose2d,\n    NNCFConvTranspose3d: nn.ConvTranspose3d,\n}\nNNCF_CONV_MODULES_MAP = {k.__name__: v.__name__ for k, v in NNCF_CONV_MODULES_DICT.items()}\nNNCF_CONV_MODULES = list(NNCF_CONV_MODULES_MAP.keys())\n\n\nclass RNNCellBaseNNCF(nn.Module):\n    __constants__ = [\'input_size\', \'hidden_size\', \'bias\']\n\n    def __init__(self, input_size, hidden_size, bias, num_chunks):\n        super(RNNCellBaseNNCF, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        linear_ih = nn.Linear(input_size, num_chunks * hidden_size, self.bias)\n        linear_hh = nn.Linear(hidden_size, num_chunks * hidden_size, self.bias)\n        self.weight_ih = linear_ih.weight\n        self.weight_hh = linear_hh.weight\n        self.bias_ih = linear_ih.bias\n        self.bias_hh = linear_hh.bias\n        self.linear_list = [linear_ih, linear_hh]\n        self.reset_parameters()\n\n    def extra_repr(self):\n        s = \'{input_size}, {hidden_size}\'\n        if \'bias\' in self.__dict__ and self.bias is not True:\n            s += \', bias={bias}\'\n        if \'nonlinearity\' in self.__dict__ and self.nonlinearity != ""tanh"":\n            s += \', nonlinearity={nonlinearity}\'\n        return s.format(**self.__dict__)\n\n    def check_forward_input(self, input_):\n        if input_.size(1) != self.input_size:\n            raise RuntimeError(\n                ""input_ has inconsistent input_size: got {}, expected {}"".format(\n                    input_.size(1), self.input_size))\n\n    def check_forward_hidden(self, input_, hx, hidden_label=\'\'):\n        # type: (Tensor, Tensor, str) -> None\n        if input_.size(0) != hx.size(0):\n            raise RuntimeError(\n                ""Input batch size {} doesn\'t match hidden{} batch size {}"".format(\n                    input_.size(0), hidden_label, hx.size(0)))\n\n        if hx.size(1) != self.hidden_size:\n            raise RuntimeError(\n                ""hidden{} has inconsistent hidden_size: got {}, expected {}"".format(\n                    hidden_label, hx.size(1), self.hidden_size))\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            init.uniform_(weight, -stdv, stdv)\n\n    def forward(self, input_, hidden):\n        raise NotImplementedError\n\n\nITERATION_MODULES = Registry(\'iteration_modules\')\n\n@ITERATION_MODULES.register()\nclass LSTMCellForwardNNCF(nn.Module):\n    def __init__(self, input_linear, hidden_linear):\n        super().__init__()\n        self.input_linear = input_linear\n        self.hidden_linear = hidden_linear\n\n    def forward(self, input_, hidden):\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> (Tensor, Tensor)\n        hx, cx = hidden\n        gates = self.input_linear(input_) + self.hidden_linear(hx)\n\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = F.sigmoid(ingate)\n        forgetgate = F.sigmoid(forgetgate)\n        cellgate = F.tanh(cellgate)\n        outgate = F.sigmoid(outgate)\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n        hy = outgate * F.tanh(cy)\n        return hy, cy\n\n\nclass LSTMCellNNCF(RNNCellBaseNNCF):\n    def __init__(self, input_size=1, hidden_size=1, bias=True):\n        super(LSTMCellNNCF, self).__init__(input_size, hidden_size, bias, num_chunks=4)\n        self.cell = LSTMCellForwardNNCF(self.linear_list[0], self.linear_list[1])\n\n    def forward(self, input_, hidden=None):\n        self.check_forward_input(input_)\n        if hidden is None:\n            zeros = torch.zeros(input_.size(0), self.hidden_size, dtype=input_.dtype, device=input_.device)\n            hidden = (zeros, zeros)\n        self.check_forward_hidden(input_, hidden[0], \'[0]\')\n        self.check_forward_hidden(input_, hidden[1], \'[1]\')\n\n        return self.cell(input_, hidden)\n\n\n@ITERATION_MODULES.register()\nclass StackedRNN(nn.Module):\n    class StackedRNNResetPoint(nn.Module):\n        """"""\n        Intentionally wrap concat, which is called inside nested loops, as a separate module.\n        It allows not to add new node to nncf graph on each iteration of the loops.\n        """"""\n\n        def forward(self, all_output, input_):\n            input_ = torch.cat(all_output, input_.dim() - 1)\n            return input_\n\n    def __init__(self, inners, num_layers, lstm=False, dropout=0):\n        super().__init__()\n        self.lstm = lstm\n        self.num_layers = num_layers\n        self.num_directions = int(len(inners) / num_layers)\n        self.inners = nn.ModuleList(inners)\n        self.total_layers = self.num_layers * self.num_directions\n        self.dropout = dropout\n\n    def forward(self, input_, hidden, batch_sizes):\n        next_hidden = []\n\n        if self.lstm:\n            hidden = list(zip(*hidden))\n\n        for i in range(self.num_layers):\n            all_output = []\n            for j in range(self.num_directions):\n                l = i * self.num_directions + j\n                hy, output = self.inners[l](input_, hidden[l], batch_sizes)\n                next_hidden.append(hy)\n                all_output.append(output)\n\n            input_ = self.StackedRNNResetPoint()(all_output, input_)\n            if self.dropout != 0 and i < self.num_layers - 1:\n                input_ = F.dropout(input_, p=self.dropout, training=self.training, inplace=False)\n\n        if self.lstm:\n            next_h, next_c = zip(*next_hidden)\n            next_hidden = (\n                torch.cat(next_h, 0).view(self.total_layers, *next_h[0].size()),\n                torch.cat(next_c, 0).view(self.total_layers, *next_c[0].size())\n            )\n        else:\n            next_hidden = torch.cat(next_hidden, 0).view(\n                self.total_layers, *next_hidden[0].size())\n        return next_hidden, input_\n\n\n@ITERATION_MODULES.register()\nclass Recurrent(nn.Module):\n    def __init__(self, cell, reverse=False):\n        super().__init__()\n        self.reverse = reverse\n        self.cell = cell\n\n    def forward(self, input_, hidden, batch_sizes=None):\n        output = []\n        steps = range(input_.size(0) - 1, -1, -1) if self.reverse else range(input_.size(0))\n        for i in steps:\n            hidden = self.cell(input_[i], hidden)\n            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n\n        if self.reverse:\n            output.reverse()\n        output = torch.cat(output, 0).view(input_.size(0), *output[0].size())\n\n        return hidden, output\n\n\ndef variable_recurrent_factory():\n    def factory(cell, reverse=False):\n        if reverse:\n            return VariableRecurrentReverse(cell)\n        return VariableRecurrent(cell)\n\n    return factory\n\n\n@ITERATION_MODULES.register()\nclass VariableRecurrent(nn.Module):\n    def __init__(self, cell):\n        super().__init__()\n        self.cell = cell\n\n    def forward(self, input_, hidden, batch_sizes):\n        output = []\n        input_offset = 0\n        last_batch_size = batch_sizes[0]\n        hiddens = []\n        flat_hidden = not isinstance(hidden, tuple)\n        if flat_hidden:\n            hidden = (hidden,)\n        for batch_size in batch_sizes:\n            step_input = input_[input_offset:input_offset + batch_size]\n            input_offset += batch_size\n\n            dec = last_batch_size - batch_size\n            if dec > 0:\n                hiddens.append(tuple(h[-dec:] for h in hidden))\n                hidden = tuple(h[:-dec] for h in hidden)\n            last_batch_size = batch_size\n\n            if flat_hidden:\n                hidden = (self.cell(step_input, hidden[0]),)\n            else:\n                hidden = self.cell(step_input, hidden)\n\n            output.append(hidden[0])\n        hiddens.append(hidden)\n        hiddens.reverse()\n\n        hidden = tuple(torch.cat(h, 0) for h in zip(*hiddens))\n        assert hidden[0].size(0) == batch_sizes[0]\n        if flat_hidden:\n            hidden = hidden[0]\n        output = torch.cat(output, 0)\n        return hidden, output\n\n\n@ITERATION_MODULES.register()\nclass VariableRecurrentReverse(nn.Module):\n    def __init__(self, cell):\n        super().__init__()\n        self.cell = cell\n\n    def forward(self, input_, hidden, batch_sizes):\n        output = []\n        input_offset = input_.size(0)\n        last_batch_size = batch_sizes[-1]\n        initial_hidden = hidden\n        flat_hidden = not isinstance(hidden, tuple)\n        if flat_hidden:\n            hidden = (hidden,)\n            initial_hidden = (initial_hidden,)\n        hidden = tuple(h[:batch_sizes[-1]] for h in hidden)\n        for batch_size in reversed(batch_sizes):\n            inc = batch_size - last_batch_size\n            hidden = self.ReverseResetPoint()(batch_size, hidden, inc, initial_hidden, last_batch_size)\n            last_batch_size = batch_size\n            step_input = input_[input_offset - batch_size:input_offset]\n            input_offset -= batch_size\n\n            if flat_hidden:\n                hidden = (self.cell(step_input, hidden[0]),)\n            else:\n                hidden = self.cell(step_input, hidden)\n            output.append(hidden[0])\n\n        output.reverse()\n        output = torch.cat(output, 0)\n        if flat_hidden:\n            hidden = hidden[0]\n        return hidden, output\n\n    @ITERATION_MODULES.register()\n    class ReverseResetPoint(nn.Module):\n        """"""\n        Intentionally wrap concat undef if condition as a separate module\n        to prevent adding new node to nncf graph on each iteration\n        """"""\n\n        def forward(self, batch_size, hidden, inc, initial_hidden, last_batch_size):\n            if inc > 0:\n                hidden = tuple(torch.cat((h, ih[last_batch_size:batch_size]), 0)\n                               for h, ih in zip(hidden, initial_hidden))\n            return hidden\n\n\nclass NNCF_RNN(nn.Module):\n    """"""Common class for RNN modules. Currently, LSTM is supported only""""""\n\n    def __init__(self, mode=\'LSTM\', input_size=1, hidden_size=1, num_layers=1, batch_first=False,\n                 dropout=0, bidirectional=False, bias=True):\n        super(NNCF_RNN, self).__init__()\n        self.mode = mode\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n\n        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \\\n            isinstance(dropout, bool):\n            raise ValueError(""dropout should be a number in range [0, 1] ""\n                             ""representing the probability of an element being ""\n                             ""zeroed"")\n        if dropout > 0 and num_layers == 1:\n            warnings.warn(""dropout option adds dropout after all but last ""\n                          ""recurrent layer, so non-zero dropout expects ""\n                          ""num_layers greater than 1, but got dropout={} and ""\n                          ""num_layers={}"".format(dropout, num_layers))\n\n        if mode == \'LSTM\':\n            gate_size = 4 * hidden_size\n            self.cell_type = LSTMCellForwardNNCF\n        else:\n            # elif mode == \'GRU\':\n            #     gate_size = 3 * hidden_size\n            # elif mode == \'RNN_TANH\':\n            #     gate_size = hidden_size\n            # elif mode == \'RNN_RELU\':\n            #     gate_size = hidden_size\n            # else:\n            raise ValueError(""Unrecognized RNN mode: "" + mode)\n\n        self._all_weights = []\n        self.cells = []\n        for layer in range(num_layers):\n            for direction in range(self.num_directions):\n                layer_input_size = input_size if layer == 0 else hidden_size * self.num_directions\n                linear_ih = nn.Linear(layer_input_size, gate_size, bias)\n                linear_hh = nn.Linear(hidden_size, gate_size, bias)\n                self.cells.append(self.cell_type(linear_ih, linear_hh))\n                params = (linear_ih.weight, linear_hh.weight, linear_ih.bias, linear_hh.bias)\n                suffix = \'_reverse\' if direction == 1 else \'\'\n                weight_names = [\'weight_ih_l{}{}\', \'weight_hh_l{}{}\']\n                if bias:\n                    weight_names += [\'bias_ih_l{}{}\', \'bias_hh_l{}{}\']\n                weight_names = [x.format(layer, suffix) for x in weight_names]\n                for name, param in zip(weight_names, params):\n                    setattr(self, name, param)\n                self._all_weights.append(weight_names)\n\n        self.reset_parameters()\n        self.variable_length = True\n        self.rnn_impl = self.get_rnn_impl(self.variable_length, self.cells)\n\n    def get_rnn_impl(self, variable_length, cells):\n        if variable_length:\n            rec_factory = variable_recurrent_factory()\n        else:\n            rec_factory = Recurrent\n        inners = []\n        for layer_idx in range(self.num_layers):\n            idx = layer_idx * self.num_directions\n            if self.bidirectional:\n                layer_inners = [rec_factory(cells[idx]), rec_factory(cells[idx + 1], reverse=True)]\n            else:\n                layer_inners = [rec_factory(cells[idx]), ]\n            inners.extend(layer_inners)\n        return StackedRNN(inners,\n                          self.num_layers,\n                          (self.mode == \'LSTM\'),\n                          dropout=self.dropout)\n\n    def check_forward_args(self, input_, hidden, batch_sizes):\n        is_input_packed = batch_sizes is not None\n        expected_input_dim = 2 if is_input_packed else 3\n        if input_.dim() != expected_input_dim:\n            raise RuntimeError(\n                \'input_ must have {} dimensions, got {}\'.format(\n                    expected_input_dim, input_.dim()))\n        if self.input_size != input_.size(-1):\n            raise RuntimeError(\n                \'input_.size(-1) must be equal to input_size. Expected {}, got {}\'.format(\n                    self.input_size, input_.size(-1)))\n\n        if is_input_packed:\n            mini_batch = int(batch_sizes[0])\n        else:\n            mini_batch = input_.size(0) if self.batch_first else input_.size(1)\n\n        expected_hidden_size = (mini_batch, self.hidden_size)\n\n        def check_hidden_size(hx, expected_hidden_size, msg=\'Expected hidden size {}, got {}\'):\n            expected_size = self.num_layers * self.num_directions\n            if expected_size != len(hx):\n                raise RuntimeError(\'Expected number of hidden states {}, got {}\'.format(expected_size, len(hx)))\n            for element in hx:\n                if tuple(element.size()) != expected_hidden_size:\n                    raise RuntimeError(msg.format(expected_hidden_size, tuple(element.size())))\n\n        if self.mode == \'LSTM\':\n            check_hidden_size(hidden[0], expected_hidden_size,\n                              \'Expected hidden[0] size {}, got {}\')\n            check_hidden_size(hidden[1], expected_hidden_size,\n                              \'Expected hidden[1] size {}, got {}\')\n        else:\n            check_hidden_size(hidden, expected_hidden_size)\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            init.uniform_(weight, -stdv, stdv)\n\n    @property\n    def all_weights(self):\n        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]\n\n    @staticmethod\n    def apply_permutation(tensor, permutation, dim=1):\n        # type: (Tensor, Tensor, int) -> Tensor\n        return tensor.index_select(dim, permutation)\n\n    def permute_hidden(self, hx, permutation):\n        # type: (Tuple[Tensor, Tensor], Optional[Tensor]) -> Tuple[Tensor, Tensor]\n        if permutation is None:\n            return hx\n        return self.apply_permutation(hx[0], permutation), self.apply_permutation(hx[1], permutation)\n\n    def prepare_hidden(self, hx, permutation):\n        # type: (Tuple[Tuple[Tensor], Tuple[Tensor]], Optional[Tensor]) -> Tuple[Tuple[Tensor], Tuple[Tensor]]\n        if permutation is None:\n            return hx\n        split_size = len(hx[0])\n        concat_hx = torch.cat([torch.unsqueeze(t, 0) for t in hx[0]])\n        concat_cx = torch.cat([torch.unsqueeze(t, 0) for t in hx[1]])\n        permuted_hidden = self.apply_permutation(concat_hx, permutation), self.apply_permutation(concat_cx, permutation)\n        hc = permuted_hidden[0].chunk(split_size, 0)\n        cc = permuted_hidden[1].chunk(split_size, 0)\n        hidden = (tuple(torch.squeeze(c, 0) for c in hc), tuple(torch.squeeze(c, 0) for c in cc))\n        return hidden\n\n    def forward(self, input_, hidden=None):\n        is_packed = isinstance(input_, PackedSequence)\n\n        sorted_indices = None\n        unsorted_indices = None\n        if is_packed:\n            input_, batch_sizes, sorted_indices, unsorted_indices = input_\n            max_batch_size = int(batch_sizes[0])\n        else:\n            batch_sizes = None\n            max_batch_size = input_.size(0) if self.batch_first else input_.size(1)\n\n        if hidden is None:\n            num_directions = 2 if self.bidirectional else 1\n            hidden = input_.new_zeros(self.num_layers * num_directions,\n                                      max_batch_size, self.hidden_size,\n                                      requires_grad=False)\n            if self.mode == \'LSTM\':\n                hidden = (hidden, hidden)\n        else:\n            # Each batch of the hidden state should match the input sequence that\n            # the user believes he/she is passing in.\n            hidden = self.prepare_hidden(hidden, sorted_indices)\n\n        self.check_forward_args(input_, hidden, batch_sizes)\n\n        is_currently_variable = batch_sizes is not None\n        if self.variable_length and not is_currently_variable or not self.variable_length and is_currently_variable:\n            # override rnn_impl, it\'s assumed that this should happen very seldom, as\n            # usually there\'s only one mode active whether variable length, or constant ones\n            self.rnn_impl = self.get_rnn_impl(is_currently_variable, self.cells)\n\n        if self.batch_first and batch_sizes is None:\n            input_ = input_.transpose(0, 1)\n\n        hidden, output = self.rnn_impl(input_, hidden, batch_sizes)\n\n        if self.batch_first and batch_sizes is None:\n            output = output.transpose(0, 1)\n\n        if is_packed:\n            output = PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)\n\n        return output, self.permute_hidden(hidden, unsorted_indices)\n'"
pytorch_toolkit/nncf/nncf/model_creation.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom os import path as osp\nfrom typing import Callable, Any, Tuple\n\nfrom torch.nn import Module\n\nfrom nncf.algo_selector import create_compression_algorithm_builders\nfrom nncf.compression_method_api import CompressionAlgorithmController\nfrom nncf.config import Config\nfrom nncf.debug import is_debug, set_debug_log_dir\nfrom nncf.dynamic_graph.graph_builder import create_input_infos, GraphBuilder, create_dummy_forward_fn\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.utils import is_main_process\n\n\ndef create_compressed_model(model: Module, config: Config, dummy_forward_fn: Callable[[Module], Any] = None,\n                            dump_graphs=True) -> Tuple[CompressionAlgorithmController, NNCFNetwork]:\n    """"""\n    The main function used to produce a model ready for compression fine-tuning from an original PyTorch\n    model and a configuration object.\n    dummy_forward_fn\n    :param model: The original model. Should have its parameters already loaded from a checkpoint or another\n    source.\n    :param config: A configuration object used to determine the exact compression modifications to be applied\n    to the model\n    :param dummy_forward_fn: will be used instead of a *forward* function call to build\n    the internal graph representation via tracing. Specifying this is useful when the original training pipeline\n    has special formats of data loader output or has additional *forward* arguments other than input tensors.\n    Otherwise, the *forward* call of the model during graph tracing will be made with mock tensors according\n    to the shape specified in the config object.\n    :param dump_graphs: Whether or not should also dump the internal graph representation of the\n    original and compressed models in the .dot format into the log directory.\n    :return: A controller for the compression algorithm (or algorithms, in which case the controller\n    is an instance of CompositeCompressionController) and the model ready for compression wrapped\n    as an object of NNCFNetwork.""""""\n\n    if dump_graphs:\n        if dummy_forward_fn is None:\n            input_info_list = create_input_infos(config)\n            graph_builder = GraphBuilder(custom_forward_fn=\n                                         create_dummy_forward_fn(input_info_list,\n                                                                 with_input_tracing=True))\n        else:\n            graph_builder = GraphBuilder(custom_forward_fn=dummy_forward_fn)\n\n        if is_main_process():\n            graph = graph_builder.build_graph(model)\n            graph.dump_graph(osp.join(config.log_dir, ""original_graph.dot""), extended=True)\n\n    if is_debug():\n        set_debug_log_dir(config.log_dir)\n\n    input_info_list = create_input_infos(config)\n    scopes_without_shape_matching = config.get(\'scopes_without_shape_matching\', [])\n    ignored_scopes = config.get(\'ignored_scopes\')\n    target_scopes = config.get(\'target_scopes\')\n\n    compressed_model = NNCFNetwork(model, input_infos=input_info_list,\n                                   dummy_forward_fn=dummy_forward_fn,\n                                   ignored_scopes=ignored_scopes,\n                                   target_scopes=target_scopes,\n                                   scopes_without_shape_matching=scopes_without_shape_matching)\n\n    compression_algo_builder_list = create_compression_algorithm_builders(config)\n\n    for builder in compression_algo_builder_list:\n        compressed_model = builder.apply_to(compressed_model)\n    compression_ctrl = compressed_model.commit_compression_changes()\n\n    if dump_graphs and is_main_process() and compression_algo_builder_list:\n        if dummy_forward_fn is None:\n            compressed_graph_builder = GraphBuilder(custom_forward_fn=\n                                                    create_dummy_forward_fn(input_info_list,\n                                                                            with_input_tracing=False))\n        else:\n            compressed_graph_builder = GraphBuilder(custom_forward_fn=dummy_forward_fn)\n\n        graph = compressed_graph_builder.build_graph(compressed_model, compressed_model.get_tracing_context())\n        graph.dump_graph(osp.join(config.log_dir, ""compressed_graph.dot""), extended=True)\n\n    return compression_ctrl, compressed_model\n'"
pytorch_toolkit/nncf/nncf/module_operations.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\n\nclass BaseOp(nn.Module):\n    def __init__(self, op):\n        super().__init__()\n        self.op = op\n\n    @property\n    def operand(self):\n        return self.op\n\n    def forward(self, *inputs):\n        return self.op(*inputs)\n\n\nclass UpdateInputs(BaseOp):\n    def __call__(self, _, inputs):\n        return super().__call__(*inputs)\n\n\nclass UpdateParameter(BaseOp):\n    def __init__(self, param_name, op):\n        super().__init__(op)\n        self._param_name = param_name\n\n    def __call__(self, module, _):\n        if not hasattr(module, self._param_name):\n            raise TypeError(\'{} should have {} attribute\'.format(type(module), self._param_name))\n\n        value = getattr(module, self._param_name)\n        result = super().__call__(value)\n        setattr(module, self._param_name, result)\n\n\nclass UpdateWeight(UpdateParameter):\n    def __init__(self, op):\n        super().__init__(""weight"", op)\n'"
pytorch_toolkit/nncf/nncf/nncf_logger.py,0,"b'import logging\nimport sys\n\nNNCF_LOGGER_NAME = ""nncf""\n\nlogger = logging.getLogger(NNCF_LOGGER_NAME)\n_LOGGER_INITIALIZED = False\n\nif not _LOGGER_INITIALIZED:\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    fmt = logging.Formatter(""%(levelname)s:%(name)s:%(message)s"")\n    stdout_handler.setFormatter(fmt)\n    stdout_handler.setLevel(logging.INFO)\n    logger.addHandler(stdout_handler)\n    logger.setLevel(logging.INFO)\n\n\ndef set_log_level(level):\n    logger.setLevel(level)\n    for handler in logger.handlers:\n        handler.setLevel(level)\n\n\ndef disable_logging():\n    logger.handlers = []\n'"
pytorch_toolkit/nncf/nncf/nncf_network.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\nfrom collections import OrderedDict\nfrom enum import Enum\nfrom typing import List, Callable, Tuple, Dict, Optional\n\nimport functools\nimport networkx as nx\nimport torch\nfrom copy import deepcopy\nfrom torch import nn\n\nfrom nncf.debug import CombinedDebugInterface, debuggable_forward, is_debug\nfrom nncf.dynamic_graph.context import TracingContext\nfrom nncf.dynamic_graph.graph import NNCFGraph, InputAgnosticOperationExecutionContext, OperationExecutionContext\nfrom nncf.dynamic_graph.graph import ShapeIgnoringTensorMetaComparator\nfrom nncf.dynamic_graph.graph_builder import GraphBuilder, PostGraphBuildActing, create_dummy_forward_fn, ModelInputInfo\nfrom nncf.dynamic_graph.graph_matching import NodeExpression\nfrom nncf.dynamic_graph.patch_pytorch import ignore_scope, nncf_model_input, MODEL_INPUT_OP_NAME\nfrom nncf.dynamic_graph.operator_metatypes import OPERATOR_METATYPES\nfrom nncf.dynamic_graph.transform_graph import replace_modules_by_nncf_modules\nfrom nncf.hw_config import HWConfig\nfrom nncf.layers import NNCF_MODULES\nfrom nncf.quantization.layers import QUANTIZATION_MODULES\nfrom nncf.utils import get_all_modules_by_type, get_state_dict_names_with_modules\nfrom nncf.nncf_logger import logger as nncf_logger\n\nMODEL_WRAPPED_BY_NNCF_ATTR_NAME = \'nncf_module\'\n\n\nclass CompressionModuleType(Enum):\n    FUNCTION_QUANTIZER = 0\n    ACTIVATION_QUANTIZER = 1\n\n\n@functools.total_ordering\nclass OperationPriority(Enum):\n    DEFAULT_PRIORITY = 0\n    SPARSIFICATION_PRIORITY = 2\n    QUANTIZATION_PRIORITY = 11\n    PRUNING_PRIORITY = 1\n\n    def __lt__(self, other):\n        # pylint: disable=comparison-with-callable\n        return self.value < other.value\n\n\nclass InsertionType(Enum):\n    OPERATOR_PRE_HOOK = 0\n    OPERATOR_POST_HOOK = 1\n    NNCF_MODULE_PRE_OP = 2\n    NNCF_MODULE_POST_OP = 3\n\n    def __eq__(self, other):\n        # pylint: disable=comparison-with-callable\n        if isinstance(other, InsertionType):\n            return self.value == other.value\n        return self.value == other\n\n\nclass InsertionInfo:\n    def __init__(self, op_exec_context: OperationExecutionContext,\n                 is_input=False,\n                 is_output=False,\n                 shape_to_operate_on=None):\n        self.op_exec_context = op_exec_context  # type: OperationExecutionContext\n        self.is_input = is_input\n        self.is_output = is_output\n        self.shape_to_operate_on = shape_to_operate_on\n\n    def __eq__(self, other: \'InsertionInfo\'):\n        return self.op_exec_context == other.op_exec_context\n\n    def __hash__(self):\n        return self.op_exec_context.__hash__()\n\n\nclass InsertionPoint:\n    def __init__(self, ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                 insertion_type: InsertionType):\n        self.ia_op_exec_context = ia_op_exec_context\n        self.insertion_type = insertion_type\n\n    def __eq__(self, other: \'InsertionPoint\'):\n        return self.insertion_type == other.insertion_type and self.ia_op_exec_context == other.ia_op_exec_context\n\n    def __str__(self):\n        return str(self.insertion_type) + "" "" + str(self.ia_op_exec_context)\n\n    def __hash__(self):\n        return hash(str(self))\n\n\nclass InsertionCommand:\n    def __init__(self, point: InsertionPoint, fn: Callable,\n                 priority: OperationPriority = OperationPriority.DEFAULT_PRIORITY):\n        self.insertion_point = point  # type: InsertionPoint\n        self.fn = fn  # type: Callable\n        self.priority = priority  # type: OperationPriority\n\n\nclass LoadStateListener:\n    """"""\n        Resets the initialization flags (`initialized`) for all quantization modules on `load_state_dict` call.\n        These flags are used to update not loaded params (from checkpoint or model\'s state)\n        on initialization stage of algorithm.\n        Flags reset is required on each call of `load_state_dict`, because internal method (`build_graph`)\n        restores model state by calling this method.\n    """"""\n\n    def __init__(self, model, all_quantizations):\n        for prefix, module in all_quantizations.items():\n            module.state_dict_name = prefix\n        # pylint: disable=protected-access\n        self.hook = model._register_load_state_dict_pre_hook(\n            functools.partial(self.hook_fn, quantize_modules=all_quantizations.values()))\n\n    def hook_fn(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs,\n                quantize_modules):\n        for module in quantize_modules:\n            module.initialized = False\n\n    def close(self):\n        self.hook.remove()\n\n\nclass InsertionPointGraphNodeType(Enum):\n    INSERTION_POINT = 0\n    OPERATOR = 1\n\n\nclass InsertionPointGraph(nx.DiGraph):\n    """"""\n    This graph is built from the NNCFGraph representation of the model control flow graph and adds ephemeral\n    ""insertion point nodes"" into the NNCF model graph representation corresponding to operator pre- and\n    post-hooks. Module pre-op and post-op insertion points are currently not reflected here, but they are\n    probably not required for quantizing activations, for which the quantizer propagation makes sense.\n    This ""insertion point graph"" representation is useful for quantizer propagation and for referencing\n    the compression algorithm hooks to the model operations to which they are applied to.\n    """"""\n    NODE_TYPE_NODE_ATTR = ""node_type""\n    INSERTION_POINT_DATA_NODE_ATTR = ""insertion_point_data""\n    IS_IN_NNCF_MODULE_NODE_ATTR = ""is_in_nncf_module""\n    REGULAR_NODE_REF_NODE_ATTR = ""regular_node_ref""\n    ASSOCIATED_IP_NODE_KEYS_NODE_ATTR = ""associated_ip_node_keys""\n    OPERATOR_METATYPE_NODE_ATTR = ""op_meta""\n\n    PRE_HOOK_ID_PREFIX = ""PRE HOOK ""  # NB: Do not use colon (\':\') in node keys! Causes trouble for .dot file export.\n    POST_HOOK_ID_PREFIX = ""POST HOOK ""\n\n    def __init__(self, model_nx_graph: nx.DiGraph):\n        super().__init__()\n        self._base_nx_graph = deepcopy(model_nx_graph)\n\n        for node_key, node in self._base_nx_graph.nodes.items():\n            attrs = {InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR: node,\n                     InsertionPointGraph.NODE_TYPE_NODE_ATTR: InsertionPointGraphNodeType.OPERATOR,\n                     InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR: set(),\n                     InsertionPointGraph.OPERATOR_METATYPE_NODE_ATTR: None}\n            self.add_node(node_key, **attrs)\n        for from_node, to_node in self._base_nx_graph.edges:\n            self.add_edge(from_node, to_node)\n\n        # TODO: Add insertion points for module pre- and post-ops.\n        # Should roughly look so: first, determine subsets of nodes belonging to each\n        # separate NNCF module (via scope analysis), then for each subset find input/output\n        # edges using a corresponding NNCFGraph function; add a pre-op insertion point node as the\n        # sink for input edges and connect it to input edge destinations, then add a post-op\n        # insertion point as the source of output edges and connect it to output edge origins.\n\n        node_keys_working_set = [deepcopy(node_key) for node_key in self.nodes.keys()]\n        for operator_node_key in node_keys_working_set:\n            original_node = self.nodes[operator_node_key][InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR]\n            ia_op_exec_context = original_node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR].input_agnostic\n\n            # Pre-hook insertion point nodes\n            pre_hook_insertion_point = InsertionPoint(ia_op_exec_context,\n                                                      InsertionType.OPERATOR_PRE_HOOK)\n            attrs = {\n                InsertionPointGraph.NODE_TYPE_NODE_ATTR: InsertionPointGraphNodeType.INSERTION_POINT,\n                InsertionPointGraph.INSERTION_POINT_DATA_NODE_ATTR: pre_hook_insertion_point,\n            }\n            ip_node_key = self.get_pre_hook_node_key(str(operator_node_key))\n            self.add_node(ip_node_key, **attrs)\n            in_edges = list(self.in_edges(operator_node_key))\n            for from_node_key, to_node_key in in_edges:\n                self.remove_edge(from_node_key, to_node_key)\n                self.add_edge(from_node_key, ip_node_key)\n            self.add_edge(ip_node_key, operator_node_key)\n            operator_node = self.nodes[operator_node_key]\n            operator_node[InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR].add(ip_node_key)\n\n            # Post-hook insertion point nodes\n            post_hook_insertion_point = InsertionPoint(ia_op_exec_context,\n                                                       InsertionType.OPERATOR_POST_HOOK)\n            attrs = {\n                InsertionPointGraph.NODE_TYPE_NODE_ATTR: InsertionPointGraphNodeType.INSERTION_POINT,\n                InsertionPointGraph.INSERTION_POINT_DATA_NODE_ATTR: post_hook_insertion_point\n            }\n            ip_node_key = self.get_post_hook_node_key(str(operator_node_key))\n            self.add_node(ip_node_key, **attrs)\n            out_edges = list(self.out_edges(operator_node_key))\n            for from_node_key, to_node_key in out_edges:\n                self.remove_edge(from_node_key, to_node_key)\n                self.add_edge(ip_node_key, to_node_key)\n                # TODO: introduce separate insertion points for operator outputs if\n                # the outputs are semantically different\n            self.add_edge(operator_node_key, ip_node_key)\n            operator_node = self.nodes[operator_node_key]\n            operator_node[InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR].add(ip_node_key)\n\n    def get_ip_graph_with_merged_hw_optimized_operations(self,\n                                                         hw_config: Optional[HWConfig] = None) -> \'InsertionPointGraph\':\n        merged_ip_graph = deepcopy(self)\n        pattern = self._get_mergeable_operator_patterns(hw_config)\n        from nncf.dynamic_graph.graph_matching import search_all\n        matches = search_all(self._base_nx_graph, pattern)\n        for match in matches:\n            if len(match) == 1:\n                continue\n\n            input_node_key = match[0]\n            output_node_key = match[-1]\n            in_edges = list(self.in_edges(input_node_key))\n            out_edges = list(self.out_edges(output_node_key))\n\n            assert len(in_edges) <= 1  # TODO: change to == 1 when input nodes are handled correctly\n\n            if in_edges:\n                in_edge_key = in_edges[0]\n                in_edge_copy = deepcopy(self.edges[in_edge_key])\n            out_edge_copies_dict = {}\n            for out_edge_key in out_edges:\n                out_edge_copies_dict[out_edge_key] = deepcopy(self.edges[out_edge_key])\n\n            conserved_edges_list = out_edges\n            if in_edges:\n                conserved_edges_list.append(in_edge_key)\n\n            merged_node_attrs = deepcopy(self.nodes[input_node_key])\n            merged_node_attrs[InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR] = set()\n            merged_node_key = """"\n            for node_key in match:\n                ip_node_keys = self.nodes[node_key][InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR]\n                for ip_node_key in ip_node_keys:\n                    should_keep_ip_node = False\n                    for edge_key in conserved_edges_list:\n                        if ip_node_key in edge_key:\n                            should_keep_ip_node = True\n                            break\n                    if should_keep_ip_node:\n                        merged_node_attrs[InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR].add(ip_node_key)\n                    else:\n                        merged_ip_graph.remove_node(ip_node_key)\n                merged_ip_graph.remove_node(node_key)\n                merged_node_key += node_key + \'\\n\'\n\n            merged_ip_graph.add_node(merged_node_key, **merged_node_attrs)\n            if in_edges:\n                merged_ip_graph.add_edge(in_edge_key[0], merged_node_key, **in_edge_copy)\n            for out_edge_key, out_edge_attrs in out_edge_copies_dict.items():\n                merged_ip_graph.add_edge(merged_node_key, out_edge_key[1], **out_edge_attrs)\n\n        return merged_ip_graph\n\n    @staticmethod\n    def get_pre_hook_node_key(node_key: str):\n        return InsertionPointGraph.PRE_HOOK_ID_PREFIX + node_key\n\n    @staticmethod\n    def get_post_hook_node_key(node_key: str):\n        return InsertionPointGraph.POST_HOOK_ID_PREFIX + node_key\n\n    def _get_mergeable_operator_patterns(self, hw_config: Optional[HWConfig] = None) -> NodeExpression:\n        """"""Resulting pattern should have single input; the operation with inputs to\n        quantize should be the input operation; outputs should only be produced by one output node.""""""\n        # TODO: Implement ""repeating expressions"" so that any number of ""mergeable"" operations\n        # immediately following a linear/convolutional/matrix op are merged into one block\n        import nncf.dynamic_graph.patterns as p\n        pattern = p.LINEAR_OPS + p.ANY_BN_RELU_COMBO | p.LINEAR_OPS + p.ELTWISE_UNIFORM_OPS\n        return pattern\n\n    def get_op_nodes_in_scope(self, scope: \'Scope\') -> List:\n        matching_ip_graph_op_nodes_list = []\n        for node in self.nodes().values():\n            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:\n                nncf_graph_node_ref = node[InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR]\n                op_exec_context = nncf_graph_node_ref[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n                op_scope = op_exec_context.input_agnostic.scope_in_model\n                if op_scope in scope:\n                    matching_ip_graph_op_nodes_list.append(node)\n        return matching_ip_graph_op_nodes_list\n\n# pylint: disable=too-many-public-methods\n@ignore_scope\nclass NNCFNetwork(nn.Module, PostGraphBuildActing):\n\n    def __init__(self, module, input_infos: List[ModelInputInfo] = None,\n                 dummy_forward_fn=None, scopes_without_shape_matching=None,\n                 ignored_scopes=None, target_scopes=None):\n        super().__init__()\n        self.set_nncf_wrapped_model(module)\n        self.input_infos = input_infos\n        self.ignored_scopes = ignored_scopes\n        self.target_scopes = target_scopes\n        self._dummy_forward_fn = dummy_forward_fn\n        self._nncf_module_scopes = []  # type: List[Scope]\n        self.scopes_without_shape_matching = scopes_without_shape_matching\n        self.debug_interface = CombinedDebugInterface() if is_debug() else None\n        self._extra_module_types = []  # type: List[CompressionModuleType]\n        # pylint:disable=line-too-long\n        self._insertions_into_original_graph = {}  # type: Dict[InsertionPoint, List[Tuple[Callable, OperationPriority]]]\n\n        device = next(module.parameters()).device\n\n        # all modules should be replaced prior to graph building\n        self._replace_modules_by_nncf_modules(device)\n\n        _orig_context = TracingContext()\n        _orig_graph_build_forward_fn = self._get_dummy_forward_fn_for_graph_building(with_input_tracing=True)\n\n        self._graph_builder = GraphBuilder(_orig_graph_build_forward_fn)\n\n        _orig_context.add_node_comparators([MODEL_INPUT_OP_NAME], ShapeIgnoringTensorMetaComparator())\n        if self.scopes_without_shape_matching:\n            _orig_context.add_node_comparators(scopes_without_shape_matching,\n                                               ShapeIgnoringTensorMetaComparator())\n\n        self._original_graph = self._graph_builder.build_graph(self.get_nncf_wrapped_model(), _orig_context)\n\n        self._compressed_context = TracingContext()\n\n        self._dummy_forward_fn = self._get_dummy_forward_fn_for_graph_building(with_input_tracing=False)\n\n        self._compressed_context.add_node_comparators([MODEL_INPUT_OP_NAME], ShapeIgnoringTensorMetaComparator())\n        if self.scopes_without_shape_matching:\n            self._compressed_context.add_node_comparators(scopes_without_shape_matching,\n                                                          ShapeIgnoringTensorMetaComparator())\n        self._load_listener = None\n\n        self._builders = []  # type: List[\'CompressionAlgorithmBuilder\']\n\n    @debuggable_forward\n    def forward(self, *args, **kwargs):\n        with self._compressed_context as ctx:  # type: TracingContext\n            ctx.base_module_thread_local_replica = self\n            arglist = list(args)\n            for idx, tensor in enumerate(arglist):  # TODO: extend to all tensors in args/kwargs hierarchy\n                if isinstance(tensor, torch.Tensor):\n                    arglist[idx] = nncf_model_input(tensor)\n            args = tuple(arglist)\n            retval = self.get_nncf_wrapped_model()(*args, **kwargs)\n        return retval\n\n    def register_algorithm(self, builder: \'CompressionAlgorithmBuilder\'):\n        """"""Should be called during *builder*\'s *apply_to* method, otherwise there will be no corresponding\n        controller returned by the network on the *commit_compression_changes* stage""""""\n        self._builders.append(builder)\n\n    # Cannnot use property syntax here, otherwise the wrapped module will end up\n    # being twice in the same checkpoint with different prefixes\n    def get_nncf_wrapped_model(self):\n        return getattr(self, MODEL_WRAPPED_BY_NNCF_ATTR_NAME)\n\n    def set_nncf_wrapped_model(self, value):\n        setattr(self, MODEL_WRAPPED_BY_NNCF_ATTR_NAME, value)\n\n    def get_modules_in_nncf_modules_by_type(self, types) -> Dict[\'Scope\', nn.Module]:\n        nncf_modules = self.get_nncf_modules()\n        retval = {}\n        for nncf_module_scope, nncf_module in nncf_modules.items():\n            nncf_module_scope.pop()\n            for relative_scope, target_module in get_all_modules_by_type(nncf_module, types).items():\n                retval[nncf_module_scope + relative_scope] = target_module\n        return retval\n\n    def register_insertion_command(self, command: InsertionCommand):\n        point = command.insertion_point\n        if point not in self._insertions_into_original_graph:\n            self._insertions_into_original_graph[point] = [(command.fn, command.priority)]\n        else:\n            self._insertions_into_original_graph[point].append((command.fn, command.priority))\n\n    def commit_compression_changes(self) -> \'CompressionAlgorithmController\':\n        for insertion_point, fn_list_with_priority in self._insertions_into_original_graph.items():\n            fn_list_with_priority = sorted(fn_list_with_priority, key=lambda x: x[1])\n            self._insertions_into_original_graph[insertion_point] = fn_list_with_priority\n            self._insert_at_point(insertion_point, [x[0] for x in fn_list_with_priority])\n\n        if self.debug_interface is not None:\n            self.debug_interface.init_actual(self)\n\n        quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]\n        all_quantizations = get_state_dict_names_with_modules(self, quantization_types)\n        self._load_listener = LoadStateListener(self, all_quantizations)\n\n        if not self._builders:\n            from nncf.algo_selector import NoCompressionAlgorithmController\n            return NoCompressionAlgorithmController(self)\n\n        if len(self._builders) == 1:\n            return self._builders[0].build_controller(self)\n\n        from nncf.composite_compression import CompositeCompressionAlgorithmController\n        composite_controller = CompositeCompressionAlgorithmController(self)\n        for algo_builder in self._builders:\n            composite_controller.add(algo_builder.build_controller(self))\n        return composite_controller\n\n    def _insert_at_point(self, point: InsertionPoint, fn_list: List[Callable]):\n        if point.insertion_type == InsertionType.OPERATOR_PRE_HOOK:\n            self._compressed_context.register_pre_hooks(fn_list, point.ia_op_exec_context)\n        elif point.insertion_type == InsertionType.OPERATOR_POST_HOOK:\n            self._compressed_context.register_post_hooks(fn_list, point.ia_op_exec_context)\n        else:\n            norm_target_scope = self._normalize_variable_recurrent_scope(point.ia_op_exec_context.scope_in_model)\n            norm_nncf_scopes = [self._normalize_variable_recurrent_scope(x) for x in self._nncf_module_scopes]\n            assert norm_target_scope in norm_nncf_scopes  # Required for proper Recurrent/VariableRecurrent addressing\n            nncf_module = self.get_module_by_scope(point.ia_op_exec_context.scope_in_model)\n            if point.insertion_type == InsertionType.NNCF_MODULE_PRE_OP:\n                for fn in fn_list:\n                    nncf_module.register_pre_forward_operation(fn)\n            elif point.insertion_type == InsertionType.NNCF_MODULE_POST_OP:\n                for fn in fn_list:\n                    nncf_module.register_post_forward_operation(fn)\n\n    def __getattr__(self, name):\n        wrapped_module = super().__getattr__(MODEL_WRAPPED_BY_NNCF_ATTR_NAME)\n        if hasattr(wrapped_module, name):\n            return getattr(wrapped_module, name)\n        return super().__getattr__(name)\n\n    def get_graph(self) -> NNCFGraph:\n        return self._compressed_context.graph\n\n    def get_original_graph(self) -> NNCFGraph:\n        return self._original_graph\n\n    def get_tracing_context(self) -> TracingContext:\n        return self._compressed_context\n\n    def _get_dummy_forward_fn_for_graph_building(self, with_input_tracing):\n        if self._dummy_forward_fn is None:\n            return create_dummy_forward_fn(self.input_infos,\n                                           with_input_tracing=with_input_tracing)\n        return self._dummy_forward_fn\n\n    def _replace_modules_by_nncf_modules(self, device):\n        module, self._nncf_module_scopes = replace_modules_by_nncf_modules(self.get_nncf_wrapped_model(),\n                                                                           ignored_scopes=self.ignored_scopes,\n                                                                           target_scopes=self.target_scopes)\n        self.set_nncf_wrapped_model(module.to(device))\n\n    def get_nncf_module_scopes(self) -> List[\'Scope\']:\n        return self._nncf_module_scopes\n\n    def get_nncf_modules(self) -> Dict[\'Scope\', torch.nn.Module]:\n        return get_all_modules_by_type(self.get_nncf_wrapped_model(), NNCF_MODULES)\n\n    def rebuild_graph(self, *input_args):\n        self._compressed_context.reset_graph()\n        dummy_forward_fn = self._get_dummy_forward_fn_for_graph_building(with_input_tracing=False)\n        builder = GraphBuilder(dummy_forward_fn)\n        _ = builder.build_graph(self, self._compressed_context)\n\n    def post_build_graph_actions(self):\n        # Reset initialization flags (`initialized`) for all quantization modules\n        # after dummy `load_state_dict` call.\n        quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]\n        all_quantizations = get_state_dict_names_with_modules(self, quantization_types)\n        for module in all_quantizations.values():\n            module.initialized = False\n\n    def get_post_pattern_insertion_points(self, pattern: \'NNCFNodeExpression\',\n                                          omit_nodes_in_nncf_modules=False) -> List[InsertionInfo]:\n        io_infos = self._original_graph.get_matching_nncf_graph_pattern_io_list(pattern)\n\n        insertion_infos = []\n        for io_info in io_infos:\n            # The input/output is given in terms of edges, but the post-hooks are currently applied to\n            # nodes. Multiple output edges in a pattern I/O info may originate from one and the same\n            # node, and we have to ensure that these resolve into just one insertion point - thus the usage of ""set"".\n            pattern_insertion_info_set = set()\n            if len(io_info.output_edges) > 1:\n                nncf_logger.debug(""WARNING: pattern has more than one activation output"")\n\n            for nncf_node in io_info.output_nodes:\n                pattern_insertion_info_set.add(InsertionInfo(nncf_node.op_exec_context,\n                                                             is_output=True,\n                                                             shape_to_operate_on=None))\n                # TODO: determine output shapes for output nodes to enable per-channel quantization\n\n            # Ignore input nodes in the pattern for now, rely on the _quantize_inputs functions.\n            # TODO: handle input quantization here as well\n\n            # Since this function is currently only used for activation quantization purposes via operator\n            # post-hook mechanism, we may take any edge and it will point from the same node where we will have to\n            # insert a quantizer later. However, in the future the output edges may refer to activation tensors\n            # with different sizes, in which case we have to insert different per-channel quantizers to\n            # accomodate different trainable params if there is a difference in the channel dimension.\n            # Furthermore, currently there is no distinction for single tensor output to multiple nodes and\n            # multiple tensor output to multiple nodes (""chunk"" operation is an example of the latter).\n            # The pattern may also have unexpected outputs from a node in the middle of the pattern (see\n            # ""densenet121.dot"" for an example of this) - need to decide what to do with that in terms\n            # of quantization.\n            # TODO: address the issues above.\n\n            for nncf_edge in io_info.output_edges:\n                pattern_insertion_info_set.add(InsertionInfo(nncf_edge.from_node.op_exec_context,\n                                                             is_output=False,\n                                                             shape_to_operate_on=nncf_edge.tensor_shape))\n            insertion_infos += list(pattern_insertion_info_set)\n\n        insertion_infos = list(\n            set(insertion_infos))  # Filter the overlapping insertion points from different matches (happens for GNMT)\n        insertion_infos_filtered = []\n\n        for info in insertion_infos:\n            if omit_nodes_in_nncf_modules and self.is_scope_in_nncf_module_scope(info.op_exec_context.scope_in_model):\n                continue\n            insertion_infos_filtered.append(info)\n\n        return insertion_infos_filtered\n\n    def is_scope_in_nncf_module_scope(self, scope: \'Scope\'):\n        # TODO: optimize\n        norm_nncf_scopes = [self._normalize_variable_recurrent_scope(x) for x in self._nncf_module_scopes]\n        norm_op_scope = self._normalize_variable_recurrent_scope(scope)\n        for nncf_scope in norm_nncf_scopes:\n            if norm_op_scope in nncf_scope:\n                return True\n        return False\n\n    def register_compression_module_type(self, compression_module_type: CompressionModuleType):\n        attr_name = self._compression_module_type_to_attr_name(compression_module_type)\n        if compression_module_type in self._extra_module_types:\n            raise RuntimeError(""Module type {} is already registered"".format(compression_module_type))\n        self.__setattr__(attr_name, nn.ModuleDict())\n        self._extra_module_types.append(compression_module_type)\n\n    def add_compression_module(self, module_key: str, module: nn.Module,\n                               compression_module_type: CompressionModuleType):\n        attr_name = self._compression_module_type_to_attr_name(compression_module_type)\n        if compression_module_type not in self._extra_module_types:\n            raise RuntimeError(""Module type {} was not registered"".format(compression_module_type))\n        self.__getattr__(attr_name)[module_key] = module\n\n    def get_compression_modules_by_type(self, compression_module_type: CompressionModuleType) -> nn.ModuleDict:\n        attr_name = self._compression_module_type_to_attr_name(compression_module_type)\n        if compression_module_type not in self._extra_module_types:\n            raise RuntimeError(""Module type {} was not registered"".format(compression_module_type))\n        return self.__getattr__(attr_name)\n\n    @staticmethod\n    def _compression_module_type_to_attr_name(compression_module_type: CompressionModuleType):\n        """"""Required for backward compatibility with checkpoints that store function and activation\n        quantizers directly under corresponding attributes of NNCFNetwork.""""""\n        if compression_module_type == CompressionModuleType.FUNCTION_QUANTIZER:\n            return ""function_quantizers""\n        if compression_module_type == CompressionModuleType.ACTIVATION_QUANTIZER:\n            return ""activation_quantizers""\n        raise RuntimeError(""Unknown extra module type"")\n\n    def sort_compression_modules(self, compression_module_type: CompressionModuleType):\n        attr_name = self._compression_module_type_to_attr_name(compression_module_type)\n        if compression_module_type not in self._extra_module_types:\n            raise RuntimeError(""Module type {} was not registered"".format(compression_module_type))\n        module_dict = self.__getattr__(attr_name)\n        # pylint: disable=protected-access\n        module_dict._modules = OrderedDict(sorted(module_dict._modules.items()))\n        self.__setattr__(attr_name, module_dict)\n\n    @staticmethod\n    def _normalize_variable_recurrent_scope(scope: \'Scope\'):\n        """"""\n        Two scopes pointing to an NNCF module that only differ in a Recurrent/VariableRecurrent/VariableRecurrentReverse\n        scope element actually point to one and the same module.\n        """"""\n        ret_scope = scope.copy()\n        for scope_element in ret_scope:\n            if scope_element.calling_module_class_name in [""Recurrent"", ""VariableRecurrent"",\n                                                           ""VariableRecurrentReverse""]:\n                scope_element.calling_module_class_name = ""NormalizedName_Recurrent""\n        return ret_scope\n\n    def do_dummy_forward(self, force_eval=False):\n        """"""Attention: If run with force_eval=False, this may spoil the batchnorm statistics,\n        and an eval run of the model will perform much worse than the train run. """"""\n        if force_eval:\n            train_mode = self.training\n            self.eval()\n        with torch.no_grad():\n            self._dummy_forward_fn(self)\n        if force_eval:\n            if train_mode:\n                self.train()\n\n    def get_insertion_point_graph(self) -> InsertionPointGraph:\n        ip_graph = InsertionPointGraph(self._original_graph.get_nx_graph_copy())\n\n        # Mark IP graph operator nodes with associated op metatypes\n        # Determining operator metatypes is more suited to occur at wrap_operator\n        # stage, because it might be influenced by specific non-tensor function paramters,\n        # but we have to inspect the containing module parameters as well, so the\n        # TracingContext in wrap_operator would have to retain a reference to\n        # the model that uses it. Since currently we do not need to inspect the\n        # function arguments to determine the metatype, we can do this here, but\n        # once we need to inspect the arguments, the code will have to be moved to\n        # wrap_operator.\n\n        for node_key in ip_graph.nodes:\n            ip_graph_node = ip_graph.nodes[node_key]\n            ip_graph_node_type = ip_graph_node[InsertionPointGraph.NODE_TYPE_NODE_ATTR]\n            if ip_graph_node_type == InsertionPointGraphNodeType.OPERATOR:\n                nncf_graph_node_ref = ip_graph_node[InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR]\n                op_exec_context = nncf_graph_node_ref[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n                op_name = op_exec_context.operator_name\n                scope = op_exec_context.scope_in_model\n                op_arch = OPERATOR_METATYPES.get_operator_metatype_by_op_name(op_name)\n                module = self.get_module_by_scope(scope)\n                if module is not None:\n                    subtype = op_arch.determine_subtype(containing_module=module)\n                    if subtype is not None:\n                        op_arch = subtype\n                ip_graph_node[InsertionPointGraph.OPERATOR_METATYPE_NODE_ATTR] = op_arch\n        return ip_graph\n\n    def get_module_by_scope(self, scope: \'Scope\') -> torch.nn.Module:\n        curr_module = self.get_nncf_wrapped_model()\n        for scope_element in scope[1:]:  # omit first scope element which corresponds to base module\n            if scope_element.calling_field_name is None:\n                # The module used is being created in-place every time and never stored in the model,\n                # happens for nn.Softmax in BERT implementations.\n                return None\n            # pylint: disable=protected-access\n            next_module = curr_module._modules.get(scope_element.calling_field_name)\n            if next_module is None:\n                raise RuntimeError(""Could not find a {} module member in {} module of scope {} during node search""\n                                   .format(scope_element.calling_field_name,\n                                           scope_element.calling_module_class_name,\n                                           str(scope)))\n            curr_module = next_module\n        return curr_module\n'"
pytorch_toolkit/nncf/nncf/registry.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\nclass Registry:\n    def __init__(self, name):\n        self._name = name\n        self._registry_dict = dict()\n\n    def _register(self, obj, name):\n        if name in self._registry_dict:\n            raise KeyError(\'{} is already registered in {}\'.format(name, self._name))\n        self._registry_dict[name] = obj\n\n    def register(self, name=None):\n        def wrap(obj):\n            cls_name = name\n            if cls_name is None:\n                cls_name = obj.__name__\n            self._register(obj, cls_name)\n            return obj\n\n        return wrap\n\n    def get(self, name):\n        if name not in self._registry_dict:\n            self._key_not_found(name)\n        return self._registry_dict[name]\n\n    def _key_not_found(self, name):\n        raise KeyError(""{} is unknown type of {} "".format(name, self._name))\n\n    @property\n    def registry_dict(self):\n        return self._registry_dict\n'"
pytorch_toolkit/nncf/nncf/utils.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport random\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\nimport re\nfrom typing import Dict, Callable, Any, Mapping, Sequence, Set\n\nimport numpy as np\nimport torch\nfrom torch import distributed as dist\nfrom torch.nn import Module\n\nfrom nncf.dynamic_graph.graph_builder import GraphBuilder, ModelInputInfo, create_dummy_forward_fn\n\n\ndef scopes_matched(scope_stack_0, scope_stack_1):\n    from nncf.layers import NNCF_MODULES_MAP\n    if len(scope_stack_1) > len(scope_stack_0):\n        return False\n\n    for name0, name1 in zip(scope_stack_0, scope_stack_1):\n        if name0 != name1:\n            _, m_cls0, m_name0 = parse_node_name(name0)\n            _, m_cls1, m_name1 = parse_node_name(name1)\n            if m_name0 != m_name1 or not m_cls0 in NNCF_MODULES_MAP or m_cls1 != NNCF_MODULES_MAP[m_cls0]:\n                scope = scope_stack_0[1:]\n                if scope:\n                    _, m_cls, _ = parse_node_name(scope[0])\n                    scope[0] = m_cls\n                    return scopes_matched(scope, scope_stack_1)\n                return False\n    return True\n\n\ndef in_scope_list(scope, scope_list):\n    if scope_list is None:\n        return False\n\n    checked_scope_stack = scope.split(\'/\')\n    for item in [scope_list] if isinstance(scope_list, str) else scope_list:\n        if ""{re}"" in item:\n            regex = item.replace(""{re}"", """")\n            if re.search(regex, scope):\n                return True\n        scope_stack = item.split(\'/\') if isinstance(item, str) else item\n        if scopes_matched(checked_scope_stack, scope_stack):\n            return True\n    return False\n\n\ndef parse_node_name(name):\n    slash_pos = -1\n    nbrackets = 0\n    for i, ch in enumerate(reversed(name)):\n        if ch == \']\':\n            nbrackets += 1\n        elif ch == \'[\':\n            nbrackets -= 1\n        elif ch == \'/\' and nbrackets == 0:\n            slash_pos = len(name) - i - 1\n            break\n\n    prefix = None if slash_pos < 0 else name[:slash_pos]\n\n    last_name = name[slash_pos + 1:]\n    open_bracket_pos = last_name.find(""["")\n    if open_bracket_pos < 0:\n        return prefix, last_name, None\n    return prefix, last_name[:open_bracket_pos], last_name[open_bracket_pos + 1:-1]\n\n\ndef get_node_name(module, module_name, prefix):\n    return ""{prefix}/{cls}[{name}]"".format(prefix=prefix, cls=module.__class__.__name__, name=module_name)\n\n\ndef get_all_node_names(model, input_sample_size, builder=None):\n    if not builder:\n        builder = GraphBuilder(create_dummy_forward_fn([ModelInputInfo(input_sample_size), ]))\n    graph = builder.build_graph(model)\n    return [node_name.split(\' \', 1)[1] for node_name in graph.get_all_node_keys()]\n\n\ndef get_all_modules(model, prefix=None):\n    found = OrderedDict()\n    if prefix is None:\n        prefix = model.__class__.__name__\n    for name, module in model.named_children():\n        full_node_name = get_node_name(module, name, prefix)\n        found[full_node_name] = module\n        sub_found = get_all_modules(module, prefix=full_node_name)\n        if sub_found:\n            found.update(sub_found)\n    return found\n\n\ndef get_all_modules_by_type(model, module_types, current_scope=None,\n                            ignored_scopes=None, target_scopes=None) -> Dict[\'Scope\', Module]:\n    if isinstance(module_types, str):\n        module_types = [module_types]\n    found = OrderedDict()\n    from nncf.dynamic_graph.context import Scope\n    from nncf.dynamic_graph.context import ScopeElement\n    if current_scope is None:\n        current_scope = Scope()\n        current_scope.push(ScopeElement(model.__class__.__name__))\n    for name, module in model.named_children():\n        child_scope_element = ScopeElement(module.__class__.__name__, name)\n        child_scope = current_scope.copy()\n        child_scope.push(child_scope_element)\n\n        if in_scope_list(str(child_scope), ignored_scopes):\n            continue\n\n        if target_scopes is None or in_scope_list(str(child_scope), target_scopes):\n            if module_types.count(str(type(module).__name__)) != 0:\n                found[child_scope] = module\n            sub_found = get_all_modules_by_type(module, module_types,\n                                                current_scope=child_scope,\n                                                ignored_scopes=ignored_scopes,\n                                                target_scopes=target_scopes)\n            if sub_found:\n                found.update(sub_found)\n    return found\n\n\ndef get_state_dict_names_with_modules(model, str_types=None, prefix=\'\'):\n    found = OrderedDict()\n    for name, module in model.named_children():\n        full_node_name = ""{}{}"".format(prefix, name)\n        if str_types is not None and type(module).__name__ in str_types:\n            found[full_node_name] = module\n        sub_found = get_state_dict_names_with_modules(module, str_types, prefix=full_node_name + \'.\')\n        if sub_found:\n            found.update(sub_found)\n    return found\n\n\ndef set_module_by_node_name(model, node_name, module_to_set, prefix=None):\n    if prefix is None:\n        prefix = model.__class__.__name__\n\n    for name, module in model.named_children():\n        full_node_name = get_node_name(module, name, prefix)\n        if full_node_name == node_name:\n            # pylint: disable=protected-access\n            model._modules[name] = module_to_set\n        set_module_by_node_name(module, node_name, module_to_set, full_node_name)\n\n\ndef get_module_by_node_name(model: torch.nn.Module, node_scope_str: str, prefix=None) -> torch.nn.Module:\n    if prefix is None:\n        prefix = model.__class__.__name__\n    for name, module in model.named_children():\n        full_node_name = get_node_name(module, name, prefix)\n        if full_node_name == node_scope_str:\n            return module\n        sub_result = get_module_by_node_name(module, node_scope_str, full_node_name)\n        if sub_result is not None:\n            return sub_result\n    return None\n\n\ndef apply_by_node_name(model, node_names, command=lambda x: x, prefix=None):\n    if prefix is None:\n        prefix = model.__class__.__name__\n    for name, module in model.named_children():\n        node_name = get_node_name(module, name, prefix)\n        if node_name in node_names:\n            command(module)\n        apply_by_node_name(module, node_names=node_names, command=command, prefix=node_name)\n\n\ndef manual_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n\n\ndef is_tracing_state():\n    # pylint: disable=protected-access\n    return torch._C._get_tracing_state()\n\n\n@contextmanager\ndef no_jit_trace():\n    # pylint: disable=protected-access\n    disable_tracing = torch.jit._disable_tracing()\n    disable_tracing.__enter__()\n    yield disable_tracing\n    disable_tracing.__exit__()\n\n\n\n\ndef sum_like(tensor_to_sum, ref_tensor):\n    """"""Warning: may modify tensor_to_sum""""""\n    if ref_tensor.size == 1:\n        return tensor_to_sum.sum()\n\n    for dim, size in enumerate(ref_tensor.shape):\n        if size == 1:\n            if isinstance(tensor_to_sum, np.ndarray):\n                tensor_to_sum = tensor_to_sum.sum(dim, keepdims=True)\n            else:\n                tensor_to_sum = tensor_to_sum.sum(dim, keepdim=True)\n    return tensor_to_sum\n\n\ndef get_per_channel_scale_shape(input_shape, is_weights):\n    scale_shape = [1 for _ in input_shape]\n    if is_weights:\n        scale_shape[0] = input_shape[0]  # Per weight channel scales\n    else:\n        scale_shape[1] = input_shape[1]  # Per activation channel scales\n\n    elements = 1\n    for i in scale_shape:\n        elements *= i\n    if elements == 1:\n        return 1\n\n    return scale_shape\n\n\ndef get_flat_tensor_contents_string(input_tensor):\n    retval = ""[""\n    for idx, el in enumerate(input_tensor.view(-1)):\n        if idx >= 10:\n            retval += ""... (first 10/{} elements shown only) "".format(len(input_tensor.view(-1)))\n            break\n        retval += ""{:.4f}, "".format(el.item())\n    retval += ""]""\n    return retval\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef safe_thread_call(main_call_fn, after_barrier_call_fn=None):\n    result = None\n    if is_dist_avail_and_initialized():\n        if is_main_process():\n            result = main_call_fn()\n        dist.barrier()\n        if not is_main_process():\n            result = after_barrier_call_fn() if after_barrier_call_fn else main_call_fn()\n    else:\n        result = main_call_fn()\n    return result\n\nstring_types = (str, bytes)\niteritems = lambda mapping: getattr(mapping, \'iteritems\', mapping.items)()\n\n\ndef objwalk(obj, unary_predicate: Callable[[Any], bool], apply_fn: Callable, memo=None):\n    if memo is None:\n        memo = set()\n\n    is_tuple = isinstance(obj, tuple)\n    if is_tuple:\n        obj = list(obj)\n\n    def maybe_get_iterator(obj):\n        it = None\n        if isinstance(obj, Mapping):\n            it = iteritems\n        elif isinstance(obj, (Sequence, Set)) and not isinstance(obj, string_types):\n            it = enumerate\n        return it\n\n    iterator = maybe_get_iterator(obj)\n\n    if iterator is not None:\n        if id(obj) not in memo:\n            memo.add(id(obj))\n            indices_to_apply_fn_to = set()\n            indices_vs_tuples_to_assign = {}  # type: Dict[Any, list]\n            for idx, value in iterator(obj):\n                next_level_it = maybe_get_iterator(value)\n                if next_level_it is None:\n                    if unary_predicate(value):\n                        indices_to_apply_fn_to.add(idx)\n                else:\n                    if isinstance(value, tuple):\n                        processed_tuple = objwalk(value, unary_predicate, apply_fn, memo)\n                        indices_vs_tuples_to_assign[idx] = processed_tuple\n                    else:\n                        objwalk(value, unary_predicate, apply_fn)\n            for idx in indices_to_apply_fn_to:\n                obj[idx] = apply_fn(obj[idx])\n            for idx, tpl in indices_vs_tuples_to_assign.items():\n                obj[idx] = tuple(tpl)\n\n            memo.remove(id(obj))\n    else:\n        if unary_predicate(obj):\n            return apply_fn(obj)\n\n    if is_tuple:\n        return tuple(obj)\n\n    return obj\n'"
pytorch_toolkit/nncf/nncf/version.py,0,"b'__version__ = ""1.3""\n'"
pytorch_toolkit/nncf/tests/__init__.py,0,b''
pytorch_toolkit/nncf/tests/conftest.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom pathlib import Path\n\nimport pytest\ntry:\n    import torch\n    from torch.backends import cudnn\n    from nncf.utils import manual_seed\nexcept ImportError:\n    torch = None\n\n\nTEST_ROOT = Path(__file__).parent.absolute()\nPROJECT_ROOT = TEST_ROOT.parent.absolute()\nEXAMPLES_DIR = PROJECT_ROOT / \'examples\'\n\n\n@pytest.fixture(scope=""function"", autouse=True)\ndef empty_cache():\n    yield\n    if torch:\n        torch.cuda.empty_cache()\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        ""--data"", type=str, default=None,\n        help=""Path to test datasets, e.g. CIFAR10 - for sanity tests or CIFAR100 - for weekly ones""\n    )\n    parser.addoption(\n        ""--torch-home"", type=str, default=None, help=""Path to cached test models, downloaded by torchvision""\n    )\n    parser.addoption(\n        ""--weekly-models"", type=str, default=None, help=""Path to models\' weights for weekly tests""\n    )\n    parser.addoption(\n        ""--sota-checkpoints-dir"", type=str, default=None, help=""Path to checkpoints directory for sota accuracy test""\n    )\n    parser.addoption(\n        ""--sota-data-dir"", type=str, default=None, help=""Path to datasets directory for sota accuracy test""\n    )\n    parser.addoption(\n        ""--imagenet"", action=""store_true"", default=False, help=""Enable tests with imagenet""\n    )\n    parser.addoption(\n        ""--test-install-type"", type=str, help=""Type of installation, use CPU or GPU for appropriate install""\n    )\n    parser.addoption(\n        ""--backward-compat-models"", type=str, default=None, help=""Path to NNCF-traned model checkpoints that are tested""\n                                                                 ""to be strictly loadable""\n    )\n\n\n@pytest.fixture(scope=""module"")\ndef dataset_dir(request):\n    return request.config.getoption(""--data"")\n\n\n@pytest.fixture(scope=""module"")\ndef enable_imagenet(request):\n    return request.config.getoption(""--imagenet"")\n\n\n@pytest.fixture(scope=""module"")\ndef weekly_models_path(request):\n    return request.config.getoption(""--weekly-models"")\n\n\n@pytest.fixture(scope=""module"")\ndef sota_checkpoints_dir(request):\n    return request.config.getoption(""--sota-checkpoints-dir"")\n\n\n@pytest.fixture(scope=""module"")\ndef sota_data_dir(request):\n    return request.config.getoption(""--sota-data-dir"")\n\n\n@pytest.fixture(scope=""module"")\ndef install_type(request):\n    return request.config.getoption(""--test-install-type"")\n\n\n@pytest.fixture(scope=""module"")\ndef backward_compat_models_path(request):\n    return request.config.getoption(""--backward-compat-models"")\n\n\n@pytest.fixture(autouse=True)\ndef torch_home_dir(request, monkeypatch):\n    torch_home = request.config.getoption(""--torch-home"")\n    if torch_home:\n        monkeypatch.setenv(\'TORCH_HOME\', torch_home)\n\n\n@pytest.fixture\ndef _seed():\n    manual_seed(0)\n    cudnn.deterministic = True\n    cudnn.benchmark = False\n'"
pytorch_toolkit/nncf/tests/install_checks.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n# Do not remove - these imports are for testing purposes.\n#pylint:disable=unused-import\nimport nncf\nfrom nncf import create_compressed_model\n\nimport sys\nimport torch\n\nif len(sys.argv) != 2:\n    raise RuntimeError(""Must be run with an execution type as argument (either \'cpu\' or \'cuda\')"")\nexecution_type = sys.argv[1]\n\ninput_low_tensor = torch.zeros([1])\ninput_tensor = torch.ones([1, 1, 1, 1])\ninput_high_tensor = torch.ones([1])\nscale_tensor = torch.ones([1])\nthreshold_tensor = torch.zeros([1, 1, 1, 1])\nlevels = 256\n\nif execution_type == ""cpu"":\n    from nncf.binarization.extensions import BinarizedFunctionsCPU\n    from nncf.quantization.extensions import QuantizedFunctionsCPU\n    output_tensor = QuantizedFunctionsCPU.Quantize_forward(input_tensor, input_low_tensor, input_high_tensor, levels)\n    output_tensor = BinarizedFunctionsCPU.ActivationBinarize_forward(output_tensor, scale_tensor, threshold_tensor)\n    output_tensor = BinarizedFunctionsCPU.WeightBinarize_forward(output_tensor, True)\nelif execution_type == ""cuda"":\n    input_tensor = input_tensor.cuda()\n    input_low_tensor = input_low_tensor.cuda()\n    input_high_tensor = input_high_tensor.cuda()\n    scale_tensor = scale_tensor.cuda()\n    threshold_tensor = threshold_tensor.cuda()\n    from nncf.binarization.extensions import BinarizedFunctionsCUDA\n    from nncf.quantization.extensions import QuantizedFunctionsCUDA\n    output_tensor = QuantizedFunctionsCUDA.Quantize_forward(input_tensor, input_low_tensor, input_high_tensor, levels)\n    output_tensor = BinarizedFunctionsCUDA.ActivationBinarize_forward(output_tensor, scale_tensor, threshold_tensor)\n    output_tensor = BinarizedFunctionsCUDA.WeightBinarize_forward(output_tensor, True)\nelse:\n    raise RuntimeError(""Invalid execution type!"")\n'"
pytorch_toolkit/nncf/tests/test_algo_common.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport itertools\nimport os\nfrom functools import partial\n\nimport pytest\nfrom torch import nn\nfrom torch.nn import DataParallel\n\nfrom nncf.checkpoint_loading import load_state\nfrom tests.quantization.test_algo_quantization import get_basic_quantization_config, get_basic_asym_quantization_config\nfrom tests.sparsity.magnitude.test_helpers import get_basic_magnitude_sparsity_config\nfrom tests.sparsity.rb.test_algo import get_basic_sparsity_config\nfrom tests.test_helpers import BasicConvTestModel, get_empty_config, create_compressed_model_and_algo_for_test\n\n\nclass BasicLinearTestModel(nn.Module):\n    def __init__(self, size=4):\n        super().__init__()\n        self.fc = nn.Linear(size, size)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\ndef get_const_sparsity_config():\n    config = get_empty_config()\n    config[\'compression\'] = {\'algorithm\': \'const_sparsity\'}\n    return config\n\n@pytest.mark.parametrize(\'config_provider\', (get_basic_quantization_config, get_basic_asym_quantization_config,\n                                             get_basic_sparsity_config,\n                                             get_basic_magnitude_sparsity_config, get_const_sparsity_config),\n                         ids=(\'SymQuantization\', \'AsymQuantization\', \'Sparsity\', \'MagnitudeSparsity\', \'ConstSparsity\'))\n@pytest.mark.parametrize(\'model_provider\', (BasicConvTestModel, BasicLinearTestModel),\n                         ids=(\'Conv2d\', \'Linear\'))\nclass TestCompressionAlgos:\n    def test_can_export_compressed_model(self, tmp_path, config_provider, model_provider):\n        test_path = str(tmp_path.joinpath(\'test.onnx\'))\n        model = model_provider()\n        config = config_provider()\n        _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n        compression_ctrl.export_model(test_path)\n        assert os.path.exists(test_path)\n\n\n\nQUANTIZATION = \'quantization\'\nSPARSITY_TYPES = [\'magnitude\', \'rb\', \'const\']\nSPARSITY_ALGOS = [\'_\'.join([type, \'sparsity\']) for type in SPARSITY_TYPES]  # 3S\n\nLOAD_ALGOS = list(itertools.product([QUANTIZATION], SPARSITY_ALGOS))  # Q + 3S\nLOAD_ALGOS += itertools.product(SPARSITY_ALGOS, [QUANTIZATION])  # 3S + Q\n\nSAVE_ALGOS = [[algo] for algo in SPARSITY_ALGOS]  # 3S\nSAVE_ALGOS += [[QUANTIZATION]]  # Q\nSAVE_ALGOS += LOAD_ALGOS  # Q , 3S, 3S + Q, Q+3S\n\nALGOS = list(itertools.product(SAVE_ALGOS, LOAD_ALGOS))\n\n\n@pytest.fixture(scope=\'module\', params=ALGOS,\n                ids=[\'__\'.join([\'save:\' + \'_\'.join(a[0]),\n                                \'load:\' + \'_\'.join(a[1])]) for a in ALGOS]\n                )\ndef _algos(request):\n    pair_algos = request.param\n    save_algos = pair_algos[0]\n    load_algos = pair_algos[1]\n    resume_ok = False\n    # resume expects the same list of algorithms\n    if save_algos == load_algos:\n        resume_ok = True\n\n    if len(save_algos) == len(load_algos):\n        for s, v in zip(save_algos, load_algos):\n            if s != v and (\'magnitude\' in s and \'const\' in v or \'const\' in s and \'magnitude\' in v):\n                resume_ok = True\n\n        # Priority mechanism ensures that algo permutations are irrelevant\n        if set(save_algos) == set(load_algos):\n            resume_ok = True\n        else:\n            saved_sparsity = filter(lambda x: x != QUANTIZATION, save_algos)\n            loaded_sparsity = filter(lambda x: x != QUANTIZATION, load_algos)\n\n            for s, v in zip(saved_sparsity, loaded_sparsity):\n                # resume works fine for magnitude <-> const combo, because they have similar parameters\n                if s != v and (\'magnitude\' in s and \'const\' in v or \'const\' in s and \'magnitude\' in v):\n                    resume_ok = True\n\n\n    return {\n        \'save_algos\': save_algos,\n        \'load_algos\': load_algos,\n        \'is_resume_ok\': resume_ok\n    }\n\n\nMODEL_WRAPPER = [""CPU"", ""GPU""]\nWRAPPERS = list(itertools.product(MODEL_WRAPPER, MODEL_WRAPPER))\n\n\n@pytest.fixture(scope=\'function\', params=WRAPPERS,\n                ids=[\'_\'.join([\'from:\' + w[0], \'to:\' + w[1]]) for w in WRAPPERS])\ndef _model_wrapper(request):\n    modes = request.param\n\n    def wrap_model(mode, model):\n        if mode == ""GPU"":\n            model = DataParallel(model, [0])\n        return model\n\n    return {\n        \'save_model\': partial(wrap_model, modes[0]),\n        \'resume_model\': partial(wrap_model, modes[1]),\n    }\n\n\n@pytest.mark.parametrize(\'is_resume\', (True, False), ids=[\'resume\', \'load_weights\'])\ndef test_load_state_interoperability(_algos, _model_wrapper, is_resume):\n    config_save = get_empty_config()\n    config_save[\'compression\'] = [{\'algorithm\': algo, \'params\': {}} for algo in _algos[\'save_algos\']]\n    compressed_model_save, _ = create_compressed_model_and_algo_for_test(BasicConvTestModel(), config_save)\n    model_save = _model_wrapper[\'save_model\'](compressed_model_save)\n    saved_model_state = model_save.state_dict()\n    ref_num_loaded = len(saved_model_state)\n\n    config_resume = get_empty_config()\n    config_resume[\'compression\'] = [{\'algorithm\': algo, \'params\': {}} for algo in _algos[\'load_algos\']]\n    compressed_model_resume, _ = create_compressed_model_and_algo_for_test(BasicConvTestModel(),\n                                                                           config_resume)\n    model_resume = _model_wrapper[\'resume_model\'](compressed_model_resume)\n\n    if not is_resume or (is_resume and _algos[\'is_resume_ok\']):\n        act_num_loaded = load_state(model_resume, saved_model_state, is_resume)\n\n        if (\'magnitude_sparsity\' in _algos[\'load_algos\'] or \'const_sparsity\' in _algos[\'load_algos\']) \\\n            and \'rb_sparsity\' in _algos[\'save_algos\']:\n            # no need to load _mask and _uniform\n            ref_num_loaded -= 2\n        assert act_num_loaded == ref_num_loaded\n    else:\n        with pytest.raises(RuntimeError):\n            load_state(model_resume, saved_model_state, is_resume)\n\n\nLIST_ALGOS = [None, QUANTIZATION]\nLIST_ALGOS += SPARSITY_ALGOS  # 3S\n\n\n@pytest.mark.parametrize(\'is_resume\', (True, False), ids=[\'resume\', \'load_weights\'])\n@pytest.mark.parametrize(\'algo\', tuple(LIST_ALGOS))\ndef test_ordinary_load(algo, _model_wrapper, is_resume):\n    config = get_empty_config()\n    if algo:\n        config[\'compression\'] = {\'algorithm\': algo, \'params\': {}}\n\n    compressed_model_save, _ = create_compressed_model_and_algo_for_test(BasicConvTestModel(), config)\n    model_save = _model_wrapper[\'save_model\'](compressed_model_save)\n\n    compressed_model_resume, _ = create_compressed_model_and_algo_for_test(BasicConvTestModel(), config)\n    model_resume = _model_wrapper[\'resume_model\'](compressed_model_resume)\n\n    num_loaded = load_state(model_resume, model_save.state_dict(), is_resume)\n\n    assert num_loaded == len(model_save.state_dict())\n'"
pytorch_toolkit/nncf/tests/test_backward_compat.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport pytest\nimport torch\n\nfrom examples.common.distributed import configure_distributed\nfrom examples.common.execution import ExecutionMode, prepare_model_for_execution, get_device\nfrom examples.common.model_loader import load_model\nfrom nncf.config import Config\nfrom nncf.checkpoint_loading import load_state\nfrom tests.conftest import TEST_ROOT\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test\nfrom tests.test_sanity_sample import Command, create_command_line\nfrom tests.test_compression_training import get_cli_dict_args, parse_best_acc1\n\nGLOBAL_CONFIG = {\n    TEST_ROOT.joinpath(""data"", ""configs"", ""squeezenet1_1_cifar10_rb_sparsity_int8.json""): [\n        {\n            \'checkpoint_name\': \'squeezenet1_1_custom_cifar10_rb_sparsity_int8_dp.pth\',\n            \'dataset\': ""cifar10"",\n            \'execution_mode\': ExecutionMode.GPU_DATAPARALLEL,\n        },\n        {\n            \'checkpoint_name\': \'squeezenet1_1_custom_cifar10_rb_sparsity_int8_ddp.pth\',\n            \'dataset\': ""cifar10"",\n            \'execution_mode\': ExecutionMode.MULTIPROCESSING_DISTRIBUTED,\n        },\n    ],\n}\n\nCONFIG_PARAMS = []\nfor config_path_, cases_list_ in GLOBAL_CONFIG.items():\n    for case_params_ in cases_list_:\n        CONFIG_PARAMS.append((config_path_, case_params_,))\n\n\n@pytest.fixture(scope=\'module\', params=CONFIG_PARAMS,\n                ids=[\'-\'.join([str(p[0]), p[1][\'execution_mode\']]) for p in CONFIG_PARAMS])\ndef _params(request, backward_compat_models_path):\n    if backward_compat_models_path is None:\n        pytest.skip(\'Path to models weights for backward compatibility testing is not set,\'\n                    \' use --backward-compat-models option.\')\n    config_path, case_params = request.param\n    checkpoint_path = str(os.path.join(backward_compat_models_path, case_params[\'checkpoint_name\']))\n    return {\n        \'nncf_config_path\': config_path,\n        \'checkpoint_path\': checkpoint_path,\n        \'execution_mode\': case_params[\'execution_mode\'],\n        \'dataset\': case_params[\'dataset\']\n    }\n\n\ndef test_model_can_be_loaded_with_resume(_params):\n    p = _params\n    config_path = p[\'nncf_config_path\']\n    checkpoint_path = p[\'checkpoint_path\']\n\n    config = Config.from_json(str(config_path))\n    config.execution_mode = p[\'execution_mode\']\n\n    config.current_gpu = 0\n    config.device = get_device(config)\n    config.distributed = config.execution_mode in (ExecutionMode.DISTRIBUTED, ExecutionMode.MULTIPROCESSING_DISTRIBUTED)\n    if config.distributed:\n        config.dist_url = ""tcp://127.0.0.1:9898""\n        config.dist_backend = ""nccl""\n        config.rank = 0\n        config.world_size = 1\n        configure_distributed(config)\n\n    model_name = config[\'model\']\n    model = load_model(model_name,\n                       pretrained=False,\n                       num_classes=config.get(\'num_classes\', 1000),\n                       model_params=config.get(\'model_params\'))\n\n    model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    model, _ = prepare_model_for_execution(model, config)\n\n    if config.distributed:\n        compression_ctrl.distributed()\n\n    checkpoint = torch.load(checkpoint_path, map_location=\'cpu\')\n    load_state(model, checkpoint[\'state_dict\'], is_resume=True)\n\n\ndef test_loaded_model_evals_according_to_saved_acc(_params, tmp_path):\n    p = _params\n    config_path = p[\'nncf_config_path\']\n    checkpoint_path = p[\'checkpoint_path\']\n\n    tmp_path = str(tmp_path)\n    args = {}\n    args[\'data\'] = tmp_path + \'/\' + p[\'dataset\']\n    args[\'dataset\'] = p[\'dataset\']\n    args[\'config\'] = str(config_path)\n    args[\'mode\'] = \'test\'\n    args[\'log-dir\'] = tmp_path\n    args[\'workers\'] = 4\n    args[\'seed\'] = 1\n    args[\'resume\'] = checkpoint_path\n\n    if p[\'execution_mode\'] == ExecutionMode.MULTIPROCESSING_DISTRIBUTED:\n        args[\'multiprocessing-distributed\'] = \'\'\n    else:\n        pytest.skip(""DataParallel eval takes too long for this test to be run during pre-commit"")\n\n    runner = Command(create_command_line(get_cli_dict_args(args), ""classification""))\n    res = runner.run()\n    assert res == 0\n\n    acc1 = parse_best_acc1(tmp_path)\n    assert torch.load(checkpoint_path)[\'best_acc1\'] == pytest.approx(acc1)\n'"
pytorch_toolkit/nncf/tests/test_compressed_graph.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport os\nfrom functools import partial\n\nimport networkx as nx\nimport pytest\nimport torch\nimport torch.nn as nn\nimport torchvision\n\nfrom nncf.dynamic_graph.patch_pytorch import nncf_model_input\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.algo_selector import create_compression_algorithm_builders\nfrom nncf.dynamic_graph.context import get_version_agnostic_name, TracingContext\nfrom nncf.dynamic_graph.graph import NNCFGraph\nfrom nncf.dynamic_graph.graph_builder import create_input_infos, GraphBuilder, create_dummy_forward_fn, ModelInputInfo\nfrom nncf.layers import LSTMCellNNCF, NNCF_RNN\nfrom nncf.utils import get_all_modules_by_type\nfrom tests import test_models\nfrom tests.modules.seq2seq.gnmt import GNMT\nfrom tests.modules.test_rnn import replace_lstm\nfrom tests.test_helpers import get_empty_config, create_compressed_model_and_algo_for_test\n\n\ndef get_basic_quantization_config(quantization_type, input_sample_size):\n    config = get_empty_config(input_sample_size=input_sample_size)\n    config[""compression""] = {""algorithm"": ""quantization"",\n                             ""activations"": {\n                                 ""mode"": quantization_type\n                             },\n                             ""weights"": {\n                                 ""mode"": quantization_type\n                             }}\n    return config\n\ndef get_version_agnostic_graph(nx_graph):\n    done = False\n    while not done:\n        counter = 0\n        for node_name, node_data in nx_graph.nodes().data():\n            version_specific_name = node_data[""type""]\n            version_agnostic_name = get_version_agnostic_name(version_specific_name)\n            if version_agnostic_name != version_specific_name:\n                node_data[""type""] = version_agnostic_name\n                mapping = dict(zip(nx_graph, nx_graph))  # identity mapping\n                new_node_name = node_name.replace(version_specific_name, version_agnostic_name)\n                mapping[node_name] = new_node_name\n                nx_graph = nx.relabel_nodes(nx_graph, mapping, copy=False)\n                break  # Looks like iterators will be invalidated after relabel_nodes\n            counter += 1\n        if counter == len(nx_graph.nodes().data()):\n            done = True\n\n    return nx_graph\n\n\ndef sort_dot(path):\n    with open(path, \'r\') as f:\n        content = f.readlines()\n    start_line = \'strict digraph  {\\n\'\n    end_line = \'}\\n\'\n    content.remove(start_line)\n    content.remove(end_line)\n\n    def graph_key(line, offset):\n        key = line.split(\' \')[0].replace(\'""\', \'\')\n        if \'->\' in line:\n            key += line.split(\' \')[3].replace(\'""\', \'\')\n            return int(key) + offset\n        return int(key)\n\n    sorted_content = sorted(content, key=partial(graph_key, offset=len(content)))\n    with open(path, \'w\') as f:\n        f.write(start_line)\n        f.writelines(sorted_content)\n        f.write(end_line)\n\n\ndef check_graph(graph: NNCFGraph, path_to_dot, graph_dir):\n    # pylint:disable=protected-access\n    nx_graph = graph._get_graph_to_dump()\n    data_dir = os.path.join(os.path.dirname(__file__), \'data/reference_graphs\')\n    path_to_dot = os.path.abspath(os.path.join(data_dir, graph_dir, path_to_dot))\n\n    # validate .dot file manually!\n    if not os.path.exists(path_to_dot):\n        nx.drawing.nx_pydot.write_dot(nx_graph, path_to_dot)\n        sort_dot(path_to_dot)\n\n    load_graph = nx.drawing.nx_pydot.read_dot(path_to_dot)\n    load_graph = get_version_agnostic_graph(load_graph)\n\n    # nx_graph is expected to have version-agnostic operator names already\n    for k, attrs in nx_graph.nodes.items():\n        attrs = {k: str(v) for k, v in attrs.items()}\n        load_attrs = {k: str(v).strip(\'""\') for k, v in load_graph.nodes[k].items()}\n        assert attrs == load_attrs\n\n    assert load_graph.nodes.keys() == nx_graph.nodes.keys()\n    assert nx.DiGraph(load_graph).edges == nx_graph.edges\n\n\nclass QuantizeTestCaseConfiguration:\n    def __init__(self, quant_type: str, graph_dir: str):\n        self.quant_type = quant_type\n        self.graph_dir = graph_dir\n\nQUANTIZERS = [\'symmetric\', \'asymmetric\']\n\n@pytest.fixture(scope=\'function\', params=QUANTIZERS)\ndef _case_config(request):\n    quantization_type = request.param\n    graph_dir = os.path.join(\'quantized\', quantization_type)\n    return QuantizeTestCaseConfiguration(quantization_type, graph_dir)\n\n\ndef gnmt_forward_fn(seq_len, batch_size, vocab_size):\n    def forward_fn(model, seq_len_, batch_size_, vocab_size_, batch_first_):\n        device = next(model.parameters()).device\n\n        def gen_packed_sequence():\n            seq_list = []\n            seq_lens = torch.LongTensor(batch_size_).random_(1, seq_len_ + 1).to(device)\n            seq_lens = torch.sort(seq_lens, descending=True).values\n            for seq_size in seq_lens:\n                seq_list.append(torch.LongTensor(seq_size.item()).random_(1, vocab_size_).to(device))\n            padded_seq_batch = torch.nn.utils.rnn.pad_sequence(seq_list, batch_first=batch_first_)\n            return padded_seq_batch, seq_lens\n\n        x_data, seq_lens = gen_packed_sequence()\n        input_encoder = x_data\n        input_enc_len = seq_lens\n        input_decoder = gen_packed_sequence()[0]\n\n        nncf_model_input(input_encoder)\n        nncf_model_input(input_enc_len)\n        nncf_model_input(input_decoder)\n        model(input_encoder, input_enc_len, input_decoder)\n\n    return partial(forward_fn, seq_len_=seq_len, batch_size_=batch_size, vocab_size_=vocab_size, batch_first_=False)\n\n\nTEST_MODELS_DEFAULT = [\n    (""alexnet.dot"", test_models.AlexNet, (1, 3, 32, 32)),\n    (""lenet.dot"", test_models.LeNet, (1, 3, 32, 32)),\n    (""resnet18.dot"", test_models.ResNet18, (1, 3, 32, 32)),\n    (""resnet50.dot"", test_models.ResNet50, (1, 3, 32, 32)),\n    (""vgg16.dot"", partial(test_models.VGG, \'VGG16\'), (1, 3, 32, 32)),\n    (""inception.dot"", test_models.GoogLeNet, (1, 3, 32, 32)),\n    (""densenet121.dot"", test_models.DenseNet121, (1, 3, 32, 32)),\n    (""inception_v3.dot"", partial(test_models.Inception3, aux_logits=True, transform_input=True), (2, 3, 299, 299)),\n    (""squeezenet1_0.dot"", test_models.squeezenet1_0, (1, 3, 32, 32)),\n    (""squeezenet1_1.dot"", test_models.squeezenet1_1, (1, 3, 32, 32)),\n    (""shufflenetv2.dot"", partial(test_models.ShuffleNetV2, net_size=0.5), (1, 3, 32, 32)),\n    (""shuflenet_g2.dot"", test_models.ShuffleNetG2, (1, 3, 32, 32)),\n    (""ssd_vgg.dot"", test_models.ssd_vgg300, (2, 3, 300, 300)),\n    (""ssd_mobilenet.dot"", test_models.ssd_mobilenet, (2, 3, 300, 300)),\n    (""mobilenet_v2.dot"", torchvision.models.MobileNetV2, (2, 3, 32, 32)),\n    (""resnext29_32x4d.dot"", test_models.ResNeXt29_32x4d, (1, 3, 32, 32)),\n    (""pnasnetb.dot"", test_models.PNASNetB, (1, 3, 32, 32)),\n    (""senet18.dot"", test_models.SENet18, (1, 3, 32, 32)),\n    (""preresnet50.dot"", test_models.PreActResNet50, (1, 3, 32, 32)),\n    (""unet.dot"", test_models.UNet, (1, 3, 360, 480)),\n    (""lstm_cell.dot"", LSTMCellNNCF, (1, 1)),\n    (""lstm_uni_seq.dot"", partial(NNCF_RNN, num_layers=1, bidirectional=False), (3, 1, 1)),\n    (""lstm_uni_stacked.dot"", partial(NNCF_RNN, num_layers=2, bidirectional=False), (3, 1, 1)),\n    (""lstm_bi_seq.dot"", partial(NNCF_RNN, num_layers=1, bidirectional=True), (3, 1, 1)),\n    (""lstm_bi_stacked.dot"", partial(NNCF_RNN, num_layers=2, bidirectional=True), (3, 1, 1))\n]\n\nTEST_MODELS = [(m[0], m[1], m[2]) for m in TEST_MODELS_DEFAULT]\n\n\ndef _get_model_name(dot_name):\n    if isinstance(dot_name, tuple):\n        dot_name = dot_name[0]\n    return dot_name[:dot_name.find(\'.dot\')]\n\n\ndef check_model_graph(compressed_model: NNCFNetwork, ref_dot_file_name: str, ref_dot_file_directory: str):\n    compressed_model.to(\'cuda\')\n    compressed_model.do_dummy_forward()\n    compressed_model.do_dummy_forward()\n    check_graph(compressed_model.get_graph(), ref_dot_file_name, ref_dot_file_directory)\n\n@pytest.mark.parametrize(\n    ""model_name, model_builder, input_size"", TEST_MODELS, ids=[_get_model_name(m[0]) for m in TEST_MODELS]\n)\nclass TestModelsGraph:\n    def test_build_graph(self, model_name, model_builder, input_size):\n        net = model_builder()\n        graph_builder = GraphBuilder(create_dummy_forward_fn([ModelInputInfo(input_size), ]))\n        graph = graph_builder.build_graph(net)\n        check_graph(graph, model_name, \'original\')\n\n    @pytest.mark.parametrize(\n        (""algo"", ""params""),\n        (\n            (""rb_sparsity"", {}),\n            (""magnitude_sparsity"", {}),\n            (""const_sparsity"", {})\n        ), ids=[\'RB\', \'Magnitude\', \'Const\']\n    )\n    def test_sparse_network(self, model_name, model_builder, input_size, algo, params):\n        model = model_builder()\n        from nncf.layers import NNCF_MODULES_MAP\n        sparsifiable_modules = list(NNCF_MODULES_MAP.values())\n        ref_num_sparsed = len(get_all_modules_by_type(model, sparsifiable_modules))\n\n        config = get_empty_config(input_sample_size=input_size)\n        config[""compression""] = {""algorithm"": algo, ""params"": params}\n\n        compressed_model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n        assert ref_num_sparsed == len(compression_ctrl.sparsified_module_info)\n        check_model_graph(compressed_model, model_name, algo)\n\n    def test_quantize_network(self, model_name, model_builder, input_size, _case_config):\n        model = model_builder()\n        config = get_basic_quantization_config(_case_config.quant_type, input_sample_size=input_size)\n        compressed_model, _ = create_compressed_model_and_algo_for_test(model, config)\n        check_model_graph(compressed_model, model_name, _case_config.graph_dir)\n\n    def test_sparse_quantize_network(self, model_name, model_builder, input_size):\n        model = model_builder()\n\n        from nncf.layers import NNCF_MODULES_MAP\n        sparsifiable_modules = list(NNCF_MODULES_MAP.values())\n        ref_num_sparsed = len(get_all_modules_by_type(model, sparsifiable_modules))\n        config = get_empty_config(input_sample_size=input_size)\n        config[""compression""] = [\n            {""algorithm"": ""rb_sparsity"", ""params"": {}},\n            {""algorithm"": ""quantization"", ""params"": {}}\n        ]\n\n        compressed_model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n        assert ref_num_sparsed == len(compression_ctrl.child_algos[0].sparsified_module_info)\n        check_model_graph(compressed_model, model_name, ""quantized_rb_sparsity"")\n\n\ndef test_gnmt_quantization(_case_config):\n    model = GNMT(vocab_size=32)\n    model = replace_lstm(model)\n    forward_fn_ = gnmt_forward_fn(seq_len=10, batch_size=3, vocab_size=32)\n\n    config = get_basic_quantization_config(_case_config.quant_type, input_sample_size=(3, 10))\n    config[""compression""].update({\n        ""quantizable_subgraph_patterns"": [[""linear"", ""__add__""],\n                                          [""sigmoid"", ""__mul__"", ""__add__""],\n                                          [""__add__"", ""tanh"", ""__mul__""],\n                                          [""sigmoid"", ""__mul__""]],\n        ""disable_function_quantization_hooks"": True,\n        ""ignored_scopes"": [""GNMT/ResidualRecurrentEncoder[encoder]/Embedding[embedder]"",\n                           ""GNMT/ResidualRecurrentDecoder[decoder]/Embedding[embedder]""]})\n\n    compressed_model = NNCFNetwork(model,\n                                   input_infos=create_input_infos(config),\n                                   dummy_forward_fn=forward_fn_,\n                                   scopes_without_shape_matching=\n                                   [\'GNMT/ResidualRecurrentDecoder[decoder]/RecurrentAttention[att_rnn]/\'\n                                    \'BahdanauAttention[attn]\'])\n\n    compression_algo_builder_list = create_compression_algorithm_builders(config)\n\n    for builder in compression_algo_builder_list:\n        compressed_model = builder.apply_to(compressed_model)\n    _ = compressed_model.commit_compression_changes()\n    check_model_graph(compressed_model, \'gnmt_variable.dot\', _case_config.graph_dir)\n\n\ndef test_resnet18__with_not_qinput(_case_config):\n    model = test_models.ResNet18()\n    input_shape = (1, 3, 32, 32)\n\n    config = get_basic_quantization_config(_case_config.quant_type, input_sample_size=input_shape)\n    config[""compression""].update({""quantize_inputs"": False})\n\n    compressed_model, _ = create_compressed_model_and_algo_for_test(model, config)\n    check_model_graph(compressed_model, \'resnet18_no_qinput.dot\', _case_config.graph_dir)\n\n\ndef test_resnet18__with_ignore(_case_config):\n    model = test_models.ResNet18()\n    input_shape = (1, 3, 32, 32)\n\n    config = get_basic_quantization_config(_case_config.quant_type, input_sample_size=input_shape)\n    ignored_scopes = [\'ResNet/Sequential[layer3]\', ]\n    config.update({""ignored_scopes"": ignored_scopes})  # Global config ignored_scopes for NNCF module replacement\n    config[""compression""].update({""ignored_scopes"": ignored_scopes})  # Local ignored_scopes for quantization\n\n    compressed_model, _ = create_compressed_model_and_algo_for_test(model, config)\n    check_model_graph(compressed_model, \'resnet18_ignore.dot\', _case_config.graph_dir)\n\n\ndef test_iterate_module_list():\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.ml = nn.ModuleList([nn.Conv2d(1, 1, 1), nn.Conv2d(1, 1, 1)])\n\n        def forward(self, x):\n            return [self.ml[0](x), self.ml[1](x)]\n\n    net = Net()\n\n    context = TracingContext()\n    with context:\n        _ = net(torch.zeros(1, 1, 1, 1))\n\n    check_graph(context.graph, \'case_iterate_module_list.dot\', \'original\')\n\n\ndef test_output_quantization(_case_config):\n    model = test_models.UNet()\n    input_shape = (1, 3, 360, 480)\n\n    config = get_basic_quantization_config(_case_config.quant_type, input_sample_size=input_shape)\n    config[""compression""].update({""quantize_outputs"": True})\n\n    compressed_model, _ = create_compressed_model_and_algo_for_test(model, config)\n    check_model_graph(compressed_model, \'unet_qoutput.dot\', _case_config.graph_dir)\n\n\ndef test_custom_quantizable_subgraph_patterns(_case_config):\n    model = test_models.SENet18()\n\n    input_shape = (1, 3, 32, 32)\n\n    config = get_basic_quantization_config(_case_config.quant_type, input_sample_size=input_shape)\n\n    config[""compression""].update({""quantize_outputs"": False,\n                                  ""quantizable_subgraph_patterns"": [[""sigmoid"", ""__mul__""],\n                                                                    [""__iadd__"", ""batch_norm""]]})\n\n    compressed_model, _ = create_compressed_model_and_algo_for_test(model, config)\n    check_model_graph(compressed_model, \'senet_custom_patterns.dot\', _case_config.graph_dir)\n'"
pytorch_toolkit/nncf/tests/test_compression_training.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the \'License\');\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \'AS IS\' BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport json\nimport os\nimport re\nimport tempfile\n\nimport pytest\nimport torch\nfrom pytest import approx\n\nfrom examples.common.utils import get_name\nfrom tests.conftest import TEST_ROOT\nfrom tests.test_sanity_sample import Command, create_command_line\n\n# sample\n# \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 dataset\n# \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 path\n# \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 batch\n# \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 configs\n# \xe2\x94\x82   \xe2\x94\x82     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80 config_filename\n# \xe2\x94\x82   \xe2\x94\x82     \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 expected_accuracy\n# \xe2\x94\x82   \xe2\x94\x82     \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 absolute_tolerance_train\n# \xe2\x94\x82   \xe2\x94\x82     \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 absolute_tolerance_eval\n# \xe2\x94\x82   \xe2\x94\x82     \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 execution_arg\n# \xe2\x94\x82   \xe2\x94\x82     \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 epochs\n# \xe2\x94\x82   \xe2\x94\x82     \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 weights\nGLOBAL_CONFIG = {\n    \'classification\':\n        {\n            \'cifar100\':\n                {\n                    \'configs\': {\n                        \'mobilenet_v2_sym_int8.json\': {\n                            \'execution_arg\': {\'multiprocessing-distributed\'},\n                            \'expected_accuracy\': 64.53,\n                            \'weights\': \'mobilenetV2_64.53.sd\',\n                            \'absolute_tolerance_train\': 1.5,\n                        },\n                        \'mobilenet_v2_asym_int8.json\': {\n                            \'execution_arg\': {\'multiprocessing-distributed\', \'cpu-only\'},\n                            \'expected_accuracy\': 64.53,\n                            \'weights\': \'mobilenetV2_64.53.sd\',\n                            \'absolute_tolerance_train\': 1.5,\n                        },\n                        \'inceptionV3_int8.json\': {\n                            \'expected_accuracy\': 77.53,\n                            \'weights\': \'inceptionV3_77.53.sd\',\n                            \'absolute_tolerance_eval\': 6e-2,\n                        },\n                        \'resnet50_int8.json\': {\n                            \'expected_accuracy\': 68.86,\n                            \'weights\': \'resnet50_68.86.sd\',\n                            \'absolute_tolerance_eval\': 6e-2,\n                        },\n                        \'mobilenet_v2_magnitude_sparsity_int8.json\': {\n                            \'expected_accuracy\': 64.53,\n                            \'weights\': \'mobilenetV2_64.53.sd\',\n                            \'execution_arg\': {\'multiprocessing-distributed\', \'\'},\n                            \'absolute_tolerance_train\': 1.5,\n                        },\n                        \'mobilenet_v2_rb_sparsity_int8.json\': {\n                            \'expected_accuracy\': 64.53,\n                            \'weights\': \'mobilenetV2_64.53.sd\',\n                            \'execution_arg\': {\'multiprocessing-distributed\'},\n                        }\n                    }\n                },\n            \'imagenet\':\n                {\n                    \'configs\': {\n                        \'mobilenet_v2_imagenet_sym_int8.json\': {\n                            \'execution_arg\': {\'multiprocessing-distributed\'},\n                            \'expected_accuracy\': 100,\n                            \'weights\': \'mobilenet_v2.pth.tar\'\n                        },\n                        \'mobilenet_v2_imagenet_asym_int8.json\': {\n                            \'execution_arg\': {\'multiprocessing-distributed\'},\n                            \'expected_accuracy\': 100,\n                            \'weights\': \'mobilenet_v2.pth.tar\',\n                        },\n                        \'resnet50_imagenet_sym_int8.json\': {\n                            \'execution_arg\': {\'multiprocessing-distributed\'},\n                            \'expected_accuracy\': 100,\n                        },\n                        \'resnet50_imagenet_asym_int8.json\': {\n                            \'execution_arg\': {\'multiprocessing-distributed\'},\n                            \'expected_accuracy\': 100,\n                        },\n                    }\n                }\n        }\n}\n\n\ndef get_cli_args(args):\n    cli_args = []\n    for key, val in args.items():\n        cli_args.append(\'--{}\'.format(str(key)))\n        if val:\n            cli_args.append(str(val))\n    return cli_args\n\n\ndef get_cli_dict_args(args):\n    cli_args = dict()\n    for key, val in args.items():\n        cli_key = \'--{}\'.format(str(key))\n        cli_args[cli_key] = None\n        if val:\n            cli_args[cli_key] = str(val)\n    return cli_args\n\n\ndef parse_best_acc1(tmp_path):\n    output_path = None\n    for root, _, names in os.walk(str(tmp_path)):\n        for name in names:\n            if \'output\' in name:\n                output_path = os.path.join(root, name)\n\n    assert os.path.exists(output_path)\n    with open(output_path, ""r"") as f:\n        for line in reversed(f.readlines()):\n            if line != \'\\n\':\n                matches = re.findall(""\\\\d+\\\\.\\\\d+"", line)\n                if not matches:\n                    raise RuntimeError(""Could not parse output log for accuracy!"")\n                acc1 = float(matches[0])\n                return acc1\n    raise RuntimeError(""Could not parse output log for accuracy!"")\n\n\n\nCONFIG_PARAMS = []\nfor sample_type_ in GLOBAL_CONFIG:\n    datasets = GLOBAL_CONFIG[sample_type_]\n    for dataset_name_ in datasets:\n        dataset_path = datasets[dataset_name_].get(\'path\', os.path.join(tempfile.gettempdir(), dataset_name_))\n        batch_size = datasets[dataset_name_].get(\'batch\', None)\n        configs = datasets[dataset_name_].get(\'configs\', {})\n        for config_name in configs:\n            config_params = configs[config_name]\n            execution_args = config_params.get(\'execution_arg\', [\'\'])\n            expected_accuracy_ = config_params.get(\'expected_accuracy\', 100)\n            absolute_tolerance_train_ = config_params.get(\'absolute_tolerance_train\', 1)\n            absolute_tolerance_eval_ = config_params.get(\'absolute_tolerance_eval\', 1e-3)\n            weights_path_ = config_params.get(\'weights\', None)\n            epochs = config_params.get(\'epochs\', None)\n            if weights_path_:\n                weights_path_ = os.path.join(sample_type_, dataset_name_, weights_path_)\n            for execution_arg_ in execution_args:\n                config_path_ = TEST_ROOT.joinpath(""data"", ""configs"", ""weekly"", sample_type_, dataset_name_, config_name)\n                jconfig = json.load(config_path_.open())\n                args_ = {\n                    \'data\': dataset_path,\n                    \'weights\': weights_path_,\n                    \'config\': str(config_path_)\n                }\n                if batch_size:\n                    args_[\'batch-size\'] = batch_size\n                if epochs:\n                    args_[\'epochs\'] = epochs\n                test_config_ = {\n                    \'sample_type\': sample_type_,\n                    \'expected_accuracy\': expected_accuracy_,\n                    \'absolute_tolerance_train\': absolute_tolerance_train_,\n                    \'absolute_tolerance_eval\': absolute_tolerance_eval_,\n                    \'checkpoint_name\': get_name(jconfig)\n                }\n                CONFIG_PARAMS.append(tuple([test_config_, args_, execution_arg_, dataset_name_]))\n\n\ndef get_config_name(config_path):\n    base = os.path.basename(config_path)\n    return os.path.splitext(base)[0]\n\n\n@pytest.fixture(scope=\'module\', params=CONFIG_PARAMS,\n                ids=[\'-\'.join([p[0][\'sample_type\'], get_config_name(p[1][\'config\']), p[2]]) for p in CONFIG_PARAMS])\ndef _params(request, tmp_path_factory, dataset_dir, weekly_models_path, enable_imagenet):\n    if weekly_models_path is None:\n        pytest.skip(\'Path to models weights for weekly testing is not set, use --weekly-models option.\')\n    test_config, args, execution_arg, dataset_name = request.param\n    test_config[\'timeout\'] = 2 * 60 * 60  # 2 hours, because rb sparsity + int8 works 1.5-2 hours\n    if enable_imagenet:\n        test_config[\'timeout\'] = None\n    if \'imagenet\' in dataset_name and not enable_imagenet:\n        pytest.skip(\'ImageNet tests were intentionally skipped as it takes a lot of time\')\n    if args[\'weights\']:\n        weights_path = os.path.join(weekly_models_path, args[\'weights\'])\n        if not os.path.exists(weights_path):\n            raise FileExistsError(\'Weights file does not exist: {}\'.format(weights_path))\n        args[\'weights\'] = weights_path\n    else:\n        del args[\'weights\']\n    if execution_arg:\n        args[execution_arg] = None\n    checkpoint_save_dir = str(tmp_path_factory.mktemp(\'models\'))\n    checkpoint_save_dir = os.path.join(checkpoint_save_dir, execution_arg.replace(\'-\', \'_\'))\n    args[\'checkpoint-save-dir\'] = checkpoint_save_dir\n    if dataset_dir:\n        args[\'data\'] = dataset_dir\n    return {\n        \'test_config\': test_config,\n        \'args\': args,\n    }\n\n\n@pytest.mark.dependency(name=[""train""])\ndef test_compression_train(_params, tmp_path):\n    p = _params\n    args = p[\'args\']\n    tc = p[\'test_config\']\n\n    args[\'mode\'] = \'train\'\n    args[\'log-dir\'] = tmp_path\n    args[\'workers\'] = 4\n    args[\'seed\'] = 1\n\n    runner = Command(create_command_line(get_cli_dict_args(args), tc[\'sample_type\']))\n    res = runner.run(timeout=tc[\'timeout\'])\n\n    assert res == 0\n    checkpoint_path = os.path.join(args[\'checkpoint-save-dir\'], tc[\'checkpoint_name\'] + \'_best.pth\')\n    assert os.path.exists(checkpoint_path)\n    actual_acc = torch.load(checkpoint_path)[\'best_acc1\']\n    ref_acc = tc[\'expected_accuracy\']\n    better_accuracy_tolerance = 3\n    tolerance = tc[\'absolute_tolerance_train\'] if actual_acc < ref_acc else better_accuracy_tolerance\n    assert actual_acc == approx(ref_acc, abs=tolerance)\n\n\n@pytest.mark.dependency(depends=[""train""])\ndef test_compression_eval_trained(_params, tmp_path):\n    p = _params\n    args = p[\'args\']\n    tc = p[\'test_config\']\n\n    args[\'mode\'] = \'test\'\n    args[\'log-dir\'] = tmp_path\n    args[\'workers\'] = 4\n    args[\'seed\'] = 1\n    checkpoint_path = os.path.join(args[\'checkpoint-save-dir\'], tc[\'checkpoint_name\'] + \'_best.pth\')\n    args[\'resume\'] = checkpoint_path\n    if \'weights\' in args:\n        del args[\'weights\']\n\n    runner = Command(create_command_line(get_cli_dict_args(args), tc[\'sample_type\']))\n    res = runner.run(timeout=tc[\'timeout\'])\n    assert res == 0\n\n    acc1 = parse_best_acc1(tmp_path)\n    assert torch.load(checkpoint_path)[\'best_acc1\'] == approx(acc1, abs=tc[\'absolute_tolerance_eval\'])\n'"
pytorch_toolkit/nncf/tests/test_config_schema.py,0,"b'from collections import namedtuple\nfrom pathlib import Path\nfrom typing import List\n\nimport jsonschema\nimport pytest\n\nfrom nncf.config import Config\nfrom tests.conftest import PROJECT_ROOT, TEST_ROOT\n\nGOOD_CONFIG_SOURCES = [\n    PROJECT_ROOT / Path(""examples/classification/configs""),\n    PROJECT_ROOT / Path(""examples/semantic_segmentation/configs""),\n    PROJECT_ROOT / Path(""examples/object_detection/configs""),\n    TEST_ROOT / Path(""data/configs"")\n]\n\nBAD_CONFIG_SOURCES = [\n    TEST_ROOT / Path(""data/schema_validation_bad_configs"")\n]\n\nConfigPathVsPassesSchemaVal = namedtuple(\'ConfigPathVsPassesSchemaVal\', (\'path\', \'should_pass\'))\nTEST_STRUCTS = []\n\n\ndef get_all_jsons_from_sources(source_directories_list: List[Path]) -> List[Path]:\n    retval = []\n    for source_dir in source_directories_list:\n        files = source_dir.glob(\'**/*.json\')\n        retval += files\n    return retval\n\n\ngood_config_files = get_all_jsons_from_sources(GOOD_CONFIG_SOURCES)\nfor file in good_config_files:\n    TEST_STRUCTS.append(ConfigPathVsPassesSchemaVal(file, True))\n\nbad_config_files = get_all_jsons_from_sources(BAD_CONFIG_SOURCES)\nfor file in bad_config_files:\n    TEST_STRUCTS.append(ConfigPathVsPassesSchemaVal(file, False))\n\n\n@pytest.fixture(name=""config_test_struct"", params=TEST_STRUCTS,\n                ids=[str(struct.path.relative_to(PROJECT_ROOT)) for struct in TEST_STRUCTS])\ndef _config_test_struct(request):\n    return request.param\n\n\ndef test_json_against_nncf_config_schema(config_test_struct):\n    config_path, should_pass = config_test_struct\n    if should_pass:\n        _ = Config.from_json(str(config_path))\n    else:\n        with pytest.raises(jsonschema.ValidationError):\n            _ = Config.from_json(str(config_path))\n'"
pytorch_toolkit/nncf/tests/test_context_independence.py,0,"b'import pytest\nimport os\nfrom tests import test_models\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test\nfrom tests.test_compressed_graph import check_model_graph, QUANTIZERS, QuantizeTestCaseConfiguration\n\nfrom tests.test_compressed_graph import get_basic_quantization_config\n\n\nTEST_MODELS = [((""alexnet.dot"", ""lenet.dot""),\n                (test_models.AlexNet, test_models.LeNet),\n                ((1, 3, 32, 32), (1, 3, 32, 32)))]\n\n\n@pytest.fixture(scope=\'function\', params=QUANTIZERS)\ndef _case_config(request):\n    quantization_type = request.param\n    graph_dir = os.path.join(\'quantized\', quantization_type)\n    return QuantizeTestCaseConfiguration(quantization_type, graph_dir)\n\n\n@pytest.mark.parametrize(\n    ""model_name, model_builder, input_size"", TEST_MODELS\n)\ndef test_context_independence(model_name, model_builder, input_size, _case_config):\n\n    config = get_basic_quantization_config(_case_config.quant_type, input_sample_size=input_size[0])\n    compressed_models = [create_compressed_model_and_algo_for_test(model_builder[0](), config)[0],\n                         create_compressed_model_and_algo_for_test(model_builder[1](), config)[0]]\n\n    for i, compressed_model in enumerate(compressed_models):\n        check_model_graph(compressed_model, model_name[i], _case_config.graph_dir)\n'"
pytorch_toolkit/nncf/tests/test_graph_analysis.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom collections import Counter\n\nfrom nncf.dynamic_graph.graph import NNCFGraph, NNCFGraphPatternIO, NNCFGraphEdge, NNCFNode\n\n\ndef test_graph_pattern_io_building():\n    graph = NNCFGraph()\n    #   1\n    # /   \\\n    # 2   |\n    # |   |\n    # 3   |\n    # \\   /\n    #   4\n    # / | \\\n    # 5 6 7\n    # |\n    # 8\n\n    #pylint:disable=protected-access\n    node_keys = [\'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\']\n    for idx, node_key in enumerate(node_keys):\n        attrs = {\n            NNCFGraph.ID_NODE_ATTR: idx + 1,\n            NNCFGraph.KEY_NODE_ATTR: node_key,\n            NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR: None,\n        }\n        graph._nx_graph.add_node(node_key, **attrs)\n\n    edge_attr = {NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR: None}\n    graph._nx_graph.add_edges_from([(\'1\', \'2\'), (\'1\', \'4\'), (\'2\', \'3\'), (\'3\', \'4\'), (\'4\', \'5\'),\n                                    (\'4\', \'6\'), (\'4\', \'7\'), (\'5\', \'8\')], **edge_attr)\n    graph._node_id_to_key_dict.update({k + 1: v for k, v in enumerate(node_keys)})\n\n    def make_mock_edge(from_id: int, to_id: int):\n        return NNCFGraphEdge(NNCFNode(from_id, None),\n                             NNCFNode(to_id, None), None)\n\n    def make_mock_node(id_: int):\n        return NNCFNode(id_, None)\n\n    ref_patterns_and_ios = [\n        ([\'1\', \'2\'], NNCFGraphPatternIO(input_edges=[],\n                                        input_nodes=[make_mock_node(1)],\n                                        output_edges=[make_mock_edge(2, 3),\n                                                      make_mock_edge(1, 4)],\n                                        output_nodes=[])),\n        ([\'3\'], NNCFGraphPatternIO(input_edges=[make_mock_edge(2, 3)],\n                                   input_nodes=[],\n                                   output_edges=[make_mock_edge(3, 4)],\n                                   output_nodes=[])),\n        ([\'1\', \'2\', \'3\'], NNCFGraphPatternIO(input_edges=[],\n                                             input_nodes=[make_mock_node(1)],\n                                             output_edges=[make_mock_edge(3, 4),\n                                                           make_mock_edge(1, 4)],\n                                             output_nodes=[])),\n        ([\'4\'], NNCFGraphPatternIO(input_edges=[make_mock_edge(3, 4),\n                                                make_mock_edge(1, 4)],\n                                   input_nodes=[],\n                                   output_edges=[make_mock_edge(4, 5),\n                                                 make_mock_edge(4, 6),\n                                                 make_mock_edge(4, 7)],\n                                   output_nodes=[])),\n        ([\'5\', \'6\', \'8\'], NNCFGraphPatternIO(input_edges=[make_mock_edge(4, 5),\n                                                          make_mock_edge(4, 6)],\n                                             input_nodes=[],\n                                             output_edges=[],\n                                             output_nodes=[make_mock_node(6),\n                                                           make_mock_node(8)])),\n        ([\'7\'], NNCFGraphPatternIO(input_edges=[make_mock_edge(4, 7)],\n                                   input_nodes=[],\n                                   output_edges=[],\n                                   output_nodes=[make_mock_node(7)]))\n    ]\n\n    for pattern, ref_pattern_io in ref_patterns_and_ios:\n        test_pattern_io = graph._get_nncf_graph_pattern_io_list(pattern)\n        assert Counter(test_pattern_io.input_edges) == Counter(ref_pattern_io.input_edges)\n        assert Counter(test_pattern_io.output_edges) == Counter(ref_pattern_io.output_edges)\n        assert Counter(test_pattern_io.input_nodes) == Counter(ref_pattern_io.input_nodes)\n        assert Counter(test_pattern_io.output_nodes) == Counter(ref_pattern_io.output_nodes)\n'"
pytorch_toolkit/nncf/tests/test_graph_building.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom typing import Tuple, List\n\nimport pytest\nimport torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nncf.dynamic_graph.graph import NNCFGraph\nfrom nncf.dynamic_graph.graph_builder import ModelInputInfo, create_dummy_forward_fn, GraphBuilder\nfrom tests.test_compressed_graph import get_basic_quantization_config\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test\n\nTEST_TRACING_CONTEXT = \'test\'\n\ndef test_ambiguous_function():\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.layers = nn.ModuleList([\n                nn.Conv2d(1, 1, 1),\n                nn.Conv2d(1, 1, 1)\n            ])\n\n        def forward(self, x):\n            for layer in self.layers:\n                x = F.relu(layer(x))\n\n    mod = Model()\n    input_info = ModelInputInfo((1, 1, 1, 1))\n\n    graph_builder = GraphBuilder(custom_forward_fn=create_dummy_forward_fn([input_info, ]))\n    graph = graph_builder.build_graph(mod)\n\n    unique_op_exec_contexts = set()\n    #pylint:disable=protected-access\n    for _, node in graph._nx_graph.nodes.items():\n        node_op_exec_context = node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n        assert node_op_exec_context not in unique_op_exec_contexts\n\n\ndef test_forward_trace_functor():\n    from nncf.dynamic_graph.patch_pytorch import ForwardTraceOnly\n    from nncf.dynamic_graph.trace_tensor import TracedTensor, TensorMeta\n\n    func = ForwardTraceOnly()\n    shape1, shape2 = ([32, 1, 4, 8], [1, 8, 12, 16])\n    meta1, meta2 = (TensorMeta(5, 1, shape1), TensorMeta(3, 8, shape2))\n    input_tensor1 = TracedTensor.from_torch_tensor(torch.Tensor(shape1), meta1)\n    input_tensor2 = TracedTensor.from_torch_tensor(torch.Tensor(shape2), meta2)\n\n    # 1 -> 1\n    output_tensor = func(torch.Tensor.view, input_tensor1, [-1])\n    assert output_tensor.tensor_meta == input_tensor1.tensor_meta\n\n    # 1 -> N\n    outputs = func(torch.Tensor.chunk, input_tensor1, 3)\n    for out in outputs:\n        assert out.tensor_meta == input_tensor1.tensor_meta\n\n    # N -> N (2 -> 2)\n    outputs = func(lambda x: x + [5], [input_tensor1, input_tensor2])\n    assert outputs[0].tensor_meta == input_tensor1.tensor_meta\n    assert outputs[1].tensor_meta == input_tensor2.tensor_meta\n\n    # M -> N (2 -> 3)\n    with pytest.raises(RuntimeError):\n        outputs = func(lambda x: x + [torch.Tensor(shape2)], [input_tensor1, input_tensor2])\n\n    # M -> N (2 -> 1)\n    with pytest.raises(RuntimeError):\n        outputs = func(lambda x: x[0], [input_tensor1, input_tensor2])\n\n\n\nclass ModelForTest(torch.nn.Module):\n    IN_CHANNELS = 3\n    OUT_CHANNELS = 10\n    CONV1_OUT_CHANNELS = 15\n    CONV2_IN_CHANNELS = CONV1_OUT_CHANNELS + IN_CHANNELS\n    MAXPOOL_SIZE = 2\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(self.IN_CHANNELS, self.CONV1_OUT_CHANNELS, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(15)\n        self.relu1 = nn.ReLU()\n        self.convt1 = nn.ConvTranspose2d(self.CONV1_OUT_CHANNELS, self.IN_CHANNELS, kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(self.CONV2_IN_CHANNELS, self.OUT_CHANNELS, kernel_size=1)\n\n    def forward(self, input_):\n        x = self.conv1(input_)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x_prev = x\n        x = F.max_pool2d(x, self.MAXPOOL_SIZE)\n        x = self.convt1(x)\n        x = torch.cat([x, x_prev], 1)\n        x = self.conv2(x)\n        return x\n\n\ninput_shapes = [\n    (1, 3, 224, 224),\n    (2, 3, 224, 224),\n    (1, 3, 500, 500)\n]\n\n\n@pytest.mark.parametrize(""input_shape"", input_shapes)\ndef test_activation_shape_tracing(input_shape: Tuple):\n    model = ModelForTest()\n    input_info = ModelInputInfo(input_shape)\n    graph_builder = GraphBuilder(create_dummy_forward_fn([input_info, ]))\n    graph = graph_builder.build_graph(model)\n\n    shape1 = (input_shape[0], ModelForTest.CONV1_OUT_CHANNELS, input_shape[2], input_shape[3])\n    ref_node_ids_and_output_shapes = [\n        # TODO: extend with checking input tensor size once proper input node marking is implemented\n        (""0 ModelForTest/Conv2d[conv1]/conv2d"", [shape1]),\n        (""1 ModelForTest/BatchNorm2d[bn1]/batch_norm"", [shape1]),\n        (""2 ModelForTest/ReLU[relu1]/RELU"", [shape1, shape1]),\n        (""3 ModelForTest/max_pool2d"", [(shape1[0], shape1[1],\n                                        shape1[2] // ModelForTest.MAXPOOL_SIZE,\n                                        shape1[3] // ModelForTest.MAXPOOL_SIZE)]),\n        (""4 ModelForTest/ConvTranspose2d[convt1]/conv_transpose2d"", [input_shape]),\n        (""5 ModelForTest/cat"", [(input_shape[0], ModelForTest.CONV2_IN_CHANNELS,\n                                 input_shape[2], input_shape[3])])\n\n        # TODO: extend with checking output tensor size once proper output node marking is implemented\n    ]\n    for node_id, ref_output_shapes in ref_node_ids_and_output_shapes:\n        # pylint:disable=protected-access\n        output_edges = graph._get_nncf_graph_pattern_input_output([node_id, ]).output_edges\n        output_shapes = [x.tensor_shape for x in output_edges]\n        assert output_shapes == ref_output_shapes, ""Failed for {}"".format(node_id)\n\n\nTEST_KEYWORD_1 = ""keyword1""\nTEST_KEYWORD_2 = ""keyword2""\nINPUT_INFO_CONFIG_VS_FORWARD_ARGS = [\n    ({""sample_size"": [2, 3, 300, 300],\n      ""type"": ""float"",\n      ""filler"": ""zeros""},\n     [ModelInputInfo((2, 3, 300, 300), type_str=""float"", filler=ModelInputInfo.FILLER_TYPE_ZEROS)]),\n\n    ([{""sample_size"": [1, 128],\n       ""type"": ""long"",\n       ""filler"": ""ones""},\n      {""sample_size"": [1, 128],\n       ""type"": ""long"",\n       ""filler"": ""ones""},\n      {""sample_size"": [1, 128],\n       ""type"": ""long"",\n       ""filler"": ""zeros""}], [ModelInputInfo((1, 128), type_str=""long"", filler=ModelInputInfo.FILLER_TYPE_ONES),\n                             ModelInputInfo((1, 128), type_str=""long"", filler=ModelInputInfo.FILLER_TYPE_ONES),\n                             ModelInputInfo((1, 128), type_str=""long"", filler=ModelInputInfo.FILLER_TYPE_ONES), ]),\n\n    ([{""sample_size"": [2, 3, 300, 300],\n       ""type"": ""float"",\n       ""filler"": ""zeros""},\n      {""sample_size"": [1, 128],\n       ""type"": ""long"",\n       ""filler"": ""ones"",\n       ""keyword"": TEST_KEYWORD_1}],\n     [ModelInputInfo((2, 3, 300, 300), type_str=""float"", filler=ModelInputInfo.FILLER_TYPE_ZEROS),\n      ModelInputInfo((1, 128), type_str=""long"", filler=ModelInputInfo.FILLER_TYPE_ONES, keyword=TEST_KEYWORD_1)]),\n\n    ([{""sample_size"": [8, 7],\n       ""type"": ""float"",\n       ""filler"": ""random"",\n       ""keyword"": TEST_KEYWORD_1},\n      {""sample_size"": [2, 3, 300, 300],\n       ""type"": ""float"",\n       ""filler"": ""zeros""},\n      {""sample_size"": [1, 128],\n       ""type"": ""long"",\n       ""filler"": ""ones"",\n       ""keyword"": TEST_KEYWORD_2}, ],\n     [ModelInputInfo((2, 3, 300, 300), type_str=""float"", filler=ModelInputInfo.FILLER_TYPE_ZEROS),\n      ModelInputInfo((8, 7), type_str=""float"", filler=ModelInputInfo.FILLER_TYPE_ONES, keyword=TEST_KEYWORD_1),\n      ModelInputInfo((1, 128), type_str=""long"", filler=ModelInputInfo.FILLER_TYPE_ONES, keyword=TEST_KEYWORD_2)]),\n]\n\nclass MockModel(torch.nn.Module):\n    def __init__(self, stub_forward):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.ones([1]))\n        self.stub_forward = stub_forward\n\n    def forward(self, *args, **kwargs):\n        return self.stub_forward(*args, **kwargs)\n\n@pytest.fixture(params=INPUT_INFO_CONFIG_VS_FORWARD_ARGS, name=""input_info_test_struct"")\ndef input_info_test_struct_(request):\n    return request.param\n\ndef test_input_info_specification_from_config(mocker, input_info_test_struct):\n    stub_fn = mocker.stub()\n    mock_model = MockModel(stub_fn)\n    config = get_basic_quantization_config(""symmetric"", None)\n    input_info_config_entry = input_info_test_struct[0]\n    target_argument_info = input_info_test_struct[1]  # type: List[ModelInputInfo]\n    config[""input_info""] = input_info_config_entry\n\n    _, _ = create_compressed_model_and_algo_for_test(mock_model, config)\n    forward_call_args = stub_fn.call_args[0]\n    forward_call_kwargs = stub_fn.call_args[1]\n\n    ref_args_info = list(filter(lambda x: x.keyword is None, target_argument_info))\n    ref_kw_vs_arg_info = {x.keyword: x for x in target_argument_info if x.keyword is not None}\n\n    def check_arg(arg: torch.Tensor, ref_arg_info: ModelInputInfo):\n        assert arg.shape == ref_arg_info.shape\n        assert arg.dtype == ref_arg_info.type\n\n    assert len(forward_call_args) == len(ref_args_info)\n    assert len(forward_call_kwargs) == len(ref_kw_vs_arg_info)\n    assert set(forward_call_kwargs.keys()) == set(ref_kw_vs_arg_info.keys())\n\n    for idx, arg in enumerate(forward_call_args):\n        check_arg(arg, ref_args_info[idx])\n\n    for keyword, arg in forward_call_kwargs.items():\n        check_arg(arg, ref_kw_vs_arg_info[keyword])\n'"
pytorch_toolkit/nncf/tests/test_graph_iterator.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import OrderedDict\nimport pytest\n\nimport torch.nn.functional as F\nfrom torch.nn import Module, Conv2d, BatchNorm2d, ReLU, MaxPool2d, Sequential, AvgPool2d, init\n\nfrom nncf.dynamic_graph.version_agnostic_op_names import VersionAgnosticNames\nfrom nncf.utils import get_all_modules_by_type, get_module_by_node_name, get_all_node_names, \\\n    apply_by_node_name, set_module_by_node_name, parse_node_name\n\n\n\nclass ModelForTest(Module):\n    def __init__(self, size=1):\n        super().__init__()\n        self.size = size\n        self.conv0 = Conv2d(size, size, size)\n        self.conv1 = Conv2d(size, size, size)\n        self.bn1 = BatchNorm2d(size)\n        self.bn2 = BatchNorm2d(size)\n        self.norm10 = BatchNorm2d(size)\n        self.norm20 = BatchNorm2d(size)\n        self.avgpool = AvgPool2d(size)\n        self.layer1 = Sequential(OrderedDict([\n            (\'conv01\', self.conv0),\n            (\'norm01\', self.norm10),\n            (\'relu01\', ReLU()),\n            (\'pool01\', MaxPool2d(size))\n        ]))\n        self.layer2 = Sequential(OrderedDict([\n            (\'layer1\', self.layer1),\n            (\'conv02\', self.conv0),\n            (\'relu02\', ReLU()),\n            (\'norm02\', self.norm20),\n            (\'pool02\', MaxPool2d(size))\n        ]))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = ReLU()(x)\n        x = F.relu(x)\n        x = self.bn2(x)\n        x = self.layer2(x)\n        x = self.avgpool(x)\n        return x\n\n\ndef test_get_module_by_node_name__for_non_nested_module():\n    model = ModelForTest()\n    assert get_module_by_node_name(model, \'ModelForTest/BatchNorm2d[bn1]\') == model.bn1\n\n\ndef test_get_module_by_node_name__for_nested_module():\n    model = ModelForTest()\n    assert get_module_by_node_name(model, \'ModelForTest/Sequential[layer2]/Sequential[layer1]\') == model.layer1\n\n\ndef test_get_all_layers_by_type__for_standard_type():\n    model = ModelForTest()\n    act_bn = get_all_modules_by_type(model, \'BatchNorm2d\')\n    act_bn = OrderedDict((str(k), v) for k, v in act_bn.items())\n    ref_bn = {\n        \'ModelForTest/BatchNorm2d[bn1]\': model.bn1,\n        \'ModelForTest/BatchNorm2d[bn2]\': model.bn2,\n        \'ModelForTest/BatchNorm2d[norm10]\': model.norm10,\n        \'ModelForTest/BatchNorm2d[norm20]\': model.norm20,\n        \'ModelForTest/Sequential[layer1]/BatchNorm2d[norm01]\': model.norm10,\n        \'ModelForTest/Sequential[layer2]/BatchNorm2d[norm02]\': model.norm20,\n        \'ModelForTest/Sequential[layer2]/Sequential[layer1]/BatchNorm2d[norm01]\': model.norm10,\n    }\n    assert act_bn == ref_bn\n\n\ndef test_get_all_layers_by_type__for_multiple_type():\n    model = ModelForTest()\n    act_bn = get_all_modules_by_type(model, [\'ReLU\', \'AvgPool2d\'])\n    act_bn = OrderedDict((str(k), v) for k, v in act_bn.items())\n    ref_bn = [\n        \'ModelForTest/AvgPool2d[avgpool]\',\n        \'ModelForTest/Sequential[layer1]/ReLU[relu01]\',\n        \'ModelForTest/Sequential[layer2]/Sequential[layer1]/ReLU[relu01]\',\n        \'ModelForTest/Sequential[layer2]/ReLU[relu02]\']\n    assert list(act_bn.keys()) == ref_bn\n    assert isinstance(act_bn, OrderedDict)\n\n\ndef test_get_all_layers_by_type__for_not_exact_type():\n    model = ModelForTest()\n    l = get_all_modules_by_type(model, \'Avg\')\n    assert not l\n\n\ndef test_get_all_layers_by_type__for_subtype():\n    model = ModelForTest()\n    l = get_all_modules_by_type(model, \'AvgPool2d_dummy\')\n    assert not l\n\n\nIGNORED_SCOPES = [\n    (""single"", [\'ModelForTest/Sequential[layer2]/Sequential[layer1]/ReLU[relu01]\']),\n    (""multiple"", [\'ModelForTest/Sequential[layer2]/Sequential[layer1]/ReLU[relu01]\',\n                  \'ModelForTest/Sequential[layer2]/Conv2d[conv02]\']),\n    (""common"", [\'ModelForTest/Sequential[layer1]\'])\n    ]\n\n\n@pytest.mark.parametrize(\n    ""ignored_scopes"", [s[1] for s in IGNORED_SCOPES], ids=[s[0] for s in IGNORED_SCOPES]\n)\ndef test_get_all_layers_by_type__with_ignored_scope(ignored_scopes):\n    model = ModelForTest()\n\n    model_modules = set()\n    for _, module in model.named_modules():\n        model_modules.add(module.__class__.__name__)\n    model_modules = list(model_modules)\n\n    act_modules = get_all_modules_by_type(model, model_modules, ignored_scopes=ignored_scopes)\n\n    for module_scope, _ in act_modules.items():\n        for scope in ignored_scopes:\n            assert not str(module_scope).startswith(str(scope))\n\n\ndef test_set_module_by_node_name__for_non_nested_module():\n    model = ModelForTest()\n    new_module = ReLU()\n    set_module_by_node_name(model, \'ModelForTest/BatchNorm2d[bn1]\', new_module)\n    assert new_module == get_module_by_node_name(model, \'ModelForTest/ReLU[bn1]\')\n\n\ndef test_set_module_by_node_name__for_nested_module():\n    model = ModelForTest()\n    new_module = ReLU()\n    set_module_by_node_name(model, \'ModelForTest/Sequential[layer2]/Sequential[layer1]\', new_module)\n    assert new_module == get_module_by_node_name(model, \'ModelForTest/Sequential[layer2]/ReLU[layer1]\')\n\n\ndef test_get_all_nodes():\n    model = ModelForTest()\n    ref_list = [\n        \'ModelForTest/Conv2d[conv1]/conv2d\',\n        \'ModelForTest/BatchNorm2d[bn1]/batch_norm\',\n        \'ModelForTest/ReLU/\' + VersionAgnosticNames.RELU,\n        \'ModelForTest/\' + VersionAgnosticNames.RELU,\n        \'ModelForTest/BatchNorm2d[bn2]/batch_norm\',\n        \'ModelForTest/Sequential[layer2]/Sequential[layer1]/Conv2d[conv01]/conv2d\',\n        \'ModelForTest/Sequential[layer2]/Sequential[layer1]/BatchNorm2d[norm01]/batch_norm\',\n        \'ModelForTest/Sequential[layer2]/Sequential[layer1]/ReLU[relu01]/\' + VersionAgnosticNames.RELU,\n        \'ModelForTest/Sequential[layer2]/Sequential[layer1]/MaxPool2d[pool01]/max_pool2d\',\n        \'ModelForTest/Sequential[layer2]/Conv2d[conv02]/conv2d\',\n        \'ModelForTest/Sequential[layer2]/ReLU[relu02]/\' + VersionAgnosticNames.RELU,\n        \'ModelForTest/Sequential[layer2]/BatchNorm2d[norm02]/batch_norm\',\n        \'ModelForTest/Sequential[layer2]/MaxPool2d[pool02]/max_pool2d\',\n        \'ModelForTest/AvgPool2d[avgpool]/avg_pool2d\'\n    ]\n\n    act_list = get_all_node_names(model, (1, 1, 4, 4))\n    assert ref_list == act_list\n\n\ndef test_apply_by_node_name():\n    model = ModelForTest()\n    node_name = \'ModelForTest/BatchNorm2d[bn1]\'\n    bn1 = get_module_by_node_name(model, node_name)\n    bn1.weight.data.fill_(1)\n    assert bn1.weight == 1\n    apply_by_node_name(model, [node_name], command=lambda m: init.zeros_(m.weight))\n    assert bn1.weight == 0\n\n\ndef test_parse_node_name():\n    node_names = [""conv2d"", ""Conv2d[conv1]"", ""Conv2d[conv1]/RELU[relu]""]\n    ref_class_name = [""conv2d"", ""Conv2d"", ""RELU""]\n    ref_var_name = [None, ""conv1"", ""relu""]\n    ref_prefix = [None, None, ""Conv2d[conv1]""]\n    for i, node_name in enumerate(node_names):\n        prefix, class_name, var_name = parse_node_name(node_name)\n        assert class_name == ref_class_name[i]\n        assert var_name == ref_var_name[i]\n        assert prefix == ref_prefix[i]\n'"
pytorch_toolkit/nncf/tests/test_helpers.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom collections import namedtuple\nfrom typing import Dict\nfrom typing import Tuple\n\nimport numpy as np\nimport pytest\nimport torch\nfrom copy import deepcopy\nfrom functools import partial\nfrom torch import nn\nfrom torch.nn import Module\n\nfrom nncf.compression_method_api import CompressionAlgorithmController\nfrom nncf.config import Config\nfrom nncf.dynamic_graph.context import Scope\nfrom nncf.model_creation import create_compressed_model\nfrom nncf.layers import NNCF_MODULES_MAP\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.utils import get_all_modules_by_type, objwalk\n\n\ndef fill_conv_weight(conv, value):\n    conv.weight.data.fill_(value)\n    with torch.no_grad():\n        mask = torch.eye(conv.kernel_size[0])\n        conv.weight += mask\n\n\ndef fill_bias(module, value):\n    module.bias.data.fill_(value)\n\n\ndef fill_linear_weight(linear, value):\n    linear.weight.data.fill_(value)\n    with torch.no_grad():\n        n = min(linear.in_features, linear.out_features)\n        linear.weight[:n, :n] += torch.eye(n)\n\n\ndef create_conv(in_channels, out_channels, kernel_size, weight_init, bias_init):\n    conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n    fill_conv_weight(conv, weight_init)\n    fill_bias(conv, bias_init)\n    return conv\n\n\nclass BasicConvTestModel(nn.Module):\n    def __init__(self, in_channels=1, out_channels=2, kernel_size=2, weight_init=-1, bias_init=-2):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.weight_init = weight_init\n        self.bias_init = bias_init\n        self.conv = create_conv(in_channels, out_channels, kernel_size, weight_init, bias_init)\n\n    @staticmethod\n    def default_weight():\n        return torch.tensor([[[[0., -1.],\n                               [-1., 0.]]], [[[0., -1.],\n                                              [-1., 0.]]]])\n\n    @staticmethod\n    def default_bias():\n        return torch.tensor([-2., -2])\n\n    def forward(self, x):\n        return self.conv(x)\n\n    @property\n    def weights_num(self):\n        return self.out_channels * self.kernel_size ** 2\n\n    @property\n    def bias_num(self):\n        return self.kernel_size\n\n    @property\n    def nz_weights_num(self):\n        return self.kernel_size * self.out_channels\n\n    @property\n    def nz_bias_num(self):\n        return self.kernel_size\n\n\ndef test_basic_model_has_expected_params():\n    model = BasicConvTestModel()\n    act_weights = model.conv.weight.data\n    ref_weights = BasicConvTestModel.default_weight()\n    act_bias = model.conv.bias.data\n    ref_bias = BasicConvTestModel.default_bias()\n\n    check_equal(act_bias, ref_bias)\n    check_equal(act_weights, ref_weights)\n\n    assert act_weights.nonzero().size(0) == model.nz_weights_num\n    assert act_bias.nonzero().size(0) == model.nz_bias_num\n    assert act_weights.numel() == model.weights_num\n    assert act_bias.numel() == model.bias_num\n\n\ndef test_basic_model_is_valid():\n    model = BasicConvTestModel()\n    input_ = torch.ones([1, 1, 4, 4])\n    ref_output = torch.ones((1, 2, 3, 3)) * (-4)\n    act_output = model(input_)\n    check_equal(ref_output, act_output)\n\n\nclass TwoConvTestModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = []\n        self.features.append(nn.Sequential(create_conv(1, 2, 2, -1, -2)))\n        self.features.append(nn.Sequential(create_conv(2, 1, 3, 0, 0)))\n        self.features = nn.Sequential(*self.features)\n\n    def forward(self, x):\n        return self.features(x)\n\n    @property\n    def weights_num(self):\n        return 8 + 18\n\n    @property\n    def bias_num(self):\n        return 2 + 1\n\n    @property\n    def nz_weights_num(self):\n        return 4 + 6\n\n    @property\n    def nz_bias_num(self):\n        return 2\n\n\ndef test_two_conv_model_has_expected_params():\n    model = TwoConvTestModel()\n    act_weights_1 = model.features[0][0].weight.data\n    act_weights_2 = model.features[1][0].weight.data\n    act_bias_1 = model.features[0][0].bias.data\n    act_bias_2 = model.features[1][0].bias.data\n\n    ref_weights_1 = BasicConvTestModel.default_weight()\n    channel = torch.eye(3, 3).reshape([1, 1, 3, 3])\n    ref_weights_2 = torch.cat((channel, channel), 1)\n\n    check_equal(act_weights_1, ref_weights_1)\n    check_equal(act_weights_2, ref_weights_2)\n\n    check_equal(act_bias_1, BasicConvTestModel.default_bias())\n    check_equal(act_bias_2, torch.tensor([0]))\n\n    assert act_weights_1.nonzero().size(0) + act_weights_2.nonzero().size(0) == model.nz_weights_num\n    assert act_bias_1.nonzero().size(0) + act_bias_2.nonzero().size(0) == model.nz_bias_num\n    assert act_weights_1.numel() + act_weights_2.numel() == model.weights_num\n    assert act_bias_1.numel() + act_bias_2.numel() == model.bias_num\n\n\ndef test_two_conv_model_is_valid():\n    model = TwoConvTestModel()\n    input_ = torch.ones([1, 1, 4, 4])\n    ref_output = torch.tensor([-24])\n    act_output = model(input_)\n    check_equal(ref_output, act_output)\n\n\ndef get_empty_config(model_size=4, input_sample_size=(1, 1, 4, 4)):\n    config = Config()\n    config.update({\n        ""model"": ""basic_sparse_conv"",\n        ""model_size"": model_size,\n        ""input_info"":\n            {\n                ""sample_size"": input_sample_size,\n            },\n    })\n    return config\n\n\ndef get_grads(variables):\n    return [var.grad.clone() for var in variables]\n\n\ndef check_equal(test, reference, rtol=1e-4):\n    for i, (x, y) in enumerate(zip(test, reference)):\n        y = y.cpu().detach().numpy()\n        np.testing.assert_allclose(x, y, rtol=rtol, err_msg=""Index: {}"".format(i))\n\n\ndef create_compressed_model_and_algo_for_test(model: NNCFNetwork, config) -> Tuple[\n        NNCFNetwork, CompressionAlgorithmController]:\n    algo, model = create_compressed_model(model, config, dump_graphs=False)\n    return model, algo\n\n\nclass MockModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.field = nn.Linear(1, 1)\n\n    def forward(self, *input_, **kwargs):\n        return None\n\n\ndef check_correct_nncf_modules_replacement(model: NNCFNetwork, compressed_model: NNCFNetwork) -> Tuple[\n        Dict[Scope, Module],\n        Dict[Scope, Module]]:\n    """"""\n    Check that all convolutions in model was replaced by NNCF convolution.\n    :param model: original model\n    :param compressed_model: compressed model\n    :return: list of all convolutions in  original model and list of all NNCF convolutions from compressed model\n    """"""\n    NNCF_MODULES_REVERSED_MAP = {value: key for key, value in NNCF_MODULES_MAP.items()}\n    original_modules = get_all_modules_by_type(model, list(NNCF_MODULES_MAP.values()))\n    nncf_modules = get_all_modules_by_type(compressed_model.get_nncf_wrapped_model(),\n                                           list(NNCF_MODULES_MAP.keys()))\n    assert len(original_modules) == len(nncf_modules)\n    print(original_modules, nncf_modules)\n    for scope in original_modules.keys():\n        sparse_scope = deepcopy(scope)\n        elt = sparse_scope.pop()  # type: ScopeElement\n        elt.calling_module_class_name = NNCF_MODULES_REVERSED_MAP[elt.calling_module_class_name]\n        sparse_scope.push(elt)\n        print(sparse_scope, nncf_modules)\n        assert sparse_scope in nncf_modules\n    return original_modules, nncf_modules\n\n\nclass ObjwalkTestClass:\n    def __init__(self, field: int):\n        self.field = field\n\n    def member_fn(self, val):\n        return ObjwalkTestClass(self.field + 1)\n\n    def __eq__(self, other):\n        return self.__dict__ == other.__dict__\n\nNamedTuple = namedtuple(""NamedTuple"", (""field1"", ""field2""))\n\nOBJWALK_INIT_VAL = 0\nOBJWALK_REF_VAL = OBJWALK_INIT_VAL + 1\nTEST_VS_REF_OBJECTS_TO_WALK = [\n    (0,\n     0),\n\n    (""foo"",\n     ""foo""),\n\n    (ObjwalkTestClass(OBJWALK_INIT_VAL),\n     ObjwalkTestClass(OBJWALK_REF_VAL)),\n\n    ([0, ObjwalkTestClass(OBJWALK_INIT_VAL), ""bar""],\n     [0, ObjwalkTestClass(OBJWALK_REF_VAL), ""bar""]),\n\n    ([ObjwalkTestClass(OBJWALK_INIT_VAL), ObjwalkTestClass(OBJWALK_INIT_VAL), (5, 8)],\n     [ObjwalkTestClass(OBJWALK_REF_VAL), ObjwalkTestClass(OBJWALK_REF_VAL), (5, 8)]),\n\n    (\n        {\n            ""obj1"": ObjwalkTestClass(OBJWALK_INIT_VAL),\n            ""obj2"": ObjwalkTestClass(OBJWALK_INIT_VAL)\n        },\n        {\n            ""obj1"": ObjwalkTestClass(OBJWALK_REF_VAL),\n            ""obj2"": ObjwalkTestClass(OBJWALK_REF_VAL)\n        }\n    ),\n\n    ((ObjwalkTestClass(OBJWALK_INIT_VAL), 42),\n     (ObjwalkTestClass(OBJWALK_REF_VAL), 42)),\n\n    ([(ObjwalkTestClass(OBJWALK_INIT_VAL), 8), [ObjwalkTestClass(OBJWALK_INIT_VAL), ""foo""],\n      {""bar"": ObjwalkTestClass(OBJWALK_INIT_VAL),\n       ""baz"": (ObjwalkTestClass(OBJWALK_INIT_VAL), ObjwalkTestClass(OBJWALK_INIT_VAL)),\n       ""xyzzy"": {1337: ObjwalkTestClass(OBJWALK_INIT_VAL),\n                 31337: ObjwalkTestClass(OBJWALK_INIT_VAL)}}],\n     [(ObjwalkTestClass(OBJWALK_REF_VAL), 8), [ObjwalkTestClass(OBJWALK_REF_VAL), ""foo""],\n      {""bar"": ObjwalkTestClass(OBJWALK_REF_VAL),\n       ""baz"": (ObjwalkTestClass(OBJWALK_REF_VAL), ObjwalkTestClass(OBJWALK_REF_VAL)),\n       ""xyzzy"": {1337: ObjwalkTestClass(OBJWALK_REF_VAL),\n                 31337: ObjwalkTestClass(OBJWALK_REF_VAL)}}]\n    ),\n    (\n        (0, NamedTuple(field1=ObjwalkTestClass(OBJWALK_INIT_VAL), field2=-5.3), ""bar""),\n        (0, NamedTuple(field1=ObjwalkTestClass(OBJWALK_REF_VAL), field2=-5.3), ""bar""),\n    )\n]\n\n\n@pytest.fixture(name=""objwalk_objects"", params=TEST_VS_REF_OBJECTS_TO_WALK)\ndef objwalk_objects_(request):\n    return request.param\n\n\ndef test_objwalk(objwalk_objects):\n    start_obj = objwalk_objects[0]\n    ref_obj = objwalk_objects[1]\n\n    def is_target_class(obj):\n        return isinstance(obj, ObjwalkTestClass)\n\n    fn_to_apply = partial(ObjwalkTestClass.member_fn, val=OBJWALK_REF_VAL)\n\n    test_obj = objwalk(start_obj, is_target_class, fn_to_apply)\n\n    assert test_obj == ref_obj\n'"
pytorch_toolkit/nncf/tests/test_install.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport subprocess\nimport sys\nimport pytest\nimport os\n\n\ndef test_install(install_type, tmp_path):\n    if install_type is None:\n        pytest.skip(""Please specify type of installation"")\n    tests_dir = os.path.dirname(__file__)\n    cur_dir = os.path.dirname(tests_dir)\n    install_path = str(tmp_path.joinpath(""install""))\n    if sys.version_info[:2] == (3, 5):\n        subprocess.call(""virtualenv -ppython3.5 {}"".format(install_path), shell=True)\n    else:\n        subprocess.call(""{} -m venv {}"".format(sys.executable, install_path), shell=True)\n    python_executable_with_venv = str("". {0}/bin/activate && {0}/bin/python"".format(install_path))\n    if install_type == ""CPU"":\n        subprocess.run(\n            ""{} {}/setup.py develop --cpu-only"".format(python_executable_with_venv, cur_dir), check=True, shell=True)\n        subprocess.run(\n            ""{} {}/install_checks.py cpu"".format(python_executable_with_venv, tests_dir), check=True, shell=True)\n    else:\n        subprocess.run(\n            ""{} {}/setup.py develop"".format(python_executable_with_venv, cur_dir), check=True, shell=True)\n        subprocess.run(\n            ""{} {}/install_checks.py cuda"".format(python_executable_with_venv, tests_dir), check=True, shell=True)\n'"
pytorch_toolkit/nncf/tests/test_load_model.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\n\nimport pytest\nimport torch\n\nfrom nncf.checkpoint_loading import load_state\nfrom nncf.utils import get_all_modules_by_type\n\nfrom examples.common.model_loader import load_model\nfrom tests.quantization.test_functions import check_equal\nfrom tests.test_helpers import BasicConvTestModel\n\n\n@pytest.mark.parametrize((\'model_name\', \'ceil_mode\'),\n                         ((\'squeezenet1_1\', True), (\'squeezenet1_1_custom\', False)), ids=(\'orig\', \'custom\'))\ndef test_load_sq_11_ceil_mode_is_ok(model_name, ceil_mode):\n    model = load_model(model_name, pretrained=False)\n    layers = get_all_modules_by_type(model, \'MaxPool2d\')\n    for _, pool_layer in layers.items():\n        assert pool_layer.ceil_mode == ceil_mode\n\n\ndef test_export_sq_11_custom_is_ok(tmp_path):\n    test_path = str(tmp_path.joinpath(""test.onnx""))\n    model = load_model(\'squeezenet1_1_custom\', pretrained=False)\n    dummy_input = torch.randn(1, 3, 224, 224)\n    torch.onnx.export(model, dummy_input, test_path, verbose=False)\n    os.remove(test_path)\n\n\ndef test_load_state_skips_not_matched_params__from_larger_to_smaller():\n    ref_weights = BasicConvTestModel.default_weight()\n    ref_bias = BasicConvTestModel.default_bias()\n    model_save = BasicConvTestModel(out_channels=1, weight_init=2, bias_init=2)\n    model_load = BasicConvTestModel(out_channels=2)\n\n    num_loaded = load_state(model_load, model_save.state_dict())\n\n    act_bias = model_load.conv.bias.data\n    act_weights = model_load.conv.weight.data\n    assert num_loaded == 0\n    check_equal(act_bias, ref_bias)\n    check_equal(act_weights, ref_weights)\n\n\ndef test_load_state_skips_not_matched_params__from_smaller_to_larger():\n    ref_weights = torch.tensor([[[[3, 2],\n                                  [2, 3]]]])\n    ref_bias = torch.tensor([2.])\n    model_save = BasicConvTestModel(out_channels=2)\n    model_load = BasicConvTestModel(out_channels=1, weight_init=2, bias_init=2)\n\n    num_loaded = load_state(model_load, model_save.state_dict())\n\n    assert num_loaded == 0\n    act_bias = model_load.conv.bias.data\n    act_weights = model_load.conv.weight.data\n    check_equal(act_bias, ref_bias)\n    check_equal(act_weights, ref_weights)\n'"
pytorch_toolkit/nncf/tests/test_matcher.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport networkx as nx\n\nfrom nncf.dynamic_graph.graph_matching import NodeExpression as N, search_all\n\n\ndef add_nodes(graph, types, nodes=None):\n    if nodes is None:\n        nodes = list(range(1, len(types) + 1))\n    for node, type_ in zip(nodes, types):\n        graph.add_node(node, type=type_)\n\n\ndef test_simple():\n    g = nx.DiGraph()\n    add_nodes(g, [\'a\', \'b\', \'c\', \'a\'])\n    g.add_edges_from([(1, 2), (2, 3), (3, 4)])\n\n    ex = N(\'b\') + N(\'c\')\n\n    matches = search_all(g, ex)\n    assert matches == [[2, 3]]\n\n\ndef test_two_matched():\n    g = nx.DiGraph()\n    add_nodes(g, [\'a\', \'b\', \'c\', \'a\', \'b\', \'c\'])\n    g.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 5), (5, 6)])\n\n    ex = N(\'b\') + N(\'c\')\n\n    matches = search_all(g, ex)\n    assert matches == [[2, 3], [5, 6]]\n\n\ndef test_graph_branching():\n    g = nx.DiGraph()\n    add_nodes(g, [\'a\', \'b\', \'a\', \'c\'])\n    g.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4)])\n\n    ex = N(\'a\') + N(\'b\')\n\n    matches = search_all(g, ex)\n    assert matches == [[1, 2]]\n\n\ndef test_graph_branching_other_order():\n    g = nx.DiGraph()\n    add_nodes(g, [\'a\', \'a\', \'b\', \'c\'])\n\n    g.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4)])\n\n    ex = N(\'a\') + N(\'b\')\n\n    matches = search_all(g, ex)\n    assert matches == [[1, 3]]\n\n\ndef test_alternating():\n    g = nx.DiGraph()\n    add_nodes(g, [\'a\', \'b\'])\n\n    g.add_edges_from([(1, 2)])\n\n    ex = N(\'a\') + (N(\'a\') | N(\'b\'))\n\n    matches = search_all(g, ex)\n    assert matches == [[1, 2]]\n\n\ndef test_alternating_longest():\n    g = nx.DiGraph()\n    #   b c\n    # a     d\n    #    b\n    add_nodes(g, [\'a\', \'b\', \'c\', \'b\', \'d\'])\n    g.add_edges_from([(1, 2), (2, 3), (3, 5), (1, 4), (4, 5)])\n\n    ex = N(\'a\') + (N(\'b\') | N(\'b\') + N(\'c\'))\n    ex2 = N(\'a\') + (N(\'b\') + N(\'c\') | N(\'b\'))\n\n    matches = search_all(g, ex)\n    matches2 = search_all(g, ex2)\n\n    assert matches2 == matches == [[1, 2, 3]]\n\n\ndef test_branching_expression():\n    g = nx.DiGraph()\n    #   b\n    # a   d\n    #   c\n    add_nodes(g, [\'a\', \'b\', \'c\', \'d\'])\n    g.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4)])\n    c_ = (N(\'b\') & N(\'c\'))\n    n = N(\'a\')\n    node_expression = N(\'d\')\n    ex = n + c_ + node_expression\n\n    ex = N(\'a\') + (N(\'b\') & N(\'c\')) + N(\'d\')\n\n    matches = search_all(g, ex)\n    assert matches == [[1, 2, 3, 4]]\n\n\ndef test_branching_expression3():\n    g = nx.DiGraph()\n    #   b\n    # a   d\n    #   c\n    add_nodes(g, [\'a\', \'b\', \'c\', \'d\'])\n    g.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4)])\n    c_ = N(\'b\') & (N(\'e\') | N(\'c\'))\n    n = N(\'a\')\n    node_expression = N(\'d\')\n    ex = n + c_ + node_expression\n\n    ex = N(\'a\') + (N(\'b\') & N(\'c\')) + N(\'d\')\n\n    matches = search_all(g, ex)\n    assert matches == [[1, 2, 3, 4]]\n\n\ndef test_branching_expression2():\n    g = nx.DiGraph()\n    #   b\n    # a e  d\n    #   c\n    add_nodes(g, [\'a\', \'b\', \'c\', \'d\', \'e\'])\n    g.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4), (1, 5), (5, 4)])\n    c_ = (N(\'b\') & N(\'c\') & N(\'e\'))\n    n = N(\'a\')\n    node_expression = N(\'d\')\n    ex = n + c_ + node_expression\n\n    matches = search_all(g, ex)\n    assert matches == [[1, 2, 3, 5, 4]]\n'"
pytorch_toolkit/nncf/tests/test_nncf_network.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport itertools\nfrom typing import List\n\nimport networkx as nx\nimport pytest\nimport torch\nfrom copy import deepcopy\nfrom torch import nn\n\nfrom nncf.dynamic_graph.context import Scope\nfrom nncf.dynamic_graph.graph import InputAgnosticOperationExecutionContext, NNCFGraph, OperationExecutionContext\nfrom nncf.dynamic_graph.graph_builder import ModelInputInfo\nfrom nncf.dynamic_graph.operator_metatypes import NoopMetatype\nfrom nncf.dynamic_graph.patch_pytorch import MODEL_INPUT_OP_NAME\nfrom nncf.module_operations import BaseOp\nfrom nncf.nncf_network import NNCFNetwork, InsertionCommand, InsertionPoint, InsertionType, OperationPriority, \\\n    InsertionPointGraph, InsertionPointGraphNodeType\nfrom tests.test_helpers import TwoConvTestModel, BasicConvTestModel, check_correct_nncf_modules_replacement\n\n\ndef test_disable_shape_matching():\n    class MatMulModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.dummy_param = torch.nn.Parameter(torch.ones([1]))\n\n        def forward(self, inputs):\n            half1, half2 = torch.chunk(inputs, 2, dim=2)\n            return torch.bmm(half1, half2.transpose(1, 2))\n\n    model = MatMulModel()\n\n    input_shape_1 = (3, 32, 32)\n    input_shape_2 = (4, 64, 64)\n\n    qnet_no_shape = NNCFNetwork(deepcopy(model), input_infos=[ModelInputInfo(input_shape_1), ],\n                                scopes_without_shape_matching=[\'MatMulModel\'])  # type: NNCFNetwork\n    _ = qnet_no_shape(torch.zeros(*input_shape_1))\n    graph_1 = deepcopy(qnet_no_shape.get_graph())\n\n    _ = qnet_no_shape(torch.zeros(*input_shape_2))\n    graph_2 = deepcopy(qnet_no_shape.get_graph())\n\n    keys_1 = list(graph_1.get_all_node_keys())\n    keys_2 = list(graph_2.get_all_node_keys())\n    assert len(keys_1) == 2  # 1 input node + 1 operation node\n    assert keys_1 == keys_2\n\n\n    qnet = NNCFNetwork(model, input_infos=[ModelInputInfo(input_shape_1), ])  # type: NNCFNetwork\n    _ = qnet(torch.zeros(*input_shape_1))\n    _ = qnet(torch.zeros(*input_shape_2))\n    # The second forward run should have led to an increase in registered node counts\n    # since disable_shape_matching was False and the network was run with a different\n    # shape of input tensor\n    assert qnet.get_graph().get_nodes_count() > graph_1.get_nodes_count()\n\n\ndef test_check_correct_modules_replacement():\n    model = TwoConvTestModel()\n    nncf_model = NNCFNetwork(TwoConvTestModel(), input_infos=[ModelInputInfo([1, 1, 4, 4])])  # type: NNCFNetwork\n\n    _, nncf_modules = check_correct_nncf_modules_replacement(model, nncf_model)\n    assert set(nncf_modules) == set(nncf_model.get_nncf_modules())\n\n\n# pylint: disable=protected-access\ndef test_find_node_in_nx_graph_by_scope():\n    model = TwoConvTestModel()\n    nncf_model = NNCFNetwork(deepcopy(model), input_infos=[ModelInputInfo([1, 1, 4, 4])])  # type: NNCFNetwork\n    nncf_graph = nncf_model.get_original_graph()\n\n    # Valid scopes should be successfully found\n    valid_nncf_modules = nncf_model.get_nncf_modules()\n    nodes_list = list(nncf_graph._nx_graph.nodes)\n    for module_scope, _ in valid_nncf_modules.items():\n        graph_node = nncf_graph.find_node_in_nx_graph_by_scope(module_scope)\n        assert graph_node is not None\n        assert isinstance(graph_node, dict)\n        assert graph_node[\'key\'] in nodes_list\n\n    fake_model = BasicConvTestModel()\n    fake_nncf_model = NNCFNetwork(deepcopy(fake_model), input_infos=[ModelInputInfo([1, 1, 4, 4])])\n\n    # Not valid scopes shouldn\'t be found\n    fake_nncf_modules = fake_nncf_model.get_nncf_modules()\n    for module_scope, _ in fake_nncf_modules.items():\n        graph_node = nncf_graph.find_node_in_nx_graph_by_scope(module_scope)\n        assert graph_node is None\n\n\nclass InsertionPointTestModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 1, 1, 1)\n        self.linear_wts = nn.Parameter(torch.FloatTensor(size=(100, 100)))\n        self.conv2 = nn.Conv2d(1, 1, 1, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_):\n        x = self.conv1(input_)\n        x = x.flatten()\n        x = nn.functional.linear(x, self.linear_wts)\n        x = x.reshape((1, 1, 10, 10))\n        x = self.conv2(x)\n        x = self.relu(x)\n        return x\n\n\nclass TestInsertionCommands:\n    @pytest.fixture()\n    def setup(self):\n        self.compressed_model = NNCFNetwork(InsertionPointTestModel(),\n                                            [ModelInputInfo((1, 1, 10, 10))])  # type: NNCFNetwork\n\n    conv1_module_scope = Scope.from_str(\'InsertionPointTestModel/NNCFConv2d[conv1]\')\n    conv1_module_context = InputAgnosticOperationExecutionContext(\'\', conv1_module_scope, 0)\n    point_for_conv1_weights = InsertionPoint(ia_op_exec_context=conv1_module_context,\n                                             insertion_type=InsertionType.NNCF_MODULE_PRE_OP)\n    point_for_conv1_inputs = InsertionPoint(ia_op_exec_context=conv1_module_context,\n                                            insertion_type=InsertionType.NNCF_MODULE_PRE_OP)\n    point_for_conv1_activations = InsertionPoint(ia_op_exec_context=conv1_module_context,\n                                                 insertion_type=InsertionType.NNCF_MODULE_POST_OP)\n\n    conv2_module_scope = Scope.from_str(\'InsertionPointTestModel/NNCFConv2d[conv2]\')\n    conv2_module_context = InputAgnosticOperationExecutionContext(\'\', conv2_module_scope, 0)\n    point_for_conv2_weights = InsertionPoint(ia_op_exec_context=conv2_module_context,\n                                             insertion_type=InsertionType.NNCF_MODULE_PRE_OP)\n    point_for_conv2_inputs = InsertionPoint(ia_op_exec_context=conv2_module_context,\n                                            insertion_type=InsertionType.NNCF_MODULE_PRE_OP)\n    point_for_conv2_activations = InsertionPoint(ia_op_exec_context=conv2_module_context,\n                                                 insertion_type=InsertionType.NNCF_MODULE_POST_OP)\n\n    linear_op_scope = Scope.from_str(\'InsertionPointTestModel/linear_0\')\n    linear_op_context = InputAgnosticOperationExecutionContext(\'linear\',\n                                                               linear_op_scope,\n                                                               0)\n    point_for_linear_weight_input = InsertionPoint(ia_op_exec_context=linear_op_context,\n                                                   insertion_type=InsertionType.OPERATOR_PRE_HOOK)\n    point_for_linear_activation = InsertionPoint(ia_op_exec_context=linear_op_context,\n                                                 insertion_type=InsertionType.OPERATOR_POST_HOOK)\n\n    relu_op_scope = Scope.from_str(\'InsertionPointTestModel/ReLU[relu]/relu\')\n    relu_op_context = InputAgnosticOperationExecutionContext(\'relu\',\n                                                             relu_op_scope,\n                                                             0)\n    point_for_relu_inputs = InsertionPoint(ia_op_exec_context=relu_op_context,\n                                           insertion_type=InsertionType.OPERATOR_PRE_HOOK)\n    point_for_relu_activations = InsertionPoint(ia_op_exec_context=relu_op_context,\n                                                insertion_type=InsertionType.OPERATOR_POST_HOOK)\n\n    available_points = [point_for_conv1_weights,\n                        point_for_conv2_weights,\n                        point_for_conv1_inputs,\n                        point_for_conv2_inputs,\n                        point_for_conv1_activations,\n                        point_for_conv2_activations,\n                        point_for_linear_activation,\n                        point_for_linear_weight_input,\n                        point_for_relu_activations,\n                        point_for_relu_inputs]\n\n    @pytest.mark.parametrize(""insertion_point"", available_points)\n    def test_single_insertions(self, setup, insertion_point):\n        if insertion_point.insertion_type in [InsertionType.OPERATOR_PRE_HOOK, InsertionType.OPERATOR_POST_HOOK]:\n            hook = lambda x: x\n        else:\n            hook = BaseOp(lambda x: x)\n\n        command = InsertionCommand(insertion_point, hook)\n        self.compressed_model.register_insertion_command(command)\n        self.compressed_model.commit_compression_changes()\n\n        #pylint:disable=protected-access\n        if insertion_point.insertion_type == InsertionType.OPERATOR_PRE_HOOK:\n            ctx = self.compressed_model.get_tracing_context()\n            assert ctx._pre_hooks[command.insertion_point.ia_op_exec_context][0] is hook\n        if insertion_point.insertion_type == InsertionType.OPERATOR_POST_HOOK:\n            ctx = self.compressed_model.get_tracing_context()\n            assert ctx._post_hooks[command.insertion_point.ia_op_exec_context][0] is hook\n        if insertion_point.insertion_type == InsertionType.NNCF_MODULE_PRE_OP:\n            module = self.compressed_model.get_module_by_scope(\n                command.insertion_point.ia_op_exec_context.scope_in_model)\n            assert module.pre_ops[""0""] is hook\n\n        if insertion_point.insertion_type == InsertionType.NNCF_MODULE_POST_OP:\n            module = self.compressed_model.get_module_by_scope(\n                command.insertion_point.ia_op_exec_context.scope_in_model)\n            assert module.post_ops[""0""] is hook\n\n    priority_types = [""same"", ""different""]\n    insertion_types = InsertionType\n    priority_test_cases = list(itertools.product(priority_types, insertion_types))\n\n    @staticmethod\n    def check_order(iterable1: List, iterable2: List, ordering: List):\n        for idx, order in enumerate(ordering):\n            assert iterable1[idx] is iterable2[order]\n\n    # pylint:disable=undefined-variable\n    @pytest.mark.parametrize(""case"", priority_test_cases, ids=[x[1].name + \'-\' + x[0] for x in priority_test_cases])\n    def test_priority(self, case, setup):\n        #pylint:disable=too-many-branches\n        priority_type = case[0]\n        insertion_type = case[1]\n        if insertion_type in [InsertionType.NNCF_MODULE_PRE_OP, InsertionType.NNCF_MODULE_POST_OP]:\n            hook1 = BaseOp(lambda x: x)\n            hook2 = BaseOp(lambda x: 2 * x)\n            hook3 = BaseOp(lambda x: 3 * x)\n        else:\n            hook1 = lambda x: x\n            hook2 = lambda x: 2 * x\n            hook3 = lambda x: 3 * x\n\n        if insertion_type == InsertionType.NNCF_MODULE_PRE_OP:\n            point = self.point_for_conv2_weights\n        elif insertion_type == InsertionType.NNCF_MODULE_POST_OP:\n            point = self.point_for_conv1_activations\n        elif insertion_type == InsertionType.OPERATOR_PRE_HOOK:\n            point = self.point_for_linear_weight_input\n        elif insertion_type == InsertionType.OPERATOR_POST_HOOK:\n            point = self.point_for_relu_activations\n\n        if priority_type == ""same"":\n            # Same-priority commands will be executed in registration order\n            command1 = InsertionCommand(point, hook1, OperationPriority.DEFAULT_PRIORITY)\n            command2 = InsertionCommand(point, hook2, OperationPriority.DEFAULT_PRIORITY)\n            command3 = InsertionCommand(point, hook3, OperationPriority.DEFAULT_PRIORITY)\n        else:\n            # Prioritized commands will be executed in ascending priority order\n            command1 = InsertionCommand(point, hook1, OperationPriority.SPARSIFICATION_PRIORITY)\n            command2 = InsertionCommand(point, hook2, OperationPriority.QUANTIZATION_PRIORITY)\n            command3 = InsertionCommand(point, hook3, OperationPriority.DEFAULT_PRIORITY)\n\n        self.compressed_model.register_insertion_command(command1)\n        self.compressed_model.register_insertion_command(command2)\n        self.compressed_model.register_insertion_command(command3)\n        self.compressed_model.commit_compression_changes()\n\n        hook_list = [hook1, hook2, hook3]\n\n        if priority_type == ""same"":\n            order = [0, 1, 2]\n        elif priority_type == ""different"":\n            order = [2, 0, 1]\n\n        #pylint:disable=protected-access\n        if insertion_type == InsertionType.OPERATOR_PRE_HOOK:\n            ctx = self.compressed_model.get_tracing_context()\n            self.check_order(ctx._pre_hooks[point.ia_op_exec_context], hook_list, order)\n        if insertion_type == InsertionType.OPERATOR_POST_HOOK:\n            ctx = self.compressed_model.get_tracing_context()\n            self.check_order(ctx._post_hooks[point.ia_op_exec_context], hook_list, order)\n\n        if insertion_type == InsertionType.NNCF_MODULE_PRE_OP:\n            module = self.compressed_model.get_module_by_scope(point.ia_op_exec_context.scope_in_model)\n            # Works because Pytorch ModuleDict is ordered\n            self.check_order(list(module.pre_ops.values()), hook_list, order)\n\n        if insertion_type == InsertionType.NNCF_MODULE_POST_OP:\n            module = self.compressed_model.get_module_by_scope(point.ia_op_exec_context.scope_in_model)\n            # Works because Pytorch ModuleDict is ordered\n            self.check_order(list(module.post_ops.values()), hook_list, order)\n\n\ndef get_two_branch_mock_model_graph() -> nx.DiGraph:\n    mock_node_attrs = get_mock_nncf_node_attrs()\n    mock_graph = nx.DiGraph()\n\n    #   (A)\n    #    |\n    #   (B)\n    #  /   \\\n    # (C)   (D)\n    # |     |\n    # (E)   |\n    #  \\   /\n    #   (F)\n    #    |\n    #   (G)\n    #    |\n    #   (H)\n\n    node_keys = [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\']\n    for node_key in node_keys:\n        mock_graph.add_node(node_key, **mock_node_attrs)\n\n    mock_graph.add_edges_from([(\'A\', \'B\'), (\'B\', \'C\'), (\'B\', \'D\'), (\'C\', \'E\'), (\'E\', \'F\'),\n                               (\'D\', \'F\'), (\'F\', \'G\'), (\'G\', \'H\')])\n    return mock_graph\n\n\nMOCK_OPERATOR_NAME = ""conv_transpose2d""\n\n\ndef get_mock_nncf_node_attrs():\n    return {\n        NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR: OperationExecutionContext(MOCK_OPERATOR_NAME,\n                                                                       Scope(),\n                                                                       0,\n                                                                       [None])\n    }\n\n\nclass TestInsertionPointGraph:\n    def test_insertion_point_setup(self, tmp_path):\n        # TODO: Change testing premises when module pre/post-op hooks and input/output nodes\n        # are correctly handled\n        mock_graph = get_two_branch_mock_model_graph()\n\n        ip_graph = InsertionPointGraph(mock_graph)\n        nx.drawing.nx_pydot.write_dot(ip_graph, str(tmp_path / ""test_ip_graph.dot""))\n\n        ref_node_len = 3 * len(mock_graph.nodes)  # 2 additional nodes per each operator node\n        ref_edge_len = 3 * len(mock_graph.edges)\n\n        assert len(ip_graph.nodes) == ref_node_len\n        assert len(ip_graph.edges) == ref_edge_len\n\n        for node_key, node in mock_graph.nodes.items():\n            ip_graph_op_node = ip_graph.nodes[node_key]\n            assert ip_graph_op_node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR\n            preds = list(ip_graph.predecessors(node_key))\n            succs = list(ip_graph.successors(node_key))\n            assert len(preds) == 1\n            assert len(succs) == 1\n            pre_hook_ip_node_key = preds[0]\n            post_hook_ip_node_key = succs[0]\n            pre_hook_ip_node = ip_graph.nodes[preds[0]]\n            post_hook_ip_node = ip_graph.nodes[succs[0]]\n            pre_hook_ip_node_type = pre_hook_ip_node[InsertionPointGraph.NODE_TYPE_NODE_ATTR]\n            post_hook_ip_node_type = post_hook_ip_node[InsertionPointGraph.NODE_TYPE_NODE_ATTR]\n            assert pre_hook_ip_node_type == InsertionPointGraphNodeType.INSERTION_POINT\n            assert post_hook_ip_node_type == InsertionPointGraphNodeType.INSERTION_POINT\n            ref_associated_ip_node_keys_set = {pre_hook_ip_node_key, post_hook_ip_node_key}\n            assert ref_associated_ip_node_keys_set == ip_graph_op_node[\n                InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR]\n            original_neighbours = mock_graph.neighbors(node_key)\n            for neighbour in original_neighbours:\n                # IP node insertion should not disrupt the graph superstructure\n                ip_graph_paths = list(nx.all_simple_paths(ip_graph, node_key, neighbour))\n                for path in ip_graph_paths:\n                    path = path[1:-1]\n                    for path_node_key in path:\n                        node = ip_graph.nodes[path_node_key]\n                        node_type = node[InsertionPointGraph.NODE_TYPE_NODE_ATTR]\n                        assert node_type == InsertionPointGraphNodeType.INSERTION_POINT\n\n        for node_key, node in ip_graph.nodes.items():\n            preds = list(ip_graph.predecessors(node_key))\n            succs = list(ip_graph.successors(node_key))\n            assert len(preds) != 0 or len(succs) != 0\n\n        for from_node_key, to_node_key in ip_graph.edges.keys():\n            assert from_node_key in ip_graph.nodes\n            assert to_node_key in ip_graph.nodes\n\n    def test_insertion_point_data_in_ip_nodes(self):\n        # TODO: extend for modules\n        mock_graph = nx.DiGraph()\n        ref_op_exec_context = OperationExecutionContext(""baz"",\n                                                        Scope.from_str(""Test/Scope[foo]/bar""),\n                                                        0,\n                                                        [None])\n        node_attrs = {\n            NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR: ref_op_exec_context\n        }\n\n        node_key = 0\n        mock_graph.add_node(node_key, **node_attrs)\n\n        ip_graph = InsertionPointGraph(mock_graph)\n\n        for node_key in mock_graph.nodes.keys():\n            preds = list(ip_graph.predecessors(node_key))\n            succs = list(ip_graph.successors(node_key))\n            pre_hook_ip_node = ip_graph.nodes[preds[0]]\n            post_hook_ip_node = ip_graph.nodes[succs[0]]\n\n            pre_hook_ip = pre_hook_ip_node[InsertionPointGraph.INSERTION_POINT_DATA_NODE_ATTR]\n            post_hook_ip = post_hook_ip_node[InsertionPointGraph.INSERTION_POINT_DATA_NODE_ATTR]\n            assert pre_hook_ip.insertion_type == InsertionType.OPERATOR_PRE_HOOK\n            assert post_hook_ip.insertion_type == InsertionType.OPERATOR_POST_HOOK\n\n            assert pre_hook_ip.ia_op_exec_context == ref_op_exec_context.input_agnostic\n            assert post_hook_ip.ia_op_exec_context == ref_op_exec_context.input_agnostic\n\n    def test_operator_metatype_marking(self):\n        from nncf.dynamic_graph.operator_metatypes import Conv2dMetatype, BatchNormMetatype, RELUMetatype, \\\n            MaxPool2dMetatype, \\\n            ConvTranspose2dMetatype, DepthwiseConv2dSubtype, AddMetatype, AvgPool2dMetatype, LinearMetatype\n        ref_scope_vs_metatype_dict = {\n            ""/"" + MODEL_INPUT_OP_NAME + ""_0"": NoopMetatype,\n            ""ModelForMetatypeTesting/NNCFConv2d[conv_regular]/conv2d_0"": Conv2dMetatype,\n            ""ModelForMetatypeTesting/BatchNorm2d[bn]/batch_norm_0"": BatchNormMetatype,\n            ""ModelForMetatypeTesting/RELU_0"": RELUMetatype,\n            ""ModelForMetatypeTesting/MaxPool2d[max_pool2d]/max_pool2d_0"": MaxPool2dMetatype,\n            ""ModelForMetatypeTesting/NNCFConvTranspose2d[conv_transpose]/conv_transpose2d_0"": ConvTranspose2dMetatype,\n            ""ModelForMetatypeTesting/NNCFConv2d[conv_depthwise]/conv2d_0"": DepthwiseConv2dSubtype,\n            ""ModelForMetatypeTesting/__iadd___0"": AddMetatype,\n            ""ModelForMetatypeTesting/AdaptiveAvgPool2d[adaptive_avg_pool]/adaptive_avg_pool2d_0"": AvgPool2dMetatype,\n            ""ModelForMetatypeTesting/NNCFLinear[linear]/linear_0"": LinearMetatype\n        }\n        class ModelForMetatypeTesting(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv_regular = torch.nn.Conv2d(in_channels=3,\n                                                    out_channels=16,\n                                                    kernel_size=3)\n                self.bn = torch.nn.BatchNorm2d(num_features=16)\n                self.max_pool2d = torch.nn.MaxPool2d(kernel_size=2)\n                self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=16,\n                                                               out_channels=8,\n                                                               kernel_size=3)\n                self.conv_depthwise = torch.nn.Conv2d(in_channels=8, out_channels=8,\n                                                      kernel_size=5, groups=8)\n                self.adaptive_avg_pool = torch.nn.AdaptiveAvgPool2d(output_size=1)\n                self.linear = torch.nn.Linear(in_features=8, out_features=1)\n\n            def forward(self, input_):\n                x = self.conv_regular(input_)\n                x = self.bn(x)\n                x = torch.nn.functional.relu(x)\n                x.transpose_(2, 3)\n                x = self.max_pool2d(x)\n                x = self.conv_transpose(x)\n                x = self.conv_depthwise(x)\n                x += torch.ones_like(x)\n                x = self.adaptive_avg_pool(x)\n                x = self.linear(x.flatten())\n                return x\n\n        model = ModelForMetatypeTesting()\n        nncf_network = NNCFNetwork(model, [ModelInputInfo((1, 3, 300, 300))])\n        ip_graph = nncf_network.get_insertion_point_graph()\n\n        for node in ip_graph.nodes().values():\n            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:\n                nncf_node_ref = node[InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR]\n                scope_str = str(nncf_node_ref[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR].input_agnostic)\n                assert scope_str in ref_scope_vs_metatype_dict\n                ref_metatype = ref_scope_vs_metatype_dict[scope_str]\n                assert node[InsertionPointGraph.OPERATOR_METATYPE_NODE_ATTR] == ref_metatype\n'"
pytorch_toolkit/nncf/tests/test_sanity_sample.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport json\nimport shlex\nimport signal\nimport subprocess\nimport sys\nimport threading\nimport time\n\nimport os\nimport pytest\nimport tempfile\nimport torch\n\n# pylint: disable=redefined-outer-name\nfrom examples.common.optimizer import get_default_weight_decay\nfrom examples.common.utils import get_name, is_binarization\nfrom nncf.config import Config\nfrom tests.conftest import EXAMPLES_DIR, PROJECT_ROOT, TEST_ROOT\n\n\nclass Command:\n    def __init__(self, cmd):\n        self.cmd = cmd\n        self.process = None\n        self.exec_time = -1\n        self.output = []  # store output here\n        self.kwargs = {}\n        self.timeout = False\n\n        # set system/version dependent ""start_new_session"" analogs\n        if sys.platform == ""win32"":\n            self.kwargs.update(creationflags=subprocess.CREATE_NEW_PROCESS_GROUP)\n        elif sys.version_info < (3, 2):  # assume posix\n            self.kwargs.update(preexec_fn=os.setsid)\n        else:  # Python 3.2+ and Unix\n            self.kwargs.update(start_new_session=True)\n\n    def kill_process_tree(self, pid):\n        try:\n            if sys.platform != ""win32"":\n                os.killpg(pid, signal.SIGKILL)\n            else:\n                subprocess.call([\'taskkill\', \'/F\', \'/T\', \'/PID\', str(pid)])\n        except OSError as err:\n            print(err)\n\n    def run(self, timeout=3600):\n\n        def target():\n            start_time = time.time()\n            self.process = subprocess.Popen(self.cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True,\n                                            bufsize=1, **self.kwargs)\n            self.timeout = False\n\n            self.output = []\n            for line in self.process.stdout:\n                line = line.decode(\'utf-8\')\n                self.output.append(line)\n                sys.stdout.write(line)\n\n            sys.stdout.flush()\n            self.process.stdout.close()\n\n            self.process.wait()\n            self.exec_time = time.time() - start_time\n\n        thread = threading.Thread(target=target)\n        thread.start()\n\n        thread.join(timeout)\n        if thread.is_alive():\n            try:\n                print(""Error: process taking too long to complete--terminating"" + "", [ "" + self.cmd + "" ]"")\n                self.kill_process_tree(self.process.pid)\n                self.exec_time = timeout\n                self.timeout = True\n                thread.join()\n            except OSError as e:\n                print(self.process.pid, ""Exception when try to kill task by PID, "" + e.strerror)\n                raise\n        returncode = self.process.wait()\n        print(""Process returncode = "" + str(returncode))\n        return returncode\n\n    def get_execution_time(self):\n        return self.exec_time\n\n\nclass ConfigFactory:\n    """"""Allows to modify config file before test run""""""\n\n    def __init__(self, base_config, config_path):\n        self.config = base_config\n        self.config_path = str(config_path)\n\n    def serialize(self):\n        with open(self.config_path, \'w\') as f:\n            json.dump(self.config, f)\n        return self.config_path\n\n    def __getitem__(self, item):\n        return self.config[item]\n\n    def __setitem__(self, key, value):\n        self.config[key] = value\n\n\ndef create_command_line(args, sample_type):\n    python_path = PROJECT_ROOT.as_posix()\n    executable = EXAMPLES_DIR.joinpath(sample_type, \'main.py\').as_posix()\n    cli_args = "" "".join(key if val is None else ""{} {}"".format(key, val) for key, val in args.items())\n    return ""PYTHONPATH={path} {python_exe} {main_py} {args}"".format(\n        path=python_path, main_py=executable, args=cli_args, python_exe=sys.executable\n    )\n\n\nSAMPLE_TYPES = [""classification"", ""semantic_segmentation"", ""object_detection""]\n\nDATASETS = {\n    ""classification"": [""cifar10"", ""cifar100""],\n    ""semantic_segmentation"": [""camvid"", ""camvid""],\n    ""object_detection"": [""voc""],\n}\n\nCONFIGS = {\n    ""classification"": [TEST_ROOT.joinpath(""data"", ""configs"", ""squeezenet1_1_cifar10_rb_sparsity_int8.json""),\n                       TEST_ROOT.joinpath(""data"", ""configs"", ""resnet18_cifar100_bin_xnor.json"")],\n    ""semantic_segmentation"": [TEST_ROOT.joinpath(""data"", ""configs"", ""unet_camvid_int8.json""),\n                              TEST_ROOT.joinpath(""data"", ""configs"", ""unet_camvid_rb_sparsity.json"")],\n    ""object_detection"": [TEST_ROOT.joinpath(""data"", ""configs"", ""ssd300_vgg_voc_int8.json"")]\n}\n\nBATCHSIZE_PER_GPU = {\n    ""classification"": [256, 256],\n    ""semantic_segmentation"": [2, 2],\n    ""object_detection"": [128],\n}\n\nDATASET_PATHS = {\n    ""classification"": {\n        x: lambda dataset_root: dataset_root if dataset_root else os.path.join(\n            tempfile.gettempdir(), x) for x in DATASETS[""classification""]\n    },\n    ""semantic_segmentation"": {\n        DATASETS[""semantic_segmentation""][0]: lambda dataset_root: TEST_ROOT.joinpath(""data"", ""mock_datasets"",\n                                                                                      ""camvid""),\n        DATASETS[""semantic_segmentation""][0]: lambda dataset_root: TEST_ROOT.joinpath(""data"", ""mock_datasets"", ""camvid"")\n    },\n    ""object_detection"": {\n        DATASETS[""object_detection""][0]: lambda dataset_root: TEST_ROOT.joinpath(""data"", ""mock_datasets"", ""voc"")\n    },\n}\n\nCONFIG_PARAMS = list()\nfor sample_type in SAMPLE_TYPES:\n    for tpl in list(zip(CONFIGS[sample_type], DATASETS[sample_type], BATCHSIZE_PER_GPU[sample_type])):\n        CONFIG_PARAMS.append((sample_type,) + tpl)\n\n\n@pytest.fixture(params=CONFIG_PARAMS,\n                ids=[""-"".join([p[0], p[1].name, p[2], str(p[3])]) for p in CONFIG_PARAMS])\ndef config(request, dataset_dir):\n    sample_type, config_path, dataset_name, batch_size = request.param\n    dataset_path = DATASET_PATHS[sample_type][dataset_name](dataset_dir)\n\n    with config_path.open() as f:\n        jconfig = json.load(f)\n\n    if ""checkpoint_save_dir"" in jconfig.keys():\n        del jconfig[""checkpoint_save_dir""]\n\n    jconfig[""dataset""] = dataset_name\n\n    return {\n        ""sample_type"": sample_type,\n        \'config\': jconfig,\n        ""model_name"": jconfig[""model""],\n        ""dataset_path"": dataset_path,\n        ""batch_size"": batch_size,\n    }\n\n\n@pytest.fixture(scope=""module"")\ndef case_common_dirs(tmp_path_factory):\n    return {\n        ""checkpoint_save_dir"": str(tmp_path_factory.mktemp(""models""))\n    }\n\n\ndef test_pretrained_model_export(config, tmp_path):\n    c = config\n    config_factory = ConfigFactory(c[\'config\'], tmp_path / \'config.json\')\n\n    if isinstance(config_factory[\'compression\'], list):\n        config_factory[\'compression\'][0][\'initializer\'] = {\'range\': {\'num_init_steps\': 0}}\n    else:\n        config_factory[\'compression\'][\'initializer\'] = {\'range\': {\'num_init_steps\': 0}}\n\n    onnx_path = os.path.join(str(tmp_path), ""model.onnx"")\n    args = {\n        ""--mode"": ""test"",\n        ""--config"": config_factory.serialize(),\n        ""--to-onnx"": onnx_path\n    }\n\n    runner = Command(create_command_line(args, c[""sample_type""]))\n    res = runner.run()\n    assert res == 0\n    assert os.path.exists(onnx_path)\n\n\n@pytest.mark.parametrize("" multiprocessing_distributed"",\n                         (True, False),\n                         ids=[\'distributed\', \'dataparallel\'])\ndef test_pretrained_model_eval(config, tmp_path, multiprocessing_distributed):\n    c = config\n\n    config_factory = ConfigFactory(c[\'config\'], tmp_path / \'config.json\')\n    args = {\n        ""--mode"": ""test"",\n        ""--data"": c[""dataset_path""],\n        ""--config"": config_factory.serialize(),\n        ""--log-dir"": tmp_path,\n        ""--batch-size"": c[""batch_size""] * torch.cuda.device_count(),\n        ""--workers"": 1,\n    }\n\n    if multiprocessing_distributed:\n        args[""--multiprocessing-distributed""] = None\n\n    runner = Command(create_command_line(args, c[""sample_type""]))\n    res = runner.run()\n    assert res == 0\n\n\n@pytest.mark.parametrize(\n    ""multiprocessing_distributed"", [\n        pytest.param(True, marks=pytest.mark.dependency(name=[""train_distributed""])),\n        pytest.param(False, marks=pytest.mark.dependency(name=[""train_dataparallel""]))],\n    ids=[\'distributed\', \'dataparallel\'])\ndef test_pretrained_model_train(config, tmp_path, multiprocessing_distributed, case_common_dirs):\n    c = config\n\n    checkpoint_save_dir = os.path.join(case_common_dirs[""checkpoint_save_dir""],\n                                       ""distributed"" if multiprocessing_distributed else ""data_parallel"")\n    config_factory = ConfigFactory(config[\'config\'], tmp_path / \'config.json\')\n    args = {\n        ""--mode"": ""train"",\n        ""--data"": c[""dataset_path""],\n        ""--config"": config_factory.serialize(),\n        ""--log-dir"": tmp_path,\n        ""--batch-size"": c[""batch_size""] * torch.cuda.device_count(),\n        ""--workers"": 1,\n        ""--epochs"": 1,\n        ""--checkpoint-save-dir"": checkpoint_save_dir,\n        ""--dist-url"": ""tcp://127.0.0.1:8989""\n    }\n\n    if multiprocessing_distributed:\n        args[""--multiprocessing-distributed""] = None\n\n    runner = Command(create_command_line(args, c[""sample_type""]))\n    res = runner.run()\n    assert res == 0\n    assert os.path.exists(os.path.join(checkpoint_save_dir, get_name(config_factory.config) + ""_last.pth""))\n\n\n@pytest.mark.parametrize(\n    ""multiprocessing_distributed"", [\n        pytest.param(True, marks=pytest.mark.dependency(depends=[""train_distributed""])),\n        pytest.param(False, marks=pytest.mark.dependency(depends=[""train_dataparallel""]))],\n    ids=[\'distributed\', \'dataparallel\'])\ndef test_trained_model_export(config, tmp_path, multiprocessing_distributed, case_common_dirs):\n    c = config\n\n    config_factory = ConfigFactory(config[\'config\'], tmp_path / \'config.json\')\n    ckpt_path = os.path.join(case_common_dirs[""checkpoint_save_dir""],\n                             ""distributed"" if multiprocessing_distributed else ""data_parallel"",\n                             get_name(config_factory.config) + ""_last.pth"")\n    onnx_path = os.path.join(str(tmp_path), ""model.onnx"")\n    args = {\n        ""--mode"": ""test"",\n        ""--config"": config_factory.serialize(),\n        ""--to-onnx"": onnx_path,\n        ""--weights"": ckpt_path\n    }\n\n    runner = Command(create_command_line(args, c[""sample_type""]))\n    res = runner.run()\n    assert res == 0\n    assert os.path.exists(onnx_path)\n\n\n@pytest.mark.parametrize(\n    ""multiprocessing_distributed"", [\n        pytest.param(True, marks=pytest.mark.dependency(depends=[""train_distributed""])),\n        pytest.param(False, marks=pytest.mark.dependency(depends=[""train_dataparallel""]))],\n    ids=[\'distributed\', \'dataparallel\'])\ndef test_trained_model_eval(config, tmp_path, multiprocessing_distributed, case_common_dirs):\n    c = config\n\n    config_factory = ConfigFactory(config[\'config\'], tmp_path / \'config.json\')\n    ckpt_path = os.path.join(case_common_dirs[""checkpoint_save_dir""],\n                             ""distributed"" if multiprocessing_distributed else ""data_parallel"",\n                             get_name(config_factory.config) + ""_last.pth"")\n    args = {\n        ""--mode"": ""test"",\n        ""--data"": c[""dataset_path""],\n        ""--config"": config_factory.serialize(),\n        ""--log-dir"": tmp_path,\n        ""--batch-size"": c[""batch_size""] * torch.cuda.device_count(),\n        ""--workers"": 1,\n        ""--weights"": ckpt_path,\n    }\n\n    if multiprocessing_distributed:\n        args[""--multiprocessing-distributed""] = None\n\n    runner = Command(create_command_line(args, c[""sample_type""]))\n    res = runner.run()\n    assert res == 0\n\n\n@pytest.mark.parametrize(\n    ""multiprocessing_distributed"", [\n        pytest.param(True, marks=pytest.mark.dependency(depends=[""train_distributed""])),\n        pytest.param(False, marks=pytest.mark.dependency(depends=[""train_dataparallel""]))],\n    ids=[\'distributed\', \'dataparallel\'])\ndef test_resume(config, tmp_path, multiprocessing_distributed, case_common_dirs):\n    c = config\n\n    checkpoint_save_dir = os.path.join(str(tmp_path), ""models"")\n    config_factory = ConfigFactory(config[\'config\'], tmp_path / \'config.json\')\n    ckpt_path = os.path.join(case_common_dirs[""checkpoint_save_dir""],\n                             ""distributed"" if multiprocessing_distributed else ""data_parallel"",\n                             get_name(config_factory.config) + ""_last.pth"")\n    if ""max_iter"" in config_factory.config:\n        config_factory.config[""max_iter""] += 2\n    args = {\n        ""--mode"": ""train"",\n        ""--data"": c[""dataset_path""],\n        ""--config"": config_factory.serialize(),\n        ""--log-dir"": tmp_path,\n        ""--batch-size"": c[""batch_size""] * torch.cuda.device_count(),\n        ""--workers"": 1,\n        ""--epochs"": 2,\n        ""--checkpoint-save-dir"": checkpoint_save_dir,\n        ""--resume"": ckpt_path,\n    }\n\n    if multiprocessing_distributed:\n        args[""--multiprocessing-distributed""] = None\n\n    runner = Command(create_command_line(args, c[""sample_type""]))\n    res = runner.run()\n    assert res == 0\n    assert os.path.exists(os.path.join(checkpoint_save_dir, get_name(config_factory.config) + ""_last.pth""))\n\n\n@pytest.mark.parametrize((\'algo\', \'ref_weight_decay\'),\n                         ((\'rb_sparsity\', 0),\n                          (\'const_sparsity\', 1e-4),\n                          (\'magnitude_sparsity\', 1e-4),\n                          (\'quantization\', 1e-4)))\ndef test_get_default_weight_decay(algo, ref_weight_decay):\n    config = Config()\n    config.update({""compression"": {""algorithm"": algo}})\n    assert ref_weight_decay == get_default_weight_decay(config)\n\n\ndef test_cpu_only_mode_produces_cpu_only_model(config, tmp_path, mocker):\n    c = config\n\n    config_factory = ConfigFactory(config[\'config\'], tmp_path / \'config.json\')\n    args = {\n        ""--data"": c[""dataset_path""],\n        ""--config"": config_factory.serialize(),\n        ""--log-dir"": tmp_path,\n        ""--batch-size"": c[""batch_size""] * torch.cuda.device_count(),\n        ""--workers"": 1,\n        ""--epochs"": 1,\n        ""--cpu-only"": None\n    }\n\n    command_line = "" "".join(key if val is None else ""{} {}"".format(key, val) for key, val in args.items())\n\n    if config[""sample_type""] == ""classification"":\n        import examples.classification.main as sample\n        if is_binarization(config[\'config\']):\n            mocker.patch(""examples.classification.binarization_worker.train_epoch_bin"")\n            mocker.patch(""examples.classification.binarization_worker.validate"")\n            import examples.classification.binarization_worker as bin_worker\n            bin_worker.validate.return_value = (0, 0)\n        else:\n            mocker.patch(""examples.classification.main.train_epoch"")\n            mocker.patch(""examples.classification.main.validate"")\n            sample.validate.return_value = (0, 0)\n    elif config[""sample_type""] == ""semantic_segmentation"":\n        import examples.semantic_segmentation.main as sample\n        import examples.semantic_segmentation.train\n        mocker.spy(examples.semantic_segmentation.train.Train, ""__init__"")\n    elif config[""sample_type""] == ""object_detection"":\n        import examples.object_detection.main as sample\n        mocker.patch(""examples.object_detection.main.train"")\n\n    sample.main(shlex.split(command_line))\n\n    # pylint: disable=no-member\n    if config[""sample_type""] == ""classification"":\n        if is_binarization(config[\'config\']):\n            import examples.classification.binarization_worker as bin_worker\n            model_to_be_trained = bin_worker.train_epoch_bin.call_args[0][2]  # model\n        else:\n            model_to_be_trained = sample.train_epoch.call_args[0][1]  # model\n    elif config[""sample_type""] == ""semantic_segmentation"":\n        model_to_be_trained = examples.semantic_segmentation.train.Train.__init__.call_args[0][1]  # model\n    elif config[""sample_type""] == ""object_detection"":\n        model_to_be_trained = sample.train.call_args[0][0]  # net\n\n    for p in model_to_be_trained.parameters():\n        assert not p.is_cuda\n'"
pytorch_toolkit/nncf/tests/test_sota_checkpoints.py,0,"b'import os\nimport json\nimport sys\nimport pytest\nimport subprocess\nimport re\nimport shlex\nfrom prettytable import PrettyTable\nfrom collections import OrderedDict\nfrom yattag import Doc\nfrom pathlib import Path\nfrom tests.conftest import TEST_ROOT, PROJECT_ROOT\n\nBG_COLOR_GREEN_HEX = \'ccffcc\'\nBG_COLOR_YELLOW_HEX = \'ffffcc\'\nBG_COLOR_RED_HEX = \'ffcccc\'\n\nDIFF_TARGET_MIN_GLOBAL = -0.1\nDIFF_TARGET_MAX_GLOBAL = 0.1\nDIFF_FP32_MIN_GLOBAL = -1.0\nDIFF_FP32_MAX_GLOBAL = 0.1\n\nresults_path = str(PROJECT_ROOT)\n\n\nclass TestSotaCheckpoints:\n    param_list = []\n    train_param_list = []\n    ids_list = []\n    train_ids_list = []\n    row_dict = OrderedDict()\n    color_dict = OrderedDict()\n    test = None\n    cmd = ""{} examples/{sample_type}/main.py -m {} --config {conf} \\\n         --data {dataset}/{data_name}/ --log-dir={res}/logs/ --metrics-dump \\\n          {res}/{mod_name}.json""\n\n    @staticmethod\n    def run_cmd(comm):\n        print()\n        print(comm)\n        print()\n        com_line = shlex.split(comm)\n        result = subprocess.Popen(com_line, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        err_string = None\n        while result.poll() is None:\n            stdout_line = result.stdout.readline().decode(\'utf-8\').strip()\n            if re.search(""Error:"", stdout_line):\n                err_string = stdout_line\n            if stdout_line != """":\n                print(stdout_line)\n        return err_string\n\n    @staticmethod\n    def make_table_row(test, expected_, metrics_type_, key, error_message, metric, diff_target, fp32_metric_=None,\n                       diff_fp32=None):\n        TestSotaCheckpoints.test = test\n        if metric != 0:\n            if fp32_metric_ is None:\n                fp32_metric_ = ""-""\n                diff_fp32 = ""-""\n            if test == \'eval\':\n                row = [str(key), str(expected_), str(metric), str(fp32_metric_), str(metrics_type_),\n                       str(diff_fp32), str(diff_target), str(""-"")]\n            else:\n                row = [str(key), str(expected_), str(metric), str(metrics_type_), str(diff_target), str(""-"")]\n        else:\n            if fp32_metric_ is None:\n                fp32_metric_ = ""-""\n            if test == \'eval\':\n                row = [str(key), str(expected_), str(""Not executed""), str(fp32_metric_), str(metrics_type_),\n                       str(""-""), str(""-""), str(error_message)]\n            else:\n                row = [str(key), str(expected_), str(""Not executed""), str(metrics_type_), str(""-""), str(error_message)]\n        return row\n\n    def write_results_table(self, init_table_string):\n        result_table = PrettyTable()\n        result_table.field_names = init_table_string\n        for key in self.row_dict:\n            result_table.add_row(self.row_dict[key])\n        print()\n        print(result_table)\n\n        doc, tag, text = Doc().tagtext()\n        doc.asis(\'<!DOCTYPE html>\')\n        with tag(\'p\'):\n            text(\'legend: \')\n        with tag(\'p\'):\n            with tag(\'span\', style=""Background-color: #{}"".format(BG_COLOR_GREEN_HEX)):\n                text(\'Thresholds for FP32 and Expected are passed\')\n        with tag(\'p\'):\n            with tag(\'span\', style=""Background-color: #{}"".format(BG_COLOR_YELLOW_HEX)):\n                text(\'Thresholds for Expected is failed, but for FP32 passed\')\n        with tag(\'p\'):\n            with tag(\'span\', style=""Background-color: #{}"".format(BG_COLOR_RED_HEX)):\n                text(\'Thresholds for FP32 and Expected are failed\')\n        with tag(\'table\', border=""1"", cellpadding=""5"", style=""border-collapse: collapse; border: 1px solid black;""):\n            with tag(\'tr\'):\n                for i in init_table_string:\n                    with tag(\'td\'):\n                        text(i)\n            for key in self.row_dict:\n                with tag(\'tr\', bgcolor=\'{}\'.format(self.color_dict[key])):\n                    for i in self.row_dict[key]:\n                        if i is None:\n                            i = \'-\'\n                        with tag(\'td\'):\n                            text(i)\n        f = open(\'results.html\', \'w\')\n        f.write(doc.getvalue())\n        f.close()\n\n    @staticmethod\n    def threshold_check(err, diff_target, diff_fp32_min_=None, diff_fp32_max_=None, fp32_metric=None,\n                        diff_fp32=None, diff_target_min=None, diff_target_max=None):\n        color = BG_COLOR_RED_HEX\n        within_thresholds = False\n        if not diff_target_min:\n            diff_target_min = DIFF_TARGET_MIN_GLOBAL\n        if not diff_target_max:\n            diff_target_max = DIFF_TARGET_MAX_GLOBAL\n        if not diff_fp32_min_:\n            diff_fp32_min_ = DIFF_FP32_MIN_GLOBAL\n        if not diff_fp32_max_:\n            diff_fp32_max_ = DIFF_FP32_MAX_GLOBAL\n        if err is None:\n            if fp32_metric is not None:\n                if diff_fp32_min_ < diff_fp32 < diff_fp32_max_ and diff_target_min < diff_target < diff_target_max:\n                    color = BG_COLOR_GREEN_HEX\n                    within_thresholds = True\n                elif diff_fp32_min_ < diff_fp32 < diff_fp32_max_:\n                    color = BG_COLOR_YELLOW_HEX\n            elif diff_target_min < diff_target < diff_target_max:\n                color = BG_COLOR_GREEN_HEX\n                within_thresholds = True\n        return color, within_thresholds\n\n    @staticmethod\n    def write_common_metrics_file():\n        metric_value = OrderedDict()\n        for i in TestSotaCheckpoints.ids_list:\n            with open(\'{}.json\'.format(i)) as metric_file:\n                metrics = json.load(metric_file)\n            metric_value[i] = metrics[\'Accuracy\']\n\n            if os.path.isfile(\'metrics.json\'):\n                path = Path(\'metrics.json\')\n                data = json.loads(path.read_text(encoding=\'utf-8\'))\n                data.update(metric_value)\n                path.write_text(json.dumps(data, indent=4), encoding=\'utf-8\')\n            else:\n                with open(\'metrics.json\', \'w\') as outfile:\n                    json.dump(metric_value, outfile)\n\n    @staticmethod\n    def read_metric(model_name_):\n        with open(\'{}.json\'.format(model_name_)) as metric_file:\n            metrics = json.load(metric_file)\n        return metrics[\'Accuracy\']\n\n    sota_eval_config = json.load(open(\'{}/sota_checkpoints_eval.json\'.format(TEST_ROOT)),\n                                 object_pairs_hook=OrderedDict)\n    for sample_type_ in sota_eval_config:\n        datasets = sota_eval_config[sample_type_]\n        for dataset_name in datasets:\n            model_dict = datasets[dataset_name]\n            for model_name in model_dict:\n                config_name = model_dict[model_name].get(\'config\', {})\n                reference = None\n                if model_dict[model_name].get(\'reference\', {}):\n                    reference = model_dict[model_name].get(\'reference\', {})\n                expected = model_dict[model_name].get(\'target\', {})\n                metric_type = model_dict[model_name].get(\'metric_type\', {})\n                if model_dict[model_name].get(\'resume\', {}):\n                    resume_file = model_dict[model_name].get(\'resume\', {})\n                else:\n                    resume_file = None\n                if model_dict[model_name].get(\'batch\', {}):\n                    batch = model_dict[model_name].get(\'batch\', {})\n                else:\n                    batch = None\n                diff_fp32_min = model_dict[model_name].get(\'diff_fp32_min\') if not None else None\n                diff_fp32_max = model_dict[model_name].get(\'diff_fp32_max\') if not None else None\n                diff_target_min = model_dict[model_name].get(\'diff_target_min\') if not None else None\n                diff_target_max = model_dict[model_name].get(\'diff_target_max\') if not None else None\n                param_list.append((config_name, reference, expected, metric_type, dataset_name, sample_type_,\n                                   resume_file, batch, diff_fp32_min, diff_fp32_max, model_name, diff_target_min,\n                                   diff_target_max))\n                ids_list.append(model_name)\n                if model_dict[model_name].get(\'compression_description\', {}):\n                    train_param_list.append((config_name, expected, metric_type, dataset_name, sample_type_,\n                                             model_name))\n                    train_ids_list.append(model_name)\n\n    @pytest.mark.parametrize(""config_name_, reference_, expected_, metric_type_, dataset_name_, _sample_type_,""\n                             "" resume_file_, batch_, diff_fp32_min_, diff_fp32_max_, model_name_, diff_target_min_, ""\n                             ""diff_target_max_"", param_list,\n                             ids=ids_list)\n    def test_eval(self, sota_checkpoints_dir, sota_data_dir, config_name_, reference_, expected_, metric_type_,\n                  dataset_name_, _sample_type_, resume_file_, batch_, diff_fp32_min_, diff_fp32_max_, model_name_,\n                  diff_target_min_, diff_target_max_):\n        test = ""eval""\n        os.chdir(results_path)\n        cmd = self.cmd.format(sys.executable, \'test\', conf=config_name_, dataset=sota_data_dir, data_name=dataset_name_,\n                              sample_type=_sample_type_, res=results_path, mod_name=model_name_)\n        if resume_file_:\n            resume = resume_file_\n            cmd += "" --resume {}/{}"".format(sota_checkpoints_dir, resume)\n        else:\n            cmd += "" --pretrained""\n        if batch_:\n            cmd += "" -b {}"".format(batch_)\n        err = self.run_cmd(cmd)\n        metric_value = self.read_metric(model_name_)\n\n        fp32_metric = None\n        if reference_ is not None:\n            with open(\'{}.json\'.format(reference_)) as ref_metric:\n                metrics = json.load(ref_metric)\n            fp32_metric = metrics[\'Accuracy\']\n\n        diff_target = round((metric_value - expected_), 2)\n        diff_fp32 = round((metric_value - fp32_metric), 2) if fp32_metric is not None else None\n\n        self.row_dict[model_name_] = self.make_table_row(test, expected_, metric_type_, model_name_, err,\n                                                         metric_value, diff_target, fp32_metric, diff_fp32)\n        self.color_dict[model_name_], is_accuracy_within_thresholds = self.threshold_check(err, diff_target,\n                                                                                           diff_fp32_min_,\n                                                                                           diff_fp32_max_,\n                                                                                           fp32_metric,\n                                                                                           diff_fp32,\n                                                                                           diff_target_min_,\n                                                                                           diff_target_max_)\n        assert is_accuracy_within_thresholds\n\n    @pytest.mark.parametrize(""config_name_, expected_, metric_type_, dataset_name_, _sample_type_, model_name_"",\n                             train_param_list, ids=train_ids_list)\n    def test_train(self, sota_data_dir, config_name_, expected_, metric_type_, dataset_name_, _sample_type_,\n                   model_name_):\n        os.chdir(results_path)\n        test = \'train\'\n        cmd = self.cmd.format(sys.executable, \'train\', conf=config_name_, dataset=sota_data_dir,\n                              data_name=dataset_name_, sample_type=_sample_type_, res=results_path,\n                              mod_name=model_name_)\n        err = self.run_cmd(cmd)\n        metric_value = self.read_metric(model_name_)\n        diff_target = round((metric_value - expected_), 2)\n        self.row_dict[model_name_] = self.make_table_row(test, expected_, metric_type_, model_name_, err, metric_value,\n                                                         diff_target)\n        self.color_dict[model_name_], is_accuracy_within_thresholds = self.threshold_check(err, diff_target)\n        assert is_accuracy_within_thresholds\n\n\nTsc = TestSotaCheckpoints\n\n\n@pytest.fixture(autouse=True, scope=""module"")\ndef skip_params(sota_data_dir):\n    if sota_data_dir is None:\n        pytest.skip(\'Path to datasets is not set\')\n\n\n@pytest.fixture(autouse=True, scope=""class"")\ndef results():\n    yield\n    Tsc.write_common_metrics_file()\n    if Tsc.test == ""eval"":\n        header = [""Model"", ""Expected"", ""Measured"", ""Reference FP32"", ""Metrics type"", ""Diff FP32"", ""Diff Expected"",\n                  ""Error""]\n    else:\n        header = [""Model"", ""Expected"", ""Measured"", ""Metrics type"", ""Diff Expected"", ""Error""]\n    Tsc().write_results_table(header)\n'"
pytorch_toolkit/nncf/tools/__init__.py,0,b''
pytorch_toolkit/nncf/tools/add_precision_parameter.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nfrom argparse import ArgumentParser\n\nimport torch\nfrom os import listdir, makedirs\nfrom os.path import isfile, join, exists\nfrom shutil import copyfile\n\nfrom nncf.quantization.layers import SymmetricQuantizer, AsymmetricQuantizer\n\n\ndef main(argv):\n    parser = ArgumentParser()\n    parser.add_argument(\'-i\', \'--input-folder\', help=\'Path to directory with given checkpoints to modify\',\n                        required=True)\n    parser.add_argument(\'-o\', \'--output-folder\', help=\'Path to directory to save modified checkpoints\', required=True)\n    parser.add_argument(\'-b\', \'--bitwidth\', help=\'Bitwidth to initialize quantizer\',\n                        required=False, default=8, type=int)\n    parser.add_argument(\'-v\', \'--verbose\', help=\'Print all new names of parameters\', required=False,\n                        action=\'store_true\')\n    args = parser.parse_args(args=argv)\n\n    src_dir = args.input_folder\n    dst_dir = args.output_folder\n    if not exists(dst_dir):\n        makedirs(dst_dir)\n\n    PRECISION_PARAM_NAME = \'_num_bits\'\n\n    pth_files = [(join(src_dir, f), join(dst_dir, f)) for f in listdir(src_dir) if\n                 isfile(join(src_dir, f)) and (\'.pth\' in f or \'.sd\' in f)]\n\n    files_to_copy = []\n    for pair in pth_files:\n        src_file, dst_file = pair\n        if \'binarization\' in src_file:\n            files_to_copy.append(pair)\n            continue\n        sd = pth = torch.load(src_file)\n        if \'state_dict\' in pth:\n            sd = pth[\'state_dict\']\n\n        hooks = [SymmetricQuantizer.SCALE_PARAM_NAME, AsymmetricQuantizer.INPUT_LOW_PARAM_NAME]\n        new_keys = []\n        for k in sd.keys():\n            for h in hooks:\n                if \'.\' + h in k and \'.\' + PRECISION_PARAM_NAME not in k:\n                    new_key = k.replace(h, PRECISION_PARAM_NAME)\n                    new_keys.append(new_key)\n        if new_keys:\n            print(\'\\nAdding {} {}-bit params to {}\'.format(len(new_keys), args.bitwidth, dst_file))\n            if args.verbose:\n                print(\'New keys: {}\'.format(new_keys))\n            for new_key in new_keys:\n                sd[new_key] = torch.Tensor([args.bitwidth])\n            pth[\'state_dict\'] = sd\n            torch.save(pth, dst_file)\n        else:\n            files_to_copy.append(pair)\n\n    for src_file, dst_file in files_to_copy:\n        print(""\\nCopying {}"".format(dst_file))\n        copyfile(src_file, dst_file)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n'"
pytorch_toolkit/nncf/tools/benchmark.py,0,"b'import math\nimport time\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\n\nTIME_SCALES = {\'ms\': 1000}\n\n\ndef warmup(layer, input_, runs, forward_only=False):\n    for _ in range(runs):\n        new_i = layer(input_)\n        if not forward_only:\n            new_i[0].sum().backward()\n\n\ndef run_wall(layer, input_size_, device, runs, is_print=True):\n    input_ = torch.randn(input_size_, device=torch.device(device))\n\n    # Force CUDA initialization & warm up\n    warmup(layer, input_, 100)\n\n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(runs):\n        layer.zero_grad()\n        new_i = layer(input_)\n        new_i[0].sum().backward()\n    torch.cuda.synchronize()\n    elapsed = time.time() - start\n\n    ctime, scale = list(TIME_SCALES.items())[0]\n    fbtime = elapsed / runs * scale\n\n    if is_print:\n        print(\'Forward&Backward: {0:.3f} {1}\'.format(\n            fbtime, ctime))\n\n\ndef run_profile(layer, input_size_, device, runs, forward_only=False):\n    input_ = torch.randn(input_size_, device=torch.device(device))\n\n    # Force CUDA initialization & warm up\n    warmup(layer, input_, 100, forward_only)\n\n    start = time.time()\n    forward_min = math.inf\n    forward_time = 0\n    backward_min = math.inf\n    backward_time = 0\n    for _ in range(runs):\n        layer.zero_grad()\n\n        torch.cuda.synchronize()\n        start = time.time()\n        new_i = layer(input_)\n        torch.cuda.synchronize()\n        elapsed = time.time() - start\n        forward_min = min(forward_min, elapsed)\n        forward_time += elapsed\n\n        if not forward_only:\n            torch.cuda.synchronize()\n            start = time.time()\n            new_i[0].sum().backward()\n            torch.cuda.synchronize()\n            elapsed = time.time() - start\n            backward_min = min(backward_min, elapsed)\n            backward_time += elapsed\n\n    ctime, scale = list(TIME_SCALES.items())[0]\n    forward_min *= scale\n    backward_min *= scale\n    forward_average = forward_time / runs * scale\n    backward_average = backward_time / runs * scale\n\n    print(\'Forward: min {0:.3f}{4} / avg {1:.3f}{4} | Backward: min {2:.3f}{4} / avg {3:.3f}{4}\'.format(\n        forward_min, forward_average, backward_min, backward_average, ctime))\n\n\ndef run_worker(gpu, world_size, layer, input_size_, runs):\n    dist.init_process_group(backend=\'nccl\', init_method=""tcp://127.0.0.1:8899"",\n                            world_size=world_size, rank=gpu)\n\n    device = torch.device(\'cuda:%d\' % gpu)\n    torch.cuda.set_device(gpu)\n\n    batch = (int)(input_size_[0] / world_size)\n    if gpu == 0:\n        run_size = input_size_.copy()\n        run_size[0] = input_size_[0] - batch * (world_size - 1)\n    else:\n        run_size = input_size_.copy()\n        run_size[0] = batch\n\n    run_model = layer.to(device)\n    run_model = nn.parallel.DistributedDataParallel(run_model, device_ids=[gpu])\n\n    run_wall(run_model, run_size, device, runs, (gpu == 0))\n'"
pytorch_toolkit/nncf/tools/benchmark_binarize_layers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\nimport torch\nimport torch.nn as nn\n\nfrom nncf.binarization.layers import XNORBinarize, DOREFABinarize, ActivationBinarizationScaleThreshold\nfrom tools.benchmark import run_profile\nfrom nncf.utils import get_per_channel_scale_shape\n\nNBITS = 8\nGPU_RUNS_LOW_BATCH = 10000\nGPU_RUNS_HIGH_BATCH = 100\nCPU_RUNS = 100\nLOW_BATCH_INPUT_SIZE = [1, 96, 112, 112]\nHIGH_BATCH_INPUT_SIZE = [128, 96, 112, 112]\nTEST_PARAMS_STRUCT = [(""low batch"", LOW_BATCH_INPUT_SIZE, GPU_RUNS_LOW_BATCH),\n                      (""high batch"", HIGH_BATCH_INPUT_SIZE, GPU_RUNS_HIGH_BATCH)]\n\n\n# reference impl\nclass ReferenceXNORBinarize(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        norm = x.abs().mean([1, 2, 3], keepdim=True)\n        sign = ((x > 0).type(x.dtype) * 2 - 1)\n        output = sign * norm\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\n\nclass ReferenceDOREFABinarize(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        norm = x.abs().mean()\n        sign = ((x > 0).type(x.dtype) * 2 - 1)\n        output_flat = sign * norm\n        return output_flat.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\n\nclass ReferenceActivationBinarize(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_, scale, threshold):\n        shape = [1 for s in input_.shape]\n        shape[1] = input_.shape[1]\n        t = (threshold*scale).view(shape)\n        output = (input_ > t).type(input_.dtype) * scale\n        ctx.save_for_backward(input_, scale, output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_, scale, output = ctx.saved_variables\n\n        # calc gradient for input\n        mask_lower = (input_ <= scale).type(input_.dtype)\n        grad_input = grad_output * (input_ >= 0).type(input_.dtype) * mask_lower\n\n        # calc gradient for scale\n        err = (output - input_) * scale.reciprocal()\n        grad_scale = grad_output * (mask_lower * err + (1 - mask_lower))\n        grad_scale = grad_scale.sum().view(1)\n\n        # calc gradient for threshold\n        grad_threshold = -grad_output * (input_ > 0).type(input_.dtype) * (input_ < scale).type(input_.dtype)\n\n        for idx, _ in enumerate(input_.shape):\n            if idx != 1:  # sum over all dims except activations channel\n                grad_threshold = grad_threshold.sum(idx, keepdim=True)\n\n        return grad_input, grad_scale, grad_threshold\n\n\nclass ReferenceWeightBinarizationModule(nn.Module):\n    def __init__(self, mode=\'xnor\'):\n        super().__init__()\n        self.mode = mode\n        if self.mode == \'xnor\':\n            self.binarize = ReferenceXNORBinarize.apply\n        elif self.mode == \'dorefa\':\n            self.binarize = ReferenceDOREFABinarize.apply\n\n    def forward(self, input_):\n        return self.binarize(input_)\n\n\ndef get_test_scale(num_channels):\n    torch.manual_seed(0)\n    retval = torch.Tensor(num_channels)\n    retval.random_(0, 1)\n    return retval\n\n\ndef get_test_threshold(input_shape):\n    torch.manual_seed(0)\n    threshold_shape = get_per_channel_scale_shape(input_shape, is_weights=False)\n    retval = torch.Tensor(torch.zeros(threshold_shape))\n    retval.random_(-10, 10)\n    return retval\n\n\nclass ReferenceActivationBinarizationModule(nn.Module):\n    def __init__(self, input_shape):\n        super().__init__()\n        self.input_shape = input_shape\n        self.scale = torch.nn.Parameter(get_test_scale(num_channels=1))\n        self.threshold = torch.nn.Parameter(get_test_threshold(input_shape))\n\n    def forward(self, input_):\n        return ReferenceActivationBinarize.apply(input_, self.scale, self.threshold)\n\n\nif __name__ == \'__main__\':\n    for input_name, input_size, gpu_runs in TEST_PARAMS_STRUCT:\n        print()\n        print(""CUDA "" + input_name)\n        print(""------------------------------------------------"")\n        print(""Pytorch XNOR weight binarization (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            ReferenceWeightBinarizationModule(\'xnor\').cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs,\n            forward_only=True)\n\n        print()\n        print(""Custom XNOR weight binarization (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            XNORBinarize(enabled=True).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs,\n            forward_only=True)\n\n        print()\n        print(""Pytorch DoReFa weight binarization (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            ReferenceWeightBinarizationModule(\'dorefa\').cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs,\n            forward_only=True)\n\n        print()\n        print(""Custom DoReFa weight binarization (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            DOREFABinarize(enabled=True).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs,\n            forward_only=True)\n\n        print()\n        print(""Pytorch scale/threshold activation binarization (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            ReferenceActivationBinarizationModule(input_shape=LOW_BATCH_INPUT_SIZE).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Custom scale/threshold activation binarization (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        # Init the scales now so that it does not affect the benchmark\n        act_bin_module = ActivationBinarizationScaleThreshold(input_shape=LOW_BATCH_INPUT_SIZE, enabled=True)\n        act_bin_module.scale = torch.nn.Parameter(get_test_scale(1))\n        act_bin_module.threshold = torch.nn.Parameter(get_test_threshold(LOW_BATCH_INPUT_SIZE))\n        act_bin_module.is_scale_initialized = True\n        run_profile(\n            act_bin_module.cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n\n    # CPU low batch\n    print()\n    print(""CPU low batch"")\n    print(""------------------------------------------------"")\n    print(""Pytorch XNOR weight binarization (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        ReferenceWeightBinarizationModule(\'xnor\'),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS,\n        forward_only=True)\n\n    print()\n    print(""Custom XNOR weight binarization (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        XNORBinarize(enabled=True),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS,\n        forward_only=True)\n\n    print()\n    print(""Pytorch DoReFa weight binarization (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        ReferenceWeightBinarizationModule(\'dorefa\'),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS,\n        forward_only=True)\n\n    print()\n    print(""Custom DoReFa weight binarization (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        DOREFABinarize(enabled=True),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS,\n        forward_only=True)\n\n    print()\n    print(""Pytorch scale/threshold activation binarization (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        ReferenceActivationBinarizationModule(input_shape=LOW_BATCH_INPUT_SIZE),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Custom scale/threshold activation binarization (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    # Init the scales now so that it does not affect the benchmark\n    act_bin_module = ActivationBinarizationScaleThreshold(input_shape=LOW_BATCH_INPUT_SIZE, enabled=True)\n    act_bin_module.scale = torch.nn.Parameter(get_test_scale(1))\n    act_bin_module.threshold = torch.nn.Parameter(get_test_threshold(LOW_BATCH_INPUT_SIZE))\n    act_bin_module.is_scale_initialized = True\n    run_profile(\n        act_bin_module,\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n'"
pytorch_toolkit/nncf/tools/benchmark_quantize_layers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\nimport torch\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom nncf.quantization.layers import QuantizerConfig, AsymmetricQuantizer, SymmetricQuantizer\nfrom nncf.utils import sum_like, get_per_channel_scale_shape\n\nfrom tools.benchmark import run_profile, run_wall, run_worker\n\nTIME_SCALES = {\'ms\': 1000}\nNBITS = 8\nGPU_RUNS_LOW_BATCH = 10000\nGPU_RUNS_HIGH_BATCH = 100\nCPU_RUNS = 100\nLOW_BATCH_INPUT_SIZE = [1, 96, 112, 112]\nHIGH_BATCH_INPUT_SIZE = [128, 96, 112, 112]\nTEST_PARAMS_STRUCT = [(""low batch"", LOW_BATCH_INPUT_SIZE, GPU_RUNS_LOW_BATCH),\n                      (""high batch"", HIGH_BATCH_INPUT_SIZE, GPU_RUNS_HIGH_BATCH)]\n\n\n# reference impl\nclass ReferenceQuantizeSymmetric(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_, scale, bits):\n        level_high = scale.new_tensor([2 ** (bits - 1) - 1])\n        level_low = scale.new_tensor([-(level_high + 1)])\n        s = level_high / scale\n\n        output = input_ * s\n        output = output.clamp(min=level_low[0], max=level_high[0])\n        output = output.round()\n        output = output / s\n\n        ctx.save_for_backward(input_, scale, output)\n        ctx.level_high = level_high\n        ctx.level_low = level_low\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_, scale, output = ctx.saved_tensors\n        level_high = ctx.level_high\n        level_low = ctx.level_low\n\n        alpha = float(level_low) / float(level_high)\n        mask_hi = (input_ > scale).type(input_.dtype)\n        mask_lo = (input_ < scale * alpha).type(input_.dtype)\n        mask_in = 1 - mask_hi - mask_lo\n\n        val_grad_out = mask_hi + alpha * mask_lo\n        err = (output - input_) * scale.reciprocal()\n        grad_scale = grad_output * (err * mask_in + val_grad_out)\n        grad_scale = sum_like(grad_scale, scale)\n\n        # calc gradient for input\n        grad_input = grad_output * mask_in\n\n        return grad_input, grad_scale, None\n\n\nclass ReferenceQuantize(nn.Module):\n    def __init__(self, num_bits=8, input_shape=None, is_weights=True, per_channel=False):\n        super().__init__()\n        self.input_shape = input_shape\n        self.is_weights = is_weights\n        scale_shape = 1\n        if per_channel:\n            scale_shape = get_per_channel_scale_shape(self.input_shape, self.is_weights)\n\n        self.scale = nn.Parameter(torch.ones(scale_shape))\n        self.num_bits = num_bits\n        self.level_high = 2 ** (self.num_bits - 1) - 1\n        self.level_low = -(self.level_high + 1)\n        self.quantize = ReferenceQuantizeSymmetric.apply\n\n    def get_scale(self):\n        return self.scale\n\n    def forward(self, input_):\n        return self.quantize(input_, self.scale, self.num_bits)\n\n\n\nif __name__ == \'__main__\':\n    for input_name, input_size, gpu_runs in TEST_PARAMS_STRUCT:\n        print(""CUDA "" + input_name)\n        print(""------------------------------------------------"")\n        print(""Pytorch Symmetric (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            ReferenceQuantize(NBITS).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Custom Symmetric (cuda 0 ) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            SymmetricQuantizer(QuantizerConfig(bits=NBITS)).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Pytorch Symmetric Per Weight Channel (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            ReferenceQuantize(NBITS,\n                              input_shape=input_size,\n                              per_channel=True,\n                              is_weights=True).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Custom Symmetric Per Weight Channel  (cuda 0 ) impl"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            SymmetricQuantizer(QuantizerConfig(bits=NBITS,\n                                               input_shape=input_size,\n                                               per_channel=True,\n                                               is_weights=True)).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Pytorch Symmetric Per Activation Channel (cuda 0) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            ReferenceQuantize(NBITS,\n                              input_shape=input_size,\n                              per_channel=True,\n                              is_weights=False).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Custom Symmetric Per Activation Channel  (cuda 0 ) impl"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            SymmetricQuantizer(QuantizerConfig(bits=NBITS,\n                                               input_shape=input_size,\n                                               per_channel=True,\n                                               is_weights=False)).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Custom Asymmetric (cuda 0 ) impl:"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            AsymmetricQuantizer(QuantizerConfig(bits=NBITS)).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Custom Asymmetric Per Weight Channel  (cuda 0 ) impl"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            AsymmetricQuantizer(QuantizerConfig(bits=NBITS,\n                                                input_shape=input_size,\n                                                per_channel=True,\n                                                is_weights=True)).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n        print()\n        print(""Custom Asymmetric Per Activation Channel  (cuda 0 ) impl"")\n        print(""input size: {0}"".format(input_size))\n        run_profile(\n            AsymmetricQuantizer(QuantizerConfig(bits=NBITS,\n                                                input_shape=input_size,\n                                                per_channel=True,\n                                                is_weights=False)).cuda(),\n            input_size,\n            \'cuda\',\n            gpu_runs)\n\n\n    # CPU low batch\n    print()\n    print(""CPU low batch"")\n    print(""------------------------------------------------"")\n    print(""Pytorch Symmetric(cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        ReferenceQuantize(NBITS),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Custom Symmetric (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        SymmetricQuantizer(QuantizerConfig(bits=NBITS)),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Pytorch Symmetric Per Weight Channel (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        ReferenceQuantize(NBITS,\n                          input_shape=LOW_BATCH_INPUT_SIZE,\n                          per_channel=True,\n                          is_weights=True),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Custom Symmetric Per Weight Channel  (cpu) impl"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        SymmetricQuantizer(QuantizerConfig(bits=NBITS,\n                                           input_shape=LOW_BATCH_INPUT_SIZE,\n                                           per_channel=True,\n                                           is_weights=True)),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Pytorch Symmetric Per Activation Channel (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        ReferenceQuantize(NBITS,\n                          input_shape=LOW_BATCH_INPUT_SIZE,\n                          per_channel=True,\n                          is_weights=False),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Custom Symmetric Per Activation Channel  (cpu) impl"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        SymmetricQuantizer(QuantizerConfig(bits=NBITS,\n                                           input_shape=LOW_BATCH_INPUT_SIZE,\n                                           per_channel=True,\n                                           is_weights=False)),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Custom Asymmetric (cpu) impl:"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        AsymmetricQuantizer(QuantizerConfig(bits=NBITS)),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Custom Asymmetric Per Weight Channel  (cpu) impl"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        AsymmetricQuantizer(QuantizerConfig(bits=NBITS,\n                                            input_shape=LOW_BATCH_INPUT_SIZE,\n                                            per_channel=True,\n                                            is_weights=True)),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n    print()\n    print(""Custom Asymmetric Per Activation Channel  (cpu) impl"")\n    print(""input size: {0}"".format(LOW_BATCH_INPUT_SIZE))\n    run_profile(\n        AsymmetricQuantizer(QuantizerConfig(bits=NBITS,\n                                            input_shape=LOW_BATCH_INPUT_SIZE,\n                                            per_channel=True,\n                                            is_weights=False)),\n        LOW_BATCH_INPUT_SIZE,\n        \'cpu\',\n        CPU_RUNS)\n\n\n    # CUDA DataParallel high batch\n    device_ids = range(torch.cuda.device_count())\n    print()\n    print(""CUDA DataParallel high batch"")\n    print(""------------------------------------------------"")\n    print(""Pytorch Symmetric(cuda {0}) DataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    run_profile(\n        nn.parallel.DataParallel(ReferenceQuantize(NBITS).cuda(), device_ids=device_ids),\n        HIGH_BATCH_INPUT_SIZE,\n        \'cuda\',\n        GPU_RUNS_HIGH_BATCH)\n\n    print()\n    print(""Custom Symmetric (cuda {0}) DataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    run_profile(\n        nn.parallel.DataParallel(SymmetricQuantizer(QuantizerConfig(bits=NBITS)).cuda(),\n                                 device_ids=device_ids),\n        HIGH_BATCH_INPUT_SIZE,\n        \'cuda\',\n        GPU_RUNS_HIGH_BATCH)\n\n    print()\n    print(""Custom Asymmetric (cuda {0}) DataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    run_profile(\n        nn.parallel.DataParallel(AsymmetricQuantizer(QuantizerConfig(bits=NBITS)).cuda(),\n                                 device_ids=device_ids),\n        HIGH_BATCH_INPUT_SIZE,\n        \'cuda\',\n        GPU_RUNS_HIGH_BATCH)\n\n    # CUDA DataParallel high batch\n    # wall time\n    print()\n    print(""CUDA DataParallel high batch"")\n    print(""------------------------------------------------"")\n    print(""Pytorch Symmetric(cuda {0}) DataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    run_wall(\n        nn.parallel.DataParallel(ReferenceQuantize(NBITS).cuda(), device_ids=device_ids),\n        HIGH_BATCH_INPUT_SIZE,\n        \'cuda\',\n        GPU_RUNS_HIGH_BATCH)\n\n    print()\n    print(""Custom Symmetric (cuda {0}) DataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    run_wall(\n        nn.parallel.DataParallel(SymmetricQuantizer(QuantizerConfig(bits=NBITS)).cuda(),\n                                 device_ids=device_ids),\n        HIGH_BATCH_INPUT_SIZE,\n        \'cuda\',\n        GPU_RUNS_HIGH_BATCH)\n\n    print()\n    print(""Custom Assymetric (cuda {0}) DataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    run_wall(\n        nn.parallel.DataParallel(AsymmetricQuantizer(QuantizerConfig(bits=NBITS)).cuda(),\n                                 device_ids=device_ids),\n        HIGH_BATCH_INPUT_SIZE,\n        \'cuda\',\n        GPU_RUNS_HIGH_BATCH)\n\n    # CUDA DistributedDataParallel high batch\n    # wall time\n    NGPUS_PER_NODE = len(device_ids)\n    WORLD_SIZE = NGPUS_PER_NODE\n    print()\n    print(""CUDA DistributedDataParallel high batch"")\n    print(""------------------------------------------------"")\n    print(""Pytorch Symmetric(cuda {0}) DistributedDataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    mp.spawn(\n        run_worker,\n        nprocs=NGPUS_PER_NODE,\n        args=(WORLD_SIZE, ReferenceQuantize(NBITS), TEST_PARAMS_STRUCT[1], GPU_RUNS_HIGH_BATCH))\n\n    print()\n    print(""Custom Symmetric (cuda {0}) DistributedDataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    mp.spawn(\n        run_worker,\n        nprocs=NGPUS_PER_NODE,\n        args=(WORLD_SIZE, SymmetricQuantizer(QuantizerConfig(bits=NBITS)), TEST_PARAMS_STRUCT[1],\n              GPU_RUNS_HIGH_BATCH))\n\n    print()\n    print(""Custom Asymmetric (cuda {0}) DistributedDataParallel impl:"".format(device_ids))\n    print(""input size: {0}"".format(HIGH_BATCH_INPUT_SIZE))\n    mp.spawn(\n        run_worker,\n        nprocs=NGPUS_PER_NODE,\n        args=(WORLD_SIZE, SymmetricQuantizer(QuantizerConfig(bits=NBITS)), TEST_PARAMS_STRUCT[1],\n              GPU_RUNS_HIGH_BATCH))\n'"
pytorch_toolkit/nncf/tools/convert_mobilenet_v2_to_torchvision.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nfrom argparse import ArgumentParser\n\nimport torch\nfrom os import listdir, makedirs\nfrom os.path import isfile, join, exists\n\n\ndef main(argv):\n    parser = ArgumentParser()\n    parser.add_argument(\'-i\', \'--input-folder\', help=\'Path to directory with given checkpoints to modify\',\n                        required=True)\n    parser.add_argument(\'-r\', \'--ref-folder\', help=\'Path to directory with reference (new) checkpoints\',\n                        required=True)\n    parser.add_argument(\'-o\', \'--output-folder\', help=\'Path to directory to save modified checkpoints\', required=True)\n    args = parser.parse_args(args=argv)\n\n    src_dir = args.input_folder\n    dst_dir = args.output_folder\n    ref_dir = args.ref_folder\n    if not exists(dst_dir):\n        makedirs(dst_dir)\n\n    pth_files = [(join(src_dir, f), join(dst_dir, f), join(ref_dir, f)) for f in listdir(src_dir) if\n                 isfile(join(src_dir, f)) and (\'.pth\' in f or \'.sd\' in f)]\n\n    for src_file, dst_file, ref_file in pth_files:\n        ref_sd = torch.load(ref_file)\n        if \'state_dict\' in ref_sd:\n            ref_sd = ref_sd[\'state_dict\']\n\n\n        sd = pth = torch.load(src_file)\n        if \'state_dict\' in pth:\n            sd = pth[\'state_dict\']\n\n        ref_keys = list(sorted(list(ref_sd.keys())))\n        old_keys = list(sorted(list(sd.keys())))\n\n        old_to_ref_map = {}\n\n        old_mean_0_keys = list(filter(lambda x: ""mean_0"" in x, old_keys))\n        old_mean_1_keys = list(filter(lambda x: ""mean_1"" in x, old_keys))\n        new_mean_0_keys = list(filter(lambda x: ""mean_0"" in x, ref_keys))\n\n        old_keys = list(filter(lambda x: x not in old_mean_0_keys, old_keys))\n        for idx, old_mean_1_key in enumerate(old_mean_1_keys):\n            old_to_ref_map[old_mean_1_key] = new_mean_0_keys[idx]\n            old_keys.remove(old_mean_1_key)\n            ref_keys.remove(new_mean_0_keys[idx])\n\n        first_weight_related_old_key = next(x for x in old_keys if ""activation_quantizers"" not in x)\n        first_weight_related_old_idx = old_keys.index(first_weight_related_old_key)\n\n        old_act_keys = old_keys[:first_weight_related_old_idx]\n        old_weight_keys = old_keys[first_weight_related_old_idx:]\n\n        ref_act_keys = ref_keys[:first_weight_related_old_idx]\n        ref_weight_keys = ref_keys[first_weight_related_old_idx:]\n\n        old_base_act_keys = list(filter(lambda x: ""Inverted"" in x, old_act_keys))\n        ref_base_act_keys = list(filter(lambda x: ""Inverted"" in x, ref_act_keys))\n\n        old_other_act_keys = list(filter(lambda x: x not in old_base_act_keys, old_act_keys))\n        ref_other_act_keys = list(filter(lambda x: x not in ref_base_act_keys, ref_act_keys))\n\n        for idx, old_key in enumerate(old_weight_keys):\n            old_to_ref_map[old_key] = ref_weight_keys[idx]\n\n        for idx, old_key in enumerate(old_base_act_keys):\n            old_to_ref_map[old_key] = ref_base_act_keys[idx]\n\n        for idx, old_key in enumerate(old_other_act_keys):\n            old_to_ref_map[old_key] = ref_other_act_keys[idx]\n\n        assert len(old_to_ref_map) == len(ref_sd)\n\n        new_sd = {}\n\n        for old_key, ref_key in old_to_ref_map.items():\n            new_sd[ref_key] = sd[old_key]\n\n        pth[\'state_dict\'] = new_sd\n        torch.save(pth, dst_file)\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n'"
pytorch_toolkit/nncf/tools/correct_checkpoint.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nfrom argparse import ArgumentParser\nfrom collections import OrderedDict\n\nimport torch\n\nfrom nncf.registry import Registry\n\nINCEPTION_NAME = \'inception\'\nRESNET_NAME = \'resnet\'\nMOBILENET_NAME = \'mobilenet\'\n\nKEYS_REPLACERS = Registry(""keys_replacers"")\n\n\n@KEYS_REPLACERS.register(INCEPTION_NAME)\ndef inception_replacer(k):\n    if \'RELU\' in k:\n        return k.replace(\'335\', \'0\')\n    return k\n\n\n@KEYS_REPLACERS.register(MOBILENET_NAME)\ndef mobilenet_replacer(k):\n    keywords = [\'hardtanh\', \'batch_norm\', \'__add__\']\n    if any(x in k for x in keywords):\n        return k.replace(\'63\', \'0\').replace(\'62\', \'0\').replace(\'111\', \'0\')\n    return k\n\n\n@KEYS_REPLACERS.register(RESNET_NAME)\ndef resnet_replacer(k):\n    if \'RELU\' in k:\n        return k.replace(\'96\', \'0\').replace(\'100\', \'1\').replace(\'109\', \'2\').replace(\'194\', \'0\')\n    if \'BatchNorm2d\' in k:\n        return k.replace(\'103\', \'0\').replace(\'106\', \'0\')\n    return k\n\n\ndef main(argv):\n    parser = ArgumentParser()\n    parser.add_argument(\'-i\', \'--input-model\', help=\'Path to input model file\', required=True)\n    parser.add_argument(\'-o\', \'--output-model\', help=\'Path to output model file\', required=True)\n    parser.add_argument(\'-n\', \'--name\', help=\'Name of model\', choices=[INCEPTION_NAME, RESNET_NAME, MOBILENET_NAME],\n                        required=True)\n    args = parser.parse_args(args=argv)\n\n    pth = torch.load(args.input_model)\n    sd = pth[\'state_dict\']\n\n    replace_key_fn = KEYS_REPLACERS.get(args.name)\n    new_sd = OrderedDict()\n    for k, v in sd.items():\n        new_k = replace_key_fn(k)\n        if new_k != k:\n            print(\'{}\\n{}\\n\\n\'.format(k, new_k))\n        new_sd[replace_key_fn(k)] = v\n    pth[\'state_dict\'] = new_sd\n\n    torch.save(pth, args.output_model)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n'"
pytorch_toolkit/nncf/tools/ir_utils.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import OrderedDict\n\nimport xml.etree.cElementTree as ET\nimport numpy as np\n\n\nclass Parameter:\n    def __init__(self, data, size, offset, shape):\n        self.shape = shape\n        self.offset = offset\n        self.size = size\n        self.data = data\n\n\ndef get_ir_paths(model_arg, bin_arg):\n    if model_arg.endswith("".xml""):\n        model_xml = model_arg\n    else:\n        model_xml = model_arg + "".xml""\n    if bin_arg is not None:\n        model_bin = bin_arg\n    else:\n        model_bin = model_xml.replace("".xml"", "".bin"")\n    return model_bin, model_xml\n\n\ndef find_all_parameters(buffer, model_xml):\n    ir_tree = ET.parse(model_xml)\n    all_parameters = OrderedDict()\n    for layer in ir_tree.iter(""layer""):\n        if layer.get(""type"") not in {""Convolution"", ""FullyConnected"", ""ScaleShift""}:\n            continue\n\n        get_weight_shape_fn = get_conv_weight_shape\n        if layer.get(""type"") == ""FullyConnected"":\n            get_weight_shape_fn = get_fc_weight_shape\n        if layer.get(""type"") == ""ScaleShift"":\n            get_weight_shape_fn = get_ss_weight_shape\n\n        extract_params(buffer, all_parameters, layer, get_weight_shape_fn)\n\n    return all_parameters\n\n\ndef get_conv_weight_shape(layer, input_shape, output_shape):\n    groups = int(layer.find(\'data\').get(""group"", 1))\n    kernel_size = [int(dim) for dim in layer.find(\'data\').get(""kernel"").split("","")]\n    return [output_shape[1], input_shape[1] // groups, *kernel_size]\n\n\ndef get_fc_weight_shape(layer, input_shape, output_shape):\n    return [output_shape[1], input_shape[1]]\n\n\ndef get_ss_weight_shape(layer, input_shape, output_shape):\n    return [output_shape[1]]\n\n\ndef extract_params(buffer, all_parameters, layer, get_weight_shape_fn):\n    layer_name = layer.get(""name"")\n    precision = np.float32 if layer.get(""precision"").lower() == \'fp32\' else np.float16\n    weight = layer.find(""blobs/weights"")\n    biases = layer.find(""blobs/biases"")\n    input_shape = [int(dim.text) for dim in layer.find(""input/port"")]\n    output_shape = [int(dim.text) for dim in layer.find(""output/port"")]\n\n    weight_shape = get_weight_shape_fn(layer, input_shape, output_shape)\n    weight_offset = int(weight.get(""offset""))\n    weight_size = int(weight.get(""size""))\n    weight_data = get_blob(buffer, weight_offset, weight_size, weight_shape, precision)\n    param = Parameter(weight_data, weight_size, weight_offset, weight_shape)\n    all_parameters[""{}.weight"".format(layer_name)] = param\n    if biases is not None:\n        bias_shape = [output_shape[1]]\n        bias_size = int(biases.get(""size""))\n        bias_offset = int(biases.get(""offset""))\n\n        bias_data = get_blob(buffer, bias_offset, bias_size, bias_shape, precision)\n        bias_param = Parameter(bias_data, bias_size, bias_offset, bias_shape)\n        all_parameters[""{}.bias"".format(layer_name)] = bias_param\n\n\ndef get_blob(buffer, offset, size, shape, dtype=np.float32):\n    data = np.frombuffer(buffer[offset:offset + size], dtype=dtype).copy()\n    if shape is not None:\n        data = data.reshape(shape)\n    return data\n'"
pytorch_toolkit/nncf/tools/onnx_tranformer.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nfrom argparse import ArgumentParser\n\nimport onnx\n\ndef rename_quantize(model):\n    for node in model.graph.node:\n        if node.op_type == \'Quantize\':\n            node.op_type = \'FakeQuantize\'\n            node.doc_string = \'Fake quantization operation\'\n\ndef main(argv):\n    parser = ArgumentParser()\n    parser.add_argument(\'-i\', \'--input-model\', help=\'Path to input model file\',\n                        required=True)\n    parser.add_argument(\'-o\', \'--output-model\', help=\'Path to output model file\',\n                        required=True)\n    args = parser.parse_args(args=argv)\n\n    model = onnx.load(args.input_model)\n\n    rename_quantize(model)\n\n    onnx.save(model, args.output_model)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n'"
pytorch_toolkit/nncf/tools/update_eval_results.py,0,"b'""""""\nCopyright (c) 2020 Intel Corporation\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nThis is a helper script which parses a metrics.json file containing currently measured accuracy values for checkpoints\nregistered in tests/sota_checkpoints_eval.json, and produces:\na) a file with exactly the same contents as tests/sota_checkpoints_eval.json, but with target accuracy scores\n updated to reflect what was currently measured and reported in metrics.json, and\nb) a set of .MD file snippets containing updated results tables, ready for copy-and-paste into corresponding\n README.md files (a common frontpage README and sample-specific readmes).\n\nUsage:\npython update_eval_results.py --results path/to/metrics.json --config path/to/sota_checkpoints_eval.json\n-o path/to/new_config.json\n""""""\n\n\nimport os\nimport json\nfrom collections import OrderedDict\nfrom mdutils import MdUtils\nimport argparse\n\n\ndef examples_table_maker(sample_type, sota_checkpoints_dict, metrics):\n    examples_row = [""Model"", ""Compression algorithm"", ""Dataset"", ""PyTorch compressed accuracy"", ""Config path"",\n                    ""PyTorch Checkpoint""]\n    common_row = [""Model"", ""Compression algorithm"", ""Dataset"", ""PyTorch FP32 baseline"", ""PyTorch compressed accuracy""]\n    for sample_ in sota_checkpoints_dict:\n        datasets = sota_checkpoints_dict[sample_]\n        for data_name_ in datasets:\n            dataset_name = data_conversion(data_name_)\n            model_name = datasets[data_name_]\n            for configs in model_name:\n                conf_file = model_name[configs].get(\'config\', {})\n                reference = None\n                if model_name[configs].get(\'reference\', {}):\n                    reference = model_name[configs].get(\'reference\', {})\n                if model_name[configs].get(\'resume\', {}):\n                    resume = model_name[configs].get(\'resume\', {})\n                else:\n                    resume = os.path.basename(conf_file).replace("".json"", "".pth"")\n                table_name = model_name[configs].get(\'model_description\', {})\n                if model_name[configs].get(\'compression_description\', {}):\n                    compression = model_name[configs].get(\'compression_description\', {})\n                else:\n                    compression = None\n                if compression is None and sample_type == ""classification"" and (""mobilenet_v2"" not in model_name):\n                    checkpoint_link = ""-""\n                else:\n                    checkpoint_link = \'https://download.01.org/opencv/openvino_training_extensions/models/nncf/\'\\\n                                      + resume\n                if sample_ == sample_type:\n                    row_ext = [str(table_name), str(compression), str(dataset_name), str(metrics[configs]),\n                               str(conf_file), str(checkpoint_link)]\n                    examples_row.extend(row_ext)\n                    md_table_writer(sample_, examples_row)\n                fp32_ref_metric = fp32_ref(reference, metrics)\n                common_row_ext = [str(table_name), str(compression), str(dataset_name), str(fp32_ref_metric),\n                                  str(metrics[configs])]\n                common_row.extend(common_row_ext)\n                md_table_writer(""common"", common_row)\n                if args.output is not None:\n                    model_name[configs][\'target\'] = measured_metrics[configs]\n\n\ndef fp32_ref(reference_, metrics_):\n    if reference_ is not None:\n        fp32_ref_metric = metrics_[reference_]\n    else:\n        fp32_ref_metric = ""-""\n    return fp32_ref_metric\n\n\ndef data_conversion(data_name):\n    if data_name == \'imagenet\':\n        dataset_name = \'ImageNet\'\n    elif data_name == \'camvid\':\n        dataset_name = \'CamVid\'\n    elif data_name == \'VOCdevkit\':\n        dataset_name = \'VOC12+07\'\n    else:\n        dataset_name = ""Mapillary""\n    return dataset_name\n\n\ndef lines_delete(file_name):\n    with open(file_name) as input_data:\n        lines = input_data.readlines()\n    with open(file_name, \'w\') as out:\n        out.writelines(lines[4:])\n\n\ndef md_table_writer(name, rows):\n    if name == \'common\':\n        mdfile = MdUtils(file_name=\'results_common\')\n        mdfile.new_table(columns=5, rows=int(len(rows)/5), text=rows, text_align=\'center\')\n    else:\n        mdfile = MdUtils(file_name=\'results_{}\'.format(name))\n        mdfile.new_table(columns=6, rows=int(len(rows)/6), text=rows, text_align=\'center\')\n    mdfile.create_md_file()\n    lines_delete(\'results_{}.md\'.format(name))\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--results\', \'-r\', help=\'Metrics file\')\nparser.add_argument(\'--config\', \'-c\', help=\'Config .json file\')\nparser.add_argument(\'--output\', \'-o\', help=""Added new target value"")\nargs = parser.parse_args()\nresults = args.results\nconfig = args.config\noutput = args.output\n\nmeasured_metrics = json.load(open(results, \'r\'))\nsota_checkpoints_eval = json.load(open(config), object_pairs_hook=OrderedDict)\n\nfor sample_type_ in sota_checkpoints_eval:\n    examples_table_maker(sample_type_, sota_checkpoints_eval, measured_metrics)\n\nif args.output is not None:\n    with open(output, ""w"") as write_file:\n        json.dump(sota_checkpoints_eval, write_file, indent=8)\n'"
pytorch_toolkit/object_detection/tests/export_tests_face_detection.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom common.test_case import export_test_case\nfrom common.utils import replace_text_in_file\n\n\ndef face_detection_test_case(model_name, alt_ssd_export):\n    class ExportTestCase(export_test_case(\'face-detection\', model_name, alt_ssd_export=alt_ssd_export)):\n        def setUp(self):\n            super().setUp()\n\n            assert replace_text_in_file(self.configuration_file, \'data/WIDERFace\',\n                                        \'../../data/airport\')\n            assert replace_text_in_file(self.configuration_file, \'val.json\',\n                                        \'annotation_faces_train.json\')\n\n    return ExportTestCase\n\n\nclass FaceDetection0100TestCase(face_detection_test_case(\'face-detection-0100\', True)):\n    """""" Test case for face-detection-0100 model export. """"""\n\n\nclass FaceDetection0102TestCase(face_detection_test_case(\'face-detection-0102\', True)):\n    """""" Test case for face-detection-0102 model export. """"""\n\n\nclass FaceDetection0104TestCase(face_detection_test_case(\'face-detection-0104\', True)):\n    """""" Test case for face-detection-0104 model export. """"""\n\n\nclass FaceDetection0105TestCase(face_detection_test_case(\'face-detection-0105\', False)):\n    """""" Test case for face-detection-0105 model export. """"""\n\n\nclass FaceDetection0106TestCase(face_detection_test_case(\'face-detection-0106\', False)):\n    """""" Test case for face-detection-0106 model export. """"""\n'"
pytorch_toolkit/object_detection/tests/export_tests_person_vehicle_bike_detection.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n\nfrom common.test_case import export_test_case\n\n\nclass PersonVehicleBikeDetectionCrossroad1016TestCase(\n        export_test_case(\'person-vehicle-bike-detection\',\n                         \'person-vehicle-bike-detection-crossroad-1016\',\n                         \'person_vehicle_bike_sd512_mb2_clustered_epoch_21.pth\',\n                         True)):\n    """""" Test case for person-vehicle-bike-detection-crossroad-1016 export. """"""\n'"
pytorch_toolkit/object_detection/tests/run_export_tests.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains unit tests. """"""\n\nimport sys\nimport unittest\n\n\ndef main():\n    testsuite = unittest.TestLoader().discover(\'tests\', pattern=\'export_tests_*.py\')\n    ret = not unittest.TextTestRunner(verbosity=1).run(testsuite).wasSuccessful()\n    sys.exit(ret)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/tests/run_inference_tests.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains unit tests. """"""\n\nimport unittest\n\n\nclass InferenceDummyTestCase(unittest.TestCase):\n    def test(self):\n        self.assertEqual(0, 0)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
pytorch_toolkit/object_detection/tests/run_train_tests.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains unit tests. """"""\n\nimport sys\nimport unittest\n\n\ndef main():\n    testsuite = unittest.TestLoader().discover(\'tests\', pattern=\'train_tests_*.py\')\n    ret = not unittest.TextTestRunner(verbosity=1).run(testsuite).wasSuccessful()\n    sys.exit(ret)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/tests/train_tests_face_detection.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport json\nimport os\nimport unittest\n\nfrom common.utils import replace_text_in_file, collect_ap, download_if_not_yet\n\n\ndef face_detection_test_case(model_name):\n    class Class(unittest.TestCase):\n\n        def setUp(self):\n            self.model_name = model_name\n\n            self.data_folder = \'../../data\'\n            self.work_dir = os.path.join(\'/tmp/\', self.model_name)\n            os.makedirs(self.work_dir, exist_ok=True)\n            self.configuration_file = f\'./face-detection/{self.model_name}/config.py\'\n            os.system(f\'cp {self.configuration_file} {self.work_dir}/\')\n            self.configuration_file = os.path.join(self.work_dir,\n                                                   os.path.basename(self.configuration_file))\n            self.ote_url = \'https://download.01.org/opencv/openvino_training_extensions\'\n            self.url = f\'{self.ote_url}/models/object_detection/{self.model_name}.pth\'\n            download_if_not_yet(self.work_dir, self.url)\n\n            assert replace_text_in_file(self.configuration_file, \'imgs_per_gpu=\',\n                                        \'imgs_per_gpu=1 ,#\')\n            assert replace_text_in_file(self.configuration_file, \'total_epochs = 70\',\n                                        \'total_epochs = 75\')\n            assert replace_text_in_file(self.configuration_file, \'data/WIDERFace\',\n                                        \'../../data/airport\')\n            assert replace_text_in_file(self.configuration_file, \'work_dir =\',\n                                        f\'work_dir = ""{os.path.join(self.work_dir, ""outputs"")}"" #\')\n            assert replace_text_in_file(self.configuration_file, \'train.json\',\n                                        \'annotation_faces_train.json\')\n            assert replace_text_in_file(self.configuration_file, \'val.json\',\n                                        \'annotation_faces_train.json\')\n            assert replace_text_in_file(self.configuration_file, \'resume_from = None\',\n                                        f\'resume_from = ""{os.path.join(self.work_dir, self.model_name)}.pth""\')\n\n        def test_fine_tuning(self):\n            log_file = os.path.join(self.work_dir, \'test_fine_tuning.log\')\n            os.system(\n                f\'../../external/mmdetection/tools/dist_train.sh {self.configuration_file} 1 --validate 2>&1 |\'\n                f\' tee {log_file}\')\n            ap = collect_ap(log_file)\n            self.assertEqual(len((ap)), 5)\n            self.assertLess(ap[0], ap[-1])\n\n        def test_quality_metrics(self):\n            log_file = os.path.join(self.work_dir, \'test_quality_metrics.log\')\n            os.system(\n                f\'python ../../external/mmdetection/tools/test.py \'\n                f\'{self.configuration_file} \'\n                f\'{os.path.join(self.work_dir, self.model_name + "".pth"")} \'\n                f\'--out res.pkl --eval bbox 2>&1 | tee {log_file}\')\n            ap = collect_ap(log_file)\n\n            with open(f\'tests/expected_outputs/face-detection/{self.model_name}.json\') as read_file:\n                content = json.load(read_file)\n\n            self.assertEqual(content[\'map\'], ap[0])\n\n    return Class\n\n\nclass FaceDetection0100TestCase(face_detection_test_case(\'face-detection-0100\')):\n    """""" Test case for face-detection-0100 model. """"""\n\n\nclass FaceDetection0102TestCase(face_detection_test_case(\'face-detection-0102\')):\n    """""" Test case for face-detection-0102 model. """"""\n\n\nclass FaceDetection0104TestCase(face_detection_test_case(\'face-detection-0104\')):\n    """""" Test case for face-detection-0104 model. """"""\n\n\nclass FaceDetection0105TestCase(face_detection_test_case(\'face-detection-0105\')):\n    """""" Test case for face-detection-0105 model. """"""\n\n\nclass FaceDetection0106TestCase(face_detection_test_case(\'face-detection-0106\')):\n    """""" Test case for face-detection-0106 model. """"""\n'"
pytorch_toolkit/object_detection/tests/train_tests_person_vehicle_bike_detection.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport json\nimport os\nimport unittest\n\nfrom common.utils import replace_text_in_file, collect_ap, download_if_not_yet\n\n\nclass PersonVehicleBikeDetectionCrossroad1016TestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.model_name = \'person-vehicle-bike-detection-crossroad-1016\'\n        self.snapshot_name = \'person_vehicle_bike_sd512_mb2_clustered_epoch_21.pth\'\n\n        self.data_folder = \'../../data\'\n        self.work_dir = os.path.join(\'/tmp\', self.model_name)\n        os.makedirs(self.work_dir, exist_ok=True)\n        self.configuration_file = f\'./person-vehicle-bike-detection/{self.model_name}/config.py\'\n        os.system(f\'cp {self.configuration_file} {self.work_dir}/\')\n        self.configuration_file = os.path.join(self.work_dir,\n                                               os.path.basename(self.configuration_file))\n        self.ote_url = \'https://download.01.org/opencv/openvino_training_extensions\'\n        self.url = f\'{self.ote_url}/models/object_detection/{self.snapshot_name}\'\n        download_if_not_yet(self.work_dir, self.url)\n\n        assert replace_text_in_file(self.configuration_file, \'total_epochs =\',\n                                    \'total_epochs = 25#\')\n        assert replace_text_in_file(self.configuration_file, \'work_dir =\',\n                                    f\'work_dir = ""{os.path.join(self.work_dir, ""outputs"")}"" #\')\n        assert replace_text_in_file(self.configuration_file, \'load_from = None\',\n                                    f\'load_from = ""{os.path.join(self.work_dir, self.snapshot_name)}""\')\n        assert replace_text_in_file(self.configuration_file, \'annotation_example_train.json\',\n                                    \'annotation_example_val.json\')\n        assert replace_text_in_file(self.configuration_file, \'/train\', \'/val\')\n\n    def test_fine_tuning(self):\n        log_file = os.path.join(self.work_dir, \'test_fine_tuning.log\')\n\n        os.system(\n            f\'../../external/mmdetection/tools/dist_train.sh {self.configuration_file} 1 --validate 2>&1 |\'\n            f\' tee {log_file}\')\n        ap = collect_ap(log_file)\n        self.assertEqual(len((ap)), 25)\n        self.assertLess(ap[0], ap[-1])\n\n    def test_quality_metrics(self):\n        log_file = os.path.join(self.work_dir, \'test_quality_metrics.log\')\n        os.system(\n            f\'python ../../external/mmdetection/tools/test.py \'\n            f\'{self.configuration_file} \'\n            f\'{os.path.join(self.work_dir, self.snapshot_name)} \'\n            f\'--out res.pkl --eval bbox 2>&1 | tee {log_file}\')\n        ap = collect_ap(log_file)\n\n        with open(f\'tests/expected_outputs/person-vehicle-bike-detection/{self.model_name}.json\') as read_file:\n            content = json.load(read_file)\n\n        self.assertEqual(content[\'map\'], ap[0])\n'"
pytorch_toolkit/object_detection/tools/gen_markdown_table.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This script builds markdown table with information about models. """"""\n\nimport argparse\nfrom collections import OrderedDict\nimport glob\nimport yaml\n\n\ndef parse_args():\n    """""" Parses input args. """"""\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'problem_folder\')\n    return args.parse_args()\n\n\ndef get_header(metrics):\n    """""" Get header name from metrics dict. """"""\n\n    return f\'{metrics[""display_name""]} ({metrics[""unit""]})\'\n\n\ndef extract_info(descr):\n    """""" Extracts model infor from model.yml content. """"""\n\n    info = OrderedDict()\n    info.update({\'Model Name\': descr[\'name\']})\n\n    complexity = [metrics for metrics in descr[\'metrics\'] if metrics[\'key\'] == \'complexity\'][0]\n    info.update({get_header(complexity): complexity[\'value\']})\n\n    size = [metrics for metrics in descr[\'metrics\'] if metrics[\'key\'] == \'size\'][0]\n    info.update({get_header(size): size[\'value\']})\n\n    for metrics in descr[\'metrics\']:\n        if metrics[\'key\'] not in [\'size\', \'complexity\']:\n            info.update({get_header(metrics): metrics[\'value\']})\n\n    info.update(\n        {\'Links\': f\'[snapshot]({descr[""files""][0][""source""]}), [configuration file](./{descr[""name""]}/config.py)\'})\n\n    info.update({\'GPU_NUM\': descr[\'training_gpu_num\']})\n\n    return info\n\n\ndef build_table(infos):\n    """""" Builds markdown table. """"""\n\n    table_str = \'| \'\n    for key in infos[0].keys():\n        table_str += key + \' | \'\n    table_str += \'\\n\'\n\n    table_str += \'| \'\n    for key in infos[0].keys():\n        table_str += \'--- | \'\n    table_str += \'\\n\'\n\n    for info in infos:\n        table_str += \'| \'\n        for value in info.values():\n            table_str += str(value) + \' | \'\n        table_str += \'\\n\'\n\n    return table_str\n\n\ndef main():\n    """""" Main function. """"""\n\n    args = parse_args()\n    models_infos = []\n    for x in sorted(glob.glob(f\'{args.problem_folder}/**/model.yml\', recursive=True)):\n        with open(x) as read_file:\n            models_infos.append(extract_info(yaml.load(read_file)))\n\n    keys = models_infos[0].keys()\n\n    for info in models_infos:\n        assert keys == info.keys(), f\'{keys} does not equal to {info.keys()}\'\n\n    print(build_table(models_infos))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/open_closed_eye/utils/dataset_eye.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os\nimport os.path as osp\n\n\nclass EyeDataset(Dataset):\n    def __init__(self, root_folder, mode=\'train\', transformations=None):\n        self.transformations = transformations\n        self.data = []\n        for subdir, dirs, files in os.walk(osp.join(root_folder, mode)):\n            for i, file in enumerate(files):                \n                full_path = os.path.join(subdir, file)\n                state = 1 if file[0] == \'o\' else 0\n                if mode == \'train\':\n                    self.data.append({\'filename\': full_path, \'label\': state})\n                else:\n                    self.data.append({\'filename\': full_path, \'label\': state})\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        img = Image.open(item[\'filename\']).convert(\'RGB\')\n        if self.transformations is not None:\n            img = self.transformations(img)\n        return img, item[\'label\'], item[\'filename\']\n'"
pytorch_toolkit/open_closed_eye/utils/ie_tools.py,0,"b'""""""\n Copyright (c) 2018-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nimport os\n\nimport glog as log\nimport numpy as np\nfrom openvino.inference_engine import IENetwork, IEPlugin  # pylint: disable=import-error,E0611\n\n\nclass IEModel:\n    """"""Class for inference of models in the Inference Engine format""""""\n    def __init__(self, exec_net, inputs_info, input_key, output_key):\n        self.net = exec_net\n        self.inputs_info = inputs_info\n        self.input_key = input_key\n        self.output_key = output_key\n\n    def forward(self, img):\n        """"""Performs forward pass of the wrapped IE model""""""\n        res = self.net.infer(inputs={self.input_key: np.expand_dims(img.transpose(2, 0, 1), axis=0)})\n        return np.copy(res[self.output_key])\n\n    def get_input_shape(self):\n        """"""Returns an input shape of the wrapped IE model""""""\n        return self.inputs_info[self.input_key]\n\n\ndef load_ie_model(model_xml, device, plugin_dir, cpu_extension=\'\'):\n    """"""Loads a model in the Inference Engine format""""""\n    model_bin = os.path.splitext(model_xml)[0] + "".bin""\n    # Plugin initialization for specified device and load extensions library if specified\n    plugin = IEPlugin(device=device, plugin_dirs=plugin_dir)\n    if cpu_extension and \'CPU\' in device:\n        plugin.add_cpu_extension(cpu_extension)\n    # Read IR\n    log.info(""Loading network files:\\n\\t%s\\n\\t%s"", model_xml, model_bin)\n    net = IENetwork(model=model_xml, weights=model_bin)\n\n    if ""CPU"" in plugin.device:\n        supported_layers = plugin.get_supported_layers(net)\n        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n        if not_supported_layers:\n            log.error(""Following layers are not supported by the plugin for specified device %s:\\n %s"",\n                      plugin.device, \', \'.join(not_supported_layers))\n            log.error(""Please try to specify cpu extensions library path in sample\'s command line parameters using -l ""\n                      ""or --cpu_extension command line argument"")\n            sys.exit(1)\n\n    assert len(net.inputs.keys()) == 1, ""Checker supports only single input topologies""\n    assert len(net.outputs) == 1, ""Checker supports only single output topologies""\n\n    log.info(""Preparing input blobs"")\n    input_blob = next(iter(net.inputs))\n    out_blob = next(iter(net.outputs))\n    net.batch_size = 1\n\n    # Loading model to the plugin\n    log.info(""Loading model to the plugin"")\n    exec_net = plugin.load(network=net)\n    model = IEModel(exec_net, net.inputs, input_blob, out_blob)\n    del net\n    return model\n'"
pytorch_toolkit/open_closed_eye/utils/utils.py,0,"b'""""""\n Copyright (c) 2018-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\ndef save_model_cpu(net, optim, ckpt_fname, epoch, write_solverstate=False):\n    """"""Saves model weights and optimizer state (optionally) to a file""""""\n    state_dict = net.state_dict()\n    for key in state_dict.keys():\n        state_dict[key] = state_dict[key].cpu()\n    snapshot_dict = {\n        \'epoch\': epoch,\n        \'state_dict\': state_dict}\n\n    if write_solverstate:\n        snapshot_dict[\'optimizer\'] = optim\n\n    torch.save(snapshot_dict, ckpt_fname)\n\n\ndef get_model_parameters_number(model, as_string=True):\n    """"""Returns a total number of trainable parameters in a specified model""""""\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if not as_string:\n        return params_num\n\n    if params_num // 10 ** 6 > 0:\n        flops_str = str(round(params_num / 10. ** 6, 2)) + \'M\'\n    elif params_num // 10 ** 3 > 0:\n        flops_str = str(round(params_num / 10. ** 3, 2)) + \'k\'\n    else:\n        flops_str = str(params_num)\n    return flops_str\n\n\ndef load_model_state(model, snap, device_id, eval_state=True):\n    """"""Loads model weight from a file produced by save_model_cpu""""""\n    if device_id != -1:\n        location = \'cuda:\' + str(device_id)\n    else:\n        location = \'cpu\'\n    state_dict = torch.load(snap, map_location=location)[\'state_dict\']\n\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        head = k[:7]\n        if head == \'module.\':\n            name = k[7:]  # remove `module.`\n        else:\n            name = k\n        new_state_dict[name] = v\n\n    model.load_state_dict(new_state_dict, strict=False)\n\n    if device_id != -1:\n        model.cuda(device_id)\n        cudnn.benchmark = True\n\n    if eval_state:\n        model.eval()\n    else:\n        model.train()\n\n    return model\n\n\ndef flip_tensor(x, dim):\n    """"""Flips a tensor along the specified axis""""""\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1) - 1, -1, -1),\n                                                    (\'cpu\', \'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n'"
pytorch_toolkit/person_reidentification/config/default_config.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom yacs.config import CfgNode as CN\n\n\ndef get_default_config():\n    cfg = CN()\n\n    # model\n    cfg.model = CN()\n    cfg.model.name = \'osnet_x1.0\'\n    cfg.model.pretrained = True  # Automatically load pretrained model weights if available\n    cfg.model.load_weights = \'\'  # Path to model weights\n    cfg.model.resume = \'\'  # Path to checkpoint for resume training\n    cfg.model.dropout = CN()\n    cfg.model.dropout.p = 0.0\n    cfg.model.dropout.mu = 0.1\n    cfg.model.dropout.sigma = 0.03\n    cfg.model.dropout.dist = \'none\'\n    cfg.model.feature_dim = 512\n    cfg.model.pooling_type = \'avg\'\n    cfg.model.IN_first = False\n    cfg.model.extra_blocks = False\n    cfg.model.openvino = CN()\n    cfg.model.openvino.name = \'\'  # Path to *.xml\n    cfg.model.openvino.cpu_extension = \'\'\n    cfg.model.fpn = CN()\n    cfg.model.fpn.enable = True\n    cfg.model.fpn.dim = 256\n    cfg.model.fpn.process = \'concatenation\'\n\n    # data\n    cfg.data = CN()\n    cfg.data.type = \'image\'\n    cfg.data.root = \'reid-data\'\n    cfg.data.sources = [\'market1501\']\n    cfg.data.targets = [\'market1501\']\n    cfg.data.workers = 4  # Number of data loading workers\n    cfg.data.split_id = 0  # Split index\n    cfg.data.height = 256  # Image height\n    cfg.data.width = 128  # Image width\n    cfg.data.combineall = False  # Combine train, query and gallery for training\n    cfg.data.norm_mean = [0.485, 0.456, 0.406]  # Default is imagenet mean\n    cfg.data.norm_std = [0.229, 0.224, 0.225]  # Default is imagenet std\n    cfg.data.save_dir = \'log\'  # Path to save log\n\n    # specific datasets\n    cfg.market1501 = CN()\n    cfg.market1501.use_500k_distractors = False  # Add 500k distractors to the gallery set for market1501\n    cfg.cuhk03 = CN()\n    cfg.cuhk03.labeled_images = False  # Use labeled images, if False, use detected images\n    cfg.cuhk03.classic_split = False  # Use classic split by Li et al. CVPR14\n    cfg.cuhk03.use_metric_cuhk03 = False  # Use cuhk03\'s metric for evaluation\n\n    # sampler\n    cfg.sampler = CN()\n    cfg.sampler.train_sampler = \'RandomSampler\'\n    cfg.sampler.num_instances = 4  # Number of instances per identity for RandomIdentitySampler\n\n    # video reid setting\n    cfg.video = CN()\n    cfg.video.seq_len = 15  # Number of images to sample in a tracklet\n    cfg.video.sample_method = \'evenly\'  # How to sample images from a tracklet\n    cfg.video.pooling_method = \'avg\'  # How to pool features over a tracklet\n\n    # train\n    cfg.train = CN()\n    cfg.train.optim = \'adam\'\n    cfg.train.lr = 0.0003\n    cfg.train.weight_decay = 5e-4\n    cfg.train.max_epoch = 60\n    cfg.train.start_epoch = 0\n    cfg.train.batch_size = 32\n    cfg.train.fixbase_epoch = 0  # Number of epochs to fix base layers\n    cfg.train.open_layers = [\'classifier\']  # Layers for training while keeping others frozen\n    cfg.train.staged_lr = False  # Set different lr to different layers\n    cfg.train.new_layers = [\'classifier\']  # Newly added layers with default lr\n    cfg.train.base_lr_mult = 0.1  # Learning rate multiplier for base layers\n    cfg.train.lr_scheduler = \'single_step\'\n    cfg.train.stepsize = [20]  # Stepsize to decay learning rate\n    cfg.train.gamma = 0.1  # Learning rate decay multiplier\n    cfg.train.print_freq = 20  # Print frequency\n    cfg.train.seed = 1  # Random seed\n    cfg.train.warmup = 1   # After fixbase_epoch\n    cfg.train.warmup_factor_base = 0.1\n\n    # optimizer\n    cfg.sgd = CN()\n    cfg.sgd.momentum = 0.9  # Momentum factor for sgd and rmsprop\n    cfg.sgd.dampening = 0.  # Dampening for momentum\n    cfg.sgd.nesterov = False  # Nesterov momentum\n    cfg.rmsprop = CN()\n    cfg.rmsprop.alpha = 0.99  # Smoothing constant\n    cfg.adam = CN()\n    cfg.adam.beta1 = 0.9  # Exponential decay rate for first moment\n    cfg.adam.beta2 = 0.999  # Exponential decay rate for second moment\n\n    # loss\n    cfg.loss = CN()\n    cfg.loss.name = \'softmax\'\n    cfg.loss.softmax = CN()\n    cfg.loss.softmax.label_smooth = True  # Use label smoothing regularizer\n    cfg.loss.softmax.conf_pen = 0.0\n    cfg.loss.softmax.pr_product = False\n    cfg.loss.softmax.m = 0.35\n    cfg.loss.softmax.s = 30\n    cfg.loss.triplet = CN()\n    cfg.loss.triplet.margin = 0.3  # Distance margin\n    cfg.loss.triplet.weight_t = 1.0  # Weight to balance hard triplet loss\n    cfg.loss.triplet.weight_x = 0.  # Weight to balance cross entropy loss\n\n    # metric_losses\n    cfg.metric_losses = CN()\n    cfg.metric_losses.enable = False\n    cfg.metric_losses.local_push_weight = 0.0\n\n    # regularizers\n    cfg.reg = CN()\n    cfg.reg.ow = False\n    cfg.reg.ow_beta = 1e-3\n    cfg.reg.of = False\n    cfg.reg.of_beta = 1e-6\n    cfg.reg.of_start_epoch = 23\n\n    # test\n    cfg.test = CN()\n    cfg.test.batch_size = 100\n    cfg.test.dist_metric = \'cosine\'  # Distance metric, [\'euclidean\', \'cosine\']\n    cfg.test.normalize_feature = False  # Normalize feature vectors before computing distance\n    cfg.test.ranks = [1, 5, 10, 20]  # Cmc ranks\n    cfg.test.evaluate = False  # Test only\n    cfg.test.eval_freq = -1  # Evaluation frequency (-1 means to only test after training)\n    cfg.test.start_eval = 0  # Start to evaluate after a specific epoch\n    cfg.test.rerank = False  # Use person re-ranking\n    cfg.test.visrank = False  # Visualize ranked results (only available when cfg.test.evaluate=True)\n    cfg.test.visrank_topk = 10  # Top-k ranks to visualize\n    cfg.test.visactmap = False  # Visualize CNN activation maps\n    cfg.test.apply_masks = False\n    cfg.test.use_flip = False\n\n    # Augmentations\n    cfg.data.transforms = CN()\n\n    cfg.data.transforms.random_flip = CN()\n    cfg.data.transforms.random_flip.enable = True\n    cfg.data.transforms.random_flip.p = 0.5\n\n    cfg.data.transforms.random_crop = CN()\n    cfg.data.transforms.random_crop.enable = False\n    cfg.data.transforms.random_crop.p = 0.5\n\n    cfg.data.transforms.random_gray_scale = CN()\n    cfg.data.transforms.random_gray_scale.enable = False\n    cfg.data.transforms.random_gray_scale.p = 0.5\n\n    cfg.data.transforms.random_padding = CN()\n    cfg.data.transforms.random_padding.enable = False\n    cfg.data.transforms.random_padding.p = 0.5\n    cfg.data.transforms.random_padding.padding = (0, 10)\n\n    cfg.data.transforms.random_perspective = CN()\n    cfg.data.transforms.random_perspective.enable = False\n    cfg.data.transforms.random_perspective.p = 0.5\n    cfg.data.transforms.random_perspective.distortion_scale = 0.5\n\n    cfg.data.transforms.color_jitter = CN()\n    cfg.data.transforms.color_jitter.enable = False\n    cfg.data.transforms.color_jitter.p = 0.5\n    cfg.data.transforms.color_jitter.brightness = 0.2\n    cfg.data.transforms.color_jitter.contrast = 0.15\n    cfg.data.transforms.color_jitter.saturation = 0.0\n    cfg.data.transforms.color_jitter.hue = 0.0\n\n    cfg.data.transforms.random_erase = CN()\n    cfg.data.transforms.random_erase.enable = False\n    cfg.data.transforms.random_erase.p = 0.5\n    cfg.data.transforms.random_erase.sl = 0.2\n    cfg.data.transforms.random_erase.sh = 0.4\n    cfg.data.transforms.random_erase.r1 = 0.3\n    cfg.data.transforms.random_erase.mean = (0.4914, 0.4822, 0.4465)\n\n    cfg.data.transforms.random_rotate = CN()\n    cfg.data.transforms.random_rotate.enable = False\n    cfg.data.transforms.random_rotate.p = 0.5\n    cfg.data.transforms.random_rotate.angle = (-5, 5)\n\n    cfg.data.transforms.masks = CN()\n    cfg.data.transforms.masks.enable = False\n    cfg.data.transforms.masks.p = 0.33\n    cfg.data.transforms.masks.random_color = True\n    cfg.data.transforms.masks.noise = True\n\n    cfg.data.transforms.random_figures = CN()\n    cfg.data.transforms.random_figures.enable = False\n    cfg.data.transforms.random_figures.p = 0.5\n    cfg.data.transforms.random_figures.random_color = True\n    cfg.data.transforms.random_figures.always_single_figure = False\n    cfg.data.transforms.random_figures.thicknesses = (1, 6)\n    cfg.data.transforms.random_figures.circle_radiuses = (5, 64)\n    cfg.data.transforms.random_figures.figure_prob = 0.5\n\n    cfg.data.transforms.random_patch = CN()\n    cfg.data.transforms.random_patch.enable = False\n    cfg.data.transforms.random_patch.p = 0.5\n    cfg.data.transforms.random_patch.pool_capacity = 50000\n    cfg.data.transforms.random_patch.min_sample_size = 100\n    cfg.data.transforms.random_patch.patch_min_area = 0.01\n    cfg.data.transforms.random_patch.patch_max_area = 0.5\n    cfg.data.transforms.random_patch.patch_min_ratio = 0.1\n    cfg.data.transforms.random_patch.prob_rotate = 0.5\n    cfg.data.transforms.random_patch.prob_flip_leftright = 0.5\n\n    cfg.data.transforms.random_grid = CN()\n    cfg.data.transforms.random_grid.enable = False\n    cfg.data.transforms.random_grid.p = 0.33\n    cfg.data.transforms.random_grid.color = (-1, -1, -1)\n    cfg.data.transforms.random_grid.grid_size = (24, 64)\n    cfg.data.transforms.random_grid.thickness = (1, 1)\n    cfg.data.transforms.random_grid.angle = (0, 180)\n\n    cfg.data.transforms.batch_transform = CN()\n    cfg.data.transforms.batch_transform.enable = False\n    cfg.data.transforms.batch_transform.type = \'Pairing\'\n    cfg.data.transforms.batch_transform.alpha = 1.\n    cfg.data.transforms.batch_transform.anchor_bias = 0.8\n\n    return cfg\n\n\ndef imagedata_kwargs(cfg):\n    return {\n        \'root\': cfg.data.root,\n        \'sources\': cfg.data.sources,\n        \'targets\': cfg.data.targets,\n        \'height\': cfg.data.height,\n        \'width\': cfg.data.width,\n        \'transforms\': cfg.data.transforms,\n        \'norm_mean\': cfg.data.norm_mean,\n        \'norm_std\': cfg.data.norm_std,\n        \'use_gpu\': cfg.use_gpu,\n        \'split_id\': cfg.data.split_id,\n        \'combineall\': cfg.data.combineall,\n        \'batch_size_train\': cfg.train.batch_size,\n        \'batch_size_test\': cfg.test.batch_size,\n        \'workers\': cfg.data.workers,\n        \'num_instances\': cfg.sampler.num_instances,\n        \'train_sampler\': cfg.sampler.train_sampler,\n        # image\n        \'cuhk03_labeled\': cfg.cuhk03.labeled_images,\n        \'cuhk03_classic_split\': cfg.cuhk03.classic_split,\n        \'market1501_500k\': cfg.market1501.use_500k_distractors,\n        \'apply_masks_to_test\': cfg.test.apply_masks,\n    }\n\n\ndef key_points(cfg):\n    return {\n        \'use\': cfg.data.use_keypoints,\n        \'root_path\': cfg.data.keypoints\n    }\n\n\ndef videodata_kwargs(cfg):\n    return {\n        \'root\': cfg.data.root,\n        \'sources\': cfg.data.sources,\n        \'targets\': cfg.data.targets,\n        \'height\': cfg.data.height,\n        \'width\': cfg.data.width,\n        \'transforms\': cfg.data.transforms,\n        \'norm_mean\': cfg.data.norm_mean,\n        \'norm_std\': cfg.data.norm_std,\n        \'use_gpu\': cfg.use_gpu,\n        \'split_id\': cfg.data.split_id,\n        \'combineall\': cfg.data.combineall,\n        \'batch_size_train\': cfg.train.batch_size,\n        \'batch_size_test\': cfg.test.batch_size,\n        \'workers\': cfg.data.workers,\n        \'num_instances\': cfg.sampler.num_instances,\n        \'train_sampler\': cfg.sampler.train_sampler,\n        # video\n        \'seq_len\': cfg.video.seq_len,\n        \'sample_method\': cfg.video.sample_method\n    }\n\n\ndef optimizer_kwargs(cfg):\n    return {\n        \'optim\': cfg.train.optim,\n        \'lr\': cfg.train.lr,\n        \'weight_decay\': cfg.train.weight_decay,\n        \'momentum\': cfg.sgd.momentum,\n        \'sgd_dampening\': cfg.sgd.dampening,\n        \'sgd_nesterov\': cfg.sgd.nesterov,\n        \'rmsprop_alpha\': cfg.rmsprop.alpha,\n        \'adam_beta1\': cfg.adam.beta1,\n        \'adam_beta2\': cfg.adam.beta2,\n        \'staged_lr\': cfg.train.staged_lr,\n        \'new_layers\': cfg.train.new_layers,\n        \'base_lr_mult\': cfg.train.base_lr_mult\n    }\n\n\ndef lr_scheduler_kwargs(cfg):\n    return {\n        \'lr_scheduler\': cfg.train.lr_scheduler,\n        \'stepsize\': cfg.train.stepsize,\n        \'gamma\': cfg.train.gamma,\n        \'max_epoch\': cfg.train.max_epoch,\n        \'warmup\': cfg.train.warmup,\n        \'frozen\': cfg.train.fixbase_epoch,\n        \'warmup_factor_base\': cfg.train.warmup_factor_base\n    }\n\n\ndef engine_run_kwargs(cfg):\n    return {\n        \'save_dir\': cfg.data.save_dir,\n        \'max_epoch\': cfg.train.max_epoch,\n        \'start_epoch\': cfg.train.start_epoch,\n        \'fixbase_epoch\': cfg.train.fixbase_epoch,\n        \'open_layers\': cfg.train.open_layers,\n        \'start_eval\': cfg.test.start_eval,\n        \'eval_freq\': cfg.test.eval_freq,\n        \'test_only\': cfg.test.evaluate,\n        \'print_freq\': cfg.train.print_freq,\n        \'dist_metric\': cfg.test.dist_metric,\n        \'normalize_feature\': cfg.test.normalize_feature,\n        \'visrank\': cfg.test.visrank,\n        \'visrank_topk\': cfg.test.visrank_topk,\n        \'use_metric_cuhk03\': cfg.cuhk03.use_metric_cuhk03,\n        \'ranks\': cfg.test.ranks,\n        \'rerank\': cfg.test.rerank,\n        \'visactmap\': cfg.test.visactmap,\n    }\n\n\ndef transforms(cfg):\n    return cfg.data.transforms\n\n\ndef augmentation_kwargs(cfg):\n    return {\n        \'random_flip\': cfg.data.transforms.random_flip,\n        \'random_crop\': cfg.data.transforms.random_crop,\n        \'random_gray_scale\': cfg.data.transforms.random_gray_scale,\n        \'random_padding\': cfg.data.transforms.random_padding,\n        \'random_perspective\': cfg.data.transforms.random_perspective,\n        \'color_jitter\': cfg.data.transforms.color_jitter,\n        \'random_erase\': cfg.data.transforms.random_erase,\n        \'random_rotate\': cfg.data.transforms.random_rotate,\n        \'masks\': cfg.data.transforms.masks,\n        \'random_figures\': cfg.data.transforms.random_figures,\n        \'random_grid\': cfg.data.transforms.random_grid\n    }\n'"
pytorch_toolkit/person_reidentification/data/datamanager.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\nfrom torchreid.data.datamanager import DataManager\nfrom torchreid.data.datasets import __image_datasets\n\nfrom .datasets.globalme import GlobalMe\nfrom .transforms import build_transforms\nfrom .sampler import build_train_sampler\n\n\n__image_datasets[\'globalme\'] = GlobalMe\n\n\ndef init_image_dataset(name, **kwargs):\n    """"""Initializes an image dataset.""""""\n    avai_datasets = list(__image_datasets.keys())\n    if name not in avai_datasets:\n        raise ValueError(\'Invalid dataset name. Received ""{}"", \'\n                         \'but expected to be one of {}\'.format(name, avai_datasets))\n    return __image_datasets[name](**kwargs)\n\n\nclass ImageDataManagerWithTransforms(DataManager):\n    data_type = \'image\'\n\n    def __init__(self, root=\'\', sources=None, targets=None, height=256, width=128, transforms=\'random_flip\',\n                 norm_mean=None, norm_std=None, use_gpu=True, split_id=0, combineall=False,\n                 batch_size_train=32, batch_size_test=32, workers=4, num_instances=4, train_sampler=\'\',\n                 cuhk03_labeled=False, cuhk03_classic_split=False, market1501_500k=False, apply_masks_to_test=False):\n\n        super(ImageDataManagerWithTransforms, self).__init__(\n            sources=sources, targets=targets, height=height, width=width,\n            transforms=None, norm_mean=norm_mean, norm_std=norm_std, use_gpu=use_gpu\n        )\n\n        self.transform_tr, self.transform_te = build_transforms(\n            self.height, self.width, transforms=transforms,\n            norm_mean=norm_mean, norm_std=norm_std,\n            apply_masks_to_test=apply_masks_to_test\n        )\n\n        print(\'=> Loading train (source) dataset\')\n        trainset = []\n        for name in self.sources:\n            trainset_ = init_image_dataset(\n                name,\n                transform=self.transform_tr,\n                mode=\'train\',\n                combineall=combineall,\n                root=root,\n                split_id=split_id,\n                cuhk03_labeled=cuhk03_labeled,\n                cuhk03_classic_split=cuhk03_classic_split,\n                market1501_500k=market1501_500k\n            )\n            trainset.append(trainset_)\n        trainset = sum(trainset)\n\n        self._num_train_pids = trainset.num_train_pids\n        self._num_train_cams = trainset.num_train_cams\n\n        train_sampler = build_train_sampler(\n            trainset.train, train_sampler,\n            batch_size=batch_size_train,\n            num_instances=num_instances\n        )\n\n        self.trainloader = torch.utils.data.DataLoader(\n            trainset,\n            sampler=train_sampler,\n            batch_size=batch_size_train,\n            shuffle=False,\n            num_workers=workers,\n            pin_memory=self.use_gpu,\n            drop_last=True\n        )\n\n        print(\'=> Loading test (target) dataset\')\n        self.testloader = {name: {\'query\': None, \'gallery\': None} for name in self.targets}\n        self.testdataset = {name: {\'query\': None, \'gallery\': None} for name in self.targets}\n\n        for name in self.targets:\n            # build query loader\n            queryset = init_image_dataset(\n                name,\n                transform=self.transform_te,\n                mode=\'query\',\n                combineall=combineall,\n                root=root,\n                split_id=split_id,\n                cuhk03_labeled=cuhk03_labeled,\n                cuhk03_classic_split=cuhk03_classic_split,\n                market1501_500k=market1501_500k\n            )\n            self.testloader[name][\'query\'] = torch.utils.data.DataLoader(\n                queryset,\n                batch_size=batch_size_test,\n                shuffle=False,\n                num_workers=workers,\n                pin_memory=self.use_gpu,\n                drop_last=False\n            )\n\n            # build gallery loader\n            galleryset = init_image_dataset(\n                name,\n                transform=self.transform_te,\n                mode=\'gallery\',\n                combineall=combineall,\n                verbose=False,\n                root=root,\n                split_id=split_id,\n                cuhk03_labeled=cuhk03_labeled,\n                cuhk03_classic_split=cuhk03_classic_split,\n                market1501_500k=market1501_500k\n            )\n            self.testloader[name][\'gallery\'] = torch.utils.data.DataLoader(\n                galleryset,\n                batch_size=batch_size_test,\n                shuffle=False,\n                num_workers=workers,\n                pin_memory=self.use_gpu,\n                drop_last=False\n            )\n\n            self.testdataset[name][\'query\'] = queryset.query\n            self.testdataset[name][\'gallery\'] = galleryset.gallery\n\n        print(\'\\n\')\n        print(\'  **************** Summary ****************\')\n        print(\'  train            : {}\'.format(self.sources))\n        print(\'  # train datasets : {}\'.format(len(self.sources)))\n        print(\'  # train ids      : {}\'.format(self.num_train_pids))\n        print(\'  # train images   : {}\'.format(len(trainset)))\n        print(\'  # train cameras  : {}\'.format(self.num_train_cams))\n        print(\'  test             : {}\'.format(self.targets))\n        print(\'  *****************************************\')\n        print(\'\\n\')\n'"
pytorch_toolkit/person_reidentification/data/sampler.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport numpy as np\nimport random\n\nfrom torch.utils.data.sampler import RandomSampler\n\nfrom torchreid.data.sampler import RandomIdentitySampler\n\n\ndef build_train_sampler(data_source, train_sampler, batch_size=32, num_instances=4, **kwargs):\n    """"""Builds a training sampler.\n\n    Args:\n        data_source (list): contains tuples of (img_path(s), pid, camid).\n        train_sampler (str): sampler name (default: ``RandomSampler``).\n        batch_size (int, optional): batch size. Default is 32.\n        num_instances (int, optional): number of instances per identity in a\n            batch (for ``RandomIdentitySampler``). Default is 4.\n    """"""\n    if train_sampler == \'RandomIdentitySampler\':\n        sampler = RandomIdentitySampler(data_source, batch_size, num_instances)\n    elif train_sampler == \'RandomIdentitySamplerV2\':\n        sampler = RandomIdentitySamplerV2(data_source, batch_size, num_instances)\n    else:\n        sampler = RandomSampler(data_source)\n\n    return sampler\n\n\nclass RandomIdentitySamplerV2(RandomIdentitySampler):\n    def __init__(self, data_source, batch_size, num_instances):\n        super().__init__(data_source, batch_size, num_instances)\n\n    def __iter__(self):\n        random.shuffle(self.pids)\n        output_ids = []\n        for pid in self.pids:\n            random.shuffle(self.index_dic[pid])\n            output_ids += self.index_dic[pid]\n        extra_samples = len(output_ids) % self.num_instances\n        output_ids = output_ids[: len(output_ids) - extra_samples]\n        ids = np.array(output_ids)\n        ids = ids.reshape((-1, self.num_instances))\n        np.random.shuffle(ids)\n        ids = ids.reshape((-1))\n        return iter(ids.tolist())\n\n    def __len__(self):\n        return len(self.data_source)\n'"
pytorch_toolkit/person_reidentification/data/transforms.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom PIL import Image\nimport random\nimport numpy as np\nimport math\n\nimport cv2\nfrom collections import deque\n\nimport torch\nfrom torchvision.transforms import *\nfrom torchvision.transforms import functional as F\n\n\nclass ComposeMask(Compose):\n    def __init__(self, transforms_):\n        super().__init__(transforms_)\n\n    def __call__(self, img, mask=None):\n        for t in self.transforms:\n            if isinstance(t, MaskAugmentation):\n                img = t(img, mask)\n            else:\n                img = t(img)\n        return img\n\n\nclass Random2DTranslation(object):\n    """"""Randomly translates the input image with a probability.\n\n    Specifically, given a predefined shape (height, width), the input is first\n    resized with a factor of 1.125, leading to (height*1.125, width*1.125), then\n    a random crop is performed. Such operation is done with a probability.\n\n    Args:\n        height (int): target image height.\n        width (int): target image width.\n        p (float, optional): probability that this operation takes place.\n            Default is 0.5.\n        interpolation (int, optional): desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    """"""\n\n    def __init__(self, height, width, p=0.5, interpolation=Image.BILINEAR, **kwargs):\n        self.height = height\n        self.width = width\n        self.p = p\n        self.interpolation = interpolation\n\n    def __call__(self, img, *args, **kwargs):\n        if random.uniform(0, 1) > self.p:\n            return img.resize((self.width, self.height), self.interpolation)\n\n        new_width, new_height = int(round(self.width * 1.125)), int(round(self.height * 1.125))\n        resized_img = img.resize((new_width, new_height), self.interpolation)\n        x_maxrange = new_width - self.width\n        y_maxrange = new_height - self.height\n        x1 = int(round(random.uniform(0, x_maxrange)))\n        y1 = int(round(random.uniform(0, y_maxrange)))\n        croped_img = resized_img.crop((x1, y1, x1 + self.width, y1 + self.height))\n        return croped_img\n\n\nclass RandomErasing(object):\n    """"""Randomly erases an image patch.\n\n    Origin: `<https://github.com/zhunzhong07/Random-Erasing>`_\n\n    Reference:\n        Zhong et al. Random Erasing Data Augmentation.\n\n    Args:\n        probability (float, optional): probability that this operation takes place.\n            Default is 0.5.\n        sl (float, optional): min erasing area.\n        sh (float, optional): max erasing area.\n        r1 (float, optional): min aspect ratio.\n        mean (list, optional): erasing value.\n    """"""\n\n    def __init__(self, p=0.5, sl=0.02, sh=0.4, r1=0.3, mean=(0.4914, 0.4822, 0.4465), **kwargs):\n        self.probability = p\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n\n    def __call__(self, img, *args, **kwargs):\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n\n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                return img\n        return img\n\n\nclass ColorAugmentation(object):\n    """"""Randomly alters the intensities of RGB channels.\n\n    Reference:\n        Krizhevsky et al. ImageNet Classification with Deep ConvolutionalNeural\n        Networks. NIPS 2012.\n\n    Args:\n        p (float, optional): probability that this operation takes place.\n            Default is 0.5.\n    """"""\n\n    def __init__(self, p=0.5, **kwargs):\n        self.p = p\n        self.eig_vec = torch.Tensor([\n            [0.4009, 0.7192, -0.5675],\n            [-0.8140, -0.0045, -0.5808],\n            [0.4203, -0.6948, -0.5836],\n        ])\n        self.eig_val = torch.Tensor([[0.2175, 0.0188, 0.0045]])\n\n    def _check_input(self, tensor):\n        assert tensor.dim() == 3 and tensor.size(0) == 3\n\n    def __call__(self, tensor, *args, **kwargs):\n        if random.uniform(0, 1) > self.p:\n            return tensor\n        alpha = torch.normal(mean=torch.zeros_like(self.eig_val)) * 0.1\n        quatity = torch.mm(self.eig_val * alpha, self.eig_vec)\n        tensor = tensor + quatity.view(3, 1, 1)\n        return tensor\n\n\nclass MaskAugmentation(object):\n    """"""Apply mask to image with transformed background.\n    Directory with masks must be in the same directory\n    with original images and files must have extension *.png,\n    for example:\n    path/to/Market-1501-v15.09.15/bounding_box_train/masks/0002_c1s1_000451_03.png\n    """"""\n\n    def __init__(self, p=0.33, random_color=True, noise=True, **kwargs):\n        self.p = p\n        self.bg_transforms = [\'\', ]\n        if random_color:\n            self.bg_transforms.append(\'random_color\')\n        if noise:\n            self.bg_transforms.append(\'noise\')\n        self.noise = noise\n\n    def __call__(self, image, mask):\n        if mask is None or random.uniform(0, 1) > self.p:\n            return image\n        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n        cv_image = cv2.bitwise_and(cv_image, cv_image, mask=mask[:, :, 0])\n        transform_type = self.bg_transforms[random.randint(0, len(self.bg_transforms) - 1)]\n        image = self.apply_tansform(cv_image, mask, transform_type)\n        return image\n\n    @staticmethod\n    def apply_tansform(img, mask, transform_type=\'noise\'):\n        bg = np.zeros(img.shape, dtype=\'uint8\')\n        if transform_type == \'random_color\':\n            rnd_color = np.random.randint(255, size=3, dtype=\'uint8\')\n            bg[:] = rnd_color\n        elif transform_type == \'noise\':\n            bg = np.random.randint(255, size=img.shape, dtype=\'uint8\')\n        img = np.where(mask == 0, bg, img)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        return Image.fromarray(img)\n\n\nclass RandomPatch(object):\n    """"""Random patch data augmentation.\n\n    There is a patch pool that stores randomly extracted pathces from person images.\n\n    For each input image,\n        1) we extract a random patch and store the patch in the patch pool;\n        2) randomly select a patch from the patch pool and paste it on the\n           input to simulate occlusion.\n\n    Reference:\n        - Zhou et al. Omni-Scale Feature Learning for Person Re-Identification. ICCV, 2019.\n    """"""\n\n    def __init__(self, p=0.5, pool_capacity=50000, min_sample_size=100,\n                 patch_min_area=0.01, patch_max_area=0.5, patch_min_ratio=0.1,\n                 prob_rotate=0.5, prob_flip_leftright=0.5, **kwargs):\n        self.prob_happen = p\n\n        self.patch_min_area = patch_min_area\n        self.patch_max_area = patch_max_area\n        self.patch_min_ratio = patch_min_ratio\n\n        self.prob_rotate = prob_rotate\n        self.prob_flip_leftright = prob_flip_leftright\n\n        self.patchpool = deque(maxlen=pool_capacity)\n        self.min_sample_size = min_sample_size\n\n    def generate_wh(self, W, H):\n        area = W * H\n        for attempt in range(100):\n            target_area = random.uniform(self.patch_min_area, self.patch_max_area) * area\n            aspect_ratio = random.uniform(self.patch_min_ratio, 1./self.patch_min_ratio)\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n            if w < W and h < H:\n                return w, h\n        return None, None\n\n    def transform_patch(self, patch):\n        if random.uniform(0, 1) > self.prob_flip_leftright:\n            patch = patch.transpose(Image.FLIP_LEFT_RIGHT)\n        if random.uniform(0, 1) > self.prob_rotate:\n            patch = patch.rotate(random.randint(-10, 10))\n        return patch\n\n    def __call__(self, img):\n        W, H = img.size # original image size\n\n        # collect new patch\n        w, h = self.generate_wh(W, H)\n        if w is not None and h is not None:\n            x1 = random.randint(0, W - w)\n            y1 = random.randint(0, H - h)\n            new_patch = img.crop((x1, y1, x1 + w, y1 + h))\n            self.patchpool.append(new_patch)\n\n        if len(self.patchpool) < self.min_sample_size:\n            return img\n\n        if random.uniform(0, 1) > self.prob_happen:\n            return img\n\n        # paste a randomly selected patch on a random position\n        patch = random.sample(self.patchpool, 1)[0]\n        patchW, patchH = patch.size\n        x1 = random.randint(0, W - patchW)\n        y1 = random.randint(0, H - patchH)\n        patch = self.transform_patch(patch)\n        img.paste(patch, (x1, y1))\n\n        return img\n\n\nclass RandomColorJitter(ColorJitter):\n    def __init__(self, p=0.5, brightness=0.2, contrast=0.15, saturation=0, hue=0, **kwargs):\n        self.p = p\n        super().__init__(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n\n    def __call__(self, img):\n        if random.uniform(0, 1) > self.p:\n            return img\n        transform = self.get_params(self.brightness, self.contrast,\n                                    self.saturation, self.hue)\n        return transform(img)\n\n\nclass RandomGrayScale(object):\n    """"""Random rotate\n    """"""\n\n    def __init__(self, p=0.5, **kwargs):\n        self.p = p\n\n    def __call__(self, img, *args, **kwargs):\n        if random.uniform(0, 1) > self.p:\n            return img\n        return F.to_grayscale(img, num_output_channels=3)\n\n\nclass RandomPadding(object):\n    """"""Random rotate\n    """"""\n\n    def __init__(self, p=0.5, padding=(0, 10), **kwargs):\n        self.p = p\n        self.padding_limits = padding\n\n    def __call__(self, img, *args, **kwargs):\n        if random.uniform(0, 1) > self.p:\n            return img\n        rnd_padding = [random.randint(self.padding_limits[0], self.padding_limits[1]) for _ in range(4)]\n        rnd_fill = random.randint(0, 255)\n        return F.pad(img, tuple(rnd_padding), fill=rnd_fill, padding_mode=\'constant\')\n\n\nclass RandomRotate(object):\n    """"""Random rotate\n    """"""\n\n    def __init__(self, p=0.5, angle=(-5, 5), **kwargs):\n        self.p = p\n        self.angle = angle\n\n    def __call__(self, img, *args, **kwargs):\n        if random.uniform(0, 1) > self.p:\n            return img\n        rnd_angle = random.randint(self.angle[0], self.angle[1])\n        return F.rotate(img, rnd_angle, resample=False, expand=False, center=None)\n\n\nclass RandomGrid(object):\n    """"""Random grid\n    """"""\n\n    def __init__(self, p=0.5, color=-1, grid_size=(24, 64), thickness=(1, 1), angle=(0, 180), **kwargs):\n        self.p = p\n        self.color = color\n        self.grid_size = grid_size\n        self.thickness = thickness\n        self.angle = angle\n\n    def __call__(self, img, *args, **kwargs):\n        if random.uniform(0, 1) > self.p:\n            return img\n        if self.color == (-1, -1, -1):  # Random color\n            color = tuple([random.randint(0, 256) for _ in range(3)])\n        else:\n            color = self.color\n        grid_size = random.randint(*self.grid_size)\n        thickness = random.randint(*self.thickness)\n        angle = random.randint(*self.angle)\n        return self.draw_grid(img, grid_size, color, thickness, angle)\n\n    @staticmethod\n    def draw_grid(image, grid_size, color, thickness, angle):\n        img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n        h, w = img.shape[:2]\n        mask = np.zeros((h * 8, w * 8, 3), dtype=\'uint8\')\n        mask_h, mask_w = mask.shape[:2]\n        for i in range(0, mask_h, grid_size):\n            p1 = (0, i)\n            p2 = (mask_w, i + grid_size)\n            mask = cv2.line(mask, p1, p2, (255, 255, 255), thickness)\n        for i in range(0, mask_w, grid_size):\n            p1 = (i, 0)\n            p2 = (i + grid_size, mask_h)\n            mask = cv2.line(mask, p1, p2, (255, 255, 255), thickness)\n\n        center = (mask_w // 2, mask_h // 2)\n\n        if angle > 0:\n            rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n            mask = cv2.warpAffine(mask, rot_mat, (mask_w, mask_h), flags=cv2.INTER_LINEAR)\n\n        offset = (random.randint(-16, 16), random.randint(16, 16))\n        center = (center[0] + offset[0], center[1] + offset[1])\n        mask = mask[center[1] - h // 2: center[1] + h // 2, center[0] - w // 2: center[0] + w // 2, :]\n        mask = cv2.resize(mask, (w, h))\n        assert img.shape == mask.shape\n        img = np.where(mask == 0, img, color).astype(\'uint8\')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        return Image.fromarray(img)\n\n\nclass RandomFigures(object):\n    """"""Insert random figure or some figures from the list [line, rectangle, circle]\n    with random color and thickness\n    """"""\n\n    def __init__(self, p=0.5, random_color=True, always_single_figure=False,\n                 thicknesses=(1, 6), circle_radiuses=(5, 64), figure_prob=0.5, **kwargs):\n        self.p = p\n        self.random_color = random_color\n        self.always_single_figure = always_single_figure\n        self.figures = (cv2.line, cv2.rectangle, cv2.circle)\n        self.thicknesses = thicknesses\n        self.circle_radiuses = circle_radiuses\n        self.figure_prob = figure_prob\n\n    def __call__(self, image):\n        if random.uniform(0, 1) > self.p:\n            return image\n        if self.always_single_figure:\n            figure = [self.figures[random.randint(0, len(self.figures) - 1)]]\n        else:\n            figure = []\n            for i in range(len(self.figures)):\n                if random.uniform(0, 1) > self.figure_prob:\n                    figure.append(self.figures[i])\n        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n        h, w = cv_image.shape[:2]\n        for f in figure:\n            p1 = (random.randint(0, w), random.randint(0, h))\n            p2 = (random.randint(0, w), random.randint(0, h))\n            color = tuple([random.randint(0, 256) for _ in range(3)]) if self.random_color else (0, 0, 0)\n            thickness = random.randint(*self.thicknesses)\n            if f != cv2.circle:\n                cv_image = f(cv_image, p1, p2, color, thickness)\n            else:\n                r = random.randint(*self.circle_radiuses)\n                cv_image = f(cv_image, p1, r, color, thickness)\n        img = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n        return Image.fromarray(img)\n\n\ndef build_transforms(height, width, transforms=None, norm_mean=(0.485, 0.456, 0.406),\n                     norm_std=(0.229, 0.224, 0.225), apply_masks_to_test=False, **kwargs):\n    """"""Builds train and test transform functions.\n\n    Args:\n        height (int): target image height.\n        width (int): target image width.\n        transforms (str or list of str, optional): transformations applied to model training.\n            Default is \'random_flip\'.\n        norm_mean (list or None, optional): normalization mean values. Default is ImageNet means.\n        norm_std (list or None, optional): normalization standard deviation values. Default is\n            ImageNet standard deviation values.\n    """"""\n    if transforms is None:\n        return None, None\n\n    if norm_mean is None or norm_std is None:\n        norm_mean = [0.485, 0.456, 0.406]  # imagenet mean\n        norm_std = [0.229, 0.224, 0.225]  # imagenet std\n    normalize = Normalize(mean=norm_mean, std=norm_std)\n\n    print(\'Building train transforms ...\')\n    transform_tr = []\n    if transforms.random_grid.enable:\n        print(\'+ random_grid\')\n        transform_tr += [RandomGrid(**transforms.random_grid)]\n    if transforms.random_figures.enable:\n        print(\'+ random_figures\')\n        transform_tr += [RandomFigures(**transforms.random_figures)]\n    if transforms.masks.enable:\n        print(\'+ masks\')\n        transform_tr += [MaskAugmentation(**transforms.masks)]\n    if transforms.random_padding.enable:\n        print(\'+ random_padding\')\n        transform_tr += [RandomPadding(**transforms.random_padding)]\n    transform_tr += [Resize((height, width))]\n    print(\'+ resize to {}x{}\'.format(height, width))\n    if transforms.random_flip.enable:\n        print(\'+ random flip\')\n        transform_tr += [RandomHorizontalFlip(p=transforms.random_flip.p)]\n    if transforms.random_crop.enable:\n        print(\'+ random crop (enlarge to {}x{} and \' \\\n              \'crop {}x{})\'.format(int(round(height*1.125)), int(round(width*1.125)), height, width, transforms.random_crop.p))\n        transform_tr += [Random2DTranslation(height, width, **transforms.random_crop)]\n    if transforms.random_patch.enable:\n        print(\'+ random patch\')\n        transform_tr += [RandomPatch(**transforms.random_patch)]\n    if transforms.color_jitter.enable:\n        print(\'+ color jitter\')\n        transform_tr += [RandomColorJitter(**transforms.color_jitter)]\n    if transforms.random_gray_scale.enable:\n        print(\'+ random_gray_scale\')\n        transform_tr += [RandomGrayscale(p=transforms.random_gray_scale.p)]\n    if transforms.random_perspective.enable:\n        print(\'+ random_perspective\')\n        transform_tr += [RandomPerspective(\n            p=transforms.random_perspective.p,\n            distortion_scale=transforms.random_perspective.distortion_scale)\n        ]\n    if transforms.random_rotate.enable:\n        print(\'+ random_rotate\')\n        transform_tr += [RandomRotate(**transforms.random_rotate)]\n    print(\'+ to torch tensor of range [0, 1]\')\n    transform_tr += [ToTensor()]\n    print(\'+ normalization (mean={}, std={})\'.format(norm_mean, norm_std))\n    transform_tr += [normalize]\n    if transforms.random_erase.enable:\n        print(\'+ random erase\')\n        transform_tr += [RandomErasing(**transforms.random_erase)]\n    transform_tr = ComposeMask(transform_tr)\n\n    print(\'Building test transforms ...\')\n    print(\'+ resize to {}x{}\'.format(height, width))\n    print(\'+ to torch tensor of range [0, 1]\')\n    print(\'+ normalization (mean={}, std={})\'.format(norm_mean, norm_std))\n    if apply_masks_to_test:\n        transform_te = ComposeMask([\n            MaskAugmentation(p=1.0, random_color=False, noise=False),\n            Resize((height, width)),\n            ToTensor(),\n            normalize,\n        ])\n    else:\n        transform_te = ComposeMask([\n            Resize((height, width)),\n            ToTensor(),\n            normalize,\n        ])\n\n    return transform_tr, transform_te\n'"
pytorch_toolkit/person_reidentification/engine/builder.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torchreid\nfrom .engine import ImageAMSoftmaxEngine\n\n\ndef build_engine(cfg, datamanager, model, optimizer, scheduler, writer=None, openvino_model=None):\n    if cfg.data.type == \'image\':\n        if cfg.loss.name == \'softmax\':\n            engine = ImageAMSoftmaxEngine(\n                datamanager,\n                model,\n                optimizer,\n                cfg.reg,\n                cfg.metric_losses,\n                cfg.data.transforms.batch_transform,\n                scheduler,\n                cfg.use_gpu,\n                label_smooth=cfg.loss.softmax.label_smooth,\n                conf_penalty=cfg.loss.softmax.conf_pen,\n                softmax_type=\'stock\',\n                writer=writer,\n                openvino_model=openvino_model,\n                flip_eval=cfg.test.use_flip\n            )\n        elif cfg.loss.name == \'am_softmax\':\n            engine = ImageAMSoftmaxEngine(\n                datamanager,\n                model,\n                optimizer,\n                cfg.reg,\n                cfg.metric_losses,\n                cfg.data.transforms.batch_transform,\n                scheduler,\n                cfg.use_gpu,\n                conf_penalty=cfg.loss.softmax.conf_pen,\n                softmax_type=\'am\',\n                pr_product=cfg.loss.softmax.pr_product,\n                m=cfg.loss.softmax.m,\n                s=cfg.loss.softmax.s,\n                writer=writer,\n                openvino_model=openvino_model,\n                flip_eval=cfg.test.use_flip\n            )\n    else:\n        raise Exception(\'This code supports image data type only\')\n\n    return engine\n'"
pytorch_toolkit/person_reidentification/engine/engine.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport datetime\nimport numpy as np\nimport time\nfrom os import path as osp\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchreid import metrics\nfrom torchreid.engine.image.softmax import ImageSoftmaxEngine\nfrom torchreid.utils import AverageMeter, open_specified_layers, open_all_layers, visualize_ranked_results, re_ranking\n\nfrom .losses.am_softmax import AMSoftmaxLoss\nfrom .losses.cross_entropy_loss import CrossEntropyLoss\nfrom .losses.regularizers import get_regularizer\nfrom .losses.metric import LocalPushLoss\n\n\nclass ImageAMSoftmaxEngine(ImageSoftmaxEngine):\n    r""""""Softmax-loss engine for image-reid.\n    """"""\n\n    def __init__(self, datamanager, model, optimizer, reg_cfg, metric_cfg, batch_transform_cfg, scheduler=None, use_gpu=False,\n                 softmax_type=\'stock\', label_smooth=True, conf_penalty=False,\n                 pr_product=False, m=0.35, s=10, writer=None, openvino_model=None, flip_eval=False):\n        super(ImageAMSoftmaxEngine, self).__init__(datamanager, model, optimizer, scheduler, use_gpu)\n\n        self.regularizer = get_regularizer(reg_cfg)\n        self.openvino_model = openvino_model\n        self.writer = writer\n        self.flip_eval = flip_eval\n\n        if softmax_type == \'stock\':\n            self.criterion = CrossEntropyLoss(\n                num_classes=self.datamanager.num_train_pids,\n                use_gpu=self.use_gpu,\n                label_smooth=label_smooth,\n                conf_penalty=conf_penalty\n            )\n        elif softmax_type == \'am\':\n            self.criterion = AMSoftmaxLoss(\n                num_classes=self.datamanager.num_train_pids,\n                use_gpu=self.use_gpu,\n                conf_penalty=conf_penalty,\n                m=m, s=s,\n                pr_product=pr_product\n            )\n\n        self.batch_transform_cfg = batch_transform_cfg\n        self.lambd_distr = torch.distributions.beta.Beta(self.batch_transform_cfg.alpha,\n                                                         self.batch_transform_cfg.alpha)\n\n        if metric_cfg.enable:\n            self.metric_loss = LocalPushLoss(weight=metric_cfg.local_push_weight)\n        else:\n            self.metric_loss = None\n\n    def run(self, save_dir=\'log\', max_epoch=0, start_epoch=0, fixbase_epoch=0, open_layers=None,\n            start_eval=0, eval_freq=-1, test_only=False, print_freq=10,\n            dist_metric=\'euclidean\', normalize_feature=False, visrank=False, visrank_topk=10,\n            use_metric_cuhk03=False, ranks=(1, 5, 10, 20), rerank=False, visactmap=False):\n        r""""""A unified pipeline for training and evaluating a model.\n        """"""\n        trainloader, testloader = self.datamanager.return_dataloaders()\n\n        if visrank and not test_only:\n            raise ValueError(\'visrank=True is valid only if test_only=True\')\n\n        if test_only:\n            self.test(\n                0,\n                testloader,\n                dist_metric=dist_metric,\n                normalize_feature=normalize_feature,\n                visrank=visrank,\n                visrank_topk=visrank_topk,\n                save_dir=save_dir,\n                use_metric_cuhk03=use_metric_cuhk03,\n                ranks=ranks,\n                rerank=rerank\n            )\n            return\n\n        if visactmap:\n            self.visactmap(testloader, save_dir, self.datamanager.width, self.datamanager.height, print_freq)\n            return\n\n        if self.writer is None:\n            self.writer = SummaryWriter(log_dir=save_dir)\n\n        time_start = time.time()\n        print(\'=> Start training\')\n\n        # Save zeroth checkpoint\n        self._save_checkpoint(-1, 0.0, save_dir)\n\n        for epoch in range(start_epoch, max_epoch):\n            self.train(epoch, max_epoch, trainloader, fixbase_epoch, open_layers, print_freq)\n\n            if (epoch + 1) >= start_eval and eval_freq > 0 and (epoch + 1) % eval_freq == 0:\n                rank1 = self.test(\n                    epoch,\n                    testloader,\n                    dist_metric=dist_metric,\n                    normalize_feature=normalize_feature,\n                    visrank=visrank,\n                    visrank_topk=visrank_topk,\n                    save_dir=save_dir,\n                    use_metric_cuhk03=use_metric_cuhk03,\n                    ranks=ranks\n                )\n                self._save_checkpoint(epoch, rank1, save_dir)\n\n        elapsed = round(time.time() - time_start)\n        elapsed = str(datetime.timedelta(seconds=elapsed))\n        print(\'Elapsed {}\'.format(elapsed))\n\n        if self.writer is not None:\n            self.writer.flush()\n            self.writer.close()\n\n    def train(self, epoch, max_epoch, trainloader, fixbase_epoch=0, open_layers=None, print_freq=10):\n        losses = AverageMeter()\n        reg_ow_loss = AverageMeter()\n        metric_loss = AverageMeter()\n        accs = AverageMeter()\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n\n        self.model.train()\n        if (epoch + 1) <= fixbase_epoch and open_layers is not None:\n            print(\'* Only train {} (epoch: {}/{})\'.format(open_layers, epoch+1, fixbase_epoch))\n            open_specified_layers(self.model, open_layers)\n        else:\n            open_all_layers(self.model)\n\n        num_batches = len(trainloader)\n        start_time = time.time()\n        for batch_idx, data in enumerate(trainloader):\n            data_time.update(time.time() - start_time)\n\n            imgs, pids = self._parse_data_for_train(data)\n            imgs, pids = self._apply_batch_transform(imgs, pids)\n            if self.use_gpu:\n                imgs = imgs.cuda()\n                pids = pids.cuda()\n\n            self.optimizer.zero_grad()\n            if self.metric_loss is not None:\n                embeddings, outputs = self.model(imgs, get_embeddings=True)\n            else:\n                outputs = self.model(imgs)\n\n            loss = self._compute_loss(self.criterion, outputs, pids)\n\n            if (epoch + 1) > fixbase_epoch:\n                reg_loss = self.regularizer(self.model)\n                reg_ow_loss.update(reg_loss.item(), pids.size(0))\n                loss += reg_loss\n\n            if self.metric_loss is not None:\n                metric_val = self.metric_loss(F.normalize(embeddings, dim=1),\n                                              outputs, pids)\n                loss += metric_val\n                metric_loss.update(metric_val.item(), pids.size(0))\n\n            loss.backward()\n            self.optimizer.step()\n\n            losses.update(loss.item(), pids.size(0))\n            accs.update(metrics.accuracy(outputs, pids)[0].item())\n            batch_time.update(time.time() - start_time)\n\n            if print_freq > 0 and (batch_idx + 1) % print_freq == 0:\n                eta_seconds = batch_time.avg * (num_batches-(batch_idx + 1) + (max_epoch - (epoch + 1)) * num_batches)\n                eta_str = str(datetime.timedelta(seconds=int(eta_seconds)))\n                print(\'Epoch: [{0}/{1}][{2}/{3}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'AUX Losses {aux_losses.val:.4f} ({aux_losses.avg:.4f})\\t\'\n                      \'Acc {acc.val:.2f} ({acc.avg:.2f})\\t\'\n                      \'Lr {lr:.6f}\\t\'\n                      \'eta {eta}\'.\n                      format(\n                          epoch + 1, max_epoch, batch_idx + 1, num_batches,\n                          batch_time=batch_time,\n                          data_time=data_time,\n                          aux_losses=metric_loss,\n                          loss=losses,\n                          acc=accs,\n                          lr=self.optimizer.param_groups[0][\'lr\'],\n                          eta=eta_str,\n                      )\n                )\n\n                if self.writer is not None:\n                    n_iter = epoch * num_batches + batch_idx\n                    self.writer.add_scalar(\'Train/Time\', batch_time.avg, n_iter)\n                    self.writer.add_scalar(\'Train/Data\', data_time.avg, n_iter)\n                    info = self.criterion.get_last_info()\n                    for k in info:\n                        self.writer.add_scalar(\'AUX info/\' + k, info[k], n_iter)\n                    self.writer.add_scalar(\'Loss/train\', losses.avg, n_iter)\n                    if (epoch + 1) > fixbase_epoch:\n                        self.writer.add_scalar(\'Loss/reg_ow\', reg_ow_loss.avg, n_iter)\n                    self.writer.add_scalar(\'Accuracy/train\', accs.avg, n_iter)\n                    self.writer.add_scalar(\'Learning rate\', self.optimizer.param_groups[0][\'lr\'], n_iter)\n                    if self.metric_loss is not None:\n                        self.writer.add_scalar(\'Loss/local_push_loss\',\n                                               metric_val.item(), n_iter)\n            start_time = time.time()\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n\n    @torch.no_grad()\n    def _evaluate(self, epoch, dataset_name=\'\', queryloader=None, galleryloader=None,\n                  dist_metric=\'euclidean\', normalize_feature=False, visrank=False,\n                  visrank_topk=10, save_dir=\'\', use_metric_cuhk03=False, ranks=(1, 5, 10, 20),\n                  rerank=False, iteration=0):\n        batch_time = AverageMeter()\n\n        print(\'Extracting features from query set...\')\n        qf, q_pids, q_camids = [], [], []  # query features, query person IDs and query camera IDs\n        for batch_idx, data in tqdm(enumerate(queryloader), \'Processing query...\'):\n            imgs, pids, camids = self._parse_data_for_eval(data)\n            if self.use_gpu:\n                imgs = imgs.cuda()\n            end = time.time()\n            features = self._extract_features(imgs, data[3])\n            batch_time.update(time.time() - end)\n            features = features.data.cpu()\n            qf.append(features)\n            q_pids.extend(pids)\n            q_camids.extend(camids)\n        qf = torch.cat(qf, 0)\n        q_pids = np.asarray(q_pids)\n        q_camids = np.asarray(q_camids)\n        print(\'Done, obtained {}-by-{} matrix\'.format(qf.size(0), qf.size(1)))\n\n        print(\'Extracting features from gallery set...\')\n        gf, g_pids, g_camids = [], [], []  # gallery features, gallery person IDs and gallery camera IDs\n        for batch_idx, data in tqdm(enumerate(galleryloader), \'Processing gallery...\'):\n            imgs, pids, camids = self._parse_data_for_eval(data)\n            if self.use_gpu:\n                imgs = imgs.cuda()\n            end = time.time()\n            features = self._extract_features(imgs, data[3])\n            batch_time.update(time.time() - end)\n            features = features.data.cpu()\n            gf.append(features)\n            g_pids.extend(pids)\n            g_camids.extend(camids)\n        gf = torch.cat(gf, 0)\n        g_pids = np.asarray(g_pids)\n        g_camids = np.asarray(g_camids)\n        print(\'Done, obtained {}-by-{} matrix\'.format(gf.size(0), gf.size(1)))\n\n        print(\'Speed: {:.4f} sec/batch\'.format(batch_time.avg))\n\n        if normalize_feature:\n            print(\'Normalizing features with L2 norm...\')\n            qf = F.normalize(qf, p=2, dim=1)\n            gf = F.normalize(gf, p=2, dim=1)\n\n        print(\'Computing distance matrix with metric={}...\'.format(dist_metric))\n        distmat = metrics.compute_distance_matrix(qf, gf, dist_metric)\n        distmat = distmat.numpy()\n\n        if rerank:\n            print(\'Applying person re-ranking ...\')\n            distmat_qq = metrics.compute_distance_matrix(qf, qf, dist_metric)\n            distmat_gg = metrics.compute_distance_matrix(gf, gf, dist_metric)\n            distmat = re_ranking(distmat, distmat_qq, distmat_gg)\n\n        print(\'Computing CMC and mAP ...\')\n        cmc, mAP = metrics.evaluate_rank(\n            distmat,\n            q_pids,\n            g_pids,\n            q_camids,\n            g_camids,\n            use_metric_cuhk03=use_metric_cuhk03\n        )\n        if self.writer is not None:\n            self.writer.add_scalar(\'Val/{}/mAP\'.format(dataset_name), mAP, epoch + 1)\n            for r in ranks:\n                self.writer.add_scalar(\'Val/{}/Rank-{}\'.format(dataset_name, r), cmc[r - 1], epoch + 1)\n\n        print(\'** Results **\')\n        print(\'mAP: {:.2%}\'.format(mAP))\n        print(\'CMC curve\')\n        for r in ranks:\n            print(\'Rank-{:<3}: {:.2%}\'.format(r, cmc[r-1]))\n\n        if visrank:\n            visualize_ranked_results(\n                distmat,\n                self.datamanager.return_testdataset_by_name(dataset_name),\n                self.datamanager.data_type,\n                width=self.datamanager.width,\n                height=self.datamanager.height,\n                save_dir=osp.join(save_dir, \'visrank_\' + dataset_name),\n                topk=visrank_topk\n            )\n\n        return cmc[0]\n\n    @torch.no_grad()\n    def _extract_features(self, input, img_path=None):\n        if self.openvino_model is None:\n            self.model.eval()\n            out_pytorch = self.model(input)\n            if self.flip_eval:\n                input_flipped = torch.flip(input, (3,))\n                out_pytorch += self.model(input_flipped)\n                out_pytorch /= 2.\n            return out_pytorch\n        else:\n            out_openvino = self.openvino_model.forward(img_path)\n            out_openvino = np.concatenate(out_openvino)\n            out_openvino = torch.tensor(out_openvino).to(input.device)\n            return out_openvino\n\n    def _apply_batch_transform(self, imgs, pids):\n        if self.batch_transform_cfg.enable:\n            permuted_idx = torch.randperm(imgs.shape[0])\n            lambd = self.batch_transform_cfg.anchor_bias \\\n                    + (1 - self.batch_transform_cfg.anchor_bias) \\\n                    * self.lambd_distr.sample((imgs.shape[0],))\n            lambd = lambd.view(-1, 1, 1, 1)\n            imgs = lambd * imgs + (1 - lambd) * imgs[permuted_idx]\n        return imgs, pids\n'"
pytorch_toolkit/person_reidentification/models/builder.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\n\nfrom .osnet_fpn import *\n\nfrom torchreid.models import __model_factory\n\n\n__model_factory[\'fpn_osnet_x1_0\'] = fpn_osnet_x1_0\n__model_factory[\'fpn_osnet_x0_75\'] = fpn_osnet_x0_75\n__model_factory[\'fpn_osnet_x0_5\'] = fpn_osnet_x0_5\n__model_factory[\'fpn_osnet_x0_25\'] = fpn_osnet_x0_25\n__model_factory[\'fpn_osnet_ibn_x1_0\'] = fpn_osnet_ibn_x1_0\n\n\ndef build_model(name, num_classes, loss=\'softmax\', pretrained=True,\n                use_gpu=True, dropout_cfg=None, feature_dim=512, fpn_cfg=None,\n                pooling_type=\'avg\', input_size=(256, 128), IN_first=False,\n                extra_blocks=False):\n    """"""A function wrapper for building a model.\n    """"""\n    avai_models = list(__model_factory.keys())\n    if name not in avai_models:\n        raise KeyError(\'Unknown model: {}. Must be one of {}\'.format(name, avai_models))\n    return __model_factory[name](\n        num_classes=num_classes,\n        loss=loss,\n        pretrained=pretrained,\n        use_gpu=use_gpu,\n        dropout_cfg=dropout_cfg,\n        feature_dim=feature_dim,\n        fpn_cfg=fpn_cfg,\n        pooling_type=pooling_type,\n        input_size=input_size,\n        IN_first=IN_first,\n        extra_blocks=extra_blocks\n    )\n'"
pytorch_toolkit/person_reidentification/models/openvino_wrapper.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\nimport os\n\nimport logging as log\nimport numpy as np\nfrom openvino.inference_engine import IENetwork, IECore # pylint: disable=import-error,E0611\nimport cv2 as cv\n\n\nclass IEModel:\n    """"""Class for inference of models in the Inference Engine format""""""\n    def __init__(self, exec_net, inputs_info, input_key, output_key):\n        self.net = exec_net\n        self.inputs_info = inputs_info\n        self.input_key = input_key\n        self.output_key = output_key\n        self.reqs_ids = []\n\n    def _preprocess(self, img):\n        img = cv.imread(img)\n        n, c, h, w = self.get_input_shape().shape\n        img = np.expand_dims(cv.resize(img, (w, h)).transpose(2, 0, 1), axis=0)\n        return img\n\n    def forward(self, img):\n        """"""Performs forward pass of the wrapped IE model""""""\n        res = self.net.infer(inputs={self.input_key: self._preprocess(img)})\n        return np.copy(res[self.output_key])\n\n    def forward_async(self, img):\n        id = len(self.reqs_ids)\n        self.net.start_async(request_id=id,\n                             inputs={self.input_key: self._preprocess(img)})\n        self.reqs_ids.append(id)\n\n    def grab_all_async(self):\n        outputs = []\n        for id in self.reqs_ids:\n            self.net.requests[id].wait(-1)\n            res = self.net.requests[id].outputs[self.output_key]\n            outputs.append(np.copy(res))\n        self.reqs_ids = []\n        return outputs\n\n    def get_input_shape(self):\n        """"""Returns an input shape of the wrapped IE model""""""\n        return self.inputs_info[self.input_key]\n\n\ndef load_ie_model(model_xml, device, cpu_extension=\'\', num_reqs=1):\n    """"""Loads a model in the Inference Engine format""""""\n    model_bin = os.path.splitext(model_xml)[0] + "".bin""\n    # Plugin initialization for specified device and load extensions library if specified\n    log.info(""Creating Inference Engine"")\n    ie = IECore()\n    if cpu_extension and \'CPU\' in device:\n        ie.add_extension(cpu_extension, \'CPU\')\n    # Read IR\n    log.info(""Loading network files:\\n\\t%s\\n\\t%s"", model_xml, model_bin)\n    net = IENetwork(model=model_xml, weights=model_bin)\n\n    if ""CPU"" in device:\n        supported_layers = ie.query_network(net, ""CPU"")\n        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n        if not_supported_layers:\n            log.error(""Following layers are not supported by the plugin for specified device %s:\\n %s"",\n                      device, \', \'.join(not_supported_layers))\n            log.error(""Please try to specify cpu extensions library path in sample\'s command line parameters using -l ""\n                      ""or --cpu_extension command line argument"")\n            sys.exit(1)\n\n    assert len(net.inputs.keys()) == 1 or len(net.inputs.keys()) == 2, \\\n        ""Supports topologies with only 1 or 2 inputs""\n    assert len(net.outputs) == 1 or len(net.outputs) == 4 or len(net.outputs) == 5, \\\n        ""Supports topologies with only 1, 4 or 5 outputs""\n\n    log.info(""Preparing input blobs"")\n    input_blob = next(iter(net.inputs))\n    out_blob = next(iter(net.outputs))\n    net.batch_size = 1\n\n    # Loading model to the plugin\n    log.info(""Loading model to the plugin"")\n    exec_net = ie.load_network(network=net, device_name=device, num_requests=num_reqs)\n    model = IEModel(exec_net, net.inputs, input_blob, out_blob)\n    return model\n\n\nclass OpenVINOModel:\n    """"""Wrapper class for a network returning a vector""""""\n\n    def __init__(self, model_path, cpu_extension=\'\', device=\'CPU\', max_reqs=1000):\n        self.max_reqs = max_reqs\n        self.net = load_ie_model(model_path, device, cpu_extension, num_reqs=self.max_reqs)\n\n    def forward(self, batch):\n        """"""Performs forward of the underlying network on a given batch""""""\n        assert len(batch) <= self.max_reqs\n        for frame in batch:\n            self.net.forward_async(frame)\n        outputs = self.net.grab_all_async()\n        for i in range(len(outputs)):\n            outputs[i] = np.squeeze(outputs[i])\n            outputs[i] = np.expand_dims(outputs[i], axis=0)\n        return outputs\n\n    def forward_async(self, batch):\n        """"""Performs async forward of the underlying network on a given batch""""""\n        assert len(batch) <= self.max_reqs\n        for frame in batch:\n            self.net.forward_async(frame)\n\n    def wait_and_grab(self):\n        outputs = self.net.grab_all_async()\n        return outputs\n'"
pytorch_toolkit/person_reidentification/models/osnet_fpn.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport logging as log\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torchreid.models.osnet import OSNet, ConvLayer, LightConv3x3, Conv1x1Linear, \\\n                                   ChannelGate, Conv1x1, pretrained_urls\nfrom .modules.fpn import FPN\nfrom .modules.dropout import Dropout\nfrom .modules.gmp import GeneralizedMeanPooling\n\n\n__all__ = [\'fpn_osnet_x1_0\', \'fpn_osnet_x0_75\', \'fpn_osnet_x0_5\', \'fpn_osnet_x0_25\', \'fpn_osnet_ibn_x1_0\']\n\npretrained_urls_fpn = {\n    \'fpn_osnet_x1_0\': pretrained_urls[\'osnet_x1_0\'],\n    \'fpn_osnet_x0_75\': pretrained_urls[\'osnet_x0_75\'],\n    \'fpn_osnet_x0_5\': pretrained_urls[\'osnet_x0_5\'],\n    \'fpn_osnet_x0_25\': pretrained_urls[\'osnet_x0_25\'],\n    \'fpn_osnet_ibn_x1_0\': pretrained_urls[\'osnet_ibn_x1_0\']\n}\n\n\nclass OSBlock(nn.Module):\n    """"""Omni-scale feature learning block.""""""\n\n    def __init__(self, in_channels, out_channels, IN=False, bottleneck_reduction=4,\n                 dropout_cfg=None, **kwargs):\n        super(OSBlock, self).__init__()\n        mid_channels = out_channels // bottleneck_reduction\n        self.conv1 = Conv1x1(in_channels, mid_channels)\n        self.conv2a = LightConv3x3(mid_channels, mid_channels)\n        self.conv2b = nn.Sequential(\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n        )\n        self.conv2c = nn.Sequential(\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n        )\n        self.conv2d = nn.Sequential(\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n        )\n        self.gate = ChannelGate(mid_channels)\n        self.conv3 = Conv1x1Linear(mid_channels, out_channels)\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = Conv1x1Linear(in_channels, out_channels)\n        self.IN = None\n        if IN:\n            self.IN = nn.InstanceNorm2d(out_channels, affine=True)\n        self.dropout = None\n        if dropout_cfg is not None:\n            self.dropout = Dropout(**dropout_cfg)\n\n    def forward(self, x):\n        identity = x\n        x1 = self.conv1(x)\n        x2a = self.conv2a(x1)\n        x2b = self.conv2b(x1)\n        x2c = self.conv2c(x1)\n        x2d = self.conv2d(x1)\n        x2 = self.gate(x2a) + self.gate(x2b) + self.gate(x2c) + self.gate(x2d)\n        x3 = self.conv3(x2)\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n        if self.dropout:\n            x3 = self.dropout(x3)\n        out = x3 + identity\n        if self.IN is not None:\n            out = self.IN(out)\n        return F.relu(out)\n\n\nclass OSNetFPN(OSNet):\n    """"""Omni-Scale Network.\n\n    Reference:\n        - Zhou et al. Omni-Scale Feature Learning for Person Re-Identification. ICCV, 2019.\n    """"""\n\n    def __init__(self, num_classes, blocks, layers, channels,\n                 feature_dim=256,\n                 loss=\'softmax\',\n                 instance_norm=False,\n                 dropout_cfg=None,\n                 fpn_cfg=None,\n                 pooling_type=\'avg\',\n                 input_size=(256, 128),\n                 IN_first=False,\n                 extra_blocks=False,\n                 **kwargs):\n        self.dropout_cfg = dropout_cfg\n        self.extra_blocks = extra_blocks\n        if self.extra_blocks:\n            for i, l in enumerate(layers):\n                layers[i] = l + 1\n        super(OSNetFPN, self).__init__(num_classes, blocks, layers, channels, feature_dim, loss, instance_norm)\n\n        self.feature_scales = (4, 8, 16, 16)\n        if fpn_cfg is not None:\n            self.fpn_enable = fpn_cfg.enable\n            self.fpn_dim = fpn_cfg.dim\n            self.fpn_process = fpn_cfg.process\n            assert self.fpn_process in [\'concatenation\', \'max_pooling\', \'elementwise_sum\']\n        else:\n            self.fpn_enable = False\n        self.feature_dim = feature_dim\n\n        self.use_IN_first = IN_first\n        if IN_first:\n            self.in_first = nn.InstanceNorm2d(3, affine=True)\n            self.conv1 = ConvLayer(3, channels[0], 7, stride=2, padding=3, IN=self.use_IN_first)\n\n        if self.fpn_enable:\n            self.fpn = FPN(channels, self.feature_scales, self.fpn_dim, self.fpn_dim)\n            fpn_out_dim = self.fpn_dim if self.fpn_process in [\'max_pooling\', \'elementwise_sum\'] \\\n                          else feature_dim\n            self.fc = self._construct_fc_layer(feature_dim, fpn_out_dim, dropout_cfg)\n        else:\n            self.fpn = None\n            self.fc = self._construct_fc_layer(feature_dim, channels[3], dropout_cfg)\n\n        if self.loss not in [\'am_softmax\', ]:\n            self.classifier = nn.Linear(self.feature_dim, num_classes)\n        else:\n            from engine.losses.am_softmax import AngleSimpleLinear\n            self.classifier = AngleSimpleLinear(self.feature_dim, num_classes)\n\n        if \'conv\' in pooling_type:\n            kernel_size = (input_size[0] // self.feature_scales[-1], input_size[1] // self.feature_scales[-1])\n            if self.fpn_enable:\n                self.global_avgpool = nn.Conv2d(fpn_out_dim, fpn_out_dim, kernel_size, groups=fpn_out_dim)\n            else:\n                self.global_avgpool = nn.Conv2d(channels[3], channels[3], kernel_size, groups=channels[3])\n        elif \'avg\' in pooling_type:\n            self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        elif \'gmp\' in pooling_type:\n            self.global_avgpool = GeneralizedMeanPooling()\n        else:\n            raise ValueError(\'Incorrect pooling type\')\n\n        if self.fpn_enable and self.fpn_process == \'concatenation\':\n            self.fpn_extra_conv = ConvLayer(self.fpn_dim * len(self.fpn.dims_out),\n                                            feature_dim, 3, stride=1, padding=1, IN=False)\n        else:\n            self.fpn_extra_conv = None\n\n        self._init_params()\n\n    def _make_layer(self, block, layer, in_channels, out_channels, reduce_spatial_size, IN=False):\n        layers = []\n\n        layers.append(block(in_channels, out_channels, IN=IN))\n        for i in range(1, layer):\n            layers.append(block(out_channels, out_channels, IN=IN, dropout_cfg=self.dropout_cfg))\n\n        if reduce_spatial_size:\n            layers.append(\n                nn.Sequential(\n                    Conv1x1(out_channels, out_channels),\n                    nn.AvgPool2d(2, stride=2)\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def _construct_fc_layer(self, fc_dims, input_dim, dropout_p=None):\n        if fc_dims is None or fc_dims < 0:\n            self.feature_dim = input_dim\n            return None\n\n        if isinstance(fc_dims, int):\n            fc_dims = [fc_dims]\n\n        layers = []\n        for dim in fc_dims:\n            layers.append(nn.Linear(input_dim, dim))\n            layers.append(nn.BatchNorm1d(dim))\n            if self.loss not in [\'am_softmax\', ]:\n                layers.append(nn.ReLU(inplace=True))\n            else:\n                layers.append(nn.PReLU())\n            if dropout_p is not None:\n                layers.append(nn.Dropout(p=dropout_p.p))\n            input_dim = dim\n\n        self.feature_dim = fc_dims[-1]\n\n        return nn.Sequential(*layers)\n\n    def featuremaps(self, x):\n        out = []\n        if self.use_IN_first:\n            x = self.in_first(x)\n        x = self.conv1(x)\n        x1 = self.maxpool(x)\n        out.append(x1)\n        x2 = self.conv2(x1)\n        out.append(x2)\n        x3 = self.conv3(x2)\n        out.append(x3)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)\n        out.append(x5)\n        return x5, out\n\n    def forward(self, x, return_featuremaps=False, get_embeddings=False):\n        x, feature_pyramid = self.featuremaps(x)\n        if self.fpn is not None:\n            feature_pyramid = self.fpn(feature_pyramid)\n            x = self.process_feature_pyramid(feature_pyramid)\n\n        if return_featuremaps:\n            return x\n\n        v = self.global_avgpool(x)\n        if isinstance(self.fc[0], nn.Linear):\n            v = v.view(v.size(0), -1)\n\n        if self.fc is not None:\n            if self.training:\n                v = self.fc(v)\n            else:\n                v = self.fc[0](v).view(v.size(0), -1, 1)\n                v = self.fc[1](v)\n                v = self.fc[2](v)\n        v = v.view(v.size(0), -1)\n\n        if not self.training:\n            return v\n\n        y = self.classifier(v)\n\n        if get_embeddings:\n            return v, y\n\n        if self.loss in [\'softmax\', \'adacos\', \'d_softmax\', \'am_softmax\']:\n            return y\n        elif self.loss in [\'triplet\', ]:\n            return v, y\n        else:\n            raise KeyError(""Unsupported loss: {}"".format(self.loss))\n\n    def process_feature_pyramid(self, feature_pyramid):\n        feature_pyramid = feature_pyramid[:-1]\n        target_shape = feature_pyramid[-1].shape[2:]\n        for i in range(len(feature_pyramid) - 1):\n            kernel_size = int(feature_pyramid[i].shape[2] // target_shape[0])\n            feature_pyramid[i] = nn.functional.max_pool2d(feature_pyramid[i], kernel_size=kernel_size)\n            if self.fpn_process == \'max_pooling\':\n                feature_pyramid[-1] = torch.max(feature_pyramid[i], feature_pyramid[-1])\n            elif self.fpn_process == \'elementwise_sum\':\n                feature_pyramid[-1] = torch.add(feature_pyramid[i], feature_pyramid[-1])\n            else:\n                feature_pyramid[-1] = torch.cat((feature_pyramid[i], feature_pyramid[-1]), dim=1)\n        if self.fpn_process == \'concatenation\':\n            output = self.fpn_extra_conv(feature_pyramid[-1])\n        else:\n            output = feature_pyramid[-1]\n        return output\n\n\ndef fpn_osnet_x1_0(num_classes=1000, pretrained=True, loss=\'softmax\', **kwargs):\n    model = OSNetFPN(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                     channels=[64, 256, 384, 512], loss=loss, **kwargs)\n    if pretrained:\n        init_pretrained_weights(model, key=\'fpn_osnet_x1_0\')\n    return model\n\n\ndef fpn_osnet_x0_75(num_classes=1000, pretrained=True, loss=\'softmax\', **kwargs):\n    model = OSNetFPN(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                     channels=[48, 192, 288, 384], loss=loss, **kwargs)\n    if pretrained:\n        init_pretrained_weights(model, key=\'fpn_osnet_x0_75\')\n    return model\n\n\ndef fpn_osnet_x0_5(num_classes=1000, pretrained=True, loss=\'softmax\', **kwargs):\n    model = OSNetFPN(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                     channels=[32, 128, 192, 256], loss=loss, **kwargs)\n    if pretrained:\n        init_pretrained_weights(model, key=\'fpn_osnet_x0_5\')\n    return model\n\n\ndef fpn_osnet_x0_25(num_classes=1000, pretrained=True, loss=\'softmax\', **kwargs):\n    model = OSNetFPN(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                     channels=[16, 64, 96, 128], loss=loss, **kwargs)\n    if pretrained:\n        init_pretrained_weights(model, key=\'fpn_osnet_x0_25\')\n    return model\n\n\ndef fpn_osnet_ibn_x1_0(num_classes=1000, pretrained=True, loss=\'softmax\', **kwargs):\n    # standard size (width x1.0) + IBN layer\n    model = OSNetFPN(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                     channels=[64, 256, 384, 512], loss=loss, IN=True, **kwargs)\n    if pretrained:\n        init_pretrained_weights(model, key=\'fpn_osnet_ibn_x1_0\')\n    return model\n\n\ndef init_pretrained_weights(model, key=\'\'):\n    """"""Initializes model with pretrained weights.\n\n    Layers that don\'t match with pretrained layers in name or size are kept unchanged.\n    """"""\n    import os\n    import errno\n    import gdown\n    from collections import OrderedDict\n\n    def _get_torch_home():\n        ENV_TORCH_HOME = \'TORCH_HOME\'\n        ENV_XDG_CACHE_HOME = \'XDG_CACHE_HOME\'\n        DEFAULT_CACHE_DIR = \'~/.cache\'\n        torch_home = os.path.expanduser(\n            os.getenv(ENV_TORCH_HOME,\n                      os.path.join(os.getenv(ENV_XDG_CACHE_HOME, DEFAULT_CACHE_DIR), \'torch\')))\n        return torch_home\n\n    torch_home = _get_torch_home()\n    model_dir = os.path.join(torch_home, \'checkpoints\')\n    try:\n        os.makedirs(model_dir)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            # Directory already exists, ignore.\n            pass\n        else:\n            # Unexpected OSError, re-raise.\n            raise\n    filename = key + \'_imagenet.pth\'\n    cached_file = os.path.join(model_dir, filename)\n\n    if not os.path.exists(cached_file):\n        gdown.download(pretrained_urls_fpn[key], cached_file, quiet=False)\n\n    state_dict = torch.load(cached_file)\n    model_dict = model.state_dict()\n    new_state_dict = OrderedDict()\n    matched_layers, discarded_layers = [], []\n\n    for k, v in state_dict.items():\n        if k.startswith(\'module.\'):\n            k = k[7:]  # discard module.\n\n        if k in model_dict and model_dict[k].size() == v.size():\n            new_state_dict[k] = v\n            matched_layers.append(k)\n        else:\n            discarded_layers.append(k)\n\n    model_dict.update(new_state_dict)\n    model.load_state_dict(model_dict)\n\n    if len(matched_layers) == 0:\n        log.warning(\n            \'The pretrained weights from ""{}"" cannot be loaded, \'\n            \'please check the key names manually \'\n            \'(** ignored and continue **)\'.format(cached_file))\n    else:\n        print(\'Successfully loaded imagenet pretrained weights from ""{}""\'.format(cached_file))\n        if len(discarded_layers) > 0:\n            print(\'** The following layers are discarded \'\n                  \'due to unmatched keys or layer size: {}\'.format(discarded_layers))\n'"
pytorch_toolkit/segthor/segthor/dataloader.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport random\nfrom datetime import datetime\nimport numpy as np\nfrom scipy.ndimage import affine_transform\nfrom scipy.ndimage.filters import gaussian_filter\nfrom scipy.ndimage.interpolation import map_coordinates\nimport torch\nimport torch.utils.data as data\n\nfrom segthor import loader_helper\n\n\n#https://gist.github.com/chsasank/4d8f68caf01f041a6453e67fb30f8f5a\ndef elastic_transform(image, alpha, sigma, order=3, random_state=None):\n    """"""Elastic deformation of images as described in [Simard2003]_.\n    .. [Simard2003] Simard, Steinkraus and Platt, ""Best Practices for\n       Convolutional Neural Networks applied to Visual Document Analysis"", in\n       Proc. of the International Conference on Document Analysis and\n       Recognition, 2003.\n    """"""\n    assert len(image.shape) == 3\n\n    if random_state is None:\n        random_state = np.random.RandomState(None)\n\n    shape = image.shape\n\n    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=""constant"", cval=0) * alpha\n    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=""constant"", cval=0) * alpha\n    dz = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=""constant"", cval=0) * (alpha/2.5)\n\n    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]), indexing=\'ij\')\n\n    indices = np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1)), np.reshape(z + dz, (-1, 1))\n\n    return map_coordinates(image, indices, order=order, mode=\'reflect\').reshape(shape)\n\n\n# pylint: disable=R0914,R0915\nclass SimpleReader(data.Dataset):\n    def __init__(self, path, patch_size, series=None, multiplier=1, patches_from_single_image=1):\n        super(SimpleReader, self).__init__()\n        self.path = path\n        self.patch_size = patch_size\n        self.multiplier = multiplier\n        self.patches_from_single_image = patches_from_single_image\n\n        if series is None:\n            self.series = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n        else:\n            self.series = series\n\n        self.series.sort()\n\n        self.labels_location = []\n\n        self.__cache()\n        self.real_length = len(self.series)\n\n        self.patches_from_current_image = self.patches_from_single_image\n        self.current_image_index = 0\n\n        self.__load(self.current_image_index)\n\n    @staticmethod\n    def get_data_filename(path, series):\n        return os.path.join(path, series, series+\'.nii.gz\')\n\n    @staticmethod\n    def get_label_filename(path, series):\n        return os.path.join(path, series, \'GT.nii.gz\')\n\n    def __cache(self):\n        # cache locations of the labels (bounding boxes) inside the images\n        for f in self.series:\n            label = loader_helper.read_nii(self.get_label_filename(self.path, f))\n\n            bbox = loader_helper.bbox3(label > 0)\n\n            borders = np.array(label.shape)\n            borders_low = np.array(self.patch_size) / 2.0 + 1\n            borders_high = borders - np.array(self.patch_size) / 2.0 - 1\n\n            bbox[0] = np.maximum(bbox[0]-100, borders_low)\n            bbox[1] = np.minimum(bbox[1]+100, borders_high)\n\n            self.labels_location.append(bbox)\n\n\n    def __load(self, index):\n        if self.patches_from_current_image > self.patches_from_single_image:\n            self.patches_from_current_image = 0\n            self.current_image_index = index\n            filename = self.get_data_filename(self.path, self.series[index])\n            labelname = self.get_label_filename(self.path, self.series[index])\n            self.image = loader_helper.read_nii(filename)\n            self.label = loader_helper.read_nii(labelname)\n\n            std = np.sqrt(loader_helper.mean2 - loader_helper.mean * loader_helper.mean)\n\n            self.image = (self.image - loader_helper.mean) / std\n\n        self.patches_from_current_image += 1\n\n    def __getitem__(self, index):\n        index = index % self.real_length\n        self.__load(index)\n        center = np.random.rand(3)\n\n        bbox = self.labels_location[self.current_image_index]\n\n        center = center * (bbox[1] - bbox[0]) + bbox[0]\n        left_bottom = center - np.array(self.patch_size) / 2.0\n        left_bottom = left_bottom.astype(np.int32)\n\n        data_out = self.image[left_bottom[0]:left_bottom[0] + self.patch_size[0],\n                              left_bottom[1]:left_bottom[1] + self.patch_size[1],\n                              left_bottom[2]:left_bottom[2] + self.patch_size[2]]\n\n        label_out = self.label[left_bottom[0]:left_bottom[0] + self.patch_size[0],\n                               left_bottom[1]:left_bottom[1] + self.patch_size[1],\n                               left_bottom[2]:left_bottom[2] + self.patch_size[2]]\n\n        seed = datetime.now().microsecond\n        sigma = random.random()*20 + 10\n        alpha = random.random()*4000 + 200\n\n        x_scale = 0.7 + random.random()*0.6\n        y_scale = 0.7 + random.random()*0.6\n\n        data_out = affine_transform(data_out, (x_scale, y_scale, 1), order=1)\n        data_out = elastic_transform(data_out, alpha, sigma, 1, np.random.RandomState(seed))[None]\n\n        label_out = affine_transform(label_out, (x_scale, y_scale, 1), order=0)\n        label_out = elastic_transform(label_out, alpha, sigma, 0, np.random.RandomState(seed))\n        label_out = np.eye(5)[label_out.astype(np.int32)].transpose((3, 0, 1, 2))\n\n        if random.random() > 0.5:\n            data_out = data_out[:, ::-1, :, :].copy()\n            label_out = label_out[:, ::-1, :, :].copy()\n\n\n        if random.random() > 0.5:\n            data_out = data_out[:, :, ::-1, :].copy()\n            label_out = label_out[:, :, ::-1, :].copy()\n\n        data_out = data_out * (0.6+random.random()*0.8)\n        data_out = data_out + 1.2*(random.random() - 0.5)\n\n        labels_torch = torch.from_numpy(label_out[1:].copy()).float()\n\n        return [torch.from_numpy(data_out).float(), ], \\\n               [labels_torch, ]\n\n\n    def __len__(self):\n        return self.multiplier*self.real_length\n\n\nclass FullReader(data.Dataset):\n    def __init__(self, path, series=None):\n        super(FullReader, self).__init__()\n        self.path = path\n\n        if series is None:\n            self.series = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n        else:\n            self.series = series\n\n        self.series.sort()\n\n    @staticmethod\n    def get_data_filename(path, series):\n        return os.path.join(path, series, series+\'.nii.gz\')\n\n    @staticmethod\n    def get_label_filename(path, series):\n        return os.path.join(path, series, \'GT.nii.gz\')\n\n    def __getitem__(self, index):\n\n        filename = self.get_data_filename(self.path, self.series[index])\n        labelname = self.get_label_filename(self.path, self.series[index])\n\n        image = loader_helper.read_nii(filename)\n        label = loader_helper.read_nii(labelname)\n\n        old_shape = image.shape\n        new_shape = tuple([loader_helper.closest_to_k(i, 32) for i in old_shape])\n        new_image = np.full(shape=new_shape, fill_value=-1000., dtype=np.float32)\n        new_label = np.zeros(shape=new_shape, dtype=np.float32)\n\n        new_image[:old_shape[0], :old_shape[1], :old_shape[2]] = image\n        new_label[:old_shape[0], :old_shape[1], :old_shape[2]] = label\n\n        mean = -303.0502877950004\n        mean2 = 289439.0029958802\n        std = np.sqrt(mean2 - mean * mean)\n\n        new_image = (new_image - mean) / std\n\n        new_label_out = (np.eye(5)[new_label.astype(np.int32)]).transpose((3, 0, 1, 2))\n\n        labels_torch = torch.from_numpy(new_label_out[1:].copy()).float()\n\n\n        return [torch.from_numpy(new_image[None, :, :, :]).float(), ], \\\n               [labels_torch, ]\n\n    def __len__(self):\n        return len(self.series)\n'"
pytorch_toolkit/segthor/segthor/loader_helper.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport nibabel as nib\nimport numpy as np\nfrom skimage import measure, morphology\nimport torch\n\n# dataset statistics\nmean = -303.0502877950004\nmean2 = 289439.0029958802\n\ndef read_nii(filename):\n    image = nib.load(filename)\n    return np.array(image.get_data())\n\ndef read_numpy(filename):\n    return np.load(filename)\n\ndef read_nii_header(filename):\n    return nib.load(filename)\n\n\ndef get_indices(position, center_shape, border):\n    index = [p * c for p, c in zip(position, center_shape)]\n    index_min = [i - b for i, b in zip(index, border)]\n    index_max = [i + c + b for i, c, b in zip(index, center_shape, border)]\n\n    return index_min, index_max\n\ndef read_sample(path, filename, read_annotation=True):\n    data_path = os.path.join(path, filename, filename + \'.nii.gz\')\n    label_path = os.path.join(path, filename, \'GT.nii.gz\')\n    annotation = None\n\n    header = read_nii_header(data_path)\n\n    data = np.array(header.get_data()).astype(np.float32)\n    if read_annotation:\n        annotation = read_nii(label_path)\n\n    return data, annotation, header\n\n\ndef lung_bbox(image):\n    mean_image = image.max(axis=2) > 300\n    mean_image = morphology.opening(mean_image, np.ones(shape=(5, 5)))\n    projection = np.max(mean_image, axis=0)\n    max_y = np.max(np.where(projection > 0))\n\n    mask = (image > -400)\n    mask = morphology.closing(mask, np.ones(shape=(10, 10, 10))) + 1\n    label = measure.label(mask)\n    background_label = label[0, 0, -1]\n    mask[label == background_label] = 2\n    mask = 2 - mask\n\n    bbox = bbox3(mask)\n\n    bbox[1, 2] = image.shape[2]\n    bbox[0, 2] = 0\n    bbox[1, 1] = max_y\n    bbox[0, 1] = np.maximum(bbox[0, 1] - 10, 0)\n    bbox[1, 0] = np.minimum(bbox[1, 0] + 10, image.shape[0])\n    bbox[0, 0] = np.maximum(bbox[0, 0] - 10, 0)\n\n    return bbox\n\ndef copy(data, tile_shape, index_min, index_max):\n    ret = torch.zeros(size=data.shape[:2]+tuple(tile_shape), dtype=torch.float)\n\n    index_clamp_min = np.maximum(index_min, 0)\n    index_clamp_max = np.minimum(index_max, data.shape[2:])\n\n    diff_min = [min_c - min_i for min_c, min_i in zip(index_clamp_min, index_min)]\n    diff_max = [t - (max_i - max_c) for t, max_c, max_i in zip(tile_shape, index_clamp_max, index_max)]\n\n    ret[:, :, diff_min[0]:diff_max[0], diff_min[1]:diff_max[1], diff_min[2]:diff_max[2]] = \\\n        data[:, :, index_clamp_min[0]:index_clamp_max[0], index_clamp_min[1]:index_clamp_max[1],\n             index_clamp_min[2]:index_clamp_max[2]]\n\n    return ret\n\n\ndef ravel_index(index, grid):\n    i = 0\n    prod = 1\n    for j in reversed(range(len(grid))):\n        i = i + prod * index[j]\n        prod = prod * grid[j]\n\n    return i\n\n\ndef unravel_index(index, grid):\n    i = []\n    prod = np.prod(grid)\n    for gr in grid:\n        prod = prod // gr\n        i.append(index // prod)\n        index = index % prod\n    return i\n\n\ndef copy_back(data, tile, center_shape, index_min, index_max, border):\n    index_center_min = [i + b for i, b in zip(index_min, border)]\n    index_center_max = [i - b for i, b in zip(index_max, border)]\n\n    index_clamp_min = np.maximum(index_center_min, 0)\n    index_clamp_max = np.minimum(index_center_max, data.shape[2:])\n\n    diff_min = [t + min_c - min_i for t, min_c, min_i in zip(border, index_clamp_min, index_center_min)]\n    diff_max = [b + t - (max_i - max_c) for b, t, max_c, max_i in\n                zip(border, center_shape, index_clamp_max, index_center_max)]\n\n    data[:, :, index_clamp_min[0]:index_clamp_max[0], index_clamp_min[1]:index_clamp_max[1],\n         index_clamp_min[2]:index_clamp_max[2]] = \\\n        tile[:, :, diff_min[0]:diff_max[0], diff_min[1]:diff_max[1], diff_min[2]:diff_max[2]]\n\n\ndef closest_to_k(n, k=8):\n    if n % k == 0:\n        return n\n    return ((n // k) + 1)*k\n\n\ndef leave_biggest_region(connectivity):\n    unique, counts = np.unique(connectivity, return_counts=True)\n    counts_idx = np.argsort(-counts)\n    unique = unique[counts_idx]\n\n    resulting_conectivity = np.zeros_like(connectivity)\n\n    resulting_conectivity[connectivity == unique[1]] = 1\n\n    return resulting_conectivity\n\n\ndef reject_small_regions(connectivity, ratio=0.25):\n    resulting_connectivity = connectivity.copy()\n    unique, counts = np.unique(connectivity, return_counts=True)\n\n    all_nonzero_clusters = np.prod(connectivity.shape) - np.max(counts)\n\n    for i in range(unique.shape[0]):\n        if counts[i] < ratio * all_nonzero_clusters:\n            resulting_connectivity[resulting_connectivity == unique[i]] = 0\n\n    return resulting_connectivity\n\ndef bbox3(img):\n    """"""\n    compute bounding box of the nonzero image pixels\n    :param img: input image\n    :return: bbox with shape (2, 3) and contents [min, max]\n    """"""\n    rows = np.any(img, axis=1)\n    rows = np.any(rows, axis=1)\n\n    cols = np.any(img, axis=0)\n    cols = np.any(cols, axis=1)\n\n    slices = np.any(img, axis=0)\n    slices = np.any(slices, axis=0)\n\n    rows = np.where(rows)\n    cols = np.where(cols)\n    slices = np.where(slices)\n    if rows[0].shape[0] > 0:\n        rmin, rmax = rows[0][[0, -1]]\n        cmin, cmax = cols[0][[0, -1]]\n        smin, smax = slices[0][[0, -1]]\n\n        return np.array([[rmin, cmin, smin], [rmax, cmax, smax]])\n    return np.array([[-1, -1, -1], [0, 0, 0]])\n'"
pytorch_toolkit/segthor/segthor/loss.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport torch\nimport torch.nn as nn\n\n# pylint: disable=W0212\nclass LossWrapper(nn.modules.loss._Loss):\n    def __init__(self, loss, input_index, target_index, size_average=None, reduce=None, reduction=\'elementwise_mean\'):\n        super(LossWrapper, self).__init__(size_average, reduce, reduction)\n        self.loss = loss\n        self.target_index = target_index\n        self.input_index = input_index\n\n    # pylint: disable=W0221\n    def forward(self, input, target):\n        return self.loss(input[self.input_index], target[self.target_index])\n\nclass Dice_loss_joint(nn.Module):\n    def __init__(self, index=0, priority=1):\n        super(Dice_loss_joint, self).__init__()\n        self.index = index\n        self.priority = priority\n\n    # pylint: disable=W0221\n    def forward(self, x, y):\n        assert x[self.index].shape == y[self.index].shape\n        N, C, _, _, _ = x[self.index].shape\n\n        pred = x[self.index].view(N, C, -1)\n        gt = y[self.index].view(N, C, -1)\n\n        intersection = (pred*gt).sum(dim=(0, 2)) + 1e-6\n        union = (pred**2 + gt).sum(dim=(0, 2)) + 2e-6\n\n        dice = 2.0*intersection / union\n\n        return self.priority*(1.0 - torch.mean(dice))\n'"
pytorch_toolkit/segthor/segthor/metrics.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport numpy as np\nimport SimpleITK as sitk\nimport torch\n\n\nclass Metrics():\n    def __init__(self, name):\n        self.name = name\n        self.accumulator = 0.0\n        self.samples = 0.0\n\n    #pylint: disable=W0613\n    def update(self, ground, predict):\n        self.samples = self.samples + 1\n\n    def get(self):\n        return self.accumulator / self.samples\n\n    def reset(self):\n        self.accumulator = 0.0\n        self.samples = 0.0\n\nclass Dice(Metrics):\n    def __init__(self, name=\'Dice\', input_index=0, target_index=0, classes=5):\n        super(Dice, self).__init__(name)\n        self.input_index = input_index\n        self.target_index = target_index\n        self.classes = classes\n\n    def update(self, ground, predict):\n        pred = predict[self.input_index].detach()\n        gr = ground[self.target_index].detach()\n\n        assert gr.shape == pred.shape\n\n        pred = ((torch.argmax(pred, dim=1) + 1) * (torch.max(pred, dim=1)[0] > 0.5).long()).long()\n        gr = ((torch.argmax(gr, dim=1) + 1) * (torch.max(gr, dim=1)[0] > 0.5).long()).long()\n\n        result = np.zeros(shape=(pred.shape[0], self.classes-1))\n\n        for i in range(1, self.classes):\n            p = (pred == i).float()\n            g = (gr == i).float()\n            #print(p.max(), g.max())\n            r = 2 * (p * g).sum(dim=(1, 2, 3))/((p+g).sum(dim=(1, 2, 3))+1e-6)\n            #print(r.shape)\n            result[:, i-1] = r.cpu().numpy()\n\n        self.accumulator = self.accumulator + result.mean(axis=0)\n\n        self.samples += 1\n\n\nclass Hausdorff_ITK(Metrics):\n    def __init__(self, name=\'Hausdorff_ITK\', input_index=0, target_index=0, classes=5):\n        super(Hausdorff_ITK, self).__init__(name)\n        self.input_index = input_index\n        self.target_index = target_index\n        self.classes = classes\n        self.hausdorff_distance_filter = sitk.HausdorffDistanceImageFilter()\n\n    def update(self, ground, predict):\n        pred = predict[self.input_index].detach()\n        gr = ground[self.target_index].detach()\n\n        assert gr.shape == pred.shape\n\n        pred = ((torch.argmax(pred, dim=1) + 1) * (torch.max(pred, dim=1)[0] > 0.5).long()).long().cpu().numpy()\n        gr = ((torch.argmax(gr, dim=1) + 1) * (torch.max(gr, dim=1)[0] > 0.5).long()).long().cpu().numpy()\n\n        result = np.zeros(shape=(pred.shape[0], self.classes-1))\n\n        for n in range(pred.shape[0]):\n            for i in range(1, self.classes):\n                p = (pred[n] == i).astype(np.uint8)\n                g = (gr[n] == i).astype(np.uint8)\n\n\n                r = 1e+6\n                try:\n                    self.hausdorff_distance_filter.Execute(sitk.GetImageFromArray(g), sitk.GetImageFromArray(p))\n                    r = self.hausdorff_distance_filter.GetHausdorffDistance()\n                except RuntimeError:\n                    print(""Hausdorff_ITK:RuntimeError"")\n\n                result[n, i-1] = r\n\n        self.accumulator = self.accumulator + result.mean(axis=0)\n\n        self.samples += 1\n\n\ndef print_metrics(writer, metric, prefix, epoch):\n    if isinstance(metric.get(), np.ndarray):\n        for i in range(metric.get().shape[0]):\n            writer.add_scalar(prefix + metric.name+str(i), metric.get()[i], epoch)\n    else:\n        writer.add_scalar(prefix + metric.name, metric.get(), epoch)\n\n    print(\'Epoch %d, %s %s %s\' % (epoch, prefix, metric.name, metric.get()))\n'"
pytorch_toolkit/segthor/segthor/model.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport gc\nimport torch\nimport torch.nn as nn\n\n\nclass conv(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, groups=1):\n        super(conv, self).__init__()\n\n        self.conv1 = nn.Conv3d(in_channels=in_channels, out_channels=out_channels,\n                               kernel_size=(5, 5, 3), stride=(stride, stride, 1), padding=(2, 2, 1),\n                               bias=False, groups=groups)\n\n    # pylint: disable=W0221\n    def forward(self, x):\n        out = x\n        out = self.conv1(out)\n        return out\n\nclass ResNextBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, downsample=None, width=4, compression=2):\n        super(ResNextBottleneck, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.downsample = downsample\n\n        conv_groups = out_channels // (width*compression)\n\n        self.conv_pre = nn.Conv3d(in_channels=in_channels, out_channels=out_channels//compression,\n                                  kernel_size=1, stride=1, padding=0, bias=False, groups=1)\n        self.conv1 = conv(in_channels=out_channels//compression, out_channels=out_channels//compression,\n                          stride=stride, groups=conv_groups)\n\n        self.relu1 = nn.LeakyReLU(2e-2, inplace=True)\n        self.relu2 = nn.LeakyReLU(2e-2, inplace=True)\n        self.relu3 = nn.LeakyReLU(2e-2, inplace=True)\n        self.norm1 = nn.InstanceNorm3d(num_features=out_channels//compression)\n        self.norm2 = nn.InstanceNorm3d(num_features=out_channels//compression)\n        self.norm3 = nn.InstanceNorm3d(num_features=out_channels)\n        self.conv_post = nn.Conv3d(in_channels=out_channels // compression, out_channels=out_channels,\n                                   kernel_size=1, padding=0, bias=False, groups=1)\n\n    # pylint: disable=W0221\n    def forward(self, x):\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n\n        out = x\n        out = self.conv_pre(out)\n        out = self.norm1(out)\n        out = self.relu1(out)\n\n\n        out = self.conv1(out)\n        out = self.norm2(out)\n        out = self.relu2(out)\n\n        out = self.conv_post(out)\n        out = self.norm3(out)\n\n        out = x + out\n        out = self.relu3(out)\n\n        return out\n\nclass shuffle(nn.Module):\n    def __init__(self, ratio):\n        super(shuffle, self).__init__()\n        self.ratio = ratio\n\n    # pylint: disable=W0221\n    def forward(self, x):\n        batch_size, in_channels, d, h, w = x.shape\n        out_channels = in_channels // (self.ratio*self.ratio*self.ratio)\n        out = x.view(batch_size*out_channels, self.ratio, self.ratio, self.ratio, d, h, w)\n        out = out.permute(0, 4, 1, 5, 2, 6, 3)\n\n        return out.contiguous().view(batch_size, out_channels, d*self.ratio, h*self.ratio, w*self.ratio)\n\n\nclass re_shuffle(nn.Module):\n    def __init__(self, ratio):\n        super(re_shuffle, self).__init__()\n        self.ratio = ratio\n\n    # pylint: disable=W0221\n    def forward(self, x):\n        batch_size, in_channels, d, h, w = x.shape\n\n        out_channels = in_channels * self.ratio * self.ratio * self.ratio\n        out = x.view(batch_size*in_channels, d//self.ratio, self.ratio,\n                     h//self.ratio, self.ratio, w//self.ratio, self.ratio)\n        out = out.permute(0, 2, 4, 6, 1, 3, 5)\n        out = out.contiguous().view(batch_size, out_channels, d//self.ratio, h//self.ratio, w//self.ratio)\n        return out\n\nclass UpsamplingPixelShuffle(nn.Module):\n    def __init__(self, input_channels, output_channels, ratio=2):\n        super(UpsamplingPixelShuffle, self).__init__()\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.conv = nn.Conv3d(in_channels=input_channels, out_channels=output_channels*int(ratio**3),\n                              kernel_size=1, padding=0, bias=False)\n        self.relu = nn.LeakyReLU(2e-2, inplace=True)\n        self.shuffle = shuffle(ratio=ratio)\n\n    # pylint: disable=W0221\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.shuffle(out)\n        return out\n\n# pylint: disable=R0201,R0902,R0915,W0621\nclass UNet(nn.Module):\n    def __init__(self, depth, encoder_layers, number_of_channels, number_of_outputs):\n        super(UNet, self).__init__()\n        print(\'UNet {}\'.format(number_of_channels))\n\n        self.encoder_layers = encoder_layers\n\n        self.number_of_channels = number_of_channels\n        self.number_of_outputs = number_of_outputs\n        self.depth = depth\n\n        self.conv_input = 0\n\n        self.encoder_convs = nn.ModuleList()\n\n        self.upsampling = nn.ModuleList()\n\n        self.decoder_convs = nn.ModuleList()\n\n        self.decoder_convs1x1 = nn.ModuleList()\n\n        self.attention_convs = nn.ModuleList()\n\n        self.upsampling_distance = nn.ModuleList()\n\n        self.conv_input = nn.Conv3d(in_channels=1, out_channels=self.number_of_channels[0], kernel_size=(7, 7, 3),\n                                    stride=1, padding=(3, 3, 1), bias=False)\n        self.norm_input = nn.InstanceNorm3d(num_features=self.number_of_channels[0])\n\n        conv_first_list = []\n        for _ in range(self.encoder_layers[0]):\n            conv_first_list.append(ResNextBottleneck(in_channels=self.number_of_channels[0],\n                                                     out_channels=self.number_of_channels[0], stride=1))\n\n        self.conv_first = nn.Sequential(*conv_first_list)\n\n        self.conv_middle = nn.Conv3d(in_channels=self.number_of_channels[-1], out_channels=self.number_of_channels[-1],\n                                     kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu = nn.LeakyReLU(2e-2, inplace=True)\n        self.sigmoid = nn.Sigmoid()\n\n        self.conv_output = nn.Conv3d(in_channels=self.number_of_channels[0], out_channels=self.number_of_outputs-1,\n                                     kernel_size=1, stride=1, padding=0, bias=True, groups=self.number_of_outputs-1)\n\n        self.softmax = nn.Softmax(dim=1)\n        self.construct_dencoder_convs(depth=depth, number_of_channels=number_of_channels)\n        self.construct_encoder_convs(depth=depth, number_of_channels=number_of_channels)\n        self.construct_upsampling_convs(depth=depth, number_of_channels=number_of_channels)\n\n    def _make_encoder_layer(self, in_channels, channels, blocks, stride=1, block=ResNextBottleneck):\n        downsample = None\n        if stride != 1:\n            downsample = nn.Sequential(\n                nn.Conv3d(in_channels=in_channels, out_channels=channels, kernel_size=2, stride=stride, bias=False)\n            )\n\n        layers = []\n        layers.append(block(in_channels=channels, out_channels=channels, stride=1, downsample=downsample))\n\n        for _ in range(1, blocks):\n            layers.append(block(in_channels=channels, out_channels=channels, stride=1))\n\n\n        return nn.Sequential(*layers)\n\n    def construct_encoder_convs(self, depth, number_of_channels):\n        for i in range(depth-1):\n            conv = self._make_encoder_layer(in_channels=number_of_channels[i], channels=number_of_channels[i+1],\n                                            blocks=self.encoder_layers[i+1], stride=2, block=ResNextBottleneck)\n            self.encoder_convs.append(conv)\n\n    def construct_dencoder_convs(self, depth, number_of_channels):\n        for i in range(depth):\n\n            conv_list = []\n            for _ in range(self.encoder_layers[i]):\n                conv_list.append(ResNextBottleneck(in_channels=number_of_channels[i],\n                                                   out_channels=number_of_channels[i], stride=1))\n\n            conv = nn.Sequential(\n                *conv_list\n            )\n\n            conv1x1 = nn.Conv3d(in_channels=2*number_of_channels[i], out_channels=number_of_channels[i],\n                                kernel_size=1, bias=False)\n            self.decoder_convs.append(conv)\n            self.decoder_convs1x1.append(conv1x1)\n\n    def construct_upsampling_convs(self, depth, number_of_channels):\n        for i in range(depth-1):\n            conv = UpsamplingPixelShuffle(input_channels=number_of_channels[i+1], output_channels=number_of_channels[i])\n            self.upsampling.append(conv)\n\n    # pylint: disable=W0221\n    def forward(self, x):\n        skip_connections = []\n        gc.collect()\n        input = x[0]\n\n        conv = self.conv_input(input)\n        conv = self.norm_input(conv)\n        conv = self.conv_first(conv)\n\n        for i in range(self.depth-1):\n            skip_connections.append(conv)\n            conv = self.encoder_convs[i](conv)\n\n        for i in reversed(range(self.depth-1)):\n            conv = self.upsampling[i](conv)\n            conv = self.relu(conv)\n\n            conc = torch.cat([skip_connections[i], conv], dim=1)\n            conv = self.decoder_convs1x1[i](conc)\n            conv = self.relu(conv)\n            conv = self.decoder_convs[i](conv)\n\n        out_logits = self.conv_output(conv)\n\n        out_logits = self.sigmoid(out_logits)\n\n        return [out_logits,]\n'"
pytorch_toolkit/segthor/segthor/train.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport shutil\nimport time\nimport gc\n\nimport torch\nfrom torch.autograd import Variable\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nfrom segthor import metrics\n\n\nclass TrainingState():\n    def __init__(self):\n        self.epoch = 0\n        self.train_metric = dict()\n        self.val_metric = dict()\n        # number of processed batches\n        self.global_step = 0\n        self.best_val = 0\n        self.optimizer_state = None\n        self.cuda = True\n\n\n#pylint: disable=R0913,R0914,R0915,W0212\nclass Trainer():\n    def __init__(self, name, models_root, model=None, rewrite=False, connect_tb=True):\n\n        self.model = model\n\n        assert (isinstance(self.model, (list, tuple, torch.nn.Module)) or self.model is None)\n\n        self.name = name\n        self.models_root = models_root\n        self.model_path = os.path.join(models_root, self.name)\n        self.logs_path = os.path.join(self.model_path, \'logs\')\n\n        self.state = TrainingState()\n        self.resume_training = False\n        self.eval_cpu = True\n\n        if os.path.exists(self.model_path):\n            if rewrite:\n                shutil.rmtree(self.model_path)\n            else:\n                self.resume_training = True\n\n        if not os.path.exists(self.model_path):\n            os.mkdir(self.model_path)\n            os.mkdir(self.logs_path)\n\n        if connect_tb:\n            self.tb_writer = SummaryWriter(logdir=self.logs_path)\n\n    def cuda(self):\n        if self.model is not None:\n            self.model.cuda()\n        self.state.cuda = True\n\n    def train(self, criterion, optimizer, optimizer_params, scheduler, scheduler_params, training_data_loader,\n              evaluation_data_loader, pretrained_weights, train_metrics, val_metrics,\n              track_metric, epoches, default_val, comparator, eval_cpu):\n\n        self.eval_cpu = eval_cpu\n\n        assert isinstance(criterion, (tuple, list, torch.nn.modules.loss._Loss))\n\n        # load weights if any\n        if self.resume_training:\n            # load training and continue\n            self.load_latest()\n        elif pretrained_weights is not None:\n            # load dictionary only\n            self.model.load_state_dict(pretrained_weights)\n        else:\n            self.state.best_val = default_val\n\n        if isinstance(optimizer, type):\n            optimizer = optimizer(params=self.model.parameters(), **optimizer_params)\n\n        if scheduler is not None:\n            if isinstance(scheduler, type):\n                scheduler = scheduler(optimizer=optimizer, **scheduler_params)\n\n        assert isinstance(optimizer, torch.optim.Optimizer)\n        assert isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler) or scheduler is None\n\n        if self.state.optimizer_state is not None:\n            optimizer.load_state_dict(self.state.optimizer_state)\n            print(\'Loaded optimizer state\')\n\n        # prepare dicts for metrics\n        if not self.state.train_metric:\n            for m in train_metrics:\n                self.state.train_metric[m.name] = []\n\n            for m in val_metrics:\n                self.state.val_metric[m.name] = []\n\n        gc.collect()\n\n        # training loop\n        start_epoch = self.state.epoch\n        for i in range(start_epoch, epoches):\n            tic = time.time()\n\n            self.state.global_step = self._train_one_epoch(criterion, optimizer, training_data_loader,\n                                                           train_metrics, self.state.train_metric, i,\n                                                           self.state.global_step, scheduler)\n\n            self._evaluate_and_save(evaluation_data_loader, val_metrics, track_metric, self.state.val_metric, i,\n                                    comparator)\n\n            tac = time.time()\n            print(\'Epoch %d, time %s \\n\' % (i, tac - tic))\n\n            self._save(suffix=\'_epoch_\' + str(self.state.epoch))\n            self._save(suffix=\'last_model\')\n            self.state.epoch = self.state.epoch + 1\n\n        np.random.seed(np.random.get_state()[1][0] + 16)\n\n    def predict(self, batch):\n        self.model.eval()\n\n        if self.state.cuda:\n            self.model.cuda()\n\n        with torch.no_grad():\n            assert isinstance(batch[0], list)\n            data = [Variable(b) for b in batch[0]]\n\n            if self.state.cuda:\n                data = [d.cuda() for d in data]\n\n            output = self.model(data)\n        return output\n\n    def _train_one_epoch(self, criterion, optimizer, training_data_loader, train_metrics, train_metrics_results, epoch,\n                         global_step, scheduler):\n\n        aggregate_batches = 1\n        for m in train_metrics:\n            m.reset()\n\n        if self.state.cuda:\n            self.model.cuda()\n\n        self.model.train()\n\n        optimizer.zero_grad()\n        for idx, batch in enumerate(training_data_loader):\n\n            assert (isinstance(batch[0], list) and isinstance(batch[1], list))\n            data = [Variable(b) for b in batch[0]]\n            target = [Variable(b, requires_grad=False) for b in batch[1]]\n\n            if self.state.cuda:\n                data = [d.cuda() for d in data]\n                target = [t.cuda() for t in target]\n\n            output = self.model(data)\n\n            if isinstance(criterion, (tuple, list)):\n                loss_val = [c(output, target) for c in criterion]\n                loss = sum(loss_val) / (len(loss_val))\n            else:\n                loss_val = criterion(output, target)\n                loss = loss_val\n\n            loss.backward()\n\n            if (idx+1) % aggregate_batches == 0:\n\n                optimizer.step()\n                optimizer.zero_grad()\n                if scheduler is not None:\n                    scheduler.step()\n\n            for m in train_metrics:\n                m.update(output, target)\n\n            for idxx, l in enumerate(loss_val):\n                self.tb_writer.add_scalar(\'loss/loss-{}\'.format(idxx), l.item(), global_step)\n\n            for idxx, param_group in enumerate(optimizer.param_groups):\n                self.tb_writer.add_scalar(\'misc/lr-{}\'.format(idxx), param_group[\'lr\'], global_step)\n\n            global_step = global_step + 1\n\n        for m in train_metrics:\n            train_metrics_results[m.name].append(m.get())\n            metrics.print_metrics(self.tb_writer, m, \'train/\', epoch)\n\n        self.state.optimizer_state = optimizer.state_dict()\n        return global_step\n\n    def _evaluate_and_save(self, evaluation_data_loader, val_metrics, track_metric, val_metrics_results, epoch,\n                           comparator):\n\n        for m in val_metrics:\n            m.reset()\n\n        self.model.eval()\n\n        for batch in evaluation_data_loader:\n            gc.collect()\n\n            assert (isinstance(batch[0], list) and isinstance(batch[1], list))\n\n            data = batch[0]#[b.cuda() for b in batch[0]]\n            tmp_model = self.model.module.cpu()\n            tmp_model.eval()\n            with torch.no_grad():\n                output = tmp_model(data)\n\n            target = [Variable(b, requires_grad=False) for b in batch[1]]\n            for m in val_metrics:\n                m.update(target, output)\n\n        val = 0.0\n        for m in val_metrics:\n            if m.name == track_metric:\n                val = m.get()\n\n            metrics.print_metrics(self.tb_writer, m, \'val/\', epoch)\n            val_metrics_results[m.name].append(m.get())\n\n        if comparator(val, self.state.best_val):\n            self.state.best_val = val\n            self._save(suffix=\'best_model\')\n            print(\'model saved\')\n\n    def _save(self, suffix):\n        s = {\'state\': self.state,\n             \'model\': self.model}\n\n        torch.save(s, os.path.join(self.model_path, self.name + suffix + \'.pth\'))\n\n    def _load(self, suffix):\n        print(\'loading model\')\n        s = torch.load(os.path.join(self.model_path, self.name + suffix + \'.pth\'), map_location=torch.device(\'cpu\'))\n        self.state = s[\'state\']\n        if self.model is None:\n            self.model = s[\'model\']\n        else:\n            self.model.load_state_dict(s[\'model\'].state_dict())\n\n    def load_latest(self):\n        self._load(\'last_model\')\n\n    def load_best(self):\n        self._load(\'best_model\')\n'"
pytorch_toolkit/segthor/segthor/weight_init.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n#https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5\n\n\nimport torch.nn as nn\nimport torch.nn.init as init\n\n# pylint: disable=R0912,R0915\ndef weight_init(m):\n    \'\'\'\n    Usage:\n        model = Model()\n        model.apply(weight_init)\n    \'\'\'\n    if isinstance(m, nn.Conv1d):\n        init.normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.Conv2d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.Conv3d):\n        init.kaiming_normal_(m.weight.data, a=2e-2, nonlinearity=\'leaky_relu\')\n        if m.bias is not None:\n            init.zeros_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose1d):\n        init.normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose2d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose3d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.BatchNorm1d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.BatchNorm2d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.BatchNorm3d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.Linear):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.LSTM):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.LSTMCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.GRU):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.GRUCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.ModuleList):\n        for l in m:\n            weight_init(l)\n\n\nif __name__ == \'__main__\':\n    pass\n'"
pytorch_toolkit/segthor/tools/export_onnx.py,0,"b'#!/usr/bin/env python3\r\n#\r\n# Copyright (C) 2019 Intel Corporation\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n# http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing,\r\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions\r\n# and limitations under the License.\r\n\r\nimport argparse\r\nimport os\r\nimport torch.onnx\r\nfrom segthor import train\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=""PyTorch SegTHOR export onnx"")\r\n    parser.add_argument(""--name"", default=""test"", type=str, help=""Experiment name"")\r\n    parser.add_argument(""--models_path"", default=""models"", type=str, help=""Path to models folder"")\r\n    parser.add_argument(""--input_size"", default=(64, 64, 64), help=""Input image size"", nargs=""+"", type=int)\r\n    return parser.parse_args()\r\n\r\ndef main():\r\n    opt = parse_args()\r\n    print(opt)\r\n\r\n    trainer = train.Trainer(name=opt.name, models_root=opt.models_path, rewrite=False, connect_tb=False)\r\n    trainer.load_best()\r\n    trainer.model = trainer.model.module.cpu()\r\n    trainer.model = trainer.model.train(False)\r\n    trainer.state.cuda = False\r\n\r\n    x = torch.randn(1, 1, opt.input_size[0], opt.input_size[1], opt.input_size[2], requires_grad=True)\r\n\r\n    export_dir = os.path.join(opt.models_path, opt.name)\r\n    onnx_path = os.path.join(export_dir, opt.name+"".onnx"")\r\n    torch.onnx.export(trainer.model,  # model being run\r\n                      [x,],  # model input (or a tuple for multiple inputs)\r\n                      onnx_path,  # where to save the model (can be a file or file-like object)\r\n                      export_params=True,\r\n                      verbose=True)  # store the trained parameter weights inside the model file\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
pytorch_toolkit/segthor/tools/main.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport pickle\nimport random\nimport argparse\nimport gc\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data as Data\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom segthor import dataloader\nfrom segthor import loss\nfrom segthor import metrics\nfrom segthor import train\nfrom segthor import weight_init\nfrom segthor.model import UNet\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""PyTorch SegTHOR training"")\n\n    parser.add_argument(""--batchSize"", type=int, default=1, help=""Training batch size"")\n    parser.add_argument(""--nEpochs"", type=int, default=500, help=""Number of epochs to train for"")\n    parser.add_argument(""--threads"", type=int, default=0, help=""Number of threads for data loader to use"")\n    parser.add_argument(""--train_path"", default=""./data"", type=str, help=""Path to train data"", required=True)\n    parser.add_argument(""--name"", default=""test"", type=str, help=""Experiment name"")\n    parser.add_argument(""--models_path"", default=""./models"", type=str, help=""Path to models folder"")\n    parser.add_argument(""--splits"", default=5, type=int, help=""Number of splits in CV"")\n    parser.add_argument(""--gpus"", default=4, type=int, help=""Number of gpus to use"")\n    parser.add_argument(""--seed"", default=1337, type=int, help=""Seed for random generators"")\n\n    return parser.parse_args()\n\n\ndef worker_init_fn(worker_id):\n    seed = np.random.get_state()[1][0] + worker_id\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    print(\'worker id {} seed {}\'.format(worker_id, seed))\n\n# pylint: disable=R0914,R0915\ndef main():\n    opt = parse_args()\n\n    random.seed(opt.seed)\n    torch.manual_seed(opt.seed)\n    np.random.seed(opt.seed)\n    torch.cuda.manual_seed(opt.seed)\n\n    kf = KFold(n_splits=opt.splits)\n    splits = []\n    evaluation_metrics = []\n\n    print(""CV {} splits"".format(kf.get_n_splits()))\n\n    series = [f for f in os.listdir(opt.train_path) if os.path.isdir(os.path.join(opt.train_path, f))]\n    series.sort()\n\n    for idx, (train_index, test_index) in enumerate(kf.split(series)):\n\n        print(train_index, test_index)\n\n        print(""===> Building model"")\n        layers = [1, 2, 2, 4, 6]\n        number_of_channels = [int(4*8*2**i) for i in range(1, 6)]\n        model = UNet(depth=len(layers), encoder_layers=layers,\n                     number_of_channels=number_of_channels, number_of_outputs=5)\n        model.apply(weight_init.weight_init)\n        model = torch.nn.DataParallel(module=model, device_ids=range(opt.gpus))\n\n        trainer = train.Trainer(model=model, name=opt.name+str(idx), models_root=opt.models_path, rewrite=False)\n        trainer.cuda()\n\n        gc.collect()\n\n        random.seed(opt.seed)\n        torch.manual_seed(opt.seed)\n        np.random.seed(opt.seed)\n        torch.cuda.manual_seed(opt.seed)\n\n        cudnn.benchmark = True\n\n        print(""===> Loading datasets"")\n        print(\'Train data:\', opt.train_path)\n\n\n        series_train = [series[i] for i in train_index]\n        series_val = [series[i] for i in test_index]\n        print(\'Train {}\'.format(series_train))\n        print(\'Val {}\'.format(series_val))\n\n        train_set = dataloader.SimpleReader(path=opt.train_path,\n                                            patch_size=(16*13, 16*8, 16*5),\n                                            series=series_train,\n                                            multiplier=500,\n                                            patches_from_single_image=1)\n\n        val_set = dataloader.FullReader(path=opt.train_path, series=series_val)\n\n        training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads,\n                                          batch_size=opt.batchSize, shuffle=True,\n                                          drop_last=True, worker_init_fn=worker_init_fn)\n\n        batch_sampler = Data.BatchSampler(\n            sampler=Data.SequentialSampler(val_set),\n            batch_size=1,\n            drop_last=True\n        )\n\n        evaluation_data_loader = DataLoader(dataset=val_set, num_workers=0,\n                                            batch_sampler=batch_sampler)\n\n        criterion = [loss.Dice_loss_joint(index=0, priority=1).cuda()]\n\n        print(""===> Training"")\n\n        trainer.train(criterion=criterion,\n                      optimizer=optim.SGD,\n                      optimizer_params={""lr"":1e-0,\n                                        ""weight_decay"":1e-6,\n                                        ""momentum"":0.9\n                                        },\n                      scheduler=torch.optim.lr_scheduler.MultiStepLR,\n                      scheduler_params={""milestones"":[150000, 200000, 240000, 280000],\n                                        ""gamma"":0.2},\n                      training_data_loader=training_data_loader,\n                      evaluation_data_loader=evaluation_data_loader,\n                      pretrained_weights=None,\n                      train_metrics=[\n                          metrics.Dice(name=\'Dice\', input_index=0, target_index=0),\n                      ],\n                      val_metrics=[\n                          metrics.Dice(name=\'Dice\', input_index=0, target_index=0),\n                          metrics.Hausdorff_ITK(name=\'Hausdorff_ITK\', input_index=0, target_index=0)\n                      ],\n                      track_metric=\'Dice\',\n                      epoches=opt.nEpochs,\n                      default_val=np.array([0, 0, 0, 0, 0]),\n                      comparator=lambda x, y: np.min(x)+np.mean(x) > np.min(y)+np.mean(y),\n                      eval_cpu=True\n                      )\n\n        evaluation_metrics.append(trainer.state.best_val)\n        splits.append((series_train, series_val))\n\n\n    avg_val = 0\n\n    for i in evaluation_metrics:\n        avg_val = avg_val + i\n\n    print(\'Average val {}\'.format(avg_val/len(evaluation_metrics)))\n\n    pickle.dump(evaluation_metrics, open(opt.name+\'_eval.p\', \'wb\'))\n    pickle.dump(splits, open(opt.name+\'_splits.p\', \'wb\'))\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_toolkit/segthor/tools/prepare_training_dataset.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport argparse\nimport numpy as np\nimport nibabel as nii\nfrom scipy.ndimage.filters import median_filter\nfrom scipy.ndimage import zoom\nimport tqdm\n\nfrom segthor import loader_helper\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""PyTorch SegTHOR prepare dataset for training"")\n    parser.add_argument(""--input_path"", type=str, help=""path to train data"")\n    parser.add_argument(""--output_path"", type=str, help=""path to output dataset"")\n    parser.add_argument(""--new_scale"", nargs=\'+\', default=[1.0, 1.0, 2.5], type=float,\n                        help=""spatial resolution to resample to"")\n    return parser.parse_args()\n\ndef main():\n    opt = parse_args()\n    print(opt)\n\n    file_list = [f for f in os.listdir(opt.input_path) if os.path.isdir(os.path.join(opt.input_path, f))]\n    file_list.sort()\n    print(file_list)\n\n    new_scale = np.array(opt.new_scale)\n    affine = np.diag(np.append(new_scale, 1))\n\n    for f in tqdm.tqdm(file_list):\n        image, gt, header = loader_helper.read_sample(opt.input_path, f, True)\n\n        image = median_filter(image, 3)\n\n        scale = header.header[\'pixdim\'][1:4]\n\n        scale_factor = scale / new_scale\n\n        # resample\n        image = zoom(image, scale_factor, order=3, mode=\'constant\', cval=-1024)\n        gt = zoom(gt, scale_factor, order=0, mode=\'constant\', cval=0)\n\n        bbox = loader_helper.lung_bbox(image)\n\n        image_crop = image[bbox[0, 0]:bbox[1, 0], bbox[0, 1]:bbox[1, 1], bbox[0, 2]:bbox[1, 2]]\n        gt_crop = gt[bbox[0, 0]:bbox[1, 0], bbox[0, 1]:bbox[1, 1], bbox[0, 2]:bbox[1, 2]]\n\n        os.mkdir(os.path.join(opt.output_path, f))\n        new_image = nii.Nifti1Image(image_crop.astype(np.float32), affine)\n        new_gt = nii.Nifti1Image(gt_crop.astype(np.float32), affine)\n\n        nii.save(new_image, os.path.join(opt.output_path, f, f + \'.nii.gz\'))\n        nii.save(new_gt, os.path.join(opt.output_path, f, \'GT.nii.gz\'))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/segthor/tools/test.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport argparse\nimport nibabel as nii\nimport numpy as np\nfrom scipy.ndimage import zoom\nfrom scipy.ndimage.filters import median_filter\nfrom skimage import morphology\nimport torch\nimport tqdm\n\nimport loader_helper\nimport train\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""PyTorch SegTHOR predict"")\n    parser.add_argument(""--name"", default=""test"", type=str, help=""Experiment name"")\n    parser.add_argument(""--models_path"", default=""./models"", type=str, help=""Path to models folder"")\n    parser.add_argument(""--data_path"", default=""./data"", type=str, help=""Path to the data folder"")\n    parser.add_argument(""--output_path"", default=""./output"", type=str, help=""Path to the output folder"")\n    parser.add_argument(""--new_scale"", nargs=\'+\', default=[1.0, 1.0, 2.5], type=float,\n                        help=""Spatial resolution to resample to"")\n    return parser.parse_args()\n\n# pylint: disable=R0914,R0915\ndef main():\n    opt = parse_args()\n    print(opt)\n\n    trainer = train.Trainer(name=opt.name, models_root=opt.models_path, rewrite=False, connect_tb=False)\n    trainer.load_best()\n    trainer.model = trainer.model.module.cpu()\n    trainer.state.cuda = False\n\n    files = os.listdir(opt.data_path)\n    files = [f for f in files if (f.startswith(\'Patient\') and os.path.isfile(os.path.join(opt.data_path, f)))]\n    files.sort()\n\n    for f in tqdm.tqdm(files):\n        header = loader_helper.read_nii_header(os.path.join(opt.data_path, f))\n        image = np.array(header.get_data()).astype(np.float32)\n        original_shape = image.shape\n\n        image = median_filter(image, 3)\n\n        scale = header.header[\'pixdim\'][1:4]\n\n        scale_factor = scale / opt.new_scale\n        image = zoom(image, scale_factor, order=3, mode=\'constant\', cval=-1024)\n\n        bbox = loader_helper.lung_bbox(image)\n\n        image_crop = image[bbox[0, 0]:bbox[1, 0], bbox[0, 1]:bbox[1, 1], bbox[0, 2]:bbox[1, 2]]\n\n        #=========================================\n        new_shape_crop = tuple([loader_helper.closest_to_k(i, 16) for i in image_crop.shape])\n        diff = np.array(new_shape_crop) - np.array(image_crop.shape)\n        pad_left = diff // 2\n        pad_right = diff - pad_left\n\n        new_data_crop = np.pad(image_crop, pad_width=tuple([(pad_left[i], pad_right[i]) for i in range(3)]),\n                               mode=\'reflect\')\n\n        std = np.sqrt(loader_helper.mean2 - loader_helper.mean * loader_helper.mean)\n\n        new_data_crop = (new_data_crop - loader_helper.mean) / std\n\n        new_data_crop = torch.from_numpy(new_data_crop[None, None, :, :, :]).float()\n        output = trainer.predict([[new_data_crop], ])\n        output_crop = output[0].cpu().detach().numpy()[0]\n\n        output_crop = output_crop[:, pad_left[0]:-pad_right[0] or None, pad_left[1]:-pad_right[1] or None,\n                                  pad_left[2]:-pad_right[2] or None]\n\n        new_label = np.zeros(shape=(4,)+image.shape)\n        new_label[:, bbox[0, 0]:bbox[1, 0], bbox[0, 1]:bbox[1, 1], bbox[0, 2]:bbox[1, 2]] = output_crop\n\n        # ==========================\n\n        scale_factor = np.array(original_shape) / np.array(image.shape)\n        old_labels = [zoom(new_label[i], scale_factor, order=1, mode=\'constant\', cval=0)[None] for i in range(4)]\n\n        old_label = np.concatenate(tuple(old_labels), axis=0)\n        old_label = ((np.argmax(old_label, axis=0) + 1) *\n                     np.max((old_label > np.array([0.5, 0.5, 0.5, 0.5]).reshape((-1, 1, 1, 1))).astype(np.int32),\n                            axis=0)).astype(np.int32)\n\n\n        assert old_label.shape == original_shape\n\n        eso_connectivity = morphology.label(old_label == 1)\n        heart_connectivity = morphology.label(old_label == 2)\n        trachea_connectivity = morphology.label(old_label == 3)\n        aorta_connectivity = morphology.label(old_label == 4)\n        eso_connectivity = loader_helper.reject_small_regions(eso_connectivity, ratio=0.2)\n        heart_connectivity = loader_helper.leave_biggest_region(heart_connectivity)\n        trachea_connectivity = loader_helper.leave_biggest_region(trachea_connectivity)\n        aorta_connectivity = loader_helper.leave_biggest_region(aorta_connectivity)\n\n        old_label[np.logical_and(old_label == 1, eso_connectivity == 0)] = 0\n        old_label[np.logical_and(old_label == 2, heart_connectivity == 0)] = 0\n        old_label[np.logical_and(old_label == 3, trachea_connectivity == 0)] = 0\n        old_label[np.logical_and(old_label == 4, aorta_connectivity == 0)] = 0\n\n\n        output_header = nii.Nifti1Image(old_label, header.affine)\n        nii.save(output_header, os.path.join(opt.output_path, f[:-7]+\'.nii\'))\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/segthor/tools/thoracic_segmentation.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport logging\nfrom sys import stdout\n\nfrom datetime import datetime\nfrom argparse import ArgumentParser, SUPPRESS\nfrom scipy.ndimage import zoom\nfrom scipy.ndimage.filters import median_filter\nfrom skimage import measure, morphology\n\nimport numpy as np\nimport nibabel as nii\nfrom openvino.inference_engine import IENetwork, IEPlugin\n\nlogging.basicConfig(format=""[ %(levelname)s ] %(message)s"", level=logging.DEBUG, stream=stdout)\nlogger = logging.getLogger(\'thoracic_segmentation_demo\')\n\n\ndef parse_arguments():\n    parser = ArgumentParser(add_help=False)\n    args = parser.add_argument_group(\'Options\')\n    args.add_argument(\'-h\', \'--help\', action=\'help\', default=SUPPRESS, help=\'Show this help message and exit.\')\n    args.add_argument(\'-i\', \'--path_to_input_data\', type=str, required=True,\n                      help=""Required. Path to an input folder with NIfTI data/TIFF file"")\n    args.add_argument(\'-m\', \'--path_to_model\', type=str, required=True,\n                      help=""Required. Path to an .xml file with a trained model"")\n    args.add_argument(\'-o\', \'--path_to_output\', type=str, required=True,\n                      help=""Required. Path to a folder where output files will be saved"")\n    args.add_argument(\'-d\', \'--target_device\', type=str, required=False, default=""CPU"",\n                      help=""Optional. Specify a target device to infer on: CPU. ""\n                           ""Use \\""-d HETERO:<comma separated devices list>\\"" format to specify HETERO plugin."")\n    args.add_argument(\'-l\', \'--path_to_extension\', type=str, required=False, default=None,\n                      help=""Required for CPU custom layers. ""\n                           ""Absolute path to a shared library with the kernels implementations."")\n    args.add_argument(\'-nthreads\', \'--number_threads\', type=int, required=False, default=None,\n                      help=""Optional. Number of threads to use for inference on CPU (including HETERO cases)."")\n    args.add_argument(\'-c\', \'--path_to_cldnn_config\', type=str, required=False,\n                      help=""Required for GPU custom kernels. ""\n                           ""Absolute path to an .xml file with the kernels description."")\n    return parser.parse_args()\n\n\ndef read_nii_header(filename):\n    return nii.load(filename)\n\n\ndef bbox3(img):\n    """"""\n    compute bounding box of the nonzero image pixels\n    :param img: input image\n    :return: bbox with shape (2,3) and contents [min,max]\n    """"""\n    rows = np.any(img, axis=1)\n    rows = np.any(rows, axis=1)\n\n    cols = np.any(img, axis=0)\n    cols = np.any(cols, axis=1)\n\n    slices = np.any(img, axis=0)\n    slices = np.any(slices, axis=0)\n\n    rows = np.where(rows)\n    cols = np.where(cols)\n    slices = np.where(slices)\n    if rows[0].shape[0] > 0:\n        rmin, rmax = rows[0][[0, -1]]\n        cmin, cmax = cols[0][[0, -1]]\n        smin, smax = slices[0][[0, -1]]\n\n        return np.array([[rmin, cmin, smin], [rmax, cmax, smax]])\n    return np.array([[-1, -1, -1], [0, 0, 0]])\n\n\ndef lung_bbox(image):\n    mean_image = image.max(axis=2) > 300\n    mean_image = morphology.opening(mean_image, np.ones(shape=(5, 5)))\n    projection = np.max(mean_image, axis=0)\n    max_y = np.max(np.where(projection > 0))\n\n    mask = (image > -400)\n    mask = morphology.closing(mask, np.ones(shape=(10, 10, 10))) + 1\n    label = measure.label(mask)\n    background_label = label[0, 0, -1]\n    mask[label == background_label] = 2\n    mask = 2 - mask\n\n    bbox = bbox3(mask)\n\n    bbox[1, 2] = image.shape[2]\n    bbox[0, 2] = 0\n    bbox[1, 1] = max_y\n    bbox[0, 1] = np.maximum(bbox[0, 1] - 10, 0)\n    bbox[1, 0] = np.minimum(bbox[1, 0] + 10, image.shape[0])\n    bbox[0, 0] = np.maximum(bbox[0, 0] - 10, 0)\n\n    return bbox\n\n\ndef closest_to_k(n, k=8):\n    if n % k == 0:\n        return n\n    return ((n // k) + 1)*k\n\n\ndef leave_biggest_region(connectivity):\n    unique, counts = np.unique(connectivity, return_counts=True)\n    counts_idx = np.argsort(-counts)\n    unique = unique[counts_idx]\n\n    resulting_conectivity = np.zeros_like(connectivity)\n\n    resulting_conectivity[connectivity == unique[1]] = 1\n\n    return resulting_conectivity\n\n\ndef reject_small_regions(connectivity, ratio=0.25):\n    resulting_connectivity = connectivity.copy()\n    unique, counts = np.unique(connectivity, return_counts=True)\n\n    all_nonzero_clusters = np.prod(connectivity.shape) - np.max(counts)\n\n    for i in range(unique.shape[0]):\n        if counts[i] < ratio * all_nonzero_clusters:\n            resulting_connectivity[resulting_connectivity == unique[i]] = 0\n\n    return resulting_connectivity\n\n\n# pylint: disable=R0914,R0915\ndef main():\n    args = parse_arguments()\n\n    # --------------------------------- 1. Load Plugin for inference engine ---------------------------------\n    logger.info(""Loading plugin"")\n    plugin = IEPlugin(args.target_device)\n\n    config = dict()\n    if \'CPU\' in args.target_device:\n        if args.path_to_extension:\n            plugin.add_cpu_extension(args.path_to_extension)\n        if args.number_threads is not None:\n            config.update({\'CPU_THREADS_NUM\': str(args.number_threads)})\n    else:\n        raise AttributeError(""Device {} do not support of 3D convolution. Please use CPU or HETERO:*CPU*"")\n\n    if \'GPU\' in args.target_device:\n        if args.path_to_cldnn_config:\n            config.update({\'CONFIG_FILE\': args.path_to_cldnn_config})\n            logger.info(""GPU extensions is loaded %s"", args.path_to_cldnn_config)\n\n    plugin.set_config(config)\n\n    logger.info(""Device is %s "", plugin.device)\n    logger.info(""Plugin version is %s"", plugin.version)\n\n    # --------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ---------------------\n\n    xml_filename = os.path.abspath(args.path_to_model)\n    bin_filename = os.path.abspath(os.path.splitext(xml_filename)[0] + \'.bin\')\n\n    ie_network = IENetwork(xml_filename, bin_filename)\n\n    input_info = ie_network.inputs\n    if not input_info:\n        raise AttributeError(""No inputs info is provided"")\n    if len(input_info) != 1:\n        raise AttributeError(""Only one input layer network is supported"")\n\n    input_name = next(iter(input_info))\n    out_name = next(iter(ie_network.outputs))\n    print(input_name, out_name)\n\n    # ---------------------------------------- 4. Preparing input data ----------------------------------------\n    logger.info(""Preparing inputs"")\n\n    if len(input_info[input_name].shape) != 5:\n        raise AttributeError(""Incorrect shape {} for 3d convolution network"".format(args.shape))\n\n    n, _, d, h, w = input_info[input_name].shape\n    ie_network.batch_size = n\n\n    # ------------------------------------- 4. Loading model to the plugin -------------------------------------\n    # logger.info(""Reshape of network from {} to {}"".format(input_info[input_name].shape, image_crop_pad.shape))\n    #ie_network.reshape({input_name: image_crop_pad.shape})\n    #input_info = ie_network.inputs\n\n    # logger.info(""Loading model to the plugin"")\n    executable_network = plugin.load(network=ie_network)\n    del ie_network\n\n    files = os.listdir(args.path_to_input_data)\n    files = [f for f in files if (f.startswith(\'Patient\') and os.path.isfile(os.path.join(args.path_to_input_data, f)))]\n    files.sort()\n\n    for f in files:\n        header = read_nii_header(os.path.join(args.path_to_input_data, f))\n        image = np.array(header.get_data()).astype(np.float32)\n        original_shape = image.shape\n\n        start_time = datetime.now()\n\n        image = median_filter(image, 3)\n\n        bbox = lung_bbox(image)\n\n        image_crop = image[bbox[0, 0]:bbox[1, 0], bbox[0, 1]:bbox[1, 1], bbox[0, 2]:bbox[1, 2]]\n\n        new_shape_pad = (d, h, w)\n        diff = np.array(new_shape_pad) - np.array(image_crop.shape)\n        pad_left = diff // 2\n        pad_right = diff - pad_left\n\n        image_crop_pad = np.pad(image_crop, pad_width=tuple([(pad_left[i], pad_right[i]) for i in range(3)]),\n                                mode=\'reflect\')\n\n        # dataset statistics\n        mean = -303.0502877950004\n        mean2 = 289439.0029958802\n        std = np.sqrt(mean2 - mean * mean)\n\n        image_crop_pad = (image_crop_pad - mean) / std\n\n        image_crop_pad = image_crop_pad[None, None]\n\n        preprocess_time = datetime.now() - start_time\n\n        test_im = {input_name: image_crop_pad}\n\n        # ---------------------------------------------- 5. Do inference --------------------------------------------\n        start_time = datetime.now()\n        res = executable_network.infer(test_im)\n        infer_time = datetime.now() - start_time\n\n        # ---------------------------- 6. Processing of the received inference results ------------------------------\n        result = res[out_name]\n\n        start_time = datetime.now()\n\n        output_crop = result[0, :, pad_left[0]:-pad_right[0] or None, pad_left[1]:-pad_right[1] or None,\n                             pad_left[2]:-pad_right[2] or None]\n\n        new_label = np.zeros(shape=(4,) + image.shape)\n        new_label[:, bbox[0, 0]:bbox[1, 0], bbox[0, 1]:bbox[1, 1], bbox[0, 2]:bbox[1, 2]] = output_crop\n\n        scale_factor = np.array(original_shape) / np.array(image.shape)\n        old_labels = [zoom(new_label[i], scale_factor, order=1, mode=\'constant\', cval=0)[None] for i in range(4)]\n\n        old_label = np.concatenate(tuple(old_labels), axis=0)\n        old_label = ((np.argmax(old_label, axis=0) + 1) *\n                     np.max((old_label > np.array([0.5, 0.5, 0.5, 0.5]).reshape((-1, 1, 1, 1))).astype(np.int32),\n                            axis=0)).astype(np.int32)\n\n        eso_connectivity = morphology.label(old_label == 1)\n        heart_connectivity = morphology.label(old_label == 2)\n        trachea_connectivity = morphology.label(old_label == 3)\n        aorta_connectivity = morphology.label(old_label == 4)\n        eso_connectivity = reject_small_regions(eso_connectivity, ratio=0.2)\n        heart_connectivity = leave_biggest_region(heart_connectivity)\n        trachea_connectivity = leave_biggest_region(trachea_connectivity)\n        aorta_connectivity = leave_biggest_region(aorta_connectivity)\n\n        old_label[np.logical_and(old_label == 1, eso_connectivity == 0)] = 0\n        old_label[np.logical_and(old_label == 2, heart_connectivity == 0)] = 0\n        old_label[np.logical_and(old_label == 3, trachea_connectivity == 0)] = 0\n        old_label[np.logical_and(old_label == 4, aorta_connectivity == 0)] = 0\n\n        postprocess_time = datetime.now() - start_time\n\n        logger.info(""Pre-processing time is %s; Inference time is %s; Post-processing time is %s"",\n                    preprocess_time, infer_time, postprocess_time)\n\n        # --------------------------------------------- 7. Save output -----------------------------------------------\n        output_header = nii.Nifti1Image(old_label, header.affine)\n        nii.save(output_header, os.path.join(args.path_to_output, f[:-7]+\'.nii\'))\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_toolkit/super_resolution/sr/common.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport yaml\n\ndef load_config(path):\n    """""" Load saved configuration from yaml file. """"""\n\n    if os.path.isdir(path):\n        path = os.path.join(path, \'config.yaml\')\n    with open(path, ""r"") as read_file:\n        config = yaml.safe_load(read_file)\n    return config\n\n\ndef parse_epoch(path):\n    return int(os.path.basename(path).split(\'.\')[0].split(\'-\')[-1])\n'"
pytorch_toolkit/super_resolution/sr/dataset.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport os.path as osp\nfrom random import Random, choice\nimport numpy as np\nimport cv2\nimport skimage\nfrom skimage import transform\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image as pil_image\nfrom tqdm import tqdm\n\nclass DatasetFromPairedImages(data.Dataset):\n    def __init__(self, path, suffix_lr, suffix_hr, count=None):\n        super(DatasetFromPairedImages, self).__init__()\n\n        self.path = path\n        self.suffix_lr = suffix_lr\n        self.suffix_hr = suffix_hr\n\n        images_names = os.listdir(self.path)\n        self.hr_image_names = [f for f in images_names if f.endswith(self.suffix_hr)]\n        self.hr_image_names.sort()\n\n        if count is not None:\n            self.count = count\n        else:\n            self.count = len(self.hr_image_names)\n\n    def __getitem__(self, index):\n        hr_name = self.hr_image_names[index]\n        name_root = hr_name.split(self.suffix_hr)[0]\n        lr_name = osp.join(self.path, name_root + self.suffix_lr)\n        hr_name = osp.join(self.path, name_root + self.suffix_hr)\n\n        hr_image = skimage.img_as_float32(cv2.imread(hr_name))\n        lr_image = skimage.img_as_float32(cv2.imread(lr_name))\n        bic_image = cv2.resize(lr_image, hr_image.shape[:2][::-1], cv2.INTER_CUBIC)\n\n        item = [torch.from_numpy(lr_image.transpose((2, 0, 1))).float(),\n                torch.from_numpy(bic_image.transpose((2, 0, 1))).float()],\\\n               [torch.from_numpy(hr_image.transpose((2, 0, 1))).float()]\n\n        return item\n\n    def __len__(self):\n        return self.count\n\n\nclass DatasetFromSingleImages(data.Dataset):\n    # pylint: disable=too-many-arguments\n    def __init__(self, path, patch_size=None, scale=4, aug_resize_factor_range=None, count=None,\n                 cache_images=False, seed=1337, dataset_size_factor=1):\n        super(DatasetFromSingleImages, self).__init__()\n        self.path = path\n        self.cache_images = cache_images\n        self.dataset_size_factor = dataset_size_factor\n        self.count = count\n        self.resize_factor = aug_resize_factor_range\n\n        self.patch_size = patch_size\n        if self.patch_size is not None:\n            if patch_size[0] % scale or patch_size[1] % scale:\n                raise Exception(\'ERROR: patch_size should be divisible by scale\')\n\n            self.patch_size_ds = [i // scale for i in patch_size]\n\n        self.ds_factor = scale\n\n        self.cache = []\n        self.image_names = []\n\n        self._load_images()\n\n        self.random = Random()\n        self.random.seed(seed)\n\n    def _load_images(self):\n        all_files = os.listdir(self.path)\n        files = [f for f in all_files if f.endswith((\'.bmp\', \'.png\', \'.jpg\'))]\n        files.sort()\n        cache_count = 0\n\n        max_count = len(files)\n        if self.count is not None:\n            max_count = self.count\n\n        for f in tqdm(files):\n            image_size = np.array(pil_image.open(osp.join(self.path, f)).size)\n\n            if (self.patch_size is not None and \\\n                 np.any([image_size[i] * self.resize_factor[0] < self.patch_size[i] for i in range(2)])) or \\\n               (self.patch_size is None and np.any([image_size[i] % self.ds_factor for i in range(2)])):\n                continue\n\n            if self.cache_images:\n                image = cv2.imread(osp.join(self.path, f))\n                self.cache.append(image)\n            self.image_names.append(f)\n            cache_count += 1\n\n            if cache_count >= max_count:\n                break\n\n        self.count = len(self.image_names)\n        num_skipped = len(files) - self.count\n        if num_skipped:\n            print(""[WARNING] Skipped {} images"".format(num_skipped))\n\n        assert self.count != 0\n\n    def __getitem__(self, index):\n        index = index % self.count\n        if self.cache_images:\n            image = skimage.img_as_float32(self.cache[index])\n        else:\n            image = skimage.img_as_float32(cv2.imread(osp.join(self.path, self.image_names[index])))\n\n        if self.patch_size is not None:\n\n            h, w, _ = image.shape\n\n            resize_rate = self.random.random() * (self.resize_factor[1] - self.resize_factor[0]) + self.resize_factor[0]\n\n            if w == self.patch_size[1]:\n                x = 0\n            else:\n                x = self.random.randint(0, int(w - self.patch_size[1]*resize_rate))\n\n            if h == self.patch_size_ds[0]:\n                y = 0\n            else:\n                y = self.random.randint(0, int(h - self.patch_size[0]*resize_rate))\n\n            sample = transform.resize(image[y:y + self.patch_size[0], x:x + self.patch_size[1], :],\n                                      output_shape=self.patch_size, order=3, mode=\'reflect\', anti_aliasing=True,\n                                      anti_aliasing_sigma=None, preserve_range=True)\n\n            sample_ds = transform.resize(image=sample, output_shape=self.patch_size_ds, order=3, mode=\'reflect\',\n                                         anti_aliasing=True, anti_aliasing_sigma=None, preserve_range=True)\n            cubic = cv2.resize(sample_ds, sample.shape[:2][::-1], interpolation=cv2.INTER_CUBIC)\n        else:\n\n            h, w, _ = image.shape\n            if h % self.ds_factor or w % self.ds_factor:\n                raise Exception(\'ERROR: image size should be divisible by scale\')\n\n            sample = image\n            sample_ds = transform.resize(image=sample,\n                                         output_shape=[i // self.ds_factor for i in sample.shape[:2]],\n                                         order=3,\n                                         mode=\'reflect\',\n                                         anti_aliasing=True,\n                                         anti_aliasing_sigma=None, preserve_range=True)\n\n            cubic = cv2.resize(sample_ds, sample.shape[:2][::-1], interpolation=cv2.INTER_CUBIC)\n\n        item = [torch.from_numpy(sample_ds.transpose((2, 0, 1))).float(),\n                torch.from_numpy(cubic.transpose((2, 0, 1))).float()], \\\n               [torch.from_numpy(sample.transpose((2, 0, 1))).float()]\n\n        return item\n\n    def __len__(self):\n        return self.count*self.dataset_size_factor\n\n\nclass DatasetTextImages(data.Dataset):\n    # pylint: disable=too-many-arguments\n    def __init__(self, path, patch_size=None, scale=4, aug_resize_factor_range=None,\n                 seed=1337, dataset_size_factor=1, rotate=False):\n        super(DatasetTextImages, self).__init__()\n        self.path = path\n        self.dataset_size_factor = dataset_size_factor\n        self.resize_factor = aug_resize_factor_range\n        self.patch_size = patch_size\n        if self.patch_size is not None:\n            if patch_size[0] % scale or patch_size[1] % scale:\n                raise Exception(\'ERROR: patch_size should be divisible by scale\')\n\n            self.patch_size_ds = [i // scale for i in patch_size]\n\n        self.ds_factor = scale\n\n        self.image_names = []\n\n        self._load_images()\n\n        self.random = Random()\n        self.random.seed(seed)\n\n        self.rotate = rotate\n        if self.rotate:\n            if self.patch_size is None or self.patch_size[0] != self.patch_size[1]:\n                raise Exception(\'ERROR: Disable rotation or set square patch\')\n\n    def _load_images(self):\n        all_files = os.listdir(self.path)\n        files = [f for f in all_files if f.endswith((\'.bmp\', \'.png\', \'.jpg\'))]\n        files.sort()\n        files = files[:100]\n        for f in tqdm(files):\n            pimage = pil_image.open(osp.join(self.path, f))\n            image_size = np.array(pimage.size)\n\n            if pimage.mode != \'L\':\n                # Image should be in gray scale format\n                continue\n            if self.patch_size is not None and \\\n                 np.any([image_size[i] * self.resize_factor[0] < self.patch_size[i] for i in range(2)]):\n                continue\n            if self.patch_size is None and np.any([image_size[i] % self.ds_factor for i in range(2)]):\n                print(image_size, [image_size[i] % self.ds_factor for i in range(2)])\n                continue\n\n            self.image_names.append(f)\n\n\n        self.count = len(self.image_names)\n        num_skipped = len(files) - self.count\n        if num_skipped:\n            print(""[WARNING] Skipped {} images"".format(num_skipped))\n\n        assert self.count != 0\n\n    def __getitem__(self, index):\n        index = index % self.count\n        # Read image in original format\n        image = skimage.img_as_float32(cv2.imread(osp.join(self.path, self.image_names[index]), 0))\n        image = image.reshape(image.shape[0], image.shape[1], 1)\n        if self.patch_size is not None:\n\n            h, w = image.shape[:2]\n\n            resize_rate = self.random.random() * (self.resize_factor[1] - self.resize_factor[0]) + self.resize_factor[0]\n\n            if self.rotate:\n                rotate = choice([None, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE])\n                if rotate:\n                    image = cv2.rotate(image, rotate)\n                    if len(image.shape) == 2:\n                        image = image.reshape(image.shape[0], image.shape[1], 1)\n\n            if w == self.patch_size[1]:\n                x = 0\n            else:\n                x = self.random.randint(0, int(w - self.patch_size[1]*resize_rate))\n\n            if h == self.patch_size_ds[0]:\n                y = 0\n            else:\n                y = self.random.randint(0, int(h - self.patch_size[0]*resize_rate))\n\n            sample = transform.resize(image[y:y + self.patch_size[0], x:x + self.patch_size[1], :],\n                                      output_shape=self.patch_size, order=3, mode=\'reflect\', anti_aliasing=True,\n                                      anti_aliasing_sigma=None, preserve_range=True)\n\n            sample_ds = transform.resize(image=sample, output_shape=self.patch_size_ds, order=3, mode=\'reflect\',\n                                         anti_aliasing=True, anti_aliasing_sigma=None, preserve_range=True)\n        else:\n\n            h, w, _ = image.shape\n            if h % self.ds_factor or w % self.ds_factor:\n                raise Exception(\'ERROR: image size should be divisible by scale\')\n\n            sample = image\n            sample_ds = transform.resize(image=sample,\n                                         output_shape=[i // self.ds_factor for i in sample.shape[:2]],\n                                         order=3,\n                                         mode=\'reflect\',\n                                         anti_aliasing=True,\n                                         anti_aliasing_sigma=None, preserve_range=True)\n\n        cubic = cv2.resize(sample_ds, sample.shape[:2][::-1], interpolation=cv2.INTER_CUBIC)\n        cubic = cubic.reshape(cubic.shape[0], cubic.shape[1], 1)\n\n        item = [torch.from_numpy(sample_ds.transpose((2, 0, 1))).float(),\n                torch.from_numpy(cubic.transpose((2, 0, 1))).float()], \\\n               [torch.from_numpy(sample.transpose((2, 0, 1))).float()]\n\n        return item\n\n    def __len__(self):\n        return self.count*self.dataset_size_factor\n'"
pytorch_toolkit/super_resolution/sr/metrics.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport math\nimport numpy as np\n\n\nclass Metrics():\n    def __init__(self, name):\n        self.name = name\n        self.accumulator = 0.0\n        self.samples = 0.0\n\n    # pylint: disable=unused-argument\n    def update(self, ground, predict):\n        self.samples = self.samples + 1\n\n    def get(self):\n        return self.accumulator / self.samples\n\n    def reset(self):\n        self.accumulator = 0.0\n        self.samples = 0.0\n\n\ndef _PSNR(pred, gt, shave_border=0):\n    pred = np.asarray(pred).astype(np.float)\n    gt = np.asarray(gt).astype(np.float)\n\n    height, width = pred.shape[:2]\n    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\n    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\n    imdff = (pred - gt)\n\n    if len(imdff.shape) > 2 and imdff.shape[2] == 3:\n        # RGB image\n        r = imdff[:, :, 2]\n        g = imdff[:, :, 1]\n        b = imdff[:, :, 0]\n\n        y = (r * 65.738 + g * 129.057 + b * 25.064) / 256\n    else:\n        # Gray scale\n        y = imdff\n\n    mse = np.mean(y ** 2)\n    if mse == 0:\n        return np.Infinity\n\n    return - 10 * math.log10(mse)\n\n\nclass PSNR(Metrics):\n    def __init__(self, name=\'PSNR\', border=0, input_index=0, target_index=0):\n        super(PSNR, self).__init__(name)\n        self.border = border\n        self.input_index = input_index\n        self.target_index = target_index\n\n    def update(self, ground, predict):\n        pred = predict[self.input_index].cpu().detach().numpy()\n        gr = ground[self.target_index].cpu().detach().numpy()\n\n        assert gr.shape == pred.shape, ""{} != {}"".format(gr.shape, pred.shape)\n\n        for i in range(gr.shape[0]):\n            _pred = pred[i].T\n            _gr = gr[i].T\n            psnr = _PSNR(_pred, _gr, self.border)\n            if not np.isinf(psnr):\n                self.accumulator += psnr\n                self.samples += 1\n\n\nclass RMSE(Metrics):\n    def __init__(self, name=\'RMSE\', border=4, input_index=0, target_index=0):\n        super(RMSE, self).__init__(name)\n        self.input_index = input_index\n        self.target_index = target_index\n        self.border = border\n\n    def update(self, ground, predict):\n        h, w = ground[0].shape[2:]\n        b = self.border\n        pred = predict[self.input_index].cpu().detach().numpy()[:, :, b:h-b, b:w-b]\n        gr = ground[self.target_index].cpu().detach().numpy()[:, :, b:h-b, b:w-b]\n\n        assert gr.shape == pred.shape\n\n        self.accumulator += ((((pred-gr)**2).sum(axis=(1, 2, 3))/np.prod(gr.shape[1:]))**0.5).mean()\n        self.samples += 1\n'"
pytorch_toolkit/super_resolution/sr/models.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport math\nimport torch\nimport torch.nn as nn\n\n\ndef make_model(name_of_the_model, scale):\n    if name_of_the_model == \'SRResNetLight\':\n        model = SRResNetLight(scale=scale).cuda()\n    elif name_of_the_model == \'SmallModel\':\n        model = SmallModel(scale=scale).cuda()\n    elif name_of_the_model == \'TextTransposeModel\':\n        model = TextTransposeModel(scale=scale).cuda()\n    else:\n        print(\'ERROR:\', name_of_the_model, \' model is not supported.\')\n        raise NotImplementedError\n\n    return model\n\ndef make_layer(block, num_of_layer):\n    layers = []\n    for _ in range(num_of_layer):\n        layers.append(block)\n    return nn.Sequential(*layers)\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, num_of_channels):\n        super(ResBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=num_of_channels, out_channels=num_of_channels, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.in1 = nn.InstanceNorm2d(num_of_channels, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=num_of_channels, out_channels=num_of_channels, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.in2 = nn.InstanceNorm2d(num_of_channels, affine=True)\n\n    # pylint: disable=arguments-differ\n    def forward(self, x):\n        orig = x\n        output = self.relu(self.in1(self.conv1(x)))\n        output = self.in2(self.conv2(output))\n        output = torch.add(output, orig)\n        return output\n\n\nclass SRResNetLight(nn.Module):\n    def __init__(self, scale=4, num_of_res_blocks=16, num_of_channels=32):\n        super(SRResNetLight, self).__init__()\n\n        self.scale = scale\n\n        self.conv_input = nn.Conv2d(in_channels=3, out_channels=num_of_channels, kernel_size=9, stride=1, padding=4,\n                                    bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.residual = make_layer(ResBlock(num_of_channels), num_of_res_blocks)\n\n        self.conv_mid = nn.Conv2d(in_channels=num_of_channels, out_channels=num_of_channels, kernel_size=3, stride=1,\n                                  padding=1, bias=False)\n        self.bn_mid = nn.InstanceNorm2d(num_of_channels, affine=True)\n\n        if scale == 2:\n            factor = 2\n            self.upscale = nn.Sequential(\n                nn.Conv2d(in_channels=num_of_channels, out_channels=num_of_channels*factor*factor,\n                          kernel_size=3, stride=1, padding=1, bias=False),\n                nn.PixelShuffle(factor),\n                nn.ReLU(inplace=True),\n            )\n        elif scale == 4:\n            factor = 2\n            self.upscale = nn.Sequential(\n                nn.Conv2d(in_channels=num_of_channels, out_channels=num_of_channels*factor*factor,\n                          kernel_size=3, stride=1, padding=1, bias=False),\n                nn.PixelShuffle(factor),\n                nn.ReLU(inplace=True),\n\n                nn.Conv2d(num_of_channels, num_of_channels, groups=num_of_channels, kernel_size=3, padding=1, stride=1,\n                          bias=False),\n\n                nn.Conv2d(num_of_channels, num_of_channels*factor*factor, kernel_size=1, bias=True),\n\n                nn.PixelShuffle(factor),\n                nn.ReLU(inplace=True),\n            )\n        elif scale == 3:\n            factor = 3\n            self.upscale = nn.Sequential(\n                nn.Conv2d(in_channels=num_of_channels, out_channels=num_of_channels*factor*factor,\n                          kernel_size=3, stride=1, padding=1, bias=True),\n                nn.PixelShuffle(factor),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            raise NotImplementedError\n\n        self.conv_output = nn.Conv2d(in_channels=num_of_channels, out_channels=3, kernel_size=9, stride=1, padding=4,\n                                     bias=False)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    # pylint: disable=arguments-differ\n    def forward(self, x):\n        input = x\n        if isinstance(x, (list, tuple)):\n            input = x[0]\n\n        out = self.relu(self.conv_input(input))\n        residual = out\n        out = self.residual(out)\n        out = self.bn_mid(self.conv_mid(out))\n        out = torch.add(out, residual)\n        out = self.upscale(out)\n        out = self.conv_output(out)\n        return [out]\n\n\nclass SmallBlock(nn.Module):\n    def __init__(self, channels):\n        super(SmallBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        self.relu = nn.ReLU(inplace=False)\n        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n\n    # pylint: disable=arguments-differ\n    def forward(self, x):\n        identity_data = x\n        output = self.relu(x)\n        output = self.conv1(output)\n        output = self.relu(output)\n        output = self.conv2(output)\n\n        output = torch.add(output, identity_data)\n        return output\n\n\nclass SmallModel(nn.Module):\n    def __init__(self, scale=3, num_of_ch_enc=16, num_of_ch_dec=8, num_of_res_blocks=4):\n        super(SmallModel, self).__init__()\n\n        self.conv_input = nn.Conv2d(in_channels=3, out_channels=num_of_ch_enc,\n                                    kernel_size=3, stride=1, padding=1, bias=True)\n        self.relu = nn.ReLU(inplace=False)\n        self.sigmoid = nn.Sigmoid()\n\n        self.conv_cubic1 = nn.Conv2d(in_channels=3, out_channels=num_of_ch_dec,\n                                     kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv_cubic2 = nn.Conv2d(in_channels=num_of_ch_dec, out_channels=1,\n                                     kernel_size=3, stride=1, padding=1, bias=True)\n\n        self.residual1 = SmallBlock(num_of_ch_enc)\n        self.residual2 = SmallBlock(num_of_ch_enc)\n        self.residual3 = SmallBlock(num_of_ch_enc)\n        self.residual4 = SmallBlock(num_of_ch_enc)\n\n        self.conv_mid = nn.Conv2d(in_channels=num_of_ch_enc * (num_of_res_blocks + 1), out_channels=num_of_ch_dec,\n                                  kernel_size=3, stride=1, padding=1, bias=True)\n\n        if scale == 4:\n            factor = 2\n            self.upscale = nn.Sequential(\n                nn.Conv2d(in_channels=num_of_ch_dec, out_channels=num_of_ch_dec * factor * factor,\n                          kernel_size=3, stride=1, padding=1, bias=True),\n                nn.PixelShuffle(factor),\n                nn.ReLU(inplace=True),\n\n                nn.Conv2d(num_of_ch_dec, num_of_ch_dec * factor * factor,\n                          kernel_size=3, padding=1, stride=1, bias=True),\n                nn.PixelShuffle(factor),\n                nn.ReLU(inplace=True)\n            )\n        elif scale == 3:\n            self.upscale = nn.Sequential(\n                nn.Conv2d(in_channels=num_of_ch_dec, out_channels=num_of_ch_dec * scale * scale,\n                          kernel_size=3, stride=1, padding=1, bias=True),\n                nn.PixelShuffle(scale),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            raise NotImplementedError\n\n        self.conv_output = nn.Conv2d(in_channels=num_of_ch_dec, out_channels=3,\n                                     kernel_size=3, stride=1, padding=1, bias=True)\n\n    # pylint: disable=arguments-differ\n    def forward(self, x):\n        input = x[0]\n        cubic = x[1]\n\n        c1 = self.conv_cubic1(cubic)\n        c1 = self.relu(c1)\n        c2 = self.conv_cubic2(c1)\n        c2 = self.sigmoid(c2)\n\n        in1 = self.conv_input(input)\n\n        out = self.relu(in1)\n\n        out1 = self.residual1(out)\n        out2 = self.residual2(out1)\n        out3 = self.residual3(out2)\n        out4 = self.residual4(out3)\n\n        out = torch.cat([out, out1, out2, out3, out4], dim=1)\n        out = self.conv_mid(out)\n        out = self.relu(out)\n        out = self.upscale(out)\n        out = self.conv_output(out)\n\n        return [torch.add(out*c2, cubic)]\n\n\nclass TextTransposeModel(nn.Module):\n    def __init__(self, scale=3, num_of_ch_enc=4, num_of_ch_dec=4, img_channels=1):\n        super(TextTransposeModel, self).__init__()\n        self.scale = scale\n        self.img_channels = img_channels\n\n        self.conv_input = nn.Conv2d(in_channels=self.img_channels, out_channels=num_of_ch_enc,\n                                    kernel_size=3, stride=1, padding=1, bias=True)\n        self.relu = nn.ReLU(inplace=False)\n\n\n        if scale == 3:\n            self.upscale = nn.Sequential(\n                nn.Conv2d(in_channels=num_of_ch_enc, out_channels=num_of_ch_dec * scale * scale,\n                          kernel_size=3, stride=1, padding=1, bias=True),\n                nn.ConvTranspose2d(in_channels=num_of_ch_dec * scale * scale, out_channels=num_of_ch_dec,\n                                   kernel_size=3, stride=scale, padding=0, output_padding=0),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            raise NotImplementedError\n\n        self.conv_output = nn.Conv2d(in_channels=num_of_ch_dec, out_channels=self.img_channels,\n                                     kernel_size=3, stride=1, padding=1, bias=True)\n\n    # pylint: disable=arguments-differ\n    def forward(self, x):\n        input = x[0]\n\n        out = self.conv_input(input)\n        out = self.relu(out)\n        out = self.upscale(out)\n        out = self.conv_output(out)\n\n        return [out]\n\n\nclass MSE_loss(nn.Module):\n    def __init__(self, border=4):\n        super(MSE_loss, self).__init__()\n        self.border = border\n\n    # pylint: disable=arguments-differ\n    def forward(self, x, y):\n        assert x[0].shape == y[0].shape\n\n        h, w = x[0].shape[2:]\n\n        x = x[0][:, :, self.border:h - self.border, self.border:w - self.border]\n        y = y[0][:, :, self.border:h - self.border, self.border:w - self.border]\n\n        diff = x - y\n        diff = diff ** 2\n\n        return torch.mean(diff)\n'"
pytorch_toolkit/super_resolution/sr/trainer.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport shutil\nimport time\nimport torch\nfrom torch.autograd import Variable\nfrom tensorboardX import SummaryWriter\n\n\nclass TrainingState():\n    def __init__(self):\n        self.epoch = 0\n        self.train_metric = dict()\n        self.val_metric = dict()\n        # number of processed batches\n        self.global_step = 0\n        self.best_val = 0\n        self.optimizer_state = None\n        self.cuda = True\n\n\nclass Trainer():\n    def __init__(self, name, models_root, model=None, resume=False):\n\n        self.model = model\n\n        assert (isinstance(self.model, (list, tuple, torch.nn.Module)) or self.model is None)\n\n        self.name = name\n        self.models_root = models_root\n        self.model_path = os.path.join(models_root, self.name)\n        self.logs_path = os.path.join(self.model_path, \'logs\')\n\n        self.state = TrainingState()\n        self.resume_training = False\n\n        if os.path.exists(self.model_path):\n            if resume:\n                self.resume_training = True\n            else:\n                shutil.rmtree(self.model_path)\n\n        if not os.path.exists(self.model_path):\n            os.makedirs(self.model_path)\n            os.makedirs(self.logs_path)\n\n        self.tb_writer = SummaryWriter(logdir=self.logs_path)\n\n    # pylint: disable=too-many-arguments\n    def train(self, criterion, optimizer, optimizer_params, scheduler, scheduler_params, training_data_loader,\n              evaluation_data_loader, pretrained_weights, train_metrics, val_metrics,\n              track_metric, epoches, default_val=0, comparator=lambda x, y: x > y):\n\n        # pylint: disable=protected-access\n        assert isinstance(criterion, (tuple, list, torch.nn.modules.loss._Loss))\n\n        # load weights if any\n        if self.resume_training:\n            # load training and continue\n            self.load_latest()\n        elif pretrained_weights is not None:\n            # load dictionary only\n            self.model.load_state_dict(pretrained_weights)\n        else:\n            self.state.best_val = default_val\n\n        if isinstance(optimizer, type):\n            optimizer = optimizer(params=self.model.parameters(), **optimizer_params)\n\n        if scheduler is not None:\n            if isinstance(scheduler, type):\n                scheduler = scheduler(optimizer=optimizer, **scheduler_params)\n\n        assert isinstance(optimizer, torch.optim.Optimizer)\n        assert isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler) or scheduler is None\n\n        if self.state.optimizer_state is not None:\n            optimizer.load_state_dict(self.state.optimizer_state)\n\n        # prepare dicts for metrics\n        if not self.state.train_metric:\n            for m in train_metrics:\n                self.state.train_metric[m.name] = []\n\n            for m in val_metrics:\n                self.state.val_metric[m.name] = []\n\n        # training loop\n        start_epoch = self.state.epoch\n        for i in range(start_epoch, epoches):\n            tic = time.time()\n\n            if scheduler is not None:\n                scheduler.step()\n\n            self.state.global_step = self._train_one_epoch(criterion, optimizer, training_data_loader, train_metrics,\n                                                           self.state.train_metric, i, self.state.global_step)\n\n            self._evaluate_and_save(evaluation_data_loader, val_metrics, track_metric, self.state.val_metric, i,\n                                    comparator)\n\n            tac = time.time()\n            print(\'Epoch %d, time %s \\n\' % (i, tac - tic))\n\n            self._save(suffix=\'epoch_\' + str(self.state.epoch))\n            self._save(suffix=\'last_model\')\n            self.state.epoch = self.state.epoch + 1\n\n    def predict(self, batch):\n        self.model.eval()\n        assert isinstance(batch[0], list)\n        data = [Variable(b) for b in batch[0]]\n\n        if self.state.cuda:\n            data = [d.cuda() for d in data]\n\n        output = self.model(data)\n        return output\n\n    def _train_one_epoch(self, criterion, optimizer, training_data_loader, train_metrics, train_metrics_results, epoch,\n                         global_step):\n\n        for m in train_metrics:\n            m.reset()\n\n        self.model.train()\n\n        for batch in training_data_loader:\n            assert (isinstance(batch[0], list) and isinstance(batch[1], list))\n            data = [Variable(b) for b in batch[0]]\n            target = [Variable(b, requires_grad=False) for b in batch[1]]\n\n            if self.state.cuda:\n                data = [d.cuda() for d in data]\n                target = [t.cuda() for t in target]\n\n            output = self.model(data)\n\n            if isinstance(criterion, (tuple, list)):\n                loss_val = [c(output, target) for c in criterion]\n                loss = sum(loss_val)\n            else:\n                loss = loss_val = criterion(output, target)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            torch.cuda.synchronize()\n\n            for m in train_metrics:\n                m.update(output, target)\n\n            for idx, l in enumerate(loss_val):\n                self.tb_writer.add_scalar(\'loss/loss-{}\'.format(idx), l.item(), global_step)\n            global_step = global_step + 1\n\n        for m in train_metrics:\n            train_metrics_results[m.name].append(m.get())\n            self.tb_writer.add_scalar(\'train/\' + m.name, m.get(), epoch)\n            print(\'Epoch %d, Training %s %s\' % (epoch, m.name, m.get()))\n\n        self.state.optimizer_state = optimizer.state_dict()\n        return global_step\n\n    def _evaluate_and_save(self, evaluation_data_loader, val_metrics, track_metric, val_metrics_results, epoch,\n                           comparator):\n\n        for m in val_metrics:\n            m.reset()\n\n        self.model.eval()\n\n        for batch in evaluation_data_loader:\n            assert isinstance(batch[0], list) and isinstance(batch[1], list)\n            data = [Variable(b) for b in batch[0]]\n            target = [Variable(b, requires_grad=False) for b in batch[1]]\n\n            if self.state.cuda:\n                data = [d.cuda() for d in data]\n                target = [t.cuda() for t in target]\n\n            output = self.model(data)\n\n            for m in val_metrics:\n                m.update(target, output)\n\n        val = 0.0\n        for m in val_metrics:\n            if m.name == track_metric:\n                val = m.get()\n\n            self.tb_writer.add_scalar(\'val/\' + m.name, m.get(), epoch)\n            val_metrics_results[m.name].append(m.get())\n            print(\'Epoch %d, Validation %s %s\' % (epoch, m.name, m.get()))\n\n        if comparator(val, self.state.best_val):\n            self.state.best_val = val\n            self._save(suffix=\'best_model\')\n            print(\'model saved\')\n\n    def _save(self, suffix):\n        s = {\'state\': self.state,\n             \'model\': self.model}\n\n        torch.save(s, os.path.join(self.model_path, self.name + \'_\' + suffix + \'.pth\'))\n\n    def _load(self, suffix):\n        print(\'loading model...\')\n        model_path = os.path.join(self.model_path, self.name + \'_\' + suffix + \'.pth\')\n        s = torch.load(model_path)\n        self.state = s[\'state\']\n        if self.model is None:\n            self.model = s[\'model\']\n        else:\n            self.model.load_state_dict(s[\'model\'].state_dict())\n\n    def load_latest(self):\n        self._load(\'last_model\')\n\n    def load_best(self):\n        self._load(\'best_model\')\n'"
pytorch_toolkit/super_resolution/tools/export.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport argparse\nimport os\nimport subprocess\nimport torch.onnx\nfrom sr.trainer import Trainer\nfrom sr.common import load_config\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'PyTorch SR export to onnx and IR\')\n    parser.add_argument(\'--exp_name\', default=\'test\', type=str, help=\'experiment name\')\n    parser.add_argument(\'--models_path\', default=\'./models\', type=str, help=\'path to models folder\')\n    parser.add_argument(\'--input_size\', type=int, nargs=\'+\', default=(200, 200), help=\'Input image size\')\n    parser.add_argument(\'--data_type\', default=\'FP32\', choices=[\'FP32\', \'FP16\'], help=\'Data type of IR\')\n    parser.add_argument(\'--output_dir\', default=None, help=\'Output Directory\')\n    return parser.parse_args()\n\n\ndef execute_mo(input_model, output_dir, name, data_type, scale_values):\n    command = [\n        \'mo.py\',\n        \'--input_model={}\'.format(input_model),\n        \'--output_dir={}\'.format(output_dir),\n        \'--model_name={}\'.format(name),\n        \'--data_type={}\'.format(data_type),\n        \'--scale_values={}\'.format(scale_values)\n    ]\n    subprocess.call(command)\n\n\ndef main():\n    args = parse_args()\n\n    models_path = args.models_path\n    exp_name = args.exp_name\n    model_dir = os.path.join(models_path, exp_name)\n\n    config = load_config(model_dir)\n\n    input_size = args.input_size\n    scale = config[\'scale\']\n\n    if config[\'model\'] == \'TextTransposeModel\':\n        x = torch.randn(1, 1, input_size[0], input_size[1], requires_grad=True).cuda()\n        input_blob = [x]\n        model_name = f""text_super_resoluton_scale_{scale}""\n        scale_values = ""0[255]""\n    else:\n        x = torch.randn(1, 3, input_size[0], input_size[1], requires_grad=True).cuda()\n        cubic = torch.randn(1, 3, scale*input_size[0], scale*input_size[1], requires_grad=True).cuda()\n        input_blob = [x, cubic]\n        model_name = f""super_resoluton_scale_{scale}""\n        scale_values = ""0[255],1[255]""\n\n    trainer = Trainer(name=exp_name, models_root=models_path, resume=True)\n    trainer.load_latest()\n\n    trainer.model = trainer.model.train(False)\n\n    export_dir = args.output_dir if args.output_dir else os.path.join(model_dir, \'export\')\n\n    model_onnx_path = os.path.join(export_dir, model_name+\'.onnx\')\n\n    if not os.path.exists(export_dir):\n        os.makedirs(export_dir)\n\n    torch.onnx.export(trainer.model,      # model being run\n                      input_blob,         # model input (or a tuple for multiple inputs)\n                      model_onnx_path,    # where to save the model\n                      export_params=True,\n                      verbose=True)       # store the trained parameter weights inside the model file\n\n    ir_export_dir = os.path.join(export_dir, \'IR\', args.data_type)\n    execute_mo(model_onnx_path, ir_export_dir, model_name, args.data_type, scale_values)\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_toolkit/super_resolution/tools/infer.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport os\nimport warnings\nimport cv2\nimport skimage\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\ndef parse_args():\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', help=\'Path to checkpoint\', required=True, type=str)\n    parser.add_argument(\'--scale\', type=int, default=4, help=\'Upsampling factor for SR\')\n    parser.add_argument(\'--output_dir\', default=None, help=\'Output debugirectory\')\n    parser.add_argument(\'input_image\', help=\'Image with license plate\')\n    return parser.parse_args()\n\n\ndef image_to_blob(image):\n    blob = image.copy()\n    blob = blob.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n    blob = skimage.img_as_float32(blob)\n    blob = np.array([blob])\n    return torch.from_numpy(blob).float().cuda()\n\n\ndef blob_to_img(blob):\n    blob = blob.cpu().detach().numpy()\n    blob = np.clip(blob, 0.0, 1.0)\n    blob = blob.transpose((1, 2, 0))  # Change data layout from CHW to HWC\n\n    # Suppression skimage warning:\n    #    UserWarning: Possible precision loss when converting from float32 to uint8\n    with warnings.catch_warnings():\n        warnings.simplefilter(\'ignore\')\n        blob = skimage.img_as_ubyte(blob)\n    return blob\n\n\ndef main():\n    args = parse_args()\n\n    # Load model\n    model = torch.load(args.model)[\'model\']\n    model.eval()\n\n    # Prepare input blobs\n    image = cv2.imread(args.input_image)\n\n    ih, iw = image.shape[:2]\n    cubic = cv2.resize(image, (iw*args.scale, ih*args.scale), interpolation=cv2.INTER_CUBIC)\n\n    blob1 = image_to_blob(image)\n    blob2 = image_to_blob(cubic)\n\n    # Inference\n    result = model([Variable(blob1), Variable(blob2)])\n\n    # Postprocessing\n    out_img = blob_to_img(result[0][0])\n\n    outpur_dir = args.output_dir if args.output_dir else os.path.dirname(args.input_image)\n    out_path = os.path.join(outpur_dir, \'sr_\' + os.path.basename(args.input_image))\n    cv2.imwrite(out_path, out_img)\n    print(\'Saved: \', out_path)\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/super_resolution/tools/infer_ie.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport os\nimport warnings\nimport cv2\nimport skimage\nimport numpy as np\nfrom openvino.inference_engine import IENetwork, IEPlugin\n\n\ndef build_argparser():\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', type=str, required=True, help=\'Path to xml file with a trained model\')\n    parser.add_argument(\'--device\', help=\'Specify the target device to infer on. (default: %(default)s)\',\n                        choices=[\'CPU\', \'GPU\', \'MYRIAD\'], default=\'CPU\')\n    parser.add_argument(\'--output_dir\', default=None, help=\'Output debugirectory\')\n    parser.add_argument(\'input_image\', help=\'Image\')\n    return parser.parse_args()\n\n\ndef load_ir_model(model_xml, device):\n    model_bin = os.path.splitext(model_xml)[0] + \'.bin\'\n\n    # initialize plugin and read IR\n    plugin = IEPlugin(device=device)\n    net = IENetwork(model=model_xml, weights=model_bin)\n    exec_net = plugin.load(network=net)\n\n    input_blobs = net.inputs.keys()\n    inputs = [(b, net.inputs[b].shape) for b in input_blobs]\n\n    out_blob = next(iter(net.outputs))\n    del net\n\n    return exec_net, inputs, out_blob\n\n\ndef image_to_blob(image, shape):\n    blob = image.copy()\n    blob = blob.transpose((2, 0, 1))  # from HWC to CHW\n    blob = blob.reshape(shape)\n    return blob\n\n\ndef blob_to_img(blob):\n    blob = blob.transpose((1, 2, 0))   # from CHW to HWC\n    blob = np.clip(blob, 0.0, 1.0)\n\n    # Suppression skimage warning:\n    #    UserWarning: Possible precision loss when converting from float32 to uint8\n    with warnings.catch_warnings():\n        warnings.simplefilter(\'ignore\')\n        blob = skimage.img_as_ubyte(blob)\n    return blob\n\ndef main():\n    args = build_argparser()\n    exec_net, inputs, out_blob = load_ir_model(args.model, args.device)\n\n    # Prepare input blobs\n    ih, iw = inputs[0][1][2:]\n    image = cv2.imread(args.input_image)\n    if image.shape[0] != ih or image.shape[1] != iw:\n        image = image[0:ih, 0:iw]\n\n    cubic = cv2.resize(image, (inputs[1][1][3], inputs[1][1][2]), interpolation=cv2.INTER_CUBIC)\n\n    blob1 = image_to_blob(image, (inputs[0][1]))\n    blob2 = image_to_blob(cubic, (inputs[1][1]))\n\n    # inference\n    result = exec_net.infer(inputs={inputs[0][0]: blob1, inputs[1][0]: blob2})\n\n    # Postprocessing\n    out_img = blob_to_img(result[out_blob][0])\n\n    # Save image\n    outpur_dir = args.output_dir if args.output_dir else os.path.dirname(args.input_image)\n    out_path = os.path.join(outpur_dir, \'sr_\' + os.path.basename(args.input_image))\n    cv2.imwrite(out_path, out_img)\n    print(\'Saved: \', out_path)\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/super_resolution/tools/test.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport argparse\nimport time\nimport torch.utils.data as Data\nfrom tqdm import tqdm\nfrom sr.metrics import PSNR\nfrom sr.dataset import DatasetFromSingleImages, DatasetTextImages\nfrom sr.trainer import Trainer\nfrom sr.common import load_config\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'PyTorch SR test\')\n    parser.add_argument(\'--test_data_path\', default=\'\', type=str, help=\'path to test data\')\n    parser.add_argument(\'--exp_name\', default=\'test\', type=str, help=\'experiment name\')\n    parser.add_argument(\'--models_path\', default=\'./models\', type=str, help=\'path to models folder\')\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    config = load_config(os.path.join(args.models_path, args.exp_name))\n\n    if config[\'model\'] == \'TextTransposeModel\':\n        test_set = DatasetTextImages(path=args.test_data_path, patch_size=None,\n                                     aug_resize_factor_range=None, scale=config[\'scale\'])\n    else:\n        test_set = DatasetFromSingleImages(path=args.test_data_path, patch_size=None,\n                                           aug_resize_factor_range=None, scale=config[\'scale\'])\n\n    batch_sampler = Data.BatchSampler(\n        sampler=Data.SequentialSampler(test_set),\n        batch_size=1,\n        drop_last=True\n    )\n\n    evaluation_data_loader = Data.DataLoader(dataset=test_set, num_workers=0, batch_sampler=batch_sampler)\n\n    trainer = Trainer(name=args.exp_name, models_root=args.models_path, resume=True)\n    trainer.load_best()\n\n    psnr = PSNR(name=\'PSNR\', border=config[\'border\'])\n\n    tic = time.time()\n    count = 0\n    for batch in tqdm(evaluation_data_loader):\n        output = trainer.predict(batch=batch)\n        psnr.update(batch[1], output)\n        count += 1\n\n    toc = time.time()\n\n    print(\'FPS: {}, SAMPLES: {}\'.format(float(count) / (toc - tic), count))\n    print(\'PSNR: {}\'.format(psnr.get()))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/super_resolution/tools/train.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport argparse\nimport random\nimport shutil\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data as Data\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\n\nfrom sr.dataset import DatasetFromSingleImages, DatasetTextImages\nfrom sr.trainer import Trainer\nfrom sr.metrics import PSNR, RMSE\nfrom sr.models import make_model, MSE_loss\nfrom sr.common import load_config\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Super Resolution PyTorch\')\n    parser.add_argument(\'--config\', help=\'Path to config file\')\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    config = load_config(args.config)\n    print(\'Config:\', config)\n\n    print(\'===> Building model\')\n    scale = config[\'scale\']\n    model = make_model(config[\'model\'], scale)\n\n    if config[\'init_checkpoint\']:\n        s = torch.load(config[\'init_checkpoint\'])\n        model.load_state_dict(s[\'model\'].state_dict())\n\n    trainer = Trainer(model=model, name=config[\'exp_name\'],\n                      models_root=config[\'models_path\'], resume=config[\'resume\'])\n\n    # Copy config in train directory agter create trainer object\n    model_path = os.path.join(config[\'models_path\'], config[\'exp_name\'])\n    shutil.copyfile(args.config, os.path.join(model_path, \'config.yaml\'))\n\n    random.seed(config[\'seed\'])\n    torch.manual_seed(config[\'seed\'])\n    np.random.seed(config[\'seed\'])\n    torch.cuda.manual_seed(config[\'seed\'])\n\n    cudnn.benchmark = True\n\n    print(\'===> Loading datasets\')\n\n    if config[\'model\'] == \'TextTransposeModel\':\n        train_set = DatasetTextImages(path=config[\'train_path\'],\n                                      patch_size=config[\'patch_size\'],\n                                      aug_resize_factor_range=config[\'aug_resize_factor_range\'],\n                                      scale=scale,\n                                      seed=config[\'seed\'],\n                                      dataset_size_factor=config[\'num_of_patches_per_image\'],\n                                      rotate=config[\'rotate\'])\n\n        val_set = DatasetTextImages(path=config[\'validation_path\'],\n                                    patch_size=None,\n                                    aug_resize_factor_range=None,\n                                    scale=scale,\n                                    seed=config[\'seed\'])\n\n    else:\n        train_set = DatasetFromSingleImages(path=config[\'train_path\'],\n                                            patch_size=config[\'patch_size\'],\n                                            aug_resize_factor_range=config[\'aug_resize_factor_range\'],\n                                            scale=scale,\n                                            count=config[\'num_of_train_images\'],\n                                            cache_images=False,\n                                            seed=config[\'seed\'],\n                                            dataset_size_factor=config[\'num_of_patches_per_image\'])\n\n        val_set = DatasetFromSingleImages(path=config[\'validation_path\'],\n                                          patch_size=None,\n                                          aug_resize_factor_range=None,\n                                          scale=scale,\n                                          count=config[\'num_of_val_images\'],\n                                          cache_images=False,\n                                          seed=config[\'seed\'])\n\n    training_data_loader = DataLoader(dataset=train_set, num_workers=config[\'num_of_data_loader_threads\'],\n                                      batch_size=config[\'batch_size\'], shuffle=True, drop_last=True)\n\n    batch_sampler = Data.BatchSampler(\n        sampler=Data.SequentialSampler(val_set),\n        batch_size=1,\n        drop_last=True\n    )\n\n    evaluation_data_loader = DataLoader(dataset=val_set, num_workers=0,\n                                        batch_sampler=batch_sampler)\n    print(\'===> Building model\')\n    criterion = [MSE_loss(config[\'border\']).cuda()]\n\n    print(\'===> Training\')\n\n    trainer.train(criterion=criterion,\n                  optimizer=optim.Adam,\n                  optimizer_params={\'lr\': 1e-3},\n                  scheduler=torch.optim.lr_scheduler.MultiStepLR,\n                  scheduler_params={\'milestones\': config[\'milestones\']},\n                  training_data_loader=training_data_loader,\n                  evaluation_data_loader=evaluation_data_loader,\n                  pretrained_weights=None,\n                  train_metrics=[PSNR(name=\'PSNR\', border=config[\'border\']),\n                                 RMSE(name=\'RMSE\', border=config[\'border\'])],\n                  val_metrics=[PSNR(name=\'PSNR\', border=config[\'border\']),\n                               RMSE(name=\'RMSE\', border=config[\'border\'])],\n                  track_metric=\'PSNR\',\n                  epoches=config[\'num_of_epochs\']\n                  )\n\n\nif __name__ == \'__main__\':\n    try:\n        torch.multiprocessing.set_start_method(\'spawn\')\n    except RuntimeError:\n        pass\n    main()\n'"
pytorch_toolkit/text_spotting/text_spotting/__init__.py,0,b''
pytorch_toolkit/text_spotting/tools/cluster_bboxes.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport json\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'--coco_annotation_json\', required=True)\n    args.add_argument(\'--num_clusters\', type=int, required=True)\n    args.add_argument(\'--image_size\', type=int, nargs=2, required=True)\n\n    return args.parse_args()\n\n\ndef main():\n    """""" Clusters bboxes. """"""\n\n    args = parse_args()\n\n    with open(args.coco_annotation_json) as file:\n        annotation = json.load(file)\n\n    images_id_to_size = dict()\n    for image_info in annotation[\'images\']:\n        images_id_to_size[image_info[\'id\']] = {\n            \'width\': image_info[\'width\'],\n            \'height\': image_info[\'height\']\n        }\n\n    widths = []\n    heights = []\n\n    for bbox_info in annotation[\'annotations\']:\n        bbox_width = bbox_info[\'bbox\'][2] / images_id_to_size[bbox_info[\'image_id\']][\'width\']\n        bbox_height = bbox_info[\'bbox\'][3] / images_id_to_size[bbox_info[\'image_id\']][\'height\']\n\n        widths.append(bbox_width)\n        heights.append(bbox_height)\n\n    sizes = np.array([widths, heights]).transpose()\n\n    kmeans = KMeans(n_clusters=args.num_clusters, random_state=0).fit(sizes)\n    centers = kmeans.cluster_centers_.copy()\n\n    plt.scatter(widths, heights)\n    plt.scatter(centers[:, 0], centers[:, 1])\n\n    centers *= args.image_size\n    centers = sorted(centers, key=lambda x: x[0] * x[1])\n\n    print(\'widths\', \', \'.join([str(c[0]) for c in centers]))\n    print(\'heights\', \', \'.join([str(c[1]) for c in centers]))\n\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/text_spotting/tools/convert_to_onnx.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport json\nimport logging\n\nfrom segmentoly.utils.logging import setup_logging\nfrom segmentoly.utils.stats import add_flops_counting_methods, flops_to_string, \\\n    print_model_with_flops\nfrom segmentoly.utils.weights import load_checkpoint\nfrom text_spotting.models.text_detectors import make_text_detector\nfrom text_spotting.utils.onnx import onnx_export, export_to_onnx_text_recognition_encoder, \\\n    export_to_onnx_text_recognition_decoder\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    parser = argparse.ArgumentParser(description=\'Export Mask R-CNN network to ONNX\')\n\n    parser.add_argument(\'--model\', type=str, required=True,\n                        help=\'Path to configuration file implementing text spotting model.\')\n    parser.add_argument(\'--ckpt\', type=str, required=True,\n                        help=\'Checkpoint file path to load network weights from.\')\n    parser.add_argument(\'--input_size\', type=int, nargs=2, required=True,\n                        help=\'Input resolution.\')\n    parser.add_argument(\'--output_folder\', type=str, required=True,\n                        help=\'Path to the output ONNX file.\')\n    parser.add_argument(\'--verbose\', action=\'store_true\',\n                        help=\'Run export in verbose mode.\')\n    parser.add_argument(\'--check\', action=\'store_true\',\n                        help=\'Run ONNX model checker after export.\')\n    parser.add_argument(\'--show_flops\', action=\'store_true\',\n                        help=\'Estimate computational complexity of the network.\')\n\n    return parser.parse_args()\n\n\ndef main():\n    """""" Does export to onnx. """"""\n\n    args = parse_args()\n\n    with open(args.model) as file:\n        config = json.load(file)\n    text_spotter = make_text_detector(**config[\'model\'])(2, fc_detection_head=False,\n                                                         shape=args.input_size)\n    load_checkpoint(text_spotter, args.ckpt)\n    text_spotter.export_mode = True\n\n    net = text_spotter\n\n    if args.show_flops:\n        net = add_flops_counting_methods(net)\n        net.reset_flops_count()\n        net.start_flops_count()\n\n    # Export of text detection part (Mask-RCNN subgraph).\n    printable_graph = onnx_export(net, args.input_size, args.output_folder, check=args.check,\n                                  verbose=args.verbose)\n    if args.verbose:\n        logging.info(printable_graph)\n\n    if args.show_flops:\n        net.stop_flops_count()\n        logging.info(\'Computational complexity of text detection part: {}\'.format(\n            flops_to_string(net.compute_average_flops_cost())))\n        if args.verbose:\n            print_model_with_flops(net)\n\n    # Export of text recognition encoder\n    net = text_spotter.text_recogn_head.encoder\n    if args.show_flops:\n        net = add_flops_counting_methods(net)\n        net.reset_flops_count()\n        net.start_flops_count()\n\n    printable_graph = export_to_onnx_text_recognition_encoder(\n        net, text_spotter.text_recogn_head.input_feature_size, args.output_folder)\n    if args.verbose:\n        logging.info(printable_graph)\n\n    if args.show_flops:\n        net.stop_flops_count()\n        logging.info(\'Computational complexity of text recognition encoder part: {}\'.format(\n            flops_to_string(net.compute_average_flops_cost())))\n        if args.verbose:\n            print_model_with_flops(net)\n\n    # Export of text recognition decoder\n    net = text_spotter.text_recogn_head.decoder\n    if args.show_flops:\n        net = add_flops_counting_methods(net)\n        net.reset_flops_count()\n        net.start_flops_count()\n\n    printable_graph = export_to_onnx_text_recognition_decoder(\n        net, text_spotter.text_recogn_head.input_feature_size, args.output_folder)\n    if args.verbose:\n        logging.info(printable_graph)\n\n    if args.show_flops:\n        net.stop_flops_count()\n        logging.info(\'Computational complexity of text recognition decoder part: {}\'.format(\n            flops_to_string(net.compute_average_flops_cost())))\n        if args.verbose:\n            print_model_with_flops(net)\n\n\nif __name__ == \'__main__\':\n    setup_logging()\n    main()\n'"
pytorch_toolkit/text_spotting/tools/create_dataset.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport json\nimport os\n\nfrom text_spotting.datasets.datasets import TextOnlyCocoAnnotation\nfrom text_spotting.datasets.datasets import str_to_class\nfrom text_spotting.datasets.factory import root_data_dir\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'--config\', help=\'Path to dataset configuration file (json).\',\n                      required=True)\n    args.add_argument(\'--output\', help=\'Path where to save annotation (json).\',\n                      required=True)\n    args.add_argument(\'--visualize\', action=\'store_true\', help=\'Visualize annotation.\')\n    return args.parse_args()\n\n\ndef main():\n    """""" Loads configuration file and creates dataset. """"""\n\n    args = parse_args()\n    with open(args.config) as file:\n        config = json.load(file)\n\n    assert isinstance(config, list)\n    ann = TextOnlyCocoAnnotation()\n    for dataset in config:\n        assert isinstance(dataset, dict)\n        if os.path.islink(root_data_dir()):\n            dataset[\'kwargs\'][\'root\'] = os.readlink(root_data_dir())\n        else:\n            dataset[\'kwargs\'][\'root\'] = os.path.abspath(root_data_dir())\n        ann += str_to_class[dataset[\'name\']](**dataset[\'kwargs\'])()\n\n    ann.write(args.output)\n\n    ann = TextOnlyCocoAnnotation(args.output, os.path.dirname(args.output))\n    if args.visualize:\n        ann.visualize(put_text=True, imshow_delay=1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/text_spotting/tools/demo.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport json\nimport logging\nimport sys\n\nimport cv2\nfrom tqdm import tqdm\n\nimport torch\nfrom segmentoly.data.dataparallel import collate\nfrom segmentoly.data.transforms import Resize, Normalize, ToTensor, Compose\nfrom segmentoly.datasets.images import ImagesDataset\nfrom segmentoly.datasets.video import VideoDataset\nfrom segmentoly.utils.logging import setup_logging\nfrom segmentoly.utils.profile import Timer\nfrom segmentoly.utils.stats import add_flops_counting_methods, flops_to_string, \\\n    print_model_with_flops\nfrom segmentoly.utils.tracker import StaticIOUTracker\nfrom segmentoly.utils.weights import load_checkpoint\nfrom text_spotting.models.openvino_net import TextMaskRCNNOpenVINO\nfrom text_spotting.models.text_detectors import make_text_detector\nfrom text_spotting.utils.postprocess import postprocess\nfrom text_spotting.utils.visualizer import TextVisualizer\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    parser = argparse.ArgumentParser(description=\'Run instance segmentation live\')\n    subparsers = parser.add_subparsers(help=\'Backend\', dest=\'backend\')\n\n    parser.add_argument(\'-pt\', \'--prob_threshold\', default=0.8, type=float,\n                        help=\'Probability threshold for detections filtering.\')\n    parser.add_argument(\'--mean_pixel\', default=(0.0, 0.0, 0.0), type=float, nargs=3,\n                        metavar=\'<num>\',\n                        help=\'Mean pixel value to subtract from image.\')\n    parser.add_argument(\'--std_pixel\', default=(1.0, 1.0, 1.0), type=float, nargs=3,\n                        metavar=\'<num>\',\n                        help=\'STD pixel value to divide an image.\')\n    parser.add_argument(\'--rgb\', action=\'store_true\',\n                        help=\'Use RGB instead of BGR.\')\n    parser.add_argument(\'--size\', dest=\'size\', default=None, type=int, nargs=2,\n                        metavar=\'<num>\',\n                        help=\'Input resolution in a (height, width) format.\')\n\n    data_source_parser = parser.add_mutually_exclusive_group(required=True)\n    data_source_parser.add_argument(\'-v\', \'--video\', default=None, type=str,\n                                    help=\'Path to a video file or numeric camera ID.\')\n    data_source_parser.add_argument(\'-i\', \'--images\', default=None, type=str,\n                                    help=\'Path to an image or a folder with images.\')\n\n    parser.add_argument(\'--show_scores\', action=\'store_true\',\n                        help=\'Show detection scores.\')\n    parser.add_argument(\'--show_boxes\', action=\'store_true\',\n                        help=\'Show bounding boxes.\')\n    parser.add_argument(\'--show_fps\', action=\'store_true\',\n                        help=\'Show FPS.\')\n    parser.add_argument(\'--delay\', default=0, type=int)\n\n    openvino_parser = subparsers.add_parser(\'openvino\')\n    openvino_parser.add_argument(\'--detector_model\', dest=\'openvino_detector_model_path\', type=str,\n                                 required=True, metavar=\'""<path>""\',\n                                 help=\'XML file with model description for text detection part \'\n                                      \'(OpenVINO format)\')\n    openvino_parser.add_argument(\'--encoder_model\', dest=\'openvino_encoder_model_path\', type=str,\n                                 required=True, metavar=\'""<path>""\',\n                                 help=\'XML file with model description for text detection part \'\n                                      \'(OpenVINO format)\')\n    openvino_parser.add_argument(\'--decoder_model\', dest=\'openvino_decoder_model_path\', type=str,\n                                 required=True, metavar=\'""<path>""\',\n                                 help=\'XML file with model description for text detection part \'\n                                      \'(OpenVINO format)\')\n    openvino_parser.add_argument(\'--show_pc\', \'--show_performance_counters\',\n                                 dest=\'show_performance_counters\',\n                                 help=\'Show OpenVINO performance counters.\',\n                                 action=\'store_true\')\n\n    pytorch_parser = subparsers.add_parser(\'pytorch\')\n    pytorch_parser.add_argument(\'--model\', dest=\'pytorch_model_class\', type=str, required=True,\n                                help=\'Path to module and class implementing Mask R-CNN instance \'\n                                     \'segmentation model.\')\n    pytorch_parser.add_argument(\'--weights\', dest=\'checkpoint_file_path\', required=True,\n                                help=\'File with models weights\')\n    pytorch_parser.add_argument(\'--show_flops\',\n                                help=\'Show FLOPs.\',\n                                action=\'store_true\')\n    pytorch_parser.add_argument(\'--show_layers_flops\',\n                                help=\'Show FLOPs for all modules.\',\n                                action=\'store_true\')\n\n    return parser.parse_args()\n\n\ndef main(args):\n    """""" Demonstrates text spotter. """"""\n\n    transforms = Compose(\n        [\n            Resize(size=args.size),\n            ToTensor(),\n            Normalize(mean=args.mean_pixel, std=args.std_pixel, rgb=args.rgb),\n        ]\n    )\n\n    classes = [\'\', \'text\']\n\n    logging.info(\'Using {} backend\'.format(args.backend))\n\n    logging.info(\'Loading network...\')\n    batch_size = 1\n    if args.backend == \'pytorch\':\n        with open(args.pytorch_model_class) as file:\n            config = json.load(file)\n        net = make_text_detector(**config[\'model\'])(len(classes),\n                                                    force_max_output_size=False, shape=args.size)\n        net.eval()\n        load_checkpoint(net, args.checkpoint_file_path)\n        if torch.cuda.is_available():\n            net = net.cuda()\n        net = add_flops_counting_methods(net)\n        net.reset_flops_count()\n        net.start_flops_count()\n    elif args.backend == \'openvino\':\n        net = TextMaskRCNNOpenVINO(args.openvino_detector_model_path,\n                                   args.openvino_encoder_model_path,\n                                   args.openvino_decoder_model_path,\n                                   collect_perf_counters=args.show_performance_counters)\n    else:\n        raise ValueError(\'Unknown backend ""{}""\'.format(args.backend))\n\n    viz = TextVisualizer(0.9, classes, confidence_threshold=args.prob_threshold,\n                         show_boxes=args.show_boxes, show_scores=args.show_scores)\n\n    inference_timer = Timer(cuda_sync=True, warmup=1)\n    timer = Timer(cuda_sync=False, warmup=1)\n    timer.tic()\n\n    logging.info(\'Configuring data source...\')\n    if args.video:\n        try:\n            args.video = int(args.video)\n        except ValueError:\n            pass\n        demo_dataset = VideoDataset(args.video, labels=classes, transforms=transforms)\n        num_workers = 0\n        tracker = StaticIOUTracker()\n    else:\n        demo_dataset = ImagesDataset(args.images, labels=classes, transforms=transforms)\n        num_workers = 1\n        tracker = None\n\n    data_loader = torch.utils.data.DataLoader(\n        demo_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=collate\n    )\n\n    logging.info(\'Processing data...\')\n    frames_num = len(demo_dataset)\n    for data_batch in tqdm(iter(data_loader), total=frames_num if frames_num != sys.maxsize else 0):\n        im_data = data_batch[\'im_data\']\n        im_info = data_batch[\'im_info\']\n        if torch.cuda.is_available():\n            im_data = [i.cuda() for i in im_data]\n            im_info = [i.cuda() for i in im_info]\n        with torch.no_grad(), inference_timer:\n            boxes, classes, scores, _, masks, text_probs = net(im_data, im_info)\n\n        meta = data_batch[\'meta\'][0]\n        scores, classes, boxes, masks, text_probs = postprocess(\n            scores, classes, boxes, masks, text_probs,\n            im_h=meta[\'original_size\'][0],\n            im_w=meta[\'original_size\'][1],\n            im_scale_y=meta[\'processed_size\'][0] / meta[\'original_size\'][0],\n            im_scale_x=meta[\'processed_size\'][1] / meta[\'original_size\'][1],\n            full_image_masks=True,\n            encode_masks=False,\n            confidence_threshold=args.prob_threshold)\n\n        masks_ids = tracker(masks, classes) if tracker is not None else None\n        image = data_batch[\'original_image\'][0]\n        visualization = viz(image, boxes, classes, scores, segms=masks, ids=masks_ids,\n                            text_log_softmax=text_probs)\n        fps = 1 / timer.toc()\n        if args.show_fps:\n            visualization = cv2.putText(visualization, \'FPS: {:>2.2f}\'.format(fps),\n                                        (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n        cv2.imshow(\'result\', visualization)\n        key = cv2.waitKey(args.delay)\n        if key == 27:\n            break\n        timer.tic()\n\n    if inference_timer.average_time > 0:\n        logging.info(\'Average inference FPS: {:3.2f}\'.format(1 / inference_timer.average_time))\n\n    if args.backend == \'pytorch\':\n        net.stop_flops_count()\n        if args.show_flops:\n            logging.info(\n                \'Average FLOPs:  {}\'.format(flops_to_string(net.compute_average_flops_cost())))\n        if args.show_layers_flops:\n            logging.info(\'Thorough computational complexity statistics:\')\n            print_model_with_flops(net)\n        if torch.cuda.is_available():\n            logging.info(\'GPU memory footprint:\')\n            logging.info(\n                \'\\tMax allocated: {:.2f} MiB\'.format(torch.cuda.max_memory_allocated() / 1024 ** 2))\n            logging.info(\n                \'\\tMax cached:    {:.2f} MiB\'.format(torch.cuda.max_memory_cached() / 1024 ** 2))\n    else:\n        if args.show_performance_counters:\n            net.print_performance_counters()\n\n    cv2.destroyAllWindows()\n    del net\n\n\nif __name__ == \'__main__\':\n    setup_logging()\n    main(parse_args())\n'"
pytorch_toolkit/text_spotting/tools/test.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport json\nimport logging\n\nfrom tqdm import tqdm\n\nimport torch\nfrom segmentoly.data.dataparallel import collate, ShallowDataParallel\nfrom segmentoly.data.transforms import Resize, Normalize, ToTensor, Compose\nfrom segmentoly.utils.logging import setup_logging\nfrom segmentoly.utils.profile import Timer\nfrom segmentoly.utils.stats import add_flops_counting_methods, flops_to_string, \\\n    print_model_with_flops\nfrom segmentoly.utils.weights import load_checkpoint\nfrom text_spotting.data.alphabet import AlphabetDecoder\nfrom text_spotting.datasets.factory import get_dataset\nfrom text_spotting.models.openvino_net import TextMaskRCNNOpenVINO\nfrom text_spotting.models.text_detectors import make_text_detector\nfrom text_spotting.utils.postprocess import postprocess_batch\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    parser = argparse.ArgumentParser(description=\'Evaluate instance segmentation\')\n    subparsers = parser.add_subparsers(help=\'Backend\', dest=\'backend\')\n\n    parser.add_argument(\'--dataset\', help=\'Dataset name.\', metavar=\'""<name>""\', )\n    parser.add_argument(\'--nw\', dest=\'num_workers\', default=8, type=int, metavar=\'<num>\',\n                        help=\'Number of data loading workers.\')\n    parser.add_argument(\'-pt\', \'--prob_threshold\', default=0.8, type=float,\n                        help=\'Probability threshold for detections filtering.\')\n    parser.add_argument(\'--mean_pixel\', default=(0.0, 0.0, 0.0), type=float, nargs=3,\n                        metavar=\'<num>\',\n                        help=\'Mean pixel value to subtract from image.\')\n    parser.add_argument(\'--std_pixel\', default=(1.0, 1.0, 1.0), type=float, nargs=3,\n                        metavar=\'<num>\',\n                        help=\'STD pixel value to divide an image.\')\n    parser.add_argument(\'--rgb\', action=\'store_true\',\n                        help=\'Use RGB instead of BGR.\')\n    parser.add_argument(\'--size\', dest=\'size\', default=None, type=int, nargs=2,\n                        metavar=\'<num>\',\n                        help=\'Input resolution in a (height, width) format.\')\n    parser.add_argument(\'--visualize\', action=\'store_true\', help=\'Visualize text spotting results.\')\n\n    openvino_parser = subparsers.add_parser(\'openvino\')\n    openvino_parser.add_argument(\'--detector_model\', dest=\'openvino_detector_model_path\', type=str,\n                                 required=True, metavar=\'""<path>""\',\n                                 help=\'XML file with model description for text detection part \'\n                                      \'(OpenVINO format)\')\n    openvino_parser.add_argument(\'--encoder_model\', dest=\'openvino_encoder_model_path\', type=str,\n                                 required=True, metavar=\'""<path>""\',\n                                 help=\'XML file with model description for text detection part \'\n                                      \'(OpenVINO format)\')\n    openvino_parser.add_argument(\'--decoder_model\', dest=\'openvino_decoder_model_path\', type=str,\n                                 required=True, metavar=\'""<path>""\',\n                                 help=\'XML file with model description for text detection part \'\n                                      \'(OpenVINO format)\')\n    openvino_parser.add_argument(\'--show_pc\', \'--show_performance_counters\',\n                                 dest=\'show_performance_counters\',\n                                 help=\'Show OpenVINO performance counters.\',\n                                 action=\'store_true\')\n\n    pytorch_parser = subparsers.add_parser(\'pytorch\')\n    pytorch_parser.add_argument(\'--model\', dest=\'pytorch_model_class\', type=str, required=True,\n                                help=\'Path to module and class implementing Mask R-CNN instance \'\n                                     \'segmentation model.\')\n    pytorch_parser.add_argument(\'--weights\', dest=\'checkpoint_file_path\', required=True,\n                                help=\'File with models weights\')\n    pytorch_parser.add_argument(\'--show_flops\',\n                                help=\'Show FLOPs.\',\n                                action=\'store_true\')\n    pytorch_parser.add_argument(\'--show_layers_flops\',\n                                help=\'Show FLOPs for all modules.\',\n                                action=\'store_true\')\n\n    return parser.parse_args()\n\n\ndef main(args):\n    """""" Tests text spotter. """"""\n\n    transforms = Compose(\n        [\n            Resize(size=args.size),\n            ToTensor(),\n            Normalize(mean=args.mean_pixel, std=args.std_pixel, rgb=args.rgb),\n        ]\n    )\n    dataset = get_dataset(args.dataset, False, False, transforms,\n                          alphabet_decoder=AlphabetDecoder())\n    logging.info(dataset)\n    num_workers = args.num_workers\n\n    inference_timer = Timer()\n\n    logging.info(\'Using {} backend\'.format(args.backend))\n\n    logging.info(\'Loading network...\')\n    batch_size = 1\n    if args.backend == \'pytorch\':\n        with open(args.pytorch_model_class) as file:\n            config = json.load(file)\n        net = make_text_detector(**config[\'model\'])(dataset.classes_num,\n                                                    force_max_output_size=False, shape=args.size)\n        net.eval()\n        load_checkpoint(net, args.checkpoint_file_path)\n        if torch.cuda.is_available():\n            net = net.cuda()\n        net = add_flops_counting_methods(net)\n        net.reset_flops_count()\n        net.start_flops_count()\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = True\n            net = net.cuda()\n            net = ShallowDataParallel(net)\n    elif args.backend == \'openvino\':\n        net = TextMaskRCNNOpenVINO(args.openvino_detector_model_path,\n                                   args.openvino_encoder_model_path,\n                                   args.openvino_decoder_model_path,\n                                   collect_perf_counters=args.show_performance_counters)\n    else:\n        raise ValueError(\'Unknown backend ""{}""\'.format(args.backend))\n\n    logging.info(\'Using batch size {}\'.format(batch_size))\n    logging.info(\'Number of prefetching processes {}\'.format(num_workers))\n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=collate\n    )\n\n    logging.info(\'Processing dataset...\')\n    boxes_all = []\n    masks_all = []\n    classes_all = []\n    scores_all = []\n    text_probs_all = []\n    for data_batch in tqdm(iter(data_loader)):\n        batch_meta = data_batch[\'meta\']\n        actual_batch_size = len(batch_meta)\n        with torch.no_grad(), inference_timer:\n            boxes, classes, scores, batch_ids, masks, text_probs = net(**data_batch)\n\n        im_heights = [meta[\'original_size\'][0] for meta in batch_meta]\n        im_widths = [meta[\'original_size\'][1] for meta in batch_meta]\n        im_scale_y = [meta[\'processed_size\'][0] / meta[\'original_size\'][0] for meta in batch_meta]\n        im_scale_x = [meta[\'processed_size\'][1] / meta[\'original_size\'][1] for meta in batch_meta]\n        scores, classes, boxes, masks, text_probs = postprocess_batch(\n            batch_ids, scores, classes, boxes, masks, text_probs, actual_batch_size,\n            im_h=im_heights,\n            im_w=im_widths,\n            im_scale_y=im_scale_y,\n            im_scale_x=im_scale_x,\n            full_image_masks=True, encode_masks=True,\n            confidence_threshold=args.prob_threshold)\n\n        boxes_all.extend(boxes)\n        masks_all.extend(masks)\n        classes_all.extend(classes)\n        scores_all.extend(scores)\n        text_probs_all.extend(text_probs)\n\n    try:\n        del data_loader\n    except ConnectionResetError:\n        pass\n\n    logging.info(\'Evaluating results...\')\n    evaluation_results = dataset.evaluate(scores_all, classes_all, boxes_all, masks_all,\n                                          text_probs_all, dump=\'dump\', visualize=args.visualize)\n    logging.info(evaluation_results)\n\n    logging.info(\'Average inference time {}\'.format(inference_timer.average_time))\n\n    if args.backend == \'pytorch\':\n        if torch.cuda.is_available():\n            net = net.module\n        net.stop_flops_count()\n        if args.show_flops:\n            logging.info(\n                \'Average FLOPs:  {}\'.format(flops_to_string(net.compute_average_flops_cost())))\n        if args.show_layers_flops:\n            logging.info(\'Thorough computational complexity statistics:\')\n            print_model_with_flops(net)\n    else:\n        if args.show_performance_counters:\n            net.print_performance_counters()\n\n    del net\n\n\nif __name__ == \'__main__\':\n    setup_logging()\n    main(parse_args())\n'"
pytorch_toolkit/text_spotting/tools/train.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport warnings\n\nwarnings.simplefilter(""ignore"", UserWarning)\nimport argparse\n\nimport logging\nimport os.path as osp\nimport resource\nfrom collections import OrderedDict\nimport json\nimport sys\nfrom tqdm import tqdm\n\nfrom segmentoly.data.dataparallel import collate\nfrom segmentoly.data.transforms import *\n\nfrom segmentoly.utils.logging import setup_logging, TextLogger, TensorboardLogger\nfrom segmentoly.utils.lr_scheduler import MultiStepLRWithWarmUp\nfrom segmentoly.utils.training_engine import DefaultMaskRCNNTrainingEngine\nfrom segmentoly.utils.weights import load_checkpoint\n\nfrom text_spotting.models.text_detectors import make_text_detector\nfrom text_spotting.data.transforms import *\nfrom text_spotting.datasets.factory import get_dataset\nfrom text_spotting.data.alphabet import AlphabetDecoder\nfrom text_spotting.utils.postprocess import postprocess\n\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    args = argparse.ArgumentParser()\n    args.add_argument(\'config\')\n    args.add_argument(\'--work_dir\')\n    return args.parse_args()\n\n\nclass Trainer(DefaultMaskRCNNTrainingEngine):\n\n    def __init__(self, work_dir, config):\n        super().__init__()\n        self.identifier = config[\'identifier\']\n        self.description = config[\'description\']\n        self.root_directory = work_dir if work_dir else osp.join(osp.dirname(osp.abspath(__file__)),\n                                                                 \'..\')\n        self.run_directory = self.create_run_directory(osp.join(self.root_directory, \'models\'))\n\n        setup_logging(file_path=osp.join(self.run_directory, \'log.txt\'))\n\n        logger.info(\'Running {}\'.format(self.identifier))\n        logger.info(self.description)\n        logger.info(\'Working directory ""{}""\'.format(self.run_directory))\n\n        self.batch_size = config[\'training_details\'][\'batch_size\']\n        self.virtual_iter_size = config[\'training_details\'][\'virtual_iter_size\']\n\n        model_class = make_text_detector(**config[\'model\'])\n\n        alphabet_decoder = AlphabetDecoder()\n\n        # Training dataset.\n        training_transforms = Compose([\n                                          getattr(sys.modules[__name__], k)(**v) for k, v in\n                                          config[\'training_transforms\'].items()\n                                      ] + [AlphabetDecodeTransform(alphabet_decoder)])\n\n        training_dataset_name = config[\'training_dataset_name\']\n        logger.info(\'Training dataset {}\'.format(training_dataset_name))\n        training_dataset = get_dataset(training_dataset_name, True, True, training_transforms,\n                                       alphabet_decoder=alphabet_decoder,\n                                       remove_images_without_text=True)\n        logger.info(training_dataset)\n        self.training_data_loader = torch.utils.data.DataLoader(\n            training_dataset, batch_size=self.batch_size, num_workers=0,\n            shuffle=True, drop_last=True, collate_fn=collate\n        )\n\n        # Validation datasets.\n        validation_transforms = Compose([\n            getattr(sys.modules[__name__], k)(**v) for k, v in\n            config[\'validation_transforms\'].items()\n        ])\n        self.confidence_threshold = config[\'validation_confidence_threshold\']\n        validation_datasets = []\n        validation_dataset_name = config[\'validation_dataset_name\']\n        logger.info(\'Validation dataset #{}: {}\'.format(len(validation_datasets) + 1,\n                                                        validation_dataset_name))\n        validation_datasets.append(\n            get_dataset(validation_dataset_name, False, False, validation_transforms,\n                        alphabet_decoder=alphabet_decoder))\n        logger.info(validation_datasets[-1])\n\n        self.validation_data_loaders = []\n        for validation_dataset in validation_datasets:\n            self.validation_data_loaders.append(torch.utils.data.DataLoader(\n                validation_dataset,\n                batch_size=1, num_workers=8,\n                shuffle=False, drop_last=False, collate_fn=collate)\n            )\n        self.validate_every = config[\'training_details\'][\'validate_every\']\n\n        for validation_dataset in validation_datasets:\n            assert training_dataset.classes_num == validation_dataset.classes_num\n\n        # Model and optimizer.\n        logger.info(\'Model:\')\n\n        self.model = model_class(cls_num=training_dataset.classes_num, shape=config[\'shape\'],\n                                 num_chars=len(alphabet_decoder.alphabet))\n\n        logger.info(self.model)\n\n        self.training_iterations_num = config[\'training_details\'][\'training_iterations_num\']\n        lr_scheduler_milestones = config[\'training_details\'][\'lr_scheduler_milestones\']\n        base_lr = config[\'training_details\'][\'base_lr\']\n        weight_decay = config[\'training_details\'][\'weight_decay\']\n        logger.info(\'Optimizer:\')\n        self.optimizer = torch.optim.SGD(self.setup_optimizer(self.model, base_lr, weight_decay),\n                                         lr=base_lr, weight_decay=weight_decay, momentum=0.9)\n        logger.info(self.optimizer)\n        logger.info(\'Learning Rate scheduler:\')\n        self.lr_scheduler = MultiStepLRWithWarmUp(\n            self.optimizer,\n            milestones=lr_scheduler_milestones,\n            warmup_iters=1000,\n            warmup_method=\'linear\',\n            warmup_factor_base=0.333,\n            gamma=0.1,\n            last_epoch=0\n        )\n        logger.info(self.lr_scheduler)\n\n        self.start_step = 0\n        if \'backbone_checkpoint\' in config and config[\'backbone_checkpoint\']:\n            checkpoint_file_path = osp.join(self.root_directory, config[\'backbone_checkpoint\'])\n            if not osp.exists(checkpoint_file_path):\n                raise IOError(\'Initial checkpoint file ""{}"" does not exist. \'\n                              \'Please fetch pre-trained backbone networks using \'\n                              \'tools/download_pretrained_weights.py script first.\'.format(\n                    checkpoint_file_path))\n            logger.info(\'Loading weights from ""{}""\'.format(checkpoint_file_path))\n            load_checkpoint(self.model.backbone, checkpoint_file_path, verbose=True,\n                            skip_prefix=\'text_recogn\')\n\n        if \'checkpoint\' in config and config[\'checkpoint\']:\n            checkpoint_file_path = osp.join(self.root_directory, config[\'checkpoint\'])\n            if not osp.exists(checkpoint_file_path):\n                raise IOError(\'Checkpoint file ""{}"" does not exist. \'.format(checkpoint_file_path))\n            logger.info(\'Loading weights from ""{}""\'.format(checkpoint_file_path))\n            load_checkpoint(self.model, checkpoint_file_path, verbose=True)\n\n        # Loggers and misc. stuff.\n        self.loggers = [TextLogger(logger), TensorboardLogger(self.run_directory)]\n        self.log_every = 50\n\n        self.checkpoint_every = config[\'training_details\'][\'checkpoint_every\']\n\n    def validate(self, net, data_loader, idx=0):\n        net.eval()\n        logging.info(\'Processing the dataset...\')\n        boxes_all = []\n        masks_all = []\n        classes_all = []\n        scores_all = []\n        text_all = []\n        for data_batch in tqdm(iter(data_loader)):\n            im_data = data_batch[\'im_data\']\n            im_info = data_batch[\'im_info\']\n            with torch.no_grad():\n                boxes, classes, scores, _, masks, text_probs = net(im_data, im_info)\n            meta = data_batch[\'meta\'][0]\n            scores, classes, boxes, masks, text_probs = postprocess(\n                scores, classes, boxes, masks, text_probs,\n                im_h=meta[\'original_size\'][0],\n                im_w=meta[\'original_size\'][1],\n                im_scale_y=meta[\'processed_size\'][0] / meta[\'original_size\'][0],\n                im_scale_x=meta[\'processed_size\'][1] / meta[\'original_size\'][1],\n                full_image_masks=True,\n                encode_masks=True,\n                confidence_threshold=self.confidence_threshold)\n\n            boxes_all.append(boxes)\n            masks_all.append(masks)\n            classes_all.append(classes)\n            scores_all.append(scores)\n            text_all.append(text_probs)\n\n        logging.info(\'Evaluating results...\')\n        evaluation_results = data_loader.dataset.evaluate(scores_all, classes_all, boxes_all,\n                                                          masks_all, text_all)\n        evaluation_results = {\'val{}/{}\'.format(idx, k): v for k, v in evaluation_results.items()}\n        return evaluation_results\n\n\nif __name__ == \'__main__\':\n    assert sys.version_info[0] == 3 and sys.version_info[1] > 6\n\n    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n    resource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n    args = parse_args()\n\n    with open(args.config, \'r\') as f:\n        config = json.load(f, object_pairs_hook=OrderedDict)\n\n    experiment = Trainer(args.work_dir, config)\n    torch.backends.cudnn.benchmark = False\n    experiment.run(experiment.start_step)\n    logger.info(\'Done.\')\n'"
tensorflow_toolkit/action_detection/action_detection/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/tools/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/image_retrieval/image_retrieval/common.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport numpy as np\n\nimport cv2\n\n\ndef max_central_square_crop(image):\n    \'\'\' Makes max-sized central squared crop. \'\'\'\n\n    height, width = image.shape[:2]\n\n    if width > height:\n        image = image[:, (width - height) // 2:(width - height) // 2 + height]\n    else:\n        image = image[(height - width) // 2:(height - width) // 2 + width, :]\n\n    return image\n\n\ndef preproces_image(image):\n    \'\'\' Scales and subtracts mean value from image. \'\'\'\n\n    image = image / 127.5 - 1.0\n    return image\n\n\ndef depreprocess_image(image):\n    \'\'\' Makes transform which is inverse to preprocessing. \'\'\'\n\n    image = (image + 1.0) * 127.5\n    image = image.astype(np.uint8)\n    return image\n\n\ndef fit_to_max_size(image, max_size):\n    \'\'\' Fits input image to max_size. \'\'\'\n\n    if image.shape[0] > max_size or image.shape[1] > max_size:\n        if image.shape[0] > image.shape[1]:\n            image = cv2.resize(image, (int(image.shape[1] / (image.shape[0] / max_size)), max_size))\n        else:\n            image = cv2.resize(image, (max_size, int(image.shape[0] / (image.shape[1] / max_size))))\n\n    return image\n\n\ndef crop_resize(image, input_size):\n    \'\'\' Makes max-sized central crop, resizes to input_size. \'\'\'\n\n    image = max_central_square_crop(image)\n    image = cv2.resize(image, (input_size, input_size))\n    return image\n\n\ndef crop_resize_shift_scale(image, input_size):\n    \'\'\' Makes max-sized central crop, resizes to input_size, scales and subtracts mean values. \'\'\'\n\n    image = crop_resize(image, input_size)\n    image = preproces_image(image)\n    image = np.expand_dims(image, axis=0)\n    return image\n\n\ndef central_crop(image, divide_by, shift):\n    \'\'\' Makes central crops dividing input image by number of equal cells. \'\'\'\n\n    height, width = image.shape[0:2]\n    image = image[height // divide_by * shift: height // divide_by * (divide_by - shift),\n                  width // divide_by * shift: width // divide_by * (divide_by - shift)]\n    return image\n\n\ndef from_list(path, multiple_images_per_label=True):\n    \'\'\' Loads images list. \'\'\'\n\n    images_path = []\n    labels = []\n    is_real = []\n\n    text_label_to_class_id = {}\n\n    uniques_labels = set()\n\n    root = os.path.dirname(os.path.abspath(path))\n\n    with open(path) as opened_file:\n        for line in opened_file.readlines():\n            line = line.strip().split(\' \')\n            if len(line) == 2:\n                image_path, label = line\n                real = False\n            else:\n                image_path, label, real = line\n                real = real.lower() == \'r\'\n\n            text_label_to_class_id[os.path.basename(image_path).split(\'.\')[0]] = int(label)\n\n            if not multiple_images_per_label and label in uniques_labels:\n                continue\n\n            uniques_labels.add(label)\n\n            is_real.append(real)\n            images_path.append(os.path.join(root, image_path))\n            labels.append(int(label))\n\n    return images_path, labels, is_real, text_label_to_class_id\n'"
tensorflow_toolkit/image_retrieval/image_retrieval/dataset.py,47,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport collections\nimport json\nimport random\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nfrom image_retrieval.common import preproces_image, depreprocess_image, fit_to_max_size, from_list\n\n\ndef blur(image):\n    kernel = np.ones((3, 3), np.float32) / 9\n    image = cv2.filter2D(image, -1, kernel)\n    return image\n\n\ndef gray_noise(image):\n    if np.mean(image) > 100:\n        gray = np.random.uniform(0.0, 100.0, image.shape[0:2])\n        gray3 = np.array([gray, gray, gray])\n        gray3 = np.transpose(gray3, (1, 2, 0))\n        gray3 = cv2.blur(gray3, ksize=(7, 7))\n        image -= gray3\n        image = np.clip(image, 0.0, 255.0)\n\n    return image\n\n\n@tf.function\ndef tf_random_crop_and_resize(image, input_size):\n    min_size = tf.minimum(tf.shape(image)[0], tf.shape(image)[1])\n    crop_size = tf.random.uniform((), min_size // 2, min_size, dtype=tf.int32)\n\n    crop = tf.image.random_crop(image, (crop_size, crop_size, 3))\n\n    var_thr = 100\n\n    for _ in tf.range(10):\n        moments = tf.nn.moments(tf.reshape(crop, (-1, 3)), axes=0)\n\n        if tf.less(tf.reduce_sum(moments[1]), tf.constant(var_thr, dtype=tf.float32)):\n            crop = tf.image.random_crop(image, (crop_size, crop_size, 3))\n        else:\n            break\n\n    moments = tf.nn.moments(tf.reshape(crop, (-1, 3)), axes=0)\n    if tf.less(tf.reduce_sum(moments[1]), tf.constant(var_thr, dtype=tf.float32)):\n        crop = tf.image.random_crop(image, (tf.shape(image)[0], tf.shape(image)[1], 3))\n\n    crop = tf.cast(tf.expand_dims(crop, axis=0), tf.float32)\n    crop = tf.image.resize(crop, (input_size, input_size))\n    crop = tf.squeeze(crop, axis=0)\n\n    return crop\n\n\n@tf.function\ndef tf_distort_color(image):\n    """""" Distorts color. """"""\n\n    image = image / 255.0\n    image = image[:, :, ::-1]\n\n    brightness_max_delta = 16. / 255.\n\n    color_ordering = tf.random.uniform([], maxval=5, dtype=tf.int32)\n    if tf.equal(color_ordering, 0):\n        image = tf.image.random_brightness(image, max_delta=brightness_max_delta)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.1)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif tf.equal(color_ordering, 1):\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=brightness_max_delta)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.1)\n    elif tf.equal(color_ordering, 2):\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.1)\n        image = tf.image.random_brightness(image, max_delta=brightness_max_delta)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n    elif tf.equal(color_ordering, 3):\n        image = tf.image.random_hue(image, max_delta=0.1)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=brightness_max_delta)\n\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    image = image * 255\n    image = image[:, :, ::-1]\n\n    return image\n\n\nclass Dataset:\n\n    def __init__(self, images_paths, labels, is_real, input_size, batch_size, params,\n                 return_original=False):\n        self.images_paths = images_paths\n        self.input_size = input_size\n        self.batch_size = batch_size\n        self.params = params\n        self.return_original = return_original\n\n        self.loaded_images = []\n        self.labels = Dataset.reassign_labels(labels)\n        self.is_real = is_real\n\n        if self.params[\'preload\']:\n            self.preload()\n            if self.params[\'pretile\']:\n                self.pretile()\n\n        self.images_indexes_per_class = collections.defaultdict(list)\n        for index, label in enumerate(self.labels):\n            self.images_indexes_per_class[label].append(index)\n\n        if self.params[\'weighted_sampling\']:\n            self.calc_sampling_probs()\n\n    def calc_sampling_probs(self):\n        \'\'\' Counts number of images per class and returns probability distribution so that\n            distribution of images classes becomes uniform.\n        \'\'\'\n\n        frequency = {l: self.labels.count(l) for l in set(self.labels)}\n\n        probs = np.empty((len(self.labels)), dtype=np.float32)\n        for idx, l in enumerate(self.labels):\n            probs[idx] = 1.0 / frequency[l]\n        self.probs = probs / np.sum(probs)\n\n    def preload(self):\n        \'\'\' Pre-loads images in RAM. \'\'\'\n\n        for image_path in self.images_paths:\n            self.loaded_images.append(cv2.imread(image_path))\n\n    def pretile(self):\n        \'\'\' Pre-tiles images in RAM. Makes training faster but requires huge amount of RAM. \'\'\'\n\n        tiled_labels = []\n        tiled_is_real = []\n        tiled_loaded_images = []\n\n        for read_image, label, real in zip(self.loaded_images, self.labels, self.is_real):\n            if not real:\n                for n in range(2, self.params[\'max_tiling\'] + 1):\n                    image = self.tile(read_image, n)\n\n                    tiled_labels.append(label)\n                    tiled_is_real.append(real)\n                    tiled_loaded_images.append(image)\n\n        self.labels.extend(tiled_labels)\n        self.is_real.extend(tiled_is_real)\n        self.loaded_images.extend(tiled_loaded_images)\n\n\n    def tile(self, image, n):\n        \'\'\' Tiles images taking their aspect ratios into account. \'\'\'\n\n        aspect_ratio = image.shape[1] / image.shape[0]\n        if aspect_ratio < 1:\n            w_repeats = n\n            h_repeats = max(1 if n != self.params[\'max_tiling\'] else 2, int(n * aspect_ratio))\n        else:\n            h_repeats = n\n            w_repeats = max(1 if n != self.params[\'max_tiling\'] else 2, int(n / aspect_ratio))\n\n        image = np.tile(image, (h_repeats, w_repeats, 1))\n\n        fit_size = self.input_size * 3\n        if image.shape[0] > fit_size or image.shape[1] > fit_size:\n            image = fit_to_max_size(image, self.input_size * 3)\n\n        return image\n\n    def sample_index(self):\n        \'\'\' Samples indexes. \'\'\'\n\n        choices = list(range(len(self.labels)))\n        if self.params[\'weighted_sampling\']:\n            choices = np.random.choice(choices, len(self.labels), p=self.probs)\n        elif self.params[\'shuffle\']:\n            np.random.shuffle(choices)\n\n        # duplication is required for triplet loss at least.\n        duplicated_choices = []\n        for choice in choices:\n            for _ in range(self.params[\'duplicate_n_times\']):\n                duplicated_choices.append(int(\n                    np.random.choice(\n                        self.images_indexes_per_class[self.labels[choice]],\n                        1)))\n\n        for choice in duplicated_choices:\n            yield [choice]\n\n    def read(self, index):\n        \'\'\' Reads an image from RAM or disk and returns it with corresponding class label. \'\'\'\n\n        if self.params[\'preload\']:\n            image = self.loaded_images[index[0]].astype(np.float32)\n        else:\n            image = cv2.imread(self.images_paths[index[0]]).astype(np.float32)\n\n        if not self.params[\'pretile\'] and not self.is_real[index[0]]:\n            n = random.randint(1, self.params[\'max_tiling\'])\n            image = self.tile(image, n)\n\n        return image, self.labels[index[0]]\n\n    def cv2_rotate(self, image):\n        \'\'\' Rotates images on random angle using opencv. \'\'\'\n\n        c_xy = image.shape[1] / 2, image.shape[0] / 2\n        angle = random.uniform(-self.params[\'add_rot_angle\'],\n                               self.params[\'add_rot_angle\']) * 57.2958\n\n        if self.params[\'rot90\']:\n            angle += random.randint(0, 3) * 180\n\n        rotation_matrix = cv2.getRotationMatrix2D(c_xy, angle, 1)\n        img_rotation = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n        return img_rotation\n\n    def cv2_noise_and_blur(self, image):\n        \'\'\' Adds noise making image darker and blur.\'\'\'\n\n        image = image.astype(np.float32)\n\n        if self.params[\'apply_gray_noise\'] and np.random.choice([True, False]):\n            image = gray_noise(image)\n\n        if self.params[\'blur\'] and np.random.choice([True, False]):\n            image = blur(image)\n\n        return image\n\n    def train_preprocess(self, choice):\n        \'\'\' Applies training preprocessing. \'\'\'\n\n        original, label = tf.numpy_function(self.read, [choice], [tf.float32, tf.int64])\n        image = tf_random_crop_and_resize(original, self.input_size)\n        image, = tf.numpy_function(self.cv2_noise_and_blur, [image], [tf.float32])\n        if self.params[\'horizontal_flip\']:\n            image = tf.image.random_flip_left_right(image)\n        if self.params[\'vertical_flip\']:\n            image = tf.image.random_flip_up_down(image)\n        if self.params[\'add_rot_angle\'] > 0 or self.params[\'rot90\']:\n            image, = tf.numpy_function(self.cv2_rotate, [image], [tf.float32])\n        image = tf_distort_color(image)\n        image = preproces_image(image)\n        if self.return_original:\n            return image, label, original\n        return image, label\n\n    def __call__(self, *args, **kwargs):\n        \'\'\' Returns tf.data.Dataset instance as well as number of classes in training set. \'\'\'\n\n        dataset = tf.data.Dataset.from_generator(self.sample_index, (tf.int32),\n                                                 (tf.TensorShape([1])))\n        dataset = dataset.map(self.train_preprocess,\n                              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        if not self.return_original:\n            dataset = dataset.batch(self.batch_size, drop_remainder=True)\n            dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n            dataset = dataset.repeat()\n\n        return dataset, len(set(self.labels))\n\n    @staticmethod\n    def create_from_list(path, input_size, batch_size, params, return_original=False):\n        \'\'\' Creates Dataset instance from path to images list.\n            Images list has following format:\n            <relative_path_to_image> <class_label>\n        \'\'\'\n\n        impaths, labels, is_real, _ = from_list(path)\n\n        return Dataset(impaths, labels, is_real, input_size, batch_size, params, return_original)()\n\n    @staticmethod\n    def reassign_labels(labels):\n        \'\'\' Re-assign class labels so that they starts from 0 and ends with (num_classes - 1). \'\'\'\n\n        unique_labels = list(set(labels))\n        return [unique_labels.index(l) for l in labels]\n\n\ndef main():\n    import argparse\n    import time\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'--gallery\', required=True)\n    args.add_argument(\'--input_size\', default=224, type=int)\n    args.add_argument(\'--augmentation_config\', required=True)\n    args = args.parse_args()\n\n    with open(args.augmentation_config) as f:\n        augmentation_config = json.load(f)\n\n    dataset, _ = Dataset.create_from_list(args.gallery, args.input_size, 1,\n                                          augmentation_config, True)\n\n    t = time.time()\n    for preprocessed, label, original in dataset.take(1000):\n        cv2.imshow(\'preprocessed\', depreprocess_image(preprocessed.numpy()))\n        cv2.imshow(\'original\', original.numpy().astype(np.uint8))\n        print(label)\n        if cv2.waitKey(0) == 27:\n            break\n    print(time.time() - t)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/image_retrieval/image_retrieval/frames_provider.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\n\nimport cv2\n\n\nclass FramesProvider:\n    def __init__(self, images_list_path):\n        self.impaths = []\n        self.probe_classes = []\n\n        with open(images_list_path) as f:\n            content = [line.strip().split() for line in f.readlines() if line.strip()]\n\n        root = os.path.dirname(images_list_path)\n\n        for impath, label in content:\n            self.impaths.append(os.path.join(root, impath))\n            self.probe_classes.append(int(label))\n\n    def __iter__(self):\n        for impath, probe_class in zip(self.impaths, self.probe_classes):\n            image = cv2.imread(impath)\n            yield image, probe_class\n'"
tensorflow_toolkit/image_retrieval/image_retrieval/image_retrieval.py,2,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom tqdm import tqdm\n\nimport numpy as np\n\nimport cv2\nfrom sklearn.metrics.pairwise import cosine_distances\n\nfrom image_retrieval.common import from_list, preproces_image\n\ndef nothing(image):\n    return image\n\n\nclass ImageRetrieval:\n\n    def __init__(self, model_path, model_backend, model, gallery_path, input_size, cpu_extensions,\n                 multiple_images_per_label=False):\n        self.impaths, self.gallery_classes, _, self.text_label_to_class_id = from_list(\n            gallery_path, multiple_images_per_label)\n\n        self.input_size = input_size\n\n        self.preprocess = preproces_image\n\n        if model is None or isinstance(model, str):\n            if model_backend == \'tf\':\n                import tensorflow as tf\n                from image_retrieval.model import keras_applications_mobilenetv2, \\\n                    keras_applications_resnet50\n\n                if model == \'resnet50\':\n                    self.model = keras_applications_resnet50(\n                        tf.keras.layers.Input(shape=(input_size, input_size, 3)))\n                if model == \'mobilenet_v2\':\n                    self.model = keras_applications_mobilenetv2(\n                        tf.keras.layers.Input(shape=(input_size, input_size, 3)))\n\n                self.model.load_weights(model_path)\n            else:\n                from openvino.inference_engine import IENetwork, IECore\n                class IEModel():\n\n                    def __init__(self, model_path):\n                        ie = IECore()\n                        if cpu_extensions:\n                            ie.add_extension(cpu_extensions, \'CPU\')\n\n                        path = \'.\'.join(model_path.split(\'.\')[:-1])\n                        self.net = IENetwork(model=path + \'.xml\', weights=path + \'.bin\')\n                        self.exec_net = ie.load_network(network=self.net, device_name=\'CPU\')\n\n                    def predict(self, image):\n                        assert len(image.shape) == 4\n\n                        image = np.transpose(image, (0, 3, 1, 2))\n                        out = self.exec_net.infer(inputs={\'Placeholder\': image})[\n                            \'model/tf_op_layer_mul/mul/Normalize\']\n\n                        return out\n\n                self.model = IEModel(model_path)\n                self.preprocess = nothing\n        else:\n            self.model = model\n\n        self.embeddings = self.compute_gallery_embeddings()\n\n    def compute_embedding(self, image):\n        image = cv2.resize(image, (self.input_size, self.input_size))\n        image = self.preprocess(image)\n        image = np.expand_dims(image, axis=0)\n        embedding = self.model.predict(image)\n        return embedding\n\n    def search_in_gallery(self, embedding):\n        distances = cosine_distances(embedding, self.embeddings).reshape([-1])\n        sorted_indexes = np.argsort(distances)\n        return sorted_indexes, distances\n\n    def compute_gallery_embeddings(self):\n        images = []\n\n        for full_path in tqdm(self.impaths, desc=\'Reading gallery images.\'):\n            image = cv2.imread(full_path)\n            if image is None:\n                print(""ERROR: cannot find image, full_path ="", full_path)\n            image = cv2.resize(image, (self.input_size, self.input_size))\n            image = self.preprocess(image)\n            image = np.expand_dims(image, axis=0)\n            images.append(image)\n\n        embeddings = [None for _ in self.impaths]\n\n        index = 0\n        for image in tqdm(images, desc=\'Computing embeddings of gallery images.\'):\n            embeddings[index] = self.model.predict(image).reshape([-1])\n            index += 1\n\n        return embeddings\n'"
tensorflow_toolkit/image_retrieval/image_retrieval/losses.py,9,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\n\n# pylint: disable=abstract-method\nclass AMSoftmaxLogits(tf.keras.layers.Layer):\n    def __init__(self, units, **kwargs):\n        super(AMSoftmaxLogits, self).__init__(**kwargs)\n        self.units = units\n        self.kernel = None\n\n    def build(self, input_shape):\n        # pylint: disable=no-value-for-parameter\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                      initializer=tf.keras.initializers.get(\'glorot_uniform\'))\n\n    # pylint: disable=arguments-differ\n    def call(self, inputs):\n        kernel = tf.nn.l2_normalize(self.kernel, axis=0, epsilon=1e-13)\n        inputs = tf.nn.l2_normalize(inputs, axis=1, epsilon=1e-13)\n\n        cos_theta = K.dot(inputs, kernel)\n        cos_theta = K.clip(cos_theta, -1.0, 1.0)\n\n        return tf.identity(cos_theta)\n\n\ndef am_softmax_loss(num_classes, s, m):\n    if s is None:\n        s = math.sqrt(2) * math.log(num_classes - 1)\n\n    def loss(y_true, cos_theta):\n        phi_theta = cos_theta - m\n        y_true_reshaped = tf.reshape(y_true, (-1,))\n        y_true_one_hot = tf.one_hot(tf.cast(y_true_reshaped, tf.int32), num_classes)\n\n        output = tf.where(tf.cast(y_true_one_hot, tf.bool), phi_theta, cos_theta)\n        output = output * s\n\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true, output)\n\n        return loss\n\n    return loss\n\n\ndef triplet_loss(margin):\n    def loss(labels, embeddings):\n        from tensorflow_addons.losses import triplet_semihard_loss\n        return triplet_semihard_loss(labels, embeddings, margin=margin)\n\n    return loss\n'"
tensorflow_toolkit/image_retrieval/image_retrieval/metrics.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport collections\n\nimport numpy as np\n\nfrom image_retrieval.frames_provider import FramesProvider\nfrom image_retrieval.image_retrieval import ImageRetrieval\n\n\ndef test_model(model_path, model_backend, model, gallery_path, test_images, input_size,\n               cpu_extension=None):\n    img_retrieval = ImageRetrieval(model_path, model_backend, model, gallery_path, input_size,\n                                   cpu_extension)\n    frames = FramesProvider(test_images)\n\n    top1_counters = []\n    top5_counters = []\n    top10_counters = []\n    mean_positions = []\n\n    results = collections.defaultdict(list)\n\n    for image, probe_class in frames:\n        if image is not None:\n            probe_embedding = img_retrieval.compute_embedding(image)\n\n            sorted_indexes, _ = img_retrieval.search_in_gallery(probe_embedding)\n\n            sorted_classes = [img_retrieval.gallery_classes[i] for i in sorted_indexes]\n            position = sorted_classes.index(probe_class)\n            results[probe_class].append(position)\n\n    global_top1 = 0\n    global_counter = 0\n    for probe_class in sorted(results.keys()):\n        top1, top5, top10 = 0, 0, 0\n        for p in results[probe_class]:\n            if p < 1:\n                top1 += 1\n                global_top1 += 1\n            if p < 5:\n                top5 += 1\n            if p < 10:\n                top10 += 1\n\n            global_counter += 1\n\n        top1 /= len(results[probe_class])\n        top5 /= len(results[probe_class])\n        top10 /= len(results[probe_class])\n        mean_position = np.mean(results[probe_class])\n\n        print(\'{0}\\t{1:4.2f}\\t{2:4.2f}\\t{3:4.2f}\\t{4:4.2f}\'.format(probe_class, top1, top5, top10,\n                                                                   mean_position))\n\n        top1_counters.append(top1)\n        top5_counters.append(top5)\n        top10_counters.append(top10)\n\n        mean_positions.append(mean_position)\n\n    print(\n        \'AVERAGE: top1: {0:4.3f}    top5: {1:4.3f}    top10: {2:4.3f}    mean_index: {3:4.3f}\'.format(\n            np.mean(top1_counters),\n            np.mean(top5_counters),\n            np.mean(top10_counters),\n            np.mean(mean_positions)\n        ))\n\n    print(\'AVERAGE top1 over all queries: {0:6.3f}\'.format(global_top1 / global_counter))\n\n    return np.mean(top1_counters), np.mean(top5_counters), np.mean(top10_counters), np.mean(\n        mean_positions)\n'"
tensorflow_toolkit/image_retrieval/image_retrieval/model.py,8,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport tensorflow as tf\n\nEMBEDDINGS_DIM = 256\nWEIGHT_DECAY = 0.0001\n\n\ndef l2_normalized_embeddings(inputs):\n    output = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n    output = tf.reshape(output, [-1, 1, 1, output.shape[-1]])\n    output = tf.keras.layers.Conv2D(\n        filters=EMBEDDINGS_DIM, kernel_size=1, padding=\'same\',\n        kernel_regularizer=tf.keras.regularizers.l2(l=WEIGHT_DECAY))(output)\n\n    output = tf.reshape(output, [-1, EMBEDDINGS_DIM])\n    output = tf.nn.l2_normalize(output * 1000, axis=1, epsilon=1e-13)\n    return output\n\n\ndef keras_applications_mobilenetv2(inputs, alpha=1.0):\n    from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n\n    base_model = MobileNetV2(alpha=alpha, include_top=False,\n                             weights=\'imagenet\', input_tensor=inputs)\n\n    output = l2_normalized_embeddings(base_model.output)\n    return tf.keras.Model(inputs, output)\n\n\ndef keras_applications_resnet50(inputs):\n    from tensorflow.keras.applications.resnet50 import ResNet50\n\n    base_model = ResNet50(include_top=False, weights=\'imagenet\', input_tensor=inputs)\n    output = l2_normalized_embeddings(base_model.output)\n\n    return tf.keras.Model(inputs, output)\n'"
tensorflow_toolkit/image_retrieval/tools/export.py,12,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.python.tools.freeze_graph import freeze_graph\n\nfrom image_retrieval.model import keras_applications_mobilenetv2, keras_applications_resnet50\n\ntf.compat.v1.disable_v2_behavior()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--model_weights\', required=True, help=\'Path to model weights.\')\n    parser.add_argument(\'--input_size\', default=224, type=int, help=\'Input image size.\')\n    parser.add_argument(\'--model\', choices=[\'resnet50\', \'mobilenet_v2\'], required=True)\n    parser.add_argument(\'--data_type\', default=\'FP32\', choices=[\'FP32\', \'FP16\'],\n                        help=\'Data type of IR\')\n    parser.add_argument(\'--output_dir\', default=None, help=\'Output Directory\')\n    return parser.parse_args()\n\n\ndef print_flops(graph):\n    """""" Prints information about FLOPs. """"""\n\n    with graph.as_default():\n        flops = tf.compat.v1.profiler.profile(\n            graph, options=tf.compat.v1.profiler.ProfileOptionBuilder.float_operation())\n        print(\'\')\n        if flops.total_float_ops > 10 ** 9:\n            print(\'Operations number: {} GFlops\'.format(flops.total_float_ops / 10 ** 9))\n        elif flops.total_float_ops > 10 ** 6:\n            print(\'Operations number: {} MFlops\'.format(flops.total_float_ops / 10 ** 6))\n        elif flops.total_float_ops > 10 ** 3:\n            print(\'Operations number: {} KFlops\'.format(flops.total_float_ops / 10 ** 3))\n\n    return flops\n\n\ndef load_frozen_graph(frozen_graph_filename):\n    """""" Loads and returns frozen graph. """"""\n\n    with tf.io.gfile.GFile(frozen_graph_filename, ""rb"") as f:\n        graph_def = tf.compat.v1.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def, name=\'\')\n\n    return graph\n\n\ndef main():\n    args = parse_args()\n\n    input_tensor = tf.compat.v1.placeholder(dtype=tf.float32,\n                                            shape=[1, args.input_size, args.input_size, 3])\n    if args.model == \'resnet50\':\n        model = keras_applications_resnet50(\n            tf.keras.layers.Input(tensor=input_tensor))\n    elif args.model == \'mobilenet_v2\':\n        model = keras_applications_mobilenetv2(\n            tf.keras.layers.Input(tensor=input_tensor))\n    else:\n        raise Exception(\'unknown model\')\n\n    embedding = model(input_tensor, training=False)\n\n    with tf.compat.v1.Session() as sess:\n        model.load_weights(args.model_weights)\n        model_dir = os.path.dirname(args.model_weights)\n        export_folder = args.output_dir if args.output_dir else os.path.join(model_dir, \'export\')\n\n        tf.compat.v1.saved_model.simple_save(sess, export_folder,\n                                             inputs={\'input\': input_tensor},\n                                             outputs={embedding.name[:-2]: embedding})\n\n        frozen_graph_path = os.path.join(export_folder, \'frozen_graph.pb\')\n\n        freeze_graph(\n            input_graph=None,\n            input_saver=\'\',\n            input_binary=True,\n            input_checkpoint=\'\',\n            output_node_names=embedding.name[:-2],\n            restore_op_name=\'save/restore_all\',\n            filename_tensor_name=\'save/Const:0\',\n            output_graph=frozen_graph_path,\n            clear_devices=True,\n            initializer_nodes=\'\',\n            input_meta_graph=None,\n            input_saved_model_dir=export_folder,\n        )\n\n        graph = load_frozen_graph(frozen_graph_path)\n        print_flops(graph)\n\n        print(""Path to frozen graph: %s"" % frozen_graph_path)\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/image_retrieval/tools/test.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport logging as log\n\nfrom image_retrieval.metrics import test_model\n\n\ndef parse_args():\n    args = argparse.ArgumentParser()\n    args.add_argument(\'--model_weights\', required=True, help=\'Path to model weights.\')\n    args.add_argument(\'--gallery\', required=True, help=\'Gallery images list.\')\n    args.add_argument(\'--test_images\', required=True, help=\'Test images list.\')\n    args.add_argument(\'--input_size\', default=224, type=int, help=\'Input image size.\')\n    args.add_argument(\'--model\', choices=[\'resnet50\', \'mobilenet_v2\'], default=\'mobilenet_v2\')\n    args.add_argument(\'--ie\', choices=[\'tf\', \'ie\'], required=True)\n    args.add_argument(\'--cpu_extension\', help=\'Path to lib cpu extensions.\')\n\n    return args.parse_args()\n\n\ndef main():\n    LOG_FORMAT = \'%(levelno)s|%(asctime)s|%(filename)s:%(lineno)d|%(funcName)s|%(message)s\'\n    log.basicConfig(format=LOG_FORMAT, datefmt=""%Y-%m-%d %H:%M:%S"")\n    log.getLogger().setLevel(log.WARN)\n\n    args = parse_args()\n\n    test_model(model_path=args.model_weights,\n               model_backend=args.ie,\n               model=args.model,\n               gallery_path=args.gallery,\n               test_images=args.test_images,\n               input_size=args.input_size,\n               cpu_extension=args.cpu_extension)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/image_retrieval/tools/train.py,19,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport json\nimport os\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom pygit2 import Repository\nfrom sklearn.metrics.pairwise import cosine_distances\n\nfrom image_retrieval.dataset import Dataset, depreprocess_image\nfrom image_retrieval.losses import am_softmax_loss, triplet_loss, AMSoftmaxLogits\nfrom image_retrieval.metrics import test_model\nfrom image_retrieval.model import keras_applications_mobilenetv2, keras_applications_resnet50\n\n\ndef parse_args():\n    args = argparse.ArgumentParser()\n    args.add_argument(\'--gallery\', required=True, help=\'Gallery images list.\')\n    args.add_argument(\'--test_gallery\', help=\'Test gallery images list.\')\n    args.add_argument(\'--test_images\', help=\'Test images list.\')\n    args.add_argument(\'--input_size\', type=int, default=224, help=\'Input image size.\')\n    args.add_argument(\'--train_dir\', required=True, help=\'Training folder.\')\n    args.add_argument(\'--model_weights\', required=False, help=\'Path to model weights.\')\n    args.add_argument(\'--steps_per_epoch\', default=10000, type=int)\n    args.add_argument(\'--loss\', required=True)\n    args.add_argument(\'--model\', choices=[\'resnet50\', \'mobilenet_v2\'], required=True)\n    args.add_argument(\'--max_iters\', default=400000, type=int)\n    args.add_argument(\'--lr_init\', type=float, default=0.001)\n    args.add_argument(\'--lr_drop_value\', type=float, default=0.1)\n    args.add_argument(\'--lr_drop_step\', type=int, default=100000)\n    args.add_argument(\'--dump_hard_examples\', action=\'store_true\')\n    args.add_argument(\'--batch_size\', type=int, default=256)\n    args.add_argument(\'--augmentation_config\', required=True)\n\n    return args.parse_args()\n\n\ndef latest_checkpoint_number(train_dir):\n    latest_checkpoint = tf.train.latest_checkpoint(train_dir)\n    if latest_checkpoint:\n        return int(latest_checkpoint.split(\'-\')[-1])\n    return -1\n\n\ndef collect_hard_images(images, labels, distances, indices, positive):\n    hard_examples = []\n\n    already_in = set()\n\n    for pair in indices:\n        if positive:\n            if labels[pair[0]] != labels[pair[1]]:\n                continue\n        else:\n            if labels[pair[0]] == labels[pair[1]]:\n                continue\n\n        if (pair[0], pair[1]) in already_in or (pair[1], pair[0]) in already_in:\n            continue\n        else:\n            already_in.add((pair[0], pair[1]))\n\n        concatenated = np.concatenate((images[pair[0]], images[pair[1]]), axis=1)\n\n        header = np.zeros((50, concatenated.shape[1], 3))\n\n        text = str(labels[pair[0]]) + \'-\' + str(labels[pair[1]]) + \': \' + str(\n            distances[pair[0], pair[1]])\n\n        cv2.putText(header, text, (0, 50), 1, 2.0, (255, 255, 255), 2)\n\n        concatenated = np.concatenate((header, concatenated), axis=0)\n\n        concatenated = concatenated / 255.0\n\n        hard_examples.append(concatenated)\n\n        if len(hard_examples) == 10:\n            break\n\n    hard_examples = np.array(hard_examples)\n\n    return hard_examples\n\n\ndef greatest_loss(images, labels, embeddings):\n    arr = cosine_distances(embeddings, embeddings)\n    args_max = np.dstack(np.unravel_index(np.argsort(-arr.ravel()),\n                                          (embeddings.shape[0], embeddings.shape[0])))[0]\n\n    args_min = args_max[::-1]\n\n    np_images = depreprocess_image(images.numpy())[:, :, :, ::-1]\n    np_labels = labels.numpy()\n\n    hard_positives = collect_hard_images(np_images, np_labels, arr, args_max, True)\n    hard_negatives = collect_hard_images(np_images, np_labels, arr, args_min, False)\n\n    return hard_positives, hard_negatives\n\n\ndef collect_hard_examples(model, dataset, dir):\n    embeddings_folder = os.path.join(dir, \'embs\')\n    os.makedirs(embeddings_folder, exist_ok=True)\n    for x, y in dataset.take(1):\n        predicted_embeddings = model.predict(x)\n        return greatest_loss(x, y, predicted_embeddings)\n\n\ndef save_args(args, path):\n    with open(path, \'w\') as f:\n        json.dump(args.__dict__, f)\n\n\ndef save_git_info(path):\n    repo = Repository(os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../../..""))\n    info = {\n        ""branch"": repo.head.shorthand,\n        ""commit_hex"": repo.revparse_single(\'HEAD\').hex,\n        ""commit_message"": repo.revparse_single(\'HEAD\').message,\n    }\n\n    with open(path, \'w\') as f:\n        json.dump(info, f)\n\n\n# pylint: disable=R0912,R0915\ndef main():\n    args = parse_args()\n    if args.model == \'resnet50\':\n        model = keras_applications_resnet50(\n            tf.keras.layers.Input(shape=(args.input_size, args.input_size, 3)))\n    elif args.model == \'mobilenet_v2\':\n        model = keras_applications_mobilenetv2(\n            tf.keras.layers.Input(shape=(args.input_size, args.input_size, 3)))\n    else:\n        raise Exception(\'unknown model\')\n\n    with open(args.augmentation_config) as f:\n        augmentation_config = json.load(f)\n\n    dataset, num_classes = Dataset.create_from_list(args.gallery, args.input_size, args.batch_size,\n                                                    augmentation_config)\n\n    if args.model_weights:\n        model.load_weights(args.model_weights)\n    elif os.path.exists(args.train_dir):\n        latest_checkpoint = tf.train.latest_checkpoint(args.train_dir)\n        if latest_checkpoint:\n            model.load_weights(latest_checkpoint)\n            print(\'loaded\', latest_checkpoint)\n    else:\n        os.makedirs(args.train_dir)\n\n    save_args(args, os.path.join(args.train_dir, \'args.json\'))\n    save_git_info(os.path.join(args.train_dir, \'git_info.json\'))\n\n    if args.loss.startswith(\'amsoftmax\'):\n        _, s, m = args.loss.split(\'_\')\n        s, m = float(s), float(m)\n        print(s, m)\n        loss_function = am_softmax_loss(num_classes, s, m)\n        training_model = tf.keras.Sequential([\n            model,\n            AMSoftmaxLogits(num_classes)\n        ])\n    elif args.loss.startswith(\'triplet\'):\n        _, margin = args.loss.split(\'_\')\n        margin = float(margin)\n        loss_function = triplet_loss(margin=margin)\n        training_model = model\n    else:\n        raise Exception(\'unknown loss\')\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        args.lr_init,\n        decay_steps=args.lr_drop_step,\n        decay_rate=args.lr_drop_value,\n        staircase=True)\n\n    training_model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(lr_schedule))\n    if args.model_weights:\n        training_model.optimizer.iterations.assign(0)\n\n    with tf.summary.create_file_writer(args.train_dir + ""/logs"").as_default():\n        while True:\n            cur_step = training_model.optimizer.iterations.numpy()\n            print(\'cur_step\', cur_step)\n            lr = training_model.optimizer.lr(cur_step).numpy()\n            print(\'lr\', lr)\n\n            history = training_model.fit(dataset, steps_per_epoch=args.steps_per_epoch)\n\n            cur_step = training_model.optimizer.iterations.numpy()\n            lr = training_model.optimizer.lr(cur_step).numpy()\n\n            # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n            tf.summary.scalar(\'training/loss\', data=history.history[\'loss\'][-1], step=cur_step)\n            tf.summary.scalar(\'training/lr\', data=lr, step=cur_step)\n            tf.summary.scalar(\'training/batch_size\', data=args.batch_size, step=cur_step)\n\n            save_to = os.path.join(args.train_dir, \'weights-{}\'.format(cur_step))\n            model.save_weights(save_to)\n\n            print(\'Saved: {}\'.format(save_to))\n\n            if args.dump_hard_examples:\n                hard_positives, hard_negatives = collect_hard_examples(model, dataset,\n                                                                       args.train_dir)\n\n                hard_positives = tf.convert_to_tensor(hard_positives)\n                hard_negatives = tf.convert_to_tensor(hard_negatives)\n\n                # pylint: disable=redundant-keyword-arg\n                tf.summary.image(\'hard_positives\', hard_positives, cur_step, max_outputs=10)\n                tf.summary.image(\'hard_negatives\', hard_negatives, cur_step, max_outputs=10)\n\n            if args.test_images:\n                if args.test_gallery:\n                    gallery = args.test_gallery\n                else:\n                    gallery = args.gallery\n\n                top1, top5, top10, mean_pos = test_model(model_path=None, model_backend=None,\n                                                         model=model,\n                                                         gallery_path=gallery,\n                                                         test_images=args.test_images,\n                                                         input_size=args.input_size)\n\n                tf.summary.scalar(\'test/top1\', data=top1, step=cur_step)\n                tf.summary.scalar(\'test/top5\', data=top5, step=cur_step)\n                tf.summary.scalar(\'test/top10\', data=top10, step=cur_step)\n                tf.summary.scalar(\'test/mean_pos\', data=mean_pos, step=cur_step)\n\n            if cur_step > args.max_iters:\n                break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/lpr/chinese_lp/config.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nfrom lpr.trainer import LPRVocab\n\ninput_shape = (24, 94, 3)  # (height, width, channels)\nuse_h_concat = False\nuse_oi_concat = False\nmax_lp_length = 20\nrnn_cells_num = 128\n\n# Licens plate patterns\nlpr_patterns = [\n  \'^<[^>]*>[A-Z][0-9A-Z]{5}$\',\n  \'^<[^>]*>[A-Z][0-9A-Z][0-9]{3}<police>$\',\n  \'^<[^>]*>[A-Z][0-9A-Z]{4}<[^>]*>$\',  # <Guangdong>, <Hebei>\n  \'^WJ<[^>]*>[0-9]{4}[0-9A-Z]$\',\n]\n\n# Path to the folder where all training and evaluation artifacts will be located\nmodel_dir = os.path.realpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), \'..\', \'model\'))\nif not os.path.exists(model_dir):\n  os.makedirs(model_dir)\n\n\nclass train:\n  # Path to annotation file with training data in per line format: <path_to_image_with_license_plate label>\n  file_list_path = \'../../data/synthetic_chinese_license_plates/Synthetic_Chinese_License_Plates/train\'\n\n  batch_size = 32\n  steps = 250000\n  learning_rate = 0.001\n  grad_noise_scale = 0.001\n  opt_type = \'Adam\'\n\n  save_checkpoints_steps = 1000      # Number of training steps when checkpoint should be saved\n  display_iter = 100\n\n  apply_basic_aug = False\n  apply_stn_aug = True\n  apply_blur_aug = False\n\n  need_to_save_weights = True\n  need_to_save_log = True\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""  # Environment variable to control CUDA device used for training\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n\nclass eval:\n  # Path to annotation file with validation data in per line format: <path_to_image_with_license_plate label>\n  file_list_path = \'../../data/synthetic_chinese_license_plates/Synthetic_Chinese_License_Plates/val\'\n  checkpoint = \'\'\n  batch_size = 1\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""  # Environment variable to control CUDA device used for training\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n\nclass infer:\n  # Path to text file with list of images in per line format: <path_to_image_with_license_plate>\n  file_list_path = \'../../data/synthetic_chinese_license_plates/Synthetic_Chinese_License_Plates/test_infer\'\n  checkpoint = \'\'\n  batch_size = 1\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""  # Environment variable to control CUDA device used for training\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n\nvocab, r_vocab, num_classes = LPRVocab.create_vocab(train.file_list_path,\n                                                    eval.file_list_path,\n                                                    use_h_concat,\n                                                    use_oi_concat)\n'"
tensorflow_toolkit/lpr/chinese_lp/config_test.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nfrom lpr.trainer import LPRVocab\n\ninput_shape = (24, 94, 3)  # (height, width, channels)\nuse_h_concat = False\nuse_oi_concat = False\nmax_lp_length = 20\nrnn_cells_num = 128\n\n# Licens plate patterns\nlpr_patterns = [\n  \'^<[^>]*>[A-Z][0-9A-Z]{5}$\',\n  \'^<[^>]*>[A-Z][0-9A-Z][0-9]{3}<police>$\',\n  \'^<[^>]*>[A-Z][0-9A-Z]{4}<[^>]*>$\',  # <Guangdong>, <Hebei>\n  \'^WJ<[^>]*>[0-9]{4}[0-9A-Z]$\',\n]\n\n# Path to the folder where all training and evaluation artifacts will be located\nmodel_dir = os.path.realpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), \'..\', \'model\'))\nif not os.path.exists(model_dir):\n  os.makedirs(model_dir)\n\n\nclass train:\n  # Path to annotation file with training data in per line format: <path_to_image_with_license_plate label>\n  file_list_path = \'../../data/synthetic_chinese_license_plates/Synthetic_Chinese_License_Plates_Mini/train\'\n\n  batch_size = 32\n  steps = 1000\n  learning_rate = 0.001\n  grad_noise_scale = 0.001\n  opt_type = \'Adam\'\n\n  save_checkpoints_steps = 500      # Number of training steps when checkpoint should be saved\n  display_iter = 10\n\n  apply_basic_aug = False\n  apply_stn_aug = True\n  apply_blur_aug = False\n\n  need_to_save_weights = True\n  need_to_save_log = True\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""  # Environment variable to control CUDA device used for training\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n\nclass eval:\n  # Path to annotation file with validation data in per line format: <path_to_image_with_license_plate label>\n  file_list_path = \'../../data/synthetic_chinese_license_plates/Synthetic_Chinese_License_Plates_Mini/val\'\n  checkpoint = \'\'\n  batch_size = 1\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""  # Environment variable to control CUDA device used for training\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n\nclass infer:\n  # Path to text file with list of images in per line format: <path_to_image_with_license_plate>\n  file_list_path = \'../../data/synthetic_chinese_license_plates/Synthetic_Chinese_License_Plates_Mini/test_infer\'\n  checkpoint = \'\'\n  batch_size = 1\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""  # Environment variable to control CUDA device used for training\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n\nvocab, r_vocab, num_classes = LPRVocab.create_vocab(train.file_list_path,\n                                                    eval.file_list_path,\n                                                    use_h_concat,\n                                                    use_oi_concat)\n'"
tensorflow_toolkit/lpr/lpr/__init__.py,0,b'from tfutils.helpers import import_transformer\n\nimport_transformer()\n'
tensorflow_toolkit/lpr/lpr/trainer.py,50,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport re\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom lpr.networks.lprnet import LPRNet\nfrom spatial_transformer import transformer\n\n\nclass InputData:\n  # pylint: disable=too-many-arguments\n  def __init__(self, batch_size, input_shape, file_list_path,\n               apply_basic_aug=False, apply_stn_aug=True, apply_blur_aug=False):\n    self.batch_size = batch_size\n    self.input_shape = input_shape\n    self.file_list_path = file_list_path\n    self.apply_basic_aug = apply_basic_aug\n    self.apply_stn_aug = apply_stn_aug\n    self.apply_blur_aug = apply_blur_aug\n\n  def input_fn(self):\n    file_src = tf.train.string_input_producer([self.file_list_path])\n    image, label = read_data(self.batch_size, self.input_shape, file_src)\n\n    if self.apply_basic_aug:\n      image = augment(image)\n\n    if self.apply_stn_aug:\n      image = augment_with_stn(image)\n\n    # blur/sharpen augmentation\n    if self.apply_blur_aug:\n      data, = tf.py_func(random_blur, [image], [tf.float32])\n      data.set_shape([self.batch_size] + list(self.input_shape))  # [batch_size, height, width, channels_num]\n    else:\n      data = image\n\n    return data, label\n\n\ndef read_data(batch_size, input_shape, file_src):\n  reader = tf.TextLineReader()\n  _, value = reader.read(file_src)\n  filename, label = tf.decode_csv(value, [[\'\'], [\'\']], \' \')\n  image_file = tf.read_file(filename)\n\n  height, width, channels_num = input_shape\n  rgb_image = tf.image.decode_png(image_file, channels=channels_num)\n  rgb_image_float = tf.image.convert_image_dtype(rgb_image, tf.float32)\n  resized_images = tf.image.resize_images(rgb_image_float, [height, width])\n  resized_images.set_shape(input_shape)\n\n  min_after_dequeue = 30000\n  capacity = min_after_dequeue + 3 * batch_size\n  image_batch, label_batch = tf.train.shuffle_batch([resized_images, label], batch_size=batch_size, capacity=capacity,\n                                                    min_after_dequeue=min_after_dequeue)\n  return image_batch, label_batch\n\n\n# Function for basic image augmentation - photometric distortions\ndef augment(images):\n  augmented = tf.image.random_brightness(images, max_delta=0.2)\n  augmented = tf.image.random_contrast(augmented, lower=0.8, upper=1.2)\n  augmented = tf.add(augmented, tf.truncated_normal(tf.shape(augmented), stddev=0.02))\n  return augmented\n\n\n# Function for STN image augmentation - geometric transformations with STN\ndef augment_with_stn(images):\n  identity = identity_transform(images)\n  noise = tf.truncated_normal(identity.get_shape(), stddev=0.1)\n  # curriculum_rate = tf.clip_by_value(0.0001 * tf.cast(global_step, tf.float32), 0.0, 1.0)\n  # noise = tf.scalar_mul(curriculum_rate, noise)\n  return apply_stn(images, tf.add(identity, noise))\n\n\n# Function for identity transformation\ndef identity_transform(images):\n  shape = images.get_shape()\n  ident = tf.constant(np.array([[[1., 0, 0], [0, 1., 0]]]).astype(\'float32\'))\n  return tf.tile(ident, [shape[0].value, 1, 1])\n\n\n# Function wrapper for STN application\ndef apply_stn(images, transform_params):\n  shape = images.get_shape()\n  out_size = (shape[1], shape[2])\n  warped = transformer(images, transform_params, out_size)\n  warped.set_shape(shape)\n  return warped\n\n\ndef random_blur(images):\n  result = []\n  for k in range(images.shape[0]):\n    samples = np.random.normal(scale=1.)\n    kernel = np.array([[0., samples, 0.], [samples, 1. - 4. * samples, samples], [0., samples, 0.]])\n    result.append(cv2.filter2D(images[k], -1, kernel).astype(np.float32))\n  return np.array(result)\n\n\n# Function for construction whole network\ndef inference(rnn_cells_num, input, num_classes):\n  cnn = LPRNet.lprnet(input)\n\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      normalizer_fn=slim.batch_norm,\n                      weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n                      weights_regularizer=slim.l2_regularizer(0.0005)):\n    classes = slim.conv2d(cnn, num_classes, [1, 13])\n    pattern = slim.fully_connected(slim.flatten(classes), rnn_cells_num)  # patterns number\n    width = int(cnn.get_shape()[2])\n    pattern = tf.reshape(pattern, (-1, 1, 1, rnn_cells_num))\n    pattern = tf.tile(pattern, [1, 1, width, 1])\n    # pattern = slim.fully_connected(pattern, num_classes * width, normalizer_fn=None, activation_fn=tf.nn.sigmoid)\n    # pattern = tf.reshape(pattern, (-1, 1, width, num_classes))\n\n  inf = tf.concat(axis=3, values=[classes, pattern])  # skip connection over RNN\n  inf = slim.conv2d(inf, num_classes, [1, 1], normalizer_fn=None,\n                    activation_fn=None)  # fully convolutional linear activation\n\n  inf = tf.squeeze(inf, [1])\n\n  return inf\n\n\nclass CTCUtils:\n  vocab = {}\n  r_vocab = {}\n\n  # Generate CTC from labels\n  @staticmethod\n  def compute_ctc_from_labels(labels):\n    x_ix = []\n    x_val = []\n    batch_size = labels.shape[0]\n    for batch_i in range(batch_size):\n      label = labels[batch_i]\n      for time, val in enumerate(encode(label.decode(\'utf-8\'), CTCUtils.vocab)):\n        x_ix.append([batch_i, time])\n        x_val.append(val)\n    x_shape = [batch_size, np.asarray(x_ix).max(0)[1] + 1]\n\n    return [np.array(x_ix), np.array(x_val), np.array(x_shape)]\n\n  # Function for computing simple accuracy metric\n  @staticmethod\n  def accuracy(gt_labels, values):\n    prediction = decode(values, CTCUtils.r_vocab)\n\n    encoded_gt_labels = list()\n    for gt_label in gt_labels:\n      encoded_label = encode(gt_label.decode(\'utf-8\'), CTCUtils.vocab)\n      encoded_gt_labels.append(encoded_label)\n\n    gt_labels = decode(np.array(encoded_gt_labels), CTCUtils.r_vocab)\n\n    batch_size = len(gt_labels)\n    mean_accuracy = 0.0\n\n    for k in range(batch_size):\n      if gt_labels[k] == prediction[k]:\n        mean_accuracy += 1.0 / batch_size\n    return mean_accuracy\n\n\n# Function for encoding actual LPR label in vector of numbers\ndef encode(label_string, vocab):\n  return [vocab[c] for c in re.findall(\'(<[^>]*>|.)\', label_string)]\n\n\n# Function for decoding vector of numbers into LPR label\ndef decode(values, reverse_vocab):\n  result = []\n  for j in range(values.shape[0]):\n    string_label = \'\'\n    for value in values[j]:\n      string_label += reverse_vocab[value]\n    result.append(string_label)\n  return result\n\n\ndef decode_beams(vals, r_vocab):\n  beams_list = []\n  for val in vals:\n    decoded_number = \'\'\n    for code in val:\n      decoded_number += r_vocab[code]\n    beams_list.append(decoded_number)\n  return beams_list\n\n\ndef decode_ie_output(vals, r_vocab):\n  vals = vals.flatten()\n  decoded_number = \'\'\n  for val in vals:\n    if val < 0:\n      break\n    decoded_number += r_vocab[val]\n  return decoded_number\n\n\nclass LPRVocab:\n  @staticmethod\n  def create_vocab(train_list_path, val_list_path, use_h_concat=False, use_oi_concat=False):\n    [vocab, r_vocab, num_classes] = LPRVocab._create_standard_vocabs(train_list_path, val_list_path)\n    if use_h_concat:\n      [vocab, r_vocab, num_classes] = LPRVocab._concat_all_hieroglyphs(vocab, r_vocab)\n    if use_oi_concat:\n      [vocab, r_vocab, num_classes] = LPRVocab._concat_oi(vocab, r_vocab)\n\n    return vocab, r_vocab, num_classes\n\n  @staticmethod\n  def _char_range(char1, char2):\n    """"""Generates the characters from `char1` to `char2`, inclusive.""""""\n    for char_code in range(ord(char1), ord(char2) + 1):\n      yield chr(char_code)\n\n  # Function for reading special symbols\n  @staticmethod\n  def _read_specials(filepath):\n    characters = set()\n    with open(filepath, \'r\') as file_:\n      for line in file_:\n        current_label = line.split(\' \')[-1].strip()\n        characters = characters.union(re.findall(\'(<[^>]*>|.)\', current_label))\n    return characters\n\n  @staticmethod\n  def _create_standard_vocabs(train_list_path, val_list_path):\n    chars = set().union(LPRVocab._char_range(\'A\', \'Z\')).union(LPRVocab._char_range(\'0\', \'9\'))\n    chars = chars.union(LPRVocab._read_specials(train_list_path)).union(LPRVocab._read_specials(val_list_path))\n    chars = list(chars)\n    chars.sort()\n    chars.append(\'_\')\n    num_classes = len(chars)\n    vocab = dict(zip(chars, range(num_classes)))\n    r_vocab = dict(zip(range(num_classes), chars))\n    r_vocab[-1] = \'\'\n    return [vocab, r_vocab, num_classes]\n\n  # Function for treating all hieroglyphs as 1 class\n  @staticmethod\n  def _concat_all_hieroglyphs(vocab_before, r_vocab_before):\n    chars = vocab_before.keys()\n    tf.logging.info(""Old number of classes: {}"".format(len(chars)))\n\n    # Get all hieroglyphs from train and test\n    hieroglyphs = list((set(chars) - set(LPRVocab._char_range(\'A\', \'Z\'))) - set(LPRVocab._char_range(\'0\', \'9\')))\n\n    tf.logging.info(\'Total hieroglyphs num: {}\'.format(len(hieroglyphs)))\n    tf.logging.info(\'Vocabulary before: \')\n    tf.logging.info(vocab_before)\n\n    chars = list(set().union(LPRVocab._char_range(\'A\', \'Z\')).union(LPRVocab._char_range(\'0\', \'9\')))\n    chars.sort()\n    new_num_classes = len(chars)\n    vocab_after = dict(zip(chars, range(new_num_classes)))\n    vocab_after.update(dict(zip(hieroglyphs, [new_num_classes] * len(hieroglyphs))))\n    vocab_after[\'_\'] = new_num_classes + 1\n    new_num_classes += 2\n\n    tf.logging.info(\'Vocabulary after: \')\n    tf.logging.info(vocab_after)\n\n    tf.logging.info(\'Reverse vocabulary before: \')\n    tf.logging.info(r_vocab_before)\n\n    r_vocab_after = dict((v, k) for k, v in vocab_after.iteritems())\n    r_vocab_after[-1] = \'\'\n\n    tf.logging.info(\'Reverse vocabulary after: \')\n    tf.logging.info(r_vocab_after)\n\n    tf.logging.info(\'New number of classes: {}\'.format(new_num_classes))\n    tf.logging.info(\'Vocabulary len: {}\'.format(len(vocab_after)))\n    tf.logging.info(\'Reverse vocabulary length: {}\'.format(len(r_vocab_after)))\n\n    return [vocab_after, r_vocab_after, new_num_classes]\n\n  # Function for treating O/0, I/1 as 1 class\n  @staticmethod\n  def _concat_oi(vocab_before, r_vocab_before):\n    chars = vocab_before.keys()\n    tf.logging.info(\'Old number of classes: {}\'.format(len(chars)))\n\n    tf.logging.info(\'Vocabulary before: \')\n    tf.logging.info(vocab_before)\n\n    # Remove \'0\' and \'1\'\n    chars = list(set(chars) - set([\'0\', \'1\']))\n    chars.sort()\n    new_num_classes = len(chars)\n    vocab_after = dict(zip(chars, range(new_num_classes)))\n    vocab_after[\'0\'] = vocab_after[\'O\']\n    vocab_after[\'1\'] = vocab_after[\'I\']\n    vocab_after[\'_\'] = new_num_classes + 1\n    new_num_classes += 1\n\n    tf.logging.info(\'Vocabulary after: \')\n    tf.logging.info(vocab_after)\n\n    tf.logging.info(\'Reverse vocabulary before: \')\n    tf.logging.info(r_vocab_before)\n\n    r_vocab_after = dict((v, k) for k, v in vocab_after.iteritems())\n    r_vocab_after[-1] = \'\'\n\n    tf.logging.info(\'Reverse vocabulary after: \')\n    tf.logging.info(r_vocab_after)\n\n    tf.logging.info(\'New number of classes: {}\'.format(new_num_classes))\n    tf.logging.info(\'Vocabulary len: {}\'.format(len(vocab_after)))\n    tf.logging.info(\'Reverse vocabulary length: {}\'.format(len(r_vocab_after)))\n\n    return [vocab_after, r_vocab_after, new_num_classes]\n'"
tensorflow_toolkit/lpr/lpr/utils.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport re\nfrom lpr.trainer import encode, decode_beams\n\ndef dataset_size(fname):\n  count = 0\n  with open(fname, \'r\') as file_:\n    for _ in file_:\n      count += 1\n  return count\n\ndef lpr_pattern_check(label, lpr_patterns):\n  for pattern in lpr_patterns:\n    if re.match(pattern, label):\n      return True\n  return False\n\ndef edit_distance(string1, string2):\n  len1 = len(string1) + 1\n  len2 = len(string2) + 1\n  tbl = {}\n  for i in range(len1):\n    tbl[i, 0] = i\n  for j in range(len2):\n    tbl[0, j] = j\n  for i in range(1, len1):\n    for j in range(1, len2):\n      cost = 0 if string1[i - 1] == string2[j - 1] else 1\n      tbl[i, j] = min(tbl[i, j - 1] + 1, tbl[i - 1, j] + 1, tbl[i - 1, j - 1] + cost)\n\n  return tbl[i, j]\n\n\ndef accuracy(label, val, vocab, r_vocab, lpr_patterns):\n  pred = decode_beams(val, r_vocab)\n  label_len = len(label)\n  acc, acc1 = 0, 0\n  num = 0\n  for i in range(label_len):\n    if not lpr_pattern_check(label[i].decode(\'utf-8\'), lpr_patterns):  # GT label fails\n      print(\'GT label fails: \' + label[i].decode(\'utf-8\'))\n      continue\n    best = pred[i]\n    edd = edit_distance(encode(label[i].decode(\'utf-8\'), vocab), encode(best, vocab))\n    if edd <= 1:\n      acc1 += 1\n    if label[i].decode(\'utf-8\') == best:\n      acc += 1\n    else:\n      if label[i].decode(\'utf-8\') not in pred[i]:\n        print(\'Check GT label: \' + label[i].decode(\'utf-8\'))\n      print(label[i].decode(\'utf-8\') + \' -- \' + best + \' Edit Distance: \' + str(edd))\n    num += 1\n  return float(acc), float(acc1), num\n'"
tensorflow_toolkit/lpr/tools/eval.py,29,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport os\nimport random\nimport sys\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom lpr.trainer import inference\nfrom lpr.utils import accuracy, dataset_size\nfrom tfutils.helpers import load_module\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Perform evaluation of a trained model\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\n\ndef read_data(height, width, channels_num, list_file_name, batch_size=10):\n  reader = tf.TextLineReader()\n  _, value = reader.read(list_file_name)\n  filename, label = tf.decode_csv(value, [[\'\'], [\'\']], \' \')\n\n  image_file = tf.read_file(filename)\n  rgb_image = tf.image.decode_png(image_file, channels=channels_num)\n  rgb_image_float = tf.image.convert_image_dtype(rgb_image, tf.float32)\n  resized_image = tf.image.resize_images(rgb_image_float, [height, width])\n  resized_image.set_shape([height, width, channels_num])\n\n  image_batch, label_batch, file_batch = tf.train.batch([resized_image, label, image_file], batch_size=batch_size)\n  return image_batch, label_batch, file_batch\n\n\ndef data_input(height, width, channels_num, filename, batch_size=1):\n  files_string_producer = tf.train.string_input_producer([filename])\n  image, label, filename = read_data(height, width, channels_num, files_string_producer, batch_size)\n  return image, label, filename\n\n# pylint: disable=too-many-branches, too-many-statements, too-many-locals\ndef validate(config):\n  if hasattr(config.eval, \'random_seed\'):\n    np.random.seed(config.eval.random_seed)\n    tf.set_random_seed(config.eval.random_seed)\n    random.seed(config.eval.random_seed)\n\n  if hasattr(config.eval.execution, \'CUDA_VISIBLE_DEVICES\'):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = config.train.execution.CUDA_VISIBLE_DEVICES\n\n  height, width, channels_num = config.input_shape\n  rnn_cells_num = config.rnn_cells_num\n\n  graph = tf.Graph()\n  with graph.as_default():\n    with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=False):\n      inp_data, label_val, file_names = data_input(height, width, channels_num,\n                                                   config.eval.file_list_path, batch_size=config.eval.batch_size)\n\n      prob = inference(rnn_cells_num, inp_data, config.num_classes)\n      prob = tf.transpose(prob, (1, 0, 2))  # prepare for CTC\n\n      data_length = tf.fill([tf.shape(prob)[1]], tf.shape(prob)[0])  # input seq length, batch size\n\n      result = tf.nn.ctc_greedy_decoder(prob, data_length, merge_repeated=True)\n\n      predictions = tf.to_int32(result[0][0])\n      d_predictions = tf.sparse_to_dense(predictions.indices,\n                                         [tf.shape(inp_data, out_type=tf.int64)[0], config.max_lp_length],\n                                         predictions.values, default_value=-1, name=\'d_predictions\')\n\n      init = tf.initialize_all_variables()\n      saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n\n  # session\n  conf = tf.ConfigProto()\n  if hasattr(config.eval.execution, \'per_process_gpu_memory_fraction\'):\n    conf.gpu_options.per_process_gpu_memory_fraction = config.train.execution.per_process_gpu_memory_fraction\n  if hasattr(config.eval.execution, \'allow_growth\'):\n    conf.gpu_options.allow_growth = config.train.execution.allow_growth\n\n  sess = tf.Session(graph=graph, config=conf)\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  sess.run(init)\n\n\n  checkpoints_dir = config.model_dir\n  latest_checkpoint = None\n  wait_iters = 0\n\n  if not os.path.exists(os.path.join(checkpoints_dir, \'eval\')):\n    os.mkdir(os.path.join(checkpoints_dir, \'eval\'))\n  writer = tf.summary.FileWriter(os.path.join(checkpoints_dir, \'eval\'), sess.graph)\n\n\n  while True:\n    if config.eval.checkpoint != \'\':\n      new_checkpoint = config.eval.checkpoint\n    else:\n      new_checkpoint = tf.train.latest_checkpoint(checkpoints_dir)\n    if latest_checkpoint != new_checkpoint:\n      latest_checkpoint = new_checkpoint\n      saver.restore(sess, latest_checkpoint)\n      current_step = tf.train.load_variable(latest_checkpoint, \'global_step\')\n\n      test_size = dataset_size(config.eval.file_list_path)\n      time_start = time.time()\n\n      mean_accuracy, mean_accuracy_minus_1 = 0.0, 0.0\n\n      steps = int(test_size / config.eval.batch_size) if int(test_size / config.eval.batch_size) else 1\n      num = 0\n      for _ in range(steps):\n        val, slabel, _ = sess.run([d_predictions, label_val, file_names])\n        acc, acc1, num_ = accuracy(slabel, val, config.vocab, config.r_vocab, config.lpr_patterns)\n        mean_accuracy += acc\n        mean_accuracy_minus_1 += acc1\n        num += num_\n\n      writer.add_summary(\n        tf.Summary(value=[tf.Summary.Value(tag=\'evaluation/acc\', simple_value=float(mean_accuracy / num)),\n                          tf.Summary.Value(tag=\'evaluation/acc-1\', simple_value=float(mean_accuracy_minus_1 / num))]),\n        current_step)\n      print(\'Test acc: {}\'.format(mean_accuracy / num))\n      print(\'Test acc-1: {}\'.format(mean_accuracy_minus_1 / num))\n      print(\'Time per step: {} for test size {}\'.format(time.time() - time_start / steps, test_size))\n    else:\n      if wait_iters % 12 == 0:\n        sys.stdout.write(\'\\r\')\n        for _ in range(11 + wait_iters // 12):\n          sys.stdout.write(\' \')\n        sys.stdout.write(\'\\r\')\n        for _ in range(1 + wait_iters // 12):\n          sys.stdout.write(\'|\')\n      else:\n        sys.stdout.write(\'.\')\n      sys.stdout.flush()\n      time.sleep(5)\n      wait_iters += 1\n    if config.eval.checkpoint != \'\':\n      break\n\n\n  coord.request_stop()\n  coord.join(threads)\n  sess.close()\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n  validate(cfg)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/lpr/tools/export.py,15,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport os\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.framework import graph_io\n\nfrom lpr.trainer import inference\nfrom tfutils.helpers import load_module, execute_mo\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Export model in IE format\')\n  parser.add_argument(\'--data_type\', default=\'FP32\', choices=[\'FP32\', \'FP16\'], help=\'Data type of IR\')\n  parser.add_argument(\'--output_dir\', default=None, help=\'Output Directory\')\n  parser.add_argument(\'--checkpoint\', default=None, help=\'Default: latest\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\n\ndef freezing_graph(config, checkpoint, output_dir):\n  if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n  shape = (None,) + tuple(config.input_shape) # NHWC, dynamic batch\n  graph = tf.Graph()\n  with graph.as_default():\n    with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=False):\n      input_tensor = tf.placeholder(dtype=tf.float32, shape=shape, name=\'input\')\n      prob = inference(config.rnn_cells_num, input_tensor, config.num_classes)\n      prob = tf.transpose(prob, (1, 0, 2))\n      data_length = tf.fill([tf.shape(prob)[1]], tf.shape(prob)[0])\n      result = tf.nn.ctc_greedy_decoder(prob, data_length, merge_repeated=True)\n      predictions = tf.to_int32(result[0][0])\n      tf.sparse_to_dense(predictions.indices, [tf.shape(input_tensor, out_type=tf.int64)[0], config.max_lp_length],\n                         predictions.values, default_value=-1, name=\'d_predictions\')\n      init = tf.initialize_all_variables()\n      saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n\n  sess = tf.Session(graph=graph)\n  sess.run(init)\n  saver.restore(sess, checkpoint)\n  frozen = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, [""d_predictions""])\n  tf.train.write_graph(sess.graph, output_dir, \'graph.pbtxt\', as_text=True)\n  path_to_frozen_model = graph_io.write_graph(frozen, output_dir, \'graph.pb.frozen\', as_text=False)\n  return path_to_frozen_model\n\ndef main(_):\n  args = parse_args()\n  config = load_module(args.path_to_config)\n\n  checkpoint = args.checkpoint if args.checkpoint else tf.train.latest_checkpoint(config.model_dir)\n  print(checkpoint)\n  if not checkpoint or not os.path.isfile(checkpoint+\'.index\'):\n    raise FileNotFoundError(str(checkpoint))\n\n  step = checkpoint.split(\'.\')[-2].split(\'-\')[-1]\n  output_dir = args.output_dir if args.output_dir else os.path.join(config.model_dir, \'export_{}\'.format(step))\n\n  # Freezing graph\n  frozen_dir = os.path.join(output_dir, \'frozen_graph\')\n  frozen_graph = freezing_graph(config, checkpoint, frozen_dir)\n\n  # Export to IR\n  export_dir = os.path.join(output_dir, \'IR\', args.data_type)\n\n  mo_params = {\n    \'framework\': \'tf\',\n    \'model_name\': \'lpr\',\n    \'input\': \'input\',\n    \'output\': \'d_predictions\',\n    \'reverse_input_channels\': True,\n    \'scale\': 255,\n    \'input_shape\': [1] + list(config.input_shape),\n    \'data_type\': args.data_type,\n  }\n\n  execute_mo(mo_params, frozen_graph, export_dir)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/lpr/tools/infer.py,5,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\nfrom tfutils.helpers import load_module\nfrom lpr.trainer import decode_beams\n\n\ndef load_graph(frozen_graph_filename):\n  with tf.gfile.GFile(frozen_graph_filename, \'rb\') as file:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(file.read())\n  with tf.Graph().as_default() as graph:\n    tf.import_graph_def(graph_def)\n  return graph\n\ndef display_license_plate(number, license_plate_img):\n  size = cv2.getTextSize(number, cv2.FONT_HERSHEY_SIMPLEX, 0.55, 2)\n  text_width = size[0][0]\n  text_height = size[0][1]\n\n  height, width, _ = license_plate_img.shape\n  license_plate_img = cv2.copyMakeBorder(license_plate_img, 0, text_height + 10, 0,\n                                         0 if text_width < width else text_width - width,\n                                         cv2.BORDER_CONSTANT, value=(255, 255, 255))\n  cv2.putText(license_plate_img, number, (0, height + text_height + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 0), 2)\n\n  return license_plate_img\n\ndef build_argparser():\n  parser = ArgumentParser()\n  parser.add_argument(\'--model\', help=\'Path to frozen graph file with a trained model.\', required=True, type=str)\n  parser.add_argument(\'--config\', help=\'Path to a config.py\', required=True, type=str)\n  parser.add_argument(\'--output\', help=\'Output image\')\n  parser.add_argument(\'input_image\', help=\'Image with license plate\')\n  return parser\n\n\ndef main():\n  args = build_argparser().parse_args()\n\n  graph = load_graph(args.model)\n  config = load_module(args.config)\n\n  image = cv2.imread(args.input_image)\n  img = cv2.resize(image, (94, 24))\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  img = np.float32(img)\n  img = np.multiply(img, 1.0/255.0)\n\n  input = graph.get_tensor_by_name(""import/input:0"")\n  output = graph.get_tensor_by_name(""import/d_predictions:0"")\n\n  with tf.Session(graph=graph) as sess:\n    results = sess.run(output, feed_dict={input: [img]})\n    print(results)\n\n    decoded_lp = decode_beams(results, config.r_vocab)[0]\n    print(decoded_lp)\n\n    img_to_display = display_license_plate(decoded_lp, image)\n\n    if args.output:\n      cv2.imwrite(args.output, img_to_display)\n    else:\n      cv2.imshow(\'License Plate\', img_to_display)\n      cv2.waitKey(0)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tensorflow_toolkit/lpr/tools/infer_checkpoint.py,24,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport argparse\nimport os\nimport random\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom lpr.utils import dataset_size\nfrom lpr.trainer import inference, decode_beams\nfrom tfutils.helpers import load_module\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Infer of a trained model\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\n\ndef read_data(height, width, channels_num, list_file_name, batch_size=1):\n  reader = tf.TextLineReader()\n  _, value = reader.read(list_file_name)\n  filename = value\n  image_filename = tf.read_file(filename)\n  rgb_image = tf.image.decode_png(image_filename, channels=channels_num)\n  rgb_image_float = tf.image.convert_image_dtype(rgb_image, tf.float32)\n  resized_image = tf.image.resize_images(rgb_image_float, [height, width])\n  resized_image.set_shape([height, width, channels_num])\n\n  image_batch, file_batch = tf.train.batch([resized_image, filename], batch_size=batch_size,\n                                           allow_smaller_final_batch=True)\n  return image_batch, file_batch\n\n\ndef data_input(height, width, channels_num, filename, batch_size=1):\n  files_string_producer = tf.train.string_input_producer([filename])\n  image, filename = read_data(height, width, channels_num, files_string_producer, batch_size)\n  return image, filename\n\n# pylint: disable=too-many-statements, too-many-locals\ndef infer(config):\n  if hasattr(config.infer, \'random_seed\'):\n    np.random.seed(config.infer.random_seed)\n    tf.set_random_seed(config.infer.random_seed)\n    random.seed(config.infer.random_seed)\n\n  if hasattr(config.infer.execution, \'CUDA_VISIBLE_DEVICES\'):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = config.train.execution.CUDA_VISIBLE_DEVICES\n\n  height, width, channels_num = config.input_shape\n  rnn_cells_num = config.rnn_cells_num\n\n  graph = tf.Graph()\n\n  with graph.as_default():\n    with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=False):\n      inp_data, filenames = data_input(height, width, channels_num, config.infer.file_list_path,\n                                       batch_size=config.infer.batch_size)\n\n      prob = inference(rnn_cells_num, inp_data, config.num_classes)\n      prob = tf.transpose(prob, (1, 0, 2))  # prepare for CTC\n\n      data_length = tf.fill([tf.shape(prob)[1]], tf.shape(prob)[0])  # input seq length, batch size\n\n      result = tf.nn.ctc_greedy_decoder(prob, data_length, merge_repeated=True)\n\n      predictions = tf.to_int32(result[0][0])\n      d_predictions = tf.sparse_to_dense(predictions.indices,\n                                         [tf.shape(inp_data, out_type=tf.int64)[0], config.max_lp_length],\n                                         predictions.values, default_value=-1, name=\'d_predictions\')\n\n      init = tf.initialize_all_variables()\n      saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n\n  # session\n  conf = tf.ConfigProto()\n  if hasattr(config.eval.execution, \'per_process_gpu_memory_fraction\'):\n    conf.gpu_options.per_process_gpu_memory_fraction = config.train.execution.per_process_gpu_memory_fraction\n  if hasattr(config.eval.execution, \'allow_growth\'):\n    conf.gpu_options.allow_growth = config.train.execution.allow_growth\n\n  sess = tf.Session(graph=graph, config=conf)\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  sess.run(init)\n\n  latest_checkpoint = config.infer.checkpoint\n  if config.infer.checkpoint == \'\':\n    latest_checkpoint = tf.train.latest_checkpoint(config.model_dir)\n\n  saver.restore(sess, latest_checkpoint)\n\n  infer_size = dataset_size(config.infer.file_list_path)\n  steps = int(infer_size / config.infer.batch_size) if int(infer_size / config.infer.batch_size) else 1\n\n  for _ in range(steps):\n\n    vals, batch_filenames = sess.run([d_predictions, filenames])\n    print(batch_filenames)\n    pred = decode_beams(vals, config.r_vocab)\n\n    for i, filename in enumerate(batch_filenames):\n      filename = filename.decode(\'utf-8\')\n\n\n      img = cv2.imread(filename)\n      size = cv2.getTextSize(pred[i], cv2.FONT_HERSHEY_SIMPLEX, 0.55, 2)\n      text_width = size[0][0]\n      text_height = size[0][1]\n\n      img_he, img_wi, _ = img.shape\n      img = cv2.copyMakeBorder(img, 0, text_height + 10, 0,\n                               0 if text_width < img_wi else text_width - img_wi, cv2.BORDER_CONSTANT,\n                               value=(255, 255, 255))\n      cv2.putText(img, pred[i], (0, img_he + text_height + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 0), 2)\n\n      cv2.imshow(\'License Plate\', img)\n      key = cv2.waitKey(0)\n      if key == 27:\n        break\n\n  coord.request_stop()\n  coord.join(threads)\n  sess.close()\n\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n  infer(cfg)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/lpr/tools/infer_ie.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nfrom argparse import ArgumentParser\nimport logging as log\nimport sys\nimport os\nimport cv2\nfrom openvino.inference_engine import IENetwork, IEPlugin\nfrom lpr.trainer import decode_ie_output\nfrom tfutils.helpers import load_module\n\n\ndef build_argparser():\n  parser = ArgumentParser()\n  parser.add_argument(""--model"", help=""Path to an .xml file with a trained model."", required=True, type=str)\n  parser.add_argument(""--cpu_extension"",\n                      help=""MKLDNN (CPU)-targeted custom layers. ""\n                           ""Absolute path to a shared library with the kernels implementation"", type=str, default=None)\n  parser.add_argument(""--plugin_dir"", help=""Path to a plugin folder"", type=str, default=None)\n  parser.add_argument(""--device"",\n                      help=""Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. Sample ""\n                           ""will look for a suitable plugin for device specified (CPU by default)"", default=""CPU"",\n                      type=str)\n  parser.add_argument(\'--config\', help=\'Path to a config.py\', required=True)\n  parser.add_argument(\'--output\', help=\'Output image\')\n  parser.add_argument(\'input_image\', help=\'Image with license plate\')\n  return parser\n\n\ndef display_license_plate(number, license_plate_img):\n  size = cv2.getTextSize(number, cv2.FONT_HERSHEY_SIMPLEX, 0.55, 2)\n  text_width = size[0][0]\n  text_height = size[0][1]\n\n  height, width, _ = license_plate_img.shape\n  license_plate_img = cv2.copyMakeBorder(license_plate_img, 0, text_height + 10, 0,\n                                         0 if text_width < width else text_width - width,\n                                         cv2.BORDER_CONSTANT, value=(255, 255, 255))\n  cv2.putText(license_plate_img, number, (0, height + text_height + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 0), 2)\n\n  return license_plate_img\n\ndef load_ir_model(model_xml, device, plugin_dir, cpu_extension):\n  model_bin = os.path.splitext(model_xml)[0] + "".bin""\n\n  # initialize plugin\n  log.info(""Initializing plugin for %s device..."", device)\n  plugin = IEPlugin(device=device, plugin_dirs=plugin_dir)\n  if cpu_extension and \'CPU\' in device:\n    plugin.add_cpu_extension(cpu_extension)\n\n  # read IR\n  log.info(""Reading IR..."")\n  net = IENetwork(model=model_xml, weights=model_bin)\n\n  if ""CPU"" in device:\n    supported_layers = plugin.get_supported_layers(net)\n    not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n    if not_supported_layers:\n      log.error(""Following layers are not supported by the plugin for specified device %s:\\n %s"",\n                device, \', \'.join(not_supported_layers))\n      log.error(""Please try to specify cpu extensions library path in sample\'s command line parameters using ""\n                ""--cpu_extension command line argument"")\n      sys.exit(1)\n\n  # input / output check\n  assert len(net.inputs.keys()) == 1, ""LPRNet must have only single input""\n  assert len(net.outputs) == 1, ""LPRNet must have only single output topologies""\n\n  input_blob = next(iter(net.inputs))\n  out_blob = next(iter(net.outputs))\n  log.info(""Loading IR to the plugin..."")\n  exec_net = plugin.load(network=net)\n  shape = net.inputs[input_blob].shape # pylint: disable=E1136\n  del net\n\n  return exec_net, plugin, input_blob, out_blob, shape\n\n\ndef main():\n  log.basicConfig(format=""[ %(levelname)s ] %(message)s"", level=log.INFO, stream=sys.stdout)\n  args = build_argparser().parse_args()\n  cfg = load_module(args.config)\n  exec_net, plugin, input_blob, out_blob, shape = load_ir_model(args.model, args.device,\n                                                                args.plugin_dir, args.cpu_extension)\n  n_batch, channels, height, width = shape\n\n\n  image = cv2.imread(args.input_image)\n  img_to_display = image.copy()\n  in_frame = cv2.resize(image, (width, height))\n  in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n  in_frame = in_frame.reshape((n_batch, channels, height, width))\n\n  result = exec_net.infer(inputs={input_blob: in_frame})\n  lp_code = result[out_blob][0]\n  lp_number = decode_ie_output(lp_code, cfg.r_vocab)\n  print(\'Output: {}\'.format(lp_number))\n  img_to_display = display_license_plate(lp_number, img_to_display)\n  if args.output:\n    cv2.imwrite(args.output, img_to_display)\n  else:\n    cv2.imshow(\'License Plate\', img_to_display)\n    cv2.waitKey(0)\n\n  del exec_net\n  del plugin\n\n\nif __name__ == \'__main__\':\n  sys.exit(main() or 0)\n'"
tensorflow_toolkit/lpr/tools/train.py,31,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport random\nimport os\nimport argparse\nimport numpy as np\nimport tensorflow as tf\nfrom lpr.trainer import CTCUtils, inference, InputData\nfrom tfutils.helpers import load_module\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Perform training of a model\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  parser.add_argument(\'--init_checkpoint\', default=None, help=\'Path to checkpoint\')\n  return parser.parse_args()\n\n# pylint: disable=too-many-locals, too-many-statements\ndef train(config, init_checkpoint):\n  if hasattr(config.train, \'random_seed\'):\n    np.random.seed(config.train.random_seed)\n    tf.set_random_seed(config.train.random_seed)\n    random.seed(config.train.random_seed)\n\n  if hasattr(config.train.execution, \'CUDA_VISIBLE_DEVICES\'):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = config.train.execution.CUDA_VISIBLE_DEVICES\n\n  CTCUtils.vocab = config.vocab\n  CTCUtils.r_vocab = config.r_vocab\n\n  input_train_data = InputData(batch_size=config.train.batch_size,\n                               input_shape=config.input_shape,\n                               file_list_path=config.train.file_list_path,\n                               apply_basic_aug=config.train.apply_basic_aug,\n                               apply_stn_aug=config.train.apply_stn_aug,\n                               apply_blur_aug=config.train.apply_blur_aug)\n\n\n  graph = tf.Graph()\n  with graph.as_default():\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    input_data, input_labels = input_train_data.input_fn()\n\n    prob = inference(config.rnn_cells_num, input_data, config.num_classes)\n    prob = tf.transpose(prob, (1, 0, 2))  # prepare for CTC\n\n    data_length = tf.fill([tf.shape(prob)[1]], tf.shape(prob)[0])  # input seq length, batch size\n    ctc = tf.py_func(CTCUtils.compute_ctc_from_labels, [input_labels], [tf.int64, tf.int64, tf.int64])\n    ctc_labels = tf.to_int32(tf.SparseTensor(ctc[0], ctc[1], ctc[2]))\n\n    predictions = tf.to_int32(\n      tf.nn.ctc_beam_search_decoder(prob, data_length, merge_repeated=False, beam_width=10)[0][0])\n    tf.sparse_tensor_to_dense(predictions, default_value=-1, name=\'d_predictions\')\n    tf.reduce_mean(tf.edit_distance(predictions, ctc_labels, normalize=False), name=\'error_rate\')\n\n    loss = tf.reduce_mean(\n      tf.nn.ctc_loss(inputs=prob, labels=ctc_labels, sequence_length=data_length, ctc_merge_repeated=True), name=\'loss\')\n\n    learning_rate = tf.train.piecewise_constant(global_step, [150000, 200000],\n                                                [config.train.learning_rate, 0.1 * config.train.learning_rate,\n                                                 0.01 * config.train.learning_rate])\n    opt_loss = tf.contrib.layers.optimize_loss(loss, global_step, learning_rate, config.train.opt_type,\n                                               config.train.grad_noise_scale, name=\'train_step\')\n\n    tf.global_variables_initializer()\n    saver = tf.train.Saver(max_to_keep=1000, write_version=tf.train.SaverDef.V2, save_relative_paths=True)\n\n  conf = tf.ConfigProto()\n  if hasattr(config.train.execution, \'per_process_gpu_memory_fraction\'):\n    conf.gpu_options.per_process_gpu_memory_fraction = config.train.execution.per_process_gpu_memory_fraction\n  if hasattr(config.train.execution, \'allow_growth\'):\n    conf.gpu_options.allow_growth = config.train.execution.allow_growth\n\n  session = tf.Session(graph=graph, config=conf)\n  coordinator = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=session, coord=coordinator)\n\n  session.run(\'init\')\n\n  if init_checkpoint:\n    tf.logging.info(\'Initialize from: \' + init_checkpoint)\n    saver.restore(session, init_checkpoint)\n  else:\n    lastest_checkpoint = tf.train.latest_checkpoint(config.model_dir)\n    if lastest_checkpoint:\n      tf.logging.info(\'Restore from: \' + lastest_checkpoint)\n      saver.restore(session, lastest_checkpoint)\n\n  writer = None\n  if config.train.need_to_save_log:\n    writer = tf.summary.FileWriter(config.model_dir, session.graph)\n\n  graph.finalize()\n\n\n  for i in range(config.train.steps):\n    curr_step, curr_learning_rate, curr_loss, curr_opt_loss = session.run([global_step, learning_rate, loss, opt_loss])\n\n    if i % config.train.display_iter == 0:\n      if config.train.need_to_save_log:\n\n        writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\'train/loss\',\n                                                              simple_value=float(curr_loss)),\n                                             tf.Summary.Value(tag=\'train/learning_rate\',\n                                                              simple_value=float(curr_learning_rate)),\n                                             tf.Summary.Value(tag=\'train/optimization_loss\',\n                                                              simple_value=float(curr_opt_loss))\n                                             ]),\n                           curr_step)\n        writer.flush()\n\n      tf.logging.info(\'Iteration: \' + str(curr_step) + \', Train loss: \' + str(curr_loss))\n\n    if ((curr_step % config.train.save_checkpoints_steps == 0 or curr_step == config.train.steps)\n        and config.train.need_to_save_weights):\n      saver.save(session, config.model_dir + \'/model.ckpt-{:d}.ckpt\'.format(curr_step))\n\n  coordinator.request_stop()\n  coordinator.join(threads)\n  session.close()\n\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n  train(cfg, args.init_checkpoint)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/person_vehicle_bike_detector/tools/create_crossroad_extra_tf_records.py,22,"b'# Copyright 2019 Intel Corporation\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Convert raw COCO dataset to TFRecord for object_detection.\n\nPlease note that this tool creates sharded output files.\n\nExample usage:\n    python create_coco_tf_record.py --logtostderr \\\n      --train_image_dir=""${TRAIN_IMAGE_DIR}"" \\\n      --val_image_dir=""${VAL_IMAGE_DIR}"" \\\n      --train_annotations_file=""${TRAIN_ANNOTATIONS_FILE}"" \\\n      --val_annotations_file=""${VAL_ANNOTATIONS_FILE}"" \\\n      --testdev_annotations_file=""${TESTDEV_ANNOTATIONS_FILE}"" \\\n      --output_dir=""${OUTPUT_DIR}""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\nimport io\nimport json\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport PIL.Image\nimport contextlib2\nfrom pycocotools import mask\n\nfrom object_detection.dataset_tools import tf_record_creation_util\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\n\nflags = tf.app.flags  # pylint: disable=invalid-name\ntf.flags.DEFINE_boolean(\'include_masks\', False,\n                        \'Whether to include instance segmentations masks \'\n                        \'(PNG encoded) in the result. default: False.\')\ntf.flags.DEFINE_string(\'train_image_dir\', \'\',\n                       \'Training image directory.\')\ntf.flags.DEFINE_string(\'val_image_dir\', \'\',\n                       \'Validation image directory.\')\ntf.flags.DEFINE_string(\'train_annotations_file\', \'\',\n                       \'Training annotations JSON file.\')\ntf.flags.DEFINE_string(\'val_annotations_file\', \'\',\n                       \'Validation annotations JSON file.\')\ntf.flags.DEFINE_string(\'output_dir\', \'/tmp/\', \'Output data directory.\')\n\nFLAGS = flags.FLAGS\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef create_tf_example(image,  # pylint: disable=invalid-name, too-many-locals\n                      annotations_list,\n                      image_dir,\n                      category_index,\n                      include_masks=False):\n  """"""Converts image and annotations to a tf.Example proto.\n\n  Args:\n    image: dict with keys:\n      [u\'license\', u\'file_name\', u\'coco_url\', u\'height\', u\'width\',\n      u\'date_captured\', u\'flickr_url\', u\'id\']\n    annotations_list:\n      list of dicts with keys:\n      [u\'segmentation\', u\'area\', u\'iscrowd\', u\'image_id\',\n      u\'bbox\', u\'category_id\', u\'id\']\n      Notice that bounding box coordinates in the official COCO dataset are\n      given as [x, y, width, height] tuples using absolute coordinates where\n      x, y represent the top-left (0-indexed) corner.  This function converts\n      to the format expected by the Tensorflow Object Detection API (which is\n      which is [ymin, xmin, ymax, xmax] with coordinates normalized relative\n      to image size).\n    image_dir: directory containing the image files.\n    category_index: a dict containing COCO category information keyed\n      by the \'id\' field of each category.  See the\n      label_map_util.create_category_index function.\n    include_masks: Whether to include instance segmentations masks\n      (PNG encoded) in the result. default: False.\n  Returns:\n    example: The converted tf.Example\n    num_annotations_skipped: Number of (invalid) annotations that were ignored.\n\n  Raises:\n    ValueError: if the image pointed to by data[\'filename\'] is not a valid JPEG\n  """"""\n  image_height = image[\'height\']\n  image_width = image[\'width\']\n  filename = os.path.basename(image[\'image\'])\n  image_id = image[\'id\']\n\n  full_path = os.path.join(image_dir, filename)\n  with tf.gfile.GFile(full_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  xmin = []\n  xmax = []\n  ymin = []\n  ymax = []\n  is_crowd = []\n  category_names = []\n  category_ids = []\n  area = []\n  encoded_mask_png = []\n  num_annotations_skipped = 0\n  for object_annotations in annotations_list:\n    (x, y, width, height) = tuple(object_annotations[\'bbox\'])  # pylint: disable=invalid-name\n    if width <= 0 or height <= 0:\n      num_annotations_skipped += 1\n      continue\n    if x + width > image_width or y + height > image_height:\n      num_annotations_skipped += 1\n      continue\n    xmin.append(float(x) / image_width)\n    xmax.append(float(x + width) / image_width)\n    ymin.append(float(y) / image_height)\n    ymax.append(float(y + height) / image_height)\n    is_crowd.append(object_annotations[\'iscrowd\'])\n    category_id = int(object_annotations[\'category_id\'])\n    category_ids.append(category_id)\n    category_names.append(category_index[category_id][\'name\'].encode(\'utf8\'))\n    area.append(object_annotations[\'area\'])\n\n    if include_masks:\n      run_len_encoding = mask.frPyObjects(object_annotations[\'segmentation\'],\n                                          image_height, image_width)\n      binary_mask = mask.decode(run_len_encoding)\n      if not object_annotations[\'iscrowd\']:\n        binary_mask = np.amax(binary_mask, axis=2)\n      pil_image = PIL.Image.fromarray(binary_mask)\n      output_io = io.BytesIO()\n      pil_image.save(output_io, format=\'PNG\')\n      encoded_mask_png.append(output_io.getvalue())\n  feature_dict = {\n    \'image/height\':\n        dataset_util.int64_feature(image_height),\n    \'image/width\':\n        dataset_util.int64_feature(image_width),\n    \'image/filename\':\n        dataset_util.bytes_feature(filename.encode(\'utf8\')),\n    \'image/source_id\':\n        dataset_util.bytes_feature(str(image_id).encode(\'utf8\')),\n    \'image/key/sha256\':\n        dataset_util.bytes_feature(key.encode(\'utf8\')),\n    \'image/encoded\':\n        dataset_util.bytes_feature(encoded_jpg),\n    \'image/format\':\n        dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n    \'image/object/bbox/xmin\':\n        dataset_util.float_list_feature(xmin),\n    \'image/object/bbox/xmax\':\n        dataset_util.float_list_feature(xmax),\n    \'image/object/bbox/ymin\':\n        dataset_util.float_list_feature(ymin),\n    \'image/object/bbox/ymax\':\n        dataset_util.float_list_feature(ymax),\n    \'image/object/class/text\':\n        dataset_util.bytes_list_feature(category_names),\n    \'image/object/is_crowd\':\n        dataset_util.int64_list_feature(is_crowd),\n    \'image/object/area\':\n        dataset_util.float_list_feature(area),\n  }\n  if include_masks:\n    feature_dict[\'image/object/mask\'] = (\n      dataset_util.bytes_list_feature(encoded_mask_png))\n  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n  return key, example, num_annotations_skipped\n\n\ndef _create_tf_record_from_coco_annotations(  # pylint: disable=invalid-name, too-many-locals\n    annotations_file, image_dir, output_path, include_masks, num_shards):\n  """"""Loads COCO annotation json files and converts to tf.Record format.\n\n  Args:\n    annotations_file: JSON file containing bounding box annotations.\n    image_dir: Directory containing the image files.\n    output_path: Path to output tf.Record file.\n    include_masks: Whether to include instance segmentations masks\n      (PNG encoded) in the result. default: False.\n    num_shards: number of output file shards.\n  """"""\n  with contextlib2.ExitStack() as tf_record_close_stack, \\\n        tf.gfile.GFile(annotations_file, \'r\') as fid:\n    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n      tf_record_close_stack, output_path, num_shards)\n    groundtruth_data = json.load(fid)\n    images = groundtruth_data[\'images\']\n    category_index = label_map_util.create_category_index(\n      groundtruth_data[\'categories\'])\n\n    annotations_index = {}\n    if \'annotations\' in groundtruth_data:\n      tf.logging.info(\n        \'Found groundtruth annotations. Building annotations index.\')\n      for annotation in groundtruth_data[\'annotations\']:\n        image_id = annotation[\'image_id\']\n        if image_id not in annotations_index:\n          annotations_index[image_id] = []\n        annotations_index[image_id].append(annotation)\n    missing_annotation_count = 0\n    for image in images:\n      image_id = image[\'id\']\n      if image_id not in annotations_index:\n        missing_annotation_count += 1\n        annotations_index[image_id] = []\n    tf.logging.info(\'%d images are missing annotations.\',\n                    missing_annotation_count)\n\n    total_num_annotations_skipped = 0\n    for idx, image in enumerate(images):\n      if idx % 100 == 0:\n        tf.logging.info(\'On image %d of %d\', idx, len(images))\n      annotations_list = annotations_index[image[\'id\']]\n      _, tf_example, num_annotations_skipped = create_tf_example(\n        image, annotations_list, image_dir, category_index, include_masks)\n      total_num_annotations_skipped += num_annotations_skipped\n      shard_idx = idx % num_shards\n      output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n    tf.logging.info(\'Finished writing, skipped %d annotations.\',\n                    total_num_annotations_skipped)\n\n\ndef main(_):\n  assert FLAGS.train_image_dir, \'`train_image_dir` missing.\'\n  assert FLAGS.train_annotations_file, \'`train_annotations_file` missing.\'\n  assert FLAGS.val_annotations_file, \'`val_annotations_file` missing.\'\n  assert FLAGS.val_image_dir, \'`val_image_dir` missing.\'\n\n  if not tf.gfile.IsDirectory(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n  train_output_path = os.path.join(FLAGS.output_dir, \'crossroad_train.record\')\n  val_output_path = os.path.join(FLAGS.output_dir, \'crossroad_val.record\')\n\n  _create_tf_record_from_coco_annotations(\n    FLAGS.train_annotations_file,\n    FLAGS.train_image_dir,\n    train_output_path,\n    FLAGS.include_masks,\n    num_shards=10)\n  _create_tf_record_from_coco_annotations(\n    FLAGS.val_annotations_file,\n    FLAGS.val_image_dir,\n    val_output_path,\n    FLAGS.include_masks,\n    num_shards=10)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
tensorflow_toolkit/person_vehicle_bike_detector/tools/infer.py,5,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport tensorflow as tf\nimport cv2\nimport numpy as np\n\nfrom object_detection.utils import label_map_util\n\ndef build_argparser():\n  parser = ArgumentParser()\n  parser.add_argument(""--model"", help=""Path to frozen graph"", required=True, type=str)\n  parser.add_argument(""--label_map"", help=""Path to frozen graph"", default=""dataset/crossroad_label_map.pbtxt"", type=str)\n  parser.add_argument(\'--output\', \'-o\', help=\'Output image\')\n  parser.add_argument(\'input_image\', help=\'Image with license plate\')\n  return parser.parse_args()\n\n\ndef load_graph(frozen_graph_filename):\n  with tf.gfile.GFile(frozen_graph_filename, \'rb\') as file:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(file.read())\n  with tf.Graph().as_default() as graph:\n    tf.import_graph_def(graph_def)\n  return graph\n\n\ndef draw(image, output, label_map, conf_threshold=0.25, bbox_color=(50, 255, 50)):\n  num_detections = output[0][0]\n  detection_boxes = output[1][0]\n  detection_scores = output[2][0]\n  detection_classes = output[3][0]\n\n  height, width = image.shape[:2]\n  for i in range(0, int(num_detections)):\n    if detection_scores[i] > conf_threshold:\n      class_id = int(detection_classes[i])\n      ymin = int(detection_boxes[i][0] * height)\n      xmin = int(detection_boxes[i][1] * width)\n      ymax = int(detection_boxes[i][2] * height)\n      xmax = int(detection_boxes[i][3] * width)\n      label = ""{0}: {1:.2f}"".format(label_map[class_id][\'name\'], detection_scores[i])\n\n      cv2.rectangle(image, (xmin, ymin), (xmax, ymax), bbox_color, 2)\n      label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.55, 1)\n      cv2.rectangle(image, (xmin, ymin-label_size[0][1]), (xmin+label_size[0][0], ymin+label_size[1]),\n                    (255, 255, 255), cv2.FILLED)\n      cv2.putText(image, label, (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 0), 2)\n\n\ndef main():\n  args = build_argparser()\n\n  label_map = label_map_util.create_category_index_from_labelmap(args.label_map, use_display_name=True)\n\n  graph = load_graph(args.model)\n\n  image = cv2.imread(args.input_image)\n  img = cv2.resize(image, (512, 512))\n  img = np.float32(img)\n\n  t_input = graph.get_tensor_by_name(""import/image_tensor:0"")\n  t_output = [\n    graph.get_tensor_by_name(""import/num_detections:0""),\n    graph.get_tensor_by_name(""import/detection_boxes:0""),\n    graph.get_tensor_by_name(""import/detection_scores:0""),\n    graph.get_tensor_by_name(""import/detection_classes:0"")\n  ]\n  with tf.Session(graph=graph) as sess:\n    output = sess.run(t_output, feed_dict={t_input: [img]})\n\n  draw(image, output, label_map)\n\n  if args.output:\n    cv2.imwrite(args.output, image)\n  else:\n    cv2.imshow(\'Image\', image)\n    cv2.waitKey(0)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tensorflow_toolkit/person_vehicle_bike_detector/tools/infer_ie.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport os\nimport sys\nimport logging as log\nimport numpy as np\nimport cv2\nfrom object_detection.utils import label_map_util\nfrom openvino.inference_engine import IENetwork, IEPlugin\n\n\ndef build_argparser():\n  parser = ArgumentParser()\n  parser.add_argument(""--model"", help=""Path to frozen graph"", required=True, type=str)\n  parser.add_argument(""--cpu_extension"",\n                      help=""MKLDNN (CPU)-targeted custom layers. ""\n                           ""Absolute path to a shared library with the kernels implementation"", type=str, default=None)\n  parser.add_argument(""--plugin_dir"", help=""Path to a plugin folder"", type=str, default=None)\n  parser.add_argument(""--device"",\n                      help=""Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. Sample ""\n                           ""will look for a suitable plugin for device specified (CPU by default)"", default=""CPU"",\n                      type=str)\n  parser.add_argument(""--label_map"", help=""Path to frozen graph"", default=""dataset/crossroad_label_map.pbtxt"", type=str)\n  parser.add_argument(\'--output\', \'-o\', help=\'Output image\')\n  parser.add_argument(\'input_image\', help=\'Image with license plate\')\n  return parser.parse_args()\n\n\ndef load_ir_model(model_xml, device, plugin_dir, cpu_extension):\n  model_bin = os.path.splitext(model_xml)[0] + "".bin""\n\n  # initialize plugin\n  log.info(""Initializing plugin for %s device..."", device)\n  plugin = IEPlugin(device=device, plugin_dirs=plugin_dir)\n  if cpu_extension and \'CPU\' in device:\n    plugin.add_cpu_extension(cpu_extension)\n\n  # read IR\n  net = IENetwork(model=model_xml, weights=model_bin)\n\n  if ""CPU"" in device:\n    supported_layers = plugin.get_supported_layers(net)\n    not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n    if not_supported_layers:\n      log.error(""Following layers are not supported by the plugin for specified device %s:\\n %s"",\n                plugin.device, \', \'.join(not_supported_layers))\n      log.error(""Please try to specify cpu extensions library path in sample\'s command line parameters using ""\n                ""--cpu_extension command line argument"")\n      sys.exit(1)\n\n  input_blob = next(iter(net.inputs))\n  out_blob = next(iter(net.outputs))\n  exec_net = plugin.load(network=net, num_requests=2)\n  shape = net.inputs[input_blob].shape  # pylint: disable=E1136\n  del net\n\n  return exec_net, plugin, input_blob, out_blob, shape\n\n\ndef draw(image, detections, label_map, conf_threshold=0.25, bbox_color=(50, 255, 50)):\n  height, width = image.shape[:2]\n  for obj in detections:\n    _, class_id, score, xmin, ymin, xmax, ymax = obj\n    class_id = int(class_id)\n    if score > conf_threshold:\n      xmin = int(xmin * width)\n      ymin = int(ymin * height)\n      xmax = int(xmax * width)\n      ymax = int(ymax * height)\n      label = ""{0}: {1:.2f}"".format(label_map[class_id][\'name\'], score)\n      cv2.rectangle(image, (xmin, ymin), (xmax, ymax), bbox_color, 2)\n      label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.55, 1)\n      cv2.rectangle(image, (xmin, ymin-label_size[0][1]), (xmin+label_size[0][0], ymin+label_size[1]),\n                    (255, 255, 255), cv2.FILLED)\n      cv2.putText(image, label, (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 0), 2)\n\n\ndef main():\n  args = build_argparser()\n\n  label_map = label_map_util.create_category_index_from_labelmap(args.label_map, use_display_name=True)\n  exec_net, _, input_blob, _, shape = load_ir_model(args.model, args.device, args.plugin_dir, args.cpu_extension)\n  net_height, net_width = shape[2:4]\n  image = cv2.imread(args.input_image)\n  img = cv2.resize(image, (net_width, net_height))\n  img = np.float32(img)\n  # Change data layout from HWC to CHW\n  img = img.transpose((2, 0, 1))  # pylint: disable=E1111,E1121\n  img = img.reshape(shape)\n  res = exec_net.infer(inputs={input_blob: img})\n\n  detections = res[\'DetectionOutput\'][0][0]\n\n  draw(image, detections, label_map)\n\n  if args.output:\n    cv2.imwrite(args.output, image)\n  else:\n    cv2.imshow(\'Image\', image)\n    cv2.waitKey(0)\n\nif __name__ == ""__main__"":\n  main()\n'"
tensorflow_toolkit/ssd_detector/coco/config.py,4,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n# pylint: disable=line-too-long\n\nimport os\nimport matplotlib; matplotlib.use(\'Agg\')  # pylint: disable=multiple-statements\nfrom ssd_detector.readers.object_detector_json import ObjectDetectorJson\n\nCURRENT_DIR = os.path.dirname(os.path.realpath(__file__))\nROOT_DIR = os.path.normpath(os.path.join(CURRENT_DIR, ""../../..""))\n\n# See more details about parameters in TensorFlow documentation tf.estimator\nclass train:\n  annotation_path = ""instances_train2017.json""  # Path to the annotation file\n  cache_type = ""ENCODED""  # Type of data to save in memory, possible options: \'FULL\', \'ENCODED\', \'NONE\'\n\n  batch_size = 32                    # Number of images in the batch\n  steps = 50000000                   # Number of steps for which to train model\n  max_steps = None                   # Number of total steps for which to train model\n  save_checkpoints_steps = 1000      # Number of training steps when checkpoint should be saved\n  keep_checkpoint_every_n_hours = 6  # Checkpoint should be saved forever after every n hours\n  save_summary_steps = 100           # Number of steps when the summary information should be saved\n  random_seed = 666                  # Random seed\n\n  fill_with_current_image_mean = False  # Parameter of data transformer\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""             # Environment variable to control CUDA device used for training\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True                    # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 2\n    inter_op_parallelism_threads = 8\n    transformer_parallel_calls = 4  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 8   # Number of batches to prefetch\n\n\nclass eval:\n  annotation_path = {\n    ""test"": os.path.join(CURRENT_DIR, ""instances_val2017.json"")\n  }  # Dictionary with paths to annotations and its short names which will be displayed in the TensorBoard\n  datasets = [""test""]  # List of names from annotation_path dictionary on which evaluation will be launched\n  vis_num = 12                  # Select random images for visualization in the TensorBoard\n  save_images_step = 2          # Save images every 2-th evaluation\n  batch_size = 8                # Number of images in the batch\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""             # Environment variable to control CUDA device used for evaluation\n    per_process_gpu_memory_fraction = 0.5  # Fix extra memory allocation issue\n    allow_growth = True                    # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 1\n    inter_op_parallelism_threads = 1\n    transformer_parallel_calls = 1  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 1   # Number of batches to prefetch\n\n\nclass infer:\n  out_subdir = ""predictions""  # Name of folder in model directory where output json files with detections will be saved\n  batch_size = 32             # Number of images in the batch\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""             # Environment variable to control cuda device used for training\n    per_process_gpu_memory_fraction = 0.5  # Fix extra memory allocation issue\n    allow_growth = True                    # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 2\n    inter_op_parallelism_threads = 8\n    transformer_parallel_calls = 4  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 8   # Number of batches to prefetch\n\n\ninput_shape = (256, 256, 3)  # Input shape of the model (width, height, channels)\nclasses = ObjectDetectorJson.get_classes_from_coco_annotation(os.path.join(CURRENT_DIR, train.annotation_path))\nMODEL_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                         \'model\')  # Path to the folder where all training and evaluation artifacts will be located\nif not os.path.exists(MODEL_DIR):\n  os.makedirs(MODEL_DIR)\n\n\ndef learning_rate_schedule():  # Function which controls learning rate during training\n  import tensorflow as tf\n  return tf.train.exponential_decay(\n    learning_rate=0.004,\n    global_step=tf.train.get_or_create_global_step(),\n    decay_steps=800000,\n    decay_rate=0.95,\n    staircase=True)\n\n\ndef optimizer(learning_rate):\n  import tensorflow as tf\n  return tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1.0)\n\n\ndetector_params = {\n  ""num_classes"": len(classes),  # Number of classes to detect\n  ""priors_rule"": ""object_detection_api"",    # Prior boxes rule for SSD, possible options: \'caffe\', \'object_detection_api\', \'custom\'\n  ""mobilenet_version"": ""v2"",                # Version of mobilenet backbone, possible options: \'v1\', \'v2\'\n  ""initial_weights_path"": """",               # Path to initial weights\n  ""depth_multiplier"": 1.0,                  # MobileNet channels multiplier\n  ""weight_regularization"": 4e-5,            # L2 weight regularization\n  ""learning_rate"": learning_rate_schedule,  # Learning rate\n  ""optimizer"": optimizer,                   # Optimizer\n  ""collect_priors_summary"": False,          # Option to collect priors summary for further analysis\n}\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/__init__.py,0,b'from tfutils.helpers import import_research_models\n\nimport_research_models()\n'
tensorflow_toolkit/ssd_detector/ssd_detector/trainer.py,42,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport math\nimport os\nimport pickle\nimport random\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.control_flow_ops import with_dependencies\n\nfrom ssd_detector.networks.mobilenet_ssd import MobileNetSSD\nfrom ssd_detector.readers.object_detector_json import ObjectDetectorJson\nfrom ssd_detector.toolbox.loss import MultiboxLoss\nfrom ssd_detector.toolbox.transformer import AnnotatedDataTransformer\nfrom ssd_detector.toolbox.summary import create_tensors_and_streaming_ops_for_assigned_priors, \\\n  get_detailed_assigned_priors_summary_tf, write_histogram_2d_tf\n\n\ndef create_session(config, type):\n  if type == \'train\':\n    random_seed = config.train.random_seed\n  else:\n    random_seed = 666\n\n  np.random.seed(random_seed)\n  tf.set_random_seed(random_seed)\n  random.seed(random_seed)\n\n  config_type = getattr(config, type).execution\n\n  if hasattr(config_type, \'CUDA_VISIBLE_DEVICES\'):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = config_type.CUDA_VISIBLE_DEVICES\n\n  intra_op_parallelism_threads = config_type.intra_op_parallelism_threads if \\\n    hasattr(config_type, \'intra_op_parallelism_threads\') else 0\n  inter_op_parallelism_threads = config_type.inter_op_parallelism_threads if \\\n    hasattr(config_type, \'inter_op_parallelism_threads\') else 0\n  session_config = tf.ConfigProto(allow_soft_placement=True,\n                                  intra_op_parallelism_threads=intra_op_parallelism_threads,\n                                  inter_op_parallelism_threads=inter_op_parallelism_threads)\n  if hasattr(config_type, \'per_process_gpu_memory_fraction\'):\n    session_config.gpu_options.per_process_gpu_memory_fraction = config_type.per_process_gpu_memory_fraction\n  if hasattr(config_type, \'allow_growth\'):\n    session_config.gpu_options.allow_growth = config_type.allow_growth\n\n  return session_config\n\n\n# pylint: disable=too-many-instance-attributes\nclass InputValData:\n  # pylint: disable=too-many-arguments\n  def __init__(self, batch_size, input_shape, json_path, classes, num_parallel_calls=2, prefetch_size=2):\n    self.batch_size = batch_size\n    self.input_shape = input_shape\n    self.json_path = json_path\n    self.num_parallel_calls = num_parallel_calls\n    self.prefetch_size = prefetch_size\n\n    ObjectDetectorJson.init_cache(self.json_path, cache_type=\'NONE\', classes=classes)\n\n    dataset, self.dataset_size = ObjectDetectorJson.create_dataset(self.json_path, classes=classes)\n    _, self.transform_param = MobileNetSSD.create_transform_parameters(*input_shape[:2])\n    self.transformer = AnnotatedDataTransformer(self.transform_param, is_training=False)\n\n    print(\'Total evaluation steps: {}\'.format(math.ceil(self.dataset_size / self.batch_size)))\n\n    transform_fn = lambda value: ObjectDetectorJson.transform_fn(value, self.transformer)\n    map_fn = lambda value: tf.py_func(transform_fn, [value], (tf.float32, tf.string))\n    self.dataset = dataset.map(map_fn, num_parallel_calls=num_parallel_calls)\n    self.dataset = self.dataset.batch(self.batch_size).prefetch(prefetch_size)\n\n  def input_fn(self):\n    images, annotation = self.dataset.make_one_shot_iterator().get_next()\n    images.set_shape([None] + list(self.input_shape))\n    return images, annotation\n\n  @staticmethod\n  def sample_data(json_path, num_samples, input_shape, classes, seed=666):\n    if num_samples == 0:\n      return None\n\n    data, _ = ObjectDetectorJson.json_iterator(json_path, classes)\n    data = [x for x in data()]\n    # data = ObjectDetectorJson.convert_coco_to_toolbox_format(COCO(json_path), classes)\n\n    ObjectDetectorJson.init_cache(json_path, cache_type=\'NONE\', classes=classes)\n\n    rng = random.Random(seed)\n    selected_items = rng.sample(range(len(data)), num_samples)\n\n    _, transform_param = MobileNetSSD.create_transform_parameters(*input_shape[:2])\n    transformer = AnnotatedDataTransformer(transform_param, is_training=False)\n\n    transform_fn = lambda value: ObjectDetectorJson.transform_fn(value, transformer, add_original_image=True)\n    return [transform_fn(data[i]) for i in selected_items]\n\n\nclass InputTrainData:\n  # pylint: disable=dangerous-default-value,too-many-arguments\n  def __init__(self, batch_size, input_shape, json_path, cache_type=\'NONE\', classes=[\'bg\'],\n               fill_with_current_image_mean=True, num_parallel_calls=4, prefetch_size=16):\n    self.batch_size = batch_size\n    self.input_shape = input_shape\n    self.json_path = json_path\n    self.cache_type = cache_type\n    self.num_parallel_calls = num_parallel_calls\n    self.prefetch_size = prefetch_size\n    self.classes = classes\n\n    ObjectDetectorJson.init_cache(self.json_path, cache_type, classes=classes)\n\n    self.train_dataset, self.dataset_size = ObjectDetectorJson.create_dataset(self.json_path, classes)\n    self.train_transform_param, _ = MobileNetSSD.create_transform_parameters(input_shape[0], input_shape[1],\n                                                                             fill_with_current_image_mean)\n    self.train_transformer = AnnotatedDataTransformer(self.train_transform_param, is_training=True)\n\n  def input_fn(self):\n    transform_fn = lambda value: ObjectDetectorJson.transform_fn(value, self.train_transformer,\n                                                                 cache_type=self.cache_type)\n\n    def transform_batch_fn(value):\n      images = []\n      annotations = []\n      for val in value:\n        img, annot = transform_fn(val)\n        images.append(img)\n        annotations.append(annot)\n      return images, annotations\n\n    map_fn_batch = lambda value: tf.py_func(transform_batch_fn, [value], (tf.float32, tf.string))\n\n    dataset = self.train_dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=self.dataset_size))\n    dataset = dataset.batch(self.batch_size).map(map_fn_batch, num_parallel_calls=self.num_parallel_calls)\n    dataset = dataset.prefetch(self.prefetch_size)\n\n    images, annotation = dataset.make_one_shot_iterator().get_next()\n\n    images.set_shape([self.batch_size] + list(self.input_shape))\n    return images, annotation\n\n\nclass InputInferData:\n  # pylint: disable=too-many-arguments\n  def __init__(self, path_to_video, input_shape, batch_size, num_parallel_calls=4, prefetch_size=32):\n    self.path_to_video = path_to_video\n    self.input_shape = input_shape\n    self.cap = cv2.VideoCapture(self.path_to_video)\n    self.num_parallel_calls = num_parallel_calls\n    self.prefetch_size = prefetch_size\n    self.batch_size = batch_size\n\n  @staticmethod\n  def transform_fn(value, transformer):\n    image = pickle.loads(value).astype(np.float32)\n    transformed_image, _ = transformer.transform(image, {})\n    return transformed_image.astype(np.float32)\n\n\n  def create_dataset(self):\n    def generator():\n      while self.cap.isOpened():\n        ret, frame = self.cap.read()\n        if ret:\n          yield pickle.dumps(frame.astype(np.float32))\n\n    return tf.data.Dataset.from_generator(generator, tf.string, tf.TensorShape([]))\n\n\n  def input_fn(self):\n    dataset = self.create_dataset()\n\n    _, transform_param = MobileNetSSD.create_transform_parameters(*self.input_shape[:2])\n    transformer = AnnotatedDataTransformer(transform_param, is_training=False)\n\n    transform_fn = lambda value: InputInferData.transform_fn(value, transformer)\n    map_fn = lambda value: tf.py_func(transform_fn, [value], tf.float32)\n    dataset = dataset.map(map_fn, num_parallel_calls=self.num_parallel_calls).batch(self.batch_size).prefetch(\n      self.prefetch_size)\n\n    image = dataset.make_one_shot_iterator().get_next()\n    image.set_shape([None] + list(self.input_shape))\n    return image\n\n\n# pylint: disable=too-many-locals,too-many-statements\ndef detection_model(features, labels, mode, params):\n  num_classes = params[\'num_classes\']\n  initial_weights_path = params.get(\'initial_weights_path\', \'\')\n  log_dir = params[\'log_dir\']\n  collect_priors_summary = params[\'collect_priors_summary\']\n\n  data_format = params.get(\'data_format\', \'NHWC\')\n  depth_multiplier = params.get(\'depth_multiplier\', 1.0)\n  priors_rule = params.get(\'priors_rule\', \'caffe\')\n  custom_priors = params.get(\'priors\', [])\n  learning_rate = params.get(\'learning_rate\', 0.01)\n  steps_per_epoch = params.get(\'steps_per_epoch\', 1)\n  mobilenet_version = params.get(\'mobilenet_version\', \'v2\')\n  weight_regularization = params.get(\'weight_regularization\', 4e-5)\n  optimizer_func = params.get(\'optimizer\', lambda learning_rate: tf.train.AdagradOptimizer(learning_rate=learning_rate))\n\n  # Override default FileWriter. Don\'t store the graph definition.\n  # pylint: disable=protected-access\n  tf.summary.FileWriterCache._cache[log_dir] = tf.summary.FileWriter(log_dir, graph=None)\n\n  if callable(learning_rate):\n    learning_rate = learning_rate()\n\n  is_training = mode == tf.estimator.ModeKeys.TRAIN\n\n  ssd = MobileNetSSD(input_tensor=features, num_classes=num_classes, depth_multiplier=depth_multiplier,\n                     is_training=is_training, data_format=data_format, priors_rule=priors_rule,\n                     priors=custom_priors, mobilenet_version=mobilenet_version,\n                     weight_regularization=weight_regularization)  # 1. Build model\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    decoded_predictions = ssd.detection_output(use_plain_caffe_format=False)\n    return tf.estimator.EstimatorSpec(mode, predictions=decoded_predictions)\n\n  assert mode in(tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL)\n  targets = ssd.create_targets(labels)  # 2. Build GT from annotation\n\n  if collect_priors_summary:\n    with tf.name_scope(\'summary/\'):\n      assigned_priors = create_tensors_and_streaming_ops_for_assigned_priors(targets, ssd.priors_info, num_classes)\n      detailed_assigned_priors = get_detailed_assigned_priors_summary_tf(assigned_priors, ssd.priors_info)\n\n  loss_func = MultiboxLoss(neg_pos_ratio=3.0)  # 3. Build loss-object\n\n  eval_iteration = tf.get_variable(\'eval_iteration\', initializer=0, dtype=tf.int32, trainable=False)\n  if mode == tf.estimator.ModeKeys.EVAL:\n    eval_print_steps = steps_per_epoch // 50\n    eval_print_steps = 1 if eval_print_steps == 0 else eval_print_steps\n\n    every_eval_print_steps = tf.equal(tf.mod(eval_iteration + 1, eval_print_steps), 0)\n    eval_iteration = tf.assign(eval_iteration, eval_iteration + 1)\n    targets = with_dependencies([eval_iteration], targets)\n\n    loss = loss_func.eval_summary(targets, ssd.predictions)\n    loss = tf.cond(every_eval_print_steps,\n                   lambda: tf.Print(loss, [tf.round(100 * eval_iteration / steps_per_epoch), loss], \'[%][loss]: \'),\n                   lambda: loss)\n\n    eval_metric_ops = {}\n    for key, val in loss_func.eval_tensors.items():\n      eval_metric_ops[\'loss_function/\' + key] = tf.metrics.mean(val)\n\n    if collect_priors_summary:\n      for key, metric_ops in assigned_priors.items():  # We need only update ops\n        eval_metric_ops[key] = metric_ops\n\n      for key, assigned_priors_tensor in detailed_assigned_priors.items():\n        eval_metric_ops[\'prior_histogram/\' + key] = (assigned_priors_tensor, tf.no_op())\n\n    decoded_predictions = ssd.detection_output(use_plain_caffe_format=False)\n    eval_metric_ops[\'predictions\'] = tf.contrib.metrics.streaming_concat(decoded_predictions)\n\n    return tf.estimator.EstimatorSpec(\n      mode,\n      loss=loss,\n      eval_metric_ops=eval_metric_ops\n    )\n\n  assert mode == tf.estimator.ModeKeys.TRAIN\n  if initial_weights_path:\n    tf.logging.info(\'Initialize from: \' + initial_weights_path)\n    ssd.load_weights(initial_weights_path)\n\n  bboxes = ssd._decode_boxes(ssd.predictions[\'locs\'], priors=ssd.priors[0, 0], variance=ssd.priors[0, 1])\n  loss = loss_func.loss(targets, ssd.predictions, bboxes)  # 4. Compute loss with NMS\n\n  if collect_priors_summary:\n    with tf.name_scope(\'summary/\'):\n      loss = with_dependencies([operation for key, (_, operation) in assigned_priors.items()], loss)\n\n    for name, assigned_priors_tensor in detailed_assigned_priors.items():\n      tf.summary.scalar(name, tf.reduce_sum(assigned_priors_tensor))\n\n    py_func_ops = []\n    priors_dir = os.path.join(log_dir, \'priors\')\n\n    with tf.name_scope(\'write_histogram\'):\n      every_epoch = tf.equal(tf.mod(tf.train.get_global_step() + 1, steps_per_epoch), 0)\n      for name, (group, _) in assigned_priors.items():\n        def write_hist2d():\n          # pylint: disable=cell-var-from-loop\n          return tf.py_func(write_histogram_2d_tf,\n                            [group, pickle.dumps(ssd.priors_info), name, tf.train.get_global_step(), priors_dir],\n                            tf.bool)\n\n        write_hist2d_once_per_epoch = tf.cond(every_epoch, write_hist2d, tf.no_op)\n        py_func_ops.append(write_hist2d_once_per_epoch)\n\n      loss = with_dependencies(py_func_ops, loss)\n\n  optimizer = optimizer_func(learning_rate)\n  tf.summary.scalar(\'learning_rate\', learning_rate)\n\n  regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n  regularization_loss = tf.add_n(regularization_losses, name=\'loss_function/regularization_losses_sum\')\n  total_loss = tf.add(loss, regularization_loss, name=\'loss_function/total_loss\')\n\n  tf.summary.scalar(\'loss_function/regularization_loss\', regularization_loss)\n\n  with tf.variable_scope(\'train_loop\'):\n    train_op = optimizer.minimize(total_loss, global_step=tf.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode, loss=total_loss, train_op=train_op)\n'"
tensorflow_toolkit/ssd_detector/tools/eval.py,17,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport math\nimport multiprocessing as mp\nimport os\nimport pickle\nimport sys\nimport time\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom cachetools import cached, Cache\n\nfrom ssd_detector.trainer import create_session, detection_model, InputValData\nfrom ssd_detector.toolbox.coco_metrics_eval import calc_coco_metrics\nfrom ssd_detector.toolbox.summary import group_ssd_heads, write_histogram_2d\nfrom tfutils.helpers import draw_bboxes, load_module\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Perform evaluation of a detection model\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\n\n@cached(Cache(100))\ndef load_coco(path):\n  from pycocotools.coco import COCO\n  return COCO(path)\n\n\n# pylint: disable=too-many-locals,too-many-arguments\ndef eval_dataset(annotations, config, eval_name, checkpoint_path, session_config, sample_images=None,\n                 dump_priors_info=True):\n  log_dir = os.path.join(config.MODEL_DIR, \'eval_\' + eval_name)\n  run_config = tf.estimator.RunConfig(session_config=session_config)\n\n  # Override default FileWriter. Don\'t store the graph definition.\n  # pylint: disable=protected-access\n  tf.summary.FileWriterCache._cache[log_dir] = tf.summary.FileWriter(log_dir, graph=None)\n\n  input_data = InputValData(config.eval.batch_size, config.input_shape, config.eval.annotation_path[eval_name],\n                            classes=config.classes,\n                            num_parallel_calls=config.eval.execution.transformer_parallel_calls,\n                            prefetch_size=config.eval.execution.transformer_prefetch_size)\n\n  config.detector_params[\'log_dir\'] = log_dir\n  config.detector_params[\'steps_per_epoch\'] = math.ceil(input_data.dataset_size / input_data.batch_size)\n\n  detector_params = config.detector_params.copy()\n  if not dump_priors_info:\n    detector_params[\'collect_priors_summary\'] = False\n  predictor = tf.estimator.Estimator(\n    model_fn=detection_model,\n    params=detector_params,\n    model_dir=config.MODEL_DIR,\n    config=run_config\n  )\n\n  eval_results = predictor.evaluate(input_fn=input_data.input_fn, name=eval_name, checkpoint_path=checkpoint_path)\n  writer = tf.summary.FileWriterCache.get(log_dir)\n  predictions = eval_results[\'predictions\']\n\n  if checkpoint_path is not None:\n    step = tf.train.load_variable(checkpoint_path, \'global_step\')\n  else:\n    step = tf.train.load_variable(config.MODEL_DIR, \'global_step\')\n\n  if dump_priors_info:\n    summaries = []\n    for key, assigned_priors in eval_results.items():\n      if key.startswith(\'prior_histogram/\'):\n        name = key.replace(\'prior_histogram/\', \'\', 1)\n        summaries.append(tf.Summary.Value(tag=name, simple_value=np.sum(assigned_priors)))\n    if summaries:\n      writer.add_summary(tf.Summary(value=summaries), step)\n\n    group = group_ssd_heads(eval_results)\n\n    write_histogram_2d(group, step, log_dir, use_lognorm=True)\n    write_histogram_2d(group, step, log_dir, use_lognorm=False)\n\n  metrics = calc_coco_metrics(annotations, predictions, config.classes)\n  summaries = [tf.Summary.Value(tag=\'accuracy/\' + name, simple_value=val) for name, val in metrics.items()]\n  writer.add_summary(tf.Summary(value=summaries), step)\n\n  if sample_images is not None:\n    preprocessed_images = [item[0] for item in sample_images]\n    annotations = [pickle.loads(item[1]) for item in sample_images]\n    images = [item[2] for item in sample_images]\n\n    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x=np.array(preprocessed_images),\n      y=None,\n      num_epochs=1,\n      batch_size=config.eval.batch_size,\n      shuffle=False\n    )\n\n    predictions = predictor.predict(input_fn=predict_input_fn)\n    predictions = [pred for pred in predictions]  # Get values from the generator\n\n    images = draw_bboxes(images, annotations, predictions, config.classes, conf_threshold=0.1)\n\n    def write_images(images, step, writer):\n      summaries = []\n      for idx, img in enumerate(images):\n        encoded_image = cv2.imencode(\'.jpg\', img)[1].tostring()\n        img_sum = tf.Summary.Image(encoded_image_string=encoded_image, height=img.shape[0], width=img.shape[1])\n        summaries.append(tf.Summary.Value(tag=\'img/{0}\'.format(idx), image=img_sum))\n\n      summary = tf.Summary(value=summaries)\n      writer.add_summary(summary, step)\n\n    write_images(images, step, writer)\n\n  writer.flush()\n\n\n@cached(Cache(100))\ndef get_sample_images(annotation_path, params):\n  return InputValData.sample_data(annotation_path, *pickle.loads(params))\n\n\ndef eval_once(config, checkpoint, save_sample_prediction, dump_priors_info=True):\n  session_config = create_session(config, \'eval\')\n  print(\'\\nEvaluating {0}\'.format(checkpoint))\n  print(\'=============================================================\')\n\n  for dataset_name in config.eval.datasets:\n    start = time.time()\n\n    if save_sample_prediction:\n      sample_images = get_sample_images(config.eval.annotation_path[dataset_name],\n                                        pickle.dumps((config.eval.vis_num, config.input_shape, config.classes)))\n    else:\n      sample_images = None\n\n    annotation = load_coco(config.eval.annotation_path[dataset_name])\n\n    proc = mp.Process(target=eval_dataset,\n                      args=(annotation, config, dataset_name, checkpoint, session_config, sample_images,\n                            dump_priors_info))\n    proc.start()\n    proc.join()\n\n    finish = time.time()\n    print(\'=============================================================\')\n    print(\'[{0}]: {1} evaluation time = {2}\\n\'.format(checkpoint, dataset_name, finish - start))\n\n\ndef eval_loop(config):\n  _ = create_session(config, \'eval\')\n  latest_checkpoint = None\n  wait_iters = 0\n  save_images_step = 0\n  dump_priors_info = True\n  while True:\n    new_checkpoint = tf.train.latest_checkpoint(config.MODEL_DIR)\n    if latest_checkpoint != new_checkpoint:\n      latest_checkpoint = new_checkpoint\n\n      save_sample_prediction = save_images_step % config.eval.save_images_step == 0 if \\\n        config.eval.save_images_step != 0 else False\n      eval_once(config, latest_checkpoint, save_sample_prediction, dump_priors_info)\n      dump_priors_info = False\n\n      save_images_step += 1\n      wait_iters = 0\n    else:\n      if wait_iters % 12 == 0:\n        sys.stdout.write(\'\\r\')\n        for _ in range(11 + wait_iters // 12):\n          sys.stdout.write(\' \')\n        sys.stdout.write(\'\\r\')\n        for _ in range(1 + wait_iters // 12):\n          sys.stdout.write(\'|\')\n      else:\n        sys.stdout.write(\'.\')\n      sys.stdout.flush()\n      time.sleep(5)\n      wait_iters += 1\n\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n  eval_loop(cfg)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.WARN)\n  tf.app.run(main)\n'"
tensorflow_toolkit/ssd_detector/tools/export.py,8,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport os\n\nimport tensorflow as tf\nfrom tfutils.helpers import dump_frozen_graph, load_module, execute_mo\nfrom ssd_detector.networks.mobilenet_ssd import MobileNetSSD\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Export model in IE format\')\n  parser.add_argument(\'--model_name\', default=\'vlp\')\n  parser.add_argument(\'--data_type\', default=\'FP32\', choices=[\'FP32\', \'FP16\'], help=\'Data type of IR\')\n  parser.add_argument(\'--output_dir\', default=None, help=\'Output Directory\')\n  parser.add_argument(\'--checkpoint\', default=None, help=\'Default: latest\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\n\ndef freezing_graph(config, checkpoint, output_dir):\n  if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n  detector_params = config.detector_params.copy()\n  with tf.Session() as sess:\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=(None,) + tuple(config.input_shape))\n\n    for unnecessary_param in [\'initial_weights_path\',\n                              \'learning_rate\',\n                              \'optimizer\',\n                              \'weights_decay_factor\',\n                              \'collect_priors_summary\']:\n      if unnecessary_param in detector_params:\n        del detector_params[unnecessary_param]\n\n    ssd = MobileNetSSD(input_tensor=input_tensor, is_training=False, **detector_params)\n    ssd.detection_output()\n    # For eval.py\n    tf.get_variable(\'eval_iteration\', initializer=0, dtype=tf.int32, trainable=False)\n    tf.get_variable(\'global_step\', initializer=tf.constant_initializer(0, dtype=tf.int64), shape=(), dtype=tf.int64,\n                    trainable=False)\n\n    train_param, _ = ssd.create_transform_parameters(width=config.input_shape[0], height=config.input_shape[1])\n\n    saver = tf.train.Saver()\n    saver.restore(sess, checkpoint)\n\n    mean_values = [train_param.mean_value for _ in range(3)]\n    print(mean_values)\n    print(train_param.scale)\n    print(1./train_param.scale)\n\n    ssd_config = ssd.get_config_for_tfmo()\n    graph_file = os.path.join(output_dir, \'graph.pb\')\n    frozen = dump_frozen_graph(sess, graph_file, ssd_config[\'output_nodes\'])\n\n    # Generate custom_operations_config for mo\n    ssd_config_path = frozen.replace(\'.pb.frozen\', \'.tfmo.json\')\n    with open(ssd_config_path, mode=\'w\') as file:\n      file.write(ssd_config[\'json\'])\n\n    return frozen, ssd_config_path, train_param, ssd_config\n\n\ndef main(_):\n  args = parse_args()\n  config = load_module(args.path_to_config)\n\n  checkpoint = args.checkpoint if args.checkpoint else tf.train.latest_checkpoint(config.MODEL_DIR)\n  print(checkpoint)\n  if not checkpoint or not os.path.isfile(checkpoint+\'.index\'):\n    raise FileNotFoundError(str(checkpoint))\n\n  step = checkpoint.split(\'-\')[-1]\n  output_dir = args.output_dir if args.output_dir else os.path.join(config.MODEL_DIR, \'export_{}\'.format(step))\n\n  # Freezing graph\n  frozen_dir = os.path.join(output_dir, \'frozen_graph\')\n  frozen_graph, ssd_config_path, train_param, ssd_config = freezing_graph(config, checkpoint, frozen_dir)\n\n  # Export to IR\n  export_dir = os.path.join(output_dir, \'IR\', args.data_type)\n  input_shape = [1] + list(config.input_shape) # Add batch size 1 in shape\n\n  scale = 1./train_param.scale\n  mean_value = [train_param.mean_value for _ in range(3)]\n  mo_params = {\n    \'model_name\': args.model_name,\n    \'output\': \',\'.join(ssd_config[\'cut_points\']),\n    \'input_shape\': input_shape,\n    \'scale\': scale,\n    \'mean_value\': mean_value,\n    \'tensorflow_use_custom_operations_config\': ssd_config_path,\n    \'data_type\':args.data_type,\n  }\n  execute_mo(mo_params, frozen_graph, export_dir)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/ssd_detector/tools/infer.py,5,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\nfrom tfutils.helpers import draw_bboxes\n\n\ndef load_graph(frozen_graph_filename):\n  with tf.gfile.GFile(frozen_graph_filename, \'rb\') as file:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(file.read())\n  with tf.Graph().as_default() as graph:\n    tf.import_graph_def(graph_def)\n  return graph\n\ndef display_license_plate(number, license_plate_img):\n  size = cv2.getTextSize(number, cv2.FONT_HERSHEY_SIMPLEX, 0.55, 2)\n  text_width = size[0][0]\n  text_height = size[0][1]\n\n  height, width, _ = license_plate_img.shape\n  license_plate_img = cv2.copyMakeBorder(license_plate_img, 0, text_height + 10, 0,\n                                         0 if text_width < width else text_width - width,\n                                         cv2.BORDER_CONSTANT, value=(255, 255, 255))\n  cv2.putText(license_plate_img, number, (0, height + text_height + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 0), 2)\n\n  return license_plate_img\n\ndef build_argparser():\n  parser = ArgumentParser()\n  parser.add_argument(\'--model\', help=\'Path to frozen graph file with a trained model.\', required=True, type=str)\n  parser.add_argument(\'input_image\', help=\'Image with license plate\')\n  return parser\n\n\ndef main():\n  args = build_argparser().parse_args()\n\n  graph = load_graph(args.model)\n  image = cv2.imread(args.input_image)\n\n  img = cv2.resize(image, (256, 256))\n  img = np.float32(img)\n  img = np.add(img, -127.5)\n  img = np.multiply(img, 1.0/127.5)\n\n  input = graph.get_tensor_by_name(""import/MobilenetV2/input:0"")\n  output = graph.get_tensor_by_name(""import/DetectionOutput/Reshape:0"")\n\n  with tf.Session(graph=graph) as sess:\n    results = sess.run(output, feed_dict={input: [img]})\n    img_to_display = draw_bboxes([image], [{}], results, [\'\', \'vehicle\', \'lp\'])[0]\n    cv2.imshow(\'\', img_to_display)\n    cv2.waitKey(0)\n\nif __name__ == ""__main__"":\n  main()\n'"
tensorflow_toolkit/ssd_detector/tools/infer_checkpoint.py,5,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport argparse\nimport json\nimport os\nimport pickle\n\nimport cv2\nimport tensorflow as tf\nfrom tfutils.helpers import load_module\nfrom ssd_detector.readers.object_detector_json import ObjectDetectorJson\nfrom ssd_detector.trainer import create_session, detection_model, InputInferData, InputValData\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser(\n    description=\'Perform inference of a detection model on a video file or an annotation file in JSON format\')\n\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n\n  input_type_group = parser.add_mutually_exclusive_group(required=True)\n  input_type_group.add_argument(\'--json\', dest=\'input_type\', action=\'store_const\', const=\'json\',\n                                help=\'Get images from annotation file in JSON format\')\n  input_type_group.add_argument(\'--video\', dest=\'input_type\', action=\'store_const\', const=\'video\',\n                                help=\'Get images from video file\')\n  input_type_group.set_defaults(input_type=\'json\')\n\n  parser.add_argument(\'--input\', help=\'Path to the input file\', required=True)\n  parser.add_argument(\'--conf_threshold\', type=float, help=\'Confidence threshold for detector\', default=0.1)\n\n  dump_to_json_group = parser.add_mutually_exclusive_group()\n  dump_to_json_group.add_argument(\'--dump-to-json\', dest=\'dump_to_json\', action=\'store_true\')\n  dump_to_json_group.add_argument(\'--no-dump-to-json\', dest=\'dump_to_json\', action=\'store_false\')\n  dump_to_json_group.set_defaults(dump_to_json=True)\n\n  show_group = parser.add_mutually_exclusive_group()\n  show_group.add_argument(\'--show\', dest=\'show\', action=\'store_true\')\n  show_group.add_argument(\'--no-show\', dest=\'show\', action=\'store_false\')\n  show_group.set_defaults(show=False)\n\n  dump_output_video_group = parser.add_mutually_exclusive_group()\n  dump_output_video_group.add_argument(\'--dump-to-video\', dest=\'dump_output_video\', action=\'store_true\')\n  dump_output_video_group.add_argument(\'--no-dump-to-video\', dest=\'dump_output_video\', action=\'store_false\')\n  dump_output_video_group.set_defaults(dump_output_video=False)\n\n  parser.add_argument(\'--path_to_output_video\', help=\'Path to output video with predictions\', default=\'output.avi\')\n  return parser.parse_args()\n\n\n# pylint: disable=too-many-locals\ndef process_image(predictions, img_size, img_id, conf_threshold, classes):\n  img_width, img_height = img_size\n\n  coco_detections = []\n  for prediction in predictions:\n    det_label = int(prediction[0])\n    det_conf = float(prediction[1])\n\n    if conf_threshold and det_conf < conf_threshold:\n      continue\n\n    top_left_x = float(prediction[2] * img_width)\n    top_left_y = float(prediction[3] * img_height)\n    bottom_right_x = float(prediction[4] * img_width)\n    bottom_right_y = float(prediction[5] * img_height)\n    if det_label >= len(classes):\n      print(\'Wrong label: {0}\'.format(det_label))\n      exit(1)\n\n    obj_width = round(bottom_right_x - top_left_x, 1)\n    obj_height = round(bottom_right_y - top_left_y, 1)\n\n    coco_det = dict()\n    coco_det[\'image_id\'] = img_id\n    coco_det[\'category_id\'] = det_label\n    coco_det[\'bbox\'] = [round(top_left_x, 1), round(top_left_y, 1), obj_width, obj_height]\n    coco_det[\'score\'] = det_conf\n    coco_detections.append(coco_det)\n\n  return coco_detections\n\n\ndef draw_detections(img, coco_detections):\n  for det in coco_detections:\n    x, y, width, height = det[\'bbox\'] # pylint: disable=invalid-name\n    top_left = (int(x), int(y))\n    bottom_right = (int(x + width), int(y + height))\n    cv2.rectangle(img, top_left, bottom_right, (255, 0, 0), 2)\n    cv2.putText(img, \'{0}: {1}\'.format(det[\'category_id\'], det[\'score\']), top_left,\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n  return img\n\n\n# pylint: disable=too-many-arguments\ndef predict_on_video(predictions, path_to_video, classes, show=False, conf_threshold=None, dump_output_video=False,\n                     path_to_output_video=\'output.avi\'):\n  output = []\n\n  cap = cv2.VideoCapture(path_to_video)\n  fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n  width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n  height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n  fps = cap.get(cv2.CAP_PROP_FPS)\n  if dump_output_video:\n    out = cv2.VideoWriter(path_to_output_video, fourcc, fps, (int(width), int(height)))\n  for i, pred in enumerate(predictions):\n    if cap.isOpened():\n      _, frame = cap.read()\n      det = process_image(pred[:, 1:], (width, height), i, conf_threshold, classes)\n      output.extend(det)\n      img = draw_detections(frame, det)\n      if show:\n        cv2.imshow(\'detections\', img)\n        key = cv2.waitKey(10)\n        if key == 27:\n          break\n      if dump_output_video:\n        out.write(img)\n    else:\n      break\n\n  cap.release()\n  if dump_output_video:\n    out.release()\n\n  return output\n\n\ndef predict_on_json(predictions, annotation_path, classes, show=False, conf_threshold=None, dump_output_video=False,\n                    path_to_output_video=\'output.avi\', width=640, height=480, fps=30):\n  annotation_generator, _ = ObjectDetectorJson.json_iterator(annotation_path, classes)\n  annotation_data = [pickle.loads(x) for x in annotation_generator()]\n\n  output = []\n  if dump_output_video:\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    out = cv2.VideoWriter(path_to_output_video, fourcc, fps, (int(width), int(height)))\n  for i, pred in enumerate(predictions):\n    annotation = annotation_data[i]\n    image_size = annotation[\'image_size\']\n    img_path = annotation[\'image\']\n    img_id = annotation[\'image_id\']\n    det = process_image(pred[:, 1:], image_size, img_id, conf_threshold, classes)\n    output.extend(det)\n    frame = cv2.imread(img_path)\n    frame = cv2.resize(frame, tuple(image_size))\n    img = draw_detections(frame, det)\n    if show:\n      cv2.imshow(\'detections\', img)\n      key = cv2.waitKey(10)\n      if key == 27:\n        break\n    if dump_output_video:\n      img_resized = cv2.resize(img, (width, height))\n      out.write(img_resized)\n  if dump_output_video:\n    out.release()\n\n  return output\n\n\ndef infer(config, source, path, conf_threshold=None, dump_to_json=False,\n          show=False, dump_output_video=False, path_to_output_video=\'output.avi\'):\n  session_config = create_session(config, \'infer\')\n\n  out_dir = os.path.join(config.MODEL_DIR, config.infer.out_subdir)\n  if not os.path.exists(out_dir):\n    os.mkdir(out_dir)\n\n  run_config = tf.estimator.RunConfig(session_config=session_config)\n\n  config.detector_params[\'log_dir\'] = config.MODEL_DIR\n\n  predictor = tf.estimator.Estimator(model_fn=detection_model,\n                                     params=config.detector_params, model_dir=config.MODEL_DIR, config=run_config)\n\n  checkpoint_path = tf.train.latest_checkpoint(config.MODEL_DIR)\n\n  basename = os.path.basename(path)\n  filename = os.path.splitext(basename)[0]\n  name = \'{0}.json\'.format(filename)\n  output_json_path = os.path.join(out_dir, name)\n\n  if source == \'video\':\n    input_data = InputInferData(path, config.input_shape, config.infer.batch_size)\n  elif source == \'json\':\n    input_data = InputValData(batch_size=config.infer.batch_size, input_shape=config.input_shape, json_path=path,\n                              classes=config.classes,\n                              num_parallel_calls=config.infer.execution.transformer_parallel_calls,\n                              prefetch_size=config.infer.execution.transformer_prefetch_size)\n\n  predictions = predictor.predict(input_fn=input_data.input_fn, checkpoint_path=checkpoint_path)\n  if source == \'video\':\n    predictions = predict_on_video(predictions, path, config.classes, show, conf_threshold, dump_output_video,\n                                   path_to_output_video)\n  elif source == \'json\':\n    predictions = predict_on_json(predictions, path, config.classes, show, conf_threshold, dump_output_video,\n                                  path_to_output_video)\n\n  if dump_to_json:\n    with open(output_json_path, \'w\') as output_file:\n      json.dump(predictions, output_file, sort_keys=True, indent=4)\n\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n\n  infer(cfg, args.input_type, args.input, args.conf_threshold, args.dump_to_json, args.show,\n        args.dump_output_video, args.path_to_output_video)\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/ssd_detector/tools/infer_ie.py,0,"b'from __future__ import print_function\nfrom argparse import ArgumentParser\nimport json\nimport logging as log\nimport os\nimport sys\n\nimport cv2\nfrom pycocotools.coco import COCO\n\nfrom openvino.inference_engine import IENetwork, IEPlugin\nfrom ssd_detector.readers.object_detector_json import ObjectDetectorJson\n\n\ndef build_argparser():\n  parser = ArgumentParser()\n  parser.add_argument(""-m"", ""--model"", help=""Path to an .xml file with a trained model."", required=True, type=str)\n  parser.add_argument(""-it"", ""--input_type"",\n                      help=""Input type for the model, could be video file or annotation file in JSON format or \'cam\'""\n                           ""for capturing video stream from camera"",\n                      default=""json"", choices=[""video"", ""json"", ""cam""], required=True, )\n  parser.add_argument(""-i"", ""--input"", help=""Path to video file or annotation file in json format"", required=True,\n                      type=str)\n  parser.add_argument(""-l"", ""--cpu_extension"",\n                      help=""MKLDNN (CPU)-targeted custom layers.Absolute path to a shared library with the kernels ""\n                           ""impl."", type=str, default=None)\n  parser.add_argument(""-pp"", ""--plugin_dir"", help=""Path to a plugin folder"", type=str, default=None)\n  parser.add_argument(""-d"", ""--device"",\n                      help=""Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. Sample ""\n                           ""will look for a suitable plugin for device specified (CPU by default)"", default=""CPU"",\n                      type=str)\n  parser.add_argument(""-pt"", ""--prob_threshold"", help=""Probability threshold for detections filtering"",\n                      default=0.5, type=float)\n  parser.add_argument(""--dump_predictions_to_json"", help=""Dump predictions to json file"", default=True)\n  parser.add_argument(""--output_json_path"", help=""Path to output json file with predictions"",\n                      default=""ie_preidctions.json"")\n  parser.add_argument(""--show"", dest=\'show\', action=\'store_true\', help=""Show predictions"")\n  parser.add_argument(""--dump_output_video"", help=""Save output video with predictions"", default=True)\n  parser.add_argument(""--path_to_output_video"", help=""Path to output video with predictions"", default=""output.avi"")\n  parser.add_argument(""--delay"", help=""Delay before show next image"", default=10, type=int)\n  return parser\n\n\nclass Input:\n  def __init__(self, input_type, input):\n    self.input_type = input_type\n    self.item_counter = 0\n\n    assert input_type in (\'json\', \'video\', \'cam\')\n\n    if input_type == \'json\':\n      coco_annotation = COCO(input)\n      annotation_directory = os.path.join(os.getcwd(), os.path.dirname(input))\n      classes = ObjectDetectorJson.get_classes_from_coco_annotation(input)\n      self.json_data = ObjectDetectorJson.convert_coco_to_toolbox_format(coco_annotation, classes,\n                                                                         annotation_directory)\n    if input_type == \'video\':\n      self.cap = cv2.VideoCapture(input)\n\n    if input_type == \'cam\':\n      self.cap = cv2.VideoCapture(0)\n\n  def get_next_item(self):\n    assert self.input_type in (\'json\', \'video\', \'cam\')\n\n    if self.input_type == \'json\':\n      image_size = self.json_data[self.item_counter][\'image_size\']\n      img = cv2.imread(self.json_data[self.item_counter][\'image\'])\n      img = cv2.resize(img, tuple(image_size))\n      annot = self.json_data[self.item_counter][\'image_id\']\n      self.item_counter += 1\n      return img, annot\n\n    if self.input_type == \'cam\' or \'video\':\n      _, img = self.cap.read()\n      self.item_counter += 1\n      return img, self.item_counter\n\n    return None, None\n\n  def is_finished(self):\n    assert self.input_type in (\'json\', \'video\', \'cam\')\n\n    if self.input_type == \'json\':\n      return self.item_counter >= len(self.json_data)\n\n    if self.input_type == \'cam\' or \'video\':\n      return not self.cap.isOpened()\n\n    return None, None\n\n\n# pylint: disable=too-many-locals,too-many-statements\ndef main():\n  log.basicConfig(format=""[ %(levelname)s ] %(message)s"", level=log.INFO, stream=sys.stdout)\n  args = build_argparser().parse_args()\n  model_xml = args.model\n  model_bin = os.path.splitext(model_xml)[0] + "".bin""\n  # Plugin initialization for specified device and load extensions library if specified\n  log.info(""Initializing plugin for %s device..."", args.device)\n  plugin = IEPlugin(device=args.device, plugin_dirs=args.plugin_dir)\n  if args.cpu_extension and \'CPU\' in args.device:\n    plugin.add_cpu_extension(args.cpu_extension)\n\n  # Read IR\n  log.info(""Reading IR..."")\n  net = IENetwork(model=model_xml, weights=model_bin)\n\n  if ""CPU"" in plugin.device:\n    supported_layers = plugin.get_supported_layers(net)\n    not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n    if not_supported_layers:\n      log.error(""Following layers are not supported by the plugin for specified device %s:\\n %s"",\n                plugin.device, \', \'.join(not_supported_layers))\n      log.error(""Please try to specify cpu extensions library path in sample\'s command line parameters using -l ""\n                ""or --cpu_extension command line argument"")\n      sys.exit(1)\n  assert len(net.inputs.keys()) == 1, ""Sample supports only single input topologies""\n  assert len(net.outputs) == 1, ""Sample supports only single output topologies""\n  input_blob = next(iter(net.inputs))\n  out_blob = next(iter(net.outputs))\n  log.info(""Loading IR to the plugin..."")\n  exec_net = plugin.load(network=net, num_requests=2)\n\n  # Read and pre-process input image\n  batch, channels, height, width = net.inputs[input_blob].shape\n  del net\n\n  predictions = []\n  data = Input(args.input_type, args.input)\n  cur_request_id = 0\n\n  fps = 25\n  out_width = 640\n  out_height = 480\n  if args.dump_output_video:\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    out = cv2.VideoWriter(args.path_to_output_video, fourcc, fps, (int(out_width), int(out_height)))\n\n  while not data.is_finished():\n    frame, img_id = data.get_next_item()\n    initial_h, initial_w, _ = frame.shape\n    in_frame = cv2.resize(frame, (width, height))\n    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n    in_frame = in_frame.reshape((batch, channels, height, width))\n\n    exec_net.start_async(request_id=cur_request_id, inputs={input_blob: in_frame})\n    if exec_net.requests[cur_request_id].wait(-1) == 0:\n\n      # Parse detection results of the current request\n      res = exec_net.requests[cur_request_id].outputs[out_blob]\n      coco_detections = []\n      for obj in res[0][0]:\n        # Draw only objects when probability more than specified threshold\n        if obj[2] > args.prob_threshold:\n          top_left_x = float(obj[3] * initial_w)\n          top_left_y = float(obj[4] * initial_h)\n          bottom_right_x = float(obj[5] * initial_w)\n          bottom_right_y = float(obj[6] * initial_h)\n\n          obj_width = round(bottom_right_x - top_left_x, 1)\n          obj_height = round(bottom_right_y - top_left_y, 1)\n          class_id = int(obj[1])\n\n          coco_det = {}\n          coco_det[\'image_id\'] = img_id\n          coco_det[\'category_id\'] = class_id\n          coco_det[\'bbox\'] = [round(top_left_x, 1), round(top_left_y, 1), obj_width, obj_height]\n          coco_det[\'score\'] = round(float(obj[2]), 1)\n          coco_detections.append(coco_det)\n\n          # Draw box and label\\class_id\n          cv2.rectangle(frame, (int(top_left_x), int(top_left_y)), (int(bottom_right_x), int(bottom_right_y)),\n                        (255, 0, 0), 2)\n          cv2.putText(frame, str(class_id) + \' \' + str(round(obj[2] * 100, 1)) + \' %\',\n                      (int(top_left_x), int(top_left_y) - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n      predictions.extend(coco_detections)\n\n    if args.dump_output_video:\n      img_resized = cv2.resize(frame, (out_width, out_height))\n      out.write(img_resized)\n\n    if args.show:\n      cv2.imshow(""Detection Results"", frame)\n      key = cv2.waitKey(args.delay)\n      if key == 27:\n        break\n\n  if args.dump_predictions_to_json:\n    with open(args.output_json_path, \'w\') as output_file:\n      json.dump(predictions, output_file, sort_keys=True, indent=4)\n\n  cv2.destroyAllWindows()\n  del exec_net\n  del plugin\n\n\nif __name__ == \'__main__\':\n  sys.exit(main() or 0)\n'"
tensorflow_toolkit/ssd_detector/tools/train.py,4,"b""import argparse\nimport os\n\nimport cv2\nimport tensorflow as tf\n\nfrom tfutils.helpers import load_module\nfrom ssd_detector.trainer import create_session, detection_model, InputTrainData\n\n\nos.environ['MKL_NUM_THREADS'] = '1'\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description='Perform training of a detection model')\n  parser.add_argument('path_to_config', help='Path to a config.py')\n  return parser.parse_args()\n\n\ndef train(config):\n  cv2.setNumThreads(1)\n\n  session_config = create_session(config, 'train')\n\n  input_data = InputTrainData(batch_size=config.train.batch_size, input_shape=config.input_shape,\n                              json_path=config.train.annotation_path,\n                              fill_with_current_image_mean=config.train.fill_with_current_image_mean,\n                              cache_type=config.train.cache_type,\n                              classes=config.classes,\n                              num_parallel_calls=config.train.execution.transformer_parallel_calls,\n                              prefetch_size=config.train.execution.transformer_prefetch_size)\n\n  steps_per_epoch = config.train.save_checkpoints_steps if config.train.save_checkpoints_steps \\\n    else input_data.dataset_size // input_data.batch_size\n\n  run_config = tf.estimator.RunConfig(session_config=session_config,\n                                      keep_checkpoint_every_n_hours=config.train.keep_checkpoint_every_n_hours,\n                                      save_summary_steps=config.train.save_summary_steps,\n                                      save_checkpoints_steps=steps_per_epoch,\n                                      tf_random_seed=config.train.random_seed)\n\n  config.detector_params['steps_per_epoch'] = steps_per_epoch\n  config.detector_params['log_dir'] = config.MODEL_DIR\n\n  predictor = tf.estimator.Estimator(\n    model_fn=detection_model,\n    params=config.detector_params,\n    model_dir=config.MODEL_DIR,\n    config=run_config\n  )\n\n  predictor.train(input_fn=input_data.input_fn, steps=config.train.steps, max_steps=config.train.max_steps)\n\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n  train(cfg)\n\n\nif __name__ == '__main__':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n"""
tensorflow_toolkit/ssd_detector/vlp/config.py,8,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n# pylint: disable=line-too-long\n\nimport os\nimport matplotlib; matplotlib.use(\'Agg\')  # pylint: disable=multiple-statements\nfrom ssd_detector.readers.object_detector_json import ObjectDetectorJson\n\nCURRENT_DIR = os.path.dirname(os.path.realpath(__file__))\nROOT_DIR = os.path.normpath(os.path.join(CURRENT_DIR, ""../../..""))\n\n# See more details about parameters in TensorFlow documentation tf.estimator\nclass train:\n  annotation_path = os.path.join(ROOT_DIR, ""./data/vlp_test/annotations_train.json"")  # Path to the annotation file\n  cache_type = ""ENCODED""  # Type of data to save in memory, possible options: \'FULL\', \'ENCODED\', \'NONE\'\n\n  batch_size = 32                    # Number of images in the batch\n  steps = 65000                      # Number of steps for which to train model\n  max_steps = None                   # Number of total steps for which to train model\n  save_checkpoints_steps = 1000      # Number of training steps when checkpoint should be saved\n  keep_checkpoint_every_n_hours = 6  # Checkpoint should be saved forever after every n hours\n  save_summary_steps = 100           # Number of steps when the summary information should be saved\n  random_seed = 666                  # Random seed\n\n  fill_with_current_image_mean = True  # Parameter of data transformer\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""             # Environment variable to control CUDA device used for training\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 2\n    inter_op_parallelism_threads = 8\n    transformer_parallel_calls = 4  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 8   # Number of batches to prefetch\n\n\nclass eval:\n  annotation_path = {\n    ""train"": os.path.join(ROOT_DIR, ""./data/vlp_test/annotations_train.json""),\n    ""test"": os.path.join(ROOT_DIR, ""./data/vlp_test/annotations_test.json"")\n  }  # Dictionary with paths to annotations and its short names which will be displayed in the TensorBoard\n  datasets = [""train"", ""test""]  # List of names from annotation_path dictionary on which evaluation will be launched\n  vis_num = 2                  # Select random images for visualization in the TensorBoard\n  save_images_step = 2          # Save images every 2-th evaluation\n  batch_size = 2                # Number of images in the batch\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""             # Environment variable to control CUDA device used for evaluation\n    per_process_gpu_memory_fraction = 0.5  # Fix extra memory allocation issue\n    allow_growth = True                    # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 1\n    inter_op_parallelism_threads = 1\n    transformer_parallel_calls = 1  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 1   # Number of batches to prefetch\n\n\nclass infer:\n  out_subdir = ""predictions""  # Name of folder in model directory where output json files with detections will be saved\n  batch_size = 32             # Number of images in the batch\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""             # Environment variable to control cuda device used for training\n    per_process_gpu_memory_fraction = 0.5  # Fix extra memory allocation issue\n    allow_growth = True                    # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 2\n    inter_op_parallelism_threads = 8\n    transformer_parallel_calls = 4  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 8   # Number of batches to prefetch\n\n\ninput_shape = (256, 256, 3)  # Input shape of the model (width, height, channels)\nclasses = ObjectDetectorJson.get_classes_from_coco_annotation(os.path.join(CURRENT_DIR, train.annotation_path))\nMODEL_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                         \'model\')  # Path to the folder where all training and evaluation artifacts will be located\nif not os.path.exists(MODEL_DIR):\n  os.makedirs(MODEL_DIR)\n\n\ndef learning_rate_schedule():  # Function which controls learning rate during training\n  import tensorflow as tf\n  step = tf.train.get_or_create_global_step()\n  lr = tf.case([(tf.less(step, 1000), lambda: tf.constant(0.0004)),\n                (tf.less(step, 10000), lambda: tf.constant(0.01)),\n                (tf.less(step, 40000), lambda: tf.constant(0.005)),\n                (tf.less(step, 55000), lambda: tf.constant(0.0005)),\n                (tf.less(step, 65000), lambda: tf.constant(0.00005))])\n  return lr\n\n\ndef optimizer(learning_rate):\n  import tensorflow as tf\n  return tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n\n\ndetector_params = {\n  ""num_classes"": len(classes),  # Number of classes to detect\n  ""priors_rule"": ""custom"",      # Prior boxes rule for SSD, possible options: \'caffe\', \'object_detection_api\', \'custom\'\n  ""priors"": [\n    [(0.068, 0.03), (0.052, 0.097)],\n    [(0.18, 0.087), (0.11, 0.33), (0.43, 0.1)],\n    [(0.26, 0.27), (0.34, 0.4), (0.2, 0.55)],\n    [(0.37, 0.52)],\n    [(0.48, 0.45)],\n    [(0.63, 0.64), (0.77, 0.77), (0.95, 0.95)]\n  ],\n  ""mobilenet_version"": ""v2"",                # Version of mobilenet backbone, possible options: \'v1\', \'v2\'\n  ""initial_weights_path"": """",               # Path to initial weights\n  ""depth_multiplier"": 0.35,                 # MobileNet channels multiplier\n  ""weight_regularization"": 1e-3,            # L2 weight regularization\n  ""learning_rate"": learning_rate_schedule,  # Learning rate\n  ""optimizer"": optimizer,                   # Optimizer\n  ""collect_priors_summary"": False,          # Option to collect priors summary for further analysis\n}\n'"
tensorflow_toolkit/ssd_mobilenet_fpn_602/tools/create_coco_tfrecord.py,21,"b'# Copyright 2019 Intel Corporation\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""This script helps generate similar training and validation splits\nas the ones used in TensorFlow Object Detection API, which is much\ndifferent than organic coco train2017 and val2017.\nBy selecting ~8k specific images(called minival) for validation,\ntotal training image set would be [coco_train2017+coco_val2017-minival]\n(called train_plus in this script).\n\nThis script is designed for benchmarking a fine-tuned model with\nretraining or transfer learning based on a TensorFlow Object\nDetection API pretrained model to have a fair and reliable validation\nresult, since the pretrained models are claimed having been trained with\nthe same splits.\n\nPlease follow below dataset structure:\n\\_dataset\n  \\_images\n  | \\_train2017\n  | | |_train_image0\n  | | |_train_image1\n  | | ...\n  | | |_train_image118286\n  | \\_val2017\n  | | |_val_image0\n  | | |_val_image1\n  | | ...\n  | | |_val_image4999\n  \\_annotations\n  | |_instances_train2017.json\n  | |_instances_val2017.json\n  |_mscoco_minival_ids.txt\n\nNOTE: `train2017` and `val2017` folders are supposed to exist at passed-in\n`--images` directory. `instances_train2017.json` and `instances_val2017.json`\nare supposed to exist at passed-in `--annotations` directory.\n""""""\n\nimport json\nimport hashlib\nimport os\nimport contextlib2\n\nimport tensorflow as tf\nfrom object_detection.dataset_tools import tf_record_creation_util\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\n# pylint: disable=too-many-locals,too-many-arguments\n\nflags = tf.app.flags\ntf.flags.DEFINE_string(\n        \'image_folder\',\n        None,\n        \'Folder of images, must include `train2017` and `val2017` folders.\')\ntf.flags.DEFINE_string(\n        \'annotation_folder\',\n        None,\n        \'Folder of annotations, must include `instances_train2017.json` \'\n        \'and `instances_val2017.json` files.\')\ntf.flags.DEFINE_string(\n        \'coco_minival_ids_file\',\n        None,\n        \'File of selected id in coco2017 for validation.\')\ntf.flags.DEFINE_string(\n        \'output_folder\',\n        None,\n        \'Folder of output tfrecord\')\nFLAGS = flags.FLAGS\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nTRAIN_FOLDER_NAME = \'train2017\'\nTRAIN_ANNOTATION_NAME = \'instances_train2017.json\'\nTRAIN_TFRECORD_NAME = \'coco_train2017_plus.record\'\nVAL_FOLDER_NAME = \'val2017\'\nVAL_ANNOTATION_NAME = \'instances_val2017.json\'\nVAL_TFRECORD_NAME = \'coco_minival2017.record\'\n\n\ndef create_tfrecord_from_coco(train_image_folder,\n                              val_image_folder,\n                              train_annotation_file,\n                              val_annotation_file,\n                              train_tfrecord,\n                              val_tfrecord,\n                              coco_minival_ids_file):\n    """"""Create tfrecord from organic coco dataset and annotations.\n\n    :param train_image_folder: folder of organic training images;\n    :param val_image_folder: folder of organic validation images;\n    :param train_annotation_file: file path to organic training annotations;\n    :param val_annotation_file: file path to organic validation annotations;\n    :param train_tfrecord: path and prefix of output training tfrecord;\n    :param val_tfrecord: path and prefix of output validation tfrecord;\n    :param coco_minival_ids_file: a file containing a list of minival coco id\n                                  for validation, this file could usually be found\n                                  as `mscoco_minival_ids.txt` at tensorflow/models\n                                  repository;\n    """"""\n    minival_list = parse_minival_ids(coco_minival_ids_file)\n    images, annotations, categories = merge_annotations(\n        train_annotation_file, val_annotation_file)\n\n    tf.logging.info(\'number of total images is {}\'.format(len(images)))\n    tf.logging.info(\'number of total annotations is {}\'.format(len(annotations)))\n\n    category_index = label_map_util.create_category_index(categories)\n    annotation_index = generate_annotation_index(annotations)\n    train_plus_images, minival_images = split_minival(images, minival_list)\n\n    tf.logging.info(\'category index is {}\'.format(category_index))\n    tf.logging.info(\'number of train plus images is {}\'\n                    .format(len(train_plus_images)))\n    tf.logging.info(\'number of minival images is {}\'\n                    .format(len(minival_images)))\n\n    image_folders = [train_image_folder, val_image_folder]\n    write_tfrecord(image_list=train_plus_images,\n                   anno_index=annotation_index,\n                   category_index=category_index,\n                   image_folders=image_folders,\n                   output_tfrecord=train_tfrecord)\n    write_tfrecord(image_list=minival_images,\n                   anno_index=annotation_index,\n                   category_index=category_index,\n                   image_folders=image_folders,\n                   output_tfrecord=val_tfrecord,\n                   tfrecord_shards=10)\n\n\ndef create_tf_example(image,\n                      anno_list,\n                      image_folders,\n                      category_index):\n    """"""Create a tf example for a single image.\n\n    :param image: coco-like image object;\n    :param anno_list: a list of coco-like annotation objects;\n    :param image_folders: a list of folders which may contain required image;\n    :param category_index: category index dictionary generated by `label_map_util`;\n    """"""\n    image_h = image[\'height\']\n    image_w = image[\'width\']\n    filename = image[\'file_name\']\n    image_id = image[\'id\']\n    if image_w <= 0 and image_h <= 0:\n        raise ValueError(\'image width or height is smaller than or equal to zero!\')\n\n    # Traverse all image folders since we\'re not sure where exactly the file is.\n    image_path = None\n    for image_folder in image_folders:\n        tmp_path = os.path.join(image_folder, filename)\n        if os.path.exists(tmp_path):\n            image_path = tmp_path\n            break\n    if image_path is None:\n        raise OSError(\'image_path {} does not exist!\'.format(image_path))\n\n    with tf.gfile.GFile(image_path, \'rb\') as f:\n        encoded_jpg = f.read()\n    if not encoded_jpg:\n        raise ValueError(\'cannot read image from image_path!\')\n\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n\n    x1s = []\n    y1s = []\n    x2s = []\n    y2s = []\n    is_crowd = []\n    category_names = []\n    category_ids = []\n    area = []\n\n    for anno in anno_list:\n        x1, y1, w, h = tuple(anno[\'bbox\'])\n        if w <= 0 and h <= 0:\n            raise ValueError(\'annotation w or h value is wrong\')\n\n        x2, y2 = x1+w, y1+h\n        if x1 < 0 or y1 < 0:\n            raise ValueError(\'annotation coordinates is out of image\')\n        if x2 > image_w or y2 > image_h:\n            raise ValueError(\'annotation coordinates is out of image\')\n\n        x1s.append(float(x1)/image_w)\n        y1s.append(float(y1)/image_h)\n        x2s.append(float(x2)/image_w)\n        y2s.append(float(y2)/image_h)\n        is_crowd.append(anno[\'iscrowd\'])\n        area.append(anno[\'area\'])\n\n        category_id = int(anno[\'category_id\'])\n        category_name = category_index[category_id][\'name\'].encode(\'utf8\')\n        category_ids.append(category_id)\n        category_names.append(category_name)\n\n    feature_dict = {\n        \'image/height\': dataset_util.int64_feature(image_h),\n        \'image/width\': dataset_util.int64_feature(image_w),\n        \'image/filename\': dataset_util.bytes_feature(filename.encode(\'utf8\')),\n        \'image/source_id\': dataset_util.bytes_feature(str(image_id).encode(\'utf8\')),\n        \'image/key/sha256\': dataset_util.bytes_feature(key.encode(\'utf8\')),\n        \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n        \'image/format\': dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n        \'image/object/bbox/xmin\': dataset_util.float_list_feature(x1s),\n        \'image/object/bbox/xmax\': dataset_util.float_list_feature(x2s),\n        \'image/object/bbox/ymin\': dataset_util.float_list_feature(y1s),\n        \'image/object/bbox/ymax\': dataset_util.float_list_feature(y2s),\n        \'image/object/class/text\': dataset_util.bytes_list_feature(category_names),\n        \'image/object/is_crowd\': dataset_util.int64_list_feature(is_crowd),\n        \'image/object/area\': dataset_util.float_list_feature(area),\n    }\n    # Masks are ignored here.\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n\n    return key, example\n\n\ndef generate_annotation_index(annotations):\n    """"""Turn annotations into annotation index dictionary with\n    image id as key and list of coco-like annotation objects as value.\n\n    :param annotations: coco-like annotations list;\n    :return: annotation index dictionary with image id as key and\n             list of coco-like annotation objects as value;\n    """"""\n    annotation_index = {}\n    for annotation in annotations:\n        image_id = annotation[\'image_id\']\n        if image_id not in annotation_index:\n            annotation_index[image_id] = []\n        annotation_index[image_id].append(annotation)\n    return annotation_index\n\n\ndef load_annotation(annotation_file):\n    """"""Load a coco annotation file.\n\n    :param annotation_file: a coco annotation file;\n    :return: images object in coco annotation file;\n             annotations object in coco annotation file;\n             categories object in coco annotation file;\n    """"""\n    assert os.path.exists(annotation_file), \\\n        \'annotation file {} does not exist!\'.format(annotation_file)\n    with tf.gfile.GFile(annotation_file, \'r\') as f:\n        info = json.load(f)\n    assert info, \\\n        \'annotation is None, please check the annotation file\'\n\n    images = info[\'images\']\n    annotations = info[\'annotations\']\n    categories = info[\'categories\']\n\n    return images, annotations, categories\n\n\ndef merge_annotations(train_annotation_file, val_annotation_file):\n    """"""Load organic annotation files and merge training and validation annotations.\n\n    :param train_annotation_file: organic coco training annotation file;\n    :param val_annotation_file: organic coco validation annotation file;\n    :return: merged images object with both training and validation images;\n             merged annotations with both training and validation annotations;\n             categories object in coco annotation file;\n    """"""\n    train_images, train_annotations, categories = \\\n        load_annotation(train_annotation_file)\n    val_images, val_annotations, _ = \\\n        load_annotation(val_annotation_file)\n\n    tf.logging.info(\'number of organic train2017 images is {}\'\n                    .format(len(train_images)))\n    tf.logging.info(\'number of organic val2017 images is {}\'\n                    .format(len(val_images)))\n\n    train_images.extend(val_images)\n    train_annotations.extend(val_annotations)\n\n    return train_images, train_annotations, categories\n\n\ndef parse_minival_ids(coco_minival_ids_file):\n    """"""Load minival ids file to a list.\n\n    :param coco_minival_ids_file: path of coco minival ids file;\n    :return: list of selected minival id for validation;\n    """"""\n    assert os.path.exists(coco_minival_ids_file), \\\n        \'minival_ids_file {} does not exist!\'.format(coco_minival_ids_file)\n    with open(coco_minival_ids_file, \'r\') as f:\n        minival_list = [int(line) for line in f.readlines()]\n    assert minival_list,\\\n        \'minival ids is None, please check the coco_minival_ids_file\'\n\n    return minival_list\n\n\ndef split_minival(images, minival_list):\n    """"""Split total images to training and validation splits according\n    to selected minival id.\n\n    :param images: list of total coco-like images(including organic\n                   training and validation splits);\n    :param minival_list: list of selected minival id for validation;\n    :return: brand new train_plus split and minival split according;\n    """"""\n    train_plus_images = []\n    minival_images = []\n    for image in images:\n        image_id = image[\'id\']\n        if image_id not in minival_list:\n            train_plus_images.append(image)\n        else:\n            minival_images.append(image)\n    return train_plus_images, minival_images\n\n\ndef write_tfrecord(image_list,\n                   anno_index,\n                   category_index,\n                   image_folders,\n                   output_tfrecord,\n                   tfrecord_shards=100,\n                   logging_interval=1000):\n    """"""Translate coco-like annotation format into tfrecord and write down.\n\n    :param image_list: a list of coco-like image objects;\n    :param anno_index: annotation index dictionary with image id as key and list of\n                       coco-like annotation objects as value;\n    :param category_index: category index dictionary generated by `label_map_util`;\n    :param image_folders: a list of folders which may contain required image;\n    :param output_tfrecord: path and prefix of output tfrecord;\n    :param tfrecord_shards: number of generated tfrecord shards;\n    :param logging_interval: log will be printed out per logging interval;\n    """"""\n    length = len(image_list)\n\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        tfrecord_writer = tf_record_creation_util.open_sharded_output_tfrecords(\n            tf_record_close_stack, output_tfrecord, tfrecord_shards)\n        skipped = 0\n        for i, image in enumerate(image_list):\n            try:\n                if i % logging_interval == 0:\n                    tf.logging.info(\'Processing {}/{} image\'.format(i, length))\n                image_id = image[\'id\']\n\n                if image_id not in anno_index:\n                    skipped += 1\n                    continue\n                anno_list = anno_index[image_id]\n                _, tf_example = create_tf_example(\n                    image, anno_list, image_folders, category_index)\n                shard_index = i % tfrecord_shards\n                tfrecord_writer[shard_index].write(tf_example.SerializeToString())\n            except (ValueError, OSError):\n                skipped += 1\n\n    tf.logging.info(\'Finished writing, total {} annotations, \'\n                    \'skipped {} images.\'.format(length, skipped))\n\n\ndef main(_):\n    image_folder = FLAGS.image_folder\n    annotation_folder = FLAGS.annotation_folder\n    coco_minival_ids_file = FLAGS.coco_minival_ids_file\n    output_folder = FLAGS.output_folder\n\n    assert image_folder,\\\n        \'required argument `image_folder` is missing\'\n    assert annotation_folder,\\\n        \'required argument `annotation_folder` is missing\'\n    assert coco_minival_ids_file,\\\n        \'required argument `coco_minival_ids_file` is missing\'\n    assert output_folder,\\\n        \'required argument `output_folder` is missing\'\n\n    if not tf.gfile.IsDirectory(output_folder):\n        tf.gfile.MakeDirs(output_folder)\n\n    train_image = os.path.join(image_folder, TRAIN_FOLDER_NAME)\n    train_annotation = os.path.join(annotation_folder, TRAIN_ANNOTATION_NAME)\n    train_tfrecord = os.path.join(output_folder, TRAIN_TFRECORD_NAME)\n    val_image = os.path.join(image_folder, VAL_FOLDER_NAME)\n    val_annotation = os.path.join(annotation_folder, VAL_ANNOTATION_NAME)\n    val_tfrecord = os.path.join(output_folder, VAL_TFRECORD_NAME)\n\n    create_tfrecord_from_coco(train_image, val_image,\n                              train_annotation, val_annotation,\n                              train_tfrecord, val_tfrecord,\n                              coco_minival_ids_file)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
tensorflow_toolkit/text_detection/tests/unittests.py,2,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains unit tests. """"""\n\nimport os\nimport unittest\nimport tempfile\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom text_detection.annotation import TextDetectionDataset, write_to_tfrecords\nfrom text_detection.dataset import  TFRecordDataset\nfrom text_detection.loss import ClassificationLoss, LinkageLoss\nfrom text_detection.model import pixel_link_model\nfrom text_detection.metrics import test\n\n\nclass TestCreateAnnotaion(unittest.TestCase):\n    """""" Tests set for annotation. """"""\n\n    def setUp(self):\n        """""" setUp method for tests. """"""\n\n        self.folder = \'./data\'\n        self.dataset = TextDetectionDataset.read_from_toy_dataset(self.folder)\n\n    def test_num_frames(self):\n        """""" Test for checking number of frames in annotation. """"""\n\n        self.assertEqual(len(self.dataset.annotation), 20)\n\n    def test_image_paths(self):\n        """""" Test for checking correctness of images paths in annotation. """"""\n\n        for index, frame in enumerate(self.dataset.annotation):\n            self.assertEqual(frame[\'image_path\'],\n                             os.path.join(self.folder, \'img_{}.jpg\'.format(index // 4 + 1)))\n\n    def test_bboxes_nums(self):\n        """""" Test for checking number of bounding boxes in annotation. """"""\n\n        bboxes_num = [8, 8, 8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4]\n        for index, frame in enumerate(self.dataset.annotation):\n            self.assertEqual(bboxes_num[index], len(frame[\'bboxes\']))\n\n\nclass TestWriteAndReadAnnotaion(unittest.TestCase):\n    """""" Tests set for annotation io. """"""\n\n    def setUp(self):\n        """""" setUp method for tests. """"""\n\n        self.folder = \'./data\'\n        dataset = TextDetectionDataset.read_from_toy_dataset(self.folder)\n        _, path = tempfile.mkstemp()\n        path += \'.json\'\n        dataset.write(path)\n        self.dataset = TextDetectionDataset(path)\n\n    def test_num_frames(self):\n        """""" Test for checking number of frames in annotation. """"""\n\n        self.assertEqual(len(self.dataset.annotation), 20)\n\n    def test_image_paths(self):\n        """""" Test for checking correctness of images paths in annotation. """"""\n\n        for index, frame in enumerate(self.dataset.annotation):\n            self.assertEqual(frame[\'image_path\'],\n                             os.path.join(self.folder, \'img_{}.jpg\'.format(index // 4 + 1)))\n\n    def test_bboxes_nums(self):\n        """""" Test for checking number of bounding boxes in annotation. """"""\n\n        bboxes_num = [8, 8, 8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4]\n        for index, frame in enumerate(self.dataset.annotation):\n            self.assertEqual(bboxes_num[index], len(frame[\'bboxes\']))\n\n\nclass TestCreateTFRecordDataset(unittest.TestCase):\n    """""" Tests set for TFRecordDataset. """"""\n\n    def setUp(self):\n        """""" setUp method for tests. """"""\n\n        self.folder = \'./data\'\n        dataset = TextDetectionDataset.read_from_toy_dataset(self.folder)\n        _, path = tempfile.mkstemp()\n        path += \'.json\'\n        dataset.write(path)\n\n        _, self.output_path = tempfile.mkstemp()\n        self.output_path += \'.tfrecord\'\n        write_to_tfrecords(output_path=self.output_path, datasets=[path])\n\n    def test_dataset_size(self):\n        """""" Test for checking dataset size. """"""\n\n        _, size = TFRecordDataset(self.output_path, {\'model_type\': \'mobilenet_v2_ext\'}, test=True)()\n        self.assertEqual(size, 20)\n\n    def test_validation_dataset_image_shape(self):\n        """""" Test for checking validation dataset images shapes. """"""\n\n        dataset, _ = TFRecordDataset(self.output_path, {\'model_type\': \'mobilenet_v2_ext\'}, test=True)()\n        for image, _, _ in dataset:\n            self.assertEqual(image.numpy().shape, (768, 1280, 3))\n\n    def test_training_dataset_image_shape(self):\n        """""" Test for checking training dataset images shapes. """"""\n\n        config = {\n            \'model_type\': \'mobilenet_v2_ext\',\n            \'imagenet_preprocessing\': False,\n            \'batch_size\': 2,\n            \'weights_decay\': 0.00001,\n            \'train_image_shape\': [512, 512],\n            \'score_map_shape\': [128, 128],\n            \'rotate\': True,\n            \'rotation_prob\': 0.5,\n            \'distort_color\': True,\n            \'random_crop\': True,\n            \'min_object_covered\': 0.1,\n            \'bbox_crop_overlap\': 0.2,\n            \'crop_aspect_ratio_range\': (0.5, 2.),\n            \'area_range\': [0.1, 1],\n            \'using_shorter_side_filtering\': True,\n            \'min_shorter_side\': 10,\n            \'max_shorter_side\': np.infty,\n            \'min_area\': 300,\n            \'min_height\': 10,\n            \'max_neg_pos_ratio\': 3,\n            \'num_neighbours\': 8,\n            \'num_classes\': 2,\n            \'ignore_label\': -1,\n            \'background_label\': 0,\n            \'text_label\': 1,\n            \'num_replicas\': 1\n        }\n\n        dataset, _ = TFRecordDataset(self.output_path, config, test=False)()\n        counter = 0\n        for image, _ in dataset:\n            self.assertEqual(image.numpy().shape, (2, 512, 512, 3))\n            counter += 1\n            if counter > 20:\n                break\n\n        self.assertEqual(counter, 21)\n\n\nclass TestTraining(unittest.TestCase):\n    """""" Tests set for training. """"""\n\n    def setUp(self):\n        """""" setUp method for tests. """"""\n\n        self.folder = \'./data\'\n        dataset = TextDetectionDataset.read_from_toy_dataset(self.folder)\n        _, path = tempfile.mkstemp()\n        path += \'.json\'\n        dataset.write(path)\n\n        _, self.output_path = tempfile.mkstemp()\n        self.output_path += \'.tfrecord\'\n        write_to_tfrecords(output_path=self.output_path, datasets=[path])\n\n        self.config = {\n            \'model_type\': \'mobilenet_v2_ext\',\n            \'imagenet_preprocessing\': False,\n            \'batch_size\': 5,\n            \'weights_decay\': 0.00001,\n            \'train_image_shape\': [256, 256],\n            \'score_map_shape\': [64, 64],\n            \'rotate\': True,\n            \'rotation_prob\': 0.5,\n            \'distort_color\': True,\n            \'random_crop\': True,\n            \'min_object_covered\': 0.1,\n            \'bbox_crop_overlap\': 0.2,\n            \'crop_aspect_ratio_range\': (0.5, 2.),\n            \'area_range\': [0.1, 1],\n            \'using_shorter_side_filtering\': True,\n            \'min_shorter_side\': 10,\n            \'max_shorter_side\': np.infty,\n            \'min_area\': 300,\n            \'min_height\': 10,\n            \'max_neg_pos_ratio\': 3,\n            \'num_neighbours\': 8,\n            \'num_classes\': 2,\n            \'ignore_label\': -1,\n            \'background_label\': 0,\n            \'text_label\': 1,\n            \'num_replicas\': 1\n        }\n\n        self.model = pixel_link_model(tf.keras.Input(shape=(256, 256, 3)), self.config)\n        optimizer = tf.optimizers.SGD(learning_rate=0.0001, momentum=0.9)\n        self.model.compile(loss=[ClassificationLoss(self.config),\n                                 LinkageLoss(self.config)], optimizer=optimizer)\n\n        _, self.saved_weights = tempfile.mkstemp()\n        self.saved_weights = self.saved_weights + \'-1\'\n        self.model.save_weights(self.saved_weights)\n        self.history = None\n\n    def test_loss_history(self):\n        """""" Test for checking loss history. """"""\n\n        dataset, _ = TFRecordDataset(self.output_path, self.config, test=False)()\n        self.history = self.model.fit(dataset, epochs=10, steps_per_epoch=4)\n        self.assertGreater(self.history.history[\'loss\'][0], self.history.history[\'loss\'][-1])\n\n    def test_eval_can_be_run(self):\n        """""" Test for checking an ability to run evaluation. """"""\n\n        class Args:\n            """""" Arguments for evaluation. """"""\n\n            def __init__(self, weights):\n                self.imshow_delay = -1\n                self.weights = weights\n                self.resolution = (256, 256)\n\n        dataset, _ = TFRecordDataset(self.output_path, self.config, test=True)()\n        test(Args(self.saved_weights), self.config, model=self.model, dataset=dataset)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tensorflow_toolkit/text_detection/text_detection/__init__.py,0,b''
tensorflow_toolkit/text_detection/text_detection/annotation.py,8,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport json\nimport tempfile\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\n\nclass TextDetectionDataset:\n    """""" TextDetectionDataset list of following instances\n        {\n        \'image_path\',\n        \'bboxes\':\n            [\n                {\n                    \'quadrilateral\': [int, int, int, int, int, int, int, int],\n                    \'transcription\': str\n                    \'language\': str\n                    \'readable\': bool\n                },\n                ...\n            ]\n        }\n    """"""\n\n    def __init__(self, path=None):\n        if path is None:\n            self.annotation = []\n        else:\n            if os.path.exists(path):\n                with open(path) as read_file:\n                    self.annotation = json.load(read_file)\n\n    def __add__(self, dataset):\n        text_detection_dataset = TextDetectionDataset()\n        text_detection_dataset.annotation = self.annotation + dataset.annotation\n        return text_detection_dataset\n\n    def __len__(self):\n        return len(self.annotation)\n\n    def write(self, path):\n        """""" Writes dataset annotation as json file. """"""\n\n        with open(path, \'w\') as read_file:\n            json.dump(self.annotation, read_file)\n\n    def visualize(self, put_text, imshow_delay=1):\n        """""" Visualizes annotation using cv2.imshow from OpenCV. Press `Esc` to exit. """"""\n\n        for frame in tqdm(self.annotation):\n            image = cv2.imread(frame[\'image_path\'], cv2.IMREAD_COLOR)\n            for bbox in frame[\'bboxes\']:\n                lwd = 2\n                color = (0, 255, 0)\n                if not bbox[\'readable\']:\n                    color = (128, 128, 128)\n                points = bbox[\'quadrilateral\']\n                if put_text:\n                    cv2.putText(image, bbox[\'transcription\'], tuple(points[0:2]), 1, 1.0, color)\n                cv2.line(image, tuple(points[0:2]), tuple(points[2:4]), color, lwd)\n                cv2.line(image, tuple(points[2:4]), tuple(points[4:6]), color, lwd)\n                cv2.line(image, tuple(points[4:6]), tuple(points[6:8]), color, lwd)\n                cv2.line(image, tuple(points[6:8]), tuple(points[0:2]), color, lwd)\n            try:\n                image = cv2.resize(image, (1920, 1080))\n                cv2.imshow(\'image\', image)\n                k = cv2.waitKey(imshow_delay)\n                if k == 27:\n                    break\n            except:\n                print(\'Error: image is empty or corrupted: \', frame[\'image_path\'])\n\n    @staticmethod\n    def read_from_icdar2015(images_folder, annotations_folder, is_training):\n        """""" Converts annotation from ICDAR 2015 format to internal format. """"""\n\n        def parse_line(line):\n            line = line.split(\',\')\n            quadrilateral = [int(x) for x in line[:8]]\n            transcription = \',\'.join(line[8:])\n            readable = True\n            language = \'english\'\n            if transcription == \'###\':\n                transcription = \'\'\n                readable = False\n                language = \'\'\n            return {\'quadrilateral\': quadrilateral, \'transcription\': transcription,\n                    \'readable\': readable, \'language\': language}\n\n        dataset = TextDetectionDataset()\n\n        n_images = 1000 if is_training else 500\n        for i in range(1, n_images + 1):\n            image_path = os.path.join(images_folder, \'img_{}.jpg\'.format(i))\n            annotation_path = os.path.join(annotations_folder, \'gt_img_{}.txt\'.format(i))\n\n            frame = {\'image_path\': image_path,\n                     \'bboxes\': []}\n\n            with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                content = [line.strip() for line in read_file.readlines()]\n                for line in content:\n                    frame[\'bboxes\'].append(parse_line(line))\n\n            dataset.annotation.append(frame)\n\n        return dataset\n\n    @staticmethod\n    def read_from_icdar2013(images_folder, annotations_folder, is_training):\n        """""" Converts annotation from ICDAR 2013 format to internal format. """"""\n\n        def parse_line(line, sep):\n            line = line.split(sep)\n            xmin, ymin, xmax, ymax = [int(x) for x in line[:4]]\n            assert xmin < xmax\n            assert ymin < ymax\n            quadrilateral = [xmin, ymin, xmax, ymin, xmax, ymax, xmin, ymax]\n            transcription = (sep.join(line[4:]))[1:-1]\n            return {\'quadrilateral\': quadrilateral, \'transcription\': transcription,\n                    \'readable\': True, \'language\': \'english\'}\n\n        dataset = TextDetectionDataset()\n\n        begin, end = (100, 328 + 1) if is_training else (1, 233 + 1)\n        gt_format = \'gt_{}.txt\' if is_training else \'gt_img_{}.txt\'\n        img_format = \'{}.jpg\' if is_training else \'img_{}.jpg\'\n\n        for i in range(begin, end):\n            frame = {\'image_path\': os.path.join(images_folder, img_format.format(i)), \'bboxes\': []}\n            annotation_path = os.path.join(annotations_folder, gt_format.format(i))\n\n            with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                for line in [line.strip() for line in read_file.readlines()]:\n                    frame[\'bboxes\'].append(parse_line(line, sep=\' \' if is_training else \', \'))\n\n            dataset.annotation.append(frame)\n\n        return dataset\n\n    @staticmethod\n    def read_from_msra_td500(folder):\n        """""" Converts annotation from MSRA-TD500 format to internal format. """"""\n\n        def parse_line(line):\n            line = line.split(\' \')\n            _, difficult, top_left_x, top_left_y, width, height, rotation = [float(x) for x in line]\n            box = cv2.boxPoints(((top_left_x + width / 2, top_left_y + height / 2),\n                                 (width, height), rotation * 57.2958))\n            quadrilateral = [int(x) for x in box.reshape([-1])]\n            readable = difficult == 0\n            return {\'quadrilateral\': quadrilateral, \'transcription\': \'\',\n                    \'readable\': readable, \'language\': \'\'}\n\n        dataset = TextDetectionDataset()\n\n        for image_name in sorted(os.listdir(folder)):\n            if image_name.endswith(\'JPG\'):\n                image_path = os.path.join(folder, image_name)\n                annotation_path = os.path.join(folder, image_name.replace(\'.JPG\', \'.gt\'))\n\n                frame = {\'image_path\': image_path,\n                         \'bboxes\': []}\n\n                with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                    content = [line.strip() for line in read_file.readlines()]\n                    for line in content:\n                        frame[\'bboxes\'].append(parse_line(line))\n\n                dataset.annotation.append(frame)\n\n        return dataset\n\n    @staticmethod\n    def read_from_coco_text(path, no_boxes_is_ok=False, sets=None):\n        """""" Converts annotation from COCO-TEXT format to internal format. """"""\n\n        if sets is None:\n            sets = [\'train\']\n\n        dataset = TextDetectionDataset()\n\n        with open(path) as read_file:\n\n            json_loaded = json.load(read_file)\n\n            for id, value in json_loaded[\'imgs\'].items():\n                image_path = os.path.join(os.path.dirname(path), \'train2014\', value[\'file_name\'])\n                dataset_type = value[\'set\']\n\n                if dataset_type not in sets:\n                    continue\n\n                bboxes = []\n                for annotation_id  in json_loaded[\'imgToAnns\'][id]:\n                    annotation_value = json_loaded[\'anns\'][str(annotation_id)]\n\n                    text = annotation_value[\'utf8_string\']\n                    language = annotation_value[\'language\']\n                    readable = annotation_value[\'legibility\'] == \'legible\'\n\n                    mask = np.reshape(np.array(annotation_value[\'mask\'], np.int32), (-1, 2))\n                    box = cv2.boxPoints(cv2.minAreaRect(mask))\n                    quadrilateral = [int(x) for x in box.reshape([-1])]\n\n                    bboxes.append({\n                        \'quadrilateral\': quadrilateral,\n                        \'transcription\': text,\n                        \'readable\': readable,\n                        \'language\': language})\n\n\n                if no_boxes_is_ok or bboxes:\n                    dataset.annotation.append({\n                        \'image_path\': image_path,\n                        \'bboxes\': bboxes})\n\n        return dataset\n\n    @staticmethod\n    def read_from_toy_dataset(folder):\n        """""" Converts annotation from toy dataset (available) to internal format. """"""\n\n        def parse_line(line):\n            line = line.split(\',\')\n            quadrilateral = [int(x) for x in line[:8]]\n            transcription = \',\'.join(line[8:])\n            readable = True\n            language = \'\'\n            if transcription == \'###\':\n                transcription = \'\'\n                readable = False\n                language = \'\'\n            return {\'quadrilateral\': quadrilateral, \'transcription\': transcription,\n                    \'readable\': readable, \'language\': language}\n\n        dataset = TextDetectionDataset()\n\n        n_images = 5\n        for i in range(1, n_images + 1):\n            image_path = os.path.join(folder, \'img_{}.jpg\'.format(i))\n            annotation_path = os.path.join(folder, \'gt_img_{}.txt\'.format(i))\n\n            frame = {\'image_path\': image_path,\n                     \'bboxes\': []}\n\n            with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                content = [line.strip() for line in read_file.readlines()]\n                for line in content:\n                    frame[\'bboxes\'].append(parse_line(line))\n\n            # for batch 20\n            for _ in range(4):\n                dataset.annotation.append(frame)\n\n        return dataset\n\n    @staticmethod\n    def read_from_icdar2019_mlt(folder):\n        """""" Converts annotation from toy dataset (available) to internal format. """"""\n\n        def parse_line(line):\n            line = line.split(\',\')\n            quadrilateral = [int(x) for x in line[:8]]\n            language = line[8]\n            transcription = \',\'.join(line[9:])\n            readable = True\n            if transcription == \'###\':\n                transcription = \'\'\n                readable = False\n            return {\'quadrilateral\': quadrilateral, \'transcription\': transcription,\n                    \'readable\': readable, \'language\': language}\n\n        dataset = TextDetectionDataset()\n\n\n        for image_part in [1, 2]:\n            for image_path in os.listdir(os.path.join(folder, \'ImagesPart{}\'.format(image_part))):\n                annotation_path = os.path.join(folder, \'train_gt_t13\', image_path)[:-3] + \'txt\'\n                image_path = os.path.join(folder, \'ImagesPart{}\'.format(image_part), image_path)\n\n                frame = {\'image_path\': image_path, \'bboxes\': []}\n\n                with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                    content = [line.strip() for line in read_file.readlines()]\n                    for line in content:\n                        frame[\'bboxes\'].append(parse_line(line))\n\n                dataset.annotation.append(frame)\n\n        return dataset\n\n    @staticmethod\n    def read_from_icdar2017_mlt(folder, _):\n        """""" Converts annotation from toy dataset (available) to internal format. """"""\n\n        def parse_line(line):\n            line = line.split(\',\')\n            quadrilateral = [int(x) for x in line[:8]]\n            language = line[8]\n            transcription = \',\'.join(line[9:])\n            readable = True\n            if transcription == \'###\':\n                transcription = \'\'\n                readable = False\n            return {\'quadrilateral\': quadrilateral, \'transcription\': transcription,\n                    \'readable\': readable, \'language\': language}\n\n        dataset = TextDetectionDataset()\n\n        for image_path in os.listdir(os.path.join(folder, \'ch8_validation_images\')):\n            annotation_path = os.path.join(folder, \'ch8_validation_localization_transcription_gt_v2\',\n                                           \'gt_\' + image_path)[:-3] + \'txt\'\n            image_path = os.path.join(folder, \'ch8_validation_images\', image_path)\n\n            frame = {\'image_path\': image_path, \'bboxes\': []}\n\n            with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                content = [line.strip() for line in read_file.readlines()]\n                for line in content:\n                    frame[\'bboxes\'].append(parse_line(line))\n\n            dataset.annotation.append(frame)\n\n        return dataset\n\n\n\ndef write_to_tfrecords(output_path, datasets, resize_to=None, imshow_delay=-1):\n    """""" Write datasets to tf record file. """"""\n\n    assert isinstance(datasets, list)\n\n    def visualize(image, bboxes, imshow_delay):\n        for xmin, ymin, xmax, ymax in bboxes:\n            xmin, ymin, xmax, ymax = xmin * weight, ymin * height, xmax * weight, ymax * height\n            xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n            image = cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 255, 255), 2)\n\n        image = cv2.resize(image, image_resized.shape[0:2][::-1])\n        cv2.imshow(\'image\', image)\n        cv2.waitKey(imshow_delay)\n\n    ignore_label = -1\n    text_label = 1\n\n    tmpfile = os.path.join(tempfile.mkdtemp(), \'image.png\')\n\n    with tf.io.TFRecordWriter(output_path) as tfrecord_writer:\n        for dataset_path in datasets:\n            for frame in tqdm(TextDetectionDataset(dataset_path).annotation):\n                image = cv2.imread(frame[\'image_path\'], cv2.IMREAD_COLOR)\n                image_resized = cv2.resize(image, resize_to) if resize_to is not None else image\n                cv2.imwrite(tmpfile, image_resized)\n                image_data = tf.io.gfile.GFile(tmpfile, \'rb\').read()\n\n                shape = image.shape\n                height, weight = shape[0:2]\n\n                bboxes = []\n                labels = []\n                labels_text = []\n                oriented_bboxes = []\n\n                for bbox in frame[\'bboxes\']:\n                    oriented_box = np.asarray(bbox[\'quadrilateral\'], dtype=np.float32)\n                    oriented_box = oriented_box / ([weight, height] * 4)\n                    np.clip(oriented_box, 0.0, 1.0, out=oriented_box)\n\n                    x_coordinates = oriented_box.reshape(4, 2)[:, 0]\n                    y_coordinates = oriented_box.reshape(4, 2)[:, 1]\n                    bboxes.append([x_coordinates.min(), y_coordinates.min(),\n                                   x_coordinates.max(), y_coordinates.max()])\n                    labels_text.append(bbox[\'transcription\'].encode(""utf8""))\n                    oriented_bboxes.append(oriented_box)\n\n                    if not bbox[\'readable\']:\n                        labels.append(ignore_label)\n                    else:\n                        labels.append(text_label)\n\n                example = convert_to_example(image_data, labels, labels_text,\n                                             bboxes, oriented_bboxes, shape)\n                tfrecord_writer.write(example.SerializeToString())\n\n                if imshow_delay >= 0:\n                    visualize(image, bboxes, imshow_delay)\n\n\ndef convert_to_example(image_data, labels, labels_text, bboxes, oriented_bboxes, shape):\n    """""" Convert dataset element to tf.train.Example. """"""\n\n    oriented_bboxes = np.asarray(oriented_bboxes)\n    bboxes = np.asarray(bboxes)\n\n    def get_list(obj, idx):\n        if len(obj) > 0:\n            return list(obj[:, idx])\n        return []\n\n    def float_feature(feature):\n        return tf.train.Feature(float_list=tf.train.FloatList(value=feature))\n\n    def byte_feature(feature):\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=feature))\n\n    def int64_feature(feature):\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=feature))\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=list(shape))),\n        \'image/object/bbox/xmin\': float_feature(get_list(bboxes, 0)),\n        \'image/object/bbox/ymin\': float_feature(get_list(bboxes, 1)),\n        \'image/object/bbox/xmax\': float_feature(get_list(bboxes, 2)),\n        \'image/object/bbox/ymax\': float_feature(get_list(bboxes, 3)),\n        \'image/object/bbox/x1\': float_feature(get_list(oriented_bboxes, 0)),\n        \'image/object/bbox/y1\': float_feature(get_list(oriented_bboxes, 1)),\n        \'image/object/bbox/x2\': float_feature(get_list(oriented_bboxes, 2)),\n        \'image/object/bbox/y2\': float_feature(get_list(oriented_bboxes, 3)),\n        \'image/object/bbox/x3\': float_feature(get_list(oriented_bboxes, 4)),\n        \'image/object/bbox/y3\': float_feature(get_list(oriented_bboxes, 5)),\n        \'image/object/bbox/x4\': float_feature(get_list(oriented_bboxes, 6)),\n        \'image/object/bbox/y4\': float_feature(get_list(oriented_bboxes, 7)),\n        \'image/object/bbox/label\': int64_feature(labels),\n        \'image/object/bbox/label_text\': byte_feature(labels_text),\n        \'image/format\': byte_feature([b\'JPEG\']),\n        \'image/encoded\': byte_feature([image_data])}))\n    return example\n'"
tensorflow_toolkit/text_detection/text_detection/common.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport yaml\n\ndef load_config(path):\n    """""" Load saved configuration from yaml file. """"""\n\n    with open(path, ""r"") as read_file:\n        config = yaml.load(read_file)\n    return config\n\n\ndef parse_epoch(path):\n    return int(os.path.basename(path).split(\'.\')[0].split(\'-\')[-1])\n'"
tensorflow_toolkit/text_detection/text_detection/dataset.py,138,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains TFRecordDataset class. """"""\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import math_ops\n\nimport cv2\n\n\ndef points_to_contour(points):\n    """""" Converts points to contour. """"""\n\n    return np.asarray([[list(point)] for point in points], dtype=np.int32)\n\n\ndef points_to_contours(points):\n    """""" Converts points to contours. """"""\n\n    return np.asarray([points_to_contour(points)])\n\n\ndef get_neighbours(x_coord, y_coord):\n    """""" Returns 8-point neighbourhood of given point. """"""\n\n    return [(x_coord - 1, y_coord - 1), (x_coord, y_coord - 1), (x_coord + 1, y_coord - 1), \\\n            (x_coord - 1, y_coord), (x_coord + 1, y_coord), \\\n            (x_coord - 1, y_coord + 1), (x_coord, y_coord + 1), (x_coord + 1, y_coord + 1)]\n\n\ndef is_valid_coord(x_coord, y_coord, width, height):\n    """""" Returns true if given point inside image frame. """"""\n\n    return 0 <= x_coord < width and 0 <= y_coord < height\n\n\ndef tf_min_area_rect(x_coords, y_coords):\n    """""" Returns rotated rectangles for given set of points. """"""\n\n    def min_area_rect(x_coords, y_coords):\n        num_rects = x_coords.shape[0]\n        box = np.empty((num_rects, 5), dtype=np.float32)\n        for idx in range(num_rects):\n            points = zip(x_coords[idx, :], y_coords[idx, :])\n            (center_x, center_y), (width, height), theta = \\\n                cv2.minAreaRect(points_to_contour(points))\n            box[idx, :] = [center_x, center_y, width, height, theta]\n        return box\n\n    rects = tf.numpy_function(min_area_rect, [x_coords, y_coords], x_coords.dtype)\n    rects.set_shape([None, 5])\n    return rects\n\n\ndef safe_divide(numerator, denominator, name):\n    """""" Performs safe division. """"""\n\n    return tf.where(\n        math_ops.greater(denominator, 0),\n        math_ops.divide(numerator, denominator),\n        tf.zeros_like(numerator),\n        name=name)\n\n\ndef bboxes_intersection(bbox_ref, bboxes):\n    """""" Computes bounding boxes intersection. """"""\n\n    bboxes = tf.transpose(a=bboxes)\n    bbox_ref = tf.transpose(a=bbox_ref)\n    int_ymin = tf.maximum(bboxes[0], bbox_ref[0])\n    int_xmin = tf.maximum(bboxes[1], bbox_ref[1])\n    int_ymax = tf.minimum(bboxes[2], bbox_ref[2])\n    int_xmax = tf.minimum(bboxes[3], bbox_ref[3])\n    height = tf.maximum(int_ymax - int_ymin, 0.)\n    widths = tf.maximum(int_xmax - int_xmin, 0.)\n    inter_vol = height * widths\n    bboxes_vol = (bboxes[2] - bboxes[0]) * (bboxes[3] - bboxes[1])\n    scores = safe_divide(inter_vol, bboxes_vol, \'intersection\')\n    return scores\n\n\ndef random_rotate90(image, bboxes, x_coords, y_coords):\n    """""" Randomly rotate image and bounding boxes by 0, 90, 180, 270 degrees. """"""\n    rotate_by_90_k_times = tf.random.uniform([], maxval=4, dtype=tf.int32)\n    image = tf.image.rot90(image, k=rotate_by_90_k_times)\n    bboxes, x_coords, y_coords = rotate90(bboxes, x_coords, y_coords, rotate_by_90_k_times)\n    return image, bboxes, x_coords, y_coords\n\n\ndef rotate_point_by_90(x_coord, y_coord, rotate_by_90_k_times):\n    """""" Rotate point by 90 degrees (clockwise). """"""\n\n    cos = tf.constant([1.0, 0.0, -1.0, 0.0])\n    sin = tf.constant([0.0, -1.0, 0.0, 1.0])\n\n    x1_coord = x_coord - 0.5\n    y1_coord = y_coord - 0.5\n\n    x_coord = x1_coord * cos[rotate_by_90_k_times] - y1_coord * sin[rotate_by_90_k_times] + 0.5\n    y_coord = x1_coord * sin[rotate_by_90_k_times] + y1_coord * cos[rotate_by_90_k_times] + 0.5\n\n    return x_coord, y_coord\n\n\ndef rotate90(bboxes, x_coords, y_coords, k):\n    """""" Rotate bounding boxes by 90 degrees.""""""\n\n    ymin, xmin, ymax, xmax = [bboxes[:, i] for i in range(4)]\n    xmin, ymin = rotate_point_by_90(xmin, ymin, k)\n    xmax, ymax = rotate_point_by_90(xmax, ymax, k)\n\n    new_xmin = tf.minimum(xmin, xmax)\n    new_xmax = tf.maximum(xmin, xmax)\n\n    new_ymin = tf.minimum(ymin, ymax)\n    new_ymax = tf.maximum(ymin, ymax)\n\n    bboxes = tf.stack([new_ymin, new_xmin, new_ymax, new_xmax])\n    bboxes = tf.transpose(a=bboxes)\n\n    x_coords, y_coords = rotate_point_by_90(x_coords, y_coords, k)\n    return bboxes, x_coords, y_coords\n\n\ndef bboxes_resize(bbox_ref, bboxes, x_coords, y_coords):\n    """""" Resize bounding box relatively to reference bounding box. """"""\n\n    h_ref = bbox_ref[2] - bbox_ref[0]\n    w_ref = bbox_ref[3] - bbox_ref[1]\n\n    bboxes = bboxes - tf.stack([bbox_ref[0], bbox_ref[1], bbox_ref[0], bbox_ref[1]])\n    x_coords = x_coords - bbox_ref[1]\n    y_coords = y_coords - bbox_ref[0]\n\n    bboxes = bboxes / tf.stack([h_ref, w_ref, h_ref, w_ref])\n    x_coords = x_coords / w_ref\n    y_coords = y_coords / h_ref\n\n    return bboxes, x_coords, y_coords\n\n\ndef tf_prepare_groundtruth_for_image(x_coords, y_coords, labels, config):\n    """""" Generate groundtruth data for given image. """"""\n\n    height, width = config[\'score_map_shape\']\n    num_neighbours = config[\'num_neighbours\']\n\n    def prepare_groundtruth_for_image(normed_xs, normed_ys, labels):\n\n\n        num_positive_bboxes = np.sum(np.asarray(labels) == config[\'text_label\'])\n        x_coords = normed_xs * width\n        y_coords = normed_ys * height\n\n        mask = np.zeros([height, width], np.int32)\n        segm_labels = np.ones([height, width], np.int32) * config[\'background_label\']\n        segm_weights = np.zeros([height, width], np.float32)\n\n        link_labels = np.zeros([height, width, num_neighbours], np.int32)\n        link_weights = np.ones([height, width, num_neighbours], np.float32)\n\n        bbox_masks = []\n        pos_mask = mask.copy()\n        for bbox_idx, (bbox_xs, bbox_ys) in enumerate(zip(x_coords, y_coords)):\n            if labels[bbox_idx] == config[\'background_label\']:\n                continue\n\n            bbox_mask = mask.copy()\n            bbox_contours = points_to_contours(zip(bbox_xs, bbox_ys))\n            cv2.drawContours(bbox_mask, bbox_contours, -1, 1, -1)\n\n            bbox_masks.append(bbox_mask)\n\n            if labels[bbox_idx] == config[\'text_label\']:\n                pos_mask += bbox_mask\n\n        pos_mask = np.asarray(pos_mask == 1, dtype=np.int32)\n        num_positive_pixels = np.sum(pos_mask)\n\n        sum_mask = np.sum(bbox_masks, axis=0)\n        not_overlapped_mask = sum_mask == 1\n\n        for bbox_idx, bbox_mask in enumerate(bbox_masks):\n            bbox_label = labels[bbox_idx]\n            if bbox_label == config[\'ignore_label\']:\n                segm_labels += bbox_mask * not_overlapped_mask * config[\'ignore_label\']\n                continue\n\n            if labels[bbox_idx] == config[\'background_label\']:\n                continue\n\n            text_boxes_mask = bbox_mask * pos_mask\n            segm_labels += text_boxes_mask * bbox_label\n\n            num_bbox_pixels = np.sum(text_boxes_mask)\n            if num_bbox_pixels > 0:\n                per_bbox_weight = num_positive_pixels * 1.0 / num_positive_bboxes\n                per_pixel_weight = per_bbox_weight / num_bbox_pixels\n                segm_weights += text_boxes_mask * per_pixel_weight\n\n            bbox_point_cords = np.where(text_boxes_mask)\n            link_labels[bbox_point_cords] = 1\n\n            new_bbox_contours = cv2.findContours(text_boxes_mask.astype(np.uint8), cv2.RETR_CCOMP,\n                                                 cv2.CHAIN_APPROX_SIMPLE)[-2]\n\n            text_boxes_border_mask = mask.copy()\n            cv2.drawContours(text_boxes_border_mask, new_bbox_contours, -1, 1, 3)\n            text_boxes_border_mask *= text_boxes_mask\n\n            border_points = zip(*np.where(text_boxes_border_mask))\n\n            def in_bbox(neighbour_x, neighbour_y):\n                return text_boxes_mask[neighbour_y, neighbour_x]\n\n            for y_coord, x_coord in border_points:\n                neighbours = get_neighbours(x_coord, y_coord)\n                for n_idx, (neighbour_x, neighbour_y) in enumerate(neighbours):\n                    if not is_valid_coord(neighbour_x, neighbour_y, width, height) \\\n                            or not in_bbox(neighbour_x, neighbour_y):\n                        link_labels[y_coord, x_coord, n_idx] = 0\n\n        link_weights *= np.expand_dims(segm_weights, axis=-1)\n\n        return segm_labels, segm_weights, link_labels, link_weights\n\n    segm_labels, segm_weights, link_labels, link_weights = \\\n        tf.numpy_function(prepare_groundtruth_for_image,\n                          [x_coords, y_coords, labels],\n                          [tf.int32, tf.float32, tf.int32, tf.float32]\n                          )\n\n    segm_labels.set_shape([height, width])\n    segm_weights.set_shape([height, width])\n    link_labels.set_shape([height, width, num_neighbours])\n    link_weights.set_shape([height, width, num_neighbours])\n\n    return segm_labels, segm_weights, link_labels, link_weights\n\n\ndef bboxes_filter_overlap(labels, bboxes, x_coords, y_coords, config):\n    """""" Filter boxes with inappropriate size.""""""\n\n    scores = bboxes_intersection(tf.constant([0, 0, 1, 1], bboxes.dtype), bboxes)\n\n    threshold = config[\'bbox_crop_overlap\']\n    assign_value = config[\'ignore_label\']\n    if assign_value is not None:\n        mask = scores < threshold\n        mask = tf.logical_and(mask, tf.equal(labels, config[\'text_label\']))\n        labels = tf.where(mask, tf.ones_like(labels) * assign_value, labels)\n    else:\n        mask = scores > threshold\n        labels = tf.boolean_mask(tensor=labels, mask=mask)\n        bboxes = tf.boolean_mask(tensor=bboxes, mask=mask)\n        x_coords = tf.boolean_mask(tensor=x_coords, mask=mask)\n        y_coords = tf.boolean_mask(tensor=y_coords, mask=mask)\n    return labels, bboxes, x_coords, y_coords\n\n\nclass TFRecordDataset:\n    """""" Dataset that is used in training. """"""\n\n    def __init__(self, path, config, test=False):\n        self.config = config\n        self.test = test\n\n        dataset = tf.data.TFRecordDataset(path)\n        self.size = sum(1 for _ in dataset)\n\n        if not self.test:\n            dataset = dataset.shuffle(1000 * self.config[\'batch_size\'])\n\n        dataset = dataset.map(self.parse_tf_record,\n                              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        if not self.test:\n            dataset = dataset.map(self.augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            dataset = dataset.map(self.preprocess_input_train)\n            dataset = dataset.map(self.prepare_groundtruth,\n                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            dataset = dataset.batch(self.config[\'batch_size\'] * self.config[\'num_replicas\'],\n                                    drop_remainder=True)\n            dataset = dataset.repeat()\n        else:\n            dataset = dataset.map(self.preprocess_input_test)\n\n        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n        self.dataset = dataset\n\n    def __call__(self):\n        return self.dataset, self.size\n\n    def parse_tf_record(self, example_proto):\n        """""" Parses tf record. """"""\n\n        keys_to_features = {\n            \'image/encoded\': tf.io.FixedLenFeature((), tf.string, default_value=\'\'),\n            \'image/format\': tf.io.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n            \'image/shape\': tf.io.FixedLenFeature([3], tf.int64),\n            \'image/object/bbox/xmin\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/ymin\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/xmax\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/ymax\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/x1\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/x2\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/x3\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/x4\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/y1\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/y2\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/y3\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/y4\': tf.io.VarLenFeature(dtype=tf.float32),\n            \'image/object/bbox/label\': tf.io.VarLenFeature(dtype=tf.int64),\n        }\n\n        parsed_features = tf.io.parse_single_example(serialized=example_proto,\n                                                     features=keys_to_features)\n\n        image = tf.image.decode_jpeg(parsed_features[\'image/encoded\'])\n        glabel = tf.sparse.to_dense(parsed_features[\'image/object/bbox/label\'])\n        gbboxes = tf.transpose(\n            a=tf.stack([tf.sparse.to_dense(parsed_features[\'image/object/bbox/ymin\']),\n                        tf.sparse.to_dense(parsed_features[\'image/object/bbox/xmin\']),\n                        tf.sparse.to_dense(parsed_features[\'image/object/bbox/ymax\']),\n                        tf.sparse.to_dense(parsed_features[\'image/object/bbox/xmax\'])]))\n\n        x1_coord = tf.sparse.to_dense(parsed_features[\'image/object/bbox/x1\'])\n        x2_coord = tf.sparse.to_dense(parsed_features[\'image/object/bbox/x2\'])\n        x3_coord = tf.sparse.to_dense(parsed_features[\'image/object/bbox/x3\'])\n        x4_coord = tf.sparse.to_dense(parsed_features[\'image/object/bbox/x4\'])\n        y1_coord = tf.sparse.to_dense(parsed_features[\'image/object/bbox/y1\'])\n        y2_coord = tf.sparse.to_dense(parsed_features[\'image/object/bbox/y2\'])\n        y3_coord = tf.sparse.to_dense(parsed_features[\'image/object/bbox/y3\'])\n        y4_coord = tf.sparse.to_dense(parsed_features[\'image/object/bbox/y4\'])\n\n        x_coords = tf.transpose(a=tf.stack([x1_coord, x2_coord, x3_coord, x4_coord]))\n        y_coords = tf.transpose(a=tf.stack([y1_coord, y2_coord, y3_coord, y4_coord]))\n\n        image.set_shape([None, None, 3])\n\n        if self.test:\n            stacked = tf.stack([x_coords, y_coords])\n            return image, stacked, glabel\n\n        return image, glabel, gbboxes, x_coords, y_coords\n\n    def preprocess_input(self, image):\n        image = tf.cast(image, tf.float32)\n        shape = image.shape\n\n        def preprocess(image):\n            if self.config[\'model_type\'] in [\'ka_resnet50\', \'ka_vgg16\']:\n                return tf.keras.applications.resnet50.preprocess_input(image)\n            elif self.config[\'model_type\'] in [\'ka_mobilenet_v2_1_0\', \'ka_mobilenet_v2_1_4\']:\n                return tf.keras.applications.mobilenet_v2.preprocess_input(image)\n            elif self.config[\'model_type\'] in [\'mobilenet_v2_ext\']:\n                return image\n            elif self.config[\'model_type\'] in [\'ka_xception\']:\n                return tf.keras.applications.xception.preprocess_input(image)\n            else:\n                raise Exception(\'model_type is not specified.\')\n\n        image = tf.numpy_function(preprocess, [image], image.dtype)\n        image.set_shape(shape)\n\n        return image\n\n    def preprocess_input_train(self, image, labels, x_coords, y_coords):\n        return self.preprocess_input(image), labels, x_coords, y_coords\n\n    def preprocess_input_test(self, image, stacked, glabel):\n        return self.preprocess_input(image), stacked, glabel\n\n    def augment(self, image, labels, bboxes, x_coords, y_coords):\n        """""" Augments dataset sample. """"""\n\n        assert image.dtype == tf.uint8\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n        if self.config[\'rotate\']:\n            image, bboxes, x_coords, y_coords = self.rotate(image, bboxes, x_coords, y_coords)\n        if self.config[\'random_crop\']:\n            image, labels, bboxes, x_coords, y_coords = self.random_crop(image, labels, bboxes,\n                                                                         x_coords, y_coords)\n        image = self.resize(image)\n        if self.config[\'using_shorter_side_filtering\']:\n            labels, bboxes, x_coords, y_coords = self.bboxes_filter_by_shorter_side(labels, bboxes,\n                                                                                    x_coords,\n                                                                                    y_coords)\n        if self.config[\'distort_color\']:\n            image = self.distort_color(image)\n        image = image * 255.\n        return image, labels, x_coords, y_coords\n\n    @tf.function\n    def rotate(self, image, bboxes, x_coords, y_coords):\n        """""" Randomly rotates image and boxes. """"""\n\n        rnd = tf.random.uniform((), minval=0, maxval=1)\n        if tf.less(rnd, self.config[\'rotation_prob\']):\n            image, bboxes, x_coords, y_coords = random_rotate90(image, bboxes, x_coords, y_coords)\n        return image, bboxes, x_coords, y_coords\n\n    @tf.function\n    def random_crop(self, image, labels, bboxes, x_coords, y_coords, max_attempts=200):\n        """""" Makes a random crop. """"""\n\n        num_bboxes = tf.shape(input=bboxes)[0]\n\n        if tf.equal(num_bboxes, 0):\n            xmin = tf.random.uniform((1, 1), minval=0, maxval=0.9)\n            ymin = tf.random.uniform((1, 1), minval=0, maxval=0.9)\n            xmax = xmin + tf.constant(0.1, dtype=tf.float32)\n            ymax = ymin + tf.constant(0.1, dtype=tf.float32)\n            bboxes = tf.concat([ymin, xmin, ymax, xmax], axis=1)\n            labels = tf.constant([self.config[\'background_label\']], dtype=tf.int64)\n            x_coords = tf.concat([xmin, xmax, xmax, xmin], axis=1)\n            y_coords = tf.concat([ymin, ymin, ymax, ymax], axis=1)\n\n        bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n            tf.shape(input=image),\n            bounding_boxes=tf.expand_dims(bboxes, 0),\n            min_object_covered=self.config[\'min_object_covered\'],\n            aspect_ratio_range=self.config[\'crop_aspect_ratio_range\'],\n            area_range=self.config[\'area_range\'],\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        distort_bbox = distort_bbox[0, 0]\n\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        cropped_image.set_shape([None, None, 3])\n        bboxes, x_coords, y_coords = bboxes_resize(distort_bbox, bboxes, x_coords, y_coords)\n        labels, bboxes, x_coords, y_coords = bboxes_filter_overlap(labels, bboxes, x_coords,\n                                                                   y_coords, self.config)\n        return cropped_image, labels, bboxes, x_coords, y_coords\n\n    def resize(self, image):\n        """""" Resizes an image. """"""\n\n        return tf.squeeze(\n            tf.image.resize(tf.expand_dims(image, 0), self.config[\'train_image_shape\']))\n\n    def bboxes_filter_by_shorter_side(self, labels, bboxes, x_coords, y_coords):\n        """""" Filter boxes by their shorter side.""""""\n\n        min_height = self.config[\'min_shorter_side\']\n        max_height = self.config[\'max_shorter_side\']\n        assign_value = self.config[\'ignore_label\']\n\n        x_coords = x_coords * self.config[\'train_image_shape\'][1]\n        y_coords = y_coords * self.config[\'train_image_shape\'][0]\n\n        bbox_rects = tf_min_area_rect(x_coords, y_coords)\n        widths, heights = bbox_rects[:, 2], bbox_rects[:, 3]\n        shorter_sides = tf.minimum(widths, heights)\n        if assign_value is not None:\n            mask = tf.logical_or(shorter_sides < min_height, shorter_sides > max_height)\n            mask = tf.logical_and(mask, tf.equal(labels, self.config[\'text_label\']))\n            labels = tf.where(mask, tf.ones_like(labels) * assign_value, labels)\n        else:\n            mask = tf.logical_and(shorter_sides >= min_height, shorter_sides <= max_height)\n            labels = tf.boolean_mask(tensor=labels, mask=mask)\n            bboxes = tf.boolean_mask(tensor=bboxes, mask=mask)\n            x_coords = tf.boolean_mask(tensor=x_coords, mask=mask)\n            y_coords = tf.boolean_mask(tensor=y_coords, mask=mask)\n        x_coords = x_coords / self.config[\'train_image_shape\'][1]\n        y_coords = y_coords / self.config[\'train_image_shape\'][0]\n        return labels, bboxes, x_coords, y_coords\n\n    @tf.function\n    def distort_color(self, image):\n        """""" Distorts color. """"""\n\n        color_ordering = tf.random.uniform([], maxval=4, dtype=tf.int32)\n        if tf.equal(color_ordering, 0):\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        elif tf.equal(color_ordering, 1):\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n        elif tf.equal(color_ordering, 2):\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        elif tf.equal(color_ordering, 3):\n            image = tf.image.random_hue(image, max_delta=0.2)\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n    def prepare_groundtruth(self, image, labels, x_coords, y_coords):\n        """""" Prepares groundtruth. """"""\n\n        segm_labels, segm_weights, link_labels, link_weights = \\\n            tf_prepare_groundtruth_for_image(x_coords, y_coords, labels, self.config)\n\n        segm_labels = tf.expand_dims(segm_labels, axis=-1)\n        segm_weights = tf.expand_dims(segm_weights, axis=-1)\n\n        segm_labels = tf.cast(segm_labels, tf.float32)\n        link_labels = tf.cast(link_labels, tf.float32)\n\n        cls_output = tf.keras.layers.concatenate([segm_labels, segm_weights])\n        output = tf.keras.layers.concatenate([cls_output, link_labels, link_weights])\n\n        return image, (cls_output, tf.expand_dims(output, axis=-1))\n'"
tensorflow_toolkit/text_detection/text_detection/evaluation.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains evaluation procedure. """"""\n\nimport numpy as np\nimport cv2\nimport Polygon as plg\n\nIOU_CONSTRAINT = 0.5\nAREA_PRECISION_CONSTRAINT = 0.5\n\n\ndef polygon_from_points(points):\n    """""" Returns a Polygon object to use with the Polygon2 class from a list of 8 points:\n        x1,y1,x2,y2,x3,y3,x4,y4\n    """"""\n\n    point_mat = np.array(points[:8]).astype(np.int32).reshape(4, 2)\n    return plg.Polygon(point_mat)\n\n\ndef draw_gt_polygons(image, gt_polygons, gt_dont_care_nums):\n    """""" Draws groundtruth polygons on image. """"""\n\n    for point_idx, polygon in enumerate(gt_polygons):\n        color = (128, 128, 128) if point_idx in gt_dont_care_nums else (255, 0, 0)\n        for i in range(4):\n            pt1 = int(polygon[0][i][0]), int(polygon[0][i][1])\n            pt2 = int(polygon[0][(i + 1) % 4][0]), int(polygon[0][(i + 1) % 4][1])\n            cv2.line(image, pt1, pt2, color, 2)\n    return image\n\n\ndef draw_pr_polygons(image, pr_polygons, pr_dont_care_nums, pr_matched_nums, pr_confidences_list):\n    """""" Draws predicted polygons on image. """"""\n\n    for point_idx, _ in enumerate(pr_polygons):\n        if pr_confidences_list[point_idx] > 0.25:\n            polygon = pr_polygons[point_idx]\n            color = (0, 0, 255)\n            if point_idx in pr_dont_care_nums:\n                color = (255, 255, 255)\n            if point_idx in pr_matched_nums:\n                color = (0, 255, 0)\n            for i in range(4):\n                pt1 = int(polygon[0][i][0]), int(polygon[0][i][1])\n                pt2 = int(polygon[0][(i + 1) % 4][0]), int(polygon[0][(i + 1) % 4][1])\n                cv2.line(image, pt1, pt2, color, 2)\n    return image\n\n\ndef get_union(polygon1, polygon2):\n    """""" Returns area of union of two polygons. """"""\n\n    return polygon1.area() + polygon2.area() - get_intersection(polygon1, polygon2)\n\n\ndef get_intersection_over_union(polygon1, polygon2):\n    """""" Returns intersection over union of two polygons. """"""\n\n    union = get_union(polygon1, polygon2)\n    return get_intersection(polygon1, polygon2) / union if union else 0.0\n\n\ndef get_intersection(polygon1, polygon2):\n    """""" Returns are of intersection of two polygons. """"""\n\n    intersection = polygon1 & polygon2\n    if len(intersection) == 0:\n        return 0\n    return intersection.area()\n\n\ndef compute_ap(conf_list, match_list, num_gt_care):\n    """""" Returns average precision metrics. """"""\n\n    correct = 0\n    average_precision = 0\n    if conf_list:\n        conf_list = np.array(conf_list)\n        match_list = np.array(match_list)\n        sorted_ind = np.argsort(-conf_list)\n        match_list = match_list[sorted_ind]\n\n        for idx, matched in enumerate(match_list):\n            if matched:\n                correct += 1\n                average_precision += float(correct) / (idx + 1)\n\n        if num_gt_care > 0:\n            average_precision /= num_gt_care\n\n    return average_precision\n\n\ndef parse_gt_objects(gt_annotation):\n    """""" Parses groundtruth objects from annotation. """"""\n\n    gt_polygons_list = []\n    gt_dont_care_polygon_nums = []\n    for gt_object in gt_annotation:\n        polygon = polygon_from_points(gt_object[\'points\'])\n        gt_polygons_list.append(polygon)\n        if gt_object[\'transcription\'] == \'###\':\n            gt_dont_care_polygon_nums.append(len(gt_polygons_list) - 1)\n\n    return gt_polygons_list, gt_dont_care_polygon_nums\n\n\ndef parse_pr_objects(pr_annotation):\n    """""" Parses predicted objects from annotation. """"""\n\n    pr_polygons_list = []\n    pr_confidences_list = []\n    for pr_object in pr_annotation:\n        polygon = polygon_from_points(pr_object[\'points\'])\n        pr_polygons_list.append(polygon)\n        pr_confidences_list.append(pr_object[\'confidence\'])\n\n    return pr_polygons_list, pr_confidences_list\n\n\ndef match_dont_care_objects(gt_polygons_list, gt_dont_care_polygon_nums, pr_polygons_list):\n    """""" Matches ignored objects. """"""\n\n    pr_dont_care_polygon_nums = []\n\n    if gt_dont_care_polygon_nums:\n        for pr_polygon_idx, pr_polygon in enumerate(pr_polygons_list):\n            for dont_care_polygon_num in gt_dont_care_polygon_nums:\n                intersected_area = get_intersection(gt_polygons_list[dont_care_polygon_num],\n                                                    pr_polygon)\n                pd_dimensions = pr_polygon.area()\n                precision = 0 if pd_dimensions == 0 else intersected_area / pd_dimensions\n                if precision > AREA_PRECISION_CONSTRAINT:\n                    pr_dont_care_polygon_nums.append(pr_polygon_idx)\n                    break\n\n    return pr_dont_care_polygon_nums\n\n\ndef match(gt_polygons_list, gt_dont_care_polygon_nums, pr_polygons_list, pr_dont_care_polygon_nums):\n    """""" Matches all objects. """"""\n\n    pr_matched_nums = []\n\n    output_shape = [len(gt_polygons_list), len(pr_polygons_list)]\n    iou_mat = np.empty(output_shape)\n    gt_rect_mat = np.zeros(len(gt_polygons_list), np.int8)\n    pr_rect_mat = np.zeros(len(pr_polygons_list), np.int8)\n    for gt_idx, gt_polygon in enumerate(gt_polygons_list):\n        for pr_idx, pr_polygon in enumerate(pr_polygons_list):\n            iou_mat[gt_idx, pr_idx] = get_intersection_over_union(gt_polygon, pr_polygon)\n\n    for gt_idx, _ in enumerate(gt_polygons_list):\n        for pr_idx, _ in enumerate(pr_polygons_list):\n            if gt_rect_mat[gt_idx] == 0 and pr_rect_mat[pr_idx] == 0 \\\n                    and gt_idx not in gt_dont_care_polygon_nums \\\n                    and pr_idx not in pr_dont_care_polygon_nums:\n                if iou_mat[gt_idx, pr_idx] > IOU_CONSTRAINT:\n                    gt_rect_mat[gt_idx] = 1\n                    pr_rect_mat[pr_idx] = 1\n                    pr_matched_nums.append(pr_idx)\n\n    return pr_matched_nums\n\n\ndef eval(pr_annotations, gt_annotations, images=None, imshow_delay=1):\n    """""" Annotation format:\n        {""image_path"": [\n                            {""points"": [x1,y1,x2,y2,x3,y3,x4,y4],\n                             ""confidence"": float,\n                             ""transcription"", str}\n                        ],\n         ""image_path"": [points],\n\n         ### - is a transcription of non-valid word.\n\n    """"""\n\n    assert len(pr_annotations) == len(gt_annotations)\n\n    matched_sum = 0\n    num_global_care_gt = 0\n    num_global_care_pr = 0\n\n    arr_global_confidences = []\n    arr_global_matches = []\n\n    for frame_id, _ in enumerate(pr_annotations):\n        gt_polygons_list, gt_dont_care_polygon_nums = parse_gt_objects(gt_annotations[frame_id])\n        pr_polygons_list, pr_confidences_list = parse_pr_objects(pr_annotations[frame_id])\n\n        pr_dont_care_polygon_nums = match_dont_care_objects(\n            gt_polygons_list, gt_dont_care_polygon_nums, pr_polygons_list)\n\n        pr_matched_nums = []\n\n        if gt_polygons_list and pr_polygons_list:\n            pr_matched_nums = match(gt_polygons_list, gt_dont_care_polygon_nums, pr_polygons_list,\n                                    pr_dont_care_polygon_nums)\n\n            matched_sum += len(pr_matched_nums)\n\n            for pr_num in range(len(pr_polygons_list)):\n                if pr_num not in pr_dont_care_polygon_nums:\n                    # we exclude the don\'t care detections\n                    matched = pr_num in pr_matched_nums\n                    arr_global_confidences.append(pr_confidences_list[pr_num])\n                    arr_global_matches.append(matched)\n\n        num_global_care_gt += len(gt_polygons_list) - len(gt_dont_care_polygon_nums)\n        num_global_care_pr += len(pr_polygons_list) - len(pr_dont_care_polygon_nums)\n\n        if images is not None:\n            image = images[frame_id]\n            draw_gt_polygons(image, gt_polygons_list, gt_dont_care_polygon_nums)\n            draw_pr_polygons(image, pr_polygons_list, pr_dont_care_polygon_nums,\n                             pr_matched_nums, pr_confidences_list)\n            cv2.imshow(\'result\', image)\n            k = cv2.waitKey(imshow_delay)\n            if k == 27:\n                return -1, -1, -1\n\n    method_recall = 0 if num_global_care_gt == 0 else float(matched_sum) / num_global_care_gt\n    method_precision = 0 if num_global_care_pr == 0 else float(matched_sum) / num_global_care_pr\n    denominator = method_precision + method_recall\n    method_hmean = 0 if denominator == 0 else 2.0 * method_precision * method_recall / denominator\n\n    average_precision = compute_ap(arr_global_confidences, arr_global_matches, num_global_care_gt)\n\n    return method_recall, method_precision, method_hmean, average_precision\n'"
tensorflow_toolkit/text_detection/text_detection/loss.py,33,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains loss definition. """"""\n\nimport tensorflow as tf\n\n\nclass ClassificationLoss():\n    """""" Classification loss for pixel-wise segmentation. """"""\n\n    def __init__(self, conf):\n        self.conf = conf\n\n    @tf.function\n    def __call__(self, groundtruth, segm_logits, sample_weight=None):\n        segm_labels, segm_weights = groundtruth[:, :, :, :1], groundtruth[:, :, :, 1:2]\n        segm_labels = tf.cast(segm_labels, tf.int32)\n\n        def hard_negative_mining(scores, n_pos, neg_mask):\n            if n_pos > 0:\n                n_neg = n_pos * self.conf[\'max_neg_pos_ratio\']\n            else:\n                n_neg = 10000\n\n            max_neg_entries = tf.reduce_sum(input_tensor=tf.cast(neg_mask, tf.int32))\n\n            n_neg = tf.minimum(n_neg, max_neg_entries)\n            if n_neg > 0:\n                neg_conf = tf.boolean_mask(tensor=scores, mask=neg_mask)\n                vals, _ = tf.nn.top_k(-neg_conf, k=n_neg)\n                threshold = vals[-1]\n                selected_neg_mask = tf.logical_and(neg_mask, scores <= -threshold)\n            else:\n                selected_neg_mask = tf.zeros_like(neg_mask)\n\n            return tf.cast(selected_neg_mask, tf.int32)\n\n        def batch_hard_negative_mining(neg_conf, pos_mask, neg_mask):\n            selected_neg_mask = []\n            for image_idx in range(self.conf[\'batch_size\']):\n                image_neg_conf = neg_conf[image_idx, :]\n                image_neg_mask = neg_mask[image_idx, :]\n                image_pos_mask = pos_mask[image_idx, :]\n                n_pos = tf.reduce_sum(input_tensor=tf.cast(image_pos_mask, tf.int32))\n                selected_neg_mask.append(\n                    hard_negative_mining(image_neg_conf, n_pos, image_neg_mask))\n\n            selected_neg_mask = tf.stack(selected_neg_mask)\n            return selected_neg_mask\n\n        segm_labels_flatten = tf.reshape(segm_labels, [self.conf[\'batch_size\'], -1])\n        pos_segm_weights_flatten = tf.reshape(segm_weights, [self.conf[\'batch_size\'], -1])\n\n        pos_mask = tf.equal(segm_labels_flatten, self.conf[\'text_label\'])\n        neg_mask = tf.equal(segm_labels_flatten, self.conf[\'background_label\'])\n\n        n_pos = tf.reduce_sum(input_tensor=tf.cast(pos_mask, dtype=tf.float32))\n\n        segm_logits_flatten = tf.reshape(segm_logits, [self.conf[\'batch_size\'], -1,\n                                                       self.conf[\'num_classes\']])\n        segm_scores_flatten = tf.nn.softmax(segm_logits_flatten)\n\n        segm_loss = tf.keras.metrics.sparse_categorical_crossentropy(\n            y_pred=segm_scores_flatten, y_true=tf.cast(pos_mask, dtype=tf.int32))\n\n        segm_neg_scores = segm_scores_flatten[:, :, 0]\n        selected_neg_pixel_mask = batch_hard_negative_mining(segm_neg_scores, pos_mask, neg_mask)\n        segm_weights = pos_segm_weights_flatten + tf.cast(selected_neg_pixel_mask, tf.float32)\n        n_neg = tf.cast(tf.reduce_sum(input_tensor=selected_neg_pixel_mask), tf.float32)\n\n        loss = 0.0\n        if n_neg + n_pos > 0:\n            loss = tf.reduce_sum(input_tensor=segm_loss * segm_weights) / (n_neg + n_pos) * 2.0\n\n        loss = loss / self.conf[\'num_replicas\']\n\n        return loss\n\n\nclass LinkageLoss():\n    """""" Classification loss for pixel-wise neighbourhood. """"""\n\n    def __init__(self, conf):\n        self.conf = conf\n\n    @tf.function\n    def __call__(self, groundtruth, link_logits, sample_weight=None):\n        groundtruth = tf.squeeze(groundtruth, axis=-1)\n        segm_labels, link_labels, link_weights = \\\n            groundtruth[:, :, :, :1], groundtruth[:, :, :, 2:10], groundtruth[:, :, :, 10:18]\n        link_labels = tf.cast(link_labels, tf.int32)\n\n        n_pos = tf.reduce_sum(tf.cast(tf.equal(segm_labels, self.conf[\'text_label\']), tf.float32))\n\n        loss = 0.0\n        if n_pos > 0:\n            link_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=link_logits, labels=link_labels)\n\n            def get_loss(label):\n                mask = tf.equal(link_labels, label)\n                weights = link_weights * tf.cast(mask, tf.float32)\n                n_links = tf.reduce_sum(input_tensor=weights)\n                loss = tf.reduce_sum(input_tensor=link_loss * weights)\n                loss = tf.math.divide_no_nan(loss, n_links)\n\n                return loss\n\n            neg_link_loss = get_loss(self.conf[\'background_label\'])\n            pos_link_loss = get_loss(self.conf[\'text_label\'])\n            loss = pos_link_loss + neg_link_loss\n\n        loss = loss / self.conf[\'num_replicas\']\n\n        return loss\n'"
tensorflow_toolkit/text_detection/text_detection/metrics.py,7,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport cv2\nfrom tqdm import tqdm\nimport numpy as np\nimport tensorflow as tf\n\nfrom text_detection.model import pixel_link_model\nfrom text_detection.evaluation import eval\nfrom text_detection.dataset import get_neighbours, is_valid_coord, TFRecordDataset\nfrom text_detection.common import parse_epoch\n\n\ndef decode_image(segm_scores, link_scores, segm_conf_threshold, link_conf_threshold):\n    """""" Convert softmax scores to mask. """"""\n\n    segm_mask = segm_scores >= segm_conf_threshold\n    link_mask = link_scores >= link_conf_threshold\n    points = list(zip(*np.where(segm_mask)))\n    height, width = np.shape(segm_mask)\n    group_mask = dict.fromkeys(points, -1)\n\n    def find_parent(point):\n        return group_mask[point]\n\n    def set_parent(point, parent):\n        group_mask[point] = parent\n\n    def is_root(point):\n        return find_parent(point) == -1\n\n    def find_root(point):\n        root = point\n        update_parent = False\n        while not is_root(root):\n            root = find_parent(root)\n            update_parent = True\n\n        if update_parent:\n            set_parent(point, root)\n\n        return root\n\n    def join(point1, point2):\n        root1 = find_root(point1)\n        root2 = find_root(point2)\n\n        if root1 != root2:\n            set_parent(root1, root2)\n\n    def get_all():\n        root_map = {}\n\n        def get_index(root):\n            if root not in root_map:\n                root_map[root] = len(root_map) + 1\n            return root_map[root]\n\n        mask = np.zeros_like(segm_mask, dtype=np.int32)\n        for point in points:\n            point_root = find_root(point)\n            bbox_idx = get_index(point_root)\n            mask[point] = bbox_idx\n        return mask\n\n    for point in points:\n        y_coord, x_coord = point\n        neighbours = get_neighbours(x_coord, y_coord)\n        for n_idx, (neighbour_x, neighbour_y) in enumerate(neighbours):\n            if is_valid_coord(neighbour_x, neighbour_y, width, height):\n                link_value = link_mask[y_coord, x_coord, n_idx]\n                segm_value = segm_mask[neighbour_y, neighbour_x]\n                if link_value and segm_value:\n                    join(point, (neighbour_y, neighbour_x))\n\n    mask = get_all()\n    return mask\n\n\ndef rect_to_xys(rect, image_shape):\n    """""" Converts rotated rectangle to points. """"""\n\n    height, width = image_shape[0:2]\n\n    def get_valid_x(x_coord):\n        return np.clip(x_coord, 0, width - 1)\n\n    def get_valid_y(y_coord):\n        return np.clip(y_coord, 0, height - 1)\n\n    rect = ((rect[0], rect[1]), (rect[2], rect[3]), rect[4])\n    points = cv2.boxPoints(rect)\n    points = np.int0(points)\n    for i_xy, (x_coord, y_coord) in enumerate(points):\n        x_coord = get_valid_x(x_coord)\n        y_coord = get_valid_y(y_coord)\n        points[i_xy, :] = [x_coord, y_coord]\n    points = np.reshape(points, -1)\n    return points\n\n\ndef min_area_rect(contour):\n    """""" Returns minimum area rectangle. """"""\n\n    (center_x, cencter_y), (width, height), theta = cv2.minAreaRect(contour)\n    return [center_x, cencter_y, width, height, theta], width * height\n\n\ndef mask_to_bboxes(mask, config, image_shape):\n    """""" Converts mask to bounding boxes. """"""\n\n    image_h, image_w = image_shape[0:2]\n\n    min_area = config[\'min_area\']\n    min_height = config[\'min_height\']\n\n    bboxes = []\n    max_bbox_idx = mask.max()\n    mask = cv2.resize(mask, (image_w, image_h), interpolation=cv2.INTER_NEAREST)\n\n    for bbox_idx in range(1, max_bbox_idx + 1):\n        bbox_mask = (mask == bbox_idx).astype(np.uint8)\n        cnts = cv2.findContours(bbox_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)[-2]\n        if len(cnts) == 0:\n            continue\n        cnt = cnts[0]\n        rect, rect_area = min_area_rect(cnt)\n\n        box_width, box_height = rect[2:-1]\n        if min(box_width, box_height) < min_height:\n            continue\n\n        if rect_area < min_area:\n            continue\n\n        xys = rect_to_xys(rect, image_shape)\n        bboxes.append(xys)\n\n    return bboxes\n\n\ndef decode_batch(segm_scores, link_scores, config):\n    """""" Returns boxes mask for each input image in batch.""""""\n\n    batch_size = segm_scores.shape[0]\n    batch_mask = []\n    for image_idx in range(batch_size):\n        image_pos_pixel_scores = segm_scores[image_idx, :, :]\n        image_pos_link_scores = link_scores[image_idx, :, :, :]\n        mask = decode_image(image_pos_pixel_scores, image_pos_link_scores,\n                            config[\'segm_conf_thr\'], config[\'link_conf_thr\'])\n        batch_mask.append(mask)\n    return np.asarray(batch_mask, np.int32)\n\n\ndef to_boxes(image_data, segm_pos_scores, link_pos_scores, conf):\n    """""" Returns boxes for each image in batch. """"""\n\n    mask = decode_batch(segm_pos_scores, link_pos_scores, conf)[0, ...]\n    bboxes = mask_to_bboxes(mask, conf, image_data.shape)\n\n    return bboxes\n\n\ndef softmax(logits):\n    """""" Returns softmax given logits. """"""\n\n    max_logits = np.max(logits, axis=-1, keepdims=True)\n    numerator = np.exp(logits - max_logits)\n    denominator = np.sum(numerator, axis=-1, keepdims=True)\n    return numerator / denominator\n\n\ndef test(args, config, model=None, dataset=None):\n    """"""This function performs testing of text detection neural network.""""""\n\n    print(\'Evaluating:\', args.weights)\n\n    config[\'segm_conf_thr\'] = 0.8\n    config[\'link_conf_thr\'] = 0.8\n\n    if model is None:\n        model = pixel_link_model(tf.keras.Input(shape=list(args.resolution)[::-1] + [3]), config)\n    model.load_weights(args.weights)\n    if dataset is None:\n        dataset, _ = TFRecordDataset(args.dataset, config, test=True)()\n\n    pr_annotations = []\n    gt_annotations = []\n\n    for image_tensor, coordinates_tensor, labels_tensor in tqdm(dataset, desc=\'Evaluation\'):\n        original_image = image_tensor.numpy().astype(np.float32)\n\n        x_coordinates, y_coordinates = coordinates_tensor.numpy()\n        labels = labels_tensor.numpy()\n        x_coordinates *= original_image.shape[1]\n        y_coordinates *= original_image.shape[0]\n\n        gt_annotation = []\n        for i in range(x_coordinates.shape[0]):\n            bbox_xs = x_coordinates[i].reshape([-1, 1])\n            bbox_ys = y_coordinates[i].reshape([-1, 1])\n            points = np.hstack([bbox_xs, bbox_ys]).reshape([-1]).astype(np.int32).tolist()\n            gt_annotation.append(\n                {\'points\': points, \'transcription\': \'###\' if labels[i] == -1 else \'GOOD_WORD\'})\n\n        gt_annotations.append(gt_annotation)\n        image = original_image.astype(np.float32)\n        image = cv2.resize(image, tuple(args.resolution))\n        segm_logits, link_logits = model.predict(np.array([image]))\n\n        segm_scores = softmax(segm_logits)\n        link_scores = softmax(link_logits)\n        bboxes = to_boxes(original_image, segm_scores[:, :, :, 1],\n                          link_scores[:, :, :, :, 1], config)\n        pr_annotations.append(\n            [{\'points\': bbox.reshape([-1]), \'confidence\': 1.0} for bbox in bboxes])\n\n        if args.imshow_delay >= 0:\n            eval([pr_annotations[-1]], [gt_annotations[-1]],\n                 [cv2.cvtColor(original_image.astype(np.uint8), cv2.COLOR_RGB2BGR)],\n                 args.imshow_delay)\n\n    method_recall, method_precision, method_hmean, _ = eval(pr_annotations, gt_annotations)\n\n    epoch = parse_epoch(args.weights)\n    ema = \'ema\' in os.path.basename(args.weights)\n\n    if ema:\n        tf.summary.scalar(\'ema/hmean\', data=method_hmean, step=epoch)\n        tf.summary.scalar(\'ema/precision\', data=method_precision, step=epoch)\n        tf.summary.scalar(\'ema/recall\', data=method_recall, step=epoch)\n    else:\n        tf.summary.scalar(\'common/hmean\', data=method_hmean, step=epoch)\n        tf.summary.scalar(\'common/precision\', data=method_precision, step=epoch)\n        tf.summary.scalar(\'common/recall\', data=method_recall, step=epoch)\n\n    return method_recall, method_precision, method_hmean\n'"
tensorflow_toolkit/text_detection/text_detection/model.py,23,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains architecture of text detector. """"""\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, SeparableConv2D\n\n\ndef mobilenet_v2(inputs, original_stride, weights_decay=0):\n    """""" Contains MobileNet_v2 definition.\n\n    This is NOT original MobileNet_v2.\n    * Conv2D biases are ON\n    * Extra 1x1 convs are added (SeparableConv2D instead of DepthwiseConv2D)\n    * First mobile_net_block contains more layers than original MobileNet_v2.\n\n    """"""\n\n    def mobile_net_block(inputs, expand_to, strided, num_outputs):\n        # Following expand layer should be only if input_filters < output_fildetes\n        net = Conv2D(filters=expand_to, kernel_size=1, padding=\'same\',\n                     kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(inputs)\n        net = BatchNormalization()(net)\n        net = ReLU(max_value=6)(net)\n\n        net = SeparableConv2D(filters=expand_to, kernel_size=3, strides=2 if strided else 1,\n                              padding=\'same\',\n                              kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(net)\n        net = BatchNormalization()(net)\n        net = ReLU(max_value=6)(net)\n\n        net = Conv2D(filters=num_outputs, kernel_size=1, padding=\'same\',\n                     kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(net)\n        net = BatchNormalization()(net)\n\n        if not strided and net.get_shape().as_list()[-1] == inputs.get_shape().as_list()[-1]:\n            return tf.keras.layers.Add()([inputs, net])\n\n        return net\n\n    end_points = {}\n\n    net = BatchNormalization(name=\'data_bn\')(inputs)\n\n    net = Conv2D(filters=32, kernel_size=3, strides=2, padding=\'same\',\n                 kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(net)\n    net = BatchNormalization()(net)\n    net = ReLU(max_value=6)(net)\n\n    net = mobile_net_block(net, strided=False, expand_to=32, num_outputs=16)\n    end_points[\'2x\'] = net\n\n    net = mobile_net_block(net, strided=True, expand_to=96, num_outputs=24)\n    net = mobile_net_block(net, strided=False, expand_to=144, num_outputs=24)\n    end_points[\'4x\'] = net\n\n    net = mobile_net_block(net, strided=True, expand_to=144, num_outputs=32)\n\n    net = mobile_net_block(net, strided=False, expand_to=192, num_outputs=32)\n    net = mobile_net_block(net, strided=False, expand_to=192, num_outputs=32)\n    if original_stride:\n        end_points[\'8x\'] = net\n\n    net = mobile_net_block(net, strided=original_stride, expand_to=192, num_outputs=64)\n    net = mobile_net_block(net, strided=False, expand_to=384, num_outputs=64)\n    net = mobile_net_block(net, strided=False, expand_to=384, num_outputs=64)\n    net = mobile_net_block(net, strided=False, expand_to=384, num_outputs=64)\n    if not original_stride:\n        end_points[\'8x\'] = net\n\n    net = mobile_net_block(net, strided=not original_stride, expand_to=384, num_outputs=96)\n    net = mobile_net_block(net, strided=False, expand_to=576, num_outputs=96)\n    net = mobile_net_block(net, strided=False, expand_to=576, num_outputs=96)\n    end_points[\'16x\'] = net\n\n    net = mobile_net_block(net, strided=True, expand_to=576, num_outputs=160)\n\n    net = mobile_net_block(net, strided=False, expand_to=960, num_outputs=160)\n    net = mobile_net_block(net, strided=False, expand_to=960, num_outputs=160)\n    net = mobile_net_block(net, strided=False, expand_to=960, num_outputs=320)\n\n    net = Conv2D(filters=1280, kernel_size=1, padding=\'same\',\n                 kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(net)\n    net = BatchNormalization()(net)\n    net = ReLU(max_value=6)(net)\n    end_points[\'32x\'] = net\n\n    return end_points\n\n\ndef fcn_head(inputs, num_classes, name, weights_decay=0):\n    """""" Defines FCN head. """"""\n\n    x32 = tf.keras.layers.Conv2D(\n        filters=num_classes, strides=1, kernel_size=1,\n        kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(inputs[\'32x\'])\n\n    x32_upscaled = tf.keras.layers.UpSampling2D(interpolation=\'bilinear\')(x32)\n    x16 = tf.keras.layers.Add()([tf.keras.layers.Conv2D(\n        filters=num_classes, strides=1, kernel_size=1,\n        kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(inputs[\'16x\']), x32_upscaled])\n\n    x16_upscaled = tf.keras.layers.UpSampling2D(interpolation=\'bilinear\')(x16)\n    x08 = tf.keras.layers.Add()([tf.keras.layers.Conv2D(\n        filters=num_classes, strides=1, kernel_size=1,\n        kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(inputs[\'8x\']), x16_upscaled])\n\n    x08_upscaled = tf.keras.layers.UpSampling2D(interpolation=\'bilinear\')(x08)\n    x04 = tf.keras.layers.Add(name=name)([\n        tf.keras.layers.Conv2D(\n            filters=num_classes, strides=1, kernel_size=1,\n            kernel_regularizer=tf.keras.regularizers.l2(weights_decay))(inputs[\'4x\']), x08_upscaled\n    ])\n\n    return x04\n\n\ndef keras_applications_mobilenetv2(inputs, alpha):\n    from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n\n    base_model = MobileNetV2(alpha=alpha, include_top=False,\n                             weights=\'imagenet\', input_tensor=inputs)\n\n    outputs = {\'4x\': base_model.get_layer(\'block_2_add\').output,\n               \'8x\': base_model.get_layer(\'block_5_add\').output,\n               \'16x\': base_model.get_layer(\'block_12_add\').output,\n               \'32x\': base_model.get_layer(\'out_relu\').output}\n\n    return outputs\n\n\ndef keras_applications_vgg16(inputs):\n    from tensorflow.keras.applications.vgg16 import VGG16\n\n    base_model = VGG16(input_tensor=inputs, weights=\'imagenet\', include_top=False)\n\n    outputs = {\'4x\': base_model.get_layer(\'block3_conv3\').output,\n               \'8x\': base_model.get_layer(\'block4_conv3\').output,\n               \'16x\': base_model.get_layer(\'block5_conv3\').output,\n               \'32x\': base_model.get_layer(\'block5_pool\').output}\n\n    return outputs\n\n\ndef keras_applications_resnet50(inputs):\n    from tensorflow.keras.applications.resnet50 import ResNet50\n\n    base_model = ResNet50(input_tensor=inputs, weights=\'imagenet\', include_top=False)\n\n    try:\n        outputs = {\'4x\': base_model.get_layer(\'activation_9\').output,\n                   \'8x\': base_model.get_layer(\'activation_21\').output,\n                   \'16x\': base_model.get_layer(\'activation_39\').output,\n                   \'32x\': base_model.get_layer(\'activation_48\').output}\n    except:\n        outputs = {\'4x\': base_model.get_layer(\'activation_58\').output,\n                   \'8x\': base_model.get_layer(\'activation_70\').output,\n                   \'16x\': base_model.get_layer(\'activation_88\').output,\n                   \'32x\': base_model.get_layer(\'activation_97\').output}\n\n    return outputs\n\n\ndef keras_applications_xception(inputs):\n    from tensorflow.keras.applications.xception import Xception\n\n    base_model = Xception(input_tensor=inputs, weights=\'imagenet\', include_top=False)\n\n    tf.keras.utils.plot_model(base_model, \'model.png\')\n\n    for layer in base_model.layers:\n        print(layer.name, layer.output)\n\n    outputs = {\n        \'4x\': tf.keras.layers.ZeroPadding2D(((0, 1), (0, 1)))(base_model.get_layer(\'add\').output),\n        \'8x\': base_model.get_layer(\'add_1\').output,\n        \'16x\': base_model.get_layer(\'add_10\').output,\n        \'32x\': base_model.get_layer(\'block14_sepconv2_act\').output}\n\n    return outputs\n\n\ndef pixel_link_model(inputs, config):\n    """""" PixelLink architecture. """"""\n\n    if config[\'model_type\'] == \'mobilenet_v2_ext\':\n        backbone = mobilenet_v2(inputs, original_stride=False,\n                                weights_decay=config[\'weights_decay\'])\n    elif config[\'model_type\'] == \'ka_resnet50\':\n        backbone = keras_applications_resnet50(inputs)\n    elif config[\'model_type\'] == \'ka_vgg16\':\n        backbone = keras_applications_vgg16(inputs)\n    elif config[\'model_type\'] == \'ka_mobilenet_v2_1_0\':\n        backbone = keras_applications_mobilenetv2(inputs, alpha=1.0)\n    elif config[\'model_type\'] == \'ka_mobilenet_v2_1_4\':\n        backbone = keras_applications_mobilenetv2(inputs, alpha=1.4)\n    elif config[\'model_type\'] == \'ka_xception\':\n        backbone = keras_applications_xception(inputs)\n\n    segm_logits = fcn_head(backbone, num_classes=2, name=\'segm_logits\',\n                           weights_decay=config[\'weights_decay\'])\n    link_logits = fcn_head(backbone, num_classes=16, name=\'link_logits_\',\n                           weights_decay=config[\'weights_decay\'])\n\n    new_shape = tf.shape(link_logits)[1], tf.shape(link_logits)[2], 8, 2\n    link_logits = tf.keras.layers.Reshape(new_shape, name=\'link_logits\')(link_logits)\n\n    return tf.keras.Model(inputs, [segm_logits, link_logits])\n'"
tensorflow_toolkit/text_detection/tools/create_dataset.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module allows you to create and write tf record dataset. """"""\n\nimport argparse\nimport os\n\nfrom text_detection.annotation import write_to_tfrecords\n\n\ndef parse_args():\n    """""" Parses arguments. """"""\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'--input_datasets\', required=True, help=\'Comma-separated datasets paths.\')\n    args.add_argument(\'--output\', required=True, help=\'Path where output tf record will be written to.\')\n    args.add_argument(\'--imshow_delay\', type=int, default=-1,\n                      help=\'If it is non-negative, this script will draw detected and groundtruth boxes\')\n\n    return args.parse_args()\n\n\ndef main():\n    """""" Main function. """"""\n\n    args = parse_args()\n    os.makedirs(os.path.dirname(args.output), exist_ok=True)\n    write_to_tfrecords(output_path=args.output, datasets=args.input_datasets.split(\',\'),\n                       imshow_delay=args.imshow_delay)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/text_detection/tools/export.py,13,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module performs freezing of text detection neural network. """"""\n\nimport argparse\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.python.tools.freeze_graph import freeze_graph\n\nfrom text_detection.model import pixel_link_model\nfrom text_detection.common import load_config\n\n\ntf.compat.v1.disable_v2_behavior()\n\n\ndef arg_parser():\n    """""" Returns argument parser. """"""\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--weights\', help=\'Path to trained weights.\')\n    parser.add_argument(\'--resolution\', nargs=2, type=int, default=(1280, 768))\n    parser.add_argument(\'--config\', required=True, help=\'Path to training configuration file.\')\n    parser.add_argument(\'--output_dir\', default=None, help=\'Output Directory\')\n    return parser\n\ndef print_flops(graph):\n    """""" Prints information about FLOPs. """"""\n\n    with graph.as_default():\n        flops = tf.compat.v1.profiler.profile(\n            graph, options=tf.compat.v1.profiler.ProfileOptionBuilder.float_operation())\n        print(\'\')\n        if flops.total_float_ops > 10 ** 9:\n            print(\'Operations number: {} GFlops\'.format(flops.total_float_ops / 10 ** 9))\n        elif flops.total_float_ops > 10 ** 6:\n            print(\'Operations number: {} MFlops\'.format(flops.total_float_ops / 10 ** 6))\n        elif flops.total_float_ops > 10 ** 3:\n            print(\'Operations number: {} KFlops\'.format(flops.total_float_ops / 10 ** 3))\n\n    return flops\n\n\ndef load_frozen_graph(frozen_graph_filename):\n    """""" Loads and returns frozen graph. """"""\n\n    with tf.io.gfile.GFile(frozen_graph_filename, ""rb"") as file:\n        graph_def = tf.compat.v1.GraphDef()\n        graph_def.ParseFromString(file.read())\n\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def, name=\'\')\n\n    return graph\n\n\ndef freeze(args, config):\n    """""" Exports model to TF 1.x saved_model (simple_save) and freezes graph. """"""\n    tf.keras.backend.set_learning_phase(0)\n    input_tensor = tf.compat.v1.placeholder(dtype=tf.float32,\n                                            shape=[1, None, None, 3])\n    model = pixel_link_model(tf.keras.Input(tensor=input_tensor), config=config)\n    segm_logits, link_logits = model(input_tensor, training=False)\n\n    link_logits = tf.reshape(link_logits, tf.concat([tf.shape(link_logits)[0:3], [config[\'num_neighbours\'] * 2]], -1))\n\n    export_folder = args.output_dir if args.output_dir else os.path.join(os.path.dirname(args.weights), \'export\')\n\n    with tf.compat.v1.Session() as sess:\n        model.load_weights(args.weights)\n\n        tf.compat.v1.saved_model.simple_save(sess, export_folder,\n                                             inputs={\'input\': input_tensor},\n                                             outputs={segm_logits.name[:-2]: segm_logits,\n                                                      link_logits.name[:-2]: link_logits})\n\n        frozen_graph_path = os.path.join(export_folder, \'frozen_graph.pb\')\n\n        output_node_names = (segm_logits.name[:-2], link_logits.name[:-2])\n        freeze_graph(\n            input_graph=None,\n            input_saver=\'\',\n            input_binary=True,\n            input_checkpoint=\'\',\n            output_node_names=\',\'.join(output_node_names),\n            restore_op_name=\'save/restore_all\',\n            filename_tensor_name=\'save/Const:0\',\n            output_graph=frozen_graph_path,\n            clear_devices=True,\n            initializer_nodes=\'\',\n            input_meta_graph=None,\n            input_saved_model_dir=export_folder,\n        )\n\n        graph = load_frozen_graph(frozen_graph_path)\n        print_flops(graph)\n\n        print(\'\')\n        print(\'Output tensor names for using in InferenceEngine:\')\n        print(\'     model/link_logits_/add\')\n        print(\'     model/segm_logits/add\')\n        print(\'\')\n        print(\'Run model_optimizer to get IR: mo.py --input_model {} --framework tf\'.format(\n            frozen_graph_path))\n\n\ndef main():\n    """""" Main function. """"""\n    args = arg_parser().parse_args()\n    config = load_config(args.config)\n    freeze(args, config)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/text_detection/tools/prepare_annotation.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module allows you to convert source dataset to internal format. """"""\n\nimport argparse\nfrom text_detection.annotation import TextDetectionDataset\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'--out_annotation\', help=\'Path where annotaion will be saved to.\',\n                      required=True)\n    args.add_argument(\'--in_annotation\', help=\'Path to annotation in source format.\')\n    args.add_argument(\'--images\', help=\'Path to dataset images.\', required=True)\n    args.add_argument(\'--type\', choices=[\'icdar15\', \'toy\', \'icdar17_mlt\', \'icdar19_mlt\', \'cocotext_v2\'],\n                      help=\'Source dataset type/name.\', required=True)\n    args.add_argument(\'--train\', action=\'store_true\')\n    args.add_argument(\'--imshow_delay\', type=int, default=-1,\n                      help=\'If it is non-negative, this script will draw detected and groundtruth\'\n                           \'boxes\')\n\n    return args.parse_args()\n\n\ndef main():\n    """""" Main function. """"""\n    args = parse_args()\n\n    if args.type == \'icdar15\':\n        text_detection_dataset = TextDetectionDataset.read_from_icdar2015(\n            args.images, args.annotation, is_training=args.train)\n    elif args.type == \'icdar19_mlt\':\n        text_detection_dataset = TextDetectionDataset.read_from_icdar2019_mlt(args.images)\n    elif args.type == \'icdar17_mlt\':\n        text_detection_dataset = TextDetectionDataset.read_from_icdar2017_mlt(args.images, is_training=False)\n    elif args.type == \'cocotext_v2\':\n        text_detection_dataset = TextDetectionDataset.read_from_coco_text(args.images)\n    elif args.type == \'toy\':\n        text_detection_dataset = TextDetectionDataset.read_from_toy_dataset(args.images)\n\n    text_detection_dataset.write(args.out_annotation)\n    if args.imshow_delay >= 0:\n        text_detection_dataset.visualize(put_text=True, imshow_delay=args.imshow_delay)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/text_detection/tools/test.py,2,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module performs testing of text detection neural network. """"""\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom text_detection.model import pixel_link_model\nfrom text_detection.metrics import test\nfrom text_detection.common import load_config, parse_epoch\n\n\ndef arg_parser():\n    """""" Returns argument parser. """"""\n\n    parser = argparse.ArgumentParser(description=\'Runs an evaluation of text detection.\')\n\n    parser.add_argument(\'--weights\')\n    parser.add_argument(\'--weights_folder\')\n    parser.add_argument(\'--resolution\', nargs=2, type=int, default=(1280, 768))\n    parser.add_argument(\'--dataset\', required=True)\n    parser.add_argument(\'--config\', required=True)\n    parser.add_argument(\'--imshow_delay\', type=int, default=-1,\n                        help=\'If it is non-negative, this script will draw detected and groundtruth\'\n                             \'boxes\')\n\n    return parser\n\n\ndef main():\n    """""" Main function. """"""\n\n    args = arg_parser().parse_args()\n    config = load_config(args.config)\n\n    if args.weights:\n        print(args.weights, \'(recall, precision, method_hmean)\', test(args, config))\n    elif args.weights_folder:\n        args.weights_folder = os.path.abspath(args.weights_folder)\n\n        try:\n            with open(os.path.join(args.weights_folder, \'evaluations.txt\')) as opened_file:\n                already_tested_weights = [line.strip().split()[0] for line in opened_file.readlines()]\n        except:\n            already_tested_weights = []\n\n        model = pixel_link_model(tf.keras.Input(shape=list(args.resolution)[::-1] + [3]), config)\n\n        newly_tested = []\n        with tf.summary.create_file_writer(\n                os.path.join(args.weights_folder, \'../logs\')).as_default():\n\n            weights_list = [\'.\'.join(x.split(\'.\')[:-1])\n                            for x in os.listdir(args.weights_folder) if x.startswith(\'model\')]\n            weights_list = list(set(weights_list))\n            weights_list = [os.path.join(args.weights_folder, x) for x in weights_list]\n            weights_list = [x for x in weights_list if x not in already_tested_weights]\n            weights_list = sorted(weights_list, key=lambda x: parse_epoch(x))\n\n            for weights in weights_list:\n                args.weights = weights\n                result = test(args, config, model=model)\n                newly_tested.append(args.weights + str(result[-1]))\n                already_tested_weights.append(args.weights)\n                print(args.weights, \'(recall, precision, method_hmean)\', result)\n\n                with open(os.path.join(args.weights_folder, \'evaluations.txt\'), \'a+\') as opened_file:\n                    opened_file.write(\'{} {}\\n\'.format(args.weights, result[-1]))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/text_detection/tools/train.py,22,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module performs training of text detection neural network. """"""\n\nimport argparse\nimport os\nimport test\nimport yaml\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom text_detection.loss import ClassificationLoss, LinkageLoss\nfrom text_detection.model import pixel_link_model\nfrom text_detection.dataset import TFRecordDataset\n\n\ndef arg_parser():\n    """""" Parses arguments. """"""\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'--train_dir\', required=True, help=\'Training folder.\')\n    args.add_argument(\'--model_type\', choices=[\'mobilenet_v2_ext\',\n                                               \'ka_vgg16\',\n                                               \'ka_resnet50\',\n                                               \'ka_mobilenet_v2_1_0\',\n                                               \'ka_mobilenet_v2_1_4\',\n                                               \'ka_xception\'],\n                      required=True)\n    args.add_argument(\'--learning_rate\', type=float, default=0.01, help=\'Learning rate\')\n    args.add_argument(\'--momentum\', type=float, default=0.9,\n                      help=\'The momentum for the MomentumOptimizer\')\n    args.add_argument(\'--weights\', help=\'Path to pretrained weights.\')\n    args.add_argument(\'--train_dataset\', required=True, help=\'Training dataset path.\')\n    args.add_argument(\'--test_dataset\', help=\'Test dataset path.\')\n    args.add_argument(\'--test_resolution\', type=int, nargs=2, default=[1280, 768],\n                      help=\'Test image resolution.\')\n    args.add_argument(\'--epochs_per_evaluation\', type=int, default=1)\n    args.add_argument(\'--num_epochs\', type=int, default=1000000)\n    args.add_argument(\'--config\', required=True, help=\'Path to configuration file.\')\n\n    return args\n\n\ndef config_initialization(args):\n    """""" Initializes training configuration. """"""\n\n    with open(args.config) as opened_file:\n        config = yaml.load(opened_file)\n\n    config[\'model_type\'] = args.model_type\n    config[\'imagenet_preprocessing\'] = args.model_type != \'mobilenet_v2_ext\'\n\n    return config\n\n\ndef save_config(config, folder):\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    with open(os.path.join(folder, \'configuration.yaml\'), \'w\') as read_file:\n        yaml.dump(config, read_file)\n\n\nclass ExponentialMovingAverageCallback(tf.keras.callbacks.Callback):\n    """""" Callback for Exponential Moving Average computation. """"""\n\n    def __init__(self, model, epoch):\n        super(ExponentialMovingAverageCallback, self).__init__()\n\n        self.model = model\n        self.decay = 0.9999\n        self.epoch = epoch\n        self.averages = {}\n\n\n        self.num_updates = 0\n        with tf.name_scope(\'ema\'):\n            for var in model.trainable_variables:\n                self.averages[var] = tf.Variable(var)\n\n    # pylint: disable=unused-argument\n    def on_batch_end(self, batch, logs=None):\n        """""" Exponential Moving Average computation callback. """"""\n        if self.epoch == 0:\n            decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))\n        else:\n            decay = self.decay\n\n        for var in self.model.trainable_variables:\n            self.averages[var].assign(decay * self.averages[var] + (1 - decay) * var)\n\n        self.num_updates += 1\n\n    def copy_weights_from_model(self):\n        for var in self.model.trainable_variables:\n            self.averages[var].assign(var)\n\n    def copy_weights_to_model(self):\n        for var in self.model.trainable_variables:\n            var.assign(self.averages[var])\n\n\nclass Args:\n    """""" Test arguments. """"""\n\n    def __init__(self):\n        self.weights = None\n        self.resolution = None\n        self.imshow_delay = -1\n\n\ndef train(args, config):\n    """""" This function performs training of text detection neural network. """"""\n\n    def get_weights_path(latest_checkpoint, is_ema):\n        base_name = os.path.basename(latest_checkpoint)\n        path = os.path.join(args.train_dir, \'weights\',\n                            base_name + (\'.ema\' if is_ema else \'\') + \'.save_weights\')\n        return path\n\n    def get_epoch(latest_checkpoint):\n        if latest_checkpoint is None:\n            return 0\n        return int(latest_checkpoint.split(\'-\')[-1])\n\n    save_config(config, args.train_dir)\n    config[\'num_replicas\'] = 1\n\n    dataset, size = TFRecordDataset(args.train_dataset, config)()\n\n    model = pixel_link_model(\n        inputs=tf.keras.Input(shape=config[\'train_image_shape\'] + [3]),\n        config=config)\n\n    loss = [ClassificationLoss(config), LinkageLoss(config)]\n\n    optimizer = tf.optimizers.SGD(learning_rate=args.learning_rate, momentum=args.momentum)\n    model.compile(loss=loss, optimizer=optimizer)\n\n    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n\n    if args.weights:\n        model.load_weights(args.weights)\n\n    latest_checkpoint = tf.train.latest_checkpoint(args.train_dir)\n    if latest_checkpoint is not None:\n        checkpoint.restore(latest_checkpoint)\n        # Here is a workaround how to save/load EMA weights.\n        model.load_weights(get_weights_path(latest_checkpoint, is_ema=True))\n\n    ema_cb = ExponentialMovingAverageCallback(model, get_epoch(latest_checkpoint))\n    ema_cb.copy_weights_from_model()\n\n    if args.test_dataset:\n        model_test = pixel_link_model(tf.keras.Input(shape=args.test_resolution[::-1] + [3]),\n                                      config=config)\n        test_args = Args()\n        test_args.resolution = tuple(args.test_resolution)\n        dataset_test, _ = TFRecordDataset(args.test_dataset, config, test=True)()\n\n    with tf.summary.create_file_writer(args.train_dir + ""/logs"").as_default():\n        for _ in range(int(np.ceil(args.num_epochs / args.epochs_per_evaluation))):\n            latest_checkpoint_before_fit = tf.train.latest_checkpoint(args.train_dir)\n            if latest_checkpoint_before_fit is not None:\n                model.load_weights(get_weights_path(latest_checkpoint_before_fit, is_ema=False))\n\n            history = model.fit(dataset, epochs=args.epochs_per_evaluation,\n                                steps_per_epoch=size // config[\'batch_size\'], callbacks=[ema_cb])\n            checkpoint.save(os.path.join(args.train_dir, \'model\'))\n\n            latest_checkpoint_after_fit = tf.train.latest_checkpoint(args.train_dir)\n\n            # Save weights.\n            model.save_weights(get_weights_path(latest_checkpoint_after_fit, is_ema=False))\n\n            # Save ema weights.\n            ema_cb.copy_weights_to_model()\n            model.save_weights(get_weights_path(latest_checkpoint_after_fit, is_ema=True))\n\n            epoch = get_epoch(latest_checkpoint_after_fit)\n\n            tf.summary.scalar(\'training/loss\', data=history.history[\'loss\'][-1], step=epoch)\n            tf.summary.scalar(\'training/segm_logits_loss\',\n                              data=history.history[\'segm_logits_loss\'][-1] * config[\'num_replicas\'],\n                              step=epoch)\n            tf.summary.scalar(\'training/link_logits_loss\',\n                              data=history.history[\'link_logits_loss\'][-1] * config[\'num_replicas\'],\n                              step=epoch)\n            tf.summary.scalar(\'training/learning_rate\', data=args.learning_rate, step=epoch)\n            tf.summary.scalar(\'training/batch_size\', data=config[\'batch_size\'], step=epoch)\n\n            if args.test_dataset:\n                test_args.weights = get_weights_path(latest_checkpoint_after_fit, is_ema=False)\n                recall, precision, hmean = test.test(test_args, config, model=model_test,\n                                                     dataset=dataset_test)\n                print(\'{} (recall, precision, hmean) = ({:.4f}, {:.4f}, {:.4f})\'.format(\n                    test_args.weights, recall, precision, hmean))\n                tf.summary.scalar(\'common/hmean\', data=hmean, step=epoch)\n                tf.summary.scalar(\'common/precision\', data=precision, step=epoch)\n                tf.summary.scalar(\'common/recall\', data=recall, step=epoch)\n\n                test_args.weights = get_weights_path(latest_checkpoint_after_fit, is_ema=True)\n                recall, precision, hmean = test.test(test_args, config, model=model_test,\n                                                     dataset=dataset_test)\n                print(\'{} (recall, precision, hmean) = ({:.4f}, {:.4f}, {:.4f})\'.format(\n                    test_args.weights, recall, precision, hmean))\n                tf.summary.scalar(\'ema/hmean\', data=hmean, step=epoch)\n                tf.summary.scalar(\'ema/precision\', data=precision, step=epoch)\n                tf.summary.scalar(\'ema/recall\', data=recall, step=epoch)\n\n\ndef main():\n    """""" Main function. """"""\n\n    args = arg_parser().parse_args()\n    config = config_initialization(args)\n    train(args, config)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/text_recognition/tests/unittests.py,7,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n"""""" This module contains unit tests. """"""\n\nimport unittest\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom text_recognition.dataset import Dataset\nfrom text_recognition.model import TextRecognition\n\n\nclass TestCreateAnnotaion(unittest.TestCase):\n    """""" Tests set for annotation. """"""\n\n    def setUp(self):\n        """""" setUp method for tests. """"""\n\n        self.image_width = 120\n        self.image_height = 32\n        self.dataset = Dataset(\'../../data/text_recognition/annotation.txt\',\n                               self.image_width, self.image_height, repeat=1)\n\n    def test_num_frames(self):\n        """""" Test for checking number of frames in annotation. """"""\n\n        self.assertEqual(len(self.dataset), 64)\n\n    def test_images_shapes(self):\n        """""" Test for checking images shapes. """"""\n\n        get_next = self.dataset().make_one_shot_iterator().get_next()\n\n        with tf.Session() as sess:\n            x, _ = sess.run(get_next)\n            assert x.shape == (1, self.image_height, self.image_width, 1)\n\n\nclass TestTraining(unittest.TestCase):\n    """""" Tests set for training. """"""\n\n    def setUp(self):\n        """""" setUp method for tests. """"""\n\n        self.seq_length = 30\n        self.batch_size = 64\n\n        self.image_width = 120\n        self.image_height = 32\n        self.dataset = Dataset(\'../../data/text_recognition/annotation.txt\',\n                               self.image_width, self.image_height,\n                               repeat=None, batch_size=self.batch_size)\n\n    def test_training_loss(self):\n        """""" Test for checking that training loss decreases. """"""\n\n        model = TextRecognition(is_training=True, num_classes=self.dataset.num_classes)\n\n        next_sample = self.dataset().make_one_shot_iterator().get_next()\n        model_out = model(inputdata=next_sample[0])\n\n        ctc_loss = tf.reduce_mean(tf.nn.ctc_loss(labels=next_sample[1], inputs=model_out,\n                                                 sequence_length=self.seq_length * np.ones(self.batch_size)))\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n        with tf.control_dependencies(update_ops):\n            optimizer = tf.train.AdadeltaOptimizer(0.1).minimize(loss=ctc_loss)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            l0 = sess.run(ctc_loss)\n            print(\'loss before\', l0)\n\n            for _ in range(10):\n                sess.run(optimizer)\n\n            l1 = sess.run(ctc_loss)\n            print(\'loss after\', l1)\n            assert l1 < l0\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tensorflow_toolkit/text_recognition/text_recognition/dataset.py,9,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains Dataset class that operates with training data. """"""\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\n\nclass Dataset():\n    """""" Class for working with training datasets. """"""\n\n    def __init__(self, annotation_path, image_width, image_height, batch_size=1, shuffle=False, repeat=None):\n\n        impaths, labels = Dataset.parse_datasets_arg(annotation_path)\n        self.batch_size = batch_size\n        self.image_height = image_height\n        self.image_width = image_width\n        self.size = len(impaths)\n        self.char_to_int, self.int_to_char, self.num_classes = Dataset.create_character_maps()\n\n        dataset = tf.data.Dataset.from_tensor_slices((impaths, labels))\n        if shuffle:\n            dataset = dataset.shuffle(len(impaths), reshuffle_each_iteration=True)\n        dataset = dataset.map(\n            lambda filename, label: tuple(\n                tf.py_func(self.read_py_function, [filename, label], [tf.float32, tf.string])))\n        dataset = dataset.map(\n            lambda image, label: tuple(\n                tf.py_func(self.convert_labels_to_int32_array, [image, label],\n                           [tf.float32, tf.int32])))\n        dataset = dataset.map(\n            lambda image, label: tuple((image, self.to_sparse_tensor(label))))\n\n        dataset = dataset.batch(batch_size, drop_remainder=True)\n        dataset = dataset.map(self.set_shapes)\n        dataset = dataset.repeat(repeat)\n        dataset = dataset.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)\n\n        self.dataset = dataset\n\n    def __call__(self):\n        return self.dataset\n\n    def __len__(self):\n        return self.size\n\n    def convert_labels_to_int32_array(self, image, label):\n        """""" Converts text to integer representation. """"""\n\n        values = np.array([self.char_to_int[y] for y in label.decode(\'utf-8\').lower()],\n                          dtype=np.int32)\n        return image, values\n\n    def set_shapes(self, image, labels):\n        """""" Sets shapes for tensors. """"""\n\n        image.set_shape([self.batch_size, self.image_height, self.image_width, 1])\n        return image, labels\n\n    def read_py_function(self, filename, label):\n        """""" Reads and pre-processes an image. """"""\n\n        try:\n            image = cv2.imread(filename.decode(\'utf-8\'), cv2.IMREAD_COLOR)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            image = image.astype(np.float32)\n            image = cv2.resize(image, (self.image_width, self.image_height))\n            image = np.expand_dims(image, axis=-1)\n        except:\n            print(filename)\n            print(image.shape)\n            raise Exception\n        return image, label\n\n    @staticmethod\n    def create_character_maps():\n        """""" Creates character-to-int and int-to-character maps. """"""\n\n        alfabet = \'0123456789abcdefghijklmnopqrstuvwxyz\'\n        char_to_int = {}\n        int_to_char = []\n        for i, l in enumerate(alfabet):\n            char_to_int[l] = i\n            int_to_char.append(l)\n        return char_to_int, int_to_char, len(char_to_int) + 1\n\n    @staticmethod\n    def parse_datasets_arg(annotation_path):\n        """""" Parses datasets argument. """"""\n\n        impaths = []\n        labels = []\n        for annpath in annotation_path.split(\',\'):\n            annotation_folder = os.path.dirname(annpath)\n            with open(annpath, encoding=""utf-8-sig"") as f:\n                content = np.array([line.strip().split() for line in f.readlines()])\n                impaths_local = content[:, 0]\n                impaths_local = [os.path.join(annotation_folder, line) for line in impaths_local]\n                labels_local = content[:, 1]\n                impaths.extend(impaths_local)\n                labels.extend(labels_local)\n\n        return impaths, labels\n\n    @staticmethod\n    def to_sparse_tensor(dense_tensor):\n        """""" Converts dense tensor to sparse. """"""\n\n        indices = tf.where(tf.not_equal(dense_tensor, -1))\n        values = tf.gather_nd(dense_tensor, indices)\n        shape = tf.shape(dense_tensor, out_type=tf.int64)\n        return tf.SparseTensor(indices, values, shape)\n\n    @staticmethod\n    def sparse_tensor_to_str(sparse_tensor, int_to_char):\n        """""" Converts sparse tensor to text string. """"""\n\n        indices_set = set(sparse_tensor.indices[:, 0])\n        result = {}\n        for ind in indices_set:\n            elements = sparse_tensor.indices[:, 0] == ind\n            result[ind] = \'\'.join(\n                [int_to_char[tmp] for tmp in sparse_tensor.values[np.ix_(elements)]])\n        return result\n'"
tensorflow_toolkit/text_recognition/text_recognition/model.py,22,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains architecture of Text Recognition model.""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport tensorflow.contrib.slim as slim\n\n\nclass TextRecognition:\n    """""" Text recognition model definition. """"""\n\n    def __init__(self, is_training, num_classes, backbone_dropout=0.0):\n        self.is_training = is_training\n        self.lstm_dim = 256\n        self.num_classes = num_classes\n        self.backbone_dropout = backbone_dropout\n\n    def __call__(self, inputdata):\n        with tf.variable_scope(\'shadow\'):\n            features = self.feature_extractor(inputdata=inputdata)\n            logits = self.encoder_decoder(inputdata=tf.squeeze(features, axis=1))\n\n        return logits\n\n    # pylint: disable=too-many-locals\n    def feature_extractor(self, inputdata):\n        """""" Extracts features from input text image. """"""\n\n        with slim.arg_scope([slim.conv2d], padding=\'SAME\',\n                            weights_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                            weights_regularizer=slim.l2_regularizer(0.00025),\n                            biases_initializer=None, activation_fn=None):\n            with slim.arg_scope([slim.batch_norm], updates_collections=None):\n                bn0 = slim.batch_norm(inputdata, 0.9, scale=True, is_training=self.is_training,\n                                      activation_fn=None)\n\n                dropout1 = slim.dropout(bn0, keep_prob=1.0 - self.backbone_dropout,\n                                        is_training=self.is_training)\n                conv1 = slim.conv2d(dropout1, num_outputs=64, kernel_size=3)\n                bn1 = slim.batch_norm(conv1, 0.9, scale=True, is_training=self.is_training,\n                                      activation_fn=tf.nn.relu)\n                pool1 = slim.max_pool2d(bn1, kernel_size=2, stride=2)\n\n                dropout2 = slim.dropout(pool1, keep_prob=1.0 - self.backbone_dropout,\n                                        is_training=self.is_training)\n                conv2 = slim.conv2d(dropout2, num_outputs=128, kernel_size=3)\n                bn2 = slim.batch_norm(conv2, 0.9, scale=True, is_training=self.is_training,\n                                      activation_fn=tf.nn.relu)\n                pool2 = slim.max_pool2d(bn2, kernel_size=2, stride=2)\n\n                dropout3 = slim.dropout(pool2, keep_prob=1.0 - self.backbone_dropout,\n                                        is_training=self.is_training)\n                conv3 = slim.conv2d(dropout3, num_outputs=256, kernel_size=3)\n                bn3 = slim.batch_norm(conv3, 0.9, scale=True, is_training=self.is_training,\n                                      activation_fn=tf.nn.relu)\n\n                dropout4 = slim.dropout(bn3, keep_prob=1.0 - self.backbone_dropout,\n                                        is_training=self.is_training)\n                conv4 = slim.conv2d(dropout4, num_outputs=256, kernel_size=3)\n                bn4 = slim.batch_norm(conv4, 0.9, scale=True, is_training=self.is_training,\n                                      activation_fn=tf.nn.relu)\n                pool4 = slim.max_pool2d(bn4, kernel_size=[2, 1], stride=[2, 1])\n\n                dropout5 = slim.dropout(pool4, keep_prob=1.0 - self.backbone_dropout,\n                                        is_training=self.is_training)\n                conv5 = slim.conv2d(dropout5, num_outputs=512, kernel_size=3)\n                bn5 = slim.batch_norm(conv5, 0.9, scale=True, is_training=self.is_training,\n                                      activation_fn=tf.nn.relu)\n\n                dropout6 = slim.dropout(bn5, keep_prob=1.0 - self.backbone_dropout,\n                                        is_training=self.is_training)\n                conv6 = slim.conv2d(dropout6, num_outputs=512, kernel_size=3)\n                bn6 = slim.batch_norm(conv6, 0.9, scale=True, is_training=self.is_training,\n                                      activation_fn=tf.nn.relu)\n                pool6 = slim.max_pool2d(bn6, kernel_size=[2, 1], stride=[2, 1])\n\n                dropout7 = slim.dropout(pool6, keep_prob=1.0 - self.backbone_dropout,\n                                        is_training=self.is_training)\n                conv7 = slim.conv2d(dropout7, num_outputs=512, kernel_size=2, stride=[2, 1])\n                bn7 = slim.batch_norm(conv7, 0.9, scale=True, is_training=self.is_training,\n                                      activation_fn=tf.nn.relu)\n\n        return bn7\n\n    def encoder_decoder(self, inputdata):\n        """""" LSTM-based encoder-decoder module. """"""\n\n        with tf.variable_scope(\'LSTMLayers\'):\n            [batch_size, width, _] = inputdata.get_shape().as_list()\n\n            with tf.variable_scope(\'encoder\'):\n                forward_cells = []\n                backward_cells = []\n\n                for _ in range(2):\n                    forward_cells.append(tf.nn.rnn_cell.LSTMCell(self.lstm_dim))\n                    backward_cells.append(tf.nn.rnn_cell.LSTMCell(self.lstm_dim))\n\n                encoder_layer, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n                    forward_cells, backward_cells, inputdata, dtype=tf.float32)\n\n            with tf.variable_scope(\'decoder\'):\n                forward_cells = []\n                backward_cells = []\n\n                for _ in range(2):\n                    forward_cells.append(tf.nn.rnn_cell.LSTMCell(self.lstm_dim))\n                    backward_cells.append(tf.nn.rnn_cell.LSTMCell(self.lstm_dim))\n\n                decoder_layer, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n                    forward_cells, backward_cells, encoder_layer, dtype=tf.float32)\n\n            rnn_reshaped = tf.reshape(decoder_layer, [batch_size * width, -1])\n\n            logits = slim.fully_connected(rnn_reshaped, self.num_classes, activation_fn=None)\n            logits = tf.reshape(logits, [batch_size, width, self.num_classes])\n            rnn_out = tf.transpose(logits, (1, 0, 2))\n\n        return rnn_out\n'"
tensorflow_toolkit/text_recognition/tools/export.py,5,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This script allows you to freeze Text Recognition model. """"""\n\nimport argparse\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.python.tools.freeze_graph import freeze_graph\n\nfrom text_recognition.model import TextRecognition\nfrom text_recognition.dataset import Dataset\nfrom tfutils.helpers import execute_mo\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--checkpoint\', required=True, help=\'Pretrained model path.\')\n    parser.add_argument(\'--data_type\', default=\'FP32\', choices=[\'FP32\', \'FP16\'], help=\'Data type of IR\')\n    parser.add_argument(\'--output_dir\', default=None, help=\'Output Directory\')\n    return parser.parse_args()\n\n\ndef freezing_graph(sess, graph_file, output_node_names):\n    """""" Saves model as frozen graph.""""""\n\n    assert graph_file.endswith(\'.pb\')\n\n    directory = os.path.dirname(graph_file)\n    base = os.path.basename(graph_file)\n    ckpt = graph_file.replace(\'.pb\', \'.ckpt\')\n    frozen = graph_file.replace(\'.pb\', \'.pb.frozen\')\n\n    os.system(\'mkdir -p {}\'.format(directory))\n    print(\'>> Saving `{}`... \'.format(graph_file), end=\'\')\n    tf.train.write_graph(sess.graph, directory, base, as_text=False)\n    print(\'Done\')\n\n    print(\'>> Saving `{}`... \'.format(ckpt), end=\'\')\n    tf.train.Saver().save(sess, ckpt, write_meta_graph=False)\n    print(\'Done\')\n\n    print(\'>> Running `freeze_graph.py`... \')\n    print(\'Outputs:\\n  {}\'.format(\', \'.join(output_node_names)))\n\n    freeze_graph(input_graph=graph_file,\n                 input_saver=\'\',\n                 input_binary=True,\n                 input_checkpoint=ckpt,\n                 output_node_names=\',\'.join(output_node_names),\n                 restore_op_name=\'save/restore_all\',\n                 filename_tensor_name=\'save/Const:0\',\n                 output_graph=frozen,\n                 clear_devices=True,\n                 initializer_nodes=\'\',\n                 saved_model_tags=\'serve\')\n\n    return frozen\n\n\ndef main():\n    """""" Main freezing function. """"""\n\n    args = parse_args()\n\n    image_width = 120\n    image_height = 32\n\n    _, _, num_classes = Dataset.create_character_maps()\n\n    model = TextRecognition(is_training=False, num_classes=num_classes)\n    model_out = model(inputdata=tf.placeholder(tf.float32, [1, image_height, image_width, 1]))\n\n    output_dir = args.output_dir if args.output_dir else os.path.join(os.path.dirname(args.checkpoint), \'export\')\n\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        saver.restore(sess=sess, save_path=args.checkpoint)\n        graph_file = os.path.join(output_dir, \'graph.pb\')\n        frozen_graph = freezing_graph(sess, graph_file, output_node_names=[model_out.name[:-2]])\n\n    mo_params = {\n        \'model_name\': \'text_recognition\',\n        \'data_type\': args.data_type,\n    }\n\n    export_ir_dir = os.path.join(output_dir, \'IR\', args.data_type)\n    execute_mo(mo_params, frozen_graph, export_ir_dir)\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/text_recognition/tools/test.py,4,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This script allows you to test Text Recognition model. """"""\n\nimport argparse\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nimport cv2\n\nfrom text_recognition.model import TextRecognition\nfrom text_recognition.dataset import Dataset\n\n\ndef parse_args():\n    """""" Parases input arguments. """"""\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--annotation_path\', required=True, help=\'Annotation path.\')\n    parser.add_argument(\'--weights_path\', required=True, help=\'Model weights path.\')\n    parser.add_argument(\'--show\', action=\'store_true\', help=\'Show images.\')\n\n    return parser.parse_args()\n\n\ndef main():\n    """""" Main testing funciton. """"""\n\n    args = parse_args()\n\n    sequence_length = 30\n    image_width = 120\n    image_height = 32\n\n    dataset = Dataset(args.annotation_path, image_width, image_height, repeat=1)\n    next_sample = dataset().make_one_shot_iterator().get_next()\n\n    model = TextRecognition(is_training=False, num_classes=dataset.num_classes)\n    images_ph = tf.placeholder(tf.float32, [1, image_height, image_width, 1])\n    model_out = model(inputdata=images_ph)\n    decoded, _ = tf.nn.ctc_beam_search_decoder(model_out, sequence_length * np.ones(1),\n                                               merge_repeated=False)\n\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        saver.restore(sess=sess, save_path=args.weights_path)\n\n        correct = 0.0\n        dataset_len = len(dataset)\n        for _ in tqdm(range(dataset_len)):\n            images_batch, labels_batch = sess.run(next_sample)\n\n            preds, _ = sess.run([decoded, model_out], feed_dict={images_ph: images_batch})\n\n            try:\n                predicted = Dataset.sparse_tensor_to_str(preds[0], dataset.int_to_char)[0]\n                expected = Dataset.sparse_tensor_to_str(labels_batch, dataset.int_to_char)[\n                    0].lower()\n            except:\n                print(\'Could not find a word\')\n                continue\n\n            correct += 1 if predicted == expected else 0\n\n            if args.show and predicted != expected:\n                image = np.reshape(images_batch, [image_height, image_width, -1]).astype(np.uint8)\n                cv2.imshow(\'image\', image)\n                print(\'pr, gt\', predicted, expected)\n                k = cv2.waitKey(0)\n                if k == 27:\n                    sess.close()\n                    return\n\n        print(\'accuracy\', correct / dataset_len)\n\n    return\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/text_recognition/tools/train.py,24,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This script allows you to train Text Recognition model. """"""\n\nimport argparse\nimport os\nimport time\nimport numpy as np\nimport tensorflow as tf\n\nfrom text_recognition.model import TextRecognition\nfrom text_recognition.dataset import Dataset\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--annotation_path\', type=str, required=True, help=\'Training annotation path.\')\n    parser.add_argument(\'--annotation_path_test\', type=str, required=False, default=\'\', help=\'Test annotation path.\')\n    parser.add_argument(\'--weights_path\', type=str, help=\'Pretrained model weights.\')\n    parser.add_argument(\'--reg\', action=\'store_true\', help=\'Use weights regularization.\')\n    parser.add_argument(\'--backbone_dropout\', type=float, default=0.0, help=\'Use dropout\')\n    parser.add_argument(\'--learning_rate\', type=float, default=0.1)\n    parser.add_argument(\'--num_steps\', type=int, default=1000000)\n\n    return parser.parse_args()\n\n# pylint: disable=too-many-locals,too-many-statements\ndef main():\n    """""" Main training function. """"""\n\n    args = parse_args()\n\n    seq_length = 30\n    batch_size = 64\n\n    image_width, image_height = 120, 32\n\n    handle = tf.placeholder(tf.string, shape=[])\n\n    dataset_train = Dataset(args.annotation_path, image_width, image_height, batch_size=batch_size,\n                            shuffle=True)\n\n    iterator_train = dataset_train().make_initializable_iterator()\n\n    if args.annotation_path_test != \'\':\n        dataset_test = Dataset(args.annotation_path_test, image_width, image_height,\n                               batch_size=batch_size, shuffle=False, repeat=1)\n        iterator_test = dataset_test().make_initializable_iterator()\n\n    iterator = tf.data.Iterator.from_string_handle(\n        handle, dataset_train().output_types, dataset_train().output_shapes,\n        dataset_train().output_classes)\n    next_sample = iterator.get_next()\n\n    is_training_ph = tf.placeholder(tf.bool)\n\n    model = TextRecognition(is_training=is_training_ph, num_classes=dataset_train.num_classes,\n                            backbone_dropout=args.backbone_dropout)\n    model_out = model(inputdata=next_sample[0])\n\n    ctc_loss = tf.reduce_mean(tf.nn.ctc_loss(labels=next_sample[1], inputs=model_out,\n                                             sequence_length=seq_length * np.ones(batch_size)))\n\n    reg_loss = tf.losses.get_regularization_loss()\n    loss = ctc_loss\n\n    if args.reg:\n        loss += reg_loss\n\n    decoded, _ = tf.nn.ctc_beam_search_decoder(model_out, seq_length * np.ones(batch_size),\n                                               merge_repeated=False)\n\n    edit_dist = tf.edit_distance(tf.cast(decoded[0], tf.int32), next_sample[1])\n    crw = tf.nn.zero_fraction(edit_dist)\n\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n    starter_learning_rate = args.learning_rate\n    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n                                               1000000, 0.1, staircase=True)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n    with tf.control_dependencies(update_ops):\n        optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss=loss,\n                                                                       global_step=global_step)\n\n    # Set tf summary\n    train_start_time = time.strftime(\'%Y-%m-%d-%H-%M-%S\', time.localtime(time.time()))\n\n    model_descr = str(train_start_time)\n\n    tboard_save_path = \'tboard/\' + model_descr\n\n    if not os.path.exists(tboard_save_path):\n        os.makedirs(tboard_save_path)\n    tf.summary.scalar(name=\'ctc_loss\', tensor=ctc_loss)\n    tf.summary.scalar(name=\'reg_loss\', tensor=reg_loss)\n    tf.summary.scalar(name=\'total_loss\', tensor=loss)\n    tf.summary.scalar(name=\'Learning_Rate\', tensor=learning_rate)\n    merge_summary_op = tf.summary.merge_all()\n\n    test_acc_ph = tf.placeholder(dtype=np.float32)\n    test_acc_summary = tf.summary.scalar(name=\'test_acc_ph\', tensor=test_acc_ph)\n\n    # Set saver configuration\n    saver = tf.train.Saver(max_to_keep=1000)\n    model_save_dir = \'model/\' + model_descr\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    model_name = \'model_\' + model_descr + \'.ckpt\'\n    model_save_path = os.path.join(model_save_dir, model_name)\n\n    summary_writer = tf.summary.FileWriter(tboard_save_path)\n\n    with tf.Session() as sess:\n        sess.run(iterator_train.initializer)\n        if args.weights_path is None:\n            print(\'Training from scratch\')\n            init = tf.global_variables_initializer()\n            sess.run(init)\n        else:\n            print(\'Restore model from {:s}\'.format(args.weights_path))\n            saver.restore(sess=sess, save_path=args.weights_path)\n\n        training_handle = sess.run(iterator_train.string_handle())\n        if args.annotation_path_test != \'\':\n            test_handle = sess.run(iterator_test.string_handle())\n\n        for _ in range(args.num_steps):\n            _, c, step, summary = sess.run([optimizer, ctc_loss, global_step, merge_summary_op],\n                                           feed_dict={is_training_ph: True,\n                                                      handle: training_handle})\n\n            if step % 100 == 0:\n                summary_writer.add_summary(summary=summary, global_step=step)\n                print(\'Iter: {:d} cost= {:9f}\'.format(step, c))\n\n            if step % 1000 == 0:\n                saver.save(sess=sess, save_path=model_save_path, global_step=global_step)\n\n                if args.annotation_path_test:\n                    sess.run(iterator_test.initializer)\n\n                    correct = 0.0\n                    for _ in range(len(dataset_test) // batch_size):\n                        correct += sess.run(crw,\n                                            feed_dict={is_training_ph: False, handle: test_handle})\n\n                    test_accuracy = correct / (len(dataset_test) // batch_size)\n\n                    print(\'Iter: {:d} cost= {:9f} TEST accuracy= {:9f}\'.format(step, c,\n                                                                               test_accuracy))\n\n                    summary = sess.run(test_acc_summary, feed_dict={test_acc_ph: test_accuracy})\n                    summary_writer.add_summary(summary=summary, global_step=step)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/utils/tfutils/helpers.py,4,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nfrom importlib import util\nfrom os import path, system\nimport sys\n\nimport subprocess\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\n\ndef import_research_models():\n  research_dir = path.realpath(path.join(path.dirname(__file__), \'../../../external/models/research/\'))\n  sys.path.append(research_dir)\n  sys.path.append(path.join(research_dir, \'slim\'))\n\n\ndef import_transformer():\n  transformer_dir = path.realpath(path.join(path.dirname(__file__), \'../../../external/models/research/transformer\'))\n  sys.path.append(transformer_dir)\n\n\ndef load_module(module_name):\n  # TODO: replace on\n  # __import__(module_name)\n  # return sys.modules[module_name]\n  spec = util.spec_from_file_location(""module.name"", module_name)\n  module = util.module_from_spec(spec)\n  spec.loader.exec_module(module)\n  return module\n\n\n# pylint: disable=too-many-locals\ndef draw_bboxes(val_images, annotations, predictions, classes, conf_threshold=0.5):\n  font = cv2.FONT_HERSHEY_TRIPLEX\n  font_scale = 0.6\n  font_thickness = 1\n  alpha = 0.5\n  rect_thickness = 2\n\n  images = []\n\n  for im_idx, _ in enumerate(val_images):\n    img = val_images[im_idx].copy()\n    height, width = img.shape[:2]\n\n    annotation = annotations[im_idx]\n\n    for _, bboxes in annotation.items():\n      for bbox in bboxes:\n        top_left = int(round(bbox.xmin * width)), int(round(bbox.ymin * height))\n        bottom_right = int(round(bbox.xmax * width)), int(round(bbox.ymax * height))\n        cv2.rectangle(img, top_left, bottom_right, (0, 255, 0), thickness=rect_thickness)\n\n    det_label = predictions[im_idx][:, 1]\n    det_conf = predictions[im_idx][:, 2]\n    det_xmin = predictions[im_idx][:, 3]\n    det_ymin = predictions[im_idx][:, 4]\n    det_xmax = predictions[im_idx][:, 5]\n    det_ymax = predictions[im_idx][:, 6]\n\n    order = np.argsort(np.array(det_conf))[::-1]\n\n    for bb_idx in order:\n      score = det_conf[bb_idx]\n\n      if score == 0:\n        break\n\n      if score < conf_threshold:  # last\n        color = (128, 0, 128)\n      else:\n        color = (255, 0, 0)\n\n      top_left = int(round(det_xmin[bb_idx] * width)), int(round(det_ymin[bb_idx] * height))\n      bottom_right = int(round(det_xmax[bb_idx] * width)), int(round(det_ymax[bb_idx] * height))\n\n      top_left = tuple(np.clip(top_left, (0, 0), (width - 1, height - 1)))\n      bottom_right = tuple(np.clip(bottom_right, (0, 0), (width - 1, height - 1)))\n      cv2.rectangle(img, top_left, bottom_right, color, thickness=rect_thickness)\n\n      label = classes[int(det_label[bb_idx])] if classes else int(det_label[bb_idx])\n\n      display_txt = \'{0}:{1:0.2f}\'.format(label, score)\n\n      txt_size = cv2.getTextSize(display_txt, font, font_scale, font_thickness)[0]\n\n      # Fill background for text with alpha-blending\n      text_tl = (top_left[0], top_left[1])\n      text_tl = tuple(np.clip(text_tl, (0, 0), (width - 1, height - 1)))\n      text_br = (top_left[0] + txt_size[0], top_left[1] + txt_size[1])\n      text_br = tuple(np.clip(text_br, (0, 0), (width - 1, height - 1)))\n      roi = img[text_tl[1]:text_br[1], text_tl[0]:text_br[0]].astype(np.float32)\n      roi *= alpha\n      roi += (1. - alpha) * 255\n      img[text_tl[1]:text_br[1], text_tl[0]:text_br[0]] = roi.astype(np.uint8)\n      cv2.putText(img, display_txt, (text_tl[0], text_tl[1] + txt_size[1]), font, font_scale, (0, 128, 0),\n                  font_thickness)\n\n      if score < conf_threshold:\n        break\n\n    images.append(img)\n\n  return images\n\n\ndef estimate_inputs_outputs(graph):\n  unlikely_output = [\'Const\', \'Assign\', \'NoOp\', \'Placeholder\', \'Assert\',\n                     \'switch_t\', \'switch_f\', \'IsVariableInitialized\', \'Save\', \'SaveV2\']\n  outputs = []\n  inputs = []\n  for node in graph.as_graph_def().node:\n    if node.op == \'Placeholder\':\n      inputs.append(node.name)\n\n    if node.op not in unlikely_output:\n      if node.name.split(\'/\')[-1] not in unlikely_output:\n        operation = graph.get_operation_by_name(node.name)\n\n        children_count = sum(1 for out in operation.outputs for _ in out.consumers())\n        if children_count == 0:\n          outputs.append(node.name)\n\n  return inputs, outputs\n\n\ndef dump_frozen_graph(sess, graph_file, output_node_names=None):\n  assert graph_file.endswith(\'.pb\')\n  assert output_node_names is None or isinstance(output_node_names, list)\n  output_node_names = output_node_names or estimate_inputs_outputs(sess.graph)[1]\n\n  dir_ = path.dirname(graph_file)\n  base = path.basename(graph_file)\n  ckpt = graph_file.replace(\'.pb\', \'.ckpt\')\n  frozen = graph_file.replace(\'.pb\', \'.pb.frozen\')\n\n  system(\'mkdir -p {}\'.format(dir_))\n  print(\'>> Saving `{}`... \'.format(graph_file))\n  tf.train.write_graph(sess.graph, dir_, base, as_text=False)\n  tf.train.write_graph(sess.graph, dir_, base + ""txt"", as_text=True)\n  print(\'Done\')\n\n  print(\'>> Saving `{}`... \'.format(ckpt))\n  with sess.graph.as_default():\n    saver = tf.train.Saver()\n    saver.save(sess, ckpt, write_meta_graph=False)\n  print(\'Done\')\n\n  print(\'>> Freezing graph to `{}`... \'.format(frozen))\n  print(\'Outputs:\\n  {}\'.format(\', \'.join(output_node_names)))\n\n  from tensorflow.python.tools.freeze_graph import freeze_graph\n  freeze_graph(input_graph=graph_file,\n               input_saver=\'\',\n               input_binary=True,\n               input_checkpoint=ckpt,\n               output_node_names=\',\'.join(output_node_names),\n               restore_op_name=\'save/restore_all\',\n               filename_tensor_name=\'save/Const:0\',\n               output_graph=frozen,\n               clear_devices=True,\n               initializer_nodes=\'\',\n               saved_model_tags=\'serve\')\n\n  return frozen\n\n\ndef download_archive_and_extract(url, target_dir):\n  from io import BytesIO\n  from zipfile import ZipFile\n  from urllib.request import urlopen\n  import urllib\n\n  try:\n    resp = urlopen(url)\n  except urllib.error.HTTPError as exception:\n    tf.logging.error(\'Not found: {}\'.format(url))\n    raise exception\n\n  zipfile = ZipFile(BytesIO(resp.read()))\n  zipfile.extractall(target_dir)\n\n\ndef execute_mo(config, frozen, output_dir):\n  command = [\n    \'mo.py\',\n    \'--framework=tf\',\n    \'--input_model={}\'.format(frozen),\n    \'--output_dir={}\'.format(output_dir),\n  ]\n\n  for arg, value in config.items():\n    if not value:\n      continue\n    if isinstance(value, bool):\n        command.append(\'--{}\'.format(arg))\n    elif isinstance(value, (str, int, float)):\n      command.append(\'--{}={}\'.format(arg, value))\n    elif isinstance(value, list):\n      command.append(\'--{}=[{}]\'.format(arg, \',\'.join([str(x) for x in value])))\n    else:\n      raise Exception(\'Unexpected format of value in mo_config: {}\'.format(value))\n\n  print(\'\')\n  print(\' \'.join(command))\n\n  subprocess.call(command)\n'"
tensorflow_toolkit/vehicle_attributes/cars_100/config.py,1,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\ninput_shape = (72, 72, 3)  # (height, width, channels)\nmodel_dir = \'model\'\n\nclass train:\n  batch_size = 32\n  steps = 2000000\n\n  random_seed = 666\n\n  save_checkpoints_steps = 1000      # Number of training steps when checkpoint should be saved\n  keep_checkpoint_every_n_hours = 1  # Checkpoint should be saved forever after every n hours\n  save_summary_steps = 100           # Number of steps when the summary information should be saved\n\n  num_parallel_calls = 4\n  prefetch_size = 4\n\n  annotation_path = \'../../data/cars_100/cars_100_train.json\'\n  use_pretrained_weights = True\n  pretrained_ckpt = \'vehicle-attributes-barrier-0103/model.ckpt-2000000\'\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 2\n    inter_op_parallelism_threads = 8\n    transformer_parallel_calls = 4  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 8   # Number of batches to prefetch\n\nclass eval:\n  batch_size = 32\n\n  annotation_path = \'../../data/cars_100/cars_100_test.json\'\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 2\n    inter_op_parallelism_threads = 8\n    transformer_parallel_calls = 4  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 8   # Number of batches to prefetch\n\nclass infer:\n  annotation_path = \'../../data/cars_100/cars_100_test.json\'\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""\n    intra_op_parallelism_threads = 0\n\ndef optimizer(learning_rate):\n  import tensorflow as tf\n  return tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n\nresnet_params = {\n  ""learning_rate"": 0.001,                                 # Learning rate\n  ""optimizer"": optimizer,                                 # Optimizer\n  ""pretrained_ckpt"": train.pretrained_ckpt,               # Trained model\n  ""use_pretrained_weights"": train.use_pretrained_weights  # Use pretrained model weights\n}\n'"
tensorflow_toolkit/vehicle_attributes/cars_100/config_test.py,1,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\ninput_shape = (72, 72, 3)  # (height, width, channels)\nmodel_dir = \'model\'\n\nclass train:\n  batch_size = 32\n  steps = 1000\n\n  random_seed = 666\n\n  save_checkpoints_steps = 1000      # Number of training steps when checkpoint should be saved\n  keep_checkpoint_every_n_hours = 1  # Checkpoint should be saved forever after every n hours\n  save_summary_steps = 100           # Number of steps when the summary information should be saved\n\n  num_parallel_calls = 4\n  prefetch_size = 4\n\n  annotation_path = \'../../data/cars_100/cars_100_train.json\'\n  use_pretrained_weights = True\n  pretrained_ckpt = \'vehicle-attributes-barrier-0103/model.ckpt-2000000\'\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 2\n    inter_op_parallelism_threads = 8\n    transformer_parallel_calls = 4  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 8   # Number of batches to prefetch\n\nclass eval:\n  batch_size = 32\n\n  annotation_path = \'../../data/cars_100/cars_100_test.json\'\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""\n    per_process_gpu_memory_fraction = 0.8  # Fix extra memory allocation issue\n    allow_growth = True  # Option which attempts to allocate only as much GPU memory based on runtime allocations\n\n    intra_op_parallelism_threads = 2\n    inter_op_parallelism_threads = 8\n    transformer_parallel_calls = 4  # Number of parallel threads in data transformer/augmentation\n    transformer_prefetch_size = 8   # Number of batches to prefetch\n\nclass infer:\n  annotation_path = \'../../data/cars_100/cars_100_test.json\'\n\n  class execution:\n    CUDA_VISIBLE_DEVICES = ""0""\n    intra_op_parallelism_threads = 0\n\ndef optimizer(learning_rate):\n  import tensorflow as tf\n  return tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n\nresnet_params = {\n  ""learning_rate"": 0.001,                                 # Learning rate\n  ""optimizer"": optimizer,                                 # Optimizer\n  ""pretrained_ckpt"": train.pretrained_ckpt,               # Trained model\n  ""use_pretrained_weights"": train.use_pretrained_weights  # Use pretrained model weights\n}\n'"
tensorflow_toolkit/vehicle_attributes/tools/eval.py,6,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport time\nimport sys\nimport argparse\nimport pprint\n\nimport tensorflow as tf\n\nfrom tfutils.helpers import load_module\nfrom vehicle_attributes.trainer import create_session, resnet_v1_10_1, InputEvalData\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Perform evaluation of a trained vehicle attributes model\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\ndef eval_loop(estimator, eval_data, config):\n  latest_checkpoint = None\n  wait_iters = 0\n  save_images_step = 0\n  new_checkpoint = None\n  while True:\n    while new_checkpoint is None:\n      time.sleep(1)\n      new_checkpoint = tf.train.latest_checkpoint(config.model_dir)\n    new_checkpoint = tf.train.latest_checkpoint(config.model_dir)\n    if latest_checkpoint != new_checkpoint:\n      latest_checkpoint = new_checkpoint\n\n      print(\'\\nEvaluating {0}\'.format(latest_checkpoint))\n      print(\'=============================================================\')\n\n      start = time.time()\n\n      eval_results = estimator.evaluate(input_fn=eval_data.input_fn, name=\'val\')\n      def without_keys(rdict, keys):\n        return {x: rdict[x] for x in rdict if x not in keys}\n      results = without_keys(eval_results, {""global_step"", ""loss""})\n      printer = pprint.PrettyPrinter(indent=4)\n      printer.pprint(results)\n\n      finish = time.time()\n\n      print(\'=============================================================\')\n      print(\'[{0}]: {1} evaluation time = {2}\\n\'.format(latest_checkpoint, \'val\', finish - start))\n\n      save_images_step += 1\n      wait_iters = 0\n    else:\n      if wait_iters % 12 == 0:\n        sys.stdout.write(\'\\r\')\n        for _ in range(11 + wait_iters // 12):\n          sys.stdout.write(\' \')\n        sys.stdout.write(\'\\r\')\n        for _ in range(1 + wait_iters // 12):\n          sys.stdout.write(\'|\')\n      else:\n        sys.stdout.write(\'.\')\n      sys.stdout.flush()\n      time.sleep(0.5)\n      wait_iters += 1\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n\n  session_config = create_session(cfg, \'eval\')\n\n  run_config = tf.estimator.RunConfig(session_config=session_config)\n\n  va_estimator = tf.estimator.Estimator(\n    model_fn=resnet_v1_10_1,\n    params=cfg.resnet_params,\n    model_dir=cfg.model_dir,\n    config=run_config)\n\n  eval_data = InputEvalData(batch_size=cfg.eval.batch_size,\n                            input_shape=cfg.input_shape,\n                            json_path=cfg.eval.annotation_path)\n\n  eval_loop(va_estimator, eval_data, cfg)\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/vehicle_attributes/tools/export.py,9,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport argparse\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom vehicle_attributes.networks.resnet_10_bn import resnet_v1_10, resnet_arg_scope\nfrom tfutils.helpers import load_module, execute_mo\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Export vehicle attributes model in IE format\')\n  parser.add_argument(\'--mo\', default=\'mo.py\', help=""Path to model optimizer \'mo.py\' script"")\n  parser.add_argument(\'--mo_config\', default=\'cars_100/mo.yaml\', help=""Path config for model optimizer"")\n  parser.add_argument(\'--data_type\', default=\'FP32\', choices=[\'FP32\', \'FP16\'], help=\'Data type of IR\')\n  parser.add_argument(\'--output_dir\', default=None, help=\'Output Directory\')\n  parser.add_argument(\'--checkpoint\', default=None, help=\'Default: latest\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\ndef freezing_graph(config, checkpoint, output_dir):\n  with tf.Session() as sess:\n    shape = [None] + list(config.input_shape)\n    inputs = tf.placeholder(dtype=tf.float32, shape=shape, name=""input"")\n    with slim.arg_scope(resnet_arg_scope()):\n      _ = resnet_v1_10(inputs, is_training=False)\n    saver = tf.train.Saver()\n\n    saver.restore(sess, os.path.abspath(checkpoint))\n\n    frozen = tf.graph_util.convert_variables_to_constants(sess,\n                                                          sess.graph.as_graph_def(),\n                                                          [\'resnet_v1_10/type\', \'resnet_v1_10/color\'])\n\n    tf.train.write_graph(frozen, output_dir, \'graph.pb_txt\', as_text=True)\n    frozen_path = tf.train.write_graph(frozen, output_dir, \'graph.pb.frozen\', as_text=False)\n    return frozen_path\n\ndef main(_):\n  args = parse_args()\n  config = load_module(args.path_to_config)\n\n  checkpoint = args.checkpoint if args.checkpoint else tf.train.latest_checkpoint(config.model_dir)\n  if not checkpoint or not os.path.isfile(checkpoint+\'.index\'):\n    raise FileNotFoundError(str(checkpoint))\n\n  step = checkpoint.split(\'.\')[-1].split(\'-\')[-1]\n  output_dir = args.output_dir if args.output_dir else os.path.join(config.model_dir, \'export_{}\'.format(step))\n\n  # Freezing graph\n  frozen_dir = os.path.join(output_dir, \'frozen_graph\')\n  frozen_graph = freezing_graph(config, checkpoint, frozen_dir)\n\n  # Export to IR\n  export_dir = os.path.join(output_dir, \'IR\', args.data_type)\n\n  mo_params = {\n    \'model_name\': \'vehicle_attributes\',\n    \'scale\': 255,\n    \'input_shape\': [1] + list(config.input_shape),\n    \'data_type\': args.data_type,\n  }\n  execute_mo(mo_params, frozen_graph, export_dir)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/vehicle_attributes/tools/infer.py,5,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\nCLASS_MAP = {\n  0: \'car\',\n  1: \'bus\',\n  2: \'track\',\n  3: \'van\',\n}\n\ndef load_graph(frozen_graph_filename):\n  with tf.gfile.GFile(frozen_graph_filename, \'rb\') as file:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(file.read())\n  with tf.Graph().as_default() as graph:\n    tf.import_graph_def(graph_def)\n  return graph\n\ndef normalized_to_absolute(prediction):\n  colorcar = np.zeros((1, 1, 3), dtype=np.uint8)\n  for i in range(3):\n    if prediction[i] < 0:\n      colorcar[0, 0, i] = 0\n    elif prediction[i] > 1:\n      colorcar[0, 0, i] = 255\n    else:\n      colorcar[0, 0, i] = prediction[i]*255\n  return colorcar\n\ndef build_argparser():\n  parser = ArgumentParser()\n  parser.add_argument(\'--model\', \'-m\', help=\'Path to frozen graph file with a trained model.\', required=True, type=str)\n  parser.add_argument(\'--output\', \'-o\', help=\'Output image\')\n  parser.add_argument(\'input_image\', help=\'Image with license plate\')\n  return parser\n\n\ndef main():\n  args = build_argparser().parse_args()\n\n  graph = load_graph(args.model)\n\n  image = cv2.imread(args.input_image)\n  img = cv2.resize(image, (72, 72))\n  img = np.float32(img)\n  img = np.multiply(img, 1.0/255.0)\n\n  input = graph.get_tensor_by_name(""import/input:0"")\n  output = [\n    graph.get_tensor_by_name(""import/resnet_v1_10/type:0""),\n    graph.get_tensor_by_name(""import/resnet_v1_10/color:0"")\n  ]\n\n  with tf.Session(graph=graph) as sess:\n    results = sess.run(output, feed_dict={input: [img]})\n\n    vtype = CLASS_MAP.get(np.argmax(results[0][0]), \'undefined\')\n    colorcar = normalized_to_absolute(results[1][0])\n    rgb_color = cv2.cvtColor(colorcar, cv2.COLOR_LAB2BGR)[0, 0].tolist()\n\n    print(""Type: %s"" % vtype)\n    print(""Color: %s"" % rgb_color)\n\n    cv2.rectangle(image, (0, 0), (30, 30), rgb_color, -1)\n    font = cv2.FONT_HERSHEY_PLAIN\n    cv2.putText(image, vtype, (0, 15), font, 1, (0, 0, 0), 2, cv2.LINE_AA)\n\n    if args.output:\n      cv2.imwrite(args.output, image)\n    else:\n      cv2.imshow(\'Vehicle_attributes\', image)\n      cv2.waitKey(0)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
tensorflow_toolkit/vehicle_attributes/tools/infer_checkpoint.py,5,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport random\nimport json\nimport argparse\nimport numpy as np\n\nimport cv2\nimport tensorflow as tf\n\nfrom colormath.color_diff import delta_e_cie1976\nfrom colormath.color_objects import LabColor\n\nfrom tfutils.helpers import load_module\nfrom vehicle_attributes.trainer import create_session, resnet_v1_10_1\nfrom vehicle_attributes.readers.vehicle_attributes_json import BarrierAttributesJson\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Perform inference of vehicle attributes model\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\ndef normalized_to_absolute(prediction):\n  colorcar = np.zeros((1, 1, 3), dtype=np.uint8)\n  for i in range(3):\n    if prediction[i] < 0:\n      colorcar[0, 0, i] = 0\n    elif prediction[i] > 1:\n      colorcar[0, 0, i] = 255\n    else:\n      colorcar[0, 0, i] = prediction[i]*255\n  return colorcar\n\n# pylint: disable=too-many-locals, too-many-statements, invalid-name, too-many-boolean-expressions, len-as-condition\ndef infer(config):\n  session_config = create_session(config, \'infer\')\n\n  run_config = tf.estimator.RunConfig(session_config=session_config)\n\n  va_estimator = tf.estimator.Estimator(\n    model_fn=resnet_v1_10_1,\n    params=config.resnet_params,\n    model_dir=config.model_dir,\n    config=run_config)\n\n  with open(config.infer.annotation_path) as f:\n    data = json.load(f)\n    pic = 0\n    summ = 0\n    random.seed(666)\n    for _ in range(len(data)):\n      pic = random.randint(0, len(data)-1)\n      images, annotations = BarrierAttributesJson.get_annotations(data[pic])\n      if len(images) == 0:\n        pic += 1\n        continue\n      print(""pic = "", pic)\n      predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x=np.array([images], dtype=np.float32).reshape([-1] + list(config.input_shape)),\n        num_epochs=1,\n        shuffle=False)\n      predict = va_estimator.predict(input_fn=predict_input_fn)\n\n      img = cv2.imread(data[pic][\'image\'], -1)\n      cv2.namedWindow(""example"")\n\n      cars = []\n\n      for y in range(len(data[pic][\'objects\'])):\n        if data[pic][\'objects\'][y][\'label\'] == \'vehicle\' and \'bbox\' in data[pic][\'objects\'][y] and \\\n          len(data[pic][\'objects\'][y][\'bbox\']) != 0 and \'color_bbox\' in data[pic][\'objects\'][y][\'attributes\'] and \\\n            len(data[pic][\'objects\'][y][\'attributes\'][\'color_bbox\']) != 0 and \\\n              \'type\' in data[pic][\'objects\'][y][\'attributes\']:\n          cars.append(y)\n      it = 0\n      summ_temp = 0\n      for i in predict:\n        colorcar = normalized_to_absolute(i[\'color_lab\'])\n        n = cars[it]\n\n        bbox_car = data[pic][\'objects\'][n][\'bbox\']\n        color_detected = LabColor(colorcar[0][0][0], colorcar[0][0][1], colorcar[0][0][2])\n        colorcar_rgb = cv2.cvtColor(colorcar, cv2.COLOR_LAB2BGR)[0, 0].tolist()\n        cv2.rectangle(img, (int(bbox_car[0]), int(bbox_car[1])), (int(bbox_car[2]), int(bbox_car[3])),\n                      colorcar_rgb,\n                      thickness=5)\n\n        l2diss = 10000\n        tempy = 0\n\n        for j, item in enumerate(annotations):\n          colorcar_given = normalized_to_absolute(item[4:7])\n          color_given = LabColor(colorcar_given[0][0][0], colorcar_given[0][0][1], colorcar_given[0][0][2])\n          l2diss_temp = delta_e_cie1976(color_given, color_detected)\n          if l2diss_temp <= l2diss:\n            l2diss = l2diss_temp\n            tempy = j\n        colorcar_given = normalized_to_absolute(annotations[tempy][4:7])\n        colorcar_given_rgb = cv2.cvtColor(colorcar_given, cv2.COLOR_LAB2BGR)[0, 0].tolist()\n        color_given = LabColor(colorcar_given[0][0][0], colorcar_given[0][0][1], colorcar_given[0][0][2])\n        l2diss = delta_e_cie1976(color_given, color_detected)\n\n        vtype = BarrierAttributesJson.one_hot_annotation_to_type(i[\'types_class\'])\n        gttype = data[pic][\'objects\'][n][\'attributes\'][\'type\']\n        if gttype in (\'suv\', \'mpv\', \'other\'):\n          gttype = \'car\'\n        if gttype == \'pickup\':\n          gttype = \'truck\'\n\n        overlay = img.copy()\n        cv2.rectangle(img, (0, 0 + 60 * it), (120, 60 + 60 * it), (255, 255, 255), -1)\n        cv2.addWeighted(overlay, 0.3, img, 1.0 - 0.3, 0.0, img)\n        cv2.rectangle(img, (5, 5 + 60 * it), (15, 15 + 60 * it), colorcar_rgb, -1)\n        cv2.rectangle(img, (5, 25 + 60 * it), (15, 35 + 60 * it), colorcar_given_rgb, -1)\n        font = cv2.FONT_HERSHEY_PLAIN\n        cv2.putText(img, vtype, (35, 15 + 60 * it), font, 1, (0, 0, 0), 2, cv2.LINE_AA)\n        cv2.putText(img, gttype + "" gt"", (35, 35 + 60 * it), font, 1, (0, 0, 0), 2, cv2.LINE_AA)\n        it += 1\n        summ_temp += l2diss\n      summ_temp /= it\n      summ += summ_temp\n      pic += 1\n      cv2.imshow(""example"", img)\n      press = cv2.waitKey(0)\n      if press == 27:\n        break\n  summ /= len(data)\n  print(summ)\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n  infer(cfg)\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/vehicle_attributes/tools/infer_ie.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport sys\nimport os\nfrom argparse import ArgumentParser\nimport logging as log\nimport numpy as np\nimport cv2\n\nfrom openvino.inference_engine import IENetwork, IEPlugin\n\nCLASS_MAP = {\n  0: \'car\',\n  1: \'bus\',\n  2: \'track\',\n  3: \'van\',\n}\n\ndef normalized_to_absolute(prediction):\n  colorcar = np.zeros((1, 1, 3), dtype=np.uint8)\n  for i in range(3):\n    if prediction[i] < 0:\n      colorcar[0, 0, i] = 0\n    elif prediction[i] > 1:\n      colorcar[0, 0, i] = 255\n    else:\n      colorcar[0, 0, i] = prediction[i]*255\n  return colorcar\n\ndef build_argparser():\n  parser = ArgumentParser()\n  parser.add_argument(\'-m\', \'--model\', help=\'Path to an .xml file with a trained model.\', required=True, type=str)\n  parser.add_argument(\'-l\', \'--cpu_extension\',\n                      help=\'MKLDNN (CPU)-targeted custom layers. \\\n                        Absolute path to a shared library with the kernels implementation\',\n                      type=str, default=None)\n  parser.add_argument(\'-pp\', \'--plugin_dir\', help=\'Path to a plugin folder\', type=str, default=None)\n  parser.add_argument(\'-d\', \'--device\',\n                      help=\'Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. Sample \'\n                           \'will look for a suitable plugin for device specified (CPU by default)\', default=\'CPU\',\n                      type=str)\n  parser.add_argument(\'--output\', \'-o\', help=\'Output image\')\n  parser.add_argument(\'input_image\', help=\'Image with a vehicle\')\n  return parser\n\ndef load_ir_model(model_xml, device, plugin_dir, cpu_extension):\n  model_bin = os.path.splitext(model_xml)[0] + "".bin""\n\n  # initialize plugin\n  log.info(""Initializing plugin for %s device..."", device)\n  plugin = IEPlugin(device=device, plugin_dirs=plugin_dir)\n  if cpu_extension and \'CPU\' in device:\n    plugin.add_cpu_extension(cpu_extension)\n\n  # read IR\n  net = IENetwork(model=model_xml, weights=model_bin)\n\n  if ""CPU"" in device:\n    supported_layers = plugin.get_supported_layers(net)\n    not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n    if not_supported_layers:\n      log.error(""Following layers are not supported by the plugin for specified device %s:\\n %s"",\n                plugin.device, \', \'.join(not_supported_layers))\n      log.error(""Please try to specify cpu extensions library path in sample\'s command line parameters using ""\n                ""--cpu_extension command line argument"")\n      sys.exit(1)\n\n  input_blob = next(iter(net.inputs))\n  out_blob = next(iter(net.outputs))\n  exec_net = plugin.load(network=net, num_requests=2)\n  shape = net.inputs[input_blob].shape  # pylint: disable=E1136\n  del net\n\n  return exec_net, plugin, input_blob, out_blob, shape\n\ndef main():\n  log.basicConfig(format=""[ %(levelname)s ] %(message)s"", level=log.INFO, stream=sys.stdout)\n  args = build_argparser().parse_args()\n\n  exec_net, _, input_blob, _, shape = load_ir_model(args.model, args.device, args.plugin_dir, args.cpu_extension)\n  net_height, net_width = shape[2:4]\n\n  frame = cv2.imread(args.input_image)\n  in_frame = cv2.resize(frame, (net_height, net_width), interpolation=cv2.INTER_CUBIC)\n  in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n\n  res = exec_net.infer({input_blob: in_frame})\n  vtype = CLASS_MAP.get(np.argmax(res[\'resnet_v1_10/type\'][0]), \'undefined\')\n\n  colorcar = normalized_to_absolute(res[\'resnet_v1_10/color\'][0])\n  rgb_color = cv2.cvtColor(colorcar, cv2.COLOR_LAB2BGR)[0, 0].tolist()\n\n  print(""Type: %s"" % vtype)\n  print(""Color: %s"" % rgb_color)\n\n  cv2.rectangle(frame, (0, 0), (30, 30), rgb_color, -1)\n  font = cv2.FONT_HERSHEY_PLAIN\n  cv2.putText(frame, vtype, (0, 15), font, 1, (0, 0, 0), 2, cv2.LINE_AA)\n\n  if args.output:\n    cv2.imwrite(args.output, frame)\n  else:\n    cv2.imshow(\'Vehicle_attributes\', frame)\n    cv2.waitKey(0)\n\n\nif __name__ == \'__main__\':\n  sys.exit(main() or 0)\n'"
tensorflow_toolkit/vehicle_attributes/tools/train.py,4,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport argparse\nimport cv2\nimport tensorflow as tf\n\nfrom vehicle_attributes.trainer import create_session, resnet_v1_10_1, InputTrainData\nfrom tfutils.helpers import load_module\n\ndef parse_args():\n  parser = argparse.ArgumentParser(description=\'Perform training of vehicle attributes model\')\n  parser.add_argument(\'path_to_config\', help=\'Path to a config.py\')\n  return parser.parse_args()\n\ndef train(config):\n  cv2.setNumThreads(1)\n\n  session_config = create_session(config, \'train\')\n\n  run_config = tf.estimator.RunConfig(session_config=session_config,\n                                      keep_checkpoint_every_n_hours=config.train.keep_checkpoint_every_n_hours,\n                                      save_summary_steps=config.train.save_summary_steps,\n                                      save_checkpoints_steps=config.train.save_checkpoints_steps,\n                                      tf_random_seed=config.train.random_seed)\n\n  va_predictor = tf.estimator.Estimator(\n    model_fn=resnet_v1_10_1,\n    params=config.resnet_params,\n    model_dir=config.model_dir,\n    config=run_config)\n\n  input_data = InputTrainData(batch_size=config.train.batch_size,\n                              input_shape=config.input_shape,\n                              json_path=config.train.annotation_path)\n\n  va_predictor.train(\n    input_fn=input_data.input_fn,\n    steps=config.train.steps,\n    hooks=[])\n\ndef main(_):\n  args = parse_args()\n  cfg = load_module(args.path_to_config)\n  train(cfg)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
tensorflow_toolkit/vehicle_attributes/vehicle_attributes/__init__.py,0,b'from tfutils.helpers import import_research_models\n\nimport_research_models()\n'
tensorflow_toolkit/vehicle_attributes/vehicle_attributes/trainer.py,35,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\nimport random\nimport numpy as np\n\nimport tensorflow as tf\n\nimport tensorflow.contrib.slim as slim\n\nfrom vehicle_attributes.readers.vehicle_attributes_json import BarrierAttributesJson\nfrom vehicle_attributes.networks.resnet_10_bn import resnet_v1_10, resnet_arg_scope\nfrom vehicle_attributes.utils import get_checkpoint_variable_names\n\ndef set_initial_weights(init_ckpt=None):\n  if init_ckpt:\n    variables = slim.get_variables_to_restore()\n    varnames = get_checkpoint_variable_names(init_ckpt)\n    vars_to_restore = {v.name[:-2]: v for v in variables if v.name[:-2] in varnames}\n\n    tf.train.init_from_checkpoint(init_ckpt, vars_to_restore)\n\ndef create_session(config, type):\n  if type == \'train\':\n    random_seed = config.train.random_seed\n  else:\n    random_seed = 666\n\n  np.random.seed(random_seed)\n  tf.set_random_seed(random_seed)\n  random.seed(random_seed)\n\n  config_type = getattr(config, type).execution\n\n  if hasattr(config_type, \'CUDA_VISIBLE_DEVICES\'):\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\n    os.environ[""CUDA_VISIBLE_DEVICES""] = config_type.CUDA_VISIBLE_DEVICES\n\n  intra_op_parallelism_threads = config_type.intra_op_parallelism_threads if \\\n    hasattr(config_type, \'intra_op_parallelism_threads\') else 0\n  inter_op_parallelism_threads = config_type.inter_op_parallelism_threads if \\\n    hasattr(config_type, \'inter_op_parallelism_threads\') else 0\n  session_config = tf.ConfigProto(allow_soft_placement=True,\n                                  intra_op_parallelism_threads=intra_op_parallelism_threads,\n                                  inter_op_parallelism_threads=inter_op_parallelism_threads)\n  if hasattr(config_type, \'per_process_gpu_memory_fraction\'):\n    session_config.gpu_options.per_process_gpu_memory_fraction = config_type.per_process_gpu_memory_fraction\n  if hasattr(config_type, \'allow_growth\'):\n    session_config.gpu_options.allow_growth = config_type.allow_growth\n\n  return session_config\n\n# pylint: disable=too-many-locals\ndef resnet_v1_10_1(features,\n                   labels,\n                   mode,\n                   params):\n\n  learning_rate = params.get(\'learning_rate\', 0.001)\n  optimizer_func = params.get(\'optimizer\', lambda learning_rate: tf.train.AdagradOptimizer(learning_rate=learning_rate))\n  use_pretrained_weights = params[\'use_pretrained_weights\']\n  init_ckpt = params[\'pretrained_ckpt\']\n  inputs = features\n  is_training = bool(mode == tf.estimator.ModeKeys.TRAIN)\n  with slim.arg_scope(resnet_arg_scope()):\n    backbone = resnet_v1_10(inputs,\n                            is_training=is_training,\n                            scope=\'resnet_v1_10\')\n\n  def type_to_one_hot(res_type):\n    softmax = tf.nn.softmax(res_type)\n    mask = tf.greater_equal(softmax, softmax[tf.argmax(softmax)])\n    return tf.cast(mask, dtype=tf.int32)\n\n  predictions = {\n    ""types"": backbone[1][\'predictions_types\'],\n    ""color_lab"": backbone[1][\'predictions_color\'],\n    ""types_class"": tf.map_fn(type_to_one_hot, backbone[1][\'predictions_types\'], dtype=tf.int32)\n  }\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  assert mode in(tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL)\n\n  loss_color = tf.losses.mean_squared_error(labels=labels[:, 4:7], predictions=predictions[\'color_lab\'])/2.\n  loss_type = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(labels[:, 0:4], 1),\n                                                                            logits=predictions[\'types\']))\n  loss = tf.cond(tf.train.get_global_step() < 100000,\n                 lambda: loss_color + 0.00001 *loss_type,\n                 lambda: loss_color + 0.1 * loss_type)\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    if use_pretrained_weights:\n      set_initial_weights(init_ckpt)\n\n    optimizer = optimizer_func(learning_rate=learning_rate)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = slim.learning.create_train_op(total_loss=loss,\n                                               optimizer=optimizer,\n                                               global_step=tf.train.get_global_step())\n    tf.summary.scalar(\'learning_rate\', learning_rate)\n    tf.summary.scalar(\'loss_color\', loss_color)\n    tf.summary.scalar(\'loss_type\', loss_type)\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n  eval_metric_ops = {\n    ""Mean absolute error of l color component"": tf.metrics.mean_absolute_error(labels=labels[:, 4]*255, \\\n      predictions=predictions[""color_lab""][:, 0]*255),\n    ""Mean absolute error of a color component"": tf.metrics.mean_absolute_error(labels=labels[:, 5]*255, \\\n      predictions=predictions[""color_lab""][:, 1]*255),\n    ""Mean absolute error of b color component"": tf.metrics.mean_absolute_error(labels=labels[:, 6]*255, \\\n      predictions=predictions[""color_lab""][:, 2]*255),\n    ""Color mean absolute error"": tf.metrics.mean_absolute_error(labels=labels[:, 4:7]*255,\n                                                                predictions=predictions[""color_lab""]*255),\n    ""Type accuracy - average"": tf.metrics.accuracy(labels=labels[:, 0:4], predictions=predictions[\'types_class\']),\n    ""Type accuracy - car"": tf.metrics.accuracy(labels=labels[:, 0], predictions=predictions[\'types_class\'][:, 0]),\n    ""Type accuracy - bus"": tf.metrics.accuracy(labels=labels[:, 1], predictions=predictions[\'types_class\'][:, 1]),\n    ""type accuracy - truck"": tf.metrics.accuracy(labels=labels[:, 2], predictions=predictions[\'types_class\'][:, 2]),\n    ""Type accuracy - van"": tf.metrics.accuracy(labels=labels[:, 3], predictions=predictions[\'types_class\'][:, 3])\n  }\n\n  return tf.estimator.EstimatorSpec(\n    mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\nclass InputTrainData:\n  # pylint: disable=too-many-arguments\n  def __init__(self, batch_size, input_shape, json_path, cache_type=\'NONE\',\n               num_parallel_calls=4, prefetch_size=4):\n    self.batch_size = batch_size\n    self.input_shape = input_shape\n    self.json_path = json_path\n    self.cache_type = cache_type\n    self.num_parallel_calls = num_parallel_calls\n    self.prefetch_size = prefetch_size\n\n    dataset_size = BarrierAttributesJson.init_cache(json_path, cache_type)\n    self.dataset_size = dataset_size\n\n  # pylint: disable=unnecessary-lambda\n  def input_fn(self):\n    train_dataset = BarrierAttributesJson.create_dataset(self.dataset_size)\n\n    transform_fn = lambda value: BarrierAttributesJson.transform_fn(value)\n\n    map_fn = lambda value: tf.py_func(transform_fn, [value], (tf.float32, tf.float32))\n\n    dataset = train_dataset.shuffle(buffer_size=self.dataset_size, reshuffle_each_iteration=True)\n    dataset = dataset.repeat().map(map_fn, num_parallel_calls=self.num_parallel_calls)\n    dataset = dataset.batch(self.batch_size).prefetch(self.prefetch_size)\n\n    images, annotation = dataset.make_one_shot_iterator().get_next()\n\n    images.set_shape([None] + list(self.input_shape))\n    annotation.set_shape([None, 7])\n\n    return images, annotation\n\nclass InputEvalData:\n  # pylint: disable=too-many-arguments\n  def __init__(self, batch_size, input_shape, json_path, cache_type=\'NONE\',\n               num_parallel_calls=4, prefetch_size=4):\n    self.batch_size = batch_size\n    self.input_shape = input_shape\n    self.json_path = json_path\n    self.cache_type = cache_type\n    self.num_parallel_calls = num_parallel_calls\n    self.prefetch_size = prefetch_size\n\n    dataset_size = BarrierAttributesJson.init_cache(json_path, cache_type)\n    self.dataset_size = dataset_size\n\n  # pylint: disable=unnecessary-lambda\n  def input_fn(self):\n    infer_dataset = BarrierAttributesJson.create_dataset(self.dataset_size)\n\n    transform_fn = lambda value: BarrierAttributesJson.transform_fn(value)\n\n    map_fn = lambda value: tf.py_func(transform_fn, [value], (tf.float32, tf.float32))\n\n    dataset = infer_dataset.map(map_fn, num_parallel_calls=self.num_parallel_calls)\n    dataset = dataset.batch(self.batch_size).prefetch(self.prefetch_size)\n\n    images, annotation = dataset.make_one_shot_iterator().get_next()\n\n    images.set_shape([None] + list(self.input_shape))\n    annotation.set_shape([None, 7])\n\n    return images, annotation\n'"
tensorflow_toolkit/vehicle_attributes/vehicle_attributes/utils.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\ndef get_checkpoint_variable_names(ckpt):\n  from tensorflow.python import pywrap_tensorflow\n  reader = pywrap_tensorflow.NewCheckpointReader(ckpt)\n  vars_dict = reader.get_variable_to_shape_map()\n  return [v for v in vars_dict.keys()]\n'"
pytorch_toolkit/action_recognition/action_recognition/models/__init__.py,0,b''
pytorch_toolkit/action_recognition/action_recognition/models/densenet_3d.py,0,"b'import math\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom action_recognition.utils import get_fine_tuning_parameters\n\n__all__ = [\n    \'DenseNet\', \'densenet121\', \'densenet169\', \'densenet201\', \'densenet264\'\n]\n\n\ndef densenet121(**kwargs):\n    model = DenseNet(\n        num_init_features=64,\n        growth_rate=32,\n        block_config=(6, 12, 24, 16),\n        **kwargs)\n    return model\n\n\ndef densenet169(**kwargs):\n    model = DenseNet(\n        num_init_features=64,\n        growth_rate=32,\n        block_config=(6, 12, 32, 32),\n        **kwargs)\n    return model\n\n\ndef densenet201(**kwargs):\n    model = DenseNet(\n        num_init_features=64,\n        growth_rate=32,\n        block_config=(6, 12, 48, 32),\n        **kwargs)\n    return model\n\n\ndef densenet264(**kwargs):\n    model = DenseNet(\n        num_init_features=64,\n        growth_rate=32,\n        block_config=(6, 12, 64, 48),\n        **kwargs)\n    return model\n\n\nclass SqBn(nn.BatchNorm2d):\n    def forward(self, input):\n        return super().forward(input.squeeze(2))\n\n\nclass _DenseLayer(nn.Sequential):\n\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm.1\', nn.BatchNorm3d(num_input_features))\n        self.add_module(\'relu.1\', nn.ReLU(inplace=True))\n        self.add_module(\'conv.1\',\n                        nn.Conv3d(\n                            num_input_features,\n                            bn_size * growth_rate,\n                            kernel_size=1,\n                            stride=1,\n                            bias=False))\n        self.add_module(\'norm.2\', nn.BatchNorm3d(bn_size * growth_rate))\n        self.add_module(\'relu.2\', nn.ReLU(inplace=True))\n        self.add_module(\'conv.2\',\n                        nn.Conv3d(\n                            bn_size * growth_rate,\n                            growth_rate,\n                            kernel_size=3,\n                            stride=1,\n                            padding=1,\n                            bias=False))\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(\n                new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate,\n                 drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate,\n                                growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm3d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\',\n                        nn.Conv3d(\n                            num_input_features,\n                            num_output_features,\n                            kernel_size=1,\n                            stride=1,\n                            bias=False))\n        self.add_module(\'pool\', nn.AvgPool3d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    """"""Densenet-BC model class\n    Args:\n        growth_rate (int) - how many filters to add each layer (k in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n\n    def __init__(self,\n                 sample_size,\n                 sample_duration,\n                 growth_rate=32,\n                 block_config=(6, 12, 24, 16),\n                 num_init_features=64,\n                 bn_size=4,\n                 drop_rate=0,\n                 num_classes=1000):\n\n        super(DenseNet, self).__init__()\n\n        self.sample_size = sample_size\n        self.sample_duration = sample_duration\n\n        # First convolution\n        self.features = nn.Sequential(\n            OrderedDict([\n                (\'conv0\',\n                 nn.Conv3d(\n                     3,\n                     num_init_features,\n                     kernel_size=7,\n                     stride=(1, 2, 2),\n                     padding=(3, 3, 3),\n                     bias=False)),\n                (\'norm0\', nn.BatchNorm3d(num_init_features)),\n                (\'relu0\', nn.ReLU(inplace=True)),\n                (\'pool0\', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)),\n            ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(\n                    num_input_features=num_features,\n                    num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', SqBn(num_features))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        last_duration = int(math.ceil(self.sample_duration / 16))\n        last_size = int(math.floor(self.sample_size / 32))\n        out = F.avg_pool2d(\n            out, kernel_size=(last_size, last_size)).view(\n            features.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/inception_i3d.py,0,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom ..utils import get_fine_tuning_parameters\n\n\ndef calc_same_padding(kernel_shape, stride=None):\n    if stride is None:\n        stride = (1,) * len(kernel_shape)\n    return [(ks - 1) // 2 for ks, st in zip(kernel_shape, stride)]\n\n\ndef pad_same(input, kernel_size, stride=(1, 1, 1), dilation=(1, 1, 1), value=0):\n    t_left, t_right = get_pad_value(input.size(2), kernel_size[0], stride[0], dilation[0])\n    rows_left, rows_right = get_pad_value(input.size(3), kernel_size[1], stride[1], dilation[1])\n    cols_left, cols_right = get_pad_value(input.size(4), kernel_size[2], stride[2], dilation[2])\n\n    input = F.pad(input, (cols_left, cols_right, rows_left, rows_right, t_left, t_right), value=value)\n    return input\n\n\ndef get_pad_value(input_size, filter_size, stride, dilation):\n    effective_filter_size = (filter_size - 1) * dilation + 1\n    out_size = (input_size + stride - 1) // stride\n    padding_needed = max(0, (out_size - 1) * stride + effective_filter_size - input_size)\n\n    padding_left = padding_needed // 2\n    padding_right = (padding_needed - 1) // 2 + 1\n    return padding_left, padding_right\n\n\nclass Unit3d(nn.Module):\n\n    def __init__(self, input_channels, output_channels, kernel_shape=(1, 1, 1), stride=(1, 1, 1), use_batch_norm=True,\n                 use_bias=False, use_relu=True, padding_valid=True):\n        super().__init__()\n\n        self.conv_3d = nn.Conv3d(input_channels, output_channels, kernel_size=kernel_shape, stride=stride,\n                                 padding=calc_same_padding(kernel_shape, stride) if not padding_valid else 0,\n                                 bias=use_bias)\n\n        if use_batch_norm:\n            self.batch_norm = nn.BatchNorm3d(output_channels)\n\n            # self.batch_norm.weight.data.ones_()\n        if use_relu:\n            self.relu = nn.ReLU()\n\n        self.use_batch_norm = use_batch_norm\n        self.use_relu = use_relu\n        self.padding_valid = padding_valid\n\n    def forward(self, x):\n        x = self.conv_3d(pad_same(x, self.conv_3d.kernel_size, self.conv_3d.stride) if self.padding_valid else x)\n        first_conv = x\n        if self.use_batch_norm:\n            x = self.batch_norm(x)\n        if self.use_relu:\n            x = self.relu(x)\n\n        return x\n\n\nclass InceptionBlock(nn.Module):\n    def __init__(self, input_channels, branch_channels):\n        super().__init__()\n\n        self.Branch_0_Conv3d_0a_1x1 = Unit3d(input_channels, branch_channels[0], kernel_shape=(1, 1, 1))\n\n        self.Branch_1_Conv3d_0a_1x1 = Unit3d(input_channels, branch_channels[1], kernel_shape=(1, 1, 1))\n        self.Branch_1_Conv3d_0b_3x3 = Unit3d(branch_channels[1], branch_channels[2], kernel_shape=(3, 3, 3))\n\n        self.Branch_2_Conv3d_0a_1x1 = Unit3d(input_channels, branch_channels[3], kernel_shape=(1, 1, 1))\n        self.Branch_2_Conv3d_0b_3x3 = Unit3d(branch_channels[3], branch_channels[4], kernel_shape=(3, 3, 3))\n\n        self.Branch_3_MaxPool3d_0a_3x3 = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(1, 1, 1),\n                                                      padding=0)\n        self.Branch_3_Conv3d_0b_1x1 = Unit3d(input_channels, branch_channels[5], kernel_shape=(1, 1, 1))\n\n    def forward(self, x, endpoint=False):\n        branch0 = self.Branch_0_Conv3d_0a_1x1(x)\n\n        branch1 = self.Branch_1_Conv3d_0a_1x1(x)\n        branch1 = self.Branch_1_Conv3d_0b_3x3(branch1)\n\n        branch2 = self.Branch_2_Conv3d_0a_1x1(x)\n        branch2 = self.Branch_2_Conv3d_0b_3x3(branch2)\n\n        branch3 = self.Branch_3_MaxPool3d_0a_3x3(\n            pad_same(x, self.Branch_3_MaxPool3d_0a_3x3.kernel_size, self.Branch_3_MaxPool3d_0a_3x3.stride, value=-999))\n        branch3 = self.Branch_3_Conv3d_0b_1x1(branch3)\n        inner_endpoint = branch3\n\n        if not endpoint:\n            return torch.cat((branch0, branch1, branch2, branch3), dim=1)\n        else:\n            return torch.cat((branch0, branch1, branch2, branch3), dim=1), inner_endpoint\n\n\nclass InceptionI3D(nn.Module):\n    def __init__(self, input_channels=3, num_classes=400, pretrain=False, dropout_rate=0.):\n        super().__init__()\n\n        assert pretrain is False, ""Pretrain is not implemented""\n\n        self.Conv3d_1a_7x7 = Unit3d(input_channels, 64, kernel_shape=(7, 7, 7), stride=(2, 2, 2), padding_valid=True)\n\n        self.MaxPool3d_2a_3x3 = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2),\n                                             padding=0)\n        self.Conv3d_2b_1x1 = Unit3d(64, 64, kernel_shape=(1, 1, 1))\n        self.Conv3d_2c_3x3 = Unit3d(64, 192, kernel_shape=(3, 3, 3))\n\n        self.MaxPool3d_3a_3x3 = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2),\n                                             padding=0)\n        self.Mixed_3b = InceptionBlock(192, [64, 96, 128, 16, 32, 32])\n        self.Mixed_3c = InceptionBlock(256, [128, 128, 192, 32, 96, 64])\n\n        self.MaxPool3d_4a_3x3 = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(2, 2, 2),\n                                             padding=0)\n        self.Mixed_4b = InceptionBlock(480, [192, 96, 208, 16, 48, 64])\n        self.Mixed_4c = InceptionBlock(512, [160, 112, 224, 24, 64, 64])\n        self.Mixed_4d = InceptionBlock(512, [128, 128, 256, 24, 64, 64])\n        self.Mixed_4e = InceptionBlock(512, [112, 144, 288, 32, 64, 64])\n        self.Mixed_4f = InceptionBlock(528, [256, 160, 320, 32, 128, 128])\n\n        self.MaxPool3d_5a_2x2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2),\n                                             padding=0)\n        self.Mixed_5b = InceptionBlock(832, [256, 160, 320, 32, 128, 128])\n        self.Mixed_5c = InceptionBlock(832, [384, 192, 384, 48, 128, 128])\n\n        self.avg_pool = nn.AvgPool3d(kernel_size=(2, 7, 7), stride=(1, 1, 1))\n        self.dropout = nn.Dropout3d(dropout_rate)\n        self.logits = Unit3d(1024, num_classes, kernel_shape=(1, 1, 1), use_batch_norm=False, use_relu=False,\n                             use_bias=True)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)\n\n        x = self.Conv3d_1a_7x7(x)\n\n        x = self.MaxPool3d_2a_3x3(pad_same(x, self.MaxPool3d_2a_3x3.kernel_size, self.MaxPool3d_2a_3x3.stride))\n        x = self.Conv3d_2b_1x1(x)\n        x = self.Conv3d_2c_3x3(x)\n\n        x = self.MaxPool3d_3a_3x3(pad_same(x, self.MaxPool3d_3a_3x3.kernel_size, self.MaxPool3d_3a_3x3.stride))\n        x = self.Mixed_3b(x)\n        x = self.Mixed_3c(x)\n\n        x = self.MaxPool3d_4a_3x3(pad_same(x, self.MaxPool3d_4a_3x3.kernel_size, self.MaxPool3d_4a_3x3.stride))\n        x = self.Mixed_4b(x)\n        x = self.Mixed_4c(x)\n        x = self.Mixed_4d(x)\n        x = self.Mixed_4e(x)\n        x = self.Mixed_4f(x)\n\n        x = self.MaxPool3d_5a_2x2(pad_same(x, self.MaxPool3d_5a_2x2.kernel_size, self.MaxPool3d_5a_2x2.stride))\n        x = self.Mixed_5b(x)\n        x = self.Mixed_5c(x)\n\n        x = self.avg_pool(x)\n        x = self.dropout(x)\n        logits = self.logits(x)\n\n        result = logits.mean(dim=2).squeeze(-1).squeeze(-1)\n        return result\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/lstm_attention.py,0,"b'from torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import models as models\n\nfrom ..utils import get_fine_tuning_parameters, load_state\nfrom .backbone import make_encoder\nfrom .modules import (Attention, AttentionLSTM, StateInitZero, squash_dims,\n                      unsquash_dim)\n\n\nclass VisualAttentionLSTM(nn.Module):\n    """"""LSTM architecture with attention mechanism (https://arxiv.org/pdf/1511.04119.pdf)""""""\n\n    def __init__(self, embed_size, sequence_size, encoder=\'resnet34\', n_classes=400, input_size=224, pretrained=True,\n                 use_attention=False, num_layers=1, bidirectional=False):\n        super().__init__()\n        self.use_attention = use_attention\n\n        # backbone\n        encoder = make_encoder(encoder, input_size=input_size, pretrained=pretrained)\n        self.resnet = encoder.features  # name is kept for compatibility with older checkpoints\n\n        self.dropout = nn.Dropout(p=0.5)\n\n        # self.state_init = StateInitFC(resnet1_channel_size, embed_size)\n        bidirectional_mult = 2 if bidirectional else 1\n        self.state_init = StateInitZero(embed_size, num_layers=num_layers * bidirectional_mult, batch_first=True)\n\n        if use_attention:\n            self.lstm = AttentionLSTM(encoder.features_shape[0], embed_size, encoder.features_shape[1] ** 2,\n                                      batch_first=True, num_layers=num_layers, dropout=0.2)\n        else:\n            self.lstm = nn.LSTM(encoder.features_shape[0], embed_size, num_layers=num_layers, dropout=0.2,\n                                batch_first=False, bidirectional=True)\n\n        self.fc = nn.Linear(bidirectional_mult * embed_size, n_classes)\n        self.out_dropout = nn.Dropout(0.5)\n\n        self.embed_size = embed_size\n        self.sequence_size = sequence_size\n        self.last_feature_size = encoder.features_shape[1]\n\n        self.init_weights()\n\n    def init_weights(self):\n        """"""Initialize the weights.""""""\n        self.fc.weight.data.normal_(0.0, 0.02)\n        self.fc.bias.data.fill_(0)\n\n    def forward(self, images):\n        """"""Extract the image feature vectors.""""""\n        # (B x T x C x H x W) -> (B*T x C x H x W)\n        images = squash_dims(images, (0, 1))\n        features = self.resnet(images)\n        features = self.dropout(features)\n\n        features = unsquash_dim(features, 0, (-1, self.sequence_size))\n        hx, cx = self.state_init(features)\n        # early_features = early_features.transpose(0, 1)  # to T x B x H x W\n\n        # no attention\n        if not self.use_attention:\n            features = F.avg_pool2d(squash_dims(features, (0, 1)), 7)\n            features = unsquash_dim(features, 0, (-1, self.sequence_size))\n            features = features.squeeze(-1).squeeze(-1).transpose(0, 1)\n\n        ys, hidden = self.lstm(features, (hx, cx))\n        ys = ys.transpose(0, 1)\n\n        ys = self.fc(ys)\n        ys = ys.mean(1)\n        return ys\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n\n    def load_checkpoint(self, state_dict):\n        load_state(self, state_dict, \'fc\')\n\n\nclass ResnetAttSingleInput(nn.Module):\n    """"""ONNX Exportable variant of the LSTM-Attenion model""""""\n\n    def __init__(self, embed_size, sequence_size, n_classes=400, input_size=224, pretrained=True, resnet_size=50):\n        """"""Load the pretrained ResNet and replace top fc layer.""""""\n        super().__init__()\n\n        # backbone\n        resnet_cls = getattr(models, ""resnet{}"".format(resnet_size))\n        resnet_model = resnet_cls(pretrained=pretrained)\n\n        modules = list(resnet_model.children())[:-2]  # delete the last fc layer.\n        self.resnet1 = nn.Sequential(*modules)\n        self.dropout = nn.Dropout(p=0.5)\n\n        resnet1_channel_size = resnet_model.fc.in_features\n        resnet1_spatial_size = input_size // 32\n        self.last_feature_size = resnet1_spatial_size\n        self.embed_size = embed_size\n        self.sequence_size = sequence_size\n\n        num_layers = 1\n        self.attn = Attention(embed_size, None, self.last_feature_size * self.last_feature_size)\n        self.lstm = nn.LSTM(resnet1_channel_size, embed_size, num_layers=num_layers, dropout=0.2, batch_first=False)\n\n        self.fc = nn.Linear(embed_size, n_classes)\n        self.out_dropout = nn.Dropout(0.5)\n\n    def forward(self, images, hx, cx):\n        """"""Extract the image feature vectors.""""""\n        # (B x T x C x H x W) -> (B*T x C x H x W)\n        # images = squash_dims(images, (0, 1))\n        features = self.resnet1(images)\n\n        # features = unsquash_dim(features, 0, (-1, self.sequence_size))\n        features = unsquash_dim(features, 0, (-1, 1))\n        v = squash_dims(features[0].transpose(1, 0), (2, 3))\n        feature, attention = self.attn(hx[0], v, v)\n        feature = feature.permute((1, 0))\n\n        ys, (hx, cx) = self.lstm(feature.unsqueeze(0), (hx, cx))\n        ys = self.fc(ys)\n        ys = ys.mean(1)\n        return ys, hx, cx\n\n    def trainable_parameters(self):\n        return get_fine_tuning_parameters(self)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/mobilenet_3d.py,0,"b'import math\n\nimport torch.nn as nn\n\n\nclass MobileNet(nn.Module):\n    @staticmethod\n    def conv_bn(inp, oup, stride):\n        return nn.Sequential(\n            nn.Conv3d(\n                inp,\n                oup,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                bias=False),\n            nn.BatchNorm3d(oup),\n            nn.ReLU(inplace=True)\n        )\n\n    @staticmethod\n    def conv_dw(inp, oup, stride):\n        return nn.Sequential(\n            nn.Conv3d(\n                inp,\n                inp,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                groups=inp,\n                bias=False),\n            nn.BatchNorm3d(inp),\n            nn.ReLU(inplace=True),\n\n            nn.Conv3d(\n                inp,\n                oup,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=False),\n            nn.BatchNorm3d(oup),\n            nn.ReLU(inplace=True)\n        )\n\n    def __init__(self, sample_size, sample_duration, num_classes=400, last_fc=True):\n        super(MobileNet, self).__init__()\n\n        self.last_fc = last_fc\n\n        self.model = nn.Sequential(\n            self.conv_bn(3, 32, 2),\n            self.conv_dw(32, 64, 1),\n            self.conv_dw(64, 128, 2),\n            self.conv_dw(128, 128, 1),\n            self.conv_dw(128, 256, 2),\n            self.conv_dw(256, 256, 1),\n            self.conv_dw(256, 512, 2),\n            self.conv_dw(512, 512, 1),\n            self.conv_dw(512, 512, 1),\n            self.conv_dw(512, 512, 1),\n            self.conv_dw(512, 512, 1),\n            self.conv_dw(512, 512, 1),\n            self.conv_dw(512, 1024, 2),\n            self.conv_dw(1024, 1024, 1),\n        )\n\n        last_duration = math.ceil(sample_duration / 16)\n        last_size = math.ceil(sample_size / 32)\n        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n\n        self.fc = nn.Linear(1024, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.model(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        if self.last_fc:\n            x = self.fc(x)\n\n        return x\n\n\nclass DepthWiseBlock(nn.Module):\n    def __init__(self, inp, oup, stride=1):\n        super(DepthWiseBlock, self).__init__()\n        self.conv1 = nn.Conv3d(\n            inp,\n            inp,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            groups=inp,\n            bias=False)\n        self.bn1 = nn.BatchNorm3d(inp)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv3d(\n            inp,\n            oup,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False)\n        self.bn2 = nn.BatchNorm3d(oup)\n        self.inplanes = inp\n        self.outplanes = oup\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.stride == 1 and self.inplanes == self.outplanes:\n            out += residual\n\n        out = self.relu(out)\n\n        return out\n\n\nclass MobileNetResidual(nn.Module):\n    @staticmethod\n    def conv_bn(inp, oup, stride):\n        return nn.Sequential(\n            nn.Conv3d(\n                inp,\n                oup,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                bias=False),\n            nn.BatchNorm3d(oup),\n            nn.ReLU(inplace=True)\n        )\n\n    def __init__(self, sample_size, sample_duration, num_classes=400, last_fc=True):\n        super(MobileNetResidual, self).__init__()\n\n        self.last_fc = last_fc\n\n        self.model = nn.Sequential(\n            self.conv_bn(3, 32, 2),\n            DepthWiseBlock(32, 64, 1),\n            DepthWiseBlock(64, 128, (1, 2, 2)),\n            DepthWiseBlock(128, 128, 1),\n            DepthWiseBlock(128, 256, 2),\n            DepthWiseBlock(256, 256, 1),\n            DepthWiseBlock(256, 512, 2),\n            DepthWiseBlock(512, 512, 1),\n            DepthWiseBlock(512, 512, 1),\n            DepthWiseBlock(512, 512, 1),\n            DepthWiseBlock(512, 512, 1),\n            DepthWiseBlock(512, 512, 1),\n            DepthWiseBlock(512, 1024, 2),\n            DepthWiseBlock(1024, 1024, 1),\n        )\n\n        last_duration = math.ceil(sample_duration / 16)\n        last_size = math.ceil(sample_size / 32)\n        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.fc = nn.Linear(1024, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.model(x)\n\n        x = self.avgpool(x)\n        x = self.dropout(x)\n\n        x = x.view(x.size(0), -1)\n        if self.last_fc:\n            x = self.fc(x)\n\n        return x\n'"
pytorch_toolkit/action_recognition/action_recognition/models/multi_frame_baseline.py,0,"b'import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\nfrom ..utils import get_fine_tuning_parameters\nfrom .backbone import make_encoder\nfrom .modules import squash_dims, unsquash_dim\n\n\nclass MultiFrameBaseline(nn.Module):\n    """"""Simple baseline that runs a classifier on each frame independently and averages logits.""""""\n\n    def __init__(self, sample_duration, encoder=\'resnet34\', n_classes=400, input_size=224, pretrained=True,\n                 input_channels=3):\n        """"""Average prediction over multiple frames""""""\n        super().__init__()\n\n        # backbone\n        encoder = make_encoder(encoder, input_size=input_size, input_channels=input_channels, pretrained=pretrained)\n        self.resnet = encoder.features  # name is kept for compatibility with older checkpoints\n        self.embed_size = encoder.features_shape[0]\n        self.last_feature_size = encoder.features_shape[1]\n        self.fc = nn.Linear(encoder.features_shape[0], n_classes)\n        self.dropout = nn.Dropout2d(0.5)\n\n        self.n_classes = n_classes\n        self.input_channels = input_channels\n        self.input_size = input_size\n        self.sequence_size = sample_duration\n        self.init_weights()\n\n    def init_weights(self):\n        """"""Initialize the weights.""""""\n        self.fc.weight.data.normal_(0.0, 0.02)\n        self.fc.bias.data.fill_(0)\n\n    def forward(self, images):\n        """"""Extract the image feature vectors.""""""\n        # (B x T x C x H x W) -> (B*T x C x H x W)\n        batch_size = images.shape[0]\n        images = squash_dims(images, (0, 1))\n\n        features = self.resnet(images)\n        # features = self.dropout(features)\n\n        features = F.avg_pool2d(features, self.last_feature_size)  # (B*T) x C\n        features = unsquash_dim(features, 0, (batch_size, -1))\n        ys = self.fc(features.squeeze(-1).squeeze(-1))\n\n        return ys.mean(1)\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n\n\nclass MultiFrameBaselineEncoder(MultiFrameBaseline):\n    def forward(self, images):\n        batch_size = images.shape[0]\n        if images.dim() == 5:\n            # If input tensor contains temporal dimension, combine it with the batch one:\n            # (B x T x C x H x W) -> (B*T x C x H x W)\n            images = squash_dims(images, (0, 1))\n        features = self.resnet(images)\n        features = F.avg_pool2d(features, self.last_feature_size, 1)  # (B*T) x C\n        features = features.squeeze(-1).squeeze(-1)\n        features = self.fc(features)\n        if images.dim() == 5:\n            # Separate temporal and batch dimensions back.\n            # (B*T x embd_size) -> (B x T x embd_size)\n            features = unsquash_dim(features, 0, (batch_size, -1))\n        return features\n\n    def export_onnx(self, export_path):\n        first_param = next(self.parameters())\n        input_tensor = first_param.new_zeros(1, self.input_channels, self.input_size, self.input_size)\n        with torch.no_grad():\n            torch.onnx.export(self, (input_tensor,), export_path,\n                              input_names=[\'image\'], output_names=[\'features\'],\n                              verbose=True)\n\n\nclass MultiFrameBaselineDecoder(MultiFrameBaseline):\n    def forward(self, features):\n        return features.mean(1)\n\n    def export_onnx(self, export_path):\n        first_param = next(self.parameters())\n        input_tensor = first_param.new_zeros(1, self.sequence_size, self.n_classes)\n        with torch.no_grad():\n            torch.onnx.export(self, (input_tensor,), export_path,\n                              input_names=[\'features\'], output_names=[\'logits\'],\n                              verbose=True)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/pre_act_resnet_3d.py,0,"b'import math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom ..utils import get_fine_tuning_parameters\n\n__all__ = [\n    \'PreActivationResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n    \'resnet152\', \'resnet200\'\n]\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(\n        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n        out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass PreActivationBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(PreActivationBasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm3d(inplanes)\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass PreActivationBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(PreActivationBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm3d(inplanes)\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass PreActivationResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 sample_size,\n                 sample_duration,\n                 shortcut_type=\'B\',\n                 num_classes=400):\n        self.inplanes = 64\n        super(PreActivationResNet, self).__init__()\n        self.conv1 = nn.Conv3d(\n            3,\n            64,\n            kernel_size=(7, 7, 7),\n            stride=(2, 2, 2),\n            padding=(1, 3, 3),\n            bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(\n            block, 128, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(\n            block, 256, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(\n            block, 512, layers[3], shortcut_type, stride=2)\n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AdaptiveAvgPool3d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n\n\ndef resnet18(**kwargs):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    model = PreActivationResNet(PreActivationBasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef resnet34(**kwargs):\n    """"""Constructs a ResNet-34 model.\n    """"""\n    model = PreActivationResNet(PreActivationBasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = PreActivationResNet(PreActivationBottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet101(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = PreActivationResNet(PreActivationBottleneck, [3, 4, 23, 3],\n                                **kwargs)\n    return model\n\n\ndef resnet152(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = PreActivationResNet(PreActivationBottleneck, [3, 8, 36, 3],\n                                **kwargs)\n    return model\n\n\ndef resnet200(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = PreActivationResNet(PreActivationBottleneck, [3, 24, 36, 3],\n                                **kwargs)\n    return model\n'"
pytorch_toolkit/action_recognition/action_recognition/models/r3d.py,0,"b'""""""\nThis is an re-implementation of R3D and R(2+1) topologies from paper:\n\nTran, Du, et al. ""A Closer Look at Spatiotemporal Convolutions for Action Recognition.""\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n""""""\nimport collections\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\nfrom action_recognition.utils import drop_last, get_fine_tuning_parameters\n\n__all__ = [\'R3D\', \'R3D_18\', \'R3D_34\', \'R3D_101\', \'R3D_152\', \'R2p1D_18\', \'R2p1D_34\', \'R2p1D_101\', \'R2p1D_152\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\nR3D_MODELS = {\n    \'r3d\': lambda args, encoder: R3D_18(\n        num_classes=args.n_classes\n    ),\n    \'r2+1d\': lambda args, encoder: R2p1D_18(\n        num_classes=args.n_classes,\n    ),\n\n    \'r3d_18\': lambda args, encoder: R3D_18(\n        num_classes=args.n_classes\n    ),\n    \'r2+1d_18\': lambda args, encoder: R2p1D_18(\n        num_classes=args.n_classes,\n    ),\n    \'r3d_34\': lambda args, encoder: R3D_34(\n        num_classes=args.n_classes\n    ),\n    \'r2+1d_34\': lambda args, encoder: R2p1D_34(\n        num_classes=args.n_classes,\n    ),\n    \'r3d_50\': lambda args, encoder: R3D_50(\n        num_classes=args.n_classes\n    ),\n    \'r2+1d_50\': lambda args, encoder: R2p1D_50(\n        num_classes=args.n_classes,\n    ),\n    \'r3d_101\': lambda args, encoder: R3D_101(\n        num_classes=args.n_classes\n    ),\n    \'r2+1d_101\': lambda args, encoder: R2p1D_101(\n        num_classes=args.n_classes,\n    ),\n    \'r3d_152\': lambda args, encoder: R3D_152(\n        num_classes=args.n_classes\n    ),\n    \'r2+1d_152\': lambda args, encoder: R2p1D_152(\n        num_classes=args.n_classes,\n    ),\n}\n\n\ndef make_conv(in_planes, out_planes, middle_planes=None, kernel_size=(3, 3, 3), stride=(1, 1, 1), decomposed=True,\n              bias=False):\n    padding = (kernel_size[0] // 2, kernel_size[1] // 2, kernel_size[2] // 2)\n    if decomposed:\n        i = 3 * in_planes * out_planes * kernel_size[1] * kernel_size[2]\n        i /= in_planes * kernel_size[1] * kernel_size[2] + 3 * out_planes\n        if middle_planes is None:\n            middle_planes = int(i)\n        return nn.Sequential(\n            nn.Conv3d(in_planes, middle_planes, kernel_size=(1, kernel_size[1], kernel_size[2]),\n                      padding=(0, padding[1], padding[2]), stride=stride, bias=bias),\n            nn.BatchNorm3d(middle_planes),\n            nn.ReLU(),\n            nn.Conv3d(middle_planes, out_planes, kernel_size=(kernel_size[0], 1, 1),\n                      padding=(padding[0], 0, 0), stride=1, bias=bias)\n        )\n    else:\n        return nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n\n\nclass BasicBlockR3D(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, decomposed=True):\n        super(BasicBlockR3D, self).__init__()\n        self.conv1 = make_conv(inplanes, planes, kernel_size=(3, 3, 3), stride=stride, decomposed=decomposed)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = make_conv(planes, planes, kernel_size=(3, 3, 3), decomposed=decomposed)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass BottleneckR3D(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, decomposed=True):\n        super(BottleneckR3D, self).__init__()\n        self.conv1 = make_conv(inplanes, planes, kernel_size=(1, 1, 1), decomposed=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = make_conv(planes, planes, kernel_size=(3, 3, 3), stride=stride, decomposed=decomposed)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = make_conv(planes, planes * 4, kernel_size=(1, 1, 1), decomposed=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass R3D(nn.Module):\n\n    def __init__(self, block, layers, num_classes=400, num_channels=3, decomposed=True):\n        self.inplanes = 64\n        super(R3D, self).__init__()\n\n        self.decomposed = decomposed\n\n        self.conv1 = make_conv(num_channels, 64, middle_planes=45, kernel_size=(3, 7, 7), stride=(1, 2, 2),\n                               decomposed=decomposed)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], downsample=True)\n        self.layer3 = self._make_layer(block, 256, layers[2], downsample=True)\n        self.layer4 = self._make_layer(block, 512, layers[3], downsample=True)\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                torch.nn.init.kaiming_normal_(m.weight, nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, downsample=False):\n        downsample_layer = None\n        first_stride = (2, 2, 2) if downsample else (1, 1, 1)\n        if downsample or self.inplanes != planes * block.expansion:\n            downsample_layer = nn.Sequential(\n                make_conv(self.inplanes, planes * block.expansion, kernel_size=(1, 1, 1), stride=first_stride,\n                          decomposed=False),\n                nn.BatchNorm3d(planes * block.expansion),\n            )\n\n        layers = [block(self.inplanes, planes, first_stride, downsample_layer, decomposed=self.decomposed)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, decomposed=self.decomposed))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        # x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n\n\ndef check_conv1_params(model, pretrained_weights):\n    if model.conv1.in_channels != pretrained_weights[\'conv1.weight\'].size(1):\n        # get mean over RGB channels weights\n        rgb_mean = torch.mean(pretrained_weights[\'conv1.weight\'], dim=1)\n\n        expand_ratio = model.conv1.in_channels // pretrained_weights[\'conv1.weight\'].size(1)\n        pretrained_weights[\'conv1.weight\'] = pretrained_weights[\'conv1.weight\'].repeat(1, expand_ratio, 1, 1)\n        # pretrained_weights[\'conv1.weight\'] = rgb_mean.unsqueeze(1).repeat(1, model.conv1.in_channels, 1, 1)\n\n\ndef average_conv1_weights(old_params, in_channels):\n    new_params = collections.OrderedDict()\n    layer_count = 0\n    all_key_list = old_params.keys()\n    for layer_key in drop_last(all_key_list, 2):\n        if layer_count == 0:\n            rgb_weight = old_params[layer_key]\n            rgb_weight_mean = torch.mean(rgb_weight, dim=1)\n            flow_weight = rgb_weight_mean.unsqueeze(1).repeat(1, in_channels, 1, 1)\n            if isinstance(flow_weight, torch.autograd.Variable):\n                new_params[layer_key] = flow_weight.data\n            else:\n                new_params[layer_key] = flow_weight\n            layer_count += 1\n        else:\n            new_params[layer_key] = old_params[layer_key]\n            layer_count += 1\n\n    return new_params\n\n\ndef load_pretrained_resnet(model, resnet_name=\'resnet34\', num_channels=3):\n    if num_channels == 3:\n        pretrained_weights = model_zoo.load_url(model_urls[resnet_name])\n        check_conv1_params(model, pretrained_weights)\n        model.load_state_dict(pretrained_weights)\n    else:\n        pretrained_dict = model_zoo.load_url(model_urls[resnet_name])\n        model_dict = model.state_dict()\n\n        new_pretrained_dict = average_conv1_weights(pretrained_dict, num_channels)\n\n        # 1. filter out unnecessary keys\n        new_pretrained_dict = {k: v for k, v in new_pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(new_pretrained_dict)\n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef R3D_18(pretrained=False, **kwargs):\n    """"""Constructs a R3D-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = R3D(BasicBlockR3D, [2, 2, 2, 2], decomposed=False, **kwargs)\n\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet18\', num_channels)\n    return model\n\n\ndef R3D_34(pretrained=False, **kwargs):\n    """"""Constructs a R3D-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = R3D(BasicBlockR3D, [3, 4, 6, 3], decomposed=False, **kwargs)\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet34\', num_channels)\n    return model\n\n\ndef R2p1D_18(**kwargs):\n    """"""Constructs a R2+1D-34 model.""""""\n    model = R3D(BasicBlockR3D, [2, 2, 2, 2], decomposed=True, **kwargs)\n    return model\n\n\ndef R2p1D_34(**kwargs):\n    """"""Constructs a R2+1D-34 model.""""""\n    model = R3D(BasicBlockR3D, [3, 4, 6, 3], decomposed=True, **kwargs)\n    return model\n\n\ndef R3D_50(pretrained=False, **kwargs):\n    """"""Constructs a R3D-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = R3D(BottleneckR3D, [3, 4, 6, 3], decomposed=False, **kwargs)\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet50\', num_channels)\n    return model\n\n\ndef R3D_101(pretrained=False, **kwargs):\n    """"""Constructs a R3D-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = R3D(BottleneckR3D, [3, 4, 23, 3], decomposed=False, **kwargs)\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet101\', num_channels)\n    return model\n\n\ndef R3D_152(pretrained=False, **kwargs):\n    """"""Constructs a R3D-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = R3D(BottleneckR3D, [3, 8, 36, 3], decomposed=False, **kwargs)\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet152\', num_channels)\n    return model\n\n\ndef R2p1D_50(**kwargs):\n    """"""Constructs a R2+1D-50 model.""""""\n    model = R3D(BottleneckR3D, [3, 4, 6, 3], decomposed=True, **kwargs)\n    return model\n\n\ndef R2p1D_101(**kwargs):\n    """"""Constructs a R2+1D-101 model.""""""\n    model = R3D(BottleneckR3D, [3, 4, 23, 3], decomposed=True, **kwargs)\n    return model\n\n\ndef R2p1D_152(**kwargs):\n    """"""Constructs a R2+1D-152 model.""""""\n    model = R3D(BottleneckR3D, [3, 8, 36, 3], decomposed=True, **kwargs)\n    return model\n'"
pytorch_toolkit/action_recognition/action_recognition/models/resnext_3d.py,0,"b'import math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n__all__ = [\'ResNeXt\', \'resnet50\', \'resnet101\']\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(\n        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n        out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass ResNeXtBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, cardinality, stride=1,\n                 downsample=None):\n        super(ResNeXtBottleneck, self).__init__()\n        mid_planes = cardinality * int(planes / 32)\n        self.conv1 = nn.Conv3d(inplanes, mid_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(mid_planes)\n        self.conv2 = nn.Conv3d(\n            mid_planes,\n            mid_planes,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            groups=cardinality,\n            bias=False)\n        self.bn2 = nn.BatchNorm3d(mid_planes)\n        self.conv3 = nn.Conv3d(\n            mid_planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 sample_size,\n                 sample_duration,\n                 shortcut_type=\'B\',\n                 cardinality=32,\n                 num_classes=400):\n        self.inplanes = 64\n        super(ResNeXt, self).__init__()\n        self.conv1 = nn.Conv3d(\n            3,\n            64,\n            kernel_size=7,\n            stride=(1, 2, 2),\n            padding=(3, 3, 3),\n            bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 128, layers[0], shortcut_type,\n                                       cardinality)\n        self.layer2 = self._make_layer(\n            block, 256, layers[1], shortcut_type, cardinality, stride=2)\n        self.layer3 = self._make_layer(\n            block, 512, layers[2], shortcut_type, cardinality, stride=2)\n        self.layer4 = self._make_layer(\n            block, 1024, layers[3], shortcut_type, cardinality, stride=2)\n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(cardinality * 32 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self,\n                    block,\n                    planes,\n                    blocks,\n                    shortcut_type,\n                    cardinality,\n                    stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, cardinality, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, cardinality))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet101(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef resnet152(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n'"
pytorch_toolkit/action_recognition/action_recognition/models/video_transformer.py,0,"b'import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\nfrom .backbone import make_encoder\nfrom .modules import Identity, squash_dims, unsquash_dim\nfrom .modules.self_attention import DecoderBlock, PositionEncoding\nfrom ..utils import get_fine_tuning_parameters, load_state\n\n\nclass VideoTransformer(nn.Module):\n    def __init__(self, embed_size, sequence_size, encoder=\'resnet34\', n_classes=400, input_size=224, pretrained=True,\n                 input_channels=3, num_layers=4, layer_norm=True):\n        super().__init__()\n\n        # backbone\n        encoder = make_encoder(encoder, input_size=input_size, pretrained=pretrained, input_channels=input_channels)\n        self.resnet = encoder.features  # name is kept for compatibility with older checkpoints\n        self.last_feature_size = encoder.features_shape[1]\n        self.embed_size = embed_size\n\n        if encoder.features_shape[0] != embed_size:\n            self.reduce_conv = nn.Conv2d(encoder.features_shape[0], embed_size, 1)\n        else:\n            self.reduce_conv = Identity()\n\n        self.sequence_size = sequence_size\n\n        self.self_attention_decoder = SelfAttentionDecoder(embed_size, embed_size, [8] * num_layers,\n                                                           sequence_size, layer_norm=layer_norm)\n        self.fc = nn.Linear(embed_size, n_classes)\n        self.dropout = nn.Dropout2d(0.8)\n\n        self.init_weights()\n        self.input_channels = input_channels\n        self.input_size = input_size\n\n    def init_weights(self):\n        """"""Initialize the weights.""""""\n        self.fc.weight.data.normal_(0.0, 0.02)\n        self.fc.bias.data.fill_(0)\n\n    def forward(self, rgb_clip):\n        """"""Extract the image feature vectors.""""""\n        # (B x T x C x H x W) -> (B*T x C x H x W)\n        rgb_clip = squash_dims(rgb_clip, (0, 1))\n\n        features = self.resnet(rgb_clip)\n        features = self.reduce_conv(features)\n\n        features = F.avg_pool2d(features, 7)  # (B*T) x C\n        features = unsquash_dim(features, 0, (-1, self.sequence_size))\n        ys = self.self_attention_decoder(features[..., 0, 0])\n        # ys = self.dropout(ys)\n        ys = self.fc(ys)\n\n        return ys.mean(1)\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n\n    def load_checkpoint(self, state_dict):\n        load_state(self, state_dict, \'fc\')\n\n\nclass VideoTransformerEncoder(VideoTransformer):\n    def forward(self, rgb_frame):\n        features = self.resnet(rgb_frame)\n        features = self.reduce_conv(features)\n        features = F.avg_pool2d(features, 7)\n        return features\n\n    def export_onnx(self, export_path):\n        first_param = next(self.parameters())\n        input_tensor = first_param.new_zeros(1, self.input_channels, self.input_size, self.input_size)\n        with torch.no_grad():\n            torch.onnx.export(self, (input_tensor,), export_path, verbose=True)\n\n\nclass VideoTransformerDecoder(VideoTransformer):\n    def forward(self, features):\n        ys = self.self_attention_decoder(features)\n        ys = self.fc(ys)\n        return ys.mean(1)\n\n    def export_onnx(self, export_path):\n        first_param = next(self.parameters())\n        input_tensor = first_param.new_zeros(1, self.sequence_size, self.embed_size)\n        with torch.no_grad():\n            torch.onnx.export(self, (input_tensor,), export_path, verbose=True)\n\n\nclass SelfAttentionDecoder(nn.Module):\n    def __init__(self, input_size, hidden_size, n_heads, sequence_size, inner_hidden_factor=2, layer_norm=True):\n        super().__init__()\n\n        input_sizes = [hidden_size] * len(n_heads)\n        input_sizes[0] = input_size\n        hidden_sizes = [hidden_size] * len(n_heads)\n\n        self.position_encoding = PositionEncoding(sequence_size, hidden_size)\n\n        self.layers = nn.ModuleList([\n            DecoderBlock(inp_size, hid_size, hid_size * inner_hidden_factor, n_head, hid_size // n_head,\n                         hid_size // n_head, layer_norm=layer_norm)\n            for i, (inp_size, hid_size, n_head) in enumerate(zip(input_sizes, hidden_sizes, n_heads))\n        ])\n\n    def forward(self, x):\n        outputs, attentions = [], []\n        b, t, c = x.size()\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x, attn = layer(x)\n\n            outputs.append(x)\n        return x\n'"
pytorch_toolkit/action_recognition/action_recognition/models/vtn_motion.py,0,"b'import torch\nfrom torch import nn\n\nfrom ..utils import get_fine_tuning_parameters, load_state\nfrom .video_transformer import VideoTransformer\n\n\nclass RGBDiff(nn.Module):\n    def __init__(self, dim=1):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, image):\n        """"""\n        Args:\n            image (torch.Tensor):  (N x T x C x H x W)\n\n        """"""\n        diffs = []\n        for i in range(1, image.size(self.dim)):\n            prev = image.index_select(self.dim, image.new_tensor(i - 1, dtype=torch.long))\n            current = image.index_select(self.dim, image.new_tensor(i, dtype=torch.long))\n            diffs.append(current - prev)\n\n        return torch.cat(diffs, dim=self.dim)\n\n\nclass VideoTransformerMotion(nn.Module):\n    def __init__(self, embed_size, sequence_size, encoder_name, n_classes=400, input_size=224, pretrained=True,\n                 mode=\'rfbdiff\', layer_norm=True):\n        """"""Load the pretrained ResNet and replace top fc layer.""""""\n        super().__init__()\n        self.mode = mode\n        motion_sequence_size = sequence_size\n        input_channels = 3\n        if self.mode == ""flow"":\n            input_channels = 2\n        elif self.mode == ""rgbdiff"":\n            motion_sequence_size = motion_sequence_size - 1\n            self.rgb_diff = RGBDiff()\n        else:\n            raise Exception(""Unsupported mode "" + self.mode)\n\n        self.motion_decoder = VideoTransformer(embed_size, motion_sequence_size, encoder_name, n_classes=n_classes,\n                                               input_size=input_size, pretrained=pretrained,\n                                               input_channels=input_channels, layer_norm=layer_norm)\n\n    def forward(self, clip):\n        """"""Extract the image feature vectors.""""""\n        if self.mode == ""rgbdiff"":\n            clip = self.rgb_diff(clip)\n        logits_motion = self.motion_decoder(clip)\n\n        return logits_motion\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n\n    def load_checkpoint(self, state_dict):\n        load_state(self, state_dict, \'motion_decoder.fc\')\n'"
pytorch_toolkit/action_recognition/action_recognition/models/vtn_two_stream.py,0,"b'import torch\nfrom torch import nn\n\nfrom action_recognition.models.vtn_motion import VideoTransformerMotion\n\nfrom ..utils import get_fine_tuning_parameters, load_state\nfrom .video_transformer import VideoTransformer\n\n\nclass VideoTransformerTwoStream(nn.Module):\n    def __init__(self, embed_size, sequence_size, encoder_name=\'resnet34\', n_classes=400, input_size=224,\n                 pretrained=True, motion_path=None, rgb_path=None, mode=\'rgbdiff\', layer_norm=True):\n        """"""Load the pretrained ResNet and replace top fc layer.""""""\n        super().__init__()\n\n        self.rgb_recoder = VideoTransformer(embed_size, sequence_size, encoder_name, n_classes=n_classes,\n                                            input_size=input_size, pretrained=pretrained, num_layers=4,\n                                            layer_norm=layer_norm)\n\n        self.motion_decoder = VideoTransformerMotion(embed_size, sequence_size, encoder_name, n_classes=n_classes,\n                                                     input_size=input_size, pretrained=pretrained, mode=mode,\n                                                     layer_norm=layer_norm)\n\n        if motion_path and rgb_path:\n            self.load_separate_trained(motion_path, rgb_path)\n\n    def load_separate_trained(self, motion_path, rgb_path):\n        print(""Loading rgb model from: {}"".format(rgb_path))\n        rgb_checkpoint = torch.load(rgb_path.as_posix())\n        self.rgb_recoder.load_checkpoint(rgb_checkpoint[\'state_dict\'])\n\n        print(""Loading motion model from: {}"".format(motion_path))\n        motion_checkpoint = torch.load(motion_path.as_posix())\n        self.motion_decoder.load_checkpoint(motion_checkpoint[\'state_dict\'])\n\n    def forward(self, rgb_clip=None, flow_clip=None):\n        """"""Extract the image feature vectors.""""""\n        logits_rgb = self.rgb_recoder(rgb_clip)\n        motion_input = rgb_clip\n        if flow_clip is not None:\n            motion_input = flow_clip\n        logits_motion = self.motion_decoder(motion_input)\n\n        return 0.5 * logits_rgb + 0.5 * logits_motion\n\n    def trainable_parameters(self):\n        param_groups = [\n            (\'trainable\', {\'re\': r\'\'}),\n        ]\n\n        return get_fine_tuning_parameters(self, param_groups)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/wide_resnet_3d.py,0,"b'import math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n__all__ = [\'WideResNet\', \'resnet50\']\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(\n        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n        out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass WideBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(WideBottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass WideResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 sample_size,\n                 sample_duration,\n                 k=1,\n                 shortcut_type=\'B\',\n                 num_classes=400):\n        self.inplanes = 64\n        super(WideResNet, self).__init__()\n        self.conv1 = nn.Conv3d(\n            3,\n            64,\n            kernel_size=7,\n            stride=(1, 2, 2),\n            padding=(3, 3, 3),\n            bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64 * k, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(\n            block, 128 * k, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(\n            block, 256 * k, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(\n            block, 512 * k, layers[3], shortcut_type, stride=2)\n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(512 * k * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = WideResNet(WideBottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n'"
pytorch_toolkit/face_recognition/model/backbones/__init__.py,0,b''
pytorch_toolkit/face_recognition/model/backbones/resnet.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom model.blocks.resnet_blocks import Bottleneck, BasicBlock\nfrom model.blocks.shared_blocks import make_activation\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000, activation=nn.ReLU, head=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = make_activation(nn.ReLU)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], activation=activation)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, activation=activation)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, activation=activation)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, activation=activation)\n        self.avgpool = nn.Conv2d(512 * block.expansion, 512 * block.expansion, 7,\n                                 groups=512 * block.expansion, bias=False)\n        self.head = head\n        if not self.head:\n            self.output_channels = 512 * block.expansion\n        else:\n            self.fc = nn.Conv2d(512 * block.expansion, num_classes, 1, stride=1, padding=0, bias=False)\n            self.output_channels = num_classes\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, activation=nn.ReLU):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, activation=activation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, activation=activation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        if self.head:\n            x = self.fc(x)\n\n        return x\n\n    def get_output_channels(self):\n        return self.output_channels\n\n\ndef resnet50(**kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet34(**kwargs):\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n'"
pytorch_toolkit/face_recognition/model/backbones/rmnet.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import OrderedDict\n\nimport torch.nn as nn\nfrom ..blocks.rmnet_blocks import RMBlock\n\n\nclass RMNetBody(nn.Module):\n    def __init__(self, block=RMBlock, blocks_per_stage=(None, 4, 8, 10, 11), trunk_width=(32, 32, 64, 128, 256),\n                 bottleneck_width=(None, 8, 16, 32, 64)):\n        super(RMNetBody, self).__init__()\n        assert len(blocks_per_stage) == len(trunk_width) == len(bottleneck_width)\n        self.dim_out = trunk_width[-1]\n\n        stages = [nn.Sequential(OrderedDict([\n            (\'data_bn\', nn.BatchNorm2d(3)),\n            (\'conv1\', nn.Conv2d(3, trunk_width[0], kernel_size=3, stride=2, padding=1, bias=False)),\n            (\'bn1\', nn.BatchNorm2d(trunk_width[0])),\n            (\'relu1\', nn.ReLU(inplace=True))])), ]\n\n        for i, (blocks_num, w, wb) in enumerate(zip(blocks_per_stage, trunk_width, bottleneck_width)):\n            # Zeroth stage is already added.\n            if i == 0:\n                continue\n            stage = []\n            # Do not downscale input to the first stage.\n            if i > 1:\n                stage.append(block(trunk_width[i - 1], wb, w, downsample=True))\n            for _ in range(blocks_num):\n                stage.append(block(w, wb, w))\n            stages.append(nn.Sequential(*stage))\n\n        self.stages = nn.Sequential(OrderedDict([(\'stage_{}\'.format(i), stage) for i, stage in enumerate(stages)]))\n\n        self.init_weights()\n\n    def init_weights(self):\n        m = self.stages[0][0]  # [\'data_bn\']\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n        m = self.stages[0][1]  # [\'conv1\']\n        nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n        m = self.stages[0][2]  # [\'bn1\']\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n        # All other blocks should be initialized internally during instantiation.\n\n    def forward(self, x):\n        return self.stages(x)\n'"
pytorch_toolkit/face_recognition/model/backbones/se_resnet.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\nimport torch.nn as nn\n\nfrom model.blocks.se_resnet_blocks import SEBottleneck\n\n\nclass SEResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000, activation=nn.ReLU, head=False):\n        super(SEResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], activation=activation)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, activation=activation)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, activation=activation)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, activation=activation)\n        self.avgpool = nn.Conv2d(512 * block.expansion, 512 * block.expansion, 7,\n                                 groups=512 * block.expansion, bias=False)\n        self.head = head\n        if not self.head:\n            self.output_channels = 512 * block.expansion\n        else:\n            self.fc = nn.Conv2d(512 * block.expansion, num_classes, 1, stride=1, padding=0, bias=False)\n            self.output_channels = num_classes\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, activation=nn.ReLU):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, activation=activation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, activation=activation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        if self.head:\n            x = self.fc(x)\n\n        return x\n\n    def get_output_channels(self):\n        return self.output_channels\n\n\ndef se_resnet50(**kwargs):\n    model = SEResNet(SEBottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef se_resnet101(**kwargs):\n    model = SEResNet(SEBottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef se_resnet152(**kwargs):\n    model = SEResNet(SEBottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n'"
pytorch_toolkit/face_recognition/model/backbones/se_resnext.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\nimport torch.nn as nn\n\nfrom model.blocks.se_resnext_blocks import SEBottleneckX\n\n\nclass SEResNeXt(nn.Module):\n    def __init__(self, block, layers, cardinality=32, num_classes=1000, activation=nn.ReLU, head=False):\n        super(SEResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0], activation=activation)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, activation=activation)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, activation=activation)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, activation=activation)\n        self.avgpool = nn.Conv2d(512 * block.expansion, 512 * block.expansion, 7,\n                                 groups=512 * block.expansion, bias=False)\n        self.head = head\n        if not self.head:\n            self.output_channels = 512 * block.expansion\n        else:\n            self.fc = nn.Conv2d(512 * block.expansion, num_classes, 1, stride=1, padding=0, bias=False)\n            self.output_channels = num_classes\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, activation=nn.ReLU):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, self.cardinality, stride, downsample, activation=activation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, self.cardinality, activation=activation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        if self.head:\n            x = self.fc(x)\n\n        return x\n\n    def get_output_channels(self):\n        return self.output_channels\n\n\ndef se_resnext50(**kwargs):\n    model = SEResNeXt(SEBottleneckX, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef se_resnext101(**kwargs):\n    model = SEResNeXt(SEBottleneckX, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef se_resnext152(**kwargs):\n    model = SEResNeXt(SEBottleneckX, [3, 8, 36, 3], **kwargs)\n    return model\n'"
pytorch_toolkit/face_recognition/model/backbones/shufflenet_v2.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom model.blocks.shufflenet_v2_blocks import ShuffleInvertedResidual, conv_bn, conv_1x1_bn\n\n\nclass ShuffleNetV2Body(nn.Module):\n    def __init__(self, input_size=224, width_mult=1.):\n        super(ShuffleNetV2Body, self).__init__()\n\n        assert input_size % 32 == 0\n\n        self.stage_repeats = [4, 8, 4]\n        if width_mult == 0.5:\n            self.stage_out_channels = [-1, 24, 48, 96, 192, 1024]\n        elif width_mult == 1.0:\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif width_mult == 1.5:\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif width_mult == 2.0:\n            self.stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n        else:\n            raise ValueError(""Unsupported width multiplier"")\n\n        # building first layer\n        self.bn_first = nn.BatchNorm2d(3)\n        input_channel = self.stage_out_channels[1]\n        self.conv1 = conv_bn(3, input_channel, 2)\n\n        self.features = []\n\n        # building inverted residual blocks\n        for idxstage in range(len(self.stage_repeats)):\n            numrepeat = self.stage_repeats[idxstage]\n            output_channel = self.stage_out_channels[idxstage+2]\n            for i in range(numrepeat):\n                if i == 0:\n                    self.features.append(ShuffleInvertedResidual(input_channel, output_channel,\n                                                                 2, 2, activation=nn.PReLU))\n                else:\n                    self.features.append(ShuffleInvertedResidual(input_channel, output_channel,\n                                                                 1, 1, activation=nn.PReLU))\n                input_channel = output_channel\n\n        self.features = nn.Sequential(*self.features)\n        self.conv_last = conv_1x1_bn(input_channel, self.stage_out_channels[-1], activation=nn.PReLU)\n        self.init_weights()\n\n    @staticmethod\n    def get_downscale_factor():\n        return 16\n\n    def init_weights(self):\n        m = self.bn_first\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n\n    def get_num_output_channels(self):\n        return self.stage_out_channels[-1]\n\n    def forward(self, x):\n        x = self.conv1(self.bn_first(x))\n        x = self.features(x)\n        x = self.conv_last(x)\n        return x\n'"
pytorch_toolkit/face_recognition/model/blocks/__init__.py,0,b''
pytorch_toolkit/face_recognition/model/blocks/mobilenet_v2_blocks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom model.blocks.shared_blocks import SELayer\n\n\nclass InvertedResidual(nn.Module):\n    """"""Implementation of the modified Inverted residual block""""""\n    def __init__(self, in_channels, out_channels, stride, expand_ratio, outp_size=None):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n\n        self.inv_block = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * expand_ratio, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(in_channels * expand_ratio),\n            nn.PReLU(),\n\n            nn.Conv2d(in_channels * expand_ratio, in_channels * expand_ratio, 3, stride, 1,\n                      groups=in_channels * expand_ratio, bias=False),\n            nn.BatchNorm2d(in_channels * expand_ratio),\n            nn.PReLU(),\n\n            nn.Conv2d(in_channels * expand_ratio, out_channels, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(out_channels),\n            SELayer(out_channels, 8, nn.PReLU, outp_size)\n        )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.inv_block(x)\n\n        return self.inv_block(x)\n'"
pytorch_toolkit/face_recognition/model/blocks/resnet_blocks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom model.blocks.shared_blocks import make_activation\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, activation=nn.ReLU):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.act1 = make_activation(activation)\n\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.act2 = make_activation(activation)\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.act3 = make_activation(activation)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.act2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.act3(out)\n\n        return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, activation=nn.ReLU):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = make_activation(activation)\n        self.conv2 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n'"
pytorch_toolkit/face_recognition/model/blocks/rmnet_blocks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom model.blocks.shared_blocks import make_activation\n\n\nclass RMBlock(nn.Module):\n    def __init__(self, input_planes, squeeze_planes, output_planes, downsample=False, dropout_ratio=0.1,\n                 activation=nn.ELU):\n        super(RMBlock, self).__init__()\n        self.downsample = downsample\n        self.input_planes = input_planes\n        self.output_planes = output_planes\n\n        self.squeeze_conv = nn.Conv2d(input_planes, squeeze_planes, kernel_size=1, bias=False)\n        self.squeeze_bn = nn.BatchNorm2d(squeeze_planes)\n\n        self.dw_conv = nn.Conv2d(squeeze_planes, squeeze_planes, groups=squeeze_planes, kernel_size=3, padding=1,\n                                 stride=2 if downsample else 1, bias=False)\n        self.dw_bn = nn.BatchNorm2d(squeeze_planes)\n\n        self.expand_conv = nn.Conv2d(squeeze_planes, output_planes, kernel_size=1, bias=False)\n        self.expand_bn = nn.BatchNorm2d(output_planes)\n\n        self.activation = make_activation(activation)\n\n        self.dropout_ratio = dropout_ratio\n\n        if self.downsample:\n            self.skip_conv = nn.Conv2d(input_planes, output_planes, kernel_size=1, bias=False)\n            self.skip_conv_bn = nn.BatchNorm2d(output_planes)\n\n        self.init_weights()\n\n    def init_weights(self):\n        for m in self.children():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        residual = x\n        out = self.activation(self.squeeze_bn(self.squeeze_conv(x)))\n        out = self.activation(self.dw_bn(self.dw_conv(out)))\n        out = self.expand_bn(self.expand_conv(out))\n        if self.dropout_ratio > 0:\n            out = F.dropout(out, p=self.dropout_ratio, training=self.training, inplace=True)\n        if self.downsample:\n            residual = F.max_pool2d(x, kernel_size=2, stride=2, padding=0)\n            residual = self.skip_conv(residual)\n            residual = self.skip_conv_bn(residual)\n        out += residual\n        return self.activation(out)\n'"
pytorch_toolkit/face_recognition/model/blocks/se_resnet_blocks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom model.blocks.shared_blocks import make_activation\n\n\nclass SEBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, activation=nn.ReLU):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n\n        self.relu1 = make_activation(activation)\n        self.relu2 = make_activation(activation)\n        self.relu3 = make_activation(activation)\n        self.relu4 = make_activation(activation)\n\n        # SE\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_down = nn.Conv2d(planes * 4, planes // 4, kernel_size=1, bias=False)\n        self.conv_up = nn.Conv2d(planes // 4, planes * 4, kernel_size=1, bias=False)\n        self.sig = nn.Sigmoid()\n\n        # Downsample\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out1 = self.global_pool(out)\n        out1 = self.conv_down(out1)\n        out1 = self.relu3(out1)\n        out1 = self.conv_up(out1)\n        out1 = self.sig(out1)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        res = out1 * out + residual\n        res = self.relu4(res)\n\n        return res\n'"
pytorch_toolkit/face_recognition/model/blocks/se_resnext_blocks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom model.blocks.shared_blocks import SELayer\nfrom model.blocks.shared_blocks import make_activation\n\n\nclass SEBottleneckX(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None, activation=nn.ReLU):\n        super(SEBottleneckX, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n\n        self.conv2 = nn.Conv2d(planes * 2, planes * 2, kernel_size=3, stride=stride,\n                               padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 2)\n\n        self.conv3 = nn.Conv2d(planes * 2, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n\n        self.selayer = SELayer(planes * 4, 16, activation)\n\n        self.relu1 = make_activation(activation)\n        self.relu2 = make_activation(activation)\n        self.relu3 = make_activation(activation)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out = self.selayer(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu3(out)\n\n        return out\n'"
pytorch_toolkit/face_recognition/model/blocks/shared_blocks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\ndef make_activation(activation):\n    """"""Factory for activation functions""""""\n    if activation != nn.PReLU:\n        return activation(inplace=True)\n\n    return activation()\n\n\nclass SELayer(nn.Module):\n    """"""Implementation of the Squeeze-Excitaion layer from https://arxiv.org/abs/1709.01507""""""\n    def __init__(self, inplanes, squeeze_ratio=8, activation=nn.PReLU, size=None):\n        super(SELayer, self).__init__()\n        assert squeeze_ratio >= 1\n        assert inplanes > 0\n        if size is not None:\n            self.global_avgpool = nn.AvgPool2d(size)\n        else:\n            self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(inplanes, int(inplanes / squeeze_ratio), kernel_size=1, stride=1)\n        self.conv2 = nn.Conv2d(int(inplanes / squeeze_ratio), inplanes, kernel_size=1, stride=1)\n        self.relu = make_activation(activation)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.global_avgpool(x)\n        out = self.conv1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.sigmoid(out)\n        return x * out\n\n\nclass ScaleFilter(nn.Module):\n    """"""Implementaion of the ScaleFilter regularizer""""""\n    def __init__(self, q):\n        super(ScaleFilter, self).__init__()\n        assert 0 < q < 1\n        self.q = q\n\n    def forward(self, x):\n        if not self.training:\n            return x\n\n        scale_factors = 1. + self.q \\\n                - 2*self.q*torch.rand(x.shape[1], 1, 1, dtype=torch.float32, requires_grad=False).to(x.device)\n        return x * scale_factors\n'"
pytorch_toolkit/face_recognition/model/blocks/shufflenet_v2_blocks.py,0,"b'""""""\n Copyright (c) 2018 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom model.blocks.shared_blocks import make_activation\n\n\ndef conv_bn(inp, oup, stride, activation=nn.ReLU):\n    conv = nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        make_activation(activation)\n    )\n    nn.init.kaiming_normal_(conv[0].weight, mode=\'fan_out\')\n    return conv\n\n\ndef conv_1x1_bn(inp, oup, activation=nn.ReLU):\n    conv = nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        make_activation(activation)\n    )\n    nn.init.kaiming_normal_(conv[0].weight, mode=\'fan_out\')\n    return conv\n\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n    channels_per_group = num_channels // groups\n    # reshape\n    x = x.view(batchsize, groups, channels_per_group, height, width)\n    x = torch.transpose(x, 1, 2).contiguous()\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n    return x\n\n\nclass ShuffleInvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, benchmodel, activation=nn.ReLU):\n        super(ShuffleInvertedResidual, self).__init__()\n        self.benchmodel = benchmodel\n        self.stride = stride\n        assert stride in [1, 2]\n\n        oup_inc = oup//2\n\n        if self.benchmodel == 1:\n            # assert inp == oup_inc\n            self.branch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                make_activation(activation),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                make_activation(activation),\n            )\n        else:\n            self.branch1 = nn.Sequential(\n                # dw\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                # pw-linear\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                make_activation(activation),\n            )\n\n            self.branch2 = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                make_activation(activation),\n                # dw\n                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                # pw-linear\n                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup_inc),\n                make_activation(activation),\n            )\n        self.init_weights()\n\n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), 1)\n\n    def init_weights(self):\n        for m in self.children():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        if self.benchmodel == 1:\n            x1 = x[:, :(x.shape[1]//2), :, :]\n            x2 = x[:, (x.shape[1]//2):, :, :]\n            out = self._concat(x1, self.branch2(x2))\n        elif self.benchmodel == 2:\n            out = self._concat(self.branch1(x), self.branch2(x))\n\n        return channel_shuffle(out, 2)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/data/__init__.py,0,b''
pytorch_toolkit/instance_segmentation/segmentoly/data/dataparallel.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel._functions import Scatter\n\n\nclass ScatterShallow(object):\n\n    @staticmethod\n    def recursive_apply(target_gpus, dim, input):\n        if isinstance(input, torch.Tensor):\n            return Scatter.apply(target_gpus, None, dim, input)[0]\n        elif isinstance(input, list):\n            return [ScatterShallow.recursive_apply(target_gpus, dim, i) for i in input]\n        elif isinstance(input, tuple):\n            return (ScatterShallow.recursive_apply(target_gpus, dim, i) for i in input)\n        elif isinstance(input, dict):\n            return {k: ScatterShallow.recursive_apply(target_gpus, dim, v) for k, v in input.items()}\n        return input\n\n    @staticmethod\n    def apply(target_gpus, dim, input):\n        # Output is list with size = number of GPUs.\n        output = [[] for _ in target_gpus]\n\n        chunk_size = len(input) // len(target_gpus)\n        chunk_indexes = np.full(len(target_gpus) + 1, 0, dtype=int)  # [0, 0, 0...]\n        chunk_indexes[1:] = chunk_size  # [0, n, n...]\n        chunk_indexes[1:len(input) % len(target_gpus) + 1] += 1  # [0, n + 1, n + 1, n, n...]\n        chunk_indexes = np.cumsum(chunk_indexes)  # [0, n1, n2, n3...]\n\n        chunks = []\n        for i, j in zip(chunk_indexes[:-1], chunk_indexes[1:]):\n            chunks.append(input[i:j])\n\n        used_gpus = 0\n        for gpu_id, chunk in enumerate(chunks):\n            target_gpu = target_gpus[gpu_id]\n            # Calculate number of GPUs which have got data\n            if len(chunk) > 0:\n                used_gpus += 1\n            output[gpu_id] = ScatterShallow.recursive_apply([target_gpu], dim, chunk)\n\n        return output[:used_gpus]\n\n\ndef scatter(inputs, target_gpus, dim=0):\n    """"""Reimplemented case if object is list, case for dict removed.\n    Other cases are the same as in the base class\n    """"""\n    def scatter_map(obj):\n        if isinstance(obj, torch.Tensor):\n            result = Scatter.apply(target_gpus, None, dim, obj)\n            return result\n        if isinstance(obj, list) and len(obj) > 0:\n            result = ScatterShallow.apply(target_gpus, dim, obj)\n            return result\n        # `inputs` is either a tuple for positional arguments or a dict for keyword arguments,\n        # so just recursively go deeper.\n        if isinstance(obj, tuple) and len(obj) > 0:\n            result = list(zip(*map(scatter_map, obj)))\n            return result\n        if isinstance(obj, dict) and len(obj) > 0:\n            keys_and_values = list(zip(*map(scatter_map, obj.items())))\n            result = list(map(type(obj), keys_and_values))\n            return result\n        return [obj for targets in target_gpus]\n\n    try:\n        return scatter_map(inputs)\n    finally:\n        scatter_map = None\n\n\ndef scatter_kwargs(inputs, kwargs, target_gpus, dim=0):\n    """"""This function is the same as in the base class `nn.DataParallel`\n    except using of reimplemented function `scatter`\n    """"""\n    inputs = scatter(inputs, target_gpus, dim) if inputs else []\n    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n    if len(inputs) < len(kwargs):\n        inputs.extend([() for _ in range(len(kwargs) - len(inputs))])\n    elif len(kwargs) < len(inputs):\n        kwargs.extend([{} for _ in range(len(inputs) - len(kwargs))])\n    inputs = tuple(inputs)\n    kwargs = tuple(kwargs)\n    return inputs, kwargs\n\n\nclass ShallowDataParallel(nn.DataParallel):\n    """"""Define custom method scatter\n    """"""\n    def __init__(self, module, device_ids=None, output_device=None, dim=0):\n        super().__init__(module, device_ids, output_device, dim)\n\n    def scatter(self, inputs, kwargs, device_ids):\n        return scatter_kwargs(inputs, kwargs, device_ids, self.dim)\n\n\ndef collate(samples):\n    batch = defaultdict(list)\n    for sample in samples:\n        for k, v in sample.items():\n            batch[k].append(v)\n    batch[\'batch_idx\'] = list(range(len(samples)))\n    return batch\n'"
pytorch_toolkit/instance_segmentation/segmentoly/data/transforms.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport random\n\nimport cv2\nimport numpy as np\nimport torch\nfrom torchvision.transforms.functional import normalize\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, sample):\n        for t in self.transforms:\n            sample = t(sample)\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'(\'\n        for t in self.transforms:\n            format_string += \'\\n\'\n            format_string += \'    {}\'.format(t)\n        format_string += \'\\n)\'\n        return format_string\n\n\nclass Resize(object):\n    def __init__(self, max_size=None, window_size=None, size=None):\n        assert int(max_size is not None) + int(window_size is not None) + int(size is not None) == 1\n\n        self.short_side_max = None\n        self.long_side_max = None\n        if max_size is not None:\n            self.short_side_max, self.long_side_max = max_size\n\n        self.height_max = None\n        self.width_max = None\n        if window_size is not None:\n            self.height_max, self.width_max = window_size\n\n        self.height = None\n        self.width = None\n        if size is not None:\n            self.height, self.width = size\n\n    def get_scale(self, image_size):\n        if self.height is not None:\n            scale_x, scale_y = self.width / image_size[1], self.height / image_size[0]\n        elif self.height_max is not None:\n            im_scale = min(self.height_max / image_size[0], self.width_max / image_size[1])\n            scale_x, scale_y = im_scale, im_scale\n        else:\n            im_scale = min(self.short_side_max / min(image_size), self.long_side_max / max(image_size))\n            scale_x, scale_y = im_scale, im_scale\n        return scale_x, scale_y\n\n    def __call__(self, sample):\n        image_size = sample[\'image\'].shape[:2]\n        scale_x, scale_y = self.get_scale(image_size)\n\n        # Resize image.\n        sample[\'image\'] = cv2.resize(sample[\'image\'], None, fx=scale_x, fy=scale_y)\n        h, w = sample[\'image\'].shape[:2]\n\n        # Resize boxes.\n        if \'gt_boxes\' in sample:\n            sample[\'gt_boxes\'] *= [scale_x, scale_y, scale_x, scale_y]\n            sample[\'gt_boxes\'] = np.clip(sample[\'gt_boxes\'], 0, [w - 1, h - 1, w - 1, h - 1])\n\n        # Resize masks.\n        if \'gt_masks\' in sample:\n            sample[\'gt_masks\'] = [[np.clip(part * [scale_x, scale_y], 0, [w - 1, h - 1]) for part in obj]\n                                  for obj in sample[\'gt_masks\']]\n\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'[\'\n        if self.height is not None:\n            format_string += \'size = [{}, {}]\'.format(self.height, self.width)\n        elif self.height_max is not None:\n            format_string += \'widow_size = [{}, {}]\'.format(self.height_max, self.width_max)\n        else:\n            format_string += \'max_size = [{}, {}]\'.format(self.short_side_max, self.long_side_max)\n        format_string += \']\'\n        return format_string\n\n\nclass RandomResize(object):\n    def __init__(self, mode, sizes=None, heights=None, widths=None):\n        assert mode in {\'max_size\', \'window_size\', \'size\'}\n        self.mode = mode\n        self.sizes = sizes\n        self.heights = heights\n        self.widths = widths\n\n        self.size = self.sizes[0] if self.sizes is not None else (self.heights[0], self.widths[0])\n\n    def update(self):\n        if self.sizes is not None:\n            self.size = self.sizes[int(random.random() * len(self.sizes))]\n        else:\n            height_idx = int(random.random() * len(self.heights))\n            width_idx = int(random.random() * len(self.widths))\n            self.size = (self.heights[height_idx], self.widths[width_idx])\n\n    def __call__(self, sample):\n        self.update()\n        resizer = Resize(**{self.mode: self.size})\n        return resizer(sample)\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'[\'\n        format_string += \'mode = {}\'.format(self.mode)\n        if self.size is not None:\n            format_string += \', sizes = {}\'.format(str(self.sizes))\n        else:\n            format_string += \', heights = {}, widths = {}\'.format(str(self.heights), str(self.widths))\n        format_string += \']\'\n        return format_string\n\n\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n        self.do_flip = False\n\n    def update(self):\n        self.do_flip = random.random() < self.prob\n\n    def __call__(self, sample):\n        self.update()\n        if self.do_flip:\n            sample[\'image\'] = cv2.flip(sample[\'image\'], 1)\n            width = sample[\'image\'].shape[1]\n\n            # Flip boxes.\n            if \'gt_boxes\' in sample:\n                boxes = sample[\'gt_boxes\']\n                boxes[:, 0], boxes[:, 2] = width - boxes[:, 2] - 1, width - boxes[:, 0] - 1\n                sample[\'gt_boxes\'] = boxes\n\n            # Flip masks.\n            if \'gt_masks\' in sample:\n                polygons = sample[\'gt_masks\']\n                for i, obj in enumerate(polygons):\n                    for j, part in enumerate(obj):\n                        polygons[i][j][:, 0] = width - polygons[i][j][:, 0] - 1\n                sample[\'gt_masks\'] = polygons\n\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'[prob = {}]\'.format(self.prob)\n        return format_string\n\n\nclass FilterByBoxSize(object):\n    def __init__(self, min_box_size, max_box_size):\n        self.min_box_size = list(min_box_size)\n        self.max_box_size = list(max_box_size)\n\n    def __call__(self, sample):\n        if \'gt_boxes\' in sample:\n            boxes = sample[\'gt_boxes\']\n            widths = boxes[:, 2] - boxes[:, 0] + 1\n            heights = boxes[:, 3] - boxes[:, 1] + 1\n            valid_boxes = np.logical_and(np.logical_and(self.min_box_size[0] < widths, widths < self.max_box_size[0]),\n                                         np.logical_and(self.min_box_size[1] < heights, heights < self.max_box_size[1]))\n            if not valid_boxes.any():\n                sample[\'gt_boxes\'] = np.zeros([1, 4], dtype=np.float32)\n                if \'gt_classes\' in sample:\n                    sample[\'gt_classes\'] = np.zeros(1)\n                if \'gt_is_ignored\' in sample:\n                    sample[\'gt_is_ignored\'] = np.ones(1)\n                if \'gt_masks\' in sample:\n                    sample[\'gt_masks\'] = [[np.zeros([4, 2], dtype=np.float32)], ]\n            else:\n                boxes = boxes[valid_boxes]\n                sample[\'gt_boxes\'] = boxes\n                if \'gt_classes\' in sample:\n                    sample[\'gt_classes\'] = sample[\'gt_classes\'][valid_boxes]\n                if \'gt_is_ignored\' in sample:\n                    sample[\'gt_is_ignored\'] = sample[\'gt_is_ignored\'][valid_boxes]\n                if \'gt_masks\' in sample:\n                    masks = sample[\'gt_masks\']\n                    sample[\'gt_masks\'] = list(mask for i, mask in enumerate(masks) if valid_boxes[i])\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__\n        format_string += \'[min = {}, max = {}]\'.format(str(self.min_box_size), str(self.max_box_size))\n        return format_string\n\n\nclass ToTensor(object):\n    def __call__(self, sample):\n        sample[\'image\'] = torch.as_tensor(sample[\'image\'], dtype=torch.float32).permute(2, 0, 1)\n        if \'gt_boxes\' in sample:\n            sample[\'gt_boxes\'] = torch.as_tensor(sample[\'gt_boxes\'], dtype=torch.float32)\n        if \'gt_classes\' in sample:\n            sample[\'gt_classes\'] = torch.as_tensor(sample[\'gt_classes\'])\n        if \'gt_is_ignored\' in sample:\n            sample[\'gt_is_ignored\'] = torch.as_tensor(sample[\'gt_is_ignored\'])\n        # Intentionally, do not convert segmentation polygons to Tensors\n        # cause ShallowDataParallel will distribute all Tensors between GPUs and we want\n        # these polygons to leave at a CPU side.\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__\n        return format_string\n\n\nclass Normalize(object):\n    def __init__(self, mean, std, rgb=False):\n        self.mean = mean\n        self.std = std\n        self.rgb = rgb\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n        if self.rgb:\n            image = torch.stack((image[2], image[1], image[0]), dim=0)\n        sample[\'image\'] = normalize(image, self.mean, self.std)\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \\\n                        \'[mean = {}, std = {}, rgb = {}]\'.format(str(list(self.mean)),\n                                                                 str(list(self.std)),\n                                                                 str(self.rgb))\n        return format_string\n'"
pytorch_toolkit/instance_segmentation/segmentoly/datasets/__init__.py,0,b'from .coco import COCODataset\nfrom .factory import get_dataset\nfrom .images import ImagesDataset\nfrom .instance_dataset import InstanceDataset\nfrom .video import VideoDataset\n'
pytorch_toolkit/instance_segmentation/segmentoly/datasets/coco.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport json\nimport os.path as osp\nimport tempfile\nfrom collections import OrderedDict\n\nimport cv2\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom .instance_dataset import InstanceDataset\n\n\nclass COCODataset(InstanceDataset):\n    def __init__(self, root_dir_path, ann_file_path, with_gt, remove_images_without_gt, transforms=None):\n        super().__init__(with_gt)\n\n        self.root_dir_path = root_dir_path\n        self.coco = COCO(ann_file_path)\n        self.ids = list(self.coco.imgs.keys())\n        self.ids = sorted(self.ids)\n        if remove_images_without_gt:\n            self.ids = [\n                img_id\n                for img_id in self.ids\n                if len(self.coco.getAnnIds(imgIds=img_id, iscrowd=False)) > 0\n            ]\n\n        self.transforms = transforms\n\n        # Set up dataset classes\n        category_ids = self.coco.getCatIds()\n        categories = [c[\'name\'] for c in self.coco.loadCats(category_ids)]\n        self.category_to_id_map = dict(zip(categories, category_ids))\n        self.classes = [\'__background__\'] + categories\n        self.classes_num = len(self.classes)\n        self.json_category_id_to_contiguous_id = {\n            v: i + 1 for i, v in enumerate(self.coco.getCatIds())\n        }\n        self.contiguous_category_id_to_json_id = {\n            v: k for k, v in self.json_category_id_to_contiguous_id.items()\n        }\n        self.contiguous_category_id_to_json_id[0] = 0\n\n    def __getitem__(self, index):\n        coco = self.coco\n        img_id = self.ids[index]\n        ann_ids = coco.getAnnIds(imgIds=img_id, iscrowd=False)\n        anno = coco.loadAnns(ann_ids)\n        path = coco.loadImgs(img_id)[0][\'file_name\']\n        original_image = cv2.imread(osp.join(self.root_dir_path, path))\n        original_image_size = original_image.shape[:2]\n        h, w = original_image_size\n\n        sample = {\'image\': original_image, }\n        if self.with_gt:\n            boxes = [obj[\'bbox\'] for obj in anno]\n            boxes = np.asarray(boxes, dtype=np.float32).reshape(-1, 4)\n            if len(boxes) > 0:\n                boxes[:, 2] += boxes[:, 0] - 1\n                boxes[:, 3] += boxes[:, 1] - 1\n                boxes = np.clip(boxes, 0, [w - 1, h - 1, w - 1, h - 1])\n                sample[\'gt_boxes\'] = boxes\n\n            classes = [obj[\'category_id\'] for obj in anno]\n            classes = [self.json_category_id_to_contiguous_id[c] for c in classes]\n            classes = np.asarray(classes)\n            assert len(classes) == len(boxes)\n            if len(classes) > 0:\n                sample[\'gt_classes\'] = classes\n\n            polygons = [obj[\'segmentation\'] for obj in anno]\n            polygons = [[np.clip(np.asarray(part, dtype=np.float32).reshape(-1, 2), 0, [[w - 1, h - 1]]) for part in obj]\n                        for obj in polygons]\n            assert len(polygons) == len(boxes)\n            if len(polygons) > 0:\n                sample[\'gt_masks\'] = polygons\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        IM_HEIGHT = 1\n        IM_WIDTH = 2\n        out_sample = dict(index=index,\n                          original_image=original_image,\n                          meta=dict(original_size=original_image_size,\n                                    processed_size=sample[\'image\'].shape[1:3]),\n                          im_data=sample[\'image\'],\n                          im_info=torch.as_tensor([sample[\'image\'].shape[IM_HEIGHT],\n                                                   sample[\'image\'].shape[IM_WIDTH],\n                                                   1.0],\n                                                  dtype=torch.float32))\n        if self.with_gt:\n            out_sample.update(dict(gt_boxes=sample[\'gt_boxes\'],\n                                   gt_labels=sample[\'gt_classes\'],\n                                   gt_masks=sample[\'gt_masks\'],\n                                   gt_is_ignored=torch.zeros(sample[\'gt_classes\'].shape, dtype=torch.int32)\n                                   ))\n        return out_sample\n\n    def get_image_info(self, idx):\n        image_id = self.ids[idx]\n        image_data = self.coco.imgs[image_id]\n        return image_data\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {:,}\\n\'.format(self.__len__())\n        fmt_str += \'    Number of classes: {}\\n\'.format(self.classes_num)\n        tmp = \'    Transforms: \'\n        fmt_str += \'{}{}\\n\'.format(tmp, self.transforms.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\n    def convert_predictions_to_coco_format(self, classes, scores, boxes, masks):\n\n        def convert_box(b):\n            x1, y1, x2, y2 = b\n            return [x1, y1, x2 - x1 + 1, y2 - y1 + 1]\n\n        coco_predictions = []\n        for i, (im_classes, im_scores, im_boxes, im_masks) in enumerate(zip(classes, scores, boxes, masks)):\n            image_id = self.ids[i]\n\n            if len(im_classes) == 0:\n                continue\n\n            im_boxes = im_boxes.tolist()\n            im_scores = im_scores.tolist()\n            im_classes = im_classes.tolist()\n            im_classes_ids = [self.contiguous_category_id_to_json_id[j] for j in im_classes]\n            encoded_masks = []\n            for mask in im_masks:\n                if \'counts\' in mask:\n                    encoded_masks.append(mask)\n                else:\n                    encoded_mask = mask_util.encode(np.array(mask[:, :, np.newaxis].astype(np.uint8), order=\'F\'))[0]\n                    encoded_mask[\'counts\'] = encoded_mask[\'counts\'].decode(\'utf-8\')\n                    encoded_masks.append(encoded_mask)\n\n            coco_predictions.extend(\n                [\n                    {\n                        \'image_id\': image_id,\n                        \'category_id\': class_id,\n                        \'bbox\': convert_box(box),\n                        \'segmentation\': encoded_mask,\n                        \'score\': score,\n                    }\n                    for class_id, score, box, encoded_mask in zip(im_classes_ids, im_scores, im_boxes, encoded_masks)\n                ]\n            )\n        return coco_predictions\n\n    def evaluate(self, scores, classes, boxes, masks, output_dir=\'\', iou_types=(\'bbox\', \'segm\')):\n        predictions_coco = self.convert_predictions_to_coco_format(classes, scores, boxes, masks)\n\n        results = COCOResults(*iou_types)\n        for iou_type in iou_types:\n            with tempfile.NamedTemporaryFile() as f:\n                file_path = f.name\n                if output_dir:\n                    file_path = osp.join(output_dir, iou_type + \'.json\')\n                res = evaluate_predictions_on_coco(self.coco, predictions_coco, file_path, iou_type)\n                results.update(res)\n        flat_results = {}\n        for iou_type, metrics in results.results.items():\n            for k, v in metrics.items():\n                flat_results[\'{}/{}\'.format(iou_type, k)] = v\n        return flat_results\n\n\ndef evaluate_predictions_on_coco(coco_gt, coco_results, json_result_file, iou_type=\'bbox\'):\n    with open(json_result_file, \'w\') as f:\n        json.dump(coco_results, f)\n\n    coco_dt = coco_gt.loadRes(str(json_result_file)) if coco_results else COCO()\n\n    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n    return coco_eval\n\n\nclass COCOResults(object):\n    METRICS = {\n        \'bbox\': [\'AP\', \'AP50\', \'AP75\', \'APs\', \'APm\', \'APl\'],\n        \'segm\': [\'AP\', \'AP50\', \'AP75\', \'APs\', \'APm\', \'APl\'],\n    }\n\n    def __init__(self, *iou_types):\n        allowed_types = (\'bbox\', \'segm\')\n        assert all(iou_type in allowed_types for iou_type in iou_types)\n        results = OrderedDict()\n        for iou_type in iou_types:\n            results[iou_type] = OrderedDict(\n                [(metric, -1) for metric in COCOResults.METRICS[iou_type]]\n            )\n        self.results = results\n\n    def update(self, coco_eval):\n        if coco_eval is None:\n            return\n\n        assert isinstance(coco_eval, COCOeval)\n        s = coco_eval.stats\n        iou_type = coco_eval.params.iouType\n        res = self.results[iou_type]\n        metrics = COCOResults.METRICS[iou_type]\n        for idx, metric in enumerate(metrics):\n            res[metric] = s[idx]\n\n    def __repr__(self):\n        return repr(self.results)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/datasets/factory.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\n\nfrom .coco import COCODataset\n\n\ndef get_dataset(name, with_gt, remove_images_without_gt, transforms=None,\n                root_data_dir=osp.join(osp.dirname(osp.abspath(__file__)), \'..\', \'..\', \'data\')):\n    coco_root = osp.join(root_data_dir, \'coco\')\n    if name == \'coco_2017_train\':\n        dataset = COCODataset(osp.join(coco_root, \'images\', \'train2017\'),\n                              osp.join(coco_root, \'annotations\', \'instances_train2017.json\'),\n                              with_gt, remove_images_without_gt, transforms)\n    elif name == \'coco_2017_val\':\n        dataset = COCODataset(osp.join(coco_root, \'images\', \'val2017\'),\n                              osp.join(coco_root, \'annotations\', \'instances_val2017.json\'),\n                              with_gt, remove_images_without_gt, transforms)\n    else:\n        raise ValueError(\'Invalid dataset identifier ""{}"".\'.format(name))\n    return dataset\n'"
pytorch_toolkit/instance_segmentation/segmentoly/datasets/images.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport os.path as osp\n\nimport cv2\nimport torch\n\nfrom .instance_dataset import InstanceDataset\n\n\nclass ImagesDataset(InstanceDataset):\n    def __init__(self, path, labels, extensions=None, skip_non_images=True, transforms=None):\n        super().__init__(with_gt=False)\n        self.classes = labels\n        self.classes_num = len(labels)\n\n        self.transforms = transforms\n\n        self.skip_non_images = skip_non_images\n        self.images = []\n        if osp.isdir(path):\n            self.images = list(osp.join(path, i)\n                               for i in sorted(os.listdir(path))\n                               if osp.isfile(osp.join(path, i)))\n        elif osp.isfile(path):\n            self.images = [path, ]\n        else:\n            raise ValueError(\'""path"" is neither an image file, not a directory with images.\')\n\n        if extensions is not None:\n            extensions = set(extensions)\n            self.images = [i for i in self.images\n                           if osp.splitext(i) in extensions]\n\n        self.pos = 0\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        image = None\n\n        read_one_more_image = True\n        while read_one_more_image:\n            if self.pos >= len(self.images):\n                raise StopIteration\n            try:\n                image_file_path = self.images[self.pos]\n                self.pos += 1\n                image = cv2.imread(image_file_path)\n                read_one_more_image = False\n            except Exception:\n                read_one_more_image = self.skip_non_images\n                image = None\n\n        if image is None:\n            raise StopIteration\n\n        processed_image = image\n        if self.transforms is not None:\n            processed_image = self.transforms({\'image\': image})[\'image\']\n\n        sample = dict(original_image=image,\n                      meta=dict(original_size=image.shape[:2],\n                                processed_size=processed_image.shape[1:3]),\n                      im_data=processed_image,\n                      im_info=torch.tensor([processed_image.shape[1], processed_image.shape[2], 1.0],\n                                           dtype=torch.float32))\n        return sample\n\n    def evaluate(self, *args, **kwargs):\n        pass\n'"
pytorch_toolkit/instance_segmentation/segmentoly/datasets/instance_dataset.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom abc import abstractmethod\n\nimport torch.utils.data as data\n\n\nclass InstanceDataset(data.Dataset):\n    def __init__(self, with_gt):\n        self.classes = ()\n        self.classes_num = 0\n        self.with_gt = with_gt\n\n    @abstractmethod\n    def evaluate(self, *args, **kwargs):\n        pass\n'"
pytorch_toolkit/instance_segmentation/segmentoly/datasets/video.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport sys\n\nimport cv2\nimport torch\n\nfrom .instance_dataset import InstanceDataset\n\n\nclass VideoDataset(InstanceDataset):\n    def __init__(self, path, labels, transforms=None):\n        super().__init__(with_gt=False)\n        self.classes = labels\n        self.classes_num = len(labels)\n\n        self.transforms = transforms\n\n        self.video = cv2.VideoCapture(path)\n        assert self.video.isOpened()\n        # Unfortunately, one frame lag is always there.\n        self.video.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n\n    def __len__(self):\n        return sys.maxsize\n\n    def __getitem__(self, index):\n        status, frame = self.video.read()\n        if not status:\n            self.video.release()\n            raise StopIteration\n\n        processed_image = frame\n        if self.transforms is not None:\n            processed_image = self.transforms({\'image\': frame})[\'image\']\n\n        sample = dict(original_image=frame,\n                      meta=dict(original_size=frame.shape[:2],\n                                processed_size=processed_image.shape[1:3]),\n                      im_data=processed_image,\n                      im_info=torch.tensor([processed_image.shape[1], processed_image.shape[2], 1.0],\n                                           dtype=torch.float32))\n        return sample\n\n    def evaluate(self, *args, **kwargs):\n        pass\n'"
pytorch_toolkit/instance_segmentation/segmentoly/extensions/__init__.py,0,b''
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/__init__.py,0,b''
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/base.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom torch import nn\n\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.dims_in = ()\n        self.scales_in = ()\n        self.dims_out = ()\n        self.scales_out = ()\n\n    def forward(self, *inputs):\n        raise NotImplementedError\n\n\ndef duplicate(x, n, copy=False):\n    if copy:\n        return list([x for _ in range(n)])\n    else:\n        return [x, ] * n\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/deformable_conv.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.nn.modules.utils import _pair\n\nfrom ..extensions._EXTRA import (deform_conv_forward_cuda, deform_conv_backward_input_cuda,\n                                 deform_conv_backward_parameters_cuda)\n\n\ndef conv_offset2d(input,\n                  offset,\n                  weight,\n                  stride=1,\n                  padding=0,\n                  dilation=1,\n                  deform_groups=1,\n                  kernel_size=1):\n\n    if input is not None and input.dim() != 4:\n        raise ValueError(\'Expected 4D tensor as input, got {}D tensor instead.\'.format(input.dim()))\n\n    return ConvOffset2dFunction.apply(input, offset, weight, _pair(stride),\n                                      _pair(padding), _pair(dilation), deform_groups, _pair(kernel_size))\n\n\nclass ConvOffset2dFunction(Function):\n\n    @staticmethod\n    def symbolic(g, input, offset, weight, stride=1, padding=0, dilation=1, deformable_groups=1, kernel_size=1):\n        return g.op(\'DeformableConv2D\', input, offset, weight,\n                    strides_i=stride, pads_i=[p for pair in zip(padding, padding) for p in pair],\n                    dilations_i=dilation, deformable_groups_i=deformable_groups, kernel_shape_i=kernel_size)\n\n    @staticmethod\n    def forward(ctx, input, offset, weight, stride=1, padding=0, dilation=1, deformable_groups=1, kernel_size=1):\n        ctx.stride = _pair(stride)\n        ctx.padding = _pair(padding)\n        ctx.dilation = _pair(dilation)\n        ctx.deformable_groups = deformable_groups\n\n        output = input.new(*ConvOffset2dFunction._output_size(input, weight, ctx.stride, ctx.padding, ctx.dilation))\n        bufs_ = [input.new_empty(input.shape), input.new_empty(input.shape)]  # columns, ones\n        ctx.save_for_backward(input, offset, weight, bufs_[0], bufs_[1])\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            if isinstance(input, torch.autograd.Variable):\n                if not isinstance(input.data, torch.cuda.FloatTensor):\n                    raise NotImplementedError\n            else:\n                if not isinstance(input, torch.cuda.FloatTensor):\n                    raise NotImplementedError\n            deform_conv_forward_cuda(\n                input, weight, offset, output, bufs_[0], bufs_[1],\n                weight.size(3), weight.size(2), ctx.stride[1], ctx.stride[0],\n                ctx.padding[1], ctx.padding[0], ctx.dilation[1], ctx.dilation[0],\n                ctx.deformable_groups)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, offset, weight, bufs0, bufs1 = ctx.saved_tensors\n        bufs_ = [bufs0, bufs1]\n\n        grad_input = grad_offset = grad_weight = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            if isinstance(grad_output, torch.autograd.Variable):\n                if not isinstance(grad_output.data, torch.cuda.FloatTensor):\n                    raise NotImplementedError\n            else:\n                if not isinstance(grad_output, torch.cuda.FloatTensor):\n                    raise NotImplementedError\n            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n                grad_input = input.new_zeros(input.size())\n                grad_offset = offset.new_zeros(offset.size())\n                deform_conv_backward_input_cuda(\n                    input, offset, grad_output, grad_input,\n                    grad_offset, weight, bufs_[0], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.deformable_groups)\n\n            if ctx.needs_input_grad[2]:\n                grad_weight = weight.new_zeros(weight.size())\n                deform_conv_backward_parameters_cuda(\n                    input, offset, grad_output,\n                    grad_weight, bufs_[0], bufs_[1], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.deformable_groups, 1)\n\n        return grad_input, grad_offset, grad_weight, None, None, None, None, None\n\n    @staticmethod\n    def _output_size(input, weight, stride, padding, dilation):\n        channels = weight.size(0)\n        output_size = (input.size(0), channels)\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = padding[d]\n            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1\n            str = stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // str + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\n                \'convolution input is too small (output would be {})\'.format(\n                    \'x\'.join(map(str, output_size))))\n        return output_size\n\n\nclass ConvOffset2d(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 num_deformable_groups=1):\n        super(ConvOffset2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.num_deformable_groups = num_deformable_groups\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels, *self.kernel_size))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, offset):\n        return conv_offset2d(input, offset, self.weight, self.stride,\n                             self.padding, self.dilation,\n                             self.num_deformable_groups, self.kernel_size)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/detection_output.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom .nms_function import nms\nfrom ..utils.boxes import bbox_transform, clip_boxes_to_image\nfrom ..utils.profile import Timer, DummyTimer\n\n\nclass DetectionOutputFunction(torch.autograd.Function):\n    _timers = defaultdict(DummyTimer)\n\n    @staticmethod\n    def symbolic(g, all_rois, all_box_deltas, all_cls_scores, im_info, batch_idx, num_classes,\n                 post_nms_count=2000, nms_threshold=0.7, score_threshold=0.01, max_detections_per_image=100,\n                 class_agnostic_box_regression=False, force_max_output_size=False):\n        return g.op(\'ExperimentalDetectronDetectionOutput\', all_rois, all_box_deltas, all_cls_scores, im_info,\n                    num_classes_i=num_classes,\n                    post_nms_count_i=post_nms_count, nms_threshold_f=nms_threshold, score_threshold_f=score_threshold,\n                    max_detections_per_image_i=max_detections_per_image,\n                    class_agnostic_box_regression_i=int(class_agnostic_box_regression),\n                    deltas_weights_f=[10, 10, 5, 5],\n                    max_delta_log_wh_f=np.log(1000. / 16.),\n                    outputs=4)\n\n    @staticmethod\n    def forward(ctx, all_rois, all_box_deltas, all_cls_scores, im_info, batch_idx, num_classes,\n                post_nms_count=2000, nms_threshold=0.7, score_threshold=0.01, max_detections_per_image=100,\n                class_agnostic_box_regression=False, force_max_output_size=False):\n        if isinstance(all_rois, torch.Tensor):\n            all_rois = [all_rois, ]\n        device = all_box_deltas.device\n        out_boxes = []\n        out_scores = []\n        out_classes = []\n\n        images_in_batch = im_info.size(0)\n        slice_point_start = 0\n        for image_idx in range(images_in_batch):\n            im_boxes = [torch.empty(0, 4, dtype=all_box_deltas.dtype, device=device), ]\n            im_scores = [torch.empty(0, dtype=all_box_deltas.dtype, device=device), ]\n            im_classes = [torch.empty(0, dtype=torch.long, device=device), ]\n            with DetectionOutputFunction._timers[\'detection_output.batch_mask\']:\n                rois = all_rois[image_idx]\n                image_rois_num = rois.shape[0]\n                slice_point_end = slice_point_start + image_rois_num\n                box_deltas = all_box_deltas[slice_point_start:slice_point_end]\n                cls_scores = all_cls_scores[slice_point_start:slice_point_end]\n                slice_point_start = slice_point_end\n\n            with DetectionOutputFunction._timers[\'detection_output.apply_deltas\']:\n                box_deltas = box_deltas.reshape([-1, box_deltas.shape[-1]])\n                if class_agnostic_box_regression:\n                    # Remove predictions for bg class (compatible with MSRA code)\n                    box_deltas = box_deltas[:, -4:]\n                pred_boxes = bbox_transform(rois, box_deltas, weights=(10., 10., 5., 5.))\n                pred_boxes = clip_boxes_to_image(pred_boxes.view(-1, 4),\n                                                 width=int(im_info[image_idx, 1]),\n                                                 height=int(im_info[image_idx, 0])).view(*pred_boxes.shape)\n                if class_agnostic_box_regression:\n                    pred_boxes = pred_boxes.repeat(1, num_classes + 1)\n\n            with DetectionOutputFunction._timers[\'detection_output.per_class_proc\']:\n                # Apply threshold on detection probabilities and apply NMS\n                score_mask = torch.transpose(cls_scores > score_threshold, 1, 0).contiguous()\n                # Skip j = 0, because it\'s the background class\n                for j in range(1, num_classes):\n                    with DetectionOutputFunction._timers[\'detection_output.score_thresholding\']:\n                        mask = score_mask[j]\n                        valid_boxes = torch.nonzero(mask).view(-1)\n                        if len(valid_boxes) == 0:\n                            continue\n                        scores = cls_scores[valid_boxes, j]\n                        boxes = pred_boxes[valid_boxes, j * 4:(j + 1) * 4]\n\n                    if nms_threshold > 0:\n                        with DetectionOutputFunction._timers[\'detection_output.nms\']:\n                            boxes, scores = nms(boxes, scores, nms_threshold)\n\n                    if post_nms_count > 0:\n                        with DetectionOutputFunction._timers[\'detection_output.post_nms_topN\']:\n                            boxes = boxes[:post_nms_count, :]\n                            scores = scores[:post_nms_count]\n\n                    with DetectionOutputFunction._timers[\'detection_output.concat_results\']:\n                        n = boxes.size(0)\n                        im_boxes.append(boxes)\n                        im_scores.append(scores)\n                        im_classes.append(torch.full((n,), j, dtype=torch.long, device=device))\n\n            im_boxes = torch.cat(im_boxes, dim=0)\n            im_scores = torch.cat(im_scores, dim=0)\n            im_classes = torch.cat(im_classes, dim=0)\n\n            boxes_num = im_boxes.shape[0]\n            if boxes_num > max_detections_per_image:\n                topk_indices = torch.topk(im_scores, max_detections_per_image)[1]\n                im_boxes = im_boxes[topk_indices]\n                im_scores = im_scores[topk_indices]\n                im_classes = im_classes[topk_indices]\n            elif force_max_output_size and boxes_num < max_detections_per_image:\n                extra_boxes_num = max_detections_per_image - boxes_num\n                im_boxes = torch.cat((im_boxes, im_boxes.new_zeros((extra_boxes_num, 4))), dim=0)\n                im_scores = torch.cat((im_scores, im_scores.new_zeros((extra_boxes_num,))), dim=0)\n                im_classes = torch.cat((im_classes, im_classes.new_zeros((extra_boxes_num,))), dim=0)\n\n            out_boxes.append(im_boxes)\n            out_scores.append(im_scores)\n            out_classes.append(im_classes)\n\n        if batch_idx is None:\n            out_batch_ids = torch.cat(tuple(torch.full((len(b),), i, device=b.device, dtype=torch.long)\n                                            for i, b in enumerate(out_boxes)), dim=0)\n        else:\n            out_batch_ids = torch.cat(tuple(torch.full((len(b),), i, device=b.device, dtype=torch.long)\n                                            for i, b in zip(batch_idx, out_boxes)), dim=0)\n        out_boxes = torch.cat(out_boxes, dim=0)\n        out_scores = torch.cat(out_scores, dim=0)\n        out_classes = torch.cat(out_classes, dim=0)\n\n        # print_timing_stats(DetectionOutputFunction._timers)\n\n        return out_boxes, out_classes, out_scores, out_batch_ids\n\n\ndef detection_output(all_rois, all_box_deltas, all_cls_scores, im_info, batch_idx,\n                     num_classes, post_nms_count=2000, nms_threshold=0.7, score_threshold=0.01,\n                     max_detections_per_image=100, class_agnostic_box_regression=False, force_max_output_size=False):\n    return DetectionOutputFunction.apply(all_rois, all_box_deltas, all_cls_scores, im_info, batch_idx, num_classes,\n                                         post_nms_count, nms_threshold, score_threshold, max_detections_per_image,\n                                         class_agnostic_box_regression, force_max_output_size)\n\n\nclass DetectionOutput(nn.Module):\n    def __init__(self, num_classes, post_nms_count=2000, nms_threshold=0.7, score_threshold=0.01,\n                 max_detections_per_image=100, class_agnostic_box_regression=False, force_max_output_size=False):\n        super().__init__()\n        self.force_max_output_size = force_max_output_size\n        self._num_classes = num_classes\n        self._post_nms_count = post_nms_count\n        self._nms_threshold = nms_threshold\n        self._score_threshold = score_threshold\n        self._max_detections_per_image = max_detections_per_image\n        self._class_agnostic_box_regression = class_agnostic_box_regression\n        self._timers = defaultdict(Timer)\n\n    def forward(self, all_rois, all_box_deltas, all_cls_scores, im_info, batch_idx=None):\n        return detection_output(all_rois, all_box_deltas, all_cls_scores, im_info, batch_idx, self._num_classes,\n                                self._post_nms_count, self._nms_threshold, self._score_threshold,\n                                self._max_detections_per_image, self._class_agnostic_box_regression,\n                                self.force_max_output_size)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/fpn.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\n\nfrom .base import FeatureExtractor, duplicate\nfrom .group_norm import GroupNorm\nfrom ..utils.weights import xavier_fill, get_group_gn\n\n\nclass TopDownLateral(nn.Module):\n    def __init__(self, dim_in_top, dim_in_lateral, dim_out, zero_init_lateral=False, group_norm=False):\n        super().__init__()\n        self.dim_in_top = dim_in_top\n        self.dim_in_lateral = dim_in_lateral\n        self.dim_out = dim_out\n        self.zero_init_lateral = zero_init_lateral\n        self.group_norm = group_norm\n\n        if self.group_norm:\n            self.conv_lateral = nn.Sequential(\n                nn.Conv2d(dim_in_lateral, self.dim_out, 1, 1, 0),\n                GroupNorm(get_group_gn(self.dim_out), self.dim_out, eps=1e-5)\n            )\n        else:\n            self.conv_lateral = nn.Conv2d(dim_in_lateral, self.dim_out, 1, 1, 0)\n\n        self._init_weights()\n\n    def forward(self, top_blob, lateral_blob):\n        # Lateral 1x1 conv\n        lat = self.conv_lateral(lateral_blob)\n        # Top-down 2x upsampling\n        factor = lat.shape[-1] // top_blob.shape[-1]\n        assert lat.shape[-2] // top_blob.shape[-2] == factor\n        td = F.interpolate(top_blob, scale_factor=int(factor), mode=\'nearest\') if factor > 1 else top_blob\n        # Sum lateral and top-down\n        lat += td\n        return lat\n\n    def _init_weights(self):\n        if self.group_norm:\n            conv = self.conv_lateral[0]\n        else:\n            conv = self.conv_lateral\n\n        if self.zero_init_lateral:\n            nn.init.constant_(conv.weight, 0)\n        else:\n            xavier_fill(conv.weight)\n        if conv.bias is not None:\n            nn.init.constant_(conv.bias, 0)\n\n\nclass FPN(FeatureExtractor):\n    def __init__(self, dims_in, scales_in, dims_internal, dims_out, topdown_lateral_block=TopDownLateral,\n                 group_norm=False, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dims_in = dims_in\n        self.scales_in = scales_in\n        self.dims_out = dims_out\n        self.scales_out = list(scales_in)\n        self.scales_out.append(self.scales_out[-1] * 2)\n        self.group_norm = group_norm\n\n        if not isinstance(dims_in, (tuple, list)):\n            dims_in = (dims_in, )\n        n = len(dims_in)\n        if not isinstance(dims_internal, (tuple, list)):\n            dims_internal = duplicate(dims_internal, len(dims_in))\n        if not isinstance(dims_out, (tuple, list)):\n            dims_out = duplicate(dims_out, len(dims_in))\n        assert len(dims_in) == len(dims_internal) == len(dims_out)\n        self.dims_out = dims_out\n\n        if self.group_norm:\n            self.conv_top = nn.Sequential(\n                nn.Conv2d(dims_in[-1], dims_internal[0], 1, 1, 0),\n                GroupNorm(get_group_gn(self.dims_out[0]), self.dims_out[0], eps=1e-5)\n            )\n        else:\n            self.conv_top = nn.Conv2d(dims_in[-1], dims_internal[0], 1, 1, 0)\n\n        self.topdown_lateral = nn.ModuleList()\n        self.posthoc = nn.ModuleList()\n        # Add top-down and lateral connections\n        for dim_in, dim_inside in zip(reversed(dims_in[:-1]), reversed(dims_internal[:-1])):\n            self.topdown_lateral.append(topdown_lateral_block(dim_inside, dim_in, dim_inside, group_norm=self.group_norm))\n        # Post-hoc scale-specific 3x3 convs\n        for dim_inside, dim_out in zip(dims_internal, dims_out):\n            if self.group_norm:\n                self.posthoc.append(nn.Sequential(\n                        nn.Conv2d(dim_inside, dim_out, 3, 1, 1),\n                        GroupNorm(get_group_gn(self.dims_out[0]), self.dims_out[0], eps=1e-5)\n                ))\n            else:\n                self.posthoc.append(nn.Conv2d(dim_inside, dim_out, 3, 1, 1))\n\n        self.extra_maxpool = nn.MaxPool2d(kernel_size=1, stride=2, padding=0)\n        self.init_weights()\n\n    def init_weights(self):\n        for m in (self.conv_top, *self.posthoc):\n            if isinstance(m, nn.Conv2d):\n                xavier_fill(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        if isinstance(x, Tensor):\n            x = [x, ]\n        assert len(self.dims_in) == len(x)\n\n        top_blob = self.conv_top(x[-1])\n        fpn_output_blobs = [self.posthoc[0](top_blob)]\n        for lateral_blob, topdown_lateral_module, posthoc_module in \\\n                zip(reversed(x[:-1]), self.topdown_lateral, self.posthoc[1:]):\n            top_blob = topdown_lateral_module(top_blob, lateral_blob)\n            fpn_output_blobs.append(posthoc_module(top_blob))\n        fpn_output_blobs = [self.extra_maxpool(fpn_output_blobs[0]), ] + fpn_output_blobs\n\n        return list(reversed(fpn_output_blobs))\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/group_norm.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\n\nclass GroupNormFunctionStub(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, input, num_groups, weight, bias, eps):\n        return g.op(\'ExperimentalDetectronGroupNorm\', input, weight, bias, num_groups_i=num_groups, eps_f=eps)\n\n    @staticmethod\n    def forward(ctx, input, num_groups, weight, bias, eps):\n        return input\n\n\ngroup_norm_stub = GroupNormFunctionStub.apply\n\n\nclass GroupNorm(torch.nn.GroupNorm):\n    def __init__(self, num_groups, num_channels, eps=1e-5, affine=True):\n        super().__init__(num_groups, num_channels, eps, affine)\n        self.use_stub = False\n\n    def forward(self, input):\n        if not self.use_stub:\n            return super().forward(input)\n        else:\n            return group_norm_stub(input, self.num_groups, self.weight, self.bias, self.eps)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/losses.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom torch import nn\n\n\ndef accuracy(predictions, targets):\n    return (targets.int() == predictions.int()).float().mean()\n\n\ndef precision(predictions, targets):\n    mask = predictions.int() == 1\n    return (targets[mask].int() == 1).sum().float() / mask.sum().float()\n\n\ndef recall(predictions, targets):\n    mask = targets.int() == 1\n    return (predictions[mask].int() == 1).sum().float() / mask.sum().float()\n\n\ndef rpn_loss_cls(cls_targets, cls_scores, reduction=\'valid_mean\'):\n    batch_size = len(cls_targets)\n    assert batch_size == cls_scores.shape[0]\n    assert reduction in (\'sum\', \'mean\', \'valid_mean\')\n\n    loss_per_image = [None for _ in range(batch_size)]\n    accuracy_per_image = []\n    precision_per_image = []\n    recall_per_image = []\n    non_negatives_num = 0.0\n    for idx in range(batch_size):\n        batch_cls_targets = cls_targets[idx]\n        with torch.no_grad():\n            weight = (batch_cls_targets >= 0).float()\n            non_negatives_num += weight.sum().item()\n        batch_cls_scores = cls_scores[idx].permute(1, 2, 0).reshape(-1)\n        loss = nn.functional.binary_cross_entropy_with_logits(batch_cls_scores, batch_cls_targets.float(), weight,\n                                                              reduction=\'mean\' if reduction == \'mean\' else \'sum\')\n        valid_mask = batch_cls_targets >= 0\n        pred = (batch_cls_scores > 0.5)[valid_mask].int()\n        gt = batch_cls_targets[valid_mask]\n        im_accuracy = accuracy(pred, gt)\n        accuracy_per_image.append(im_accuracy)\n        im_precision = precision(pred, gt)\n        precision_per_image.append(im_precision)\n        im_recall = recall(pred, gt)\n        recall_per_image.append(im_recall)\n        loss_per_image[idx] = loss\n\n    loss = torch.sum(torch.stack(loss_per_image, dim=0))\n    if reduction == \'valid_mean\':\n        non_negatives_num = max(non_negatives_num, 1.0)\n        loss /= non_negatives_num\n    return loss, \\\n           torch.mean(torch.stack(accuracy_per_image, dim=0)), \\\n           torch.mean(torch.stack(precision_per_image, dim=0)), \\\n           torch.mean(torch.stack(recall_per_image, dim=0))\n\n\ndef rpn_loss_reg(cls_targets, reg_targets, box_deltas, reduction=\'valid_mean\'):\n    batch_size = len(cls_targets)\n    assert batch_size == len(reg_targets)\n    assert batch_size == box_deltas.shape[0]\n    assert reduction in (\'sum\', \'valid_mean\')\n\n    loss_per_image = [None for _ in range(batch_size)]\n    non_negatives_num = 0.0\n    for idx in range(batch_size):\n        batch_rpn_bbox_deltas = box_deltas[idx, ...].permute(1, 2, 0).reshape(-1, 4)\n        with torch.no_grad():\n            batch_cls_targets = cls_targets[idx]\n            positive_samples_mask = batch_cls_targets > 0\n            non_negatives_num += (batch_cls_targets >= 0).sum().item()\n        loss = smooth_l1_loss(batch_rpn_bbox_deltas[positive_samples_mask],\n                              reg_targets[idx][positive_samples_mask], None, None,\n                              beta=1 / 9, normalize=False)\n        loss_per_image[idx] = loss\n    loss = torch.sum(torch.stack(loss_per_image, dim=0))\n    if reduction == \'valid_mean\':\n        non_negatives_num = max(non_negatives_num, 1.0)\n        loss /= non_negatives_num\n    return loss\n\n\ndef detection_loss_cls(box_class_scores, classification_targets):\n    all_targets = torch.cat(classification_targets)\n    assert all_targets.shape[0] == box_class_scores.shape[0]\n    loss = nn.functional.cross_entropy(box_class_scores, all_targets, ignore_index=-1)\n    return loss\n\n\ndef detection_loss_reg(box_deltas, classification_targets, regression_targets, class_agnostic_regression=False):\n    device = box_deltas.device\n\n    all_cls_targets = torch.cat(classification_targets)\n    all_reg_targets = torch.cat(regression_targets, dim=0)\n    assert all_cls_targets.shape[0] == all_reg_targets.shape[0]\n    assert all_cls_targets.shape[0] == box_deltas.shape[0]\n\n    # Regression loss is computed only for positive boxes.\n    valid_boxes = all_cls_targets > 0\n    valid_cls_targets = all_cls_targets[valid_boxes]\n    valid_reg_targets = all_reg_targets[valid_boxes, :]\n    deltas = box_deltas[valid_boxes, :]\n\n    if deltas.numel() == 0:\n        return torch.tensor(0, dtype=torch.float32, device=device, requires_grad=False)\n    if class_agnostic_regression:\n        loss = smooth_l1_loss(deltas, valid_reg_targets, None, None,\n                              beta=1 / 9, normalize=True)\n    else:\n        with torch.no_grad():\n            box_coordinates_num = 4\n            boxes_num = deltas.shape[0]\n            classes_num = deltas.shape[1] // box_coordinates_num\n            mask = torch.zeros((boxes_num, classes_num), dtype=torch.bool, device=device)\n            idx = torch.stack((torch.arange(boxes_num, device=device), valid_cls_targets), dim=1).t_()\n            mask.index_put_(tuple(idx), torch.tensor([1], dtype=torch.bool, device=device))\n            # Mask that selects target regression values (4 values corresponding to a box of a ground truth class)\n            # from the whole output blob.\n            expanded_mask = mask.unsqueeze(-1).expand(boxes_num, classes_num, box_coordinates_num).reshape(\n                boxes_num, -1)\n        loss = smooth_l1_loss(torch.masked_select(deltas, expanded_mask).reshape(boxes_num, box_coordinates_num),\n                              valid_reg_targets, None, None, beta=1, normalize=False)\n        loss /= all_reg_targets.shape[0]\n\n    return loss\n\n\ndef mask_loss(mask_predictions, classification_targets, mask_targets):\n    gt_labels = classification_targets\n    if isinstance(gt_labels, (list, tuple)):\n        gt_labels = torch.cat(gt_labels)\n    gt_masks = mask_targets\n    if isinstance(gt_masks, (list, tuple)):\n        gt_masks = torch.cat(gt_masks, dim=0)\n    rois_num = mask_predictions.shape[0]\n    masks = mask_predictions[torch.arange(rois_num, device=mask_predictions.device), gt_labels]\n    weight = (gt_masks > -1).float()\n    loss = torch.nn.functional.binary_cross_entropy_with_logits(masks.view(-1), gt_masks.view(-1),\n                                                                weight.view(-1), reduction=\'sum\')\n    loss /= weight.sum()\n    return loss\n\n\ndef smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights=None, bbox_outside_weights=None,\n                   beta=1.0, normalize=True):\n    """"""\n    SmoothL1(x) = 0.5 * x^2 / beta      if |x| < beta\n                  |x| - 0.5 * beta      otherwise.\n    1 / n * sum_i alpha_out[i] * SmoothL1(alpha_in[i] * (y_hat[i] - y[i])).\n    n is the number of batch elements in the input predictions\n    """"""\n    box_diff = bbox_pred - bbox_targets\n    if bbox_inside_weights is not None:\n        box_diff = bbox_inside_weights * box_diff\n    abs_in_box_diff = torch.abs(box_diff)\n    smooth_l1_sign = (abs_in_box_diff < beta).detach().float()\n    loss = smooth_l1_sign * 0.5 * torch.pow(box_diff, 2) / beta + \\\n           (1 - smooth_l1_sign) * (abs_in_box_diff - (0.5 * beta))\n    if bbox_outside_weights is not None:\n        loss = bbox_outside_weights * loss\n    n = 1\n    if normalize:\n        n = loss.size(0)  # batch size\n    loss_box = loss.view(-1).sum(0) / n\n    return loss_box\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/nms_function.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\nfrom ..extensions._EXTRA import nms as nms_impl\n\n\nclass NMSFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, boxes, scores, threshold):\n        if scores.dim() == 1:\n            scores = scores.unsqueeze(1)\n        keep = nms_impl(torch.cat((boxes, scores), dim=1), threshold)\n        return boxes[keep, :], scores.squeeze(1)[keep]\n\n    @staticmethod\n    def backward(ctx, boxes):\n        raise NotImplementedError\n\n\nnms = NMSFunction.apply\n\n\n# # Original author: Francisco Massa:\n# # https://github.com/fmassa/object-detection.torch\n# # Ported to PyTorch by Max deGroot (02/01/2017)\n# def nms(boxes, scores, overlap=0.5, top_k=200):\n#     """"""Apply non-maximum suppression at test time to avoid detecting too many\n#     overlapping bounding boxes for a given object.\n#     Args:\n#         boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n#         scores: (tensor) The class predscores for the img, Shape:[num_priors].\n#         overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n#         top_k: (int) The Maximum number of box preds to consider.\n#     Return:\n#         The indices of the kept boxes with respect to num_priors.\n#     """"""\n#\n#     scores = scores.view(-1)\n#     keep = torch.zeros_like(scores, dtype=torch.long)\n#     # keep = scores.new(scores.size(0)).zero_().long()\n#     if boxes.numel() == 0:\n#         return keep, 0\n#     x1 = boxes[:, 0]\n#     y1 = boxes[:, 1]\n#     x2 = boxes[:, 2]\n#     y2 = boxes[:, 3]\n#     area = torch.mul(x2 - x1, y2 - y1)\n#     v, idx = scores.sort()  # sort in ascending order\n#     # I = I[v >= 0.01]\n#     idx = idx[-top_k:]  # indices of the top-k largest vals\n#     xx1 = boxes.new()\n#     yy1 = boxes.new()\n#     xx2 = boxes.new()\n#     yy2 = boxes.new()\n#     w = boxes.new()\n#     h = boxes.new()\n#\n#     # keep = torch.Tensor()\n#     count = 0\n#     while idx.numel() > 0:\n#         i = idx[-1]  # index of current largest val\n#         # keep.append(i)\n#         keep[count] = i\n#         count += 1\n#         if idx.size(0) == 1:\n#             break\n#         idx = idx[:-1]  # remove kept element from view\n#         # load bboxes of next highest vals\n#         torch.index_select(x1, 0, idx, out=xx1)\n#         torch.index_select(y1, 0, idx, out=yy1)\n#         torch.index_select(x2, 0, idx, out=xx2)\n#         torch.index_select(y2, 0, idx, out=yy2)\n#         # store element-wise max with next highest score\n#         xx1 = torch.clamp(xx1, min=x1[i])\n#         yy1 = torch.clamp(yy1, min=y1[i])\n#         xx2 = torch.clamp(xx2, max=x2[i])\n#         yy2 = torch.clamp(yy2, max=y2[i])\n#         w.resize_as_(xx2)\n#         h.resize_as_(yy2)\n#         w = xx2 - xx1\n#         h = yy2 - yy1\n#         # check sizes of xx1 and xx2.. after each iteration\n#         w = torch.clamp(w, min=0.0)\n#         h = torch.clamp(h, min=0.0)\n#         inter = w * h\n#         # IoU = i / (area(a) + area(b) - i)\n#         rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n#         union = (rem_areas - inter) + area[i]\n#         IoU = inter / union  # store result in iou\n#         # keep only elements with an IoU <= overlap\n#         idx = idx[IoU.le(overlap)]\n#     return keep, count\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/openvino_net.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\n\nimport numpy as np\n\nfrom ..utils.blob import to_numpy\nfrom ..utils.profile import PerformanceCounters\n\n\nclass OpenVINONet(object):\n\n    def __init__(self, xml_file_path, bin_file_path, device=\'CPU\',\n                 plugin_dir=None, cpu_extension_lib_path=None, collect_perf_counters=False):\n\n        from openvino.inference_engine import IENetwork, IEPlugin\n\n        logging.info(\'Creating {} plugin...\'.format(device))\n        self.plugin = IEPlugin(device=device, plugin_dirs=plugin_dir)\n        if cpu_extension_lib_path and \'CPU\' in device:\n            logging.info(\'Adding CPU extensions...\')\n            self.plugin.add_cpu_extension(cpu_extension_lib_path)\n\n        # Read IR\n        logging.info(\'Reading network from IR...\')\n        self.net = IENetwork(model=xml_file_path, weights=bin_file_path)\n\n        if self.plugin.device == \'CPU\':\n            logging.info(\'Check that all layers are supported...\')\n            supported_layers = self.plugin.get_supported_layers(self.net)\n            not_supported_layers = [l for l in self.net.layers.keys() if l not in supported_layers]\n            if len(not_supported_layers) != 0:\n                unsupported_info = \'\\n\\t\'.join(\'{} ({} with params {})\'.format(layer_id,\n                                                                               self.net.layers[layer_id].type,\n                                                                               str(self.net.layers[layer_id].params))\n                                               for layer_id in not_supported_layers)\n                logging.warning(\'Following layers are not supported \'\n                                \'by the plugin for specified device {}:\'\n                                \'\\n\\t{}\'.format(self.plugin.device, unsupported_info))\n                logging.warning(\'Please try to specify cpu extensions library path.\')\n                raise ValueError(\'Some of the layers are not supported.\')\n\n        logging.info(\'Loading network to plugin...\')\n        self.exec_net = self.plugin.load(network=self.net, num_requests=1)\n\n        self.perf_counters = None\n        if collect_perf_counters:\n            self.perf_counters = PerformanceCounters()\n\n    def __call__(self, inputs):\n        outputs = self.exec_net.infer(inputs)\n        if self.perf_counters:\n            perf_counters = self.exec_net.requests[0].get_perf_counts()\n            self.perf_counters.update(perf_counters)\n        return outputs\n\n    def print_performance_counters(self):\n        if self.perf_counters:\n            self.perf_counters.print()\n\n    def __del__(self):\n        del self.net\n        del self.exec_net\n        del self.plugin\n\n\nclass MaskRCNNOpenVINO(OpenVINONet):\n    def __init__(self, *args, **kwargs):\n        super(MaskRCNNOpenVINO, self).__init__(*args, **kwargs)\n        required_input_keys = {\'im_data\', \'im_info\'}\n        assert required_input_keys == set(self.net.inputs.keys())\n        required_output_keys = {\'boxes\', \'scores\', \'classes\', \'raw_masks\'}\n        assert required_output_keys.issubset(self.net.outputs.keys())\n\n        self.n, self.c, self.h, self.w = self.net.inputs[\'im_data\'].shape\n        assert self.n == 1, \'Only batch 1 is supported.\'\n\n    def __call__(self, im_data, im_info, **kwargs):\n        im_data = to_numpy(im_data[0])\n        im_info = to_numpy(im_info[0])\n        if (self.h - im_data.shape[1] < 0) or (self.w - im_data.shape[2] < 0):\n            raise ValueError(\'Input image should resolution of {}x{} or less, \'\n                             \'got {}x{}.\'.format(self.w, self.h, im_data.shape[2], im_data.shape[1]))\n        im_data = np.pad(im_data, ((0, 0),\n                                   (0, self.h - im_data.shape[1]),\n                                   (0, self.w - im_data.shape[2])),\n                         mode=\'constant\', constant_values=0).reshape(1, self.c, self.h, self.w)\n        im_info = im_info.reshape(1, *im_info.shape)\n        output = super().__call__(dict(im_data=im_data, im_info=im_info))\n\n        classes = output[\'classes\']\n        valid_detections_mask = classes > 0\n        classes = classes[valid_detections_mask]\n        boxes = output[\'boxes\'][valid_detections_mask]\n        scores = output[\'scores\'][valid_detections_mask]\n        masks = output[\'raw_masks\'][valid_detections_mask]\n        return boxes, classes, scores, np.full(len(classes), 0, dtype=np.int32), masks\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/panet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom .base import FeatureExtractor, duplicate\nfrom .deformable_conv import ConvOffset2d\nfrom .group_norm import GroupNorm\nfrom ..utils.weights import xavier_fill, msra_fill, get_group_gn\n\n\nclass PANet(FeatureExtractor):\n    """"""Base class for PANet includes common methods for its features""""""\n    def __init__(self, group_norm=True):\n        """"""\n        :param group_norm: if True, 2d convolutions are continued by group normalization layer\n        """"""\n        super().__init__()\n        self.group_norm = group_norm\n\n    def forward(self, *inputs, use_stub=False):\n        raise NotImplementedError\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, ConvOffset2d)):\n                msra_fill(m.weight)\n                if hasattr(m, \'bias\') and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                xavier_fill(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def _conv2d_block(self, dim_in, dim_out, kernel, stride, padding, bias):\n        if self.group_norm:\n            return nn.Sequential(\n                nn.Conv2d(dim_in, dim_out, kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n                GroupNorm(get_group_gn(dim_out), dim_out, eps=1e-5),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(dim_in, dim_out, kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n                nn.ReLU(inplace=True)\n            )\n\n    def _linear_block(self, dim_in, dim_out):\n        if self.group_norm:\n            return nn.Sequential(\n                nn.Linear(dim_in, dim_out),\n                GroupNorm(get_group_gn(dim_out), dim_out, eps=1e-5)\n            )\n        else:\n            return nn.Linear(dim_in, dim_out)\n\n    def _gn_relu_block(self, channels):\n        if self.group_norm:\n            return nn.Sequential(\n                GroupNorm(get_group_gn(channels), channels, eps=1e-5),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            return nn.ReLU(inplace=True)\n\n\nclass BottomUpPathAugmentation(PANet):\n    """"""Feature bottom-up-path-augmentation with usual 2d convolutions 3x3""""""\n    def __init__(self, output_levels, dims_in, scales_in, dim_out, group_norm):\n        """"""\n        Initialization with the next parameters:\n        :param output_levels: number of levels (feature maps) from FPN\n        :param dims_in: number of channels in input feature maps\n        :param scales_in: scale of each input feature map\n        :param dim_out: number of channels in output feature maps\n        :param group_norm: if True, 2d convolutions are continued by group normalization layer\n        """"""\n        super().__init__(group_norm)\n        self.output_levels = output_levels\n        self.conv1 = nn.ModuleList()\n        self.conv2 = nn.ModuleList()\n        self.maxpool = nn.MaxPool2d(kernel_size=1, stride=2, padding=0)\n\n        self.scales_in = scales_in\n        self.scales_out = scales_in\n        self.dims_in = dims_in\n        self.dim_out = dim_out\n        dims_out = dim_out\n        if not isinstance(dims_out, (tuple, list)):\n            dims_out = duplicate(dims_out, len(dims_in))\n        self.dims_out = dims_out\n\n        for i in range(self.output_levels - 2):\n            self.conv1.append(\n                self._conv2d_block(dims_in[i], dim_out, kernel=3, stride=2, padding=1, bias=False)\n            )\n            self.conv2.append(\n                self._conv2d_block(dims_in[i], dim_out, kernel=3, stride=1, padding=1, bias=False)\n            )\n        self._init_weights()\n\n    def forward(self, x, use_stub=False):\n        """"""\n        :param x: (list) FPN output in order [P2, P3, P4, P5, P6]\n        :return: list [N2, N3, N4, N5, N6], where N2 == P2, N6 = max pooling from N5.\n                 Ni = ((Ni-1 -> conv1 -> ReLU) + Pi) -> conv2 -> ReLU if 2 < i < 6\n        """"""\n        out = []\n        out.append(x[0])  # N2 = P2 from FPN\n        for i in range(self.output_levels - 2):  # Except 2 levels: P2 and P6\n            out.append(self.conv1[i](out[-1]))\n            out[-1] = out[-1] + x[i + 1]\n            out[-1] = self.conv2[i](out[-1])\n        # Apply max pooling to N5\n        out.append(self.maxpool(out[-1]))\n        return out\n\n\nclass BottomUpPathAugmentationWithDeformConv(PANet):\n    """"""Feature bottom-up-path-augmentation where 2d convolutions 3x3 are replaced by\n    deformable convolutions with the same parameters (kernel_size, stride, padding, bias)\n    """"""\n    def __init__(self, output_levels, dims_in, scales_in, dim_out, group_norm, num_deformable_groups=1):\n        """"""\n        Initialization with the next parameters:\n        :param output_levels: number of levels (feature maps) from FPN\n        :param dims_in: number of channels in input feature maps\n        :param dim_out: number of channels in output feature maps\n        :param group_norm: if set, 2d convolutions are continued by group normalization layer\n        :param num_deformable_groups:\n        """"""\n        super().__init__(group_norm)\n        self.output_levels = output_levels\n        self.num_deformable_groups = num_deformable_groups\n\n        self.offset1 = nn.ModuleList()\n        self.conv1 = nn.ModuleList()\n        self.gn_relu1 = nn.ModuleList()\n\n        self.offset2 = nn.ModuleList()\n        self.conv2 = nn.ModuleList()\n        self.gn_relu2 = nn.ModuleList()\n\n        self.scales_in = scales_in\n        self.scales_out = scales_in\n        self.dims_in = dims_in\n        self.dim_out = dim_out\n        dims_out = dim_out\n        if not isinstance(dims_out, (tuple, list)):\n            dims_out = duplicate(dims_out, len(dims_in))\n        self.dims_out = dims_out\n\n        self.maxpool = nn.MaxPool2d(kernel_size=1, stride=2, padding=0)\n\n        deform_out_dim = self.num_deformable_groups * 2 * 3 * 3\n\n        for i in range(self.output_levels - 2):\n            # Conv1 block\n            self.offset1.append(nn.Conv2d(dims_in[i], deform_out_dim, kernel_size=3, stride=2, padding=1, bias=False))\n            self.conv1.append(ConvOffset2d(dims_in[i], dim_out, kernel_size=(3, 3), stride=2, padding=1,\n                                           num_deformable_groups=self.num_deformable_groups))\n            self.gn_relu1.append(self._gn_relu_block(dim_out))\n\n            # Conv2 block\n            self.offset2.append(nn.Conv2d(dims_in[i], deform_out_dim, kernel_size=3, stride=1, padding=1, bias=False))\n            self.conv2.append(ConvOffset2d(dims_in[i], dim_out, kernel_size=(3, 3), stride=1, padding=1,\n                                           num_deformable_groups=self.num_deformable_groups))\n            self.gn_relu2.append(self._gn_relu_block(dim_out))\n\n        self._init_weights()\n\n    def _init_weights(self):\n        super()._init_weights()\n        for i in range(self.output_levels - 2):\n            nn.init.constant_(self.offset1[i].weight, 0)\n            nn.init.constant_(self.offset2[i].weight, 0)\n\n    def forward(self, x):\n        """"""\n        :param x: (list) FPN output in order [P2, P3, P4, P5, P6]\n        :return: list [N2, N3, N4, N5, N6], where N2 == P2, N6 = max pooling from N5.\n                 Ni = ((Ni-1 -> offset1) -> conv1 -> ReLU + Pi) -> offset2 -> conv2 -> ReLU if 2 < i < 6\n        """"""\n        out = []\n        out.append(x[0])  # N2 = P2 from FPN\n        for i in range(self.output_levels - 2):  # Except 2 levels: P2 and P6\n            offset = self.offset1[i](out[-1])\n            out.append(self.conv1[i](out[-1], offset))\n            out[-1] = self.gn_relu1[i](out[-1])\n\n            out[-1] = out[-1] + x[i + 1]\n\n            offset = self.offset2[i](out[-1])\n            out[-1] = self.conv2[i](out[-1], offset)\n            out[-1] = self.gn_relu2[i](out[-1])\n        # Apply max pooling to N5\n        out.append(self.maxpool(out[-1]))\n        return out\n\n\n\nclass BboxHead(PANet):\n    """"""Detection head with features from PANet""""""\n    def __init__(self, dim_in, dim_out, resolution_in, cls_num, cls_agnostic_bbox_regression=False,\n                 afp_levels_num=4, heavier_head=False, conv_head_dim=256, num_convs=4,\n                 group_norm=False):\n        """"""\n        Initialization with the next parameters:\n        :param dim_in: channels number in input tensor\n        :param dim_out: channels number in output tensor\n        :param resolution_in: spatial resolution of input tensor\n        :param cls_num: number of classes to train\n        :param cls_agnostic_bbox_regression: Use a class agnostic bounding box\n               regressor instead of the default per-class regressor\n        :param afp_levels_num: number of levels (feature maps) from FPN or bottom-up-path-augmentation\n        :param heavier_head: if True, heavier-head feature from PANet is switched on\n        :param conv_head_dim: channels number in convolutions used in heavier_head\n        :param num_convs: number of convolutions in heavier_head, by default equal to 4\n        :param group_norm: if True, 2d convolutions are continued by group normalization layer\n        """"""\n        super().__init__(group_norm)\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        self.resolution_in = resolution_in\n        self.cls_num = cls_num\n        self.cls_agnostic_bbox_regression = cls_agnostic_bbox_regression\n\n        self.heavier_head = heavier_head\n        self.levels_num = afp_levels_num\n\n        # Define parameters for heavier_head\n        if heavier_head:\n            self.num_convs = num_convs\n            self.conv_head_dim = conv_head_dim\n\n        self.fc1 = nn.ModuleList()\n        self.define_fc1()\n\n        self.fc2 = None\n        self.define_fc2()\n\n        if self.heavier_head:\n            self.fc = nn.Linear(dim_in * resolution_in * resolution_in, dim_out)\n\n        self.cls_score = nn.Linear(dim_out, cls_num)\n        box_out_dims = 4 * (1 if cls_agnostic_bbox_regression else cls_num)\n        self.bbox_pred = nn.Linear(dim_out, box_out_dims)\n\n        self._init_weights()\n\n    def define_fc1(self):\n        for _ in range(self.levels_num):\n            if not self.heavier_head:\n                self.fc1.append(\n                    self._linear_block(self.dim_in * self.resolution_in * self.resolution_in, self.dim_out)\n                )\n            else:\n                self.fc1.append(\n                    self._conv2d_block(self.dim_in, self.conv_head_dim, kernel=3, stride=1, padding=1, bias=False)\n                )\n\n    def define_fc2(self):\n        if not self.heavier_head:\n            if self.group_norm:\n                self.fc2 = nn.Sequential(\n                    nn.Linear(self.dim_out, self.dim_out),\n                    GroupNorm(get_group_gn(self.dim_out), self.dim_out, eps=1e-5),\n                    nn.ReLU(inplace=True)\n                )\n            else:\n                self.fc2 = nn.Sequential(\n                    nn.Linear(self.dim_out, self.dim_out),\n                    nn.ReLU(inplace=True)\n                )\n        else:\n            module_list = []\n            for i in range(self.num_convs - 1):\n                module_list.extend(\n                    self._conv2d_block(self.dim_in, self.conv_head_dim, kernel=3, stride=1, padding=1, bias=False)\n                )\n            self.fc2 = nn.Sequential(*module_list)\n\n    def _init_weights(self):\n        super()._init_weights()\n        nn.init.normal_(self.cls_score.weight, std=0.01)\n        nn.init.constant_(self.cls_score.bias, 0)\n        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n        nn.init.constant_(self.bbox_pred.bias, 0)\n\n    def forward(self, x):\n        batch_size = int(x.shape[1])\n        for i in range(self.levels_num):\n            if self.heavier_head:\n                y = self.fc1[i](x[i])\n            else:\n                y = nn.functional.relu(self.fc1[i](x[i].view(batch_size, -1)), inplace=True)\n\n            if i == 0:\n                pooled_feature = y\n            else:\n                pooled_feature = torch.max(pooled_feature, y)\n\n        x = self.fc2(pooled_feature)\n\n        if self.heavier_head:\n            x = nn.functional.relu(self.fc(x.view(batch_size, -1)), inplace=True)\n\n        cls_score = self.cls_score(x)\n        if not self.training:\n            cls_score = nn.functional.softmax(cls_score, dim=1)\n        bbox_pred = self.bbox_pred(x)\n\n        return cls_score, bbox_pred\n\n\nclass MaskHead(PANet):\n    """"""Segmentation head with features from PANet""""""\n    def __init__(self, dim_in, num_cls, dim_internal=256,\n                 afp_levels_num=4, fully_connected_fusion=True, in_resolution=14,\n                 group_norm=False):\n        """"""\n        Initialization with the next parameters:\n        :param dim_in: number of channels in an input tensor\n        :param num_cls: number of classes\n        :param dim_internal: number of output channels in convolutions\n        :param afp_levels_num: number of levels (feature maps) from FPN or bottom-up-path-augmentation\n        :param fully_connected_fusion: if True, fully-connected-fusion feature from PANet is switched on\n        :param in_resolution: spatial resolution of input tensor\n        :param group_norm: if True, 2d convolutions are continued by group normalization layer\n        """"""\n        super().__init__(group_norm)\n        self.dim_in = dim_in\n        self.dim_out = dim_internal\n        self.out_resolution = in_resolution * 2\n\n        self.fully_connected_fusion = fully_connected_fusion\n        self.levels_num = afp_levels_num\n\n        # conv1 for every feature map after ROIAlign\n        self.conv1 = nn.ModuleList()\n        for i in range(self.levels_num):\n            self.conv1.append(self._conv2d_block(dim_in, dim_internal, kernel=3, stride=1, padding=1, bias=False))\n\n        # Other convs for mask head excluding conv1 as in original Mask-RCNN\n        self.conv2 = self._conv2d_block(dim_internal, dim_internal, kernel=3, stride=1, padding=1, bias=False)\n        self.conv3 = self._conv2d_block(dim_internal, dim_internal, kernel=3, stride=1, padding=1, bias=False)\n        self.conv4 = self._conv2d_block(dim_internal, dim_internal, kernel=3, stride=1, padding=1, bias=False)\n\n        self.upconv = nn.ConvTranspose2d(dim_internal, dim_internal, kernel_size=2, stride=2, padding=0)\n        self.segm = nn.Conv2d(dim_internal, num_cls, 1, 1, 0)\n\n        if self.fully_connected_fusion:\n            self.conv4_fc = self._conv2d_block(dim_internal, dim_internal, kernel=3, stride=1, padding=1, bias=False)\n            dim_in = dim_internal\n            dim_internal = dim_internal // 2\n            self.conv5_fc = self._conv2d_block(dim_in, dim_internal, kernel=3, stride=1, padding=1, bias=False)\n            self.fc = nn.Sequential(\n                nn.Linear(dim_internal * in_resolution * in_resolution, self.out_resolution * self.out_resolution),\n                nn.ReLU(inplace=True)\n            )\n\n        self._init_weights()\n\n    def forward(self, x):\n        pooled_feature = self.conv1[0](x[0])\n        for i in range(1, self.levels_num):\n            pooled_feature = torch.max(pooled_feature, self.conv1[i](x[i]))\n\n        x = self.conv2(pooled_feature)\n        x = self.conv3(x)\n\n        if self.fully_connected_fusion:\n            y = self.conv4_fc(x)\n\n        x = self.conv4(x)\n        x = nn.functional.relu(self.upconv(x), inplace=True)\n        x = self.segm(x)\n\n        if self.fully_connected_fusion:\n            y = self.conv5_fc(y)\n            y = y.view(int(y.size(0)), -1)\n            y = self.fc(y)\n            y = y.view(int(y.size(0)), 1, self.out_resolution, self.out_resolution)\n            x = x + y\n\n        if not self.training:\n            x = torch.sigmoid(x)\n        return x\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/prior_box.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom math import sqrt\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\n\ndef meshgrid(a, b):\n    x = a.repeat(len(b))\n    y = b.repeat(len(a), 1).t().contiguous().view(-1)\n    return x, y\n\n\nclass PriorGridGenerator(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, prior_boxes, feature_map, im_data, w=0, h=0, stride_x=0, stride_y=0, flatten=True):\n        return g.op(\'ExperimentalDetectronPriorGridGenerator\', prior_boxes, feature_map, im_data,\n                    w_i=int(w), h_i=int(h), stride_x_f=stride_x, stride_y_f=stride_y, flatten_i=int(flatten))\n\n    @staticmethod\n    def forward(ctx, prior_boxes, feature_map, im_data, w=0, h=0, stride_x=0, stride_y=0, flatten=True):\n        device = prior_boxes.device\n\n        grid_w = w if w else feature_map.shape[-1]\n        grid_h = h if h else feature_map.shape[-2]\n        s_x = stride_x if stride_x else im_data.shape[-1] / feature_map.shape[-1]\n        s_y = stride_y if stride_y else im_data.shape[-2] / feature_map.shape[-2]\n        # grid_w = feature_map.shape[-1]\n        # grid_h = feature_map.shape[-2]\n        # s_x = im_data.shape[-1] / feature_map.shape[-1]\n        # s_y = im_data.shape[-2] / feature_map.shape[-2]\n        # print(w, grid_w, h, grid_h, stride_x, s_x, stride_y, s_y, feature_map.shape, im_data.shape)\n        # grid_w, grid_h, s_x, s_y = w, h, stride_x, stride_y\n\n        shift_x, shift_y = meshgrid(\n            torch.linspace(0, grid_w - 1, grid_w, dtype=torch.float32) * s_x + s_x * 0.5,\n            torch.linspace(0, grid_h - 1, grid_h, dtype=torch.float32) * s_y + s_y * 0.5)\n        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y)).t().to(device=device, dtype=torch.float32)\n        prior_boxes_grid = (prior_boxes.unsqueeze(0) + shifts.unsqueeze(1))\n        if flatten:\n            prior_boxes_grid = prior_boxes_grid.reshape(-1, 4)\n        else:\n            prior_boxes_grid = prior_boxes_grid.reshape(grid_h, grid_w, -1, 4)\n        return prior_boxes_grid\n\n\nclass PriorBox(nn.Module):\n    def __init__(self, widths=None, heights=None, aspect_ratios=None, min_size=None, max_size=None, flatten=True,\n                 use_cache=True):\n        super().__init__()\n        self.flatten = flatten\n\n        assert (widths is not None and heights is not None) != \\\n               (min_size is not None and max_size is not None and aspect_ratios is not None),\\\n            \'Please specify either prior boxes widths and heights, or their min/max sizes and aspect ratios.\'\n        self.widths = widths\n        self.heights = heights\n        if self.widths is None or self.heights is None:\n            self.widths = [min_size, sqrt(min_size * max_size)]\n            self.heights = [min_size, sqrt(min_size * max_size)]\n            for ar in aspect_ratios:\n                self.widths.extend([min_size * sqrt(ar), min_size / sqrt(ar)])\n                self.heights.extend([min_size / sqrt(ar), min_size * sqrt(ar)])\n\n        self.prior_boxes_numpy = []\n        for w, h in zip(self.widths, self.heights):\n            self.prior_boxes_numpy.append([-w / 2, -h / 2, w / 2, h / 2])\n        self.prior_boxes_numpy = np.array(self.prior_boxes_numpy, dtype=np.float32)\n\n        self.use_cache = use_cache\n        self.register_buffer(\'prior_boxes\', torch.from_numpy(np.copy(self.prior_boxes_numpy)))\n\n    @staticmethod\n    def meshgrid(a, b):\n        x = a.repeat(len(b))\n        y = b.repeat(len(a), 1).t().contiguous().view(-1)\n        return x, y\n\n    def priors_num(self):\n        return self.prior_boxes.size(0)\n\n    def forward(self, feature_map, im_data, w=0, h=0, stride_x=0, stride_y=0):\n        if self.use_cache:\n            prior_boxes = self.prior_boxes\n        else:\n            prior_boxes = torch.from_numpy(self.prior_boxes_numpy).to(feature_map.device)\n        return PriorGridGenerator.apply(prior_boxes, feature_map, im_data, int(w), int(h),\n                                        stride_x, stride_y, self.flatten)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/proposal.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\nfrom .nms_function import nms\nfrom ..utils.boxes import bbox_transform, clip_boxes_to_image\n\n\ndef generate_proposals(all_anchors, rpn_output, im_info,\n                       pre_nms_count=12000, post_nms_count=2000, nms_threshold=0.7, min_size=0,\n                       force_max_output_size=False):\n    bbox_deltas, _, scores = rpn_output\n    device_id = scores.device\n    assert device_id == bbox_deltas.device\n\n    batch_size = scores.shape[0]\n    assert batch_size == bbox_deltas.shape[0]\n\n    if not (all_anchors.dim() == 2 and all_anchors.shape[1] == 4):\n        all_anchors = all_anchors.view(-1, 4)\n\n    rois = []\n    roi_probs = []\n    for idx in range(batch_size):\n        im_boxes, im_probs = GenerateProposalsSingleImage.apply(im_info[idx], all_anchors,\n                                                                bbox_deltas[idx], scores[idx],\n                                                                pre_nms_count, post_nms_count,\n                                                                nms_threshold, min_size, force_max_output_size)\n        rois.append(im_boxes)\n        roi_probs.append(im_probs)\n\n    return rois, roi_probs\n\n\ndef generate_proposals_single_image(im_info, all_anchors, bbox_deltas, scores,\n                                    pre_nms_count=12000, post_nms_count=2000, nms_threshold=0.7, min_size=0,\n                                    force_max_output_size=False):\n    # Transpose and reshape predicted bbox transformations to get them\n    # into the same order as the anchors:\n    #   - bbox deltas will be (4 * A, H, W) format from conv output\n    #   - transpose to (H, W, 4 * A)\n    #   - reshape to (H * W * A, 4) where rows are ordered by (H, W, A)\n    #     in slowest to fastest order to match the enumerated anchors\n    bbox_deltas = bbox_deltas.permute(1, 2, 0).contiguous().view(-1, 4)\n\n    # Same story for the scores:\n    #   - scores are (A, H, W) format from conv output\n    #   - transpose to (H, W, A)\n    #   - reshape to (H * W * A, 1) where rows are ordered by (H, W, A)\n    #     to match the order of anchors and bbox_deltas\n    scores = scores.permute(1, 2, 0).contiguous().view(-1)\n\n    assert not torch.isnan(scores).any(), \'Scores blob contains NaN values.\'\n    # 4. sort all (proposal, score) pairs by score from highest to lowest\n    # 5. take top self._pre_nms_count (e.g. 6000)\n    if pre_nms_count <= 0 or pre_nms_count >= scores.numel():\n        sorted_scores, order = torch.sort(scores, dim=0, descending=True)\n    else:\n        # Avoid sorting possibly large arrays; First partition to get top K\n        # unsorted and then sort just those (~20x faster for 200k scores)\n        sorted_scores, order = torch.topk(scores, pre_nms_count, largest=True, sorted=True)\n    bbox_deltas = torch.index_select(bbox_deltas, 0, order)\n    all_anchors = torch.index_select(all_anchors, 0, order)\n    scores = sorted_scores\n\n    # Transform anchors into proposals via bbox transformations\n    proposals = bbox_transform(all_anchors, bbox_deltas, weights=None)\n\n    # 2. clip proposals to image (may result in proposals with zero area\n    # that will be removed in the next step)\n    proposals = clip_boxes_to_image(proposals, int(im_info[0]), int(im_info[1]))\n\n    # 3. remove predicted boxes with either height or width < min_size\n    keep = filter_boxes(proposals, min_size, im_info)\n    torch.index_select(proposals, 0, keep, out=proposals)\n    torch.index_select(scores, 0, keep, out=scores)\n\n    # 6. apply loose nms (e.g. threshold = 0.7)\n    # 7. take after_nms_topN (e.g. 300)\n    # 8. return the top proposals (-> RoIs top)\n\n    if pre_nms_count > 0:\n        proposals = proposals[:pre_nms_count, :]\n        scores = scores[:pre_nms_count]\n\n    if nms_threshold > 0:\n        proposals, scores = nms(proposals, scores, nms_threshold)\n\n    if post_nms_count > 0:\n        proposals = proposals[:post_nms_count, :]\n        scores = scores[:post_nms_count]\n\n    proposals_num = proposals.shape[0]\n    if force_max_output_size and proposals_num < post_nms_count:\n        fake_proposals = proposals.new_zeros((post_nms_count - proposals_num, 4))\n        fake_proposals[:, 2:] = 1.0\n        proposals = torch.cat((proposals, fake_proposals), dim=0)\n        scores = torch.cat((scores, scores.new_zeros((post_nms_count - proposals_num,))), dim=0)\n\n    return proposals, scores\n\n\ndef filter_boxes(boxes, min_size, im_info):\n    x0 = boxes[:, 0]\n    y0 = boxes[:, 1]\n    ws = boxes[:, 2] - x0 + 1\n    hs = boxes[:, 3] - y0 + 1\n    keep = torch.nonzero((ws >= min_size) & (hs >= min_size)).squeeze()\n    return keep\n\n\nclass GenerateProposalsSingleImage(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, im_info, all_anchors, bbox_deltas, scores,\n                 pre_nms_count=12000, post_nms_count=2000, nms_threshold=0.7, min_size=0, force_max_output_size=False):\n        return g.op(""ExperimentalDetectronGenerateProposalsSingleImage"", im_info, all_anchors, bbox_deltas, scores,\n                    pre_nms_count_i=pre_nms_count, post_nms_count_i=post_nms_count, nms_threshold_f=nms_threshold,\n                    min_size_f=min_size, outputs=2)\n\n    @staticmethod\n    def forward(ctx, im_info, all_anchors, bbox_deltas, scores,\n                pre_nms_count=12000, post_nms_count=2000, nms_threshold=0.7, min_size=0, force_max_output_size=False):\n        return generate_proposals_single_image(im_info, all_anchors, bbox_deltas, scores,\n                                               pre_nms_count, post_nms_count, nms_threshold, min_size,\n                                               force_max_output_size)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/proposal_gt_matcher.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom ..utils.boxes import jaccard, bbox_transform_inv\nfrom ..utils.segms import polys_to_mask_wrt_box\n\n\nclass ProposalGTMatcher(nn.Module):\n    def __init__(self, positive_threshold=0.5, negative_threshold=0.3,\n                 positive_fraction=0.3, ignore_threshold=0.5, batch_size=512, target_mask_size=(14, 14)):\n        super().__init__()\n        self.positive_overlap_range = (positive_threshold, 1.1)\n        self.negative_overlap_range = (0.0, negative_threshold)\n        self.ignore_threshold = ignore_threshold\n        self.ensure_closest_box = True\n        self.fg_fraction = positive_fraction\n        self.batch_size = batch_size\n        self.target_mask_size = target_mask_size\n\n    def forward(self, boxes, gt_boxes, gt_labels, gt_masks=None):\n        batch_size = len(gt_boxes)\n        assert batch_size == len(gt_labels)\n\n        sampled_boxes = []\n        cls_targets = []\n        reg_targets = []\n        mask_targets = []\n        for idx in range(batch_size):\n            im_gt_masks = gt_masks[idx] if gt_masks is not None else None\n            image_sampled_boxes, image_cls_targets, image_reg_targets, image_mask_targets = \\\n                self.forward_single_image(boxes[idx], gt_boxes[idx], gt_labels[idx], im_gt_masks)\n            sampled_boxes.append(image_sampled_boxes)\n            cls_targets.append(image_cls_targets)\n            reg_targets.append(image_reg_targets)\n            mask_targets.append(image_mask_targets)\n        return sampled_boxes, cls_targets, reg_targets, mask_targets\n\n    def forward_single_image(self, boxes, gt_boxes, gt_labels, gt_masks=None):\n        device = boxes.device\n\n        # Add ground truth boxes to also sample those as positives later.\n        boxes = torch.cat((gt_boxes, boxes.view(-1, 4)), dim=0)\n        boxes_num = boxes.shape[0]\n        box_to_label = torch.zeros(boxes_num, dtype=torch.long, device=device)\n\n        if len(gt_boxes) > 0:\n            # Compute overlaps between the boxes and the GT boxes.\n            box_by_gt_overlap = jaccard(boxes, gt_boxes)\n            # For each box, amount of overlap with most overlapping GT box\n            # and mapping from box to GT box that has highest overlap.\n            box_to_gt_max, box_to_gt_idx = box_by_gt_overlap.max(dim=1)\n            matched_boxes_mask = box_to_gt_max > 0\n            # Record max overlaps with the class of the appropriate gt box\n            matched_box_to_label = gt_labels[box_to_gt_idx[matched_boxes_mask]]\n            box_to_label.masked_scatter_(matched_boxes_mask, matched_box_to_label.long())\n\n        # Subsample positive labels if we have too many.\n        positive_mask = box_to_gt_max >= self.positive_overlap_range[0]\n        fg_num = int(positive_mask.sum().item())\n        target_fg_num = int(self.fg_fraction * self.batch_size)\n        # Due to issues with torch.multinomial do subsampling in a loop\n        # to ensure proper number of samples to be selected.\n        while target_fg_num < fg_num:\n            disable_inds = torch.multinomial(positive_mask.to(torch.float32), fg_num - target_fg_num, replacement=False)\n            positive_mask[disable_inds] = 0\n            fg_num = int(positive_mask.sum().item())\n        target_fg_num = fg_num\n        assert target_fg_num == positive_mask.sum().item()\n\n        # Subsample negative labels if we have too many.\n        negative_mask = (self.negative_overlap_range[0] <= box_to_gt_max) & \\\n                        (box_to_gt_max < self.negative_overlap_range[1])\n        bg_num = int(negative_mask.sum().item())\n        assert bg_num > 0\n        target_bg_num = self.batch_size - target_fg_num\n        assert target_bg_num > 0\n        if target_bg_num < bg_num:\n            negative_indices = negative_mask.nonzero().reshape(-1)\n            assert negative_indices.numel() > 0\n            # torch.randperm tends to throw a SEGFAULT in a multi-GPU setup,\n            # so using numpy.random.permutation here as a workaround.\n            shuffled_order = torch.from_numpy(np.random.permutation(negative_indices.numel())).to(device=device)\n            # shuffled_order = torch.randperm(negative_indices.numel(), device=device)\n            assert 0 < bg_num - target_bg_num < len(shuffled_order)\n            assert (shuffled_order < negative_indices.numel()).all()\n            negative_mask.index_fill_(0, negative_indices[shuffled_order[:bg_num - target_bg_num]], 0)\n        else:\n            target_bg_num = bg_num\n        assert target_bg_num == negative_mask.sum().item()\n\n        sampled_boxes = torch.cat((boxes[positive_mask, :], boxes[negative_mask, :]), dim=0)\n        sampled_labels = torch.cat((box_to_label[positive_mask],\n                                    torch.zeros(target_bg_num, dtype=torch.long, device=device)))\n\n        # Get target for bounding box regression.\n        sampled_gt_boxes = gt_boxes[box_to_gt_idx[positive_mask], :]\n        sampled_boxes_targets = torch.zeros((target_bg_num, 4), dtype=torch.float32, device=device)\n        if sampled_gt_boxes.numel() > 0:\n            sampled_boxes_targets = torch.cat((bbox_transform_inv(sampled_boxes[:target_fg_num, :],\n                                                                  sampled_gt_boxes),\n                                               sampled_boxes_targets), dim=0)\n\n        sampled_masks_targets = None\n        if gt_masks is not None:\n            sampled_boxes_cpu = sampled_boxes.cpu().detach().numpy()\n            # Get target for box segmentation.\n            sampled_gt_masks = [gt_masks[i] for i in box_to_gt_idx[positive_mask]]\n            sampled_masks_targets = []\n            for i in range(target_fg_num):\n                gt_polygon = sampled_gt_masks[i]\n                box = sampled_boxes_cpu[i]\n                mask = polys_to_mask_wrt_box(gt_polygon, box, self.target_mask_size)\n                sampled_masks_targets.append(torch.tensor(mask, dtype=torch.float32).reshape(*self.target_mask_size))\n            if len(sampled_masks_targets) > 0:\n                sampled_masks_targets = torch.cat((torch.stack(sampled_masks_targets, dim=0),\n                                                   torch.full((target_bg_num, *self.target_mask_size), -1, dtype=torch.float32)),\n                                                  dim=0).to(device)\n            else:\n                sampled_masks_targets = torch.empty((0, *self.target_mask_size), dtype=torch.float32, device=device)\n\n        return sampled_boxes, sampled_labels, sampled_boxes_targets, sampled_masks_targets\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/roi_align.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom ..extensions._EXTRA import roi_align_forward, roi_align_backward\n\n\nclass ROIAlignFunction(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, features, rois, aligned_height=7, aligned_width=7, spatial_scale=1 / 16, sampling_ratio=0):\n        return g.op(\'ExperimentalDetectronROIAlign\', features, rois, aligned_height_i=aligned_height,\n                    aligned_width_i=aligned_width,\n                    spatial_scale_f=spatial_scale, sampling_ratio_i=sampling_ratio)\n\n    @staticmethod\n    def forward(ctx, features, rois, aligned_height=7, aligned_width=7, spatial_scale=1 / 16, sampling_ratio=0):\n\n        if isinstance(rois, list) or isinstance(rois, tuple):\n            device = features.device\n            rois_blob = torch.empty((0, 5), dtype=torch.float32, device=device)\n            for i, im_rois in enumerate(rois):\n                rois_blob = torch.cat((rois_blob,\n                                       torch.cat((torch.full((len(im_rois), 1), i, dtype=torch.float32, device=device),\n                                                  im_rois), dim=1)\n                                       ), dim=0)\n        else:\n            rois_blob = rois\n        feature_size = features.size()\n\n        ctx.feature_size = feature_size\n        ctx.aligned_height = aligned_height\n        ctx.aligned_width = aligned_width\n        ctx.spatial_scale = spatial_scale\n        ctx.sampling_ratio = sampling_ratio\n\n        ctx.save_for_backward(rois_blob)\n\n        output_blob = roi_align_forward(features, rois_blob, spatial_scale,\n                                        aligned_height, aligned_width, sampling_ratio)\n        return output_blob\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n\n        rois, = ctx.saved_variables\n\n        assert (ctx.feature_size is not None and grad_output.is_cuda)\n        batch_size, num_channels, data_height, data_width = ctx.feature_size\n\n        grad_blob = roi_align_backward(grad_output, rois, ctx.spatial_scale,\n                                       ctx.aligned_height, ctx.aligned_width,\n                                       batch_size, num_channels, data_height, data_width,\n                                       ctx.sampling_ratio)\n\n        return grad_blob, None, None, None, None, None\n\n\ndef roi_align(features, rois, aligned_height=7, aligned_width=7, spatial_scale=1 / 16, sampling_ratio=0):\n    return ROIAlignFunction.apply(features, rois, aligned_height, aligned_width, spatial_scale, sampling_ratio)\n\n\nclass ROIAlign(nn.Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale, sampling_ratio):\n        super().__init__()\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n        self.sampling_ratio = int(sampling_ratio)\n\n    def forward(self, features, rois):\n        return roi_align(features, rois, aligned_height=self.aligned_height, aligned_width=self.aligned_width,\n                         spatial_scale=self.spatial_scale, sampling_ratio=self.sampling_ratio)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/roi_distributor.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom torch import nn\n\n\nclass ROIsDistributorFunction(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, rois, levels_num, canonical_level=4, canonical_scale=224):\n        return g.op(\'ExperimentalDetectronROIsRedistributor\', rois, levels_num_i=levels_num, canonical_level_i=canonical_level,\n                    canonical_scale_i=canonical_scale, outputs=levels_num)\n\n    @staticmethod\n    def forward(ctx, rois, levels_num, canonical_level=4, canonical_scale=224):\n        areas = ((rois[:, 2] - rois[:, 0] + 1) * (rois[:, 3] - rois[:, 1] + 1)).view(-1)\n        areas.sqrt_()\n        areas /= canonical_scale\n        areas += 1e-6\n        areas.log2_()\n        target_levels = torch.floor(areas + canonical_level)\n        target_levels.clamp_(min=0, max=levels_num - 1)\n        level_indices = tuple((target_levels == i).nonzero().view(-1) for i in range(levels_num))\n        return level_indices\n\n\ndef redistribute_rois(rois, levels_num, canonical_level=4, canonical_scale=224):\n    return ROIsDistributorFunction.apply(rois, levels_num, canonical_level, canonical_scale)\n\n\nclass ROIsDistributor(nn.Module):\n    def __init__(self, levels_num, canonical_level=2, canonical_scale=224):\n        super().__init__()\n        self.levels_num = levels_num\n        self.canonical_level = canonical_level\n        self.canonical_scale = canonical_scale\n\n    def forward(self, rois, scores):\n        return redistribute_rois(rois, self.levels_num, self.canonical_level, self.canonical_scale)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/roi_feature_extractor.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\nfrom .roi_align import roi_align\nfrom .roi_distributor import redistribute_rois\n\n\ndef topk_rois_logic(image_rois, image_rois_probs, max_rois):\n    topk_indices = torch.topk(image_rois_probs, min(max_rois, image_rois_probs.shape[0]))[1]\n    image_rois = image_rois[topk_indices]\n    return image_rois\n\n\nclass TopKROIsStub(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, image_rois, image_rois_probs, max_rois):\n        return g.op(\'ExperimentalDetectronTopKROIs\', image_rois, image_rois_probs, max_rois_i=max_rois)\n\n    @staticmethod\n    def forward(ctx, image_rois, image_rois_probs, max_rois):\n        return topk_rois_logic(image_rois, image_rois_probs, max_rois)\n\n\ndef topk_rois(image_rois, image_rois_probs, max_rois, use_stub=False):\n    func = TopKROIsStub.apply if use_stub else topk_rois_logic\n    return func(image_rois, image_rois_probs, max_rois)\n\n\ndef extract_roi_features(rois, feature_pyramid, pyramid_scales, output_size=7, sampling_ratio=2,\n                         distribute_rois_between_levels=True, preserve_rois_order=True, use_stub=False):\n    func = extract_roi_features_single_image_stub if use_stub else extract_roi_features_single_image\n    roi_features = []\n    for i, image_rois in enumerate(rois):\n        if len(image_rois):\n            image_roi_features, image_rois = func(image_rois, feature_pyramid, pyramid_scales,\n                                                  image_id=i, output_size=output_size, sampling_ratio=sampling_ratio,\n                                                  distribute_rois_between_levels=distribute_rois_between_levels,\n                                                  preserve_rois_order=preserve_rois_order)\n        else:\n            image_roi_features = None\n        roi_features.append(image_roi_features)\n        rois[i] = image_rois\n\n    return roi_features, rois\n\n\ndef extract_roi_features_single_image_stub(image_rois, feature_pyramid, pyramid_scales,\n                                           image_id, output_size, sampling_ratio,\n                                           distribute_rois_between_levels=True, preserve_rois_order=True):\n    class ROIFeatureExtractorStub(torch.autograd.Function):\n        @staticmethod\n        def symbolic(g, rois, *features):\n            return g.op(\'ExperimentalDetectronROIFeatureExtractor\', rois, *features,\n                        pyramid_scales_i=pyramid_scales, image_id_i=image_id,\n                        output_size_i=output_size, sampling_ratio_i=sampling_ratio,\n                        distribute_rois_between_levels_i=int(distribute_rois_between_levels),\n                        preserve_rois_order_i=int(preserve_rois_order), outputs=2)\n\n        @staticmethod\n        def forward(ctx, rois, *features):\n            return extract_roi_features_single_image(rois, features,\n                                                     pyramid_scales, image_id, output_size,\n                                                     sampling_ratio, distribute_rois_between_levels,\n                                                     preserve_rois_order)\n\n    return ROIFeatureExtractorStub.apply(image_rois, *feature_pyramid)\n\n\ndef extract_roi_features_single_image(image_rois, feature_pyramid, pyramid_scales, image_id,\n                                      output_size, sampling_ratio, distribute_rois_between_levels=True,\n                                      preserve_rois_order=True):\n    with torch.no_grad():\n        if distribute_rois_between_levels:\n            # Split proposals between feature pyramid levels.\n            rois_to_levels = redistribute_rois(image_rois, levels_num=len(feature_pyramid), canonical_level=2)\n            # Permute rois so that rois for each level are stored consecutively.\n            all_rois_to_levels = torch.cat(rois_to_levels, dim=0)\n            image_rois_reordered = image_rois[all_rois_to_levels]\n            # Split Tensor of reordered ROIs to a list of Tensor.\n            # Basically do the following\n            # rois_per_level = image_rois_reordered.split([len(rois_ids) for rois_ids in rois_to_levels], dim=0)\n            # but as long as Tensor.split handles empty slices incorrectly, do it in a more verbose way.\n            rois_per_level = []\n            i = 0\n            for j in [len(rois_ids) for rois_ids in rois_to_levels]:\n                rois_per_level.append(image_rois_reordered[i:i + j])\n                i += j\n            assert i == image_rois.shape[0]\n        else:\n            image_rois_reordered = image_rois\n            rois_per_level = [image_rois for _ in feature_pyramid]\n\n    # Extract features for ROIs from corresponding feature pyramid levels.\n    image_roi_features = []\n    for rois, feature_map, scale in zip(rois_per_level, feature_pyramid, pyramid_scales):\n        if rois.numel():\n            fmap = feature_map[image_id:image_id + 1]\n            image_roi_features.append(roi_align(fmap, [rois, ],\n                                                aligned_height=output_size, aligned_width=output_size,\n                                                spatial_scale=float(1 / scale),\n                                                sampling_ratio=sampling_ratio))\n    if distribute_rois_between_levels:\n        image_roi_features = torch.cat(image_roi_features, dim=0)\n        if preserve_rois_order:\n            # Return back to original rois order.\n            reverse_indices = torch.sort(all_rois_to_levels)[1]\n            image_roi_features = image_roi_features.index_select(0, reverse_indices)\n    else:\n        image_roi_features = torch.stack(image_roi_features, dim=0)\n        image_rois = image_rois_reordered\n\n    return image_roi_features, image_rois\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/rpn.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .losses import smooth_l1_loss\n\n\nclass RPN(nn.Module):\n    def __init__(self, dim_in, dim_internal, priors_num, class_activation_mode=\'softmax\'):\n        super().__init__()\n        assert class_activation_mode in (\'softmax\', \'sigmoid\')\n        self.dim_in = dim_in\n        self.dim_internal = dim_internal\n        self.priors_num = priors_num\n        self.dim_score = priors_num * 2 if class_activation_mode == \'softmax\' else priors_num\n        self.class_activation_mode = class_activation_mode\n\n        self.conv = nn.Conv2d(dim_in, dim_internal, 3, 1, 1)\n        self.cls_score = nn.Conv2d(dim_internal, self.dim_score, 1, 1, 0)\n        self.bbox_deltas = nn.Conv2d(dim_internal, 4 * priors_num, 1, 1, 0)\n\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.normal_(self.conv.weight, std=0.01)\n        nn.init.constant_(self.conv.bias, 0)\n        nn.init.normal_(self.cls_score.weight, std=0.01)\n        nn.init.constant_(self.cls_score.bias, 0)\n        nn.init.normal_(self.bbox_deltas.weight, std=0.01)\n        nn.init.constant_(self.bbox_deltas.bias, 0)\n\n    def forward(self, x):\n        conv = F.relu(self.conv(x), inplace=True)\n        cls_scores = self.cls_score(conv)\n        bbox_deltas = self.bbox_deltas(conv)\n        if self.class_activation_mode == \'softmax\':\n            b, c, h, w = cls_scores.shape\n            cls_scores = cls_scores.view(b, 2, -1, h, w)\n            cls_probs = F.softmax(cls_scores, dim=1)[:, 1].squeeze(dim=1)\n        else:\n            cls_probs = torch.sigmoid(cls_scores)\n\n        return bbox_deltas, cls_scores, cls_probs\n\n\ndef rpn_loss(rpn_gt_matcher, im_info, gt_boxes, priors, rpn_bbox_deltas, rpn_cls_scores):\n    batch_size = len(gt_boxes)\n    assert batch_size == rpn_bbox_deltas.shape[0]\n    assert batch_size == rpn_cls_scores.shape[0]\n\n    device = rpn_bbox_deltas.device\n    im_info = im_info.to(device)\n\n    rpn_loss_cls = []\n    rpn_loss_bbox = []\n    # For each element of the batch.\n    for idx in range(batch_size):\n        with torch.no_grad():\n            labels, bbox_targets, bbox_inw, bbox_outw = \\\n                rpn_gt_matcher(priors.view(-1, 4), gt_boxes[idx].to(device), None, im_info[idx, ...])\n\n        # Classification loss.\n        weight = (labels >= 0).float()\n        batch_rpn_cls_scores = rpn_cls_scores[idx, 0, ...].permute(1, 2, 0).reshape(-1)\n        loss_rpn_cls = nn.functional.binary_cross_entropy_with_logits(batch_rpn_cls_scores, labels.float(),\n                                                                      weight, size_average=False)\n        loss_rpn_cls /= weight.sum()\n\n        # Regression loss.\n        batch_rpn_bbox_deltas = rpn_bbox_deltas[idx, ...].permute(1, 2, 0).reshape(-1, 4)\n        loss_rpn_bbox = smooth_l1_loss(batch_rpn_bbox_deltas, bbox_targets, bbox_inw, bbox_outw,\n                                       beta=1 / 9, normalize=False)\n\n        rpn_loss_cls.append(loss_rpn_cls)\n        rpn_loss_bbox.append(loss_rpn_bbox)\n\n    rpn_loss_cls = torch.stack(rpn_loss_cls, dim=0)\n    rpn_loss_bbox = torch.stack(rpn_loss_bbox, dim=0)\n    return rpn_loss_cls, rpn_loss_bbox\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/rpn_gt_matcher.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom ..utils.boxes import jaccard, bbox_transform_inv\n\n\ndef unmap(values, indices, n, dim=0, fill=-1):\n    shape = list(values.shape)\n    shape[dim] = n\n    out = torch.full(shape, fill, dtype=values.dtype, device=values.device)\n    out.index_copy_(dim, indices, values)\n    return out\n\n\nclass RPNGTMatcher(nn.Module):\n    def __init__(self, straddle_threshold=0, positive_threshold=0.5, negative_threshold=0.3,\n                 positive_fraction=0.3, batch_size=512):\n        super().__init__()\n        self.straddle_thresh = straddle_threshold\n        self.positive_overlap_range = (positive_threshold, 1.1)\n        self.negative_overlap_range = (0.0, negative_threshold)\n        self.ensure_closest_box = True\n        self.fg_fraction = positive_fraction\n        self.batch_size = batch_size\n\n    def forward(self, boxes, gt_boxes, gt_labels, gt_ignore_labels, im_info):\n        batch_size = len(gt_boxes)\n        assert batch_size == len(gt_labels)\n        assert batch_size == im_info.shape[0]\n\n        rpn_cls_targets = []\n        rpn_reg_targets = []\n        for idx in range(batch_size):\n            cls_targets, reg_targets, bbox_inw, bbox_outw = \\\n                self.forward_single_image(boxes.view(-1, 4), gt_boxes[idx], gt_labels[idx],\n                                          gt_ignore_labels[idx], im_info[idx])\n            del bbox_inw\n            del bbox_outw\n            rpn_cls_targets.append(cls_targets)\n            rpn_reg_targets.append(reg_targets)\n\n        return rpn_cls_targets, rpn_reg_targets\n\n    def forward_single_image(self, boxes, gt_boxes, gt_labels, gt_ignore_labels, im_info):\n        device = boxes.device\n        im_height, im_width = im_info[:2]\n        total_boxes_num = boxes.shape[0]\n\n        # Filter out boxes that go outside the image more than allowed.\n        # Set threshold to -1 (or a large value) to keep all boxes.\n        if self.straddle_thresh >= 0:\n            inds_inside = (\n                (boxes[:, 0] >= -self.straddle_thresh) &\n                (boxes[:, 1] >= -self.straddle_thresh) &\n                (boxes[:, 2] < im_width + self.straddle_thresh + 1) &\n                (boxes[:, 3] < im_height + self.straddle_thresh + 1)\n            ).nonzero().view(-1)\n            anchors = boxes[inds_inside, :]\n        else:\n            inds_inside = torch.arange(total_boxes_num, type=torch.long, device=boxes.device)\n            anchors = boxes\n        num_inside = len(inds_inside)\n\n        if num_inside == 0:\n            return torch.full((total_boxes_num, ), -1, dtype=torch.float32, device=device), \\\n                   torch.full((total_boxes_num, 4), 0, dtype=torch.float32, device=device), \\\n                   torch.full((total_boxes_num, 4), 0, dtype=torch.float32, device=device), \\\n                   torch.full((total_boxes_num, 4), 0, dtype=torch.float32, device=device)\n\n        # Compute box labels:\n        # label == 1 is positive, 0 is negative, -1 is don\'t care (ignore).\n        labels = torch.full((num_inside, ), -1, dtype=torch.int, device=device)\n\n        # Exclude ignored (crowd) boxes.\n        gt_boxes = gt_boxes[gt_ignore_labels == 0]\n        if len(gt_boxes) > 0:\n            # Compute overlaps between the boxes and the GT boxes.\n            anchor_by_gt_overlap = jaccard(anchors, gt_boxes)\n            # For each box, amount of overlap with most overlapping GT box\n            # and mapping from box to GT box that has highest overlap.\n            anchor_to_gt_max, anchor_to_gt_idx = anchor_by_gt_overlap.max(dim=1)\n            # For each GT box, amount of overlap with most overlapping box\n            # and mapping from GT box to a box that has highest overlap.\n            gt_to_anchor_max, gt_to_anchor_idx = anchor_by_gt_overlap.max(dim=0, keepdim=True)\n            # Find all boxes that share the max overlap amount if it\'s larger than 0\n            # (this includes many ties).\n            gt_to_anchor_max[gt_to_anchor_max == 0.0] = 1.0\n            anchors_with_max_overlap = (anchor_by_gt_overlap == gt_to_anchor_max).max(dim=1)[0]\n            # Positive (foreground) label: for each GT use boxes with highest overlap (including ties).\n            labels[anchors_with_max_overlap] = 1\n            # Positive (foreground) label: boxes that have overlap with GT more than the threshold.\n            labels[anchor_to_gt_max >= self.positive_overlap_range[0]] = 1\n\n        # Subsample positive labels if we have too many.\n        positive_mask = labels > 0\n        fg_num = int(positive_mask.sum().item())\n        # assert fg_num > 0\n        target_fg_num = int(self.fg_fraction * self.batch_size)\n        # assert target_fg_num > 0\n        if target_fg_num < fg_num:\n            positive_indices = positive_mask.nonzero().reshape(-1)\n            # torch.randperm tends to throw a SEGFAULT in a multi-GPU setup,\n            # so using numpy.random.permutation here as a workaround.\n            shuffled_order = torch.from_numpy(np.random.permutation(positive_indices.numel())).to(device=device)\n            # shuffled_order = torch.randperm(positive_indices.numel(), device=device)\n            assert 0 < fg_num - target_fg_num <= len(shuffled_order)\n            labels.index_fill_(0, positive_indices[shuffled_order[:fg_num - target_fg_num]], -1)\n        else:\n            target_fg_num = fg_num\n        assert target_fg_num == (labels > 0).sum().item()\n\n        # Subsample negative labels if we have too many.\n        # Samples with replacement, but since the set of background indices is large,\n        # most samples will not have repeats.\n        target_bg_num = int((self.batch_size - (labels > 0).sum()).item())\n        negative_mask = anchor_to_gt_max < self.negative_overlap_range[1]\n        if negative_mask.sum().item() > target_bg_num:\n            enable_inds = torch.multinomial(negative_mask.to(torch.float32), target_bg_num, replacement=True)\n            labels.index_fill_(0, enable_inds, 0)\n\n        # Get target for bounding box regression.\n        bbox_targets = torch.zeros((num_inside, 4), dtype=torch.float32, device=device)\n        positive_mask = labels > 0\n        if positive_mask.sum() > 0:\n            bbox_targets.masked_scatter_(positive_mask.unsqueeze(1), bbox_transform_inv(\n                anchors[positive_mask, :], gt_boxes[anchor_to_gt_idx[positive_mask], :], (1.0, 1.0, 1.0, 1.0)\n            ))\n\n        # Bbox regression loss has the form:\n        #   loss(x) = weight_outside * L(weight_inside * x)\n        # Inside weights allow us to set zero loss on an element-wise basis\n        # Bbox regression is only trained on positive examples so we set their\n        # weights to 1 and 0 otherwise.\n        positive_mask.unsqueeze_(1)\n        bbox_inside_weights = torch.zeros((num_inside, 4), dtype=torch.float32, device=device)\n        bbox_inside_weights.masked_fill_(positive_mask, 1.0)\n\n        # We need to average regression loss by the total number of boxes selected.\n        # Outside weights are used to scale each element-wise loss so the final\n        # average over the mini-batch is correct.\n        bbox_outside_weights = torch.zeros((num_inside, 4), dtype=torch.float32, device=device)\n        non_negative_mask = labels >= 0\n        non_negative_mask.unsqueeze_(1)\n        num_examples = max(non_negative_mask.sum().item(), 1.0)\n        bbox_outside_weights.masked_fill_(non_negative_mask, 1.0 / num_examples)\n\n        # Map up to original set (i.e. all) of boxes.\n        labels = unmap(labels, inds_inside, total_boxes_num, fill=-1)\n        bbox_targets = unmap(bbox_targets, inds_inside, total_boxes_num, fill=0)\n        bbox_inside_weights = unmap(bbox_inside_weights, inds_inside, total_boxes_num, fill=0)\n        bbox_outside_weights = unmap(bbox_outside_weights, inds_inside, total_boxes_num, fill=0)\n\n        return labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/__init__.py,0,b''
pytorch_toolkit/instance_segmentation/segmentoly/utils/blob.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\n\ndef to_numpy(x):\n    if isinstance(x, torch.Tensor):\n        x = x.detach().cpu().numpy()\n    return x\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/boxes.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nimport torch\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy + 1), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0] + 1) *\n              (box_a[:, 3] - box_a[:, 1] + 1)).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2] - box_b[:, 0] + 1) *\n              (box_b[:, 3] - box_b[:, 1] + 1)).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef clip_boxes_to_image(boxes, height, width):\n    """"""Clip an array of boxes to an image with the given height and width.""""""\n    if boxes.numel() > 0:\n        boxes[:, 0].clamp_(min=0, max=width - 1)\n        boxes[:, 1].clamp_(min=0, max=height - 1)\n        boxes[:, 2].clamp_(min=0, max=width - 1)\n        boxes[:, 3].clamp_(min=0, max=height - 1)\n    return boxes\n\n\ndef bbox_transform(boxes, deltas, bbox_xform_clip=np.log(1000. / 16.), weights=(1.0, 1.0, 1.0, 1.0)):\n    device_id = boxes.device\n    if boxes.shape[0] == 0:\n        return torch.zeros((0, deltas.shape[1]), dtype=torch.float32, device=device_id)\n\n    n = deltas.shape[0]\n    deltas = deltas.view(n, -1, 4).permute(2, 0, 1)\n    boxes = boxes.permute(1, 0).view(4, -1, 1)\n    if weights is not None:\n        weights = torch.tensor(weights, dtype=torch.float32, device=device_id).view(4, 1, 1)\n        deltas /= weights\n\n    dxy = deltas[:2]\n    dwh = deltas[2:]\n    dwh.clamp_(max=bbox_xform_clip).exp_().mul_(0.5)\n    dxy -= dwh\n    dwh *= 2\n    dwh += dxy\n\n    dx0, dy0, dx1, dy1 = deltas\n    x0, y0, x1, y1 = boxes\n\n    w = x1 - x0 + 1\n    h = y1 - y0 + 1\n    cx = x0 + 0.5 * w\n    cy = y0 + 0.5 * h\n    x0new = w * dx0 + cx\n    y0new = h * dy0 + cy\n    x1new = w * dx1 + cx - 1\n    y1new = h * dy1 + cy - 1\n\n    pred_boxes = torch.cat((x0new, y0new, x1new, y1new), 0).view(4, n, -1).permute(1, 2, 0).reshape(n, -1)\n\n    return pred_boxes\n\n\ndef expand_boxes(boxes, scale):\n    """"""Expand an array of boxes by a given scale.""""""\n    w_half = (boxes[:, 2] - boxes[:, 0]) * .5\n    h_half = (boxes[:, 3] - boxes[:, 1]) * .5\n    x_c = (boxes[:, 2] + boxes[:, 0]) * .5\n    y_c = (boxes[:, 3] + boxes[:, 1]) * .5\n\n    w_half *= scale\n    h_half *= scale\n\n    boxes_exp = np.zeros(boxes.shape)\n    boxes_exp[:, 0] = x_c - w_half\n    boxes_exp[:, 2] = x_c + w_half\n    boxes_exp[:, 1] = y_c - h_half\n    boxes_exp[:, 3] = y_c + h_half\n\n    return boxes_exp\n\n\ndef bbox_transform_inv(boxes, gt_boxes, weights=(10., 10., 5., 5.)):\n    """"""\n    Inverse transform that computes target bounding-box regression deltas\n    given proposal boxes and ground-truth boxes. The weights argument should be\n    a 4-tuple of multiplicative weights that are applied to the regression\n    target.\n\n    In older versions of this code (and in py-faster-rcnn), the weights were set\n    such that the regression deltas would have unit standard deviation on the\n    training dataset. Presently, rather than computing these statistics exactly,\n    we use a fixed set of weights (10., 10., 5., 5.) by default. These are\n    approximately the weights one would get from COCO using the previous unit\n    stdev heuristic.\n    """"""\n\n    ex_widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    ex_heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ex_ctr_x = boxes[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = boxes[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0] + 1.0\n    gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1] + 1.0\n    gt_ctr_x = gt_boxes[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_boxes[:, 1] + 0.5 * gt_heights\n\n    wx, wy, ww, wh = weights\n    targets_dx = wx * (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = wy * (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = gt_widths / ex_widths\n    targets_dw.log_()\n    targets_dw *= ww\n    targets_dh = gt_heights / ex_heights\n    targets_dh.log_()\n    targets_dh *= wh\n\n    targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=0)\n    targets.transpose_(1, 0)\n    return targets\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/env.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nclass SuccessExit(KeyboardInterrupt):\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        return self.message\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/logging.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\nimport sys\nfrom datetime import timedelta\n\nfrom tensorboardX import SummaryWriter\n\n\ndef setup_logging(level=logging.INFO, file_path=None, stream=sys.stdout):\n    log_format = \'{levelname} {asctime} {filename}:{lineno:>4d}] {message}\'\n    date_format = \'%d-%m-%y %H:%M:%S\'\n    handlers = [logging.StreamHandler(stream), ]\n    if file_path:\n        handlers.append(logging.FileHandler(file_path))\n    logging.basicConfig(level=level, format=log_format, datefmt=date_format, style=\'{\', handlers=handlers)\n\n\nclass TrainingLogger(object):\n    def close(self):\n        raise NotImplementedError\n\n    def __call__(self, **kwargs):\n        raise NotImplementedError\n\n\nclass TextLogger(TrainingLogger):\n    def __init__(self, logger, *args, delimiter=\', \', max_width=140, **kwargs):\n        self.logger = logger\n        self.delimiter = delimiter\n        self.max_width = max_width\n\n    def close(self):\n        pass\n\n    def _format_float(self, v):\n        try:\n            v = float(v)\n            res = \'{:.6}\'.format(v)\n        except (ValueError, TypeError):\n            res = \'{}\'.format(v)\n        return res\n\n    def _format_value(self, k, v):\n        return \'{}: {}\'.format(k, self._format_float(v))\n\n    def _soft_wrap(self, line):\n        lines = []\n        while len(line) > self.max_width:\n            p = line.rfind(self.delimiter, 0, self.max_width)\n            if p > 0:\n                lines.append(line[:p + len(self.delimiter)])\n                line = line[p + len(self.delimiter):]\n        return lines\n\n    def _estimate_time(self, timer, step, total_steps):\n        elapsed_time = timedelta(seconds=timer.total_time)\n        elapsed_time = str(elapsed_time).split(\'.\')[0]\n        left_time = timedelta(seconds=timer.average_time * (total_steps - step))\n        left_time = str(left_time).split(\'.\')[0]\n        secs_per_iter = timer.average_time\n        return \'time elapsed/~left: {} / {} ({:.3} sec/it)\'.format(elapsed_time, left_time, secs_per_iter)\n\n    def __call__(self, step, total_steps, lr, loss, metrics, timers=None, **kwargs):\n        main_log_line = \'Step {} / {}\'.format(step, total_steps)\n        if lr is not None:\n            main_log_line += \', lr: {}\'.format(self._format_float(lr))\n        if loss is not None:\n            main_log_line += \', loss: {}\'.format(self._format_float(loss))\n        log_lines = [main_log_line, ]\n        metrics_log = []\n        for k, v in sorted(metrics.items()):\n            metrics_log.append(self._format_value(k, v))\n        metrics_log = self._soft_wrap(self.delimiter.join(metrics_log))\n        log_lines.extend(metrics_log)\n\n        if timers:\n            if \'total\' in timers:\n                log_lines.append(self._estimate_time(timers[\'total\'], step, total_steps))\n            timers_log = []\n            for k, v in sorted(timers.items()):\n                timers_log.append(self._format_value(k, v.average_time))\n            timers_log = self._soft_wrap(self.delimiter.join(timers_log))\n            log_lines.extend(timers_log)\n        self.logger.info(\'\\n  \'.join(log_lines))\n\n\nclass TensorboardLogger(TrainingLogger):\n    def __init__(self, log_dir, *args, **kwargs):\n        self.summary_writer = SummaryWriter(log_dir)\n\n    def close(self):\n        self.summary_writer.close()\n\n    def __call__(self, step, total_steps, lr, loss, metrics, timers=None, **kwargs):\n        if lr is not None:\n            self.summary_writer.add_scalar(\'learning_rate\', lr, step)\n        for k, v in metrics.items():\n            self.summary_writer.add_scalar(k, v, step)\n        if timers:\n            for k, v in timers.items():\n                self.summary_writer.add_scalar(\'timers/\' + k, v.smoothed_time, step)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/lr_scheduler.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom bisect import bisect_right\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass MultiStepLRWithWarmUp(_LRScheduler):\n    def __init__(self,\n                 optimizer,\n                 milestones,\n                 warmup_iters,\n                 warmup_method=\'linear\',\n                 warmup_factor_base=0.33,\n                 gamma=0.1,\n                 last_epoch=-1):\n        if warmup_method not in {\'constant\', \'linear\'}:\n            raise KeyError(\'Unknown warm up method: {}\'.format(warmup_method))\n        self.milestones = sorted(milestones)\n        self.gamma = gamma\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n        self.warmup_factor_base = warmup_factor_base\n        # Base class calls method `step` which increases `last_epoch` by 1 and then calls\n        # method `get_lr` with this value. If `last_epoch` is not equal to -1, we drop\n        # the first step, so to avoid this dropping do small fix by subtracting 1\n        if last_epoch > -1:\n            last_epoch = last_epoch - 1\n        elif last_epoch < -1:\n            raise ValueError(\'Learning rate scheduler got incorrect parameter last_epoch = {}\'.format(last_epoch))\n        super(MultiStepLRWithWarmUp, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        # During warm up change learning rate on every step according to warmup_factor\n        if self.last_epoch < self.warmup_iters:\n            if self.warmup_method == \'constant\':\n                warmup_factor = self.warmup_factor_base\n            elif self.warmup_method == \'linear\':\n                alpha = self.last_epoch / self.warmup_iters\n                warmup_factor = self.warmup_factor_base * (1 - alpha) + alpha\n            return [base_lr * warmup_factor for base_lr in self.base_lrs]\n        # On the last step of warm up set learning rate equal to base LR\n        elif self.last_epoch == self.warmup_iters:\n            return [base_lr for base_lr in self.base_lrs]\n        # After warm up increase LR according to defined in `milestones` values of steps\n        else:\n            return [base_lr * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n                    for base_lr in self.base_lrs]\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \\\n                        \'[warmup_method = {}, warmup_factor_base = {}, warmup_iters = {},\' \\\n                        \' milestones = {}, gamma = {}]\'.format(self.warmup_method, self.warmup_factor_base,\n                                                               self.warmup_iters, str(list(self.milestones)),\n                                                               self.gamma)\n        return format_string\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/onnx.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nimport onnx\nimport torch\nimport logging\n\n\ndef get_input_size(image_size, scale, max_size, divisor=1):\n    image_size = np.asarray(image_size)\n\n    # Get scale factor for image resize.\n    im_scale = float(scale) / float(min(image_size))\n    if np.round(im_scale * max(image_size)) > max_size:\n        im_scale = float(max_size) / float(max(image_size))\n\n    # Get resized image size.\n    resized_image_size = image_size * im_scale\n    input_size = np.round(resized_image_size).astype(np.int)\n\n    # Pad the image for size be a multiple of `divisor`.\n    input_size[0] = (input_size[0] + divisor - 1) // divisor * divisor\n    input_size[1] = (input_size[1] + divisor - 1) // divisor * divisor\n\n    return input_size\n\n\ndef onnx_export(net, input_size, output_file_path, check=False, verbose=False):\n    net.eval()\n    if hasattr(net, \'force_max_output_size\'):\n        # Force model to output maximal possible number of proposals and detections.\n        net.force_max_output_size = True\n    # Use ONNX stubs for all the blocks that don\'t support direct export.\n    for m in net.modules():\n        if hasattr(m, \'use_stub\'):\n            m.use_stub = True\n\n    im_data = np.random.randn(1, 3, input_size[0], input_size[1]).astype(np.float32)\n    im_info = np.asarray([[input_size[0], input_size[1], 1.0]], dtype=np.float32)\n    im_info = torch.tensor(im_info)\n    im_data = torch.tensor(im_data)\n    if torch.cuda.is_available():\n        net = net.cuda()\n        im_info = im_info.cuda()\n        im_data = im_data.cuda()\n    im_data.requires_grad = True\n    im_info.requires_grad = True\n\n    input_names = [\'im_data\', \'im_info\']\n    outputs_names = [\'boxes\', \'classes\', \'scores\', \'batch_ids\', \'raw_masks\']\n    torch.onnx.export(net, (im_data, im_info), output_file_path,\n                      verbose=verbose, input_names=input_names, output_names=outputs_names)\n\n    net_from_onnx = onnx.load(output_file_path)\n    if check:\n        try:\n            onnx.checker.check_model(net_from_onnx)\n            logging.info(\'ONNX check passed.\')\n        except onnx.onnx_cpp2py_export.checker.ValidationError as ex:\n            logging.warning(\'ONNX check failed.\')\n            logging.warning(ex)\n\n    return onnx.helper.printable_graph(net_from_onnx.graph)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/postprocess.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport cv2\nimport numpy as np\nimport pycocotools.mask as mask_util\n\nfrom .blob import to_numpy\nfrom .boxes import expand_boxes\n\n\ndef postprocess_batch(batch_ids, scores, classes, boxes, raw_cls_masks,\n                      batch_size, im_h, im_w, im_scale_y=None, im_scale_x=None, im_scale=None,\n                      full_image_masks=True, encode_masks=False,\n                      confidence_threshold=0.0):\n    boxes_all = [np.empty((0, 4), dtype=np.float32) for _ in range(batch_size)]\n    scores_all = [np.empty((0, ), dtype=np.float32) for _ in range(batch_size)]\n    classes_all = [np.empty((0, ), dtype=np.float32) for _ in range(batch_size)]\n    raw_masks_all = [None for _ in range(batch_size)]\n    masks_all = [[] for _ in range(batch_size)]\n\n    if batch_ids is None:\n        return scores_all, classes_all, boxes_all, masks_all\n\n    scale_x = im_scale_x\n    scale_y = im_scale_y\n    if im_scale is not None:\n        scale_x = im_scale\n        scale_y = im_scale\n    assert len(scale_x) == len(scale_y)\n\n    batch_ids = to_numpy(batch_ids)\n\n    num_objs_per_batch = []\n    for i in range(batch_size):\n        num_objs_per_batch.append(np.count_nonzero(batch_ids == i))\n\n    begin = 0\n    for i in range(0, len(num_objs_per_batch)):\n        end = begin + num_objs_per_batch[i]\n        # Scale boxes back to the original image\n        boxes_all[i] = boxes[begin:end]\n        scores_all[i] = scores[begin:end]\n        classes_all[i] = classes[begin:end]\n        raw_masks_all[i] = raw_cls_masks[begin:end]\n        begin = end\n\n    # Resize segmentation masks to fit corresponding bounding boxes.\n    for i in range(batch_size):\n        scores_all[i], classes_all[i], boxes_all[i], masks_all[i] = \\\n            postprocess(scores_all[i], classes_all[i], boxes_all[i], raw_masks_all[i],\n                        im_h[i], im_w[i], scale_y[i], scale_x[i], None,\n                        full_image_masks, encode_masks,\n                        confidence_threshold)\n\n    return scores_all, classes_all, boxes_all, masks_all\n\n\ndef postprocess(scores, classes, boxes, raw_cls_masks,\n                im_h, im_w, im_scale_y=None, im_scale_x=None, im_scale=None,\n                full_image_masks=True, encode_masks=False,\n                confidence_threshold=0.0):\n    no_detections = (np.empty((0, ), dtype=np.float32), np.empty((0, ), dtype=np.float32),\\\n                     np.empty((0, 4), dtype=np.float32), [])\n    if scores is None:\n        return no_detections\n\n    scale = im_scale\n    if scale is None:\n        assert (im_scale_x is not None) and (im_scale_y is not None)\n        scale = [im_scale_x, im_scale_y, im_scale_x, im_scale_y]\n\n    scores = to_numpy(scores)\n    classes = to_numpy(classes)\n    boxes = to_numpy(boxes)\n    raw_cls_masks = to_numpy(raw_cls_masks)\n\n    confidence_filter = scores > confidence_threshold\n    scores = scores[confidence_filter]\n    classes = classes[confidence_filter]\n    boxes = boxes[confidence_filter]\n    raw_cls_masks = list(segm for segm, is_valid in zip(raw_cls_masks, confidence_filter) if is_valid)\n\n    if len(scores) == 0:\n        return no_detections\n\n    boxes = boxes / scale\n    classes = classes.astype(np.uint32)\n    masks = []\n    for box, cls, raw_mask in zip(boxes, classes, raw_cls_masks):\n        raw_cls_mask = raw_mask[cls, ...]\n        mask = segm_postprocess(box, raw_cls_mask, im_h, im_w, full_image_masks, encode_masks)\n        masks.append(mask)\n\n    return scores, classes, boxes, masks\n\n\ndef segm_postprocess(box, raw_cls_mask, im_h, im_w, full_image_mask=True, encode=False):\n    # Add zero border to prevent upsampling artifacts on segment borders.\n    raw_cls_mask = np.pad(raw_cls_mask, ((1, 1), (1, 1)), \'constant\', constant_values=0)\n    extended_box = expand_boxes(box[np.newaxis, :],\n                                raw_cls_mask.shape[0] / (raw_cls_mask.shape[0] - 2.0))[0]\n    extended_box = extended_box.astype(int)\n    w, h = np.maximum(extended_box[2:] - extended_box[:2] + 1, 1)\n    x0, y0 = np.clip(extended_box[:2], a_min=0, a_max=[im_w, im_h])\n    x1, y1 = np.clip(extended_box[2:] + 1, a_min=0, a_max=[im_w, im_h])\n\n    raw_cls_mask = cv2.resize(raw_cls_mask, (w, h)) > 0.5\n    mask = raw_cls_mask.astype(np.uint8)\n\n    if full_image_mask:\n        # Put an object mask in an image mask.\n        im_mask = np.zeros((im_h, im_w), dtype=np.uint8)\n        im_mask[y0:y1, x0:x1] = mask[(y0 - extended_box[1]):(y1 - extended_box[1]),\n                                     (x0 - extended_box[0]):(x1 - extended_box[0])]\n    else:\n        original_box = box.astype(int)\n        x0, y0 = np.clip(original_box[:2], a_min=0, a_max=[im_w, im_h])\n        x1, y1 = np.clip(original_box[2:] + 1, a_min=0, a_max=[im_w, im_h])\n        im_mask = np.ascontiguousarray(mask[(y0 - original_box[1]):(y1 - original_box[1]),\n                                            (x0 - original_box[0]):(x1 - original_box[0])])\n\n    if encode:\n        im_mask = mask_util.encode(np.array(im_mask[:, :, np.newaxis].astype(np.uint8), order=\'F\'))[0]\n        im_mask[\'counts\'] = im_mask[\'counts\'].decode(\'utf-8\')\n\n    return im_mask\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/profile.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport gc\nimport operator as op\nimport time\nfrom collections import defaultdict\nfrom functools import reduce, wraps\n\nimport numpy as np\nimport torch\n\n\nclass Timer(object):\n\n    def __init__(self, warmup=0, smoothing=0.5, cuda_sync=True):\n        self.warmup = warmup\n        self.cuda_sync = cuda_sync\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n        self.smoothed_time = 0.\n        self.smoothing_alpha = smoothing\n        self.min_time = float(\'inf\')\n        self.max_time = 0.\n        self.reset()\n\n    def tic(self):\n        if self.cuda_sync and torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.start_time = time.time()\n\n    def toc(self, average=True, smoothed=False):\n        if self.cuda_sync and torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.diff = time.time() - self.start_time\n        self.calls += 1\n        if self.calls <= self.warmup:\n            return self.diff\n\n        self.total_time += self.diff\n        self.average_time = self.total_time / (self.calls - self.warmup)\n        self.smoothed_time = self.smoothed_time * self.smoothing_alpha + self.diff * (1.0 - self.smoothing_alpha)\n        self.min_time = min(self.min_time, self.diff)\n        self.max_time = max(self.max_time, self.diff)\n        if average:\n            return self.average_time\n        elif smoothed:\n            return self.smoothed_time\n        else:\n            return self.diff\n\n    def __enter__(self):\n        self.tic()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.toc()\n\n    def reset(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n\nclass DummyTimer(Timer):\n    def __init__(self):\n        super().__init__()\n\n    def tic(self):\n        pass\n\n    def toc(self, *args, **kwargs):\n        return 0\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\ndef timed(func):\n    @wraps(func)\n    def wrapper_timer(self, *args, **kwargs):\n        if not hasattr(self, \'_timers\'):\n            self._timers = defaultdict(Timer)\n        with self._timers[func.__name__]:\n            value = func(self, *args, **kwargs)\n        return value\n    return wrapper_timer\n\n\ndef print_timing_stats(timers, key=\'average_time\'):\n    print(\'{:>40}: {:>10} [{:>10}, {:>10}] {:>10} {:>10}\'.format(\'name\', \'average\', \'min\', \'max\', \'#calls\', \'total\'))\n    for k, v in sorted(timers.items(), key=lambda x: op.attrgetter(key)(x[1]), reverse=True):\n        print(\'{:>40}: {:10.2f} [{:10.2f}, {:10.2f}] {:10d} {:10.2f}\'.format(k, 1000 * v.average_time,\n                                                                             1000 * v.min_time, 1000 * v.max_time,\n                                                                             v.calls, 1000 * v.total_time))\n    print(\'-\' * 40)\n\n\ndef pretty_shape(shape):\n    if shape is None:\n        return \'None\'\n    return \'\xc3\x97\'.join(map(str, shape))\n\n\ndef pretty_size(size, units=\'G\', precision=2, base=1024):\n    if units is None:\n        if size // (base ** 3) > 0:\n            val = str(round(size / (base ** 3), precision))\n            units = \'G\'\n        elif size // (base ** 2) > 0:\n            val = str(round(size / (base ** 2), precision))\n            units = \'M\'\n        elif size // base > 0:\n            val = str(round(size / base, precision))\n            units = \'K\'\n        else:\n            val = str(size)\n            units = \'\'\n    else:\n        if units == \'G\':\n            val = str(round(size / (base ** 3), precision))\n        elif units == \'M\':\n            val = str(round(size / (base ** 2), precision))\n        elif units == \'K\':\n            val = str(round(size / base, precision))\n        else:\n            val = str(size)\n    \n    return val, units\n\n\ndef dump_tensors(gpu_only=True):\n    """"""Prints a list of the Tensors being tracked by the garbage collector.""""""\n    total_size = 0\n    for obj in gc.get_objects():\n        try:\n            if torch.is_tensor(obj):\n                if not gpu_only or obj.is_cuda:\n                    print(\'%s:%s%s %s\' % (type(obj).__name__,\n                                          \' GPU\' if obj.is_cuda else \'\',\n                                          \' pinned\' if obj.is_pinned else \'\',\n                                          pretty_shape(obj.size())))\n                    total_size += obj.numel()\n            elif hasattr(obj, \'data\') and torch.is_tensor(obj.data):\n                if not gpu_only or obj.is_cuda:\n                    print(\'%s \xe2\x86\x92 %s:%s%s%s%s %s\' % (type(obj).__name__,\n                                                   type(obj.data).__name__,\n                                                   \' GPU\' if obj.is_cuda else \'\',\n                                                   \' pinned\' if obj.data.is_pinned else \'\',\n                                                   \' grad\' if obj.requires_grad else \'\',\n                                                   \' volatile\' if obj.volatile else \'\',\n                                                   pretty_shape(obj.data.size())))\n                    total_size += obj.data.numel()\n        except Exception as e:\n            pass\n    print(\'Total size:\', total_size)\n\n\ndef list_allocated_tensors():\n    memtable = []\n    for obj in gc.get_objects():\n        if torch.is_tensor(obj):\n            memtable.append(dict(obj=obj,\n                                 size=(reduce(op.mul, obj.size())\n                                       if len(obj.size()) > 0\n                                       else 0) * obj.element_size()))\n    memtable = sorted(memtable, key=op.itemgetter(\'size\'))\n    for i, item in enumerate(memtable):\n        obj = item[\'obj\']\n        print(\'{:03}: {:>10} {:>30} {:>25} {:>10}\'.format(i, item[\'size\'], str(np.array(obj.shape)),\n                                                          str(obj.type()), str(obj.device)))\n\n\ndef list_parameters(module):\n    memtable = []\n    for name, x in module.named_parameters():\n        memtable.append(dict(name=name,\n                             shape=np.array(x.data.shape),\n                             size=int(x.data.numel() * x.data.element_size()),\n                             has_grad=x.requires_grad,\n                             grad_shape=np.array(x.grad.shape) if x.requires_grad else None,\n                             grad_size=int(x.grad.numel() * x.grad.element_size()) if x.requires_grad else 0\n                             )\n                        )\n    total_data_size = 0\n    total_grad_size = 0\n    for i, item in enumerate(memtable):\n        print(\'{:03} {:>60}: {:>15} {:>15} {:>15} {:>15}\'.format(i,\n                                                                 item[\'name\'],\n                                                                 pretty_size(item[\'size\'], units=\'M\')[0],\n                                                                 pretty_shape(item[\'shape\']),\n                                                                 pretty_size(item[\'grad_size\'], units=\'M\')[0],\n                                                                 pretty_shape(item[\'grad_shape\'])))\n        total_data_size += item[\'size\']\n        total_grad_size += item[\'grad_size\']\n\n    total_mem_size = list(pretty_size(total_data_size)) + list(pretty_size(total_grad_size))\n    print(\'TOTAL MEMORY USAGE FOR MODEL PARAMETERS: data: {} {}B grad: {} {}B\'.format(*total_mem_size))\n\n\nclass FeatureMapsTracer(object):\n    fwd_tensors_registry = set()\n    bwd_tensors_registry = set()\n\n    @staticmethod\n    def reset(*args, **kwargs):\n        FeatureMapsTracer.summary_fwd()\n        FeatureMapsTracer.summary_bwd()\n        del FeatureMapsTracer.fwd_tensors_registry\n        FeatureMapsTracer.fwd_tensors_registry = set()\n        del FeatureMapsTracer.bwd_tensors_registry\n        FeatureMapsTracer.bwd_tensors_registry = set()\n\n    @staticmethod\n    def summary_fwd(*args, **kwargs):\n        total_data_size = FeatureMapsTracer.get_total_size(list(FeatureMapsTracer.fwd_tensors_registry))\n        print(\'TOTAL FORWARD DATA BLOBS SIZE: {} {}B\'.format(*pretty_size(total_data_size)))\n\n    @staticmethod\n    def summary_bwd(*args, **kwargs):\n        total_data_size = FeatureMapsTracer.get_total_size(list(FeatureMapsTracer.bwd_tensors_registry))\n        print(\'TOTAL BACKWARD GRAD BLOBS SIZE: {} {}B\'.format(*pretty_size(total_data_size)))\n\n    @staticmethod\n    def list_tensors(x):\n        tensors = []\n        if isinstance(x, (list, tuple)):\n            for i in x:\n                tensors.extend(FeatureMapsTracer.list_tensors(i))\n        elif isinstance(x, dict):\n            for i in x.values():\n                tensors.extend(FeatureMapsTracer.list_tensors(i))\n        elif isinstance(x, torch.Tensor):\n            tensors.append(x)\n        return tensors\n\n    @staticmethod\n    def get_shapes(tensors):\n        shapes = [x.shape for x in tensors]\n        return shapes\n\n    @staticmethod\n    def shapes_to_str(shapes):\n        return \'[\' + \', \'.join([pretty_shape(shape) for shape in shapes]) + \']\'\n\n    @staticmethod\n    def get_total_size(tensors):\n        total_size = 0\n        for x in tensors:\n            total_size += int(x.numel() * x.element_size())\n        return total_size\n\n    @staticmethod\n    def forward(module, inputs, outputs, verbose=False):\n        input_tensors = FeatureMapsTracer.list_tensors(inputs)\n        inputs_shapes = FeatureMapsTracer.get_shapes(input_tensors)\n        inputs_shapes_str = FeatureMapsTracer.shapes_to_str(inputs_shapes)\n        inputs_size = FeatureMapsTracer.get_total_size(input_tensors)\n        FeatureMapsTracer.fwd_tensors_registry.update(set(input_tensors))\n\n        output_tensors = FeatureMapsTracer.list_tensors(outputs)\n        outputs_shapes = FeatureMapsTracer.get_shapes(output_tensors)\n        outputs_shapes_str = FeatureMapsTracer.shapes_to_str(outputs_shapes)\n        outputs_size = FeatureMapsTracer.get_total_size(output_tensors)\n        FeatureMapsTracer.fwd_tensors_registry.update(set(output_tensors))\n\n        if verbose:\n            print(\'fwd {:>20}: {:>15} {:>15} {:>15} {:>15}\'.format(module._get_name(),\n                                                                   pretty_size(inputs_size, units=\'M\')[0],\n                                                                   inputs_shapes_str,\n                                                                   pretty_size(outputs_size, units=\'M\')[0],\n                                                                   outputs_shapes_str))\n\n    @staticmethod\n    def backward(module, inputs, outputs, verbose=False):\n        input_tensors = FeatureMapsTracer.list_tensors(inputs)\n        inputs_shapes = FeatureMapsTracer.get_shapes(input_tensors)\n        inputs_shapes_str = FeatureMapsTracer.shapes_to_str(inputs_shapes)\n        inputs_size = FeatureMapsTracer.get_total_size(input_tensors)\n        FeatureMapsTracer.bwd_tensors_registry.update(set(input_tensors))\n\n        output_tensors = FeatureMapsTracer.list_tensors(outputs)\n        outputs_shapes = FeatureMapsTracer.get_shapes(output_tensors)\n        outputs_shapes_str = FeatureMapsTracer.shapes_to_str(outputs_shapes)\n        outputs_size = FeatureMapsTracer.get_total_size(output_tensors)\n        FeatureMapsTracer.bwd_tensors_registry.update(set(output_tensors))\n\n        if verbose:\n            print(\'bwd {:>20}: {:>15} {:>15} {:>15} {:>15}\'.format(module._get_name(),\n                                                                   pretty_size(inputs_size, units=\'M\')[0],\n                                                                   inputs_shapes_str,\n                                                                   pretty_size(outputs_size, units=\'M\')[0],\n                                                                   outputs_shapes_str))\n\n    @staticmethod\n    def add_fwd_hooks(module):\n\n        def register_per_layer_hooks(m):\n            m.register_forward_hook(FeatureMapsTracer.forward)\n\n        module.register_forward_pre_hook(FeatureMapsTracer.reset)\n        module.apply(register_per_layer_hooks)\n\n    @staticmethod\n    def add_bwd_hooks(module):\n\n        def register_per_layer_hooks(m):\n            m.register_backward_hook(FeatureMapsTracer.backward)\n\n        module.apply(register_per_layer_hooks)\n\n    @staticmethod\n    def add_hooks(module):\n        FeatureMapsTracer.add_fwd_hooks(module)\n        FeatureMapsTracer.add_bwd_hooks(module)\n\n\nclass PerformanceCounters(object):\n    def __init__(self):\n        self.pc = {}\n\n    def update(self, pc):\n        for layer, stats in pc.items():\n            if layer not in self.pc:\n                self.pc[layer] = dict(layer_type=stats[\'layer_type\'],\n                                      exec_type=stats[\'exec_type\'],\n                                      status=stats[\'status\'],\n                                      real_time=stats[\'real_time\'],\n                                      calls=1)\n            else:\n                self.pc[layer][\'real_time\'] += stats[\'real_time\']\n                self.pc[layer][\'calls\'] += 1\n\n    def print(self):\n        print(\'Performance counters:\')\n        print(\' \'.join([\'name\', \'layer_type\', \'exec_type\', \'status\', \'real_time(us)\']))\n        for layer, stats in self.pc.items():\n            print(\'{} {} {} {} {}\'.format(layer, stats[\'layer_type\'], stats[\'exec_type\'],\n                                          stats[\'status\'], stats[\'real_time\'] / stats[\'calls\']))\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/segms.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nimport pycocotools.mask as mask_util\n\n\ndef polys_to_mask_wrt_box(polygons, box, resolution):\n    if not isinstance(resolution, (list, tuple)):\n        resolution = (resolution, resolution)\n\n    w = box[2] - box[0]\n    h = box[3] - box[1]\n\n    w = np.maximum(w, 1)\n    h = np.maximum(h, 1)\n\n    polygons_norm = []\n    for poly in polygons:\n        p = np.array(poly, dtype=np.float32).flatten()\n        p[0::2] = (p[0::2] - box[0]) * resolution[0] / w\n        p[1::2] = (p[1::2] - box[1]) * resolution[1] / h\n        polygons_norm.append(p)\n\n    if len(polygons_norm) > 0:\n        rle = mask_util.frPyObjects(polygons_norm, resolution[0], resolution[1])\n        mask = np.array(mask_util.decode(rle), dtype=np.float32)\n        # Flatten in case polygons was a list\n        mask = np.sum(mask, axis=2)\n        mask = np.array(mask > 0, dtype=np.float32)\n    else:\n        mask = np.full(resolution, -1, dtype=np.float32)\n    return mask\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/stats.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nimport torch\n\n\ndef get_model_parameters_number(model, as_string=True):\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if not as_string:\n        return params_num\n\n    if params_num // 10 ** 6 > 0:\n        return str(round(params_num / 10 ** 6, 2)) + \'M\'\n    elif params_num // 10 ** 3:\n        return str(round(params_num / 10 ** 3, 2)) + \'k\'\n\n    return str(params_num)\n\n\ndef add_flops_counting_methods(net_main_module):\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    # Adding variables necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    """"""\n\n    batches_count = self.__batch_counter__\n    flops_sum = 0\n    for module in self.modules():\n        if is_supported_instance(module):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    """"""\n    add_batch_counter_hook_function(self)\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    """"""\n    remove_batch_counter_hook_function(self)\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    """"""\n    add_batch_counter_variables_or_reset(self)\n    self.apply(add_flops_counter_variable_or_reset)\n\n\ndef add_flops_mask(module, mask):\n    def add_flops_mask_func(module):\n        if isinstance(module, torch.nn.Conv2d):\n            module.__mask__ = mask\n    module.apply(add_flops_mask_func)\n\n\ndef remove_flops_mask(module):\n    module.apply(add_flops_mask_variable_or_reset)\n\n\n# ---- Internal functions\ndef is_supported_instance(module):\n    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.ReLU) \\\n       or isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.MaxPool2d) \\\n       or isinstance(module, torch.nn.AvgPool2d):\n        return True\n\n    return False\n\n\ndef empty_flops_counter_hook(module, input, output):\n    module.__flops__ += 0\n\n\ndef relu_flops_counter_hook(module, input, output):\n    active_elements_count = output.numel()\n    module.__flops__ += int(active_elements_count)\n\n\ndef linear_flops_counter_hook(module, input, output):\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__flops__ += int(batch_size * input.shape[1] * output.shape[1])\n\n\ndef pool_flops_counter_hook(module, input, output):\n    input = input[0]\n    module.__flops__ += int(np.prod(input.shape))\n\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    output_height, output_width = output.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = kernel_height * kernel_width * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * output_height * output_width\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += int(overall_flops)\n\n\ndef batch_counter_hook(module, input, output):\n    batch_size = 1\n    if len(input) > 0:\n        # Can have multiple inputs, getting the first one\n        input = input[0]\n        batch_size = len(input)\n    else:\n        pass\n        # print(\'Warning! No positional inputs found for a module, assuming batch size is 1.\')\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            return\n\n        if isinstance(module, torch.nn.Conv2d):\n            handle = module.register_forward_hook(conv_flops_counter_hook)\n        elif isinstance(module, torch.nn.ReLU):\n            handle = module.register_forward_hook(relu_flops_counter_hook)\n        elif isinstance(module, torch.nn.Linear):\n            handle = module.register_forward_hook(linear_flops_counter_hook)\n        elif isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d):\n            handle = module.register_forward_hook(pool_flops_counter_hook)\n        else:\n            handle = module.register_forward_hook(empty_flops_counter_hook)\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n            del module.__flops_handle__\n# --- Masked flops counting\n\n\ndef print_model_with_flops(model, units=\'GMac\', precision=3):\n    total_flops = model.compute_average_flops_cost()\n\n    def accumulate_flops(self):\n        if is_supported_instance(self):\n            return self.__flops__ / model.__batch_counter__\n        else:\n            sum = 0\n            for m in self.children():\n                sum += m.accumulate_flops()\n            return sum\n\n    def flops_repr(self):\n        accumulated_flops_cost = self.accumulate_flops()\n        return \', \'.join([flops_to_string(accumulated_flops_cost, units=units, precision=precision),\n                          \'{:.3%} MACs\'.format(accumulated_flops_cost / total_flops),\n                          self.original_extra_repr()])\n\n    def add_extra_repr(m):\n        m.accumulate_flops = accumulate_flops.__get__(m)\n        flops_extra_repr = flops_repr.__get__(m)\n        if m.extra_repr != flops_extra_repr:\n            m.original_extra_repr = m.extra_repr\n            m.extra_repr = flops_extra_repr\n            assert m.extra_repr != m.original_extra_repr\n\n    def del_extra_repr(m):\n        if hasattr(m, \'original_extra_repr\'):\n            m.extra_repr = m.original_extra_repr\n            del m.original_extra_repr\n        if hasattr(m, \'accumulate_flops\'):\n            del m.accumulate_flops\n\n    model.apply(add_extra_repr)\n    print(model)\n    model.apply(del_extra_repr)\n\n\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__mask__ = None\n\n\ndef flops_to_string(flops, units=\'GMac\', precision=2):\n    if units is None:\n        if flops // 10**9 > 0:\n            return str(round(flops / 10.**9, precision)) + \' GMac\'\n        elif flops // 10**6 > 0:\n            return str(round(flops / 10.**6, precision)) + \' MMac\'\n        elif flops // 10**3 > 0:\n            return str(round(flops / 10.**3, precision)) + \' KMac\'\n        else:\n            return str(flops) + \' Mac\'\n    else:\n        if units == \'GMac\':\n            return str(round(flops / 10.**9, precision)) + \' \' + units\n        elif units == \'MMac\':\n            return str(round(flops / 10.**6, precision)) + \' \' + units\n        elif units == \'KMac\':\n            return str(round(flops / 10.**3, precision)) + \' \' + units\n        else:\n            return str(flops) + \' Mac\'\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/tracker.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\n\n\nclass StaticIOUTracker(object):\n    def __init__(self, iou_threshold=0.5, age_threshold=10):\n        super().__init__()\n        self.history = []\n        self.history_areas = []\n        self.history_classes = []\n        self.ids = []\n        self.age = []\n        self.iou_threshold = iou_threshold\n        self.age_threshold = age_threshold\n        self.last_id = 0\n\n    def affinity(self, masks, classes):\n        areas = [np.count_nonzero(mask) for mask in masks]\n        affinity_matrix = np.zeros((len(masks), len(self.history)), dtype=np.float32)\n        for i, (history_mask, history_area, history_class) in \\\n                enumerate(zip(self.history, self.history_areas, self.history_classes)):\n            for j, (mask, area, cls) in enumerate(zip(masks, areas, classes)):\n                if cls != history_class:\n                    continue\n                intersection = np.count_nonzero(np.logical_and(history_mask, mask))\n                union = history_area + area - intersection\n                iou = intersection / union if union > 0 else 0\n                affinity_matrix[j, i] = iou\n        return affinity_matrix, areas\n\n    def __call__(self, masks, classes):\n        # Get affinity with history.\n        affinity_matrix, areas = self.affinity(masks, classes)\n\n        # Make assignment of currents masks to existing tracks.\n        assignment = []\n        indices = np.arange(len(self.history))\n        for i in range(len(masks)):\n            j = 0\n            affinity_score = -1.0\n            if affinity_matrix.shape[1] > 0:\n                j = np.argmax(affinity_matrix[i])\n                affinity_score = affinity_matrix[i, j]\n            if affinity_score > self.iou_threshold:\n                assignment.append(indices[j])\n                affinity_matrix = np.delete(affinity_matrix, j, 1)\n                indices = np.delete(indices, j)\n            else:\n                assignment.append(None)\n\n        # Increase age for existing tracks.\n        for i in range(len(self.age)):\n            self.age[i] += 1\n\n        # Update existing tracks.\n        for i, j in enumerate(assignment):\n            if j is not None:\n                self.history[j] = masks[i]\n                self.history_areas[j] = areas[i]\n                self.age[j] = 0\n                assignment[i] = self.ids[j]\n\n        # Prune out too old tracks.\n        alive = tuple(i for i, age in enumerate(self.age) if age < self.age_threshold)\n        self.history = list(self.history[i] for i in alive)\n        self.history_areas = list(self.history_areas[i] for i in alive)\n        self.history_classes = list(self.history_classes[i] for i in alive)\n        self.age = list(self.age[i] for i in alive)\n        self.ids = list(self.ids[i] for i in alive)\n\n        # Save new tracks.\n        for i, j in enumerate(assignment):\n            if j is None:\n                self.history.append(masks[i])\n                self.history_areas.append(areas[i])\n                self.history_classes.append(classes[i])\n                self.age.append(0)\n                self.ids.append(self.last_id)\n                assignment[i] = self.last_id\n                self.last_id += 1\n\n        return assignment\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/training_engine.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\nimport math\nimport os\nimport traceback\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom os import path as osp\n\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nfrom segmentoly.data.dataparallel import ShallowDataParallel\nfrom segmentoly.data.transforms import *\nfrom segmentoly.utils.env import SuccessExit\nfrom segmentoly.utils.postprocess import postprocess\nfrom segmentoly.utils.profile import Timer\nimport segmentoly.utils.weights as weight_utils\n\n\nclass TrainingEngine(object):\n    def __init__(self):\n        self.identifier = None\n        self.description = \'\'\n\n    @staticmethod\n    def save_ckpt(output_dir, step, model, optimizer, save_extra_dict=True):\n        """"""Save checkpoint""""""\n        ckpt_dir = os.path.join(output_dir, \'ckpt\')\n        if not os.path.exists(ckpt_dir):\n            os.makedirs(ckpt_dir)\n        save_name = os.path.join(ckpt_dir, \'model_step_{}.pth\'.format(step))\n        if isinstance(model, nn.DataParallel):\n            model = model.module\n\n        # Create and save a dictionary with correctly matched parameter and its momentum buffer\n        corrected_matcher = {}\n        if save_extra_dict:\n            for group in optimizer.param_groups:\n                for p in group[\'params\']:\n                    param_state = optimizer.state[p]\n                    if \'momentum_buffer\' in param_state:\n                        buffer = param_state[\'momentum_buffer\'].cpu()\n                        corrected_matcher[p.cpu()] = buffer\n\n        torch.save(\n            {\'step\': step,\n             \'model\': model.state_dict(),\n             \'optimizer\': optimizer.state_dict(),\n             \'optimizer_corrected\': corrected_matcher\n             },\n            save_name\n        )\n        logging.info(\'save model to ""{}""\'.format(save_name))\n\n    @staticmethod\n    def load_checkpoint(net, optimizer, load_ckpt=None, load_backbone=None, resume=False):\n        start_step = 0\n        if load_ckpt:\n            logging.info(\'loading checkpoint ""{}""\'.format(load_ckpt))\n            checkpoint = torch.load(load_ckpt, map_location=lambda storage, loc: storage)\n            weight_utils.load_rcnn_ckpt(net, checkpoint[\'model\'])\n            if resume:\n                start_step = checkpoint[\'step\']\n                optimizer.load_state_dict(checkpoint[\'optimizer\'])\n\n                corrected_matcher = checkpoint.get(\'optimizer_corrected\', {})\n                if len(corrected_matcher) > 0:\n                    for group in optimizer.param_groups:\n                        for param_original in group[\'params\']:\n                            if \'momentum_buffer\' in optimizer.state[param_original]:\n                                for param_loaded, buffer in corrected_matcher.items():\n                                    shapes_are_equal = np.array_equal(list(param_original.shape),\n                                                                      list(param_loaded.shape))\n                                    if shapes_are_equal and torch.all(\n                                        torch.eq(param_original.data, param_loaded.data.cuda())):\n                                        optimizer.state[param_original][\'momentum_buffer\'] = buffer.cuda()\n                                        break\n                else:\n                    # If a checkpoint does not have additional dictionary with matched\n                    # parameters and its buffers, just match them by shapes\n                    if len(optimizer.state) > 0:  # It means that a checkpoint has momentum_buffer\n                        used_buffers = {}\n                        copy_buf = None\n                        for p in optimizer.state.keys():\n                            used_buffers[p] = False\n                        for group in optimizer.param_groups:\n                            for param in group[\'params\']:\n                                for p, buffer in optimizer.state.items():\n                                    if \'momentum_buffer\' not in buffer:\n                                        continue\n                                    if np.array_equal(list(param.shape), list(buffer[\'momentum_buffer\'].shape)) and not \\\n                                       used_buffers[p]:\n                                        copy_buf = optimizer.state[param][\'momentum_buffer\'].cuda()\n                                        optimizer.state[param][\'momentum_buffer\'] = buffer[\'momentum_buffer\'].cuda()\n                                        optimizer.state[p][\'momentum_buffer\'] = copy_buf.cuda()\n                                        used_buffers[param] = True\n                        del used_buffers\n                        del copy_buf\n                logging.info(\'Resume training from {} step\'.format(start_step))\n\n            del checkpoint\n            torch.cuda.empty_cache()\n\n        if load_backbone:\n            logging.info(\'loading backbone weights from ""{}""\'.format(load_backbone))\n            assert hasattr(net, \'backbone\')\n            weight_utils.load_checkpoint(net.backbone, load_backbone)\n\n        return start_step\n\n    @staticmethod\n    def adjust_virtual_iteration_size(available_gpus_num, total_batch_size, per_gpu_batch_size):\n        """"""Adjust training parameters taking into account available hardware resources.""""""\n\n        virtual_iter_size = int(math.ceil(total_batch_size / per_gpu_batch_size / available_gpus_num))\n        effective_batch_size = available_gpus_num * per_gpu_batch_size * virtual_iter_size\n        logging.info(\'Virtual iter size set to {}\'.format(virtual_iter_size))\n        logging.info(\'Effective batch size {}\'.format(effective_batch_size))\n        return virtual_iter_size\n\n    @staticmethod\n    def create_run_directory(root_directory):\n        # Determine output directory.\n        run_name = datetime.now().strftime(\'%b%d-%H-%M-%S\')\n        run_directory = osp.join(root_directory, run_name)\n        # Create output directory if necessary.\n        if not os.path.exists(run_directory):\n            os.makedirs(run_directory)\n        return run_directory\n\n    @staticmethod\n    def set_random_seeds():\n        # torch.backends.cudnn.deterministic = True\n        torch.cuda.manual_seed(0xACE)\n        torch.cuda.manual_seed_all(0xACE)\n        np.random.seed(0xACE)\n\n    def run(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass DefaultMaskRCNNTrainingEngine(TrainingEngine):\n    def __init__(self):\n        super().__init__()\n        self.set_random_seeds()\n\n        self.identifier = \'Default MaskRCNN training experiment template\'\n        self.description = \'This is just a template for actual training of MaskRCNN like networks.\'\n        self.root_directory = \'./\'\n        self.run_directory = \'./\'\n\n        self.batch_size = 1\n        self.virtual_iter_size = 16\n\n        self.training_data_loader = None\n        self.validation_data_loaders = []\n        self.validate_every = 5000\n\n        self.model = None\n\n        self.training_iterations_num, lr_scheduler_milestones = self.training_schedule(1)\n        self.optimizer = None\n        self.lr_scheduler = None\n\n        self.loggers = []\n        self.log_every = 20\n\n        self.checkpoint_every = 5000\n\n    @staticmethod\n    def training_schedule(multiplier):\n        training_iterations_num = int(90000 * multiplier)\n        milestones = [int(i * multiplier) for i in (60000, 80000)]\n        return training_iterations_num, milestones\n\n    def setup_optimizer(self, net, base_lr, base_weight_decay):\n        gn_param_nameset = set()\n        for name, module in net.named_modules():\n            if isinstance(module, nn.GroupNorm):\n                gn_param_nameset.add(name + \'.weight\')\n                gn_param_nameset.add(name + \'.bias\')\n        gn_params = []\n        gn_param_names = []\n        bias_params = []\n        bias_param_names = []\n        nonbias_params = []\n        nonbias_param_names = []\n        nograd_param_names = []\n        for key, value in dict(net.named_parameters()).items():\n            if value.requires_grad:\n                if \'bias\' in key:\n                    bias_params.append(value)\n                    bias_param_names.append(key)\n                elif key in gn_param_nameset:\n                    gn_params.append(value)\n                    gn_param_names.append(key)\n                else:\n                    nonbias_params.append(value)\n                    nonbias_param_names.append(key)\n            else:\n                nograd_param_names.append(key)\n        assert (gn_param_nameset - set(nograd_param_names) - set(bias_param_names)) == set(gn_param_names)\n\n        # Learning rate of 0 is a dummy value to be set properly at the start of training\n        params = [\n            {\'params\': nonbias_params,\n             \'lr\': 0,\n             \'initial_lr\': base_lr,\n             \'weight_decay\': base_weight_decay},\n            {\'params\': bias_params,\n             \'lr\': 0,\n             \'initial_lr\': base_lr,\n             \'weight_decay\': 0},\n            {\'params\': gn_params,\n             \'lr\': 0,\n             \'initial_lr\': base_lr,\n             \'weight_decay\': 0}\n        ]\n        return params\n\n    def validate(self, net, data_loader, idx=0):\n        net.eval()\n        logging.info(\'Processing the dataset...\')\n        boxes_all = []\n        masks_all = []\n        classes_all = []\n        scores_all = []\n        for data_batch in tqdm(iter(data_loader)):\n            im_data = data_batch[\'im_data\']\n            im_info = data_batch[\'im_info\']\n            with torch.no_grad():\n                boxes, classes, scores, _, masks = net(im_data, im_info)\n            meta = data_batch[\'meta\'][0]\n            scores, classes, boxes, masks = postprocess(scores, classes, boxes, masks,\n                                                        im_h=meta[\'original_size\'][0],\n                                                        im_w=meta[\'original_size\'][1],\n                                                        im_scale_y=meta[\'processed_size\'][0] /\n                                                                   meta[\'original_size\'][0],\n                                                        im_scale_x=meta[\'processed_size\'][1] /\n                                                                   meta[\'original_size\'][1],\n                                                        full_image_masks=True, encode_masks=True)\n            boxes_all.append(boxes)\n            masks_all.append(masks)\n            classes_all.append(classes)\n            scores_all.append(scores)\n\n        logging.info(\'Evaluating results...\')\n        evaluation_results = data_loader.dataset.evaluate(scores_all, classes_all, boxes_all, masks_all)\n        evaluation_results = {\'val{}/{}\'.format(idx, k): v for k, v in evaluation_results.items()}\n        return evaluation_results\n\n    @staticmethod\n    def update_metrics(aggregated_outputs, flattened_outputs):\n        for k, v in flattened_outputs.items():\n            aggregated_outputs[k].append(v)\n        return aggregated_outputs\n\n    def check_experimental_setup(self):\n        assert self.model is not None, \'Please specify the model to train.\'\n        assert self.optimizer is not None, \'Please specify optimizer.\'\n        assert self.lr_scheduler is not None, \'Please specify learning rate scheduler.\'\n        assert self.training_data_loader is not None, \'Please specify data loader for training data.\'\n\n    def run(self, start_step=0, *args, **kwargs):\n        self.check_experimental_setup()\n\n        available_gpus_num = torch.cuda.device_count() if torch.cuda.is_available() else 1\n        logging.info(\'Using {} GPUs for training\'.format(available_gpus_num))\n        # virtual_iter_size = self.adjust_virtual_iteration_size(available_gpus_num, self.batch_size_total,\n        #                                                        self.batch_size_per_gpu)\n        virtual_iter_size = self.virtual_iter_size\n\n        training_data_iterator = iter(self.training_data_loader)\n\n        if torch.cuda.is_available():\n            self.model.cuda()\n\n        model = ShallowDataParallel(self.model)\n\n        self.save_ckpt(self.run_directory, start_step, model, self.optimizer, False)\n\n        torch.cuda.empty_cache()\n\n        timers = defaultdict(Timer)\n        logging.info(\'Start training...\')\n        step = start_step\n        try:\n            for step in range(start_step, self.training_iterations_num):\n                with timers[\'total\']:\n                    if not self.model.training:\n                        model.train()\n                    self.lr_scheduler.step()\n                    lr = self.optimizer.param_groups[0][\'lr\']\n                    self.optimizer.zero_grad()\n\n                    metrics = defaultdict(list)\n                    minibatch_losses = []\n\n                    for virtual_iter in range(virtual_iter_size):\n                        try:\n                            data_batch = next(training_data_iterator)\n                        except StopIteration:\n                            training_data_iterator = iter(self.training_data_loader)\n                            data_batch = next(training_data_iterator)\n\n                        with timers[\'forward\']:\n                            minibatch_metrics, loss = model(**data_batch)\n\n                        # Average losses from different GPUs.\n                        total_loss = torch.mean(loss)\n                        with timers[\'backward\']:\n                            (total_loss / virtual_iter_size).backward()\n                        minibatch_losses.append(total_loss.detach_())\n\n                        self.update_metrics(metrics, minibatch_metrics)\n\n                    if step % self.log_every == 0:\n                        for k, v in metrics.items():\n                            # Average over all effective batch elements.\n                            metrics[k] = torch.mean(torch.cat(v))\n                        for training_logger in self.loggers:\n                            training_logger(step, self.training_iterations_num,\n                                            lr, sum(minibatch_losses) / len(minibatch_losses), metrics, timers)\n\n                    with timers[\'optimization_step\']:\n                        self.optimizer.step()\n\n                    if (step + 1) % self.checkpoint_every == 0:\n                        logging.info(\'Saving snapshot...\')\n                        self.save_ckpt(self.run_directory, step + 1, model, self.optimizer, False)\n\n                    if (step + 1) % self.validate_every == 0:\n                        logging.info(\'Running validation...\')\n                        with timers[\'validation\']:\n                            for validation_data_loader in self.validation_data_loaders:\n                                validation_results = self.validate(model, validation_data_loader)\n                                for training_logger in self.loggers:\n                                    training_logger(step + 1, self.training_iterations_num,\n                                                    None, None,\n                                                    validation_results, None)\n\n            # Save last checkpoint.\n            self.save_ckpt(self.run_directory, step + 1, model, self.optimizer, False)\n            raise SuccessExit(\'Program has finished successfully\')\n\n        except (RuntimeError, KeyboardInterrupt, SuccessExit) as err:\n            if not isinstance(err, SuccessExit):\n                logging.info(\'Save checkpoint on exception ...\')\n                self.save_ckpt(self.run_directory, step + 1, model, self.optimizer, False)\n                stack_trace_msg = \'\\n\' + str(traceback.format_exc())\n                logging.warning(stack_trace_msg)\n\n        finally:\n            try:\n                del self.training_data_loader\n            except ConnectionResetError:\n                pass\n\n            try:\n                for i, _ in enumerate(self.validation_data_loaders):\n                    del self.validation_data_loaders[i]\n            except ConnectionResetError:\n                pass\n\n            for training_logger in self.loggers:\n                training_logger.close()\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/visualizer.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport cv2\nimport numpy as np\n\n\nclass Visualizer(object):\n\n    def __init__(self, class_labels, confidence_threshold=0.5, show_boxes=False,\n                 show_masks=True, show_scores=False):\n        super().__init__()\n        self.class_labels = class_labels\n        self.confidence_threshold = confidence_threshold\n        self.class_color_palette = np.asarray([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n        self.instance_color_palette = self.color_palette\n        self.show_masks = show_masks\n        self.show_boxes = show_boxes\n        self.show_scores = show_scores\n\n    def __call__(self, image, boxes, classes, scores, segms=None, ids=None):\n        result = image.copy()\n\n        # Filter out detections with low confidence.\n        filter_mask = scores > self.confidence_threshold\n        scores = scores[filter_mask]\n        classes = classes[filter_mask]\n        boxes = boxes[filter_mask]\n\n        if self.show_masks and segms is not None:\n            segms = list(segm for segm, show in zip(segms, filter_mask) if show)\n            result = self.overlay_masks(result, segms, classes, ids)\n\n        if self.show_boxes:\n            result = self.overlay_boxes(result, boxes, classes)\n\n        result = self.overlay_class_names(result, boxes, classes, scores,\n                                          show_score=self.show_scores)\n        return result\n\n    def compute_colors_for_labels(self, labels):\n        colors = labels[:, None] * self.class_color_palette\n        colors = (colors % 255).astype(np.uint8)\n        return colors\n\n    def overlay_boxes(self, image, boxes, classes):\n        colors = self.compute_colors_for_labels(classes).tolist()\n        for box, color in zip(boxes, colors):\n            box = box.astype(int)\n            top_left, bottom_right = box[:2].tolist(), box[2:].tolist()\n            image = cv2.rectangle(\n                image, tuple(top_left), tuple(bottom_right), tuple(color), 1\n            )\n        return image\n\n    def overlay_masks(self, image, masks, classes, ids=None):\n        colors = self.compute_colors_for_labels(classes).tolist()\n\n        segments_image = image.copy()\n        aggregated_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n        aggregated_colored_mask = np.zeros(image.shape, dtype=np.uint8)\n        black = np.zeros(3, dtype=np.uint8)\n\n        for i, (mask, color) in enumerate(zip(masks, colors)):\n            mask = mask.astype(np.uint8)\n            color_idx = i if ids is None else ids[i]\n            mask_color = self.instance_color_palette[color_idx % len(self.instance_color_palette)].tolist()\n            cv2.bitwise_or(aggregated_mask, mask, dst=aggregated_mask)\n            cv2.bitwise_or(aggregated_colored_mask, np.asarray(mask_color, dtype=np.uint8),\n                           dst=aggregated_colored_mask, mask=mask)\n\n        # Fill the area occupied by all instances with a colored instances mask image.\n        cv2.bitwise_and(segments_image, black, dst=segments_image, mask=aggregated_mask)\n        cv2.bitwise_or(segments_image, aggregated_colored_mask, dst=segments_image, mask=aggregated_mask)\n        # Blend original image with the one, where instances are colored.\n        # As a result instances masks become transparent.\n        cv2.addWeighted(image, 0.5, segments_image, 0.5, 0, dst=image)\n\n        return image\n\n    def overlay_class_names(self, image, boxes, classes, scores, show_score=True):\n        labels = [self.class_labels[i] for i in classes]\n        template = \'{}: {:.2f}\' if show_score else \'{}\'\n        white = (255, 255, 255)\n\n        for box, score, label in zip(boxes, scores, labels):\n            s = template.format(label, score)\n            textsize = cv2.getTextSize(s, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n            position = ((box[:2] + box[2:] - textsize) / 2).astype(int)\n            cv2.putText(image, s, tuple(position), cv2.FONT_HERSHEY_SIMPLEX, .5, white, 1)\n\n        return image\n\n    color_palette = np.array([[0, 113, 188],\n                              [216, 82, 24],\n                              [236, 176, 31],\n                              [125, 46, 141],\n                              [118, 171, 47],\n                              [76, 189, 237],\n                              [161, 19, 46],\n                              [76, 76, 76],\n                              [153, 153, 153],\n                              [255, 0, 0],\n                              [255, 127, 0],\n                              [190, 190, 0],\n                              [0, 255, 0],\n                              [0, 0, 255],\n                              [170, 0, 255],\n                              [84, 84, 0],\n                              [84, 170, 0],\n                              [84, 255, 0],\n                              [170, 84, 0],\n                              [170, 170, 0],\n                              [170, 255, 0],\n                              [255, 84, 0],\n                              [255, 170, 0],\n                              [255, 255, 0],\n                              [0, 84, 127],\n                              [0, 170, 127],\n                              [0, 255, 127],\n                              [84, 0, 127],\n                              [84, 84, 127],\n                              [84, 170, 127],\n                              [84, 255, 127],\n                              [170, 0, 127],\n                              [170, 84, 127],\n                              [170, 170, 127],\n                              [170, 255, 127],\n                              [255, 0, 127],\n                              [255, 84, 127],\n                              [255, 170, 127],\n                              [255, 255, 127],\n                              [0, 84, 255],\n                              [0, 170, 255],\n                              [0, 255, 255],\n                              [84, 0, 255],\n                              [84, 84, 255],\n                              [84, 170, 255],\n                              [84, 255, 255],\n                              [170, 0, 255],\n                              [170, 84, 255],\n                              [170, 170, 255],\n                              [170, 255, 255],\n                              [255, 0, 255],\n                              [255, 84, 255],\n                              [255, 170, 255],\n                              [42, 0, 0],\n                              [84, 0, 0],\n                              [127, 0, 0],\n                              [170, 0, 0],\n                              [212, 0, 0],\n                              [255, 0, 0],\n                              [0, 42, 0],\n                              [0, 84, 0],\n                              [0, 127, 0],\n                              [0, 170, 0],\n                              [0, 212, 0],\n                              [0, 255, 0],\n                              [0, 0, 42],\n                              [0, 0, 84],\n                              [0, 0, 127],\n                              [0, 0, 170],\n                              [0, 0, 212],\n                              [0, 0, 255],\n                              [0, 0, 0],\n                              [36, 36, 36],\n                              [72, 72, 72],\n                              [109, 109, 109],\n                              [145, 145, 145],\n                              [182, 182, 182],\n                              [218, 218, 218],\n                              [255, 255, 255]], dtype=np.uint8)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/weight_converters.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\nimport pickle\nfrom collections import OrderedDict\nfrom shutil import copyfile\n\nimport torch\n\nfrom ..rcnn.backbones.resnet import ResBlock, ResBlockWithFusedBN, ResNetBody\n\n\ndef as_is_converter(src_model_path, dst_model_path):\n    copyfile(src_model_path, dst_model_path)\n\n\ndef maskrcnn_benchmark_models_converter(src_model_path, dst_model_path, weights_mapping):\n    state_dict = torch.load(src_model_path, map_location=\'cpu\')\n    state_dict = state_dict[\'model\']\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        if k in weights_mapping:\n            if weights_mapping[k]:\n                new_state_dict[weights_mapping[k]] = v\n        else:\n            logging.warning(\'Warning! No mapping for {}\'.format(k))\n    state_dict = new_state_dict\n    torch.save(dict(model=state_dict), dst_model_path)\n\n\ndef maskrcnn_benchmark_resnet50_c4_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = MaskRCNNBenchmarkWeightsMapper.resnet((3, 4, 6))\n    mapping.update(MaskRCNNBenchmarkWeightsMapper.heads())\n    maskrcnn_benchmark_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef maskrcnn_benchmark_resnet50_fpn_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = MaskRCNNBenchmarkWeightsMapper.resnet((3, 4, 6, 3))\n    mapping.update(MaskRCNNBenchmarkWeightsMapper.fpn_heads())\n    maskrcnn_benchmark_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef maskrcnn_benchmark_resnet101_fpn_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = MaskRCNNBenchmarkWeightsMapper.resnet((3, 4, 23, 3))\n    mapping.update(MaskRCNNBenchmarkWeightsMapper.fpn_heads())\n    maskrcnn_benchmark_models_converter(src_model_path, dst_model_path, mapping)\n\n\nclass MaskRCNNBenchmarkWeightsMapper(object):\n    @staticmethod\n    def resnet_stem():\n        mapping = {\n            \'module.backbone.body.stem.conv1.weight\': \'backbone.stages.stage_0.conv1.weight\',\n            \'module.backbone.body.stem.bn1.weight\': \'backbone.stages.stage_0.bn1.weight\',\n            \'module.backbone.body.stem.bn1.bias\': \'backbone.stages.stage_0.bn1.bias\',\n            \'module.backbone.body.stem.bn1.running_mean\': \'backbone.stages.stage_0.bn1.running_mean\',\n            \'module.backbone.body.stem.bn1.running_var\': \'backbone.stages.stage_0.bn1.running_var\'\n        }\n        return mapping\n\n    @staticmethod\n    def resnet_block(stage_idx, block_idx):\n        mapping_template = {\n            \'module.backbone.body.layer{}.{}.conv1.weight\': \'backbone.stages.stage_{}.{}.conv1.weight\',\n            \'module.backbone.body.layer{}.{}.bn1.weight\': \'backbone.stages.stage_{}.{}.bn1.weight\',\n            \'module.backbone.body.layer{}.{}.bn1.bias\': \'backbone.stages.stage_{}.{}.bn1.bias\',\n            \'module.backbone.body.layer{}.{}.bn1.running_mean\': \'backbone.stages.stage_{}.{}.bn1.running_mean\',\n            \'module.backbone.body.layer{}.{}.bn1.running_var\': \'backbone.stages.stage_{}.{}.bn1.running_var\',\n            \'module.backbone.body.layer{}.{}.conv2.weight\': \'backbone.stages.stage_{}.{}.conv2.weight\',\n            \'module.backbone.body.layer{}.{}.bn2.weight\': \'backbone.stages.stage_{}.{}.bn2.weight\',\n            \'module.backbone.body.layer{}.{}.bn2.bias\': \'backbone.stages.stage_{}.{}.bn2.bias\',\n            \'module.backbone.body.layer{}.{}.bn2.running_mean\': \'backbone.stages.stage_{}.{}.bn2.running_mean\',\n            \'module.backbone.body.layer{}.{}.bn2.running_var\': \'backbone.stages.stage_{}.{}.bn2.running_var\',\n            \'module.backbone.body.layer{}.{}.conv3.weight\': \'backbone.stages.stage_{}.{}.conv3.weight\',\n            \'module.backbone.body.layer{}.{}.bn3.weight\': \'backbone.stages.stage_{}.{}.bn3.weight\',\n            \'module.backbone.body.layer{}.{}.bn3.bias\': \'backbone.stages.stage_{}.{}.bn3.bias\',\n            \'module.backbone.body.layer{}.{}.bn3.running_mean\': \'backbone.stages.stage_{}.{}.bn3.running_mean\',\n            \'module.backbone.body.layer{}.{}.bn3.running_var\': \'backbone.stages.stage_{}.{}.bn3.running_var\',\n        }\n        mapping = {k.format(stage_idx, block_idx): v.format(stage_idx, block_idx)\n                   for k, v in mapping_template.items()}\n        return mapping\n\n    @staticmethod\n    def resnet_downscale_block(stage_idx, block_idx):\n        mapping_template = {\n            \'module.backbone.body.layer{}.{}.downsample.0.weight\': \'backbone.stages.stage_{}.{}.downsample.0.weight\',\n            \'module.backbone.body.layer{}.{}.downsample.1.weight\': \'backbone.stages.stage_{}.{}.downsample.1.weight\',\n            \'module.backbone.body.layer{}.{}.downsample.1.bias\': \'backbone.stages.stage_{}.{}.downsample.1.bias\',\n            \'module.backbone.body.layer{}.{}.downsample.1.running_mean\': \'backbone.stages.stage_{}.{}.downsample.1.running_mean\',\n            \'module.backbone.body.layer{}.{}.downsample.1.running_var\': \'backbone.stages.stage_{}.{}.downsample.1.running_var\'\n        }\n        mapping = {k.format(stage_idx, block_idx): v.format(stage_idx, block_idx)\n                   for k, v in mapping_template.items()}\n        mapping.update(MaskRCNNBenchmarkWeightsMapper.resnet_block(stage_idx, block_idx))\n        return mapping\n\n    @staticmethod\n    def resnet_stage(stage_idx, blocks_num):\n        mapping = {}\n        mapping.update(MaskRCNNBenchmarkWeightsMapper.resnet_downscale_block(stage_idx, 0))\n        for i in range(1, blocks_num):\n            mapping.update(MaskRCNNBenchmarkWeightsMapper.resnet_block(stage_idx, i))\n        return mapping\n\n    @staticmethod\n    def resnet(blocks_per_stage):\n        mapping = {}\n        mapping.update(MaskRCNNBenchmarkWeightsMapper.resnet_stem())\n        for stage_idx, blocks_num in enumerate(blocks_per_stage, 1):\n            mapping.update(MaskRCNNBenchmarkWeightsMapper.resnet_stage(stage_idx, blocks_num))\n        return mapping\n\n    @staticmethod\n    def fpn_heads():\n        mapping = {\n            \'module.backbone.fpn.fpn_inner1.weight\': \'fpn.topdown_lateral.2.conv_lateral.weight\',\n            \'module.backbone.fpn.fpn_inner1.bias\': \'fpn.topdown_lateral.2.conv_lateral.bias\',\n            \'module.backbone.fpn.fpn_layer1.weight\': \'fpn.posthoc.3.weight\',\n            \'module.backbone.fpn.fpn_layer1.bias\': \'fpn.posthoc.3.bias\',\n            \'module.backbone.fpn.fpn_inner2.weight\': \'fpn.topdown_lateral.1.conv_lateral.weight\',\n            \'module.backbone.fpn.fpn_inner2.bias\': \'fpn.topdown_lateral.1.conv_lateral.bias\',\n            \'module.backbone.fpn.fpn_layer2.weight\': \'fpn.posthoc.2.weight\',\n            \'module.backbone.fpn.fpn_layer2.bias\': \'fpn.posthoc.2.bias\',\n            \'module.backbone.fpn.fpn_inner3.weight\': \'fpn.topdown_lateral.0.conv_lateral.weight\',\n            \'module.backbone.fpn.fpn_inner3.bias\': \'fpn.topdown_lateral.0.conv_lateral.bias\',\n            \'module.backbone.fpn.fpn_layer3.weight\': \'fpn.posthoc.1.weight\',\n            \'module.backbone.fpn.fpn_layer3.bias\': \'fpn.posthoc.1.bias\',\n            \'module.backbone.fpn.fpn_inner4.weight\': \'fpn.conv_top.weight\',\n            \'module.backbone.fpn.fpn_inner4.bias\': \'fpn.conv_top.bias\',\n            \'module.backbone.fpn.fpn_layer4.weight\': \'fpn.posthoc.0.weight\',\n            \'module.backbone.fpn.fpn_layer4.bias\': \'fpn.posthoc.0.bias\',\n            \'module.rpn.anchor_generator.cell_anchors.0\': \'\',\n            \'module.rpn.anchor_generator.cell_anchors.1\': \'\',\n            \'module.rpn.anchor_generator.cell_anchors.2\': \'\',\n            \'module.rpn.anchor_generator.cell_anchors.3\': \'\',\n            \'module.rpn.anchor_generator.cell_anchors.4\': \'\',\n            \'module.rpn.head.conv.weight\': \'rpn.conv.weight\',\n            \'module.rpn.head.conv.bias\': \'rpn.conv.bias\',\n            \'module.rpn.head.cls_logits.weight\': \'rpn.cls_score.weight\',\n            \'module.rpn.head.cls_logits.bias\': \'rpn.cls_score.bias\',\n            \'module.rpn.head.bbox_pred.weight\': \'rpn.bbox_deltas.weight\',\n            \'module.rpn.head.bbox_pred.bias\': \'rpn.bbox_deltas.bias\',\n            \'module.roi_heads.box.feature_extractor.fc6.weight\': \'detection_head.fc1.weight\',\n            \'module.roi_heads.box.feature_extractor.fc6.bias\': \'detection_head.fc1.bias\',\n            \'module.roi_heads.box.feature_extractor.fc7.weight\': \'detection_head.fc2.weight\',\n            \'module.roi_heads.box.feature_extractor.fc7.bias\': \'detection_head.fc2.bias\',\n            \'module.roi_heads.box.predictor.cls_score.weight\': \'detection_head.cls_score.weight\',\n            \'module.roi_heads.box.predictor.cls_score.bias\': \'detection_head.cls_score.bias\',\n            \'module.roi_heads.box.predictor.bbox_pred.weight\': \'detection_head.bbox_pred.weight\',\n            \'module.roi_heads.box.predictor.bbox_pred.bias\': \'detection_head.bbox_pred.bias\',\n            \'module.roi_heads.mask.feature_extractor.mask_fcn1.weight\': \'mask_head.conv_fcn.0.weight\',\n            \'module.roi_heads.mask.feature_extractor.mask_fcn1.bias\': \'mask_head.conv_fcn.0.bias\',\n            \'module.roi_heads.mask.feature_extractor.mask_fcn2.weight\': \'mask_head.conv_fcn.2.weight\',\n            \'module.roi_heads.mask.feature_extractor.mask_fcn2.bias\': \'mask_head.conv_fcn.2.bias\',\n            \'module.roi_heads.mask.feature_extractor.mask_fcn3.weight\': \'mask_head.conv_fcn.4.weight\',\n            \'module.roi_heads.mask.feature_extractor.mask_fcn3.bias\': \'mask_head.conv_fcn.4.bias\',\n            \'module.roi_heads.mask.feature_extractor.mask_fcn4.weight\': \'mask_head.conv_fcn.6.weight\',\n            \'module.roi_heads.mask.feature_extractor.mask_fcn4.bias\': \'mask_head.conv_fcn.6.bias\',\n            \'module.roi_heads.mask.predictor.conv5_mask.weight\': \'mask_head.upconv.weight\',\n            \'module.roi_heads.mask.predictor.conv5_mask.bias\': \'mask_head.upconv.bias\',\n            \'module.roi_heads.mask.predictor.mask_fcn_logits.weight\': \'mask_head.segm.weight\',\n            \'module.roi_heads.mask.predictor.mask_fcn_logits.bias\': \'mask_head.segm.bias\'\n        }\n        return mapping\n\n    @staticmethod\n    def heads():\n        mapping = {\n            \'module.rpn.anchor_generator.cell_anchors.0\': \'\',\n            \'module.rpn.head.conv.weight\': \'rpn.conv.weight\',\n            \'module.rpn.head.conv.bias\': \'rpn.conv.bias\',\n            \'module.rpn.head.cls_logits.weight\': \'rpn.cls_score.weight\',\n            \'module.rpn.head.cls_logits.bias\': \'rpn.cls_score.bias\',\n            \'module.rpn.head.bbox_pred.weight\': \'rpn.bbox_deltas.weight\',\n            \'module.rpn.head.bbox_pred.bias\': \'rpn.bbox_deltas.bias\',\n\n            \'module.roi_heads.box.predictor.cls_score.weight\': \'detection_head.cls_score.weight\',\n            \'module.roi_heads.box.predictor.cls_score.bias\': \'detection_head.cls_score.bias\',\n            \'module.roi_heads.box.predictor.bbox_pred.weight\': \'detection_head.bbox_pred.weight\',\n            \'module.roi_heads.box.predictor.bbox_pred.bias\': \'detection_head.bbox_pred.bias\',\n\n            \'module.roi_heads.mask.predictor.conv5_mask.weight\': \'mask_head.upconv5.weight\',\n            \'module.roi_heads.mask.predictor.conv5_mask.bias\': \'mask_head.upconv5.bias\',\n            \'module.roi_heads.mask.predictor.mask_fcn_logits.weight\': \'mask_head.segm.weight\',\n            \'module.roi_heads.mask.predictor.mask_fcn_logits.bias\': \'mask_head.segm.bias\'\n        }\n        common_feature_extractor_mapping = MaskRCNNBenchmarkWeightsMapper.resnet_downscale_block(4, 0)\n        common_feature_extractor_mapping.update(MaskRCNNBenchmarkWeightsMapper.resnet_block(4, 1))\n        common_feature_extractor_mapping.update(MaskRCNNBenchmarkWeightsMapper.resnet_block(4, 2))\n        mapping.update({k.replace(\'backbone.body\', \'roi_heads.box.feature_extractor.head\'):\n                            v.replace(\'backbone.stages.stage_4\', \'common_detection_mask_head\')\n                        for k, v in common_feature_extractor_mapping.items()})\n        mapping.update({k.replace(\'backbone.body\', \'roi_heads.mask.feature_extractor.head\'): \'\'\n                        for k, v in common_feature_extractor_mapping.items()})\n        return mapping\n\n\ndef detectron_models_converter(src_model_path, dst_model_path, weights_mapping):\n    with open(src_model_path, \'rb\') as fp:\n        src_blobs = pickle.load(fp, encoding=\'latin1\')\n\n    if \'blobs\' in src_blobs:\n        src_blobs = src_blobs[\'blobs\']\n    state_dict = OrderedDict()\n    for k, v in src_blobs.items():\n        if k in weights_mapping:\n            if weights_mapping[k]:\n                state_dict[weights_mapping[k]] = torch.as_tensor(v)\n        elif k.endswith(\'_momentum\'):\n            pass\n        else:\n            logging.warning(\'Warning! No mapping for {}\'.format(k))\n    torch.save(dict(model=state_dict), dst_model_path)\n\n\ndef detectron_resnet50_c4_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 6), prefix=\'backbone.\')\n    mapping.update(DetectronWeightsMapper.heads())\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet50_fpn_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 6, 3), prefix=\'backbone.\')\n    mapping.update(DetectronWeightsMapper.fpn_heads((3, 4, 6, 3)))\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet50_gn_fpn_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 6, 3), prefix=\'backbone.\', normalization=\'gn\')\n    mapping.update(DetectronWeightsMapper.fpn_heads((3, 4, 6, 3)))\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet101_fpn_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 23, 3), prefix=\'backbone.\')\n    mapping.update(DetectronWeightsMapper.fpn_heads((3, 4, 23, 3)))\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet101_gn_fpn_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 23, 3), prefix=\'backbone.\', normalization=\'gn\')\n    mapping.update(DetectronWeightsMapper.fpn_heads((3, 4, 23, 3)))\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet152_fpn_mask_rcnn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 8, 36, 3), prefix=\'backbone.\')\n    mapping.update(DetectronWeightsMapper.fpn_heads((3, 8, 36, 3)))\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet50_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 6, 3))\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet50_gn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 6, 3), normalization=\'gn\')\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet101_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 23, 3))\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet101_gn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 4, 23, 3), normalization=\'gn\')\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet152_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 8, 36, 3))\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\ndef detectron_resnet152_gn_converter(src_model_path, dst_model_path):\n    mapping = DetectronWeightsMapper.resnet((3, 8, 36, 3), normalization=\'gn\')\n    detectron_models_converter(src_model_path, dst_model_path, mapping)\n\n\nclass DetectronWeightsMapper(object):\n    @staticmethod\n    def resnet_stem(prefix=\'\', normalization=\'bn\'):\n        mapping_template_weights = {\n            \'conv1_w\': \'{}stages.stage_0.conv1.weight\',\n            \'conv1_b\': \'\',\n        }\n        mapping = {k: v.format(prefix)\n                   for k, v in mapping_template_weights.items()}\n        p = \'res_\' if normalization == \'bn\' else \'\'\n        mapping_template_norm = {\n            \'{}conv1_{}_s\': \'{}stages.stage_0.bn1.weight\',\n            \'{}conv1_{}_b\': \'{}stages.stage_0.bn1.bias\',\n        }\n        mapping.update({k.format(p, normalization): v.format(prefix)\n                        for k, v in mapping_template_norm.items()})\n        return mapping\n\n    @staticmethod\n    def resnet_block(stage_idx, block_idx, prefix=\'\', normalization=\'bn\'):\n        mapping_template_weights = {\n            \'res{}_{}_branch2a_w\': \'{}stages.stage_{}.{}.conv1.weight\',\n            \'res{}_{}_branch2a_b\': \'\',\n            \'res{}_{}_branch2b_w\': \'{}stages.stage_{}.{}.conv2.weight\',\n            \'res{}_{}_branch2b_b\': \'\',\n            \'res{}_{}_branch2c_w\': \'{}stages.stage_{}.{}.conv3.weight\',\n            \'res{}_{}_branch2c_b\': \'\',\n        }\n        mapping = {k.format(stage_idx + 1, block_idx): v.format(prefix, stage_idx, block_idx)\n                   for k, v in mapping_template_weights.items()}\n        mapping_template_norm = {\n            \'res{}_{}_branch2a_{}_s\': \'{}stages.stage_{}.{}.bn1.weight\',\n            \'res{}_{}_branch2a_{}_b\': \'{}stages.stage_{}.{}.bn1.bias\',\n            \'res{}_{}_branch2b_{}_s\': \'{}stages.stage_{}.{}.bn2.weight\',\n            \'res{}_{}_branch2b_{}_b\': \'{}stages.stage_{}.{}.bn2.bias\',\n            \'res{}_{}_branch2c_{}_s\': \'{}stages.stage_{}.{}.bn3.weight\',\n            \'res{}_{}_branch2c_{}_b\': \'{}stages.stage_{}.{}.bn3.bias\',\n        }\n        mapping.update({k.format(stage_idx + 1, block_idx, normalization):\n                            v.format(prefix, stage_idx, block_idx)\n                        for k, v in mapping_template_norm.items()})\n        return mapping\n\n    @staticmethod\n    def resnet_downscale_block(stage_idx, block_idx, prefix=\'\', normalization=\'bn\'):\n        mapping_template_weights = {\n            \'res{}_{}_branch1_w\': \'{}stages.stage_{}.{}.downsample.0.weight\',\n            \'res{}_{}_branch1_b\': \'\',\n        }\n        mapping = {k.format(stage_idx + 1, block_idx): v.format(prefix, stage_idx, block_idx)\n                   for k, v in mapping_template_weights.items()}\n        mapping_template_norm = {\n            \'res{}_{}_branch1_{}_s\': \'{}stages.stage_{}.{}.downsample.1.weight\',\n            \'res{}_{}_branch1_{}_b\': \'{}stages.stage_{}.{}.downsample.1.bias\',\n        }\n        mapping.update({k.format(stage_idx + 1, block_idx, normalization): v.format(prefix, stage_idx, block_idx)\n                        for k, v in mapping_template_norm.items()})\n        mapping.update(DetectronWeightsMapper.resnet_block(stage_idx, block_idx, prefix, normalization))\n        return mapping\n\n    @staticmethod\n    def resnet_stage(stage_idx, blocks_num, prefix=\'\', normalization=\'bn\'):\n        mapping = DetectronWeightsMapper.resnet_downscale_block(stage_idx, 0, prefix, normalization)\n        for i in range(1, blocks_num):\n            mapping.update(DetectronWeightsMapper.resnet_block(stage_idx, i, prefix, normalization))\n        return mapping\n\n    @staticmethod\n    def resnet(blocks_per_stage, prefix=\'\', normalization=\'bn\'):\n        mapping = DetectronWeightsMapper.resnet_stem(prefix, normalization)\n        for stage_idx, blocks_num in enumerate(blocks_per_stage, 1):\n            mapping.update(DetectronWeightsMapper.resnet_stage(stage_idx, blocks_num, prefix, normalization))\n        return mapping\n\n    @staticmethod\n    def fpn_heads(blocks_per_stage):\n        assert len(blocks_per_stage) >= 4\n        mapping = {\n            \'fpn_inner_res5_{}_sum_w\'.format(blocks_per_stage[3] - 1): \'fpn.conv_top.weight\',\n            \'fpn_inner_res5_{}_sum_b\'.format(blocks_per_stage[3] - 1): \'fpn.conv_top.bias\',\n            \'fpn_inner_res4_{}_sum_lateral_w\'.format(blocks_per_stage[2] - 1): \'fpn.topdown_lateral.0.conv_lateral.weight\',\n            \'fpn_inner_res4_{}_sum_lateral_b\'.format(blocks_per_stage[2] - 1): \'fpn.topdown_lateral.0.conv_lateral.bias\',\n            \'fpn_inner_res3_{}_sum_lateral_w\'.format(blocks_per_stage[1] - 1): \'fpn.topdown_lateral.1.conv_lateral.weight\',\n            \'fpn_inner_res3_{}_sum_lateral_b\'.format(blocks_per_stage[1] - 1): \'fpn.topdown_lateral.1.conv_lateral.bias\',\n            \'fpn_inner_res2_{}_sum_lateral_w\'.format(blocks_per_stage[0] - 1): \'fpn.topdown_lateral.2.conv_lateral.weight\',\n            \'fpn_inner_res2_{}_sum_lateral_b\'.format(blocks_per_stage[0] - 1): \'fpn.topdown_lateral.2.conv_lateral.bias\',\n            \'fpn_res5_{}_sum_w\'.format(blocks_per_stage[3] - 1): \'fpn.posthoc.0.weight\',\n            \'fpn_res5_{}_sum_b\'.format(blocks_per_stage[3] - 1): \'fpn.posthoc.0.bias\',\n            \'fpn_res4_{}_sum_w\'.format(blocks_per_stage[2] - 1): \'fpn.posthoc.1.weight\',\n            \'fpn_res4_{}_sum_b\'.format(blocks_per_stage[2] - 1): \'fpn.posthoc.1.bias\',\n            \'fpn_res3_{}_sum_w\'.format(blocks_per_stage[1] - 1): \'fpn.posthoc.2.weight\',\n            \'fpn_res3_{}_sum_b\'.format(blocks_per_stage[1] - 1): \'fpn.posthoc.2.bias\',\n            \'fpn_res2_{}_sum_w\'.format(blocks_per_stage[0] - 1): \'fpn.posthoc.3.weight\',\n            \'fpn_res2_{}_sum_b\'.format(blocks_per_stage[0] - 1): \'fpn.posthoc.3.bias\',\n\n            \'conv_rpn_fpn2_w\': \'rpn.conv.weight\',\n            \'conv_rpn_fpn2_b\': \'rpn.conv.bias\',\n            \'rpn_cls_logits_fpn2_w\': \'rpn.cls_score.weight\',\n            \'rpn_cls_logits_fpn2_b\': \'rpn.cls_score.bias\',\n            \'rpn_bbox_pred_fpn2_w\': \'rpn.bbox_deltas.weight\',\n            \'rpn_bbox_pred_fpn2_b\': \'rpn.bbox_deltas.bias\',\n\n            \'fc6_w\': \'detection_head.fc1.weight\',\n            \'fc6_b\': \'detection_head.fc1.bias\',\n            \'fc7_w\': \'detection_head.fc2.weight\',\n            \'fc7_b\': \'detection_head.fc2.bias\',\n            \'cls_score_w\': \'detection_head.cls_score.weight\',\n            \'cls_score_b\': \'detection_head.cls_score.bias\',\n            \'bbox_pred_w\': \'detection_head.bbox_pred.weight\',\n            \'bbox_pred_b\': \'detection_head.bbox_pred.bias\',\n\n            \'_[mask]_fcn1_w\': \'mask_head.conv_fcn.0.weight\',\n            \'_[mask]_fcn1_b\': \'mask_head.conv_fcn.0.bias\',\n            \'_[mask]_fcn2_w\': \'mask_head.conv_fcn.2.weight\',\n            \'_[mask]_fcn2_b\': \'mask_head.conv_fcn.2.bias\',\n            \'_[mask]_fcn3_w\': \'mask_head.conv_fcn.4.weight\',\n            \'_[mask]_fcn3_b\': \'mask_head.conv_fcn.4.bias\',\n            \'_[mask]_fcn4_w\': \'mask_head.conv_fcn.6.weight\',\n            \'_[mask]_fcn4_b\': \'mask_head.conv_fcn.6.bias\',\n            \'conv5_mask_w\': \'mask_head.upconv.weight\',\n            \'conv5_mask_b\': \'mask_head.upconv.bias\',\n            \'mask_fcn_logits_w\': \'mask_head.segm.weight\',\n            \'mask_fcn_logits_b\': \'mask_head.segm.bias\'\n        }\n        return mapping\n\n    @staticmethod\n    def heads():\n        mapping = {\n            \'conv_rpn_w\': \'rpn.conv.weight\',\n            \'conv_rpn_b\': \'rpn.conv.bias\',\n            \'rpn_cls_logits_w\': \'rpn.cls_score.weight\',\n            \'rpn_cls_logits_b\': \'rpn.cls_score.bias\',\n            \'rpn_bbox_pred_w\': \'rpn.bbox_deltas.weight\',\n            \'rpn_bbox_pred_b\': \'rpn.bbox_deltas.bias\',\n\n            \'res5_0_branch2a_w\': \'common_detection_mask_head.0.conv1.weight\',\n            \'res5_0_branch2a_b\': \'\',\n            \'res5_0_branch2a_bn_s\': \'common_detection_mask_head.0.bn1.weight\',\n            \'res5_0_branch2a_bn_b\': \'common_detection_mask_head.0.bn1.bias\',\n            \'res5_0_branch2b_w\': \'common_detection_mask_head.0.conv2.weight\',\n            \'res5_0_branch2b_b\': \'\',\n            \'res5_0_branch2b_bn_s\': \'common_detection_mask_head.0.bn2.weight\',\n            \'res5_0_branch2b_bn_b\': \'common_detection_mask_head.0.bn2.bias\',\n            \'res5_0_branch2c_w\': \'common_detection_mask_head.0.conv3.weight\',\n            \'res5_0_branch2c_b\': \'\',\n            \'res5_0_branch2c_bn_s\': \'common_detection_mask_head.0.bn3.weight\',\n            \'res5_0_branch2c_bn_b\': \'common_detection_mask_head.0.bn3.bias\',\n            \'res5_0_branch1_w\': \'common_detection_mask_head.0.downsample.0.weight\',\n            \'res5_0_branch1_b\': \'\',\n            \'res5_0_branch1_bn_s\': \'common_detection_mask_head.0.downsample.1.weight\',\n            \'res5_0_branch1_bn_b\': \'common_detection_mask_head.0.downsample.1.bias\',\n            \'res5_1_branch2a_w\': \'common_detection_mask_head.1.conv1.weight\',\n            \'res5_1_branch2a_b\': \'\',\n            \'res5_1_branch2a_bn_s\': \'common_detection_mask_head.1.bn1.weight\',\n            \'res5_1_branch2a_bn_b\': \'common_detection_mask_head.1.bn1.bias\',\n            \'res5_1_branch2b_w\': \'common_detection_mask_head.1.conv2.weight\',\n            \'res5_1_branch2b_b\': \'\',\n            \'res5_1_branch2b_bn_s\': \'common_detection_mask_head.1.bn2.weight\',\n            \'res5_1_branch2b_bn_b\': \'common_detection_mask_head.1.bn2.bias\',\n            \'res5_1_branch2c_w\': \'common_detection_mask_head.1.conv3.weight\',\n            \'res5_1_branch2c_b\': \'\',\n            \'res5_1_branch2c_bn_s\': \'common_detection_mask_head.1.bn3.weight\',\n            \'res5_1_branch2c_bn_b\': \'common_detection_mask_head.1.bn3.bias\',\n            \'res5_2_branch2a_w\': \'common_detection_mask_head.2.conv1.weight\',\n            \'res5_2_branch2a_b\': \'\',\n            \'res5_2_branch2a_bn_s\': \'common_detection_mask_head.2.bn1.weight\',\n            \'res5_2_branch2a_bn_b\': \'common_detection_mask_head.2.bn1.bias\',\n            \'res5_2_branch2b_w\': \'common_detection_mask_head.2.conv2.weight\',\n            \'res5_2_branch2b_b\': \'\',\n            \'res5_2_branch2b_bn_s\': \'common_detection_mask_head.2.bn2.weight\',\n            \'res5_2_branch2b_bn_b\': \'common_detection_mask_head.2.bn2.bias\',\n            \'res5_2_branch2c_w\': \'common_detection_mask_head.2.conv3.weight\',\n            \'res5_2_branch2c_b\': \'\',\n            \'res5_2_branch2c_bn_s\': \'common_detection_mask_head.2.bn3.weight\',\n            \'res5_2_branch2c_bn_b\': \'common_detection_mask_head.2.bn3.bias\',\n\n            \'cls_score_w\': \'detection_head.cls_score.weight\',\n            \'cls_score_b\': \'detection_head.cls_score.bias\',\n            \'bbox_pred_w\': \'detection_head.bbox_pred.weight\',\n            \'bbox_pred_b\': \'detection_head.bbox_pred.bias\',\n\n            \'conv5_mask_w\': \'mask_head.upconv5.weight\',\n            \'conv5_mask_b\': \'mask_head.upconv5.bias\',\n            \'mask_fcn_logits_w\': \'mask_head.segm.weight\',\n            \'mask_fcn_logits_b\': \'mask_head.segm.bias\'\n        }\n        return mapping\n\n\ndef fuse_conv_and_bn(conv, bn, conv_bn_fused=None):\n    if conv_bn_fused is None:\n        conv_bn_fused = torch.nn.Conv2d(\n            conv.in_channels,\n            conv.out_channels,\n            kernel_size=conv.kernel_size,\n            stride=conv.stride,\n            padding=conv.padding,\n            groups=conv.groups,\n            bias=True\n        )\n    stddev_bn = torch.sqrt(bn.eps + bn.running_var)\n    w_bn = bn.weight.div(stddev_bn)\n    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(stddev_bn)\n\n    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n    if conv.bias is not None:\n        b_conv = conv.bias\n    else:\n        b_conv = torch.zeros(conv.weight.size(0))\n\n    conv_bn_fused.weight.copy_(torch.mm(torch.diag(w_bn), w_conv).view(conv_bn_fused.weight.size()))\n    conv_bn_fused.bias.copy_(b_conv.mul(w_bn) + b_bn)\n    return conv_bn_fused\n\n\ndef fuse_bns_in_resblock(src_block, dst_block):\n    assert isinstance(src_block, ResBlock)\n    assert isinstance(dst_block, ResBlockWithFusedBN)\n\n    with torch.no_grad():\n        # Main branch.\n        fuse_conv_and_bn(src_block.conv1, src_block.bn1, dst_block.conv1)\n        fuse_conv_and_bn(src_block.conv2, src_block.bn2, dst_block.conv2)\n        fuse_conv_and_bn(src_block.conv3, src_block.bn3, dst_block.conv3)\n\n        # Residual branch.\n        if src_block.downsample is not None:\n            assert hasattr(dst_block, \'downsample\')\n            fuse_conv_and_bn(src_block.downsample[0], src_block.downsample[1], dst_block.downsample)\n\n    return dst_block\n\n\ndef fuse_bns_resnet(resnet):\n    assert isinstance(resnet, ResNetBody)\n    resnet_fused_bn = ResNetBody(block_counts=resnet.block_counts, res_block=ResBlockWithFusedBN,\n                                 num_groups=resnet.num_groups, width_per_group=resnet.width_per_group,\n                                 res5_dilation=resnet.res5_dilation)\n    resnet_fused_bn.load_state_dict(resnet.state_dict(), strict=False)\n    dst_modules = dict(resnet_fused_bn.named_modules())\n    for name, m in resnet.named_modules():\n        if isinstance(m, ResBlock):\n            assert name in dst_modules, name\n            fuse_bns_in_resblock(m, dst_modules[name])\n    return resnet_fused_bn\n'"
pytorch_toolkit/instance_segmentation/segmentoly/utils/weights.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\nimport math\nimport operator\nfrom functools import reduce\n\ntry:\n    from termcolor import colored\nexcept ImportError:\n    colored = lambda x, *args, **kwargs: x\n\nimport torch\nfrom torch.nn import init\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_checkpoint(model, ckpt_path, mapping=None, verbose=False, skip_prefix=\'\'):\n    if mapping is None:\n        mapping = {}\n\n    ckpt = torch.load(ckpt_path, map_location=\'cpu\')\n    if \'model\' in ckpt:\n        ckpt = ckpt[\'model\']\n\n    from collections import OrderedDict\n    source_state = OrderedDict([(mapping.get(k, k), v) for k, v in ckpt.items()])\n    if skip_prefix:\n        source_state = OrderedDict([(k, v) for k, v in source_state.items() if not k.startswith(skip_prefix)])\n\n    target_state = model.state_dict()\n    if verbose:\n        logger.info(colored(\'missed weights:\', \'red\'))\n        logger.info(\'\\n\'.join([colored(x, \'red\') for x in target_state.keys() if x not in source_state]))\n        logger.info(colored(\'extra weights:\', \'magenta\'))\n        logger.info(\'\\n\'.join([colored(x, \'magenta\') for x in source_state.keys() if x not in target_state]))\n\n    source_state = OrderedDict({k:v for k, v in source_state.items() if k in target_state.keys()})\n\n    for name in source_state:\n        # Avoid error when number of classes was changed and sizes of tensors are different\n        tensor = operator.attrgetter(name)(model)\n        if tensor.data.shape != source_state[name].shape:\n            if tensor.data.numel() == source_state[name].numel():\n                source_state[name] = source_state[name].reshape(tensor.data.shape)\n            else:\n                logger.warning(\'Model and checkpoint have different sizes of tensors in \\\'{}\\\': {} vs {}. \'\n                               \'Weights will not be loaded\'.format(name, tensor.data.shape, source_state[name].shape))\n                continue\n\n    model.load_state_dict(state_dict=source_state, strict=False)\n\n\ndef load_rcnn_ckpt(model, ckpt):\n    """"""Load checkpoint""""""\n    state_dict = {}\n    for name in ckpt:\n        # Avoid error when number of classes was changed and sizes of tensors are different\n        tensor = operator.attrgetter(name)(model)\n        if tensor.data.shape != ckpt[name].shape:\n            if tensor.data.numel() == ckpt[name].numel():\n                ckpt[name] = ckpt[name].reshape(tensor.data.shape)\n            else:\n                logger.warning(\'Model and checkpoint have different sizes of tensors in \\\'{}\\\': {} vs {}. \'\n                               \'Weights will not be loaded\'.format(name, tensor.data.shape, ckpt[name].shape))\n                continue\n        state_dict[name] = ckpt[name]\n    model.load_state_dict(state_dict, strict=False)\n\n\ndef xavier_fill(tensor):\n    """"""Caffe2 XavierFill Implementation""""""\n    size = reduce(operator.mul, tensor.shape, 1)\n    fan_in = size / tensor.shape[0]\n    scale = math.sqrt(3 / fan_in)\n    return init.uniform_(tensor, -scale, scale)\n\n\ndef msra_fill(tensor):\n    """"""Caffe2 MSRAFill Implementation""""""\n    size = reduce(operator.mul, tensor.shape, 1)\n    fan_out = size / tensor.shape[1]\n    scale = math.sqrt(2 / fan_out)\n    return init.normal_(tensor, 0, scale)\n\n\ndef freeze_params(root_module, layer_type, freeze=True):\n    for m in root_module.modules():\n        if isinstance(m, layer_type):\n            for p in m.parameters():\n                p.requires_grad = not freeze\n\n\ndef set_train_mode(root_module, layer_type, mode=True):\n    for m in root_module.modules():\n        if isinstance(m, layer_type):\n            m.train(mode)\n\n\ndef get_group_gn(dim, dim_per_gp=-1, num_groups=32):\n    """"""Get number of groups used by GroupNorm, based on number of channels.""""""\n\n    assert dim_per_gp == -1 or num_groups == -1, \\\n        \'GroupNorm: can only specify G or C/G.\'\n\n    if dim_per_gp > 0:\n        assert dim % dim_per_gp == 0\n        group_gn = dim // dim_per_gp\n    else:\n        assert dim % num_groups == 0\n        group_gn = num_groups\n    return group_gn\n'"
pytorch_toolkit/nncf/examples/classification/binarization_worker.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport time\n\n\nimport copy\nimport os.path as osp\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\n\nfrom examples.classification.main import create_data_loaders, validate, AverageMeter, accuracy, get_lr, create_datasets\nfrom examples.common.distributed import configure_distributed\nfrom examples.common.example_logger import logger\nfrom examples.common.execution import ExecutionMode, get_device, prepare_model_for_execution\nfrom examples.common.model_loader import load_model\nfrom examples.common.utils import configure_logging, print_args, make_additional_checkpoints, get_name, print_statistics\nfrom nncf.binarization.algo import BinarizationController\nfrom nncf.model_creation import create_compressed_model\nfrom nncf.checkpoint_loading import load_state\nfrom nncf.utils import manual_seed, is_main_process\n\n\nclass KDLossCalculator:\n    def __init__(self, original_model, temperature=1.0):\n        self.original_model = original_model\n        self.original_model.eval()\n        self.temperature = temperature\n\n    def loss(self, inputs, binarized_network_outputs):\n        T = self.temperature\n        with torch.no_grad():\n            ref_output = self.original_model(inputs).detach()\n        kd_loss = -(nn.functional.log_softmax(binarized_network_outputs / T, dim=1) *\n                    nn.functional.softmax(ref_output / T, dim=1)).mean() * (T * T * binarized_network_outputs.shape[1])\n        return kd_loss\n\n\ndef get_binarization_optimizer(params_to_optimize, binarization_config):\n    params = binarization_config.get(""params"", {})\n    base_lr = params.get(""base_lr"", 1e-3)\n    base_wd = params.get(""base_wd"", 1e-5)\n    return torch.optim.Adam(params_to_optimize,\n                            lr=base_lr,\n                            weight_decay=base_wd)\n\n\nclass BinarizationOptimizerScheduler:\n    def __init__(self, optimizer, binarization_config):\n        params = binarization_config.get(\'params\', {})\n        self.base_lr = binarization_config.get(""base_lr"", 1e-3)\n        self.lr_poly_drop_start_epoch = params.get(\'lr_poly_drop_start_epoch\', None)\n        self.lr_poly_drop_duration_epochs = params.get(\'lr_poly_drop_duration_epochs\', 30)\n        self.disable_wd_start_epoch = params.get(\'disable_wd_start_epoch\', None)\n        self.optimizer = optimizer\n        self.last_epoch = 0\n\n    def step(self, epoch_fraction):\n        epoch_float = self.last_epoch + epoch_fraction\n        if self.lr_poly_drop_start_epoch is not None:\n            start = self.lr_poly_drop_start_epoch\n            finish = self.lr_poly_drop_start_epoch + self.lr_poly_drop_duration_epochs\n            if start <= epoch_float < finish:\n                lr = self.base_lr * pow(float(finish - epoch_float) / float(self.lr_poly_drop_duration_epochs), 2.1)\n                for group in self.optimizer.param_groups:\n                    group[\'lr\'] = lr\n\n        if self.disable_wd_start_epoch is not None:\n            if epoch_float > self.disable_wd_start_epoch:\n                for group in self.optimizer.param_groups:\n                    group[\'weight_decay\'] = 0.0\n\n    def epoch_step(self, epoch=None):\n        if epoch is not None:\n            self.last_epoch = epoch\n        self.last_epoch += 1\n\n    def load_state_dict(self, state_dict):\n        self.__dict__.update(state_dict)\n\n    def state_dict(self):\n        return self.__dict__\n\n\ndef main_worker_binarization(current_gpu, config):\n    config.current_gpu = current_gpu\n    config.distributed = config.execution_mode in (ExecutionMode.DISTRIBUTED, ExecutionMode.MULTIPROCESSING_DISTRIBUTED)\n    if config.distributed:\n        configure_distributed(config)\n\n    config.device = get_device(config)\n\n    if is_main_process():\n        configure_logging(logger, config)\n        print_args(config)\n\n    if config.seed is not None:\n        manual_seed(config.seed)\n        cudnn.deterministic = True\n        cudnn.benchmark = False\n\n    # create model\n    model_name = config[\'model\']\n    weights = config.get(\'weights\')\n    model = load_model(model_name,\n                       pretrained=config.get(\'pretrained\', True) if weights is None else False,\n                       num_classes=config.get(\'num_classes\', 1000),\n                       model_params=config.get(\'model_params\'))\n\n    original_model = copy.deepcopy(model)\n    compression_ctrl, model = create_compressed_model(model, config)\n    if not isinstance(compression_ctrl, BinarizationController):\n        raise RuntimeError(""The binarization sample worker may only be run with the binarization algorithm!"")\n\n    if weights:\n        load_state(model, torch.load(weights, map_location=\'cpu\'))\n\n    model, _ = prepare_model_for_execution(model, config)\n    original_model.to(config.device)\n\n    if config.distributed:\n        compression_ctrl.distributed()\n\n    is_inception = \'inception\' in model_name\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss()\n    criterion = criterion.to(config.device)\n\n    params_to_optimize = model.parameters()\n\n    compression_config = config[\'compression\']\n    binarization_config = compression_config if isinstance(compression_config, dict) else compression_config[0]\n    optimizer = get_binarization_optimizer(params_to_optimize, binarization_config)\n    optimizer_scheduler = BinarizationOptimizerScheduler(optimizer, binarization_config)\n    kd_loss_calculator = KDLossCalculator(original_model)\n\n    resuming_checkpoint = config.resuming_checkpoint\n    best_acc1 = 0\n    # optionally resume from a checkpoint\n    if resuming_checkpoint is not None:\n        model, config, optimizer, optimizer_scheduler, kd_loss_calculator, compression_ctrl, best_acc1 = \\\n            resume_from_checkpoint(resuming_checkpoint, model,\n                                   config, optimizer, optimizer_scheduler, kd_loss_calculator, compression_ctrl)\n\n    if config.to_onnx is not None:\n        compression_ctrl.export_model(config.to_onnx)\n        logger.info(""Saved to {}"".format(config.to_onnx))\n        return\n\n    if config.execution_mode != ExecutionMode.CPU_ONLY:\n        cudnn.benchmark = True\n\n    # Data loading code\n    train_dataset, val_dataset = create_datasets(config)\n    train_loader, train_sampler, val_loader = create_data_loaders(config, train_dataset, val_dataset)\n\n    if config.mode.lower() == \'test\':\n        print_statistics(compression_ctrl.statistics())\n        validate(val_loader, model, criterion, config)\n\n    if config.mode.lower() == \'train\':\n        if not resuming_checkpoint:\n            compression_ctrl.initialize(data_loader=train_loader, criterion=criterion)\n\n        batch_multiplier = (binarization_config.get(""params"", {})).get(""batch_multiplier"", 1)\n        train_bin(config, compression_ctrl, model, criterion, is_inception, optimizer_scheduler, model_name, optimizer,\n                  train_loader, train_sampler, val_loader, kd_loss_calculator, batch_multiplier, best_acc1)\n\n\ndef train_bin(config, compression_ctrl, model, criterion, is_inception, optimizer_scheduler, model_name, optimizer,\n              train_loader, train_sampler, val_loader, kd_loss_calculator, batch_multiplier, best_acc1=0):\n    for epoch in range(config.start_epoch, config.epochs):\n        config.cur_epoch = epoch\n        if config.distributed:\n            train_sampler.set_epoch(epoch)\n\n        # train for one epoch\n        train_epoch_bin(train_loader, batch_multiplier, model, criterion, optimizer, optimizer_scheduler,\n                        kd_loss_calculator, compression_ctrl, epoch, config, is_inception)\n\n        # compute compression algo statistics\n        stats = compression_ctrl.statistics()\n\n        acc1 = best_acc1\n        if epoch % config.test_every_n_epochs == 0:\n            # evaluate on validation set\n            acc1, _ = validate(val_loader, model, criterion, config)\n\n        # remember best acc@1 and save checkpoint\n        is_best = acc1 > best_acc1\n        best_acc1 = max(acc1, best_acc1)\n\n        # update compression scheduler state at the end of the epoch\n        compression_ctrl.scheduler.epoch_step()\n        optimizer_scheduler.epoch_step()\n\n        if is_main_process():\n            print_statistics(stats)\n\n            checkpoint_path = osp.join(config.checkpoint_save_dir, get_name(config) + \'_last.pth\')\n            checkpoint = {\n                \'epoch\': epoch + 1,\n                \'arch\': model_name,\n                \'state_dict\': model.state_dict(),\n                \'original_model_state_dict\': kd_loss_calculator.original_model.state_dict(),\n                \'best_acc1\': best_acc1,\n                \'optimizer\': optimizer.state_dict(),\n                \'compression_scheduler\': compression_ctrl.scheduler.state_dict(),\n                \'optimizer_scheduler\': optimizer_scheduler.state_dict()\n            }\n\n            torch.save(checkpoint, checkpoint_path)\n            make_additional_checkpoints(checkpoint_path, is_best, epoch + 1, config)\n\n            for key, value in stats.items():\n                if isinstance(value, (int, float)):\n                    config.tb.add_scalar(""compression/statistics/{0}"".format(key), value, len(train_loader) * epoch)\n\n\ndef resume_from_checkpoint(resuming_checkpoint, model, config, optimizer, optimizer_scheduler, kd_loss_calculator,\n                           compression_ctrl):\n    best_acc1 = 0\n    if osp.isfile(resuming_checkpoint):\n        logger.info(""=> loading checkpoint \'{}\'"".format(resuming_checkpoint))\n        checkpoint = torch.load(resuming_checkpoint, map_location=\'cpu\')\n        load_state(model, checkpoint[\'state_dict\'], is_resume=True)\n        if config.mode.lower() == \'train\' and config.to_onnx is None:\n            config.start_epoch = checkpoint[\'epoch\']\n            best_acc1 = checkpoint[\'best_acc1\']\n            kd_loss_calculator.original_model.load_state_dict(checkpoint[\'original_model_state_dict\'])\n            compression_ctrl.scheduler.load_state_dict(checkpoint[\'compression_scheduler\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            optimizer_scheduler.load_state_dict(checkpoint[\'optimizer_scheduler\'])\n            logger.info(""=> loaded checkpoint \'{}\' (epoch: {}, best_acc1: {:.3f})""\n                        .format(resuming_checkpoint, checkpoint[\'epoch\'], best_acc1))\n        else:\n            logger.info(""=> loaded checkpoint \'{}\'"".format(resuming_checkpoint))\n    else:\n        raise FileNotFoundError(""no checkpoint found at \'{}\'"".format(resuming_checkpoint))\n    return model, config, optimizer, optimizer_scheduler, kd_loss_calculator, compression_ctrl, best_acc1\n\n\ndef train_epoch_bin(train_loader, batch_multiplier, model, criterion, optimizer,\n                    optimizer_scheduler: BinarizationOptimizerScheduler, kd_loss_calculator: KDLossCalculator,\n                    compression_ctrl, epoch, config, is_inception=False):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    kd_losses_meter = AverageMeter()\n    criterion_losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    compression_scheduler = compression_ctrl.scheduler\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input_, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input_ = input_.to(config.device)\n        target = target.to(config.device)\n\n        # compute output\n        if is_inception:\n            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n            output, aux_outputs = model(input_)\n            loss1 = criterion(output, target)\n            loss2 = criterion(aux_outputs, target)\n            criterion_loss = loss1 + 0.4 * loss2\n        else:\n            output = model(input_)\n            criterion_loss = criterion(output, target)\n\n        # compute KD loss\n        kd_loss = kd_loss_calculator.loss(input_, output)\n        loss = criterion_loss + kd_loss\n\n        # measure accuracy and record loss\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input_.size(0))\n        comp_loss_val = kd_loss.item()\n        kd_losses_meter.update(comp_loss_val, input_.size(0))\n        criterion_losses.update(criterion_loss.item(), input_.size(0))\n        top1.update(acc1, input_.size(0))\n        top1.update(acc1, input_.size(0))\n        top5.update(acc5, input_.size(0))\n\n        # compute gradient and do SGD step\n        if i % batch_multiplier == 0:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        else:\n            loss.backward()\n\n        compression_scheduler.step()\n        optimizer_scheduler.step(float(i) / len(train_loader))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % config.print_freq == 0:\n            logger.info(\n                \'{rank}: \'\n                \'Epoch: [{0}][{1}/{2}] \'\n                \'Lr: {3:.3} \'\n                \'Wd: {4:.3} \'\n                \'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) \'\n                \'Data: {data_time.val:.3f} ({data_time.avg:.3f}) \'\n                \'CE_loss: {ce_loss.val:.4f} ({ce_loss.avg:.4f}) \'\n                \'KD_loss: {kd_loss.val:.4f} ({kd_loss.avg:.4f}) \'\n                \'Loss: {loss.val:.4f} ({loss.avg:.4f}) \'\n                \'Acc@1: {top1.val:.3f} ({top1.avg:.3f}) \'\n                \'Acc@5: {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                    epoch, i, len(train_loader), get_lr(optimizer), get_wd(optimizer), batch_time=batch_time,\n                    data_time=data_time, ce_loss=criterion_losses, kd_loss=kd_losses_meter,\n                    loss=losses, top1=top1, top5=top5,\n                    rank=\'{}:\'.format(config.rank) if config.multiprocessing_distributed else \'\'\n                ))\n\n        if is_main_process():\n            global_step = len(train_loader) * epoch\n            config.tb.add_scalar(""train/learning_rate"", get_lr(optimizer), i + global_step)\n            config.tb.add_scalar(""train/criterion_loss"", criterion_losses.avg, i + global_step)\n            config.tb.add_scalar(""train/kd_loss"", kd_losses_meter.avg, i + global_step)\n            config.tb.add_scalar(""train/loss"", losses.avg, i + global_step)\n            config.tb.add_scalar(""train/top1"", top1.avg, i + global_step)\n            config.tb.add_scalar(""train/top5"", top5.avg, i + global_step)\n\n            for stat_name, stat_value in compression_ctrl.statistics().items():\n                if isinstance(stat_value, (int, float)):\n                    config.tb.add_scalar(\'train/statistics/{}\'.format(stat_name), stat_value, i + global_step)\n\n\ndef get_wd(optimizer):\n    return optimizer.param_groups[0][\'weight_decay\']\n'"
pytorch_toolkit/nncf/examples/classification/main.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\nimport sys\nimport time\nfrom pathlib import Path\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport warnings\nfrom functools import partial\nfrom shutil import copyfile\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchvision.datasets import CIFAR10, CIFAR100\n\nfrom examples.common.argparser import get_common_argument_parser\nfrom examples.common.distributed import configure_distributed\nfrom examples.common.example_logger import logger\nfrom examples.common.execution import ExecutionMode, get_device, get_execution_mode, \\\n    prepare_model_for_execution, start_worker\nfrom examples.common.model_loader import load_model\nfrom examples.common.optimizer import get_parameter_groups, make_optimizer\nfrom examples.common.utils import configure_logging, configure_paths, create_code_snapshot, \\\n    print_args, make_additional_checkpoints, get_name, is_binarization, print_statistics\nfrom examples.common.utils import write_metrics\nfrom nncf.utils import manual_seed, safe_thread_call, is_main_process\n\nfrom nncf import Config, create_compressed_model, load_state\nfrom nncf.dynamic_graph.graph_builder import create_input_infos\nmodel_names = sorted(name for name in models.__dict__\n                     if name.islower() and not name.startswith(""__"")\n                     and callable(models.__dict__[name]))\n\ndef get_argument_parser():\n    parser = get_common_argument_parser()\n    parser.add_argument(\n        ""--dataset"",\n        help=""Dataset to use."",\n        choices=[""imagenet"", ""cifar100"", ""cifar10""],\n        default=None\n    )\n    parser.add_argument(\'--test-every-n-epochs\', default=1, type=int,\n                        help=\'Enables running validation every given number of epochs\')\n    return parser\n\n\ndef main(argv):\n    parser = get_argument_parser()\n    args = parser.parse_args(args=argv)\n    config = Config.from_json(args.config)\n    config.update_from_args(args, parser)\n    if config.dist_url == ""env://"":\n        config.update_from_env()\n\n    configure_paths(config)\n    copyfile(args.config, osp.join(config.log_dir, \'config.json\'))\n    source_root = Path(__file__).absolute().parents[2]  # nncf root\n    create_code_snapshot(source_root, osp.join(config.log_dir, ""snapshot.tar.gz""))\n\n    if config.seed is not None:\n        warnings.warn(\'You have chosen to seed training. \'\n                      \'This will turn on the CUDNN deterministic setting, \'\n                      \'which can slow down your training considerably! \'\n                      \'You may see unexpected behavior when restarting \'\n                      \'from checkpoints.\')\n\n    config.execution_mode = get_execution_mode(config)\n\n    if config.metrics_dump is not None:\n        write_metrics(0, config.metrics_dump)\n\n    if not is_binarization(config):\n        start_worker(main_worker, config)\n    else:\n        from examples.classification.binarization_worker import main_worker_binarization\n        start_worker(main_worker_binarization, config)\n\n\ndef main_worker(current_gpu, config):\n    config.current_gpu = current_gpu\n    config.distributed = config.execution_mode in (ExecutionMode.DISTRIBUTED, ExecutionMode.MULTIPROCESSING_DISTRIBUTED)\n    if config.distributed:\n        configure_distributed(config)\n\n    config.device = get_device(config)\n\n    if is_main_process():\n        configure_logging(logger, config)\n        print_args(config)\n\n    if config.seed is not None:\n        manual_seed(config.seed)\n        cudnn.deterministic = True\n        cudnn.benchmark = False\n\n    # create model\n    model_name = config[\'model\']\n    weights = config.get(\'weights\')\n    model = load_model(model_name,\n                       pretrained=config.get(\'pretrained\', True) if weights is None else False,\n                       num_classes=config.get(\'num_classes\', 1000),\n                       model_params=config.get(\'model_params\'))\n    compression_ctrl, model = create_compressed_model(model, config)\n    if weights:\n        load_state(model, torch.load(weights, map_location=\'cpu\'))\n    model, _ = prepare_model_for_execution(model, config)\n    if config.distributed:\n        compression_ctrl.distributed()\n\n    is_inception = \'inception\' in model_name\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss()\n    criterion = criterion.to(config.device)\n\n    params_to_optimize = get_parameter_groups(model, config)\n    optimizer, lr_scheduler = make_optimizer(params_to_optimize, config)\n\n    resuming_checkpoint = config.resuming_checkpoint\n    best_acc1 = 0\n    # optionally resume from a checkpoint\n    if resuming_checkpoint is not None:\n        model, config, optimizer, compression_ctrl, best_acc1 = \\\n            resume_from_checkpoint(resuming_checkpoint, model,\n                                   config, optimizer, compression_ctrl)\n\n    if config.to_onnx is not None:\n        compression_ctrl.export_model(config.to_onnx)\n        logger.info(""Saved to {}"".format(config.to_onnx))\n        return\n\n    if config.execution_mode != ExecutionMode.CPU_ONLY:\n        cudnn.benchmark = True\n\n    # Data loading code\n    train_dataset, val_dataset = create_datasets(config)\n    train_loader, train_sampler, val_loader = create_data_loaders(config, train_dataset, val_dataset)\n\n    if config.mode.lower() == \'test\':\n        print_statistics(compression_ctrl.statistics())\n        validate(val_loader, model, criterion, config)\n\n    if config.mode.lower() == \'train\':\n        if not resuming_checkpoint:\n            compression_ctrl.initialize(data_loader=train_loader, criterion=criterion)\n        train(config, compression_ctrl, model, criterion, is_inception, lr_scheduler, model_name, optimizer,\n              train_loader, train_sampler, val_loader, best_acc1)\n\n\ndef train(config, compression_ctrl, model, criterion, is_inception, lr_scheduler, model_name, optimizer,\n          train_loader, train_sampler, val_loader, best_acc1=0):\n    for epoch in range(config.start_epoch, config.epochs):\n        config.cur_epoch = epoch\n        if config.distributed:\n            train_sampler.set_epoch(epoch)\n\n        # train for one epoch\n        train_epoch(train_loader, model, criterion, optimizer, compression_ctrl, epoch, config, is_inception)\n\n        # Learning rate scheduling should be applied after optimizer\xe2\x80\x99s update\n        lr_scheduler.step(epoch if not isinstance(lr_scheduler, ReduceLROnPlateau) else best_acc1)\n\n        # update compression scheduler state at the end of the epoch\n        compression_ctrl.scheduler.epoch_step()\n\n        # compute compression algo statistics\n        stats = compression_ctrl.statistics()\n\n        acc1 = best_acc1\n        if epoch % config.test_every_n_epochs == 0:\n            # evaluate on validation set\n            acc1, _ = validate(val_loader, model, criterion, config)\n\n        # remember best acc@1 and save checkpoint\n        is_best = acc1 > best_acc1\n        best_acc1 = max(acc1, best_acc1)\n        acc = best_acc1 / 100\n        if config.metrics_dump is not None:\n            write_metrics(acc, config.metrics_dump)\n        if is_main_process():\n            print_statistics(stats)\n\n            checkpoint_path = osp.join(config.checkpoint_save_dir, get_name(config) + \'_last.pth\')\n            checkpoint = {\n                \'epoch\': epoch + 1,\n                \'arch\': model_name,\n                \'state_dict\': model.state_dict(),\n                \'best_acc1\': best_acc1,\n                \'acc1\': acc1,\n                \'optimizer\': optimizer.state_dict(),\n                \'scheduler\': compression_ctrl.scheduler.state_dict()\n            }\n\n            torch.save(checkpoint, checkpoint_path)\n            make_additional_checkpoints(checkpoint_path, is_best, epoch + 1, config)\n\n            for key, value in stats.items():\n                if isinstance(value, (int, float)):\n                    config.tb.add_scalar(""compression/statistics/{0}"".format(key), value, len(train_loader) * epoch)\n\n\ndef resume_from_checkpoint(resuming_checkpoint, model, config, optimizer, compression_ctrl):\n    best_acc1 = 0\n    if osp.isfile(resuming_checkpoint):\n        logger.info(""=> loading checkpoint \'{}\'"".format(resuming_checkpoint))\n        checkpoint = torch.load(resuming_checkpoint, map_location=\'cpu\')\n        load_state(model, checkpoint[\'state_dict\'], is_resume=True)\n        if config.mode.lower() == \'train\' and config.to_onnx is None:\n            config.start_epoch = checkpoint[\'epoch\']\n            best_acc1 = checkpoint[\'best_acc1\']\n            compression_ctrl.scheduler.load_state_dict(checkpoint[\'scheduler\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            logger.info(""=> loaded checkpoint \'{}\' (epoch: {}, best_acc1: {:.3f})""\n                        .format(resuming_checkpoint, checkpoint[\'epoch\'], best_acc1))\n        else:\n            logger.info(""=> loaded checkpoint \'{}\'"".format(resuming_checkpoint))\n    else:\n        raise FileNotFoundError(""no checkpoint found at \'{}\'"".format(resuming_checkpoint))\n    return model, config, optimizer, compression_ctrl, best_acc1\n\n\ndef get_dataset(dataset_config, config, transform, is_train):\n    if dataset_config == \'imagenet\':\n        prefix = \'train\' if is_train else \'val\'\n        return datasets.ImageFolder(osp.join(config.dataset_dir, prefix), transform)\n    return create_cifar(config, dataset_config, is_train, transform)\n\n\ndef create_cifar(config, dataset_config, is_train, transform):\n    create_cifar_fn = None\n    if dataset_config == \'cifar100\':\n        create_cifar_fn = partial(CIFAR100, config.dataset_dir, train=is_train, transform=transform)\n    if dataset_config == \'cifar10\':\n        create_cifar_fn = partial(CIFAR10, config.dataset_dir, train=is_train, transform=transform)\n    if create_cifar_fn:\n        return safe_thread_call(partial(create_cifar_fn, download=True), partial(create_cifar_fn, download=False))\n    return None\n\n\ndef create_datasets(config):\n    dataset_config = config.dataset if config.dataset is not None else \'imagenet\'\n    dataset_config = dataset_config.lower()\n    assert dataset_config in [\'imagenet\', \'cifar100\', \'cifar10\'], ""Unknown dataset option""\n\n    if dataset_config == \'imagenet\':\n        normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                                         std=(0.229, 0.224, 0.225))\n    elif dataset_config == \'cifar100\':\n        normalize = transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n                                         std=(0.2023, 0.1994, 0.2010))\n    elif dataset_config == \'cifar10\':\n        normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5),\n                                         std=(0.5, 0.5, 0.5))\n\n    input_info_list = create_input_infos(config)\n    image_size = input_info_list[0].shape[-1]\n    size = int(image_size / 0.875)\n    val_transform = transforms.Compose([\n        transforms.Resize(size),\n        transforms.CenterCrop(image_size),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    val_dataset = get_dataset(dataset_config, config, val_transform, is_train=False)\n    if config.mode.lower() == ""test"":\n        return None, val_dataset\n\n    train_transforms = transforms.Compose([\n        transforms.RandomResizedCrop(image_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    train_dataset = get_dataset(dataset_config, config, train_transforms, is_train=True)\n\n    return train_dataset, val_dataset\n\n\ndef create_data_loaders(config, train_dataset, val_dataset):\n    pin_memory = config.execution_mode != ExecutionMode.CPU_ONLY\n\n    # When using a single GPU per process and per\n    # DistributedDataParallel, we need to divide the batch size\n    # ourselves based on the total number of GPUs we have\n    batch_size = int(config.batch_size)\n    workers = int(config.workers)\n    if config.execution_mode == ExecutionMode.MULTIPROCESSING_DISTRIBUTED:\n        batch_size //= config.ngpus_per_node\n        workers //= config.ngpus_per_node\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=batch_size, shuffle=False,\n        num_workers=workers, pin_memory=pin_memory)\n\n    if config.mode.lower() == ""test"":\n        return None, None, val_loader\n\n    train_sampler = None\n    if config.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),\n        num_workers=workers, pin_memory=pin_memory, sampler=train_sampler)\n    return train_loader, train_sampler, val_loader\n\n\ndef train_epoch(train_loader, model, criterion, optimizer, compression_ctrl, epoch, config, is_inception=False):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    compression_losses = AverageMeter()\n    criterion_losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    compression_scheduler = compression_ctrl.scheduler\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input_, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input_ = input_.to(config.device)\n        target = target.to(config.device)\n\n        # compute output\n        if is_inception:\n            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n            output, aux_outputs = model(input_)\n            loss1 = criterion(output, target)\n            loss2 = criterion(aux_outputs, target)\n            criterion_loss = loss1 + 0.4 * loss2\n        else:\n            output = model(input_)\n            criterion_loss = criterion(output, target)\n\n        # compute compression loss\n        compression_loss = compression_ctrl.loss()\n        loss = criterion_loss + compression_loss\n\n        # measure accuracy and record loss\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input_.size(0))\n        comp_loss_val = compression_loss.item() if isinstance(compression_loss, torch.Tensor) else compression_loss\n        compression_losses.update(comp_loss_val, input_.size(0))\n        criterion_losses.update(criterion_loss.item(), input_.size(0))\n        top1.update(acc1, input_.size(0))\n        top5.update(acc5, input_.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        compression_scheduler.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % config.print_freq == 0:\n            logger.info(\n                \'{rank}: \'\n                \'Epoch: [{0}][{1}/{2}] \'\n                \'Lr: {3:.3} \'\n                \'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) \'\n                \'Data: {data_time.val:.3f} ({data_time.avg:.3f}) \'\n                \'CE_loss: {ce_loss.val:.4f} ({ce_loss.avg:.4f}) \'\n                \'CR_loss: {cr_loss.val:.4f} ({cr_loss.avg:.4f}) \'\n                \'Loss: {loss.val:.4f} ({loss.avg:.4f}) \'\n                \'Acc@1: {top1.val:.3f} ({top1.avg:.3f}) \'\n                \'Acc@5: {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                    epoch, i, len(train_loader), get_lr(optimizer), batch_time=batch_time,\n                    data_time=data_time, ce_loss=criterion_losses, cr_loss=compression_losses,\n                    loss=losses, top1=top1, top5=top5,\n                    rank=\'{}:\'.format(config.rank) if config.multiprocessing_distributed else \'\'\n                ))\n\n        if is_main_process():\n            global_step = len(train_loader) * epoch\n            config.tb.add_scalar(""train/learning_rate"", get_lr(optimizer), i + global_step)\n            config.tb.add_scalar(""train/criterion_loss"", criterion_losses.avg, i + global_step)\n            config.tb.add_scalar(""train/compression_loss"", compression_losses.avg, i + global_step)\n            config.tb.add_scalar(""train/loss"", losses.avg, i + global_step)\n            config.tb.add_scalar(""train/top1"", top1.avg, i + global_step)\n            config.tb.add_scalar(""train/top5"", top5.avg, i + global_step)\n\n            for stat_name, stat_value in compression_ctrl.statistics().items():\n                if isinstance(stat_value, (int, float)):\n                    config.tb.add_scalar(\'train/statistics/{}\'.format(stat_name), stat_value, i + global_step)\n\n\ndef validate(val_loader, model, criterion, config):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (input_, target) in enumerate(val_loader):\n            input_ = input_.to(config.device)\n            target = target.to(config.device)\n\n            # compute output\n            output = model(input_)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            losses.update(loss, input_.size(0))\n            top1.update(acc1, input_.size(0))\n            top5.update(acc5, input_.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % config.print_freq == 0:\n                logger.info(\n                    \'{rank}\'\n                    \'Test: [{0}/{1}] \'\n                    \'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) \'\n                    \'Loss: {loss.val:.4f} ({loss.avg:.4f}) \'\n                    \'Acc@1: {top1.val:.3f} ({top1.avg:.3f}) \'\n                    \'Acc@5: {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                        i, len(val_loader), batch_time=batch_time, loss=losses,\n                        top1=top1, top5=top5,\n                        rank=\'{}:\'.format(config.rank) if config.multiprocessing_distributed else \'\'\n                    ))\n\n        if is_main_process():\n            config.tb.add_scalar(""val/loss"", losses.avg, len(val_loader) * config.get(\'cur_epoch\', 0))\n            config.tb.add_scalar(""val/top1"", top1.avg, len(val_loader) * config.get(\'cur_epoch\', 0))\n            config.tb.add_scalar(""val/top5"", top5.avg, len(val_loader) * config.get(\'cur_epoch\', 0))\n\n        logger.info(\' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\\n\'.format(top1=top1, top5=top5))\n\n        acc = top1.avg / 100\n        if config.metrics_dump is not None:\n            write_metrics(acc, config.metrics_dump)\n\n    return top1.avg, top5.avg\n\n\nclass AverageMeter:\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = None\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the accuracy over the k top predictions for the specified values of k""""""\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size).item())\n        return res\n\n\ndef get_lr(optimizer):\n    return optimizer.param_groups[0][\'lr\']\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n'"
pytorch_toolkit/nncf/examples/common/__init__.py,0,b''
pytorch_toolkit/nncf/examples/common/argparser.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom nncf.config import CustomArgumentParser\nfrom nncf.hw_config import HWConfigType\n\n\ndef get_common_argument_parser():\n    """"""Defines command-line arguments, and parses them.\n\n    """"""\n    parser = CustomArgumentParser()\n\n    parser.add_argument(\'-c\', \'--config\', help=\'Path to a config file with task/model-specific parameters\',\n                        required=True)\n\n    parser.add_argument(\'--hw-config\', help=\'Type of the hardware configuration for compression algorithms\',\n                        type=str,\n                        default=None,\n                        dest=""hw_config_type"",\n                        choices=[t.value for t in HWConfigType])\n\n    parser.add_argument(\n        ""--mode"",\n        ""-m"",\n        choices=[\'train\', \'test\'],\n        default=\'train\',\n        help=(""train: performs training and validation; test: tests the model""\n              ""found in \\""--save_dir\\"" with name \\""--name\\"" on the validation split of \\""--dataset\\""; ""))\n\n    parser.add_argument(\'--metrics-dump\', type=str, help=\'Name of metrics collecting .json file\')\n    model_init_mode = parser.add_mutually_exclusive_group()\n    model_init_mode.add_argument(\n        ""--resume"",\n        metavar=\'PATH\',\n        type=str,\n        default=None,\n        dest=\'resuming_checkpoint\',\n        help=""Specifies the .pth file with the saved model to be tested (for \\""-m test\\""""\n             ""or to be resumed from (for \\""-m train\\""). The model architecture should ""\n             ""correspond to what is specified in the config file, and the checkpoint file""\n             ""must have all necessary optimizer/compression algorithm/metric states required."")\n    model_init_mode.add_argument(\n        ""--weights"",\n        metavar=\'PATH\',\n        type=str,\n        default=None,\n        help=""Attempt to load the model state from the specified .pth file. ""\n             ""This allows to start new compression algorithm from scratch with initializing model by given state"")\n\n    parser.add_argument(\n        ""--checkpoint-save-dir"",\n        metavar=\'PATH\',\n        type=str,\n        default=None,\n        help=""Specifies the directory for the trained model checkpoints to be saved to"")\n\n    parser.add_argument(\n        ""--pretrained"",\n        dest=""pretrained"",\n        help=""Use pretrained models from the model zoo"",\n        action=""store_true"",\n    )\n\n    execution_type = parser.add_mutually_exclusive_group()\n    execution_type.add_argument(\n        ""--gpu-id"",\n        type=int,\n        metavar=\'N\',\n        help=""The ID of the GPU training will be performed on, without any parallelization""\n    )\n    execution_type.add_argument(\n        ""--multiprocessing-distributed"",\n        action=\'store_true\',\n        help=""Specifies that the computations should be parallelized using ""\n             ""PyTorch DistributedDataParallel with training launched ""\n             ""in a separate process for each available GPU. This is the ""\n             ""fastest way to use PyTorch for either single-node or ""\n             ""multi-node data parallel training""\n    )\n    execution_type.add_argument(\'--cpu-only\', action=\'store_true\',\n                                help=\'Specifies that the computation should be performed\'\n                                     \'using CPU only\')\n\n    parser.add_argument(\'--world-size\', default=1, type=int,\n                        help=\'Sets the number of nodes participating in training\')\n    parser.add_argument(\'--dist-url\', default=\'tcp://127.0.0.1:8899\',\n                        help=\'URL used to set up distributed training\')\n    parser.add_argument(\'--rank\', default=0, type=int,\n                        help=\'Node rank for distributed training\')\n    parser.add_argument(\'--dist-backend\', default=\'nccl\', type=str,\n                        help=\'Distributed backend\')\n\n    # Hyperparameters\n    parser.add_argument(\n        ""--batch-size"",\n        ""-b"",\n        type=int,\n        default=10,\n        metavar=\'N\',\n        help=""Batch size. Will be split equally between multiple GPUs in the ""\n             ""--multiprocessing-distributed mode.""\n             ""Default: 10"")\n    parser.add_argument(\n        ""--epochs"",\n        type=int,\n        default=300,\n        help=""Number of training epochs. Default: 300"")\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'Set starting epoch number manually (useful on restarts)\')\n    parser.add_argument(\'--seed\', default=None, type=int,\n                        help=\'Specific seed for initializing pseudo-random number\'\n                             \'generators.\')\n\n    # Dataset\n    parser.add_argument(\n        ""--data"",\n        dest=""dataset_dir"",\n        type=str,\n        help=""Path to the root directory of the selected dataset. "")\n\n    # Settings\n    parser.add_argument(\n        \'-j\', \'--workers\',\n        type=int,\n        metavar=\'N\',\n        default=4,\n        help=""Number of subprocesses to use for data loading. Default: 4"")\n    parser.add_argument(\n        ""--print-step"",\n        action=\'store_true\',\n        help=""Print loss every step"")\n    parser.add_argument(\n        ""--imshow-batch"",\n        action=\'store_true\',\n        help=(""Displays batch images when loading the dataset and making ""\n              ""predictions.""))\n\n    # Storage settings\n    parser.add_argument(\n        ""--log-dir"",\n        type=str,\n        default=\'runs\',\n        help=""The directory where models and TensorboardX summaries""\n             "" are saved. Default: runs"")\n\n    parser.add_argument(\'--save-freq\', default=5, type=int,\n                        help=\'Checkpoint save frequency (epochs). Default: 5\')\n\n    parser.add_argument(\'--to-onnx\', type=str, metavar=\'PATH\', default=None,\n                        help=\'Export to ONNX model by given path\')\n\n    # Display\n    parser.add_argument(\'-p\', \'--print-freq\', default=10, type=int,\n                        metavar=\'N\', help=\'Print frequency (batch iterations). \'\n                                          \'Default: 10)\')\n\n    return parser\n'"
pytorch_toolkit/nncf/examples/common/distributed.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import print_function\n\nimport os\nimport torch\nfrom torch import distributed as dist\nfrom torch.utils.data import Sampler\n\nfrom examples.common.example_logger import logger\n\n\ndef configure_distributed(config):\n    if config.dist_url == ""env://"" and config.rank == -1:\n        config.rank = int(os.environ[""RANK""])\n    config.ngpus_per_node = torch.cuda.device_count()\n\n    if config.current_gpu is not None:\n        # Distributed multiprocessing\n        config.rank = config.rank * config.ngpus_per_node + config.current_gpu\n\n    logger.info(\'| distributed init (rank {}): {}\'.format(\n        config.rank, config.dist_url))\n    dist.init_process_group(backend=config.dist_backend, init_method=config.dist_url,\n                            world_size=config.world_size, rank=config.rank)\n    config.world_size = dist.get_world_size()\n\n\nclass DistributedSampler(Sampler):\n    def __init__(self, dataset, rank=None, world_size=None):\n        super().__init__(dataset)\n        if world_size is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            world_size = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.world_size = world_size\n        self.rank = rank\n        indices = list(range(len(dataset)))\n        self.samples_per_rank = (len(indices) - 1) // self.world_size + 1\n        self.indices = indices[self.rank * self.samples_per_rank: (self.rank + 1) * self.samples_per_rank]\n\n        if len(self.indices) < self.samples_per_rank:\n            # Workaround for mock datasets with a small number of entries\n            pad = [0] * (self.samples_per_rank - len(self.indices))\n            self.indices += pad\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    def __len__(self):\n        return len(self.indices)\n'"
pytorch_toolkit/nncf/examples/common/example_logger.py,0,"b'import logging\nimport sys\n\nlogger = logging.getLogger(""example"")\n_LOGGER_INITIALIZED = False\n\nif not _LOGGER_INITIALIZED:\n    logger.setLevel(logging.INFO)\n    hdl = logging.StreamHandler(stream=sys.stdout)\n    hdl.setFormatter(logging.Formatter(""%(message)s""))\n    hdl.setLevel(logging.INFO)\n    logger.addHandler(hdl)\n'"
pytorch_toolkit/nncf/examples/common/execution.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.multiprocessing as mp\n\n\nclass ExecutionMode:\n    CPU_ONLY = ""cpu_only""\n    SINGLE_GPU = ""single_gpu""\n\n    # Multiple nodes, each with 1 process utilising all local GPUs\n    DISTRIBUTED = ""distributed""\n\n    # Multiple nodes, each with 1 process for each local GPU\n    MULTIPROCESSING_DISTRIBUTED = ""multiprocessing_distributed""\n\n    # Single node with 1 process utilising all local GPUs\n    GPU_DATAPARALLEL = ""gpu_dataparallel""\n\n\ndef get_execution_mode(config):\n    if config.cpu_only:\n        return ExecutionMode.CPU_ONLY\n    if config.gpu_id is not None:\n        return ExecutionMode.SINGLE_GPU\n    if config.multiprocessing_distributed:\n        return ExecutionMode.MULTIPROCESSING_DISTRIBUTED\n    if config.world_size > 1:\n        return ExecutionMode.DISTRIBUTED\n    return ExecutionMode.GPU_DATAPARALLEL\n\n\ndef get_device(config):\n    if config.execution_mode == ExecutionMode.CPU_ONLY:\n        return ""cpu""\n    if config.current_gpu is not None:\n        return ""cuda:{}"".format(config.current_gpu)\n\n    return ""cuda""\n\n\ndef prepare_model_for_execution(model, config):\n    model_without_dp = model\n\n    # TODO: enable this. SyncBatchNorm only works with GPU-tensors, so it cannot\n    # be moved to create_compressed_model, but if we do the conversion here, then\n    # the dynamic graph becomes incorrect.\n    #\n    # if config.distributed:\n    #     try:\n    #         from torch.nn import SyncBatchNorm\n    #         model = SyncBatchNorm.convert_sync_batchnorm(model)\n    #     except ImportError:\n    #         print(""Current PyTorch version does not support SyncBatchNorm!"")\n\n    model.to(config.device)\n\n    if config.execution_mode == ExecutionMode.MULTIPROCESSING_DISTRIBUTED:\n        # For multiprocessing distributed, DistributedDataParallel constructor\n        # should always set the single device scope, otherwise,\n        # DistributedDataParallel will use all available devices.\n        torch.cuda.set_device(config.current_gpu)\n        model = torch.nn.parallel.distributed.DistributedDataParallel(model, device_ids=[config.current_gpu])\n        model_without_dp = model.module\n\n    if config.execution_mode == ExecutionMode.DISTRIBUTED:\n        # DistributedDataParallel will divide and allocate batch_size to all\n        # available GPUs if device_ids are not set\n        model = torch.nn.parallel.DistributedDataParallel(model)\n        model_without_dp = model.module\n\n    if config.execution_mode == ExecutionMode.SINGLE_GPU:\n        torch.cuda.set_device(config.current_gpu)\n\n    if config.execution_mode == ExecutionMode.GPU_DATAPARALLEL:\n        # DataParallel will divide and allocate batch_size to all available GPUs\n        model = torch.nn.DataParallel(model)\n        model_without_dp = model.module\n\n    return model, model_without_dp\n\n\ndef start_worker(main_worker, config):\n    if config.execution_mode == ExecutionMode.CPU_ONLY:\n        main_worker(current_gpu=None, config=config)\n        return\n\n    if config.execution_mode == ExecutionMode.SINGLE_GPU:\n        main_worker(current_gpu=config.gpu_id, config=config)\n        return\n\n    if config.execution_mode == ExecutionMode.GPU_DATAPARALLEL:\n        main_worker(current_gpu=None, config=config)\n        return\n\n    if config.execution_mode == ExecutionMode.MULTIPROCESSING_DISTRIBUTED:\n        # Since we have ngpus_per_node processes per node, the total world_size\n        # needs to be adjusted accordingly\n        config.ngpus_per_node = torch.cuda.device_count()\n        config.world_size = config.ngpus_per_node * config.world_size\n        # Use torch.multiprocessing.spawn to launch distributed processes: the\n        # main_worker process function\n        mp.spawn(main_worker, nprocs=config.ngpus_per_node, args=(config,))\n'"
pytorch_toolkit/nncf/examples/common/model_loader.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom functools import partial\n\nimport torchvision.models\n\nimport examples.common.models as custom_models\nfrom examples.common.example_logger import logger\nfrom nncf.utils import safe_thread_call\n\n\ndef load_model(model, pretrained=True, num_classes=1000, model_params=None):\n    logger.info(""Loading model: {}"".format(model))\n    if model_params is None:\n        model_params = {}\n    if model in torchvision.models.__dict__:\n        load_model_fn = partial(torchvision.models.__dict__[model], num_classes=num_classes, pretrained=pretrained,\n                                **model_params)\n    elif model in custom_models.__dict__:\n        load_model_fn = partial(custom_models.__dict__[model], num_classes=num_classes, pretrained=pretrained,\n                                **model_params)\n    else:\n        raise Exception(""Undefined model name"")\n    return safe_thread_call(load_model_fn)\n'"
pytorch_toolkit/nncf/examples/common/optimizer.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport re\n\nfrom torch.optim import SGD, Adam\nfrom torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau, StepLR, LambdaLR, ExponentialLR\n\n\ndef get_parameter_groups(model, config):\n    optim_config = config.get(\'optimizer\', {})\n    base_lr = optim_config.get(\'base_lr\', 1e-4)\n    weight_decay = optim_config.get(\'weight_decay\', get_default_weight_decay(config))\n\n    if \'parameter_groups\' not in optim_config:\n        return [\n            {\'lr\': base_lr, \'weight_decay\': weight_decay, \'params\': model.parameters()}\n        ]\n\n    param_groups = optim_config.parameter_groups\n    for param_name, param in model.named_parameters():\n        group = None\n        found = False\n        for group in param_groups:\n            # find first matched group for a given param\n            if re.search(group.get(\'re\', \'\'), param_name):\n                found = True\n                break\n        if found:\n            group.setdefault(\'params\', []).append(param)\n    return param_groups\n\n\ndef make_optimizer(params_to_optimize, config):\n    optim_config = config.get(\'optimizer\', {})\n\n    optim_type = optim_config.get(\'type\', \'adam\').lower()\n    optim_params = optim_config.get(""optimizer_params"", {})\n    if optim_type == \'adam\':\n        optim = Adam(params_to_optimize, **optim_params)\n    elif optim_type == \'sgd\':\n        optim_params = optim_config.get(""optimizer_params"", {""momentum"": 0.9})\n        optim = SGD(params_to_optimize, **optim_params)\n    else:\n        raise KeyError(""Unknown optimizer type: {}"".format(optim_type))\n\n    scheduler_type = optim_config.get(\'schedule_type\', \'step\').lower()\n    scheduler_params = optim_config.get(""scheduler_params"", {})\n\n    gamma = optim_config.get(\'gamma\', 0.1)\n    if scheduler_type == \'multistep\':\n        scheduler = MultiStepLR(optim, optim_config.get(\'steps\'), gamma=gamma,\n                                **scheduler_params)\n    elif scheduler_type == \'step\':\n        scheduler = StepLR(optim, step_size=optim_config.get(\'step\', 30), gamma=gamma,\n                           **scheduler_params)\n    elif scheduler_type == \'plateau\':\n        scheduler_params = optim_config.get(""scheduler_params"", {\'threshold\': 0.1})\n        scheduler = ReduceLROnPlateau(optim, factor=gamma, mode=\'max\', threshold_mode=\'abs\',\n                                      **scheduler_params)\n    elif scheduler_type == \'poly\':\n        scheduler_params = optim_config.get(""scheduler_params"", {\'power\': 0.9})\n        power = scheduler_params.power\n        poly_lambda = lambda epoch: (1 - epoch / config.epochs) ** power\n        scheduler = LambdaLR(optim, poly_lambda)\n    elif scheduler_type == \'exp\':\n        scheduler = ExponentialLR(optim, gamma)\n    else:\n        raise KeyError(""Unknown scheduler type: {}"".format(scheduler_type))\n\n    return optim, scheduler\n\n\ndef get_default_weight_decay(config):\n    compression_configs = config.get(\'compression\', {})\n    if isinstance(compression_configs, dict):\n        compression_configs = [compression_configs]\n\n    weight_decay = 1e-4\n    for compression_config in compression_configs:\n        if compression_config.get(\'algorithm\') == \'rb_sparsity\':\n            weight_decay = 0\n\n    return weight_decay\n'"
pytorch_toolkit/nncf/examples/common/utils.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport datetime\nimport json\nimport logging\nimport pdb\nimport sys\nfrom os import path as osp\nfrom pathlib import Path\n\nimport os\nimport tarfile\nfrom shutil import copyfile\nfrom tensorboardX import SummaryWriter\nfrom texttable import Texttable\n\nfrom examples.common.example_logger import logger as default_logger\n\nGENERAL_LOG_FILE_NAME = ""output.log""\nNNCF_LOG_FILE_NAME = ""nncf_output.log""\n\n\ndef get_name(config):\n    dataset = config.get(\'dataset\', \'imagenet\')\n    if dataset is None:\n        dataset = \'imagenet\'\n    retval = config[""model""] + ""_"" + dataset\n    compression_config = config.get(\'compression\', [])\n    if not isinstance(compression_config, list):\n        compression_config = [compression_config, ]\n    for algo_dict in compression_config:\n        algo_name = algo_dict[""algorithm""]\n        if algo_name == ""quantization"":\n            initializer = algo_dict.get(""initializer"", {})\n            precision = initializer.get(""precision"", {})\n            if precision:\n                retval += ""_mixed_int""\n            else:\n                activations = algo_dict.get(\'activations\', {})\n                a_bits = activations.get(\'bits\', 8)\n                weights = algo_dict.get(\'weights\', {})\n                w_bits = weights.get(\'bits\', 8)\n                if a_bits == w_bits:\n                    retval += ""_int{}"".format(a_bits)\n                else:\n                    retval += ""_a_int{}_w_int{}"".format(a_bits, w_bits)\n        else:\n            retval += ""_{}"".format(algo_name)\n    return retval\n\n\ndef write_metrics(acc, filename):\n    avg = round(acc * 100, 2)\n    metrics = {""Accuracy"": avg}\n    if os.path.isfile(filename):\n        path = Path(filename)\n        data = json.loads(path.read_text(encoding=\'utf-8\'))\n        data.update(metrics)\n        path.write_text(json.dumps(data, indent=2), encoding=\'utf-8\')\n    else:\n        with open(filename, \'w\') as outfile:\n            json.dump(metrics, outfile)\n\n\ndef configure_paths(config):\n    d = datetime.datetime.now()\n    run_id = \'{:%Y-%m-%d__%H-%M-%S}\'.format(d)\n    config.name = get_name(config)\n    config.log_dir = osp.join(config.log_dir, ""{}/{}"".format(config.name, run_id))\n    os.makedirs(config.log_dir)\n\n    if config.checkpoint_save_dir is None:\n        config.checkpoint_save_dir = config.log_dir\n\n    # create aux dirs\n    config.intermediate_checkpoints_path = config.log_dir + \'/intermediate_checkpoints\'\n    os.makedirs(config.intermediate_checkpoints_path)\n    os.makedirs(config.checkpoint_save_dir, exist_ok=True)\n\n\ndef configure_logging(sample_logger, config):\n    config.tb = SummaryWriter(config.log_dir)\n\n    training_pipeline_log_file_handler = logging.FileHandler(osp.join(config.log_dir, GENERAL_LOG_FILE_NAME))\n    training_pipeline_log_file_handler.setFormatter(logging.Formatter(""%(message)s""))\n    sample_logger.addHandler(training_pipeline_log_file_handler)\n\n    nncf_log_file_handler = logging.FileHandler(osp.join(config.log_dir, NNCF_LOG_FILE_NAME))\n    nncf_log_file_handler.setFormatter(logging.Formatter(""%(levelname)s:%(name)s:%(message)s""))\n    from nncf.nncf_logger import logger as nncf_logger\n    nncf_logger.addHandler(nncf_log_file_handler)\n\n\ndef is_on_first_rank(config):\n    return not config.multiprocessing_distributed or (config.multiprocessing_distributed\n                                                      and config.rank % config.ngpus_per_node == 0)\n\n\ndef create_code_snapshot(root, dst_path, extensions=("".py"", "".json"", "".cpp"", "".cu"")):\n    """"""Creates tarball with the source code""""""\n    with tarfile.open(str(dst_path), ""w:gz"") as tar:\n        for path in Path(root).rglob(""*""):\n            if \'.git\' in path.parts:\n                continue\n            if path.suffix.lower() in extensions:\n                tar.add(path.as_posix(), arcname=path.relative_to(root).as_posix(), recursive=True)\n\n\ndef print_args(config, logger=default_logger):\n    for arg in sorted(config):\n        logger.info(""{: <27s}: {}"".format(arg, config.get(arg)))\n\n\ndef make_link(src, dst, exists_ok=True):\n    if osp.exists(dst) and exists_ok:\n        os.remove(dst)\n    dev1 = os.stat(osp.dirname(dst)).st_dev\n    dev2 = os.stat(src).st_dev\n    if dev1 != dev2:\n        copyfile(src, dst)\n    else:\n        os.link(src, dst)\n\n\ndef make_additional_checkpoints(checkpoint_path, is_best, epoch, config):\n    if is_best:\n        best_path = osp.join(config.checkpoint_save_dir, \'{}_best.pth\'.format(config.name))\n        copyfile(checkpoint_path, best_path)\n    if epoch % config.save_freq == 0:\n        intermediate_checkpoint = osp.join(config.intermediate_checkpoints_path,\n                                           \'epoch_{}.pth\'.format(epoch))\n        copyfile(checkpoint_path, intermediate_checkpoint)\n\n\n# pylint:disable=no-member\nclass ForkedPdb(pdb.Pdb):\n    """"""A Pdb subclass that may be used\n    from a forked multiprocessing child\n\n    """"""\n\n    def interaction(self, *args, **kwargs):\n        _stdin = sys.stdin\n        try:\n            sys.stdin = open(\'/dev/stdin\')\n            pdb.Pdb.interaction(self, *args, **kwargs)\n        finally:\n            sys.stdin = _stdin\n\n\ndef is_binarization(config):\n    compression_config = config.get(\'compression\', {})\n    if isinstance(compression_config, list):\n        compression_config = compression_config[0]\n    algo_type = compression_config.get(""algorithm"")\n    if algo_type is not None and algo_type == ""binarization"":\n        return True\n    return False\n\n\ndef print_statistics(stats, logger=default_logger):\n    for key, val in stats.items():\n        if isinstance(val, Texttable):\n            logger.info(key)\n            logger.info(val.draw())\n        else:\n            logger.info(""{}: {}"".format(key, val))\n'"
pytorch_toolkit/nncf/examples/object_detection/__init__.py,0,b''
pytorch_toolkit/nncf/examples/object_detection/dataset.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import namedtuple\n\nimport cv2\nimport numpy as np\nimport torch\nfrom examples.object_detection.datasets.coco import COCODataset\nfrom examples.object_detection.datasets.voc0712 import VOCDetection, VOCAnnotationTransform\nfrom examples.object_detection.utils.augmentations import SSDAugmentation\nfrom nncf.dynamic_graph.graph_builder import create_input_infos\n\nVOC_MEAN = (0.406, 0.456, 0.485)\nVOC_STD = (0.255, 0.224, 0.229)\n\nPreprocessing = namedtuple(\'Preprocessing\', (\'mean\', \'std\', \'normalize_coef\', \'rgb\'))\n\n\ndef get_preprocessing(config):\n    if \'preprocessing\' not in config:\n        return Preprocessing(VOC_MEAN, VOC_STD, 255, True)\n    preprocessing_config = config.preprocessing\n    return Preprocessing(\n        preprocessing_config.get(\'mean\', VOC_MEAN),\n        preprocessing_config.get(\'std\', VOC_STD),\n        preprocessing_config.get(\'normalize_coef\', 255),\n        preprocessing_config.get(\'rgb\', True)\n    )\n\n\ndef get_training_dataset(dataset_name, path_to_annotations, path_to_imgs, config):\n    # for VOC path_to_imgs = path_to_annotations = voc_root\n    assert dataset_name in [\'voc\', \'coco\']\n    preprocessing = get_preprocessing(config)\n    input_info_list = create_input_infos(config)\n    image_size = input_info_list[0].shape[-1]\n    ssd_transform = SSDAugmentation(\n        image_size,\n        preprocessing.mean,\n        preprocessing.std,\n        preprocessing.normalize_coef\n    )\n    if dataset_name == \'voc\':\n        training_dataset = VOCDetection(\n            path_to_imgs,\n            transform=ssd_transform,\n            target_transform=VOCAnnotationTransform(keep_difficult=False),\n            return_image_info=False,\n            rgb=preprocessing.rgb\n        )\n    if dataset_name == \'coco\':\n        training_dataset = COCODataset(\n            path_to_annotations, path_to_imgs,\n            transform=ssd_transform,\n            scale_bboxes=True,\n            return_image_info=False,\n            rgb=preprocessing.rgb\n        )\n\n    return training_dataset\n\n\ndef get_testing_dataset(dataset_name, path_to_annotations, path_to_imgs, config):\n    # for VOC path_to_imgs = path_to_annotations = voc_root\n    assert dataset_name in [\'voc\', \'coco\']\n    preprocessing = get_preprocessing(config)\n    input_info_list = create_input_infos(config)\n    image_size = input_info_list[0].shape[-1]\n    transform = BaseTransform(\n        image_size,\n        preprocessing.mean,\n        preprocessing.std,\n        preprocessing.normalize_coef\n    )\n    if dataset_name == \'voc\':\n        testing_dataset = VOCDetection(\n            path_to_imgs, [(\'2007\', \'test\')],\n            transform=transform,\n            target_transform=VOCAnnotationTransform(keep_difficult=True),\n            return_image_info=True,\n            rgb=preprocessing.rgb\n        )\n    if dataset_name == \'coco\':\n        testing_dataset = COCODataset(\n            path_to_annotations, path_to_imgs,\n            transform=transform,\n            scale_bboxes=False,\n            return_image_info=True,\n            rgb=preprocessing.rgb\n        )\n\n    return testing_dataset\n\n\ndef detection_collate(batch):\n    """"""Custom collate fn for dealing with batches of images that have a different\n    number of associated object annotations (bounding boxes).\n\n    Arguments:\n        batch: (tuple) A tuple of tensor images and lists of annotations\n\n    Return:\n        A tuple containing:\n            1) (tensor) batch of images stacked on their 0 dim\n            2) (list of tensors) annotations for a given image are stacked on 0 dim\n    """"""\n    targets = []\n    imgs = []\n    others = tuple([] for _ in batch[0][2:])\n    for sample in batch:\n        imgs.append(sample[0])\n        targets.append(torch.FloatTensor(sample[1]))\n\n        for o, b in zip(others, sample[2:]):\n            o.append(b)\n\n    return (torch.stack(imgs, 0), targets) + others\n\n\ndef base_transform(image, size, mean, std, normalize_coef):\n    x = cv2.resize(image, (size, size)).astype(np.float32)\n    x /= normalize_coef\n    x -= mean\n    x /= std\n    x = x.astype(np.float32)\n    return x\n\n\nclass BaseTransform:\n    def __init__(self, size, mean, std, normalize_coef):\n        self.size = size\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.normalize_coef = normalize_coef\n\n    def __call__(self, image, boxes=None, labels=None):\n        return base_transform(image, self.size, self.mean, self.std, self.normalize_coef), boxes, labels\n'"
pytorch_toolkit/nncf/examples/object_detection/eval.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import print_function\n\nimport os\nimport time\nimport pickle\nimport pathlib\n\nimport numpy as np\nimport torch\nfrom torch import distributed as dist\nfrom torch.nn import functional as F\n\nfrom nncf.utils import is_main_process, is_dist_avail_and_initialized, get_world_size\n\nfrom examples.common.example_logger import logger\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\n\nclass Timer:\n    """"""A simple timer.""""""\n\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        return self.diff\n\n\ndef evaluate_detections(box_list, dataset, use_07=True):\n    cachedir = os.path.join(\'cache\', \'annotations_cache\')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = use_07\n    logger.info(\'VOC07 metric? {}\'.format(\'Yes\' if use_07_metric else \'No\'))\n    for cls_ind, cls in enumerate(dataset.classes):  # for each class\n        class_boxes = box_list[box_list[:, 1] == cls_ind + 1]\n        ap, _, _ = voc_eval(  # calculate rec, prec, ap\n            class_boxes, dataset, cls, cachedir,\n            ovthresh=0.5, use_07_metric=use_07_metric)\n        aps += [ap]\n        logger.info(\'AP for {} = {:.4f}\'.format(cls, ap))\n    mAp = np.mean(aps)\n    logger.info(\'Mean AP = {:.4f}\'.format(mAp))\n    return mAp\n\n\ndef voc_ap(rec, prec, use_07_metric=True):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef voc_eval(class_detections,\n             dataset,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=True):\n    # cachedir caches the annotations in a pickle file\n    # first load gt\n    gt, imagenames = load_detection_annotations(cachedir, dataset)\n    image_bboxes, npos = extract_gt_bboxes(classname, dataset, gt, imagenames)\n\n    image_names = dataset.get_img_names()\n    image_ids = [image_names[int(i)] for i in class_detections[:, 0]]\n    BB = class_detections[:, 3:]\n    confidence = class_detections[:, 2]\n\n    # sort by confidence\n    sorted_ind = np.argsort(confidence)[::-1]\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        R = image_bboxes[image_ids[d]]\n        bb = BB[d, :].astype(float)\n        matched_iou = -np.inf\n        BBGT = R[\'bbox\'].astype(float)\n        if BBGT.size > 0:\n            # compute overlaps\n            # intersection\n            matched_ind, matched_iou = match_bbox(BBGT, bb)\n\n        if matched_iou > ovthresh:\n            if not R[\'difficult\'][matched_ind]:\n                if not R[\'det\'][matched_ind]:\n                    tp[d] = 1.\n                    R[\'det\'][matched_ind] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n    return compute_detection_metrics(fp, tp, npos, use_07_metric)\n\n\ndef extract_gt_bboxes(classname, dataset, gt, imagenames):\n    # extract gt objects for this class\n    class_gt = {}\n    npos = 0\n    for imagename in imagenames:\n        img_gt_objects_for_class = [rec for rec in gt[imagename] if dataset.classes[rec[\'label_idx\']] == classname]\n        bbox = np.asarray([x[\'bbox\'] for x in img_gt_objects_for_class])\n        difficult = []\n        for x in img_gt_objects_for_class:\n            if \'difficult\' in x:\n                difficult.append(x[\'difficult\'])\n            else:\n                difficult.append(False)\n        difficult = np.array(difficult).astype(np.bool)\n        det = [False] * len(img_gt_objects_for_class)\n        npos = npos + sum(~difficult)\n        class_gt[imagename] = {\n            \'bbox\': bbox,\n            \'difficult\': difficult,\n            \'det\': det\n        }\n    return class_gt, npos\n\n\ndef load_detection_annotations(cachedir, dataset):\n    cachefile = os.path.join(cachedir, \'annots_{}.pkl\'.format(dataset.name))\n    imagenames = dataset.get_img_names()\n    if is_main_process():\n        if not os.path.isfile(cachefile):\n            # load annots\n            gt = {}\n            for i, imagename in enumerate(imagenames):\n                _, gt[imagename] = dataset.pull_anno(i)\n\n                if i % 100 == 0:\n                    logger.info(\'Reading annotation for {:d}/{:d}\'.format(\n                        i + 1, len(imagenames)))\n            # save\n            logger.info(\'Saving cached annotations to {:s}\'.format(cachefile))\n            pathlib.Path(cachedir).mkdir(parents=True, exist_ok=True)\n            with open(cachefile, \'wb\') as f:\n                pickle.dump(gt, f)\n    if is_dist_avail_and_initialized():\n        dist.barrier()\n    with open(cachefile, \'rb\') as f:\n        gt = pickle.load(f)\n    return gt, imagenames\n\n\ndef compute_detection_metrics(fp, tp, n_positives, use_07_metric=True):\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(n_positives)\n    # avoid divide by zero in case the first detection matches a difficult\n    # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = voc_ap(rec, prec, use_07_metric)\n    return ap, prec, rec\n\n\ndef match_bbox(gt_boxes, bbox):\n    ixmin = np.maximum(gt_boxes[:, 0], bbox[0])\n    iymin = np.maximum(gt_boxes[:, 1], bbox[1])\n    ixmax = np.minimum(gt_boxes[:, 2], bbox[2])\n    iymax = np.minimum(gt_boxes[:, 3], bbox[3])\n    iw = np.maximum(ixmax - ixmin, 0.)\n    ih = np.maximum(iymax - iymin, 0.)\n    inters = iw * ih\n    uni = ((bbox[2] - bbox[0]) * (bbox[3] - bbox[1]) +\n           (gt_boxes[:, 2] - gt_boxes[:, 0]) *\n           (gt_boxes[:, 3] - gt_boxes[:, 1]) - inters)\n    overlaps = inters / uni\n    matched_ind = np.argmax(overlaps)\n    matched_iou = np.max(overlaps)\n    return matched_ind, matched_iou\n\n\ndef gather_detections(all_detections, samples_per_rank):\n    world_size = get_world_size()\n    result = [torch.zeros(samples_per_rank, *all_detections.shape[1:]).cuda() for _ in range(world_size)]\n    all_detections = F.pad(all_detections, [0, 0, 0, 0, 0, samples_per_rank - all_detections.size(0)])\n    dist.all_gather(result, all_detections.cuda())\n    return torch.cat(result)\n\n\ndef convert_detections(all_detection):\n    """"""Returns (num_dets x 7) array with detections (img_ing, label, conf, x0, y0, x1, y1)""""""\n    all_boxes = []\n    for img_ind, dets in enumerate(all_detection):\n        # remove predictions with zero confidence\n        mask = dets[:, 2].gt(0.).expand(7, dets.size(0)).t()\n        dets = torch.masked_select(dets, mask).view(-1, 7).cpu()\n\n        if dets.size() == (0,):\n            continue\n\n        boxes = np.c_[np.full(dets.size(0), img_ind), dets[:, 1:].numpy()]\n        all_boxes.append(boxes)\n    return np.vstack(all_boxes)\n\n\ndef predict_detections(data_loader, device, net):\n    num_batches = len(data_loader)\n    all_detections = []\n    timer = Timer()\n    for batch_ind, (ims, _gts, hs, ws) in enumerate(data_loader):\n        x = ims.to(device)\n        hs = x.new_tensor(hs).view(-1, 1)\n        ws = x.new_tensor(ws).view(-1, 1)\n\n        timer.tic()\n        batch_detections = net(x)\n        top_k = batch_detections.size(2)\n        batch_detections = batch_detections.view(-1, top_k, 7)\n        detect_time = timer.toc(average=False)\n\n        batch_detections[..., 3] *= ws\n        batch_detections[..., 5] *= ws\n        batch_detections[..., 4] *= hs\n        batch_detections[..., 6] *= hs\n\n        all_detections.append(batch_detections.cpu())\n        logger.info(\'Detect for batch: {:d}/{:d} {:.3f}s\'.format(batch_ind + 1, num_batches, detect_time))\n    if all_detections:\n        return torch.cat(all_detections)\n    return None  # No predictions\n\n\ndef test_net(net, device, data_loader, distributed=False):\n    """"""Test a Fast R-CNN network on an image database.""""""\n    logger.info(""Testing..."")\n    num_images = len(data_loader.dataset)\n    batch_detections = predict_detections(data_loader, device, net)\n    if distributed:\n        batch_detections = gather_detections(batch_detections, data_loader.sampler.samples_per_rank)\n    batch_detections = batch_detections[:num_images]\n    all_boxes = convert_detections(batch_detections)\n\n    logger.info(\'Evaluating detections\')\n    return evaluate_detections(all_boxes, data_loader.dataset)\n'"
pytorch_toolkit/nncf/examples/object_detection/main.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\nimport os.path as osp\nimport sys\nimport time\nfrom pathlib import Path\n\nimport torch\nimport torch.utils.data as data\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom examples.common.argparser import get_common_argument_parser\nfrom examples.common.distributed import DistributedSampler, configure_distributed\nfrom examples.common.execution import ExecutionMode, get_device, get_execution_mode\nfrom examples.common.execution import prepare_model_for_execution, start_worker\nfrom examples.common.utils import get_name, make_additional_checkpoints, print_statistics\nfrom examples.common.utils import write_metrics\nfrom examples.common.optimizer import get_parameter_groups, make_optimizer\nfrom examples.common.utils import configure_logging, configure_paths, create_code_snapshot, is_on_first_rank, print_args\nfrom examples.object_detection.dataset import detection_collate, get_testing_dataset, get_training_dataset\nfrom examples.object_detection.eval import test_net\nfrom examples.object_detection.layers.modules import MultiBoxLoss\nfrom examples.object_detection.model import build_ssd\nfrom examples.common.example_logger import logger\n\n\nfrom nncf import Config, create_compressed_model, load_state\n\nfrom nncf.dynamic_graph.graph_builder import create_input_infos\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\n\ndef get_option(args, config, key, default=None):\n    """"""Gets key option from args if it is provided, otherwise tries to get it from config""""""\n    if hasattr(args, key) and getattr(args, key) is not None:\n        return getattr(args, key)\n    return config.get(key, default)\n\n\ndef get_argument_parser():\n    parser = get_common_argument_parser()\n\n    parser.add_argument(\'--basenet\', default=\'\', help=\'pretrained base model, should be located in save_folder\')\n    parser.add_argument(\'--test-interval\', default=5000, type=int, help=\'test interval\')\n    parser.add_argument(""--dataset"", help=""Dataset to use."", choices=[""voc"", ""coco""], default=None)\n    parser.add_argument(\'--train_imgs\', help=\'path to training images or VOC root directory\')\n    parser.add_argument(\'--train_anno\', help=\'path to training annotations or VOC root directory\')\n    parser.add_argument(\'--test_imgs\', help=\'path to testing images or VOC root directory\')\n    parser.add_argument(\'--test_anno\', help=\'path to testing annotations or VOC root directory\')\n    return parser\n\n\ndef main(argv):\n    parser = get_argument_parser()\n    args = parser.parse_args(args=argv)\n    config = Config.from_json(args.config)\n    config.update_from_args(args, parser)\n    configure_paths(config)\n    source_root = Path(__file__).absolute().parents[2]  # nncf root\n    create_code_snapshot(source_root, osp.join(config.log_dir, ""snapshot.tar.gz""))\n\n    config.execution_mode = get_execution_mode(config)\n\n    if config.dataset_dir is not None:\n        config.train_imgs = config.train_anno = config.test_imgs = config.test_anno = config.dataset_dir\n    start_worker(main_worker, config)\n\n\ndef main_worker(current_gpu, config):\n    #################################\n    # Setup experiment environment\n    #################################\n    config.current_gpu = current_gpu\n    config.distributed = config.execution_mode in (ExecutionMode.DISTRIBUTED, ExecutionMode.MULTIPROCESSING_DISTRIBUTED)\n    if config.distributed:\n        configure_distributed(config)\n    if is_on_first_rank(config):\n        configure_logging(logger, config)\n        print_args(config)\n\n    config.device = get_device(config)\n    config.start_iter = 0\n\n    ##########################\n    # Prepare metrics log file\n    ##########################\n\n    if config.metrics_dump is not None:\n        write_metrics(0, config.metrics_dump)\n\n    ##################\n    # Prepare model\n    ##################\n\n    compression_ctrl, net = create_model(config)\n    if config.distributed:\n        config.batch_size //= config.ngpus_per_node\n        config.workers //= config.ngpus_per_node\n        compression_ctrl.distributed()\n\n    ###########################\n    # Criterion and optimizer\n    ###########################\n\n    params_to_optimize = get_parameter_groups(net, config)\n    optimizer, lr_scheduler = make_optimizer(params_to_optimize, config)\n\n    criterion = MultiBoxLoss(\n        config,\n        config[\'num_classes\'],\n        overlap_thresh=0.5,\n        prior_for_matching=True,\n        bkg_label=0,\n        neg_mining=True,\n        neg_pos=3,\n        neg_overlap=0.5,\n        encode_target=False,\n        device=config.device\n    )\n\n    ###########################\n    # Load checkpoint\n    ###########################\n\n    resuming_checkpoint = config.resuming_checkpoint\n    if resuming_checkpoint:\n        logger.info(\'Resuming training, loading {}...\'.format(resuming_checkpoint))\n        checkpoint = torch.load(resuming_checkpoint, map_location=\'cpu\')\n        # use checkpoint itself in case of only state dict is saved\n        # i.e. checkpoint is created with `torch.save(module.state_dict())`\n        state_dict = checkpoint.get(\'state_dict\', checkpoint)\n        load_state(net, state_dict, is_resume=True)\n        if config.mode.lower() == \'train\' and config.to_onnx is None:\n            compression_ctrl.scheduler.load_state_dict(checkpoint[\'scheduler\'])\n            optimizer.load_state_dict(checkpoint.get(\'optimizer\', optimizer.state_dict()))\n            config.start_iter = checkpoint.get(\'iter\', 0) + 1\n\n    if config.to_onnx:\n        compression_ctrl.export_model(config.to_onnx)\n        logger.info(""Saved to {}"".format(config.to_onnx))\n        return\n\n    ###########################\n    # Prepare data\n    ###########################\n\n    test_data_loader, train_data_loader = create_dataloaders(config)\n\n    if config.mode.lower() == \'test\':\n        with torch.no_grad():\n            print_statistics(compression_ctrl.statistics())\n            net.eval()\n            mAp = test_net(net, config.device, test_data_loader, distributed=config.distributed)\n            if config.metrics_dump is not None:\n                write_metrics(mAp, config.metrics_dump)\n            return\n\n    if not resuming_checkpoint:\n        compression_ctrl.initialize(train_data_loader)\n\n    train(net, compression_ctrl, train_data_loader, test_data_loader, criterion, optimizer, config, lr_scheduler)\n\n\ndef create_dataloaders(config):\n    logger.info(\'Loading Dataset...\')\n    train_dataset = get_training_dataset(config.dataset, config.train_anno, config.train_imgs, config)\n    logger.info(""Loaded {} training images"".format(len(train_dataset)))\n    if config.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n                                                                        num_replicas=config.ngpus_per_node,\n                                                                        rank=config.rank)\n    else:\n        train_sampler = None\n    train_data_loader = data.DataLoader(\n        train_dataset, config.batch_size,\n        num_workers=config.workers,\n        shuffle=(train_sampler is None),\n        collate_fn=detection_collate,\n        pin_memory=True,\n        sampler=train_sampler\n    )\n    test_dataset = get_testing_dataset(config.dataset, config.test_anno, config.test_imgs, config)\n    logger.info(""Loaded {} testing images"".format(len(test_dataset)))\n    if config.distributed:\n        test_sampler = DistributedSampler(test_dataset, config.rank, config.world_size)\n    else:\n        test_sampler = None\n    test_data_loader = data.DataLoader(\n        test_dataset, config.batch_size,\n        num_workers=config.workers,\n        shuffle=False,\n        collate_fn=detection_collate,\n        pin_memory=True,\n        drop_last=False,\n        sampler=test_sampler\n    )\n    return test_data_loader, train_data_loader\n\n\ndef create_model(config):\n    input_info_list = create_input_infos(config)\n    image_size = input_info_list[0].shape[-1]\n    ssd_net = build_ssd(config.model, config.ssd_params, image_size, config.num_classes, config)\n    compression_ctrl, ssd_net = create_compressed_model(ssd_net, config)\n    weights = config.get(\'weights\')\n    if weights:\n        sd = torch.load(weights, map_location=\'cpu\')\n        load_state(ssd_net, sd)\n    ssd_net.train()\n    model, _ = prepare_model_for_execution(ssd_net, config)\n    return compression_ctrl, model\n\n\ndef train_step(batch_iterator, compression_ctrl, config, criterion, net, train_data_loader):\n    batch_loss_l = torch.tensor(0.).to(config.device)\n    batch_loss_c = torch.tensor(0.).to(config.device)\n    batch_loss = torch.tensor(0.).to(config.device)\n    for _ in range(0, config.iter_size):\n        # load train data\n        try:\n            images, targets = next(batch_iterator)\n        except StopIteration:\n            logger.debug(""StopIteration: can not load batch"")\n            batch_iterator = iter(train_data_loader)\n            break\n\n        images = images.to(config.device)\n        targets = [anno.requires_grad_(False).to(config.device) for anno in targets]\n\n        # forward\n        out = net(images)\n        # backprop\n        loss_l, loss_c = criterion(out, targets)\n        loss_comp = compression_ctrl.loss()\n        loss = loss_l + loss_c + loss_comp\n        batch_loss += loss\n        loss.backward()\n        batch_loss_l += loss_l\n        batch_loss_c += loss_c\n    return batch_iterator, batch_loss, batch_loss_c, batch_loss_l, loss_comp\n\n\ndef train(net, compression_ctrl, train_data_loader, test_data_loader, criterion, optimizer, config, lr_scheduler):\n    net.train()\n    # loss counters\n    loc_loss = 0  # epoch\n    conf_loss = 0\n\n    epoch_size = len(train_data_loader)\n    logger.info(\'Training {} on {} dataset...\'.format(config.model, train_data_loader.dataset.name))\n    batch_iterator = None\n\n    t_start = time.time()\n    print_statistics(compression_ctrl.statistics())\n\n    best_mAp = 0\n    test_freq_in_epochs = max(config.test_interval // epoch_size, 1)\n\n    for iteration in range(config.start_iter, config[\'max_iter\']):\n        if (not batch_iterator) or (iteration % epoch_size == 0):\n            # create batch iterator\n            batch_iterator = iter(train_data_loader)\n\n        epoch = iteration // epoch_size\n\n        if (iteration + 1) % epoch_size == 0:\n            compression_ctrl.scheduler.epoch_step(epoch)\n\n            is_best = False\n\n            if (epoch + 1) % test_freq_in_epochs == 0:\n                if is_on_first_rank(config):\n                    print_statistics(compression_ctrl.statistics())\n                with torch.no_grad():\n                    net.eval()\n                    mAP = test_net(net, config.device, test_data_loader, distributed=config.multiprocessing_distributed)\n                    if mAP > best_mAp:\n                        is_best = True\n                        best_mAp = mAP\n                    net.train()\n\n            # Learning rate scheduling should be applied after optimizer\xe2\x80\x99s update\n            if not isinstance(lr_scheduler, ReduceLROnPlateau):\n                lr_scheduler.step(epoch)\n            else:\n                lr_scheduler.step(mAP)\n\n            if is_on_first_rank(config):\n                logger.info(\'Saving state, iter: {}\'.format(iteration))\n\n                checkpoint_file_path = osp.join(config.checkpoint_save_dir, ""{}_last.pth"".format(get_name(config)))\n                torch.save({\n                    \'state_dict\': net.state_dict(),\n                    \'optimizer\': optimizer.state_dict(),\n                    \'iter\': config[\'max_iter\'],\n                    \'scheduler\': compression_ctrl.scheduler.state_dict()\n                }, str(checkpoint_file_path))\n                make_additional_checkpoints(checkpoint_file_path,\n                                            is_best=is_best,\n                                            epoch=epoch + 1,\n                                            config=config)\n\n        compression_ctrl.scheduler.step(iteration - config.start_iter)\n\n        optimizer.zero_grad()\n        batch_iterator, batch_loss, batch_loss_c, batch_loss_l, loss_comp = train_step(\n            batch_iterator, compression_ctrl, config, criterion, net, train_data_loader\n        )\n        optimizer.step()\n\n\n        batch_loss_l = batch_loss_l / config.iter_size\n        batch_loss_c = batch_loss_c / config.iter_size\n        model_loss = (batch_loss_l + batch_loss_c) / config.iter_size\n        batch_loss = batch_loss / config.iter_size\n\n        loc_loss += batch_loss_l.item()\n        conf_loss += batch_loss_c.item()\n\n        ###########################\n        # Logging\n        ###########################\n\n        if is_on_first_rank(config):\n            config.tb.add_scalar(""train/loss_l"", batch_loss_l.item(), iteration)\n            config.tb.add_scalar(""train/loss_c"", batch_loss_c.item(), iteration)\n            config.tb.add_scalar(""train/loss"", batch_loss.item(), iteration)\n\n        if iteration % config.print_freq == 0:\n            t_finish = time.time()\n            t_elapsed = t_finish - t_start\n            t_start = time.time()\n            logger.info(\'{}: iter {} epoch {} || Loss: {:.4} || Time {:.4}s || lr: {} || CR loss: {}\'.format(\n                config.rank, iteration, epoch, model_loss.item(), t_elapsed, optimizer.param_groups[0][\'lr\'],\n                loss_comp.item() if isinstance(loss_comp, torch.Tensor) else loss_comp\n            ))\n\n    if config.metrics_dump is not None:\n        write_metrics(best_mAp, config.metrics_dump)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n'"
pytorch_toolkit/nncf/examples/object_detection/model.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom examples.object_detection.models.ssd_mobilenet import build_ssd_mobilenet\nfrom examples.object_detection.models.ssd_vgg import build_ssd_vgg\n\n\ndef build_ssd(net_name, cfg, ssd_dim, num_classes, config):\n    assert net_name in [\'ssd_vgg\', \'ssd_mobilenet\']\n    if net_name == \'ssd_vgg\':\n        model = build_ssd_vgg(cfg, ssd_dim, num_classes, config)\n    if net_name == \'ssd_mobilenet\':\n        model = build_ssd_mobilenet(cfg, ssd_dim, num_classes, config)\n\n    return model\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/main.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n# Major parts of this sample reuse code from:\n# https://github.com/davidtvs/PyTorch-ENet\n# https://github.com/pytorch/vision/tree/master/references/segmentation\n\nimport functools\nimport os\nimport sys\nfrom os import path as osp\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom nncf.utils import is_main_process\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport examples.semantic_segmentation.utils.data as data_utils\nimport examples.semantic_segmentation.utils.loss_funcs as loss_funcs\nimport examples.semantic_segmentation.utils.transforms as JT\n\nfrom examples.common.argparser import get_common_argument_parser\nfrom examples.common.distributed import configure_distributed\nfrom examples.common.execution import ExecutionMode, get_device, get_execution_mode, \\\n    prepare_model_for_execution, start_worker\nfrom examples.common.model_loader import load_model\nfrom examples.common.optimizer import make_optimizer\nfrom examples.common.utils import configure_logging, configure_paths, make_additional_checkpoints, print_args, \\\n    write_metrics, print_statistics\nfrom examples.semantic_segmentation.metric import IoU\nfrom examples.semantic_segmentation.test import Test\nfrom examples.semantic_segmentation.train import Train\nfrom examples.semantic_segmentation.utils.checkpoint import load_checkpoint, save_checkpoint\nfrom examples.common.example_logger import logger\nfrom nncf import Config, create_compressed_model, load_state\n\ndef get_arguments(args):\n    parser = get_common_argument_parser()\n    parser.add_argument(\n        ""--dataset"",\n        help=""Dataset to use."",\n        choices=[""camvid"", ""cityscapes"", ""mapillary""],\n        default=None\n    )\n    return parser.parse_args(args=args)\n\n\ndef get_preprocessing_transforms(config):\n    transforms = []\n    for k, v in config.preprocessing.items():\n        if k == ""resize"":\n            transforms.append(JT.Resize((v[""height""], v[""width""])))\n    return transforms\n\n\ndef get_augmentations_transforms(config):\n    transforms = []\n    for k, v in config.augmentations.items():\n        if k == ""random_hflip"":\n            transforms.append(JT.RandomHorizontalFlip(v))\n        elif k == ""random_crop"":\n            transforms.append(JT.RandomCrop(v))\n        elif k == ""random_resize"":\n            transforms.append(JT.RandomResize(v[""min_size""], v[""max_size""]))\n        elif k == ""random_scale_aligned"":\n            transforms.append(JT.RandomScaleAligned(**v))\n        elif k == ""resize"":\n            transforms.append(JT.Resize((v[""height""], v[""width""])))\n        elif k == ""random_sized_crop"":\n            transforms.append(JT.RandomSizedCrop(v))\n    return transforms\n\n\ndef get_joint_transforms(is_train, config):\n    joint_transforms = []\n    if is_train and ""augmentations"" in config:\n        joint_transforms += get_augmentations_transforms(config)\n\n    if ""preprocessing"" in config:\n        joint_transforms += get_preprocessing_transforms(config)\n        joint_transforms.append(JT.ToTensor())\n        if ""normalize"" in config[""preprocessing""]:\n            v = config[""preprocessing""][""normalize""]\n            joint_transforms.append(JT.Normalize(v[""mean""], v[""std""]))\n    else:\n        joint_transforms.append(JT.ToTensor())\n    return JT.Compose(joint_transforms)\n\n\ndef get_class_weights(train_set, num_classes, config):\n    # Get class weights from the selected weighing technique\n    logger.info(""\\nWeighing technique: {}"".format(config.weighing))\n    weighing = config.get(\'weighing\', \'none\')\n    if isinstance(weighing, list):\n        # Class weights were directly specified in config\n        return np.asarray(weighing)\n\n    train_loader_for_weight_count = torch.utils.data.DataLoader(\n        train_set,\n        batch_size=1, collate_fn=data_utils.collate_fn)\n    logger.info(""Computing class weights..."")\n    logger.info(""(this can take a while depending on the dataset size)"")\n    if weighing.lower() == \'enet\':\n        class_weights = data_utils.enet_weighing(train_loader_for_weight_count, num_classes)\n    elif weighing.lower() == \'mfb\':\n        class_weights = data_utils.median_freq_balancing(train_loader_for_weight_count, num_classes)\n    else:\n        class_weights = None\n    return class_weights\n\n\ndef get_dataset(dataset_name: str) -> torch.utils.data.Dataset:\n    # Import the requested dataset\n    if dataset_name.lower() == \'camvid\':\n        from examples.semantic_segmentation.datasets import CamVid as dataset\n        # Remove the road_marking class from the CamVid dataset as it\'s merged\n        # with the road class\n        if \'road_marking\' in dataset.color_encoding:\n            del dataset.color_encoding[\'road_marking\']\n    elif dataset_name.lower() == \'cityscapes\':\n        from examples.semantic_segmentation.datasets import Cityscapes as dataset\n    elif dataset_name.lower() == \'mapillary\':\n        from examples.semantic_segmentation.datasets import Mapillary as dataset\n    else:\n        # Should never happen...but just in case it does\n        raise RuntimeError(""\\""{0}\\"" is not a supported dataset."".format(\n            dataset_name))\n    return dataset\n\n\ndef load_dataset(dataset, config):\n    logger.info(""\\nLoading dataset...\\n"")\n\n    logger.info(""Selected dataset: {}"".format(config.dataset))\n    logger.info(""Dataset directory: {}"".format(config.dataset_dir))\n\n    transforms_train = get_joint_transforms(is_train=True, config=config)\n    transforms_val = get_joint_transforms(is_train=False, config=config)\n\n    # Get selected dataset\n    train_set = dataset(\n        root=config.dataset_dir,\n        image_set=\'train\',\n        transforms=transforms_train)\n\n    val_set = dataset(\n        config.dataset_dir,\n        image_set=\'val\',\n        transforms=transforms_val)\n\n    # Samplers\n    if config.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_set)\n    else:\n        train_sampler = torch.utils.data.RandomSampler(train_set)\n\n    batch_size = config.batch_size\n    num_workers = config.workers\n\n    if config.multiprocessing_distributed:\n        batch_size //= config.ngpus_per_node\n        num_workers //= config.ngpus_per_node\n\n    # Loaders\n    train_loader = torch.utils.data.DataLoader(\n        train_set,\n        batch_size=batch_size,\n        sampler=train_sampler, num_workers=num_workers,\n        collate_fn=data_utils.collate_fn, drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        val_set,\n        batch_size=1, num_workers=num_workers,\n        collate_fn=data_utils.collate_fn)\n\n    # Get encoding between pixel values in label images and RGB colors\n    class_encoding = train_set.color_encoding\n\n    # Get number of classes to predict\n    num_classes = len(class_encoding)\n\n    # Print information for debugging\n    logger.info(""Number of classes to predict: {}"".format(num_classes))\n    logger.info(""Train dataset size: {}"".format(len(train_set)))\n    logger.info(""Validation dataset size: {}"".format(len(val_set)))\n\n    # Get a batch of samples to display\n    if config.mode.lower() == \'test\':\n        images, labels = iter(val_loader).next()\n    else:\n        images, labels = iter(train_loader).next()\n    logger.info(""Image size: {}"".format(images.size()))\n    logger.info(""Label size: {}"".format(labels.size()))\n    logger.info(""Class-color encoding: {}"".format(class_encoding))\n\n    # Show a batch of samples and labels\n    if config.imshow_batch and config.mode.lower() != \'test\':\n        logger.info(""Close the figure window to continue..."")\n        label_to_rgb = T.Compose([\n            data_utils.LongTensorToRGBPIL(class_encoding),\n            T.ToTensor()\n        ])\n        color_labels = data_utils.batch_transform(labels, label_to_rgb)\n        data_utils.imshow_batch(images, color_labels)\n\n    class_weights = get_class_weights(train_set, num_classes, config)\n\n    if class_weights is not None:\n        class_weights = torch.from_numpy(class_weights).float().to(config.device)\n        # Set the weight of the unlabeled class to 0\n        ignore_unlabeled = config.get(""ignore_unlabeled"", True)\n        if ignore_unlabeled and (\'unlabeled\' in class_encoding):\n            ignore_index = list(class_encoding).index(\'unlabeled\')\n            class_weights[ignore_index] = 0\n\n    logger.info(""Class weights: {}"".format(class_weights))\n\n    return (train_loader, val_loader), class_weights\n\n\ndef get_aux_loss_dependent_params(model_without_dp, class_weights, aux_lr, config):\n    if config.model == ""icnet"":\n        params_to_optimize = model_without_dp.parameters()\n        criterion = functools.partial(loss_funcs.cross_entropy_icnet, weight=class_weights)\n        return params_to_optimize, criterion\n\n    model_params_config = config.get(\'model_params\', {})\n    is_aux_loss = model_params_config.get(\'aux_loss\', False)\n    if is_aux_loss:\n        params_to_optimize = [\n            {""params"": [p for p in model_without_dp.backbone.parameters() if p.requires_grad]},\n            {""params"": [p for p in model_without_dp.classifier.parameters() if p.requires_grad]},\n        ]\n        params = [p for p in model_without_dp.aux_classifier.parameters() if p.requires_grad]\n        params_to_optimize.append({""params"": params, ""lr"": aux_lr})\n        criterion = functools.partial(loss_funcs.cross_entropy_aux, weight=class_weights)\n    else:\n        params_to_optimize = model_without_dp.parameters()\n        criterion = functools.partial(loss_funcs.cross_entropy, weight=class_weights)\n    return params_to_optimize, criterion\n\n\n# pylint: disable=too-many-branches\n# pylint: disable=too-many-statements\ndef train(model, model_without_dp, compression_ctrl, train_loader, val_loader, class_weights, class_encoding, config):\n    logger.info(""\\nTraining...\\n"")\n\n    # Check if the network architecture is correct\n    logger.info(model)\n\n    optim_config = config.get(\'optimizer\', {})\n    optim_params = optim_config.get(\'optimizer_params\', {})\n    lr = optim_params.get(""lr"", 1e-4)\n\n    params_to_optimize, criterion = get_aux_loss_dependent_params(model_without_dp,\n                                                                  class_weights,\n                                                                  lr * 10,\n                                                                  config)\n\n    optimizer, lr_scheduler = make_optimizer(params_to_optimize, config)\n\n    # Evaluation metric\n\n    ignore_index = None\n    ignore_unlabeled = config.get(""ignore_unlabeled"", True)\n    if ignore_unlabeled and (\'unlabeled\' in class_encoding):\n        ignore_index = list(class_encoding).index(\'unlabeled\')\n\n    metric = IoU(len(class_encoding), ignore_index=ignore_index)\n\n    best_miou = -1\n    resuming_checkpoint = config.resuming_checkpoint\n    # Optionally resume from a checkpoint\n    if resuming_checkpoint is not None:\n        model, optimizer, start_epoch, best_miou, _ = \\\n            load_checkpoint(\n                model, resuming_checkpoint, config.device,\n                optimizer, compression_ctrl.scheduler)\n        logger.info(""Resuming from model: Start epoch = {0} ""\n                    ""| Best mean IoU = {1:.4f}"".format(start_epoch, best_miou))\n        config.start_epoch = start_epoch\n\n    # Start Training\n    train_obj = Train(model, train_loader, optimizer, criterion, compression_ctrl, metric, config.device,\n                      config.model)\n    val_obj = Test(model, val_loader, criterion, metric, config.device,\n                   config.model)\n\n    for epoch in range(config.start_epoch, config.epochs):\n        logger.info("">>>> [Epoch: {0:d}] Training"".format(epoch))\n\n        if config.distributed:\n            train_loader.sampler.set_epoch(epoch)\n\n        epoch_loss, (iou, miou) = train_obj.run_epoch(config.print_step)\n        if not isinstance(lr_scheduler, ReduceLROnPlateau):\n            # Learning rate scheduling should be applied after optimizer\xe2\x80\x99s update\n            lr_scheduler.step(epoch)\n        compression_ctrl.scheduler.epoch_step()\n\n        logger.info("">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}"".\n                    format(epoch, epoch_loss, miou))\n\n        if is_main_process():\n            config.tb.add_scalar(""train/loss"", epoch_loss, epoch)\n            config.tb.add_scalar(""train/mIoU"", miou, epoch)\n            config.tb.add_scalar(""train/learning_rate"", optimizer.param_groups[0][\'lr\'], epoch)\n            config.tb.add_scalar(""train/compression_loss"", compression_ctrl.loss(), epoch)\n\n            for key, value in compression_ctrl.statistics().items():\n                if isinstance(value, (int, float)):\n                    config.tb.add_scalar(""compression/statistics/{0}"".format(key), value, epoch)\n\n        if (epoch + 1) % config.save_freq == 0 or epoch + 1 == config.epochs:\n            logger.info("">>>> [Epoch: {0:d}] Validation"".format(epoch))\n\n            loss, (iou, miou) = val_obj.run_epoch(config.print_step)\n\n            logger.info("">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}"".\n                        format(epoch, loss, miou))\n\n            if is_main_process():\n                config.tb.add_scalar(""val/mIoU"", miou, epoch)\n                config.tb.add_scalar(""val/loss"", loss, epoch)\n                for i, (key, class_iou) in enumerate(zip(class_encoding.keys(), iou)):\n                    config.tb.add_scalar(""{}/mIoU_Cls{}_{}"".format(config.dataset, i, key), class_iou, epoch)\n\n            is_best = miou > best_miou\n            best_miou = max(miou, best_miou)\n\n            if config.metrics_dump is not None:\n                write_metrics(best_miou, config.metrics_dump)\n\n            if isinstance(lr_scheduler, ReduceLROnPlateau):\n                # Learning rate scheduling should be applied after optimizer\xe2\x80\x99s update\n                lr_scheduler.step(best_miou)\n\n            # Print per class IoU on last epoch or if best iou\n            if epoch + 1 == config.epochs or is_best:\n                for key, class_iou in zip(class_encoding.keys(), iou):\n                    logger.info(""{0}: {1:.4f}"".format(key, class_iou))\n\n            # Save the model if it\'s the best thus far\n            if is_main_process():\n                checkpoint_path = save_checkpoint(model,\n                                                  optimizer, epoch + 1, best_miou,\n                                                  compression_ctrl.scheduler, config)\n\n                make_additional_checkpoints(checkpoint_path, is_best, epoch + 1, config)\n                print_statistics(compression_ctrl.statistics())\n\n    return model\n\n\ndef test(model, test_loader, class_weights, class_encoding, config):\n    logger.info(""\\nTesting...\\n"")\n\n    _, criterion = get_aux_loss_dependent_params(model,\n                                                 class_weights,\n                                                 0,\n                                                 config)\n\n    # Evaluation metric\n\n    ignore_index = None\n\n    ignore_unlabeled = config.get(""ignore_unlabeled"", True)\n    if ignore_unlabeled and (\'unlabeled\' in class_encoding):\n        ignore_index = list(class_encoding).index(\'unlabeled\')\n\n    metric = IoU(len(class_encoding), ignore_index=ignore_index)\n\n    # Test the trained model on the test set\n    test_obj = Test(model, test_loader, criterion, metric, config.device, config.model)\n\n    logger.info("">>>> Running test dataset"")\n\n    loss, (iou, miou) = test_obj.run_epoch(config.print_step)\n    class_iou = dict(zip(class_encoding.keys(), iou))\n\n    logger.info("">>>> Avg. loss: {0:.4f} | Mean IoU: {1:.4f}"".format(loss, miou))\n    if config.metrics_dump is not None:\n        write_metrics(miou, config.metrics_dump)\n\n    # Print per class IoU\n    for key, class_iou in zip(class_encoding.keys(), iou):\n        logger.info(""{0}: {1:.4f}"".format(key, class_iou))\n\n    # Show a batch of samples and labels\n    if config.imshow_batch:\n        logger.info(""A batch of predictions from the test set..."")\n        images, gt_labels = iter(test_loader).next()\n        color_predictions = predict(model, images, class_encoding, config)\n\n        from examples.common.models.segmentation.unet import UNet, center_crop\n        if isinstance(model, UNet):\n            # UNet predicts center image crops\n            outputs_size_hw = (color_predictions.size()[2], color_predictions.size()[3])\n            gt_labels = center_crop(gt_labels, outputs_size_hw).contiguous()\n        data_utils.show_ground_truth_vs_prediction(images, gt_labels, color_predictions, class_encoding)\n\n\ndef predict(model, images, class_encoding, config):\n    images = images.to(config.device)\n\n    model.eval()\n    with torch.no_grad():\n        predictions = model(images)\n\n    if isinstance(predictions, dict):\n        predictions = predictions[\'out\']\n\n    # Predictions is one-hot encoded with ""num_classes"" channels.\n    # Convert it to a single int using the indices where the maximum (1) occurs\n    _, predictions = torch.max(predictions.data, 1)\n\n    color_predictions = data_utils.label_to_color(predictions, class_encoding)\n    return color_predictions\n\n\ndef main_worker(current_gpu, config):\n    config.current_gpu = current_gpu\n    config.distributed = config.execution_mode in (ExecutionMode.DISTRIBUTED, ExecutionMode.MULTIPROCESSING_DISTRIBUTED)\n    if config.distributed:\n        configure_distributed(config)\n\n    if is_main_process():\n        configure_logging(logger, config)\n        print_args(config)\n\n    logger.info(config)\n\n    config.device = get_device(config)\n    dataset = get_dataset(config.dataset)\n    color_encoding = dataset.color_encoding\n    num_classes = len(color_encoding)\n\n    if config.metrics_dump is not None:\n        write_metrics(0, config.metrics_dump)\n\n    weights = config.get(\'weights\')\n    model = load_model(config.model,\n                       pretrained=config.get(\'pretrained\', True) if weights is None else False,\n                       num_classes=num_classes,\n                       model_params=config.get(\'model_params\', {}))\n    compression_ctrl, model = create_compressed_model(model, config)\n    if weights:\n        sd = torch.load(weights, map_location=\'cpu\')\n        load_state(model, sd)\n\n    model, model_without_dp = prepare_model_for_execution(model, config)\n\n    if config.distributed:\n        compression_ctrl.distributed()\n\n    resuming_checkpoint = config.resuming_checkpoint\n\n    if resuming_checkpoint is not None:\n        if not config.pretrained:\n            # Load the previously saved model state\n            model, _, _, _, _ = \\\n                load_checkpoint(model, resuming_checkpoint, config.device,\n                                compression_scheduler=compression_ctrl.scheduler)\n\n    if config.to_onnx is not None:\n        compression_ctrl.export_model(config.to_onnx)\n        logger.info(""Saved to {}"".format(config.to_onnx))\n        return\n\n    if config.mode.lower() == \'test\':\n        logger.info(model)\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n        params = sum([np.prod(p.size()) for p in model_parameters])\n        logger.info(""Trainable argument count:{params}"".format(params=params))\n\n        model = model.to(config.device)\n        loaders, w_class = load_dataset(dataset, config)\n        _, val_loader = loaders\n        test(model, val_loader, w_class, color_encoding, config)\n        print_statistics(compression_ctrl.statistics())\n    elif config.mode.lower() == \'train\':\n        loaders, w_class = load_dataset(dataset, config)\n        train_loader, val_loader = loaders\n        if not resuming_checkpoint:\n            compression_ctrl.initialize(train_loader)\n        train(model, model_without_dp, compression_ctrl, train_loader, val_loader, w_class, color_encoding, config)\n    else:\n        # Should never happen...but just in case it does\n        raise RuntimeError(\n            ""\\""{0}\\"" is not a valid choice for execution mode."".format(\n                config.mode))\n\n\ndef main(argv):\n    parser = get_common_argument_parser()\n    arguments = parser.parse_args(args=argv)\n    config = Config.from_json(arguments.config)\n    config.update_from_args(arguments, parser)\n    if config.dist_url == ""env://"":\n        config.update_from_env()\n\n    if config.mode.lower() != \'test\':\n        if not osp.exists(config.log_dir):\n            os.makedirs(config.log_dir)\n\n        config.log_dir = str(config.log_dir)\n        configure_paths(config)\n        logger.info(""Save directory: {}"".format(config.log_dir))\n    else:\n        config.log_dir = ""/tmp/""\n\n    config.execution_mode = get_execution_mode(config)\n    start_worker(main_worker, config)\n\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/test.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom tqdm import tqdm\nimport torch\n\nfrom examples.common.example_logger import logger\nfrom examples.semantic_segmentation.utils.loss_funcs import do_model_specific_postprocessing\n\n\nclass Test:\n    """"""Tests the ``model`` on the specified test dataset using the\n    data loader, and loss criterion.\n\n    Keyword arguments:\n    - model (``nn.Module``): the model instance to test.\n    - data_loader (``Dataloader``): Provides single or multi-process\n    iterators over the dataset.\n    - criterion (``Optimizer``): The loss criterion.\n    - metric (```Metric``): An instance specifying the metric to return.\n    - device (``torch.device``): An object representing the device on which\n    tensors are allocated.\n    - model_name: Name of the model to be trained - determines model-specific processing\n    of the results (i.e. whether center crop should be applied, what outputs should be counted in metrics, etc.)\n\n    """"""\n\n    def __init__(self, model, data_loader, criterion, metric, device, model_name):\n        self.model = model\n        self.data_loader = data_loader\n        self.criterion = criterion\n        self.metric = metric\n        self.device = device\n        self.model_name = model_name\n\n    def run_epoch(self, iteration_loss=False):\n        """"""Runs an epoch of validation.\n\n        Keyword arguments:\n        - iteration_loss (``bool``, optional): Prints loss at every step.\n\n        Returns:\n        - The epoch loss (float), and the values of the specified metrics\n\n        """"""\n        self.model.eval()\n        epoch_loss = 0.0\n        self.metric.reset()\n        for step, batch_data in tqdm(enumerate(self.data_loader), total=len(self.data_loader)):\n            # Get the inputs and labels\n            inputs = batch_data[0].to(self.device)\n            labels = batch_data[1].to(self.device)\n\n            with torch.no_grad():\n                # Forward propagation\n                outputs = self.model(inputs)\n\n                labels, loss_outputs, metric_outputs = do_model_specific_postprocessing(self.model_name,\n                                                                                        labels,\n                                                                                        outputs)\n\n                # Loss computation\n                loss = self.criterion(loss_outputs, labels)\n\n            # Keep track of loss for current epoch\n            epoch_loss += loss.item()\n\n            self.metric.add(metric_outputs.detach(), labels.detach())\n\n            if iteration_loss:\n                logger.info(""[Step: {}] Iteration loss: {:.4f}"".format(step, loss.item()))\n\n        return epoch_loss / len(self.data_loader), self.metric.value()\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/train.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom examples.semantic_segmentation.utils.loss_funcs import do_model_specific_postprocessing\nfrom examples.common.example_logger import logger\n\nclass Train:\n    """"""Performs the training of ``model`` given a training dataset data\n    loader, the optimizer, and the loss criterion.\n\n    Keyword arguments:\n    - model (``nn.Module``): the model instance to train.\n    - data_loader (``Dataloader``): Provides single or multi-process\n    iterators over the dataset.\n    - optim (``Optimizer``): The optimization algorithm.\n    - criterion (``Optimizer``): The loss criterion.\n    - metric (```Metric``): An instance specifying the metric to return.\n    - device (``torch.device``): An object representing the device on which\n    tensors are allocated.\n    - model_name: Name of the model to be trained - determines model-specific processing\n    of the results (i.e. whether center crop should be applied, what outputs should be counted in metrics, etc.)\n    """"""\n\n    def __init__(self, model, data_loader, optim, criterion, compression_ctrl, metric, device, model_name):\n        self.model = model\n        self.data_loader = data_loader\n        self.optim = optim\n        self.criterion = criterion\n        self.compression_ctrl = compression_ctrl\n        self.metric = metric\n        self.device = device\n        self.model_name = model_name\n\n    def run_epoch(self, iteration_loss=False):\n        """"""Runs an epoch of training.\n\n        Keyword arguments:\n        - iteration_loss (``bool``, optional): Prints loss at every step.\n\n        Returns:\n        - The epoch loss (float).\n\n        """"""\n        compression_scheduler = self.compression_ctrl.scheduler\n\n        self.model.train()\n        epoch_loss = 0.0\n        self.metric.reset()\n        for step, batch_data in enumerate(self.data_loader):\n            # Get the inputs and labels\n            inputs = batch_data[0].to(self.device)\n            labels = batch_data[1].to(self.device)\n\n            # Forward propagation\n            outputs = self.model(inputs)\n\n            labels, loss_outputs, metric_outputs = do_model_specific_postprocessing(self.model_name, labels, outputs)\n\n            # Loss computation\n            loss = self.criterion(loss_outputs, labels)\n\n            compression_loss = self.compression_ctrl.loss()\n            loss += compression_loss\n\n            # Backpropagation\n            self.optim.zero_grad()\n            loss.backward()\n            self.optim.step()\n\n            compression_scheduler.step()\n\n            # Keep track of loss for current epoch\n            epoch_loss += loss.item()\n\n            # Keep track of the evaluation metric\n            self.metric.add(metric_outputs.detach(), labels.detach())\n\n            if iteration_loss:\n                logger.info(""[Step: {}] Iteration loss: {:.4f}"".format(step, loss.item()))\n\n        return epoch_loss / len(self.data_loader), self.metric.value()\n'"
pytorch_toolkit/nncf/nncf/binarization/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/binarization/algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom collections import OrderedDict\nfrom typing import List\n\nimport torch\nfrom texttable import Texttable\nfrom torch import nn\n\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.config import Config\nfrom nncf.dynamic_graph.graph import InputAgnosticOperationExecutionContext\nfrom nncf.layers import NNCFConv2d\nfrom nncf.nncf_network import InsertionCommand, InsertionPoint, InsertionType, OperationPriority\nfrom nncf.module_operations import UpdateWeight, UpdateInputs\nfrom nncf.binarization.layers import BINARIZATION_MODULES, BinarizationMode, WeightBinarizer, ActivationBinarizer, \\\n    ActivationBinarizationScaleThreshold\nfrom nncf.binarization.schedulers import BINARIZATION_SCHEDULERS\nfrom nncf.algo_selector import COMPRESSION_ALGORITHMS\nfrom nncf.compression_method_api import CompressionAlgorithmBuilder, CompressionAlgorithmController\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\n\n@COMPRESSION_ALGORITHMS.register(\'binarization\')\nclass BinarizationBuilder(CompressionAlgorithmBuilder):\n    def __init__(self, config):\n        super().__init__(config)\n        self.mode = self.config.get(\'mode\', BinarizationMode.XNOR)\n\n    def apply_to(self, target_model: NNCFNetwork) -> NNCFNetwork:\n        insertion_commands = self._binarize_weights_and_module_inputs(target_model)\n        for command in insertion_commands:\n            target_model.register_insertion_command(command)\n\n        target_model.register_algorithm(self)\n        return target_model\n\n    def __create_binarize_module(self):\n        return BINARIZATION_MODULES.get(self.mode)()\n\n    def _binarize_weights_and_module_inputs(self, target_model: NNCFNetwork) -> List[InsertionCommand]:\n        device = next(target_model.parameters()).device\n        modules = target_model.get_nncf_modules()\n\n        insertion_commands = []\n        for scope, module in modules.items():\n            scope_str = str(scope)\n\n            if not self._should_consider_scope(scope_str):\n                nncf_logger.info(""Ignored adding binarizers in scope: {}"".format(scope_str))\n                continue\n\n            if isinstance(module, torch.nn.modules.Conv2d):\n                nncf_logger.info(""Adding Weight binarizer in scope: {}"".format(scope_str))\n                op_weights = UpdateWeight(\n                    self.__create_binarize_module()\n                ).to(device)\n\n                nncf_logger.info(""Adding Activation binarizer in scope: {}"".format(scope_str))\n                op_inputs = UpdateInputs(ActivationBinarizationScaleThreshold(module.weight.shape)).to(device)\n\n                insertion_commands.append(InsertionCommand(\n                    InsertionPoint(\n                        InputAgnosticOperationExecutionContext("""", scope, 0),\n                        InsertionType.NNCF_MODULE_PRE_OP), op_weights, OperationPriority.QUANTIZATION_PRIORITY))\n\n                insertion_commands.append(InsertionCommand(\n                    InsertionPoint(\n                        InputAgnosticOperationExecutionContext("""", scope, 0),\n                        InsertionType.NNCF_MODULE_PRE_OP), op_inputs, OperationPriority.QUANTIZATION_PRIORITY))\n        return insertion_commands\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n        return BinarizationController(target_model, self.config)\n\n\nclass BinarizationController(CompressionAlgorithmController):\n    def __init__(self, target_model: NNCFNetwork, params: Config):\n        super().__init__(target_model)\n\n        self.is_distributed = False\n        scheduler_cls = BINARIZATION_SCHEDULERS.get(""staged"")\n        self._scheduler = scheduler_cls(self, params)\n        self._compute_and_display_flops_binarization_rate()\n\n    def distributed(self):\n        self.is_distributed = True\n\n    def enable_activation_binarization(self):\n        if self._model is not None:\n            for _, m in self._model.named_modules():\n                if isinstance(m, ActivationBinarizer):\n                    m.enable()\n\n    def enable_weight_binarization(self):\n        if self._model is not None:\n            for _, m in self._model.named_modules():\n                if isinstance(m, WeightBinarizer):\n                    m.enable()\n\n    def _compute_and_display_flops_binarization_rate(self):\n        net = self._model\n        weight_list = {}\n        state_dict = net.state_dict()\n        for n, v in state_dict.items():\n            weight_list[n] = v.clone()\n\n        ops_dict = OrderedDict()\n\n        def get_hook(name):\n            def compute_flops_hook(self, input_, output):\n                name_type = str(type(self).__name__)\n                if isinstance(self, (nn.Conv2d, nn.ConvTranspose2d)):\n                    ks = self.weight.data.shape\n                    ops_count = ks[0] * ks[1] * ks[2] * ks[3] * output.shape[3] * output.shape[2]\n                elif isinstance(self, nn.Linear):\n                    ops_count = input_[0].shape[1] * output.shape[1]\n                else:\n                    return\n                ops_dict[name] = (name_type, ops_count, isinstance(self, NNCFConv2d))\n\n            return compute_flops_hook\n\n        hook_list = [m.register_forward_hook(get_hook(n)) for n, m in net.named_modules()]\n\n        net.do_dummy_forward(force_eval=True)\n\n        for h in hook_list:\n            h.remove()\n\n        # restore all parameters that can be corrupted due forward pass\n        for n, v in state_dict.items():\n            state_dict[n].data.copy_(weight_list[n].data)\n\n        ops_bin = 0\n        ops_total = 0\n\n        for layer_name, (layer_type, ops, is_binarized) in ops_dict.items():\n            ops_total += ops\n            if is_binarized:\n                ops_bin += ops\n\n        table = Texttable()\n        header = [""Layer name"", ""Layer type"", ""Binarized"", ""MAC count"", ""MAC share""]\n        table_data = [header]\n\n        for layer_name, (layer_type, ops, is_binarized) in ops_dict.items():\n            drow = {h: 0 for h in header}\n            drow[""Layer name""] = layer_name\n            drow[""Layer type""] = layer_type\n            drow[""Binarized""] = \'Y\' if is_binarized else \'N\'\n            drow[""MAC count""] = ""{:.3f}G"".format(ops*1e-9)\n            drow[""MAC share""] = ""{:2.1f}%"".format(ops / ops_total * 100)\n            row = [drow[h] for h in header]\n            table_data.append(row)\n\n        table.add_rows(table_data)\n        nncf_logger.info(table.draw())\n        nncf_logger.info(""Total binarized MAC share: {:.1f}%"".format(ops_bin / ops_total * 100))\n'"
pytorch_toolkit/nncf/nncf/binarization/binarize_functions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport warnings\n\nimport torch\nfrom nncf.definitions import get_install_type\nif get_install_type() == \'GPU\':\n    from .extensions import BinarizedFunctionsCUDA\n\n\nclass XNORBinarizeFn(torch.autograd.Function):\n    """""" Binarizes x into `scale` * { +1; -1}, where +1 or -1 are chosen based\n        on whether the x element value is >0 or <0. `scale` is determined as mean of absolute\n        values, per input channel (0-th dimension of x). """"""\n    @staticmethod\n    def symbolic(g, x):\n        zero = g.constant(0, [1], \'float\')\n        zero = g.op(""Unsqueeze"", zero, axes_i=[1, 2, 3])\n        scale = g.op(""Abs"", x)\n        scale = g.op(""ReduceMean"", scale, axes_i=[1, 2, 3])\n        scale_neg = g.op(""Neg"", scale)\n        return g.op(""FakeQuantize"", x, zero, zero, scale_neg, scale, levels_i=2)\n\n    @staticmethod\n    def forward(ctx, x):\n        if x.is_cuda:\n            output = BinarizedFunctionsCUDA.WeightBinarize_forward(x, True)\n        else:\n            # Current CPU kernel implementations do not improve performance\n            # output = BinarizedFunctionsCPU.WeightBinarize_forward(x, True)\n            norm = x.abs().mean([1, 2, 3], keepdim=True)\n            sign = ((x > 0).type(x.dtype) * 2 - 1)\n            output = sign * norm\n            return output\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\n\nclass DOREFABinarizeFn(torch.autograd.Function):\n    """""" Binarizes x into `scale` * { +1; -1}, where +1 or -1 are chosen based\n        on whether the x element value is >0 or <0. `scale` is determined as mean of absolute\n        values of the entire x tensor. """"""\n    @staticmethod\n    def symbolic(g, x):\n        zero = g.constant(0, [1], \'float\')\n        zero = g.op(""Unsqueeze"", zero, axes_i=[1, 2, 3])\n        scale = g.op(""Abs"", x)\n        scale = g.op(""ReduceMean"", scale, axes_i=[0, 1, 2, 3])\n        scale_neg = g.op(""Neg"", scale)\n        return g.op(""FakeQuantize"", x, zero, zero, scale_neg, scale, levels_i=2)\n\n    @staticmethod\n    def forward(ctx, x):\n        if x.is_cuda:\n            output = BinarizedFunctionsCUDA.WeightBinarize_forward(x, False)\n        else:\n            # Current CPU kernel implementations do not improve performance\n            # output = BinarizedFunctionsCPU.WeightBinarize_forward(x, False)\n            norm = x.abs().mean()\n            sign = ((x > 0).type(x.dtype) * 2 - 1)\n            output_flat = sign * norm\n            return output_flat.view_as(x)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\n\n# Activation binarization function\nclass ActivationBinarizationScaleThresholdFn(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, x, scale, threshold):\n        zero = g.constant(0, [1], \'float\')\n        zero = g.op(""Unsqueeze"", zero, axes_i=[0, 2, 3])\n        threshold = g.op(""Mul"", threshold, scale)\n        scale = g.op(""Unsqueeze"", scale, axes_i=[0, 2, 3])\n        return g.op(""FakeQuantize"", x, threshold, threshold, zero, scale, levels_i=2)\n\n    @staticmethod\n    def forward(ctx, input_, scale, threshold):\n        if input_.is_cuda:\n            output = BinarizedFunctionsCUDA.ActivationBinarize_forward(input_, scale, threshold)\n        else:\n            # Current CPU kernel implementations do not improve performance\n            # output = BinarizedFunctionsCPU.ActivationBinarize_forward(input_, scale, threshold)\n            shape = [1 for s in input_.shape]\n            shape[1] = input_.shape[1]\n            t = (threshold * scale).view(shape)\n            output = (input_ > t).type(input_.dtype) * scale\n            ctx.save_for_backward(input_, scale, output)\n        ctx.save_for_backward(input_, scale, output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if grad_output.is_cuda:\n            if not grad_output.is_contiguous():\n                warnings.warn(""grad_output is not contiguous!"", RuntimeWarning)\n                grad_output = grad_output.contiguous()\n\n        input_, scale, output = ctx.saved_variables\n\n        if input_.is_cuda:\n            grad_input, grad_scale, grad_threshold = BinarizedFunctionsCUDA.ActivationBinarize_backward(grad_output,\n                                                                                                        input_,\n                                                                                                        scale, output)\n        else:\n            # Current CPU kernel implementations do not improve performance\n            # grad_input, grad_scale, grad_threshold = BinarizedFunctionsCPU.ActivationBinarize_backward(grad_output,\n            #                                                                                           input_,\n            #                                                                                           scale, output)\n            # calc gradient for input\n            mask_lower = (input_ <= scale).type(input_.dtype)\n            grad_input = grad_output * (input_ >= 0).type(input_.dtype) * mask_lower\n\n            # calc gradient for scale\n            err = (output - input_) * scale.reciprocal()\n            grad_scale = grad_output * (mask_lower * err + (1 - mask_lower))\n            grad_scale = grad_scale.sum().view(1)\n\n            # calc gradient for threshold\n            grad_threshold = -grad_output * (input_ > 0).type(input_.dtype) * (input_ < scale).type(input_.dtype)\n\n            for idx, _ in enumerate(input_.shape):\n                if idx != 1:  # sum over all dims except activations channel\n                    grad_threshold = grad_threshold.sum(idx, keepdim=True)\n\n        return grad_input, grad_scale, grad_threshold\n'"
pytorch_toolkit/nncf/nncf/binarization/extensions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport pathlib\nimport os.path\nfrom nncf.definitions import get_install_type\n\nfrom torch.utils.cpp_extension import load\n\n\nif ""VIRTUAL_ENV"" in os.environ:\n    build_dir = os.path.join(os.environ[""VIRTUAL_ENV""], ""torch_extensions"")\n    pathlib.Path(build_dir).mkdir(parents=True, exist_ok=True)\n    os.environ[""TORCH_EXTENSIONS_DIR""] = build_dir\n\next_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""cpu"")\nBinarizedFunctionsCPU = load(\n    \'binarized_functions_cpu\', [\n        os.path.join(ext_dir, \'functions_cpu.cpp\')\n    ],\n    verbose=False\n)\n\nif get_install_type() == ""GPU"":\n    ext_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""cuda"")\n    BinarizedFunctionsCUDA = load(\n        \'binarized_functions_cuda\', [\n            os.path.join(ext_dir, \'functions_cuda.cpp\'),\n            os.path.join(ext_dir, \'functions_cuda_kernel.cu\')\n        ],\n        verbose=False\n    )\n'"
pytorch_toolkit/nncf/nncf/binarization/layers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom nncf.dynamic_graph.patch_pytorch import register_operator\nfrom nncf.layer_utils import COMPRESSION_MODULES\nfrom nncf.registry import Registry\nfrom nncf.utils import get_per_channel_scale_shape\nfrom nncf.binarization.binarize_functions import XNORBinarizeFn, DOREFABinarizeFn\nfrom nncf.binarization.binarize_functions import ActivationBinarizationScaleThresholdFn\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\nBINARIZATION_MODULES = Registry(\'binarization_modules\')\n\n\nclass BinarizationMode:\n    XNOR = ""xnor""\n    DOREFA = ""dorefa""\n\n\nclass BaseBinarizer(nn.Module):\n    def __init__(self, enabled=False):\n        super().__init__()\n        self.register_buffer(\'enabled\', torch.IntTensor([0]))\n        if enabled:\n            self.is_enabled = True\n\n    @property\n    def is_enabled(self):\n        return self.enabled[0] == 1\n\n    @is_enabled.setter\n    def is_enabled(self, value: bool):\n        self.enabled[0] = 1 if value else 0\n\n    def forward(self, x):\n        if self.is_enabled:\n            return self.binarize(x)\n        return x\n\n    def binarize(self, x):\n        raise NotImplementedError\n\n    def enable(self):\n        self.is_enabled = True\n\n\nclass WeightBinarizer(BaseBinarizer):\n    def binarize(self, x):\n        raise NotImplementedError\n\n\nclass ActivationBinarizer(BaseBinarizer):\n    def binarize(self, x):\n        raise NotImplementedError\n\n\n@COMPRESSION_MODULES.register()\n@BINARIZATION_MODULES.register(BinarizationMode.XNOR)\nclass XNORBinarize(WeightBinarizer):\n    def binarize(self, x):\n        return xnor_binarize_op(x)\n\n\n@register_operator()\ndef xnor_binarize_op(x):\n    return XNORBinarizeFn.apply(x)\n\n\n@COMPRESSION_MODULES.register()\n@BINARIZATION_MODULES.register(BinarizationMode.DOREFA)\nclass DOREFABinarize(WeightBinarizer):\n    def binarize(self, x):\n        return dorefa_binarize_op(x)\n\n\n@register_operator()\ndef dorefa_binarize_op(x):\n    return DOREFABinarizeFn.apply(x)\n\n\n# Activation binarization module\nclass ActivationBinarizationScaleThreshold(ActivationBinarizer):\n    def __init__(self, input_shape, enabled=False, desc=""""):\n        super().__init__(enabled)\n\n        self.input_shape = input_shape\n\n        self.scale = torch.nn.Parameter(torch.Tensor([0]), requires_grad=enabled)\n        self.scale.data.zero_()\n\n        # Need scale_initialized as buffer for it to appear in the model state dict\n        self.register_buffer(\'scale_initialized\', torch.IntTensor([0]))\n\n        threshold_shape = get_per_channel_scale_shape(self.input_shape, is_weights=False)\n        self.threshold = torch.nn.Parameter(torch.ones(threshold_shape), requires_grad=enabled)\n        self.threshold.data.zero_()\n        self.bin = activation_bin_scale_threshold_op\n\n    @property\n    def is_scale_initialized(self):\n        return self.scale_initialized[0] == 1\n\n    @is_scale_initialized.setter\n    def is_scale_initialized(self, value: bool):\n        self.scale_initialized[0] = 1 if value else 0\n\n    def binarize(self, x):\n        if self.training and not self.is_scale_initialized:\n            # init scale using nonbinarized activation statistics\n            d = x.detach().data.contiguous().view(-1)\n            top_num = max(1, round(d.shape[0]*0.001))\n            topk_res = d.topk(top_num)\n            scale = topk_res[0].min()\n            nncf_logger.info(""Binarized activation scale set to: {}"".format(scale.item()))\n            self.scale.data[:] = scale.log()\n            self.is_scale_initialized = True\n\n        x = self.bin(x, self.scale.exp(), self.threshold.sigmoid())\n\n        return x\n\n    def enable(self):\n        super().enable()\n        self.scale.requires_grad_(True)\n        self.threshold.requires_grad_(True)\n\n\n@register_operator()\ndef activation_bin_scale_threshold_op(x, scale, threshold):\n    return ActivationBinarizationScaleThresholdFn.apply(x, scale, threshold)\n'"
pytorch_toolkit/nncf/nncf/binarization/schedulers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom nncf.algo_selector import Registry\nfrom nncf.compression_method_api import CompressionScheduler\nfrom nncf.config import Config\n\nBINARIZATION_SCHEDULERS = Registry(""binarization_schedulers"")\n\n\n@BINARIZATION_SCHEDULERS.register(""staged"")\nclass StagedBinarizationScheduler(CompressionScheduler):\n    def __init__(self, binarization_algo, config=None):\n        super().__init__()\n        if config is None:\n            config = Config()\n        c = config[\'params\']\n        self.config = config\n        self.algo = binarization_algo\n        self.activations_bin_start_epoch = c.get(\'activations_bin_start_epoch\', 1)\n        self.weights_bin_start_epoch = c.get(\'weights_bin_start_epoch\', 1)\n        self._set_binarization_status()\n\n    def epoch_step(self, epoch=None):\n        super().epoch_step(epoch)\n        self._set_binarization_status()\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self._set_binarization_status()\n\n    def _set_binarization_status(self):\n        if self.last_epoch >= self.activations_bin_start_epoch:\n            self.algo.enable_activation_binarization()\n        if self.last_epoch >= self.weights_bin_start_epoch:\n            self.algo.enable_weight_binarization()\n\n    def _calc_density_level(self):\n        raise NotImplementedError\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/context.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport re\nimport threading\nfrom collections import deque\nfrom contextlib import contextmanager\nfrom typing import Callable, List\nfrom copy import deepcopy\n\nfrom nncf.debug import is_debug\nfrom nncf.dynamic_graph.graph import InputAgnosticOperationExecutionContext\nfrom nncf.dynamic_graph.graph import NNCFGraph, NNCFNode\nfrom nncf.dynamic_graph.trace_tensor import make_input_infos\nfrom nncf.dynamic_graph.version_agnostic_op_names import get_version_agnostic_name\n\n_CURRENT_CONTEXT = None\n\n\nclass OperatorInput:\n    def __init__(self, op_args, op_kwargs):\n        self.op_args = op_args\n        self.op_kwargs = op_kwargs\n\n\nclass ScopeElement:\n    def __init__(self, calling_module_class_name: str, calling_field_name: str = None):\n        self.calling_module_class_name = calling_module_class_name\n        self.calling_field_name = calling_field_name\n\n    def __str__(self):\n        if self.calling_field_name is None:\n            return self.calling_module_class_name\n        return ""{cls}[{name}]"".format(cls=self.calling_module_class_name,\n                                      name=self.calling_field_name)\n\n    def __eq__(self, other: \'ScopeElement\'):\n        return (self.calling_module_class_name == other.calling_module_class_name) and \\\n               (self.calling_field_name == other.calling_field_name)\n\n    def __hash__(self):\n        return hash((self.calling_module_class_name, self.calling_field_name))\n\n    @staticmethod\n    def from_str(string: str):\n        matches = re.search(r""(.*)\\[(.*)\\]|(.*)"", string)\n        if matches is None:\n            raise RuntimeError(""Invalid scope element string"")\n        if matches.groups()[0] is None and matches.groups()[1] is None:\n            return ScopeElement(matches.groups()[2])\n        if matches.groups()[0] is not None and matches.groups()[1] is not None:\n            return ScopeElement(matches.groups()[0], matches.groups()[1])\n        raise RuntimeError(""Could not parse the scope element string"")\n\n\nclass Scope:\n    def __init__(self, scope_elements: List[ScopeElement] = None):\n        if scope_elements is not None:\n            self.scope_elements = scope_elements\n        else:\n            self.scope_elements = []\n\n    def __str__(self):\n        return \'/\'.join([str(scope_el) for scope_el in self.scope_elements])\n\n    def __hash__(self):\n        return hash(str(self))\n\n    def __eq__(self, other: \'Scope\'):\n        return self.scope_elements == other.scope_elements\n\n    def __getitem__(self, key):\n        return self.scope_elements[key]\n\n    def __contains__(self, item: \'Scope\'):\n        """"""Idiom: (\'A/B/C\' in \'A/B\') == True""""""\n        if len(self.scope_elements) > len(item.scope_elements):\n            return False\n        for i in range(len(self.scope_elements)):\n            if self.scope_elements[i] != item.scope_elements[i]:\n                return False\n        return True\n\n    def __add__(self, rhs):\n        init_list = self.scope_elements + rhs.scope_elements\n        return Scope(init_list)\n\n    def copy(self):\n        return Scope(deepcopy(self.scope_elements))\n\n    def push(self, scope_element: ScopeElement):\n        self.scope_elements.append(scope_element)\n\n    def pop(self) -> ScopeElement:\n        return self.scope_elements.pop()\n\n    @staticmethod\n    def from_str(string: str) -> \'Scope\':\n        elts = string.split(\'/\')\n        return Scope([ScopeElement.from_str(s) for s in elts])\n\n\n# pylint: disable=too-many-public-methods\nclass TracingContext:\n    def __init__(self):\n        self.graph = NNCFGraph()\n\n        self._save_context = None\n        self._post_hooks = {}\n        self._pre_hooks = {}\n        self._num_nested_hooks = 0\n\n        self._thread_local = threading.local()\n\n        self._n_instance = 0\n        self._cond = threading.Condition()\n        self.is_tracing = True\n        self._input_comparators_per_scope = []\n\n    def __enter__(self):\n        global _CURRENT_CONTEXT\n        self._save_context = _CURRENT_CONTEXT\n        _CURRENT_CONTEXT = self\n        self._init_thread_local()\n        if is_debug():\n            self.reset_node_call_counters()\n\n        return self\n\n    def __exit__(self, *args):\n        self.reset_scope_operator_call_counters()\n        self.leave()\n\n    def find_operator_node(self, inputs,\n                           ia_op_exec_context: InputAgnosticOperationExecutionContext) -> NNCFNode:\n        with self._cond:\n            self._n_instance += 1\n        tensor_metas = make_input_infos(inputs)\n\n        node = self.graph.find_node(ia_op_exec_context, tensor_metas, self._input_comparators_per_scope)\n\n        with self._cond:\n            self._n_instance -= 1\n            self._cond.notify_all()\n\n        if node is None:\n            with self._cond:\n                while self._n_instance > 0:\n                    self._cond.wait()\n                # Another thread may have added a node inside this block,\n                # so we need to check again if a node is already added.\n                node = self.graph.find_node(ia_op_exec_context, tensor_metas, self._input_comparators_per_scope)\n                if node is None:\n                    node = self.graph.add_node(ia_op_exec_context, tensor_metas, self._input_comparators_per_scope,\n                                               inputs)\n        return node\n\n    def get_caller_context(self, operator_type: str) -> InputAgnosticOperationExecutionContext:\n        """"""\n        Designed to work in the following way - for each scope the context will track the number of the calls to the\n        operators with the name operator_type (call_order). The counter values are preserved until reset by a\n        corresponding member function of the context, which must be called after each model iteration - this is\n        usually handled inside NNCF. This mechanism allows to discern between multiple function calls inside the same\n        module that would each require their own instance of compression layers - for instance, multiple `relu`\n        function calls (either on their own or inside a `for` cycle), and at the same moment allow the checkpoints to\n        be loaded if the model had changed in the meantime in a way that does not impact the major function call\n        order (e.g. if comments were added to the .py file with the model)\n        """"""\n        version_agnostic_operator_type = get_version_agnostic_name(operator_type)\n\n        call_order = self.get_operator_call_count_in_scope(version_agnostic_operator_type, self.scope)\n\n        ia_op_exec_context = InputAgnosticOperationExecutionContext(version_agnostic_operator_type,\n                                                                    self.scope,\n                                                                    call_order)\n        return ia_op_exec_context\n\n    def reset_scope_operator_call_counters(self):\n        """"""\n        Must be called after each ""forward"" operation of the model that is made\n        within this context\n        """"""\n        self._thread_local.operator_counters = {}\n\n    @staticmethod\n    def _get_operator_counter_key(operator_name: str, scope: Scope):\n        return ""{}_{}"".format(str(scope), operator_name)\n\n    def register_operator_call(self, operator_name: str, scope: Scope):\n        key = self._get_operator_counter_key(operator_name, scope)\n        if key in self._thread_local.operator_counters:\n            self._thread_local.operator_counters[key] += 1\n        else:\n            self._thread_local.operator_counters[key] = 1\n\n    def get_operator_call_count_in_scope(self, operator_name: str, scope: Scope):\n        key = self._get_operator_counter_key(operator_name, scope)\n        if key in self._thread_local.operator_counters:\n            return self._thread_local.operator_counters[key]\n        return 0\n\n    def reset_operator_call_count_in_scope(self, scope):\n        scoped_op_name = str(scope)\n        for key in self._thread_local.operator_counters.keys():\n            if scoped_op_name in key:\n                self._thread_local.operator_counters[key] = 0\n\n    def enter(self):\n        global _CURRENT_CONTEXT\n        self._save_context = _CURRENT_CONTEXT\n        _CURRENT_CONTEXT = self\n        self._init_thread_local()\n\n    def leave(self):\n        global _CURRENT_CONTEXT\n        _CURRENT_CONTEXT = self._save_context\n        self._save_context = None\n\n    def push_scope(self, scope_module):\n        relative_scopes_list = self._get_scope_relative_to_last_registered_module_call(scope_module)\n        self.scope_modules.append(scope_module)\n        self.relative_scopes_stack.append(relative_scopes_list)\n\n    def pop_scope(self):\n        self.relative_scopes_stack.pop()\n        self.scope_modules.pop()\n\n    def register_pre_hooks(self, fn_list: List[Callable], ia_op_exec_context: InputAgnosticOperationExecutionContext):\n        if ia_op_exec_context in self._pre_hooks:\n            raise KeyError(""Pre hook for context {} is already registered"".format(str(ia_op_exec_context)))\n        self._pre_hooks[ia_op_exec_context] = fn_list\n\n    def execute_pre_hooks(self, ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                          op_inputs: OperatorInput) -> OperatorInput:\n        in_op = getattr(self, \'in_operator\', False)\n        self.in_operator = False\n        self._thread_local.num_nested_hooks += 1\n        if ia_op_exec_context in self._pre_hooks:\n            for hook in self._pre_hooks[ia_op_exec_context]:\n                op_inputs = hook(op_inputs)\n        self._thread_local.num_nested_hooks -= 1\n        self.in_operator = in_op\n        return op_inputs\n\n    def register_post_hooks(self, fn_list: List[Callable], ia_op_exec_context: InputAgnosticOperationExecutionContext):\n        if ia_op_exec_context in self._post_hooks:\n            raise KeyError(""Post hook for context {} is already registered"".format(str(ia_op_exec_context)))\n        self._post_hooks[ia_op_exec_context] = fn_list\n\n    def execute_post_hooks(self, ia_op_exec_context: InputAgnosticOperationExecutionContext, outputs):\n        in_op = getattr(self, \'in_operator\', False)\n        self.in_operator = False\n        self._thread_local.num_nested_hooks += 1\n        if ia_op_exec_context in self._post_hooks:\n            for hook in self._post_hooks[ia_op_exec_context]:\n                outputs = hook(outputs)\n        self._thread_local.num_nested_hooks -= 1\n        self.in_operator = in_op\n        return outputs\n\n    def disable_tracing(self):\n        self.is_tracing = False\n\n    def enable_tracing(self):\n        self.is_tracing = True\n\n    def add_node_comparators(self, scopes_to_apply: List[str],\n                             node_input_comparator: \'TensorMetaComparator\' = None):\n        self._input_comparators_per_scope.append((node_input_comparator, scopes_to_apply))\n\n    @property\n    def base_module_thread_local_replica(self):\n        self._init_thread_local()\n        return self._thread_local.base_module_replica\n\n    @base_module_thread_local_replica.setter\n    def base_module_thread_local_replica(self, value):\n        self._init_thread_local()\n        self._thread_local.base_module_replica = value\n\n    @property\n    def in_operator(self):\n        self._init_thread_local()\n        return self._thread_local.in_operator\n\n    @in_operator.setter\n    def in_operator(self, val):\n        self._init_thread_local()\n        self._thread_local.in_operator = val\n\n    @property\n    def scope_modules(self):\n        self._init_thread_local()\n        return self._thread_local.scope_modules\n\n    @property\n    def relative_scopes_stack(self) -> List[Scope]:\n        self._init_thread_local()\n        return self._thread_local.scopes\n\n    def _init_thread_local(self):\n        # todo: master node part!\n        tl = self._thread_local\n        if getattr(tl, \'ready\', False):\n            return\n        tl.ready = True\n        tl.scopes = []\n        tl.scope_modules = []\n        tl.in_operator = False\n        tl.num_nested_hooks = 0\n        tl.base_module_replica = None\n        tl.operator_counters = {}\n        tl.node_call_tracker = {}\n\n    def register_node_call(self, node_key: str):\n        if node_key in self._thread_local.node_call_tracker:\n            self._thread_local.node_call_tracker[node_key] += 1\n        else:\n            self._thread_local.node_call_tracker[node_key] = 1\n\n    def reset_node_call_counters(self):\n        for k, _ in self._thread_local.node_call_tracker.items():\n            self._thread_local.node_call_tracker[k] = 0\n\n    def get_node_call_counter_dict(self):\n        return self._thread_local.node_call_tracker\n\n    def _get_scope_relative_to_last_registered_module_call(self, module) -> Scope:\n        module_class = module.__class__.__name__\n        if not self.scope_modules:\n            return Scope([ScopeElement(module_class), ])\n        q = deque([(tuple(), self.scope_modules[-1])])\n        while q:\n            scope_parts, top = q.popleft()\n            if module is top:\n                return Scope(list(scope_parts))\n            for name, child in top.named_children():\n                scope_element = ScopeElement(child.__class__.__name__, name)\n                q.append((scope_parts + (scope_element,), child))\n        return Scope([ScopeElement(module_class), ])\n\n    @property\n    def scope(self) -> Scope:\n        stack_copy = self.relative_scopes_stack.copy()\n        scope_el_list = []\n        for relative_scope in stack_copy:\n            for scope_element in relative_scope.scope_elements:\n                scope_el_list.append(scope_element)\n        return Scope(scope_el_list)\n\n    def reset_graph(self):\n        self.graph = NNCFGraph()\n\n\n@contextmanager\ndef no_nncf_trace():\n    ctx = get_current_context()\n    if ctx is not None:\n        ctx.disable_tracing()\n    yield\n    if ctx is not None:\n        ctx.enable_tracing()\n\n\ndef get_current_context() -> TracingContext:\n    return _CURRENT_CONTEXT\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/function_input_quantization.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nclass FunctionQuantizationInfo:\n    """"""A FunctionQuantizationInfo(\'foo\', [0, 2, 3]) will specify that 0-th, 2-nd and 3-rd arguments\n    of the function torch.nn.functional.foo will be considered for quantization.""""""\n    def __init__(self, name: str, positions_of_args_to_quantize: list):\n        self.name = name\n        self.positions_of_args_to_quantize = positions_of_args_to_quantize\n\n\n# Specification for function inputs to be quantized\n# E.g.: x = torch.nn.functional.linear(input, weight, bias)\n\nFUNCTIONS_TO_QUANTIZE = [\n    FunctionQuantizationInfo(\'linear\', [0, 1])\n]\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/graph.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\nfrom _warnings import warn\nfrom typing import Callable, List, Optional, Tuple, Any\n\nimport networkx as nx\nfrom copy import deepcopy\nfrom networkx.drawing.nx_agraph import to_agraph\nfrom torch import Tensor\n\nfrom nncf.dynamic_graph.graph_matching import Expression, NodeExpression, search_all, get_edge_boundaries\nfrom nncf.dynamic_graph.trace_tensor import TensorMeta, TracedTensor\nfrom nncf.layers import ITERATION_MODULES\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\n\n# pylint: disable=too-many-public-methods\nclass TensorMetaComparator:\n    def __call__(self, lhs: TensorMeta, rhs: TensorMeta) -> bool:\n        raise NotImplementedError\n\n\nclass DefaultTensorMetaComparator(TensorMetaComparator):\n    def __call__(self, lhs: TensorMeta, rhs: TensorMeta) -> bool:\n        return TensorMeta.default_comparator(lhs, rhs)\n\n\nclass ShapeIgnoringTensorMetaComparator(TensorMetaComparator):\n    def __call__(self, lhs: TensorMeta, rhs: TensorMeta) -> bool:\n        return lhs.creator_id == rhs.creator_id and lhs.index == rhs.index\n\n\nclass ShapeOnlyTensorMetaComparator(TensorMetaComparator):\n    def __call__(self, lhs: TensorMeta, rhs: TensorMeta) -> bool:\n        return lhs.shape[1:] == rhs.shape[1:]\n\n\nclass InputsMatcher:\n    def __call__(self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta],\n                 tm_comparators: List[TensorMetaComparator]) -> bool:\n        raise NotImplementedError\n\n\nclass FirstInputsMatcher(InputsMatcher):\n    def __call__(self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta],\n                 tm_comparators: List[TensorMetaComparator]) -> bool:\n        if not node_inputs or not real_inputs:\n            return False\n\n        if not node_inputs[0] or not real_inputs[0]:\n            return False\n\n        for tm_comparator in tm_comparators:\n            if not tm_comparator(node_inputs[0], real_inputs[0]):\n                return False\n        return True\n\n\nclass DefaultInputsMatcher(InputsMatcher):\n    def __call__(self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta],\n                 tm_comparators: List[TensorMetaComparator]) -> bool:\n        if node_inputs is None and real_inputs:\n            return False\n\n        for saved_input, actual_input in zip(node_inputs, real_inputs):\n            if saved_input is None and actual_input is None:\n                continue\n            if (saved_input is None) != (actual_input is None):\n                return False\n            for tm_comparator in tm_comparators:\n                if not tm_comparator(saved_input, actual_input):\n                    return False\n        return True\n\n\nclass InputAgnosticOperationExecutionContext:\n    def __init__(self, operator_name: str, scope_in_model: \'Scope\', call_order: int):\n        self.operator_name = operator_name\n        self.scope_in_model = scope_in_model\n        self.call_order = call_order\n\n    def __eq__(self, other: \'InputAgnosticOperationExecutionContext\'):\n        return (self.operator_name == other.operator_name) and \\\n               (self.scope_in_model == other.scope_in_model) and \\\n               (self.call_order == other.call_order)\n\n    def __str__(self):\n        return str(self.scope_in_model) + \'/\' + \\\n               self.operator_name + ""_"" + str(self.call_order)\n\n    def __hash__(self):\n        return hash((self.operator_name, self.scope_in_model, self.call_order))\n\n\nclass OperationExecutionContext:\n    """"""Information that allows to uniquely identify an operation inside the NNCF graph,\n    i.e. determine whether an execution of the operator inside the module has already been\n    registered as a node in the graph or not (in the latter case a new node would have to\n    be created""""""\n\n    def __init__(self,\n                 operator_name: str,\n                 scope_in_model: \'Scope\',\n                 call_order: int,\n                 tensor_metas: List[TensorMeta],\n                 tm_comparators: List[TensorMetaComparator] = None,\n                 input_matcher: InputsMatcher = None):\n        self.input_agnostic = InputAgnosticOperationExecutionContext(operator_name, scope_in_model, call_order)\n        # This should be a list with a length equal to the number of inputs.\n        # ""None"" values in this list correspond to non-tensor input elements.\n        self.tensor_metas = tensor_metas\n        self.tm_comparators = tm_comparators if tm_comparators else [\n            DefaultTensorMetaComparator()]\n        self.input_matcher = input_matcher if input_matcher else DefaultInputsMatcher()\n\n    def __eq__(self, other: \'OperationExecutionContext\'):\n        return (self.input_agnostic == other.input_agnostic) and \\\n               self.input_matcher(self.tensor_metas, other.tensor_metas, self.tm_comparators)\n\n    def __hash__(self):\n        return hash((self.operator_name, tuple(self.scope_in_model), self.call_order,\n                     tuple(self.tensor_metas)))\n\n    def __str__(self):\n        input_info_str = """"\n        for meta in self.tensor_metas:\n            if meta is None:\n                input_info_str += ""N;""\n            else:\n                input_info_str += str(meta) + "";""\n\n        return super().__str__() + \'(\' + input_info_str + \')\'\n\n    @property\n    def operator_name(self):\n        return self.input_agnostic.operator_name\n\n    @property\n    def scope_in_model(self) -> \'Scope\':\n        return self.input_agnostic.scope_in_model\n\n    @property\n    def call_order(self):\n        return self.input_agnostic.call_order\n\n\nclass NNCFNode:\n    def __init__(self, node_id: int, op_exec_context: OperationExecutionContext):\n        self.node_id = node_id\n        self.op_exec_context = op_exec_context\n\n    def __str__(self):\n        return str(self.node_id) + "" "" + str(self.op_exec_context)\n\n    def __hash__(self):\n        return hash(str(self))\n\n    def __eq__(self, other):\n        return self.node_id == other.node_id and self.op_exec_context == other.op_exec_context\n\n\nclass DefaultScopeNodeMatcher:\n    def __init__(self, node_id_to_key_dict, nx_graph, nx_node_to_nncf_node):\n        self._node_id_to_key_dict = node_id_to_key_dict\n        self._nx_graph = nx_graph\n        self._nx_node_to_nncf_node = nx_node_to_nncf_node\n        self._inputless_nx_nodes = dict()\n\n    def get_node_by_id(self, node_id):\n        return self._nx_graph.nodes[self._node_id_to_key_dict[node_id]]\n\n    def _find_nodes_with_matching_context_among_inputless(self, op_exec_context: OperationExecutionContext):\n        node_candidates = {}\n        for nx_node_key, nx_node in self._inputless_nx_nodes.items():\n            if nx_node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR] == op_exec_context:\n                node_candidates[nx_node_key] = nx_node\n        return node_candidates\n\n    def _find_nodes_with_matching_context_and_inputs(self, op_exec_context: OperationExecutionContext):\n        node_candidates = {}\n        for info in op_exec_context.tensor_metas:\n            if info is None or info.creator_id is None:\n                continue\n            creator_id = info.creator_id\n            for successor_node_key in self._nx_graph.successors(self._node_id_to_key_dict[creator_id]):\n                successor_node = self._nx_graph.nodes[successor_node_key]\n                if op_exec_context == successor_node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]:\n                    node_candidates[successor_node_key] = successor_node\n        return node_candidates\n\n    def add_node(self, op_exec_context: OperationExecutionContext, inputs) -> NNCFNode:\n        node_id = len(self._node_id_to_key_dict)\n\n        name_parts = (str(op_exec_context.scope_in_model), op_exec_context.operator_name)\n        node_key = \'{idx} {uri}\'.format(uri=\'/\'.join(name_parts), idx=node_id)\n\n        nncf_logger.debug(""New node added to NNCF graph: {}"".format(node_key))\n\n        self._node_id_to_key_dict[node_id] = node_key\n        attrs = {\n            NNCFGraph.ID_NODE_ATTR: node_id,\n            NNCFGraph.KEY_NODE_ATTR: node_key,\n            NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR: op_exec_context,\n        }\n        self._nx_graph.add_node(node_key, **attrs)\n\n        has_traced_inputs = False\n        for info in op_exec_context.tensor_metas:\n            if info is None or info.creator_id is None:\n                continue\n            parent = self._node_id_to_key_dict[info.creator_id]\n            self._nx_graph.add_edge(parent, node_key)\n            has_traced_inputs = True\n            self._nx_graph.edges[parent, node_key][NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR] = info.shape\n\n        if not has_traced_inputs:\n            self._inputless_nx_nodes[node_key] = self._nx_graph.nodes[node_key]\n\n        return self._nx_node_to_nncf_node(self._nx_graph.nodes[node_key])\n\n    def find_node(self, ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                  tensor_metas: List[TensorMeta],\n                  tm_comparators: List[TensorMetaComparator]) -> NNCFNode:\n        op_exec_context = OperationExecutionContext(ia_op_exec_context.operator_name,\n                                                    ia_op_exec_context.scope_in_model,\n                                                    ia_op_exec_context.call_order,\n                                                    tensor_metas,\n                                                    tm_comparators=tm_comparators)\n        nncf_node_candidates = []\n        node_candidates = self._find_nodes_with_matching_context_and_inputs(op_exec_context)\n        if not node_candidates:\n            node_candidates = self._find_nodes_with_matching_context_among_inputless(op_exec_context)\n\n        for nx_node in node_candidates.values():\n            nncf_node_candidates.append(NNCFNode(nx_node[NNCFGraph.ID_NODE_ATTR],\n                                                 op_exec_context))\n        result = None\n        if len(nncf_node_candidates) == 1:\n            result = nncf_node_candidates[0]\n        if len(nncf_node_candidates) > 1:\n            warn(""More than one node matches input"")\n            result = nncf_node_candidates[0]\n\n        return result\n\n\nclass IterationScopeNodeMatcher(DefaultScopeNodeMatcher):\n    def __init__(self, node_id_to_key_dict, nx_graph, nx_node_to_nncf_node):\n        super().__init__(node_id_to_key_dict, nx_graph, nx_node_to_nncf_node)\n        self._first_iteration_nodes = {}  # type: {str: {str: NNCFNode}}\n\n    @staticmethod\n    def _get_iteration_scopes(scope: \'Scope\') -> List[str]:\n        results = []\n        scope_name = str(scope)\n        for iter_scope in ITERATION_MODULES.registry_dict:\n            if iter_scope in scope_name:\n                results.append(iter_scope)\n        return results\n\n    def save_first_iteration_node(self, inputs, node: NNCFNode):\n        """"""\n        It finds and saves ""starting"" points of iteration for further matching with them on next iteration,\n        instead of adding new nodes for each iteration. ""Starting"" points of iteration are nodes\n            * that have at least one input node, which is outside of iteration scope\n            * or whose all inputs are not TracedTensor\n        """"""\n        op_exec_context = node.op_exec_context\n        name = node\n        iter_scopes = self._get_iteration_scopes(op_exec_context.scope_in_model)\n        if iter_scopes:\n            for iter_scope in iter_scopes:\n                if iter_scope not in self._first_iteration_nodes:\n                    self._first_iteration_nodes[iter_scope] = {}\n                first_nodes = self._first_iteration_nodes[iter_scope]\n                has_input_outside_iteration = False\n                not_traced_count = 0\n                for i in inputs:\n                    if isinstance(i, Tensor):\n                        has_input_outside_iteration = True\n                        break\n                    if not isinstance(i, TracedTensor):\n                        not_traced_count += 1\n                        continue\n                    creator_id = i.tensor_meta.creator_id\n                    creator_node = self.get_node_by_id(creator_id)\n                    creator_node_op_exec_ctx = creator_node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n                    within_scopes = self._get_iteration_scopes(creator_node_op_exec_ctx.scope_in_model)\n                    if iter_scope not in within_scopes:\n                        has_input_outside_iteration = True\n                if not_traced_count == len(inputs):\n                    has_input_outside_iteration = True\n                if has_input_outside_iteration:\n                    node_name = str(op_exec_context.input_agnostic)\n                    first_nodes[node_name] = node\n                    nncf_logger.debug(\'Found first iteration node: {} in scope: {}\'.format(name, iter_scope))\n\n    def add_node(self, op_exec_context: OperationExecutionContext, inputs) -> NNCFNode:\n        node = super().add_node(op_exec_context, inputs)\n        self.save_first_iteration_node(inputs, node)\n        return node\n\n    def find_node(self,\n                  ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                  tensor_metas: List[TensorMeta],\n                  tm_comparators: List[TensorMetaComparator]) -> NNCFNode:\n        nncf_node_candidates = []\n        iter_scopes = self._get_iteration_scopes(ia_op_exec_context.scope_in_model)\n        # compare meta information about first input nodes during the matching. During the iteration some nodes may\n        # change number of inputs, e.g. on concat of hidden outputs\n        input_matcher = FirstInputsMatcher()\n        op_exec_context = OperationExecutionContext(ia_op_exec_context.operator_name,\n                                                    ia_op_exec_context.scope_in_model,\n                                                    ia_op_exec_context.call_order,\n                                                    tensor_metas,\n                                                    input_matcher=input_matcher,\n                                                    tm_comparators=tm_comparators)\n        node_candidates = self._find_nodes_with_matching_context_and_inputs(op_exec_context)\n        if not node_candidates:\n            op_exec_context = OperationExecutionContext(ia_op_exec_context.operator_name,\n                                                        ia_op_exec_context.scope_in_model,\n                                                        ia_op_exec_context.call_order,\n                                                        tensor_metas,\n                                                        tm_comparators=tm_comparators)\n            node_candidates = self._find_nodes_with_matching_context_among_inputless(op_exec_context)\n            if not node_candidates and iter_scopes:\n                # ignore information about node creator and index of input\n                comparators = tm_comparators + [ShapeOnlyTensorMetaComparator()]\n                op_exec_context = OperationExecutionContext(ia_op_exec_context.operator_name,\n                                                            ia_op_exec_context.scope_in_model,\n                                                            ia_op_exec_context.call_order,\n                                                            tensor_metas,\n                                                            tm_comparators=comparators)\n                # match with starting points of iteration\n                iter_nodes = self._match_first_iteration_nodes(op_exec_context, iter_scopes)\n                for node in iter_nodes.items():\n                    nncf_node_candidates.append(node[1])\n\n        for nx_node in node_candidates.values():\n            nncf_node_candidates.append(NNCFNode(nx_node[NNCFGraph.ID_NODE_ATTR],\n                                                 op_exec_context))\n\n        result = None\n        if len(nncf_node_candidates) == 1:\n            result = nncf_node_candidates[0]\n        if len(nncf_node_candidates) > 1:\n            warn(""More than one node matches input"")\n            result = nncf_node_candidates[0]\n\n        return result\n\n    def _match_first_iteration_nodes(self, op_exec_context: OperationExecutionContext, iter_scopes):\n        node_candidates = {}\n        for iter_scope in iter_scopes:\n            if iter_scope in self._first_iteration_nodes:\n                for name, node in self._first_iteration_nodes[iter_scope].items():\n                    if op_exec_context == node.op_exec_context:\n                        node_candidates[name] = node\n                        break\n                if node_candidates:\n                    break\n        return node_candidates\n\n\nclass NodeManager:\n    def __init__(self, node_id_to_key_dict, nx_graph, nx_node_to_nncf_node):\n        self.base_matcher = DefaultScopeNodeMatcher(node_id_to_key_dict, nx_graph, nx_node_to_nncf_node)\n        self.iteration_matcher = IterationScopeNodeMatcher(node_id_to_key_dict, nx_graph, nx_node_to_nncf_node)\n\n    # TODO: optimize by matching exact module type\n    @staticmethod\n    def _within_iteration(scope: \'Scope\'):\n        scope_name = str(scope)\n        for iter_scope in ITERATION_MODULES.registry_dict:\n            if iter_scope in scope_name:\n                return True\n        return False\n\n    def choose_matcher(self, ia_op_exec_context: InputAgnosticOperationExecutionContext) -> DefaultScopeNodeMatcher:\n        if self._within_iteration(ia_op_exec_context.scope_in_model):\n            return self.iteration_matcher\n        return self.base_matcher\n\n    @staticmethod\n    def choose_tm_comparators(ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                              input_comparators_per_scope:\n                              List[Tuple[TensorMetaComparator, List[str]]]) -> List[TensorMetaComparator]:\n        result = []\n        for pairs in input_comparators_per_scope:\n            comparator, scopes = pairs\n            for scope in scopes:\n                if scope in str(ia_op_exec_context):\n                    result.append(comparator)\n        return result\n\n    def find_node(self, ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                  tensor_metas: List[TensorMeta],\n                  input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]]) -> NNCFNode:\n        matcher = self.choose_matcher(ia_op_exec_context)\n        comparators = self.choose_tm_comparators(ia_op_exec_context, input_comparators_per_scope)\n        return matcher.find_node(ia_op_exec_context, tensor_metas, comparators)\n\n    def add_node(self, ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                 tensor_metas: List[TensorMeta],\n                 tm_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]],\n                 inputs) -> NNCFNode:\n        matcher = self.choose_matcher(ia_op_exec_context)\n        tm_comparators = self.choose_tm_comparators(ia_op_exec_context, tm_comparators_per_scope)\n        op_exec_context = OperationExecutionContext(ia_op_exec_context.operator_name,\n                                                    ia_op_exec_context.scope_in_model,\n                                                    ia_op_exec_context.call_order,\n                                                    tensor_metas,\n                                                    tm_comparators=tm_comparators)\n\n        return matcher.add_node(op_exec_context, inputs)\n\n\nclass NNCFGraphEdge:\n    def __init__(self, from_node: NNCFNode, to_node: NNCFNode, tensor_shape: Tuple):\n        self.from_node = from_node\n        self.to_node = to_node\n        self.tensor_shape = tensor_shape\n\n    def __str__(self):\n        return str(self.from_node) + "" -> "" + str(self.tensor_shape) + "" -> "" + str(self.to_node)\n\n    def __hash__(self):\n        return hash(str(self))\n\n    def __eq__(self, other):\n        return self.from_node == other.from_node and self.to_node == other.to_node \\\n               and self.tensor_shape == other.tensor_shape\n\n\nclass NNCFGraphPatternIO:\n    def __init__(self, input_edges: List[NNCFGraphEdge], output_edges: List[NNCFGraphEdge],\n                 input_nodes: List[NNCFNode],\n                 output_nodes: List[NNCFNode],\n                 ):\n        self.input_edges = input_edges\n        self.output_edges = output_edges\n        self.input_nodes = input_nodes\n        self.output_nodes = output_nodes\n\n\nclass NNCFGraph:\n    ID_NODE_ATTR = ""id""\n    KEY_NODE_ATTR = ""key""\n    OP_EXEC_CONTEXT_NODE_ATTR = ""op_exec_context""\n    ACTIVATION_SHAPE_EDGE_ATTR = ""activation_shape""\n\n    def __init__(self):\n        self._nx_graph = nx.DiGraph()\n        self._node_id_to_key_dict = dict()\n        self.match_manager = NodeManager(self._node_id_to_key_dict, self._nx_graph, self._nx_node_to_nncf_node)\n        self._input_nncf_nodes = []\n\n    def find_node(self,\n                  ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                  tensor_metas: List[TensorMeta],\n                  input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]]) -> NNCFNode:\n        return self.match_manager.find_node(ia_op_exec_context, tensor_metas, input_comparators_per_scope)\n\n    def add_node(self, ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                 tensor_metas: List[TensorMeta],\n                 input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]],\n                 inputs) -> NNCFNode:\n        node = self.match_manager.add_node(ia_op_exec_context, tensor_metas, input_comparators_per_scope, inputs)\n\n        from nncf.dynamic_graph.patch_pytorch import MODEL_INPUT_OP_NAME\n        if node.op_exec_context.operator_name == MODEL_INPUT_OP_NAME:  # TODO: refactorable model input node name\n            self._input_nncf_nodes.append(node)\n        return node\n\n    def get_nx_node_by_key(self, key: str):\n        return self._nx_graph.nodes[key]\n\n    def get_node_id_by_iap_context(self, iap_ctx: InputAgnosticOperationExecutionContext) -> str:\n        for node_key, node in self._nx_graph.nodes.items():\n            if node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR].input_agnostic == iap_ctx:\n                return node_key\n        raise AttributeError(\'Failed to get node by context={}\'.format(str(iap_ctx)))\n\n    def get_successors(self, node_name: str):\n        return self._nx_graph.successors(node_name)\n\n    def get_all_node_keys(self):\n        return self._node_id_to_key_dict.copy().values()\n\n    def get_all_node_idxs(self):\n        return self._node_id_to_key_dict.keys()\n\n    def get_node_key_by_id(self, node_id):\n        return self._node_id_to_key_dict[node_id]\n\n    def get_matching_nncf_graph_pattern_io_list(self, expression: Expression) -> List[NNCFGraphPatternIO]:\n        matched_node_key_sequences = search_all(self._nx_graph, expression)\n        pattern_ios = [self._get_nncf_graph_pattern_io_list(match) for match in matched_node_key_sequences]\n        return pattern_ios\n\n    def dump_graph(self, path, extended=False):\n        nx.drawing.nx_pydot.write_dot(self._get_graph_to_dump(extended), path)\n\n    def is_output_node(self, node: NNCFNode) -> bool:\n        return not list(self._nx_graph.successors(self._node_id_to_key_dict[node.node_id]))\n\n    def get_nx_graph_copy(self) -> nx.DiGraph:\n        return deepcopy(self._nx_graph)\n\n    def get_input_nodes(self) -> List[NNCFNode]:\n        return self._input_nncf_nodes\n\n    def get_graph_outputs(self) -> List[NNCFNode]:\n        outputs = []\n        for nx_node_key, deg in self._nx_graph.out_degree():\n            if deg == 0:\n                outputs.append(self._nx_node_to_nncf_node(self._nx_graph.nodes[nx_node_key]))\n        return outputs\n\n    def get_next_nodes(self, node: NNCFNode) -> Optional[List[NNCFNode]]:\n        nx_node_keys = self._nx_graph.succ[self._node_id_to_key_dict[node.node_id]]\n        return [self._nx_node_to_nncf_node(self._nx_graph.nodes[key]) for key in nx_node_keys]\n\n    def get_previous_nodes(self, node: NNCFNode) -> Optional[List[NNCFNode]]:\n        nx_node_keys = self._nx_graph.pred[self._node_id_to_key_dict[node.node_id]]\n        return [self._nx_node_to_nncf_node(self._nx_graph.nodes[key]) for key in nx_node_keys]\n\n    def get_inputs_count(self, node: NNCFNode) -> int:\n        return self._nx_graph.in_degree()[self._node_id_to_key_dict[node.node_id]]\n\n    def traverse_graph(self, curr_node: NNCFNode, traverse_function: Callable[[NNCFNode], Tuple[bool, List[Any]]],\n                       traverse_forward: bool = True):\n        output = []\n        return self._traverse_graph_recursive_helper(curr_node, traverse_function, output, traverse_forward)\n\n    def _traverse_graph_recursive_helper(self, curr_node: NNCFNode,\n                                         traverse_function: Callable[[NNCFNode], Tuple[bool, List[Any]]],\n                                         output: List[Any], traverse_forward: bool):\n        is_finished, output = traverse_function(curr_node, output)\n        get_nodes_fn = self.get_next_nodes if traverse_forward else self.get_previous_nodes\n        if not is_finished:\n            for node in get_nodes_fn(curr_node):\n                self._traverse_graph_recursive_helper(node, traverse_function, output, traverse_forward)\n        return output\n\n    def get_nodes_count(self):\n        return self._nx_graph.number_of_nodes()\n\n    @staticmethod\n    def node_type_fn(node: dict) -> str:\n        return node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR].operator_name\n\n    def get_output_shapes_for_ia_op_exec_context(self,\n                                                 ia_op_exec_context: InputAgnosticOperationExecutionContext)\\\n                                                 -> List[Tuple]:\n        node_key = self.get_node_id_by_iap_context(ia_op_exec_context)\n        succs = list(self._nx_graph.successors(node_key))\n        edge_list = [self._nx_graph.edges[node_key, to_node_key] for to_node_key in succs]\n        return [edge[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR] for edge in edge_list]\n\n    def _get_graph_to_dump(self, extended=False) -> nx.DiGraph:\n        """"""The graph to dump has certain node attributes omitted, compared to the graph stored\n         inside NNCFGraph.""""""\n        out_graph = nx.DiGraph()\n        for node_name, node in self._nx_graph.nodes.items():\n            op_exec_context = node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n            scope_str = str(op_exec_context.scope_in_model)\n            out_graph.add_node(node_name, type=op_exec_context.operator_name,\n                               id=node[NNCFGraph.ID_NODE_ATTR],\n                               scope=scope_str)\n        if extended:\n            for u, v in self._nx_graph.edges:\n                out_graph.add_edge(u, v, label=self._nx_graph.edges[u, v][NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR])\n        else:\n            for u, v in self._nx_graph.edges:\n                out_graph.add_edge(u, v)\n\n        return out_graph\n\n    def _get_topologically_last_nodes(self, matches: List[List[str]]) -> List[str]:\n        topological_order = {node: k for k, node in enumerate(nx.topological_sort(self._nx_graph))}\n        insertion_points = {max(match, key=topological_order.__getitem__) for match in matches}\n        for match in matches:\n            for node in match:\n                if len(list(self._nx_graph.successors(node))) > 1:\n                    insertion_points.add(node)\n\n        return list(insertion_points)\n\n    def _get_nncf_graph_pattern_input_output(self, match: List[str]) -> NNCFGraphPatternIO:\n        out_edge_boundary = list(nx.edge_boundary(self._nx_graph, match, data=True))\n        complement = list(filter(lambda x: x not in match, self._nx_graph.nodes.keys()))\n        in_edge_boundary = list(nx.edge_boundary(self._nx_graph, complement, data=True))\n        boundary = in_edge_boundary + out_edge_boundary\n        input_nncf_edges = []\n        output_nncf_edges = []\n        input_nncf_nodes = []\n        output_nncf_nodes = []\n        for key in match:\n            # Currently we treat the nodes without incoming edges as ""input"" and the nodes without\n            # outcoming edges as ""output"".\n            # A proper way to find the input nodes would be to mark the tensors arriving at NNCFNetwork\'s\n            # ""forward"" as input, then drop the marking once the first operation with an input tensor\n            # has been done; the node corresponding to this operation would be ""input"" by definition.\n            # Same with output nodes - should check the model output for TracedTensors and mark the\n            # nodes from which such tensors originated as ""output"".\n            # TODO: implement the functionality above.\n            if not list(self._nx_graph.successors(key)):\n                output_nncf_nodes.append(self._nx_node_to_nncf_node(self._nx_graph.nodes[key]))\n            if not list(self._nx_graph.predecessors(key)):\n                input_nncf_nodes.append(self._nx_node_to_nncf_node(self._nx_graph.nodes[key]))\n\n        for nx_edge in boundary:\n            from_node_key = nx_edge[0]\n            to_node_key = nx_edge[1]\n            data = nx_edge[2]\n            nncf_edge = NNCFGraphEdge(self._nx_node_to_nncf_node(self._nx_graph.nodes[from_node_key]),\n                                      self._nx_node_to_nncf_node(self._nx_graph.nodes[to_node_key]),\n                                      data[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR])\n            if from_node_key in match:\n                output_nncf_edges.append(nncf_edge)\n            elif to_node_key in match:\n                input_nncf_edges.append(nncf_edge)\n            else:\n                raise RuntimeError(""Invalid graph expression supplied!"")\n\n        return NNCFGraphPatternIO(input_nncf_edges, output_nncf_edges,\n                                  input_nncf_nodes, output_nncf_nodes)\n\n    def _get_nncf_graph_pattern_io_list(self, match: List[str]) -> NNCFGraphPatternIO:\n        in_edge_boundary, out_edge_boundary = get_edge_boundaries(match, self._nx_graph)\n        boundary = in_edge_boundary + out_edge_boundary\n        input_nncf_edges = []\n        output_nncf_edges = []\n        input_nncf_nodes = []\n        output_nncf_nodes = []\n        for key in match:\n            # Currently we treat the nodes without incoming edges as ""input"" and the nodes without\n            # outcoming edges as ""output"".\n            # A proper way to find the input nodes would be to mark the tensors arriving at NNCFNetwork\'s\n            # ""forward"" as input, then drop the marking once the first operation with an input tensor\n            # has been done; the node corresponding to this operation would be ""input"" by definition.\n            # Same with output nodes - should check the model output for TracedTensors and mark the\n            # nodes from which such tensors originated as ""output"".\n            # TODO: implement the functionality above.\n            if not list(self._nx_graph.successors(key)):\n                output_nncf_nodes.append(self._nx_node_to_nncf_node(self._nx_graph.nodes[key]))\n            if not list(self._nx_graph.predecessors(key)):\n                input_nncf_nodes.append(self._nx_node_to_nncf_node(self._nx_graph.nodes[key]))\n\n        for nx_edge in boundary:\n            from_node_key = nx_edge[0]\n            to_node_key = nx_edge[1]\n            data = nx_edge[2]\n            nncf_edge = NNCFGraphEdge(self._nx_node_to_nncf_node(self._nx_graph.nodes[from_node_key]),\n                                      self._nx_node_to_nncf_node(self._nx_graph.nodes[to_node_key]),\n                                      data[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR])\n            if from_node_key in match:\n                output_nncf_edges.append(nncf_edge)\n            elif to_node_key in match:\n                input_nncf_edges.append(nncf_edge)\n            else:\n                raise RuntimeError(""Invalid graph expression supplied!"")\n\n        return NNCFGraphPatternIO(input_nncf_edges, output_nncf_edges,\n                                  input_nncf_nodes, output_nncf_nodes)\n\n    @staticmethod\n    def _nx_node_to_nncf_node(nx_node) -> \'NNCFNode\':\n        return NNCFNode(nx_node[NNCFGraph.ID_NODE_ATTR],\n                        nx_node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR])\n\n    def find_node_in_nx_graph_by_scope(self, scope: \'Scope\') -> Optional[dict]:\n        """"""\n        Looking for node with scope == scope in networkx graph.\n        :param self: graphs to work on\n        :param scope: Scope to find in graph\n        :return: node from networkx graph for graph (or None if such scope is not found)\n        """"""\n        nodes = self._nx_graph.nodes\n        for node_key in nodes:\n            if nodes[node_key][NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR].scope_in_model == scope:\n                return nodes[node_key]\n        return None\n\n    def visualize_graph(self, path):\n        out_graph = nx.DiGraph()\n        for node_name, node in self._nx_graph.nodes.items():\n            op_exec_context = node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n            scope_str = str(op_exec_context.scope_in_model)\n            out_graph.add_node(node_name, type=op_exec_context.operator_name,\n                               id=node[NNCFGraph.ID_NODE_ATTR],\n                               scope=scope_str)\n        for u, v in self._nx_graph.edges:\n            out_graph.add_edge(u, v, label=self._nx_graph.edges[u, v][NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR])\n        try:\n            A = to_agraph(out_graph)\n            A.layout(\'dot\')\n            A.draw(path)\n        except ImportError:\n            warn(""Graphviz is not installed - no graph visualization will be done"")\n\n\nclass NNCFNodeExpression(NodeExpression):\n    def __init__(self, node_type: str = None, filter_fn=None):\n        super().__init__(node_type, filter_fn, node_type_fn=NNCFGraph.node_type_fn)\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/graph_builder.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom typing import Callable, Any, List, Optional\nfrom copy import deepcopy\n\nimport torch\n\n\nclass ModelInputInfo:\n    FILLER_TYPE_ONES = ""ones""\n    FILLER_TYPE_ZEROS = ""zeros""\n    FILLER_TYPE_RANDOM = ""random""\n    FILLER_TYPES = [FILLER_TYPE_ONES, FILLER_TYPE_ZEROS, FILLER_TYPE_RANDOM]\n\n    def __init__(self, shape: tuple, type_str: str = ""float"", keyword=None, filler=None):\n        self.shape = shape\n        self.type = self._string_to_torch_type(type_str)\n        self.keyword = keyword\n        if filler is None:\n            self.filler = self.FILLER_TYPE_ONES\n        else:\n            self.filler = filler\n            if self.filler not in self.FILLER_TYPES:\n                raise RuntimeError(""Unknown input filler type: {}"".format(filler))\n\n    def _string_to_torch_type(self, string):\n        if string == ""long"":\n            return torch.long\n        return torch.float32\n\n\ndef create_input_infos(config) -> List[ModelInputInfo]:\n    input_infos = config.get(""input_info"", [])\n    if isinstance(input_infos, dict):\n        return [ModelInputInfo(input_infos.get(""sample_size""),\n                               input_infos.get(""type""),\n                               input_infos.get(""keyword""),\n                               input_infos.get(""filler"")), ]\n    if isinstance(input_infos, list):\n        if not input_infos:\n            return [ModelInputInfo((1, 3, 224, 224))]\n        return [ModelInputInfo(info_dict.get(""sample_size""),\n                               info_dict.get(""type""),\n                               info_dict.get(""keyword""),\n                               info_dict.get(""filler"")) for info_dict in input_infos]\n    raise RuntimeError(""Invalid input_infos specified in config - should be either dict or list of dicts"")\n\n\ndef create_mock_tensor(input_info: ModelInputInfo, device: str):\n    args = {""size"": input_info.shape, ""dtype"": input_info.type, ""device"": device}\n    if input_info.filler == ModelInputInfo.FILLER_TYPE_ZEROS:\n        return torch.zeros(**args)\n    if input_info.filler == ModelInputInfo.FILLER_TYPE_ONES:\n        return torch.ones(**args)\n    if input_info.filler == ModelInputInfo.FILLER_TYPE_RANDOM:\n        return torch.rand(**args)\n    raise RuntimeError\n\n\nclass GraphBuilder:\n    def __init__(self, custom_forward_fn: Callable[[torch.nn.Module], Any]):\n        self.custom_forward_fn = custom_forward_fn\n\n    def build_graph(self, model: torch.nn.Module, context_to_use: Optional[\'TracingContext\'] = None) -> \'NNCFGraph\':\n        sd = deepcopy(model.state_dict())\n\n        from nncf.dynamic_graph.context import TracingContext\n        if context_to_use is None:\n            context_to_use = TracingContext()\n\n        with context_to_use as _ctx:\n            self.custom_forward_fn(model)\n        model.load_state_dict(sd)\n\n        if isinstance(model, PostGraphBuildActing):\n            model.post_build_graph_actions()\n        return context_to_use.graph\n\n\nclass PostGraphBuildActing:\n    def post_build_graph_actions(self):\n        pass\n\n\ndef create_dummy_forward_fn(input_infos: List[ModelInputInfo], with_input_tracing=False):\n    from nncf.dynamic_graph.patch_pytorch import nncf_model_input\n\n    def default_dummy_forward_fn(model):\n        device = next(model.parameters()).device\n        args_list = [create_mock_tensor(info, device) for info in input_infos if info.keyword is None]\n        kwargs = {info.keyword: create_mock_tensor(info, device) for info in input_infos if info.keyword is not None}\n\n        if with_input_tracing:\n            for idx, tensor in enumerate(args_list):\n                args_list[idx] = nncf_model_input(tensor)\n            for key, tensor in kwargs.items():\n                kwargs[key] = nncf_model_input(tensor)\n\n        args = tuple(args_list)\n        return model(*args, **kwargs)\n\n    return default_dummy_forward_fn\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/graph_matching.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom itertools import chain, combinations\nfrom typing import Callable, List\n\nimport numpy as np\nimport networkx as nx\n\n\ndef powerset(iterable, min_r=1, max_r=None):\n    if not isinstance(iterable, list):\n        s = list(iterable)\n    else:\n        s = iterable\n    if max_r is None:\n        max_r = len(s)\n    return chain.from_iterable(combinations(s, r) for r in range(min_r, max_r + 1))\n\n\nclass Expression:\n    def _match(self, nodes, graph):\n        return NotImplementedError\n\n    def __add__(self, other):\n        return ConcatExpression([self, other])\n\n    def __or__(self, other):\n        return AlternatingExpression([self, other])\n\n    def __and__(self, other):\n        return BranchingExpression([self, other])\n\n    def _iterate_alternatives(self, nodes):\n        return powerset(nodes, min_r=1)\n\n    def match(self, nodes, graph):\n        all_matches = []\n        for n in self._iterate_alternatives(nodes):\n            result = self._match(n, graph)\n            if not result:\n                continue\n\n            n, following = result\n            following = list(following)\n            if not isinstance(n, list):\n                n = [n]\n\n            all_matches.append((n, following))\n        if not all_matches:\n            return None, None\n        return max(all_matches, key=lambda x: len(x[0]))\n\n\nclass ConcatExpression(Expression):\n    def __init__(self, expressions):\n        self.expressions = expressions\n\n    def _match(self, nodes, graph):\n        assert len(self.expressions) > 1\n        full_match = []\n        for ex in self.expressions:\n            matches, following = ex.match(nodes, graph)\n\n            if not matches:\n                return None\n\n            full_match += matches\n\n            nodes = following\n        return full_match, following\n\n    def __add__(self, other):\n        return ConcatExpression(self.expressions + [other])\n\n\nclass AlternatingExpression(Expression):\n    def __init__(self, expressions, greedy_match=False, greedy_consume=True):\n        self.greedy_match = greedy_match\n        self.greedy_consume = greedy_consume\n        self.expressions = expressions\n\n    def _match(self, nodes, graph):\n        assert len(self.expressions) > 1\n        all_matches = []\n        for ex in self.expressions:\n            matched, following = ex.match(nodes, graph)\n            if not matched:\n                continue\n\n            if self.greedy_match:\n                return matched, following\n\n            all_matches.append((matched, following))\n\n        if self.greedy_consume:\n            if not all_matches:\n                return None\n            return max(all_matches, key=lambda x: len(x[0]))\n        return None\n\n    def __or__(self, other):\n        return AlternatingExpression(self.expressions + [other])\n\n\nclass BranchingExpression(Expression):\n    def __init__(self, expressions):\n        self.expressions = expressions\n\n    def _iterate_alternatives(self, nodes):\n        return powerset(nodes, len(self.expressions), len(self.expressions))\n\n    def _match(self, nodes, graph):\n        assert len(self.expressions) > 1\n        if len(nodes) != len(self.expressions):\n            # need to try all possible assignments\n            return None\n\n        matches = [[] for _ in range(len(self.expressions))]\n        for i, ex in enumerate(self.expressions):\n            any_matched = False\n            for node_name in nodes:\n                matched, following = ex.match([node_name], graph)\n                matches[i].append((matched, following))\n\n                if matched:\n                    any_matched = True\n\n            if not any_matched:\n                return None\n\n        return self._assign_matches(matches)\n\n    def _assign_matches(self, matches):\n        """"""Assign every expression to some match""""""\n        assignments = np.full(len(matches[0]), -1)\n        used = np.full(len(matches), False)\n\n        def _find_assignment(i):\n            if used[i]:\n                return False\n            used[i] = True\n            for j in range(len(matches[0])):\n                if not matches[i][j][0]:\n                    continue\n                if assignments[j] == -1 or _find_assignment(assignments[j]):\n                    assignments[j] = i\n                    return True\n            return False\n\n        for i in range(len(self.expressions)):\n            used[...] = False\n            _find_assignment(i)\n\n        all_matches = set()\n        all_followings = set()\n        for i in range(len(matches[0])):\n            if assignments[i] != -1:\n                match, follow = matches[assignments[i]][i]\n                all_matches.update(match)\n                all_followings.update(follow)\n\n        # assume matches dot not end in other match\n        if all_matches & all_followings:\n            return None\n        return list(all_matches), list(all_followings)\n\n    def __and__(self, other):\n        return BranchingExpression(self.expressions + [other])\n\n\nclass NodeExpression(Expression):\n    def __init__(self, node_type: str = None, filter_fn=None, node_type_fn: Callable[[dict], str] = None):\n        self.filter = filter_fn\n        self.node_type = node_type\n        if node_type_fn is None:\n            self.node_type_fn = lambda x: x[\'type\']\n        else:\n            self.node_type_fn = node_type_fn\n\n    def _iterate_alternatives(self, nodes):\n        for node in nodes:\n            yield [node]\n\n    def _match(self, nodes, graph):\n        if len(nodes) != 1:\n            return None\n\n        node_name = nodes[0]\n        node = graph.nodes[node_name]\n        node_type = self.node_type_fn(node)\n        if self.node_type == node_type:\n            if self.filter and not self.filter(node):\n                return None\n\n            following = graph.successors(node_name)\n            return node_name, following\n        return None\n\n\ndef get_edge_boundaries(match: List[str], graph: nx.DiGraph):\n    out_edge_boundary = list(nx.edge_boundary(graph, match, data=True))\n    complement = list(filter(lambda x: x not in match, graph.nodes.keys()))\n    in_edge_boundary = list(nx.edge_boundary(graph, complement, data=True))\n    return in_edge_boundary, out_edge_boundary\n\n\ndef search_all(graph: nx.DiGraph, expression: Expression) -> List[List[str]]:\n    """"""Returns list of node key lists that match the expression.""""""\n    matches = []\n    matched_nodes = set()\n    weakly_subgraphs = [graph.subgraph(c) for c in nx.weakly_connected_components(graph)]\n    for subgraph in weakly_subgraphs:\n        dfs_order = nx.topological_sort(subgraph)\n        for node in dfs_order:\n            match, _ = expression.match([node], graph)\n\n            if node in matched_nodes:\n                continue\n\n            if match:\n                for mn in match:\n                    matched_nodes.add(mn)\n                matches.append(match)\n    return matches\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/operator_metatypes.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom copy import copy\nfrom typing import List, Optional\n\nimport torch\n\nfrom nncf.dynamic_graph.patch_pytorch import CustomTraceFunction, ForwardTraceOnly, MODEL_INPUT_OP_NAME\nfrom nncf.dynamic_graph.version_agnostic_op_names import get_version_agnostic_name\nfrom nncf.hw_config_op_names import HWConfigOpName\nfrom nncf.registry import Registry\n\n\nclass OperatorMetatype:\n    """"""Base class for grouping PyTorch operators based on their semantic meaning.\n    Each derived class represents a single semantic group - for example, AddMetatype would\n    group together \'__iadd__\', \'__add__\' and \'__radd__\' operations which all define elementwise\n    tensor addition.\n    Derived classes also specify which PyTorch functions in which modules should be patched\n    and in what manner, so that the entire group of operations is visible in the internal graph\n    representation. Grouping also allows efficient application of HW specifics to compression of\n    certain operation groups.\n    """"""\n    name = """"\n\n    # Wrapping specifications for operator calls of the following kind:\n    # torch.nn.functional.conv2d\n    torch_nn_functional_patch_spec = None  # type: Optional[PatchSpec]\n\n    # Wrapping specifications for operator calls of the following kind:\n    # torch.cat\n    torch_module_patch_spec = None  # type: Optional[PatchSpec]\n\n    # Wrapping specifications for operator calls of the following kind:\n    # x = torch.Tensor(...)\n    # x1 = x.view(...)\n    torch_tensor_patch_spec = None  # type: Optional[PatchSpec]\n\n    # Names of functions registered as operators via @register_operator to be associated\n    # with this metatype\n    external_op_names = []  # type: List[str]\n\n    hw_config_names = []  # type: List[HWConfigOpName]\n\n    subtypes = []  # type: List[OperatorSubtype]\n\n    @classmethod\n    def get_all_aliases(cls: \'OperatorMetatype\') -> List[str]:\n        # TODO: disambiguate overlapping function names\n        retval = copy(cls.external_op_names)\n        if cls.torch_nn_functional_patch_spec is not None:\n            for fn_name in cls.torch_nn_functional_patch_spec.underlying_function_names:\n                retval.append(fn_name)\n        if cls.torch_module_patch_spec is not None:\n            for fn_name in cls.torch_module_patch_spec.underlying_function_names:\n                retval.append(fn_name)\n        if cls.torch_tensor_patch_spec is not None:\n            for fn_name in cls.torch_tensor_patch_spec.underlying_function_names:\n                retval.append(fn_name)\n        return retval\n\n    @classmethod\n    def determine_subtype(cls,\n                          containing_module: Optional[torch.nn.Module] = None,\n                          function_args=None,\n                          functions_kwargs=None) -> Optional[\'OperatorSubtype\']:\n        matches = []\n        for subtype in cls.subtypes:\n            if subtype.matches(containing_module,\n                               function_args,\n                               functions_kwargs):\n                matches.append(subtype)\n        assert len(matches) <= 1, ""Multiple subtypes match operator call "" \\\n                                  ""- cannot determine single subtype.""\n        if not matches:\n            return None\n\n        return matches[0]\n\n\nclass PatchSpec:\n    def __init__(self,\n                 underlying_function_names: List[str],\n                 custom_trace_fn: CustomTraceFunction = None):\n        """"""\n        :param underlying_function_names: All function names in this list will be wrapped with NNCF\n        wrappers that allow corresponding function calls to be registered in NNCF internal graph\n        representation of the PyTorch model and to be afterwards considered for compression.\n        :param custom_trace_fn: Will be called instead of the regular node search/insertion step\n        during the corresponding operator call. Useful to specify this for nodes that have no effect on compression\n        and therefore not vital to graph representation, but that should still be accounted for so that the\n        graph representation does not become disjoint.""""""\n        self.underlying_function_names = underlying_function_names\n        self.custom_trace_fn = custom_trace_fn\n\n\nclass OperatorSubtype(OperatorMetatype):\n    """"""Exact specialization of OperatorMetatype that can only be determined via operator argument\n    inspection or owning module attribute inspection, and that may have specialized compression method\n    configuration other than the one used for general operations having the type of OperatorMetatype.""""""\n\n    @classmethod\n    def matches(cls, containing_module: Optional[torch.nn.Module] = None,\n                function_args=None,\n                functions_kwargs=None) -> bool:\n        raise NotImplementedError\n\n\nclass OperatorMetatypeRegistry(Registry):\n    def __init__(self, name):\n        super().__init__(name)\n        self._op_name_to_op_meta_dict = {}\n\n    def register(self, name=None):\n        name_ = name\n        super_register = super()._register\n\n        def wrap(obj: \'OperatorMetatype\'):\n            cls_name = name_\n            if cls_name is None:\n                cls_name = obj.__name__\n            super_register(obj, cls_name)\n            op_names = obj.get_all_aliases()\n            for name in op_names:\n                name = get_version_agnostic_name(name)\n                if name not in self._op_name_to_op_meta_dict:\n                    self._op_name_to_op_meta_dict[name] = obj\n                else:\n                    assert self._op_name_to_op_meta_dict[name] == obj, \\\n                        ""Inconsistent operator metatype registry - single patched op name maps to multiple metatypes!""\n            return obj\n\n        return wrap\n\n    def get_operator_metatype_by_op_name(self, op_name: str) -> \'OperatorMetatype\':\n        return self._op_name_to_op_meta_dict[op_name]\n\n\nOPERATOR_METATYPES = OperatorMetatypeRegistry(""operator_metatypes"")\n\n\n@OPERATOR_METATYPES.register()\nclass NoopMetatype(OperatorMetatype):\n    name = ""noop""\n    external_op_names = [MODEL_INPUT_OP_NAME]\n\n\n@OPERATOR_METATYPES.register()\nclass DepthwiseConv1dSubtype(OperatorSubtype):\n    hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]\n\n    @classmethod\n    def matches(cls, containing_module: Optional[torch.nn.Module] = None,\n                function_args=None,\n                functions_kwargs=None) -> bool:\n        if containing_module.groups == containing_module.in_channels:\n            return True\n        return False\n\n\n@OPERATOR_METATYPES.register()\nclass Conv1dMetatype(OperatorMetatype):\n    name = ""conv1d""\n    hw_config_names = [HWConfigOpName.CONVOLUTION]\n    subtypes = [DepthwiseConv1dSubtype]\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass DepthwiseConv2dSubtype(OperatorSubtype):\n    hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]\n\n    @classmethod\n    def matches(cls, containing_module: Optional[torch.nn.Module] = None,\n                function_args=None,\n                functions_kwargs=None) -> bool:\n        if containing_module.groups == containing_module.in_channels:\n            return True\n        return False\n\n\n@OPERATOR_METATYPES.register()\nclass Conv2dMetatype(OperatorMetatype):\n    name = ""conv2d""\n    hw_config_names = [HWConfigOpName.CONVOLUTION]\n    torch_nn_functional_patch_spec = PatchSpec([name])\n    subtypes = [DepthwiseConv2dSubtype]\n\n\n@OPERATOR_METATYPES.register()\nclass DepthwiseConv3dSubtype(OperatorSubtype):\n    hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]\n\n    @classmethod\n    def matches(cls, containing_module: Optional[torch.nn.Module] = None,\n                function_args=None,\n                functions_kwargs=None) -> bool:\n        if containing_module.groups == containing_module.in_channels:\n            return True\n        return False\n\n\n@OPERATOR_METATYPES.register()\nclass Conv3dMetatype(OperatorMetatype):\n    name = ""conv3d""\n    hw_config_names = [HWConfigOpName.CONVOLUTION]\n    subtypes = [DepthwiseConv3dSubtype]\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass ConvTranspose2dMetatype(OperatorMetatype):\n    name = ""conv_transpose2d""\n    hw_config_names = [HWConfigOpName.CONVOLUTION]\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass ConvTranspose3dMetatype(OperatorMetatype):\n    name = ""conv_transpose3d""\n    hw_config_names = [HWConfigOpName.CONVOLUTION]\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass LinearMetatype(OperatorMetatype):\n    name = ""linear""\n    hw_config_names = [HWConfigOpName.MATMUL]\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass HardTanhMetatype(OperatorMetatype):\n    name = ""hardtanh""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass TanhMetatype(OperatorMetatype):\n    name = ""tanh""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n    torch_module_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass ELUMetatype(OperatorMetatype):\n    name = ""elu""\n    torch_nn_functional_patch_spec = PatchSpec([name, ""elu_""])\n\n\n@OPERATOR_METATYPES.register()\nclass PRELUMetatype(OperatorMetatype):\n    name = ""prelu""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass LayerNormMetatype(OperatorMetatype):\n    name = ""layer_norm""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.MVN]\n\n\n@OPERATOR_METATYPES.register()\nclass GELUMetatype(OperatorMetatype):\n    name = ""gelu""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass SigmoidMetatype(OperatorMetatype):\n    name = ""sigmoid""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n    torch_module_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass AddMetatype(OperatorMetatype):\n    name = ""add""\n    torch_tensor_patch_spec = PatchSpec([""__add__"",\n                                         ""__iadd__"",\n                                         ""__radd__""])\n    hw_config_names = [HWConfigOpName.ADD]\n\n\n@OPERATOR_METATYPES.register()\nclass SubMetatype(OperatorMetatype):\n    name = ""sub""\n    torch_tensor_patch_spec = PatchSpec([""__sub__"",\n                                         ""__isub__"",\n                                         ""__rsub__""])\n    hw_config_names = [HWConfigOpName.SUBTRACT]\n\n\n@OPERATOR_METATYPES.register()\nclass MulMetatype(OperatorMetatype):\n    name = ""mul""\n    torch_tensor_patch_spec = PatchSpec([""__mul__"",\n                                         ""__imul__"",\n                                         ""__rmul__""])\n    hw_config_names = [HWConfigOpName.MULTIPLY]\n\n\n@OPERATOR_METATYPES.register()\nclass DivMetatype(OperatorMetatype):\n    name = ""div""\n    torch_module_patch_spec = PatchSpec([name])\n    torch_tensor_patch_spec = PatchSpec([""__div__"",\n                                         ""__idiv__"",\n                                         ""__truediv__""])\n    hw_config_names = [HWConfigOpName.DIVIDE]\n\n\n@OPERATOR_METATYPES.register()\nclass ExpMetatype(OperatorMetatype):\n    name = ""exp""\n    torch_module_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass ErfMetatype(OperatorMetatype):\n    name = ""erf""\n    torch_module_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass MatMulMetatype(OperatorMetatype):\n    name = ""matmul""\n    torch_module_patch_spec = PatchSpec([name, ""bmm""])\n    torch_tensor_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass MeanMetatype(OperatorMetatype):\n    name = ""mean""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.REDUCEMEAN]\n\n\n@OPERATOR_METATYPES.register()\nclass RoundMetatype(OperatorMetatype):\n    name = ""round""\n    torch_tensor_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass DropoutMetatype(OperatorMetatype):\n    name = ""dropout""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass ThresholdMetatype(OperatorMetatype):\n    name = ""threshold""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass BatchNormMetatype(OperatorMetatype):\n    name = ""batch_norm""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass AvgPool2dMetatype(OperatorMetatype):\n    name = ""avg_pool2d""\n    hw_config_names = [HWConfigOpName.AVGPOOL]\n    torch_nn_functional_patch_spec = PatchSpec([name, ""adaptive_avg_pool2d""])\n\n\n@OPERATOR_METATYPES.register()\nclass AvgPool3dMetatype(OperatorMetatype):\n    name = ""avg_pool3d""\n    hw_config_names = [HWConfigOpName.AVGPOOL]\n    torch_nn_functional_patch_spec = PatchSpec([name, ""adaptive_avg_pool3d""])\n\n\n@OPERATOR_METATYPES.register()\nclass MaxPool2dMetatype(OperatorMetatype):\n    name = ""max_pool2d""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.MAXPOOL]\n\n\n@OPERATOR_METATYPES.register()\nclass MaxPool3dMetatype(OperatorMetatype):\n    name = ""max_pool3d""\n    torch_nn_functional_patch_spec = PatchSpec([name, ""adaptive_max_pool3d""])\n    hw_config_names = [HWConfigOpName.MAXPOOL]\n\n\n@OPERATOR_METATYPES.register()\nclass MaxUnpool3dMetatype(OperatorMetatype):\n    name = ""max_unpool3d""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass PadMetatype(OperatorMetatype):\n    name = ""pad""\n    torch_nn_functional_patch_spec = PatchSpec([name], ForwardTraceOnly())\n\n\n@OPERATOR_METATYPES.register()\nclass CatMetatype(OperatorMetatype):\n    name = ""cat""\n    torch_module_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.CONCAT]\n\n\n@OPERATOR_METATYPES.register()\nclass RELUMetatype(OperatorMetatype):\n    name = ""relu""\n    torch_module_patch_spec = PatchSpec([name, ""relu_""])\n\n\n@OPERATOR_METATYPES.register()\nclass MaxMetatype(OperatorMetatype):\n    name = ""max""\n    torch_module_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.MAXIMUM,\n                       HWConfigOpName.REDUCEMAX]\n\n\n@OPERATOR_METATYPES.register()\nclass MinMetatype(OperatorMetatype):\n    name = ""min""\n    torch_module_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.MINIMUM]\n\n\n@OPERATOR_METATYPES.register()\nclass ARangeMetatype(OperatorMetatype):\n    name = ""arange""\n    torch_module_patch_spec = PatchSpec([name], ForwardTraceOnly())\n\n\n@OPERATOR_METATYPES.register()\nclass TransposeMetatype(OperatorMetatype):\n    name = ""transpose""\n    torch_module_patch_spec = PatchSpec([name], ForwardTraceOnly())\n    torch_tensor_patch_spec = PatchSpec([name, ""permute""], ForwardTraceOnly())\n    hw_config_names = [HWConfigOpName.TRANSPOSE]\n\n\n@OPERATOR_METATYPES.register()\nclass GatherMetatype(OperatorMetatype):\n    name = ""gather""\n    torch_module_patch_spec = PatchSpec([""index_select"", ], ForwardTraceOnly())\n    torch_tensor_patch_spec = PatchSpec([""index_select"", ""__getitem__""], ForwardTraceOnly())\n\n\n@OPERATOR_METATYPES.register()\nclass ScatterMetatype(OperatorMetatype):\n    name = ""scatter""\n    torch_tensor_patch_spec = PatchSpec([""masked_fill"", ""masked_fill_""])\n\n\n@OPERATOR_METATYPES.register()\nclass ReshapeMetatype(OperatorMetatype):\n    name = ""reshape""\n    torch_module_patch_spec = PatchSpec([""squeeze"", ""flatten"", ""unsqueeze""], ForwardTraceOnly())\n    torch_tensor_patch_spec = PatchSpec([name, ""view"", ""flatten"", ""squeeze"", ""unsqueeze""], ForwardTraceOnly())\n    hw_config_names = [HWConfigOpName.RESHAPE,\n                       HWConfigOpName.SQUEEZE,\n                       HWConfigOpName.UNSQUEEZE,\n                       HWConfigOpName.FLATTEN]\n\n\n@OPERATOR_METATYPES.register()\nclass ContiguousMetatype(OperatorMetatype):\n    name = ""contiguous""\n    torch_tensor_patch_spec = PatchSpec([name], ForwardTraceOnly())\n\n\n@OPERATOR_METATYPES.register()\nclass SplitMetatype(OperatorMetatype):\n    name = ""split""\n    torch_tensor_patch_spec = PatchSpec([""chunk""], ForwardTraceOnly())\n    hw_config_names = [HWConfigOpName.SPLIT]\n\n\n@OPERATOR_METATYPES.register()\nclass ExpandMetatype(OperatorMetatype):\n    name = ""expand""\n    torch_tensor_patch_spec = PatchSpec([name], ForwardTraceOnly())\n\n\n# Non-quantizable ops\n@OPERATOR_METATYPES.register()\nclass EmbeddingMetatype(OperatorMetatype):\n    name = ""embedding""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass SoftmaxMetatype(OperatorMetatype):\n    name = ""softmax""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n\n\n@OPERATOR_METATYPES.register()\nclass LessMetatype(OperatorMetatype):\n    name = ""__lt__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.LESS]\n\n\n@OPERATOR_METATYPES.register()\nclass LessEqualMetatype(OperatorMetatype):\n    name = ""__le__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.LESSEQUAL]\n\n\n@OPERATOR_METATYPES.register()\nclass GreaterMetatype(OperatorMetatype):\n    name = ""__gt__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.GREATER]\n\n\n@OPERATOR_METATYPES.register()\nclass GreaterEqualMetatype(OperatorMetatype):\n    name = ""__ge__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.GREATEREQUAL]\n\n\n@OPERATOR_METATYPES.register()\nclass ModMetatype(OperatorMetatype):\n    name = ""__mod__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.FLOORMOD]\n\n\n@OPERATOR_METATYPES.register()\nclass EqualsMetatype(OperatorMetatype):\n    name = ""__eq__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.EQUAL]\n\n\n@OPERATOR_METATYPES.register()\nclass NotEqualMetatype(OperatorMetatype):\n    name = ""__ne__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.NOTEQUAL]\n\n\n@OPERATOR_METATYPES.register()\nclass LogicalOrMetatype(OperatorMetatype):\n    name = ""__or__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.LOGICALOR]\n\n\n@OPERATOR_METATYPES.register()\nclass LogicalXorMetatype(OperatorMetatype):\n    name = ""__xor__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.LOGICALXOR]\n\n\n@OPERATOR_METATYPES.register()\nclass LogicalAndMetatype(OperatorMetatype):\n    name = ""__and__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.LOGICALAND]\n\n\n@OPERATOR_METATYPES.register()\nclass LogicalNotMetatype(OperatorMetatype):\n    name = ""logical_not_""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.LOGICALNOT]\n\n\n@OPERATOR_METATYPES.register()\nclass PowerMetatype(OperatorMetatype):\n    name = ""__pow__""\n    torch_tensor_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.POWER]\n\n\n@OPERATOR_METATYPES.register()\nclass InterpolateMetatype(OperatorMetatype):\n    name = ""interpolate""\n    torch_nn_functional_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.INTERPOLATE]\n\n\n@OPERATOR_METATYPES.register()\nclass RepeatMetatype(OperatorMetatype):\n    name = ""repeat_interleave""\n    torch_module_patch_spec = PatchSpec([name])\n    hw_config_names = [HWConfigOpName.TILE]\n\n\n@OPERATOR_METATYPES.register()\nclass CloneMetatype(OperatorMetatype):\n    name = ""clone""\n    torch_tensor_patch_spec = PatchSpec([name], ForwardTraceOnly())\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/patch_pytorch.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom typing import Callable, List\n\nimport warnings\nfrom torch import Tensor\nfrom torch.nn import DataParallel\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom nncf.dynamic_graph.trace_tensor import TracedTensor, flatten_args\nfrom nncf.dynamic_graph.wrappers import wrap_operator, wrap_module_call, ignore_scope\n\n\nclass CustomTraceFunction:\n    def __call__(self, operator: Callable, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass ForwardTraceOnly(CustomTraceFunction):\n    def __call__(self, operator: Callable, *args, **kwargs):\n        """""" This wrapper override will result in the operator not being added to graph,\n        but the result will still have TracedTensors with parent IDs left the same as in input.\n        Useful for operators which are not likely to be present in patterns considered for\n        compression, but still have to be accounted for so that the NNCF internal graph representation\n        does not become disjoint. """"""\n\n        result = operator(*args, **kwargs)\n\n        fargs = flatten_args(args, kwargs)\n        input_traced_tensor_indices = [i for i in range(len(fargs)) if isinstance(fargs[i], TracedTensor)]\n\n        if isinstance(result, (list, tuple)):\n            output_tensors_to_be_traced_indices = [i for i in range(len(result)) if\n                                                   isinstance(result[i], Tensor)]\n\n            was_tuple = isinstance(result, tuple)\n            result = list(result)\n            if len(input_traced_tensor_indices) == 1:\n                # Broadcast one and the same creator ID of input to all outputs\n                for out_idx in output_tensors_to_be_traced_indices:\n                    result[out_idx] = TracedTensor.from_torch_tensor(result[out_idx],\n                                                                     fargs[input_traced_tensor_indices[\n                                                                         0]].tensor_meta)\n            elif len(input_traced_tensor_indices) != len(output_tensors_to_be_traced_indices):\n                raise RuntimeError(""Unable to forward trace through operator {} - ""\n                                   ""input and output tensor count mismatch!"".format(operator.__name__))\n            else:\n                # Assume that output tensor order corresponds to input tensor order\n                for in_idx, out_idx in zip(input_traced_tensor_indices, output_tensors_to_be_traced_indices):\n                    result[out_idx] = TracedTensor.from_torch_tensor(result[out_idx],\n                                                                     fargs[in_idx].tensor_meta)\n            if was_tuple:\n                result = tuple(result)\n        elif len(input_traced_tensor_indices) > 1:\n            raise RuntimeError(""Unable to forward trace through operator {} - ""\n                               ""input and output tensor count mismatch!"".format(operator.__name__))\n        elif input_traced_tensor_indices:\n            return TracedTensor.from_torch_tensor(result,\n                                                  fargs[input_traced_tensor_indices[0]].tensor_meta)\n        # No traced tensors in input, return a usual torch.Tensor as well\n        return result\n\n\nclass PatchedOperatorInfo:\n    def __init__(self, name: str, custom_trace_fn: CustomTraceFunction = None):\n        """"""custom_trace_fn will be called instead of the regular node search/insertion step\n        during the corresponding operator call""""""\n        self.name = name\n        self.custom_trace_fn = custom_trace_fn\n\n\ndef register_operator(name=None):\n    def wrap(operator):\n        op_name = name\n        if op_name is None:\n            op_name = operator.__name__\n        return wrap_operator(operator, PatchedOperatorInfo(op_name))\n\n    return wrap\n\n    # TODO: Use same wrapper for model.forward() calls\n\n\ndef torch_jit_script_wrapper(*args, **kwargs):\n    # Torch JIT cannot work with NNCF-modified operators,\n    # so at each import of a @torch.jit.script-decorated\n    # function we need to un-patch the torch operators\n    unpatch_torch_operators()\n\n    # This import statement is required, otherwise we get a\n    # ""RuntimeError: undefined value torch"" inside the real torch.jit.script\n\n    # pylint:disable=unused-import,redefined-outer-name,reimported\n\n    retval = _ORIG_JIT_SCRIPT(*args, **kwargs)\n    patch_torch_operators()\n    return retval\n\n\ndef get_arg_positions_to_quantize(op_name: str):\n    from nncf.dynamic_graph.function_input_quantization import FUNCTIONS_TO_QUANTIZE\n    return next((x.positions_of_args_to_quantize for x in FUNCTIONS_TO_QUANTIZE\n                 if x.name == op_name), None)\n\n\nclass OriginalOpInfo:\n    def __init__(self, name: str, namespace, op):\n        self.name = name\n        self.namespace = namespace\n        self.op = op\n\n\nORIGINAL_OPERATORS = []  # type: List[OriginalOpInfo]\n_JIT_ALREADY_WRAPPED = False\n_OPERATORS_ALREADY_WRAPPED = False\n_ORIG_JIT_SCRIPT = None\n\n\ndef patch_torch_jit_script():\n    import torch\n    orig = getattr(torch.jit, ""script"")\n    ORIGINAL_OPERATORS.append(OriginalOpInfo(""script"", torch.jit, orig))\n    global _ORIG_JIT_SCRIPT\n    _ORIG_JIT_SCRIPT = orig\n    setattr(torch.jit, ""script"", torch_jit_script_wrapper)\n\n\ndef patch_namespace_opname(namespace, patched_op_info: PatchedOperatorInfo):\n    name = patched_op_info.name\n    if hasattr(namespace, name):\n        orig = getattr(namespace, name)\n        ORIGINAL_OPERATORS.append(OriginalOpInfo(name, namespace, orig))\n        setattr(namespace, name, wrap_operator(orig, patched_op_info))\n    else:\n        warnings.warn(""Not patching {} since it is missing in this version of PyTorch""\n                      .format(name))\n\n\ndef patch_namespace_by_patchspec(namespace, patchspec: \'PatchSpec\'):\n    for op_name in patchspec.underlying_function_names:\n        patched_op_info = PatchedOperatorInfo(op_name, patchspec.custom_trace_fn)\n        patch_namespace_opname(namespace, patched_op_info)\n\n\ndef patch_torch_operators():\n    # Only patch torch.jit.script during first patch_torch_operators call\n    global _JIT_ALREADY_WRAPPED\n    if not _JIT_ALREADY_WRAPPED:\n        patch_torch_jit_script()\n        _JIT_ALREADY_WRAPPED = True\n\n    # Do not patch operators twice as well\n    global _OPERATORS_ALREADY_WRAPPED\n    if _OPERATORS_ALREADY_WRAPPED:\n        return\n    _OPERATORS_ALREADY_WRAPPED = True\n\n    # patch operators\n    import torch.nn.functional as F\n    import torch\n    from nncf.dynamic_graph.operator_metatypes import OPERATOR_METATYPES\n    for op_meta_class in OPERATOR_METATYPES.registry_dict.values():  # type: OperatorMetatype\n        if op_meta_class.torch_nn_functional_patch_spec is not None:\n            ps = op_meta_class.torch_nn_functional_patch_spec\n            patch_namespace_by_patchspec(F, ps)\n        if op_meta_class.torch_module_patch_spec is not None:\n            ps = op_meta_class.torch_module_patch_spec\n            patch_namespace_by_patchspec(torch, ps)\n        if op_meta_class.torch_tensor_patch_spec is not None:\n            ps = op_meta_class.torch_tensor_patch_spec\n            patch_namespace_by_patchspec(TracedTensor, ps)\n\n    ORIGINAL_OPERATORS.append(OriginalOpInfo(""__call__"", torch.nn.Module, torch.nn.Module.__call__))\n    torch.nn.Module.__call__ = wrap_module_call(torch.nn.Module.__call__)\n    ignore_scope(DataParallel)\n    ignore_scope(DistributedDataParallel)\n\n\ndef unpatch_torch_operators():\n    global _OPERATORS_ALREADY_WRAPPED\n    if not _OPERATORS_ALREADY_WRAPPED:\n        return\n    _OPERATORS_ALREADY_WRAPPED = False\n\n    for orig_op_info in ORIGINAL_OPERATORS:\n        setattr(orig_op_info.namespace, orig_op_info.name, orig_op_info.op)\n\n\n@register_operator()\ndef nncf_model_input(tensor: \'torch.Tensor\'):\n    return tensor\n\n\n# Access via _original op because by this moment the nncf_model_input name will already be wrapped by wrap_operator\n# and its __name__ attribute changed correspondingly.\n# pylint:disable=protected-access\nMODEL_INPUT_OP_NAME = nncf_model_input._original_op.__name__\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/patterns.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom nncf.dynamic_graph.graph import NNCFNodeExpression as N\nfrom nncf.dynamic_graph.version_agnostic_op_names import VersionAgnosticNames\n\nLINEAR_OPS = N(\'linear\') | N(\'conv2d\') | N(\'conv_transpose2d\') | N(\'conv3d\') | N(\'conv_transpose3d\')\n\nRELU = N(VersionAgnosticNames.RELU) | N(\'hardtanh\')\n\nBN = N(\'batch_norm\') | N(\'batch_norm3d\')\n\nANY_BN_RELU_COMBO = BN + RELU | RELU + BN | BN | RELU\n\nPOOLING = N(\'adaptive_avg_pool2d\') | N(\'adaptive_avg_pool3d\') | N(\'avg_pool2d\') | N(\'avg_pool3d\')\n\nNON_RELU_ACTIVATIONS = N(\'elu\') | N(\'elu_\') | N(\'prelu\') | N(\'sigmoid\')\n\nSINGLE_OPS = NON_RELU_ACTIVATIONS | POOLING | N(\'mean\') | N(\'layer_norm\')\n\nARITHMETIC = N(\'__iadd__\') | N(\'__add__\') | N(\'__mul__\') | N(\'__rmul__\')\n\nELTWISE_UNIFORM_OPS = BN | RELU | NON_RELU_ACTIVATIONS\n\nMATMUL = N(\'matmul\') | N(\'bmm\')\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/trace_tensor.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom typing import Iterable\n\nimport numpy as np\nimport torch\n\n\nclass TensorMeta:\n    @staticmethod\n    def default_comparator(lhs: \'TensorMeta\', rhs: \'TensorMeta\'):\n        return lhs.index == rhs.index and lhs.creator_id == rhs.creator_id and lhs.shape[1:] == rhs.shape[1:]\n\n    def __init__(self, creator_id, index, shape):\n        self.creator_id = creator_id\n        self.index = index\n        self.shape = tuple(int(dim) for dim in shape)  # Handle cases when shape is a tuple of Tensors\n\n    def __eq__(self, other):\n        if not isinstance(other, TensorMeta):\n            return False\n        return self.default_comparator(self, other)\n\n    def __hash__(self):\n        return hash((self.creator_id, self.index, self.shape))\n\n    def __str__(self):\n        return ""C{}_I{}_"".format(self.creator_id, self.index) + ""S"" + ""x"".join([str(s) for s in self.shape])\n\n\nclass TracedTensor(torch.Tensor):\n    # pylint: disable=abstract-method\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.tensor_meta = None\n\n    @staticmethod\n    def from_torch_tensor(tensor, tensor_meta: TensorMeta):\n        tensor.tensor_meta = tensor_meta\n        tensor.__class__ = TracedTensor\n        return tensor\n\n\ndef is_iterable(item):\n    non_iterable_types = (str, bytes, bytearray, torch.Tensor, np.ndarray)\n    return isinstance(item, Iterable) and not isinstance(item, non_iterable_types)\n\n\ndef flatten(items):\n    it = items.items() if hasattr(items, \'items\') else iter(items)\n    for item in it:\n        if is_iterable(item):\n            for i in flatten(item):\n                yield i\n        else:\n            yield item\n\n\ndef flatten_args(args, kwargs):\n    return list(flatten(args)) + list(flatten(kwargs))\n\n\ndef trace_tensors(operator_output, node: \'NNCFNode\'):\n    if isinstance(operator_output, (list, tuple)):\n        output_ = []\n        for i, x in enumerate(operator_output):\n            meta = TensorMeta(node.node_id, i, x.shape)\n            output_.append(TracedTensor.from_torch_tensor(x, meta))\n        return operator_output.__class__(output_)\n    if isinstance(operator_output, torch.Tensor):\n        meta = TensorMeta(node.node_id, 0, operator_output.shape)\n        return TracedTensor.from_torch_tensor(operator_output, meta)\n    raise ValueError(""Unknown return type. Can not trace function call"")\n\n\ndef make_input_infos(inputs):\n    input_infos = []\n    for i, node_input in enumerate(inputs):\n        if isinstance(node_input, TracedTensor):\n            input_infos.append(node_input.tensor_meta)\n        elif isinstance(node_input, torch.Tensor) and not isinstance(node_input, TracedTensor):\n            meta = TensorMeta(None, i, node_input.shape)\n            input_infos.append(meta)\n        else:\n            input_infos.append(None)\n    return input_infos\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/transform_graph.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom functools import partial\nfrom torch import nn\nfrom typing import List\n\nfrom nncf.layers import NNCF_MODULES_DICT, NNCF_MODULES\nfrom nncf.utils import  in_scope_list\nfrom nncf.dynamic_graph.context import Scope, ScopeElement\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\ndef is_nncf_module(module):\n    for nncf_module_name in NNCF_MODULES:\n        if module.__class__.__name__ == nncf_module_name:\n            return True\n    return False\n\n\ndef replace_module_by_nncf_module(module: nn.Module):\n    for nncf_module_type, module_type in NNCF_MODULES_DICT.items():\n        if module.__class__.__name__ == module_type.__name__:\n            nncf_module = module\n            if not module.__class__.__name__ == nncf_module_type.__name__:\n                nncf_module = nncf_module_type.from_module(module)\n            return nncf_module\n    return module\n\n\ndef replace_modules_by_nncf_modules(model: nn.Module, ignored_scopes=None,\n                                    target_scopes=None) -> (nn.Module, List[Scope]):\n    replace_fn = partial(replace_module_by_nncf_module)\n    affected_scopes = []  # type: List\n    return replace_modules(model, replace_fn, affected_scopes,\n                           ignored_scopes=ignored_scopes, target_scopes=target_scopes)\n\n\ndef replace_modules(model: nn.Module, replace_fn, affected_scopes, ignored_scopes=None, target_scopes=None, memo=None,\n                    current_scope=None):\n    if memo is None:\n        memo = set()\n        current_scope = Scope()\n        current_scope.push(ScopeElement(model.__class__.__name__))\n\n    if model in memo:\n        return model, affected_scopes\n\n    memo.add(model)\n    for name, module in model.named_children():\n        if module is None:\n            continue\n\n        child_scope_element = ScopeElement(module.__class__.__name__, name)\n        child_scope = current_scope.copy()\n        child_scope.push(child_scope_element)\n        replaced_module = replace_fn(module)\n\n        if replaced_module is not None:\n            replaced_scope_element = ScopeElement(replaced_module.__class__.__name__, name)\n            replaced_scope = current_scope.copy()\n            replaced_scope.push(replaced_scope_element)\n            if module is not replaced_module:\n                if in_scope_list(str(child_scope), ignored_scopes):\n                    nncf_logger.info(""Ignored wrapping modules in scope: {}"".format(child_scope))\n                    continue\n\n                if target_scopes is None or in_scope_list(str(child_scope), target_scopes):\n                    nncf_logger.info(""Wrapping module {} by {}"".format(str(child_scope),\n                                                                       str(replaced_scope)))\n                    if isinstance(model, nn.Sequential):\n                        # pylint: disable=protected-access\n                        model._modules[name] = replaced_module\n                    else:\n                        setattr(model, name, replaced_module)\n                    affected_scopes.append(replaced_scope)\n            elif is_nncf_module(replaced_module):\n                # Got an NNCF-wrapped module from previous compression stage, track its scope as well\n                affected_scopes.append(replaced_scope)\n        _, affected_scopes = replace_modules(module, replace_fn, affected_scopes, ignored_scopes, target_scopes,\n                                             memo, child_scope)\n    return model, affected_scopes\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/utils.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom graphviz import Digraph\n\ngraph_theme = {\n    ""background_color"": ""#FFFFFF"",\n    ""fill_color"": ""#E8E8E8"",\n    ""outline_color"": ""#000000"",\n    ""font_color"": ""#000000"",\n    ""font_name"": ""Times"",\n    ""font_size"": ""10"",\n    ""margin"": ""0,0"",\n    ""padding"": ""1.0,1.0"",\n}\n\n\ndef draw_dot(context):\n    graph = context.graph\n    dot = Digraph()\n\n    dot.attr(""graph"",\n             bgcolor=graph_theme[""background_color""],\n             color=graph_theme[""outline_color""],\n             fontsize=graph_theme[""font_size""],\n             fontcolor=graph_theme[""font_color""],\n             fontname=graph_theme[""font_name""],\n             margin=graph_theme[""margin""],\n             # rankdir=""LR"",\n             pad=graph_theme[""padding""])\n    dot.attr(""node"", shape=""box"",\n             style=""filled"", margin=""0,0"",\n             fillcolor=graph_theme[""fill_color""],\n             color=graph_theme[""outline_color""],\n             fontsize=graph_theme[""font_size""],\n             fontcolor=graph_theme[""font_color""],\n             fontname=graph_theme[""font_name""])\n    dot.attr(""edge"", style=""solid"",\n             color=graph_theme[""outline_color""],\n             fontsize=graph_theme[""font_size""],\n             fontcolor=graph_theme[""font_color""],\n             fontname=graph_theme[""font_name""])\n\n    for node in graph.nodes:\n        dot.node(graph.nodes[node][\'name\'])\n        for child in graph.successors(node):\n            dot.edge(node, child)\n    return dot\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/version_agnostic_op_names.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nclass VersionAgnosticNames:\n    RELU = ""RELU""\n\n\nclass TorchOpInfo:\n    def __init__(self, torch_version: str, version_agnostic_name: str):\n        self.torch_version = torch_version\n        self.version_agnostic_name = version_agnostic_name\n\n\nOPERATOR_NAME_LOOKUP_TABLE = {\n    ""relu_""     : TorchOpInfo(""1.1.0"", VersionAgnosticNames.RELU),\n    ""relu""      : TorchOpInfo(""unknown"", VersionAgnosticNames.RELU)\n}\n\n\ndef get_version_agnostic_name(version_specific_name: str):\n    if version_specific_name not in OPERATOR_NAME_LOOKUP_TABLE:\n        return version_specific_name\n\n    return OPERATOR_NAME_LOOKUP_TABLE[version_specific_name].version_agnostic_name\n'"
pytorch_toolkit/nncf/nncf/dynamic_graph/wrappers.py,0,"b'import warnings\n\nfrom torch.nn import DataParallel\n\nfrom nncf.debug import is_debug\nfrom nncf.dynamic_graph.context import get_current_context, OperatorInput\nfrom nncf.dynamic_graph.trace_tensor import flatten_args, trace_tensors\nfrom nncf.layers import ITERATION_MODULES\n\n_IGNORED_SCOPES = []\n\ndef _warn_data_parallel():\n    if getattr(_warn_data_parallel, \'warned_once\', False):\n        return\n    _warn_data_parallel.warned_once = True\n    warnings.warn(""You are using DataParallel, which may cause significant performance issues with dynamic graph ""\n                  ""building. Consider using distributed training (DistributedDataParallel) instead"")\n\ndef ignore_scope(cls):\n    if cls not in _IGNORED_SCOPES:\n        _IGNORED_SCOPES.append(cls)\n    return cls\n\n\ndef wrap_operator(operator, operator_info: \'PatchedOperatorInfo\'):\n    # do not wrap function twice\n    _orig_op = getattr(operator, \'_original_op\', None)\n    if _orig_op is not None:\n        raise Exception(""Operator: {} is already wrapped"".format(_orig_op.__name__))\n\n    def wrapped(*args, **kwargs):\n        ctx = get_current_context()\n        if not ctx or getattr(ctx, \'in_operator\', False) or not ctx.is_tracing:\n            op1 = operator(*args, **kwargs)\n            return op1\n\n        ctx.in_operator = True\n\n        if operator_info.custom_trace_fn is not None:\n            result = operator_info.custom_trace_fn(operator, *args, **kwargs)\n        else:\n            ia_op_exec_context = ctx.get_caller_context(operator_info.name)\n            ctx.register_operator_call(ia_op_exec_context.operator_name, ia_op_exec_context.scope_in_model)\n\n            op_input = OperatorInput(list(args), kwargs)\n            processed_input = ctx.execute_pre_hooks(ia_op_exec_context, op_input)\n            args = tuple(processed_input.op_args)\n            kwargs = processed_input.op_kwargs\n            fargs = flatten_args(args, kwargs)\n\n            node = ctx.find_operator_node(fargs, ia_op_exec_context)\n            if is_debug():\n                ctx.register_node_call(ctx.graph.get_node_key_by_id(node.node_id))\n\n            result = operator(*args, **kwargs)\n\n            result = trace_tensors(result, node)\n            result = ctx.execute_post_hooks(ia_op_exec_context, result)\n\n        ctx.in_operator = False\n        return result\n\n    # pylint: disable=protected-access\n    wrapped._original_op = operator\n    return wrapped\n\n\ndef wrap_module_call(module_call):\n    def wrapped(self, *args, **kwargs):\n        ctx = get_current_context()\n        if not ctx or self.__class__ in _IGNORED_SCOPES:\n            if isinstance(self, DataParallel):\n                _warn_data_parallel()\n            return module_call(self, *args, **kwargs)\n        ctx.push_scope(self)\n        retval = module_call(self, *args, **kwargs)\n        if type(self).__name__ in ITERATION_MODULES.registry_dict.keys():\n            ctx.reset_operator_call_count_in_scope(ctx.scope)\n        ctx.pop_scope()\n        return retval\n\n    return wrapped\n'"
pytorch_toolkit/nncf/nncf/pruning/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/pruning/base_algo.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom typing import List, Dict\n\nfrom functools import partial, update_wrapper\nfrom texttable import Texttable\nfrom torch import nn\n\nfrom nncf.compression_method_api import CompressionAlgorithmBuilder, \\\n    CompressionAlgorithmController\nfrom nncf.dynamic_graph.graph import InputAgnosticOperationExecutionContext\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.nncf_network import NNCFNetwork, InsertionPoint, InsertionCommand, InsertionType, OperationPriority\nfrom nncf.pruning.filter_pruning.layers import apply_filter_binary_mask\nfrom nncf.pruning.utils import get_bn_for_module_scope, \\\n    get_first_pruned_modules, get_last_pruned_modules, is_conv_with_downsampling\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\n\nclass PrunedModuleInfo:\n    BN_MODULE_NAME = \'bn_module\'\n\n    def __init__(self, module_name: str, module: nn.Module, operand, related_modules: Dict):\n        self.module_name = module_name\n        self.module = module\n        self.operand = operand\n        self.related_modules = related_modules\n\n\nclass BasePruningAlgoBuilder(CompressionAlgorithmBuilder):\n    def __init__(self, config):\n        super().__init__(config)\n        params = config.get(\'params\', {})\n        self._params = params\n\n        self.prune_first = params.get(\'prune_first_conv\', False)\n        self.prune_last = params.get(\'prune_last_conv\', False)\n        self.prune_batch_norms = params.get(\'prune_batch_norms\', False)\n        self.prune_downsample_convs = params.get(\'prune_downsample_convs\', False)\n\n        self._pruned_module_info = []\n\n    def apply_to(self, target_model: NNCFNetwork) -> NNCFNetwork:\n        insertion_commands = self._prune_weights(target_model)\n        for command in insertion_commands:\n            target_model.register_insertion_command(command)\n        target_model.register_algorithm(self)\n        return target_model\n\n    def _prune_weights(self, target_model: NNCFNetwork):\n        device = next(target_model.parameters()).device\n        modules_to_prune = target_model.get_nncf_modules()\n        insertion_commands = []\n\n        input_non_pruned_modules = get_first_pruned_modules(target_model,\n                                                            self.get_types_of_pruned_modules() + [\'linear\'])\n        output_non_pruned_modules = get_last_pruned_modules(target_model,\n                                                            self.get_types_of_pruned_modules() + [\'linear\'])\n\n        for module_scope, module in modules_to_prune.items():\n            # Check that we need to prune weights in this op\n            if not self._is_pruned_module(module):\n                continue\n\n            module_scope_str = str(module_scope)\n            if not self._should_consider_scope(module_scope_str):\n                nncf_logger.info(""Ignored adding Weight Pruner in scope: {}"".format(module_scope_str))\n                continue\n\n            if not self.prune_first and module in input_non_pruned_modules:\n                nncf_logger.info(""Ignored adding Weight Pruner in scope: {} because""\n                                 "" this scope is one of the first convolutions"".format(module_scope_str))\n                continue\n            if not self.prune_last and module in output_non_pruned_modules:\n                nncf_logger.info(""Ignored adding Weight Pruner in scope: {} because""\n                                 "" this scope is one of the last convolutions"".format(module_scope_str))\n                continue\n\n            if not self.prune_downsample_convs and is_conv_with_downsampling(module):\n                nncf_logger.info(""Ignored adding Weight Pruner in scope: {} because""\n                                 "" this scope is convolution with downsample"".format(module_scope_str))\n                continue\n\n            nncf_logger.info(""Adding Weight Pruner in scope: {}"".format(module_scope_str))\n            operation = self.create_weight_pruning_operation(module)\n            hook = UpdateWeight(operation).to(device)\n            insertion_commands.append(\n                InsertionCommand(\n                    InsertionPoint(\n                        InputAgnosticOperationExecutionContext("""", module_scope, 0),\n                        InsertionType.NNCF_MODULE_PRE_OP\n                    ),\n                    hook,\n                    OperationPriority.PRUNING_PRIORITY\n                )\n            )\n\n            related_modules = {}\n            if self.prune_batch_norms:\n                related_modules[\'bn_module\'] = get_bn_for_module_scope(target_model, module_scope)\n\n            self._pruned_module_info.append(\n                PrunedModuleInfo(module_scope_str, module, hook.operand, related_modules))\n\n        return insertion_commands\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n        return BasePruningAlgoController(target_model, self._pruned_module_info, self._params)\n\n    def create_weight_pruning_operation(self, module):\n        raise NotImplementedError\n\n    def _is_pruned_module(self, module: nn.Module):\n        """"""\n        Return whether this module should be pruned or not.\n        """"""\n        raise NotImplementedError\n\n    def get_types_of_pruned_modules(self):\n        """"""\n        Returns list of operation types that should be pruned.\n        """"""\n        raise NotImplementedError\n\n\nclass BasePruningAlgoController(CompressionAlgorithmController):\n    def __init__(self, target_model: NNCFNetwork,\n                 pruned_module_info: List[PrunedModuleInfo], params: dict):\n        super().__init__(target_model)\n        self.pruned_module_info = pruned_module_info\n        self.prune_first = params.get(\'prune_first_conv\', False)\n        self.prune_last = params.get(\'prune_last_conv\', False)\n        self.prune_batch_norms = params.get(\'prune_batch_norms\', False)\n        self.zero_grad = params.get(\'zero_grad\', True)\n        self._hooks = []\n\n    def freeze(self):\n        raise NotImplementedError\n\n    def set_pruning_rate(self, pruning_rate):\n        raise NotImplementedError\n\n    def zero_grads_for_pruned_modules(self):\n        """"""\n        This function register hook that will set gradients for pruned filters to zero.\n        """"""\n        self._clean_hooks()\n\n        def hook(grad, mask):\n            mask = mask.to(grad.device)\n            return apply_filter_binary_mask(mask, grad)\n\n        for minfo in self.pruned_module_info[:1]:\n            mask = minfo.operand.binary_filter_pruning_mask\n            weight = minfo.module.weight\n            partial_hook = update_wrapper(partial(hook, mask=mask), hook)\n            self._hooks.append(weight.register_hook(partial_hook))\n            if minfo.module.bias is not None:\n                bias = minfo.module.bias\n                partial_hook = update_wrapper(partial(hook, mask=mask), hook)\n                self._hooks.append(bias.register_hook(partial_hook))\n\n    def _clean_hooks(self):\n        for h in self._hooks:\n            h.remove()\n        self._hooks = []\n\n    def _get_mask(self, minfo: PrunedModuleInfo):\n        """"""\n        Returns pruning mask for minfo.module.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def pruning_rate_for_weight(minfo: PrunedModuleInfo):\n        """"""\n        Calculates sparsity rate for all weight elements.\n        """"""\n        weight = minfo.module.weight\n        pruning_rate = 1 - weight.nonzero().size(0) / weight.view(-1).size(0)\n        return pruning_rate\n\n    @staticmethod\n    def pruning_rate_for_filters(minfo: PrunedModuleInfo):\n        """"""\n        Calculates sparsity rate for weight filter-wise.\n        """"""\n        weight = minfo.module.weight\n        filters_sum = weight.view(weight.size(0), -1).sum(axis=1)\n        pruning_rate = 1 - len(filters_sum.nonzero()) / filters_sum.size(0)\n        return pruning_rate\n\n    def pruning_rate_for_mask(self, minfo: PrunedModuleInfo):\n        mask = self._get_mask(minfo)\n        pruning_rate = mask.nonzero().size(0) / max(mask.view(-1).size(0), 1)\n        return pruning_rate\n\n    def mask_shape(self, minfo: PrunedModuleInfo):\n        mask = self._get_mask(minfo)\n        return mask.shape\n\n    def statistics(self):\n        stats = super().statistics()\n        table = Texttable()\n        header = [""Name"", ""Weight\'s Shape"", ""Mask Shape"", ""Mask zero %"", ""PR"", ""Filter PR""]\n        data = [header]\n\n        for minfo in self.pruned_module_info:\n            drow = {h: 0 for h in header}\n            drow[""Name""] = minfo.module_name\n            drow[""Weight\'s Shape""] = list(minfo.module.weight.size())\n\n            drow[""Mask Shape""] = list(self.mask_shape(minfo))\n\n            drow[""Mask zero %""] = 1.0 - self.pruning_rate_for_mask(minfo)\n\n            drow[""PR""] = self.pruning_rate_for_weight(minfo)\n\n            drow[""Filter PR""] = self.pruning_rate_for_filters(minfo)\n\n            row = [drow[h] for h in header]\n            data.append(row)\n        table.add_rows(data)\n\n        stats[""pruning_statistic_by_module""] = table\n        return self.add_algo_specific_stats(stats)\n\n    @staticmethod\n    def add_algo_specific_stats(stats):\n        return stats\n'"
pytorch_toolkit/nncf/nncf/pruning/schedulers.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport numpy as np\nimport scipy.optimize\nfrom nncf.compression_method_api import CompressionScheduler\nfrom nncf.config import Config\nfrom nncf.registry import Registry\n\nPRUNING_SCHEDULERS = Registry(""pruning_schedulers"")\n\n\nclass PruningScheduler(CompressionScheduler):\n    def __init__(self, pruning_algo, params: Config = None):\n        super().__init__()\n        if params is None:\n            self._params = Config()\n        else:\n            self._params = params\n\n        self.algo = pruning_algo\n\n        # Number of initial steps of training before pruning\n        self.num_init_steps = self._params.get(\'num_init_steps\', 0)\n        self.pruning_steps = self._params.get(\'pruning_steps\', 100)\n\n        # Pruning rates\n        self.initial_pruning = self._params.get(\'pruning_init\', 0)\n        self.pruning_target = self._params.get(\'pruning_target\', 0.5)\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self._set_pruning_level()\n\n    def epoch_step(self, epoch=None):\n        super().epoch_step(epoch)\n        self._set_pruning_level()\n\n    def _set_pruning_level(self):\n        self.algo.set_pruning_rate(self.current_pruning_level)\n\n        if self.last_epoch >= (self.pruning_steps + self.num_init_steps):\n            self.algo.freeze()\n\n    def _calc_density_level(self):\n        raise NotImplementedError\n\n    @property\n    def current_pruning_level(self):\n        if self.last_epoch >= self.num_init_steps:\n            return 1 - self._calc_density_level()\n        return 0\n\n\n@PRUNING_SCHEDULERS.register(""baseline"")\nclass BaselinePruningScheduler(PruningScheduler):\n    """"""\n    Baseline scheduler that setting max pruning rate after num_init_steps epoch\n    and freeze algorithm after it.\n    """"""\n    def __init__(self, pruning_algo, config=None):\n        super().__init__(pruning_algo, config)\n        self._set_pruning_level()\n\n    def _calc_density_level(self):\n        min_density = 1 - self.pruning_target\n        return min_density\n\n    def _set_pruning_level(self):\n        self.algo.set_pruning_rate(self.current_pruning_level)\n        if self.last_epoch >= self.num_init_steps:\n            self.algo.freeze()\n\n\n@PRUNING_SCHEDULERS.register(""exponential"")\nclass ExponentialPruningScheduler(PruningScheduler):\n    """"""\n    Calculates pruning rate progressively according to the formula\n    P = 1 - a * exp(- k * epoch)\n    Where:\n    epoch - epoch number\n    P - pruning rate for current epoch\n    a, k - params\n    """"""\n    def __init__(self, pruning_algo, config=None):\n        super().__init__(pruning_algo, config)\n        self.a, self.k = self._init_exp(self.initial_pruning, self.pruning_target, pruning_steps=self.pruning_steps)\n        self._set_pruning_level()\n\n    def _calc_density_level(self):\n        curr_density = self.a * np.exp(-self.k * (self.last_epoch - self.num_init_steps))\n        min_density = 1 - self.pruning_target\n        return min_density if curr_density < min_density else curr_density\n\n    @staticmethod\n    def _init_exp(initial_pruning, max_pruning, pruning_steps=20):\n        p1 = (0, 1 - initial_pruning)\n        p2 = (pruning_steps, 1 - max_pruning)\n        k = np.log(p2[1] / p1[1]) / (p1[0] - p2[0])\n        a = p1[1] / np.exp(-k * p1[0])\n        return a, k\n\n\n@PRUNING_SCHEDULERS.register(""exponential_with_bias"")\nclass ExponentialWithBiasPruningScheduler(PruningScheduler):\n    """"""\n    Calculates pruning rate progressively according to the formula\n    P = a * exp(- k * epoch) + b\n    Where:\n    epoch - epoch number\n    P - pruning rate for current epoch\n    a, b, k - params\n    """"""\n    def __init__(self, pruning_algo, config=None):\n        super().__init__(pruning_algo, config)\n        self.a, self.b, self.k = self._init_exp(self.pruning_steps, self.initial_pruning, self.pruning_target)\n        self._set_pruning_level()\n\n    def _calc_density_level(self):\n        curr_density = 1 - (self.a * np.exp(-self.k * (self.last_epoch - self.num_init_steps)) + self.b)\n        min_density = 1 - self.pruning_target\n        return min_density if curr_density < min_density else curr_density\n\n    @staticmethod\n    def _init_exp(E_max, P_min, P_max, D=1 / 8):\n        """"""\n        Find a, b, k for system (from SPFP paper):\n        1. P_min = a + b\n        2. P_max = a * exp(-k * E_max) + b\n        3. 3/4 * P_max = a *  exp(-k * E_max * D) + b\n        Where P_min, P_max - minimal and goal levels of pruning rate\n        E_max - number of epochs for pruning\n        """"""\n        def get_b(a, k):\n            return P_min - a\n\n        def get_a(k):\n            return (3 / 4 * P_max - P_min) / (np.exp(- D * k * E_max) - 1)\n\n        def f_to_solve(x):\n            y = np.exp(D * x * E_max)\n            return 1 / 3 * y + 1 / (y ** 7) - 4 / 3\n\n        k = scipy.optimize.fsolve(f_to_solve, [1])[0]\n        a = get_a(k)\n        b = get_b(a, k)\n        return a, b, k\n'"
pytorch_toolkit/nncf/nncf/pruning/utils.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport math\nfrom collections import deque\n\nimport torch\nfrom functools import partial\n\nimport networkx as nx\n\nfrom nncf.dynamic_graph.context import Scope\nfrom nncf.dynamic_graph.graph import NNCFGraph, NNCFNode\nfrom nncf.nncf_network import NNCFNetwork\n\n\n# pylint: disable=protected-access\ndef get_rounded_pruned_element_number(total, sparsity_rate, multiple_of=8):\n    """"""\n    Calculates number of sparsified elements (approximately sparsity rate) from total such as\n    number of remaining items will be multiple of some value.\n    Always rounds number of remaining elements up.\n    :param total: total elements number\n    :param sparsity_rate: prorortion of zero elements in total.\n    :param multiple_of:\n    :return: number of elements to be zeroed\n    """"""\n    remaining_elems = math.ceil((total - total * sparsity_rate) / multiple_of) * multiple_of\n    return max(total - remaining_elems, 0)\n\n\ndef get_bn_node_for_conv(graph: nx.Graph, conv_node: dict):\n    out_edges = graph.out_edges(conv_node[\'key\'])\n    for _, out_node_key in out_edges:\n        out_node = graph.nodes[out_node_key]\n        if out_node[\'op_exec_context\'].operator_name == \'batch_norm\':\n            return out_node\n    return None\n\n\ndef get_bn_for_module_scope(target_model: NNCFNetwork, module_scope: Scope):\n    """"""\n    Returns batch norm module that corresponds to module_scope convolution.\n    :param target_model: NNCFNetwork to work with\n    :param module_scope:\n    :return: batch norm module\n    """"""\n    graph = target_model.get_original_graph()\n    module_graph_node = graph.find_node_in_nx_graph_by_scope(module_scope)\n    bn_graph_node = get_bn_node_for_conv(graph._nx_graph, module_graph_node)\n    bn_module = None\n    if bn_graph_node:\n        bn_module = target_model.get_module_by_scope(bn_graph_node[\'op_exec_context\'].scope_in_model)\n    return bn_module\n\n\ndef find_first_ops_with_type(nncf_graph: NNCFGraph, nodes, required_types, forward: bool = True):\n    """"""\n    Looking for first nodes with type from pruned_ops_types that are reachable from nodes.\n    :param nncf_graph: NNCFGraph to work with\n    :param nodes: nodes from which search begins\n    :param required_types: types of nodes for search\n    :param forward: whether the search will be forward or backward\n    :return:\n    """"""\n    graph = nncf_graph._nx_graph\n    get_edges_fn = graph.out_edges if forward else graph.in_edges\n\n    found_nodes = []\n    visited = {n: False for n in graph.nodes}\n    node_stack = deque(nodes)\n    while node_stack:\n        last_node = node_stack.pop()\n        last_node_type = nncf_graph.node_type_fn(last_node)\n\n        if not visited[last_node[\'key\']]:\n            visited[last_node[\'key\']] = True\n        else:\n            continue\n\n        if last_node_type not in required_types:\n            edges = get_edges_fn(last_node[\'key\'])\n            for in_node_name, out_node_name in edges:\n                cur_node = graph.nodes[out_node_name] if forward else graph.nodes[in_node_name]\n\n                if not visited[cur_node[\'key\']]:\n                    node_stack.append(cur_node)\n        else:\n            found_nodes.append(last_node)\n    return found_nodes\n\n\ndef traverse_function(node: NNCFNode, output, nncf_graph: NNCFGraph, required_types, visited):\n    nx_node = nncf_graph._nx_graph.nodes[nncf_graph.get_node_key_by_id(node.node_id)]\n    node_type = nncf_graph.node_type_fn(nx_node)\n    if visited[node.node_id]:\n        return True, output\n    visited[node.node_id] = True\n\n    if node_type not in required_types:\n        return False, output\n\n    output.append(node)\n    return True, output\n\n\ndef get_first_pruned_modules(target_model: NNCFNetwork, pruned_ops_types):\n    """"""\n    Looking for first pruned modules in target model.\n    First == layer of pruned type, that there is a path from the input such that there are no other\n    pruned operations on it.\n    :param pruned_ops_types: types of modules that will be pruned\n    :param target_model: model to work with\n    :return: list of all first pruned modules\n    """"""\n    graph = target_model.get_original_graph()  # NNCFGraph here\n    graph_roots = graph.get_input_nodes()  # NNCFNodes here\n\n    visited = {node_id: False for node_id in graph.get_all_node_idxs()}\n    partial_traverse_function = partial(traverse_function, nncf_graph=graph, required_types=pruned_ops_types,\n                                        visited=visited)\n\n    first_pruned_nodes = []\n    for root in graph_roots:\n        first_pruned_nodes.extend(graph.traverse_graph(root, partial_traverse_function))\n    first_pruned_modules = [target_model.get_module_by_scope(n.op_exec_context.scope_in_model)\n                            for n in first_pruned_nodes]\n    return first_pruned_modules\n\n\ndef get_last_pruned_modules(target_model: NNCFNetwork, pruned_ops_types):\n    """"""\n    Looking for last pruned modules in target model.\n    Last == layer of pruned type, that there is a path from this layer to the model output\n    such that there are no other pruned operations on it.\n    :param pruned_ops_types: types of modules that will be pruned\n    :param target_model: model to work with\n    :return: list of all last pruned modules\n    """"""\n    graph = target_model.get_original_graph()  # NNCFGraph here\n    graph_outputs = graph.get_graph_outputs()  # NNCFNodes here\n\n    visited = {node_id: False for node_id in graph.get_all_node_idxs()}\n    partial_traverse_function = partial(traverse_function, nncf_graph=graph, required_types=pruned_ops_types,\n                                        visited=visited)\n    last_pruned_nodes = []\n    for output in graph_outputs:\n        last_pruned_nodes.extend(graph.traverse_graph(output, partial_traverse_function, False))\n\n    last_pruned_modules = [target_model.get_module_by_scope(n.op_exec_context.scope_in_model)\n                           for n in last_pruned_nodes]\n    return last_pruned_modules\n\n\ndef is_conv_with_downsampling(conv_module):\n    return not torch.all(torch.tensor(conv_module.stride) == 1)\n'"
pytorch_toolkit/nncf/nncf/quantization/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n# Required for correct QUANTIZATION_MODULES registry functioning\nfrom . import layers\n'"
pytorch_toolkit/nncf/nncf/quantization/algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import OrderedDict\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional\n\nimport functools\nimport networkx as nx\nimport numpy as np\nimport operator\nimport shutil\nimport torch\nfrom texttable import Texttable\nfrom torch import nn\n\nfrom nncf.algo_selector import COMPRESSION_ALGORITHMS\nfrom nncf.compression_method_api import CompressionAlgorithmBuilder, CompressionAlgorithmController\nfrom nncf.config import Config\nfrom nncf.debug import is_debug, DebugInterface, CallCountTracker\nfrom nncf.dynamic_graph.context import OperatorInput, TracingContext, \\\n    InputAgnosticOperationExecutionContext, Scope\nfrom nncf.dynamic_graph.function_input_quantization import FUNCTIONS_TO_QUANTIZE\nfrom nncf.dynamic_graph.graph import NNCFNode\nfrom nncf.dynamic_graph.graph import NNCFNodeExpression as N, NNCFGraph\nfrom nncf.dynamic_graph.patch_pytorch import get_arg_positions_to_quantize\nfrom nncf.dynamic_graph.transform_graph import is_nncf_module\nfrom nncf.hw_config import HWConfig\nfrom nncf.initialization import DataLoaderInitializeRunner\nfrom nncf.module_operations import UpdateWeight, UpdateInputs\nfrom nncf.nncf_logger import logger as nncf_logger\nfrom nncf.nncf_network import NNCFNetwork, CompressionModuleType, InsertionInfo, InsertionCommand, OperationPriority, \\\n    InsertionPoint, InsertionType, InsertionPointGraph, InsertionPointGraphNodeType\nfrom nncf.quantization.init_precision import PrecisionInitializerFactory\nfrom nncf.quantization.layers import QUANTIZATION_MODULES, QuantizationMode, QuantizerConfig, BaseQuantizer\nfrom nncf.quantization.quantizer_propagation import QuantizerPropagationSolver, QuantizerPropagationStateGraph\nfrom nncf.utils import get_all_modules_by_type, in_scope_list, is_main_process\nfrom nncf.utils import get_state_dict_names_with_modules\n\n\nclass QuantizerSetupType(Enum):\n    PATTERN_BASED = ""pattern_based""\n    PROPAGATION_BASED = ""propagation_based""\n\n\nclass QuantizationConstraints:\n    REF_QCONF_OBJ = QuantizerConfig()\n\n    def __init__(self, **kwargs):\n        """"""Use attribute names of QuantizerConfig as arguments\n        to set up constraints.\n        E.g. QuantizationConstraint(bits=8, per_channel=True) will set up\n        a constraint that corresponds to all 8-bit per-channel quantizers, either\n        symmetric or asymmetric, either signed or unsigned.""""""\n\n        for attr_name in kwargs:\n            if not hasattr(QuantizationConstraints.REF_QCONF_OBJ, attr_name):\n                raise RuntimeError(""Invalid constraint - QuantizerConfig has no attribute \'{}\'"".format(attr_name))\n        self.qconf_attr_vs_constraint_dict = kwargs\n\n    def apply_constraints_to(self, qconfig: QuantizerConfig) -> QuantizerConfig:\n        for attr_name, constraint in self.qconf_attr_vs_constraint_dict.items():\n            if constraint is not None:\n                setattr(qconfig, attr_name, constraint)\n        return qconfig\n\n    def is_config_compatible(self, qconfig: QuantizerConfig) -> bool:\n        is_compatible = True\n        for attr_name, constraint in self.qconf_attr_vs_constraint_dict.items():\n            if constraint is not None:\n                qconf_attr_value = getattr(qconfig, attr_name)\n                if qconf_attr_value != constraint:\n                    is_compatible = False\n        return is_compatible\n\n\nclass QuantizerGroup(Enum):\n    ACTIVATIONS = ""activations""\n    WEIGHTS = ""weights""\n\n\n@COMPRESSION_ALGORITHMS.register(\'quantization\')\nclass QuantizationBuilder(CompressionAlgorithmBuilder):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.quantize_inputs = self.config.get(\'quantize_inputs\', True)\n        self.quantize_outputs = self.config.get(\'quantize_outputs\', False)\n        self.disable_function_quantization_hooks = self.config.get(\'disable_function_quantization_hooks\', False)\n\n        self._debug_interface = QuantizationDebugInterface() if is_debug() else None\n\n        self._quantized_weight_modules_registry = OrderedDict()\n        self._quantized_inputs_modules_registry = OrderedDict()\n        self._weight_quantizers = OrderedDict()  # Quantizers applied via UpdateWeights\n        self._non_weight_quantizers = OrderedDict()  # All the other quantizers\n        self._processed_function_quantizers = set()\n        self._processed_input_agnostic_op_exec_contexts = set()\n\n        self.global_quantizer_contraints = {}  # type: Dict[QuantizerGroup, QuantizationConstraints]\n        self._ignored_scopes_per_group = {}  # type: Dict[QuantizerGroup, List[str]]\n        self._target_scopes_per_group = {}  # type: Dict[QuantizerGroup, List[str]]\n\n        for quantizer_group in QuantizerGroup:\n            self._parse_group_params(self.config, quantizer_group)\n\n        self.quantizer_setup_type = QuantizerSetupType.PATTERN_BASED  # TODO: determine from config\n        self.quantizable_subgraph_patterns = self.config.get(\'quantizable_subgraph_patterns\', None)\n        self.hw_config = None\n        hw_config_type = self.config.get(""hw_config_type"")\n        if hw_config_type is not None:\n            hw_config_path = HWConfig.get_path_to_hw_config(hw_config_type)\n            self.hw_config = HWConfig.from_json(hw_config_path)\n            self.quantizer_setup_type = QuantizerSetupType.PROPAGATION_BASED\n\n    def _parse_group_params(self, quant_config: Config, quantizer_group: QuantizerGroup):\n        group_name = quantizer_group.value\n        params_dict = quant_config.get(group_name, {})\n        self.global_quantizer_contraints[quantizer_group] = QuantizationConstraints(\n            bits=params_dict.get(\'bits\'),\n            mode=params_dict.get(\'mode\'),\n            signedness_to_force=params_dict.get(\'signed\'),\n            per_channel=params_dict.get(\'per_channel\')\n        )\n        self._ignored_scopes_per_group[quantizer_group] = params_dict.get(\'ignored_scopes\')\n        self._target_scopes_per_group[quantizer_group] = params_dict.get(\'target_scopes\')\n\n    def apply_to(self, target_model: NNCFNetwork) -> NNCFNetwork:\n        insertion_commands = self._quantize_weights(target_model) + self._quantize_activations(target_model)\n        if self.quantize_inputs:\n            insertion_commands += self._quantize_inputs(target_model, insertion_commands)\n\n        quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]\n\n        # At this point the NNCF module quantization modules are not in the target_model yet,\n        # therefore it is extended with the corresponding registries tracked during weights/inputs quantizations\n        self._all_quantizations = get_state_dict_names_with_modules(target_model, quantization_types)\n        self._all_quantizations.update(self._quantized_weight_modules_registry)\n        self._all_quantizations.update(self._quantized_inputs_modules_registry)\n\n        for command in insertion_commands:\n            target_model.register_insertion_command(command)\n\n        target_model.register_algorithm(self)\n\n        if self._debug_interface is not None:\n            target_model.debug_interface.add_interface(self._debug_interface)\n        return target_model\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n        return QuantizationController(target_model,\n                                      self.config,\n                                      self._debug_interface,\n                                      self._quantized_weight_modules_registry,\n                                      self._quantized_inputs_modules_registry,\n                                      self._weight_quantizers,\n                                      self._non_weight_quantizers)\n\n    def __get_default_qconfig(self, constraints: QuantizationConstraints = None):\n        qconfig = QuantizerConfig(bits=8,\n                                  mode=QuantizationMode.SYMMETRIC,\n                                  signedness_to_force=None,\n                                  per_channel=False)\n        if constraints is not None:\n            qconfig = constraints.apply_constraints_to(qconfig)\n        return qconfig\n\n    def __get_scoped_quantizer_config(self, target_model: NNCFNetwork,\n                                      parent_module_scope_str: str, is_weights=False, input_shape=None):\n        group = QuantizerGroup.WEIGHTS if is_weights else QuantizerGroup.ACTIVATIONS\n        qconfig = self.__get_default_qconfig(constraints=self.global_quantizer_contraints[group])\n        qconfig.is_weights = is_weights\n\n        scope_overrides = self.config.get(""scope_overrides"", {})\n        for overridden_scope in scope_overrides.keys():\n            if in_scope_list(parent_module_scope_str, overridden_scope):\n                config_overrides = scope_overrides[overridden_scope]\n                if config_overrides.get(""bits"") is not None:\n                    qconfig.bits = config_overrides[""bits""]\n                if config_overrides.get(""mode"") is not None:\n                    qconfig.mode = config_overrides[""mode""]\n                if config_overrides.get(""per_channel"") is not None:\n                    qconfig.per_channel = config_overrides[""per_channel""]\n                if config_overrides.get(""signed"") is not None:\n                    qconfig.signedness_to_force = config_overrides[""signed""]\n        if qconfig.per_channel:\n            if is_weights:\n                module = target_model.get_module_by_scope(Scope.from_str(parent_module_scope_str))\n                qconfig.input_shape = module.weight.shape\n            elif input_shape is not None:\n                qconfig.input_shape = input_shape\n            else:\n                raise RuntimeError(""Unable to use per channel quantization for module {} activations -""\n                                   "" input shape is unknown"".format(parent_module_scope_str))\n        return qconfig\n\n    def __create_quantize_module(self, qconfig: QuantizerConfig):\n        quantizer_cls = QUANTIZATION_MODULES.get(qconfig.mode)\n        return quantizer_cls(qconfig)\n\n    def _make_quantizable_subgraph_pattern(self):\n        full_pattern = self._make_default_quantizable_subgraph_pattern()\n        if self.quantizable_subgraph_patterns is not None:\n            for pattern in self.quantizable_subgraph_patterns:\n                if not isinstance(pattern, str):\n                    custom_pattern = functools.reduce(operator.add,\n                                                      [N(node) for node in pattern])\n                else:\n                    custom_pattern = N(pattern)\n                full_pattern = full_pattern | custom_pattern\n        return full_pattern\n\n    def _quantize_weights(self, target_model: NNCFNetwork) -> List[InsertionCommand]:\n        device = next(target_model.parameters()).device\n        modules = target_model.get_nncf_modules()\n        insertion_commands = []\n        if self.hw_config is not None:\n            meta_vs_qconfig_map = self.hw_config.get_metatype_vs_quantizer_configs_map(for_weights=True)\n        for module_scope, module in modules.items():\n            if not self._should_consider_scope_for_group(str(module_scope), QuantizerGroup.WEIGHTS):\n                nncf_logger.info(""Ignored adding Weight quantizer in scope: {}"".format(module_scope))\n                continue\n\n            self._quantized_weight_modules_registry[str(module_scope)] = module\n\n            nncf_logger.info(""Adding signed Weight quantizer in scope: {}"".format(module_scope))\n            if self.hw_config is None:\n                qconfig = self.__get_scoped_quantizer_config(target_model, str(module_scope), is_weights=True)\n            else:\n                associated_ops = target_model.get_insertion_point_graph().get_op_nodes_in_scope(module_scope)\n                if not associated_ops:\n                    raise RuntimeError(\n                        ""Could not find a patched operation corresponding to NNCF module scope {}"".format(\n                            str(module_scope)))\n                assert len(associated_ops) == 1, ""NNCF module has more than 1 associated graph operation node - "" \\\n                                                 ""cannot make sure that weight quantization will be correct""\n                graph_operation = associated_ops[0]\n                metatype = graph_operation[InsertionPointGraph.OPERATOR_METATYPE_NODE_ATTR]\n                qconfig_list = meta_vs_qconfig_map[metatype]\n\n                try:\n                    qconfig = self._select_final_qconfig(qconfig_list,\n                                                         self.global_quantizer_contraints[QuantizerGroup.WEIGHTS])\n                except RuntimeError:\n                    err_msg = ""Quantization parameter constraints specified in NNCF config are incompatible with HW ""\n                    err_msg += ""capabilities as specified in HW config type \'{}\'. "".format(self.hw_config.target_device)\n                    err_msg += ""First conflicting quantizer location: {}"".format(str(module_scope))\n                    raise RuntimeError(err_msg)\n\n                qconfig.input_shape = module.weight.shape\n\n            quantizer = self.__create_quantize_module(qconfig)\n            op = UpdateWeight(quantizer).to(device)\n            # TODO: separate insertion point semantic for weights and activations\n            insertion_commands.append(InsertionCommand(\n                InsertionPoint(\n                    InputAgnosticOperationExecutionContext("""", module_scope, 0),\n                    InsertionType.NNCF_MODULE_PRE_OP), op, OperationPriority.QUANTIZATION_PRIORITY))\n            self._weight_quantizers[self.WeightQuantizerKey(module_scope)] = quantizer\n        return insertion_commands\n\n    class ActivationQuantizationHook:\n        """"""Cannot simply register the quantizer module as a callable hook, since we need to call\n        a thread-local version of the quantizer module during base module execution.""""""\n\n        def __init__(self, context: TracingContext, ia_op_exec_context: InputAgnosticOperationExecutionContext,\n                     debug_interface: \'QuantizationDebugInterface\' = None):\n            self.compressed_context = context\n            self.ia_op_exec_context = ia_op_exec_context\n            self.debug_interface = debug_interface\n\n        def __call__(self, *args, **kwargs):\n            if self.debug_interface is not None:\n                self.debug_interface.register_activation_quantize_call(str(self.ia_op_exec_context))\n            replica = self.compressed_context.base_module_thread_local_replica\n            return replica.activation_quantizers[str(self.ia_op_exec_context)](*args, **kwargs)\n\n    def _quantize_activations(self, target_model: NNCFNetwork) -> List[InsertionCommand]:\n        target_model.register_compression_module_type(CompressionModuleType.ACTIVATION_QUANTIZER)\n\n        if self.quantizer_setup_type == QuantizerSetupType.PATTERN_BASED:\n            insertion_commands = self._quantize_post_pattern_activations(target_model)\n        elif self.quantizer_setup_type == QuantizerSetupType.PROPAGATION_BASED:\n            insertion_point_graph = target_model.get_insertion_point_graph()\n            if self._debug_interface:\n                self._debug_interface.visualize_insertion_point_graph(insertion_point_graph)\n            prop_graph_solver = QuantizerPropagationSolver(debug_interface=self._debug_interface,\n                                                           hw_config=self.hw_config)\n            merged_ip_graph = insertion_point_graph.get_ip_graph_with_merged_hw_optimized_operations(self.hw_config)\n            insertion_data = prop_graph_solver.run_on_ip_graph(merged_ip_graph)\n            insertion_commands = []\n\n            original_nncf_graph = target_model.get_original_graph()\n            for insertion_info, quantizer_config_list in insertion_data.items():\n                # Tailored for post-hook quantization and first output quantization only\n                quantizer_input_shape = original_nncf_graph.get_output_shapes_for_ia_op_exec_context(\n                    insertion_info.op_exec_context.input_agnostic)[0]\n\n                try:\n                    quantizer_config = self._select_final_qconfig(quantizer_config_list,\n                                                                  self.global_quantizer_contraints[\n                                                                      QuantizerGroup.ACTIVATIONS])\n                except RuntimeError:\n                    err_msg = ""Quantization parameter constraints specified in NNCF config are incompatible with HW ""\n                    err_msg += ""capabilities as specified in HW config type \'{}\'. "".format(self.hw_config.target_device)\n                    err_msg += ""First conflicting quantizer location: ""\n                    err_msg += str(insertion_info.op_exec_context.input_agnostic)\n                    raise RuntimeError(err_msg)\n\n                quantizer_config.input_shape = quantizer_input_shape\n                insertion_commands.append(\n                    self._quantize_single_activation(target_model, insertion_info, quantizer_config))\n        else:\n            raise RuntimeError(""Invalid quantizer setup type!"")\n\n        if not self.disable_function_quantization_hooks:\n            insertion_commands += self._quantize_free_function_inputs(target_model)\n        return insertion_commands\n\n    def _select_final_qconfig(self, quantizer_config_list: Optional[List[QuantizerConfig]],\n                              constraints: QuantizationConstraints) -> QuantizerConfig:\n        if quantizer_config_list is None:\n            # TODO: This case corresponds to allowing to use any quantization configuration\n            # supported by HW. Need to parse this from HW config instead of using a global\n            # default config.\n            return self.__get_default_qconfig()\n\n        constrained_quantizer_config_list = list(filter(\n            constraints.is_config_compatible,\n            quantizer_config_list\n        ))\n\n        if not constrained_quantizer_config_list:\n            raise RuntimeError()\n\n        # Quantizer config list entries should arrive in the same order as they are listed\n        # in the HW config, where they are sorted by descending order of priority\n        return constrained_quantizer_config_list[0]\n\n    def _quantize_post_pattern_activations(self, target_model: NNCFNetwork) -> List[InsertionCommand]:\n        pattern = self._make_quantizable_subgraph_pattern()\n        target_insertion_infos = target_model.get_post_pattern_insertion_points(pattern)\n        insertion_commands = []\n\n        for insertion_info in target_insertion_infos:\n            ia_op_exec_context = insertion_info.op_exec_context.input_agnostic\n            operator_scope_str = str(ia_op_exec_context)\n\n            if not self.quantize_outputs and insertion_info.is_output:\n                nncf_logger.info(""Ignored adding Activation Quantize ""\n                                 ""in scope (output scope, quantize_outputs=False): {}"".format(operator_scope_str))\n                continue\n            if not self._should_consider_scope_for_group(operator_scope_str, QuantizerGroup.ACTIVATIONS):\n                nncf_logger.info(""Ignored adding Activation quantizer in scope: {}"".format(operator_scope_str))\n                continue\n\n            qconfig = self.__get_scoped_quantizer_config(target_model, operator_scope_str,\n                                                         is_weights=False,\n                                                         input_shape=insertion_info.shape_to_operate_on)\n            insertion_commands.append(self._quantize_single_activation(target_model, insertion_info, qconfig))\n\n        # NOTE: Order of activations must be the same to correctly broadcast parameters (e.g. scales) in distributed\n        # mode (see call of `_dist_broadcast_coalesced` in torch/nn/parallel/distributed.py for more details)\n        # pylint: disable=protected-access\n        target_model.sort_compression_modules(CompressionModuleType.ACTIVATION_QUANTIZER)\n        return insertion_commands\n\n    def _quantize_single_activation(self, target_model: NNCFNetwork,\n                                    insertion_info: InsertionInfo,\n                                    quantizer_config: QuantizerConfig) -> InsertionCommand:\n        ia_op_exec_context = insertion_info.op_exec_context.input_agnostic\n        quantizer_key = self.NonWeightQuantizerKey(ia_op_exec_context)\n        operator_scope_str = str(quantizer_key.ia_op_exec_context)\n        device = next(target_model.parameters()).device\n\n        if ia_op_exec_context in self._processed_input_agnostic_op_exec_contexts:\n            raise RuntimeError(\n                ""Ambiguous call to {fn} with call order {co} in current scope. ""\n                ""Cannot insert quantization hooks ""\n                ""automatically!"".format(fn=ia_op_exec_context.operator_name, co=ia_op_exec_context.call_order)\n            )\n        self._processed_input_agnostic_op_exec_contexts.add(ia_op_exec_context)\n\n        assert operator_scope_str not in target_model.get_compression_modules_by_type(\n            CompressionModuleType.ACTIVATION_QUANTIZER)\n\n        quantizer = self.__create_quantize_module(quantizer_config).to(device)\n        target_model.add_compression_module(operator_scope_str, quantizer,\n                                            CompressionModuleType.ACTIVATION_QUANTIZER)\n        self._non_weight_quantizers[quantizer_key] = quantizer\n\n        nncf_logger.info(""Adding {} Activation Quantize in scope: {}"".format(\n            ""signed"" if quantizer.signed else\n            ""unsigned"", operator_scope_str\n        ))\n\n        hook = self.ActivationQuantizationHook(target_model.get_tracing_context(),\n                                               ia_op_exec_context,\n                                               self._debug_interface)\n\n        self._processed_input_agnostic_op_exec_contexts.add(ia_op_exec_context)\n        return InsertionCommand(InsertionPoint(ia_op_exec_context,\n                                               InsertionType.OPERATOR_POST_HOOK),\n                                hook,\n                                OperationPriority.QUANTIZATION_PRIORITY)\n\n    def _quantize_inputs(self, target_model: NNCFNetwork,\n                         prev_weight_and_activation_quantizer_insertion_commands: List[InsertionCommand]) -> \\\n            List[InsertionCommand]:\n        device = next(target_model.parameters()).device\n        graph_roots = target_model.get_original_graph().get_input_nodes()\n\n        # Have to handle the situation when the input node of the network is an NNCF module -\n        # to quantize inputs in this case we will have to use UpdateInputs module pre-op,\n\n        # Traverse down starting from graph roots to search for the first node which belongs to a NNCF module\n        # and has no UpdateInputs pre-op\n\n        def traverse_function(node: NNCFNode, output) -> Tuple[bool, List[NNCFNode]]:\n            module = target_model.get_module_by_scope(node.op_exec_context.scope_in_model)\n            if is_nncf_module(module):\n                current_node_scope = node.op_exec_context.scope_in_model\n                module_op_insertion_commands = []\n                for comm in prev_weight_and_activation_quantizer_insertion_commands:\n                    if current_node_scope in comm.insertion_point.ia_op_exec_context.scope_in_model:\n                        module_op_insertion_commands.append(comm)\n                pre_op_insertion_commands = filter(\n                    lambda comm: comm.insertion_point.insertion_type == InsertionType.NNCF_MODULE_PRE_OP,\n                    module_op_insertion_commands)\n                update_inputs_count = sum(1 for comm in pre_op_insertion_commands if isinstance(comm.fn, UpdateInputs))\n                if update_inputs_count == 0:\n                    output.append(node)\n                    return True, output\n            else:\n                current_node_ia_op_exec_context = node.op_exec_context.input_agnostic\n                op_hook_insertion_commands = []\n                for comm in prev_weight_and_activation_quantizer_insertion_commands:\n                    if current_node_ia_op_exec_context == comm.insertion_point.ia_op_exec_context:\n                        op_hook_insertion_commands.append(comm)\n                if op_hook_insertion_commands:\n                    return True, output\n\n            return False, output\n\n        nncf_module_input_nodes = set()\n        for node in graph_roots:\n            scope_str = str(node.op_exec_context.scope_in_model)\n            if self._should_consider_scope_for_group(scope_str, QuantizerGroup.ACTIVATIONS):\n                nncf_module_input_nodes.update(\n                    target_model.get_original_graph().traverse_graph(node, traverse_function))\n\n        insertion_commands = []\n        nncf_scope_module_dict = target_model.get_nncf_modules()\n        for module_input_node in nncf_module_input_nodes:\n            op_scope = module_input_node.op_exec_context.input_agnostic.scope_in_model\n            module = None\n            scope = None\n            for nncf_scope, nncf_module in nncf_scope_module_dict.items():\n                if op_scope in nncf_scope:\n                    module = nncf_module\n                    scope = nncf_scope\n                    break\n\n            self._quantized_inputs_modules_registry[str(scope)] = module\n\n            # Only use the shape of the 0-th input info specified in config. TODO: fix this\n            input_shape = target_model.input_infos[0].shape if target_model.input_infos is not None else None\n            qconfig = self.__get_scoped_quantizer_config(target_model, str(scope), is_weights=False,\n                                                         input_shape=input_shape)\n            quantizer = self.__create_quantize_module(qconfig)\n\n            nncf_logger.info(""Adding {} NNCF module input quantizer in scope: {}"".format(\n                ""signed"" if quantizer.signed else ""unsigned"", str(scope)\n            ))\n\n            # TODO: separate insertion point semantic for weights and activations\n            insertion_commands.append(\n                InsertionCommand(InsertionPoint(InputAgnosticOperationExecutionContext("""", scope, 0),\n                                                InsertionType.NNCF_MODULE_PRE_OP),\n                                 UpdateInputs(quantizer).to(device),\n                                 OperationPriority.QUANTIZATION_PRIORITY))\n            ia_op_exec_context = module_input_node.op_exec_context.input_agnostic\n            self._non_weight_quantizers[self.InputQuantizerKey(ia_op_exec_context)] = quantizer\n\n        return insertion_commands\n\n    def _should_consider_scope_for_group(self, scope_str: str, group: QuantizerGroup) -> bool:\n        if self.target_scopes is not None or self._target_scopes_per_group[group] is not None:\n            if in_scope_list(scope_str, self.target_scopes):\n                return True\n            if in_scope_list(scope_str, self._target_scopes_per_group[group]):\n                return True\n\n            return False\n\n        if in_scope_list(scope_str, self.ignored_scopes):\n            return False\n        if in_scope_list(scope_str, self._ignored_scopes_per_group[group]):\n            return False\n\n        return True\n\n    class QuantizerKey:\n        def get_base(self):\n            raise NotImplementedError\n\n        def get_suffix(self) -> str:\n            raise NotImplementedError\n\n        def __str__(self):\n            return str(self.get_base()) + self.get_suffix()\n\n        def __hash__(self):\n            return hash((self.get_base(), self.get_suffix()))\n\n    class WeightQuantizerKey(QuantizerKey):\n        def __init__(self, scope: \'Scope\'):\n            self.scope = scope\n\n        def get_base(self) -> \'Scope\':\n            return self.scope\n\n        def get_suffix(self) -> str:\n            return \'module_weight\'\n\n    class NonWeightQuantizerKey(QuantizerKey):\n        def __init__(self, ia_op_exec_context: InputAgnosticOperationExecutionContext):\n            self.ia_op_exec_context = ia_op_exec_context\n\n        def get_base(self) -> \'InputAgnosticOperationExecutionContext\':\n            return self.ia_op_exec_context\n\n        def get_suffix(self) -> str:\n            return \'\'\n\n    class InputQuantizerKey(NonWeightQuantizerKey):\n        def get_base(self) -> \'Scope\':\n            return self.ia_op_exec_context.scope_in_model\n\n        def get_suffix(self) -> str:\n            return \'module_input\'\n\n    class FunctionQuantizerKey(NonWeightQuantizerKey):\n        def __init__(self, ia_op_exec_context: InputAgnosticOperationExecutionContext, input_arg_idx: int):\n            super().__init__(ia_op_exec_context)\n            self.input_arg_idx = input_arg_idx\n\n        def get_suffix(self) -> str:\n            return ""_input"" + str(self.input_arg_idx)\n\n    class FunctionQuantizationPreHook:\n        """"""Cannot simply register the quantizer module as a callable hook, since we need to call\n        a thread-local version of the quantizer module during base module execution.""""""\n\n        def __init__(self, context: TracingContext, func_in_quant_info: \'FunctionQuantizerKey\',\n                     debug_interface: \'QuantizationDebugInterface\' = None):\n            self.compressed_context = context\n            self.func_in_quant_info = func_in_quant_info\n            self.debug_interface = debug_interface\n\n        def __call__(self, op_inputs: OperatorInput):\n            quantizer_dict_key = str(self.func_in_quant_info)\n            if self.debug_interface is not None:\n                self.debug_interface.register_function_quantizer_call(quantizer_dict_key)\n            replica = self.compressed_context.base_module_thread_local_replica\n            idx = self.func_in_quant_info.input_arg_idx\n            op_inputs.op_args[idx] = replica.function_quantizers[quantizer_dict_key](op_inputs.op_args[idx])\n            return op_inputs\n\n    def _quantize_free_function_inputs(self, target_model: NNCFNetwork) -> List[InsertionCommand]:\n        device = next(target_model.parameters()).device\n\n        if not FUNCTIONS_TO_QUANTIZE:\n            return []\n        pattern = N(FUNCTIONS_TO_QUANTIZE[0].name)\n        for i in range(1, len(FUNCTIONS_TO_QUANTIZE)):\n            pattern |= N(FUNCTIONS_TO_QUANTIZE[i].name)\n\n        target_insertion_infos = target_model.get_post_pattern_insertion_points(pattern,\n                                                                                omit_nodes_in_nncf_modules=True)\n        insertion_commands = []\n\n        target_model.register_compression_module_type(CompressionModuleType.FUNCTION_QUANTIZER)\n        for insertion_info in target_insertion_infos:\n            ia_op_exec_context = insertion_info.op_exec_context.input_agnostic\n            scope_str = str(ia_op_exec_context.scope_in_model)\n\n            if not self._should_consider_scope_for_group(scope_str, QuantizerGroup.ACTIVATIONS):\n                nncf_logger.info(""Ignored adding function input quantizer in scope: {}"".format(scope_str))\n                continue\n\n            function_arg_positions_to_quantize = get_arg_positions_to_quantize(ia_op_exec_context.operator_name)\n            assert function_arg_positions_to_quantize is not None, ""Function with inputs to be quantized has "" \\\n                                                                   ""no info struct registered in "" \\\n                                                                   ""QUANTIZED_INPUT_FUNCTIONS!""\n\n            for input_arg_idx in function_arg_positions_to_quantize:\n                ip_arg_quant_key = self.FunctionQuantizerKey(ia_op_exec_context, input_arg_idx)\n\n                if ip_arg_quant_key in self._processed_function_quantizers:\n                    raise RuntimeError(\n                        ""Ambiguous call to {fn} with call order {co} and argname {arg} in current scope. ""\n                        ""Cannot insert quantization hooks ""\n                        ""automatically!"".format(fn=ia_op_exec_context.operator_name,\n                                                co=ia_op_exec_context.call_order,\n                                                arg=input_arg_idx)\n                    )\n\n                self._processed_function_quantizers.add(ip_arg_quant_key)\n\n                ip_arg_quant_name = str(ip_arg_quant_key)\n                assert ip_arg_quant_name not in target_model.get_compression_modules_by_type(\n                    CompressionModuleType.FUNCTION_QUANTIZER)\n                input_shape = insertion_info.op_exec_context.tensor_metas[0].shape\n\n                qconfig = self.__get_scoped_quantizer_config(target_model, scope_str,\n                                                             is_weights=False,\n                                                             input_shape=input_shape)\n                quantizer_module = self.__create_quantize_module(qconfig).to(device)\n                target_model.add_compression_module(ip_arg_quant_name, quantizer_module,\n                                                    CompressionModuleType.FUNCTION_QUANTIZER)\n\n                nncf_logger.info(""Adding {} Function Quantize: {}"".format(\n                    ""signed"" if quantizer_module.signed else\n                    ""unsigned"", ip_arg_quant_name))\n\n                hook = self.FunctionQuantizationPreHook(target_model.get_tracing_context(),\n                                                        ip_arg_quant_key,\n                                                        self._debug_interface)\n                insertion_commands.append(InsertionCommand(InsertionPoint(ia_op_exec_context,\n                                                                          InsertionType.OPERATOR_PRE_HOOK),\n                                                           hook,\n                                                           OperationPriority.QUANTIZATION_PRIORITY))\n                self._non_weight_quantizers[ip_arg_quant_key] = quantizer_module\n        # NOTE: Order of input quantizers must be the same to correctly broadcast parameters (e.g. scales) in\n        # distributed mode (see call of `_dist_broadcast_coalesced` in torch/nn/parallel/distributed.py for more\n        # details) pylint: disable=protected-access\n        target_model.sort_compression_modules(CompressionModuleType.FUNCTION_QUANTIZER)\n        return insertion_commands\n\n    @staticmethod\n    def _make_default_quantizable_subgraph_pattern():\n        import nncf.dynamic_graph.patterns as p\n        pattern = p.LINEAR_OPS | p.ARITHMETIC | p.ANY_BN_RELU_COMBO | \\\n                  p.LINEAR_OPS + p.ANY_BN_RELU_COMBO | p.ARITHMETIC + p.ANY_BN_RELU_COMBO | p.SINGLE_OPS | p.MATMUL\n        return pattern\n\n\nclass QuantizationController(CompressionAlgorithmController):\n    def __init__(self, target_model: NNCFNetwork,\n                 quantization_config: Config,\n                 debug_interface: \'QuantizationDebugInterface\',\n                 quantized_weight_modules_registry: Dict[Scope, torch.nn.Module],\n                 quantized_inputs_modules_registry: Dict[Scope, torch.nn.Module],\n                 weight_quantizers: Dict[QuantizationBuilder.WeightQuantizerKey, torch.nn.Module],\n                 non_weight_quantizers: Dict[QuantizationBuilder.NonWeightQuantizerKey, torch.nn.Module]):\n        super().__init__(target_model)\n        self.debug_interface = debug_interface\n        self.quantization_config = quantization_config\n\n        self.quantized_weight_modules_registry = quantized_weight_modules_registry\n        self.quantized_inputs_modules_registry = quantized_inputs_modules_registry\n        self.weight_quantizers = weight_quantizers\n        self.non_weight_quantizers = non_weight_quantizers\n        self.all_quantizations = OrderedDict()\n        self.all_quantizations.update(self.weight_quantizers)\n        self.all_quantizations.update(self.non_weight_quantizers)\n        self.is_distributed = False\n\n    def distributed(self):\n        self.is_distributed = True\n\n    def initialize(self, data_loader=None, criterion=None):\n        """"""\n        For the quantization there are 2 types of initializations: range and precision.\n        First method calculates per-layer activation statistics on training dataset in order to choose proper output\n        range for quantization. Precision initialization happens based on measure - layers\' sensitivity to\n        perturbations. The measure is calculated by estimation of average trace of Hessian for modules using Hutchinson\n        algorithm.\n        Parameters for quantization algorithm:\n            \'data_loader\' - provides an iterable over the given dataset, instance of \'torch.utils.data.DataLoader\'\n            \'criterion\' - loss function, instance of `torch.nn.modules.loss._Loss`,\n        """"""\n\n        initializer_config = self.quantization_config.get(\'initializer\', {})\n        init_range_config = initializer_config.get(\'range\', {})\n        num_init_steps = init_range_config.get(\'num_init_steps\', 1)\n        if num_init_steps < 0:\n            raise AttributeError(\'Number of step to initialize must be >= 0\')\n        if num_init_steps > 0:\n            global_init_type = init_range_config.get(\'type\', \'mean_min_max\')\n\n            modules_to_init = OrderedDict()\n            scope_overrides = self.quantization_config.get(""scope_overrides"", {})\n\n            for class_type in QUANTIZATION_MODULES.registry_dict.values():\n                quantization_type = class_type.__name__\n                module_dict = get_all_modules_by_type(self._model, quantization_type)\n                for scope, module in module_dict.items():\n                    init_type = global_init_type\n                    for overridden_scope in scope_overrides.keys():\n                        if in_scope_list(str(scope), overridden_scope):\n                            initializer_config = scope_overrides[overridden_scope].get(\'initializer\', {})\n                            init_type = initializer_config.get(""type"", global_init_type)\n                    modules_to_init[str(scope)] = (module, init_type)\n\n            # NOTE: Order of modules must be the same to correctly broadcast parameters (e.g. input_low\n            # and input_range)\n            modules_to_init = OrderedDict(sorted(modules_to_init.items()))\n\n            runner = DataLoaderInitializeRunner(self._model, modules_to_init)\n            if self.is_distributed:\n                # Multi-process data loading heavily slows down collecting statistics. The best option, when data\n                # fetching is done in the same process a DataLoader is initialized, i.e. num_workers should be 0.\n                num_workers = data_loader.num_workers\n                data_loader.num_workers = 0\n\n                runner.run(data_loader, num_init_steps, self.is_distributed)\n                data_loader.num_workers = num_workers\n            else:\n                runner.run(data_loader, num_init_steps, self.is_distributed)\n            self._model.rebuild_graph()\n        init_precision_config = initializer_config.get(\'precision\', None)\n        if init_precision_config:\n            precision_init_type = init_precision_config.get(\'type\', \'manual\')\n\n            params = self.quantization_config.get(\'activations\', {})\n            default_activation_bitwidth = params.get(\'bits\', 8)\n            params = self.quantization_config.get(\'weights\', {})\n            default_weight_bitwidth = params.get(\'bits\', 8)\n\n            init_impl = PrecisionInitializerFactory.create(precision_init_type)\n            initializer = init_impl(self, init_precision_config, default_activation_bitwidth, default_weight_bitwidth,\n                                    criterion, data_loader, self.is_distributed)\n            initializer.apply_init()\n            if is_main_process():\n                nncf_logger.info(\'Bitwidth distribution\\n{}\'.format(self.get_bit_stats().draw()))\n\n    def get_weights_activation_quantizers_pairs(self) -> List[Tuple[List[BaseQuantizer], BaseQuantizer]]:\n        """"""\n        finds all neighbour weight and input activation quantizers that share the same module (e.g. conv or linear).\n        Single activation quantizer can be in pair with multiple neighbour weight quantizers, e.g. like in SqueezeNet,\n        when two Convolutions share the same input activation.\n        :return: list of pairs - (list of weight quantizers, activation quantizer)\n        """"""\n        pairs = []\n        qimr = OrderedDict(sorted(self.quantized_inputs_modules_registry.items()))\n        for _, quantized_module in qimr.items():\n            weight_quantizer = None\n            activation_quantizer = None\n            for ops in quantized_module.pre_ops.values():\n                if isinstance(ops, UpdateWeight):\n                    weight_quantizer = ops.op\n                if isinstance(ops, UpdateInputs):\n                    activation_quantizer = ops.op\n            pairs.append(([weight_quantizer], activation_quantizer))\n\n        nncf_network = self._model\n        nncf_graph = nncf_network.get_original_graph()\n        non_weight_quantizers = {key: quantizer for key, quantizer in self.non_weight_quantizers.items() if\n                                 not isinstance(key, QuantizationBuilder.InputQuantizerKey)}\n\n        def traverse_graph(curr_nx_node_key: str, weight_quantizers: List[nn.Module]) -> Optional[List[nn.Module]]:\n            nx_node = nncf_graph.get_nx_node_by_key(curr_nx_node_key)\n            module_scope = nx_node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR].scope_in_model\n            module = nncf_network.get_module_by_scope(module_scope)\n            if is_nncf_module(module):\n                if hasattr(module, \'pre_ops\'):\n                    for ops in module.pre_ops.values():\n                        if isinstance(ops, UpdateWeight):\n                            weight_quantizers.append(ops.op)\n            else:\n                for succ_nx_node_key in nncf_graph.get_successors(curr_nx_node_key):\n                    return traverse_graph(succ_nx_node_key, weight_quantizers)\n            return weight_quantizers\n\n        # pylint: disable=unnecessary-lambda\n        for quantizer_key in sorted(non_weight_quantizers, key=lambda x: str(x)):\n            activation_ctx = quantizer_key.ia_op_exec_context\n            post_hooked_nx_node_key = nncf_graph.get_node_id_by_iap_context(activation_ctx)\n            weight_quantizers = []\n            for next_nx_node_key in nncf_graph.get_successors(post_hooked_nx_node_key):\n                weight_quantizers = traverse_graph(next_nx_node_key, weight_quantizers)\n            if weight_quantizers:\n                activation_quantizer = self.non_weight_quantizers[quantizer_key]\n                pairs.append((weight_quantizers, activation_quantizer))\n        return pairs\n\n    def get_bit_stats(self):\n        table = Texttable()\n        BITS = \'num_bits\'\n        WEIGHTS_RATIO = \'% weights\'\n        ACTIVATIONS_RATIO = \'% activations\'\n        TOTAL_RATIO = \'% total\'\n\n        header = [BITS, WEIGHTS_RATIO, ACTIVATIONS_RATIO, TOTAL_RATIO]\n\n        bits = set()\n        num_all_quantizations = len(self.all_quantizations)\n        for quantizer in self.all_quantizations.values():\n            bits.add(quantizer.num_bits)\n\n        bits_stat = {}\n        for h in header:\n            bits_stat[h] = {}\n            for b in bits:\n                bits_stat[h][b] = 0\n\n        for quantizer in self.all_quantizations.values():  # type: BaseQuantizer\n            num_bits = quantizer.num_bits\n            bits_stat[TOTAL_RATIO][num_bits] += 1\n            type_ = WEIGHTS_RATIO if quantizer.is_weights else ACTIVATIONS_RATIO\n            bits_stat[type_][num_bits] += 1\n\n        data = [header]\n\n        for num_bits in bits:\n            drow = {h: 0 for h in header}\n            for column_name in header[1:]:\n                drow[column_name] = (bits_stat[column_name][num_bits] / num_all_quantizations) * 100\n            drow[BITS] = num_bits\n            row = [drow[h] for h in header]\n            data.append(row)\n        table.add_rows(data)\n        return table\n\n\nclass QuantizationDebugInterface(DebugInterface):\n    QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME = \'quantized_modules\'\n    ACTIVATION_QUANTIZERS_TRACKER_NAME = \'activation_quantizers\'\n    FUNCTION_QUANTIZERS_TRACKER_NAME = \'function_quantizers\'\n\n    def __init__(self):\n        self.call_trackers = {\n            self.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME: CallCountTracker(\n                QuantizationDebugInterface.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME),\n            self.ACTIVATION_QUANTIZERS_TRACKER_NAME: CallCountTracker(\n                QuantizationDebugInterface.ACTIVATION_QUANTIZERS_TRACKER_NAME),\n            self.FUNCTION_QUANTIZERS_TRACKER_NAME: CallCountTracker(\n                self.FUNCTION_QUANTIZERS_TRACKER_NAME)\n        }\n        self.graph_size = 0\n\n        from nncf.debug import DEBUG_LOG_DIR\n        self.dump_dir = Path(DEBUG_LOG_DIR) / Path(""debug_dumps"")\n        self.dump_dir.mkdir(parents=True, exist_ok=True)\n        self.scale_dump_dir = self.dump_dir / Path(""scale"")\n        self.prop_graph_dump_dir = self.dump_dir / Path(""quant_prop"")\n        if self.prop_graph_dump_dir.exists():\n            shutil.rmtree(str(self.prop_graph_dump_dir))\n        self.forward_call_count = 0\n        self._strict_forward = False\n\n    def init_actual(self, owner_model: NNCFNetwork):\n        quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]\n        quantizers_in_nncf_modules = owner_model.get_modules_in_nncf_modules_by_type(quantization_types)\n        nncf_module_quantizations_id_list = [str(scope) for scope in\n                                             quantizers_in_nncf_modules.keys()]  # type: List[str]\n\n        activation_quantizer_id_list = owner_model.get_compression_modules_by_type(\n            CompressionModuleType.ACTIVATION_QUANTIZER).keys()  # type: List[str]\n        function_input_quantizer_id_list = owner_model.get_compression_modules_by_type(\n            CompressionModuleType.FUNCTION_QUANTIZER).keys()  # type: List[str]\n        self.call_trackers[self.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME].init_with_key_list(\n            nncf_module_quantizations_id_list)\n        self.call_trackers[self.ACTIVATION_QUANTIZERS_TRACKER_NAME].init_with_key_list(\n            activation_quantizer_id_list)\n        self.call_trackers[self.FUNCTION_QUANTIZERS_TRACKER_NAME].init_with_key_list(\n            function_input_quantizer_id_list)\n        if self.scale_dump_dir.exists():\n            shutil.rmtree(str(self.scale_dump_dir))\n        self.scale_dump_dir.mkdir(parents=True, exist_ok=True)\n        self._strict_forward = True\n\n    def pre_forward_actions(self, module: \'NNCFNetwork\'):\n        self.reset_counters()\n\n    def post_forward_actions(self, module: \'NNCFNetwork\'):\n        self.register_forward_call()\n        # pylint:disable=protected-access\n        ctx = module.get_tracing_context()\n        self.set_graph_size(ctx.graph.get_nodes_count())\n\n        quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]\n        nncf_module_quantizations = module.get_modules_in_nncf_modules_by_type(\n            quantization_types)  # type: Dict[\'Scope\', nn.Module]\n\n        for qm_scope, qm_module in nncf_module_quantizations.items():\n            # Important - this will not work for DataParallel since it copies the\n            # entire parent module for each thread and the `call_count` attributes\n            # are incremented for thread local copies of `qm_module`, which are not\n            # the same as the master copies of `qm_module` iterated over at this point\n            self.register_quantizer_module_call(str(qm_scope), qm_module.call_count)\n            self.dump_scale(qm_module.get_trainable_params(), str(qm_scope))\n            qm_module.reset_call_counter()\n        self.print_call_stats()\n\n        call_dict = ctx.get_node_call_counter_dict()\n        total_calls = sum(call_dict.values())\n        nncf_logger.debug(""{} nodes called out of total {}"".format(total_calls,\n                                                                   ctx.graph.get_nodes_count()))\n        if self._strict_forward:\n            for tracker in self.call_trackers.values():\n                if tracker.get_never_called_keys():\n                    # This will always trigger for DataParallel - disregard or disable debug mode\n                    # for DataParallel runs\n                    raise RuntimeError(""{} has never called modules: {}!"".format(\n                        tracker.name, tracker.get_never_called_keys()))\n\n    def dump_scale(self, quantizer_scale_params: Dict[str, torch.Tensor], quantizer_name: str):\n        import re\n        quantizer_normalized_name = re.sub(r\'[^\\w\\-_\\. ]\', \'_\', quantizer_name)\n        for scale_param_name, scale_param in quantizer_scale_params.items():\n            fname = ""{}_{}.txt"".format(quantizer_normalized_name, scale_param_name)\n            with open(str(self.scale_dump_dir / fname), ""ba"") as file:\n                np.savetxt(file, scale_param.cpu().numpy())\n\n    def reset_counters(self):\n        for tracker in self.call_trackers.values():\n            tracker.reset()\n\n    def register_quantizer_module_call(self, key, counts=None):\n        self.call_trackers[self.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME].register_call(key, counts)\n\n    def register_activation_quantize_call(self, key: str):\n        self.call_trackers[self.ACTIVATION_QUANTIZERS_TRACKER_NAME].register_call(key)\n\n    def register_function_quantizer_call(self, key: str):\n        self.call_trackers[self.FUNCTION_QUANTIZERS_TRACKER_NAME].register_call(key)\n\n    def print_call_stats(self):\n        nncf_logger.debug("" Graph size: {} nodes"".format(self.graph_size))\n        for tracker in self.call_trackers.values():\n            msg = "" {} tracker:"".format(tracker.name)\n            msg += "" {} total calls;"".format(tracker.get_total_call_count())\n\n            never_called = tracker.get_never_called_keys()\n            if never_called:\n                msg += "" {} entries never called;"".format(len(never_called))\n\n            overcalled = tracker.get_overcalled_keys_with_call_counts()\n            if overcalled:\n                msg += "" {} entries called more than once;"".format(len(overcalled))\n            nncf_logger.debug(msg)\n\n    def set_graph_size(self, new_size):\n        if new_size != self.graph_size:\n            nncf_logger.debug(\'\\n\')\n            nncf_logger.debug(\n                "" warning - graph size has changed from {} to {} since last forward"".format(self.graph_size,\n                                                                                            new_size))\n        self.graph_size = new_size\n\n    def register_forward_call(self):\n        self.forward_call_count += 1\n\n    def visualize_quantizer_propagation(self,\n                                        prop_solver: QuantizerPropagationSolver,\n                                        prop_graph: QuantizerPropagationStateGraph,\n                                        iteration: str):\n        self.prop_graph_dump_dir.mkdir(parents=True, exist_ok=True)\n        fname = ""quant_prop_iter_{}.dot"".format(iteration)\n        prop_solver.debug_visualize(prop_graph,\n                                    self.prop_graph_dump_dir / Path(fname))\n\n    def visualize_insertion_point_graph(self, insertion_point_graph: InsertionPointGraph):\n        out_graph = nx.MultiDiGraph()\n        for node_key, node in insertion_point_graph.nodes.items():\n            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.INSERTION_POINT:\n                insertion_point_data = node[InsertionPointGraph.INSERTION_POINT_DATA_NODE_ATTR]  # type: InsertionPoint\n                label = ""IP: {}"".format(insertion_point_data.insertion_type)\n                out_graph.add_node(node_key, label=label, color=""red"")\n            elif node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:\n                out_graph.add_node(node_key)\n            else:\n                raise RuntimeError(""Invalid InsertionPointGraph node!"")\n        for u, v in insertion_point_graph.edges:\n            out_graph.add_edge(u, v)\n\n        for node_key, node in insertion_point_graph.nodes.items():\n            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:\n                for ip_node_key in node[InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR]:\n                    out_graph.add_edge(node_key, ip_node_key, style=""dashed"", headport=\'e\', tailport=\'e\')\n\n        nx.drawing.nx_pydot.write_dot(out_graph, self.dump_dir / Path(""insertion_point_graph.dot""))\n'"
pytorch_toolkit/nncf/nncf/quantization/extensions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport pathlib\nimport os.path\nfrom nncf.definitions import get_install_type\nfrom torch.utils.cpp_extension import load\n\n\nif ""VIRTUAL_ENV"" in os.environ:\n    build_dir = os.path.join(os.environ[""VIRTUAL_ENV""], ""torch_extensions"")\n    pathlib.Path(build_dir).mkdir(parents=True, exist_ok=True)\n    os.environ[""TORCH_EXTENSIONS_DIR""] = build_dir\n\next_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""cpu"")\nQuantizedFunctionsCPU = load(\n    \'quantized_functions_cpu\', [\n        os.path.join(ext_dir, \'functions_cpu.cpp\')\n    ],\n    verbose=False\n)\nif get_install_type() == \'GPU\':\n    ext_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""cuda"")\n    QuantizedFunctionsCUDA = load(\n        \'quantized_functions_cuda\', [\n            os.path.join(ext_dir, \'functions_cuda.cpp\'),\n            os.path.join(ext_dir, \'functions_cuda_kernel.cu\')\n        ],\n        verbose=False\n    )\nelse:\n    QuantizedFunctionsCUDA = None\n'"
pytorch_toolkit/nncf/nncf/quantization/hessian_trace.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom typing import List, Union\n\nimport torch\nfrom torch import Tensor\nfrom torch import nn\nfrom torch.nn import Parameter\nfrom torch.nn.modules.loss import _Loss\nfrom torch.utils.data import DataLoader\n\nfrom nncf.utils import is_main_process\nfrom nncf.initialization import wrap_data_loader, InitializingDataLoader\nfrom nncf.nncf_logger import logger as nncf_logger\n\n\nclass ParameterHandler:\n    def __init__(self, parameters: List[Parameter], device: torch.device):\n        self._device = device\n        self._parameters = parameters\n\n    @property\n    def parameters(self) -> List[Parameter]:\n        return self._parameters\n\n    def get_gradients(self) -> List[Union[Tensor, float]]:\n        gradients = []\n        for parameter in self.parameters:\n            gradients.append(0. if parameter.grad is None else parameter.grad + 0.)\n        return gradients\n\n    def sample_rademacher_like_params(self) -> List[Tensor]:\n        def sample(parameter):\n            r = torch.randint_like(parameter, high=2, device=self._device)\n            return r.masked_fill_(r == 0, -1)\n\n        return [sample(p) for p in self.parameters]\n\n    def sample_normal_like_params(self) -> List[Tensor]:\n        return [torch.randn(p.size(), device=self._device) for p in self.parameters]\n\n\nclass GradientsCalculator:\n\n    def __init__(self, model: nn.Module, criterion: _Loss, data_loader, num_data_iter: int,\n                 paramerter_handler: ParameterHandler):\n        self._model = model\n        self._criterion = criterion\n        self._data_loader = data_loader\n        self._num_data_iter = num_data_iter\n        self._parameter_handler = paramerter_handler\n        self.num_iter = 0\n\n    def __iter__(self):\n        self.data_loader_iter = iter(self._data_loader)\n        self.num_iter = 0\n        return self\n\n    def __next__(self):\n        if self.num_iter >= self._num_data_iter:\n            raise StopIteration\n        self.num_iter += 1\n        inputs, targets, dataloader_kwargs = next(self.data_loader_iter)\n        self._model.zero_grad()\n        outputs = self._model(inputs, **dataloader_kwargs)\n        loss = self._criterion(outputs, targets)\n        loss.backward(create_graph=True)\n        grads = self._parameter_handler.get_gradients()\n        self._model.zero_grad()\n        return grads\n\n\nclass HessianTraceEstimator:\n    """"""\n    Performs estimation of Hessian Trace based on Hutchinson algorithm.\n    """"""\n\n    def __init__(self, model: nn.Module, criterion: _Loss, device: torch.device, data_loader: DataLoader,\n                 num_data_points: int):\n        self._model = model.eval()\n        parameters = [p for p in model.parameters() if p.requires_grad]\n        self._parameter_handler = ParameterHandler(parameters, device)\n        self._batch_size = data_loader.batch_size\n        data_loader = wrap_data_loader(data_loader, InitializingDataLoader, {}, device)\n        self._num_data_iter = num_data_points // self._batch_size if num_data_points >= self._batch_size else 1\n        self._gradients_calculator = GradientsCalculator(self._model, criterion, data_loader, self._num_data_iter,\n                                                         self._parameter_handler)\n        self._diff_eps = 1e-6\n\n    def get_average_traces(self, max_iter=100, tolerance=1e-3) -> Tensor:\n        """"""\n        Estimates average hessian trace for each parameter\n        :param max_iter: maximum number of iterations for Hutchinson algorithm\n        :param tolerance: - minimum relative tolerance for stopping the algorithm.\n        It\'s calculated  between mean average trace from previous iteration and current one.\n        :return: Tensor with average hessian trace per parameter\n        """"""\n        avg_total_trace = 0.\n        avg_traces_per_iter = []  # type: List[Tensor]\n        mean_avg_traces_per_param = None\n\n        for i in range(max_iter):\n            avg_traces_per_iter.append(self._calc_avg_traces_per_param())\n\n            mean_avg_traces_per_param = self._get_mean(avg_traces_per_iter)\n            mean_avg_total_trace = torch.sum(mean_avg_traces_per_param)\n\n            diff_avg = abs(mean_avg_total_trace - avg_total_trace) / (avg_total_trace + self._diff_eps)\n            if diff_avg < tolerance:\n                return mean_avg_traces_per_param\n            avg_total_trace = mean_avg_total_trace\n            if is_main_process():\n                nncf_logger.info(\'{}# difference_avg={} avg_trace={}\'.format(i, diff_avg, avg_total_trace))\n\n        return mean_avg_traces_per_param\n\n    def _calc_avg_traces_per_param(self) -> Tensor:\n        v = self._parameter_handler.sample_rademacher_like_params()\n        vhp = self._parameter_handler.sample_normal_like_params()\n        num_all_data = self._num_data_iter * self._batch_size\n        for gradients in self._gradients_calculator:\n            vhp_curr = torch.autograd.grad(gradients,\n                                           self._parameter_handler.parameters,\n                                           grad_outputs=v,\n                                           only_inputs=True,\n                                           retain_graph=False)\n            vhp = [a + b * float(self._batch_size) + 0. for a, b in zip(vhp, vhp_curr)]\n        vhp = [a / float(num_all_data) for a in vhp]\n        avg_traces_per_param = torch.stack([torch.sum(a * b) / a.size().numel() for (a, b) in zip(vhp, v)])\n        return avg_traces_per_param\n\n    @staticmethod\n    def _get_mean(data: List[Tensor]) -> Tensor:\n        return torch.mean(torch.stack(data), dim=0)\n'"
pytorch_toolkit/nncf/nncf/quantization/init_precision.py,0,"b'import itertools\nfrom collections import OrderedDict\nfrom typing import List, Dict, Union\n\nimport os\nimport shutil\nimport torch\nfrom torch import Tensor, nn\nfrom torch.nn.modules.loss import _Loss\nfrom torch.utils.data import DataLoader\n\nfrom nncf.config import Config\nfrom nncf.debug import is_debug\nfrom nncf.dynamic_graph.context import no_nncf_trace\nfrom nncf.utils import is_main_process, in_scope_list, get_all_modules_by_type\nfrom nncf.nncf_logger import logger as nncf_logger\nfrom nncf.nncf_network import CompressionModuleType, NNCFNetwork\nfrom nncf.quantization.layers import QUANTIZATION_MODULES\nfrom nncf.quantization.hessian_trace import HessianTraceEstimator\n\n\nclass ManualPrecisionInitializer:\n    def __init__(self, algo: \'QuantizationController\', config: Config,\n                 default_activation_bitwidth: int, default_weight_bitwidth: int,\n                 criterion: _Loss, data_loader: DataLoader, is_distributed: bool = False):\n        self._algo = algo\n        self._model = self._algo._model  # type: NNCFNetwork\n        self._bitwidth_per_scope = config.get(\'bitwidth_per_scope\', {})  # type: List[List]\n        self._default_activation_bitwidth = default_activation_bitwidth\n        self._default_weight_bitwidth = default_weight_bitwidth\n        self._criterion = criterion\n        self._data_loader = data_loader\n        self._is_distributed = is_distributed\n\n        self._all_quantizations = {}\n        self._ordered_weight_quantizations = []\n        for class_type in QUANTIZATION_MODULES.registry_dict.values():\n            quantization_type = class_type.__name__\n            act_module_dict = self._model.get_compression_modules_by_type(\n                CompressionModuleType.ACTIVATION_QUANTIZER)\n            func_module_dict = self._model.get_compression_modules_by_type(CompressionModuleType.FUNCTION_QUANTIZER)\n            weight_module_dict = self._model.get_nncf_wrapped_model()\n            self._all_quantizations.update(get_all_modules_by_type(act_module_dict, quantization_type))\n            self._all_quantizations.update(get_all_modules_by_type(func_module_dict, quantization_type))\n            ops_quantizations = get_all_modules_by_type(weight_module_dict, quantization_type)\n            self._ordered_weight_quantizations.extend([q for q in ops_quantizations.values() if q.is_weights])\n            self._all_quantizations.update(ops_quantizations)\n\n    def apply_init(self):\n        for pair in self._bitwidth_per_scope:\n            if len(pair) != 2:\n                raise ValueError(\'Invalid format of bitwidth per scope: [int, str] is expected\')\n            bitwidth = pair[0]\n            scope_name = pair[1]\n            is_matched = False\n            for scope, quantizer in self._all_quantizations.items():\n                if in_scope_list(str(scope), scope_name):\n                    quantizer.num_bits = bitwidth\n                    is_matched = True\n            if not is_matched:\n                raise ValueError(\n                    \'Invalid scope name `{}`, failed to assign bitwidth {} to it\'.format(scope_name, bitwidth))\n\n\nclass HAWQPrecisionInitializer(ManualPrecisionInitializer):\n    def __init__(self, algo: \'QuantizationController\', config: Config,\n                 default_activation_bitwidth: int, default_weight_bitwidth: int, criterion: _Loss,\n                 data_loader: DataLoader, is_distributed: bool = False):\n        super().__init__(algo, config, default_activation_bitwidth, default_weight_bitwidth,\n                         criterion, data_loader, is_distributed)\n        self._traces_per_layer_path = config.get(\'traces_per_layer_path\', None)\n        self._num_data_points = config.get(\'num_data_points\', 200)\n        self._iter_number = config.get(\'iter_number\', 200)\n        self._tolerance = config.get(\'tolerance\', 1e-5)\n        self._bits = config.get(\'bits\', [4, 8])\n\n    def apply_init(self):\n        runner = HessianAwarePrecisionInitializeRunner(self._algo, self._model, self._data_loader,\n                                                       self._num_data_points,\n                                                       self._all_quantizations, self._ordered_weight_quantizations,\n                                                       self._bits, self._traces_per_layer_path)\n        runner.run(self._criterion, self._iter_number, self._tolerance)\n        self._model.rebuild_graph()\n        if self._is_distributed:\n            # NOTE: Order of quantization modules must be the same on GPUs to correctly broadcast num_bits\n            sorted_quantizers = OrderedDict(sorted(self._all_quantizations.items(), key=lambda x: str(x[0])))\n            for quantizer in sorted_quantizers.values():  # type: BaseQuantizer\n                quantizer.broadcast_num_bits()\n            if is_main_process():\n                str_bw = [str(element) for element in self.get_bitwidth_per_scope(sorted_quantizers)]\n                nncf_logger.info(\'\\n\'.join([\'\\n\\""bitwidth_per_scope\\"": [\', \',\\n\'.join(str_bw), \']\']))\n\n    def get_bitwidth_per_scope(self, sorted_quantizers: Dict[\'Scope\', nn.Module]) -> List[List[Union[int, str]]]:\n        full_bitwidth_per_scope = []\n        for scope, quantizer in sorted_quantizers.items():\n            override_weight_bitwidth = quantizer.is_weights and quantizer.num_bits != self._default_weight_bitwidth\n            override_act_bitwidth = not quantizer.is_weights and quantizer.num_bits != self._default_activation_bitwidth\n            if override_weight_bitwidth or override_act_bitwidth:\n                full_bitwidth_per_scope.append([quantizer.num_bits, str(scope)])\n        return full_bitwidth_per_scope\n\n\nclass PrecisionInitializerFactory:\n    @staticmethod\n    def create(init_type: str):\n        if init_type == ""manual"":\n            return ManualPrecisionInitializer\n        if init_type == ""hawq"":\n            return HAWQPrecisionInitializer\n        raise NotImplementedError\n\n\nclass PerturbationObserver:\n    def __init__(self, device):\n        super().__init__()\n        self.device = device\n        self.perturbation = None\n        self.numels = None\n\n    def calc_perturbation(self, module, inputs: torch.Tensor, output: torch.Tensor):\n        input_ = inputs[0] if isinstance(inputs, tuple) else inputs\n        with no_nncf_trace():\n            self.perturbation = torch.norm(input_ - output, p=2) ** 2\n            self.numels = input_.size().numel()\n            self.input_norm = torch.norm(input_, p=2) ** 2\n\n    def reset(self):\n        self.perturbation = None\n        self.numels = None\n\n    def get_observation(self):\n        return self.perturbation\n\n    def get_numels(self):\n        return self.numels\n\n    def get_input_norm(self):\n        return self.input_norm\n\n\nclass Perturbations:\n    def __init__(self):\n        self._perturbations = {}  # type: Dict[int, Dict[int, Tensor]]\n\n    def add(self, layer_id: int, bitwidth: int, perturbation: Tensor):\n        if layer_id in self._perturbations:\n            self._perturbations[layer_id].update({bitwidth: perturbation})\n        else:\n            self._perturbations[layer_id] = {bitwidth: perturbation}\n\n    def get(self, layer_id: int, bitwidth: int) -> Tensor:\n        layer_perturbations = self._perturbations[layer_id]\n        return layer_perturbations[bitwidth]\n\n    def get_all(self) -> Dict[int, Dict[int, Tensor]]:\n        return self._perturbations\n\n\nclass TracesPerLayer:\n    def __init__(self, traces_per_layer: Tensor):\n        self._traces_per_layer = traces_per_layer\n        self._traces_order = [i[0] for i in\n                              sorted(enumerate(traces_per_layer), reverse=False, key=lambda x: x[1])]\n\n    def get(self, index: int) -> Tensor:\n        return self._traces_per_layer[index]\n\n    def get_order_of_traces(self) -> List[int]:\n        return self._traces_order\n\n    def get_all(self) -> Tensor:\n        return self._traces_per_layer\n\n\nclass HessianAwarePrecisionInitializeRunner:\n    def __init__(self,\n                 algo: \'QuantizationController\',\n                 model: NNCFNetwork,\n                 data_loader: DataLoader,\n                 num_data_points: int,\n                 all_quantizations: Dict[\'Scope\', nn.Module],\n                 ordered_weight_quantizations: List[nn.Module],\n                 bits: List[int],\n                 traces_per_layer_path: str = \'\'):\n        super().__init__()\n        self._algo = algo\n        self._model = model\n        self._all_quantizations = all_quantizations\n        self._weights_to_init = ordered_weight_quantizations\n        self._bits = bits\n        self._traces_per_layer_path = traces_per_layer_path\n        self._device = next(self._model.parameters()).device\n        self._data_loader = data_loader\n        self._num_data_points = num_data_points\n\n    def run(self, criterion: _Loss, iter_number=200, tolerance=1e-5):\n        disabled_gradients = self.disable_quantizer_gradients(self._all_quantizations,\n                                                              self._algo.quantized_weight_modules_registry, self._model)\n\n        traces_per_layer = self._calc_traces(criterion, iter_number, tolerance)\n\n        self.enable_quantizer_gradients(self._model, self._all_quantizations, disabled_gradients)\n\n        num_weights = len(self._weights_to_init)\n        bits_configurations = self.get_constrained_configs(self._bits, num_weights)\n\n        perturbations, weight_observers = self.calc_quantization_noise()\n\n        configuration_metric = self.calc_hawq_metric_per_configuration(bits_configurations, perturbations,\n                                                                       traces_per_layer, self._device)\n\n        chosen_config_per_layer = self.choose_configuration(configuration_metric, bits_configurations,\n                                                            traces_per_layer.get_order_of_traces())\n        self.set_chosen_config(chosen_config_per_layer)\n        ordered_metric_per_layer = self.get_metric_per_layer(chosen_config_per_layer, perturbations, traces_per_layer)\n        if is_debug():\n            self.HAWQDump(bits_configurations, configuration_metric, perturbations,\n                          weight_observers, traces_per_layer, self._bits).run()\n        return ordered_metric_per_layer\n\n    @staticmethod\n    def disable_quantizer_gradients(all_quantizations: Dict[\'Scope\', nn.Module],\n                                    quantized_weight_modules_registry: Dict[\'Scope\', torch.nn.Module],\n                                    model: nn.Module) -> List[str]:\n        """"""\n        Disables gradients of all parameters, except for layers that have quantizers for weights.\n        :param all_quantizations: quantizers per scope\n        :param quantized_weight_modules_registry: quantizers for weights per scope\n        :param model: model to access all parameters\n        :return: list of names of the parameters that were originally disabled\n        """"""\n        for module in all_quantizations.values():\n            module.init_stage = True\n            module.disable_gradients()\n        # remember gradients of quantized modules that were enabled\n        gradients_to_enable = []\n        for quantized_module in quantized_weight_modules_registry.values():\n            for param_name, param in quantized_module.named_parameters():\n                if param.requires_grad:\n                    gradients_to_enable.append(param_name)\n        disabled_gradients = []\n        # disable all gradients, except already disabled\n        for param_name, param in model.named_parameters():\n            if not param.requires_grad:\n                disabled_gradients.append(param_name)\n            else:\n                param.requires_grad = False\n        # enable gradients of quantized modules that were disabled\n        for quantized_module in quantized_weight_modules_registry.values():\n            for param_name, param in quantized_module.named_parameters():\n                if param_name in gradients_to_enable and not \'bias\' in param_name:\n                    param.requires_grad = True\n        return disabled_gradients\n\n    def _calc_traces(self, criterion: _Loss, iter_number: int, tolerance: float) -> TracesPerLayer:\n        if self._traces_per_layer_path:\n            return TracesPerLayer(torch.load(self._traces_per_layer_path))\n\n        trace_estimator = HessianTraceEstimator(self._model, criterion, self._device, self._data_loader,\n                                                self._num_data_points)\n        avg_traces = trace_estimator.get_average_traces(max_iter=iter_number, tolerance=tolerance)\n        return TracesPerLayer(avg_traces)\n\n    @staticmethod\n    def enable_quantizer_gradients(model: nn.Module, all_quantizations: Dict[\'Scope\', nn.Module],\n                                   disabled_gradients: List):\n        """"""\n        Enables gradients of all parameters back, except for ones that were originally disabled\n        :param all_quantizations: quantizers per scope\n        :param model: model to access all parameters\n        :param disabled_gradients:  list of names of the parameters that were originally disabled\n        """"""\n        for param_name, param in model.named_parameters():\n            if param_name not in disabled_gradients:\n                param.requires_grad = True\n        for module in all_quantizations.values():\n            module.init_stage = False\n            module.enable_gradients()\n\n    @staticmethod\n    def get_constrained_configs(bits_: List[int], num_layers: int) -> List[List[int]]:\n        bits = sorted(bits_)\n        m = len(bits)\n        L = num_layers\n        bit_configs = []\n        for j in range(1, m + 1):\n            for combo_bits in itertools.combinations(bits, j):\n                for combo_partitions in itertools.combinations(list(range(1, L)), j - 1):\n                    bit_config = []\n                    prev_p = 0\n                    for (p, b) in zip(combo_partitions + (L,), combo_bits):\n                        bit_config += [b] * (p - prev_p)\n                        prev_p = p\n                    bit_configs.append(bit_config)\n        return bit_configs\n\n    def calc_quantization_noise(self) -> [Perturbations, List[PerturbationObserver]]:\n        hook_handles = []\n        observers = []\n        for i, module in enumerate(self._weights_to_init):\n            observer = PerturbationObserver(self._device)\n            hook_handles.append(module.register_forward_hook(observer.calc_perturbation))\n            observers.append(observer)\n\n        perturbations = Perturbations()\n        for b in self._bits:\n            for wi in self._weights_to_init:\n                wi.num_bits = b\n\n            self._model.do_dummy_forward(force_eval=True)\n\n            for i, observer in enumerate(observers):\n                perturbations.add(layer_id=i, bitwidth=b, perturbation=observer.get_observation())\n\n        for handle in hook_handles:\n            handle.remove()\n        return perturbations, observers\n\n    @staticmethod\n    def calc_hawq_metric_per_configuration(bits_configurations: List[List[int]], perturbations: Perturbations,\n                                           traces_per_layer: TracesPerLayer, device) -> List[Tensor]:\n        configuration_metric = []\n        for bits_config in bits_configurations:\n            hawq_metric = torch.Tensor([0]).to(device)\n            for i, layer_bits in enumerate(bits_config):\n                order = traces_per_layer.get_order_of_traces()[i]\n                hawq_metric += traces_per_layer.get(order) * perturbations.get(layer_id=order,\n                                                                               bitwidth=layer_bits)\n            configuration_metric.append(hawq_metric)\n        return configuration_metric\n\n    def choose_configuration(self, configuration_metric: List[Tensor], bits_configurations: List[List[int]],\n                             traces_order: List[int]) -> List[int]:\n        num_weights = len(traces_order)\n        ordered_config = [0] * num_weights\n        median_metric = torch.Tensor(configuration_metric).to(self._device).median()\n        configuration_index = configuration_metric.index(median_metric)\n        bit_configuration = bits_configurations[configuration_index]\n        for i, bitwidth in enumerate(bit_configuration):\n            ordered_config[traces_order[i]] = bitwidth\n        if is_main_process():\n            nncf_logger.info(\'Chosen HAWQ configuration (bitwidth per weightable layer)={}\'.format(ordered_config))\n            nncf_logger.debug(\'Order of the weightable layers in the HAWQ configuration={}\'.format(traces_order))\n        return ordered_config\n\n    def set_chosen_config(self, weight_bits_per_layer: List[int]):\n        for wq, bits in zip(self._weights_to_init, weight_bits_per_layer):\n            wq.num_bits = bits\n        pairs = self._algo.get_weights_activation_quantizers_pairs()\n        for pair in pairs:\n            wqs, aq = pair\n            aq.num_bits = max([wq.num_bits for wq in wqs])\n\n    def get_metric_per_layer(self, chosen_config_per_layer: List[int], perturbations: Perturbations,\n                             traces_per_layer: TracesPerLayer):\n        metric_per_layer = []\n        for i, layer_bits in enumerate(chosen_config_per_layer):\n            metric_per_layer.append(traces_per_layer.get(i) * perturbations.get(i, layer_bits))\n        ordered_metric_per_layer = [i[0] for i in\n                                    sorted(enumerate(metric_per_layer), reverse=True, key=lambda x: x[1])]\n        return ordered_metric_per_layer\n\n    class HAWQDump:\n        def __init__(self, bits_configurations: List[List[int]], configuration_metric: List[Tensor],\n                     perturbations: Perturbations, weight_observers: List[PerturbationObserver],\n                     traces_per_layer: TracesPerLayer, bits: List[int]):\n            self._bits_configurations = bits_configurations\n            self._configuration_metric = configuration_metric\n            self._num_weights = len(weight_observers)\n            self._perturbations = perturbations\n            self._weight_observers = weight_observers\n\n            self._dump_dir = ""hawq_dumps""\n            if os.path.exists(self._dump_dir):\n                shutil.rmtree(self._dump_dir)\n            os.makedirs(self._dump_dir, exist_ok=True)\n\n            self._traces_order = traces_per_layer.get_order_of_traces()\n            self._traces_per_layer = traces_per_layer.get_all()\n\n            num_of_weights = []\n            norm_of_weights = []\n            for i in range(self._num_weights):\n                order = self._traces_order[i]\n                num_of_weights.append(self._weight_observers[order].get_numels())\n                norm_of_weights.append(self._weight_observers[order].get_input_norm())\n            self._num_weights_per_layer = torch.Tensor(num_of_weights)\n            self._norm_weights_per_layer = torch.Tensor(norm_of_weights)\n\n            bits_in_megabyte = 2 ** 23\n            self._model_sizes = []\n            for bits_config in self._bits_configurations:\n                size = torch.sum(torch.Tensor(bits_config) * self._num_weights_per_layer).item() / bits_in_megabyte\n                self._model_sizes.append(size)\n            self._bits = bits\n\n        def run(self):\n            self._dump_avg_traces()\n            self._dump_density_of_quantization_noise()\n            self._dump_metric()\n            self._dump_perturbations_ratio()\n\n        def _dump_avg_traces(self):\n            import matplotlib.pyplot as plt\n            dump_file = os.path.join(self._dump_dir, \'avg_traces_per_layer\')\n            torch.save(self._traces_per_layer, dump_file)\n            fig = plt.figure()\n            fig.suptitle(\'Average Hessian Trace\')\n            ax = fig.add_subplot(2, 1, 1)\n            ax.set_yscale(\'log\')\n            ax.set_xlabel(\'weight quantizers\')\n            ax.set_ylabel(\'average hessian trace\')\n            ax.plot(self._traces_per_layer.cpu().numpy())\n            plt.savefig(dump_file)\n\n        def _dump_metric(self):\n            import matplotlib.pyplot as plt\n            list_to_plot = [cm.item() for cm in self._configuration_metric]\n            fig = plt.figure()\n            fig.suptitle(\'Pareto Frontier\')\n            ax = fig.add_subplot(2, 1, 1)\n            ax.set_yscale(\'log\')\n            ax.set_xlabel(\'Model Size (MB)\')\n            ax.set_ylabel(\'Metric value (total perturbation)\')\n            ax.scatter(self._model_sizes, list_to_plot, s=20, facecolors=\'none\', edgecolors=\'r\')\n            cm = torch.Tensor(self._configuration_metric)\n            cm_m = cm.median().item()\n            configuration_index = self._configuration_metric.index(cm_m)\n            ms_m = self._model_sizes[configuration_index]\n            ax.scatter(ms_m, cm_m, s=30, facecolors=\'none\', edgecolors=\'b\', label=\'median from all metrics\')\n            ax.legend()\n            plt.savefig(os.path.join(self._dump_dir, \'Pareto_Frontier\'))\n            nncf_logger.info(\'Distribution of HAWQ metrics: min_value={:.3f}, max_value={:.3f}, median_value={:.3f}, \'\n                             \'median_index={}, total_number={}\'.format(cm.min().item(), cm.max().item(), cm_m,\n                                                                       configuration_index,\n                                                                       len(self._configuration_metric)))\n\n        def _dump_density_of_quantization_noise(self):\n            noise_per_config = []  # type: List[Tensor]\n            for bits_config in self._bits_configurations:\n                qnoise = 0\n                for i in range(self._num_weights):\n                    layer_bits = bits_config[i]\n                    order = self._traces_order[i]\n                    qnoise += self._perturbations.get(layer_id=order, bitwidth=layer_bits)\n                noise_per_config.append(qnoise)\n\n            list_to_plot = [cm.item() for cm in noise_per_config]\n            import matplotlib.pyplot as plt\n            fig = plt.figure()\n            fig.suptitle(\'Density of quantization noise\')\n            ax = fig.add_subplot(2, 1, 1)\n            ax.set_yscale(\'log\')\n            ax.set_xlabel(\'Blocks\')\n            ax.set_ylabel(\'Noise value\')\n            ax.scatter(self._model_sizes, list_to_plot, s=20, alpha=0.3)\n            ax.legend()\n            plt.savefig(os.path.join(self._dump_dir, \'Density_of_quantization_noise\'))\n\n        def _dump_perturbations_ratio(self):\n            import matplotlib.pyplot as plt\n            fig = plt.figure()\n            fig.suptitle(\'Quantization noise vs Average Trace\')\n            ax = fig.add_subplot(2, 1, 1)\n            ax.set_xlabel(\'Blocks\')\n            ax.set_yscale(\'log\')\n            b = max(self._bits)\n            perturb = [p[b] for p in self._perturbations.get_all().values()]\n            ax.plot([p / m / n for p, m, n in zip(perturb, self._num_weights_per_layer, self._norm_weights_per_layer)],\n                    label=\'normalized {}-bit noise\'.format(b))\n            ax.plot(perturb, label=\'{}-bit noise\'.format(b))\n            ax.plot(self._traces_per_layer.cpu().numpy(), label=\'trace\')\n            ax.plot([n * p for n, p in zip(self._traces_per_layer.cpu(), perturb)], label=\'trace * noise\')\n            ax.legend()\n            plt.savefig(os.path.join(self._dump_dir, \'Quantization_noise_vs_Average_Trace\'))\n'"
pytorch_toolkit/nncf/nncf/quantization/init_range.py,0,"b'import queue\nfrom typing import List\n\nimport numpy as np\nimport torch\n\nfrom nncf.dynamic_graph.context import no_nncf_trace\nfrom .layers import BaseQuantizer\nfrom ..utils import get_flat_tensor_contents_string\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\n\nclass QuantizeRangeInitializer:\n    def __init__(self, quantize_module: BaseQuantizer):\n        self.quantize_module = quantize_module\n        self.device = next(self.quantize_module.parameters()).device\n        self.scale_shape = self.quantize_module.scale_shape\n\n    def register_input(self, x: torch.Tensor):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def forward_hook(self, module, input_, output):\n        return self.register_input(input_[0])\n\n    def apply_init(self):\n        raise NotImplementedError\n\n\ndef max_reduce_like(input_, ref_tensor_shape):\n    numel = np.prod(ref_tensor_shape)\n    if numel == 1:\n        return input_.max()\n    tmp_max = input_\n    for dim_idx, dim in enumerate(ref_tensor_shape):\n        if dim == 1:\n            tmp_max, _ = torch.max(tmp_max, dim_idx, keepdim=True)\n    return tmp_max\n\n\ndef min_reduce_like(input_, ref_tensor_shape):\n    numel = np.prod(ref_tensor_shape)\n    if numel == 1:\n        return input_.min()\n    tmp_min = input_\n    for dim_idx, dim in enumerate(ref_tensor_shape):\n        if dim == 1:\n            tmp_min, _ = torch.min(tmp_min, dim_idx, keepdim=True)\n    return tmp_min\n\n\ndef get_channel_count_and_dim_idx(scale_shape):\n    channel_dim_idx = 0\n    channel_count = 1\n    if not isinstance(scale_shape, int):\n        for dim_idx, dim in enumerate(scale_shape):\n            if dim != 1:\n                channel_dim_idx = dim_idx\n                channel_count = 1\n    return channel_count, channel_dim_idx\n\n\ndef split_into_channels(input_: np.ndarray, scale_shape) -> List[np.ndarray]:\n    channel_count, channel_dim_idx = get_channel_count_and_dim_idx(scale_shape)\n    channel_first_tensor = np.moveaxis(input_, channel_dim_idx, 0)\n    ret_list = []\n    for i in range(channel_count):\n        ret_list.append(channel_first_tensor[i, ...])\n    return ret_list\n\n\nclass MinMaxInitializer(QuantizeRangeInitializer):\n    def __init__(self, quantize_module: \'BaseQuantizer\', is_distributed,\n                 log_module_name: str = None):\n        super().__init__(quantize_module)\n        self.min_values = torch.ones(self.scale_shape).to(self.device) * np.inf\n        self.max_values = torch.ones(self.scale_shape).to(self.device) * (-np.inf)\n        self.is_distributed = is_distributed\n        self.log_module_name = log_module_name\n\n    def register_input(self, x: torch.Tensor):\n        with no_nncf_trace():\n            self.min_values = torch.min(min_reduce_like(x, self.scale_shape),\n                                        self.min_values)\n            self.max_values = torch.max(max_reduce_like(x, self.scale_shape),\n                                        self.max_values)\n\n    def reset(self):\n        self.min_values = torch.ones(self.scale_shape).to(self.device) * np.inf\n        self.max_values = torch.ones(self.scale_shape).to(self.device) * (-np.inf)\n\n    def apply_init(self):\n        nncf_logger.debug(""Statistics: min={} max={}"".format(get_flat_tensor_contents_string(self.min_values),\n                                                             get_flat_tensor_contents_string(self.max_values)))\n        self.quantize_module.apply_minmax_init(self.min_values, self.max_values, self.is_distributed,\n                                               self.log_module_name)\n\n\nclass MeanMinMaxInitializer(QuantizeRangeInitializer):\n    def __init__(self, quantize_module: \'BaseQuantizer\', is_distributed,\n                 log_module_name: str = None):\n        super().__init__(quantize_module)\n        self.is_distributed = is_distributed\n        self.log_module_name = log_module_name\n        self.all_min_values = []\n        self.all_max_values = []\n\n    def register_input(self, x: torch.Tensor):\n        with no_nncf_trace():\n            self.all_min_values.append(min_reduce_like(x, self.scale_shape))\n            self.all_max_values.append(max_reduce_like(x, self.scale_shape))\n\n    def reset(self):\n        self.all_min_values.clear()\n        self.all_max_values.clear()\n\n    def apply_init(self):\n        min_values = torch.ones(self.scale_shape).to(self.device) * (-np.inf)\n        max_values = torch.ones(self.scale_shape).to(self.device) * np.inf\n        if self.all_min_values:\n            stacked_min = torch.stack(self.all_min_values)\n            min_values = stacked_min.mean(dim=0).view(self.scale_shape)\n        if self.all_max_values:\n            stacked_max = torch.stack(self.all_max_values)\n            max_values = stacked_max.mean(dim=0).view(self.scale_shape)\n        nncf_logger.debug(""Statistics: min={} max={}"".format(get_flat_tensor_contents_string(min_values),\n                                                             get_flat_tensor_contents_string(max_values)))\n        self.quantize_module.apply_minmax_init(min_values, max_values, self.is_distributed,\n                                               self.log_module_name)\n\n\nclass ThreeSigmaInitializer(QuantizeRangeInitializer):\n    def __init__(self, quantize_module: \'BaseQuantizer\', is_distributed,\n                 log_module_name: str = None):\n        super().__init__(quantize_module)\n        self.input_history = queue.Queue()\n        self.is_distributed = is_distributed\n        self.log_module_name = log_module_name\n\n    def register_input(self, x: torch.Tensor):\n        with no_nncf_trace():\n            self.input_history.put(x.detach().cpu().numpy())\n\n    def reset(self):\n        self.input_history = queue.Queue()\n\n    def apply_init(self):\n        self.medians = torch.ones(self.scale_shape).to(self.device)\n        self.median_absolute_deviations = torch.ones(self.scale_shape).to(self.device)\n\n        channel_count, _ = get_channel_count_and_dim_idx(self.scale_shape)\n        per_channel_history = [None for i in range(channel_count)]\n        while not self.input_history.empty():\n            entry = self.input_history.get()\n            split = split_into_channels(entry, self.scale_shape)\n            for i in range(channel_count):\n                flat_channel_split = split[i].flatten()\n\n                # For post-RELU quantizers exact zeros may prevail and lead to\n                # zero mean and MAD - discard them\n                flat_channel_split = flat_channel_split[flat_channel_split != 0]\n\n                if per_channel_history[i] is None:\n                    per_channel_history[i] = flat_channel_split\n                else:\n                    per_channel_history[i] = np.concatenate([per_channel_history[i], flat_channel_split])\n        per_channel_median = [np.median(channel_hist) for channel_hist in per_channel_history]\n        per_channel_mad = []\n        for idx, median in enumerate(per_channel_median):\n            # Constant factor depends on the distribution form - assuming normal\n            per_channel_mad.append(1.4826 * np.median(abs(per_channel_history[idx] - median)))\n\n        numpy_median = np.asarray(per_channel_median)\n        numpy_mad = np.asarray(per_channel_mad)\n        median_tensor = torch.from_numpy(numpy_median).to(self.device, dtype=torch.float)\n        mad_tensor = torch.from_numpy(numpy_mad).to(self.device, dtype=torch.float)\n\n        nncf_logger.debug(""Statistics: median={} MAD={}"".format(get_flat_tensor_contents_string(median_tensor),\n                                                                get_flat_tensor_contents_string(mad_tensor)))\n        self.quantize_module.apply_minmax_init(median_tensor - 3 * mad_tensor, median_tensor + 3 * mad_tensor,\n                                               self.is_distributed,\n                                               self.log_module_name)\n'"
pytorch_toolkit/nncf/nncf/quantization/layers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom functools import partial\nfrom torch import distributed\n\nfrom nncf.debug import is_debug\nfrom nncf.functions import clamp\nfrom nncf.nncf_logger import logger as nncf_logger\nfrom .quantize_functions import symmetric_quantize, asymmetric_quantize\nfrom ..layer_utils import COMPRESSION_MODULES\nfrom ..registry import Registry\nfrom ..utils import get_per_channel_scale_shape, get_flat_tensor_contents_string\n\nQUANTIZATION_MODULES = Registry(\'quantization_modules\')\nINITIALIZABLE_MODULES = Registry(\'initializable_modules\')\n\n\nclass QuantizationMode:\n    SYMMETRIC = ""symmetric""\n    ASYMMETRIC = ""asymmetric""\n\n\nclass QuantizerConfig:\n    def __init__(self, bits=8,\n                 mode=QuantizationMode.SYMMETRIC,\n                 signedness_to_force=None,\n                 per_channel=False,\n                 input_shape=None,\n                 is_weights=False):\n        self.bits = bits\n        self.mode = mode\n        self.signedness_to_force = signedness_to_force\n        self.per_channel = per_channel\n        self.is_weights = is_weights\n        self.input_shape = input_shape\n        # TODO: add optional level_low and level_high setting to be parsed from HW config\n\n    def __eq__(self, other):\n        return self.__dict__ == other.__dict__\n\n    def __str__(self):\n        return ""B:{bits} M:{mode} SGN:{signedness} W:{is_weights} PC:{per_channel}"".format(\n            bits=self.bits,\n            mode=\'S\' if self.mode == QuantizationMode.SYMMETRIC else \'A\',\n            signedness=\'ANY\' if self.signedness_to_force is None else (\'S\' if self.signedness_to_force else \'U\'),\n            is_weights=\'Y\' if self.is_weights else \'N\',\n            per_channel=\'Y\' if self.per_channel else \'N\')\n\n    def __hash__(self):\n        return hash(str(self))\n\n    def __lt__(self, other):\n        return self.bits < other.bits or \\\n               (self.mode == QuantizationMode.SYMMETRIC and other.mode == QuantizationMode.ASYMMETRIC) or \\\n               (self.signedness_to_force is None and other.signedness_to_force is not None) or \\\n               (not self.per_channel and other.per_channel)\n\n\nclass BaseQuantizer(nn.Module):\n    def __init__(self, config: QuantizerConfig):\n        super().__init__()\n        self.input_shape = config.input_shape\n        self.per_channel = config.per_channel\n        self.is_weights = config.is_weights\n        self.signedness_to_force = config.signedness_to_force\n        self._num_bits = nn.Parameter(torch.IntTensor([config.bits]), requires_grad=False)\n\n        self.level_high = 0\n        self.level_low = 0\n        self.levels = 0\n\n        self.init_stage = False\n        self.initialized = False\n        self.state_dict_name = None\n        self.call_count = 0\n        self.scale_shape = 1\n\n        class LoadStateListener:\n            """"""\n               Check whether a quantization module are going to be updated by new values from state_dict or checkpoint.\n            """"""\n\n            def __init__(self, module):\n                # pylint: disable=protected-access\n                self.hook = module._register_load_state_dict_pre_hook(partial(self.hook_fn, module=module))\n\n            def hook_fn(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs,\n                        module):\n                if module.state_dict_name:\n                    for module_key in module.state_dict().keys():\n                        candidate = module.state_dict_name + \'.\' + module_key\n                        if candidate in state_dict:\n                            module.initialized = True\n\n            def close(self):\n                self.hook.remove()\n\n        self.load_listener = LoadStateListener(self)\n\n    def enable_gradients(self):\n        return NotImplementedError\n\n    def disable_gradients(self):\n        return NotImplementedError\n\n    def forward(self, x):\n        if is_debug():\n            self.call_count += 1\n        if self.init_stage:\n            return x\n        self.set_level_ranges()\n        return self.quantize(x)\n\n    def quantize(self, x):\n        raise NotImplementedError\n\n    def reset_call_counter(self):\n        self.call_count = 0\n\n    def get_trainable_params(self) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n\n    def apply_minmax_init(self, min_values, max_values, distributed_,\n                          log_module_name: str = None):\n        raise NotImplementedError\n\n    def set_level_ranges(self):\n        raise NotImplementedError\n\n    @property\n    def signed(self):\n        return NotImplementedError\n\n    @property\n    def num_bits(self):\n        return self._num_bits.item()\n\n    @num_bits.setter\n    def num_bits(self, num_bits: int):\n        self._num_bits.fill_(num_bits)\n\n    def broadcast_num_bits(self, src: int = 0):\n        distributed.broadcast(self._num_bits, src=src)\n\n\n@COMPRESSION_MODULES.register()\n@QUANTIZATION_MODULES.register(QuantizationMode.SYMMETRIC)\nclass SymmetricQuantizer(BaseQuantizer):\n    SCALE_PARAM_NAME = \'scale\'\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.signed_tensor = nn.Parameter(torch.IntTensor([0]), requires_grad=False)\n        self.collect_scale_statistics = False\n        if self.per_channel:\n            self.scale_shape = get_per_channel_scale_shape(self.input_shape, self.is_weights)\n        self.scale = nn.Parameter(torch.ones(self.scale_shape), requires_grad=True)\n        self.eps = 1e-16\n\n    def enable_gradients(self):\n        self.scale.requires_grad = True\n\n    def disable_gradients(self):\n        self.scale.requires_grad = False\n\n    def set_level_ranges(self):\n        if self.signed:\n            self.level_high = 2 ** (self.num_bits - 1) - 1\n            self.level_low = -(self.level_high + 1)\n            if self.is_weights:\n                self.level_low += 1\n        else:\n            self.level_high = 2 ** self.num_bits - 1\n            self.level_low = 0\n        self.levels = 2 ** self.num_bits\n        if self.is_weights:\n            self.levels -= 1\n\n    @property\n    def signed(self):\n        return self.signed_tensor.item() == 1\n\n    @signed.setter\n    def signed(self, signed: bool):\n        self.signed_tensor.fill_(signed)\n\n    def quantize(self, x):\n        return symmetric_quantize(x, self.levels, self.level_low, self.level_high, self.scale, self.eps)\n\n    def get_trainable_params(self) -> Dict[str, torch.Tensor]:\n        return {self.SCALE_PARAM_NAME: self.scale.detach()}\n\n    def apply_minmax_init(self, min_values, max_values, distributed_, log_module_name: str = None):\n        if self.initialized:\n            nncf_logger.debug(""Skipped initializing {} - loaded from checkpoint"".format(log_module_name))\n            return\n        if torch.any(torch.eq(min_values, np.inf)) or torch.any(torch.eq(max_values, -np.inf)):\n            raise AttributeError(\'Statistics is not collected for {}\'.format(log_module_name))\n        sign = torch.any(torch.lt(min_values, 0))\n        if self.signedness_to_force is not None and sign != self.signedness_to_force:\n            nncf_logger.warning(""Forcing signed to {} for module {}"".format(self.signedness_to_force, log_module_name))\n            sign = self.signedness_to_force\n        self.signed = int(sign)\n\n        abs_max = torch.max(torch.abs(max_values), torch.abs(min_values))\n        SCALE_LOWER_THRESHOLD = 0.1\n        self.scale.fill_(SCALE_LOWER_THRESHOLD)\n        self.scale.masked_scatter_(torch.gt(abs_max, SCALE_LOWER_THRESHOLD), abs_max)\n\n        if distributed_:\n            distributed.broadcast(self.scale, 0)\n            distributed.broadcast(self.signed_tensor, 0)\n\n        nncf_logger.info(\n            ""Set sign: {} and scale: {} for {}"".format(self.signed,\n                                                       get_flat_tensor_contents_string(self.scale),\n                                                       log_module_name))\n\n\n@COMPRESSION_MODULES.register()\n@QUANTIZATION_MODULES.register(QuantizationMode.ASYMMETRIC)\nclass AsymmetricQuantizer(BaseQuantizer):\n    INPUT_LOW_PARAM_NAME = \'input_low\'\n    INPUT_RANGE_PARAM_NAME = \'input_range\'\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.scale_shape = 1\n        if self.per_channel:\n            self.scale_shape = get_per_channel_scale_shape(self.input_shape, self.is_weights)\n\n        self.input_low = nn.Parameter(torch.zeros(self.scale_shape), requires_grad=True)\n        self.input_range = nn.Parameter(torch.ones(self.scale_shape), requires_grad=True)\n        self.eps = 1e-16\n\n    def enable_gradients(self):\n        self.input_low.requires_grad = True\n        self.input_range.requires_grad = True\n\n    def disable_gradients(self):\n        self.input_low.requires_grad = False\n        self.input_range.requires_grad = False\n\n    @property\n    def signed(self):\n        return True\n\n    def set_level_ranges(self):\n        self.level_high = 2 ** self.num_bits - 1\n        self.level_low = 0\n        self.levels = 2 ** self.num_bits\n\n    def quantize(self, x):\n        return asymmetric_quantize(x, self.levels, self.level_low, self.level_high, self.input_low, self.input_range,\n                                   self.eps)\n\n    def get_trainable_params(self) -> Dict[str, torch.Tensor]:\n        return {self.INPUT_LOW_PARAM_NAME: self.input_low.detach(),\n                self.INPUT_RANGE_PARAM_NAME: self.input_range.detach()}\n\n    def apply_minmax_init(self, min_values, max_values, distributed_, log_module_name: str = None):\n        if self.initialized:\n            nncf_logger.debug(""Skipped initializing {} - loaded from checkpoint"".format(log_module_name))\n            return\n        if torch.any(torch.eq(min_values, np.inf)) or torch.any(torch.eq(max_values, -np.inf)):\n            raise AttributeError(\'Statistics is not collected for {}\'.format(log_module_name))\n        ranges = max_values - min_values\n        max_range = torch.max(max_values - min_values)\n        eps = 1e-2\n        correction = (clamp(ranges, low=eps * max_range, high=max_range) - ranges) * 0.5\n        self.input_range.data = (ranges + 2 * correction).data\n        self.input_low.data = (min_values - correction).data\n\n        if distributed_:\n            distributed.broadcast(self.input_low, 0)\n            distributed.broadcast(self.input_range, 0)\n        nncf_logger.info(""Set input_low: {} and input_range: {} for {}""\n                         .format(get_flat_tensor_contents_string(self.input_low),\n                                 get_flat_tensor_contents_string(self.input_range), log_module_name))\n'"
pytorch_toolkit/nncf/nncf/quantization/quantize_functions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport torch\nimport warnings\n\nfrom .extensions import QuantizedFunctionsCPU, QuantizedFunctionsCUDA\nfrom ..dynamic_graph.patch_pytorch import register_operator\nfrom ..functions import STRound, clamp\nfrom ..utils import is_tracing_state, no_jit_trace\n\n\nclass QuantizeSymmetric(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_, scale, level_low, level_high, levels):\n        input_low = scale * (level_low / level_high)\n        input_range = scale - input_low\n\n        if input_.is_cuda:\n            if not input_.is_contiguous():\n                warnings.warn(""input_ is not contiguous!"", RuntimeWarning)\n                input_ = input_.contiguous()\n            output = QuantizedFunctionsCUDA.Quantize_forward(input_, input_low, input_range, levels)\n        else:\n            output = QuantizedFunctionsCPU.Quantize_forward(input_, input_low, input_range, levels)\n\n        ctx.save_for_backward(input_, input_low, input_range)\n        ctx.levels = levels\n        ctx.level_low = level_low\n        ctx.level_high = level_high\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_, input_low, input_range = ctx.saved_tensors\n        levels = ctx.levels\n        level_low = ctx.level_low\n        level_high = ctx.level_high\n\n        if grad_output.is_cuda:\n            if not grad_output.is_contiguous():\n                warnings.warn(""grad_output is not contiguous!"", RuntimeWarning)\n                grad_output = grad_output.contiguous()\n\n            grad_input, _, grad_scale = QuantizedFunctionsCUDA.Quantize_backward(\n                grad_output, input_, input_low, input_range, levels, level_low, level_high\n            )\n        else:\n            grad_input, _, grad_scale = QuantizedFunctionsCPU.Quantize_backward(\n                grad_output, input_, input_low, input_range, levels, level_low, level_high, False\n            )\n\n        return grad_input, grad_scale, None, None, None\n\n\nclass QuantizeAsymmetric(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_, input_low, input_range, level_low, level_high, levels):\n        if input_.is_cuda:\n            if not input_.is_contiguous():\n                warnings.warn(""input_ is not contiguous!"", RuntimeWarning)\n                input_ = input_.contiguous()\n            output = QuantizedFunctionsCUDA.Quantize_forward(input_, input_low, input_range, levels)\n        else:\n            output = QuantizedFunctionsCPU.Quantize_forward(input_, input_low, input_range, levels)\n\n        ctx.save_for_backward(input_, input_low, input_range)\n        ctx.levels = levels\n        ctx.level_low = level_low\n        ctx.level_high = level_high\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_, input_low, input_range = ctx.saved_tensors\n        levels = ctx.levels\n        level_low = ctx.level_low\n        level_high = ctx.level_high\n\n        if grad_output.is_cuda:\n            if not grad_output.is_contiguous():\n                warnings.warn(""grad_output is not contiguous!"", RuntimeWarning)\n                grad_output = grad_output.contiguous()\n\n            grad_input, grad_input_low, grad_input_range = QuantizedFunctionsCUDA.Quantize_backward(\n                grad_output, input_, input_low, input_range, levels, level_low, level_high\n            )\n        else:\n            grad_input, grad_input_low, grad_input_range = QuantizedFunctionsCPU.Quantize_backward(\n                grad_output, input_, input_low, input_range, levels, level_low, level_high, True\n            )\n\n        return grad_input, grad_input_low, grad_input_range, None, None, None\n\n\ndef _quantize_autograd_to_range(input_, input_low, input_high, levels):\n    input_ = input_ - input_low\n    input_range = (input_high - input_low)\n    scale = (levels - 1) / input_range\n    output = clamp(input_, low=input_.new_zeros(input_.shape), high=input_range)\n    output = output * scale\n    output = STRound.apply(output)\n    output = output * input_range / (levels - 1) + input_low\n    return output\n\n\nclass ExportQuantize(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, input_, levels, input_low, input_high, output_low, output_high):\n        return g.op(""FakeQuantize"", input_, input_low, input_high, output_low, output_high, levels_i=levels)\n\n    @staticmethod\n    def forward(ctx, input_, levels, input_low, input_high, output_low, output_high):\n        output = _quantize_autograd_to_range(input_, input_low, input_high, levels)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # backward is not used during export\n        return grad_output\n\n\n@register_operator()\ndef symmetric_quantize(input_, levels, level_low, level_high, scale, eps):\n    if is_tracing_state():\n        with no_jit_trace():\n            input_range = abs(scale) + eps\n            # todo: take bias into account during input_low/input_high calculation\n            input_low = input_range * level_low / level_high\n            input_high = input_range\n        return ExportQuantize.apply(input_, levels, input_low, input_high, input_low, input_high)\n    scale_safe = abs(scale) + eps\n    return QuantizeSymmetric.apply(input_, scale_safe, level_low, level_high, levels)\n\n\n@register_operator()\ndef asymmetric_quantize(input_, levels, level_low, level_high, input_low, input_range, eps):\n    if is_tracing_state():\n        with no_jit_trace():\n            input_range_safe = abs(input_range) + eps\n            input_low_tuned, input_range_tuned = TuneRange.apply(input_low, input_range_safe, levels)\n            input_high_tuned = input_low_tuned + input_range_tuned\n        return ExportQuantize.apply(input_, levels, input_low_tuned, input_high_tuned, input_low_tuned,\n                                    input_high_tuned)\n    input_range_safe = abs(input_range) + eps\n    input_low_tuned, input_range_tuned = TuneRange.apply(input_low, input_range_safe, levels)\n    return QuantizeAsymmetric.apply(input_, input_low_tuned, input_range_tuned, level_low, level_high, levels)\n\n\nclass TuneRange(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_low, input_range, levels):\n        input_high = input_range + input_low\n        input_low_copy = input_low.clone()\n        input_low_copy[input_low_copy > 0] = 0\n        input_high[input_high < 0] = 0\n        n = levels - 1\n        scale = levels / (input_high - input_low_copy)\n        zp = torch.round(-input_low_copy * scale)\n\n        new_input_low = torch.where(zp < n, zp / (zp - n) * input_high, input_low_copy)\n        new_input_high = torch.where(zp > 0., (zp - n) / zp * input_low_copy, input_high)\n\n        range_1 = input_high - new_input_low\n        range_2 = new_input_high - input_low_copy\n\n        mask = (range_1 > range_2).to(input_high.dtype)\n        inv_mask = (1 - mask).abs()\n\n        new_input_low = mask * new_input_low + inv_mask * input_low_copy\n        new_input_range = inv_mask * new_input_high + mask * input_high - new_input_low\n\n        return new_input_low, new_input_range\n\n    @staticmethod\n    def backward(ctx, grad_input_low, grad_input_range):\n        return grad_input_low, grad_input_range, None\n'"
pytorch_toolkit/nncf/nncf/quantization/quantizer_propagation.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport warnings\nfrom collections import deque\nfrom copy import deepcopy\nfrom enum import Enum\nfrom typing import Dict, Tuple\n\nimport networkx as nx\n\nfrom nncf.dynamic_graph.graph import OperationExecutionContext\n# pylint: disable=wildcard-import\n# pylint: disable=unused-wildcard-import\nfrom nncf.dynamic_graph.operator_metatypes import *\nfrom nncf.dynamic_graph.operator_metatypes import OPERATOR_METATYPES\nfrom nncf.nncf_network import InsertionInfo, InsertionType, InsertionPointGraph, InsertionPointGraphNodeType\nfrom nncf.quantization.layers import QuantizerConfig, QuantizationMode\n\n\nclass QuantizationTrait(Enum):\n    """"""General, hardware-agnostic specifications for the relation of operators to quantization.\n    Hardware-specific quantization configuration is handled elsewhere.""""""\n    NON_QUANTIZABLE = -1\n    QUANTIZATION_AGNOSTIC = 0\n    INPUTS_QUANTIZABLE = 1\n\n\nDEFAULT_QUANT_TRAIT_TO_OP_DICT = {\n    QuantizationTrait.INPUTS_QUANTIZABLE: [\n        Conv2dMetatype,\n        Conv3dMetatype,\n        ConvTranspose2dMetatype,\n        ConvTranspose3dMetatype,\n        LinearMetatype,\n        HardTanhMetatype,\n        TanhMetatype,\n        ELUMetatype,\n        PRELUMetatype,\n        LayerNormMetatype,\n        GELUMetatype,\n        SigmoidMetatype,\n        AddMetatype,\n        MulMetatype,\n        DivMetatype,\n        ExpMetatype,\n        ErfMetatype,\n        MatMulMetatype,\n        MeanMetatype,\n        RoundMetatype\n    ],\n    QuantizationTrait.NON_QUANTIZABLE: [\n        EmbeddingMetatype,\n        SoftmaxMetatype\n    ]\n}  # type: Dict[QuantizationTrait, List[OperatorMetatype]]\n\n\nclass PropagatingQuantizer:\n    """"""Used in conjunction with QuantizerPropagationStateGraph to keep track of\n       the allowed quantization configs corresponding to the model operation node\n       whose inputs it quantizes, and also of the nodes/edges in the model control\n       graph that this quantizer affects. It should be moved against the data flow of\n       the model, tracking the affected nodes and edges of\n       QuantizerPropagationStateGraph. No actual quantization modules are used here,\n       only the associated configs (such as bitwidths, modes, signed/unsigned\n       attributes etc.)""""""\n    def __init__(self, id_: int, quant_configs: List[QuantizerConfig], init_location_node_key: str):\n        self._potential_quant_configs = quant_configs  # type: List[QuantizerConfig]\n        self.affected_edges = set()\n        self.affected_ip_nodes = set()\n        self.propagation_path = []\n        self.current_location_node_key = init_location_node_key\n        self.last_accepting_location_node_key = None\n        self.id = id_\n\n    def __eq__(self, other):\n        return self.id == other.id\n\n    @property\n    def potential_quant_configs(self) -> List[QuantizerConfig]:\n        return self._potential_quant_configs\n\n\nclass TransitionStatus(Enum):\n    SHOULD_TRANSITION = 0\n    SHOULD_MERGE = 1\n    SHOULD_NOT_TRANSITION = 2\n\n\nclass PropagationStrategy(Enum):\n    CONSERVATIVE = 0  # While propagating up through a downward-branching node,\n                      # do not propagate if the propagation results in narrowing the list of\n                      # quantization variants available to quantizers on neighbouring branches\n    AGGRESSIVE = 1\n\n\nQuantizerPropagationStateGraphNodeType = InsertionPointGraphNodeType\n\n\nclass QuantizerPropagationStateGraph(nx.DiGraph):\n    """"""This class is based upon InsertionPointGraph and represents\n       a""chessboard"" for PropagatingQuantizer items.  It tracks the current state of\n       quantizer propagation by associating the operator and insertion point nodes and\n       edges to propagating quantizers, if any. It can move a propagating quantizer\n       via own edges and mark its progress through the graph, which is required for\n       resolving situations when multiple quantizers attempt to proceed via one and\n       the same graph node/edge. This class is mainly operated upon by the\n       QuantizerPropagationSolver objects.""""""\n    PROPAGATING_QUANTIZER_NODE_ATTR = ""propagating_quantizer""\n    AFFECTING_PROPAGATING_QUANTIZERS_ATTR = ""affecting_propagating_quantizers""\n    QUANTIZATION_TRAIT_NODE_ATTR = ""quantization_trait""\n    ALLOWED_INPUT_QUANTIZATION_TYPES_NODE_ATTR = ""allowed_input_quantization_types""\n    OPERATOR_METATYPE_NODE_ATTR = ""op_meta""\n    INSERTION_POINT_DATA_NODE_ATTR = ""insertion_point""\n    NODE_TYPE_NODE_ATTR = ""node_type""\n\n    def __init__(self, ip_graph: InsertionPointGraph):\n        super().__init__()\n        ip_graph = deepcopy(ip_graph)\n        self._created_prop_quantizer_counter = 0\n\n        for node_key, node in ip_graph.nodes.items():\n            qpg_node = {\n                self.NODE_TYPE_NODE_ATTR: node[InsertionPointGraph.NODE_TYPE_NODE_ATTR]}\n            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.INSERTION_POINT:\n                qpg_node[self.PROPAGATING_QUANTIZER_NODE_ATTR] = None\n                qpg_node[self.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] = []\n                qpg_node[self.INSERTION_POINT_DATA_NODE_ATTR] = node[\n                    InsertionPointGraph.INSERTION_POINT_DATA_NODE_ATTR]\n            elif node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:\n                qpg_node[self.ALLOWED_INPUT_QUANTIZATION_TYPES_NODE_ATTR] = set()\n                qpg_node[\n                    self.QUANTIZATION_TRAIT_NODE_ATTR] = QuantizationTrait.NON_QUANTIZABLE\n                qpg_node[self.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] = []\n                qpg_node[self.OPERATOR_METATYPE_NODE_ATTR] = node[InsertionPointGraph.OPERATOR_METATYPE_NODE_ATTR]\n            self.add_node(node_key, **qpg_node)\n\n        for from_node, to_node, edge_data in ip_graph.edges(data=True):\n            edge_data[self.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] = []\n            self.add_edge(from_node, to_node, **edge_data)\n\n    def merge_quantizer_into_path(self, prop_quantizer: PropagatingQuantizer, path: List):\n        curr_node = self.nodes[prop_quantizer.current_location_node_key]\n        curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = None\n        surviving_quantizers = []  # type: List[PropagatingQuantizer]\n        for from_node_key, to_node_key in path:\n            edge = self.edges[from_node_key, to_node_key]\n            potential_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n            if potential_quantizers:\n                surviving_quantizers = potential_quantizers\n                break\n            from_node = self.nodes[from_node_key]\n            potential_quantizer = from_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]\n            if potential_quantizer is None:\n                if from_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]:\n                    potential_quantizer = \\\n                        from_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR][0]\n\n            if potential_quantizer is not None:\n                prop_quantizer.affected_edges.add((from_node_key, to_node_key))\n                edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(prop_quantizer)\n                surviving_quantizers.append(potential_quantizer)\n                break\n\n        if surviving_quantizers:\n            for pq in surviving_quantizers:\n                pq.affected_edges.update(prop_quantizer.affected_edges)\n                for from_node_key, to_node_key in prop_quantizer.affected_edges:\n                    from_node = self.nodes[from_node_key]\n                    from_node_type = from_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n                    if from_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                        # pylint:disable=line-too-long\n                        self.nodes[from_node_key][QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(pq)\n            for affected_edge_tuple in prop_quantizer.affected_edges:\n                edge = self.edges[affected_edge_tuple]\n                affecting_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n                for pq in surviving_quantizers:\n                    affecting_quantizers.append(pq)\n            self.remove_propagating_quantizer(prop_quantizer)\n        else:\n            raise RuntimeError(""Not found surviving_quantizers!""\n                               "" Nodes quantized with quantizer #{} will be lost"".format(prop_quantizer.id))\n\n    def backtrack_propagation_until_accepting_location(self, prop_quantizer: PropagatingQuantizer) -> Optional[\n            PropagatingQuantizer]:\n        if prop_quantizer.last_accepting_location_node_key is None:\n            # The quantizer was stillborn.\n            self.remove_propagating_quantizer(prop_quantizer)\n            return None\n\n        curr_node_key = prop_quantizer.current_location_node_key\n        curr_node = self.nodes[curr_node_key]\n        curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = None\n        while prop_quantizer.current_location_node_key != prop_quantizer.last_accepting_location_node_key:\n            from_node_key, to_node_key = prop_quantizer.propagation_path.pop()\n\n            edge = self.edges[from_node_key, to_node_key]\n            edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].remove(prop_quantizer)\n            prop_quantizer.affected_edges.remove((from_node_key, to_node_key))\n            from_node = self.nodes[from_node_key]\n            from_node_type = from_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if from_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                from_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].remove(prop_quantizer)\n                prop_quantizer.affected_ip_nodes.remove(from_node_key)\n\n            to_node = self.nodes[to_node_key]\n            to_node_type = to_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if to_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                prop_quantizer.current_location_node_key = to_node_key\n\n        target_ip_node_key = prop_quantizer.current_location_node_key\n        target_node = self.nodes[target_ip_node_key]\n        target_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = prop_quantizer\n        return prop_quantizer\n\n    def add_propagating_quantizer(self, qconf_list: List[QuantizerConfig], ip_node_key: str) -> PropagatingQuantizer:\n        prop_quantizer = PropagatingQuantizer(self._get_next_prop_quantizer_id(), qconf_list, ip_node_key)\n\n        ip_node = self.nodes[ip_node_key]\n        ip_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = prop_quantizer\n        ip_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(prop_quantizer)\n\n        ip_type = ip_node[QuantizerPropagationStateGraph.INSERTION_POINT_DATA_NODE_ATTR].insertion_type\n\n        if ip_type != InsertionType.OPERATOR_PRE_HOOK:\n            # The insertion point key should immediately precede a quantizable op,\n            # otherwise it is hard to determine affected node here (although possible)\n            raise RuntimeError(""Can only add propagating quantizers into pre-hook spots!"")\n\n        affected_op_node_key = next(self.successors(ip_node_key))\n        affected_op_node = self.nodes[affected_op_node_key]\n\n        affected_op_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(prop_quantizer)\n\n        initial_edge_key = (ip_node_key, affected_op_node_key)\n        initial_edge = self.edges[initial_edge_key]\n        initial_edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(prop_quantizer)\n        prop_quantizer.affected_edges.add(initial_edge_key)\n        prop_quantizer.affected_ip_nodes.add(ip_node_key)\n        return prop_quantizer\n\n    def clone_propagating_quantizer(self, prop_quantizer: PropagatingQuantizer) -> PropagatingQuantizer:\n        cloned_prop_quant = deepcopy(prop_quantizer)\n        cloned_prop_quant.id = self._get_next_prop_quantizer_id()\n        for edge_tuple in cloned_prop_quant.affected_edges:\n            edge = self.edges[edge_tuple]\n            edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(cloned_prop_quant)\n        for node_key in cloned_prop_quant.affected_ip_nodes:\n            node = self.nodes[node_key]\n            node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(cloned_prop_quant)\n        return cloned_prop_quant\n\n    def remove_propagating_quantizer(self, prop_quantizer: PropagatingQuantizer):\n        for edge_tuple in prop_quantizer.affected_edges:\n            edge = self.edges[edge_tuple]\n            affecting_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n            affecting_quantizers.remove(prop_quantizer)\n        for node_key in prop_quantizer.affected_ip_nodes:\n            node = self.nodes[node_key]\n            affecting_quantizers = node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n            affecting_quantizers.remove(prop_quantizer)\n\n        node_key = prop_quantizer.current_location_node_key\n        self.nodes[node_key][QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = None\n        prop_quantizer.affected_ip_nodes.clear()\n        prop_quantizer.affected_edges.clear()\n\n    def propagate_quantizer_via_path(self, prop_quantizer: PropagatingQuantizer, path: List) -> PropagatingQuantizer:\n        curr_node_key = prop_quantizer.current_location_node_key\n        curr_node = self.nodes[curr_node_key]\n        existing_quantizer = curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]\n        if existing_quantizer is not None and existing_quantizer.id == prop_quantizer.id:\n            curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = None\n        for edge_tuple in path:\n            edge = self.edges[edge_tuple]\n            edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(prop_quantizer)\n            prop_quantizer.affected_edges.add(edge_tuple)\n            prop_quantizer.propagation_path.append(edge_tuple)\n            from_node_key = edge_tuple[0]\n            from_node = self.nodes[from_node_key]\n            from_node_type = from_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if from_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                from_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(prop_quantizer)\n                prop_quantizer.affected_ip_nodes.add(from_node_key)\n                if self._is_position_accepting(from_node_key):\n                    prop_quantizer.last_accepting_location_node_key = from_node_key\n\n        target_ip_node_key = path[-1][0]\n        prop_quantizer.current_location_node_key = target_ip_node_key\n        target_node = self.nodes[target_ip_node_key]\n        target_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = prop_quantizer\n        return prop_quantizer\n\n    def get_quantizable_op_nodes_immediately_dominated_by_node(self, node_key) -> List[str]:\n        ret_node_key_list = []\n\n        def recursive_helper(curr_node_key: str, target_node_list: List[str]):\n            successors = self.successors(curr_node_key)\n            for successor_key in successors:\n                successor = self.nodes[successor_key]\n                successor_node_type = successor[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n                if successor_node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:\n                    trait = successor[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]\n                    if not trait == QuantizationTrait.QUANTIZATION_AGNOSTIC:\n                        target_node_list.append(successor_key)\n                        return\n                recursive_helper(successor_key, target_node_list)\n\n        recursive_helper(node_key, ret_node_key_list)\n        return ret_node_key_list\n\n    def get_paths_to_immediately_dominating_insertion_points(self, insertion_point_node_key: str) -> List[List]:\n        """"""Paths are lists of edges.""""""\n        paths = []\n\n        def recursive_helper(curr_edge, curr_path, all_paths):\n            curr_path.append(curr_edge)\n            curr_node_key = curr_edge[0]\n            curr_node = self.nodes[curr_node_key]\n            curr_node_type = curr_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if curr_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                all_paths.append(curr_path)\n                return\n\n            for in_edge in self.in_edges(curr_node_key):\n                path_copy = deepcopy(curr_path)\n                recursive_helper(in_edge, path_copy, all_paths)\n\n        for in_edge in self.in_edges(insertion_point_node_key):\n            recursive_helper(in_edge, [], paths)\n        return paths\n\n    def get_visualized_graph(self):\n        out_graph = nx.DiGraph()\n        for node_key, node in self.nodes.items():\n            node_type = node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                insertion_point_data = node[\n                    QuantizerPropagationStateGraph.INSERTION_POINT_DATA_NODE_ATTR]  # type: InsertionPoint\n                label = ""IP: {}"".format(insertion_point_data.insertion_type)\n                out_graph.add_node(node_key, label=label, color=""red"")\n                if node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] is not None:\n                    prop_quantizer = node[\n                        QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]  # type: PropagatingQuantizer\n                    quant_node_key = ""Quantizer #{}"".format(prop_quantizer.id)\n                    quant_configs_str_list = [str(conf) for conf in prop_quantizer.potential_quant_configs]\n                    sub_label = \'[\' + \',\\n\'.join(quant_configs_str_list) + \']\'\n                    quant_node_label = quant_node_key + \'\\n\' + ""T: {}\\n"".format(sub_label)\n                    out_graph.add_node(quant_node_key,\n                                       color=""blue"", label=quant_node_label)\n                    out_graph.add_edge(quant_node_key, node_key,\n                                       style=""dashed"")\n            elif node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:\n                out_graph.add_node(node_key)\n            else:\n                raise RuntimeError(""Invalid QuantizerPropagationStateGraph node!"")\n        for u, v in self.edges:\n            edge = self.edges[u, v]\n            attrs = {}\n            affecting_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n            if affecting_quantizers:\n                label = "", "".join([str(pq.id) for pq in affecting_quantizers])\n                attrs = {""color"": ""blue"", ""label"": label}\n            out_graph.add_edge(u, v, **attrs)\n        return out_graph\n\n    def _get_next_prop_quantizer_id(self):\n        self._created_prop_quantizer_counter += 1\n        return self._created_prop_quantizer_counter\n\n    def _is_position_accepting(self, ip_node_key: str):\n        node = self.nodes[ip_node_key]\n        insertion_type = node[QuantizerPropagationStateGraph.INSERTION_POINT_DATA_NODE_ATTR].insertion_type\n        if insertion_type == InsertionType.OPERATOR_POST_HOOK:\n            return True\n        return False\n\n\nclass QuantizerPropagationSolver:\n    """"""Analyzes a fresh QuantizerPropagationStateGraph object according to HW\n       configuration supplied in the initializer and produces the list of insertion\n       commands that correspond to the final state of the quantizer propagation graph\n       when the model has the most contol flow graph edges quantized according to HW\n       capabilities.""""""\n\n    DEFAULT_QUANTIZATION_TYPES = [QuantizerConfig(\n        bits=8,\n        mode=QuantizationMode.SYMMETRIC,\n        signedness_to_force=None,\n        per_channel=False)]\n\n    def __init__(self, hw_config=None,\n                 debug_interface: \'QuantizationDebugInterface\' = None,\n                 propagation_strategy: PropagationStrategy = PropagationStrategy.AGGRESSIVE):\n        self._hw_config = hw_config\n        self._debug_interface = debug_interface\n        self._propagation_strategy = propagation_strategy  # TODO: determine from config\n        self._operator_quantization_trait_map = self.get_operator_quantization_traits_map()\n        self._operator_allowed_qconfigs_map = self._get_operator_qconfigs_map()\n        self._active_propagating_quantizers_queue = deque()\n        self._finished_propagating_quantizers = []  # type: List[PropagatingQuantizer]\n\n    def run_on_ip_graph(self, ip_graph: InsertionPointGraph) -> Dict[InsertionInfo, Optional[List[QuantizerConfig]]]:\n        """""" The main function to be used on an InsertionPointGraph to produce\n            the list of insertion commands and configs corresponding to the final quantized\n            graph state.""""""\n        quant_prop_graph = QuantizerPropagationStateGraph(ip_graph)\n        quant_prop_graph = self.set_allowed_quantization_types_for_operator_nodes(quant_prop_graph)\n        quant_prop_graph = self.setup_initial_quantizers(quant_prop_graph)\n        iteration_counter = 0\n        while self._active_propagating_quantizers_queue:\n            prop_quantizer = self._active_propagating_quantizers_queue.pop()\n            if self._debug_interface is not None:\n                self._debug_interface.visualize_quantizer_propagation(self, quant_prop_graph, str(iteration_counter))\n            quant_prop_graph = self.propagation_step(prop_quantizer, quant_prop_graph)\n            iteration_counter += 1\n\n        if self._debug_interface is not None:\n            self._debug_interface.visualize_quantizer_propagation(self, quant_prop_graph, ""final"")\n\n        retval = {}\n        for finished_prop_quantizer in self._finished_propagating_quantizers:\n            final_node_key = finished_prop_quantizer.current_location_node_key\n            final_node = quant_prop_graph.nodes[final_node_key]\n            insertion_point = final_node[\n                QuantizerPropagationStateGraph.INSERTION_POINT_DATA_NODE_ATTR]  # type: InsertionPoint\n            insertion_info = InsertionInfo(OperationExecutionContext(\n                operator_name=insertion_point.ia_op_exec_context.operator_name,\n                scope_in_model=insertion_point.ia_op_exec_context.scope_in_model,\n                call_order=insertion_point.ia_op_exec_context.call_order,\n                tensor_metas=[None]\n            ))  # TODO: fix this, rethink InsertionInfo here and elsewhere\n\n            retval[insertion_info] = finished_prop_quantizer.potential_quant_configs\n        return retval\n\n    def propagation_step(self, curr_prop_quantizer: PropagatingQuantizer,\n                         quant_prop_graph: QuantizerPropagationStateGraph) -> QuantizerPropagationStateGraph:\n        """"""Returns an updated curr_prop_quantizer state if the quantizer is not\n           yet in its final (accepting) position, and None if the quantizer is in its\n           final location.  The location before and after the step should correspond to\n           some insertion point.""""""\n        # TODO: full-fledged discrete finite automata approach? Switch to traversing a graph\n        # consisting of insertion points only, with reversed edges holding associated operator data?\n        curr_node_key = curr_prop_quantizer.current_location_node_key\n        curr_node = quant_prop_graph.nodes[curr_prop_quantizer.current_location_node_key]\n        curr_node_type = curr_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n        assert curr_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT\n\n        # Assumption: paths are at most 2 edges - either one edge to neighbouring insertion point\n        # or one edge to operation and next edge to its own neighbouring insertion point.\n        paths = quant_prop_graph.get_paths_to_immediately_dominating_insertion_points(curr_node_key)\n        if not paths:\n            prop_quantizer = quant_prop_graph.backtrack_propagation_until_accepting_location(curr_prop_quantizer)\n            if prop_quantizer is not None:\n                self._finished_propagating_quantizers.append(prop_quantizer)\n            return quant_prop_graph\n\n        surviving_prop_quantizers = []\n\n        prop_quantizers_to_process = [curr_prop_quantizer]\n        for _ in range(1, len(paths)):\n            additional_prop_quantizer = quant_prop_graph.clone_propagating_quantizer(curr_prop_quantizer)\n            prop_quantizers_to_process.append(additional_prop_quantizer)\n\n        pqs_and_paths = zip(paths, prop_quantizers_to_process)\n        for path, prop_quantizer in pqs_and_paths:\n            status = self.check_transition_via_path(prop_quantizer, path, quant_prop_graph)\n            if status == TransitionStatus.SHOULD_NOT_TRANSITION:\n                prop_quantizer = quant_prop_graph.backtrack_propagation_until_accepting_location(prop_quantizer)\n                if prop_quantizer is not None:\n                    self._finished_propagating_quantizers.append(prop_quantizer)\n            elif status == TransitionStatus.SHOULD_TRANSITION:\n                prop_quantizer = quant_prop_graph.propagate_quantizer_via_path(prop_quantizer, path)\n                surviving_prop_quantizers.append(prop_quantizer)\n            elif status == TransitionStatus.SHOULD_MERGE:\n                # The surviving quantizer will have its ""affected edges"" set extended\n                # by the corresponding set of the merged quantizer. The assumption\n                # here is that the surviving quantizer should never have to cross\n                # such a ""merge point"" while backtracking to an accepting location.\n\n                quant_prop_graph.merge_quantizer_into_path(prop_quantizer, path)\n\n        for prop_quantizer in surviving_prop_quantizers:\n            self._active_propagating_quantizers_queue.appendleft(prop_quantizer)\n        return quant_prop_graph\n\n    def get_allowed_quantizer_configs_for_operator(self, quant_det_id: OperatorMetatype) -> List[QuantizerConfig]:\n        return self._operator_allowed_qconfigs_map[quant_det_id]\n\n    def set_allowed_quantization_types_for_operator_nodes(self, quant_prop_graph: QuantizerPropagationStateGraph):\n        for node_key, node in quant_prop_graph.nodes.items():\n            node_type = node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:\n                quant_det_id = node[QuantizerPropagationStateGraph.OPERATOR_METATYPE_NODE_ATTR]\n                if quant_det_id is None:\n                    warnings.warn(""Unknown metatype for operator node: {}"".format(node_key))\n                    trait = QuantizationTrait.QUANTIZATION_AGNOSTIC\n                else:\n                    trait = self._operator_quantization_trait_map[quant_det_id]\n                node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR] = trait\n                if trait == QuantizationTrait.INPUTS_QUANTIZABLE:\n                    node[QuantizerPropagationStateGraph.ALLOWED_INPUT_QUANTIZATION_TYPES_NODE_ATTR] = \\\n                        self.get_allowed_quantizer_configs_for_operator(quant_det_id)\n        return quant_prop_graph\n\n    def get_operator_quantization_traits_map(self) -> Dict[OperatorMetatype, QuantizationTrait]:\n        # TODO: ensure that there are no name collisions between ops in different torch subpackages with the same name\n        retval = {}\n        if self._hw_config is None:\n            for op_meta in OPERATOR_METATYPES.registry_dict.values():\n                retval[op_meta] = QuantizationTrait.QUANTIZATION_AGNOSTIC  # Default value\n            for trait, meta_list in DEFAULT_QUANT_TRAIT_TO_OP_DICT.items():\n                for op_meta in meta_list:  # type: OperatorMetatype\n                    retval[op_meta] = trait\n        else:\n            op_meta_vs_qconfs_map = self._hw_config.get_metatype_vs_quantizer_configs_map()\n            for op_meta, qconf_list in op_meta_vs_qconfs_map.items():\n                if qconf_list is None:\n                    trait = QuantizationTrait.QUANTIZATION_AGNOSTIC\n                elif qconf_list:\n                    trait = QuantizationTrait.INPUTS_QUANTIZABLE\n                else:\n                    trait = QuantizationTrait.NON_QUANTIZABLE\n                retval[op_meta] = trait\n        return retval\n\n    def _get_operator_qconfigs_map(self) -> Dict[OperatorMetatype, List[QuantizerConfig]]:\n        # TODO: ensure that there are no name collisions between ops in different torch subpackages with the same name\n        retval = {}\n        if self._hw_config is None:\n            for op_meta in OPERATOR_METATYPES.registry_dict.values():\n                retval[op_meta] = QuantizationTrait.QUANTIZATION_AGNOSTIC  # Default value\n            for trait, meta_list in DEFAULT_QUANT_TRAIT_TO_OP_DICT.items():\n                if trait == QuantizationTrait.INPUTS_QUANTIZABLE:\n                    for op_meta in meta_list:  # type: OperatorMetatype\n                        retval[op_meta] = self.DEFAULT_QUANTIZATION_TYPES\n                else:\n                    for op_meta in meta_list:  # type: OperatorMetatype\n                        retval[op_meta] = []\n        else:\n            retval = self._hw_config.get_metatype_vs_quantizer_configs_map()\n        return retval\n\n\n    def debug_visualize(self, quant_prop_graph: QuantizerPropagationStateGraph, dump_path: str):\n        out_graph = quant_prop_graph.get_visualized_graph()\n        active_ids_str = "", "".join([str(pq.id) for pq in self._active_propagating_quantizers_queue])\n        finished_ids_str = "", "".join([str(pq.id) for pq in self._finished_propagating_quantizers])\n        out_graph.graph[\'graph\'] = {""label"": ""Propagating quantizers: {}\\n"" \\\n                                             ""Finished quantizers: {}"".format(active_ids_str, finished_ids_str),\n                                    ""labelloc"": ""t""}\n        nx.drawing.nx_pydot.write_dot(out_graph, dump_path)\n\n    def setup_initial_quantizers(self,\n                                 quant_prop_graph: QuantizerPropagationStateGraph) -> QuantizerPropagationStateGraph:\n        """"""Determines the initial subset of the nodes that must be quantized\n           and corresponding allowed quantization configs (possibly multiple) for each\n           quantizer.""""""\n        for node_key, node in quant_prop_graph.nodes.items():\n            node_type = node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:\n                preds = list(quant_prop_graph.predecessors(node_key))\n                if not preds:\n                    continue  # TODO: remove this once module insertion points are included in the IP graph\n                # Should be immediately preceded by an insertion point.\n                pred_ip_key = preds[0]\n                pred_node = quant_prop_graph.nodes[pred_ip_key]\n                pred_node_type = pred_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n                assert pred_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT, \\\n                    ""Invalid insertion point graph supplied for quantizer propagation!""\n\n                if node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR] in [\n                        QuantizationTrait.NON_QUANTIZABLE,\n                        QuantizationTrait.QUANTIZATION_AGNOSTIC]:\n                    continue\n\n                quant_det_id = node[QuantizerPropagationStateGraph.OPERATOR_METATYPE_NODE_ATTR]\n                qconf_list = self.get_allowed_quantizer_configs_for_operator(quant_det_id)\n                prop_quantizer = quant_prop_graph.add_propagating_quantizer(qconf_list, pred_ip_key)\n                self._active_propagating_quantizers_queue.appendleft(prop_quantizer)\n\n        return quant_prop_graph\n\n    def check_branching_transition(self, quant_prop_graph: QuantizerPropagationStateGraph,\n                                   prop_quantizer: PropagatingQuantizer,\n                                   branching_node_key: str) -> Optional[TransitionStatus]:\n        """"""If a propagating quantizer advances through a node that branches\n           downwards, the branches neighbouring to the one that the propagating quantizer\n           had just propagated from will have the precision of the quantizer imposed upon\n           them.  This is not always desirable - we might want to keep some branches in\n           higher precision than the others. For this reason, this function checks whether\n           the quantizer may safely advance through a branching node based on the possible\n           configs of the quantizers it might affect by doing so.""""""\n        dom_op_node_keys = quant_prop_graph.get_quantizable_op_nodes_immediately_dominated_by_node(\n            branching_node_key)\n        master_possible_qconfigs = prop_quantizer.potential_quant_configs\n        slave_possible_qconfigs_dict = {}\n        for op_node_key in dom_op_node_keys:\n            op_node = quant_prop_graph.nodes[op_node_key]\n            affecting_prop_quantizers = op_node[\n                QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n            if not affecting_prop_quantizers:\n                # The branch op is forced to be FP32 - should not proceed through the branch node.\n                return TransitionStatus.SHOULD_NOT_TRANSITION\n            slave_possible_qconfigs = affecting_prop_quantizers[0].potential_quant_configs\n            slave_possible_qconfigs_dict[op_node_key] = slave_possible_qconfigs\n        master_merged_qconfigs, \\\n        slave_merged_qconfigs_dict = self.get_merged_qconfigs(master_possible_qconfigs,\n                                                              slave_possible_qconfigs_dict)\n        if not master_merged_qconfigs:\n            # This quantizer\'s precision does not encompass the precisions of quantizers\n            # propagating through downward branches.\n            return TransitionStatus.SHOULD_NOT_TRANSITION\n\n        if self._propagation_strategy == PropagationStrategy.CONSERVATIVE:\n            for op_node_key, slave_merged_qconfigs_list in slave_merged_qconfigs_dict.items():\n                if len(slave_possible_qconfigs_dict[op_node_key]) != len(slave_merged_qconfigs_list):\n                    return TransitionStatus.SHOULD_NOT_TRANSITION\n\n        return None\n\n    def check_transition_via_path(self, prop_quantizer: PropagatingQuantizer, path: List,\n                                  quant_prop_graph: QuantizerPropagationStateGraph) -> TransitionStatus:\n        """"""Determines which action should be taken regarding the\n           prop_quantizer\'s propagation via path, which may be one of many possible\n           propagation paths.""""""\n        for from_node_key, to_node_key in path:\n            from_node = quant_prop_graph.nodes[from_node_key]\n\n            if len(list(quant_prop_graph.successors(from_node_key))) > 1:\n                # If a quantizer simply passes up through a downward-branching node, it may spoil the\n                # precision for operations on neighbouring branches. Consider a 4-bit quantizer rising\n                # through a branch node and an 8-bit quantizer arriving at the same node later. Therefore,\n                # prior to allowing the quantizer to pass through a branching node we need to ensure that\n                # the precision of the quantizer is a superset of precisions of the first non-quantization agnostic\n                # operations on each branch.\n                status = self.check_branching_transition(quant_prop_graph,\n                                                         prop_quantizer,\n                                                         from_node_key)\n                if status is not None:\n                    return status\n\n            from_node_type = from_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if from_node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:\n                trait = from_node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]\n                if trait in [QuantizationTrait.NON_QUANTIZABLE,\n                             QuantizationTrait.INPUTS_QUANTIZABLE]:\n                    return TransitionStatus.SHOULD_NOT_TRANSITION\n            edge = quant_prop_graph.edges[from_node_key, to_node_key]\n            potential_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n            if potential_quantizers:\n                # Assuming that multiple affecting quantizers should all have the same quantization config\n                # by construction\n                if prop_quantizer.potential_quant_configs == potential_quantizers[0].potential_quant_configs:\n                    return TransitionStatus.SHOULD_MERGE\n                return TransitionStatus.SHOULD_NOT_TRANSITION\n\n            from_node_type = from_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]\n            if from_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                potential_quantizers = from_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n                if potential_quantizers:\n                    # Affecting quantizers should have the same configs by construction, so we only\n                    # check the first\n                    if prop_quantizer.potential_quant_configs == potential_quantizers[0].potential_quant_configs:\n                        return TransitionStatus.SHOULD_MERGE\n        return TransitionStatus.SHOULD_TRANSITION\n\n    def get_merged_qconfigs(self, master_potential_qconfigs_list: List[QuantizerConfig],\n                            slave_potential_qconfigs_dict: Dict[str, List[QuantizerConfig]]) -> Tuple[\n                                List[QuantizerConfig], Dict[str, QuantizerConfig]]:\n        """"""Returns potential qconfigs lists for \'master\' and \'slave\' quantizers\n        that are compatible with each other. Compatibility is decided in terms of\n        master quantizer having configs which all have higher precision than all the\n        slave potential quantizer configs.""""""\n        final_master_merged_qconfigs_list = deepcopy(master_potential_qconfigs_list)\n        curr_slave_merged_qconfigs_dict = deepcopy(slave_potential_qconfigs_dict)\n        # TODO: implement variant solutions, i.e. for each set of resultant merged\n        # master potential qconfig lists we have, in general, different merged slave potential\n        # config lists. Currently greedy approach is used.\n        for m_qconfig in master_potential_qconfigs_list:\n            should_persist_slave_merged_qconfigs_dict = True\n            candidate_slave_merged_qconfigs_dict = deepcopy(curr_slave_merged_qconfigs_dict)\n            for node_key, s_qconfig_list in curr_slave_merged_qconfigs_dict.items():\n                for s_qconfig in s_qconfig_list:\n                    if m_qconfig < s_qconfig and s_qconfig in candidate_slave_merged_qconfigs_dict[node_key]:\n                        candidate_slave_merged_qconfigs_dict[node_key].remove(s_qconfig)\n            for _, s_qconfig_list in candidate_slave_merged_qconfigs_dict.items():\n                if not s_qconfig_list:\n                    # No options left for slave configs on one of the branches to accomodate the master\n                    # config - this master config cannot be used to be merged into.\n                    final_master_merged_qconfigs_list.remove(m_qconfig)\n                    should_persist_slave_merged_qconfigs_dict = False\n                    break\n            if should_persist_slave_merged_qconfigs_dict:\n                curr_slave_merged_qconfigs_dict = candidate_slave_merged_qconfigs_dict\n        if not final_master_merged_qconfigs_list:\n            return [], {}\n        return final_master_merged_qconfigs_list, curr_slave_merged_qconfigs_dict\n\n    def get_finished_propagating_quantizers(self):\n        return self._finished_propagating_quantizers\n\n    def get_active_propagating_quantizers_queue(self):\n        return self._active_propagating_quantizers_queue\n'"
pytorch_toolkit/nncf/nncf/sparsity/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/sparsity/base_algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import namedtuple\nfrom typing import List\n\n\nfrom texttable import Texttable\n\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.compression_method_api import CompressionAlgorithmBuilder, CompressionAlgorithmController\nfrom nncf.dynamic_graph.graph import InputAgnosticOperationExecutionContext\nfrom nncf.layer_utils import COMPRESSION_MODULES\nfrom nncf.nncf_network import InsertionCommand, InsertionPoint, InsertionType, OperationPriority\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.nncf_logger import logger as nncf_logger\n\nSparseModuleInfo = namedtuple(\'SparseModuleInfo\', [\'module_name\', \'module\', \'operand\'])\n\n\nclass BaseSparsityAlgoBuilder(CompressionAlgorithmBuilder):\n    def __init__(self, config):\n        super().__init__(config)\n        self._sparsified_module_info = []\n\n    def apply_to(self, target_model: NNCFNetwork) -> NNCFNetwork:\n        insertion_commands = self._sparsify_weights(target_model)\n        for command in insertion_commands:\n            target_model.register_insertion_command(command)\n        target_model.register_algorithm(self)\n        return target_model\n\n    def _sparsify_weights(self, target_model: NNCFNetwork) -> List[InsertionCommand]:\n        device = next(target_model.parameters()).device\n        sparsified_modules = target_model.get_nncf_modules()\n        insertion_commands = []\n        for module_scope, module in sparsified_modules.items():\n            scope_str = str(module_scope)\n\n            if not self._should_consider_scope(scope_str):\n                nncf_logger.info(""Ignored adding Weight Sparsifier in scope: {}"".format(scope_str))\n                continue\n\n            nncf_logger.info(""Adding Weight Sparsifier in scope: {}"".format(scope_str))\n            operation = self.create_weight_sparsifying_operation(module)\n            hook = UpdateWeight(operation).to(device)\n            insertion_commands.append(InsertionCommand(InsertionPoint(\n                InputAgnosticOperationExecutionContext("""", module_scope, 0),\n                InsertionType.NNCF_MODULE_PRE_OP),\n                                                       hook,\n                                                       OperationPriority.SPARSIFICATION_PRIORITY))\n            self._sparsified_module_info.append(\n                SparseModuleInfo(scope_str, module, hook.operand))\n\n        return insertion_commands\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n        return BaseSparsityAlgoController(target_model, self._sparsified_module_info)\n\n    def create_weight_sparsifying_operation(self, target_module):\n        raise NotImplementedError\n\n\nclass BaseSparsityAlgoController(CompressionAlgorithmController):\n    def __init__(self, target_model: NNCFNetwork,\n                 sparsified_module_info: List[SparseModuleInfo]):\n        super().__init__(target_model)\n        self.sparsified_module_info = sparsified_module_info\n\n    def freeze(self):\n        raise NotImplementedError\n\n    def set_sparsity_level(self, sparsity_level):\n        raise NotImplementedError\n\n    @property\n    def sparsified_weights_count(self):\n        count = 0\n        for minfo in self.sparsified_module_info:\n            count = count + minfo.module.weight.view(-1).size(0)\n        return max(count, 1)\n\n    @property\n    def sparsity_rate_for_sparsified_modules(self):\n        nonzero = 0\n        count = 0\n\n        for minfo in self.sparsified_module_info:\n            mask = minfo.operand.apply_binary_mask(minfo.module.weight)\n            nonzero = nonzero + mask.nonzero().size(0)\n            count = count + mask.view(-1).size(0)\n\n        return 1 - nonzero / max(count, 1)\n\n    @property\n    def sparsity_rate_for_model(self):\n        nonzero = 0\n        count = 0\n\n        for m in self._model.modules():\n            if isinstance(m, tuple(COMPRESSION_MODULES.registry_dict.values())):\n                continue\n\n            sparsified_module = False\n            for minfo in self.sparsified_module_info:\n                if minfo.module == m:\n                    mask = minfo.operand.apply_binary_mask(m.weight)\n                    nonzero = nonzero + mask.nonzero().size(0)\n                    count = count + mask.numel()\n\n                    if not m.bias is None:\n                        nonzero = nonzero + m.bias.nonzero().size(0)\n                        count = count + m.bias.numel()\n\n                    sparsified_module = True\n\n            if not sparsified_module:\n                for param in m.parameters(recurse=False):\n                    nonzero = nonzero + param.nonzero().size(0)\n                    count = count + param.numel()\n\n        return 1 - nonzero / max(count, 1)\n\n    def statistics(self):\n        stats = super().statistics()\n        table = Texttable()\n        header = [""Name"", ""Weight\'s Shape"", ""SR"", ""% weights""]\n        data = [header]\n\n        sparsified_weights_count = self.sparsified_weights_count\n\n        for minfo in self.sparsified_module_info:\n            drow = {h: 0 for h in header}\n            drow[""Name""] = minfo.module_name\n            drow[""Weight\'s Shape""] = list(minfo.module.weight.size())\n            mask = minfo.operand.apply_binary_mask(minfo.module.weight)\n            nonzero = mask.nonzero().size(0)\n            drow[""SR""] = 1.0 - nonzero / max(mask.view(-1).size(0), 1)\n            drow[""% weights""] = mask.view(-1).size(0) / sparsified_weights_count\n            row = [drow[h] for h in header]\n            data.append(row)\n        table.add_rows(data)\n\n        stats[""sparsity_statistic_by_module""] = table\n        stats[""sparsity_rate_for_sparsified_modules""] = self.sparsity_rate_for_sparsified_modules\n        stats[""sparsity_rate_for_model""] = self.sparsity_rate_for_model\n\n        return self.add_algo_specific_stats(stats)\n\n    def add_algo_specific_stats(self, stats):\n        return stats\n'"
pytorch_toolkit/nncf/nncf/sparsity/functions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom nncf.dynamic_graph.patch_pytorch import register_operator\n\n\n@register_operator()\ndef apply_binary_mask(mask, weight):\n    return mask * weight\n'"
pytorch_toolkit/nncf/nncf/sparsity/layers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom nncf.layer_utils import COMPRESSION_MODULES\nfrom nncf.sparsity.functions import apply_binary_mask as apply_binary_mask_impl\nfrom nncf.utils import is_tracing_state, no_jit_trace\n\n\n@COMPRESSION_MODULES.register()\nclass BinaryMask(nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.register_buffer(""_binary_mask"", torch.ones(size))\n\n    @property\n    def binary_mask(self):\n        return self._binary_mask\n\n    @binary_mask.setter\n    def binary_mask(self, tensor):\n        with torch.no_grad():\n            self._binary_mask.set_(tensor)\n\n    def forward(self, weight):\n        if is_tracing_state():\n            with no_jit_trace():\n                return weight.mul_(self.binary_mask)\n        tmp_tensor = self._calc_training_binary_mask(weight)\n        return apply_binary_mask_impl(tmp_tensor, weight)\n\n    def _calc_training_binary_mask(self, weight):\n        return self.binary_mask\n\n    def apply_binary_mask(self, weight):\n        return apply_binary_mask_impl(self.binary_mask, weight)\n'"
pytorch_toolkit/nncf/nncf/sparsity/schedulers.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom bisect import bisect_right\n\nimport numpy as np\n\nfrom ..algo_selector import Registry\nfrom ..compression_method_api import CompressionScheduler\nfrom ..config import Config\n\nSPARSITY_SCHEDULERS = Registry(""sparsity_schedulers"")\n\n\nclass SparsityScheduler(CompressionScheduler):\n    def __init__(self, sparsity_algo, params: Config = None):\n        super().__init__()\n        if params is None:\n            self._params = Config()\n        else:\n            self._params = params\n\n        self.algo = sparsity_algo\n        self.sparsity_training_steps = self._params.get(\'sparsity_training_steps\', 100)\n        self.max_step = self._params.get(\'sparsity_steps\', 90)\n        self.max_sparsity = self._params.get(\'sparsity_target\', 0.5)\n        self.initial_sparsity = self._params.get(\'sparsity_init\', 0)\n\n    def initialize(self):\n        self._set_sparsity_level()\n\n    def epoch_step(self, epoch=None):\n        super().epoch_step(epoch)\n        self._set_sparsity_level()\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self._set_sparsity_level()\n\n    def _set_sparsity_level(self):\n        self.algo.set_sparsity_level(self.current_sparsity_level)\n        if self.last_epoch >= self.sparsity_training_steps:\n            self.algo.freeze()\n\n    def _calc_density_level(self):\n        raise NotImplementedError\n\n    @property\n    def current_sparsity_level(self):\n        return 1 - self._calc_density_level()\n\n\n@SPARSITY_SCHEDULERS.register(""polynomial"")\nclass PolynomialSparseScheduler(SparsityScheduler):\n    def __init__(self, sparsity_algo, params=None):\n        super().__init__(sparsity_algo, params)\n        self.power = self._params.get(\'power\', 0.9)\n        self._set_sparsity_level()\n\n    def epoch_step(self, epoch=None):\n        super().epoch_step(epoch)\n        self._set_sparsity_level()\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self._set_sparsity_level()\n\n    def _calc_density_level(self):\n        step = (min(self.max_step, self.last_epoch) / self.max_step) ** self.power\n        current_sparsity = self.initial_sparsity + (self.max_sparsity - self.initial_sparsity) * step\n        return 1 - current_sparsity\n\n\n@SPARSITY_SCHEDULERS.register(""exponential"")\nclass ExponentialSparsityScheduler(SparsityScheduler):\n    def __init__(self, sparsity_algo, params=None):\n        super().__init__(sparsity_algo, params)\n        self.a, self.k = self._init_exp(self.initial_sparsity, self.max_sparsity, sparsity_steps=self.max_step)\n        self._set_sparsity_level()\n\n    def _calc_density_level(self):\n        curr_density = self.a * np.exp(-self.k * self.last_epoch)\n        min_density = 1 - self.max_sparsity\n        return min_density if curr_density < min_density else curr_density\n\n    @staticmethod\n    def _init_exp(initial_sparsity, max_sparsity, sparsity_steps=20):\n        p1 = (0, 1 - initial_sparsity)\n        p2 = (sparsity_steps, 1 - max_sparsity)\n        k = np.log(p2[1] / p1[1]) / (p1[0] - p2[0])\n        a = p1[1] / np.exp(-k * p1[0])\n        return a, k\n\n\n@SPARSITY_SCHEDULERS.register(""adaptive"")\nclass AdaptiveSparsityScheduler(SparsityScheduler):\n    def __init__(self, sparsity_algo, params=None):\n        super().__init__(sparsity_algo, params)\n        self.sparsity_loss = sparsity_algo.loss\n        from .rb.loss import SparseLoss\n        if not isinstance(self.sparsity_loss, SparseLoss):\n            raise TypeError(\'AdaptiveSparseScheduler expects SparseLoss, but {} is given\'.format(\n                self.sparsity_loss.__class__.__name__))\n        self.decay_step = params.get(\'step\', 0.05)\n        self.eps = params.get(\'eps\', 0.03)\n        self.patience = params.get(\'patience\', 1)\n        self.sparsity_target = self.initial_sparsity\n        self.num_bad_epochs = 0\n        self._set_sparsity_level()\n\n    def epoch_step(self, epoch=None):\n        super().step(epoch)\n        if self.sparsity_loss.current_sparsity >= self.sparsity_target - self.eps:\n            self.num_bad_epochs += 1\n\n        if self.num_bad_epochs >= self.patience:\n            self.num_bad_epochs = 0\n            self.sparsity_target = min(self.sparsity_target + self.decay_step, self.max_sparsity)\n        self._set_sparsity_level()\n\n    def state_dict(self):\n        sd = super().state_dict()\n        sd[\'num_bad_epochs\'] = self.num_bad_epochs\n        sd[\'current_sparsity_level\'] = self.sparsity_target\n        return sd\n\n    def _calc_density_level(self):\n        return 1 - self.sparsity_target\n\n\n@SPARSITY_SCHEDULERS.register(""multistep"")\nclass MultiStepSparsityScheduler(SparsityScheduler):\n    def _calc_density_level(self):\n        return 1 - self.sparsity_level\n\n    def __init__(self, sparsity_algo, params):\n        super().__init__(sparsity_algo, params)\n        self.sparsity_levels = self._params.get(\'sparsity_levels\', [0.1, 0.5])\n        self.steps = self._params.get(\'steps\', [90])\n        if len(self.steps) + 1 != len(self.sparsity_levels):\n            raise AttributeError(\'number of sparsity levels must equal to number of steps + 1\')\n\n        self.initial_sparsity = self.sparsity_level = self.sparsity_levels[0]\n        self.max_sparsity = max(self.sparsity_levels)\n        self.sparsity_algo = sparsity_algo\n        self.steps = sorted(self.steps)\n        self.max_step = self.steps[-1]\n        self.prev_ind = 0\n        self._set_sparsity_level()\n\n    def epoch_step(self, last=None):\n        super().epoch_step(last)\n        ind = bisect_right(self.steps, self.last_epoch)\n        if ind != self.prev_ind:\n            self.sparsity_level = self.sparsity_levels[ind]\n            self.prev_ind = ind\n        self._set_sparsity_level()\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        ind = bisect_right(self.steps, self.last_epoch)\n        if ind > 0:\n            self.prev_ind = ind\n            self.sparsity_level = self.sparsity_levels[ind]\n            self._set_sparsity_level()\n'"
pytorch_toolkit/nncf/tests/binarization/test_functions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nimport pytest\nimport torch\nfrom torch.autograd import Variable\n\nfrom nncf.binarization.layers import xnor_binarize_op, dorefa_binarize_op, activation_bin_scale_threshold_op\nfrom tests.test_helpers import check_equal, get_grads\n\n\n# reference impl\nclass ReferenceXNORBinarize:\n    @staticmethod\n    def forward(x):\n        norm = np.abs(x).mean((1, 2, 3), keepdims=True)\n        sign = ((x > 0).astype(x.dtype) * 2 - 1)\n        output = sign * norm\n        return output\n\n    @staticmethod\n    def backward(grad_output):\n        return grad_output\n\n\nclass ReferenceDOREFABinarize:\n    @staticmethod\n    def forward(x):\n        norm = np.abs(x).mean()\n        sign = ((x > 0).astype(x.dtype) * 2 - 1)\n        return sign * norm\n\n    @staticmethod\n    def backward(grad_output):\n        return grad_output\n\n\nclass ReferenceActivationBinarize:\n    @staticmethod\n    def forward(x, scale, threshold):\n        shape = [1 for s in x.shape]\n        shape[1] = x.shape[1]\n        t = threshold * scale\n        output = (x > t).astype(x.dtype) * scale\n        return output\n\n    @staticmethod\n    def backward(grad_output, x, scale, output):\n\n        # calc gradient for input\n        mask_lower = (x <= scale).astype(x.dtype)\n        grad_input = grad_output * (x >= 0).astype(x.dtype) * mask_lower\n\n        # calc gradient for scale\n        err = (output - x) / scale\n        grad_scale = grad_output * (mask_lower * err + (1 - mask_lower))\n        grad_scale = grad_scale.sum()\n\n        # calc gradient for threshold\n        grad_threshold = -grad_output * (x > 0).astype(x.dtype) * (x < scale).astype(x.dtype)\n\n        for idx, _ in enumerate(x.shape):\n            if idx != 1:  # activation channel dimension\n                grad_threshold = grad_threshold.sum(idx, keepdims=True)\n\n        return [grad_input, grad_scale, grad_threshold]\n\n\ndef idfn(val):\n    if isinstance(val, list):\n        return \'[{}]\'.format(\'-\'.join([str(v) for v in val]))\n\n    return None\n\n\n@pytest.fixture\ndef _seed():\n    np.random.seed(0)\n\n\ndef generate_input(input_size):\n    return (2 * np.random.random_sample(input_size) - 1) * np.random.rand() + np.random.rand()\n\n\ndef generate_scale_threshold(input_size):\n    threshold_shape = [1 for s in input_size]\n    threshold_shape[1] = input_size[1]\n    return np.random.random_sample(1), (2 * np.random.random_sample(threshold_shape) - 1)\n\n\ndef get_test_data(data_list, is_cuda=False, is_backward=False):\n    results = []\n    for data in data_list:\n        result = torch.from_numpy(data.copy())\n        if is_cuda:\n            result = result.cuda()\n        if is_backward:\n            result = Variable(result, requires_grad=True)\n        results.append(result)\n    return results\n\n\n@pytest.mark.parametrize(\'input_size\',\n                         [[1, 96, 112, 112],\n                          [1, 192, 28, 28],\n                          [1, 576, 14, 14],\n                          [32, 96, 112, 112],\n                          [32, 192, 28, 28],\n                          [32, 576, 14, 14]],\n                         ids=idfn)\n@pytest.mark.parametrize(""use_cuda"", [False, True], ids=[\'cpu\', \'cuda\'])\nclass TestParametrized:\n    @pytest.mark.parametrize(\'weight_bin_type\', [""xnor"", ""dorefa""])\n    class TestWeightBinarization:\n        def test_binarize_weights_forward(self, _seed, input_size, weight_bin_type, use_cuda):\n            ref_input = generate_input(input_size)\n\n            test_input = get_test_data([ref_input], use_cuda)[0]\n\n            if weight_bin_type == ""xnor"":\n                ref_value = ReferenceXNORBinarize.forward(ref_input)\n                test_value = xnor_binarize_op(test_input)\n            elif weight_bin_type == ""dorefa"":\n                ref_value = ReferenceDOREFABinarize.forward(ref_input)\n                test_value = dorefa_binarize_op(test_input)\n\n            check_equal(ref_value, test_value, rtol=1e-3)\n\n    def test_binarize_activations_forward(self, _seed, input_size, use_cuda):\n        ref_input = generate_input(input_size)\n        ref_scale, ref_threshold = generate_scale_threshold(input_size)\n\n        test_input, test_scale, test_threshold = get_test_data([ref_input, ref_scale, ref_threshold], use_cuda)\n\n        ref_value = ReferenceActivationBinarize.forward(ref_input, ref_scale, ref_threshold)\n        test_value = activation_bin_scale_threshold_op(test_input, test_scale, test_threshold)\n\n        check_equal(ref_value, test_value, rtol=1e-3)\n\n    def test_binarize_activations_backward(self, _seed, input_size, use_cuda):\n        ref_input = generate_input(input_size)\n        ref_scale, ref_threshold = generate_scale_threshold(input_size)\n\n        test_input, test_scale, test_threshold = get_test_data([ref_input, ref_scale, ref_threshold], use_cuda,\n                                                               is_backward=True)\n\n        ref_value = ReferenceActivationBinarize.forward(ref_input, ref_scale, ref_threshold)\n        ref_grads = ReferenceActivationBinarize.backward(np.ones(input_size), ref_input, ref_scale, ref_value)\n\n        test_value = activation_bin_scale_threshold_op(test_input, test_scale, test_threshold)\n        test_value.sum().backward()\n        test_grads = get_grads([test_input, test_scale, test_threshold])\n\n        check_equal(ref_grads, test_grads, rtol=1e-3)\n'"
pytorch_toolkit/nncf/tests/composite/test_sparsity_quantization.py,0,"b'from nncf.composite_compression import CompositeCompressionAlgorithmController\nfrom nncf.utils import get_all_modules_by_type\nfrom nncf.sparsity.rb.layers import RBSparsifyingWeight\nfrom nncf.quantization.layers import SymmetricQuantizer\nfrom nncf.module_operations import UpdateWeight, UpdateInputs\nfrom tests.test_helpers import BasicConvTestModel, create_compressed_model_and_algo_for_test\nfrom nncf.config import Config\n\n\n\ndef get_basic_sparsity_plus_quantization_config(input_sample_size=(1, 1, 4, 4)):\n    config = Config()\n    config.update({\n        ""input_info"":\n            {\n                ""sample_size"": input_sample_size,\n            },\n        ""compression"": [\n            {\n                ""algorithm"": ""rb_sparsity"",\n            },\n            {\n                ""algorithm"": ""quantization""\n            }\n        ]\n    })\n    return config\n\n\ndef test_can_quantize_inputs_for_sparsity_plus_quantization():\n    model = BasicConvTestModel()\n    config = get_basic_sparsity_plus_quantization_config()\n    sparse_quantized_model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    assert isinstance(compression_ctrl, CompositeCompressionAlgorithmController)\n\n    sparse_quantized_model_conv = get_all_modules_by_type(sparse_quantized_model, \'NNCFConv2d\')\n\n    nncf_module = next(iter(sparse_quantized_model_conv.values()))\n    assert len(nncf_module.pre_ops) == 3  # 1x weight sparsifier + 1x weight quantizer + 1x input quantizer\n    assert isinstance(nncf_module.pre_ops[\'0\'], UpdateWeight)\n    assert isinstance(nncf_module.pre_ops[\'0\'].op, RBSparsifyingWeight)\n\n    assert isinstance(nncf_module.pre_ops[\'1\'], UpdateWeight)\n    assert isinstance(nncf_module.pre_ops[\'1\'].op, SymmetricQuantizer)\n\n    assert isinstance(nncf_module.pre_ops[\'2\'], UpdateInputs)\n    assert isinstance(nncf_module.pre_ops[\'2\'].op, SymmetricQuantizer)\n'"
pytorch_toolkit/nncf/tests/modules/test_rnn.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport logging\nimport sys\nfrom collections import namedtuple\nfrom typing import List, Tuple\n\nimport copy\nimport onnx\nimport os\nimport pytest\nimport torch\nimport torch.nn.functional as F\nfrom functools import partial\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import PackedSequence\n\nfrom nncf.dynamic_graph.context import TracingContext\nfrom nncf.dynamic_graph.transform_graph import replace_modules\nfrom nncf.model_creation import create_compressed_model\nfrom nncf.layers import LSTMCellNNCF, NNCF_RNN, ITERATION_MODULES\nfrom tests.modules.seq2seq.gnmt import GNMT\nfrom tests.test_helpers import get_empty_config, get_grads, create_compressed_model_and_algo_for_test\n\n\ndef replace_lstm(model):\n    def replace_fn(module_):\n        if not isinstance(module_, nn.LSTM):\n            return module_\n        device = next(module_.parameters()).device\n        custom_lstm = NNCF_RNN(\'LSTM\', input_size=module_.input_size, hidden_size=module_.hidden_size,\n                               num_layers=module_.num_layers, bidirectional=module_.bidirectional,\n                               batch_first=module_.batch_first, dropout=module_.dropout,\n                               bias=module_.bias)\n\n        def get_param_names(bias):\n            # type: (bool) -> List[str]\n            suffixes = [\'ih\', \'hh\']\n            names = [\'weight_\' + suffix for suffix in suffixes]\n            if bias:\n                names += [\'bias_\' + suffix for suffix in suffixes]\n            return names\n\n        for l in range(custom_lstm.num_layers):\n            for d in range(custom_lstm.num_directions):\n                for name in get_param_names(custom_lstm.bias):\n                    suffix = \'_reverse\' if d == 1 else \'\'\n                    param_name = name + \'_l{}{}\'.format(l, suffix)\n                    param = getattr(module_, param_name)\n                    getattr(custom_lstm, param_name).data.copy_(param.data)\n        custom_lstm.to(device)\n        return custom_lstm\n\n    if isinstance(model, nn.LSTM):\n        return replace_fn(model)\n    affected_scopes = []\n    return replace_modules(model, replace_fn, affected_scopes)[0]\n\ndef clone_test_data(data_list):\n    # type: (LSTMTestData) -> List[torch.Tensor]\n    results = []\n    x = data_list[0]\n    result = x if isinstance(x, PackedSequence) else x.clone()\n    results.append(result)\n    for tensor_list in data_list[1:]:\n        result = ()\n        for tensor in tensor_list:\n            if isinstance(tensor, Variable):\n                sub_result = tensor.data.clone()\n                sub_result = Variable(sub_result, requires_grad=True)\n            else:\n                sub_result = tensor.clone()\n            result += (sub_result,)\n        results.append(result)\n    return results\n\n\nLSTMTestSizes = namedtuple(\'LSTMTestSizes\', [\'input_size\', \'hidden_size\', \'batch\', \'seq_length\'])\nLSTMTestData = namedtuple(\'LSTMTestData\', [\'x\', \'h0\', \'c0\', \'weight_ih\', \'weight_hh\', \'bias_ih\', \'bias_hh\'])\n\n\n@pytest.mark.parametrize(\'sizes\',\n                         [LSTMTestSizes(512, 768, 128, 50),\n                          LSTMTestSizes(3, 3, 3, 3),\n                          LSTMTestSizes(1, 1, 1, 1)], ids=lambda val: \'[{}]\'.format(\'-\'.join([str(v) for v in val])))\nclass TestLSTMCell:\n    @staticmethod\n    def generate_lstm_data(p, num_layers=1, num_directions=1, variable_length=False, sorted_=True, batch_first=True,\n                           is_cuda=False, bias=True, empty_initial=False, is_backward=False):\n        # type: (LSTMTestSizes, int, int, bool, bool, bool, bool, bool, bool, bool) -> LSTMTestData\n        num_chunks = 4\n        seq_list = []\n        if variable_length:\n            seq_lens = torch.IntTensor(p.batch).random_(1, p.seq_length + 1)\n            if sorted_:\n                seq_lens = torch.sort(seq_lens, descending=True).values\n            for seq_size in seq_lens:\n                seq_list.append(torch.randn(seq_size.item(), p.input_size))\n            padded_seq_batch = torch.nn.utils.rnn.pad_sequence(seq_list, batch_first=batch_first)\n            x_data = torch.nn.utils.rnn.pack_padded_sequence(padded_seq_batch, lengths=seq_lens,\n                                                             batch_first=batch_first, enforce_sorted=sorted_)\n\n        else:\n            size = (p.seq_length, p.batch, p.input_size)\n            if batch_first:\n                size = (p.batch, p.seq_length, p.input_size)\n            x_data = torch.randn(*size)\n\n        def wrap_tensor(tensor):\n            wrapped = tensor\n            if is_cuda:\n                wrapped = wrapped.cuda()\n            if is_backward:\n                wrapped = Variable(wrapped, requires_grad=True)\n            return wrapped\n\n        if is_cuda:\n            x_data = x_data.cuda()\n        h0, c0, wih, whh, bih, bhh = ([] for _ in range(6))\n        for layer_ in range(num_layers):\n            for _ in range(num_directions):\n                layer_input_size = p.input_size if layer_ == 0 else p.hidden_size * num_directions\n                if not empty_initial:\n                    h0.append(wrap_tensor(torch.randn(p.batch, p.hidden_size)))\n                    c0.append(wrap_tensor(torch.randn(p.batch, p.hidden_size)))\n                wih.append(wrap_tensor(torch.rand(num_chunks * p.hidden_size, layer_input_size)))\n                whh.append(wrap_tensor(torch.rand(num_chunks * p.hidden_size, p.hidden_size)))\n                if bias:\n                    bih.append(wrap_tensor(torch.rand(num_chunks * p.hidden_size)))\n                    bhh.append(wrap_tensor(torch.rand(num_chunks * p.hidden_size)))\n        result = LSTMTestData(x_data, h0, c0, wih, whh, bih, bhh)\n        return result\n\n    @staticmethod\n    def set_weights(cell, data):\n        # type: (nn.LSTMCell, LSTMTestData) -> None\n        for name in TestLSTM.get_param_names(bias=True):\n            param = getattr(data, name)\n            if param:\n                getattr(cell, name).data.copy_(param[0].data)\n\n    def test_forward_lstm_cell(self, sizes, _seed):\n        p = sizes\n        ref_data = TestLSTMCell.generate_lstm_data(p, batch_first=False)\n        test_data = LSTMTestData(*clone_test_data(ref_data))\n\n        ref_rnn = nn.LSTMCell(p.input_size, p.hidden_size)\n        TestLSTMCell.set_weights(ref_rnn, ref_data)\n        test_rnn = LSTMCellNNCF(p.input_size, p.hidden_size)\n        TestLSTMCell.set_weights(test_rnn, test_data)\n\n        for i in range(p.seq_length):\n            ref_result = ref_rnn(ref_data.x[i], (ref_data.h0[0], ref_data.c0[0]))\n            test_result = test_rnn(test_data.x[i], (test_data.h0[0], test_data.c0[0]))\n            for (ref, test) in list(zip(ref_result, test_result)):\n                torch.testing.assert_allclose(test, ref)\n\n    def test_backward_lstm_cell(self, sizes, _seed):\n        p = sizes\n        ref_data = TestLSTMCell.generate_lstm_data(p, batch_first=False, is_backward=True)\n        with torch.no_grad():\n            test_data = LSTMTestData(*clone_test_data(ref_data))\n\n        ref_rnn = nn.LSTMCell(p.input_size, p.hidden_size)\n        TestLSTMCell.set_weights(ref_rnn, ref_data)\n        test_rnn = LSTMCellNNCF(p.input_size, p.hidden_size)\n        TestLSTMCell.set_weights(test_rnn, test_data)\n\n        for i in range(p.seq_length):\n            ref_result = ref_rnn(ref_data.x[i], (ref_data.h0[0], ref_data.c0[0]))\n            test_result = test_rnn(test_data.x[i], (test_data.h0[0], test_data.c0[0]))\n            ref_result[0].sum().backward()\n            test_result[0].sum().backward()\n            ref_grads = get_grads([ref_data.h0[0], ref_data.c0[0]])\n            ref_grads += get_grads([ref_rnn.weight_ih, ref_rnn.weight_hh, ref_rnn.bias_ih, ref_rnn.bias_hh])\n            test_grads = get_grads([ref_data.h0[0], ref_data.c0[0]])\n            test_grads += get_grads([test_rnn.weight_ih, test_rnn.weight_hh, test_rnn.bias_ih, test_rnn.bias_hh])\n            for (ref, test) in list(zip(test_grads, ref_grads)):\n                torch.testing.assert_allclose(test, ref)\n\n\ndef test_export_lstm_cell(tmp_path):\n    config = get_empty_config(model_size=1, input_sample_size=(1, 1))\n    config[\'compression\'] = {\'algorithm\': \'quantization\'}\n\n    model, algo = create_compressed_model_and_algo_for_test(LSTMCellNNCF(1, 1), config)\n\n    test_path = str(tmp_path.joinpath(\'test.onnx\'))\n    algo.export_model(test_path)\n    assert os.path.exists(test_path)\n\n    onnx_num = 0\n    model = onnx.load(test_path)\n    # pylint: disable=no-member\n    for node in model.graph.node:\n        if node.op_type == \'FakeQuantize\':\n            onnx_num += 1\n    assert onnx_num == 12\n\n\n@pytest.mark.parametrize(\'sizes\',\n                         [LSTMTestSizes(512, 324, 128, 50),\n                          LSTMTestSizes(3, 3, 3, 3),\n                          LSTMTestSizes(1, 1, 1, 1)], ids=lambda val: \'[{}]\'.format(\'-\'.join([str(v) for v in val])))\n@pytest.mark.parametrize(\'bidirectional\', (True, False), ids=(\'bi\', \'uni\'))\n@pytest.mark.parametrize(""bias"", [True, False], ids=[\'bias\', \'no_bias\'])\n@pytest.mark.parametrize(\'num_layers\', [1, 2], ids=[\'single_layer\', \'stacked\'])\n@pytest.mark.parametrize(\'batch_first\', [True, False], ids=[\'batch_first\', \'seq_first\'])\n@pytest.mark.parametrize((\'variable_length\', \'sorted_\'),\n                         ([True, True],\n                          [True, False],\n                          [False, False]), ids=[\'packed_sorted\', \'packed_unsorted\', \'not_packed\'])\n@pytest.mark.parametrize(\'is_cuda\', [True, False], ids=[\'cuda\', \'cpu\'])\n@pytest.mark.parametrize(\'empty_initial\', [True, False], ids=[\'no_initial\', \'with_initial\'])\n# TODO: dropout gives different result. Looks like different random seed on CPU\n# @pytest.mark.parametrize(\'dropout\', [0, 0.9], ids=[\'no_dropout\', \'with_dropout\'])\n@pytest.mark.parametrize(\'dropout\', [0], ids=[\'no_dropout\'])\nclass TestLSTM:\n    def test_forward_lstm(self, sizes, bidirectional, num_layers, bias, batch_first, variable_length, sorted_, is_cuda,\n                          empty_initial, dropout, _seed):\n        num_directions = 2 if bidirectional else 1\n        p = sizes\n\n        ref_data = TestLSTMCell.generate_lstm_data(p, num_layers, num_directions, variable_length, sorted_, batch_first,\n                                                   is_cuda, bias, empty_initial)\n\n        ref_rnn = nn.LSTM(input_size=p.input_size, hidden_size=p.hidden_size, num_layers=num_layers,\n                          bidirectional=bidirectional, batch_first=batch_first, bias=bias, dropout=dropout)\n        self.set_ref_lstm_weights(ref_data, ref_rnn, num_layers, num_directions, bias)\n        ref_hidden = None if empty_initial else self.get_ref_lstm_hidden(ref_data)\n\n        test_data = LSTMTestData(*clone_test_data(ref_data))\n\n        class ModelWrapper(nn.Module):\n            def __init__(self, lstm):\n                super().__init__()\n                self.lstm = lstm\n\n            def forward(self, *input_):\n                return self.lstm(*input_)\n\n        wrapped_ref_rnn = ModelWrapper(ref_rnn)\n        wrapped_test_rnn = replace_lstm(copy.deepcopy(wrapped_ref_rnn))\n        test_rnn = wrapped_test_rnn.lstm\n        test_hidden = None if empty_initial else self.get_test_lstm_hidden(test_data)\n\n        if is_cuda:\n            ref_rnn.cuda()\n            test_rnn.cuda()\n        ref_output, (ref_hn, ref_cn) = ref_rnn(ref_data.x, ref_hidden)\n        test_output, (test_hn, test_cn) = test_rnn(test_data.x, test_hidden)\n\n        torch.testing.assert_allclose(test_hn[0], ref_hn[0], rtol=1e-3, atol=1e-4)\n        torch.testing.assert_allclose(test_cn[0], ref_cn[0], rtol=1e-3, atol=1e-4)\n        if variable_length:\n            torch.testing.assert_allclose(test_output.batch_sizes, ref_output.batch_sizes)\n            torch.testing.assert_allclose(test_output.data, ref_output.data, rtol=1e-2, atol=1e-3)\n            if not sorted_:\n                torch.testing.assert_allclose(test_output.sorted_indices, ref_output.sorted_indices)\n                torch.testing.assert_allclose(test_output.unsorted_indices, ref_output.unsorted_indices)\n        else:\n            torch.testing.assert_allclose(test_output, ref_output, rtol=1e-2, atol=1e-3)\n\n    def test_backward_lstm(self, sizes, bidirectional, num_layers, bias, batch_first, variable_length, sorted_, is_cuda,\n                           empty_initial, dropout, _seed):\n\n        num_directions = 2 if bidirectional else 1\n\n        p = sizes\n\n        ref_data = TestLSTMCell.generate_lstm_data(p, num_layers, num_directions, variable_length, sorted_, batch_first,\n                                                   is_cuda, bias, empty_initial, True)\n\n        ref_rnn = nn.LSTM(input_size=p.input_size, hidden_size=p.hidden_size, num_layers=num_layers,\n                          bidirectional=bidirectional, batch_first=batch_first, bias=bias, dropout=dropout)\n        self.set_ref_lstm_weights(ref_data, ref_rnn, num_layers, num_directions, bias)\n        ref_hidden = None if empty_initial else self.get_ref_lstm_hidden(ref_data)\n\n        test_data = LSTMTestData(*clone_test_data(ref_data))\n        test_rnn = replace_lstm(copy.deepcopy(ref_rnn))\n        test_hidden = None if empty_initial else self.get_test_lstm_hidden(test_data)\n\n        if is_cuda:\n            ref_rnn.cuda()\n            test_rnn.cuda()\n\n        ref_output, _ = ref_rnn(ref_data.x, ref_hidden)\n        test_output, _ = test_rnn(test_data.x, test_hidden)\n\n        ref_output[0].sum().backward()\n        test_output[0].sum().backward()\n\n        ref_grads = get_grads(self.flatten_nested_lists(ref_rnn.all_weights))\n        test_grads = get_grads(self.flatten_nested_lists(test_rnn.all_weights))\n        if not empty_initial:\n            # TODO: compare gradient of all hidden\n            ref_grads += get_grads([ref_data.h0[0], ref_data.c0[0]])\n            test_grads += get_grads([test_hidden[0][0], test_hidden[1][0]])\n        for (ref, test) in list(zip(test_grads, ref_grads)):\n            torch.testing.assert_allclose(test, ref, rtol=1e-1, atol=1e-1)\n\n    @classmethod\n    def flatten_nested_lists(cls, nested_list):\n        # type: (List) -> List[torch.Tensor]\n        return [tensor for tensor_tuple in nested_list for tensor in tensor_tuple]\n\n    @classmethod\n    def get_test_lstm_hidden(cls, data):\n        # type: (LSTMTestData) -> List[Tuple[torch.Tensor, ...]]\n        result = []\n        hidden_names = [\'h0\', \'c0\']\n        for name in hidden_names:\n            hidden_list = getattr(data, name)\n            element = ()\n            num_hidden = len(hidden_list)\n            for i in range(num_hidden):\n                element += (hidden_list[i],)\n            result.append(element)\n        return result\n\n    @classmethod\n    def get_ref_lstm_hidden(cls, data):\n        # type: (LSTMTestData) -> Tuple[torch.Tensor, torch.Tensor]\n        hidden = cls.get_test_lstm_hidden(data)\n        hidden_states = [torch.unsqueeze(tensor, dim=0) for tensor in hidden[0]]\n        cell_states = [torch.unsqueeze(tensor, dim=0) for tensor in hidden[1]]\n        return (\n            torch.cat(hidden_states, dim=0),\n            torch.cat(cell_states, dim=0)\n        )\n\n    @classmethod\n    def set_ref_lstm_weights(cls, data, nn_lstm, num_layers, num_directions, bias):\n        # type: (LSTMTestData, nn.LSTM, int, int, bool) -> None\n        for l in range(num_layers):\n            for d in range(num_directions):\n                i = l * num_directions + d\n                for name in cls.get_param_names(bias):\n                    suffix = \'_reverse\' if d == 1 else \'\'\n                    param = getattr(data, name)\n                    param_name = name + \'_l{}{}\'.format(l, suffix)\n                    getattr(nn_lstm, param_name).data.copy_(param[i].data)\n\n    @classmethod\n    def get_param_names(cls, bias):\n        # type: (bool) -> List[str]\n        suffixes = [\'ih\', \'hh\']\n        names = [\'weight_\' + suffix for suffix in suffixes]\n        if bias:\n            names += [\'bias_\' + suffix for suffix in suffixes]\n        return names\n\n\ndef test_export_stacked_bi_lstm(tmp_path):\n    p = LSTMTestSizes(3, 3, 3, 3)\n    config = get_empty_config(input_sample_size=(1, p.hidden_size, p.input_size))\n    config[\'compression\'] = {\'algorithm\': \'quantization\'}\n\n    # TODO: batch_first=True fails with building graph: ambiguous call to mul or sigmoid\n    test_rnn = NNCF_RNN(\'LSTM\', input_size=p.input_size, hidden_size=p.hidden_size, num_layers=2, bidirectional=True,\n                        batch_first=False)\n    model, algo = create_compressed_model_and_algo_for_test(test_rnn, config)\n\n    test_path = str(tmp_path.joinpath(\'test.onnx\'))\n    algo.export_model(test_path)\n    assert os.path.exists(test_path)\n\n    onnx_num = 0\n    model = onnx.load(test_path)\n    # pylint: disable=no-member\n    for node in model.graph.node:\n        if node.op_type == \'FakeQuantize\':\n            onnx_num += 1\n    assert onnx_num == 50\n\n\nclass TestNumberOfNodes:\n    logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n\n    def test_number_of_calling_fq_for_lstm(self):\n        p = LSTMTestSizes(1, 1, 1, 5)\n        num_layers = 2\n        bidirectional = True\n        num_directions = 2 if bidirectional else 1\n        bias = True\n        batch_first = False\n        config = get_empty_config(input_sample_size=(p.seq_length, p.batch, p.input_size))\n        config[\'compression\'] = {\'algorithm\': \'quantization\', \'quantize_inputs\': True}\n\n        test_data = TestLSTMCell.generate_lstm_data(p, num_layers, num_directions, bias=bias, batch_first=batch_first)\n\n        test_rnn = NNCF_RNN(\'LSTM\', input_size=p.input_size, hidden_size=p.hidden_size, num_layers=num_layers,\n                            bidirectional=bidirectional, bias=bias, batch_first=batch_first)\n        TestLSTM.set_ref_lstm_weights(test_data, test_rnn, num_layers, num_directions, bias)\n        test_hidden = TestLSTM.get_test_lstm_hidden(test_data)\n\n        model, algo = create_compressed_model_and_algo_for_test(test_rnn, config)\n\n        class Counter:\n            def __init__(self):\n                self.count = 0\n\n            def next(self):\n                self.count += 1\n\n        def hook(model, input_, counter):\n            counter.next()\n\n        counters = {}\n        for name, quantizer in algo.all_quantizations.items():\n            counter = Counter()\n            counters[name] = counter\n            quantizer.register_forward_pre_hook(partial(hook, counter=counter))\n        _ = model(test_data.x, test_hidden)\n        assert model.get_graph().get_nodes_count() == 107  # NB: may always fail in debug due to superfluous \'cat\' nodes\n        assert len(counters) == 50\n        for counter in counters.values():\n            assert counter.count == p.seq_length\n\n    def test_number_of_calling_fq_for_gnmt(self):\n        torch.cuda.set_device(0)\n        device = torch.device(\'cuda\')\n        batch_first = False\n        vocab_size = 32000\n        model_config = {\'hidden_size\': 100,\n                        \'vocab_size\': vocab_size,\n                        \'num_layers\': 4,\n                        \'dropout\': 0.2,\n                        \'batch_first\': batch_first,\n                        \'share_embedding\': True,\n                        }\n        batch_size = 128\n        sequence_size = 50\n        input_sample_size = (batch_size, sequence_size) if batch_first else (sequence_size, batch_size)\n        config = get_empty_config(input_sample_size=input_sample_size)\n        config[\'compression\'] = \\\n            {\'algorithm\': \'quantization\',\n             \'quantize_inputs\': True,\n             \'quantizable_subgraph_patterns\': [[""linear"", ""__add__""],\n                                               [""sigmoid"", ""__mul__"", ""__add__""],\n                                               [""__add__"", ""tanh"", ""__mul__""],\n                                               [""sigmoid"", ""__mul__""]],\n             \'disable_function_quantization_hooks\': True}\n        config[\'scopes_without_shape_matching\'] = \\\n            [\'GNMT/ResidualRecurrentDecoder[decoder]/RecurrentAttention[att_rnn]/BahdanauAttention[attn]\', ]\n\n        model = GNMT(**model_config)\n        model = replace_lstm(model)\n        model.to(device)\n\n        def dummy_forward_fn(model, seq_len=sequence_size):\n            def gen_packed_sequence():\n                seq_list = []\n                seq_lens = torch.LongTensor(batch_size).random_(1, seq_len + 1)\n                seq_lens = torch.sort(seq_lens, descending=True).values\n                for seq_size in seq_lens:\n                    seq_list.append(torch.LongTensor(seq_size.item()).random_(1, vocab_size).to(device))\n                padded_seq_batch = torch.nn.utils.rnn.pad_sequence(seq_list, batch_first=batch_first)\n                return padded_seq_batch, seq_lens\n\n            x_data, seq_lens = gen_packed_sequence()\n            input_encoder = x_data\n            input_enc_len = seq_lens.to(device)\n            input_decoder = gen_packed_sequence()[0]\n            model(input_encoder, input_enc_len, input_decoder)\n\n        algo, model = create_compressed_model(model, config, dummy_forward_fn, dump_graphs=False)\n        model.to(device)\n\n        class Counter:\n            def __init__(self):\n                self.count = 0\n\n            def next(self):\n                self.count += 1\n\n        def hook(model, input_, counter):\n            counter.next()\n\n        counters = {}\n        for name, quantizer in algo.all_quantizations.items():\n            counter = Counter()\n            counters[str(name)] = counter\n            quantizer.register_forward_pre_hook(partial(hook, counter=counter))\n        dummy_forward_fn(model)\n        assert model.get_graph().get_nodes_count() == 230  # NB: may always fail in debug due to superfluous \'cat\' nodes\n        assert len(counters) == 55\n        for name, counter in counters.items():\n            if \'cell\' in name or ""LSTMCellForwardNNCF"" in name:\n                assert counter.count == sequence_size, name\n            else:\n                assert counter.count == 1, name\n        new_seq_len = int(sequence_size / 2)\n        dummy_forward_fn(model, new_seq_len)\n        assert model.get_graph().get_nodes_count() == 230  # NB: may always fail in debug due to superfluous \'cat\' nodes\n        assert len(counters) == 55\n        for name, counter in counters.items():\n            if \'cell\' in name or ""LSTMCellForwardNNCF"" in name:\n                assert counter.count == sequence_size + new_seq_len, name\n            else:\n                assert counter.count == 2, name\n\n    def test_number_of_nodes_for_module_in_loop(self):\n        num_iter = 5\n\n        class LoopModule(nn.Module):\n            @ITERATION_MODULES.register(\'Inner\')\n            class Inner(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.operator1 = torch.sigmoid\n                    self.operator2 = torch.tanh\n\n                def forward(self, x):\n                    s = self.operator1(x)\n                    t = self.operator2(x)\n                    result = t + s\n                    return result\n\n                @staticmethod\n                def nodes_number():\n                    return 3\n\n            def __init__(self):\n                super().__init__()\n                self.inner = self.Inner()\n\n            def forward(self, x):\n                for _ in range(num_iter):\n                    x = self.inner(x)\n                return x\n\n            def nodes_number(self):\n                return self.inner.nodes_number()\n\n        test_module = LoopModule()\n        context = TracingContext()\n        with context as ctx:\n            _ = test_module(torch.zeros(1))\n            assert ctx.graph.get_nodes_count() == test_module.nodes_number()\n\n    def test_number_of_nodes_for_module_in_loop__not_input_node(self):\n        num_iter = 5\n\n        class LoopModule(nn.Module):\n            class Inner(nn.Module):\n                def forward(self, x):\n                    s = F.sigmoid(x)\n                    t = F.tanh(x)\n                    result = F.sigmoid(x) * t + F.tanh(x) * s\n                    return result\n\n                @staticmethod\n                def nodes_number():\n                    return 7\n\n            def __init__(self):\n                super().__init__()\n                self.inner = self.Inner()\n\n            def forward(self, x):\n                for _ in range(num_iter):\n                    x = self.inner(F.relu(x))\n                return x\n\n            def nodes_number(self):\n                return self.inner.nodes_number() + num_iter\n\n        test_module = LoopModule()\n        context = TracingContext()\n        with context as ctx:\n            _ = test_module(torch.zeros(1))\n            assert ctx.graph.get_nodes_count() == test_module.nodes_number()\n\n    def test_number_of_nodes_for_module_with_nested_loops(self):\n        num_iter = 5\n\n        class TestIterModule(nn.Module):\n            @ITERATION_MODULES.register()\n            class TestIterModule_ResetPoint(nn.Module):\n                def __init__(self, loop_module):\n                    super().__init__()\n                    self.loop_module = loop_module\n\n                def forward(self, x):\n                    return self.loop_module(F.relu(x))\n\n            def __init__(self):\n                super().__init__()\n                self.loop_module = self.LoopModule2()\n                self.reset_point = self.TestIterModule_ResetPoint(self.loop_module)\n\n            def forward(self, x):\n                for _ in range(num_iter):\n                    x = self.reset_point(x)\n                return x\n\n            class LoopModule2(nn.Module):\n\n                @ITERATION_MODULES.register()\n                class LoopModule2_ResetPoint(nn.Module):\n                    def __init__(self, inner):\n                        super().__init__()\n                        self.inner = inner\n\n                    def forward(self, x):\n                        return self.inner(F.relu(x))\n\n                def __init__(self):\n                    super().__init__()\n                    self.inner = self.Inner()\n                    self.reset_helper = self.LoopModule2_ResetPoint(self.inner)\n\n                def forward(self, x):\n                    for _ in range(num_iter):\n                        self.reset_helper(x)\n                    return x\n\n                class Inner(nn.Module):\n                    def forward(self, x):\n                        s = F.sigmoid(x)\n                        t = F.tanh(x)\n                        result = t + s\n                        return result\n\n        test_module = TestIterModule()\n        context = TracingContext()\n        with context as ctx:\n            _ = test_module(torch.zeros(1))\n            assert ctx.graph.get_nodes_count() == num_iter\n\n    def test_number_of_nodes_for_repeated_module(self):\n\n        class LoopModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.operator = F.relu\n                self.layers = nn.ModuleList([\n                    nn.Conv2d(1, 1, 1),\n                    nn.Conv2d(1, 1, 1)\n                ])\n\n            def forward(self, x):\n                for layer in self.layers:\n                    x = F.relu(layer(x))\n                return x\n\n        test_module = LoopModule()\n        context = TracingContext()\n        with context as ctx:\n            x = test_module(torch.zeros(1, 1, 1, 1))\n            assert ctx.graph.get_nodes_count() == 4  # NB: may always fail in debug due to superfluous \'cat\' nodes\n            _ = test_module(x)\n            assert ctx.graph.get_nodes_count() == 8  # NB: may always fail in debug due to superfluous \'cat\' nodes\n'"
pytorch_toolkit/nncf/tests/pruning/__init__.py,0,b''
pytorch_toolkit/nncf/tests/pruning/test_common.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport pytest\n\nfrom nncf.pruning.schedulers import BaselinePruningScheduler, ExponentialPruningScheduler, \\\n    ExponentialWithBiasPruningScheduler\nfrom tests.pruning.test_helpers import PruningTestModel, get_basic_pruning_config\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test\n\n\n@pytest.mark.parametrize(\'algo\',\n                         (\'filter_pruning\', ))\n@pytest.mark.parametrize((\'scheduler\', \'scheduler_class\'),\n                         (\n                             (\'baseline\', BaselinePruningScheduler),\n                             (\'exponential\', ExponentialPruningScheduler),\n                             (\'exponential_with_bias\', ExponentialWithBiasPruningScheduler),\n                         ))\ndef test_can_choose_scheduler(algo, scheduler, scheduler_class):\n    config = get_basic_pruning_config()\n    config[\'compression\'][\'algorithm\'] = algo\n    config[\'compression\'][\'params\'][\'schedule\'] = scheduler\n    model = PruningTestModel()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    scheduler = compression_ctrl.scheduler\n    assert isinstance(scheduler, scheduler_class)\n\n\n@pytest.mark.parametrize(\n    (""algo"", ""ref_scheduler"", ""ref_scheduler_params""),\n    ((\'filter_pruning\', BaselinePruningScheduler, {\'num_init_steps\': 0, ""pruning_steps"": 100,\n                                                   ""initial_pruning"": 0, ""pruning_target"": 0.5}),)\n)\ndef test_check_default_scheduler_params(algo, ref_scheduler, ref_scheduler_params):\n    config = get_basic_pruning_config()\n    config[\'compression\'][\'algorithm\'] = algo\n    model = PruningTestModel()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    scheduler = compression_ctrl.scheduler\n    assert isinstance(scheduler, ref_scheduler)\n    for key, value in ref_scheduler_params.items():\n        assert getattr(scheduler, key) == value\n'"
pytorch_toolkit/nncf/tests/pruning/test_helpers.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom torch import nn\n\nfrom nncf.config import Config\nfrom nncf.dynamic_graph.graph_builder import create_input_infos\nfrom tests.quantization.test_algo_quantization import OnesDatasetMock\nfrom tests.test_helpers import create_conv\n\n\nclass PruningTestModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = create_conv(1, 3, 2, 9, -2)\n        self.relu = nn.ReLU()\n        self.conv2 = create_conv(3, 1, 3, -10, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        return x\n\n\nclass PruningTestModelBranching(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = create_conv(1, 3, 2, 1, -2)\n        self.conv2 = create_conv(1, 3, 2, 2, -2)\n        self.conv3 = create_conv(1, 3, 2, 3, -2)\n        self.relu = nn.ReLU()\n        self.conv4 = create_conv(3, 1, 3, 10, 0)\n        self.conv5 = create_conv(3, 1, 3, -10, 0)\n\n    def forward(self, x):\n        x = self.conv1(x) + self.conv2(x) + self.conv3(x)\n        x = self.relu(x)\n        x = self.conv4(x) + self.conv5(x)\n        return x\n\n\nclass BigPruningTestModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = create_conv(1, 16, 2, 0, 1)\n        for i in range(16):\n            self.conv1.weight.data[i] += i\n        self.relu = nn.ReLU()\n        self.conv2 = create_conv(16, 32, 3, 20, 0)\n        for i in range(32):\n            self.conv2.weight.data[i] += i\n        self.bn = nn.BatchNorm2d(32)\n\n        self.conv3 = create_conv(32, 1, 5, 5, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = x.view(1, -1)\n        return x\n\n\ndef get_basic_pruning_config(input_sample_size=(1, 1, 4, 4)):\n    config = Config()\n    config.update({\n        ""model"": ""pruning_conv_model"",\n        ""input_info"":\n            {\n                ""sample_size"": input_sample_size,\n            },\n        ""compression"":\n            {\n                ""params"": {\n                }\n            }\n    })\n    return config\n\n\ndef get_pruning_baseline_config():\n    config = get_basic_pruning_config()\n    # Filling params\n    compression_config = config[\'compression\']\n    compression_config[\'params\'][""schedule""] = ""baseline""\n    compression_config[\'params\'][""num_init_steps""] = 1\n    return config\n\n\ndef get_pruning_exponential_config():\n    config = get_basic_pruning_config()\n    # Filling params\n    compression_config = config[\'compression\']\n    compression_config[\'params\'][""schedule""] = ""exponential_with_bias""\n    compression_config[\'params\'][""num_init_steps""] = 1\n    compression_config[\'params\'][""pruning_steps""] = 20\n    return config\n\n\ndef create_dataloader(config):\n    input_infos_list = create_input_infos(config)\n    input_sample_size = input_infos_list[0].shape\n    data_loader = torch.utils.data.DataLoader(OnesDatasetMock(input_sample_size[1:]),\n                                              batch_size=1,\n                                              num_workers=1,\n                                              shuffle=False)\n    return data_loader\n'"
pytorch_toolkit/nncf/tests/pruning/test_schedulers.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport numpy as np\nimport pytest\n\nfrom nncf.pruning.schedulers import BaselinePruningScheduler, ExponentialWithBiasPruningScheduler\nfrom tests.pruning.test_helpers import get_pruning_baseline_config, PruningTestModel, get_pruning_exponential_config\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test\n\n\ndef test_baseline_scheduler():\n    """"""\n    Test baseline scheduler parameters and changes of params during epochs.\n    """"""\n    config = get_pruning_baseline_config()\n    config[\'compression\'][\'algorithm\'] = \'filter_pruning\'\n    model = PruningTestModel()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    scheduler = compression_ctrl.scheduler\n\n    # Check default params\n    assert isinstance(scheduler, BaselinePruningScheduler)\n    assert pytest.approx(scheduler.pruning_target) == 0.5\n    assert pytest.approx(scheduler.initial_pruning) == 0.0\n    assert scheduler.num_init_steps == 1\n\n    # Check pruning params on epoch 0\n    assert pytest.approx(scheduler.current_pruning_level) == 0.0\n    assert pytest.approx(compression_ctrl.pruning_rate) == 0.0\n    assert scheduler.last_epoch == 0\n    assert compression_ctrl.frozen is False\n\n    # Check pruning params on epoch 1\n    scheduler.epoch_step()\n    assert pytest.approx(scheduler.current_pruning_level) == 0.5\n    assert pytest.approx(compression_ctrl.pruning_rate) == 0.5\n    assert scheduler.last_epoch == 1\n    assert compression_ctrl.frozen is True\n\n\ndef test_exponential_scheduler():\n    """"""\n    Test exponential with bias scheduler parameters and changes of params during epochs.\n    """"""\n    config = get_pruning_exponential_config()\n    config[\'compression\'][\'algorithm\'] = \'filter_pruning\'\n    model = PruningTestModel()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    scheduler = compression_ctrl.scheduler\n\n    # Check default params\n    assert isinstance(scheduler, ExponentialWithBiasPruningScheduler)\n    assert pytest.approx(scheduler.pruning_target) == 0.5\n    assert pytest.approx(scheduler.initial_pruning) == 0.0\n    assert scheduler.num_init_steps == 1\n    assert scheduler.pruning_steps == 20\n    assert pytest.approx(scheduler.a, abs=1e-4) == -0.5\n    assert pytest.approx(scheduler.b, abs=1e-4) == 0.5\n    assert pytest.approx(scheduler.k, abs=1e-4) == 0.5544\n\n    # Check pruning params on epoch 0\n    assert pytest.approx(scheduler.current_pruning_level) == 0.0\n    assert pytest.approx(compression_ctrl.pruning_rate) == 0.0\n    assert compression_ctrl.frozen is False\n    assert scheduler.last_epoch == 0\n\n    # Check pruning params on epoch 1 - 20\n    for i in range(20):\n        # Check pruning params on epoch 2\n        scheduler.epoch_step()\n        pruning_rate = scheduler.a * np.exp(\n            -scheduler.k * (scheduler.last_epoch - scheduler.num_init_steps)) + scheduler.b\n        assert pytest.approx(scheduler.current_pruning_level) == pruning_rate\n        assert pytest.approx(compression_ctrl.pruning_rate) == pruning_rate\n        assert compression_ctrl.frozen is False\n        assert scheduler.last_epoch == i + 1\n\n    # Check pruning params on epoch 3\n    scheduler.epoch_step()\n    assert pytest.approx(scheduler.current_pruning_level) == 0.5\n    assert pytest.approx(compression_ctrl.pruning_rate) == 0.5\n    assert compression_ctrl.frozen is True\n    assert scheduler.last_epoch == 21\n'"
pytorch_toolkit/nncf/tests/pruning/test_utils.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport pytest\n\nfrom nncf.dynamic_graph.context import Scope\nfrom nncf.pruning.filter_pruning.algo import FilterPruningBuilder\nfrom nncf.pruning.utils import get_rounded_pruned_element_number, get_bn_for_module_scope, \\\n    get_first_pruned_modules, get_last_pruned_modules\nfrom tests.pruning.test_helpers import get_basic_pruning_config, BigPruningTestModel, \\\n    PruningTestModelBranching\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test\n\n\n# pylint: disable=protected-access\n@pytest.mark.parametrize(""total,sparsity_rate,multiple_of,ref"",\n                         [(20, 0.2, None, 4),\n                          (20, 0.2, 8, 4),\n                          (20, 0.1, 2, 2),\n                          (20, 0.1, 5, 0),\n                          (20, 0.5, None, 4)\n                          ])\ndef test_get_rounded_pruned_element_number(total, sparsity_rate, multiple_of, ref):\n    if multiple_of is not None:\n        result = get_rounded_pruned_element_number(total, sparsity_rate, multiple_of)\n    else:\n        result = get_rounded_pruned_element_number(total, sparsity_rate)\n    assert ref == result\n\n    if multiple_of is not None:\n        assert (total - result) % multiple_of == 0\n\n\ndef test_get_bn_for_module_scope():\n    config = get_basic_pruning_config(input_sample_size=(1, 1, 8, 8))\n    config[\'compression\'][\'algorithm\'] = \'filter_pruning\'\n    pruned_model, _ = create_compressed_model_and_algo_for_test(BigPruningTestModel(), config)\n\n    conv1_scope = Scope.from_str(\'BigPruningTestModel/NNCFConv2d[conv1]\')\n    bn = get_bn_for_module_scope(pruned_model, conv1_scope)\n    assert bn is None\n\n    conv2_scope = Scope.from_str(\'BigPruningTestModel/NNCFConv2d[conv2]\')\n    bn = get_bn_for_module_scope(pruned_model, conv2_scope)\n    assert bn == pruned_model.bn\n\n    conv3_scope = Scope.from_str(\'BigPruningTestModel/NNCFConv2d[conv3]\')\n    bn = get_bn_for_module_scope(pruned_model, conv3_scope)\n    assert bn is None\n\n\n@pytest.mark.parametrize((\'model\', \'ref_first_module_names\'),\n                         [(BigPruningTestModel, [\'conv1\']),\n                          (PruningTestModelBranching, [\'conv1\', \'conv2\', \'conv3\']),\n                          ],\n                         )\ndef test_get_first_pruned_layers(model, ref_first_module_names):\n    config = get_basic_pruning_config(input_sample_size=(1, 1, 8, 8))\n    config[\'compression\'][\'algorithm\'] = \'filter_pruning\'\n    pruned_model, _ = create_compressed_model_and_algo_for_test(model(), config)\n\n    first_pruned_modules = get_first_pruned_modules(pruned_model,\n                                                    FilterPruningBuilder(config).get_types_of_pruned_modules())\n    ref_first_modules = [getattr(pruned_model, module_name) for module_name in ref_first_module_names]\n    assert set(first_pruned_modules) == set(ref_first_modules)\n\n\n@pytest.mark.parametrize((\'model\', \'ref_last_module_names\'),\n                         [(BigPruningTestModel, [\'conv3\']),\n                          (PruningTestModelBranching, [\'conv4\', \'conv5\']\n                           ),\n                          ],\n                         )\ndef test_get_last_pruned_layers(model, ref_last_module_names):\n    config = get_basic_pruning_config(input_sample_size=(1, 1, 8, 8))\n    config[\'compression\'][\'algorithm\'] = \'filter_pruning\'\n    pruned_model, _ = create_compressed_model_and_algo_for_test(model(), config)\n\n    first_pruned_modules = get_last_pruned_modules(pruned_model,\n                                                   FilterPruningBuilder(config).get_types_of_pruned_modules())\n    ref_last_modules = [getattr(pruned_model, module_name) for module_name in ref_last_module_names]\n    assert set(first_pruned_modules) == set(ref_last_modules)\n'"
pytorch_toolkit/nncf/tests/quantization/__init__.py,0,b''
pytorch_toolkit/nncf/tests/quantization/test_algo_quantization.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport pytest\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom copy import deepcopy\nfrom functools import partial\nfrom pytest import approx\nfrom torchvision.models import resnet50\n\nfrom examples.common.models.classification import squeezenet1_1_custom\nfrom nncf import utils\nfrom nncf.algo_selector import create_compression_algorithm_builders\nfrom nncf.compression_method_api import CompressionLoss, CompressionScheduler\nfrom nncf.config import Config\nfrom nncf.dynamic_graph.context import ScopeElement, Scope\nfrom nncf.dynamic_graph.graph_builder import create_input_infos\nfrom nncf.checkpoint_loading import load_state\nfrom nncf.hw_config import HWConfigType\nfrom nncf.initialization import InitializingDataLoader\nfrom nncf.layers import NNCFConv2d\nfrom nncf.module_operations import UpdateWeight, UpdateInputs\nfrom nncf.nncf_network import CompressionModuleType\nfrom nncf.quantization.algo import QuantizationController, QuantizationBuilder\nfrom nncf.quantization.layers import QuantizationMode, QuantizerConfig, SymmetricQuantizer, AsymmetricQuantizer, \\\n    INITIALIZABLE_MODULES, BaseQuantizer, QUANTIZATION_MODULES\nfrom nncf.utils import get_all_modules_by_type, safe_thread_call\nfrom tests.test_helpers import BasicConvTestModel, TwoConvTestModel, get_empty_config, \\\n    create_compressed_model_and_algo_for_test, MockModel, create_conv\n\n\ndef get_basic_quantization_config(model_size=4):\n    config = Config()\n    config.update({\n        ""model"": ""basic_quant_conv"",\n        ""model_size"": model_size,\n        ""input_info"":\n            {\n                ""sample_size"": (1, 1, model_size, model_size),\n            },\n        ""compression"":\n            {\n                ""algorithm"": ""quantization"",\n                ""initializer"": {\n                    ""range"": {\n                        ""num_init_steps"": 0\n                    }\n                },\n                ""params"": {}\n            }\n    })\n    return config\n\n\ndef get_basic_asym_quantization_config(model_size=4):\n    config = get_basic_quantization_config(model_size)\n    config[\'compression\'][\'activations\'] = {""mode"": ""asymmetric""}\n    config[\'compression\'][\'initializer\'][\'range\'] = {""mode"": ""asymmetric""}\n    return config\n\n\ndef get_squeezenet_quantization_config(model_size=32, batch_size=3):\n    config = get_basic_quantization_config(model_size)\n    config[\'model\'] = \'squeezenet1_1_custom\'\n    config[\'input_info\'] = {\n        ""sample_size"": (batch_size, 3, model_size, model_size),\n    }\n    return config\n\n\ndef split_quantizers(quant_model):\n    quantizers = get_all_modules_by_type(quant_model, list(INITIALIZABLE_MODULES.registry_dict.keys()))\n    weight_quantizers = []\n    activation_quantizers = []\n    for name, data in quantizers.items():\n        if \'UpdateWeight\' in name:\n            weight_quantizers.append(data)\n        else:\n            activation_quantizers.append(data)\n    return weight_quantizers, activation_quantizers\n\n\ndef compare_qconfigs(config: QuantizerConfig, quantizer: BaseQuantizer):\n    assert config.is_weights == quantizer.is_weights\n    assert config.bits == quantizer.num_bits\n    assert isinstance(quantizer, QUANTIZATION_MODULES.get(config.mode))\n    assert config.per_channel == quantizer.per_channel\n    assert config.signedness_to_force == quantizer.signedness_to_force\n\n\ndef test_quantization_configs__with_defaults():\n    model = BasicConvTestModel()\n    config = get_basic_quantization_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    assert isinstance(compression_ctrl, QuantizationController)\n    weight_quantizers = compression_ctrl.weight_quantizers\n    activation_quantizers = compression_ctrl.non_weight_quantizers\n\n    ref_weight_qconfig = QuantizerConfig(8, QuantizationMode.SYMMETRIC, None, False, None, True)\n    for wq in weight_quantizers.values():\n        compare_qconfigs(ref_weight_qconfig, wq)\n\n    ref_activation_qconfig = QuantizerConfig(8, QuantizationMode.SYMMETRIC, None, False, None, False)\n    for wq in activation_quantizers.values():\n        compare_qconfigs(ref_activation_qconfig, wq)\n\n\ndef test_quantization_configs__custom():\n    model = BasicConvTestModel()\n\n    config = get_basic_quantization_config()\n    config[\'compression\'].update({\n        ""weights"": {\n            ""mode"": ""asymmetric"",\n            ""per_channel"": True,\n            ""bits"": 4\n        },\n        ""activations"": {\n            ""mode"": ""asymmetric"",\n            ""bits"": 4,\n            ""signed"": True,\n        },\n    })\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    assert isinstance(compression_ctrl, QuantizationController)\n    weight_quantizers = compression_ctrl.weight_quantizers\n    activation_quantizers = compression_ctrl.non_weight_quantizers\n\n    ref_weight_qconfig = QuantizerConfig(bits=4,\n                                         mode=QuantizationMode.ASYMMETRIC,\n                                         signedness_to_force=None,\n                                         per_channel=True,\n                                         input_shape=None,\n                                         is_weights=True)\n    for wq in weight_quantizers.values():\n        compare_qconfigs(ref_weight_qconfig, wq)\n\n    ref_activation_qconfig = QuantizerConfig(bits=4,\n                                             mode=QuantizationMode.ASYMMETRIC,\n                                             signedness_to_force=True,\n                                             per_channel=False,\n                                             input_shape=None,\n                                             is_weights=False)\n    for wq in activation_quantizers.values():\n        compare_qconfigs(ref_activation_qconfig, wq)\n\n\n#       fq_2\n#        \\\n# fq_2 - conv_1 - fq_6\n#                   \\\n#        fq_4       add\n#         \\         /\n# fq_4 - conv_2 - fq_6\n#\ndef test_quantization_configs__with_precisions_list():\n    class ModelForTest(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = create_conv(1, 2, 2, -1, -2)\n            self.conv2 = create_conv(1, 2, 2, -1, -2)\n\n        def forward(self, x):\n            return self.conv1(x) + self.conv2(x)\n\n    model = ModelForTest()\n\n    config = get_basic_quantization_config()\n    config[\'compression\'].update({\n        ""initializer"": {\n            ""precision"": {\n                ""bitwidth_per_scope"":\n                    [[2, \'ModelForTest/NNCFConv2d[conv1]\'],\n                     [4, \'ModelForTest/NNCFConv2d[conv2]\']]\n            },\n        },\n        ""activations"": {\n            ""bits"": 6\n        }\n    })\n    model, compression_ctrl = \\\n        create_compressed_model_and_algo_for_test(model, config)  # type: NNCFNetwork, QuantizationController\n\n    device = next(model.parameters()).device\n    data_loader = TestInit.create_dataloader(False, config, device)\n    compression_ctrl.initialize(data_loader)\n\n    ref_bits = [(\'ModelForTest/NNCFConv2d[conv1]module_weight\', 2),\n                (\'ModelForTest/NNCFConv2d[conv2]module_weight\', 4),\n                (\'ModelForTest/NNCFConv2d[conv2]/conv2d_0\', 6),\n                (\'ModelForTest/NNCFConv2d[conv1]/conv2d_0\', 6),\n                (\'ModelForTest/NNCFConv2d[conv1]module_input\', 2),\n                (\'ModelForTest/NNCFConv2d[conv2]module_input\', 4)]\n\n    for key, quantizer in compression_ctrl.all_quantizations.items():\n        expected_bit = [ref_bit for (name, ref_bit) in ref_bits if name == str(key)][0]\n        assert quantizer.num_bits == expected_bit\n\n    ref_rows = [[\'2\', \'16.667\', \'16.667\', \'33.333\'],\n                [\'4\', \'16.667\', \'16.667\', \'33.333\'],\n                [\'6\', \'0\', \'33.333\', \'33.333\']]\n    table = compression_ctrl.get_bit_stats()\n    # pylint: disable=protected-access\n    assert table._rows == ref_rows\n\n\ndef compare_weights_activation_quantizers_pairs(actual_pairs, algo, ref_pair_names, model_name):\n    def get_name(name):\n        return \'/\'.join([model_name, name])\n\n    all_quantizations = {str(key): quantizer for key, quantizer in algo.all_quantizations.items()}\n    assert len(actual_pairs) == len(ref_pair_names)\n    for (wqs, aq), (wqs_names, aq_name) in zip(actual_pairs, ref_pair_names):\n        assert not aq.is_weights\n        assert aq == all_quantizations[get_name(aq_name)]\n        ref_weight_quantizers = [all_quantizations[get_name(name)] for name in wqs_names]\n        for weight_quantizer in wqs:\n            assert weight_quantizer.is_weights\n            assert weight_quantizer in ref_weight_quantizers\n\n\n#\n#  fq           fq\n#   \\            \\\n# \xd1\x81onv0 - fq - conv1\n#   /\n# fq\n#\ndef test_get_weight_activation_pairs():\n    model_cls = TwoConvTestModel\n    config = get_basic_quantization_config()\n    _, algo = create_compressed_model_and_algo_for_test(model_cls(), config)\n\n    actual_pairs = algo.get_weights_activation_quantizers_pairs()\n    ref_pair_names = [([\'Sequential[features]/Sequential[0]/NNCFConv2d[0]module_weight\'],\n                       \'Sequential[features]/Sequential[0]/NNCFConv2d[0]module_input\',\n                       ),\n                      ([\'Sequential[features]/Sequential[1]/NNCFConv2d[0]module_weight\'],\n                       \'Sequential[features]/Sequential[0]/NNCFConv2d[0]/conv2d_0\',\n                       )]\n\n    compare_weights_activation_quantizers_pairs(actual_pairs, algo, ref_pair_names, model_cls.__name__)\n\n\nclass DoubleWeightsPerActivation(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = []\n        self.conv1 = create_conv(1, 2, 2, -1, -2)\n        self.conv2 = create_conv(1, 2, 2, -1, -2)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(x)\n        return self.conv1(x), self.conv2(x)\n\n\n#              fq\n#             /\n#          conv2d\n#         /\n# relu - fq     fq\n#         \\    /\n#         conv2d\n#\ndef test_get_weight_activation_pairs__with_double_weights_per_activation():\n    model_cls = DoubleWeightsPerActivation\n    model_name = model_cls.__name__\n    config = get_basic_quantization_config()\n\n    _, algo = create_compressed_model_and_algo_for_test(model_cls(), config)\n\n    actual_pairs = algo.get_weights_activation_quantizers_pairs()\n    ref_pair_names = [([\'NNCFConv2d[conv1]module_weight\', \'NNCFConv2d[conv2]module_weight\'],\n                       \'ReLU[relu]/RELU_0\')]\n\n    compare_weights_activation_quantizers_pairs(actual_pairs, algo, ref_pair_names, model_name)\n\n\nclass DoubleWeightsPerActivationWithExtraModule(DoubleWeightsPerActivation):\n    def forward(self, x):\n        x = self.relu(x)\n        return self.conv1(torch.sigmoid(x)), self.conv2(torch.sigmoid(x))\n\n\n#                     fq\n#                      \\\n#         sigmoid - conv1d\n#         /\n# relu - fq           fq\n#         \\            \\\n#         sigmoid - conv2d\n#\ndef test_get_weight_activation_pairs__with_extra_module():\n    model_cls = DoubleWeightsPerActivationWithExtraModule\n    model_name = model_cls.__name__\n    config = get_basic_quantization_config()\n    config[""compression""].update({\n        ""quantizable_subgraph_patterns"": [[""sigmoid"", ""conv2d""]],\n        ""quantize_inputs"": False})\n\n    _, algo = create_compressed_model_and_algo_for_test(model_cls(), config)\n\n    actual_pairs = algo.get_weights_activation_quantizers_pairs()\n    ref_pair_names = [([\'NNCFConv2d[conv1]module_weight\', \'NNCFConv2d[conv2]module_weight\'],\n                       \'ReLU[relu]/RELU_0\')]\n\n    compare_weights_activation_quantizers_pairs(actual_pairs, algo, ref_pair_names, model_name)\n\n\nclass RankDatasetMock:\n    def __init__(self, input_size, rank):\n        self.input_size = input_size\n        self.rank = rank\n        super().__init__()\n\n    def __getitem__(self, index):\n        dummy_input = torch.ones(self.input_size) * (self.rank - 1) * 3\n        return dummy_input, torch.ones(1)\n\n    def __len__(self):\n        return 100\n\n\ndef scale_signed_dumping_worker(gpu, ngpus_per_node, config, tmp_path):\n    config.batch_size = 3\n    config.workers = 3\n    config.gpu = gpu\n    config.ngpus_per_node = ngpus_per_node\n    config.rank = gpu\n    config.distributed = True\n\n    torch.distributed.init_process_group(backend=""nccl"", init_method=\'tcp://127.0.0.1:8899\',\n                                         world_size=config.world_size, rank=config.rank)\n\n    model = safe_thread_call(partial(squeezenet1_1_custom, pretrained=True))\n\n    quant_model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    compression_ctrl.distributed()\n    compression_scheduler = compression_ctrl.scheduler\n\n    torch.cuda.set_device(config.gpu)\n    quant_model.cuda(config.gpu)\n    config.batch_size = int(config.batch_size / ngpus_per_node)\n    config.workers = int(config.workers / ngpus_per_node)\n    quant_model = torch.nn.parallel.DistributedDataParallel(quant_model, device_ids=[config.gpu])\n\n    criterion = torch.nn.MSELoss().cuda(config.gpu)\n    optimizer = torch.optim.Adam(quant_model.parameters(), lr=0.01)\n\n    torch.backends.cudnn.benchmark = True\n\n    input_infos_list = create_input_infos(config)\n    input_sample_size = input_infos_list[0].shape\n    data_loader = torch.utils.data.DataLoader(RankDatasetMock(input_sample_size[1:], config.rank),\n                                              batch_size=3,\n                                              num_workers=1,\n                                              shuffle=False)\n    # just to reproduce the same scale values without Dropout\n    quant_model.eval()\n    compression_ctrl.initialize(data_loader)\n\n    act_sum = 0\n    for layer in get_all_modules_by_type(quant_model, ""SymmetricQuantizer"").values():\n        act_sum += layer.scale\n    ref_sum = 3467.322\n    assert act_sum.item() == approx(ref_sum, 0.01), \\\n        \'sum of scales is not expected {} vs {} rank {}\'.format(act_sum.item(), ref_sum, config.rank)\n\n    out_file_path = get_path_after_broadcast(tmp_path, config.rank)\n    save_params(quant_model, out_file_path)\n    compression_scheduler.step()\n    for i, (input_, _) in enumerate(data_loader):\n        if i > 5:\n            break\n        output = quant_model(input_)\n        optimizer.zero_grad()\n        dummy_target = torch.randn(1000).cuda(config.gpu, non_blocking=True)\n        loss = criterion(output, dummy_target)\n        compression_scheduler.step()\n        loss.backward()\n        optimizer.step()\n        compression_scheduler.step()\n\n    out_file_path = get_path_path_after_train_iters(tmp_path, config.rank)\n    save_params(quant_model, out_file_path)\n\n\ndef get_path_path_after_train_iters(tmp_path, rank):\n    out_file_path = tmp_path / \'scale_signed_after_1_train_iter_gpu{}.pt\'.format(rank)\n    return out_file_path\n\n\ndef get_path_after_broadcast(tmp_path, rank):\n    out_file_path = tmp_path / \'scale_signed_after_broadcast_gpu{}.pt\'.format(rank)\n    return out_file_path\n\n\ndef save_params(model, out_file_path):\n    gpu_scale_signed_params = []\n    for _, layer in utils.get_all_modules_by_type(model, \'SymmetricQuantizer\').items():\n        gpu_scale_signed_params.append((layer.scale.to(torch.device(\'cpu\')),\n                                        layer.signed_tensor.to(torch.device(\'cpu\'))))\n    with out_file_path.open(\'wb\') as out_file:\n        torch.save(gpu_scale_signed_params, out_file)\n\n\ndef compare_multi_gpu_dump(config, tmp_path, get_path_fn):\n    mismatching = False\n    ref_file_path = get_path_fn(tmp_path, 0)\n    with ref_file_path.open(\'rb\') as ref_scale_file:\n        ref_scale_signed_params = torch.load(ref_scale_file)\n        for other_rank in range(1, config.world_size):\n            other_file_path = get_path_fn(tmp_path, other_rank)\n            with other_file_path.open(\'rb\') as in_file:\n                gpu_scale_signed_params = torch.load(in_file)\n                if ref_scale_signed_params != gpu_scale_signed_params:\n                    mismatching = True\n    return mismatching\n\n\ndef test_can_load_quant_algo__with_defaults():\n    model = BasicConvTestModel()\n    config = get_basic_quantization_config()\n    compression_algo_builder_list = create_compression_algorithm_builders(config)\n    assert len(compression_algo_builder_list) == 1\n    assert isinstance(compression_algo_builder_list[0], QuantizationBuilder)\n\n    quant_model, _ = create_compressed_model_and_algo_for_test(deepcopy(model), config)\n\n    model_conv = get_all_modules_by_type(model, \'Conv2d\')\n    quant_model_conv = get_all_modules_by_type(quant_model.get_nncf_wrapped_model(), \'NNCFConv2d\')\n    assert len(model_conv) == len(quant_model_conv)\n\n    for module_scope, _ in model_conv.items():\n        quant_scope = deepcopy(module_scope)  # type: Scope\n        quant_scope.pop()\n        quant_scope.push(ScopeElement(\'NNCFConv2d\', \'conv\'))\n        assert quant_scope in quant_model_conv.keys()\n\n        store = []\n        for op in quant_model_conv[quant_scope].pre_ops.values():\n            if isinstance(op, (UpdateInputs, UpdateWeight)) and isinstance(op.operand, SymmetricQuantizer):\n                assert op.__class__.__name__ not in store\n                store.append(op.__class__.__name__)\n        assert UpdateWeight.__name__ in store\n\n\ndef test_can_create_quant_loss_and_scheduler():\n    config = get_basic_quantization_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(MockModel(), config)\n\n    loss = compression_ctrl.loss\n    assert isinstance(loss, CompressionLoss)\n\n    scheduler = compression_ctrl.scheduler\n    assert isinstance(scheduler, CompressionScheduler)\n\n\ndef test_multiprocessing_distributed_shares_init_scales_signedness_across_gpus(tmp_path):\n    num_init_steps = 10\n\n    config = get_squeezenet_quantization_config()\n    config[\'compression\'][\'initializer\'] = {\'range\': {\'num_init_steps\': num_init_steps}}\n\n    ngpus_per_node = torch.cuda.device_count()\n    config.world_size = ngpus_per_node\n    torch.multiprocessing.spawn(scale_signed_dumping_worker,\n                                nprocs=ngpus_per_node,\n                                args=(ngpus_per_node, config, tmp_path),\n                                join=True)\n\n    assert not compare_multi_gpu_dump(config, tmp_path, get_path_after_broadcast)\n    assert not compare_multi_gpu_dump(config, tmp_path, get_path_path_after_train_iters)\n\n\nclass OnesDatasetMock:\n    def __init__(self, input_size):\n        self.input_size = input_size\n        super().__init__()\n\n    def __getitem__(self, index):\n        return torch.ones(self.input_size), torch.ones(1)\n\n    def __len__(self):\n        return 1\n\n\n@pytest.mark.parametrize(""wrap_dataloader"",\n                         (True, False),\n                         ids=[\'wrapped_dataloader\', \'standard_dataloader\'])\nclass TestInit:\n    @staticmethod\n    def create_algo_and_compressed_model(config):\n        model = TwoConvTestModel()\n        compressed_model, algo = create_compressed_model_and_algo_for_test(model, config)\n        return algo, compressed_model\n\n    @staticmethod\n    def create_config():\n        config = get_empty_config()\n        config[\'compression\'] = {\'algorithm\': \'quantization\', \'initializer\': {\'range\': {\'num_init_steps\': 1}}}\n        return config\n\n    @staticmethod\n    def create_dataloader(wrap_dataloader, config, device):\n        input_infos_list = create_input_infos(config)\n        input_sample_size = input_infos_list[0].shape\n        data_loader = torch.utils.data.DataLoader(OnesDatasetMock(input_sample_size[1:]),\n                                                  batch_size=1,\n                                                  num_workers=1,\n                                                  shuffle=False)\n        if wrap_dataloader:\n            data_loader = InitializingDataLoader(data_loader=data_loader,\n                                                 device=device,\n                                                 kwargs={})\n        return data_loader\n\n    @staticmethod\n    def check_sign_and_scale(model, ref_table):\n        model_conv = get_all_modules_by_type(model, \'SymmetricQuantizer\')\n        for scope, module in model_conv.items():\n            for pattern, ref_values in ref_table.items():\n                match = re.search(pattern, str(scope))\n                if match:\n                    assert isinstance(module, SymmetricQuantizer)\n                    assert module.signed == ref_values[0], \'sign is not matched for {}\'.format(str(scope))\n                    assert module.scale == ref_values[1], \'scale is not matched for {}\'.format(str(scope))\n\n    def test_scale_and_sign_init_for_quant_algo(self, wrap_dataloader):\n        config = self.create_config()\n        algo, compressed_model = self.create_algo_and_compressed_model(config)\n        device = next(compressed_model.parameters()).device\n        data_loader = self.create_dataloader(wrap_dataloader, config, device)\n\n        algo.initialize(data_loader)\n\n        self.check_sign_and_scale(compressed_model, {\n            \'.*Sequential\\\\[0\\\\].*UpdateWeight.*\': (True, 1),\n            \'.*Sequential\\\\[1\\\\].*UpdateWeight. *\': (False, 1),\n            \'.*activation_quantizers.*Sequential\\\\[0\\\\].*\': (True, 4),\n            \'.*activation_quantizers.*Sequential\\\\[1\\\\].*\': (True, 24)\n        })\n\n    def test_scale_and_sign_init_for_quant_algo__without_init_section(self, wrap_dataloader):\n        config = get_empty_config()\n        config[\'compression\'] = {\'algorithm\': \'quantization\'}\n\n        algo, compressed_model = self.create_algo_and_compressed_model(config)\n        device = next(compressed_model.parameters()).device\n        data_loader = self.create_dataloader(wrap_dataloader, config, device)\n\n        algo.initialize(data_loader)\n\n        self.check_sign_and_scale(compressed_model, {\n            \'.*Sequential\\\\[0\\\\].*UpdateWeight.*\': (True, 1),\n            \'.*Sequential\\\\[1\\\\].*UpdateWeight. *\': (False, 1),\n            \'.*activation_quantizers.*Sequential\\\\[0\\\\].*\': (True, 4),\n            \'.*activation_quantizers.*Sequential\\\\[1\\\\].*\': (True, 24)\n        })\n\n    def test_scale_and_sign_init_for_quant_algo__with_zero_init_steps(self, wrap_dataloader):\n        config = self.create_config()\n        config[\'compression\'][\'initializer\'][\'range\'][\'num_init_steps\'] = 0\n\n        algo, compressed_model = self.create_algo_and_compressed_model(config)\n        device = next(compressed_model.parameters()).device\n        data_loader = self.create_dataloader(wrap_dataloader, config, device)\n\n        algo.initialize(data_loader)\n\n        self.check_sign_and_scale(compressed_model, {\n            \'.*Sequential\\\\[0\\\\].*UpdateWeight.*\': (False, 1),\n            \'.*Sequential\\\\[1\\\\].*UpdateWeight. *\': (False, 1),\n            \'.*activation_quantizers.*Sequential\\\\[0\\\\].*\': (False, 1),\n            \'.*activation_quantizers.*Sequential\\\\[1\\\\].*\': (False, 1)\n        })\n\n    def test_scale_and_sign_init_for_quant_algo__after_load_state(self, wrap_dataloader):\n        config = self.create_config()\n        algo, compressed_model = self.create_algo_and_compressed_model(config)\n        load_state(compressed_model, {\n            \'module.features.0.0.pre_ops.0.op.signed_tensor\': torch.tensor([0.]),  # quantizer of 1st conv\'s weights\n            \'module.features.1.0.pre_ops.0.op.scale\': torch.tensor([100])  # quantizer of 2nd conv\'s weights\n        })\n\n        device = next(compressed_model.parameters()).device\n        data_loader = self.create_dataloader(wrap_dataloader, config, device)\n\n        algo.initialize(data_loader)\n\n        self.check_sign_and_scale(compressed_model, {\n            \'.*Sequential\\\\[0\\\\].*UpdateWeight.*\': (False, 1),\n            \'.*Sequential\\\\[1\\\\].*UpdateWeight. *\': (False, 100),\n            \'.*activation_quantizers.*Sequential\\\\[0\\\\].*\': (True, 4),\n            \'.*activation_quantizers.*Sequential\\\\[1\\\\].*\': (True, 24)\n        })\n\n    def test_scope_overrides(self, wrap_dataloader):\n        config = self.create_config()\n        config[""compression""][""scope_overrides""] = {\n            r""{re}NNCFConv2d\\[[0-9]*\\]$"": {\n                ""bits"": 7,\n                ""mode"": ""asymmetric"",\n            },\n            r""{re}NNCFConv2d\\[[0-9]*\\]/conv2d_0"": {\n                ""bits"": 7,\n                ""signed"": False,\n            }\n        }\n        algo, compressed_model = self.create_algo_and_compressed_model(config)\n\n        device = next(compressed_model.parameters()).device\n        data_loader = self.create_dataloader(wrap_dataloader, config, device)\n\n        algo.initialize(data_loader)\n\n        quantizers = get_all_modules_by_type(compressed_model, [\'SymmetricQuantizer\',\n                                                                \'AsymmetricQuantizer\'])\n        quantizer_str_dict = {str(k): v for k, v in quantizers.items()}\n        group_1 = [quantizer_str_dict[""NNCFNetwork/TwoConvTestModel[nncf_module]/Sequential[features]/""\n                                      ""Sequential[0]/NNCFConv2d[0]/ModuleDict[pre_ops]/UpdateWeight[0]/""\n                                      ""AsymmetricQuantizer[op]""],\n                   quantizer_str_dict[""NNCFNetwork/TwoConvTestModel[nncf_module]/Sequential[features]/""\n                                      ""Sequential[0]/NNCFConv2d[0]/ModuleDict[pre_ops]/UpdateInputs[1]/""\n                                      ""AsymmetricQuantizer[op]""],\n                   quantizer_str_dict[\'NNCFNetwork/TwoConvTestModel[nncf_module]/Sequential[features]/\'\n                                      \'Sequential[1]/NNCFConv2d[0]/ModuleDict[pre_ops]/UpdateWeight[0]/\'\n                                      \'AsymmetricQuantizer[op]\']\n                   ]\n        group_2 = [quantizer_str_dict[\'NNCFNetwork/ModuleDict[activation_quantizers]/\'\n                                      \'SymmetricQuantizer[TwoConvTestModel/Sequential[features]\'\n                                      \'/Sequential[0]/NNCFConv2d[0]/conv2d_0]\']]\n\n        for quantizer in group_1:\n            assert isinstance(quantizer, AsymmetricQuantizer)\n            assert quantizer.levels == 2 ** 7\n        for quantizer in group_2:\n            assert isinstance(quantizer, SymmetricQuantizer)\n            assert not quantizer.signed\n\n\ndef get_path_to_keys(tmp_path, rank):\n    return \'{}_{}\'.format(tmp_path, str(rank))\n\n\ndef activation_quantizers_dumping_worker(current_gpu, config, tmp_path):\n    model = resnet50(pretrained=False)\n    quant_model, _ = create_compressed_model_and_algo_for_test(model, config)\n    path = get_path_to_keys(tmp_path, current_gpu)\n    print(path)\n    with open(path, \'w\') as f:\n        f.writelines(""%s\\n"" % key for key in quant_model.activation_quantizers.keys())\n\n\ndef test_activation_quantizers_order_is_the_same__for_resnet50(tmp_path):\n    config = get_empty_config(input_sample_size=[1, 3, 224, 224])\n    config[\'compression\'] = {\'algorithm\': \'quantization\'}\n    ngpus_per_node = torch.cuda.device_count()\n\n    torch.multiprocessing.spawn(activation_quantizers_dumping_worker,\n                                nprocs=ngpus_per_node,\n                                args=(config, tmp_path),\n                                join=True)\n\n    with open(get_path_to_keys(tmp_path, 0), \'r\') as f:\n        ref_list = f.readlines()\n    for i in range(1, ngpus_per_node):\n        with open(get_path_to_keys(tmp_path, i), \'r\') as f:\n            curr_list = f.readlines()\n            assert curr_list == ref_list\n\n\ndef test_load_state_sets_initialized_flag():\n    config = get_basic_quantization_config()\n\n    model = TwoConvTestModel()\n    quant_model, _ = create_compressed_model_and_algo_for_test(model, config)\n\n    load_state(quant_model, {\n        \'module.features.0.0.pre_ops.0.op.signed_tensor\': torch.tensor([1.0]),  # quantizer of 1st conv\'s weights\n        \'module.features.1.0.pre_ops.0.op.scale\': torch.tensor([1.0])  # quantizer of 2nd conv\'s weights\n    })\n\n    quantizers = get_all_modules_by_type(quant_model, \'SymmetricQuantizer\')\n    for scope, module in quantizers.items():\n        if \'activation_quantizers\' in str(scope) or \'UpdateInputs\' in str(scope):\n            assert not module.initialized\n        else:\n            assert module.initialized\n\n\ndef test_quantize_has_proper_is_weights_flag():\n    class Model(nn.Module):\n        def __init__(self, size=1):\n            super().__init__()\n            self.size = size\n            self.conv = nn.Conv2d(size, size, size)\n\n        def forward(self, x):\n            return self.conv(x)\n\n    model = Model()\n    config = get_basic_quantization_config(model_size=2)\n    quant_model, _ = create_compressed_model_and_algo_for_test(model, config)\n\n    for module in quant_model.modules():\n        if isinstance(module, NNCFConv2d):\n            for op in module.pre_ops.values():\n                assert isinstance(op, (UpdateWeight, UpdateInputs))\n                assert op.operand.is_weights == isinstance(op, UpdateWeight)\n    for _, aq in quant_model.get_compression_modules_by_type(CompressionModuleType.ACTIVATION_QUANTIZER).items():\n        assert aq.is_weights is False\n\n\ndef test_can_quantize_free_operators(mocker):\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.weight = nn.Parameter(torch.ones([1]))\n            self.bias = nn.Parameter(torch.ones([1]))\n\n        def forward(self, x):\n            return F.linear(x, self.weight, self.bias)\n\n    mod = Model()\n    config = get_basic_quantization_config(model_size=1)\n\n    config[""compression""].update({""quantize_inputs"": False})\n    quant_model, _ = create_compressed_model_and_algo_for_test(mod, config)\n\n    quantizer_list = quant_model.get_compression_modules_by_type(CompressionModuleType.FUNCTION_QUANTIZER).values()\n    assert len(quantizer_list) == 2\n    for quantizer in quantizer_list:\n        mocker.spy(quantizer, \'quantize\')\n\n    quant_model.do_dummy_forward()\n    for quantizer in quantizer_list:\n        assert quantizer.quantize.call_count == 1\n\n\n@pytest.fixture(name=""hw_config_type"", params=HWConfigType)\ndef hw_config_type_(request):\n    return request.param\n\n\ndef test_hw_config_quantization_can_quantize_squeezenet(hw_config_type):\n    config = get_squeezenet_quantization_config()\n    config[""hw_config""] = hw_config_type.value\n    model = squeezenet1_1_custom()\n    create_compressed_model_and_algo_for_test(model, config)\n\n\nclass QuantizeInputsTestModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n        self.conv2 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n        self.conv3 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n        self.conv4 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n        self.conv5 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1)\n        self.conv6 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=2)\n        self.linear = nn.Linear(in_features=8, out_features=8)\n\n    #    (1)     (2)      (3)    (4)   (5)\n    #     |       |        |      |     |-----\\\n    #  (conv1)   (MP)     (MP)    (MP)  (MP)  |\n    #     |       |        |      |     |     |\n    #     |       |       (+)     |     |     |\n    #     |       |--\\     |      |     |     |\n    #     |       |   \\    |      |     |     |\n    #     |    (conv2) | (conv3)  |     |     |\n    #     |       |    |   |       \\   /      |\n    #     |     (AvP)  \\   |       (cat)      |\n    #     |       |     \\  |         |        |\n    #  (conv4) (linear)  \\ |      (conv6)     |\n    #     |       |      (cat)       |        |\n    #     |       |        |        (+)------/\n    #     |       |      (conv5)     |\n    #   (AvP)     |        |         |\n    #     |       |      (AvP)       |\n    #      \\      |        /         |\n    #       \\---(cat)---------------/\n\n    def forward(self, input_1, input_2, input_3, input_4, input_5):\n        x_1 = self.conv1(input_1)\n        x_1 = self.conv4(x_1)\n        x_1 = F.adaptive_avg_pool2d(x_1, output_size=1)\n        x_1 = x_1.flatten(start_dim=1)\n\n        x_2_br = F.max_pool2d(input_2, kernel_size=2)\n        x_2 = self.conv2(x_2_br)\n        x_2 = F.adaptive_avg_pool2d(x_2, output_size=1)\n        x_2 = x_2.flatten(start_dim=1)\n        x_2 = self.linear(x_2)\n\n        x_3 = F.max_pool2d(input_3, kernel_size=2)\n        x_3 = x_3 + torch.ones_like(x_3)\n        x_3 = self.conv3(x_3)\n        x_3 = x_3.flatten(start_dim=1)\n        x_2_br = x_2_br.flatten(start_dim=1)\n        x_3 = torch.cat([x_2_br, x_3], dim=-1)\n        x_3 = self.conv5(x_3.unsqueeze(2).unsqueeze(3).transpose(1, 2))\n        x_3 = F.adaptive_avg_pool2d(x_3, output_size=1)\n        x_3 = x_3.flatten(start_dim=1)\n\n        x_4 = F.max_pool2d(input_4, kernel_size=2)\n        x_5 = F.max_pool2d(input_5, kernel_size=2)\n        x_45 = torch.cat([x_4, x_5], dim=1)\n        x_45 = self.conv6(x_45)\n        x_45 = x_45.flatten(start_dim=1)\n        in_5_flat = input_5.flatten(start_dim=1)\n        x_45 += F.pad(input_5.flatten(start_dim=1), [0, x_45.shape[1] - in_5_flat.shape[1]])\n\n        return torch.cat([x_1, x_2, x_3, x_45], dim=-1)\n\n\ndef test_quantize_inputs():\n    model = QuantizeInputsTestModel()\n    config = get_basic_quantization_config()\n    config[""input_info""] = [\n        {\n            ""sample_size"": (2, 3, 32, 32),\n        },\n        {\n            ""sample_size"": (2, 3, 32, 32),\n        },\n        {\n            ""sample_size"": (2, 3, 32, 32),\n        },\n        {\n            ""sample_size"": (2, 3, 32, 32),\n        },\n        {\n            ""sample_size"": (2, 3, 32, 32),\n        }\n    ]\n\n    model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    REF_QUANTIZED_INPUT_MODULE_SCOPES = [\n        ""QuantizeInputsTestModel/NNCFConv2d[conv1]"",\n        ""QuantizeInputsTestModel/NNCFConv2d[conv2]"",\n        ""QuantizeInputsTestModel/NNCFConv2d[conv5]"",\n        ""QuantizeInputsTestModel/NNCFConv2d[conv6]"",\n    ]\n    for ref_qinput_module_scope_str in REF_QUANTIZED_INPUT_MODULE_SCOPES:\n        scope = Scope.from_str(ref_qinput_module_scope_str)\n        assert model.get_module_by_scope(scope) is not None\n        assert ref_qinput_module_scope_str in compression_ctrl.quantized_inputs_modules_registry\n\n    nncf_modules_dict = model.get_nncf_modules()\n    for scope, nncf_module in nncf_modules_dict.items():\n        scope_str = str(scope)\n        update_inputs_count = sum(1 for pre_op in nncf_module.pre_ops.values() if isinstance(pre_op, UpdateInputs))\n        if scope_str in REF_QUANTIZED_INPUT_MODULE_SCOPES:\n            assert update_inputs_count == 1\n        else:\n            assert update_inputs_count == 0\n'"
pytorch_toolkit/nncf/tests/quantization/test_functions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nimport pytest\nimport torch\nfrom torch.autograd import Variable\n\nfrom nncf.quantization.quantize_functions import asymmetric_quantize, symmetric_quantize\nfrom nncf.utils import sum_like\nfrom tests.test_helpers import get_grads, check_equal\n\nEPS = 1\n\n\nclass ReferenceQuantizeAsymmetric:\n    @staticmethod\n    def forward(input_, input_low, input_range, levels):\n        scale = (levels - 1) / input_range\n        output = input_.clip(min=input_low, max=input_low + input_range)\n        output -= input_low\n        output *= scale\n        output = output.round()\n        output = output / scale\n        output += input_low\n\n        return output\n\n    @staticmethod\n    def backward(grad_output, input_, input_low, input_range, output, level_low, level_high, range_sign):\n        mask_hi = (input_ > input_low + input_range).astype(float)\n        mask_lo = (input_ < input_low).astype(float)\n\n        mask_in = 1 - mask_hi - mask_lo\n        err = (output - input_) * np.reciprocal(input_range * range_sign)\n        grad_range = grad_output * (err * mask_in + range_sign * (level_low / level_high) * mask_lo + mask_hi)\n        grad_range = sum_like(grad_range, input_range)\n\n        grad_input = grad_output * mask_in\n\n        grad_low = grad_output * (mask_hi + mask_lo)\n        grad_low = sum_like(grad_low, input_low)\n        return [grad_input, grad_low, grad_range]\n\n    @staticmethod\n    def tune_range(input_low, input_range, levels):\n        input_high = input_range + input_low\n        input_low[input_low > 0] = 0\n        input_high[input_high < 0] = 0\n        n = levels - 1\n        scale = levels / (input_high - input_low)\n        zp = np.round(-input_low * scale)\n\n        new_input_low = np.where(zp < n, zp / (zp - n) * input_high, input_low)\n        new_input_high = np.where(zp > 0., (zp - n) / zp * input_low, input_high)\n\n        range_1 = input_high - new_input_low\n        range_2 = new_input_high - input_low\n\n        mask = (range_1 > range_2)\n        inv_mask = abs(1 - mask)\n\n        new_input_low = mask * new_input_low + inv_mask * input_low\n        new_input_range = inv_mask * new_input_high + mask * input_high - new_input_low\n\n        return new_input_low, new_input_range\n\n\ndef zero_grad(variables):\n    for variable in variables:\n        variable.grad.zero_()\n\n\ndef idfn(val):\n    if isinstance(val, list):\n        return \'[{}]\'.format(\'-\'.join([str(v) for v in val]))\n\n    return None\n\n\n@pytest.fixture\ndef _seed():\n    np.random.seed(0)\n\n\ndef generate_input(input_size):\n    return 2 * np.random.random_sample(input_size) - 1\n\n\ndef get_test_data(data_list, is_cuda=False, is_backward=False):\n    results = []\n    for data in data_list:\n        result = torch.from_numpy(data.copy())\n        if is_cuda:\n            result = result.cuda()\n        if is_backward:\n            result = Variable(result, requires_grad=True)\n        results.append(result)\n    return results\n\n\n@pytest.mark.parametrize(\'input_size\',\n                         [[1, 96, 112, 112],\n                          [1, 192, 28, 28],\n                          [1, 576, 14, 14],\n                          [32, 96, 112, 112],\n                          [32, 192, 28, 28],\n                          [32, 576, 14, 14]],\n                         ids=idfn)\n@pytest.mark.parametrize(\'bits\', (8, 4), ids=(\'8bit\', \'4bit\'))\n@pytest.mark.parametrize(""use_cuda"", [False, True], ids=[\'cpu\', \'cuda\'])\n@pytest.mark.parametrize(\'scale_mode\', [""single_scale"", ""per_channel_scale""])\n@pytest.mark.parametrize(""is_weights"", (True, False), ids=(\'weights\', \'activation\'))\nclass TestParametrized:\n    @pytest.mark.parametrize(""is_signed"", (True, False), ids=(\'signed\', \'unsigned\'))\n    class TestSymmetric:\n        @staticmethod\n        def generate_scale(input_, scale_mode, is_weights):\n            assert scale_mode in [""single_scale"", ""per_channel_scale""]\n\n            def calc_scale(input_):\n                return min(abs(input_.min()), abs(input_.max())) - input_.mean() / 4\n\n            if scale_mode == ""single_scale"":\n                return np.array([calc_scale(input_)])\n\n            if scale_mode == ""per_channel_scale"":\n                if is_weights:\n                    channel_count = input_.shape[0]\n                    if channel_count == 1:\n                        pytest.skip(""Same case as for single scale mode"")\n                    scales_shape = [1 for _ in input_.shape]\n                    scales_shape[0] = channel_count\n                    scales = np.zeros(scales_shape)\n                    for idx in range(0, channel_count):\n                        single_input_channel = input_[idx, ...]\n                        scales[idx] = calc_scale(single_input_channel)\n                else:\n                    channel_count = input_.shape[1]\n                    if channel_count == 1:\n                        pytest.skip(""Same case as for single scale mode"")\n                    scales_shape = [1 for _ in input_.shape]\n                    scales_shape[1] = channel_count\n                    scales = np.zeros(scales_shape)\n                    for idx in range(0, channel_count):\n                        single_input_channel = input_[:, idx, ...]\n                        scales[0, idx] = calc_scale(single_input_channel)\n                return scales\n\n        @staticmethod\n        def get_range_level(is_signed, is_weights, bits):\n            levels = 2 ** bits\n            if is_signed:\n                if is_weights:\n                    levels -= 1\n                level_high = 2 ** (bits - 1) - 1\n                level_low = -(level_high + 1)\n                if is_weights:\n                    level_low += 1\n            else:\n                level_high = 2 ** bits - 1\n                level_low = 0\n            return level_low, level_high, levels\n\n        def test_quantize_symmetric_forward(self, _seed, is_signed, is_weights, input_size, bits, use_cuda, scale_mode):\n            ref_input = generate_input(input_size)\n\n            ref_scale = self.generate_scale(ref_input, scale_mode, is_weights)\n\n            test_input, test_scale = get_test_data([ref_input, ref_scale], use_cuda)\n            level_low, level_high, levels = self.get_range_level(is_signed, is_weights, bits)\n\n            ref_scale = abs(ref_scale) + EPS\n            ref_input_low = ref_scale * (level_low / level_high)\n            ref_input_range = ref_scale - ref_input_low\n\n            ref_value = ReferenceQuantizeAsymmetric.forward(ref_input, ref_input_low, ref_input_range, levels)\n\n            test_value = symmetric_quantize(test_input, levels, level_low, level_high, test_scale, EPS)\n\n            check_equal(ref_value, test_value, rtol=1e-3)\n\n        def test_quantize_symmetric_backward(self, _seed, is_signed, is_weights, input_size, bits, use_cuda,\n                                             scale_mode):\n            ref_input = generate_input(input_size)\n\n            ref_scale = self.generate_scale(ref_input, scale_mode, is_weights)\n            level_low, level_high, levels = self.get_range_level(is_signed, is_weights, bits)\n            test_input, test_scale = get_test_data([ref_input, ref_scale], use_cuda, is_backward=True)\n\n            ref_scale = abs(ref_scale) + EPS\n            ref_input_low = ref_scale * (level_low / level_high)\n            ref_input_range = ref_scale - ref_input_low\n\n            ref_output = ReferenceQuantizeAsymmetric.forward(ref_input, ref_input_low, ref_input_range, levels)\n            ref_grads = ReferenceQuantizeAsymmetric.backward(np.ones(input_size), ref_input, ref_input_low,\n                                                             ref_input_range, ref_output, level_low, level_high,\n                                                             True)\n            del ref_grads[1]\n            test_value = symmetric_quantize(test_input, levels, level_low, level_high, test_scale, EPS)\n            test_value.sum().backward()\n            test_grads = get_grads([test_input, test_scale])\n\n            check_equal(ref_output, test_value)\n            check_equal(ref_grads, test_grads)\n\n    @pytest.mark.parametrize(""is_negative_range"", (True, False), ids=(\'range<0\', \'range>0\'))\n    class TestAsymmetric:\n        @staticmethod\n        def generate_range(input_, is_negative_range, scale_mode, is_weights):\n            assert scale_mode in [""single_scale"", ""per_channel_scale""]\n\n            def calc_low_and_range(input_, is_negative_range):\n                input_low = input_.min() - input_.mean() / 4\n                input_range = input_.max() - input_low\n                if is_negative_range:\n                    input_range *= -1\n                return input_low, input_range\n\n            if scale_mode == ""single_scale"":\n                input_low, input_range = calc_low_and_range(input_, is_negative_range)\n                return np.array([input_low]), np.array([input_range])\n\n            if scale_mode == ""per_channel_scale"":\n                if is_weights:\n                    channel_count = input_.shape[0]\n                    if channel_count == 1:\n                        pytest.skip(""Same case as for single scale mode"")\n                    scales_shape = [1 for _ in input_.shape]\n                    scales_shape[0] = channel_count\n                    input_low = np.zeros(scales_shape)\n                    input_range = np.zeros(scales_shape)\n                    for idx in range(0, channel_count):\n                        single_input_channel = input_[idx, ...]\n                        input_low[idx], input_range[idx] = calc_low_and_range(single_input_channel, is_negative_range)\n                else:\n                    channel_count = input_.shape[1]\n                    if channel_count == 1:\n                        pytest.skip(""Same case as for single scale mode"")\n                    scales_shape = [1 for _ in input_.shape]\n                    scales_shape[1] = channel_count\n                    input_low = np.zeros(scales_shape)\n                    input_range = np.zeros(scales_shape)\n                    for idx in range(0, channel_count):\n                        single_input_channel = input_[:, idx, ...]\n                        input_low[0, idx], input_range[0, idx] = calc_low_and_range(single_input_channel,\n                                                                                    is_negative_range)\n\n                return input_low, input_range\n\n        @staticmethod\n        def get_range_level(bits):\n            levels = 2 ** bits\n            level_low = 0\n            level_high = levels - 1\n            return level_low, level_high, levels\n\n        def test_quantize_asymmetric_forward(self, _seed, input_size, bits, use_cuda, is_negative_range, is_weights,\n                                             scale_mode):\n            level_low, level_high, levels = self.get_range_level(bits)\n            ref_input = generate_input(input_size)\n            ref_input_low, ref_input_range = self.generate_range(ref_input, is_negative_range, scale_mode, is_weights)\n            test_input, test_input_low, test_input_range = get_test_data(\n                [ref_input, ref_input_low, ref_input_range], use_cuda)\n\n            ref_input_range = abs(ref_input_range) + EPS\n            ref_input_low, ref_input_range = ReferenceQuantizeAsymmetric.tune_range(\n                ref_input_low, ref_input_range, levels)\n            ref_value = ReferenceQuantizeAsymmetric.forward(\n                ref_input, ref_input_low, ref_input_range, levels)\n            test_value = asymmetric_quantize(test_input, levels, level_low, level_high, test_input_low,\n                                             test_input_range, EPS)\n\n            check_equal(ref_value, test_value)\n\n        def test_quantize_asymmetric_backward(self, _seed, input_size, bits, use_cuda, is_negative_range, is_weights,\n                                              scale_mode):\n            level_low, level_high, levels = self.get_range_level(bits)\n            ref_input = generate_input(input_size)\n            ref_input_low, ref_input_range = self.generate_range(ref_input, is_negative_range, scale_mode, is_weights)\n            test_input, test_input_low, test_input_range = get_test_data(\n                [ref_input, ref_input_low, ref_input_range], use_cuda, is_backward=True)\n\n            range_sign = np.sign(ref_input_range)\n            ref_input_range = abs(ref_input_range) + EPS\n            ref_input_low, ref_input_range = ReferenceQuantizeAsymmetric.tune_range(\n                ref_input_low, ref_input_range, levels)\n            ref_output = ReferenceQuantizeAsymmetric.forward(ref_input, ref_input_low, ref_input_range, levels)\n            ref_grads = ReferenceQuantizeAsymmetric.backward(\n                np.ones(input_size), ref_input, ref_input_low, ref_input_range, ref_output, level_low,\n                level_high, range_sign)\n\n            test_value = asymmetric_quantize(test_input, levels, level_low, level_high, test_input_low,\n                                             test_input_range, eps=EPS)\n            test_value.sum().backward()\n            test_grads = get_grads([test_input, test_input_low, test_input_range])\n\n            check_equal(ref_grads, test_grads)\n'"
pytorch_toolkit/nncf/tests/quantization/test_precision_init.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport itertools\nimport json\nimport math\nfrom collections import namedtuple\n\nimport os\nimport pytest\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom functools import partial\nfrom torch.utils import model_zoo\nfrom torchvision.models import MobileNetV2\nfrom torchvision.transforms import transforms\n\nfrom examples.classification.main import create_cifar\nfrom examples.common.models import squeezenet1_1_custom, model_urls, OrderedDict\nfrom nncf.dynamic_graph.graph_builder import create_input_infos\nfrom nncf.checkpoint_loading import load_state\nfrom nncf.nncf_network import CompressionModuleType\nfrom nncf.quantization.hessian_trace import HessianTraceEstimator\nfrom nncf.quantization.init_precision import HessianAwarePrecisionInitializeRunner\nfrom nncf.quantization.layers import QUANTIZATION_MODULES\nfrom nncf.utils import get_all_modules_by_type, safe_thread_call\nfrom tests.conftest import TEST_ROOT\nfrom tests.quantization.test_algo_quantization import get_squeezenet_quantization_config, \\\n    get_basic_quantization_config, RankDatasetMock, compare_multi_gpu_dump\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test\n\n\ndef create_test_dataloaders(model_size, dataset_dir, batch_size):\n    normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5),\n                                     std=(0.5, 0.5, 0.5))\n\n    train_transforms = transforms.Compose([\n        transforms.RandomResizedCrop(model_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    dummy_config = type(\'dummy\', (object,), {\'dataset_dir\': dataset_dir})()\n    train_dataset = create_cifar(dummy_config, dataset_config=\'cifar10\', is_train=True, transform=train_transforms)\n    pin_memory = True\n    workers = 1\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=workers,\n                                               pin_memory=pin_memory)\n    return train_loader, train_dataset\n\n\ndef get_bitwidth_per_scope(model):\n    all_quantizations = OrderedDict()\n    for class_type in QUANTIZATION_MODULES.registry_dict.values():\n        quantization_type = class_type.__name__\n        all_quantizations.update(\n            get_all_modules_by_type(model.get_compression_modules_by_type(CompressionModuleType.ACTIVATION_QUANTIZER),\n                                    quantization_type))\n        all_quantizations.update(\n            get_all_modules_by_type(model.get_compression_modules_by_type(CompressionModuleType.FUNCTION_QUANTIZER),\n                                    quantization_type))\n        all_quantizations.update(get_all_modules_by_type(model, quantization_type))\n\n    all_quantizations = OrderedDict(sorted(all_quantizations.items(), key=lambda x: str(x[0])))\n    full_bitwidth_per_scope = []\n    for scope, quantizer in all_quantizations.items():\n        full_bitwidth_per_scope.append([quantizer.num_bits, str(scope)])\n    return full_bitwidth_per_scope\n\n\n# TODO: split into 2 functions or rename\ndef compare_with_ref_if_exists(actual_state, path_to_ref):\n    if os.path.exists(path_to_ref):\n        with open(path_to_ref, \'r\') as f:\n            assert json.load(f) == actual_state\n    else:\n        with open(path_to_ref, \'w\') as f:\n            json.dump(actual_state, f)\n\n\ndef create_hawq_test_config(batch_size, num_data_points):\n    config = get_squeezenet_quantization_config()\n    config[\'batch_size\'] = batch_size\n    config[\'compression\'].update({\n        \'initializer\': {\n            \'precision\': {\n                ""type"": ""hawq"",\n                ""bits"": [\n                    4,\n                    8,\n                    6,\n                    7,\n                    5\n                ],\n                ""num_data_points"": num_data_points,\n                ""iter_number"": 1,\n                ""tolerance"": 1e-2\n            },\n            \'range\': {\n                \'num_init_steps\': 1\n            }\n        }})\n    return config\n\n\ndef test_hawq_precision_init(_seed, dataset_dir, tmp_path, mocker):\n    num_data_points = 100\n    batch_size = 10\n    config = create_hawq_test_config(batch_size, num_data_points)\n    model = squeezenet1_1_custom(num_classes=10, pretrained=False, dropout=0)\n\n    model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    load_state(model, model_zoo.load_url(model_urls[\'squeezenet1_1\']))\n    model = model.cuda()\n    device = next(model.parameters()).device\n\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    if not dataset_dir:\n        dataset_dir = str(tmp_path)\n    train_loader, _ = create_test_dataloaders(config.model_size, dataset_dir, batch_size)\n    mocked_trace = mocker.patch(\'nncf.quantization.hessian_trace.HessianTraceEstimator.get_average_traces\')\n    num_traces = len(get_all_modules_by_type(model, \'NNCFConv2d\'))\n    mock_avg_traces = [torch.Tensor([num_traces - i]).to(device) for i in range(num_traces)]\n    mocked_trace.return_value = mock_avg_traces\n\n    compression_ctrl.initialize(criterion=criterion, data_loader=train_loader)\n    act_bitwidth_per_scope = get_bitwidth_per_scope(model)\n    path_to_ref = str(TEST_ROOT / \'data/hawq_reference/squeezenet1_1_mixed_bitwidth_per_scope.json\')\n    compare_with_ref_if_exists(act_bitwidth_per_scope, path_to_ref)\n\n\nHAWQTestParams = namedtuple(\'HAWQTestParams\', (\'iter_number\', \'batch_size\', \'num_data_points\', \'ref_trace\'))\n\n\n@pytest.mark.parametrize(""params"",\n                         (HAWQTestParams(200, 13, 100, 0.07957423478364944),\n                          HAWQTestParams(2, 13, 100, 0.062167033553123474),\n                          HAWQTestParams(2, 10, 10, 0.11200366914272308),\n                          HAWQTestParams(2, 10, 5, 0.11200366914272308)),\n                         ids=(\'until_threshold\', \'until_num_iter\', \'batch_eq_num_data\', \'batch_larger_num_data\'))\ndef test_hawq_on_single_conv_without_quantizers(_seed, dataset_dir, tmp_path, params: HAWQTestParams):\n    config = get_squeezenet_quantization_config(batch_size=params.batch_size)\n    iter_number = params.iter_number\n    tolerance = 4e-4\n\n    model = squeezenet1_1_custom(num_classes=10, pretrained=False, dropout=0)\n    load_state(model, model_zoo.load_url(model_urls[\'squeezenet1_1\']))\n    model = model.cuda()\n\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    if not dataset_dir:\n        dataset_dir = str(tmp_path)\n    data_loader, _ = create_test_dataloaders(config.model_size, dataset_dir, params.batch_size)\n    device = next(model.parameters()).device\n\n    for _, param in model.named_parameters():\n        param.requires_grad = False\n    first_conv = next(iter(get_all_modules_by_type(model, \'Conv2d\').values()))\n    first_conv.weight.requires_grad = True\n\n    trace_estimator = HessianTraceEstimator(model, criterion, device, data_loader, params.num_data_points)\n    actual_state = trace_estimator.get_average_traces(max_iter=iter_number, tolerance=tolerance)\n    assert math.isclose(actual_state.item(), params.ref_trace, rel_tol=1e-09)\n\n\ndef get_size_of_search_space(m, L):\n    def nCr(n, r):\n        f = math.factorial\n        return f(n) // f(r) // f(n - r)\n\n    ref_num = 0\n    for j in range(1, m + 1):\n        ref_num += nCr(m, j) * nCr(L - 1, j - 1)\n    return ref_num\n\n\ndef test_constrained_bit_configs():\n    bits = [4, 2, 8]\n    L = 4\n    m = len(bits)\n    all_configs = list(itertools.product(bits, repeat=L))\n\n    ref_configs = []\n    for bit_config in all_configs:\n        is_ok = True\n        for i in range(L - 1):\n            if bit_config[i + 1] < bit_config[i]:\n                is_ok = False\n                break\n        if is_ok:\n            ref_configs.append(list(bit_config))\n    actual_config = HessianAwarePrecisionInitializeRunner.get_constrained_configs(bits, L)\n    ref_num = get_size_of_search_space(m, L)\n    assert len(ref_configs) == ref_num\n    assert len(actual_config) == ref_num\n    assert sorted(actual_config) == sorted(ref_configs)\n\n\ndef get_requires_grad_per_param(model):\n    not_sorted = OrderedDict({param_name: param.requires_grad for param_name, param in model.named_parameters()})\n    return OrderedDict(sorted(not_sorted.items()))\n\n\ndef test_disable_quantizer_gradients():\n    config = get_basic_quantization_config()\n    config[\'input_info\'] = {\n        ""sample_size"": (1, 3, 10, 10),\n    }\n    model = MobileNetV2(num_classes=10)\n    model.eval()\n    model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]\n    all_quantizations = get_all_modules_by_type(model, quantization_types)\n\n    HessianAwarePrecisionInitializeRunner.disable_quantizer_gradients(\n        all_quantizations,\n        compression_ctrl.quantized_weight_modules_registry,\n        model)\n    actual_state = get_requires_grad_per_param(model)\n    path_to_ref = str(TEST_ROOT / \'data/hawq_reference/mobilenet_v2_requires_grad_per_param.json\')\n    compare_with_ref_if_exists(actual_state, path_to_ref)\n\n\ndef test_enable_quantizer_gradients():\n    config = get_basic_quantization_config()\n    config[\'input_info\'] = {\n        ""sample_size"": (1, 3, 10, 10),\n    }\n    model = MobileNetV2(num_classes=10)\n    model.eval()\n    model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]\n    all_quantizations = get_all_modules_by_type(model, quantization_types)\n\n    original = get_requires_grad_per_param(model)\n    disabled = HessianAwarePrecisionInitializeRunner.disable_quantizer_gradients(\n        all_quantizations,\n        compression_ctrl.quantized_weight_modules_registry,\n        model)\n    HessianAwarePrecisionInitializeRunner.enable_quantizer_gradients(model, all_quantizations, disabled)\n    actual = get_requires_grad_per_param(model)\n    assert original == actual\n\n\ndef get_path_to_bitwidth_dump(tmp_path, rank):\n    out_file_path = tmp_path / \'bitwidth_per_scope_gpu{}.pt\'.format(rank)\n    return out_file_path\n\n\ndef hawq_dumping_worker(gpu, ngpus_per_node, config, tmp_path):\n    config.batch_size = 3\n    config.workers = 3\n    config.gpu = gpu\n    config.ngpus_per_node = ngpus_per_node\n    config.rank = gpu\n    config.distributed = True\n\n    torch.distributed.init_process_group(backend=""nccl"", init_method=\'tcp://127.0.0.1:8899\',\n                                         world_size=config.world_size, rank=config.rank)\n\n    model = safe_thread_call(partial(squeezenet1_1_custom, pretrained=True, dropout=0))\n\n    quant_model, compression_algo = create_compressed_model_and_algo_for_test(model, config)\n    compression_algo.distributed()\n\n    torch.cuda.set_device(config.gpu)\n    quant_model.cuda(config.gpu)\n    config.batch_size = int(config.batch_size / ngpus_per_node)\n    config.workers = int(config.workers / ngpus_per_node)\n    quant_model = torch.nn.parallel.DistributedDataParallel(quant_model, device_ids=[config.gpu])\n\n    torch.backends.cudnn.benchmark = True\n\n    input_infos_list = create_input_infos(config)\n    input_sample_size = input_infos_list[0].shape\n    data_loader = torch.utils.data.DataLoader(RankDatasetMock(input_sample_size[1:], config.rank),\n                                              batch_size=3,\n                                              num_workers=1,\n                                              shuffle=False)\n    criterion = torch.nn.MSELoss().cuda(config.gpu)\n\n    # just to reproduce the same scale values without Dropout\n    quant_model.eval()\n\n    compression_algo.initialize(criterion=criterion, data_loader=data_loader)\n\n    act_bitwidth_per_scope = get_bitwidth_per_scope(quant_model.module)\n    out_file_path = get_path_to_bitwidth_dump(tmp_path, config.rank)\n    torch.save(act_bitwidth_per_scope, str(out_file_path))\n\n\ndef test_hawq_broadcast_avg_traces_in_distributed_mode(tmp_path):\n    num_data_points = 100\n    batch_size = 10\n    config = create_hawq_test_config(batch_size, num_data_points)\n\n    ngpus_per_node = torch.cuda.device_count()\n    config.world_size = ngpus_per_node\n    torch.multiprocessing.spawn(hawq_dumping_worker,\n                                nprocs=ngpus_per_node,\n                                args=(ngpus_per_node, config, tmp_path),\n                                join=True)\n\n    assert not compare_multi_gpu_dump(config, tmp_path, get_path_to_bitwidth_dump)\n'"
pytorch_toolkit/nncf/tests/quantization/test_quantizer_propagation_graph.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom copy import deepcopy\nfrom typing import List, Tuple\nfrom collections import namedtuple, Counter\n\nimport networkx as nx\nimport pytest\n\nfrom nncf.nncf_network import InsertionPointGraph\nfrom nncf.quantization.layers import QuantizerConfig\nfrom nncf.quantization.quantizer_propagation import QuantizerPropagationStateGraph as QPSG, \\\n    QuantizerPropagationStateGraphNodeType, QuantizationTrait\nfrom tests.test_nncf_network import get_two_branch_mock_model_graph, get_mock_nncf_node_attrs\n\n\ndef get_edge_paths(graph, start_node_key, finish_node_key) -> List[List[Tuple]]:\n    node_paths = list(nx.all_simple_paths(graph, start_node_key, finish_node_key))\n    edge_paths = []\n    for path in node_paths:\n        edge_paths.append([(path[i], path[i + 1]) for i in range(0, len(path) - 1)])\n    return edge_paths\n\n\ndef get_edge_paths_for_propagation(graph, start_node_key, finish_node_key) -> List[List[Tuple]]:\n    paths = get_edge_paths(graph, start_node_key, finish_node_key)\n    return [list(reversed(path)) for path in paths]\n\n\nclass TestQuantizerPropagationStateGraph:\n    @staticmethod\n    @pytest.fixture()\n    def mock_qp_graph():\n        ip_graph = InsertionPointGraph(get_two_branch_mock_model_graph())\n        yield QPSG(ip_graph)\n\n    def test_build_quantizer_propagation_state_graph_from_ip_graph(self):\n        ip_graph = InsertionPointGraph(get_two_branch_mock_model_graph())\n        quant_prop_graph = QPSG(ip_graph)\n        assert len(ip_graph.nodes) == len(quant_prop_graph.nodes)\n        assert len(ip_graph.edges) == len(quant_prop_graph.edges)\n\n        for ip_graph_node_key, ip_graph_node in ip_graph.nodes.items():\n            qpg_node = quant_prop_graph.nodes[ip_graph_node_key]\n            assert qpg_node[QPSG.NODE_TYPE_NODE_ATTR] == ip_graph_node[\n                InsertionPointGraph.NODE_TYPE_NODE_ATTR]\n            qpg_node_type = qpg_node[QPSG.NODE_TYPE_NODE_ATTR]\n            if qpg_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                assert qpg_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] is None\n                assert not qpg_node[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n                assert qpg_node[QPSG.INSERTION_POINT_DATA_NODE_ATTR] == ip_graph_node[\n                    InsertionPointGraph.INSERTION_POINT_DATA_NODE_ATTR]\n            elif qpg_node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:\n                assert not qpg_node[QPSG.ALLOWED_INPUT_QUANTIZATION_TYPES_NODE_ATTR]\n                assert qpg_node[QPSG.QUANTIZATION_TRAIT_NODE_ATTR] == QuantizationTrait.NON_QUANTIZABLE\n                assert not qpg_node[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n        for from_node, to_node, edge_data in ip_graph.edges(data=True):\n            qpg_edge_data = quant_prop_graph.edges[from_node, to_node]\n            assert not qpg_edge_data[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n            for key, value in edge_data.items():\n                assert qpg_edge_data[key] == value\n\n    def test_add_propagating_quantizer(self, mock_qp_graph):\n        ref_qconf_list = [QuantizerConfig(), QuantizerConfig(bits=6)]\n\n        target_node_key = ""F""\n        target_ip_node_key = InsertionPointGraph.get_pre_hook_node_key(target_node_key)\n        prop_quant = mock_qp_graph.add_propagating_quantizer(ref_qconf_list, target_ip_node_key)\n        assert prop_quant.potential_quant_configs == ref_qconf_list\n        assert prop_quant.current_location_node_key == target_ip_node_key\n        assert prop_quant.affected_ip_nodes == {target_ip_node_key}\n        assert prop_quant.last_accepting_location_node_key is None\n        assert prop_quant.affected_edges == {(target_ip_node_key, target_node_key)}\n        assert not prop_quant.propagation_path\n\n        for node_key, node in mock_qp_graph.nodes.items():\n            if node_key == target_node_key:\n                assert node[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] == [prop_quant]\n            elif node_key == target_ip_node_key:\n                assert node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] == prop_quant\n            else:\n                assert not node[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n                if node[QPSG.NODE_TYPE_NODE_ATTR] == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                    assert node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] is None\n\n            target_ip_node = mock_qp_graph.nodes[target_ip_node_key]\n            assert target_ip_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] == prop_quant\n\n        for from_node, to_node, edge_data in mock_qp_graph.edges.data():\n            if (from_node, to_node) == (target_ip_node_key, target_node_key):\n                assert edge_data[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] == [prop_quant]\n            else:\n                assert not edge_data[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n        with pytest.raises(RuntimeError):\n            _ = mock_qp_graph.add_propagating_quantizer(ref_qconf_list,\n                                                        InsertionPointGraph.get_post_hook_node_key(target_node_key))\n\n    START_IP_NODES_AND_PATHS_TO_DOMINATING_IP_NODES = [\n        # Non-branching case - starting from ""E"" pre-hook\n        (InsertionPointGraph.get_pre_hook_node_key(""E""),\n         [[(InsertionPointGraph.get_post_hook_node_key(""C""), InsertionPointGraph.get_pre_hook_node_key(""E""))]]),\n\n        # Non-branching case - starting from ""C"" post-hook\n        (InsertionPointGraph.get_post_hook_node_key(""C""),\n         [[(""C"", InsertionPointGraph.get_post_hook_node_key(""C"")),\n           (InsertionPointGraph.get_pre_hook_node_key(""C""), ""C"")]]),\n\n        # Branching case - starting from ""F"" pre-hook\n        (InsertionPointGraph.get_pre_hook_node_key(""F""),\n         [[(InsertionPointGraph.get_post_hook_node_key(""D""), InsertionPointGraph.get_pre_hook_node_key(""F""))],\n          [(InsertionPointGraph.get_post_hook_node_key(""E""), InsertionPointGraph.get_pre_hook_node_key(""F""))]]),\n\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=START_IP_NODES_AND_PATHS_TO_DOMINATING_IP_NODES)\n    def start_ip_node_and_path_to_dominating_node(request):\n        return request.param\n\n    def test_get_paths_to_immediately_dominating_insertion_points(self, start_ip_node_and_path_to_dominating_node,\n                                                                  mock_qp_graph):\n        start_node = start_ip_node_and_path_to_dominating_node[0]\n        ref_paths = start_ip_node_and_path_to_dominating_node[1]\n        test_paths = mock_qp_graph.get_paths_to_immediately_dominating_insertion_points(start_node)\n        def get_cat_path_list(path_list: List[List[Tuple[str, str]]]):\n            str_paths = [[str(edge[0]) +\' -> \' + str(edge[1]) for edge in path] for path in path_list]\n            cat_paths = [\';\'.join(path) for path in str_paths]\n            return cat_paths\n\n        assert Counter(get_cat_path_list(ref_paths)) == Counter(get_cat_path_list(test_paths))\n\n\n    START_TARGET_NODES = [\n        (InsertionPointGraph.get_pre_hook_node_key(""H""),\n         InsertionPointGraph.get_post_hook_node_key(""G"")),\n\n        (InsertionPointGraph.get_pre_hook_node_key(""H""),\n         InsertionPointGraph.get_pre_hook_node_key(""F"")),\n\n        (InsertionPointGraph.get_pre_hook_node_key(""F""),\n         InsertionPointGraph.get_pre_hook_node_key(""E"")),\n\n        (InsertionPointGraph.get_pre_hook_node_key(""F""),\n         InsertionPointGraph.get_post_hook_node_key(""B"")),\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=START_TARGET_NODES)\n    def start_target_nodes(request):\n        return request.param\n\n    @pytest.mark.dependency(name=""propagate_via_path"")\n    def test_quantizers_can_propagate_via_path(self, start_target_nodes, mock_qp_graph):\n        start_ip_node_key = start_target_nodes[0]\n        target_ip_node_key = start_target_nodes[1]\n\n        # From ""target"" to ""start"" since propagation direction is inverse to edge direction\n        rev_paths = get_edge_paths_for_propagation(mock_qp_graph, target_ip_node_key, start_ip_node_key)\n\n        for path in rev_paths:\n            working_graph = deepcopy(mock_qp_graph)\n            ref_prop_quant = working_graph.add_propagating_quantizer([QuantizerConfig()],\n                                                                     start_ip_node_key)\n            ref_affected_edges = deepcopy(ref_prop_quant.affected_edges)\n            ref_affected_edges.update(set(path))\n            ref_affected_ip_nodes = deepcopy(ref_prop_quant.affected_ip_nodes)\n            prop_quant = working_graph.propagate_quantizer_via_path(ref_prop_quant, path)\n            final_node_key, _ = path[-1]\n            for from_node_key, to_node_key in path:\n                edge_data = working_graph.edges[from_node_key, to_node_key]\n                assert edge_data[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] == [ref_prop_quant]\n                to_node = working_graph.nodes[to_node_key]\n                if to_node[QPSG.NODE_TYPE_NODE_ATTR] == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                    assert to_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] is None\n                from_node = working_graph.nodes[from_node_key]\n                if from_node[QPSG.NODE_TYPE_NODE_ATTR] == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                    ref_affected_ip_nodes.add(from_node_key)\n\n            final_node_key, _ = path[-1]\n            final_node = working_graph.nodes[final_node_key]\n            assert final_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] == ref_prop_quant\n\n            assert prop_quant.current_location_node_key == final_node_key\n            assert prop_quant.propagation_path == path\n            assert prop_quant.affected_edges == ref_affected_edges\n            assert prop_quant.affected_ip_nodes == ref_affected_ip_nodes\n\n    START_TARGET_ACCEPTING_NODES = [\n        (InsertionPointGraph.get_pre_hook_node_key(""H""),\n         InsertionPointGraph.get_pre_hook_node_key(""G""),\n         InsertionPointGraph.get_post_hook_node_key(""G"")),\n\n        (InsertionPointGraph.get_pre_hook_node_key(""G""),\n         InsertionPointGraph.get_post_hook_node_key(""F""),\n         InsertionPointGraph.get_post_hook_node_key(""F"")),\n\n        (InsertionPointGraph.get_pre_hook_node_key(""A""),\n         None,\n         None),\n\n        (InsertionPointGraph.get_pre_hook_node_key(""F""),\n         InsertionPointGraph.get_pre_hook_node_key(""C""),\n         InsertionPointGraph.get_post_hook_node_key(""C"")),\n\n        (InsertionPointGraph.get_pre_hook_node_key(""D""),\n         InsertionPointGraph.get_pre_hook_node_key(""A""),\n         InsertionPointGraph.get_post_hook_node_key(""A"")),\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=START_TARGET_ACCEPTING_NODES)\n    def start_target_accepting_nodes(request):\n        return request.param\n\n    @pytest.mark.dependency(depends=""propagate_via_path"")\n    def test_backtrack_propagation_until_accepting_location(self, start_target_accepting_nodes, mock_qp_graph):\n        start_ip_node_key = start_target_accepting_nodes[0]\n        target_ip_node_key = start_target_accepting_nodes[1]\n        ref_last_accepting_location = start_target_accepting_nodes[2]\n\n        prop_quant = mock_qp_graph.add_propagating_quantizer([QuantizerConfig()],\n                                                             start_ip_node_key)\n        ref_affected_edges = deepcopy(prop_quant.affected_edges)\n\n        if target_ip_node_key is not None:\n            # Here, the tested graph should have such a structure that there is only one path from target to start\n            path = get_edge_paths_for_propagation(mock_qp_graph, target_ip_node_key, start_ip_node_key)[0]\n            prop_quant = mock_qp_graph.propagate_quantizer_via_path(prop_quant, path)\n            if ref_last_accepting_location is not None:\n                resulting_path = get_edge_paths_for_propagation(mock_qp_graph,\n                                                                ref_last_accepting_location, start_ip_node_key)[0]\n                ref_affected_edges.update(set(resulting_path))\n\n        assert prop_quant.last_accepting_location_node_key == ref_last_accepting_location\n        prop_quant = mock_qp_graph.backtrack_propagation_until_accepting_location(prop_quant)\n\n        if ref_last_accepting_location is None:\n            assert prop_quant is None\n        else:\n            assert prop_quant.current_location_node_key == ref_last_accepting_location\n            assert prop_quant.affected_edges == ref_affected_edges\n            assert prop_quant.propagation_path == resulting_path\n\n        if target_ip_node_key is not None and ref_last_accepting_location is not None:\n            target_node = mock_qp_graph.nodes[target_ip_node_key]\n            accepting_node = mock_qp_graph.nodes[ref_last_accepting_location]\n            if ref_last_accepting_location != target_ip_node_key:\n                assert target_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] is None\n                assert target_ip_node_key not in prop_quant.affected_ip_nodes\n            assert accepting_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] == prop_quant\n\n    @pytest.mark.dependency(depends=""propagate_via_path"")\n    def test_clone_propagating_quantizer(self, mock_qp_graph, start_target_nodes):\n        start_ip_node_key = start_target_nodes[0]\n        target_ip_node_key = start_target_nodes[1]\n\n        # From ""target"" to ""start"" since propagation direction is inverse to edge direction\n        # Only take one path out of possible paths for this test\n        rev_path = get_edge_paths_for_propagation(mock_qp_graph, target_ip_node_key, start_ip_node_key)[0]\n\n        ref_prop_quant = mock_qp_graph.add_propagating_quantizer([QuantizerConfig()],\n                                                                 start_ip_node_key)\n\n        prop_quant = mock_qp_graph.propagate_quantizer_via_path(ref_prop_quant, rev_path)\n\n        cloned_prop_quant = mock_qp_graph.clone_propagating_quantizer(prop_quant)\n\n        assert cloned_prop_quant.affected_ip_nodes == prop_quant.affected_ip_nodes\n        assert cloned_prop_quant.affected_edges == prop_quant.affected_edges\n        assert cloned_prop_quant.propagation_path == prop_quant.propagation_path\n        assert cloned_prop_quant.current_location_node_key == prop_quant.current_location_node_key\n\n        for ip_node_key in prop_quant.affected_ip_nodes:\n            node = mock_qp_graph.nodes[ip_node_key]\n            assert cloned_prop_quant in node[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n        for from_node_key, to_node_key in prop_quant.affected_edges:\n            edge = mock_qp_graph.edges[from_node_key, to_node_key]\n            assert cloned_prop_quant in edge[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n    START_TARGET_NODES_FOR_TWO_QUANTIZERS = [\n        (InsertionPointGraph.get_pre_hook_node_key(""E""),\n         InsertionPointGraph.get_post_hook_node_key(""C""),\n         InsertionPointGraph.get_pre_hook_node_key(""H""),\n         InsertionPointGraph.get_post_hook_node_key(""G"")),\n\n        (InsertionPointGraph.get_pre_hook_node_key(""C""),\n         InsertionPointGraph.get_post_hook_node_key(""A""),\n         InsertionPointGraph.get_pre_hook_node_key(""H""),\n         InsertionPointGraph.get_pre_hook_node_key(""D"")),\n\n        # Simulated quantizer merging result\n        (InsertionPointGraph.get_pre_hook_node_key(""G""),\n         InsertionPointGraph.get_pre_hook_node_key(""E""),\n         InsertionPointGraph.get_pre_hook_node_key(""G""),\n         InsertionPointGraph.get_post_hook_node_key(""D""))\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=START_TARGET_NODES_FOR_TWO_QUANTIZERS)\n    def start_target_nodes_for_two_quantizers(request):\n        return request.param\n\n    @pytest.mark.dependency(depends=""propagate_via_path"")\n    def test_remove_propagating_quantizer(self, mock_qp_graph, start_target_nodes_for_two_quantizers):\n        start_ip_node_key_remove = start_target_nodes_for_two_quantizers[0]\n        target_ip_node_key_remove = start_target_nodes_for_two_quantizers[1]\n\n        start_ip_node_key_keep = start_target_nodes_for_two_quantizers[2]\n        target_ip_node_key_keep = start_target_nodes_for_two_quantizers[3]\n\n        # From ""target"" to ""start"" since propagation direction is inverse to edge direction\n        # Only take one path out of possible paths for this test\n        rev_path_remove = get_edge_paths_for_propagation(mock_qp_graph,\n                                                         target_ip_node_key_remove,\n                                                         start_ip_node_key_remove)[0]\n        rev_path_keep = get_edge_paths_for_propagation(mock_qp_graph,\n                                                       target_ip_node_key_keep,\n                                                       start_ip_node_key_keep)[0]\n\n        prop_quant_to_remove = mock_qp_graph.add_propagating_quantizer([QuantizerConfig()],\n                                                                       start_ip_node_key_remove)\n        prop_quant_to_remove = mock_qp_graph.propagate_quantizer_via_path(prop_quant_to_remove, rev_path_remove)\n\n        prop_quant_to_keep = mock_qp_graph.add_propagating_quantizer([QuantizerConfig()],\n                                                                     start_ip_node_key_keep)\n\n        prop_quant_to_keep = mock_qp_graph.propagate_quantizer_via_path(prop_quant_to_keep, rev_path_keep)\n\n        affected_ip_nodes = deepcopy(prop_quant_to_remove.affected_ip_nodes)\n        affected_edges = deepcopy(prop_quant_to_keep.affected_edges)\n        last_location = prop_quant_to_remove.current_location_node_key\n        ref_quant_to_keep_state_dict = deepcopy(prop_quant_to_keep.__dict__)\n\n        mock_qp_graph.remove_propagating_quantizer(prop_quant_to_remove)\n\n        last_node = mock_qp_graph.nodes[last_location]\n        assert last_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] is None\n\n        for ip_node_key in affected_ip_nodes:\n            node = mock_qp_graph.nodes[ip_node_key]\n            assert prop_quant_to_remove not in node[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n        for from_node_key, to_node_key in affected_edges:\n            edge = mock_qp_graph.edges[from_node_key, to_node_key]\n            assert prop_quant_to_remove not in edge[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n        assert prop_quant_to_keep.__dict__ == ref_quant_to_keep_state_dict\n\n        for ip_node_key in prop_quant_to_keep.affected_ip_nodes:\n            node = mock_qp_graph.nodes[ip_node_key]\n            assert prop_quant_to_keep in node[\n                QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n        for from_node_key, to_node_key in prop_quant_to_keep.affected_edges:\n            edge = mock_qp_graph.edges[from_node_key, to_node_key]\n            assert prop_quant_to_keep in edge[\n                QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n    QUANTIZABLE_NODES_START_NODES_DOMINATED_NODES = [\n        ([""D"", ""E"", ""F""],\n         {\n             ""B"": {""D"", ""E""},\n             InsertionPointGraph.get_pre_hook_node_key(""B""): {""D"", ""E""},\n             ""E"": {""F""},\n             InsertionPointGraph.get_post_hook_node_key(""D""): {""F""},\n             ""A"": {""D"", ""E""},\n             InsertionPointGraph.get_pre_hook_node_key(""G""): set()\n         }),\n        ([""C"", ""E"", ""H""],\n         {\n             InsertionPointGraph.get_pre_hook_node_key(""C""): {""C""},\n             InsertionPointGraph.get_post_hook_node_key(""C""): {""E""},\n             ""D"": {""H""},\n             InsertionPointGraph.get_pre_hook_node_key(""B""): {""C"", ""H""}, # corner case - has a branch without quantizers\n             InsertionPointGraph.get_post_hook_node_key(""H""): set()\n         })\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=QUANTIZABLE_NODES_START_NODES_DOMINATED_NODES)\n    def dominated_nodes_test_struct(request):\n        return request.param\n\n    def test_get_quantizable_op_nodes_immediately_dominated_by_node(self, mock_qp_graph, dominated_nodes_test_struct):\n        nodes_to_mark_as_quantizable = dominated_nodes_test_struct[0]\n\n        for node in mock_qp_graph.nodes.values():\n            node[QPSG.QUANTIZATION_TRAIT_NODE_ATTR] = QuantizationTrait.QUANTIZATION_AGNOSTIC\n\n        traits_to_mark_with = [QuantizationTrait.INPUTS_QUANTIZABLE,\n                               QuantizationTrait.NON_QUANTIZABLE]\n\n        for trait in traits_to_mark_with:\n            for node_key in nodes_to_mark_as_quantizable:\n                node = mock_qp_graph.nodes[node_key]\n                node[QPSG.QUANTIZATION_TRAIT_NODE_ATTR] = trait\n\n            for start_node_key, ref_dominated_quantizable_nodes_set in dominated_nodes_test_struct[1].items():\n                dominated_quantizable_nodes_list = mock_qp_graph.get_quantizable_op_nodes_immediately_dominated_by_node(\n                    start_node_key)\n                assert set(dominated_quantizable_nodes_list) == ref_dominated_quantizable_nodes_set\n\n    @staticmethod\n    def get_model_graph():\n        mock_node_attrs = get_mock_nncf_node_attrs()\n        mock_graph = nx.DiGraph()\n\n        #     (A)\n        #      |\n        #     (B)\n        #   /     \\\n        # (C)     (D)\n        #  |       |\n        # (F)     (E)\n        #\n        #\n\n        node_keys = [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\']\n        for node_key in node_keys:\n            mock_graph.add_node(node_key, **mock_node_attrs)\n\n        mock_graph.add_edges_from([(\'A\', \'B\'), (\'B\', \'C\'), (\'B\', \'D\'), (\'D\', \'E\'), (\'C\', \'F\')])\n        return mock_graph\n\n\n\n    StateQuantizerTestStruct = namedtuple(\'StateQuantizerTestStruct\',\n                                          (\'init_node_to_trait_and_configs_dict\',\n                                           \'starting_quantizer_ip_node\',\n                                           \'target_node_for_quantizer\',\n                                           \'is_merged\',\n                                           \'prop_path\'))\n\n    SetQuantizersTestStruct = namedtuple(\'SetQuantizersTestStruct\',\n                                         (\'start_set_quantizers\',\n                                          \'expected_set_quantizers\'))\n\n    MERGE_QUANTIZER_INTO_PATH_TEST_CASES = [\n        SetQuantizersTestStruct(\n            start_set_quantizers=[\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                              [QuantizerConfig()])\n                    },\n                    starting_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n                    target_node_for_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n                    is_merged=False,\n                    prop_path=None\n\n                ),\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                              [QuantizerConfig()]),\n                    },\n                    starting_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'F\'),\n                    target_node_for_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'C\'),\n                    is_merged=True,\n                    prop_path=[(InsertionPointGraph.get_post_hook_node_key(\'B\'),\n                                InsertionPointGraph.get_pre_hook_node_key(\'C\'))]\n                )\n\n            ],\n            expected_set_quantizers=[\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'PRE HOOK B\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                                       [QuantizerConfig()])\n\n                    },\n                    starting_quantizer_ip_node=[\'E\', \'F\'],\n                    target_node_for_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n                    is_merged=False,\n                    prop_path=None\n                )\n            ]\n        ),\n        SetQuantizersTestStruct(\n            start_set_quantizers=[\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                              [QuantizerConfig()])\n                    },\n                    starting_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n                    target_node_for_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n                    is_merged=False,\n                    prop_path=None\n\n                ),\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                              [QuantizerConfig()]),\n                    },\n                    starting_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'F\'),\n                    target_node_for_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n                    is_merged=True,\n                    prop_path=[(\'B\', InsertionPointGraph.get_post_hook_node_key(\'B\'))]\n                )\n\n            ],\n            expected_set_quantizers=[\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'B\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                              [QuantizerConfig()])\n\n                    },\n                    starting_quantizer_ip_node=[\'E\', \'F\'],\n                    target_node_for_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n                    is_merged=False,\n                    prop_path=None\n                )\n            ]\n        ),\n        SetQuantizersTestStruct(\n            start_set_quantizers=[\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                              [QuantizerConfig()])\n                    },\n                    starting_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n                    target_node_for_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n                    is_merged=False,\n                    prop_path=None\n\n                ),\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                              [QuantizerConfig()]),\n                    },\n                    starting_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'F\'),\n                    target_node_for_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'C\'),\n                    is_merged=True,\n                    prop_path=[(InsertionPointGraph.get_post_hook_node_key(\'B\'),\n                                InsertionPointGraph.get_pre_hook_node_key(\'C\'))]\n                )\n\n            ],\n            expected_set_quantizers=[\n                StateQuantizerTestStruct(\n                    init_node_to_trait_and_configs_dict=\n                    {\n                        \'B\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                              [QuantizerConfig()])\n\n                    },\n                    starting_quantizer_ip_node=[\'E\', \'F\'],\n                    target_node_for_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n                    is_merged=False,\n                    prop_path=None\n                )\n            ]\n        )\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=MERGE_QUANTIZER_INTO_PATH_TEST_CASES)\n    def merge_quantizer_into_path_test_struct(request):\n        return request.param\n\n    def test_merge_quantizer_into_path(self, merge_quantizer_into_path_test_struct):\n\n        mock_graph = self.get_model_graph()\n        ip_graph = InsertionPointGraph(mock_graph)\n        quant_prop_graph = QPSG(ip_graph)\n\n        for quantizers_test_struct in merge_quantizer_into_path_test_struct.start_set_quantizers:\n\n            init_node_to_trait_and_configs_dict = quantizers_test_struct.init_node_to_trait_and_configs_dict\n            starting_quantizer_ip_node = quantizers_test_struct.starting_quantizer_ip_node\n            target_node = quantizers_test_struct.target_node_for_quantizer\n            is_merged = quantizers_test_struct.is_merged\n            prop_path = quantizers_test_struct.prop_path\n            for node in quant_prop_graph.nodes.values():\n                node[QPSG.QUANTIZATION_TRAIT_NODE_ATTR] = QuantizationTrait.QUANTIZATION_AGNOSTIC\n            master_prop_quant = None\n            merged_prop_quant = []\n            for node_key, trait_and_configs_tuple in init_node_to_trait_and_configs_dict.items():\n                trait = trait_and_configs_tuple[0]\n                qconfigs = trait_and_configs_tuple[1]\n                quant_prop_graph.nodes[node_key][QPSG.QUANTIZATION_TRAIT_NODE_ATTR] = trait\n                if trait == QuantizationTrait.INPUTS_QUANTIZABLE:\n                    ip_node_key = InsertionPointGraph.get_pre_hook_node_key(node_key)\n                    prop_quant = quant_prop_graph.add_propagating_quantizer(qconfigs,\n                                                                            ip_node_key)\n                    if ip_node_key == starting_quantizer_ip_node:\n                        master_prop_quant = prop_quant\n\n            path = get_edge_paths_for_propagation(quant_prop_graph,\n                                                  target_node,\n                                                  starting_quantizer_ip_node)\n            master_prop_quant = quant_prop_graph.propagate_quantizer_via_path(master_prop_quant,\n                                                                              path[0])\n            if is_merged:\n                merged_prop_quant.append((master_prop_quant, prop_path))\n\n        for prop_quant, prop_path in merged_prop_quant:\n            quant_prop_graph.merge_quantizer_into_path(prop_quant, prop_path)\n\n        expected_quantizers_test_struct = merge_quantizer_into_path_test_struct.expected_set_quantizers\n        self.check_final_state_qpsg(quant_prop_graph, expected_quantizers_test_struct)\n\n\n    @staticmethod\n    def check_final_state_qpsg(final_quant_prop_graph, expected_quantizers_test_struct):\n        for quantizer_param in expected_quantizers_test_struct:\n            from_node_key = quantizer_param.target_node_for_quantizer\n            expected_prop_path = set()\n            target_node = quantizer_param.target_node_for_quantizer\n            for start_node in quantizer_param.starting_quantizer_ip_node:\n                added_path = get_edge_paths_for_propagation(final_quant_prop_graph,\n                                                            target_node,\n                                                            start_node)\n                expected_prop_path.update(added_path[0])\n\n            quantizer = final_quant_prop_graph.nodes[from_node_key][QPSG.PROPAGATING_QUANTIZER_NODE_ATTR]\n\n            assert quantizer is not None\n\n            for from_node_key, to_node_key in expected_prop_path:\n                assert quantizer in final_quant_prop_graph.edges[(from_node_key, to_node_key)][\n                    QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n                from_node = final_quant_prop_graph.nodes[from_node_key]\n                from_node_type = from_node[QPSG.NODE_TYPE_NODE_ATTR]\n                if from_node_type == QuantizerPropagationStateGraphNodeType.INSERTION_POINT:\n                    # pylint:disable=line-too-long\n                    assert quantizer in final_quant_prop_graph.nodes[from_node_key][QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n            assert quantizer.affected_edges == expected_prop_path\n'"
pytorch_toolkit/nncf/tests/quantization/test_quantizer_propagation_solver.py,0,"b'# pylint:disable=too-many-lines\n""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport random\nfrom collections import namedtuple\nfrom typing import Dict, List, Tuple\n\nimport networkx as nx\nimport pytest\n\nfrom nncf.dynamic_graph.context import Scope\nfrom nncf.dynamic_graph.graph import OperationExecutionContext, NNCFGraph\nfrom nncf.dynamic_graph.version_agnostic_op_names import get_version_agnostic_name\nfrom nncf.nncf_network import InsertionPointGraph, InsertionInfo, InsertionPointGraphNodeType\nfrom nncf.quantization.layers import QuantizerConfig, QuantizationMode\nfrom nncf.quantization.quantizer_propagation import QuantizerPropagationStateGraph as QPSG, \\\n    QuantizerPropagationStateGraphNodeType, QuantizationTrait, OPERATOR_METATYPES, DEFAULT_QUANT_TRAIT_TO_OP_DICT, \\\n    QuantizerPropagationSolver, TransitionStatus, PropagationStrategy, PropagatingQuantizer\nfrom tests.quantization.test_quantizer_propagation_graph import get_edge_paths_for_propagation\nfrom tests.test_nncf_network import get_mock_nncf_node_attrs\n\n\nclass TestQuantizerPropagationSolver:\n    @staticmethod\n    def get_mock_model_node_attrs_for_op_name(op_name: str) -> OperationExecutionContext:\n        return OperationExecutionContext(op_name,\n                                         Scope(),\n                                         0,\n                                         [None])\n\n    @staticmethod\n    def get_randomly_connected_model_graph(op_name_keys: List[str]) -> nx.DiGraph:\n        graph_len = len(op_name_keys)\n        mock_graph = nx.generators.gnc_graph(graph_len, seed=0)\n        shuffled_op_names = random.sample(op_name_keys, len(op_name_keys))\n        for idx, (_, node) in enumerate(mock_graph.nodes.items()):\n            node[NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR] = \\\n                TestQuantizerPropagationSolver.get_mock_model_node_attrs_for_op_name(shuffled_op_names[idx])\n        return mock_graph\n\n    @staticmethod\n    def get_sequentially_connected_model_graph(op_name_keys: List[str]) -> nx.DiGraph:\n        graph = nx.DiGraph()\n        for node_key in op_name_keys:\n            attrs = {\n                NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR:\n                    TestQuantizerPropagationSolver.get_mock_model_node_attrs_for_op_name(node_key)\n            }\n            graph.add_node(node_key, **attrs)\n\n        edges = [(op_name_keys[i], op_name_keys[i + 1]) for i in range(0, len(op_name_keys) - 1)]\n        for from_key, to_key in edges:\n            graph.add_edge(from_key, to_key)\n        return graph\n\n    def test_quantization_traits_are_unambiguous_for_op_names(self):\n        op_name_to_trait_dict = {}  # type: Dict[str, QuantizationTrait]\n        for trait, arches in DEFAULT_QUANT_TRAIT_TO_OP_DICT.items():\n            for op_meta in arches:\n                aliases = op_meta.get_all_aliases()\n                for alias in aliases:\n                    if alias in op_name_to_trait_dict:\n                        assert op_name_to_trait_dict[alias] == trait\n                    else:\n                        op_name_to_trait_dict[alias] = trait\n\n    def test_set_quantization_traits_for_quant_prop_graph_nodes(self):\n        # Test all patchable metatypes. If a patchable metatype is not registered\n        # in quantization trait-to-metatype dict, the test will fail.\n        tested_op_metatypes = list(OPERATOR_METATYPES.registry_dict.values()) # type: List[OperatorMetatype]\n        tested_op_names = []\n        for op_meta in tested_op_metatypes:\n            aliases = op_meta.get_all_aliases()\n            for alias in aliases:\n                tested_op_names.append(get_version_agnostic_name(alias))\n\n        # Edges should be irrelevant - using random graph\n        mock_graph = self.get_randomly_connected_model_graph(tested_op_names)\n        ip_graph = InsertionPointGraph(mock_graph)\n        for node in ip_graph.nodes.values():\n            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:\n                op_exec_context = node[InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR][\n                    NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n                op_name = op_exec_context.operator_name\n                ref_meta = OPERATOR_METATYPES.get_operator_metatype_by_op_name(op_name)\n                node[InsertionPointGraph.OPERATOR_METATYPE_NODE_ATTR] = ref_meta\n\n        quant_prop_graph = QPSG(ip_graph)\n        quant_prop_solver = QuantizerPropagationSolver()\n        quant_prop_graph = quant_prop_solver.set_allowed_quantization_types_for_operator_nodes(quant_prop_graph)\n        op_quant_traits_map = quant_prop_solver.get_operator_quantization_traits_map()\n\n        for qpg_node in quant_prop_graph.nodes().values():\n            if qpg_node[QPSG.NODE_TYPE_NODE_ATTR] == QuantizerPropagationStateGraphNodeType.OPERATOR:\n                quant_det_id = qpg_node[QPSG.OPERATOR_METATYPE_NODE_ATTR]\n                quant_types = qpg_node[QPSG.ALLOWED_INPUT_QUANTIZATION_TYPES_NODE_ATTR]\n                if op_quant_traits_map[quant_det_id] == QuantizationTrait.INPUTS_QUANTIZABLE:\n                    # TODO: check for correspondence of operator type and HW config to initial\n                    # quantization types\n                    assert quant_types == QuantizerPropagationSolver.DEFAULT_QUANTIZATION_TYPES\n\n    def test_setup_initial_quantizers_in_quant_prop_graph(self):\n        ops_to_quantize = [\'conv2d\', \'matmul\', \'gelu\']\n        ops_not_to_quantize = [\'batch_norm\', \'max_pool2d\', \'dropout\', \'min\', \'softmax\']\n        node_keys = ops_to_quantize + ops_not_to_quantize\n        mock_graph = self.get_sequentially_connected_model_graph(node_keys)\n\n        ip_graph = InsertionPointGraph(mock_graph)\n        for node in ip_graph.nodes.values():\n            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:\n                op_exec_context = node[InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR][\n                    NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n                op_name = op_exec_context.operator_name\n                ref_meta = OPERATOR_METATYPES.get_operator_metatype_by_op_name(op_name)\n                node[InsertionPointGraph.OPERATOR_METATYPE_NODE_ATTR] = ref_meta\n\n        qp_graph = QPSG(ip_graph)\n        quant_prop_solver = QuantizerPropagationSolver()\n        qp_graph = quant_prop_solver.set_allowed_quantization_types_for_operator_nodes(qp_graph)\n        qp_graph = quant_prop_solver.setup_initial_quantizers(qp_graph)\n\n        for node_key in ops_to_quantize:\n            pred_ip_key = next(qp_graph.predecessors(node_key))\n            node = qp_graph.nodes[node_key]\n            pred_ip_node = qp_graph.nodes[pred_ip_key]\n            prop_quant = pred_ip_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR]\n            assert prop_quant is not None\n            assert node[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] == [prop_quant]\n\n            edge = qp_graph.edges[pred_ip_key, node_key]\n            assert edge[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] == [prop_quant]\n\n        for node_key in ops_not_to_quantize:\n            pred_ip_key = next(qp_graph.predecessors(node_key))\n            node = qp_graph.nodes[node_key]\n            pred_ip_node = qp_graph.nodes[pred_ip_key]\n            assert pred_ip_node[QPSG.PROPAGATING_QUANTIZER_NODE_ATTR] is None\n\n            assert not node[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n            edge = qp_graph.edges[pred_ip_key, node_key]\n            assert not edge[QPSG.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]\n\n    MergeQConfigTestStruct = namedtuple(\'MergeQConfigTestStruct\',\n                                        (\'master_config_list_before_merge\',\n                                         \'slave_config_list_dict_before_merge\',\n                                         \'master_config_list_after_merge\',\n                                         \'slave_config_list_dict_after_merge\'))\n    QCONFIG_MASTER_SLAVE_BEFORE_AND_AFTER_MERGING = [\n        # Compatible configs on all branches\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=8)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=8)],\n                ""bar"": [QuantizerConfig(bits=8)]\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=8)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=8)],\n                ""bar"": [QuantizerConfig(bits=8)]\n            }),\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=6,\n                                                             mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""baz"": [QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)]\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=6,\n                                                            mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""baz"": [QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)]\n            }),\n\n        # Precision narrowed relative to master config on some branches, but master\n        # config is still compatible\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=6)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=4)],\n                ""bar"": [QuantizerConfig(bits=5)]\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=6)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=4)],\n                ""bar"": [QuantizerConfig(bits=5)]\n            }),\n\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=8,\n                                                             mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=4)],\n                ""bar"": [QuantizerConfig(bits=5)]\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=8,\n                                                            mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=4)],\n                ""bar"": [QuantizerConfig(bits=5)]\n            }),\n\n        # Potential master configs excluded due to conflict with a branch\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=8),\n                                             QuantizerConfig(bits=6)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=7)],\n                ""bar"": [QuantizerConfig(bits=8)],\n                ""baz"": [QuantizerConfig(bits=7)]\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=8)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=7)],\n                ""bar"": [QuantizerConfig(bits=8)],\n                ""baz"": [QuantizerConfig(bits=7)]\n            }),\n\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=7),\n                                             QuantizerConfig(bits=7,\n                                                             mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC)]\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=7,\n                                                            mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC)]\n            }),\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=7),\n                                             QuantizerConfig(bits=7,\n                                                             mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC)]\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=7,\n                                                            mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC)]\n            }),\n\n        # Master config propagation-induced config exclusion on branches:\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=6)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=8),\n                        QuantizerConfig(bits=7),\n                        QuantizerConfig(bits=6),],\n                ""bar"": [QuantizerConfig(bits=8),\n                        QuantizerConfig(bits=5)],\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=6)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=6)],\n                ""bar"": [QuantizerConfig(bits=5)]\n            }),\n\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=6)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=7),\n                        QuantizerConfig(bits=6),\n                        QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=8),\n                        QuantizerConfig(bits=5),\n                        QuantizerConfig(bits=4,\n                                        mode=QuantizationMode.ASYMMETRIC)\n                        ],\n            },\n            master_config_list_after_merge=[QuantizerConfig(bits=6)],\n            slave_config_list_dict_after_merge={\n                ""foo"": [QuantizerConfig(bits=6)],\n                ""bar"": [QuantizerConfig(bits=5)]\n            }),\n\n        # Cases with conflicts resulting in no master configs left after merge and,\n        # consequently, no propagation:\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=3)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=7),\n                        QuantizerConfig(bits=6),\n                        QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=8),\n                        QuantizerConfig(bits=5),\n                        QuantizerConfig(bits=4,\n                                        mode=QuantizationMode.ASYMMETRIC)\n                        ],\n            },\n            master_config_list_after_merge=[],\n            slave_config_list_dict_after_merge={}),\n\n        MergeQConfigTestStruct(\n            master_config_list_before_merge=[QuantizerConfig(bits=8),\n                                             QuantizerConfig(bits=4,\n                                                             mode=QuantizationMode.ASYMMETRIC)],\n            slave_config_list_dict_before_merge={\n                ""foo"": [QuantizerConfig(bits=7,\n                                        mode=QuantizationMode.ASYMMETRIC),\n                        QuantizerConfig(bits=6,\n                                        mode=QuantizationMode.ASYMMETRIC)],\n                ""bar"": [QuantizerConfig(bits=8),\n                        QuantizerConfig(bits=5)\n                        ],\n            },\n            master_config_list_after_merge=[],\n            slave_config_list_dict_after_merge={})\n\n        # TODO: extend with signed/unsigned test cases\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=QCONFIG_MASTER_SLAVE_BEFORE_AND_AFTER_MERGING)\n    def qconfig_merge_test_struct(request):\n        return request.param\n\n    def test_get_merged_qconfigs(self, qconfig_merge_test_struct):\n        quant_prop_solver = QuantizerPropagationSolver()\n        ref_merged_master_config_list = qconfig_merge_test_struct.master_config_list_after_merge\n        ref_merged_slave_config_dict_list = qconfig_merge_test_struct.slave_config_list_dict_after_merge\n\n        merged_master_config_list, merged_slave_config_dict_list = quant_prop_solver.get_merged_qconfigs(\n            qconfig_merge_test_struct.master_config_list_before_merge,\n            qconfig_merge_test_struct.slave_config_list_dict_before_merge\n        )\n\n        assert ref_merged_master_config_list == merged_master_config_list\n        assert ref_merged_slave_config_dict_list == merged_slave_config_dict_list\n\n\n    def get_branching_model_graph(self):\n        mock_node_attrs = get_mock_nncf_node_attrs()\n        mock_graph = nx.DiGraph()\n\n        #     (A)\n        #      |\n        #     (B)\n        #   /  |  \\\n        # (C) (D) (E)\n        #  |       | \\\n        # (F)     (G) (H)\n        #           \\ /\n        #           (I)\n        #            |\n        #           (J)\n\n        node_keys = [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\', \'I\', \'J\']\n        for node_key in node_keys:\n            mock_graph.add_node(node_key, **mock_node_attrs)\n\n        mock_graph.add_edges_from([(\'A\', \'B\'), (\'B\', \'C\'), (\'B\', \'D\'), (\'B\', \'E\'), (\'C\', \'F\'),\n                                   (\'E\', \'G\'), (\'E\', \'H\'), (\'G\', \'I\'), (\'H\', \'I\'), (\'I\', \'J\')])\n        return mock_graph\n\n    BranchTransitionTestStruct = namedtuple(\'BranchTransitionTestStruct\',\n                                            (  # Unspecified nodes are marked as quantization agnostic\n                                                \'init_node_to_trait_and_configs_dict\',\n                                                \'starting_master_quantizer_ip_node\',\n                                                \'target_branching_node_for_master_quantizer\',\n                                                \'strategy_vs_expected_status_dict\'))\n\n    BRANCH_TRANSITION_TEST_CASES = [\n        # Downward branches are quantization-agnostic\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'D\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: None,\n                PropagationStrategy.AGGRESSIVE: None,\n            }\n        ),\n\n        # Downward branches have compatible quantization configs\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'F\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: None,\n                PropagationStrategy.AGGRESSIVE: None,\n            }\n        ),\n\n        # A branch has a non-quantizable op\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n                \'D\': (QuantizationTrait.NON_QUANTIZABLE,\n                      []),\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n                \'D\': (QuantizationTrait.NON_QUANTIZABLE,\n                      []),\n                \'J\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'F\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n                \'D\': (QuantizationTrait.NON_QUANTIZABLE,\n                      []),\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n                \'D\': (QuantizationTrait.NON_QUANTIZABLE,\n                      []),\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        # All master configs are incompatible with branch configs\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=7)]),\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=8), QuantizerConfig(bits=6)]),\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4)]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=7, mode=QuantizationMode.ASYMMETRIC)]),\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=8), QuantizerConfig(bits=6, mode=QuantizationMode.ASYMMETRIC)]),\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4), QuantizerConfig(bits=6)]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        # Compatible quantizers exist on the branches, but each is below an incompatible quantizer\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4), QuantizerConfig(bits=6)]),\n\n                \'C\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=7, mode=QuantizationMode.ASYMMETRIC)]),\n\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=8), QuantizerConfig(bits=6, mode=QuantizationMode.ASYMMETRIC)]),\n\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4)]),\n                \'G\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4)]),\n                \'H\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4)]),\n\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'D\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        # Master config options narrowing due to transition, but otherwise transition is permitted\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=5)]),\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6)]),\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4), QuantizerConfig(bits=6)]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'E\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: None,\n                PropagationStrategy.AGGRESSIVE: None\n            }\n        ),\n\n        # Branch config options narrowing due to transition - do not transition if the strategy\n        # is conservative\n        BranchTransitionTestStruct(\n            init_node_to_trait_and_configs_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=7, mode=QuantizationMode.ASYMMETRIC)]),\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6), QuantizerConfig(bits=8)]),\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4), QuantizerConfig(bits=6)]),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'F\'),\n            target_branching_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: None\n            }\n        ),\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=BRANCH_TRANSITION_TEST_CASES)\n    def branch_transition_test_struct(request):\n        return request.param\n\n    def test_check_branching_transition(self, branch_transition_test_struct: BranchTransitionTestStruct):\n        init_node_to_trait_and_configs_dict = branch_transition_test_struct.init_node_to_trait_and_configs_dict\n        starting_master_quantizer_ip_node = branch_transition_test_struct.starting_master_quantizer_ip_node\n        target_node = branch_transition_test_struct.target_branching_node_for_master_quantizer\n        strategy_vs_status = branch_transition_test_struct.strategy_vs_expected_status_dict\n\n        # Graph preparation\n        mock_graph = self.get_branching_model_graph()\n        ip_graph = InsertionPointGraph(mock_graph)\n        quant_prop_graph = QPSG(ip_graph)\n        for node in quant_prop_graph.nodes.values():\n            node[QPSG.QUANTIZATION_TRAIT_NODE_ATTR] = QuantizationTrait.QUANTIZATION_AGNOSTIC\n\n        master_prop_quant = None\n        for node_key, trait_and_configs_tuple in init_node_to_trait_and_configs_dict.items():\n            trait = trait_and_configs_tuple[0]\n            qconfigs = trait_and_configs_tuple[1]\n            quant_prop_graph.nodes[node_key][QPSG.QUANTIZATION_TRAIT_NODE_ATTR] = trait\n            if trait == QuantizationTrait.INPUTS_QUANTIZABLE:\n                ip_node_key = InsertionPointGraph.get_pre_hook_node_key(node_key)\n                prop_quant = quant_prop_graph.add_propagating_quantizer(qconfigs,\n                                                                        ip_node_key)\n                if ip_node_key == starting_master_quantizer_ip_node:\n                    master_prop_quant = prop_quant\n\n        path = get_edge_paths_for_propagation(quant_prop_graph,\n                                              target_node,\n                                              starting_master_quantizer_ip_node)\n        master_prop_quant = quant_prop_graph.propagate_quantizer_via_path(master_prop_quant,\n                                                                          path[0])\n\n        # The propagating quantizers are in place, now check the transition\n        for strategy, ref_status in strategy_vs_status.items():\n            solver = QuantizerPropagationSolver(propagation_strategy=strategy)\n            status = solver.check_branching_transition(quant_prop_graph,\n                                                       master_prop_quant,\n                                                       target_node)\n            assert status == ref_status\n\n    PathTransitionTestStruct = namedtuple(\'PathTransitionTestStruct\',\n                                          (\'init_node_to_trait_configs_and_target_node_dict\',\n                                           # Unspecified nodes are marked as quantization agnostic\n                                           \'starting_master_quantizer_ip_node\',\n                                           \'master_quantizer_qconfigs\',\n                                           \'target_node_for_master_quantizer\',\n                                           \'strategy_vs_expected_status_dict\'))\n\n    @staticmethod\n    def prepare_propagation_graph_state(ip_graph: InsertionPointGraph,\n                                        init_node_to_trait_configs_and_target_node_dict: Dict[\n                                            str, Tuple]) -> Tuple[List[PropagatingQuantizer], QPSG]:\n        quant_prop_graph = QPSG(ip_graph)\n        prop_quantizers = []\n        for node in quant_prop_graph.nodes.values():\n            node[QPSG.QUANTIZATION_TRAIT_NODE_ATTR] = QuantizationTrait.QUANTIZATION_AGNOSTIC\n\n        for node_key, trait_configs_and_target_tuple in init_node_to_trait_configs_and_target_node_dict.items():\n            trait = trait_configs_and_target_tuple[0]\n            qconfigs = trait_configs_and_target_tuple[1]\n            target_node = trait_configs_and_target_tuple[2]\n            quant_prop_graph.nodes[node_key][QPSG.QUANTIZATION_TRAIT_NODE_ATTR] = trait\n            if trait == QuantizationTrait.INPUTS_QUANTIZABLE:\n                ip_node_key = InsertionPointGraph.get_pre_hook_node_key(node_key)\n                prop_quant = quant_prop_graph.add_propagating_quantizer(qconfigs,\n                                                                        ip_node_key)\n                path = get_edge_paths_for_propagation(quant_prop_graph,\n                                                      target_node,\n                                                      ip_node_key)\n                prop_quant = quant_prop_graph.propagate_quantizer_via_path(prop_quant, path[0])\n                prop_quantizers.append(prop_quant)\n\n        return prop_quantizers, quant_prop_graph\n\n    PATH_TRANSITION_TEST_CASES = [\n        # Transition cases\n\n        # Single propagating quantizer\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'J\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'E\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_TRANSITION,\n            }\n        ),\n\n        # Non-intersecting paths, no branch influence\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_post_hook_node_key(\'A\')),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'F\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'C\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_TRANSITION,\n            }\n        ),\n\n        # Non-intersecting paths, branch influence\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'C\')),\n                \'G\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'E\')),\n                \'H\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'E\')),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'D\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'A\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_TRANSITION,\n            }\n        ),\n\n        # Non-intersecting paths, branch influence with downward branch config narrowing\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6), QuantizerConfig(bits=8)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'C\')),\n                \'G\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6), QuantizerConfig(bits=5,\n                                                                mode=QuantizationMode.ASYMMETRIC)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'E\')),\n                \'H\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6), QuantizerConfig(bits=5,\n                                                                mode=QuantizationMode.ASYMMETRIC)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'E\')),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'D\'),\n            master_quantizer_qconfigs=[QuantizerConfig(bits=6)],\n            target_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'A\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_TRANSITION,\n            }\n        ),\n\n        # Merge cases\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'A\')),\n                \'D\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'A\'))\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'C\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_MERGE,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_MERGE,\n            }\n        ),\n\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'G\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'A\')),\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=2)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'C\'))\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'H\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_MERGE,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_MERGE,\n            }\n        ),\n\n        # No transition cases:\n\n        # Path blocked by a quantizer\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'C\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'B\')),\n                \'J\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=4)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'I\')),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'F\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'C\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        # Path blocked by a non-quantizable node\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'J\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'H\')),\n                \'B\': (QuantizationTrait.NON_QUANTIZABLE,\n                      [],\n                      None)\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'C\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'A\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n\n        # A downward branch node was marked as non-quantizable\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_post_hook_node_key(\'B\')),\n                \'D\': (QuantizationTrait.NON_QUANTIZABLE,\n                      [],\n                      None)\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'C\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        # Incompatible upstream quantizer\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(6)],\n                      InsertionPointGraph.get_post_hook_node_key(\'A\')),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'D\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'E\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(6)],\n                      InsertionPointGraph.get_post_hook_node_key(\'A\')),\n                \'C\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(6)],\n                      InsertionPointGraph.get_post_hook_node_key(\'A\')),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'D\'),\n            master_quantizer_qconfigs=[QuantizerConfig()],\n            target_node_for_master_quantizer=InsertionPointGraph.get_pre_hook_node_key(\'B\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n        # Incompatible downstream quantizers\n        PathTransitionTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6), QuantizerConfig(bits=8)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'C\')),\n                \'G\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6), QuantizerConfig(bits=5,\n                                                                mode=QuantizationMode.ASYMMETRIC)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'E\')),\n                \'H\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig(bits=6), QuantizerConfig(bits=5,\n                                                                mode=QuantizationMode.ASYMMETRIC)],\n                      InsertionPointGraph.get_pre_hook_node_key(\'E\')),\n            },\n            starting_master_quantizer_ip_node=InsertionPointGraph.get_pre_hook_node_key(\'D\'),\n            master_quantizer_qconfigs=[QuantizerConfig(bits=4)],\n            target_node_for_master_quantizer=InsertionPointGraph.get_post_hook_node_key(\'A\'),\n            strategy_vs_expected_status_dict={\n                PropagationStrategy.CONSERVATIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n                PropagationStrategy.AGGRESSIVE: TransitionStatus.SHOULD_NOT_TRANSITION,\n            }\n        ),\n\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=PATH_TRANSITION_TEST_CASES)\n    def path_transition_test_struct(request):\n        return request.param\n\n    def test_check_transition_via_path(self, path_transition_test_struct: PathTransitionTestStruct):\n        #pylint:disable=line-too-long\n        init_node_to_trait_configs_and_target_node_dict = path_transition_test_struct.init_node_to_trait_configs_and_target_node_dict\n        starting_master_quantizer_ip_node = path_transition_test_struct.starting_master_quantizer_ip_node\n        master_quantizer_qconfigs = path_transition_test_struct.master_quantizer_qconfigs\n        target_node = path_transition_test_struct.target_node_for_master_quantizer\n        strategy_vs_status = path_transition_test_struct.strategy_vs_expected_status_dict\n\n        # Graph preparation\n        mock_graph = self.get_branching_model_graph()\n        ip_graph = InsertionPointGraph(mock_graph)\n        _, quant_prop_graph = self.prepare_propagation_graph_state(ip_graph,\n                                                                   init_node_to_trait_configs_and_target_node_dict)\n\n        master_prop_quant = quant_prop_graph.add_propagating_quantizer(master_quantizer_qconfigs,\n                                                                       starting_master_quantizer_ip_node)\n        path = get_edge_paths_for_propagation(quant_prop_graph,\n                                              target_node,\n                                              starting_master_quantizer_ip_node)[0]\n\n        for strategy, ref_status in strategy_vs_status.items():\n            solver = QuantizerPropagationSolver(propagation_strategy=strategy)\n            status = solver.check_transition_via_path(master_prop_quant,\n                                                      path,\n                                                      quant_prop_graph)\n            assert status == ref_status\n\n    PropagationStepTestStruct = namedtuple(\'PropagationStepTestStruct\',\n                                           (\'init_node_to_trait_configs_and_target_node_dict\',\n                                            \'expected_finished_status\',\n                                            \'current_location_node_key_for_propagated_quant\'))\n    PROPAGATION_STEP_TEST_CASES = [\n        PropagationStepTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'A\'))\n            },\n            expected_finished_status=True,\n            current_location_node_key_for_propagated_quant=InsertionPointGraph.get_pre_hook_node_key(\'A\')\n        ),\n        PropagationStepTestStruct(\n            init_node_to_trait_configs_and_target_node_dict=\n            {\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'C\'))\n            },\n            expected_finished_status=False,\n            current_location_node_key_for_propagated_quant=InsertionPointGraph.get_pre_hook_node_key(\'C\')\n        ),\n        PropagationStepTestStruct(\n            init_node_to_trait_configs_and_target_node_dict={\n                \'F\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'A\')),\n                \'G\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'E\')),\n                \'J\': (QuantizationTrait.INPUTS_QUANTIZABLE,\n                      [QuantizerConfig()],\n                      InsertionPointGraph.get_pre_hook_node_key(\'I\'))\n            },\n            expected_finished_status=True,\n            current_location_node_key_for_propagated_quant=InsertionPointGraph.get_pre_hook_node_key(\'A\')\n        )\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=PROPAGATION_STEP_TEST_CASES)\n    def propagation_step_test_struct(request):\n        return request.param\n\n    def test_propagation_step(self, propagation_step_test_struct):\n        # pylint:disable=line-too-long\n        init_node_to_trait_configs_and_target_node_dict = propagation_step_test_struct.init_node_to_trait_configs_and_target_node_dict\n        expected_finished_status = propagation_step_test_struct.expected_finished_status\n        current_location_node_key_for_propagated_quant = propagation_step_test_struct.current_location_node_key_for_propagated_quant\n        # Graph preparation\n        mock_graph = self.get_branching_model_graph()\n        ip_graph = InsertionPointGraph(mock_graph)\n        quant_prop_solver = QuantizerPropagationSolver()\n        # pylint:disable=line-too-long\n        prop_quantizers, quant_prop_graph = self.prepare_propagation_graph_state(ip_graph,\n                                                                                 init_node_to_trait_configs_and_target_node_dict)\n        untouched_quantizers = []\n        quant_prop = None\n        for pq in prop_quantizers:\n            if pq.current_location_node_key == current_location_node_key_for_propagated_quant:\n                quant_prop = pq\n            else:\n                untouched_quantizers.append(pq)\n\n        assert quant_prop is not None\n        quant_prop_graph = quant_prop_solver.propagation_step(quant_prop, quant_prop_graph)\n\n        if expected_finished_status:\n            finished_propagating_quantizers = quant_prop_solver.get_finished_propagating_quantizers()\n            assert quant_prop in finished_propagating_quantizers\n        else:\n            active_propagating_quantizers_queue = quant_prop_solver.get_active_propagating_quantizers_queue()\n            assert quant_prop in active_propagating_quantizers_queue\n\n        for pq in untouched_quantizers:\n            assert not pq in quant_prop_solver.get_active_propagating_quantizers_queue()\n            assert not pq in quant_prop_solver.get_finished_propagating_quantizers()\n\n    RunOnIpGraphTestStruct = namedtuple(\'RunOnIpGraphTestStruct\',\n                                        (\'list_ops\',\n                                         \'expected_retval\',\n                                         \'expected_count_finished_quant\',\n                                         \'expected_count_active_quant\'))\n\n    RUN_ON_IP_GRAPH_TEST_CASES = [\n        RunOnIpGraphTestStruct(\n            list_ops=[\'conv2d\', \'batch_norm\'],\n            expected_retval={},\n            expected_count_finished_quant=0,\n            expected_count_active_quant=0\n        ),\n        RunOnIpGraphTestStruct(\n            list_ops=[\'conv2d\', \'gelu\', ""conv2d""],\n            expected_retval={\n                InsertionInfo(OperationExecutionContext(\'conv2d\', Scope(), 0, [None])): [QuantizerConfig()],\n                InsertionInfo(OperationExecutionContext(\'gelu\', Scope(), 0, [None])): [QuantizerConfig()]\n            },\n            expected_count_finished_quant=2,\n            expected_count_active_quant=0\n        )\n    ]\n\n    @staticmethod\n    @pytest.fixture(params=RUN_ON_IP_GRAPH_TEST_CASES)\n    def run_on_ip_graph_test_struct(request):\n        return request.param\n\n    def test_run_on_ip_graph(self, run_on_ip_graph_test_struct):\n        expected_retval = run_on_ip_graph_test_struct.expected_retval\n        expected_count_finished_quant = run_on_ip_graph_test_struct.expected_count_finished_quant\n        expected_count_active_quant = run_on_ip_graph_test_struct.expected_count_active_quant\n\n        # Graph preparation\n        node_keys = run_on_ip_graph_test_struct.list_ops\n        mock_graph = self.get_sequentially_connected_model_graph(node_keys)\n        ip_graph = InsertionPointGraph(mock_graph)\n\n        for node in ip_graph.nodes.values():\n            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:\n                op_exec_context = node[InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR][\n                    NNCFGraph.OP_EXEC_CONTEXT_NODE_ATTR]\n                op_name = op_exec_context.operator_name\n                ref_meta = OPERATOR_METATYPES.get_operator_metatype_by_op_name(op_name)\n                node[InsertionPointGraph.OPERATOR_METATYPE_NODE_ATTR] = ref_meta\n\n        quant_prop_solver = QuantizerPropagationSolver()\n        retval = quant_prop_solver.run_on_ip_graph(ip_graph)\n\n        assert retval == expected_retval\n\n        assert len(quant_prop_solver.get_active_propagating_quantizers_queue()) == expected_count_active_quant\n        assert len(quant_prop_solver.get_finished_propagating_quantizers()) == expected_count_finished_quant\n'"
pytorch_toolkit/nncf/tests/sparsity/__init__.py,0,b''
pytorch_toolkit/nncf/tests/sparsity/test_common.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport pytest\n\nfrom nncf.sparsity.schedulers import PolynomialSparseScheduler, ExponentialSparsityScheduler, \\\n    AdaptiveSparsityScheduler, MultiStepSparsityScheduler\nfrom tests.test_helpers import BasicConvTestModel, get_empty_config, create_compressed_model_and_algo_for_test, \\\n    MockModel\n\n\n@pytest.mark.parametrize(\'algo\',\n                         (\'magnitude_sparsity\', \'rb_sparsity\'))\n@pytest.mark.parametrize((\'schedule_type\', \'scheduler_class\'),\n                         (\n                             (\'polynomial\', PolynomialSparseScheduler),\n                             (\'exponential\', ExponentialSparsityScheduler),\n                             (\'multistep\', MultiStepSparsityScheduler)\n                         ))\n\n\ndef test_can_choose_scheduler(algo, schedule_type, scheduler_class):\n    config = get_empty_config()\n    config[\'compression\'][\'algorithm\'] = algo\n    config[\'compression\'][""params""][""schedule""] = schedule_type\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(MockModel(), config)\n    assert isinstance(compression_ctrl.scheduler, scheduler_class)\n\n\ndef test_can_create_rb_algo__with_adaptive_scheduler():\n    config = get_empty_config()\n    config[\'compression\'][\'algorithm\'] = \'rb_sparsity\'\n    config[\'compression\'][""params""][""schedule""] = \'adaptive\'\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(MockModel(), config)\n    assert isinstance(compression_ctrl.scheduler, AdaptiveSparsityScheduler)\n\n\ndef test_can_not_create_magnitude_algo__with_adaptive_scheduler():\n    config = get_empty_config()\n    config[\'compression\'][\'algorithm\'] = \'magnitude_sparsity\'\n    config[\'compression\'][""params""][""schedule""] = \'adaptive\'\n    with pytest.raises(TypeError):\n        _, _ = create_compressed_model_and_algo_for_test(MockModel(), config)\n\n\ndef get_poly_params():\n    return {\n        \'power\': 1, \'sparsity_steps\': 2, \'sparsity_init\': 0.2, \'sparsity_target\': 0.6,\n        \'sparsity_training_steps\': 4\n    }\n\n\ndef get_multistep_params():\n    return {\n        \'steps\': [2, 3, 4], \'sparsity_levels\': [0.2, 0.4, 0.5, 0.6],\n        \'sparsity_training_steps\': 4\n    }\n\n\n@pytest.mark.parametrize(\'algo\',\n                         (\'magnitude_sparsity\', \'rb_sparsity\'))\nclass TestSparseModules:\n    def test_can_create_sparse_scheduler__with_defaults(self, algo):\n        config = get_empty_config()\n        config[\'compression\'][\'algorithm\'] = algo\n        config[\'compression\'][""params""][""schedule""] = \'polynomial\'\n        _, compression_ctrl = create_compressed_model_and_algo_for_test(MockModel(), config)\n        scheduler = compression_ctrl.scheduler\n        assert scheduler.initial_sparsity == 0\n        assert scheduler.max_sparsity == 0.5\n        assert scheduler.max_step == 90\n        assert scheduler.sparsity_training_steps == 100\n\n    @pytest.mark.parametrize((\'schedule\', \'get_params\', \'ref_levels\'),\n                             ((\'polynomial\', get_poly_params, [0.2, 0.4, 0.6, 0.6, 0.6, 0.6]),\n                              (\'exponential\', get_poly_params, [0.2, 0.4343145, 0.6, 0.6, 0.6, 0.6]),\n                              (\'multistep\', get_multistep_params, [0.2, 0.2, 0.4, 0.5, 0.6, 0.6])))\n    def test_scheduler_can_do_epoch_step(self, algo, schedule, get_params, ref_levels):\n        model = BasicConvTestModel()\n        config = get_empty_config()\n        config[\'compression\'][\'algorithm\'] = algo\n        config[\'compression\'][""params""] = get_params()\n        config[\'compression\'][""params""][""schedule""] = schedule\n\n        _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n        scheduler = compression_ctrl.scheduler\n\n        assert pytest.approx(scheduler.current_sparsity_level) == ref_levels[0]\n        for ref_level in ref_levels[1:]:\n            scheduler.epoch_step()\n            assert pytest.approx(scheduler.current_sparsity_level) == ref_level\n\n        for m in compression_ctrl.sparsified_module_info:\n            if hasattr(m.operand, ""sparsify""):\n                assert not m.operand.sparsify\n'"
pytorch_toolkit/nncf/tests/test_models/__init__.py,0,b'from .alexnet import *\nfrom .densenet import *\nfrom .dpn import *\nfrom .googlenet import *\nfrom .inceptionv3 import *\nfrom .lenet import *\nfrom .mobilenet import *\nfrom .pnasnet import *\nfrom .preact_resnet import *\nfrom .resnet import *\nfrom .resnext import *\nfrom .senet import *\nfrom .shufflenet import *\nfrom .shufflenetv2 import *\nfrom .squeezenet import *\nfrom .ssd_vgg import *\nfrom .ssd_mobilenet import *\nfrom .unet import *\nfrom .vgg import *\n'
pytorch_toolkit/nncf/tests/test_models/alexnet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\n__all__ = [\'AlexNet\']\n\n\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 2 * 2, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), 256 * 2 * 2)\n        x = self.classifier(x)\n        return x\n'"
pytorch_toolkit/nncf/tests/test_models/densenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, growth_rate):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat([out, x], 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(Transition, self).__init__()\n        self.bn = nn.BatchNorm2d(in_planes)\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv(F.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n        super(DenseNet, self).__init__()\n        self.growth_rate = growth_rate\n\n        num_planes = 2 * growth_rate\n        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n\n        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n        num_planes += nblocks[0] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans1 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n        num_planes += nblocks[1] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans2 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n        num_planes += nblocks[2] * growth_rate\n        out_planes = int(math.floor(num_planes * reduction))\n        self.trans3 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n        num_planes += nblocks[3] * growth_rate\n\n        self.bn = nn.BatchNorm2d(num_planes)\n        self.linear = nn.Linear(num_planes, num_classes)\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for _ in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.trans3(self.dense3(out))\n        out = self.dense4(out)\n        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef DenseNet121():\n    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=32)\n\n\ndef DenseNet169():\n    return DenseNet(Bottleneck, [6, 12, 32, 32], growth_rate=32)\n\n\ndef DenseNet201():\n    return DenseNet(Bottleneck, [6, 12, 48, 32], growth_rate=32)\n\n\ndef DenseNet161():\n    return DenseNet(Bottleneck, [6, 12, 36, 24], growth_rate=48)\n\n\ndef densenet_cifar():\n    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n\n\ndef test():\n    net = densenet_cifar()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y)\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/dpn.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n        super(Bottleneck, self).__init__()\n        self.out_planes = out_planes\n        self.dense_depth = dense_depth\n\n        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n        self.bn2 = nn.BatchNorm2d(in_planes)\n        self.conv3 = nn.Conv2d(in_planes, out_planes + dense_depth, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes + dense_depth)\n\n        self.shortcut = nn.Sequential()\n        if first_layer:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(last_planes, out_planes + dense_depth, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_planes + dense_depth)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        x = self.shortcut(x)\n        d = self.out_planes\n        out = torch.cat([x[:, :d, :, :] + out[:, :d, :, :], x[:, d:, :, :], out[:, d:, :, :]], 1)\n        out = F.relu(out)\n        return out\n\n\nclass DPN(nn.Module):\n    def __init__(self, cfg):\n        super(DPN, self).__init__()\n        in_planes, out_planes = cfg[\'in_planes\'], cfg[\'out_planes\']\n        num_blocks, dense_depth = cfg[\'num_blocks\'], cfg[\'dense_depth\']\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.last_planes = 64\n        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n        self.linear = nn.Linear(out_planes[3] + (num_blocks[3] + 1) * dense_depth[3], 10)\n\n    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for i, block_stride in enumerate(strides):\n            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, block_stride, i == 0))\n            self.last_planes = out_planes + (i + 2) * dense_depth\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef DPN26():\n    cfg = {\n        \'in_planes\': (96, 192, 384, 768),\n        \'out_planes\': (256, 512, 1024, 2048),\n        \'num_blocks\': (2, 2, 2, 2),\n        \'dense_depth\': (16, 32, 24, 128)\n    }\n    return DPN(cfg)\n\n\ndef DPN92():\n    cfg = {\n        \'in_planes\': (96, 192, 384, 768),\n        \'out_planes\': (256, 512, 1024, 2048),\n        \'num_blocks\': (3, 4, 20, 3),\n        \'dense_depth\': (16, 32, 24, 128)\n    }\n    return DPN(cfg)\n\n\ndef test():\n    net = DPN92()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y)\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/googlenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass Inception(nn.Module):\n    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n        super(Inception, self).__init__()\n        # 1x1 conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 3x3 conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n            nn.BatchNorm2d(n3x3red),\n            nn.ReLU(True),\n            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 5x5 conv branch\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n            nn.BatchNorm2d(n5x5red),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n        )\n\n        # 3x3 pool -> 1x1 conv branch\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n            nn.BatchNorm2d(pool_planes),\n            nn.ReLU(True),\n        )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        y2 = self.b2(x)\n        y3 = self.b3(x)\n        y4 = self.b4(x)\n        return torch.cat([y1, y2, y3, y4], 1)\n\n\nclass GoogLeNet(nn.Module):\n    def __init__(self):\n        super(GoogLeNet, self).__init__()\n        self.pre_layers = nn.Sequential(\n            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(True),\n        )\n\n        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.linear = nn.Linear(1024, 10)\n\n    def forward(self, x):\n        out = self.pre_layers(x)\n        out = self.a3(out)\n        out = self.b3(out)\n        out = self.maxpool(out)\n        out = self.a4(out)\n        out = self.b4(out)\n        out = self.c4(out)\n        out = self.d4(out)\n        out = self.e4(out)\n        out = self.maxpool(out)\n        out = self.a5(out)\n        out = self.b5(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = GoogLeNet()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y.size())\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/inceptionv3.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'Inception3\', \'inception_v3\']\n\nmodel_urls = {\n    # Inception v3 ported from TensorFlow\n    \'inception_v3_google\': \'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\',\n}\n\n\ndef inception_v3(pretrained=False, **kwargs):\n    r""""""Inception v3 model architecture from\n    `""Rethinking the Inception Architecture for Computer Vision"" <http://arxiv.org/abs/1512.00567>`_.\n\n    .. note::\n        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    if pretrained:\n        if \'transform_input\' not in kwargs:\n            kwargs[\'transform_input\'] = True\n        model = Inception3(**kwargs)\n        model.load_state_dict(model_zoo.load_url(model_urls[\'inception_v3_google\']))\n        return model\n\n    return Inception3(**kwargs)\n\n\nclass Inception3(nn.Module):\n\n    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False):\n        super(Inception3, self).__init__()\n        self.aux_logits = aux_logits\n        self.transform_input = transform_input\n        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n        self.Mixed_5b = InceptionA(192, pool_features=32)\n        self.Mixed_5c = InceptionA(256, pool_features=64)\n        self.Mixed_5d = InceptionA(288, pool_features=64)\n        self.Mixed_6a = InceptionB(288)\n        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n        if aux_logits:\n            self.AuxLogits = InceptionAux(768, num_classes)\n        self.Mixed_7a = InceptionD(768)\n        self.Mixed_7b = InceptionE(1280)\n        self.Mixed_7c = InceptionE(2048)\n        self.fc = nn.Linear(2048, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n                import scipy.stats as stats\n                stddev = m.stddev if hasattr(m, \'stddev\') else 0.1\n                X = stats.truncnorm(-2, 2, scale=stddev)\n                values = torch.Tensor(X.rvs(m.weight.numel()))\n                values = values.view(m.weight.size())\n                m.weight.data.copy_(values)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        if self.transform_input:\n            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n        # N x 3 x 299 x 299\n        x = self.Conv2d_1a_3x3(x)\n        # N x 32 x 149 x 149\n        x = self.Conv2d_2a_3x3(x)\n        # N x 32 x 147 x 147\n        x = self.Conv2d_2b_3x3(x)\n        # N x 64 x 147 x 147\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 64 x 73 x 73\n        x = self.Conv2d_3b_1x1(x)\n        # N x 80 x 73 x 73\n        x = self.Conv2d_4a_3x3(x)\n        # N x 192 x 71 x 71\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 192 x 35 x 35\n        x = self.Mixed_5b(x)\n        # N x 256 x 35 x 35\n        x = self.Mixed_5c(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_5d(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_6a(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6b(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6c(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6d(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6e(x)\n        # N x 768 x 17 x 17\n        if self.training and self.aux_logits:\n            aux = self.AuxLogits(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_7a(x)\n        # N x 1280 x 8 x 8\n        x = self.Mixed_7b(x)\n        # N x 2048 x 8 x 8\n        x = self.Mixed_7c(x)\n        # N x 2048 x 8 x 8\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 2048 x 1 x 1\n        x = F.dropout(x, training=self.training)\n        # N x 2048 x 1 x 1\n        x = x.view(x.size(0), -1)\n        # N x 2048\n        x = self.fc(x)\n        # N x 1000 (num_classes)\n        if self.training and self.aux_logits:\n            return x, aux\n        return x\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, pool_features):\n        super(InceptionA, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionB(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionB, self).__init__()\n        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3(x)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n\n        outputs = [branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionC(nn.Module):\n\n    def __init__(self, in_channels, channels_7x7):\n        super(InceptionC, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n\n        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionD(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionD, self).__init__()\n        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n\n        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n\n        branch7x7x3 = self.branch7x7x3_1(x)\n        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n        outputs = [branch3x3, branch7x7x3, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionE(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionE, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n\n    def __init__(self, in_channels, num_classes):\n        super(InceptionAux, self).__init__()\n        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01\n        self.fc = nn.Linear(768, num_classes)\n        self.fc.stddev = 0.001\n\n    def forward(self, x):\n        # N x 768 x 17 x 17\n        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n        # N x 768 x 5 x 5\n        x = self.conv0(x)\n        # N x 128 x 5 x 5\n        x = self.conv1(x)\n        # N x 768 x 1 x 1\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 768 x 1 x 1\n        x = x.view(x.size(0), -1)\n        # N x 768\n        x = self.fc(x)\n        # N x 1000\n        return x\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n'"
pytorch_toolkit/nncf/tests/test_models/lenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n        return out\n'"
pytorch_toolkit/nncf/tests/test_models/mobilenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block(nn.Module):\n    \'\'\'Depthwise conv + Pointwise conv\'\'\'\n\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(Block, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        return out\n\n\nclass MobileNet(nn.Module):\n    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n    cfg = [64, (128, 2), 128, (256, 2), 256, (512, 2), 512, 512, 512, 512, 512, (1024, 2), 1024]\n\n    def __init__(self, num_classes=10):\n        super(MobileNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layers = self._make_layers(in_planes=32)\n        self.linear = nn.Linear(1024, num_classes)\n\n    def _make_layers(self, in_planes):\n        layers = []\n        for x in self.cfg:\n            out_planes = x if isinstance(x, int) else x[0]\n            stride = 1 if isinstance(x, int) else x[1]\n            layers.append(Block(in_planes, out_planes, stride))\n            in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.avg_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = MobileNet()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y.size())\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/pnasnet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SepConv(nn.Module):\n    \'\'\'Separable Convolution.\'\'\'\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride):\n        super(SepConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, out_planes,\n                               kernel_size, stride,\n                               padding=(kernel_size - 1) // 2,\n                               bias=False, groups=in_planes)\n        self.bn1 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        return self.bn1(self.conv1(x))\n\n\nclass CellA(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(CellA, self).__init__()\n        self.stride = stride\n        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n        if stride == 2:\n            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n            self.bn1 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        y1 = self.sep_conv1(x)\n        y2 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n        if self.stride == 2:\n            y2 = self.bn1(self.conv1(y2))\n        return F.relu(y1 + y2)\n\n\nclass CellB(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(CellB, self).__init__()\n        self.stride = stride\n        # Left branch\n        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n        self.sep_conv2 = SepConv(in_planes, out_planes, kernel_size=3, stride=stride)\n        # Right branch\n        self.sep_conv3 = SepConv(in_planes, out_planes, kernel_size=5, stride=stride)\n        if stride == 2:\n            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n            self.bn1 = nn.BatchNorm2d(out_planes)\n        # Reduce channels\n        self.conv2 = nn.Conv2d(2 * out_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        # Left branch\n        y1 = self.sep_conv1(x)\n        y2 = self.sep_conv2(x)\n        # Right branch\n        y3 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n        if self.stride == 2:\n            y3 = self.bn1(self.conv1(y3))\n        y4 = self.sep_conv3(x)\n        # Concat & reduce channels\n        b1 = F.relu(y1 + y2)\n        b2 = F.relu(y3 + y4)\n        y = torch.cat([b1, b2], 1)\n        return F.relu(self.bn2(self.conv2(y)))\n\n\nclass PNASNet(nn.Module):\n    def __init__(self, cell_type, num_cells, num_planes):\n        super(PNASNet, self).__init__()\n        self.in_planes = num_planes\n        self.cell_type = cell_type\n\n        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_planes)\n\n        self.layer1 = self._make_layer(num_planes, num_cells=6)\n        self.layer2 = self._downsample(num_planes * 2)\n        self.layer3 = self._make_layer(num_planes * 2, num_cells=6)\n        self.layer4 = self._downsample(num_planes * 4)\n        self.layer5 = self._make_layer(num_planes * 4, num_cells=6)\n\n        self.linear = nn.Linear(num_planes * 4, 10)\n\n    def _make_layer(self, planes, num_cells):\n        layers = []\n        for _ in range(num_cells):\n            layers.append(self.cell_type(self.in_planes, planes, stride=1))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def _downsample(self, planes):\n        layer = self.cell_type(self.in_planes, planes, stride=2)\n        self.in_planes = planes\n        return layer\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = F.avg_pool2d(out, 8)\n        out = self.linear(out.view(out.size(0), -1))\n        return out\n\n\ndef PNASNetA():\n    return PNASNet(CellA, num_cells=6, num_planes=44)\n\n\ndef PNASNetB():\n    return PNASNet(CellB, num_cells=6, num_planes=32)\n\n\ndef test():\n    net = PNASNetB()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y)\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/preact_resnet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PreActBlock(nn.Module):\n    \'\'\'Pre-activation version of the BasicBlock.\'\'\'\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    \'\'\'Pre-activation version of the original Bottleneck module.\'\'\'\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(PreActResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for block_stride in strides:\n            layers.append(block(self.in_planes, planes, block_stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef PreActResNet18():\n    return PreActResNet(PreActBlock, [2, 2, 2, 2])\n\n\ndef PreActResNet34():\n    return PreActResNet(PreActBlock, [3, 4, 6, 3])\n\n\ndef PreActResNet50():\n    return PreActResNet(PreActBottleneck, [3, 4, 6, 3])\n\n\ndef PreActResNet101():\n    return PreActResNet(PreActBottleneck, [3, 4, 23, 3])\n\n\ndef PreActResNet152():\n    return PreActResNet(PreActBottleneck, [3, 8, 36, 3])\n\n\ndef test():\n    net = PreActResNet18()\n    y = net((torch.randn(1, 3, 32, 32)))\n    print(y.size())\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/resnet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for block_stride in strides:\n            layers.append(block(self.in_planes, planes, block_stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.maxpool(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = torch.flatten(out, 1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2, 2, 2, 2])\n\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3, 4, 6, 3])\n\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3, 4, 6, 3])\n\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3, 4, 23, 3])\n\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3, 8, 36, 3])\n\n\ndef test():\n    net = ResNet18()\n    y = net(torch.randn(1, 3, 32, 32))\n    print(y.size())\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/resnext.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block(nn.Module):\n    """"""Grouped convolution block.""""""\n    expansion = 2\n\n    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n        super(Block, self).__init__()\n        group_width = cardinality * bottleneck_width\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(group_width)\n        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(group_width)\n        self.conv3 = nn.Conv2d(group_width, self.expansion * group_width, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion * group_width)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * group_width:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * group_width, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * group_width)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNeXt(nn.Module):\n    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n        super(ResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.bottleneck_width = bottleneck_width\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(num_blocks[0], 1)\n        self.layer2 = self._make_layer(num_blocks[1], 2)\n        self.layer3 = self._make_layer(num_blocks[2], 2)\n        # self.layer4 = self._make_layer(num_blocks[3], 2)\n        self.linear = nn.Linear(cardinality * bottleneck_width * 8, num_classes)\n\n    def _make_layer(self, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for block_strde in strides:\n            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, block_strde))\n            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n        # Increase bottleneck_width by 2 after each stage.\n        self.bottleneck_width *= 2\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        # out = self.layer4(out)\n        out = F.avg_pool2d(out, 8)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNeXt29_2x64d():\n    return ResNeXt(num_blocks=[3, 3, 3], cardinality=2, bottleneck_width=64)\n\n\ndef ResNeXt29_4x64d():\n    return ResNeXt(num_blocks=[3, 3, 3], cardinality=4, bottleneck_width=64)\n\n\ndef ResNeXt29_8x64d():\n    return ResNeXt(num_blocks=[3, 3, 3], cardinality=8, bottleneck_width=64)\n\n\ndef ResNeXt29_32x4d():\n    return ResNeXt(num_blocks=[3, 3, 3], cardinality=32, bottleneck_width=4)\n\n\ndef test_resnext():\n    net = ResNeXt29_2x64d()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y.size())\n\n# test_resnext()\n'"
pytorch_toolkit/nncf/tests/test_models/senet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes)\n            )\n\n        # SE layers\n        self.fc1 = nn.Conv2d(planes, planes // 16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear\n        self.fc2 = nn.Conv2d(planes // 16, planes, kernel_size=1)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n\n        # Squeeze\n        w = F.avg_pool2d(out, out.size(2))\n        w = F.relu(self.fc1(w))\n        w = F.sigmoid(self.fc2(w))\n        # Excitation\n        out = out * w  # New broadcasting feature from v0.2!\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass PreActBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n        # SE layers\n        self.fc1 = nn.Conv2d(planes, planes // 16, kernel_size=1)\n        self.fc2 = nn.Conv2d(planes // 16, planes, kernel_size=1)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n\n        # Squeeze\n        w = F.avg_pool2d(out, out.size(2))\n        w = F.relu(self.fc1(w))\n        w = F.sigmoid(self.fc2(w))\n        # Excitation\n        out = out * w\n\n        out += shortcut\n        return out\n\n\nclass SENet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(SENet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for block_stride in strides:\n            layers.append(block(self.in_planes, planes, block_stride))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef SENet18():\n    return SENet(PreActBlock, [2, 2, 2, 2])\n\n\ndef test():\n    net = SENet18()\n    y = net(torch.randn(1, 3, 32, 32))\n    print(y.size())\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/shufflenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ShuffleBlock(nn.Module):\n    def __init__(self, groups):\n        super(ShuffleBlock, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        \'\'\'Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\'\'\'\n        N, C, H, W = x.size()\n        g = self.groups\n        return x.view(N, g, C // g, H, W).permute(0, 2, 1, 3, 4).contiguous().view(N, C, H, W)\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, groups):\n        super(Bottleneck, self).__init__()\n        self.stride = stride\n\n        mid_planes = out_planes // 4\n        g = 1 if in_planes == 24 else groups\n        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes)\n        self.shuffle1 = ShuffleBlock(groups=g)\n        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_planes)\n        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes)\n\n        self.shortcut = nn.Sequential()\n        if stride == 2:\n            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.shuffle1(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        res = self.shortcut(x)\n        out = F.relu(torch.cat([out, res], 1)) if self.stride == 2 else F.relu(out + res)\n        return out\n\n\nclass ShuffleNet(nn.Module):\n    def __init__(self, cfg):\n        super(ShuffleNet, self).__init__()\n        out_planes = cfg[\'out_planes\']\n        num_blocks = cfg[\'num_blocks\']\n        groups = cfg[\'groups\']\n\n        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(24)\n        self.in_planes = 24\n        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n        self.linear = nn.Linear(out_planes[2], 10)\n\n    def _make_layer(self, out_planes, num_blocks, groups):\n        layers = []\n        for i in range(num_blocks):\n            stride = 2 if i == 0 else 1\n            cat_planes = self.in_planes if i == 0 else 0\n            layers.append(Bottleneck(self.in_planes, out_planes - cat_planes, stride=stride, groups=groups))\n            self.in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ShuffleNetG2():\n    cfg = {\n        \'out_planes\': [200, 400, 800],\n        \'num_blocks\': [4, 8, 4],\n        \'groups\': 2\n    }\n    return ShuffleNet(cfg)\n\n\ndef ShuffleNetG3():\n    cfg = {\n        \'out_planes\': [240, 480, 960],\n        \'num_blocks\': [4, 8, 4],\n        \'groups\': 3\n    }\n    return ShuffleNet(cfg)\n\n\ndef test():\n    net = ShuffleNetG2()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y)\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/shufflenetv2.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ShuffleBlock(nn.Module):\n    def __init__(self, groups=2):\n        super(ShuffleBlock, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        \'\'\'Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\'\'\'\n        N, C, H, W = x.size()\n        g = self.groups\n        return x.view(N, g, C // g, H, W).permute(0, 2, 1, 3, 4).reshape(N, C, H, W)\n\n\nclass SplitBlock(nn.Module):\n    def __init__(self, ratio):\n        super(SplitBlock, self).__init__()\n        self.ratio = ratio\n\n    def forward(self, x):\n        c = int(x.size(1) * self.ratio)\n        return x[:, :c, :, :], x[:, c:, :, :]\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, split_ratio=0.5):\n        super(BasicBlock, self).__init__()\n        self.split = SplitBlock(split_ratio)\n        in_channels = int(in_channels * split_ratio)\n        self.conv1 = nn.Conv2d(in_channels, in_channels,\n                               kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(in_channels, in_channels,\n                               kernel_size=3, stride=1, padding=1, groups=in_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(in_channels)\n        self.conv3 = nn.Conv2d(in_channels, in_channels,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(in_channels)\n        self.shuffle = ShuffleBlock()\n\n    def forward(self, x):\n        x1, x2 = self.split(x)\n        out = F.relu(self.bn1(self.conv1(x2)))\n        out = self.bn2(self.conv2(out))\n        out = F.relu(self.bn3(self.conv3(out)))\n        out = torch.cat([x1, out], 1)\n        out = self.shuffle(out)\n        return out\n\n\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DownBlock, self).__init__()\n        mid_channels = out_channels // 2\n        # left\n        self.conv1 = nn.Conv2d(in_channels, in_channels,\n                               kernel_size=3, stride=2, padding=1, groups=in_channels, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(in_channels, mid_channels,\n                               kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        # right\n        self.conv3 = nn.Conv2d(in_channels, mid_channels,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(mid_channels)\n        self.conv4 = nn.Conv2d(mid_channels, mid_channels,\n                               kernel_size=3, stride=2, padding=1, groups=mid_channels, bias=False)\n        self.bn4 = nn.BatchNorm2d(mid_channels)\n        self.conv5 = nn.Conv2d(mid_channels, mid_channels,\n                               kernel_size=1, bias=False)\n        self.bn5 = nn.BatchNorm2d(mid_channels)\n\n        self.shuffle = ShuffleBlock()\n\n    def forward(self, x):\n        # left\n        out1 = self.bn1(self.conv1(x))\n        out1 = F.relu(self.bn2(self.conv2(out1)))\n        # right\n        out2 = F.relu(self.bn3(self.conv3(x)))\n        out2 = self.bn4(self.conv4(out2))\n        out2 = F.relu(self.bn5(self.conv5(out2)))\n        # concat\n        out = torch.cat([out1, out2], 1)\n        out = self.shuffle(out)\n        return out\n\n\nclass ShuffleNetV2(nn.Module):\n    def __init__(self, net_size):\n        super(ShuffleNetV2, self).__init__()\n        out_channels = configs[net_size][\'out_channels\']\n        num_blocks = configs[net_size][\'num_blocks\']\n\n        self.conv1 = nn.Conv2d(3, 24, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(24)\n        self.in_channels = 24\n        self.layer1 = self._make_layer(out_channels[0], num_blocks[0])\n        self.layer2 = self._make_layer(out_channels[1], num_blocks[1])\n        self.layer3 = self._make_layer(out_channels[2], num_blocks[2])\n        self.conv2 = nn.Conv2d(out_channels[2], out_channels[3],\n                               kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels[3])\n        self.linear = nn.Linear(out_channels[3], 10)\n\n    def _make_layer(self, out_channels, num_blocks):\n        layers = [DownBlock(self.in_channels, out_channels)]\n        for _ in range(num_blocks):\n            layers.append(BasicBlock(out_channels))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        # out = F.max_pool2d(out, 3, stride=2, padding=1)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\nconfigs = {\n    0.5: {\n        \'out_channels\': (48, 96, 192, 1024),\n        \'num_blocks\': (3, 7, 3)\n    },\n\n    1: {\n        \'out_channels\': (116, 232, 464, 1024),\n        \'num_blocks\': (3, 7, 3)\n    },\n    1.5: {\n        \'out_channels\': (176, 352, 704, 1024),\n        \'num_blocks\': (3, 7, 3)\n    },\n    2: {\n        \'out_channels\': (224, 488, 976, 2048),\n        \'num_blocks\': (3, 7, 3)\n    }\n}\n\n\ndef test():\n    net = ShuffleNetV2(net_size=0.5)\n    x = torch.randn(3, 3, 32, 32)\n    y = net(x)\n    print(y.shape)\n\n# test()\n'"
pytorch_toolkit/nncf/tests/test_models/squeezenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n__all__ = [\'SqueezeNet\', \'squeezenet1_0\', \'squeezenet1_1\']\n\n\nclass Fire(nn.Module):\n\n    def __init__(self, inplanes, squeeze_planes,\n                 expand1x1_planes, expand3x3_planes):\n        super(Fire, self).__init__()\n        self.inplanes = inplanes\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n                                   kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n                                   kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n\nclass SqueezeNet(nn.Module):\n\n    def __init__(self, version=1.0, num_classes=1000):\n        super(SqueezeNet, self).__init__()\n        if version not in [1.0, 1.1]:\n            raise ValueError(""Unsupported SqueezeNet version {version}:""\n                             ""1.0 or 1.1 expected"".format(version=version))\n        self.num_classes = num_classes\n        if version == 1.0:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                Fire(128, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(256, 32, 128, 128),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(512, 64, 256, 256),\n            )\n        else:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256),\n            )\n        # Final convolution is initialized differently form the rest\n        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            final_conv,\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m is final_conv:\n                    init.normal_(m.weight, mean=0.0, std=0.01)\n                else:\n                    init.kaiming_uniform_(m.weight)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x.view(x.size(0), self.num_classes)\n\n\ndef squeezenet1_0(**kwargs):\n    r""""""SqueezeNet model architecture from the `""SqueezeNet: AlexNet-level\n    accuracy with 50x fewer parameters and <0.5MB model size""\n    <https://arxiv.org/abs/1602.07360>`_ paper.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = SqueezeNet(version=1.0, **kwargs)\n    return model\n\n\ndef squeezenet1_1(**kwargs):\n    r""""""SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = SqueezeNet(version=1.1, **kwargs)\n    return model\n'"
pytorch_toolkit/nncf/tests/test_models/ssd_mobilenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom examples.object_detection.layers.modules.ssd_head import MultiOutputSequential, SSDDetectionOutput\nfrom nncf.config import Config\nfrom nncf.checkpoint_loading import load_state\n\n\ndef conv_bn(inp, oup, kernel, stride, padding):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, kernel, stride, padding, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef conv_dw(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n        nn.BatchNorm2d(inp),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True),\n    )\n\n\ndef mobilenet(start_input_channels=3):\n    model = MultiOutputSequential(\n        [11, 13],\n        [\n            conv_bn(start_input_channels, 32, 3, 2, 1),\n            conv_dw(32, 64, 1),\n            conv_dw(64, 128, 2),\n            conv_dw(128, 128, 1),\n            conv_dw(128, 256, 2),\n            conv_dw(256, 256, 1),\n            conv_dw(256, 512, 2),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 1024, 2),\n            conv_dw(1024, 1024, 1)\n        ]\n    )\n    return model\n\n\ndef extra(start_input_channels):\n    return MultiOutputSequential(\n        [1, 3, 5, 7],\n        [\n            conv_bn(start_input_channels, 256, 1, 1, 0),\n            conv_bn(256, 512, 3, 2, 1),\n            conv_bn(512, 128, 1, 1, 0),\n            conv_bn(128, 256, 3, 2, 1),\n            conv_bn(256, 128, 1, 1, 0),\n            conv_bn(128, 256, 3, 2, 1),\n            conv_bn(256, 64, 1, 1, 0),\n            conv_bn(64, 128, 3, 2, 1)\n        ]\n    )\n\n\nclass MobileNetSSD(nn.Module):\n    def __init__(self, num_classes, cfg):\n        super(MobileNetSSD, self).__init__()\n        self.cfg = cfg\n        self.num_classes = num_classes\n\n        self.basenet = mobilenet()\n        self.extras = extra(1024)\n\n        NUM_INPUT_FEATURES = [512, 1024, 512, 256, 256, 128]\n        self.detection_head = SSDDetectionOutput(NUM_INPUT_FEATURES, num_classes, cfg)\n\n    def forward(self, x):\n        img_tensor = x[0].clone().unsqueeze(0)\n\n        sources, x = self.basenet(x)\n        extra_sources, x = self.extras(x)\n\n        return self.detection_head(sources + extra_sources, img_tensor)\n\n\ndef build_ssd_mobilenet(cfg, size, num_classes, config):\n    if size != 300:\n        raise ValueError(""Only Mobilenet-SSD with input size 300 is supported"")\n    mobilenet_ssd = MobileNetSSD(num_classes, cfg)\n\n    if config.basenet and (config.resuming_checkpoint is None) and (config.weights is None):\n        print(\'Loading base network...\')\n        basenet_weights = torch.load(config.basenet)[\'state_dict\']\n        new_weights = {}\n        for wn, wv in basenet_weights.items():\n            wn = wn.replace(\'model.\', \'\')\n            new_weights[wn] = wv\n\n        load_state(mobilenet_ssd.basenet, new_weights, is_resume=False)\n    return mobilenet_ssd\n\n\ndef ssd_mobilenet():\n    ssd_params = Config({\n        ""variance"": [0.1, 0.1, 0.2, 0.2],\n        ""max_sizes"": [60, 111, 162, 213, 264, 315],\n        ""min_sizes"": [30, 60, 111, 162, 213, 264],\n        ""steps"": [16, 32, 64, 100, 150, 300],\n        ""aspect_ratios"": [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n        ""clip"": False,\n        ""flip"": True,\n        ""top_k"": 200\n    })\n\n    return MobileNetSSD(21, ssd_params)\n'"
pytorch_toolkit/nncf/tests/test_models/ssd_vgg.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\n\nimport torch\nimport torch.nn as nn\nfrom examples.object_detection.layers import L2Norm\nfrom examples.object_detection.layers.modules.ssd_head import MultiOutputSequential, SSDDetectionOutput\nfrom nncf.config import Config\nfrom nncf.checkpoint_loading import load_state\n\nBASE_NUM_OUTPUTS = {\n    300: [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\', 512, 512, 512],\n    512: [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512],\n}\nEXTRAS_NUM_OUTPUTS = {\n    300: [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n    512: [256, \'S\', 512, 128, \'S\', 256, 128, \'S\', 256, 128, \'S\', 256, 128, \'K\', 256],\n}\n\nBASE_OUTPUT_INDICES = {\n    300: [12],\n    512: [12],\n}\n\nEXTRA_OUTPUT_INDICES = {\n    300: [2, 5, 7, 9],\n    512: [2, 5, 8, 11, 14],\n}\n\n\nclass SSD_VGG(nn.Module):\n    def __init__(self, cfg, size, num_classes, batch_norm=False):\n        super(SSD_VGG, self).__init__()\n        self.config = cfg\n        self.num_classes = num_classes\n        self.size = size\n        self.enable_batchmorm = batch_norm\n\n        base_layers, base_outs, base_feats = build_vgg_ssd_layers(\n            BASE_NUM_OUTPUTS[size], BASE_OUTPUT_INDICES[size], batch_norm=batch_norm\n        )\n        extra_layers, extra_outs, extra_feats = build_vgg_ssd_extra(\n            EXTRAS_NUM_OUTPUTS[size], EXTRA_OUTPUT_INDICES[size], batch_norm=batch_norm\n        )\n        self.basenet = MultiOutputSequential(base_outs, base_layers)\n        self.extras = MultiOutputSequential(extra_outs, extra_layers)\n\n        self.detection_head = SSDDetectionOutput(base_feats + extra_feats, num_classes, cfg)\n        self.L2Norm = L2Norm(512, 20, 1e-10)\n\n    def forward(self, x):\n        img_tensor = x[0].clone().unsqueeze(0)\n\n        sources, x = self.basenet(x)\n        sources[0] = self.L2Norm(sources[0])\n\n        extra_sources, x = self.extras(x)\n\n        return self.detection_head(sources + extra_sources, img_tensor)\n\n    def load_weights(self, base_file):\n        _, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file,\n                                            map_location=lambda storage, loc: storage))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\ndef make_ssd_vgg_layer(input_features, output_features, kernel=3, padding=1, dilation=1, modifier=None,\n                       batch_norm=False):\n    stride = 1\n    if modifier == \'S\':\n        stride = 2\n        padding = 1\n    elif modifier == \'K\':\n        kernel = 4\n        padding = 1\n\n    layer = [nn.Conv2d(input_features, output_features, kernel_size=kernel, stride=stride, padding=padding,\n                       dilation=dilation)]\n    if batch_norm:\n        layer.append(nn.BatchNorm2d(output_features))\n    layer.append(nn.ReLU(inplace=True))\n    return layer\n\n\ndef build_vgg_ssd_layers(num_outputs, output_inddices, start_input_channels=3, batch_norm=False):\n    vgg_layers = []\n    output_num_features = []\n    source_indices = []\n    in_planes = start_input_channels\n    modifier = None\n    for i, out_planes in enumerate(num_outputs):\n        if out_planes in (\'M\', \'C\'):\n            vgg_layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=1 if modifier == \'C\' else 0))\n            continue\n        if isinstance(out_planes, str):\n            modifier = out_planes\n            continue\n        vgg_layers.extend(make_ssd_vgg_layer(in_planes, out_planes, modifier=modifier, batch_norm=batch_norm))\n        modifier = None\n        in_planes = out_planes\n        if i in output_inddices:\n            source_indices.append(len(vgg_layers) - 1)\n            output_num_features.append(out_planes)\n\n    vgg_layers.append(nn.MaxPool2d(kernel_size=3, stride=1, padding=1))\n    vgg_layers.extend(make_ssd_vgg_layer(in_planes, 1024, kernel=3, padding=6, dilation=6, batch_norm=batch_norm))\n    vgg_layers.extend(make_ssd_vgg_layer(1024, 1024, kernel=1, batch_norm=batch_norm))\n\n    source_indices.append(len(vgg_layers) - 1)\n    output_num_features.append(1024)\n    return vgg_layers, source_indices, output_num_features\n\n\ndef build_vgg_ssd_extra(num_outputs, output_indices, statrt_input_channels=1024, batch_norm=False):\n    extra_layers = []\n    output_num_features = []\n    source_indices = []\n    in_planes = statrt_input_channels\n    modifier = None\n    kernel_sizes = (1, 3)\n    for i, out_planes in enumerate(num_outputs):\n        if isinstance(out_planes, str):\n            modifier = out_planes\n            continue\n        kernel = kernel_sizes[len(extra_layers) % 2]\n        extra_layers.extend(make_ssd_vgg_layer(in_planes, out_planes, modifier=modifier, kernel=kernel, padding=0,\n                                               batch_norm=batch_norm))\n        modifier = None\n        in_planes = out_planes\n        if i in output_indices:\n            source_indices.append(len(extra_layers) - 1)\n            output_num_features.append(out_planes)\n\n    return extra_layers, source_indices, output_num_features\n\n\ndef build_ssd_vgg(cfg, size, num_classes, config):\n    ssd_vgg = SSD_VGG(cfg, size, num_classes, batch_norm=config.get(\'batchnorm\', False))\n\n    if config.basenet and (config.resuming_checkpoint is None) and (config.weights is None):\n        print(\'Loading base network...\')\n        basenet_weights = torch.load(config.basenet)\n        new_weights = {}\n        for wn, wv in basenet_weights.items():\n            wn = wn.replace(\'features.\', \'\')\n            new_weights[wn] = wv\n\n        load_state(ssd_vgg.basenet, new_weights, is_resume=False)\n    return ssd_vgg\n\n\ndef ssd_vgg300():\n    ssd_params = Config({\n        ""clip"": False,\n        ""variance"": [0.1, 0.1, 0.2, 0.2],\n        ""max_sizes"": [60, 111, 162, 213, 264, 315],\n        ""min_sizes"": [30, 60, 111, 162, 213, 264],\n        ""steps"": [8, 16, 32, 64, 100, 300],\n        ""aspect_ratios"": [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n        ""flip"": True\n    })\n\n    return SSD_VGG(ssd_params, 300, 21, True)\n'"
pytorch_toolkit/nncf/tests/test_models/unet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\nclass UNet(nn.Module):\n    def __init__(\n            self,\n            in_channels=3,\n            n_classes=12,\n            depth=5,\n            wf=6,\n            padding=False,\n            batch_norm=True,\n            up_mode=\'upconv\',\n    ):\n        """"""\n        Implementation of\n        U-Net: Convolutional Networks for Biomedical Image Segmentation\n        (Ronneberger et al., 2015)\n        https://arxiv.org/abs/1505.04597\n        Using the default arguments will yield the exact version used\n        in the original paper\n        Args:\n            in_channels (int): number of input channels\n            n_classes (int): number of output channels\n            depth (int): depth of the network\n            wf (int): number of filters in the first layer is 2**wf\n            padding (bool): if True, apply padding such that the input shape\n                            is the same as the output.\n                            This may introduce artifacts\n            batch_norm (bool): Use BatchNorm prior to layers with an\n                               activation function\n            up_mode (str): one of \'upconv\' or \'upsample\'.\n                           \'upconv\' will use transposed convolutions for\n                           learned upsampling.\n                           \'upsample\' will use bilinear upsampling.\n        """"""\n        super(UNet, self).__init__()\n        assert up_mode in (\'upconv\', \'upsample\')\n        self.padding = padding\n        self.depth = depth\n        prev_channels = in_channels\n        self.down_path = nn.ModuleList()\n        for i in range(depth):\n            self.down_path.append(\n                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n            )\n            prev_channels = 2 ** (wf + i)\n\n        self.up_path = nn.ModuleList()\n        for i in reversed(range(depth - 1)):\n            self.up_path.append(\n                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n            )\n            prev_channels = 2 ** (wf + i)\n\n        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n\n    def forward(self, x):\n        blocks = []\n        for i, down in enumerate(self.down_path):\n            x = down(x)\n            if i != len(self.down_path) - 1:\n                blocks.append(x)\n                x = F.max_pool2d(x, 2)\n\n        for i, up in enumerate(self.up_path):\n            x = up(x, blocks[-i - 1])\n\n        x = self.last(x)\n        return x\n\n\nclass UNetConvBlock(nn.Module):\n    def __init__(self, in_size, out_size, padding, batch_norm):\n        super(UNetConvBlock, self).__init__()\n        block = []\n\n        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n        block.append(nn.ReLU())\n\n        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n        block.append(nn.ReLU())\n\n        self.block = nn.Sequential(*block)\n\n    def forward(self, x):\n        out = self.block(x)\n        return out\n\n\ndef center_crop(layer, target_size):\n    if layer.dim() == 4:\n        # Cropping feature maps\n        _, _, layer_height, layer_width = layer.size()\n        diff_y = (layer_height - target_size[0]) // 2\n        diff_x = (layer_width - target_size[1]) // 2\n        return layer[\n            :, :, diff_y: (diff_y + target_size[0]), diff_x: (diff_x + target_size[1])\n            ]\n\n    # If dimension is not 4, assume that we are cropping ground truth labels\n    assert layer.dim() == 3\n    _, layer_height, layer_width = layer.size()\n    diff_y = (layer_height - target_size[0]) // 2\n    diff_x = (layer_width - target_size[1]) // 2\n    return layer[\n        :, diff_y: (diff_y + target_size[0]), diff_x: (diff_x + target_size[1])\n        ]\n\n\nclass UNetUpBlock(nn.Module):\n    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n        super(UNetUpBlock, self).__init__()\n        if up_mode == \'upconv\':\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n        elif up_mode == \'upsample\':\n            self.up = nn.Sequential(\n                nn.Upsample(mode=\'bilinear\', scale_factor=2),\n                nn.Conv2d(in_size, out_size, kernel_size=1),\n            )\n        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n\n    def forward(self, x, bridge):\n        up = self.up(x)\n        crop1 = center_crop(bridge, up.shape[2:])\n        out = torch.cat([up, crop1], 1)\n        out = self.conv_block(out)\n\n        return out\n'"
pytorch_toolkit/nncf/tests/test_models/vgg.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nVGG_CONFIGS = {\n    \'VGG11\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'VGG13\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'VGG16\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'VGG19\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(VGG_CONFIGS[vgg_name])\n        self.classifier = nn.Linear(512, 10)\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, config):\n        layers = []\n        in_channels = 3\n        for x in config:\n            if x == \'M\':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n\n\ndef test():\n    net = VGG(\'VGG11\')\n    x = torch.randn(2, 3, 32, 32)\n    y = net(x)\n    print(y.size())\n'"
pytorch_toolkit/nncf/tools/debug/__init__.py,0,b''
pytorch_toolkit/nncf/tools/debug/common.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nfrom functools import partial\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom examples.common.model_loader import load_model\nfrom nncf.model_creation import create_compressed_model\nfrom nncf.checkpoint_loading import load_state\nfrom nncf.layers import NNCFConv2d, NNCFLinear\nfrom examples.common.utils import print_statistics\n\n\ndef dump_in_out_hook(module, inputs, output):\n    dump_out_hook(module, inputs, output)\n    dump_path = get_dump_path(module)\n    if dump_path:\n        key = 0\n        output_dir = os.path.abspath(os.path.join(dump_path, os.pardir))\n        file_name = os.path.basename(dump_path)\n        for input_ in inputs:\n            key += 1\n            input_data = input_.data.cpu().numpy().flatten()\n            dump_name = \'.\'.join([file_name, ""in"", str(key)])\n            npy_path, _ = save_dump(dump_name, output_dir, input_data)\n            add_full_dump_path(module, npy_path)\n\n\ndef dump_out_hook(module, inputs, output):\n    dump_path = get_dump_path(module)\n    if dump_path:\n        output_data = output.data.cpu().numpy().flatten()\n        output_dir = os.path.abspath(os.path.join(dump_path, os.pardir))\n        file_name = os.path.basename(dump_path)\n        dump_name = \'.\'.join([file_name, ""out""])\n        npy_path, _ = save_dump(dump_name, output_dir, output_data, force=False)\n        add_full_dump_path(module, npy_path)\n\n\ndef get_dump_path(module):\n    if hasattr(module, ""dump_path""):\n        return module.dump_path\n    return None\n\n\ndef set_dump_path(layer, path):\n    layer.dump_path = path\n\n\ndef add_full_dump_path(layer, full_path):\n    if not hasattr(layer, \'dump_full_paths\'):\n        layer.dump_full_paths = []\n    layer.dump_full_paths.append(full_path)\n\n\ndef get_full_dump_paths(layer):\n    if hasattr(layer, \'dump_full_paths\'):\n        return layer.dump_full_paths\n    return None\n\n\ndef is_weightable(layer):\n    return isinstance(layer, (nn.Conv2d, nn.Linear)) and \\\n           not isinstance(layer, (NNCFConv2d, NNCFLinear))\n\n\ndef has_sparse_quant_weights(layer, name):\n    from nncf.quantization.layers import SymmetricQuantizer\n    from nncf.sparsity.rb.layers import RBSparsifyingWeight\n    return (isinstance(layer, RBSparsifyingWeight) and (\'sparsified_weight\' in name)) or \\\n           (isinstance(layer, SymmetricQuantizer) and (\'quantized_weight\' in name))\n\n\ndef save_dump_(path, ext, saver, data, force=False):\n    full_path = \'.\'.join([path, ext])\n    if not os.path.exists(full_path) or force:\n        print(""Saving dump to {}"".format(full_path))\n        saver(full_path, data)\n    else:\n        print(""Dump already exists "" + full_path)\n    return full_path\n\n\ndef save_dump(dump_name, output_dir, data, force=False):\n    path = os.path.join(output_dir, dump_name)\n    npy_path = save_dump_(path, ""npy"", np.save, data, force)\n    txt_path = save_dump_(path, ""txt"", partial(np.savetxt, fmt=""%s""), data, force)\n    return npy_path, txt_path\n\n\ndef register_print_hooks(path, model, data_to_compare, num_layers=-1, dump_activations=False, prefix=\'\', idx=0):\n    for name, children in model.named_children():\n        name_full = ""{}{}"".format(prefix, name)\n        idx = register_print_hooks(path, children, data_to_compare, num_layers, dump_activations,\n                                   prefix=name_full + ""."", idx=idx)\n\n        within_range = (num_layers == -1) or idx < num_layers\n        has_weights = has_sparse_quant_weights(children, name_full) or is_weightable(children)\n        within_type = has_weights if not dump_activations else dump_activations\n        if within_range and within_type:\n            # always there for activations if dump_activation is enabled\n            # always there for weights if dump_activation is disabled\n            name_full = name_full.replace(\'/\', \'_\')\n            dump_path = os.path.join(path, \'.\'.join([str(idx), name_full]))\n            idx += 1\n            if is_weightable(children):\n                output_dir = os.path.abspath(os.path.join(dump_path, os.pardir))\n                file_name = os.path.basename(dump_path)\n\n                def dump_attr(attr):\n                    if hasattr(children, attr):\n                        dump_name = \'.\'.join([file_name, attr])\n                        data = children.weight.data.numpy()\n                        save_dump(dump_name, output_dir, data, force=False)\n                        data_to_compare[dump_name] = data\n\n                dump_attr(\'weight\')\n                dump_attr(\'bias\')\n            else:\n                set_dump_path(children, dump_path)\n                hook = dump_in_out_hook if dump_activations else dump_out_hook\n                children.register_forward_hook(hook)\n    return idx\n\n\ndef load_torch_model(config, cuda=False):\n    weights = config.get(\'weights\')\n    model = load_model(config.get(\'model\'),\n                       pretrained=config.get(\'pretrained\', True) if weights is None else False,\n                       num_classes=config.get(\'num_classes\', 1000),\n                       model_params=config.get(\'model_params\', {}))\n    compression_ctrl, model = create_compressed_model(model, config)\n    if weights:\n        sd = torch.load(weights, map_location=\'cpu\')\n        load_state(model, sd)\n    if cuda:\n        model = model.cuda()\n        model = torch.nn.DataParallel(model)\n    print_statistics(compression_ctrl.statistics())\n    return model\n\n\ndef compare_activations(ir_dump_txt, torch_dump_npy):\n    with open(ir_dump_txt, \'r\') as fin:\n        first_line = fin.readline()\n        if ""shape:"" in first_line:\n            data = fin.read().splitlines(True)\n            with open(ir_dump_txt, \'w\') as fout:\n                fout.writelines(data)\n    ie = np.loadtxt(ir_dump_txt, dtype=np.float32)\n    pt = np.load(torch_dump_npy)\n    print(""Size, [ MIN, MAX ]"")\n    print_info = lambda np_array: print(\n        ""{} [{:.3f}, {:.3f}]"".format(np_array.size, np_array.min().item(), np_array.max().item()))\n    print_info(ie)\n    print_info(pt)\n    print(""Maximum of absolute difference: {:.7f}"".format(abs(ie - pt).max()))\n\n\ndef print_args(args):\n    for arg in sorted(vars(args)):\n        print(""{: <27s}: {}"".format(arg, getattr(args, arg)))\n'"
pytorch_toolkit/nncf/tools/debug/compare_accuracy.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\n\nimport os\nfrom functools import partial\nfrom openvino.inference_engine import IENetwork, IEPlugin, get_version\n\nfrom nncf.config import Config\nfrom nncf.dynamic_graph.graph_builder import create_input_infos\nfrom tools.ir_utils import get_ir_paths\n\n\ndef getExecNet(plugin, net):\n    return plugin.load(network=net)\n\n\nargparser = argparse.ArgumentParser()\nargparser.add_argument(""-m"", ""--model"", help=""input IR name"", required=True)\nargparser.add_argument(""--bin"", help=""Input *.bin file name"")\nargparser.add_argument(""-o"", ""--output-dir"", help=""Output directory to dump weights"", required=True)\nargparser.add_argument(""-c"", ""--config"", type=str, help=""Model\'s config"", required=True)\nargparser.add_argument(""--cuda"", help=""inference PyTorch model on CUDA"", action=\'store_true\')\nargparser.add_argument(\'--data\', metavar=\'DIR\', help=\'path to dataset\', required=True)\nargparser.add_argument(\'--cpu-plugin-dir\', metavar=\'DIR\',\n                       help=\'path to the directory with CPU Plugin and CPU Extension libraries\', required=True)\nargparser.add_argument(""-n"", ""--num-layers"", type=int, default=-1, help=""Dump activations for given number of layers"")\nargparser.add_argument(""--dump"", action=\'store_true\', help=""Enables dump of activations"")\n\nargs = argparser.parse_args()\n\n\ndef validate_torch_model(output_dir, config, num_layers, dump, val_loader=None, cuda=False):\n    from tools.debug.common import load_torch_model, register_print_hooks\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    model = load_torch_model(config, cuda)\n\n    model_e = model.eval()\n    if dump:\n        register_print_hooks(output_dir, model_e, num_layers=num_layers, data_to_compare=None, dump_activations=True)\n\n    validate_general(val_loader, model_e, infer_pytorch_model, cuda)\n\n\ndef main():\n    model_bin, model_xml = get_ir_paths(args.model, args.bin)\n\n    config = Config.from_json(args.config)\n\n    input_infos_list = create_input_infos(config)\n    image_size = input_infos_list[0].shape[-1]\n\n    size = int(image_size / 0.875)\n\n    print(\'IE version: {}\'.format(get_version()))\n\n    # NOTE: importing torch after loading IE to plugin to avoid issue with built-in MKLDNN of PyTorch\n    plugin = IEPlugin(device=\'CPU\',\n                      plugin_dirs=args.cpu_plugin_dir)\n    plugin.add_cpu_extension(os.path.join(args.cpu_plugin_dir, ""libcpu_extension.so""))\n    net = IENetwork(model=model_xml, weights=model_bin)\n    exec_net = getExecNet(plugin, net)\n    from torch.utils.data import DataLoader\n    import torchvision.datasets as datasets\n    import torchvision.transforms as transforms\n\n    val_loader = DataLoader(\n        datasets.ImageFolder(args.data, transforms.Compose([\n            transforms.Resize(size),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])),\n        batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    config[\'log_dir\'] = args.output_dir\n\n    infer_fn = partial(infer_ie_model, net=net)\n    validate_general(val_loader, exec_net, infer_fn)\n\n    validate_torch_model(os.path.join(args.output_dir, ""PTH""), config=config, num_layers=args.num_layers,\n                         dump=args.dump, val_loader=val_loader, cuda=args.cuda)\n\n\ndef infer_ie_model(exec_net, inputs, net):\n    input_cpu = inputs.numpy()\n    input_name = next(iter(net.inputs))\n    output_name = next(iter(net.outputs))\n    res = exec_net.infer(inputs={input_name: input_cpu})\n    output = res[output_name]\n    import torch\n    torch_output = torch.from_numpy(output)\n    return torch_output\n\n\ndef infer_pytorch_model(model, inputs):\n    return model(inputs)\n\n\ndef validate_general(val_loader, model, infer_model_fn, cuda=False):\n    from examples.classification.main import AverageMeter, accuracy\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    for i, (input_, target) in enumerate(val_loader):\n        # compute output\n        output = infer_model_fn(model, input_)\n\n        if cuda:\n            target = target.cuda(None, non_blocking=True)\n        # measure accuracy and record loss\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        top1.update(acc1, input_.size(0))\n        top5.update(acc5, input_.size(0))\n\n        if i % 10 == 0:\n            print(\'IE Test : [{0}/{1}]\\t\'\n                  \'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Acc@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(i, len(val_loader), top1=top1, top5=top5))\n\n    print(\' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\'\n          .format(top1=top1, top5=top5))\n    return top1.avg, top5.avg\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/nncf/tools/debug/compare_activations.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\n\nfrom tools.debug.common import compare_activations, print_args\n\nargparser = argparse.ArgumentParser()\nargparser.add_argument(""-i"", ""--ir-dump-txt"", help=""IE dump file in text format (ieb from mkldnn_graph.cpp)"",\n                       required=True)\nargparser.add_argument(""-p"", ""--torch-dump-npy"", help=""PyTorch dump file in npy format"", required=True)\nargs = argparser.parse_args()\nprint_args(args)\n\ndef main():\n    compare_activations(args.ir_dump_txt, args.torch_dump_npy)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/nncf/tools/debug/compare_dump.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport os\nfrom os.path import join, isdir, isfile\n\nimport torch\n\nfrom tools.debug.common import print_args\n\nargparser = argparse.ArgumentParser()\nargparser.add_argument(""-r"", ""--ref-dir"",\n                       help=""Path to ref folder. Treated whether ref experiment, or dir to compare GPU dumps"",\n                       required=True)\nargparser.add_argument(""-c"", ""--cmp-dirs"",\n                       help=""List of path to dirs with experiments for comparison with ref folder 0,1,2..."", nargs=\'+\')\nargparser.add_argument(""--range"", type=int, help=""Right border of range, starting from ref dir number"", )\nargparser.add_argument(""--eps"", help=""Torelance for maximum of absolute difference"", default=None, type=float)\n\nargs = argparser.parse_args()\nprint_args(args)\n\n\ndef basename(x):\n    return os.path.basename(x)\n\n\ndef get_dirs(root_path):\n    return [join(root_path, f) for f in sorted(os.listdir(root_path), key=basename) if isdir(join(root_path, f))]\n\n\ndef get_files(root_path):\n    return [f for f in sorted(os.listdir(root_path), key=basename) if isfile(join(root_path, f))]\n\n\ndef get_dir_pairs(dump_dir):\n    dirs = get_dirs(dump_dir)\n    return [(dirs[0], cmp_dir) for cmp_dir in dirs[1:]]\n\n\ndef compare_dump_in_dir_pairs(pairs, eps=None):\n    global_max_diff = 0\n    global_count_diff = 0\n    for ref_dir, cmp_dir in pairs:\n        max_diff = compare_dump_in_file_pairs(ref_dir, cmp_dir, eps)\n        if max_diff != 0:\n            global_count_diff += 1\n        if max_diff > global_max_diff:\n            global_max_diff = max_diff\n    print(\'\\n\\nGlobal MAX abs diff: {}\\n{}/{} is different\'.format(global_max_diff, global_count_diff, len(pairs)))\n\n\ndef compare_dump_in_file_pairs(ref_dir, cmp_dir, eps):\n    ref_files = get_files(ref_dir)\n    cmp_files = get_files(cmp_dir)\n    max_diff = 0\n    count_diff = 0\n    print(\'\\n\\nCompare {} vs {}\'.format(ref_dir, cmp_dir))\n    for rf in ref_files:\n        rt = torch.load(os.path.join(ref_dir, rf))\n        if rf in cmp_files:\n            ct = torch.load(os.path.join(cmp_dir, rf))\n            rn = os.path.basename(rf)\n            diff = abs(rt - ct).max()\n            if diff != 0:\n                count_diff += 1\n            if diff > max_diff:\n                max_diff = diff\n            if eps is not None:\n                if diff >= eps:\n                    if \'scale\' in rf:\n                        print(\'____{} vs {}_____, diff={} for {}\'.format(rt.item(), ct.item(), diff, rn))\n                    else:\n                        print(\'diff={} for {}\'.format(diff, rn))\n\n        else:\n            print(\'not matched file {}\'.format(rf))\n    print(\'Max abs diff: {}\\n{}/{} is different\'.format(max_diff, count_diff, len(ref_files)))\n    return max_diff\n\n\ndef main():\n    ref_dir = args.ref_dir\n    cmp_dirs = args.cmp_dirs\n    range_ = args.range\n    dir_pairs = []\n    if cmp_dirs:\n        for cmp_dir in cmp_dirs:\n            dir_pairs += list(zip(get_dirs(cmp_dir), get_dirs(ref_dir)))\n    elif range_:\n        lb = int(os.path.basename(ref_dir))\n        parent_dir = os.path.abspath(os.path.join(ref_dir, os.pardir))\n        for i in range(lb + 1, range_ + 1):\n            cmp_dir = os.path.join(parent_dir, str(i))\n            dir_pairs += list(zip(get_dirs(cmp_dir), get_dirs(ref_dir)))\n    else:\n        dir_pairs = get_dir_pairs(ref_dir)\n\n    compare_dump_in_dir_pairs(dir_pairs, args.eps)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/nncf/tools/debug/compare_weights.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport json\nimport xml.etree.cElementTree as ET\nfrom collections import OrderedDict\n\nimport os\n\nimport numpy as np\nfrom torch import randn\n\nfrom tools.ir_utils import get_ir_paths, find_all_parameters\nfrom tools.debug.common import save_dump, register_print_hooks, load_torch_model, get_full_dump_paths, print_args\n\n\nargparser = argparse.ArgumentParser()\nargparser.add_argument(""-m"", ""--model"", help=""input IR name"", required=True)\nargparser.add_argument(""--bin"", help=""Input *.bin file name"")\nargparser.add_argument(""-o"", ""--output-dir"", help=""Output directory to dump weights"", required=True)\nargparser.add_argument(""-c"", ""--config"", type=str, default=\'config.json\', help=""Model\'s config"", required=True)\nargparser.add_argument(""-n"", ""--num-layers"", type=int, default=-1,\n                       help=""Compare weights for given number of layers"")\nargparser.add_argument(""--ignore"", help=""comma separated list of ignored layers"", default="""")\nargs = argparser.parse_args()\nprint_args(args)\n\n\ndef main():\n    model_bin, model_xml = get_ir_paths(args.model, args.bin)\n\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    ir_weights = collect_IR_weights(os.path.join(args.output_dir, ""IR""), model_xml, model_bin, args.num_layers)\n\n    config = json.load(open(args.config))\n    torch_weights = collect_torch_weights(os.path.join(args.output_dir, ""PTH""), config, args.num_layers)\n\n    assert len(ir_weights) == len(torch_weights), \'{} vs {}\'.format(len(ir_weights), len(torch_weights))\n    print(""Maximum of absolute difference - IR vs Torch"")\n    max_max = []\n    for (k1, v1), (k2, v2) in zip(ir_weights.items(), torch_weights.items()):\n        max_diff = abs(v1 - v2).max()\n        max_max.append(max_diff)\n        print(""{0:.5} - max diff [{1:}]  vs  [{2:}]"".format(max_diff, k1, k2))\n    print(""Global maximum:  {0:.5}"".format(np.max(max_max)))\n\n\ndef collect_IR_weights(output_dir, model_xml, model_bin, num_layers):\n    data_to_compare = OrderedDict()\n    print(""IR loaded from {}"".format(model_bin))\n    with open(model_bin, ""rb"") as f:\n        buffer = f.read()\n\n    ignored = args.ignore.split("","") + get_ignored_layers(model_xml, args.num_layers)\n\n    all_parameters = find_all_parameters(buffer, model_xml)\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    idx = 0\n\n    for name, param in all_parameters.items():\n        if name.split(\'.\')[0] in ignored or \'bias\' in name:\n            continue\n        if (num_layers > 0 and idx < num_layers) or (num_layers == -1):\n            name = name.replace(os.path.sep, \'_\')\n            dump_name = \'.\'.join([str(idx), name])\n            output_data = param.data.flatten()\n            save_dump(dump_name, output_dir, output_data)\n            data_to_compare[dump_name] = output_data\n            idx += 1\n    return data_to_compare\n\n\ndef collect_torch_weights(output_dir, config, num_layers):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    model = load_torch_model(config)\n    model_e = model.eval()\n\n    data_to_compare = OrderedDict()\n\n    register_print_hooks(output_dir, model_e, num_layers=num_layers, data_to_compare=data_to_compare,\n                         dump_activations=False)\n    input_ = randn(config[\'input_sample_size\'])\n    model_e(input_)\n\n    for _, module in enumerate(model_e.modules()):\n        paths = get_full_dump_paths(module)\n        if paths is not None:\n            for dump_path in paths:\n                if os.path.isfile(dump_path):\n                    data_to_compare[os.path.splitext(os.path.basename(dump_path))[0]] = np.load(dump_path)\n    return data_to_compare\n\n\ndef get_ignored_layers(model_xml, num_layers=1):\n    ir_tree = ET.parse(model_xml)\n    ignored_layers = []\n    all_supported = [l for l in ir_tree.iter(""layer"") if l.get(""type"") == (""Convolution"", ""FullyConnected"")]\n    if num_layers > 0:\n        ignored_layers += [layer.get(""name"") for layer in all_supported[num_layers:]]\n    all_bns = [l for l in ir_tree.iter(""layer"") if l.get(""type"") == ""ScaleShift""]\n    ignored_layers += [bn.get(""name"") for bn in all_bns]\n    return ignored_layers\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/nncf/tools/debug/debug_sample.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport re\nfrom pathlib import Path\n\nimport torch\n\n\ndef create_experiment_dir(dump_dir):\n    os.makedirs(dump_dir, exist_ok=True)\n    next_id = 0\n    if not re.match(r\'\\d+\', Path(dump_dir).parts[-1]):\n        ids = [int(f) for f in os.listdir(dump_dir) if f.isnumeric()]\n        if ids:\n            next_id = max(ids) + 1\n    dump_path = os.path.join(dump_dir, str(next_id))\n    os.makedirs(dump_path)\n    return dump_path\n\n\ndef register_dump_hooks(model, dump_dir, num_item_to_dump=10):\n    os.makedirs(dump_dir, exist_ok=True)\n    next_id = 0\n    if not re.match(r\'\\d+/\\d+\', Path(dump_dir).parts[-1]):\n        ids = [int(f) for f in os.listdir(dump_dir) if f.isnumeric()]\n        if ids:\n            next_id = max(ids) + 1\n    print(next_id)\n    dump_path = os.path.join(dump_dir, str(next_id))\n    os.makedirs(dump_path)\n    handles = []\n    name_idx = 0\n    for name, module in model.named_modules():\n        if name:\n            module.full_dump_path = \\\n                os.path.join(dump_path, str(""{:04d}"".format(name_idx)) + \'_\' + name.replace(\'/\', \'_\'))\n            name_idx += 1\n\n            def hook(module, inputs, output):\n                idx = 0\n                for input_ in inputs:\n                    path = module.full_dump_path + \'_input_{}\'.format(str(idx))\n                    # print(\'saving input to {}\'.format(path))\n                    data = torch.flatten(input_)[:num_item_to_dump].cpu()\n                    torch.save(data, path)\n                    idx += 1\n                path = module.full_dump_path + \'_output\'\n                # print(\'saving output to {}\'.format(path))\n                data = torch.flatten(output)[:num_item_to_dump].cpu()\n                torch.save(data, path)\n                if hasattr(module, \'weight\'):\n                    data = torch.flatten(module.weight)[:num_item_to_dump].cpu()\n                    path = module.full_dump_path + \'_weight\'\n                    print(\'saving weight to {}\'.format(path))\n                    torch.save(data, path)\n                if hasattr(module, \'scale\'):\n                    data = module.scale.cpu()\n                    path = module.full_dump_path + \'_scale\'\n                    print(\'saving scales to {}\'.format(path))\n                    torch.save(data, path)\n\n            handles.append(module.register_forward_hook(hook))\n    return handles\n'"
pytorch_toolkit/nncf/tools/magnitude_sparsity/sparsify.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport argparse\nimport xml.etree.cElementTree as ET\n\nimport numpy as np\n\nfrom tools.ir_utils import find_all_parameters, get_ir_paths\n\nargparser = argparse.ArgumentParser()\nargparser.add_argument(""-m"", ""--model"", help=""input IR name"", required=True)\nargparser.add_argument(""-b"", ""--bin"", help=""Input *.bin file name"")\nargparser.add_argument(""-o"", ""--output"", help=""Output *.bin file name"")\nargparser.add_argument(""-s"", ""--sparsity-level"", help=""Desired number of zero parameters"")\nargparser.add_argument(""--ignore"", help=""comma separated list of ignored layers"", default="""")\nargparser.add_argument(""--sparsify-first-conv"", type=bool, default=False)\nargparser.add_argument(""--sparsify-fc"", type=bool, default=True)\nargparser.add_argument(""--normed-threshold"", type=bool, default=True)\nargs = argparser.parse_args()\n\n\ndef main():\n    model_bin, model_xml = get_ir_paths(args.model, args.bin)\n\n    with open(model_bin, ""rb"") as f:\n        buffer = f.read()\n\n    ignored = args.ignore.split("","") + get_ignored_layers(model_xml, args.sparsify_first_conv, args.sparsify_fc)\n\n    all_parameters = find_all_parameters(buffer, model_xml)\n    total_params = 0\n    zero_params = 0\n    for name, param in all_parameters.items():\n        if name.split(\'.\')[0] in ignored:\n            continue\n        total_params += param.data.size\n        zero_params += (param.data == 0).sum()\n\n    print(""initial sparsity: {:.2f}%"".format((zero_params / total_params) * 100))\n\n    if args.sparsity_level:\n        sparsify(all_parameters, ignored, args.normed_threshold)\n\n    if args.output:\n        print(""saving new ir to: {}"".format(args.output))\n\n        with open(args.output, ""wb"") as f:\n            f.write(generate_new_bin(buffer, all_parameters))\n\n\ndef get_ignored_layers(model_xml, sparsify_first_conv=False, sparsify_fc=True):\n    ir_tree = ET.parse(model_xml)\n    ignored_layers = []\n    all_convs = [l for l in ir_tree.iter(""layer"") if l.get(""type"") == ""Convolution""]\n    all_fcs = [l for l in ir_tree.iter(""layer"") if l.get(""type"") == ""FullyConnected""]\n    all_bns = [l for l in ir_tree.iter(""layer"") if l.get(""type"") == ""ScaleShift""]\n    ignored_layers += [bn.get(""name"") for bn in all_bns]\n    if not sparsify_first_conv:\n        ignored_layers.append(all_convs[0].get(""name""))\n    if not sparsify_fc:\n        ignored_layers.append(all_fcs[-1].get(""name""))\n    return ignored_layers\n\n\ndef generate_new_bin(buffer, parameters):\n    new_buffer = bytearray(buffer)\n    for param in parameters.values():\n        new_buffer[param.offset:param.offset + param.size] = param.data.tobytes()\n    return bytes(new_buffer)\n\n\ndef norm(data):\n    return np.sqrt(np.sum(data ** 2))\n\n\ndef sparsify(parameters, ignored, normalize=True):\n    if normalize:\n        data_flat = [p.data.flatten() / norm(p.data) for k, p in parameters.items() if k.split(""."")[0] not in ignored]\n    else:\n        data_flat = [p.data.flatten() for k, p in parameters.items() if k.split(""."")[0] not in ignored]\n\n    data_flat = np.concatenate(data_flat)\n    data_flat = np.absolute(data_flat)\n    data_flat.sort()\n    sparsity_level = float(args.sparsity_level) / 100\n    sparsity_threshold = data_flat[int(sparsity_level * len(data_flat))]\n    for name, param in parameters.items():\n        if name.split(""."")[0] in ignored:\n            continue\n\n        if normalize:\n            param.data[np.absolute(param.data / norm(param.data)) < sparsity_threshold] = 0\n        else:\n            param.data[np.absolute(param.data) < sparsity_threshold] = 0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/face-detection/face-detection-0100/config.py,0,"b""# model settings\ninput_size = 256\nwidth_mult = 1.0\nmodel = dict(\n    type='SingleStageDetector',\n    backbone=dict(\n        type='mobilenetv2_w1',\n        out_indices=(4, 5),\n        frozen_stages=-1,\n        norm_eval=False,\n        pretrained=True,\n    ),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(int(width_mult * 96), int(width_mult * 320)),\n        num_classes=2,\n        anchor_strides=(16, 32),\n        anchor_widths=(\n            [8.0213, 21.4187, 12.544, 29.6107],\n            [122.0267, 66.048, 109.9093, 43.6053, 64.512]),\n        anchor_heights=(\n            [12.8, 33.792, 21.76, 53.9307],\n            [194.1333, 139.008, 106.24, 89.6853, 61.952]),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2),\n        depthwise_heads=True,\n        depthwise_heads_activations='relu',\n        loss_balancing=True))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.4,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    use_giou=False,\n    use_focal=False,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'CustomCocoDataset'\ndata_root = 'data/WIDERFace/'\nimg_norm_cfg = dict(mean=[0, 0, 0], std=[255, 255, 255], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.1),\n    dict(type='Resize', img_scale=(input_size, input_size), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(input_size, input_size),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    imgs_per_gpu=65,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',\n        times=2,\n        dataset=dict(\n            type=dataset_type,\n            classes=('face',),\n            ann_file=data_root + '/train.json',\n            min_size=17,\n            img_prefix=data_root,\n            pipeline=train_pipeline\n        )\n    ),\n    val=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1200,\n    warmup_ratio=1.0 / 3,\n    step=[40, 55, 65])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=100,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 70\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = 'outputs/face-detection-0100'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
pytorch_toolkit/object_detection/face-detection/face-detection-0102/config.py,0,"b""# model settings\ninput_size = 384\nwidth_mult = 1.0\nmodel = dict(\n    type='SingleStageDetector',\n    backbone=dict(\n        type='mobilenetv2_w1',\n        out_indices=(4, 5),\n        frozen_stages=-1,\n        norm_eval=False,\n        pretrained=True,\n    ),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(int(width_mult * 96), int(width_mult * 320)),\n        num_classes=2,\n        anchor_strides=(16, 32),\n        anchor_widths=([9.4 * 1.28, 25.1 * 1.28, 14.7 * 1.28, 34.7 * 1.28],\n                       [143.0 * 1.28, 77.4 * 1.28, 128.8 * 1.28, 51.1 * 1.28, 75.6 * 1.28]),\n        anchor_heights=([15.0 * 1.28, 39.6 * 1.28, 25.5 * 1.28, 63.2 * 1.28],\n                        [227.5 * 1.28, 162.9 * 1.28, 124.5 * 1.28, 105.1 * 1.28, 72.6 * 1.28]),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2),\n        depthwise_heads=True,\n        depthwise_heads_activations='relu',\n        loss_balancing=True))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.4,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    use_giou=False,\n    use_focal=False,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'CustomCocoDataset'\ndata_root = 'data/WIDERFace/'\nimg_norm_cfg = dict(mean=[0, 0, 0], std=[255, 255, 255], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.1),\n    dict(type='Resize', img_scale=(input_size, input_size), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(input_size, input_size),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    imgs_per_gpu=65,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',\n        times=2,\n        dataset=dict(\n            type=dataset_type,\n            classes=('face',),\n            ann_file=data_root + '/train.json',\n            min_size=17,\n            img_prefix=data_root,\n            pipeline=train_pipeline\n        )\n    ),\n    val=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1200,\n    warmup_ratio=1.0 / 3,\n    step=[40, 55, 65])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=100,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 70\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = 'outputs/face-detection-0102'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
pytorch_toolkit/object_detection/face-detection/face-detection-0104/config.py,0,"b""# model settings\ninput_size = 448\nwidth_mult = 1.0\nmodel = dict(\n    type='SingleStageDetector',\n    backbone=dict(\n        type='mobilenetv2_w1',\n        out_indices=(4, 5),\n        frozen_stages=-1,\n        norm_eval=False,\n        pretrained=True,\n    ),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(int(width_mult * 96), int(width_mult * 320)),\n        num_classes=2,\n        anchor_strides=(16, 32),\n        anchor_widths=(\n            [14.0373, 37.4827, 21.952, 51.8187],\n            [213.5467, 115.584, 192.3413, 76.3093, 112.896]),\n        anchor_heights=(\n            [22.4, 59.136, 38.08, 94.3787],\n            [339.7333, 243.264, 185.92, 156.9493, 108.416]),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2),\n        depthwise_heads=True,\n        depthwise_heads_activations='relu',\n        loss_balancing=True))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.4,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    use_giou=False,\n    use_focal=False,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'CustomCocoDataset'\ndata_root = 'data/WIDERFace/'\nimg_norm_cfg = dict(mean=[0, 0, 0], std=[255, 255, 255], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.1),\n    dict(type='Resize', img_scale=(input_size, input_size), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(input_size, input_size),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    imgs_per_gpu=32,\n    workers_per_gpu=4,\n    train=dict(\n        type='RepeatDataset',\n        times=2,\n        dataset=dict(\n            type=dataset_type,\n            classes=('face',),\n            ann_file=data_root + '/train.json',\n            min_size=17,\n            img_prefix=data_root,\n            pipeline=train_pipeline\n        )\n    ),\n    val=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1200,\n    warmup_ratio=1.0 / 3,\n    step=[40, 55, 65])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=100,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 70\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = 'outputs/face-detection-0104'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
pytorch_toolkit/object_detection/face-detection/face-detection-0105/config.py,0,"b""# model settings\ninput_size = 416\nwidth_mult = 1.0\nmodel = dict(\n    type='FCOS',\n    backbone=dict(\n        type='mobilenetv2_w1',\n        out_indices=(3, 4, 5),\n        frozen_stages=-1,\n        norm_eval=False,\n        pretrained=True,\n    ),\n    neck=dict(\n        type='FPN',\n        in_channels=[int(width_mult * 32), int(width_mult * 96), int(width_mult * 320)],\n        out_channels=48,\n        start_level=0,\n        add_extra_convs=True,\n        extra_convs_on_inputs=False,  # use P5\n        num_outs=5,\n        relu_before_extra_convs=True),\n    bbox_head=dict(\n        type='FCOSHead',\n        num_classes=2,\n        in_channels=48,\n        stacked_convs=4,\n        feat_channels=32,\n        strides=(8, 16, 32, 64, 128),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n# training and testing settings\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.4,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    use_giou=False,\n    use_focal=False,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'CustomCocoDataset'\ndata_root = 'data/WIDERFace/'\nimg_norm_cfg = dict(mean=[0, 0, 0], std=[255, 255, 255], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.1),\n    dict(type='Resize', img_scale=(input_size, input_size), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(input_size, input_size),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    imgs_per_gpu=32,\n    workers_per_gpu=4,\n    train=dict(\n        type='RepeatDataset',\n        times=2,\n        dataset=dict(\n            type=dataset_type,\n            classes=('face',),\n            ann_file=data_root + '/train.json',\n            min_size=10,\n            img_prefix=data_root,\n            pipeline=train_pipeline\n        )\n    ),\n    val=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1200,\n    warmup_ratio=1.0 / 3,\n    step=[40, 55, 65])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=100,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 70\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = 'outputs/face-detection-0105'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
pytorch_toolkit/object_detection/face-detection/face-detection-0106/config.py,0,"b""# model settings\ninput_size = 640\nwidth_mult = 1.0\nmodel = dict(\n    type='ATSS',\n    backbone=dict(\n        type='resnet152b',\n        out_indices=(1, 2, 3, 4),\n        frozen_stages=-1,\n        norm_eval=False,\n        pretrained=True,\n    ),\n    neck=dict(\n        type='RSSH_FPN',\n        in_channels=[int(width_mult * 256),\n                     int(width_mult * 512),\n                     int(width_mult * 1024),\n                     int(width_mult * 2048),\n                     ],\n        out_channels=256,\n        start_level=0,\n        add_extra_convs=True,\n        extra_convs_on_inputs=False,  # use P5\n        num_outs=5,\n        relu_before_extra_convs=True),\n    bbox_head=dict(\n        type='ATSSHead',\n        num_classes=2,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=128,\n        octave_base_scale=8,\n        scales_per_octave=1,\n        anchor_ratios=[1.0],\n        anchor_strides=(4, 8, 16, 32, 64),\n        target_means=[.0, .0, .0, .0],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n# training and testing settings\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(type='ATSSAssigner', topk=9),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.5),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=750)\n# model training and testing settings\n# dataset settings\ndataset_type = 'CustomCocoDataset'\ndata_root = 'data/WIDERFace/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.1),\n    dict(type='Resize', img_scale=(input_size, input_size), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(input_size, input_size),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    imgs_per_gpu=5,\n    workers_per_gpu=3,\n    train=dict(\n        type='RepeatDataset',\n        times=2,\n        dataset=dict(\n            type=dataset_type,\n            classes=('face',),\n            ann_file=data_root + '/train.json',\n            min_size=0,\n            img_prefix=data_root,\n            pipeline=train_pipeline\n        )\n    ),\n    val=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        classes=('face',),\n        ann_file=data_root + '/val.json',\n        img_prefix=data_root,\n        test_mode=True,\n        pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1200,\n    warmup_ratio=1.0 / 3,\n    step=[40, 55, 65])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=100,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 70\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = 'outputs/face-detection-0106'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
pytorch_toolkit/object_detection/face-detection/tools/box_overlaps.py,0,"b'# https://github.com/wondervictor/WiderFace-Evaluation/blob/master/box_overlaps.pyx\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Sergey Karayev\n# --------------------------------------------------------\n\n"""""" Module for computing intersection over union. """"""\n\nimport numpy as np\n\n\ndef bbox_overlaps(boxes, query_boxes):\n    """"""\n    Parameters\n    ----------\n    boxes: (N, 4) ndarray of float\n    query_boxes: (K, 4) ndarray of float\n    Returns\n    -------\n    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n    """"""\n    boxes_num = boxes.shape[0]\n    queries_num = query_boxes.shape[0]\n    overlaps = np.zeros((boxes_num, queries_num), dtype=np.float)\n    for k in range(queries_num):\n        box_area = (\n            (query_boxes[k, 2] - query_boxes[k, 0] + 1) *\n            (query_boxes[k, 3] - query_boxes[k, 1] + 1)\n        )\n        for box_id in range(boxes_num):\n            int_width = (\n                min(boxes[box_id, 2], query_boxes[k, 2]) -\n                max(boxes[box_id, 0], query_boxes[k, 0]) + 1\n            )\n            if int_width > 0:\n                int_height = (\n                    min(boxes[box_id, 3], query_boxes[k, 3]) -\n                    max(boxes[box_id, 1], query_boxes[k, 1]) + 1\n                )\n                if int_height > 0:\n                    union_area = float(\n                        (boxes[box_id, 2] - boxes[box_id, 0] + 1) *\n                        (boxes[box_id, 3] - boxes[box_id, 1] + 1) +\n                        box_area - int_width * int_height\n                    )\n                    overlaps[box_id, k] = int_width * int_height / union_area\n    return overlaps\n'"
pytorch_toolkit/object_detection/face-detection/tools/eval.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n# pylint: disable=C0301,W0622,R0914\n\nimport argparse\nimport hashlib\nimport json\nimport os\nimport subprocess\nimport tempfile\nimport yaml\n\nfrom mmcv.utils import Config\n\nMMDETECTION_TOOLS = f\'{os.path.dirname(__file__)}/../../../../external/mmdetection/tools\'\nFACE_DETECTION_TOOLS = os.path.dirname(__file__)\n\n\ndef parse_args():\n    """""" Parses input args. """"""\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'config\',\n                      help=\'A path to model training configuration file (.py).\')\n    args.add_argument(\'snapshot\',\n                      help=\'A path to pre-trained snapshot (.pth).\')\n    args.add_argument(\'out\',\n                      help=\'A path to output file where models metrics will be saved (.yml).\')\n    args.add_argument(\'--wider_dir\',\n                      help=\'Specify this  path if you would like to test your model on WiderFace dataset.\')\n\n    return args.parse_args()\n\n\ndef replace_text_in_file(path, replace_what, replace_by):\n    """""" Replaces text in file. """"""\n\n    with open(path) as read_file:\n        content = \'\\n\'.join([line.rstrip() for line in read_file.readlines()])\n        if content.find(replace_what) == -1:\n            return False\n        content = content.replace(replace_what, replace_by)\n    with open(path, \'w\') as write_file:\n        write_file.write(content)\n    return True\n\n\ndef collect_ap(path):\n    """""" Collects average precision values in log file. """"""\n\n    average_precisions = []\n    beginning = \'Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = \'\n    with open(path) as read_file:\n        content = [line.strip() for line in read_file.readlines()]\n        for line in content:\n            if line.startswith(beginning):\n                average_precisions.append(float(line.replace(beginning, \'\')))\n    return average_precisions\n\n\ndef sha256sum(filename):\n    """""" Computes sha256sum. """"""\n\n    h = hashlib.sha256()\n    b = bytearray(128*1024)\n    mv = memoryview(b)\n    with open(filename, \'rb\', buffering=0) as f:\n        for n in iter(lambda: f.readinto(mv), 0):\n            h.update(mv[:n])\n    return h.hexdigest()\n\n\ndef compute_wider_metrics(config_path, snapshot, work_dir, wider_dir, outputs):\n    """""" Computes WiderFace metrics on easy, medium, hard subsets. """"""\n\n    wider_data_folder = wider_dir\n    os.makedirs(wider_data_folder, exist_ok=True)\n\n    wider_data_zip = os.path.join(wider_data_folder, \'WIDER_val.zip\')\n    assert os.path.exists(wider_data_zip), f\'failed to find WIDER_val.zip here: {wider_data_zip}\'\n    subprocess.run(f\'unzip -q -o {wider_data_zip} -d {wider_data_folder}\'.split(\' \'), check=True)\n\n    eval_tools_zip = os.path.join(wider_data_folder, \'eval_tools.zip\')\n    if not os.path.exists(eval_tools_zip):\n        subprocess.run(\n            f\'wget http://shuoyang1213.me/WIDERFACE/support/eval_script/eval_tools.zip\'\n            f\' -O {eval_tools_zip}\'.split(\' \'), check=True)\n    subprocess.run(f\'unzip -q -o {eval_tools_zip} -d {wider_data_folder}\'.split(\' \'), check=True)\n\n    wider_annotation_zip = os.path.join(wider_data_folder, \'ider_face_split.zip\')\n    if not os.path.exists(wider_annotation_zip):\n        subprocess.run(\n            f\'wget http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/bbx_annotation/wider_face_split.zip\'\n            f\' -O {wider_annotation_zip}\'.split(\' \'), check=True)\n    subprocess.run(f\'unzip -q -o {wider_annotation_zip} -d {wider_data_folder}\'.split(\' \'), check=True)\n\n    wider_annotation = os.path.join(wider_dir, \'wider_face_split\', \'wider_face_val_bbx_gt.txt\')\n    wider_images = os.path.join(wider_dir, \'WIDER_val\', \'images\')\n    wider_coco_annotation = os.path.join(wider_dir, \'instances_val.json\')\n    subprocess.run(\n        f\'python {FACE_DETECTION_TOOLS}/wider_to_coco.py\'\n        f\' {wider_annotation} {wider_images} {wider_coco_annotation}\'.split(\' \'), check=True)\n\n    res_pkl = os.path.join(work_dir, \'wider_face_res.pkl\')\n\n    with open(os.path.join(work_dir, \'test_py_on_wider_stdout_\'), \'w\') as test_py_stdout:\n        subprocess.run(\n            f\'python {MMDETECTION_TOOLS}/test.py\'\n            f\' {config_path} {snapshot}\'\n            f\' --out {res_pkl}\'\n            f\' --update_config data.test.ann_file={wider_coco_annotation} data.test.img_prefix={wider_dir}\'.split(\' \'),\n            stdout=test_py_stdout, check=True)\n\n    wider_face_predictions = tempfile.mkdtemp()\n    subprocess.run(\n        f\'python {FACE_DETECTION_TOOLS}/test_out_to_wider_predictions.py\'\n        f\' {config_path} {res_pkl} {wider_face_predictions}\'.split(\' \'), check=True)\n    print(wider_face_predictions)\n    res_wider_metrics = os.path.join(work_dir, ""wider_metrics.json"")\n    subprocess.run(\n        f\'python {FACE_DETECTION_TOOLS}/wider_face_eval.py\'\n        f\' -g {wider_data_folder}/eval_tools/ground_truth/\'\n        f\' -p {wider_face_predictions}\'\n        f\' --out {res_wider_metrics}\'.split(\' \'), check=True)\n    with open(res_wider_metrics) as read_file:\n        content = json.load(read_file)\n        outputs.extend(content)\n    return outputs\n\n\ndef coco_ap_eval(config_path, work_dir, snapshot, res_pkl, outputs):\n    """""" Computes COCO AP. """"""\n\n    with open(os.path.join(work_dir, \'test_py_stdout\'), \'w\') as test_py_stdout:\n        subprocess.run(\n            f\'python {MMDETECTION_TOOLS}/test.py\'\n            f\' {config_path} {snapshot}\'\n            f\' --out {res_pkl} --eval bbox\'.split(\' \'), stdout=test_py_stdout, check=True)\n    average_precision = collect_ap(os.path.join(work_dir, \'test_py_stdout\'))[0]\n    outputs.append({\'key\': \'ap\', \'value\': average_precision * 100, \'unit\': \'%\', \'display_name\': \'AP @ [IoU=0.50:0.95]\'})\n    return outputs\n\n\ndef custom_ap_eval(config_path, work_dir, res_pkl, outputs):\n    """""" Computes AP on faces that are greater than 64x64. """"""\n\n    res_custom_metrics = os.path.join(work_dir, ""custom_metrics.json"")\n    subprocess.run(\n        f\'python {FACE_DETECTION_TOOLS}/wider_custom_eval.py\'\n        f\' {config_path} {res_pkl} --out {res_custom_metrics}\'.split(\' \'), check=True)\n    with open(res_custom_metrics) as read_file:\n        ap_64x64 = [x[\'average_precision\'] for x in json.load(read_file) if x[\'object_size\'][0] == 64][0]\n        outputs.append({\'key\': \'ap_64x64\', \'value\': ap_64x64, \'display_name\': \'AP for faces > 64x64\', \'unit\': \'%\'})\n    return outputs\n\n\ndef get_complexity_and_size(cfg, config_path, work_dir, outputs):\n    """""" Gets complexity and size of a model. """"""\n\n    image_shape = [x[\'img_scale\'] for x in cfg.test_pipeline if \'img_scale\' in x][0][::-1]\n    image_shape = "" "".join([str(x) for x in image_shape])\n\n    res_complexity = os.path.join(work_dir, ""complexity.json"")\n\n    subprocess.run(\n        f\'python {MMDETECTION_TOOLS}/get_flops.py\'\n        f\' {config_path}\'\n        f\' --shape {image_shape}\'\n        f\' --out {res_complexity}\'.split(\' \'), check=True)\n    with open(res_complexity) as read_file:\n        content = json.load(read_file)\n        outputs.extend(content)\n    return outputs\n\n\ndef get_file_size_and_sha256(snapshot):\n    """""" Gets size and sha256 of a file. """"""\n\n    return {\n        \'sha256\': sha256sum(snapshot),\n        \'size\': os.path.getsize(snapshot),\n        \'name\': os.path.basename(snapshot),\n        \'source\': snapshot\n    }\n\n\ndef eval(config_path, snapshot, wider_dir, out):\n    """""" Main evaluation procedure. """"""\n\n    cfg = Config.fromfile(config_path)\n\n    work_dir = tempfile.mkdtemp()\n    print(\'results are stored in:\', work_dir)\n\n    if os.path.islink(snapshot):\n        snapshot = os.path.join(os.path.dirname(snapshot), os.readlink(snapshot))\n\n    files = get_file_size_and_sha256(snapshot)\n\n    metrics = []\n    res_pkl = os.path.join(work_dir, ""res.pkl"")\n    metrics = coco_ap_eval(config_path, work_dir, snapshot, res_pkl, metrics)\n    metrics = custom_ap_eval(config_path, work_dir, res_pkl, metrics)\n\n    if wider_dir:\n        metrics = compute_wider_metrics(config_path, snapshot, work_dir, wider_dir, metrics)\n\n    metrics = get_complexity_and_size(cfg, config_path, work_dir, metrics)\n\n    for metric in metrics:\n        metric[\'value\'] = round(metric[\'value\'], 3)\n\n    outputs = {\n        \'files\': [files],\n        \'metrics\': metrics\n    }\n\n    if os.path.exists(out):\n        with open(out) as read_file:\n            content = yaml.load(read_file)\n        content.update(outputs)\n        outputs = content\n\n    with open(out, \'w\') as write_file:\n        yaml.dump(outputs, write_file)\n\n\ndef main():\n    """""" Main function. """"""\n\n    args = parse_args()\n    eval(args.config, args.snapshot, args.wider_dir, args.out)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/face-detection/tools/test_out_to_wider_predictions.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This script converts output of test.py (mmdetection) to a set of files\nthat can be passed to official WiderFace evaluation procedure.""""""\n\nimport argparse\nimport os\nfrom tqdm import tqdm\n\nimport mmcv\n\nfrom mmdet.datasets import build_dataset\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    parser = argparse.ArgumentParser(\n        description=\'This script converts output of test.py (mmdetection) to \'\n                    \'a set of files that can be passed to official WiderFace \'\n                    \'evaluation procedure.\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'input\', help=\'output result file from test.py\')\n    parser.add_argument(\'out_folder\', help=\'folder where to store WiderFace \'\n                                           \'evaluation-friendly output\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    """""" Main function. """"""\n\n    args = parse_args()\n\n    if args.input is not None and not args.input.endswith((\'.pkl\', \'.pickle\')):\n        raise ValueError(\'The input file must be a pkl file.\')\n\n    cfg = mmcv.Config.fromfile(args.config)\n    dataset = build_dataset(cfg.data.test)\n\n    results = mmcv.load(args.input)\n\n    wider_friendly_results = []\n    for i, sample in enumerate(tqdm(dataset)):\n        filename = sample[\'img_meta\'][0].data[\'filename\']\n        folder, image_name = filename.split(\'/\')[-2:]\n        wider_friendly_results.append({\'folder\': folder, \'name\': image_name[:-4],\n                                       \'boxes\': results[i][0]})\n\n    for result in wider_friendly_results:\n        folder = os.path.join(args.out_folder, result[\'folder\'])\n        os.makedirs(folder, exist_ok=True)\n        with open(os.path.join(folder, result[\'name\'] + \'.txt\'), \'w\') as write_file:\n            write_file.write(result[\'name\'] + \'\\n\')\n            write_file.write(str(len(result[\'boxes\'])) + \'\\n\')\n            for box in result[\'boxes\']:\n                box = box[0], box[1], box[2] - box[0], box[3] - box[1], box[4]\n                write_file.write(\' \'.join([str(x) for x in box]) + \'\\n\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/face-detection/tools/train_and_eval.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n# pylint: disable=C0301,W0622,R0914\n\nimport argparse\nimport subprocess\nimport os\n\nfrom mmcv.utils import Config\n\nfrom eval import eval\n\n\ndef parse_args():\n    """""" Parses input args. """"""\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\'config\',\n                      help=\'A path to model training configuration file (.py).\')\n    args.add_argument(\'gpu_num\',\n                      help=\'A number of GPU to use in training.\')\n    args.add_argument(\'out\',\n                      help=\'A path to output file where models metrics will be saved (.yml).\')\n    args.add_argument(\'--wider_dir\',\n                      help=\'Specify this  path if you would like to test your model on WiderFace dataset.\')\n\n    return args.parse_args()\n\n\ndef main():\n    """""" Main function. """"""\n\n    args = parse_args()\n\n    if args.wider_dir:\n        wider_val_zip = os.path.join(args.wider_dir, \'WIDER_val.zip\')\n        assert os.path.exists(wider_val_zip), f\'failed to find WIDER_val.zip here: {wider_val_zip}\'\n\n    mmdetection_tools = f\'{os.path.dirname(__file__)}/../../../../external/mmdetection/tools\'\n\n    subprocess.run(f\'{mmdetection_tools}/dist_train.sh\'\n                   f\' {args.config}\'\n                   f\' {args.gpu_num}\'.split(\' \'), check=True)\n\n    cfg = Config.fromfile(args.config)\n\n    eval(args.config, os.path.join(cfg.work_dir, ""latest.pth""), args.wider_dir, args.out)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/face-detection/tools/wider_custom_eval.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This script computes AveragePrecision (VOC) for faces in specific size ranges. """"""\n\n# pylint: disable=R0912,R0913,R0914,R0915,C0301,W0622,R0914,I1101\n\nfrom argparse import ArgumentParser\nfrom bisect import bisect\nfrom collections import namedtuple\nimport json\nimport numpy as np\nfrom tqdm import tqdm\n\nimport cv2\nimport mmcv\nfrom mmdet import datasets\n\n\ndef replace_text_in_file(path, replace_what, replace_by):\n    with open(path) as read_file:\n        content = \'\\n\'.join([line.rstrip() for line in read_file.readlines()])\n        if content.find(replace_what) == -1:\n            return False\n        content = content.replace(replace_what, replace_by)\n    with open(path, \'w\') as write_file:\n        write_file.write(content)\n    return True\n\n\ndef voc_ap(recall, precision, use_07_metric=False):\n    """""" average_precision = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        average_precision = 0.0\n        for threshold in np.arange(0., 1.1, 0.1):\n            if np.sum(recall >= threshold) == 0:\n                precision_at_threshold = 0\n            else:\n                precision_at_threshold = np.max(precision[recall >= threshold])\n            average_precision += precision_at_threshold / 11.\n    else:\n        # Correct AP calculation.\n        # First append sentinel values at the end.\n        mrec = np.concatenate(([0.], recall, [1.]))\n        mpre = np.concatenate(([0.], precision, [0.]))\n\n        # Compute the precision envelope.\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # To calculate area under PR curve, look for points\n        # where X axis (recall) changes value.\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # And sum (\\Delta recall) * prec.\n        average_precision = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return average_precision\n\n\ndef compute_miss_rate(miss_rates, fppis, fppi_level=0.1):\n    """""" Compute miss rate at fppi level. """"""\n\n    position = bisect(fppis, fppi_level)\n    position1 = position - 1\n    position2 = position if position < len(miss_rates) else position1\n    return 0.5 * (miss_rates[position1] + miss_rates[position2])\n\n\ndef evaluate_detections(ground_truth, predictions, class_name, overlap_threshold=0.5,\n                        allow_multiple_matches_per_ignored=True,\n                        verbose=True):\n    """""" Compute set of object detection quality metrics. """"""\n\n    Detection = namedtuple(\'Detection\', [\'image\', \'bbox\', \'score\', \'gt_match\'])\n    GT = namedtuple(\'GroundTruth\', [\'bbox\', \'is_matched\', \'is_ignored\'])\n    detections = [Detection(image=img_pred.image_path,\n                            bbox=np.array(obj_pred[""bbox""]),\n                            score=obj_pred.get(""score"", 0.0),\n                            gt_match=-1)\n                  for img_pred in predictions\n                  for obj_pred in img_pred\n                  if obj_pred[""type""] == class_name]\n\n    scores = np.array([detection.score for detection in detections])\n    sorted_ind = np.argsort(-scores)\n    detections = [detections[i] for i in sorted_ind]\n\n    gts = {}\n    for img_gt in ground_truth:\n        gts[img_gt.image_path] = GT(\n            bbox=np.vstack([np.array(obj_gt[""bbox""]) for obj_gt in img_gt]) if img_gt else np.empty(\n                (0, 4)),\n            is_matched=np.zeros(len(img_gt), dtype=bool),\n            is_ignored=np.array([obj_gt.get(""is_ignored"", False) for obj_gt in img_gt], dtype=bool))\n\n    detections_num = len(detections)\n    true_pos = np.zeros(detections_num)\n    false_pos = np.zeros(detections_num)\n\n    for i, detection in tqdm(enumerate(detections), desc=""Processing detections"",\n                             disable=not verbose):\n        image_path = detection.image\n        bboxes_gt = gts[image_path].bbox\n        bbox = detection.bbox\n        max_overlap = -np.inf\n\n        if bboxes_gt is not None and bboxes_gt.shape[0] > 0:\n            intersection_xmin = np.maximum(bboxes_gt[:, 0], bbox[0])\n            intersection_ymin = np.maximum(bboxes_gt[:, 1], bbox[1])\n            intersection_xmax = np.minimum(bboxes_gt[:, 0] + bboxes_gt[:, 2], bbox[0] + bbox[2])\n            intersection_ymax = np.minimum(bboxes_gt[:, 1] + bboxes_gt[:, 3], bbox[1] + bbox[3])\n            intersection_width = np.maximum(intersection_xmax - intersection_xmin, 0.)\n            intersection_height = np.maximum(intersection_ymax - intersection_ymin, 0.)\n            intersection = intersection_width * intersection_height\n\n            det_area = bbox[2] * bbox[3]\n            gt_area = bboxes_gt[:, 2] * bboxes_gt[:, 3]\n            union = (det_area + gt_area - intersection)\n            ignored_mask = gts[image_path].is_ignored\n            if allow_multiple_matches_per_ignored:\n                if np.any(ignored_mask):\n                    union[ignored_mask] = det_area\n\n            overlaps = intersection / union\n            # Match not ignored ground truths first.\n            if np.any(~ignored_mask):\n                overlaps_filtered = np.copy(overlaps)\n                overlaps_filtered[ignored_mask] = 0.0\n                max_overlap = np.max(overlaps_filtered)\n                argmax_overlap = np.argmax(overlaps_filtered)\n            # If match with non-ignored ground truth is not good enough,\n            # try to match with ignored ones.\n            if max_overlap < overlap_threshold and np.any(ignored_mask):\n                overlaps_filtered = np.copy(overlaps)\n                overlaps_filtered[~ignored_mask] = 0.0\n                max_overlap = np.max(overlaps_filtered)\n                argmax_overlap = np.argmax(overlaps_filtered)\n            detections[i] = detection._replace(gt_match=argmax_overlap)\n\n        if max_overlap >= overlap_threshold:\n            if not gts[image_path].is_ignored[argmax_overlap]:\n                if not gts[image_path].is_matched[argmax_overlap]:\n                    true_pos[i] = 1.\n                    gts[image_path].is_matched[argmax_overlap] = True\n                else:\n                    false_pos[i] = 1.\n            elif not allow_multiple_matches_per_ignored:\n                gts[image_path].is_matched[argmax_overlap] = True\n        else:\n            false_pos[i] = 1.\n\n    false_pos = np.cumsum(false_pos)\n    true_pos = np.cumsum(true_pos)\n\n    debug_visualization = False\n    if debug_visualization:\n        for image_path, bboxes_gt in gts.items():\n\n            print(image_path)\n            image = cv2.imread(image_path)\n            image_gt = np.copy(image)\n            for bbox in bboxes_gt.bbox:\n                cv2.rectangle(image_gt, tuple(bbox[:2]), tuple(bbox[2:] + bbox[:2]),\n                              color=(255, 255, 0), thickness=2)\n            cv2.imshow(""gt"", image_gt)\n            for detection in detections:\n                if detection.image != image_path:\n                    continue\n                bbox = detection.bbox\n                cv2.rectangle(image, tuple(bbox[:2]), tuple(bbox[2:] + bbox[:2]), color=(0, 255, 0),\n                              thickness=2)\n                if detection.gt_match is not None:\n                    bbox = bboxes_gt.bbox[detection.gt_match]\n                    cv2.rectangle(image, tuple(bbox[:2]), tuple(bbox[2:] + bbox[:2]),\n                                  color=(0, 0, 255), thickness=1)\n                cv2.imshow(""image"", image)\n                cv2.waitKey(0)\n\n    # Handle equal-score detections.\n    # Get index of the last occurrence of a score.\n    ind = len(scores) - np.unique(scores[sorted_ind[::-1]], return_index=True)[1] - 1\n    ind = ind[::-1]\n    # Though away redundant points.\n    false_pos = false_pos[ind]\n    true_pos = true_pos[ind]\n\n    total_positives_num = np.sum([np.count_nonzero(~gt.is_ignored) for gt in gts.values()])\n    recall = true_pos / float(total_positives_num)\n    # Avoid divide by zero in case the first detection matches an ignored ground truth.\n    precision = true_pos / np.maximum(true_pos + false_pos, np.finfo(np.float64).eps)\n    miss_rate = 1.0 - recall\n    fppi = false_pos / float(len(gts))\n\n    return recall, precision, miss_rate, fppi\n\n\nclass ImageAnnotation:\n    """""" Represent image annotation. """"""\n\n    def __init__(self, image_path, objects=None, ignore_regs=None):\n        self.image_path = image_path\n        self.objects = objects if objects else []\n        self.ignore_regs = ignore_regs if ignore_regs else []\n\n    def __len__(self):\n        return len(self.objects)\n\n    def __getitem__(self, item):\n        return self.objects[item]\n\n\ndef points_2_xywh(box):\n    """""" Converts [xmin, ymin, xmax, ymax] to [xmin, ymin, width, height]. """"""\n\n    box = [box[0], box[1], box[2] - box[0], box[3] - box[1]]\n    box = [int(round(x)) for x in box]\n    return box\n\n\ndef clip_bbox(bbox, im_size):\n    """""" Clips box. """"""\n\n    bbox = np.maximum(np.copy(bbox), 0)\n    xmin, ymin, width, height = bbox\n    width = min(xmin + width, im_size[0]) - xmin\n    height = min(ymin + height, im_size[1]) - ymin\n    if width == 0 and height == 0:\n        xmin = ymin = width = height = -1\n    return np.array([xmin, ymin, width, height])\n\n\ndef voc_eval(result_file, dataset, iou_thr, image_size):\n    """""" VOC AP evaluation procedure for range of face sizes. """"""\n\n    det_results = mmcv.load(result_file)\n    min_detection_confidence = 0.01\n\n    out = []\n\n    for obj_size in ((10, 1024), (32, 1024), (64, 1024), (100, 1024)):\n\n        groundtruth = []\n        predictions = []\n\n        for i, _ in enumerate(tqdm(dataset)):\n            ann = dataset.get_ann_info(i)\n            bboxes = ann[\'bboxes\']\n\n            # +1 is to compensate pre-processing in XMLDataset\n            if isinstance(dataset, datasets.XMLDataset):\n                bboxes = [np.array(bbox) + np.array((1, 1, 1, 1)) for bbox in bboxes]\n            elif isinstance(dataset, datasets.CocoDataset):\n                bboxes = [np.array(bbox) + np.array((0, 0, 1, 1)) for bbox in bboxes]\n            # convert from [xmin, ymin, xmax, ymax] to [xmin, ymin, w, h]\n            bboxes = [points_2_xywh(bbox) for bbox in bboxes]\n            # clip bboxes\n            bboxes = [clip_bbox(bbox, image_size) for bbox in bboxes]\n            # filter out boxes with to small height or with invalid size (-1)\n            ignored = [not (obj_size[0] <= b[3] <= obj_size[1]) or np.any(b == -1) for b in bboxes]\n            objects = [{\'bbox\': bbox, \'is_ignored\': ignor} for bbox, ignor in zip(bboxes, ignored)]\n            groundtruth.append(ImageAnnotation(dataset.img_infos[i][\'id\'], objects))\n\n            # filter out predictions with too low confidence\n            detections = [{\'bbox\': points_2_xywh(bbox[:4]), \'score\': bbox[4], \'type\': \'face\'} for\n                          bbox\n                          in det_results[i][0] if bbox[4] > min_detection_confidence]\n            predictions.append(ImageAnnotation(dataset.img_infos[i][\'id\'], detections))\n\n        recall, precision, miss_rates, fppis = evaluate_detections(\n            groundtruth, predictions, \'face\',\n            allow_multiple_matches_per_ignored=True,\n            overlap_threshold=iou_thr)\n\n        miss_rate = compute_miss_rate(miss_rates, fppis) * 100\n        average_precision = voc_ap(recall, precision) * 100\n\n        print(f\'image_size = {image_size}, \'\n              f\'object_size = {obj_size}, \'\n              f\'average_precision = {average_precision:.2f}%, \'\n              f\'miss_rate = {miss_rate:.2f}%\')\n\n        average_precision = average_precision if not np.isnan(average_precision) else -1.0\n\n        out.append({\'image_size\': image_size,\n                    \'object_size\': obj_size,\n                    \'average_precision\': average_precision,\n                    \'miss_rate\': miss_rate})\n    return out\n\n\ndef main():\n    """""" Main function. """"""\n\n    parser = ArgumentParser(description=\'VOC Evaluation\')\n    parser.add_argument(\'config\', help=\'config file path\')\n    parser.add_argument(\'input\', help=\'output result file from test.py\')\n    parser.add_argument(\'--imsize\', nargs=2, type=int, default=(1024, 1024),\n                        help=\'Image resolution. Used for filtering.\')\n    parser.add_argument(\'--iou-thr\', type=float, default=0.5, help=\'IoU threshold for evaluation\')\n    parser.add_argument(\'--out\', help=\'A path to file where metrics values will be saved (*.json).\')\n    args = parser.parse_args()\n\n    cfg = mmcv.Config.fromfile(args.config)\n    test_dataset = datasets.builder.build_dataset(cfg.data.test)\n    out = voc_eval(args.input, test_dataset, args.iou_thr, args.imsize)\n\n    if args.out:\n        with open(args.out, \'w\') as write_file:\n            json.dump(out, write_file, indent=4)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/face-detection/tools/wider_face_eval.py,0,"b'# https://github.com/wondervictor/WiderFace-Evaluation/blob/master/evaluation.py\n""""""\nWiderFace evaluation code\nauthor: wondervictor\nmail: tianhengcheng@gmail.com\ncopyright@wondervictor\n""""""\n\n# pylint: disable=C0301,W0622,R0914,C0103,I1101,C0411,C0200\n\nimport os\nimport tqdm\nimport pickle\nimport argparse\nimport numpy as np\nimport json\nfrom scipy.io import loadmat\nfrom box_overlaps import bbox_overlaps\n\n\ndef get_gt_boxes(gt_dir):\n    """""" gt dir: (wider_face_val.mat, wider_easy_val.mat, wider_medium_val.mat, wider_hard_val.mat)""""""\n\n    gt_mat = loadmat(os.path.join(gt_dir, \'wider_face_val.mat\'))\n    hard_mat = loadmat(os.path.join(gt_dir, \'wider_hard_val.mat\'))\n    medium_mat = loadmat(os.path.join(gt_dir, \'wider_medium_val.mat\'))\n    easy_mat = loadmat(os.path.join(gt_dir, \'wider_easy_val.mat\'))\n\n    facebox_list = gt_mat[\'face_bbx_list\']\n    event_list = gt_mat[\'event_list\']\n    file_list = gt_mat[\'file_list\']\n\n    hard_gt_list = hard_mat[\'gt_list\']\n    medium_gt_list = medium_mat[\'gt_list\']\n    easy_gt_list = easy_mat[\'gt_list\']\n\n    return facebox_list, event_list, file_list, hard_gt_list, medium_gt_list, easy_gt_list\n\n\ndef get_gt_boxes_from_txt(gt_path, cache_dir):\n    cache_file = os.path.join(cache_dir, \'gt_cache.pkl\')\n    if os.path.exists(cache_file):\n        f = open(cache_file, \'rb\')\n        boxes = pickle.load(f)\n        f.close()\n        return boxes\n\n    f = open(gt_path, \'r\')\n    state = 0\n    lines = f.readlines()\n    lines = list(map(lambda x: x.rstrip(\'\\r\\n\'), lines))\n    boxes = {}\n    f.close()\n    current_boxes = []\n    current_name = None\n    for line in lines:\n        if state == 0 and \'--\' in line:\n            state = 1\n            current_name = line\n            continue\n        if state == 1:\n            state = 2\n            continue\n\n        if state == 2 and \'--\' in line:\n            state = 1\n            boxes[current_name] = np.array(current_boxes).astype(\'float32\')\n            current_name = line\n            current_boxes = []\n            continue\n\n        if state == 2:\n            box = [float(x) for x in line.split(\' \')[:4]]\n            current_boxes.append(box)\n            continue\n\n    f = open(cache_file, \'wb\')\n    pickle.dump(boxes, f)\n    f.close()\n    return boxes\n\n\ndef read_pred_file(filepath):\n    with open(filepath, \'r\') as f:\n        lines = f.readlines()\n        img_file = lines[0].rstrip(\'\\n\\r\')\n        lines = lines[2:]\n\n    boxes = np.array(list(map(lambda x: [float(a) for a in x.rstrip(\'\\r\\n\').split(\' \')], lines))).astype(\'float\')\n    return img_file.split(\'/\')[-1], boxes\n\n\ndef get_preds(pred_dir):\n    events = os.listdir(pred_dir)\n    boxes = dict()\n    pbar = tqdm.tqdm(events)\n\n    for event in pbar:\n        pbar.set_description(\'Reading Predictions \')\n        event_dir = os.path.join(pred_dir, event)\n        event_images = os.listdir(event_dir)\n        current_event = dict()\n        for imgtxt in event_images:\n            imgname, _boxes = read_pred_file(os.path.join(event_dir, imgtxt))\n            current_event[imgname.rstrip(\'.jpg\')] = _boxes\n        boxes[event] = current_event\n    return boxes\n\n\ndef norm_score(pred):\n    """""" norm score\n    pred {key: [[x1,y1,x2,y2,s]]}\n    """"""\n\n    max_score = 0\n    min_score = 1\n\n    for _, k in pred.items():\n        for _, v in k.items():\n            if v.shape[0] == 0:\n                continue\n            _min = np.min(v[:, -1])\n            _max = np.max(v[:, -1])\n            max_score = max(_max, max_score)\n            min_score = min(_min, min_score)\n\n    diff = max_score - min_score\n    for _, k in pred.items():\n        for _, v in k.items():\n            if v.shape[0] == 0:\n                continue\n            v[:, -1] = (v[:, -1] - min_score) / diff\n\n\ndef image_eval(pred, gt, ignore, iou_thresh):\n    """""" single image evaluation\n    pred: Nx5\n    gt: Nx4\n    ignore:\n    """"""\n\n    _pred = pred.copy()\n    _gt = gt.copy()\n    pred_recall = np.zeros(_pred.shape[0])\n    recall_list = np.zeros(_gt.shape[0])\n    proposal_list = np.ones(_pred.shape[0])\n\n    _pred[:, 2] = _pred[:, 2] + _pred[:, 0]\n    _pred[:, 3] = _pred[:, 3] + _pred[:, 1]\n    _gt[:, 2] = _gt[:, 2] + _gt[:, 0]\n    _gt[:, 3] = _gt[:, 3] + _gt[:, 1]\n\n    overlaps = bbox_overlaps(_pred[:, :4], _gt)\n\n    for h in range(_pred.shape[0]):\n\n        gt_overlap = overlaps[h]\n        max_overlap, max_idx = gt_overlap.max(), gt_overlap.argmax()\n        if max_overlap >= iou_thresh:\n            if ignore[max_idx] == 0:\n                recall_list[max_idx] = -1\n                proposal_list[h] = -1\n            elif recall_list[max_idx] == 0:\n                recall_list[max_idx] = 1\n\n        r_keep_index = np.where(recall_list == 1)[0]\n        pred_recall[h] = len(r_keep_index)\n    return pred_recall, proposal_list\n\n\ndef img_pr_info(thresh_num, pred_info, proposal_list, pred_recall):\n    pr_info = np.zeros((thresh_num, 2)).astype(\'float\')\n    for t in range(thresh_num):\n\n        thresh = 1 - (t + 1) / thresh_num\n        r_index = np.where(pred_info[:, 4] >= thresh)[0]\n        if r_index.shape[0] == 0:\n            pr_info[t, 0] = 0\n            pr_info[t, 1] = 0\n        else:\n            r_index = r_index[-1]\n            p_index = np.where(proposal_list[:r_index + 1] == 1)[0]\n            pr_info[t, 0] = len(p_index)\n            pr_info[t, 1] = pred_recall[r_index]\n    return pr_info\n\n\ndef dataset_pr_info(thresh_num, pr_curve, count_face):\n    _pr_curve = np.zeros((thresh_num, 2))\n    for i in range(thresh_num):\n        _pr_curve[i, 0] = pr_curve[i, 1] / pr_curve[i, 0]\n        _pr_curve[i, 1] = pr_curve[i, 1] / count_face\n    return _pr_curve\n\n\ndef voc_ap(rec, prec):\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef evaluation(pred, gt_path, iou_thresh=0.5):\n    pred = get_preds(pred)\n    norm_score(pred)\n    facebox_list, event_list, file_list, hard_gt_list, medium_gt_list, easy_gt_list = get_gt_boxes(gt_path)\n    event_num = len(event_list)\n    thresh_num = 1000\n    settings = [\'easy\', \'medium\', \'hard\']\n    setting_gts = [easy_gt_list, medium_gt_list, hard_gt_list]\n    aps = []\n    for setting_id in range(3):\n        # different setting\n        gt_list = setting_gts[setting_id]\n        count_face = 0\n        pr_curve = np.zeros((thresh_num, 2)).astype(\'float\')\n        # [hard, medium, easy]\n        pbar = tqdm.tqdm(range(event_num))\n        for i in pbar:\n            pbar.set_description(\'Processing {}\'.format(settings[setting_id]))\n            event_name = str(event_list[i][0][0])\n            img_list = file_list[i][0]\n            pred_list = pred[event_name]\n            sub_gt_list = gt_list[i][0]\n            # img_pr_info_list = np.zeros((len(img_list), thresh_num, 2))\n            gt_bbx_list = facebox_list[i][0]\n\n            for j in range(len(img_list)):\n                pred_info = pred_list[str(img_list[j][0][0])]\n\n                gt_boxes = gt_bbx_list[j][0].astype(\'float\')\n                keep_index = sub_gt_list[j][0]\n                count_face += len(keep_index)\n\n                if gt_boxes.shape[0] == 0 or pred_info.shape[0] == 0:\n                    continue\n                ignore = np.zeros(gt_boxes.shape[0])\n                if keep_index.shape[0] != 0:\n                    ignore[keep_index - 1] = 1\n                pred_recall, proposal_list = image_eval(pred_info, gt_boxes, ignore, iou_thresh)\n\n                _img_pr_info = img_pr_info(thresh_num, pred_info, proposal_list, pred_recall)\n\n                pr_curve += _img_pr_info\n        pr_curve = dataset_pr_info(thresh_num, pr_curve, count_face)\n\n        propose = pr_curve[:, 0]\n        recall = pr_curve[:, 1]\n\n        ap = voc_ap(recall, propose)\n        aps.append(ap)\n\n    print(""==================== Results ===================="")\n    print(""Easy   Val AP: {}"".format(aps[0]))\n    print(""Medium Val AP: {}"".format(aps[1]))\n    print(""Hard   Val AP: {}"".format(aps[2]))\n    print(""================================================="")\n\n    out = [\n        {\'key\': \'widerface_e\',\n         \'value\': aps[0] * 100,\n         \'display_name\': \'WiderFace Easy\',\n         \'unit\': \'%\'},\n        {\'key\': \'widerface_m\',\n         \'value\': aps[1] * 100,\n         \'display_name\': \'WiderFace Medium\',\n         \'unit\': \'%\'},\n        {\'key\': \'widerface_h\',\n         \'value\': aps[2] * 100,\n         \'display_name\': \'WiderFace Hard\',\n         \'unit\': \'%\'}\n    ]\n    return out\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-p\', \'--pred\')\n    parser.add_argument(\'-g\', \'--gt\')\n    parser.add_argument(\'--out\')\n\n    args = parser.parse_args()\n    out = evaluation(args.pred, args.gt)\n    if args.out:\n        with open(args.out, \'w\') as write_file:\n            json.dump(out, write_file, indent=4)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/face-detection/tools/wider_to_coco.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" Converts WiderFace annotation to COCO format. """"""\n\n# pylint: disable=C0301,W0622,R0914,R0801\n\nimport json\nimport os\n\nimport argparse\nimport imagesize\nfrom tqdm import tqdm\n\n\ndef parse_wider_gt(ann_file):\n    """""" Parses wider annotation. """"""\n\n    bboxes = dict()\n    landmarks = dict()\n    with open(ann_file) as read_file:\n        content = [line.strip() for line in read_file.readlines()]\n        new_file = True\n        i = 0\n        while True:\n            if new_file:\n                image_name = content[i]\n                bboxes[image_name] = list()\n                landmarks[image_name] = list()\n                new_file = False\n                i += 1\n            else:\n                bbox_num = int(content[i])\n                if bbox_num == 0:\n                    i += 1\n                i += 1\n                for _ in range(bbox_num):\n                    xmin, ymin, width, height = [int(x) for x in content[i].split(\' \')[:4]]\n                    if width >= 0 and height >= 0:\n                        bboxes[image_name].append([xmin, ymin, width, height])\n                        landmarks[image_name].append([])\n                    else:\n                        print(\'Ignored because of invalid size: \', [xmin, ymin, width, height])\n                    i += 1\n                if i == len(content):\n                    break\n                new_file = True\n\n    return bboxes, landmarks\n\n\ndef parse_wider_gt_with_landmarks(ann_file):\n    """""" Parses wider annotation with landmarks. """"""\n\n    bboxes = dict()\n    landmarks = dict()\n    with open(ann_file) as read_file:\n        content = [line.strip() for line in read_file.readlines()]\n        new_file = True\n        i = 0\n        while True:\n            if new_file:\n                image_name = content[i][2:]\n                bboxes[image_name] = list()\n                landmarks[image_name] = list()\n                new_file = False\n                i += 1\n            else:\n                while True:\n                    if i == len(content) or content[i].startswith(\'#\'):\n                        break\n                    line_split = content[i].split(\' \')\n                    xmin, ymin, width, height = [int(x) for x in line_split[:4]]\n                    if width >= 0 and height >= 0:\n                        bboxes[image_name].append([xmin, ymin, width, height])\n                        points = [float(x) if (i + 1) % 3 != 0 else float(x) + 1 for i, x in\n                                  enumerate(line_split[4:-1])]\n                        landmarks[image_name].append(points)\n                    else:\n                        print(\'Ignored because of invalid size: \', [xmin, ymin, width, height])\n                    i += 1\n                if i == len(content):\n                    break\n                new_file = True\n\n    return bboxes, landmarks\n\n\ndef parse_args():\n    """""" Parses input arguments. """"""\n\n    parser = argparse.ArgumentParser(description=\'Convert dataset\')\n    parser.add_argument(\'input_annotation\',\n                        help=""Path to annotation file like wider_face_train_bbx_gt.txt"")\n    parser.add_argument(\'images_dir\',\n                        help=""Path to folder with images like WIDER_train/images"")\n    parser.add_argument(\'output_annotation\', help=""Path to output json file"")\n    parser.add_argument(\'--with_landmarks\', action=\'store_true\',\n                        help=""Whether to read landmarks"")\n\n    return parser.parse_args()\n\n\ndef convert_wider_annotation(ann_file, data_dir, out_file, with_landmarks):\n    """""" Converts wider annotation to COCO format. """"""\n\n    img_id = 0\n    ann_id = 0\n    cat_id = 1\n\n    ann_dict = {}\n    categories = [{""id"": 1, ""name"": \'face\'}]\n    images_info = []\n    annotations = []\n\n    if with_landmarks:\n        boxes, landmarks = parse_wider_gt_with_landmarks(ann_file)\n    else:\n        boxes, landmarks = parse_wider_gt(ann_file)\n\n    for filename in tqdm(boxes.keys()):\n        image_info = {}\n        image_info[\'id\'] = img_id\n        img_id += 1\n        image_info[\'width\'], image_info[\'height\'] = imagesize.get(os.path.join(data_dir, filename))\n        image_info[\'file_name\'] = os.path.relpath(\n            os.path.join(data_dir, filename), os.path.dirname(out_file))\n        images_info.append(image_info)\n\n        for gt_bbox, gt_landmarks in zip(boxes[filename], landmarks[filename]):\n            ann = {\n                \'id\': ann_id,\n                \'image_id\': image_info[\'id\'],\n                \'segmentation\': [],\n                \'keypoints\': gt_landmarks,\n                \'category_id\': cat_id,\n                \'iscrowd\': 0,\n                \'area\': gt_bbox[2] * gt_bbox[3],\n                \'bbox\': gt_bbox\n            }\n            ann_id += 1\n            annotations.append(ann)\n\n    ann_dict[\'images\'] = images_info\n    ann_dict[\'categories\'] = categories\n    ann_dict[\'annotations\'] = annotations\n    os.makedirs(os.path.dirname(out_file), exist_ok=True)\n\n    with open(out_file, \'w\') as outfile:\n        outfile.write(json.dumps(ann_dict))\n\n\ndef main():\n    """""" Main function. """"""\n\n    args = parse_args()\n    convert_wider_annotation(args.input_annotation, args.images_dir,\n                             args.output_annotation, args.with_landmarks)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/object_detection/person-vehicle-bike-detection/person-vehicle-bike-detection-crossroad-1016/config.py,0,"b""# model settings\ninput_size = 512\nmodel = dict(\n    type='SingleStageDetector',\n    pretrained=None,\n    backbone=dict(\n        type='SSDMobilenetV2',\n        input_size=input_size,\n        scales=6,\n        ),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(576, 1280, 512),\n        num_classes=4,\n        anchor_strides=(16, 32, 64),\n        anchor_widths=([17.137, 38.165, 70.69, 9.584, 17.634, 23.744, 6.507, 12.245, 14.749],\n                       [81.753, 153.183, 169.567, 32.148, 41.048, 52.198, 32.391, 22.397, 33.216],\n                       [110.651, 237.237, 348.269, 65.598, 82.729, 110.538, 53.24, 68.246, 105.444],\n                       ),\n        anchor_heights=([20.733, 45.464, 78.592, 29.393, 55.398, 84.88, 17.006, 28.673, 44.11],\n                        [157.379, 104.698, 210.545, 118.319, 157.328, 203.363, 36.256, 64.451, 101.718],\n                        [344.064, 243.971, 337.749, 256.941, 327.187, 428.114, 68.919, 155.867, 270.048],\n                        ),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2),\n        depthwise_heads=True,\n        depthwise_heads_activations='relu',\n        loss_balancing=False))\n# training and testing settings\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    use_giou=False,\n    use_focal=False,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# dataset settings\ndataset_type = 'CustomCocoDataset'\ndata_root = '../../data/airport'\nimg_norm_cfg = dict(\n    mean=[0, 0, 0], std=[255, 255, 255], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.3),\n    dict(type='Resize', img_scale=(input_size, input_size), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(input_size, input_size),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    imgs_per_gpu=4,\n    workers_per_gpu=0,\n    train=dict(\n        type=dataset_type,\n        classes=('vehicle', 'person', 'non-vehicle'),\n        ann_file=data_root+'/annotation_example_train.json',\n        img_prefix=data_root + '/train',\n        pipeline=train_pipeline\n        ),\n    val=dict(\n        type=dataset_type,\n        classes=('vehicle', 'person', 'non-vehicle'),\n        ann_file=data_root+'/annotation_example_val.json',\n        img_prefix=data_root + '/val',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        classes=('vehicle', 'person', 'non-vehicle'),\n        ann_file=data_root+'/annotation_example_val.json',\n        img_prefix=data_root + '/val',\n        pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.0001, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=5,\n    warmup_ratio=1.0 / 3,\n    step=[50, 75])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=1,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook'),\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 5\n# device_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = 'outputs/person-vehicle-bike-detection-crossroad-1016'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
pytorch_toolkit/object_detection/tests/common/test_case.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport json\nimport os\nimport unittest\n\nfrom common.utils import download_if_not_yet, collect_ap\n\n\ndef export_test_case(problem_name, model_name, snapshot_name=None, alt_ssd_export=False):\n    class ExportTestCase(unittest.TestCase):\n        def setUp(self):\n            self.problem_name = problem_name\n            self.model_name = model_name\n            if snapshot_name is None:\n                self.snapshot_name = f\'{self.model_name}.pth\'\n            else:\n                self.snapshot_name = snapshot_name\n\n            self.data_folder = \'../../data\'\n            self.work_dir = os.path.join(\'/tmp/\', self.model_name)\n            os.makedirs(self.work_dir, exist_ok=True)\n            self.configuration_file = f\'./{self.problem_name}/{self.model_name}/config.py\'\n            os.system(f\'cp {self.configuration_file} {self.work_dir}/\')\n            self.configuration_file = os.path.join(self.work_dir,\n                                                   os.path.basename(self.configuration_file))\n            self.ote_url = \'https://download.01.org/opencv/openvino_training_extensions\'\n            self.url = f\'{self.ote_url}/models/object_detection/{self.snapshot_name}\'\n            download_if_not_yet(self.work_dir, self.url)\n\n            self.test_export_thr = 0.01\n\n        def export_test(self, alt_ssd_export, thr):\n            if alt_ssd_export:\n                export_command_end = \'--alt_ssd_export\'\n                export_dir = os.path.join(self.work_dir, ""alt_ssd_export"")\n                log_file = os.path.join(export_dir, \'test_alt_ssd_export.log\')\n            else:\n                export_dir = os.path.join(self.work_dir, ""export"")\n                log_file = os.path.join(export_dir, \'test_export.log\')\n                export_command_end = \'\'\n\n            os.system(\n                f\'/opt/intel/openvino/bin/setupvars.sh;\'\n                f\'python ../../external/mmdetection/tools/export.py \'\n                f\'{self.configuration_file} \'\n                f\'{os.path.join(self.work_dir, self.snapshot_name)} \'\n                f\'{export_dir} \'\n                f\'openvino {export_command_end};\'\n                f\'python ../../external/mmdetection/tools/test_exported.py \'\n                f\'{self.configuration_file} \'\n                f\'{os.path.join(export_dir, ""config.xml"")} \'\n                f\'--out res.pkl --eval bbox 2>&1 | tee {log_file}\')\n\n            ap = collect_ap(log_file)\n\n            with open(f\'tests/expected_outputs/{self.problem_name}/{self.model_name}.json\') as read_file:\n                content = json.load(read_file)\n\n            self.assertGreater(ap[0], content[\'map\'] - thr)\n\n        def test_export(self):\n            self.export_test(False, self.test_export_thr)\n\n    class ExportWithAltSsdTestCase(ExportTestCase):\n\n        def setUp(self):\n            super().setUp()\n            self.test_alt_ssd_export_thr = 0.03\n\n        def test_alt_ssd_export(self):\n            self.export_test(True, self.test_alt_ssd_export_thr)\n\n    if alt_ssd_export:\n        return ExportWithAltSsdTestCase\n\n    return ExportTestCase\n'"
pytorch_toolkit/object_detection/tests/common/utils.py,0,"b'# Copyright (C) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport os\n\n\ndef replace_text_in_file(path, replace_what, replace_by):\n    with open(path) as read_file:\n        content = \'\\n\'.join([line.rstrip() for line in read_file.readlines()])\n        if content.find(replace_what) == -1:\n            return False\n        content = content.replace(replace_what, replace_by)\n    with open(path, \'w\') as write_file:\n        write_file.write(content)\n    return True\n\n\ndef collect_ap(path):\n    ap = []\n    beginning = \'Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = \'\n    with open(path) as read_file:\n        content = [line.strip() for line in read_file.readlines()]\n        for line in content:\n            if line.startswith(beginning):\n                ap.append(float(line.replace(beginning, \'\')))\n    return ap\n\n\ndef download_if_not_yet(output_folder, url):\n    os.makedirs(output_folder, exist_ok=True)\n    path = os.path.join(output_folder, os.path.basename(url))\n    if not os.path.exists(path):\n        os.system(f\'wget --no-verbose {url} -P {output_folder}\')\n    return path\n\n\ndef relative_abs_error(expected, actual):\n    return abs(expected - actual) / expected\n'"
pytorch_toolkit/person_reidentification/data/datasets/globalme.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os.path as osp\nimport glob\nimport re\nimport warnings\n\nfrom torchreid.data.datasets import ImageDataset\n\n\nclass GlobalMe(ImageDataset):\n    """"""GlobalMe.\n\n    Dataset statistics:\n        - identities: 1610.\n        - images: 0 (train) + 8450 (query) + 41107 (gallery).\n        - cameras: 8.\n    """"""\n    dataset_dir = \'globalme-reid\'\n    dataset_subdir = \'GlobalMe-reID\'\n\n    def __init__(self, root=\'\', market1501_500k=False, **kwargs):\n        self.root = osp.abspath(osp.expanduser(root))\n        self.dataset_dir = osp.join(self.root, self.dataset_dir)\n\n        # allow alternative directory structure\n        self.data_dir = self.dataset_dir\n        data_dir = osp.join(self.data_dir, self.dataset_subdir)\n        if osp.isdir(data_dir):\n            self.data_dir = data_dir\n        else:\n            warnings.warn(\'The current data structure is deprecated. Please \'\n                          \'put data folders such as ""bounding_box_train"" under \'\n                          \'""{}"".\'.format(self.dataset_subdir))\n\n        self.train_dir = osp.join(self.data_dir, \'bounding_box_train\')\n        self.query_dir = osp.join(self.data_dir, \'query\')\n        self.gallery_dir = osp.join(self.data_dir, \'bounding_box_test\')\n        self.extra_gallery_dir = osp.join(self.data_dir, \'images\')\n        self.market1501_500k = market1501_500k\n\n        required_files = [\n            self.data_dir,\n            self.train_dir,\n            self.query_dir,\n            self.gallery_dir\n        ]\n        if self.market1501_500k:\n            required_files.append(self.extra_gallery_dir)\n        self.check_before_run(required_files)\n\n        train = self.process_dir(self.train_dir, relabel=True)\n        query = self.process_dir(self.query_dir, relabel=False)\n        gallery = self.process_dir(self.gallery_dir, relabel=False)\n        if self.market1501_500k:\n            gallery += self.process_dir(self.extra_gallery_dir, relabel=False)\n\n        super(GlobalMe, self).__init__(train, query, gallery, **kwargs)\n\n    @staticmethod\n    def process_dir(dir_path, relabel=True):\n        img_paths = glob.glob(osp.join(dir_path, \'*.jpg\'))\n        pattern = re.compile(r\'([-\\d]+)_c(\\d)\')\n\n        pid_container = set()\n        for img_path in img_paths:\n            pid, _ = map(int, pattern.search(img_path).groups())\n            if pid == -1:\n                continue\n            pid_container.add(pid)\n        pid2label = {pid: label for label, pid in enumerate(pid_container)}\n\n        data = []\n        for img_path in img_paths:\n            pid, camid = map(int, pattern.search(img_path).groups())\n            if pid == -1:\n                continue\n            if relabel:\n                pid = pid2label[pid]\n            data.append((img_path, pid, camid))\n        return data\n'"
pytorch_toolkit/person_reidentification/engine/losses/am_softmax.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport math\n\nimport torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\n\nclass AngleSimpleLinear(nn.Module):\n    """"""Computes cos of angles between input vectors and weights vectors""""""\n\n    def __init__(self, in_features, out_features):\n        super(AngleSimpleLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n\n    def forward(self, x):\n        cos_theta = F.normalize(x, dim=1).mm(F.normalize(self.weight, dim=0))\n        return cos_theta.clamp(-1, 1)\n\n\ndef focal_loss(input_values, gamma):\n    """"""Computes the focal loss""""""\n    p = torch.exp(-input_values)\n    loss = (1 - p) ** gamma * input_values\n    return loss.mean()\n\n\nclass AMSoftmaxLoss(nn.Module):\n    margin_types = [\'cos\', \'arc\']\n\n    def __init__(self, num_classes, epsilon=0.1, use_gpu=True,\n                 conf_penalty=0.,\n                 margin_type=\'cos\', gamma=0., m=0.5, s=30, t=1.,\n                 pr_product=False):\n        super(AMSoftmaxLoss, self).__init__()\n        self.num_classes = num_classes\n        self.use_gpu = use_gpu\n        self.conf_penalty = conf_penalty\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n\n        assert margin_type in AMSoftmaxLoss.margin_types\n        self.margin_type = margin_type\n        assert gamma >= 0\n        self.gamma = gamma\n        assert m >= 0\n        self.m = m\n        assert s > 0\n        self.s = s\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        assert t >= 1\n        self.t = t\n        self.pr_product = pr_product\n\n    def _pr_product(self, prod):\n        alpha = torch.sqrt(1.0 - prod.pow(2.0))\n        out_prod = alpha.detach() * prod + prod.detach() * (1.0 - alpha)\n        return out_prod\n\n    def get_last_info(self):\n        return {}\n\n    def forward(self, cos_theta, target):\n        """"""\n        Args:\n            inputs (torch.Tensor): prediction matrix (before softmax) with\n                shape (batch_size, num_classes).\n            targets (torch.LongTensor): ground truth labels with shape (batch_size).\n                Each position contains the label index.\n        """"""\n        if self.pr_product:\n            cos_theta = self._pr_product(cos_theta)\n\n        if self.margin_type == \'cos\':\n            phi_theta = cos_theta - self.m\n        else:\n            sine = torch.sqrt(1.0 - torch.pow(cos_theta, 2))\n            phi_theta = cos_theta * self.cos_m - sine * self.sin_m #cos(theta+m)\n            phi_theta = torch.where(cos_theta > self.th, phi_theta, cos_theta - self.sin_m * self.m)\n\n        index = torch.zeros_like(cos_theta, dtype=torch.uint8)\n        index.scatter_(1, target.data.view(-1, 1), 1)\n        output = torch.where(index, phi_theta, cos_theta)\n\n        if self.gamma == 0 and self.t == 1.:\n            if self.conf_penalty > 0.:\n                output *= self.s\n                log_probs = self.logsoftmax(output)\n                probs = torch.exp(log_probs)\n                ent = (-probs*torch.log(probs.clamp(min=1e-12))).sum(1)\n                loss = F.relu(F.cross_entropy(output, target, reduction=\'none\') - self.conf_penalty * ent)\n                with torch.no_grad():\n                    nonzero_count = loss.nonzero().size(0)\n                return loss.sum() / nonzero_count\n            else:\n                return F.cross_entropy(self.s*output, target)\n\n        if self.t > 1:\n            h_theta = self.t - 1 + self.t*cos_theta\n            support_vecs_mask = (1 - index) * \\\n                torch.lt(torch.masked_select(phi_theta, index).view(-1, 1).repeat(1, h_theta.shape[1]) - cos_theta, 0)\n            output = torch.where(support_vecs_mask, h_theta, output)\n            return F.cross_entropy(self.s*output, target)\n\n        return focal_loss(F.cross_entropy(self.s*output, target, reduction=\'none\'), self.gamma)\n'"
pytorch_toolkit/person_reidentification/engine/losses/cross_entropy_loss.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport torch\nimport torch.nn as nn\n\n\nclass CrossEntropyLoss(nn.Module):\n    r""""""Cross entropy loss with label smoothing regularizer.\n    """"""\n\n    def __init__(self, num_classes, epsilon=0.1, use_gpu=True, label_smooth=True, conf_penalty=0.):\n        super(CrossEntropyLoss, self).__init__()\n        self.num_classes = num_classes\n        self.epsilon = epsilon if label_smooth else 0\n        self.use_gpu = use_gpu\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n        self.conf_penalty = conf_penalty\n\n    def get_last_info(self):\n        return {}\n\n    def forward(self, inputs, targets):\n        """"""\n        Args:\n            inputs (torch.Tensor): prediction matrix (before softmax) with\n                shape (batch_size, num_classes).\n            targets (torch.LongTensor): ground truth labels with shape (batch_size).\n                Each position contains the label index.\n        """"""\n        log_probs = self.logsoftmax(inputs)\n        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n        if self.use_gpu: targets = targets.cuda()\n        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n        sm_loss = (- targets * log_probs).sum(1)\n\n        if self.conf_penalty > 0.:\n            probs = torch.exp(log_probs)\n            ent = (-probs*torch.log(probs.clamp(min=1e-12))).sum(1)\n            loss = nn.functional.relu(5 * sm_loss - 0.085 * ent)\n            with torch.no_grad():\n                nonzero_count = loss.nonzero().size(0)\n            return loss.sum() / nonzero_count\n\n        return sm_loss.mean(0)\n'"
pytorch_toolkit/person_reidentification/engine/losses/metric.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass LocalPushLoss(nn.Module):\n    def __init__(self, margin=0.1, weight=1.0, smart_margin=True):\n        super(LocalPushLoss, self).__init__()\n        self.margin = margin\n        assert self.margin >= 0.0\n        self.weight = weight\n        self.smart_margin = smart_margin\n\n    def forward(self, normalized_embeddings, cos_theta, target):\n        if self.weight == 0.0:\n            return torch.FloatTensor([0.0]).mean()\n        similarity = normalized_embeddings.matmul(normalized_embeddings.permute(1, 0))\n\n        with torch.no_grad():\n            pairs_mask = target.view(-1, 1) != target.view(1, -1)\n\n            if self.smart_margin:\n                center_similarity = cos_theta[torch.arange(cos_theta.size(0), device=target.device), target]\n                threshold = center_similarity.clamp(min=self.margin).view(-1, 1) - self.margin\n            else:\n                threshold = self.margin\n            similarity_mask = similarity > threshold\n\n            mask = pairs_mask & similarity_mask\n\n        filtered_similarity = torch.where(mask, similarity - threshold, torch.zeros_like(similarity))\n        losses, _ = filtered_similarity.max(dim=-1)\n\n        return self.weight * losses.mean()\n'"
pytorch_toolkit/person_reidentification/engine/losses/regularizers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass ConvRegularizer(nn.Module):\n    def __init__(self, reg_class, controller):\n        super().__init__()\n        self.reg_instance = reg_class(controller)\n\n    def get_all_conv_layers(self, module):\n        if isinstance(module, (nn.Sequential, list)):\n            for m in module:\n                yield from self.get_all_conv_layers(m)\n\n        if isinstance(module, nn.Conv2d):\n            yield module\n\n    def forward(self, net, ignore=False):\n\n        accumulator = torch.tensor(0.0).cuda()\n\n        if ignore:\n            return accumulator\n\n        all_mods = [module for module in net.module.modules() if type(module) != nn.Sequential]\n        for conv in self.get_all_conv_layers(all_mods):\n            accumulator += self.reg_instance(conv.weight)\n\n        return accumulator\n\n\nclass SVMORegularizer(nn.Module):\n    def __init__(self, beta):\n        super().__init__()\n        self.beta = beta\n\n    def dominant_eigenvalue(self, A):  # A: \'N x N\'\n        N, _ = A.size()\n        x = torch.rand(N, 1, device=\'cuda\')\n        Ax = (A @ x)\n        AAx = (A @ Ax)\n        return AAx.permute(1, 0) @ Ax / (Ax.permute(1, 0) @ Ax)\n\n    def get_singular_values(self, A):  # A: \'M x N, M >= N\'\n        ATA = A.permute(1, 0) @ A\n        N, _ = ATA.size()\n        largest = self.dominant_eigenvalue(ATA)\n        I = torch.eye(N, device=\'cuda\')  # noqa\n        I = I * largest  # noqa\n        tmp = self.dominant_eigenvalue(ATA - I)\n        return tmp + largest, largest\n\n    def forward(self, W):  # W: \'S x C x H x W\'\n        # old_W = W\n        old_size = W.size()\n        if old_size[0] == 1:\n            return 0\n        W = W.view(old_size[0], -1).permute(1, 0)  # (C x H x W) x S\n        smallest, largest = self.get_singular_values(W)\n        return (\n            self.beta * 10 * (largest - smallest)**2\n        ).squeeze()\n\n\nclass NoneRegularizer(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, _):\n        return torch.tensor(0.0).cuda()\n\n\nmapping = {\n    False: NoneRegularizer,\n    True: SVMORegularizer,\n}\n\n\ndef get_regularizer(cfg_reg):\n    name = cfg_reg.ow\n    return ConvRegularizer(mapping[name], cfg_reg.ow_beta)\n'"
pytorch_toolkit/person_reidentification/engine/schedulers/lr_scheduler.py,0,"b'""""""\n MIT License\n\n Copyright (c) 2018 Kaiyang Zhou\n\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport torch\nfrom bisect import bisect_right\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nAVAI_SCH = [\'single_step\', \'multi_step\', \'cosine\', \'multi_step_warmup\']\n\n\ndef build_lr_scheduler(optimizer, lr_scheduler=\'single_step\', stepsize=1, gamma=0.1, max_epoch=1,\n                       frozen=20, warmup=10, warmup_factor_base=0.1):\n    """"""A function wrapper for building a learning rate scheduler.\n\n    Args:\n        optimizer (Optimizer): an Optimizer.\n        lr_scheduler (str, optional): learning rate scheduler method. Default is single_step.\n        stepsize (int or list, optional): step size to decay learning rate. When ``lr_scheduler``\n            is ""single_step"", ``stepsize`` should be an integer. When ``lr_scheduler`` is\n            ""multi_step"", ``stepsize`` is a list. Default is 1.\n        gamma (float, optional): decay rate. Default is 0.1.\n        max_epoch (int, optional): maximum epoch (for cosine annealing). Default is 1.\n\n    Examples::\n        >>> # Decay learning rate by every 20 epochs.\n        >>> scheduler = torchreid.optim.build_lr_scheduler(\n        >>>     optimizer, lr_scheduler=\'single_step\', stepsize=20\n        >>> )\n        >>> # Decay learning rate at 30, 50 and 55 epochs.\n        >>> scheduler = torchreid.optim.build_lr_scheduler(\n        >>>     optimizer, lr_scheduler=\'multi_step\', stepsize=[30, 50, 55]\n        >>> )\n    """"""\n    if lr_scheduler not in AVAI_SCH:\n        raise ValueError(\'Unsupported scheduler: {}. Must be one of {}\'.format(lr_scheduler, AVAI_SCH))\n\n    if lr_scheduler == \'single_step\':\n        if isinstance(stepsize, list):\n            stepsize = stepsize[-1]\n\n        if not isinstance(stepsize, int):\n            raise TypeError(\n                \'For single_step lr_scheduler, stepsize must \'\n                \'be an integer, but got {}\'.format(type(stepsize))\n            )\n\n        scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer, step_size=stepsize, gamma=gamma\n        )\n\n    elif lr_scheduler == \'multi_step\':\n        if not isinstance(stepsize, list):\n            raise TypeError(\n                \'For multi_step lr_scheduler, stepsize must \'\n                \'be a list, but got {}\'.format(type(stepsize))\n            )\n\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n            optimizer, milestones=stepsize, gamma=gamma\n        )\n\n    elif lr_scheduler == \'cosine\':\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, float(max_epoch)\n        )\n    elif lr_scheduler == \'multi_step_warmup\':\n        if not isinstance(stepsize, list):\n            raise TypeError(\n                \'For multi_step lr_scheduler, stepsize must \'\n                \'be a list, but got {}\'.format(type(stepsize))\n            )\n\n        scheduler = MultiStepLRWithWarmUp(\n            optimizer, milestones=stepsize, warmup_iters=warmup, frozen_iters=frozen, gamma=gamma,\n            warmup_factor_base=warmup_factor_base\n        )\n\n    return scheduler\n\n\nclass MultiStepLRWithWarmUp(_LRScheduler):\n    def __init__(self,\n                 optimizer,\n                 milestones,\n                 warmup_iters,\n                 frozen_iters,\n                 warmup_method=\'linear\',\n                 warmup_factor_base=0.1,\n                 gamma=0.1,\n                 last_epoch=-1):\n        if warmup_method not in {\'constant\', \'linear\'}:\n            raise KeyError(\'Unknown warm up method: {}\'.format(warmup_method))\n        self.milestones = sorted(milestones)\n        self.gamma = gamma\n        self.warmup_iters = warmup_iters\n        self.frozen_iters = frozen_iters\n        self.warmup_method = warmup_method\n        self.warmup_factor_base = warmup_factor_base\n        # Base class calls method `step` which increases `last_epoch` by 1 and then calls\n        # method `get_lr` with this value. If `last_epoch` is not equal to -1, we drop\n        # the first step, so to avoid this dropping do small fix by subtracting 1\n        if last_epoch > -1:\n            last_epoch = last_epoch - 1\n        elif last_epoch < -1:\n            raise ValueError(\'Learning rate scheduler got incorrect parameter last_epoch = {}\'.format(last_epoch))\n        super(MultiStepLRWithWarmUp, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        # During warm up change learning rate on every step according to warmup_factor\n        if self.last_epoch < self.frozen_iters:\n            return [base_lr for base_lr in self.base_lrs]\n        if self.last_epoch < self.frozen_iters + self.warmup_iters:\n            if self.warmup_method == \'constant\':\n                warmup_factor = self.warmup_factor_base\n            elif self.warmup_method == \'linear\':\n                alpha = (self.last_epoch - self.frozen_iters) / self.warmup_iters\n                warmup_factor = self.warmup_factor_base * (1 - alpha) + alpha\n            return [base_lr * warmup_factor for base_lr in self.base_lrs]\n        # On the last step of warm up set learning rate equal to base LR\n        elif self.last_epoch == self.frozen_iters + self.warmup_iters:\n            return [base_lr for base_lr in self.base_lrs]\n        # After warm up increase LR according to defined in `milestones` values of steps\n        else:\n            return [base_lr * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n                    for base_lr in self.base_lrs]\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \\\n                        \'[warmup_method = {}, warmup_factor_base = {}, warmup_iters = {},\' \\\n                        \' milestones = {}, gamma = {}]\'.format(self.warmup_method, self.warmup_factor_base,\n                                                               self.warmup_iters, str(list(self.milestones)),\n                                                               self.gamma)\n        return format_string\n'"
pytorch_toolkit/person_reidentification/models/modules/dropout.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Dropout(nn.Module):\n    DISTRIBUTIONS = [\'bernoulli\', \'gaussian\', \'none\']\n\n    def __init__(self, p=0.5, mu=0.5, sigma=0.2, dist=\'bernoulli\'):\n        super(Dropout, self).__init__()\n\n        self.dist = dist\n        assert self.dist in Dropout.DISTRIBUTIONS\n\n        self.p = float(p)\n        assert 0. <= self.p <= 1.\n\n        self.mu = float(mu)\n        self.sigma = float(sigma)\n        assert self.sigma > 0.\n\n    def forward(self, x):\n        if self.dist == \'bernoulli\':\n            out = F.dropout(x, self.p, self.training)\n        elif self.dist == \'gaussian\':\n            if self.training:\n                with torch.no_grad():\n                    soft_mask = x.new_empty(x.size()).normal_(self.mu, self.sigma).clamp_(0., 1.)\n\n                scale = 1. / self.mu\n                out = scale * soft_mask * x\n            else:\n                out = x\n        else:\n            out = x\n\n        return out\n'"
pytorch_toolkit/person_reidentification/models/modules/fpn.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport math\nimport operator\nfrom functools import reduce\n\nfrom torch import nn, Tensor\nfrom torch.nn import init\nimport torch.nn.functional as F\n\n\ndef xavier_fill(tensor):\n    """"""Caffe2 XavierFill Implementation""""""\n    size = reduce(operator.mul, tensor.shape, 1)\n    fan_in = size / tensor.shape[0]\n    scale = math.sqrt(3 / fan_in)\n    return init.uniform_(tensor, -scale, scale)\n\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.dims_in = ()\n        self.scales_in = ()\n        self.dims_out = ()\n        self.scales_out = ()\n\n    def forward(self, *inputs):\n        raise NotImplementedError\n\n\n# FIXME. Move it to a proper place.\ndef duplicate(x, n, copy=False):\n    if copy:\n        return list([x for _ in range(n)])\n    else:\n        return [x, ] * n\n\n\nclass TopDownLateral(nn.Module):\n    def __init__(self, dim_in_top, dim_in_lateral, dim_out, zero_init_lateral=False, group_norm=False):\n        super().__init__()\n        self.dim_in_top = dim_in_top\n        self.dim_in_lateral = dim_in_lateral\n        self.dim_out = dim_out\n        self.zero_init_lateral = zero_init_lateral\n        self.group_norm = group_norm\n\n        self.conv_lateral = nn.Conv2d(dim_in_lateral, self.dim_out, 1, 1, 0)\n\n        self._init_weights()\n\n    def forward(self, top_blob, lateral_blob):\n        # Lateral 1x1 conv\n        lat = self.conv_lateral(lateral_blob)\n        # Top-down 2x upsampling\n        if lat.shape != top_blob.shape:\n            td = F.interpolate(top_blob, scale_factor=2, mode=\'nearest\')\n        else:\n            td = top_blob\n        # Sum lateral and top-down\n        lat += td\n        return lat\n\n    def _init_weights(self):\n        if self.group_norm:\n            conv = self.conv_lateral[0]\n        else:\n            conv = self.conv_lateral\n\n        if self.zero_init_lateral:\n            nn.init.constant_(conv.weight, 0)\n        else:\n            xavier_fill(conv.weight)\n        if conv.bias is not None:\n            nn.init.constant_(conv.bias, 0)\n\n\nclass FPN(FeatureExtractor):\n    def __init__(self, dims_in, scales_in, dims_internal, dims_out, topdown_lateral_block=TopDownLateral,\n                 group_norm=False, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dims_in = dims_in\n        self.scales_in = scales_in\n        self.dims_out = dims_out\n        self.scales_out = list(scales_in)\n        self.scales_out.append(self.scales_out[-1] * 2)\n        self.group_norm = group_norm\n\n        if not isinstance(dims_in, (tuple, list)):\n            dims_in = (dims_in, )\n        n = len(dims_in)\n        if not isinstance(dims_internal, (tuple, list)):\n            dims_internal = duplicate(dims_internal, len(dims_in))\n            self.dims_internal = dims_internal\n        if not isinstance(dims_out, (tuple, list)):\n            dims_out = duplicate(dims_out, len(dims_in))\n        assert len(dims_in) == len(dims_internal) == len(dims_out)\n        self.dims_out = dims_out\n\n        self.conv_top = nn.Conv2d(dims_in[-1], dims_internal[0], 1, 1, 0)\n\n        self.topdown_lateral = nn.ModuleList()\n        self.posthoc = nn.ModuleList()\n        # Add top-down and lateral connections\n        for dim_in, dim_inside in zip(reversed(dims_in[:-1]), reversed(dims_internal[:-1])):\n            self.topdown_lateral.append(topdown_lateral_block(dim_inside, dim_in, dim_inside, group_norm=self.group_norm))\n        # Post-hoc scale-specific 3x3 convs\n        for dim_inside, dim_out in zip(dims_internal, dims_out):\n            self.posthoc.append(nn.Conv2d(dim_inside, dim_out, 3, 1, 1))\n\n        self.extra_maxpool = nn.MaxPool2d(kernel_size=1, stride=2, padding=0)\n        self.init_weights()\n\n    def init_weights(self):\n        for m in (self.conv_top, *self.posthoc):\n            if isinstance(m, nn.Conv2d):\n                xavier_fill(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        if isinstance(x, Tensor):\n            x = [x, ]\n        assert len(self.dims_in) == len(x)\n\n        top_blob = self.conv_top(x[-1])\n        fpn_output_blobs = [self.posthoc[0](top_blob)]\n        for lateral_blob, topdown_lateral_module, posthoc_module in \\\n                zip(reversed(x[:-1]), self.topdown_lateral, self.posthoc[1:]):\n            top_blob = topdown_lateral_module(top_blob, lateral_blob)\n            fpn_output_blobs.append(posthoc_module(top_blob))\n        fpn_output_blobs = [self.extra_maxpool(fpn_output_blobs[0]), ] + fpn_output_blobs\n\n        return list(reversed(fpn_output_blobs))\n'"
pytorch_toolkit/person_reidentification/models/modules/gmp.py,0,"b'""""""\n\n Copyright (c) 2019 mangye16\n\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass GeneralizedMeanPooling(nn.Module):\n    def __init__(self, norm=3, output_size=1, eps=1e-6):\n        super(GeneralizedMeanPooling, self).__init__()\n        assert norm > 0\n        self.p = nn.Parameter(torch.ones(1) * norm)\n        self.output_size = output_size\n        self.eps = eps\n\n    def forward(self, x):\n        x = x.clamp(min=self.eps).pow(self.p)\n        return F.adaptive_avg_pool2d(x, self.output_size).pow(1. / self.p)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' \\\n            + str(self.p) + \', \' \\\n            + \'output_size=\' + str(self.output_size) + \')\'\n'"
pytorch_toolkit/super_resolution/tools/text/infer.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport os\nimport warnings\nimport cv2\nimport skimage\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\ndef parse_args():\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', help=\'Path to checpoint\', required=True, type=str)\n    parser.add_argument(\'--output_dir\', default=None, help=\'Output debugirectory\')\n    parser.add_argument(\'--threshold\', default=125, type=int, help=\'Threshould for postprocessing\')\n    parser.add_argument(\'input_image\', help=\'Image with license plate\')\n    return parser.parse_args()\n\n\ndef image_to_blob(image):\n    blob = image.copy()\n    blob = blob.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n    blob = skimage.img_as_float32(blob)\n    blob = np.array([blob])\n    return torch.from_numpy(blob).float().cuda()\n\n\ndef blob_to_img(blob):\n    blob = blob.cpu().detach().numpy()\n    blob = np.clip(blob, 0.0, 1.0)\n    blob = blob.transpose((1, 2, 0))  # Change data layout from CHW to HWC\n\n    # Suppression skimage warning:\n    #    UserWarning: Possible precision loss when converting from float32 to uint8\n    with warnings.catch_warnings():\n        warnings.simplefilter(\'ignore\')\n        blob = skimage.img_as_ubyte(blob)\n    return blob\n\n\ndef main():\n    args = parse_args()\n\n    # Load model\n    model = torch.load(args.model)[\'model\']\n    model.eval()\n\n    # Prepare input blobs\n    image = cv2.imread(args.input_image, 0)\n\n    assert len(image.shape) == 2\n\n    image = image.reshape(image.shape[0], image.shape[1], 1)\n    blob1 = image_to_blob(image)\n\n    # Inference\n    result = model([Variable(blob1)])\n\n    # Postprocessing\n    out_img = blob_to_img(result[0][0])\n    out_img = np.where(out_img > args.threshold, 255, 0)\n\n    outpur_dir = args.output_dir if args.output_dir else os.path.dirname(args.input_image)\n    out_path = os.path.join(outpur_dir, \'sr_\' + os.path.basename(args.input_image))\n    cv2.imwrite(out_path, out_img)\n    print(\'Saved: \', out_path)\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_toolkit/super_resolution/tools/text/infer_ie.py,0,"b""#!/usr/bin/env python3\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom argparse import ArgumentParser\nimport os\nimport warnings\nimport cv2\nimport skimage\nimport numpy as np\nfrom openvino.inference_engine import IENetwork, IEPlugin\n\n\ndef build_argparser():\n    parser = ArgumentParser()\n    parser.add_argument('--model', type=str, required=True, help='Path to xml file with a trained model')\n    parser.add_argument('--threshold', default=125, type=int, help='Threshould for postprocessing')\n    parser.add_argument('--device', help='Specify the target device to infer on. (default: %(default)s)',\n                        choices=['CPU', 'GPU', 'MYRIAD'], default='CPU')\n    parser.add_argument('--output_dir', default=None, help='Output debugirectory')\n    parser.add_argument('input_image', help='Image')\n    return parser.parse_args()\n\n\ndef load_ir_model(model_xml, device, input_shape=None):\n    model_bin = os.path.splitext(model_xml)[0] + '.bin'\n\n    # initialize plugin and read IR\n    plugin = IEPlugin(device=device)\n    net = IENetwork(model=model_xml, weights=model_bin)\n    input_blobs = list(net.inputs.keys())\n\n    if input_shape:\n        net.reshape({input_blobs[0]: input_shape})\n\n    inputs = [(b, net.inputs[b].shape) for b in input_blobs]\n    out_blob = next(iter(net.outputs))\n    exec_net = plugin.load(network=net)\n    del net\n\n    return exec_net, inputs, out_blob\n\n\ndef image_to_blob(image, shape):\n    blob = image.copy()\n    blob = blob.reshape(shape)\n    return blob\n\n\ndef blob_to_img(blob):\n    blob = blob[0] # num_channels = 1\n    blob = np.clip(blob, 0.0, 1.0)\n\n    # Suppression skimage warning:\n    #    UserWarning: Possible precision loss when converting from float32 to uint8\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        blob = skimage.img_as_ubyte(blob)\n    return blob\n\ndef main():\n    args = build_argparser()\n\n    # Read image in gray scale format\n    image = cv2.imread(args.input_image, 0)\n    assert len(image.shape) == 2\n\n    # Prepare network\n    exec_net, inputs, out_blob = load_ir_model(args.model, args.device,\n                                               (1, 1, image.shape[0], image.shape[1]))\n\n    blob1 = image_to_blob(image, (inputs[0][1]))\n\n    # inference\n    result = exec_net.infer(inputs={inputs[0][0]: blob1})\n\n    # Postprocessing\n    out_img = blob_to_img(result[out_blob][0])\n    out_img = np.where(out_img > args.threshold, 255, 0)\n\n    # Save image\n    outpur_dir = args.output_dir if args.output_dir else os.path.dirname(args.input_image)\n    out_path = os.path.join(outpur_dir, 'sr_' + os.path.basename(args.input_image))\n    cv2.imwrite(out_path, out_img)\n    print('Saved: ', out_path)\n\nif __name__ == '__main__':\n    main()\n"""
pytorch_toolkit/text_spotting/text_spotting/data/__init__.py,0,b''
pytorch_toolkit/text_spotting/text_spotting/data/alphabet.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nclass AlphabetDecoder:\n    """"""\n    This class is used to encode text to numerical sequence and\n    to decode it back to symbolic representation.\n    """"""\n\n    NOT_FOUND = -1\n\n    def __init__(self, alphabet=None, make_lower=True):\n        if not alphabet:\n            alphabet = ""0123456789abcdefghijklmnopqrstuvwxyz""\n        self.sos = ""<""\n        assert self.sos not in alphabet\n\n        self.eos = \'>\'\n        assert self.eos not in alphabet\n\n        alphabet = self.sos + self.eos + alphabet\n        self.alphabet = alphabet\n        self.make_lower = make_lower\n\n    def encode(self, input_string):\n        """""" Encodes text to numerical representation. """"""\n\n        if not input_string:\n            return None\n\n        input_string = input_string.strip()\n\n        if self.make_lower:\n            input_string = input_string.lower()\n\n        if self.sos in input_string or self.eos in input_string:\n            return None\n\n        res = [self.alphabet.find(character) for character in input_string]\n\n        res.append(self.alphabet.find(self.eos))\n\n        if AlphabetDecoder.NOT_FOUND in res:\n            return None\n\n        return res\n\n    def decode(self, numbers):\n        """""" Decoders numerical representation to text. """"""\n        if numbers is None:\n            return None\n\n        output_string = """"\n        for number in numbers:\n            assert number < len(self.alphabet), ""number = {}"".format(number)\n            if self.alphabet[number] == self.eos:\n                break\n            output_string += self.alphabet[number]\n\n        return output_string\n\n    def encode_batch(self, strings):\n        """""" Encodes batch of texts to numerical representations. """"""\n        return [self.encode(string) for string in strings]\n\n    def batch_decode(self, numbers):\n        """""" Decodes batch of numerical representations to texts. """"""\n        return [self.decode(numbers) for numbers in numbers]\n\n    def string_in_alphabet(self, string):\n        """""" Returns True if all characters of input string are in alphabet. """"""\n        return self.encode(string) is not None\n'"
pytorch_toolkit/text_spotting/text_spotting/data/transforms.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport copy\nimport random\n\nimport cv2\nimport numpy as np\n\n\nclass RandomRotate90:\n    """""" Class for training sample random rotation by 90, 180, 270 degrees. """"""\n\n    @staticmethod\n    def rotate_image(image, angle):\n        """""" Rotates an image (angle in degrees) and expands image to avoid cropping. """"""\n\n        height, width = image.shape[:2]\n        image_center = (width / 2, height / 2)\n\n        rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.)\n\n        abs_cos = abs(rotation_mat[0, 0])\n        abs_sin = abs(rotation_mat[0, 1])\n\n        bound_w = int(height * abs_sin + width * abs_cos)\n        bound_h = int(height * abs_cos + width * abs_sin)\n\n        rotation_mat[0, 2] += bound_w / 2 - image_center[0]\n        rotation_mat[1, 2] += bound_h / 2 - image_center[1]\n\n        image = cv2.warpAffine(image, rotation_mat, (bound_w, bound_h))\n\n        return image\n\n    @staticmethod\n    def rotate_point_by_90(x_coord, y_coord, width, height, rotate_by_90_k_times):\n        """""" Rotate point by 90 degrees (clockwise). """"""\n\n        cos = [1.0, 0.0, -1.0, 0.0]\n        sin = [0.0, -1.0, 0.0, 1.0]\n\n        x1_coord = x_coord - 0.5 * width\n        y1_coord = y_coord - 0.5 * height\n\n        if rotate_by_90_k_times % 2 == 1:\n            width, height = height, width\n\n        x_coord = x1_coord * cos[rotate_by_90_k_times] - y1_coord * sin[\n            rotate_by_90_k_times] + 0.5 * width\n        y_coord = x1_coord * sin[rotate_by_90_k_times] + y1_coord * cos[\n            rotate_by_90_k_times] + 0.5 * height\n\n        return x_coord, y_coord\n\n    def __call__(self, sample):\n        angle = random.choice([0, 90, 270])\n        if angle:\n            height, width = sample[\'image\'].shape[:2]\n            sample[\'image\'] = self.rotate_image(sample[\'image\'], angle)\n\n            # Rotate boxes.\n            if \'gt_boxes\' in sample:\n                boxes = sample[\'gt_boxes\']\n                boxes[:, 0], boxes[:, 1] = self.rotate_point_by_90(boxes[:, 0], boxes[:, 1],\n                                                                   width, height, angle // 90)\n                boxes[:, 2], boxes[:, 3] = self.rotate_point_by_90(boxes[:, 2], boxes[:, 3],\n                                                                   width, height, angle // 90)\n\n                boxes[:, 0], boxes[:, 2] = np.minimum(boxes[:, 0], boxes[:, 2]), np.maximum(\n                    boxes[:, 0], boxes[:, 2])\n                boxes[:, 1], boxes[:, 3] = np.minimum(boxes[:, 1], boxes[:, 3]), np.maximum(\n                    boxes[:, 1], boxes[:, 3])\n\n                sample[\'gt_boxes\'] = boxes\n\n            # Rotate masks.\n            if \'gt_masks\' in sample:\n                polygons = sample[\'gt_masks\']\n                for i, obj in enumerate(polygons):\n                    for j, _ in enumerate(obj):\n                        polygons[i][j][:, 0], polygons[i][j][:, 1] = self.rotate_point_by_90(\n                            polygons[i][j][:, 0], polygons[i][j][:, 1], width, height, angle // 90\n                        )\n\n                sample[\'gt_masks\'] = polygons\n\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__\n        return format_string\n\n\nclass ColorJitter:\n    """""" Class for training sample color jittering. """"""\n\n    @staticmethod\n    def adjust_brightness(image, value):\n        \'\'\' Adjusts brightness of input image by value. \'\'\'\n\n        return np.clip(image + value, 0.0, 1.0)\n\n    def random_brightness(self, image):\n        \'\'\' Adjusts brightness of input image by random value. \'\'\'\n\n        max_delta = 32. / 255.\n        value = random.uniform(-max_delta, max_delta)\n        return self.adjust_brightness(image, value)\n\n    @staticmethod\n    def adjust_contrast(image, value):\n        \'\'\' Adjusts contrast of input image by value. \'\'\'\n\n        return np.clip(image * value, 0.0, 1.0)\n\n    def random_contrast(self, image):\n        \'\'\' Adjusts contrast of input image by random value. \'\'\'\n\n        lower = 0.5\n        upper = 1.5\n        value = random.uniform(lower, upper)\n        return self.adjust_contrast(image, value)\n\n    @staticmethod\n    def adjust_hue(image, value):\n        \'\'\' Adjusts hue of input image by value. \'\'\'\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        image[:, :, 0] += value * 255.0\n        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        return np.clip(image, 0.0, 1.0)\n\n    def random_hue(self, image):\n        \'\'\' Adjusts hue of input image by random value. \'\'\'\n\n        max_delta = 0.2\n        value = random.uniform(-max_delta, max_delta)\n        return self.adjust_hue(image, value)\n\n    @staticmethod\n    def adjust_saturation(image, value):\n        \'\'\' Adjusts saturation of input image by value. \'\'\'\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        image[:, :, 1] *= value\n        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        return np.clip(image, 0.0, 1.0)\n\n    def random_saturation(self, image):\n        \'\'\' Adjusts saturation of input image by random value. \'\'\'\n\n        lower = 0.5\n        upper = 1.5\n        value = random.uniform(lower, upper)\n        return self.adjust_saturation(image, value)\n\n    def __call__(self, sample):\n        assert sample[\'image\'].dtype == np.uint8, sample[\'image\'].dtype\n\n        sample[\'image\'] = sample[\'image\'].astype(np.float32) / 255.0\n\n        transforms = [\n            self.random_brightness,\n            self.random_contrast,\n            self.random_saturation,\n            self.random_hue\n        ]\n\n        random.shuffle(transforms)\n\n        for transform in transforms:\n            sample[\'image\'] = transform(sample[\'image\'])\n\n        sample[\'image\'] = (sample[\'image\'] * 255.0).astype(np.uint8)\n\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__\n        return format_string\n\n\nclass AlphabetDecodeTransform:\n    """""" Class for applying alphabet encoding. """"""\n\n    def __init__(self, alphabet_decoder):\n        self.decoder = alphabet_decoder\n\n    def __call__(self, sample):\n        if \'gt_texts\' not in sample:\n            return sample\n        gt_texts = sample[\'gt_texts\']\n        assert isinstance(gt_texts, list)\n        sample[\'gt_texts_str\'] = copy.deepcopy(gt_texts)\n        sample[\'gt_texts\'] = self.decoder.encode_batch(gt_texts)\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__\n        format_string += f""[alphabet = \'{self.decoder.alphabet}\',sos = \'{self.decoder.sos}\']""\n        return format_string\n\n\nclass FilterTextByArea:\n    """""" Class for filtering out too small text instances. """"""\n\n    def __init__(self, min_area):\n        self.min_area = min_area\n\n    def __call__(self, sample):\n        if \'gt_texts\' not in sample:\n            return sample\n        gt_texts = sample[\'gt_texts\']\n        assert isinstance(gt_texts, list)\n\n        for i, _ in enumerate(zip(sample[\'gt_texts\'])):\n            if cv2.contourArea(sample[\'gt_masks\'][i][0].astype(np.int32)) < self.min_area:\n                sample[\'gt_texts\'][i] = None\n\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__\n        format_string += f""[min_area = \'{self.min_area}\']""\n        return format_string\n\n\nclass FilterTextByLength:\n    """""" Class for filtering out too short text instances. """"""\n\n    def __init__(self, max_length):\n        self.max_length = max_length\n\n    def __call__(self, sample):\n        if \'gt_texts\' not in sample:\n            return sample\n        gt_texts = sample[\'gt_texts\']\n        assert isinstance(gt_texts, list)\n\n        for i, _ in enumerate(zip(sample[\'gt_texts\'])):\n            if sample[\'gt_texts\'][i] is not None and len(sample[\'gt_texts\'][i]) > self.max_length:\n                sample[\'gt_texts\'][i] = None\n\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__\n        format_string += f""[max_length = \'{self.max_length}\']""\n        return format_string\n\n\nclass RandomCrop:\n    """""" Class for random cropping. """"""\n\n    OUTSIDE = 0\n    INSIDE = 1\n    ON_BORDER = 2\n\n    def __init__(self, min_relative_size, attempts):\n        self.min_relative_size = min_relative_size\n        self.attempts = attempts\n\n    @staticmethod\n    def position(box, crop_height, crop_width):\n        if box[0] <= 0 and box[1] <= 0 and box[2] <= 0 and box[3] <= 0:\n            return RandomCrop.OUTSIDE\n        elif box[0] >= crop_width and box[1] >= crop_height and box[2] >= crop_width and box[\n            3] >= crop_height:\n            return RandomCrop.OUTSIDE\n        elif box[0] >= 0 and box[1] >= 0 and box[2] < crop_width and box[3] < crop_height:\n            return RandomCrop.INSIDE\n        else:\n            return RandomCrop.ON_BORDER\n\n    def __call__(self, sample):\n        height, width = sample[\'image\'].shape[:2]\n\n        for attempt in range(self.attempts):\n            crop_height = random.randint(int(self.min_relative_size * height), height)\n            crop_width = random.randint(int(self.min_relative_size * width), width)\n\n            crop_min_y = random.randint(0, height - crop_height)\n            crop_min_x = random.randint(0, width - crop_width)\n\n            if \'gt_boxes\' in sample:\n                boxes = np.copy(sample[\'gt_boxes\'])\n                boxes[:, 0] -= crop_min_x\n                boxes[:, 1] -= crop_min_y\n                boxes[:, 2] -= crop_min_x\n                boxes[:, 3] -= crop_min_y\n\n                positions = np.array(\n                    [RandomCrop.position(box, crop_height, crop_width) for box in boxes])\n                if RandomCrop.ON_BORDER in positions:\n                    continue\n                if RandomCrop.INSIDE not in positions:\n                    continue\n\n                sample[\'gt_texts\'] = np.array(sample[\'gt_texts\'])[positions == RandomCrop.INSIDE]\n                sample[\'gt_texts\'] = list(sample[\'gt_texts\'])\n                sample[\'gt_boxes\'] = boxes[positions == RandomCrop.INSIDE]\n                sample[\'gt_masks\'] = np.array(sample[\'gt_masks\'])[positions == RandomCrop.INSIDE]\n                sample[\'gt_classes\'] = sample[\'gt_classes\'][positions == RandomCrop.INSIDE]\n\n                polygons = list(sample[\'gt_masks\'])\n                for i, obj in enumerate(polygons):\n                    for j, _ in enumerate(obj):\n                        polygons[i][j][:, 0] -= crop_min_x\n                        polygons[i][j][:, 1] -= crop_min_y\n\n                sample[\'gt_masks\'] = polygons\n\n                sample[\'image\'] = sample[\'image\'][crop_min_y:crop_min_y + crop_height,\n                                  crop_min_x:crop_min_x + crop_width]\n\n                break\n\n        return sample\n\n\nclass Visualize:\n    """""" Visualizes image with rectangles, masks, texts using cv2.imshow. """"""\n\n    def __init__(self, delay):\n        self.delay = delay\n\n    def __call__(self, sample):\n        image = sample[\'image\'].copy()\n        for box, mask, text in zip(sample[\'gt_boxes\'], sample[\'gt_masks\'], sample[\'gt_texts\']):\n            xmin, ymin, xmax, ymax = [int(x) for x in box]\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 255, 255), 2)\n            cv2.drawContours(image, np.array(mask).astype(np.int32), -1, (255, 255, 255), 2)\n            cv2.putText(image, text, (xmin, ymin), 1, 1, (255, 255, 255), 2)\n        cv2.imshow(\'image\', image)\n        cv2.waitKey(self.delay)\n\n        return sample\n\n    def __repr__(self):\n        format_string = self.__class__.__name__\n        format_string += f""[delay = \'{self.delay}\']""\n        return format_string\n'"
pytorch_toolkit/text_spotting/text_spotting/datasets/__init__.py,0,b''
pytorch_toolkit/text_spotting/text_spotting/datasets/coco_text.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport os.path as osp\n\nimport cv2\nimport numpy as np\n\nimport torch\nfrom segmentoly.datasets.coco import COCODataset\nfrom .text_evaluation import text_eval, masks_to_rects\n\n\nclass COCOTextDataset(COCODataset):\n    """""" Class for representing text dataset in MSCOCO format. """"""\n\n    def __init__(self, root_dir_path, ann_file_path, with_gt, remove_images_without_gt,\n                 transforms=None, remove_images_without_text=False, alphabet_decoder=None):\n        super().__init__(root_dir_path, ann_file_path, with_gt, remove_images_without_gt,\n                         transforms)\n\n        self.remove_images_without_text = remove_images_without_text\n        self.alphabet_decoder = alphabet_decoder\n        if self.remove_images_without_text:\n            self._remove_images_wihout_text()\n\n        self.gt_annotations = []\n        self.file_names = []\n        for index, _ in enumerate(self.ids):\n            ann_ids = self.coco.getAnnIds(imgIds=self.ids[index], iscrowd=None)\n            anno = self.coco.loadAnns(ann_ids)\n            path = self.coco.loadImgs(self.ids[index])[0][\'file_name\']\n            self.file_names.append(osp.join(self.root_dir_path, path))\n\n            gt_annotation = []\n            for obj in anno:\n                gt_annotation.append(\n                    {\'points\': obj[\'segmentation\'][0],\n                     \'transcription\': \'###\' if obj[\'iscrowd\'] else obj[\'text\'][\'transcription\']})\n\n            self.gt_annotations.append(gt_annotation)\n\n    def _remove_images_wihout_text(self):\n        ids_with_text = []\n        for index, _ in enumerate(self.ids):\n            img_id = self.ids[index]\n\n            ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n            anno = self.coco.loadAnns(ann_ids)\n\n            texts = [obj.get(\'text\', {}).get(\'transcription\', """") for obj in anno]\n\n            if self._any_text_valid(texts, self.alphabet_decoder):\n                ids_with_text.append(img_id)\n\n        self.ids = ids_with_text\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n        ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=False)\n        anno = self.coco.loadAnns(ann_ids)\n        path = self.coco.loadImgs(img_id)[0][\'file_name\']\n        original_image = cv2.imread(osp.join(self.root_dir_path, path))\n\n        if original_image is None:\n            raise Exception(f\'Failed to read: {osp.join(self.root_dir_path, path)}\')\n\n        height, width = original_image.shape[:2]\n\n        sample = {\'image\': original_image, \'path\': osp.join(self.root_dir_path, path)}\n        if self.with_gt:\n            boxes = [obj[\'bbox\'] for obj in anno]\n            boxes = np.asarray(boxes, dtype=np.float32).reshape(-1, 4)\n            if len(boxes) > 0:\n                boxes[:, 2] += boxes[:, 0] - 1\n                boxes[:, 3] += boxes[:, 1] - 1\n                boxes = np.clip(boxes, 0, [width - 1, height - 1, width - 1, height - 1])\n                sample[\'gt_boxes\'] = boxes\n\n            classes = [obj[\'category_id\'] for obj in anno]\n            classes = [self.json_category_id_to_contiguous_id[c] for c in classes]\n            classes = np.asarray(classes)\n            assert len(classes) == len(boxes)\n            if len(classes) > 0:\n                sample[\'gt_classes\'] = classes\n\n            polygons = [obj[\'segmentation\'] for obj in anno]\n            polygons = [\n                [np.clip(np.asarray(part, dtype=np.float32).reshape(-1, 2), 0,\n                         [[width - 1, height - 1]])\n                 for part in obj]\n                for obj in polygons]\n            assert len(polygons) == len(boxes)\n            if len(polygons) > 0:\n                sample[\'gt_masks\'] = polygons\n\n            gt_texts = [obj.get(\'text\', {}).get(\'transcription\', """") for obj in anno]\n            assert len(gt_texts) == len(boxes)\n            if len(gt_texts) > 0:\n                sample[\'gt_texts\'] = gt_texts\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        out_sample = dict(index=index,\n                          original_image=original_image,\n                          meta=dict(original_size=[height, width],\n                                    processed_size=sample[\'image\'].shape[1:3]),\n                          im_data=sample[\'image\'],\n                          im_info=torch.as_tensor([sample[\'image\'].shape[1],\n                                                   sample[\'image\'].shape[2],\n                                                   1.0],\n                                                  dtype=torch.float32),\n                          path=sample[\'path\'])\n        if self.with_gt:\n            out_sample.update(dict(gt_boxes=sample[\'gt_boxes\'],\n                                   gt_labels=sample[\'gt_classes\'],\n                                   gt_masks=sample[\'gt_masks\'],\n                                   gt_is_ignored=torch.zeros(sample[\'gt_classes\'].shape,\n                                                             dtype=torch.int32),\n                                   gt_texts=sample[\'gt_texts\'],\n                                   gt_texts_str=sample.get(\'gt_texts_str\')\n                                   ))\n\n        return out_sample\n\n    def prepare_predictions(self, masks, text_log_softmax):\n        """"""\n        Transforms masks to polygons and decodes encoded text making predictions suitable for\n        evaluation.\n        """"""\n\n        predictions = []\n        for i, image_masks in enumerate(masks):\n            rectangles = []\n            if image_masks:\n                rectangles = masks_to_rects(image_masks, is_rle=True)\n                rectangles = [{\'points\': bbox.reshape([-1]), \'confidence\': 1.0} for bbox in\n                              rectangles]\n                if text_log_softmax:\n                    if isinstance(text_log_softmax[i], np.ndarray):\n                        texts = text_log_softmax[i]\n                    else:\n                        texts = text_log_softmax[i].cpu().numpy()\n                    encoded_transcriptions = np.argmax(texts, 2)\n                    transcriptions = self.alphabet_decoder.batch_decode(encoded_transcriptions)\n                    for rectangle, transcription in zip(rectangles, transcriptions):\n                        rectangle[\'transcription\'] = transcription\n\n            predictions.append(rectangles)\n\n        return predictions\n\n    def dump_predictions(self, folder, predictions):\n        """"""\n        Dumps prediction to folder as per image text files.\n        :param folder: dump folder\n        :param predictions: list of predictions\n        :return:\n        """"""\n        os.makedirs(folder, exist_ok=True)\n        for image_path, pr_annotation in zip(self.file_names, predictions):\n            with open(osp.join(folder, f\'res_{osp.basename(image_path)[:-3]}txt\'), \'w\') as file:\n                for obj in pr_annotation:\n                    file.write(\',\'.join([str(c) for c in obj[\'points\']]))\n                    if \'transcription\' in obj.keys():\n                        transcription = obj[\'transcription\']\n                        file.write(f\',{transcription}\')\n                    file.write(\'\\n\')\n\n    def evaluate(self, scores, classes, boxes, masks, text_log_softmax=None, output_dir=\'\',\n                 iou_types=(\'bbox\', \'segm\'), dump=None, visualize=False):\n        """"""\n        Runs COCO procedure to evaluate mAP of detection and segmentation as well as\n        ICDAR procedure to evaluate text detection and word spotting recall, recall and\n        their harmonic mean (hmean).\n        """"""\n        flat_results = super().evaluate(scores, classes, boxes, masks, output_dir, iou_types)\n\n        predictions = self.prepare_predictions(masks, text_log_softmax)\n\n        recall, precision, hmean, _ = text_eval(predictions, self.gt_annotations,\n                                                use_transcriptions=False)\n        print(\' Text detection recall={} precision={} hmean={}\'.format(recall, precision, hmean))\n        flat_results[\'text_detection/recall\'] = recall\n        flat_results[\'text_detection/precision\'] = precision\n        flat_results[\'text_detection/hmean\'] = hmean\n\n        if text_log_softmax:\n            paths = []\n            if visualize:\n                for img_id in self.ids:\n                    path = self.coco.loadImgs(img_id)[0][\'file_name\']\n                    paths.append(osp.join(self.root_dir_path, path))\n                assert len(predictions) == len(self.gt_annotations)\n                assert len(predictions) == len(paths), f\'len(predictions)={len(predictions)}, len(paths)={len(paths)}\'\n\n            recall, precision, hmean, _ = text_eval(predictions, self.gt_annotations, paths,\n                                                    imshow_delay=0, use_transcriptions=True)\n\n            print(\n                \' Text spotting  recall={} precision={} hmean={}\'.format(recall, precision, hmean))\n            flat_results[\'text_spotting/recall\'] = recall\n            flat_results[\'text_spotting/precision\'] = precision\n            flat_results[\'text_spotting/hmean\'] = hmean\n\n        if dump:\n            self.dump_predictions(dump, predictions)\n\n        return flat_results\n\n    @staticmethod\n    def _any_text_valid(texts, alphabet_decoder=None):\n        if not texts:\n            return False\n        if all((not t) for t in texts):\n            return False\n        if alphabet_decoder is None:\n            return True\n        return any(alphabet_decoder.string_in_alphabet(t) for t in texts)\n'"
pytorch_toolkit/text_spotting/text_spotting/datasets/datasets.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport copy\nimport json\nimport os\nfrom collections import defaultdict\n\nimport cv2\nimport imagesize\nimport numpy as np\nfrom tqdm import tqdm\n\n\nclass TextOnlyCocoAnnotation:\n    """""" Class for working with MSCOCO-like annotation for text. """"""\n\n    def __init__(self, path=None, root=\'\'):\n\n        self.label_map = {\'text\': 1}\n\n        self.annotation = {\n            ""type"": ""instances"",\n            ""images"": [],\n            ""categories"": [],\n            ""annotations"": [],\n        }\n        self.annotation[\'categories\'] = [{""supercategory"": ""none"", ""name"": key, ""id"": value}\n                                         for key, value in self.label_map.items()]\n\n        self.annotation[\'categories\'] = sorted(self.annotation[\'categories\'],\n                                               key=lambda x: x[""id""])\n\n        if path is not None:\n            assert os.path.exists(path), path\n            with open(path) as read_file:\n                self.annotation = json.load(read_file)\n\n        if root:\n            for image_info in self.annotation[\'images\']:\n                image_info[\'file_name\'] = os.path.join(root, image_info[\'file_name\'])\n\n        self.img_id_2_ann_id = defaultdict(list)\n        for index, ann in enumerate(self.annotation[\'annotations\']):\n            assert index == ann[\'id\']\n            self.img_id_2_ann_id[ann[\'image_id\']].append(ann[\'id\'])\n\n        self.img_path_2_img_id = dict()\n        for index, img in enumerate(self.annotation[\'images\']):\n            assert index == img[\'id\']\n            self.img_path_2_img_id[img[\'file_name\']] = index\n\n    def add_bbox(self, image_path, image_size, obj):\n        """""" Adds new text object to annotation. """"""\n\n        if image_path not in self.img_path_2_img_id:\n            self.img_path_2_img_id[image_path] = len(self.img_path_2_img_id)\n\n            self.annotation[\'images\'].append({\n                ""file_name"": image_path,\n                ""height"": image_size[1],\n                ""width"": image_size[0],\n                ""id"": self.img_path_2_img_id[image_path]\n            })\n\n        new_ann_id = len(self.annotation[\'annotations\'])\n        self.img_id_2_ann_id[self.img_path_2_img_id[image_path]].append(new_ann_id)\n        self.annotation[\'annotations\'].append({\n\n            ""bbox"": obj[\'bbox\'],  # x, y, w, h\n            ""segmentation"": obj[\'segmentation\'],\n            ""text"": obj[\'text\'],\n\n            ""ignore"": 0,\n            ""id"": new_ann_id,\n            ""image_id"": self.img_path_2_img_id[image_path],\n            ""area"": obj[\'bbox\'][2] * obj[\'bbox\'][3],\n            ""iscrowd"": 1 - int(obj[\'text\'][\'legible\']),\n            ""category_id"": self.label_map[\'text\']\n        })\n\n    def __iadd__(self, other):\n\n        for image_info in other.annotation[\'images\']:\n            ann_ids = other.img_id_2_ann_id[image_info[\'id\']]\n            for ann_id in ann_ids:\n                ann = other.annotation[\'annotations\'][ann_id]\n                self.add_bbox(image_info[\'file_name\'], (image_info[\'width\'], image_info[\'height\']),\n                              copy.deepcopy(ann))\n        return self\n\n    def write(self, path):\n        """""" Writes annotation as json file. """"""\n\n        annotation = copy.deepcopy(self.annotation)\n\n        for image_info in annotation[\'images\']:\n            image_info[\'file_name\'] = os.path.relpath(image_info[\'file_name\'],\n                                                      os.path.dirname(path))\n\n        with open(path, \'w\') as read_file:\n            json.dump(annotation, read_file)\n\n    @staticmethod\n    def _check_object_consistency(obj):\n        assert obj[\'iscrowd\'] == 1 - obj[\'text\'][\'legible\']\n\n    def visualize(self, put_text, imshow_delay=1):\n        """""" Visualizes annotation using cv2.imshow from OpenCV. Press `Esc` to exit. """"""\n\n        max_image_size = 1280, 768\n\n        for frame in tqdm(self.annotation[\'images\']):\n            image_path = frame[\'file_name\']\n            image = cv2.imread(image_path)\n            for ann_id in self.img_id_2_ann_id[frame[\'id\']]:\n                obj = self.annotation[\'annotations\'][ann_id]\n                lwd = 2\n                color = (0, 255, 0)\n                if obj[\'iscrowd\']:\n                    color = (128, 128, 128)\n                bbox = obj[\'bbox\']\n                if put_text:\n                    cv2.putText(image, obj[\'text\'][\'transcription\'], tuple(bbox[0:2]), 1, 1.0,\n                                color)\n                cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]),\n                              color, lwd)\n\n                contours = np.array(obj[\'segmentation\'])\n                contours = contours.reshape([contours.shape[0], contours.shape[1] // 2, 2])\n\n                cv2.drawContours(image, contours, 0, color, 1)\n            try:\n                if image.shape[1] > max_image_size[0] or image.shape[0] > max_image_size[1]:\n                    print(\'resized\')\n                    image = cv2.resize(image, max_image_size)\n                cv2.imshow(\'image\', image)\n                k = cv2.waitKey(imshow_delay)\n                if k == 27:\n                    break\n            except:\n                print(\'Error: image is empty or corrupted: \', frame[\'file_name\'])\n\n    def extract_text_recognition_dataset(self, path):\n        """"""  Crops text instances and saves as another dataset. """"""\n\n        os.makedirs(os.path.join(path, \'images\'))\n\n        annotation = []\n\n        for frame in tqdm(self.annotation[\'images\']):\n            image = cv2.imread(frame[\'file_name\'], cv2.IMREAD_IGNORE_ORIENTATION | cv2.IMREAD_COLOR)\n            for ann_id in self.img_id_2_ann_id[frame[\'id\']]:\n                obj = self.annotation[\'annotations\'][ann_id]\n                if obj[\'text\'][\'legible\']:\n                    bbox = obj[\'bbox\']\n                    try:\n                        transcription = obj[\'text\'][\'transcription\']\n                        if transcription.isalnum():\n                            coord_x1, coord_y1, coord_x2, coord_y2 = bbox[0], bbox[1], bbox[0] + \\\n                                                                     bbox[2], bbox[1] + bbox[3]\n                            coord_x1 = max(0, coord_x1)\n                            coord_x2 = min(image.shape[1] - 1, coord_x2)\n                            coord_y1 = max(0, coord_y1)\n                            coord_y2 = min(image.shape[0] - 1, coord_y2)\n                            crop_path = os.path.join(path, \'images\', f\'image{len(annotation)}.jpg\')\n                            annotation.append(f\'{crop_path} {transcription}\')\n                            cv2.imwrite(crop_path, image[coord_y1:coord_y2, coord_x1:coord_x2])\n                    except:\n                        print(\'Something went wrong with\', frame[\'file_name\'])\n                        break\n\n        with open(os.path.join(path, \'annotation.txt\'), \'w\') as file:\n            file.write(\'\\n\'.join(annotation))\n\n\nclass ICDAR2013DatasetConverter:\n    """""" Class for conversion of ICDAR2013 to TextOnlyCocoAnnotation. """"""\n\n    def __init__(self, images_folder, annotations_folder, is_train, root=\'\'):\n        self.images_folder = images_folder\n        self.annotations_folder = annotations_folder\n        self.is_train = is_train\n\n        if root:\n            self.annotations_folder = os.path.join(root, self.annotations_folder)\n            self.images_folder = os.path.join(root, self.images_folder)\n\n    def __call__(self, *args, **kwargs):\n        dataset = TextOnlyCocoAnnotation()\n\n        begin, end = (100, 328 + 1) if self.is_train else (1, 233 + 1)\n        gt_format = \'gt_{}.txt\' if self.is_train else \'gt_img_{}.txt\'\n        img_format = \'{}.jpg\' if self.is_train else \'img_{}.jpg\'\n\n        for i in range(begin, end):\n            image_path = os.path.join(self.images_folder, img_format.format(i))\n            annotation_path = os.path.join(self.annotations_folder, gt_format.format(i))\n\n            with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                for line in [line.strip() for line in read_file.readlines()]:\n                    image_size = imagesize.get(image_path)\n                    dataset.add_bbox(image_path, image_size, self.parse_line(line))\n\n        return dataset\n\n    def parse_line(self, line):\n        """""" Parses line of ICDAR2013 annotation. """"""\n\n        sep = \' \' if self.is_train else \', \'\n        line = line.split(sep)\n        xmin, ymin, xmax, ymax = [int(x) for x in line[:4]]\n        assert xmin < xmax\n        assert ymin < ymax\n        transcription = (sep.join(line[4:]))[1:-1]\n        word_annotation = {\n            \'bbox\': [xmin, ymin, xmax - xmin + 1, ymax - ymin + 1],\n            \'segmentation\': [[xmin, ymin, xmax, ymin, xmax, ymax, xmin, ymax]],\n            \'text\': {\n                \'transcription\': transcription,\n                \'legible\': 1,\n                \'language\': \'english\',\n            }\n        }\n        return word_annotation\n\n\nclass ICDAR2015DatasetConverter:\n    """""" Class for conversion of ICDAR2015 to TextOnlyCocoAnnotation. """"""\n\n    def __init__(self, images_folder, annotations_folder, is_train, root=\'\'):\n        self.images_folder = images_folder\n        self.annotations_folder = annotations_folder\n        self.is_train = is_train\n\n        if root:\n            self.annotations_folder = os.path.join(root, self.annotations_folder)\n            self.images_folder = os.path.join(root, self.images_folder)\n\n    @staticmethod\n    def parse_line(line):\n        """""" Parses line of ICDAR2015 annotation. """"""\n\n        line = line.split(\',\')\n        quadrilateral = [int(x) for x in line[:8]]\n        transcription = \',\'.join(line[8:])\n        legible = 1\n        language = \'english\'\n        if transcription == \'###\':\n            transcription = \'\'\n            legible = 0\n            language = \'\'\n\n        xmin = min(quadrilateral[0::2])\n        xmax = max(quadrilateral[0::2])\n\n        ymin = min(quadrilateral[1::2])\n        ymax = max(quadrilateral[1::2])\n\n        word_annotation = {\n            \'bbox\': [xmin, ymin, xmax - xmin + 1, ymax - ymin + 1],\n            \'segmentation\': [quadrilateral],\n            \'text\': {\n                \'transcription\': transcription,\n                \'legible\': legible,\n                \'language\': language,\n            }\n        }\n        return word_annotation\n\n    def __call__(self, *args, **kwargs):\n        """""" Converts annotation from ICDAR 2015 format to internal format. """"""\n\n        dataset = TextOnlyCocoAnnotation()\n\n        n_images = 1000 if self.is_train else 500\n        for i in range(1, n_images + 1):\n            image_path = os.path.join(self.images_folder, \'img_{}.jpg\'.format(i))\n            annotation_path = os.path.join(self.annotations_folder, \'gt_img_{}.txt\'.format(i))\n\n            with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                content = [line.strip() for line in read_file.readlines()]\n                for line in content:\n                    dataset.add_bbox(image_path, imagesize.get(image_path), self.parse_line(line))\n\n        return dataset\n\n\nclass ICDAR2017MLTDatasetConverter:\n    """""" Class for conversion of ICDAR2017 to TextOnlyCocoAnnotation. """"""\n\n    def __init__(self, folder, subset, is_latin_required, root=\'\'):\n        \'\'\'\n        Converts ICDAR2017 MLT to TextOnlyCocoAnnotation\n        :param folder: Folder with extracted zip archives containing images and annotation.\n        :param subset: \'train\' or \'val\'\n        :param is_latin_required: if it is True than images that do not contain latin text will be\n                                  filtered out.\n        \'\'\'\n        self.folder = folder\n        self.subset = subset\n        self.is_latin_required = is_latin_required\n\n        if root:\n            self.folder = os.path.join(root, self.folder)\n\n        assert self.subset in [\'train\', \'val\']\n\n        if self.subset == \'train\':\n            for i in range(1, 9):\n                assert os.path.exists(os.path.join(self.folder, f\'ch8_training_images_{i}\'))\n            assert os.path.exists(\n                os.path.join(self.folder, \'ch8_training_localization_transcription_gt_v2\'))\n        elif self.subset == \'val\':\n            assert os.path.exists(\n                os.path.join(self.folder, \'ch8_validation_images\'))\n            assert os.path.exists(\n                os.path.join(self.folder, \'ch8_validation_localization_transcription_gt_v2\'))\n\n    @staticmethod\n    def parse_line(line):\n        """""" Parses line of ICDAR2015 annotation. """"""\n\n        line = line.split(\',\')\n        quadrilateral = [int(x) for x in line[:8]]\n        language = line[8]\n        transcription = \',\'.join(line[9:])\n        legible = 1\n        if transcription == \'###\':\n            transcription = \'\'\n            legible = 0\n            language = \'\'\n\n        xmin = min(quadrilateral[0::2])\n        xmax = max(quadrilateral[0::2])\n\n        ymin = min(quadrilateral[1::2])\n        ymax = max(quadrilateral[1::2])\n\n        word_annotation = {\n            \'bbox\': [xmin, ymin, xmax - xmin + 1, ymax - ymin + 1],\n            \'segmentation\': [quadrilateral],\n            \'text\': {\n                \'transcription\': transcription,\n                \'legible\': legible,\n                \'language\': language,\n            }\n        }\n        return word_annotation\n\n    def collect_train_paths(self):\n        """""" Collects images and annotations paths for training set. """"""\n\n        image_paths = []\n        annotation_paths = []\n        n_images = 7200\n        for i in range(1, n_images + 1):\n            added = False\n            for extension in [\'jpg\', \'png\']:\n                image_path = os.path.join(self.folder,\n                                          f\'ch8_training_images_{(i - 1) // 1000 + 1}\',\n                                          f\'img_{i}.{extension}\')\n                if os.path.exists(image_path):\n                    image_paths.append(image_path)\n                    added = True\n                    break\n            if added:\n                annotation_paths.append(\n                    os.path.join(self.folder, \'ch8_training_localization_transcription_gt_v2\',\n                                 f\'gt_img_{i}.txt\')\n                )\n            else:\n                print(f\'Could not find: {image_path[:-3]}*\')\n        return image_paths, annotation_paths\n\n    def collect_val_paths(self):\n        """""" Collects images and annotations paths for validation set. """"""\n\n        image_paths = []\n        annotation_paths = []\n        n_images = 1800\n        for i in range(1, n_images + 1):\n            added = False\n            for extension in [\'jpg\', \'png\']:\n                image_path = os.path.join(self.folder,\n                                          \'ch8_validation_images\',\n                                          f\'img_{i}.{extension}\')\n                if os.path.exists(image_path):\n                    image_paths.append(image_path)\n                    added = True\n                    break\n            if added:\n                annotation_paths.append(\n                    os.path.join(self.folder, \'ch8_validation_localization_transcription_gt_v2\',\n                                 f\'gt_img_{i}.txt\')\n                )\n            else:\n                print(f\'Could not find: {image_path[:-3]}*\')\n        return image_paths, annotation_paths\n\n    def __call__(self, *args, **kwargs):\n        """""" Converts annotation from ICDAR 2017 format to internal format. """"""\n\n        dataset = TextOnlyCocoAnnotation()\n\n        if self.subset == \'train\':\n            image_paths, annotation_paths = self.collect_train_paths()\n        elif self.subset == \'val\':\n            image_paths, annotation_paths = self.collect_val_paths()\n\n        for image_path, annotation_path in zip(image_paths, annotation_paths):\n            word_annotations = []\n            with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                content = [line.strip() for line in read_file.readlines()]\n                for line in content:\n                    word_annotations.append(self.parse_line(line))\n            should_add = not self.is_latin_required\n            if self.is_latin_required:\n                for word_annotation in word_annotations:\n                    if word_annotation[\'text\'][\'language\'].lower() == \'latin\':\n                        should_add = True\n                        break\n            if should_add:\n                for word_annotation in word_annotations:\n                    dataset.add_bbox(image_path, imagesize.get(image_path), word_annotation)\n\n        return dataset\n\n\nclass ICDAR2019MLTDatasetConverter:\n    """""" Class for conversion of ICDAR2019 to TextOnlyCocoAnnotation. """"""\n\n    def __init__(self, folder, is_latin_required, root=\'\'):\n        \'\'\'\n        Converts ICDAR2017 MLT to TextOnlyCocoAnnotation\n        :param folder: Folder with extracted zip archives containing images and annotation.\n        :param is_latin_required: if it is True than images that do not contain latin text will be\n                                  filtered out.\n        \'\'\'\n        self.folder = folder\n        self.is_latin_required = is_latin_required\n\n        if root:\n            self.folder = os.path.join(root, self.folder)\n\n        assert os.path.exists(os.path.join(self.folder, \'ImagesPart1\'))\n        assert os.path.exists(os.path.join(self.folder, \'ImagesPart2\'))\n        assert os.path.exists(os.path.join(self.folder, \'train_gt_t13\'))\n\n    @staticmethod\n    def parse_line(line):\n        """""" Parses line of ICDAR2019 annotation. """"""\n\n        line = line.split(\',\')\n        quadrilateral = [int(x) for x in line[:8]]\n        language = line[8]\n        transcription = \',\'.join(line[9:])\n        legible = 1\n        if transcription == \'###\':\n            transcription = \'\'\n            legible = 0\n            language = \'\'\n\n        xmin = min(quadrilateral[0::2])\n        xmax = max(quadrilateral[0::2])\n\n        ymin = min(quadrilateral[1::2])\n        ymax = max(quadrilateral[1::2])\n\n        word_annotation = {\n            \'bbox\': [xmin, ymin, xmax - xmin + 1, ymax - ymin + 1],\n            \'segmentation\': [quadrilateral],\n            \'text\': {\n                \'transcription\': transcription,\n                \'legible\': legible,\n                \'language\': language,\n            }\n        }\n        return word_annotation\n\n    def collect_train_paths(self):\n        """""" Collects images and annotations paths for training set. """"""\n\n        image_paths = []\n        annotation_paths = []\n\n        n_images = 10000\n        for i in range(1, n_images + 1):\n            added = False\n            for extension in [\'jpg\', \'png\']:\n                image_path = os.path.join(self.folder,\n                                          f\'ImagesPart{(i - 1) // 5000 + 1}\',\n                                          f\'tr_img_{i:05}.{extension}\')\n                if os.path.exists(image_path):\n                    image_paths.append(image_path)\n                    added = True\n                    break\n            if added:\n                annotation_paths.append(\n                    os.path.join(self.folder, \'train_gt_t13\', f\'tr_img_{i:05}.txt\')\n                )\n            else:\n                print(f\'Could not find: {image_path[:-3]}*\')\n\n        return image_paths, annotation_paths\n\n    def __call__(self, *args, **kwargs):\n        """""" Converts annotation from ICDAR 2019 format to internal format. """"""\n\n        dataset = TextOnlyCocoAnnotation()\n\n        image_paths, annotation_paths = self.collect_train_paths()\n\n        for image_path, annotation_path in zip(image_paths, annotation_paths):\n            word_annotations = []\n            with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                content = [line.strip() for line in read_file.readlines()]\n                for line in content:\n                    word_annotations.append(self.parse_line(line))\n            should_add = not self.is_latin_required\n            if self.is_latin_required:\n                for word_annotation in word_annotations:\n                    if word_annotation[\'text\'][\'language\'].lower() == \'latin\':\n                        should_add = True\n                        break\n            if should_add:\n                for word_annotation in word_annotations:\n                    dataset.add_bbox(image_path, imagesize.get(image_path), word_annotation)\n\n        return dataset\n\n\nclass MSRATD500DatasetConverter:\n    """""" Class for conversion of MSRA-TD500 to TextOnlyCocoAnnotation. """"""\n\n    def __init__(self, folder, root=\'\'):\n        self.folder = folder\n\n        if root:\n            self.folder = os.path.join(root, self.folder)\n\n    @staticmethod\n    def parse_line(line):\n        """""" Parses line of MSRA-TD500 annotation. """"""\n\n        line = line.split(\' \')\n        _, _, top_left_x, top_left_y, width, height, rotation = [float(x) for x in line]\n        box = cv2.boxPoints(((top_left_x + width / 2, top_left_y + height / 2),\n                             (width, height), rotation * 57.2958))\n        quadrilateral = [int(x) for x in box.reshape([-1])]\n        xmin = min(quadrilateral[0::2])\n        xmax = max(quadrilateral[0::2])\n\n        ymin = min(quadrilateral[1::2])\n        ymax = max(quadrilateral[1::2])\n\n        word_annotation = {\n            \'bbox\': [xmin, ymin, xmax - xmin + 1, ymax - ymin + 1],\n            \'segmentation\': [quadrilateral],\n            \'text\': {\n                \'transcription\': \'\',\n                \'legible\': 1,\n                \'language\': \'\',\n            }\n        }\n\n        return word_annotation\n\n    def __call__(self, *args, **kwargs):\n        """""" Converts annotation from MSRA-TD500 format to internal format. """"""\n\n        dataset = TextOnlyCocoAnnotation()\n\n        for image_name in sorted(os.listdir(self.folder)):\n            if image_name.endswith(\'JPG\'):\n                image_path = os.path.join(self.folder, image_name)\n                annotation_path = os.path.join(self.folder, image_name.replace(\'.JPG\', \'.gt\'))\n\n                with open(annotation_path, encoding=\'utf-8-sig\') as read_file:\n                    content = [line.strip() for line in read_file.readlines()]\n                    for line in content:\n                        dataset.add_bbox(image_path, imagesize.get(image_path),\n                                         self.parse_line(line))\n\n        return dataset\n\n\nclass COCOTextDatasetConverter:\n    """""" Class for conversion of COCO-Text to TextOnlyCocoAnnotation. """"""\n\n    def __init__(self, path, sets=None, root=\'\'):\n        self.path = path\n\n        if root:\n            self.path = os.path.join(root, self.path)\n\n        self.sets = sets\n        if self.sets is None:\n            self.sets = [\'train\']  # \'val\n\n    @staticmethod\n    def parse_annotation_instance(annotation):\n        """""" Parses annotation instance of COCO-Text dataset. """"""\n\n        text = annotation[\'utf8_string\']\n        language = annotation[\'language\']\n        legible = int(annotation[\'legibility\'] == \'legible\')\n\n        mask = np.reshape(np.array(annotation[\'mask\'], np.int32), (-1, 2))\n        box = cv2.boxPoints(cv2.minAreaRect(mask))\n        quadrilateral = [int(x) for x in box.reshape([-1])]\n\n        xmin = min(quadrilateral[0::2])\n        xmax = max(quadrilateral[0::2])\n\n        ymin = min(quadrilateral[1::2])\n        ymax = max(quadrilateral[1::2])\n\n        word_annotation = {\n            \'bbox\': [xmin, ymin, xmax - xmin + 1, ymax - ymin + 1],\n            \'segmentation\': [quadrilateral],\n            \'text\': {\n                \'transcription\': text,\n                \'legible\': legible,\n                \'language\': language,\n            }\n        }\n\n        return word_annotation\n\n    def __call__(self):\n        """""" Converts annotation from COCO-TEXT format to internal format. """"""\n\n        dataset = TextOnlyCocoAnnotation()\n\n        with open(self.path) as read_file:\n\n            json_loaded = json.load(read_file)\n\n            for i, value in json_loaded[\'imgs\'].items():\n                image_path = os.path.join(os.path.dirname(self.path), \'train2014\',\n                                          value[\'file_name\'])\n                dataset_type = value[\'set\']\n\n                if dataset_type not in self.sets:\n                    print(dataset_type)\n                    continue\n\n                for annotation_id in json_loaded[\'imgToAnns\'][i]:\n                    annotation_value = json_loaded[\'anns\'][str(annotation_id)]\n                    word_annotation = self.parse_annotation_instance(annotation_value)\n                    dataset.add_bbox(image_path, imagesize.get(image_path), word_annotation)\n\n        return dataset\n\n\nstr_to_class = {\n    \'ICDAR2013DatasetConverter\': ICDAR2013DatasetConverter,\n    \'ICDAR2015DatasetConverter\': ICDAR2015DatasetConverter,\n    \'ICDAR2019MLTDatasetConverter\': ICDAR2019MLTDatasetConverter,\n    \'MSRATD500DatasetConverter\': MSRATD500DatasetConverter,\n    \'COCOTextDatasetConverter\': COCOTextDatasetConverter,\n}\n'"
pytorch_toolkit/text_spotting/text_spotting/datasets/factory.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os.path as osp\n\nfrom .coco_text import COCOTextDataset\n\n\ndef root_data_dir():\n    """""" Returns root data dir. """"""\n    return osp.join(osp.dirname(osp.abspath(__file__)), \'..\', \'..\', \'data\', \'coco\')\n\n\ndef get_dataset(name, with_gt, remove_images_without_gt, transforms=None,\n                root_data_dir=root_data_dir(),\n                alphabet_decoder=None,\n                remove_images_without_text=False):\n    """""" Returns dataset. """"""\n\n    dataset = COCOTextDataset(root_data_dir,\n                              osp.join(root_data_dir, name),\n                              with_gt, remove_images_without_gt, transforms,\n                              remove_images_without_text=remove_images_without_text,\n                              alphabet_decoder=alphabet_decoder)\n    return dataset\n'"
pytorch_toolkit/text_spotting/text_spotting/datasets/text_evaluation.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n"""""" This module contains evaluation procedure. """"""\n\nimport Polygon as plg\nimport cv2\nimport numpy as np\nimport pycocotools.mask as mask_utils\n\nIOU_CONSTRAINT = 0.5\nAREA_PRECISION_CONSTRAINT = 0.5\n\n\ndef masks_to_rects(masks, is_rle):\n    """""" Creates masks from rects. """"""\n\n    rects = []\n    for mask in masks:\n        decoded_mask = mask_utils.decode(mask) if is_rle else mask\n        decoded_mask = np.ascontiguousarray(decoded_mask)\n        contours, _ = cv2.findContours(decoded_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[-2:]\n\n        areas = []\n        boxes = []\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            areas.append(area)\n\n            rect = cv2.minAreaRect(contour)\n            box = cv2.boxPoints(rect)\n            box = np.int0(box)\n            boxes.append(box)\n\n        if areas:\n            i = np.argmax(areas)\n            rects.append(boxes[i])\n\n    return rects\n\n\ndef polygon_from_points(points):\n    """""" Returns a Polygon object to use with the Polygon2 class from a list of 8 points:\n        x1,y1,x2,y2,x3,y3,x4,y4\n    """"""\n\n    point_mat = np.array(points[:]).astype(np.int32).reshape(-1, 2)\n    return plg.Polygon(point_mat)\n\n\ndef draw_gt_polygons(image, gt_polygons, gt_dont_care_nums):\n    """""" Draws groundtruth polygons on image. """"""\n\n    for point_idx, polygon in enumerate(gt_polygons):\n        color = (128, 128, 128) if point_idx in gt_dont_care_nums else (255, 0, 0)\n        for i in range(len(polygon[0])):\n            pt1 = int(polygon[0][i][0]), int(polygon[0][i][1])\n            pt2 = int(polygon[0][(i + 1) % len(polygon[0])][0]), int(polygon[0][(i + 1) % len(polygon[0])][1])\n            cv2.line(image, pt1, pt2, color, 2)\n    return image\n\n\ndef draw_pr_polygons(image, pr_polygons, pr_dont_care_nums, pr_matched_nums, pr_confidences_list,\n                     pr_transcriptions=None):\n    """""" Draws predicted polygons on image. """"""\n\n    for point_idx, _ in enumerate(pr_polygons):\n        if pr_confidences_list[point_idx] > 0.25:\n            polygon = pr_polygons[point_idx]\n            color = (0, 0, 255)\n            if point_idx in pr_dont_care_nums:\n                color = (255, 255, 255)\n            if point_idx in pr_matched_nums:\n                color = (0, 255, 0)\n\n                if pr_transcriptions:\n                    pt1 = int(polygon[0][0][0]), int(polygon[0][0][1])\n                    cv2.putText(image, pr_transcriptions[point_idx], pt1, cv2.FONT_HERSHEY_SIMPLEX,\n                                1.0, (0, 255, 0), 2)\n\n            for i in range(4):\n                pt1 = int(polygon[0][i][0]), int(polygon[0][i][1])\n                pt2 = int(polygon[0][(i + 1) % 4][0]), int(polygon[0][(i + 1) % 4][1])\n                cv2.line(image, pt1, pt2, color, 2)\n    return image\n\n\ndef get_union(polygon1, polygon2):\n    """""" Returns area of union of two polygons. """"""\n\n    return polygon1.area() + polygon2.area() - get_intersection(polygon1, polygon2)\n\n\ndef get_intersection_over_union(polygon1, polygon2):\n    """""" Returns intersection over union of two polygons. """"""\n\n    union = get_union(polygon1, polygon2)\n    return get_intersection(polygon1, polygon2) / union if union else 0.0\n\n\ndef get_intersection(polygon1, polygon2):\n    """""" Returns are of intersection of two polygons. """"""\n\n    intersection = polygon1 & polygon2\n    if len(intersection) == 0:\n        return 0\n    return intersection.area()\n\n\ndef compute_ap(conf_list, match_list, num_gt_care):\n    """""" Returns average precision metrics. """"""\n\n    correct = 0\n    average_precision = 0\n    if conf_list:\n        conf_list = np.array(conf_list)\n        match_list = np.array(match_list)\n        sorted_ind = np.argsort(-conf_list)\n        match_list = match_list[sorted_ind]\n\n        for idx, matched in enumerate(match_list):\n            if matched:\n                correct += 1\n                average_precision += float(correct) / (idx + 1)\n\n        if num_gt_care > 0:\n            average_precision /= num_gt_care\n\n    return average_precision\n\n\ndef strip(text):\n    """""" Removes special characters. """"""\n\n    if text.lower().endswith(""\'s""):\n        text = text[:-2]\n    text = text.strip(\'-\')\n    for character in ""\'!?.:,*\\""()\xc2\xb7[]/"":\n        text = text.replace(character, \' \')\n    text = text.strip()\n\n    return text\n\n\ndef is_word(text):\n    """""" Checks whether input txt is a word. """"""\n\n    text = strip(text)\n\n    if \' \' in text:\n        return False\n\n    if len(text) < 3:\n        return False\n\n    forbidden_symbols = ""\xc3\x97\xc3\xb7\xce\x87""\n\n    range1 = [ord(u\'a\'), ord(u\'z\')]\n    range2 = [ord(u\'A\'), ord(u\'Z\')]\n    range3 = [ord(u\'\xc3\x80\'), ord(u\'\xc6\xbf\')]\n    range4 = [ord(u\'\xc7\x84\'), ord(u\'\xc9\xbf\')]\n    range5 = [ord(u\'\xce\x86\'), ord(u\'\xcf\xbf\')]\n    range6 = [ord(u\'-\'), ord(u\'-\')]\n\n    for char in text:\n        char_code = ord(char)\n        if char in forbidden_symbols:\n            return False\n\n        if not (range1[0] <= char_code <= range1[1] or \\\n                range2[0] <= char_code <= range2[1] or \\\n                range3[0] <= char_code <= range3[1] or \\\n                range4[0] <= char_code <= range4[1] or \\\n                range5[0] <= char_code <= range5[1] or \\\n                range6[0] <= char_code <= range6[1]):\n            return False\n\n    return True\n\n\ndef parse_gt_objects(gt_annotation, use_transcription):\n    """""" Parses groundtruth objects from annotation. """"""\n\n    gt_polygons_list = []\n    gt_transcriptions = [] if use_transcription else None\n    gt_dont_care_polygon_nums = []\n    for gt_object in gt_annotation:\n        polygon = polygon_from_points(gt_object[\'points\'])\n        gt_polygons_list.append(polygon)\n\n        transcription = gt_object[\'transcription\']\n\n        if transcription == \'###\':\n            gt_dont_care_polygon_nums.append(len(gt_polygons_list) - 1)\n        elif use_transcription:\n            if is_word(transcription):\n                transcription = strip(gt_object[\'transcription\'])\n            else:\n                gt_dont_care_polygon_nums.append(len(gt_polygons_list) - 1)\n\n        if use_transcription:\n            gt_transcriptions.append(transcription)\n\n    return gt_polygons_list, gt_dont_care_polygon_nums, gt_transcriptions\n\n\ndef parse_pr_objects(pr_annotation, use_transcription):\n    """""" Parses predicted objects from annotation. """"""\n\n    pr_polygons_list = []\n    pr_confidences_list = []\n    pr_transcriptions = [] if use_transcription else None\n    for pr_object in pr_annotation:\n        polygon = polygon_from_points(pr_object[\'points\'])\n        pr_polygons_list.append(polygon)\n        pr_confidences_list.append(pr_object[\'confidence\'])\n        if use_transcription:\n            pr_transcriptions.append(pr_object[\'transcription\'])\n\n    return pr_polygons_list, pr_confidences_list, pr_transcriptions\n\n\ndef match_dont_care_objects(gt_polygons_list, gt_dont_care_polygon_nums, pr_polygons_list):\n    """""" Matches ignored objects. """"""\n\n    pr_dont_care_polygon_nums = []\n\n    if gt_dont_care_polygon_nums:\n        for pr_polygon_idx, pr_polygon in enumerate(pr_polygons_list):\n            for dont_care_polygon_num in gt_dont_care_polygon_nums:\n                intersected_area = get_intersection(gt_polygons_list[dont_care_polygon_num],\n                                                    pr_polygon)\n                pd_dimensions = pr_polygon.area()\n                precision = 0 if pd_dimensions == 0 else intersected_area / pd_dimensions\n                if precision > AREA_PRECISION_CONSTRAINT:\n                    pr_dont_care_polygon_nums.append(pr_polygon_idx)\n                    break\n\n    return pr_dont_care_polygon_nums\n\n\ndef match(gt_polygons_list, gt_transcriptions, gt_dont_care_polygon_nums, pr_polygons_list,\n          pr_transcriptions, pr_dont_care_polygon_nums):\n    """""" Matches all objects. """"""\n\n    pr_matched_nums = []\n    pr_matched_but_not_recognized = []\n\n    output_shape = [len(gt_polygons_list), len(pr_polygons_list)]\n    iou_mat = np.empty(output_shape)\n    gt_rect_mat = np.zeros(len(gt_polygons_list), np.int8)\n    pr_rect_mat = np.zeros(len(pr_polygons_list), np.int8)\n    for gt_idx, gt_polygon in enumerate(gt_polygons_list):\n        for pr_idx, pr_polygon in enumerate(pr_polygons_list):\n            iou_mat[gt_idx, pr_idx] = get_intersection_over_union(gt_polygon, pr_polygon)\n\n    for gt_idx, _ in enumerate(gt_polygons_list):\n        for pr_idx, _ in enumerate(pr_polygons_list):\n            if gt_rect_mat[gt_idx] == 0 and pr_rect_mat[pr_idx] == 0 \\\n                    and gt_idx not in gt_dont_care_polygon_nums \\\n                    and pr_idx not in pr_dont_care_polygon_nums:\n                if iou_mat[gt_idx, pr_idx] > IOU_CONSTRAINT:\n                    gt_rect_mat[gt_idx] = 1\n                    pr_rect_mat[pr_idx] = 1\n                    if gt_transcriptions is not None and pr_transcriptions is not None:\n                        if gt_transcriptions[gt_idx].lower() == pr_transcriptions[pr_idx].lower():\n                            pr_matched_nums.append(pr_idx)\n                        else:\n                            print(gt_transcriptions[gt_idx], pr_transcriptions[pr_idx])\n                            pr_matched_but_not_recognized.append(pr_idx)\n                    else:\n                        pr_matched_nums.append(pr_idx)\n\n    return pr_matched_nums, pr_matched_but_not_recognized\n\n\ndef text_eval(pr_annotations, gt_annotations, images=None, imshow_delay=1,\n              use_transcriptions=False):\n    """""" Annotation format:\n        {""image_path"": [\n                            {""points"": [x1,y1,x2,y2,x3,y3,x4,y4],\n                             ""confidence"": float,\n                             ""transcription"", str}\n                        ],\n         ""image_path"": [points],\n\n         ### - is a transcription of non-valid word.\n\n    """"""\n\n    assert len(pr_annotations) == len(gt_annotations), str(len(pr_annotations)) + \' \' + str(\n        len(gt_annotations))\n\n    matched_sum = 0\n    num_global_care_gt = 0\n    num_global_care_pr = 0\n\n    arr_global_confidences = []\n    arr_global_matches = []\n\n    for frame_id, _ in enumerate(pr_annotations):\n        gt_polygons_list, gt_dont_care_polygon_nums, gt_transcriptions = parse_gt_objects(\n            gt_annotations[frame_id], use_transcriptions)\n        pr_polygons_list, pr_confidences_list, pr_transcriptions = parse_pr_objects(\n            pr_annotations[frame_id], use_transcriptions)\n\n        pr_dont_care_polygon_nums = match_dont_care_objects(\n            gt_polygons_list, gt_dont_care_polygon_nums, pr_polygons_list)\n\n        pr_matched_nums = []\n        pr_matched_but_not_recognized = []\n        if gt_polygons_list and pr_polygons_list:\n            pr_matched_nums, pr_matched_but_not_recognized = match(gt_polygons_list,\n                                                                   gt_transcriptions,\n                                                                   gt_dont_care_polygon_nums,\n                                                                   pr_polygons_list,\n                                                                   pr_transcriptions,\n                                                                   pr_dont_care_polygon_nums)\n\n            matched_sum += len(pr_matched_nums)\n\n            for pr_num in range(len(pr_polygons_list)):\n                if pr_num not in pr_dont_care_polygon_nums:\n                    # we exclude the don\'t care detections\n                    matched = pr_num in pr_matched_nums\n                    arr_global_confidences.append(pr_confidences_list[pr_num])\n                    arr_global_matches.append(matched)\n\n        num_global_care_gt += len(gt_polygons_list) - len(gt_dont_care_polygon_nums)\n        num_global_care_pr += len(pr_polygons_list) - len(pr_dont_care_polygon_nums)\n\n        if images:\n            image = images[frame_id]\n            if isinstance(image, str):\n                image = cv2.imread(image)\n            draw_gt_polygons(image, gt_polygons_list, gt_dont_care_polygon_nums)\n            draw_pr_polygons(image, pr_polygons_list, pr_dont_care_polygon_nums,\n                             pr_matched_nums, pr_confidences_list)\n            if use_transcriptions:\n                draw_pr_polygons(image, pr_polygons_list, pr_dont_care_polygon_nums,\n                                 pr_matched_nums + pr_matched_but_not_recognized, pr_confidences_list,\n                                 pr_transcriptions)\n            image = cv2.resize(image, (640, 480))\n            cv2.imshow(\'result\', image)\n            k = cv2.waitKey(imshow_delay)\n            if k == 27:\n                return -1, -1, -1, -1\n\n    method_recall = 0 if num_global_care_gt == 0 else float(matched_sum) / num_global_care_gt\n    method_precision = 0 if num_global_care_pr == 0 else float(matched_sum) / num_global_care_pr\n    denominator = method_precision + method_recall\n    method_hmean = 0 if denominator == 0 else 2.0 * method_precision * method_recall / denominator\n\n    average_precision = compute_ap(arr_global_confidences, arr_global_matches, num_global_care_gt)\n\n    return method_recall, method_precision, method_hmean, average_precision\n'"
pytorch_toolkit/text_spotting/text_spotting/models/__init__.py,0,b''
pytorch_toolkit/text_spotting/text_spotting/models/openvino_net.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\n\nimport numpy as np\n\nfrom segmentoly.rcnn.openvino_net import OpenVINONet\n\n\nclass TextMaskRCNNOpenVINO(OpenVINONet):\n    def __init__(self, detector_model_path, encoder_model_path, decoder_model_path, *args,\n                 **kwargs):\n        super(TextMaskRCNNOpenVINO, self).__init__(detector_model_path[:-3] + \'xml\',\n                                                   detector_model_path[:-3] + \'bin\',\n                                                   *args, **kwargs)\n        required_input_keys = {\'im_data\', \'im_info\'}\n        assert required_input_keys == set(self.net.inputs.keys())\n        required_output_keys = {\'boxes\', \'scores\', \'classes\', \'raw_masks\', \'text_features\'}\n        assert required_output_keys.issubset(self.net.outputs.keys())\n\n        self.n, self.c, self.h, self.w = self.net.inputs[\'im_data\'].shape\n        assert self.n == 1, \'Only batch 1 is supported.\'\n\n        self.text_encoder = OpenVINONet(encoder_model_path[:-3] + \'xml\',\n                                        encoder_model_path[:-3] + \'bin\')\n\n        self.text_decoder = OpenVINONet(decoder_model_path[:-3] + \'xml\',\n                                        decoder_model_path[:-3] + \'bin\')\n\n    def __call__(self, im_data, im_info, **kwargs):\n        logging.warning(\'Please do not use this code for performance evaluation purposes. \'\n                        \'This is for accuracy validation only. If you would like to see how fast it \'\n                        \'is please refer to https://github.com/opencv/open_model_zoo/blob/develop/\'\n                        \'demos/python_demos/text_spotting_demo/README.md.\')\n\n        im_data = im_data[0].cpu().numpy()\n        im_info = im_info[0].cpu().numpy()\n        if (self.h - im_data.shape[1] < 0) or (self.w - im_data.shape[2] < 0):\n            raise ValueError(\'Input image should resolution of {}x{} or less, \'\n                             \'got {}x{}.\'.format(self.w, self.h, im_data.shape[2],\n                                                 im_data.shape[1]))\n        im_data = np.pad(im_data, ((0, 0),\n                                   (0, self.h - im_data.shape[1]),\n                                   (0, self.w - im_data.shape[2])),\n                         mode=\'constant\', constant_values=0).reshape(1, self.c, self.h, self.w)\n        im_info = im_info.reshape(1, *im_info.shape)\n        output = super().__call__(dict(im_data=im_data, im_info=im_info))\n\n        classes = output[\'classes\']\n        valid_detections_mask = classes > 0\n        classes = classes[valid_detections_mask]\n        boxes = output[\'boxes\'][valid_detections_mask]\n        scores = output[\'scores\'][valid_detections_mask]\n        masks = output[\'raw_masks\'][valid_detections_mask]\n        text_features = output[\'text_features\'][valid_detections_mask, :, :]\n\n        texts = []\n        for feature in text_features:\n            feature = self.text_encoder({\'input\': feature})[\'output\']\n            feature = np.reshape(feature, (feature.shape[0], feature.shape[1], -1))\n            feature = np.transpose(feature, (0, 2, 1))\n\n            hidden = np.zeros([1, 1, 256])\n            prev_symbol = np.zeros((1,))\n\n            eos = 1\n            max_seq_len = 28\n            vocab_size = 38\n            per_feature_outputs = np.zeros([max_seq_len, vocab_size])\n\n            for i in range(max_seq_len):\n                o = self.text_decoder(\n                    {\'prev_symbol\': prev_symbol, \'prev_hidden\': hidden, \'encoder_outputs\': feature})\n                output = o[\'output\']\n                per_feature_outputs[i] = output\n                prev_symbol = np.argmax(output, axis=1)\n                if prev_symbol == eos:\n                    break\n                hidden = o[\'hidden\']\n\n            texts.append(per_feature_outputs)\n\n        texts = np.array(texts)\n\n        return boxes, classes, scores, np.full(len(classes), 0, dtype=np.int32), masks, texts\n'"
pytorch_toolkit/text_spotting/text_spotting/models/text_detectors.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\nfrom segmentoly.rcnn.model_zoo.instance_segmentation_security_0050 import \\\n    BottomUpPathAugmentationBN, FPN, PriorBox, MaskHeadBN, RPNLite, BboxHead3FC, DetectionOutput\nfrom .backbones import get_backbone\nfrom .mask_rcnn import str_to_class as str_to_text_spotter_class\nfrom .mask_rcnn.proposal_gt_matcher import ProposalGTMatcher\nfrom .text_recognition_heads import str_to_class as str_to_text_recognition_head_class\n\n\ndef make_text_detector(class_name=None,\n                       backbone=None,\n                       bupa_dim_out=0,\n                       fpn_dim_out=128,\n                       rpn_dim=128,\n                       mask_head_dim=128,\n                       prior_boxes_sizes=None,\n                       segm_roi_featuremap_resolution=7,\n                       use_text_masking=False,\n                       text_recognition_head=None):\n    class TextDetector(str_to_text_spotter_class[class_name]):\n        segmentation_roi_featuremap_resolution = segm_roi_featuremap_resolution\n        mask_text = use_text_masking\n\n        def __init__(self, cls_num, **kwargs):\n            super().__init__(cls_num, get_backbone(**backbone), **kwargs)\n\n            self.bupa = None\n            if bupa_dim_out:\n                self.bupa = BottomUpPathAugmentationBN(output_levels=5, dims_in=self.fpn.dims_out,\n                                                       scales_in=self.fpn.scales_out,\n                                                       dim_out=bupa_dim_out, group_norm=False)\n\n            r = self.segmentation_roi_featuremap_resolution\n            self.proposal_gt_matcher = ProposalGTMatcher(positive_threshold=0.5,\n                                                         negative_threshold=0.5,\n                                                         positive_fraction=0.25, batch_size=256,\n                                                         target_mask_size=(2 * r, 2 * r))\n\n        @staticmethod\n        def add_fpn(dims_in, scales_in, **kwargs):\n            return FPN(dims_in, scales_in, fpn_dim_out, fpn_dim_out, group_norm=False)\n\n        @staticmethod\n        def add_priors_generator():\n            prior_boxes = nn.ModuleList()\n            widths = prior_boxes_sizes[\'widths\']\n            heights = prior_boxes_sizes[\'heights\']\n\n            scale_factor = 1.0\n            for ws, hs in zip(widths, heights):\n                if scale_factor != 1.0:\n                    for i in range(len(ws)):\n                        ws[i] *= scale_factor\n                    for i in range(len(hs)):\n                        hs[i] *= scale_factor\n                prior_boxes.append(PriorBox(widths=ws, heights=hs, flatten=True, use_cache=True))\n            priors_per_level_num = list([priors.priors_num() for priors in prior_boxes])\n            assert priors_per_level_num[1:] == priors_per_level_num[:-1]\n            priors_num = priors_per_level_num[0]\n            return prior_boxes, priors_num\n\n        @staticmethod\n        def add_segmentation_head(features_dim_in, cls_num, **kwargs):\n            # ROI-wise segmentation part.\n            assert features_dim_in[1:] == features_dim_in[:-1]\n            mask_head = MaskHeadBN(features_dim_in[0], 6, cls_num, mask_head_dim, 1)\n            return mask_head\n\n        @staticmethod\n        def add_rpn(priors_num, features_dim_in):\n            # RPN is shared between FPN levels.\n            assert features_dim_in[1:] == features_dim_in[:-1]\n            rpn = RPNLite(features_dim_in[0], rpn_dim, priors_num, \'sigmoid\')\n            return rpn\n\n        @staticmethod\n        def add_detection_head(features_dim_in, cls_num, fc_detection_head=True, **kwargs):\n            # ROI-wise detection part.\n            assert features_dim_in[1:] == features_dim_in[:-1]\n            dim_out = 512\n            detection_head = BboxHead3FC(features_dim_in[0], dim_out, 7, cls_num,\n                                         cls_agnostic_bbox_regression=False,\n                                         fc_as_conv=not fc_detection_head)\n            detection_output = DetectionOutput(cls_num, nms_threshold=0.5, score_threshold=0.05,\n                                               post_nms_count=100, max_detections_per_image=100)\n            return detection_head, detection_output\n\n        @staticmethod\n        def add_text_recogn_head():\n            return str_to_text_recognition_head_class[text_recognition_head[\'name\']](\n                **text_recognition_head[\'param\']\n            )\n\n        @property\n        def pre_nms_rois_count(self):\n            return 2000 if self.training else 300\n\n        @property\n        def post_nms_rois_count(self):\n            return 2000 if self.training else 300\n\n        def forward_fpn(self, feature_pyramid):\n            x = self.fpn(feature_pyramid)\n            return self.bupa(x) if self.bupa else x\n\n    return TextDetector\n'"
pytorch_toolkit/text_spotting/text_spotting/utils/onnx.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\nimport os\n\nimport numpy as np\nimport onnx\n\nimport torch\nimport torch.nn as nn\n\n\ndef get_input_size(image_size, scale, max_size, divisor=1):\n    image_size = np.asarray(image_size)\n\n    # Get scale factor for image resize.\n    im_scale = float(scale) / float(min(image_size))\n    if np.round(im_scale * max(image_size)) > max_size:\n        im_scale = float(max_size) / float(max(image_size))\n\n    # Get resized image size.\n    resized_image_size = image_size * im_scale\n    input_size = np.round(resized_image_size).astype(np.int)\n\n    # Pad the image for size be a multiple of `divisor`.\n    input_size[0] = (input_size[0] + divisor - 1) // divisor * divisor\n    input_size[1] = (input_size[1] + divisor - 1) // divisor * divisor\n\n    return input_size\n\n\ndef onnx_export(net, input_size, output_folder, check=False, verbose=False):\n    os.makedirs(output_folder, exist_ok=True)\n\n    net.eval()\n    if hasattr(net, \'force_max_output_size\'):\n        # Force model to output maximal possible number of proposals and detections.\n        net.force_max_output_size = True\n    # Use ONNX stubs for all the blocks that don\'t support direct export.\n    for m in net.modules():\n        if hasattr(m, \'use_stub\'):\n            m.use_stub = True\n\n    im_data = np.random.randn(1, 3, input_size[0], input_size[1]).astype(np.float32)\n    im_info = np.asarray([[input_size[0], input_size[1], 1.0]], dtype=np.float32)\n    im_info = torch.tensor(im_info)\n    im_data = torch.tensor(im_data)\n    if torch.cuda.is_available():\n        net = net.cuda()\n        im_info = im_info.cuda()\n        im_data = im_data.cuda()\n    im_data.requires_grad = True\n    im_info.requires_grad = True\n\n    input_names = [\'im_data\', \'im_info\']\n    outputs_names = [\'boxes\', \'classes\', \'scores\', \'batch_ids\', \'raw_masks\', \'text_features\']\n    torch.onnx.export(net, (im_data, im_info), os.path.join(output_folder, \'detector.onnx\'),\n                      verbose=verbose, input_names=input_names, output_names=outputs_names)\n\n    net_from_onnx = onnx.load(os.path.join(output_folder, \'detector.onnx\'))\n    if check:\n        try:\n            onnx.checker.check_model(net_from_onnx)\n            logging.info(\'ONNX check passed.\')\n        except onnx.onnx_cpp2py_export.checker.ValidationError as ex:\n            logging.warning(\'ONNX check failed.\')\n            logging.warning(ex)\n\n    return onnx.helper.printable_graph(net_from_onnx.graph)\n\n\ndef export_to_onnx_text_recognition_encoder(net, input_size, output_folder, check=False):\n    os.makedirs(output_folder, exist_ok=True)\n\n    net.eval()\n    dim = net.dim_input\n    input = np.random.randn(1, dim, *input_size).astype(np.float32)\n    input = torch.tensor(input)\n    if torch.cuda.is_available():\n        net = net.cuda()\n        input = input.cuda()\n    input.requires_grad = True\n    torch.onnx.export(net, input, os.path.join(output_folder, \'encoder.onnx\'), verbose=True,\n                      input_names=[\'input\'], output_names=[\'output\'])\n\n    net_from_onnx = onnx.load(os.path.join(output_folder, \'encoder.onnx\'))\n    if check:\n        try:\n            onnx.checker.check_model(net_from_onnx)\n            logging.info(\'ONNX check passed.\')\n        except onnx.onnx_cpp2py_export.checker.ValidationError as ex:\n            logging.warning(\'ONNX check failed.\')\n            logging.warning(ex)\n\n    return onnx.helper.printable_graph(net_from_onnx.graph)\n\n\ndef export_to_onnx_text_recognition_decoder(net, input_size, output_folder, check=False):\n    os.makedirs(output_folder, exist_ok=True)\n\n    net.eval()\n    dim = net.hidden_size\n    prev_input = np.random.randn(1).astype(np.float32)\n    pev_hidden = np.random.randn(1, 1, dim).astype(np.float32)\n    prev_cell = np.random.randn(1, 1, dim).astype(np.float32)\n    encoder_outputs = np.random.randn(1, input_size[0] * input_size[1], dim).astype(np.float32)\n    prev_input = torch.tensor(prev_input)\n    pev_hidden = torch.tensor(pev_hidden)\n    prev_cell = torch.tensor(prev_cell)\n    encoder_outputs = torch.tensor(encoder_outputs)\n    if torch.cuda.is_available():\n        net = net.cuda()\n        prev_input = prev_input.cuda()\n        pev_hidden = pev_hidden.cuda()\n        prev_cell = prev_cell.cuda()\n        encoder_outputs = encoder_outputs.cuda()\n    prev_input.requires_grad = False\n    pev_hidden.requires_grad = True\n    prev_cell.requires_grad = True\n    encoder_outputs.requires_grad = True\n    if isinstance(net.decoder, nn.GRU):\n        torch.onnx.export(net, (prev_input, pev_hidden, encoder_outputs),\n                          os.path.join(output_folder, \'decoder.onnx\'), verbose=True,\n                          input_names=[\'prev_symbol\', \'prev_hidden\', \'encoder_outputs\'],\n                          output_names=[\'output\', \'hidden\', \'attention\']\n                          )\n    elif isinstance(net.decoder, nn.LSTM):\n        torch.onnx.export(net, (prev_input, pev_hidden, encoder_outputs, prev_cell),\n                          os.path.join(output_folder, \'decoder.onnx\'), verbose=True,\n                          input_names=[\'prev_symbol\', \'prev_hidden\', \'encoder_outputs\',\n                                       \'prev_cell\'],\n                          output_names=[\'output\', \'hidden\', \'cell\', \'attention\']\n                          )\n\n    net_from_onnx = onnx.load(os.path.join(output_folder, \'decoder.onnx\'))\n    if check:\n        try:\n            onnx.checker.check_model(net_from_onnx)\n            logging.info(\'ONNX check passed.\')\n        except onnx.onnx_cpp2py_export.checker.ValidationError as ex:\n            logging.warning(\'ONNX check failed.\')\n            logging.warning(ex)\n\n    return onnx.helper.printable_graph(net_from_onnx.graph)\n'"
pytorch_toolkit/text_spotting/text_spotting/utils/postprocess.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\n\nfrom segmentoly.utils.blob import to_numpy\nfrom segmentoly.utils.postprocess import segm_postprocess\n\n\ndef postprocess_batch(batch_ids, scores, classes, boxes, raw_cls_masks, raw_texts,\n                      batch_size, im_h, im_w, im_scale_y=None, im_scale_x=None, im_scale=None,\n                      full_image_masks=True, encode_masks=False,\n                      confidence_threshold=0.0):\n    boxes_all = [np.empty((0, 4), dtype=np.float32) for _ in range(batch_size)]\n    scores_all = [np.empty((0,), dtype=np.float32) for _ in range(batch_size)]\n    classes_all = [np.empty((0,), dtype=np.float32) for _ in range(batch_size)]\n    raw_masks_all = [None for _ in range(batch_size)]\n    masks_all = [[] for _ in range(batch_size)]\n    raw_texts_all = [None for _ in range(batch_size)]\n\n    if batch_ids is None:\n        return scores_all, classes_all, boxes_all, masks_all\n\n    scale_x = im_scale_x\n    scale_y = im_scale_y\n    if im_scale is not None:\n        scale_x = im_scale\n        scale_y = im_scale\n    assert len(scale_x) == len(scale_y)\n\n    batch_ids = to_numpy(batch_ids)\n\n    num_objs_per_batch = []\n    for i in range(batch_size):\n        num_objs_per_batch.append(np.count_nonzero(batch_ids == i))\n\n    begin = 0\n    for i in range(0, len(num_objs_per_batch)):\n        end = begin + num_objs_per_batch[i]\n        # Scale boxes back to the original image\n        boxes_all[i] = boxes[begin:end]\n        scores_all[i] = scores[begin:end]\n        classes_all[i] = classes[begin:end]\n        raw_masks_all[i] = raw_cls_masks[begin:end]\n        raw_texts_all[i] = raw_texts[begin:end]\n        begin = end\n\n    # Resize segmentation masks to fit corresponding bounding boxes.\n    for i in range(batch_size):\n        scores_all[i], classes_all[i], boxes_all[i], masks_all[i], raw_texts_all[i] = \\\n            postprocess(scores_all[i], classes_all[i], boxes_all[i], raw_masks_all[i],\n                        raw_texts_all[i],\n                        im_h[i], im_w[i], scale_y[i], scale_x[i], None,\n                        full_image_masks, encode_masks,\n                        confidence_threshold)\n\n    return scores_all, classes_all, boxes_all, masks_all, raw_texts_all\n\n\ndef postprocess(scores, classes, boxes, raw_cls_masks, raw_texts,\n                im_h, im_w, im_scale_y=None, im_scale_x=None, im_scale=None,\n                full_image_masks=True, encode_masks=False,\n                confidence_threshold=0.0):\n    no_detections = (np.empty((0,), dtype=np.float32), np.empty((0,), dtype=np.float32), \\\n                     np.empty((0, 4), dtype=np.float32), [], [])\n    if scores is None:\n        return no_detections\n\n    scale = im_scale\n    if scale is None:\n        assert (im_scale_x is not None) and (im_scale_y is not None)\n        scale = [im_scale_x, im_scale_y, im_scale_x, im_scale_y]\n\n    scores = to_numpy(scores)\n    classes = to_numpy(classes)\n    boxes = to_numpy(boxes)\n    raw_cls_masks = to_numpy(raw_cls_masks)\n    raw_texts = to_numpy(raw_texts)\n\n    confidence_filter = scores > confidence_threshold\n    scores = scores[confidence_filter]\n    classes = classes[confidence_filter]\n    boxes = boxes[confidence_filter]\n    raw_texts = raw_texts[confidence_filter]\n    raw_cls_masks = list(\n        segm for segm, is_valid in zip(raw_cls_masks, confidence_filter) if is_valid)\n\n    if len(scores) == 0:\n        return no_detections\n\n    boxes = boxes / scale\n    classes = classes.astype(np.uint32)\n    masks = []\n    for box, cls, raw_mask in zip(boxes, classes, raw_cls_masks):\n        raw_cls_mask = raw_mask[cls, ...]\n        mask = segm_postprocess(box, raw_cls_mask, im_h, im_w, full_image_masks, encode_masks)\n        masks.append(mask)\n\n    return scores, classes, boxes, masks, raw_texts\n'"
pytorch_toolkit/text_spotting/text_spotting/utils/visualizer.py,0,"b""import numpy as np\nimport cv2\n\nimport torch\n\nfrom segmentoly.utils.visualizer import Visualizer\nfrom text_spotting.data.alphabet import AlphabetDecoder\n\nclass TextVisualizer(Visualizer):\n\n    def __init__(self, text_confidence_threshold, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.alphabet_decoder = AlphabetDecoder()\n        self.text_confidence_threshold = text_confidence_threshold\n\n    def __call__(self, image, boxes, classes, scores, segms=None, ids=None, text_log_softmax=None):\n        result = image.copy()\n\n        # Filter out detections with low confidence.\n        filter_mask = scores > self.confidence_threshold\n        scores = scores[filter_mask]\n        classes = classes[filter_mask]\n        boxes = boxes[filter_mask]\n        texts = []\n        if len(text_log_softmax):\n            text_log_softmax = text_log_softmax[filter_mask]\n            texts = np.argmax(text_log_softmax, 2)\n            texts_confs = np.exp(np.max(text_log_softmax, 2))\n            texts_confs = np.mean(texts_confs, axis=1)\n            texts = [self.alphabet_decoder.decode(t) if texts_confs[i] > self.text_confidence_threshold else '' for i, t in enumerate(texts) ]\n\n        if self.show_masks and segms is not None:\n            segms = list(segm for segm, show in zip(segms, filter_mask) if show)\n            result = self.overlay_masks(result, segms, classes, texts, ids)\n\n        if self.show_boxes:\n            result = self.overlay_boxes(result, boxes, classes, texts)\n\n        result = self.overlay_texts(result, boxes, scores, texts, show_score=self.show_scores)\n\n        return result\n\n    def overlay_boxes(self, image, boxes, classes, show):\n        colors = self.compute_colors_for_labels(classes).tolist()\n        for box, color, shoud_show in zip(boxes, colors, show):\n            if shoud_show:\n                box = box.astype(int)\n                top_left, bottom_right = box[:2].tolist(), box[2:].tolist()\n                image = cv2.rectangle(\n                    image, tuple(top_left), tuple(bottom_right), tuple(color), 1\n                )\n        return image\n\n    def overlay_masks(self, image, masks, classes, show, ids=None):\n        colors = self.compute_colors_for_labels(classes).tolist()\n\n        segments_image = image.copy()\n        aggregated_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n        aggregated_colored_mask = np.zeros(image.shape, dtype=np.uint8)\n        black = np.zeros(3, dtype=np.uint8)\n\n        for i, (mask, color, shoud_show) in enumerate(zip(masks, colors, show)):\n            if shoud_show:\n                mask = mask.astype(np.uint8)\n                color_idx = i if ids is None else ids[i]\n                mask_color = self.instance_color_palette[color_idx % len(self.instance_color_palette)].tolist()\n                cv2.bitwise_or(aggregated_mask, mask, dst=aggregated_mask)\n                cv2.bitwise_or(aggregated_colored_mask, np.asarray(mask_color, dtype=np.uint8),\n                               dst=aggregated_colored_mask, mask=mask)\n\n        # Fill the area occupied by all instances with a colored instances mask image.\n        cv2.bitwise_and(segments_image, black, dst=segments_image, mask=aggregated_mask)\n        cv2.bitwise_or(segments_image, aggregated_colored_mask, dst=segments_image, mask=aggregated_mask)\n        # Blend original image with the one, where instances are colored.\n        # As a result instances masks become transparent.\n        cv2.addWeighted(image, 0.5, segments_image, 0.5, 0, dst=image)\n\n        return image\n\n\n    def overlay_texts(self, image, boxes, scores, texts, show_score=True):\n        template = '{}: {:.2f}' if show_score else '{}'\n        white = (255, 255, 255)\n\n        for box, score, label in zip(boxes, scores, texts):\n            if label:\n                s = template.format(label, score)\n                textsize = cv2.getTextSize(s, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n                position = ((box[:2] + box[2:] - textsize) / 2).astype(int)\n                cv2.putText(image, s, tuple(position), cv2.FONT_HERSHEY_SIMPLEX, .5, white, 1)\n\n        return image"""
tensorflow_toolkit/action_detection/action_detection/nn/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/action_detection/postprocessing/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/action_detection/postprocessing/detection_output.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom collections import namedtuple\n\nimport numpy as np\n\nfrom action_detection.postprocessing.metrics import matrix_iou\n\n\nDetections = namedtuple(\'Detections\', \'loc, scores\')\nActions = namedtuple(\'Detections\', \'loc, scores, action_labels, action_scores, id\')\n\n\ndef nms(input_bboxes, input_scores, threshold, keep_top_k, min_score=0.01):\n    """"""Carry out default NMS algorithm over the input boxes.\n\n    :param input_bboxes: Input boxes\n    :param input_scores: Detection scores of boxes\n    :param threshold: Min IoU value to merge boxes\n    :param keep_top_k: Max number of boxes to output\n    :param min_score: Min score value to output box\n    :return: Filtered box IDs\n    """"""\n\n    if len(input_bboxes) == 0:\n        return []\n\n    if len(input_bboxes) > keep_top_k:\n        indices = np.argsort(-input_scores)[:keep_top_k]\n        scores = input_scores[indices]\n        bboxes = input_bboxes[indices]\n    else:\n        scores = np.copy(input_scores)\n        indices = np.arange(len(scores))\n        bboxes = input_bboxes\n\n    similarity_matrix = matrix_iou(bboxes, bboxes)\n\n    out_ids = []\n    for _ in xrange(len(bboxes)):\n        bbox_id = np.argmax(scores)\n        bbox_score = scores[bbox_id]\n        if bbox_score < min_score:\n            break\n\n        out_ids.append(indices[bbox_id])\n        scores[bbox_id] = 0.0\n\n        iou_values = similarity_matrix[bbox_id]\n        scores[iou_values > threshold] = 0.0\n\n    return np.array(out_ids, dtype=np.int32)\n\n\ndef soft_nms(input_bboxes, input_scores, keep_top_k, sigma, min_score):\n    """"""Carry out Soft-NMS algorithm over the input boxes.\n\n    :param input_bboxes: Input boxes\n    :param input_scores: Detection scores of boxes\n    :param keep_top_k: Max number of boxes to output\n    :param sigma: Algorithm parameter\n    :param min_score: Min score value to output box\n    :return: Filtered box IDs\n    """"""\n\n    if len(input_bboxes) == 0:\n        return [], []\n\n    if len(input_bboxes) > keep_top_k:\n        indices = np.argsort(-input_scores)[:keep_top_k]\n        scores = input_scores[indices]\n        bboxes = input_bboxes[indices]\n    else:\n        scores = np.copy(input_scores)\n        indices = np.arange(len(scores))\n        bboxes = input_bboxes\n\n    similarity_matrix = matrix_iou(bboxes, bboxes)\n\n    out_ids = []\n    out_scores = []\n    for _ in xrange(len(bboxes)):\n        bbox_id = np.argmax(scores)\n        bbox_score = scores[bbox_id]\n        if bbox_score < min_score:\n            break\n\n        out_ids.append(indices[bbox_id])\n        out_scores.append(bbox_score)\n        scores[bbox_id] = 0.0\n\n        iou_values = similarity_matrix[bbox_id]\n        scores *= np.exp(np.negative(np.square(iou_values) / sigma))\n\n    return np.array(out_ids, dtype=np.int32), np.array(out_scores, dtype=np.float32)\n\n\ndef ssd_detection_output(batch_bboxes, batch_conf, bg_class, min_conf=0.01, out_top_k=200,\n                         nms_overlap=0.45, nms_top_k=400):\n    """"""Process network output to translate it into the bboxes with labels.\n\n    :param batch_bboxes: All bboxes\n    :param batch_conf: All detection scores\n    :param bg_class: ID of background class\n    :param min_conf: Min score value to output box\n    :param out_top_k: Max number of boxes per image to output\n    :param nms_overlap: NMS parameter\n    :param nms_top_k: NMS parameter\n    :return: List of detections\n    """"""\n\n    assert batch_bboxes.shape[:2] == batch_conf.shape[:2]\n    assert batch_bboxes.shape[2] == 4\n\n    num_classes = batch_conf.shape[-1]\n    assert num_classes > 1\n\n    all_detections = []\n    for sample_id in xrange(batch_bboxes.shape[0]):\n        sample_bboxes = batch_bboxes[sample_id]\n        sample_conf = batch_conf[sample_id]\n\n        all_sample_detections = []\n        for label in xrange(num_classes):\n            if label == bg_class:\n                continue\n\n            sample_scores = sample_conf[:, label]\n\n            valid_mask = sample_scores > min_conf\n            # noinspection PyTypeChecker\n            if np.sum(valid_mask) == 0:\n                continue\n\n            valid_bboxes = sample_bboxes[valid_mask]\n            valid_scores = sample_scores[valid_mask]\n\n            merged_ids = nms(valid_bboxes, valid_scores, nms_overlap, nms_top_k)\n            if len(merged_ids) > 0:\n                out_bboxes = valid_bboxes[merged_ids].reshape([-1, 4])\n                out_scores = valid_scores[merged_ids].reshape([-1])\n\n                for i in xrange(len(out_scores)):\n                    all_sample_detections.append((out_bboxes[i], label, out_scores[i]))\n\n        if len(all_sample_detections) > out_top_k:\n            all_sample_detections.sort(key=lambda tup: tup[2], reverse=True)\n            all_sample_detections = all_sample_detections[:out_top_k]\n\n        sample_detections = {}\n        for bbox, label, score in all_sample_detections:\n            if label not in sample_detections:\n                sample_detections[label] = {\'loc\': [bbox],\n                                            \'scores\': [score]}\n            else:\n                last_data = sample_detections[label]\n                last_data[\'loc\'].append(bbox)\n                last_data[\'scores\'].append(score)\n\n        out_sample_detections = {label: Detections(loc=np.stack(sample_detections[label][\'loc\']),\n                                                   scores=np.stack(sample_detections[label][\'scores\']))\n                                 for label in sample_detections}\n\n        all_detections.append(out_sample_detections)\n\n    return all_detections\n\n\ndef ssd_warp_gt(batch_bboxes, batch_labels, bg_class):\n    """"""Translates Ground truth boxes and labels into the internal format.\n\n    :param batch_bboxes: Bbox coordinates\n    :param batch_labels: Bbox labels\n    :param bg_class: ID of background label\n    :return: List of boxes\n    """"""\n\n    assert batch_bboxes.shape[0] == batch_labels.shape[0]\n\n    all_gt = []\n    for sample_id in xrange(batch_bboxes.shape[0]):\n        sample_bboxes = batch_bboxes[sample_id]\n        sample_labels = batch_labels[sample_id]\n\n        valid_mask = np.logical_and(sample_labels >= 0, sample_labels != bg_class)\n        if np.sum(valid_mask) == 0:\n            all_gt.append([])\n            continue\n\n        valid_bboxes = sample_bboxes[valid_mask]\n        valid_labels = sample_labels[valid_mask]\n\n        unique_labels = np.unique(valid_labels)\n        sample_detections = {}\n        for label in unique_labels:\n            label_mask = valid_labels == label\n            class_bboxes = valid_bboxes[label_mask]\n\n            sample_detections[label] = Detections(loc=class_bboxes, scores=None)\n\n        all_gt.append(sample_detections)\n\n    return all_gt\n\n\ndef action_detection_output(batch_bboxes, batch_det_conf, batch_action_conf, bg_class,\n                            min_det_conf=0.01, min_action_conf=0.01, out_top_k=400,\n                            nms_top_k=400, nms_sigma=0.6, do_nms=True):\n    """"""Process network output to translate it into the bboxes with detection scores and action labels.\n\n    :param batch_bboxes: All bboxes\n    :param batch_det_conf: All detection scores\n    :param batch_action_conf: All action scores\n    :param bg_class: ID of background class\n    :param min_det_conf: Min score value to output box\n    :param min_action_conf: Min score value for action confidence\n    :param out_top_k: Max number of boxes per image to output\n    :param nms_top_k: NMS parameter\n    :param nms_sigma: NMS parameter\n    :param do_nms: Whether to run NMS algorithm\n    :return: List of detections\n    """"""\n\n    assert batch_bboxes.shape[:2] == batch_det_conf.shape[:2]\n    assert batch_bboxes.shape[:2] == batch_action_conf.shape[:2]\n    assert batch_bboxes.shape[2] == 4\n\n    num_det_classes = batch_det_conf.shape[-1]\n    assert num_det_classes == 2\n\n    num_action_classes = batch_action_conf.shape[-1]\n    assert num_action_classes > 1\n\n    det_class = (bg_class + 1) % 2\n\n    all_detections = []\n    for sample_id in xrange(batch_bboxes.shape[0]):\n        sample_bboxes = batch_bboxes[sample_id]\n        sample_det_scores = batch_det_conf[sample_id, :, det_class]\n        sample_action_conf = batch_action_conf[sample_id]\n\n        valid_mask = sample_det_scores > min_det_conf\n        # noinspection PyTypeChecker\n        if np.sum(valid_mask) == 0:\n            all_detections.append({det_class: []})\n            continue\n\n        valid_bboxes = sample_bboxes[valid_mask]\n        valid_det_scores = sample_det_scores[valid_mask]\n        valid_det_conf = sample_action_conf[valid_mask]\n\n        if do_nms:\n            filtered_ids, filtered_scores = soft_nms(valid_bboxes, valid_det_scores, nms_top_k, nms_sigma, min_det_conf)\n        else:\n            filtered_scores = np.copy(valid_det_scores)\n            filtered_ids = np.argsort(-filtered_scores)\n\n        if len(filtered_ids) > 0:\n            out_bboxes = valid_bboxes[filtered_ids].reshape([-1, 4])\n            out_det_scores = filtered_scores.reshape([-1])\n            out_action_conf = valid_det_conf[filtered_ids].reshape([-1, num_action_classes])\n\n            if 0 < out_top_k < len(out_det_scores):\n                out_bboxes = out_bboxes[:out_top_k]\n                out_det_scores = out_det_scores[:out_top_k]\n                out_action_conf = out_action_conf[:out_top_k]\n\n            out_action_label = np.argmax(out_action_conf, axis=-1)\n            out_action_score = np.max(out_action_conf, axis=-1)\n\n            if min_action_conf is not None and min_action_conf > 0.0:\n                out_action_label[out_action_score < min_action_conf] = 0\n\n            sample_detections = Actions(loc=out_bboxes,\n                                        scores=out_det_scores,\n                                        action_labels=out_action_label,\n                                        action_scores=out_action_score,\n                                        id=None)\n            all_detections.append({det_class: sample_detections})\n        else:\n            all_detections.append({det_class: []})\n            continue\n\n    return all_detections\n\n\ndef action_warp_gt(batch_bboxes, batch_labels, bg_class, batch_track_ids=None):\n    """"""Translates Ground truth boxes and actions into the internal format.\n\n    :param batch_bboxes: Bbox coordinates\n    :param batch_labels: Bbox labels\n    :param bg_class: ID of background label\n    :param batch_track_ids: ID of track in a batch\n    :return: List of boxes\n    """"""\n\n    assert batch_bboxes.shape[0] == batch_labels.shape[0]\n\n    det_class = (bg_class + 1) % 2\n\n    all_gt = []\n    for sample_id in xrange(batch_bboxes.shape[0]):\n        sample_bboxes = batch_bboxes[sample_id]\n        sample_labels = batch_labels[sample_id]\n        sample_track_ids = batch_track_ids[sample_id] if batch_track_ids is not None else None\n\n        valid_mask = sample_labels >= 0\n        # noinspection PyTypeChecker\n        if np.sum(valid_mask) == 0:\n            all_gt.append([])\n            continue\n\n        valid_bboxes = sample_bboxes[valid_mask]\n        valid_labels = sample_labels[valid_mask]\n        valid_track_ids = sample_track_ids[valid_mask] if sample_track_ids is not None else None\n\n        sample_detections = {det_class: Actions(loc=valid_bboxes,\n                                                scores=None,\n                                                action_labels=valid_labels,\n                                                action_scores=None,\n                                                id=valid_track_ids)}\n\n        all_gt.append(sample_detections)\n\n    return all_gt\n'"
tensorflow_toolkit/action_detection/action_detection/postprocessing/metrics.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport numpy as np\n\n\ndef iou(box_a, box_b):\n    """"""Calculates IoU metric between two specified boxes.\n\n    :param box_a: Coordinates of first box\n    :param box_b: Coordinates of second box\n    :return: Scalar value of UoU metric\n    """"""\n\n    intersect_ymin = np.maximum(box_a[0], box_b[0])\n    intersect_xmin = np.maximum(box_a[1], box_b[1])\n    intersect_ymax = np.minimum(box_a[2], box_b[2])\n    intersect_xmax = np.minimum(box_a[3], box_b[3])\n\n    intersect_height = np.maximum(0.0, intersect_ymax - intersect_ymin)\n    intersect_width = np.maximum(0.0, intersect_xmax - intersect_xmin)\n\n    intersect_area = intersect_height * intersect_width\n    area_a = (box_a[2] - box_a[0]) * (box_a[3] - box_a[1])\n    area_b = (box_b[2] - box_b[0]) * (box_b[3] - box_b[1])\n\n    union_area = area_a + area_b - intersect_area\n\n    overlap_ratio = intersect_area / union_area if union_area > 0.0 else 0.0\n\n    return overlap_ratio\n\n\ndef matrix_iou(set_a, set_b):\n    """"""Calculates IoU metric between all pairs of presented sets of boxes.\n\n    :param set_a: First set of boxes\n    :param set_b: Second set of boxes\n    :return: Matrix of IoU metrics\n    """"""\n\n    intersect_ymin = np.maximum(set_a[:, 0].reshape([-1, 1]), set_b[:, 0].reshape([1, -1]))\n    intersect_xmin = np.maximum(set_a[:, 1].reshape([-1, 1]), set_b[:, 1].reshape([1, -1]))\n    intersect_ymax = np.minimum(set_a[:, 2].reshape([-1, 1]), set_b[:, 2].reshape([1, -1]))\n    intersect_xmax = np.minimum(set_a[:, 3].reshape([-1, 1]), set_b[:, 3].reshape([1, -1]))\n\n    intersect_heights = np.maximum(0.0, intersect_ymax - intersect_ymin)\n    intersect_widths = np.maximum(0.0, intersect_xmax - intersect_xmin)\n\n    intersect_areas = intersect_heights * intersect_widths\n    areas_set_a = ((set_a[:, 2] - set_a[:, 0]) * (set_a[:, 3] - set_a[:, 1])).reshape([-1, 1])\n    areas_set_b = ((set_b[:, 2] - set_b[:, 0]) * (set_b[:, 3] - set_b[:, 1])).reshape([1, -1])\n\n    areas_set_a[np.less(areas_set_a, 0.0)] = 0.0\n    areas_set_b[np.less(areas_set_b, 0.0)] = 0.0\n\n    union_areas = areas_set_a + areas_set_b - intersect_areas\n\n    overlaps = intersect_areas / union_areas\n    overlaps[np.less_equal(union_areas, 0.0)] = 0.0\n\n    return overlaps\n'"
tensorflow_toolkit/action_detection/action_detection/postprocessing/quality.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n# pylint: disable=invalid-name\n\nfrom bisect import bisect\nfrom collections import namedtuple\n\nimport numpy as np\nfrom tqdm import trange\n\nfrom action_detection.postprocessing.metrics import iou, matrix_iou\n\n\nMatchDesc = namedtuple(\'MatchDesc\', \'gt_bbox, gt_label, pred_bbox, pred_label\')\n\n\ndef calc_map_mr(predictions, gt, min_iou=0.5, fppi_level=0.1, return_all=False):\n    """"""Calculates mAP and miss-rate metrics.\n\n    :param predictions: Predicted boxes\n    :param gt: Ground truth boxes\n    :param min_iou: Min IoU value to match GT box\n    :param fppi_level: FPPI level to calculate miss-rate metric\n    :param return_all: Whether to return metrics by class\n    :return: List of metric values\n    """"""\n\n    def _match():\n        total_num_candidates = 0\n        total_num_matches = 0\n        out_matches = {}\n\n        for sample_id in trange(len(predictions), desc=\'Matching\'):\n            sample_predictions = predictions[sample_id]\n            sample_gt = gt[sample_id]\n\n            predicted_labels = list(sample_predictions)\n            gt_labels = list(sample_gt)\n\n            for label in predicted_labels:\n                if label in gt_labels:\n                    predicted_label_data = sample_predictions[label]\n                    gt_bboxes = sample_gt[label].loc\n\n                    sorted_ind = np.argsort(-predicted_label_data.scores)\n                    predicted_bboxes = predicted_label_data.loc[sorted_ind]\n                    predicted_scores = predicted_label_data.scores[sorted_ind]\n\n                    similarity_matrix = matrix_iou(predicted_bboxes, gt_bboxes)\n\n                    matches = []\n                    visited_gt = np.zeros(gt_bboxes.shape[0], dtype=np.bool)\n                    for predicted_id in xrange(predicted_bboxes.shape[0]):\n\n                        best_overlap = 0.0\n                        best_gt_id = -1\n                        for gt_id in xrange(gt_bboxes.shape[0]):\n                            if visited_gt[gt_id]:\n                                continue\n\n                            overlap_value = similarity_matrix[predicted_id, gt_id]\n                            if overlap_value > best_overlap:\n                                best_overlap = overlap_value\n                                best_gt_id = gt_id\n\n                        if best_gt_id >= 0 and best_overlap > min_iou:\n                            visited_gt[best_gt_id] = True\n\n                            matches.append(predicted_id)\n                            if len(matches) >= len(gt_bboxes):\n                                break\n\n                    tp = np.zeros([len(predicted_bboxes)], dtype=np.int32)\n                    tp[matches] = 1\n\n                    total_num_candidates += gt_bboxes.shape[0]\n                    total_num_matches += len(matches)\n\n                    if label not in out_matches:\n                        out_matches[label] = {\'tp\': tp,\n                                              \'scores\': predicted_scores,\n                                              \'num_images\': 1,\n                                              \'num_gt\': len(gt_bboxes)}\n                    else:\n                        last_data = out_matches[label]\n                        out_matches[label] = {\'tp\': np.append(last_data[\'tp\'], tp),\n                                              \'scores\': np.append(last_data[\'scores\'], predicted_scores),\n                                              \'num_images\': last_data[\'num_images\'] + 1,\n                                              \'num_gt\': last_data[\'num_gt\'] + len(gt_bboxes)}\n                else:\n                    tp = np.zeros([len(sample_predictions[label].scores)], dtype=np.int32)\n\n                    if label not in out_matches:\n                        out_matches[label] = {\'tp\': tp,\n                                              \'scores\': sample_predictions[label].scores,\n                                              \'num_images\': 1,\n                                              \'num_gt\': 0}\n                    else:\n                        last_data = out_matches[label]\n                        out_matches[label] = {\'tp\': np.append(last_data[\'tp\'], tp),\n                                              \'scores\': np.append(last_data[\'scores\'],\n                                                                  sample_predictions[label].scores),\n                                              \'num_images\': last_data[\'num_images\'] + 1,\n                                              \'num_gt\': last_data[\'num_gt\']}\n\n        matched_ratio = float(total_num_matches) / float(total_num_candidates) if total_num_candidates > 0 else 0.0\n        print(\'Matched GT bbox: {} / {} ({:.3f}%)\'\n              .format(total_num_matches, total_num_candidates, 1e2 * matched_ratio))\n\n        return out_matches\n\n    def _get_metrics(tp, scores, num_images, num_gt):\n        def _ap(in_recall, in_precision):\n            mrec = np.concatenate(([0.], in_recall, [1.]))\n            mpre = np.concatenate(([0.], in_precision, [0.]))\n\n            for i in range(mpre.size - 1, 0, -1):\n                mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n            i = np.where(mrec[1:] != mrec[:-1])[0]\n\n            return np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\n        def _miss_rate(miss_rates, fppis):\n            position = bisect(fppis, fppi_level)\n            p1 = position - 1\n            p2 = position if position < len(miss_rates) else p1\n            return 0.5 * (miss_rates[p1] + miss_rates[p2])\n\n        sorted_ind = np.argsort(-scores)\n\n        tp_sorted = np.copy(tp[sorted_ind])\n        fp_sorted = np.logical_not(tp)[sorted_ind]\n\n        tp_cumsum = np.cumsum(tp_sorted)\n        fp_cumsum = np.cumsum(fp_sorted)\n\n        ind = len(scores) - np.unique(scores[sorted_ind[::-1]], return_index=True)[1] - 1\n        ind = ind[::-1]\n\n        tp_cumsum = tp_cumsum[ind]\n        fp_cumsum = fp_cumsum[ind]\n\n        recall = tp_cumsum / float(num_gt)\n        precision = tp_cumsum / np.maximum(tp_cumsum + fp_cumsum, np.finfo(np.float64).eps)\n        miss_rates_values = 1.0 - recall\n        fppis_values = fp_cumsum / float(num_images)\n\n        # noinspection PyTypeChecker\n        mr = _miss_rate(miss_rates_values, fppis_values)\n        ap = _ap(recall, precision)\n\n        return ap, mr\n\n    all_matches = _match()\n\n    ap_values = []\n    mr_values = []\n    metrics_by_class = {}\n    for class_id, m in all_matches.iteritems():\n        if len(m[\'tp\']) == 0 or np.sum(m[\'tp\']) == 0:\n            ap_value, mr_value = 0.0, 1.0\n        else:\n            print(\'Debug. Num scores: {}, Num TP: {}, Num GT: {}, Num images: {}\'\n                  .format(len(m[\'scores\']), len(m[\'tp\']), m[\'num_gt\'], m[\'num_images\']))\n\n            ap_value, mr_value = _get_metrics(m[\'tp\'], m[\'scores\'], m[\'num_images\'], m[\'num_gt\'])\n        ap_values.append(ap_value)\n        mr_values.append(mr_value)\n\n        metrics_by_class[class_id] = ap_value, mr_value\n\n    if return_all:\n        return metrics_by_class\n    else:\n        map_metric = np.mean(ap_values) if len(ap_values) > 0 else 0.0\n        mean_mr_metric = np.mean(mr_values) if len(mr_values) > 0 else 1.0\n        return map_metric, mean_mr_metric\n\n\ndef calc_action_accuracy(predictions, gt, bg_class, num_classes, min_iou=0.5):\n    """"""Calculates action normalized accuracy.\n\n    :param predictions: Predicted boxes\n    :param gt: Ground truth boxes\n    :param bg_class: ID of background class\n    :param num_classes: Number of classes\n    :param min_iou: Min IoU value to match GT box\n    :return: Accuracy scalar value\n    """"""\n\n    det_class = (bg_class + 1) % 2\n\n    def _match(predicted_data, gt_data):\n        total_num_candidates = 0\n        total_num_matches = 0\n\n        out_matches = {}\n        for sample_id in trange(len(predicted_data), desc=\'Matching\'):\n            sample_predictions = predicted_data[sample_id][det_class]\n            sample_gt = gt_data[sample_id][det_class]\n\n            predicted_bboxes = sample_predictions.loc\n            predicted_scores = sample_predictions.scores\n            gt_bboxes = sample_gt.loc\n\n            sorted_ind = np.argsort(-predicted_scores)\n            predicted_bboxes = predicted_bboxes[sorted_ind]\n            predicted_scores = predicted_scores[sorted_ind]\n            predicted_original_ids = np.arange(len(predicted_scores))[sorted_ind]\n\n            similarity_matrix = matrix_iou(predicted_bboxes, gt_bboxes)\n\n            matches = []\n            visited_gt = np.zeros(gt_bboxes.shape[0], dtype=np.bool)\n            for predicted_id in xrange(predicted_bboxes.shape[0]):\n                best_overlap = 0.0\n                best_gt_id = -1\n                for gt_id in xrange(gt_bboxes.shape[0]):\n                    if visited_gt[gt_id]:\n                        continue\n\n                    overlap_value = similarity_matrix[predicted_id, gt_id]\n                    if overlap_value > best_overlap:\n                        best_overlap = overlap_value\n                        best_gt_id = gt_id\n\n                if best_gt_id >= 0 and best_overlap > min_iou:\n                    visited_gt[best_gt_id] = True\n\n                    matches.append((best_gt_id, predicted_original_ids[predicted_id]))\n                    if len(matches) >= len(gt_bboxes):\n                        break\n\n            out_matches[sample_id] = {det_class: matches}\n\n            total_num_candidates += gt_bboxes.shape[0]\n            total_num_matches += len(matches)\n\n        matched_ratio = float(total_num_matches) / float(total_num_candidates) if total_num_candidates > 0 else 0.0\n        print(\'Matched GT bbox: {} / {} ({:.3f}%)\'\n              .format(total_num_matches, total_num_candidates, 1e2 * matched_ratio))\n\n        return out_matches\n\n    def _confusion_matrix(all_matched_ids, predicted_data, gt_data):\n        out_cm = np.zeros([num_classes, num_classes], dtype=np.int32)\n        for sample_id in all_matched_ids:\n            sample_matched_ids = all_matched_ids[sample_id][det_class]\n            sample_gt_labels = gt_data[sample_id][det_class].action_labels\n            sample_pred_labels = predicted_data[sample_id][det_class].action_labels\n\n            for gt_id, pred_id in sample_matched_ids:\n                gt_label = sample_gt_labels[gt_id]\n                pred_label = sample_pred_labels[pred_id]\n\n                if gt_label >= num_classes:\n                    continue\n\n                out_cm[gt_label, pred_label] += 1\n\n        return out_cm\n\n    all_matches = _match(predictions, gt)\n    cm = _confusion_matrix(all_matches, predictions, gt)\n\n    return cm\n'"
tensorflow_toolkit/action_detection/tools/data/cluster_priors.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nfrom os import walk\nfrom os.path import join, exists\nfrom argparse import ArgumentParser\n\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\nclass KMeans(object):\n    """"""Presents algorithm to carry out KMeans clusterization over the prior boxes.\n    """"""\n\n    def __init__(self, train_data, test_data, num_clusters, num_iters=10, eps=1e-5, min_cluster_size=50):\n        """"""Constructor.\n\n        :param train_data: Training data\n        :param test_data: Testing data\n        :param num_clusters: Number of clusters\n        :param num_iters: Max Number of iterations\n        :param eps: Epsilon to control minimal changes value\n        :param min_cluster_size: Minimal size of cluster to keep it\n        """"""\n\n        self.num_clusters = num_clusters\n        self.num_iters = num_iters\n        assert self.num_iters is not None and self.num_iters > 0\n\n        self.train_data = train_data\n        self.test_data = test_data\n\n        self.eps = eps\n        self.centers = None\n        self.train_labels = None\n        self.test_labels = None\n        self.min_cluster_size = min_cluster_size\n\n    @staticmethod\n    def _iou(sample, candidates):\n        """"""Calculates IoU metric between all pairs of sample and specified set.\n\n        :param sample: Sample box\n        :param candidates: Set of boxes\n        :return: List of IoU values\n        """"""\n\n        sample_half_h = 0.5 * sample[0]\n        sample_half_w = 0.5 * sample[1]\n        sample_bbox = np.array([-sample_half_w, -sample_half_h,\n                                sample_half_w, sample_half_h], dtype=np.float32)\n        sample_bbox_size = sample[0] * sample[1]\n\n        candidates_half_h = 0.5 * candidates[:, 0]\n        candidates_half_w = 0.5 * candidates[:, 1]\n        candidates_bbox = [-candidates_half_w, -candidates_half_h,\n                           candidates_half_w, candidates_half_h]\n        candidates_bbox_size = candidates[:, 0] * candidates[:, 1]\n\n        inter_bbox = [np.maximum(sample_bbox[0], candidates_bbox[0]),\n                      np.maximum(sample_bbox[1], candidates_bbox[1]),\n                      np.minimum(sample_bbox[2], candidates_bbox[2]),\n                      np.minimum(sample_bbox[3], candidates_bbox[3])]\n        inter_size = (inter_bbox[2] - inter_bbox[0]) * (inter_bbox[3] - inter_bbox[1])\n\n        iou = inter_size / (sample_bbox_size + candidates_bbox_size - inter_size)\n\n        return iou\n\n    @staticmethod\n    def _estimate_data_borders(data):\n        """"""Estimates max and min valid values of input data.\n\n        :param data: Input data\n        :return: Tuple of min and max values\n        """"""\n\n        min_values = np.percentile(data, [25.0], axis=0)[0]\n        max_values = np.percentile(data, [75.0], axis=0)[0]\n        return min_values, max_values\n\n    @staticmethod\n    def _enhanced_centers(all_data, num_centers, metric, num_samples=1000, alpha=0.8, min_dist=0.0):\n        """"""Initializes centers by enhanced algorithm.\n\n        :param all_data: Data\n        :param num_centers: Number of centers to init\n        :param metric: Type of metric\n        :param num_samples: Number of samples to carry out on\n        :param alpha: Algorithm parameter\n        :param min_dist: Min distance to preserve new centers\n        :return: List of centers\n        """"""\n\n        num_samples = np.minimum(num_samples, all_data.shape[0])\n        rand_idx = np.random.randint(all_data.shape[0], size=num_samples)\n        data_subset = all_data[rand_idx, :]\n        size_thr = int(float(num_samples) / float(num_centers))\n\n        distances = np.empty([data_subset.shape[0], data_subset.shape[0]], dtype=np.float32)\n        for i in xrange(data_subset.shape[0]):\n            data_point = data_subset[i]\n            distances[i] = metric(data_point, data_subset)\n            distances[i, :(i + 1)] = 0.\n\n        factor = 1. / (1. - alpha)\n        point_metrics = np.maximum(0., factor * (distances - alpha))\n\n        points_weights = np.mean(point_metrics, axis=1)\n        points = sorted([(i, points_weights[i]) for i in xrange(data_subset.shape[0])],\n                        key=lambda tup: tup[1], reverse=True)\n\n        init_centers = []\n        for point in points:\n            if len(init_centers) == 0:\n                init_centers.append([point[0], [point[0]], point[1]])\n            else:\n                max_dist = 0\n                best_center_id = None\n                for j in xrange(len(init_centers)):\n                    cur_distance = distances[point[0], init_centers[j][0]]\n\n                    if cur_distance > max_dist:\n                        max_dist = cur_distance\n                        best_center_id = j\n\n                if max_dist > min_dist and\\\n                   best_center_id is not None and\\\n                   len(init_centers[best_center_id][1]) < size_thr:\n                    init_centers[best_center_id][1].append(point[0])\n                    init_centers[best_center_id][2] += point[1]\n                else:\n                    init_centers.append([point[0], [point[0]], point[1]])\n        print(\'Number of init centers: {}\'.format(len(init_centers)))\n\n        centers_nodes = sorted([(i, init_centers[i][2] / float(len(init_centers[i][1])))\n                                for i in xrange(len(init_centers))], key=lambda tup: tup[1], reverse=True)\n\n        centers = [data_subset[init_centers[centers_nodes[i][0]][0]] for i in xrange(num_centers)]\n        centers = np.vstack(centers)\n\n        return centers\n\n    @staticmethod\n    def _cluster_points(data, centers, metric):\n        """"""Carry out clustering stage of KMeans algorithm.\n\n        :param data: Input data\n        :param centers: Current centers\n        :param metric: Type of metric to compare data points\n        :return: Clustered data points\n        """"""\n\n        def _init_clusters(num_clusters):\n            """"""Creates list of empty centers.\n\n            :param num_clusters: Target number of centers.\n            :return: List of init centers\n            """"""\n\n            init_clusters = []\n            for _ in xrange(num_clusters):\n                init_clusters.append([])\n            return init_clusters\n\n        clusters = _init_clusters(centers.shape[0])\n        for i in xrange(data.shape[0]):\n            data_point = data[i]\n            distances = metric(data_point, centers)\n            best_center_id = np.argmax(distances)\n            clusters[best_center_id].append(data_point)\n        return clusters\n\n    @staticmethod\n    def _estimate_clusters_dist(clusters, centers, metric, calc_mean=True, ignore_empty=True):\n        """"""Calculates distances to cluster centers.\n\n        :param clusters: Current clusters\n        :param centers: Current centers\n        :param metric: Type of metric\n        :param calc_mean: Whether to calculate the mean cluster distance instead\n        :param ignore_empty: Whether to ignore empty clusters\n        :return: List of distances\n        """"""\n\n        num_clusters = len(clusters)\n        assert num_clusters == centers.shape[0]\n\n        cluster_distances = []\n        for cluster_id in xrange(num_clusters):\n            if len(clusters[cluster_id]) > 0:\n                cluster_data = clusters[cluster_id]\n                all_distances = metric(centers[cluster_id], np.vstack(cluster_data))\n                cluster_distances.append(np.median(all_distances))\n            else:\n                if not ignore_empty:\n                    cluster_distances.append(0.)\n\n        if calc_mean:\n            return np.min(cluster_distances)\n        else:\n            return cluster_distances\n\n    @staticmethod\n    def _reevaluate_centers(clusters, min_values, max_values, min_cluster_size):\n        """"""Carry out reevaluating stage of KMeans algorithm.\n\n        :param clusters: Current clusters\n        :param min_values: Min possible value of new centers\n        :param max_values: Max possible value of new centers\n        :param min_cluster_size: Minimal size of cluster to preserve it\n        :return: List of new cluster centers\n        """"""\n\n        new_centers = []\n        for cluster_id in xrange(len(clusters)):\n            if len(clusters[cluster_id]) >= min_cluster_size:\n                cluster = np.vstack(clusters[cluster_id])\n                new_center = np.median(cluster[:, :2], axis=0)\n            else:\n                print(\'Created new random center\')\n                new_center = np.random.uniform(min_values, max_values)\n            new_centers.append(new_center)\n        return np.vstack(new_centers)\n\n    @staticmethod\n    def _estimate_converge(old_centers, new_centers):\n        """"""Calculates current delta of center changes.\n\n        :param old_centers: List of previous step centers\n        :param new_centers: List of current centers\n        :return: Delta value\n        """"""\n\n        diff = np.abs(old_centers - new_centers)\n        distances = np.max(diff, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance\n\n    @staticmethod\n    def _fit_data(data, centers, metric):\n        """"""Estimates cluster Ids for the specified data.\n\n        :param data: Input data\n        :param centers: Current cluster centers\n        :param metric: Type of metric\n        :return: Estimated cluster IDs\n        """"""\n\n        labels = np.zeros([data.shape[0]], dtype=np.int32)\n        for i in xrange(data.shape[0]):\n            data_point = data[i]\n            distances = metric(data_point, centers)\n            best_center_id = np.argmax(distances)\n            labels[i] = best_center_id\n\n        return labels\n\n    def find_centers(self):\n        """"""Carry out KMeans clustering algorithm.\n        """"""\n\n        min_values, max_values = self._estimate_data_borders(self.train_data[:, :2])\n        centers = self._enhanced_centers(self.train_data[:, :2], self.num_clusters, self._iou, num_samples=20000)\n\n        best_centers = None\n        best_step_id = None\n\n        print(\'\\nKMeans iterations:\')\n        for i in xrange(self.num_iters):\n            old_centers = centers\n            clusters = self._cluster_points(self.train_data, centers, self._iou)\n            centers = self._reevaluate_centers(clusters, min_values, max_values, self.min_cluster_size)\n\n            center_distances = self._estimate_converge(old_centers, centers)\n\n            train_clusters = self._cluster_points(self.train_data, centers, self._iou)\n            train_cluster_distances = self._estimate_clusters_dist(train_clusters, centers, self._iou)\n\n            test_clusters = self._cluster_points(self.test_data, centers, self._iou)\n            test_cluster_distances = self._estimate_clusters_dist(test_clusters, centers, self._iou)\n\n            best_centers = centers\n            best_step_id = i\n\n            print(\'   #{}: dist = {:.08f}; train mIoU = {:.05f}%; test mIoU = {:.05f}%\'\n                  .format(i, center_distances,\n                          train_cluster_distances * 100.,\n                          test_cluster_distances * 100.))\n\n            if center_distances < self.eps:\n                break\n        print(\'\\nFinished. Best step: {}\'.format(best_step_id))\n\n        train_clusters = self._cluster_points(self.train_data, best_centers, self._iou)\n        train_cluster_distances = self._estimate_clusters_dist(train_clusters, best_centers, self._iou,\n                                                               calc_mean=False, ignore_empty=False)\n\n        test_clusters = self._cluster_points(self.test_data, best_centers, self._iou)\n        test_cluster_distances = self._estimate_clusters_dist(test_clusters, best_centers, self._iou,\n                                                              calc_mean=False, ignore_empty=False)\n\n        assert len(train_clusters) == len(test_clusters)\n\n        self.centers = best_centers\n        self.train_labels = self._fit_data(self.train_data, best_centers, self._iou)\n        self.test_labels = self._fit_data(self.test_data, best_centers, self._iou)\n\n        print(\'\\nEstimated centers:\')\n        size_factor = 100. / self.train_data.shape[0]\n        for i in xrange(best_centers.shape[0]):\n            bbox_h = best_centers[i, 0]\n            bbox_w = best_centers[i, 1]\n\n            print(\'   #{}: {}; size = {:.03f}%; train mIoU = {:.05f}%; test mIoU = {:.05f}%\'\n                  .format(i, [bbox_h, bbox_w],\n                          size_factor * float(len(train_clusters[i])),\n                          train_cluster_distances[i] * 100.,\n                          test_cluster_distances[i] * 100.))\n\n    def plot_data(self):\n        """"""Plots data points.\n        """"""\n\n        assert self.centers is not None\n        assert self.train_labels is not None\n\n        num_clusters = self.centers.shape[0]\n        colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, num_clusters)]  # pylint: disable=no-member\n\n        for k, col in zip(xrange(num_clusters), colors):\n            my_members = self.train_labels == k\n            plt.plot(self.train_data[my_members, 0], self.train_data[my_members, 1], \'w\',\n                     markerfacecolor=col, marker=\'.\', alpha=0.7)\n\n        for k, col in zip(xrange(num_clusters), colors):\n            cluster_center = self.centers[k]\n            plt.plot(cluster_center[0], cluster_center[1], \'o\', markerfacecolor=col,\n                     markeredgecolor=\'k\', markersize=6)\n\n        plt.show()\n\n\ndef load_data(root_dir_path, extension=\'.json\'):\n    """"""Loads data points.\n\n    :param root_dir_path: Path to start data loading\n    :param extension: Valid file extensions\n    :return: Loaded data\n    """"""\n\n    list_of_files = []\n    for dir_path, _, file_names in walk(root_dir_path):\n        list_of_files += [join(dir_path, file_name) for file_name in file_names if file_name.endswith(extension)]\n\n    out_data = []\n    for file_path in tqdm(list_of_files, desc=\'Loading data\'):\n        with open(file_path, \'r\') as read_file:\n            bboxes = json.load(read_file)\n\n        for bbox in bboxes:\n            bbox_h = bbox[\'ymax\'] - bbox[\'ymin\']\n            bbox_w = bbox[\'xmax\'] - bbox[\'xmin\']\n            out_data.append((bbox_h, bbox_w))\n\n    return np.array(out_data, dtype=np.float32)\n\n\ndef filter_data(bbox_data, enable=True, ar_low_percentile=0.1, ar_top_percentile=99.9, data_fraction=1.0,\n                min_height=None, max_height=None):\n    """"""Filter data if it\'s too large.\n\n    :param bbox_data: Input data\n    :param enable: Whether to enable filtering\n    :param ar_low_percentile: Min aspect ration percentile\n    :param ar_top_percentile: Max aspect ration percentile\n    :param data_fraction: Fraction of data to preserve\n    :param min_height: Min box height\n    :param max_height: Max box height\n    :return: Filtered data\n    """"""\n\n    if enable:\n        aspect_ratios = bbox_data[:, 0] / bbox_data[:, 1]\n        low_border = np.percentile(aspect_ratios, [float(ar_low_percentile)], axis=0, keepdims=True)[0]\n        top_border = np.percentile(aspect_ratios, [float(ar_top_percentile)], axis=0, keepdims=True)[0]\n        mask = np.logical_and(aspect_ratios > low_border, aspect_ratios < top_border)\n        filtered_bbox_data = bbox_data[mask]\n\n        if min_height is not None:\n            filtered_bbox_data = bbox_data[bbox_data[:, 0] > min_height]\n\n        if max_height is not None:\n            filtered_bbox_data = bbox_data[bbox_data[:, 0] < max_height]\n\n        if data_fraction is not None and data_fraction < 1.0:\n            out_train_data_size = int(data_fraction * filtered_bbox_data.shape[0])\n            assert out_train_data_size > 0\n\n            idx = np.random.randint(filtered_bbox_data.shape[0], size=out_train_data_size)\n            filtered_bbox_data = filtered_bbox_data[idx]\n\n        print(\'Filtered data size: {}\'.format(len(filtered_bbox_data)))\n\n        return filtered_bbox_data\n    else:\n        return bbox_data\n\n\ndef print_data_stat(data, name):\n    """"""Prints input data statistics.\n\n    :param data: Input data\n    :param name: Header name\n    """"""\n\n    bbox_heights = data[:, 0]\n    aspect_ratios = data[:, 0] / data[:, 1]\n    print(\'{} data: {} samples\\n\'\n          \'   aspect ratios: min = {}, median = {}, max = {}\\n\'\n          \'   height: min = {}, median = {}, max = {}\'\n          .format(name, data.shape[0],\n                  np.min(aspect_ratios), np.median(aspect_ratios), np.max(aspect_ratios),\n                  np.min(bbox_heights), np.median(bbox_heights), np.max(bbox_heights)))\n\n\ndef main():\n    """"""Main function.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--train_path\', \'-t\', type=str, required=True, help=\'Path to directory with annotation files\')\n    parser.add_argument(\'--val_path\', \'-v\', type=str, required=True, help=\'Path to directory with annotation files\')\n    parser.add_argument(\'--num_clusters\', \'-n\', type=int, required=True, help=\'Number of clusters\')\n    parser.add_argument(\'--num_iters\', \'-i\', type=int, required=False, default=50, help=\'Number of iterations\')\n    parser.add_argument(\'--train_fraction\', type=float, required=False, default=0.5, help=\'Fraction of train subset\')\n    parser.add_argument(\'--val_fraction\', type=float, required=False, default=0.8, help=\'Fraction of val subset\')\n    parser.add_argument(\'--min_height\', type=float, required=False, default=None, help=\'Min box height\')\n    parser.add_argument(\'--max_height\', type=float, required=False, default=None, help=\'Max box height\')\n    args = parser.parse_args()\n\n    assert exists(args.train_path)\n    assert exists(args.val_path)\n    assert args.num_clusters > 0\n    assert args.num_iters > 0\n\n    train_data = filter_data(load_data(args.train_path), data_fraction=args.train_fraction,\n                             min_height=args.min_height, max_height=args.max_height)\n    test_data = filter_data(load_data(args.val_path), data_fraction=args.val_fraction,\n                            min_height=args.min_height, max_height=args.max_height)\n\n    print_data_stat(train_data, \'Train\')\n    print_data_stat(test_data, \'Test\')\n\n    kmeans = KMeans(train_data, test_data, num_clusters=args.num_clusters, num_iters=args.num_iters)\n    kmeans.find_centers()\n    kmeans.plot_data()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/data/draw_annotation.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nfrom os import makedirs\nfrom os.path import exists, join, basename\nfrom shutil import rmtree\nfrom argparse import ArgumentParser\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom scipy.io import loadmat\nfrom json import dump as json_dump\n\n\ndef parse_annotation(annot_path, input_height, input_width):\n    """"""Loads annotation from the .mat file.\n\n    :param annot_path: Path to annotation file\n    :param input_height: Input image height\n    :param input_width: Input image width\n    :return: Dict with annotation\n    """"""\n\n    mat = loadmat(annot_path)\n\n    annot_header_name = [header for header in list(mat) if header.startswith(\'anno\')][0]\n    in_data = mat[annot_header_name][0]\n\n    num_images = len(in_data)\n\n    out_data = {}\n    for image_id in xrange(num_images):\n        image_data = in_data[image_id][0, 0]\n\n        image_name = image_data[1][0]\n        in_detections = image_data[2]\n\n        out_detections = [{\'label\': int(det[0]),\n                           \'xmin\': float(det[1]) / float(input_width),\n                           \'ymin\': float(det[2]) / float(input_height),\n                           \'xmax\': float(det[1] + det[3]) / float(input_width),\n                           \'ymax\': float(det[2] + det[4]) / float(input_height),\n                           \'track_id\': int(det[5]),\n                           \'occluded\': False}\n                                for det in in_detections]\n\n        filtered_detections = [det for det in out_detections if det[\'label\'] not in [0, 5]]\n        if len(filtered_detections) == 0:\n            continue\n\n        out_data[image_name] = filtered_detections\n\n    return out_data\n\n\ndef create_dir(dir_path):\n    """"""Creates directory if needed.\n\n    :param dir_path: Path to new directory\n    """"""\n\n    if exists(dir_path):\n        rmtree(dir_path)\n\n    makedirs(dir_path)\n\n\ndef save_data_paths(data, out_path):\n    """"""Saves paths to data into specified file.\n\n    :param data: data to save\n    :param out_path: Path to save\n    """"""\n\n    with open(out_path, \'w\') as input_stream:\n        for image_path, annot_path in tqdm(data, desc=\'Dumping image paths\'):\n            input_stream.write(\'{} {}\\n\'.format(image_path, annot_path))\n\n\ndef main():\n    """"""Main function.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--images_dir\', \'-i\', type=str, required=True, help=\'Path to directory with images\')\n    parser.add_argument(\'--annot\', \'-a\', type=str, required=True, help=\'Path to .mat file with annotation\')\n    parser.add_argument(\'--out_dir\', \'-o\', type=str, required=True, help=\'Path to save the converted annotation\')\n    parser.add_argument(\'--input_size\', type=str, required=False, default=\'1024,2048\', help=\'Input image size: HxW\')\n    args = parser.parse_args()\n\n    assert exists(args.images_dir)\n    assert exists(args.annot)\n\n    input_height, input_width = map(int, args.input_size.split(\',\'))\n\n    annotation_dir = join(args.out_dir, \'annotation\')\n    images_dir = join(args.out_dir, \'images\')\n\n    create_dir(annotation_dir)\n    create_dir(images_dir)\n\n    annotation = parse_annotation(args.annot, input_height, input_width)\n\n    dumped_paths = []\n    total_num_bboxes = 0\n    max_num_bboxes = 0\n    for image_name in tqdm(annotation, desc=\'Converting\'):\n        detections = annotation[image_name]\n\n        file_name, extension = basename(image_name).split(\'.\')\n        city_name = file_name.split(\'_\')[0]\n\n        annot_path = join(annotation_dir, \'{}.json\'.format(file_name))\n        with open(annot_path, \'w\') as out_stream:\n            json_dump(detections, out_stream)\n\n        in_image_path = join(args.images_dir, city_name, image_name)\n        src_bgr_image = cv2.imread(in_image_path, cv2.IMREAD_COLOR)\n        grayscale_image = cv2.cvtColor(src_bgr_image, cv2.COLOR_BGR2GRAY)\n\n        rgb_image = np.tile(grayscale_image.reshape(input_height, input_width, 1), (1, 1, 3))\n\n        out_image_path = join(images_dir, image_name)\n        cv2.imwrite(out_image_path, rgb_image)\n\n        dumped_paths.append((out_image_path, annot_path))\n        total_num_bboxes += len(detections)\n        max_num_bboxes = max(max_num_bboxes, len(detections))\n\n    out_data_path = join(args.out_dir, \'data.txt\')\n    save_data_paths(dumped_paths, out_data_path)\n    print(\'\\nLoaded frames: {} with {} boxes. Max number bboxes on image: {}\'\n          .format(len(annotation), total_num_bboxes, max_num_bboxes))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/data/dump_frames.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nfrom argparse import ArgumentParser\nfrom os import makedirs, listdir\nfrom os.path import exists, basename, join, isdir\nfrom shutil import rmtree\n\nimport cv2\nfrom lxml import etree\nfrom tqdm import tqdm\n\n\ndef clear_or_create(path):\n    """"""Clears directory if exists or creates new one.\n\n    :param path: Path to directory\n    """"""\n\n    if exists(path):\n        rmtree(path)\n    makedirs(path)\n\n\ndef create_next_empty_dir(root, default_name=\'dumped\'):\n    """"""Creates folder structure.\n\n    :param root: Path to root directory\n    :param default_name: Name of directory with dumped frames\n    :return: Path to new directory\n    """"""\n\n    if exists(root):\n        candidates = [join(root, o) for o in listdir(root)\n                      if isdir(join(root, o)) and o.startswith(default_name)]\n\n        if len(candidates) > 0:\n            max_index = 0\n            for candidate in candidates:\n                parts = candidate.split(\'_\')\n                if len(parts) > 1:\n                    max_index = max(max_index, int(parts[1]))\n            empty_dir_path = join(root, \'{}_{}\'.format(default_name, max_index + 1))\n        else:\n            empty_dir_path = join(root, default_name)\n    else:\n        empty_dir_path = join(root, default_name)\n\n    makedirs(empty_dir_path)\n\n    return empty_dir_path\n\n\ndef parse_tasks(file_path):\n    """"""Parse input file with tasks, where each task is presented by line in file: [path_to_annotation path_to_video]\n\n    :param file_path: Path to file with tasks\n    :return: List of decoded tasks\n    """"""\n\n    print(\'Found tasks:\')\n\n    tasks = []\n    with open(file_path, \'r\') as input_stream:\n        for line in input_stream:\n            if line.endswith(\'\\n\'):\n                line = line[:-len(\'\\n\')]\n\n            if len(line) == 0:\n                continue\n\n            annotation_path, video_path = line.split(\' \')\n\n            if not exists(annotation_path) or not exists(video_path):\n                continue\n\n            tasks.append((annotation_path, video_path))\n            print(\'   #{}: {} {}\'.format(len(tasks), annotation_path, video_path))\n\n    return tasks\n\n\ndef parse_frame_ids(annotation_path):\n    """"""Reads unique frames IDs from the specified annotation file.\n\n    :param annotation_path:\n    :return: List of unique frame IDs\n    """"""\n\n    tree = etree.parse(annotation_path)\n    root = tree.getroot()\n\n    out_frame_ids = []\n    for track in tqdm(root, desc=\'Extracting annotation\'):\n        if \'label\' not in track.attrib or track.attrib[\'label\'] != \'person\':\n            continue\n\n        for bbox in track:\n            if len(bbox) < 1:\n                continue\n\n            frame_id = int(bbox.attrib[\'frame\'])\n\n            action_name = None\n            for bbox_attr_id in xrange(len(bbox)):\n                attribute_name = bbox[bbox_attr_id].attrib[\'name\']\n                if attribute_name != \'action\':\n                    continue\n\n                action_name = bbox[bbox_attr_id].text\n\n            if action_name is None and action_name == \'\':\n                continue\n\n            out_frame_ids.append(frame_id)\n\n    unique_frame_ids = set(out_frame_ids)\n\n    print(\'Loaded {} annotated frames.\'.format(len(unique_frame_ids)))\n\n    return unique_frame_ids\n\n\ndef dump_frames(video_path, frame_ids, trg_height, trg_width, out_dir):\n    """"""Dumps frames from the specified video file.\n\n    :param video_path: Path to video file\n    :param frame_ids: List of frame IDs to dump\n    :param trg_height: Target image height\n    :param trg_width: Target image width\n    :param out_dir: Output directory to dump frames\n    :return: Number of dumped frames\n    """"""\n\n    vidcap = cv2.VideoCapture(video_path)\n\n    pbar = tqdm(total=len(frame_ids), desc=\'Dumping frames\')\n\n    success = True\n    frame_id = -1\n    num_collected_frames = 0\n\n    while success:\n        success, frame = vidcap.read()\n        frame_id += 1\n\n        if success:\n            if frame_id not in frame_ids:\n                continue\n\n            image_path = join(out_dir, \'frame_{:06}.jpg\'.format(frame_id))\n            out_image = cv2.resize(frame, (trg_width, trg_height))\n            cv2.imwrite(image_path, out_image)\n\n            pbar.update(1)\n\n            num_collected_frames += 1\n            if num_collected_frames >= len(frame_ids):\n                break\n\n    pbar.close()\n\n    return frame_id\n\n\ndef main():\n    """"""Main function to dump frames for the specified tasks.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--tasks\', \'-t\', type=str, required=True, help=\'Path to the file with tasks\')\n    parser.add_argument(\'--out_dir\', \'-o\', type=str, required=True, help=\'Path to the output directory\')\n    parser.add_argument(\'--img_size\', type=str, required=False, default=\'1080,1920\', help=\'Target image size: HxW\')\n    args = parser.parse_args()\n\n    assert exists(args.tasks)\n\n    tasks = parse_tasks(args.tasks)\n    trg_height, trg_width = map(int, args.img_size.split(\',\'))\n\n    working_dir = create_next_empty_dir(args.out_dir)\n\n    out_tasks_file_path = join(working_dir, \'tasks.txt\')\n    out_tasks_stream = open(out_tasks_file_path, \'w\')\n\n    num_frames = 0\n    for task_id, task in enumerate(tasks):\n        task_name = basename(task[0]).split(\'.\')[0]\n        print(\'\\nProcessing task {} / {}: \\\'{}\\\'\'.format(task_id + 1, len(tasks), task_name))\n\n        valid_frame_ids = parse_frame_ids(task[0])\n\n        output_task_dir = join(working_dir, task_name)\n        clear_or_create(output_task_dir)\n\n        task_num_frames = dump_frames(task[1], valid_frame_ids, trg_height, trg_width, output_task_dir)\n\n        if task_num_frames <= 0:\n            print(\'No frames are found to dump\')\n            continue\n\n        print(\'Dumped {} frames\'.format(task_num_frames))\n\n        num_frames += task_num_frames\n\n        out_tasks_stream.write(\'{} {}\\n\'.format(task[0], output_task_dir))\n\n    out_tasks_stream.close()\n\n    print(\'\\nTotal dumped {} frames\'.format(num_frames))\n    print(\'Out list of tasks is saved to: {}\'.format(out_tasks_file_path))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/data/play_gt.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nfrom argparse import ArgumentParser\nfrom os.path import exists\n\nimport cv2\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import trange\n\n\ndef parse_data(data_path):\n    with open(data_path, \'r\') as input_stream:\n        out_data = []\n        for line in input_stream:\n            if line.endswith(\'\\n\'):\n                line = line[:-len(\'\\n\')]\n\n            if len(line) == 0:\n                continue\n\n            image_path, annotation_path = line.split(\' \')\n\n            if not exists(image_path) or not exists(annotation_path):\n                continue\n\n            out_data.append((image_path, annotation_path))\n\n    return out_data\n\n\ndef draw_actions(image, bboxes):\n    image = np.copy(image)\n    image_height, image_width = image.shape[:2]\n\n    if len(bboxes) == 0:\n        return image\n\n    for bbox in bboxes:\n        ymin = int(bbox[\'ymin\'] * image_height)\n        xmin = int(bbox[\'xmin\'] * image_width)\n        ymax = int(bbox[\'ymax\'] * image_height)\n        xmax = int(bbox[\'xmax\'] * image_width)\n\n        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n\n    return image\n\n\ndef main():\n    """"""Main function to dump frames for the specified tasks.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--data\', \'-d\', type=str, required=True, help=\'Path to the file with tasks\')\n    args = parser.parse_args()\n\n    assert exists(args.data)\n\n    data_paths = parse_data(args.data)\n\n    for pair_id in trange(len(data_paths)):\n        image_path, annotation_path = data_paths[pair_id]\n\n        image = cv2.imread(image_path)\n\n        with open(annotation_path, \'r\') as read_file:\n            annotation = json.load(read_file)\n\n        image_with_annotation = draw_actions(image, annotation)\n\n        plt.imshow(image_with_annotation)\n        plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/data/prepare_pedestrian_db.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nfrom os import makedirs, listdir\nfrom os.path import exists, join, basename, isfile\nfrom json import dump as json_dump\nfrom shutil import rmtree\nfrom argparse import ArgumentParser\n\nimport yaml\nimport numpy as np\nfrom tqdm import tqdm\nfrom lxml import etree\n\n\nVALID_EXTENSIONS = [\'png\', \'jpg\']\nIDS_SHIFT_SCALE = 1000000\n\n\ndef _parse_tasks(file_path):\n    """"""Parse input text file where each row is [path_to_annotation img_h,img_w path_to_images_dir]\n       into list of task.\n\n    :param file_path: Path to file\n    :return: List of tasks\n    """"""\n\n    tasks = []\n    with open(file_path, \'r\') as file_stream:\n        for line in file_stream:\n            if line.endswith(\'\\n\'):\n                line = line[:-len(\'\\n\')]\n\n            if len(line) == 0:\n                continue\n\n            annotation_path, video_resolution, images_dir = line.split(\' \')\n            video_resolution = [int(x) for x in video_resolution.split(\',\')]\n\n            if not exists(annotation_path) or not exists(images_dir):\n                continue\n\n            tasks.append((annotation_path, images_dir, video_resolution))\n\n    return tasks\n\n\ndef _parse_images(images_dir):\n    """"""Parse valid image names from the specified directory path.\n\n    :param images_dir: Path directory with images\n    :return: List of valid image paths\n    """"""\n\n    all_files = [join(images_dir, f) for f in listdir(images_dir) if isfile(join(images_dir, f))]\n\n    data = {}\n    for file_path in all_files:\n        file_name, extension = basename(file_path).split(\'.\')\n        if extension not in VALID_EXTENSIONS:\n            continue\n\n        frame_id = int(file_name.split(\'_\')[-1])\n        if frame_id < 0:\n            continue\n\n        data[frame_id] = file_path\n\n    return data\n\n\ndef _calc_iou(box_a, box_b):\n    """"""Calculates IoU metric between two boxes.\n\n    :param box_a: First box coordinates\n    :param box_b: Second box coordinates\n    :return: Scalar IoU value\n    """"""\n\n    intersect_ymin = np.maximum(box_a[\'ymin\'], box_b[\'ymin\'])\n    intersect_xmin = np.maximum(box_a[\'xmin\'], box_b[\'xmin\'])\n    intersect_ymax = np.minimum(box_a[\'ymax\'], box_b[\'ymax\'])\n    intersect_xmax = np.minimum(box_a[\'xmax\'], box_b[\'xmax\'])\n\n    intersect_height = np.maximum(0.0, intersect_ymax - intersect_ymin)\n    intersect_width = np.maximum(0.0, intersect_xmax - intersect_xmin)\n\n    intersect_area = intersect_height * intersect_width\n    area_a = (box_a[\'ymax\'] - box_a[\'ymin\']) * (box_a[\'xmax\'] - box_a[\'xmin\'])\n    area_b = (box_b[\'ymax\'] - box_b[\'ymin\']) * (box_b[\'xmax\'] - box_b[\'xmin\'])\n\n    union_area = area_a + area_b - intersect_area\n\n    overlap_ratio = intersect_area / union_area if union_area > 0.0 else 0.0\n\n    return overlap_ratio\n\n\ndef _try_to_merge(all_tracks, candidate, id_shift, min_iou_overlap, max_time_overlap):\n    """"""Merges overlapped tracks into single if possible.\n\n    :param all_tracks: List of merged tracks\n    :param candidate: List of candidate tracks\n    :param id_shift: ID shift\n    :param min_iou_overlap: Min IoU value to merge tracks\n    :param max_time_overlap: Max time overlap to merge\n    :return: Returns True if success\n    """"""\n\n    candidate_frames_ids = list(candidate)\n    candidate_start_id = np.min(candidate_frames_ids)\n    candidate_end_id = np.max(candidate_frames_ids) + 1\n\n    best_bbox_overlap = 0.0\n    best_bbox_overlap_track_id = -1\n    for anchor_track_id in all_tracks:\n        anchor_track = all_tracks[anchor_track_id]\n        anchor_frames_ids = list(anchor_track)\n        anchor_start_id = np.min(anchor_frames_ids)\n        anchor_end_id = np.max(anchor_frames_ids) + 1\n\n        time_overlap_start = np.maximum(candidate_start_id, anchor_start_id)\n        time_overlap_end = np.minimum(candidate_end_id, anchor_end_id)\n        time_overlap = np.maximum(0, time_overlap_end - time_overlap_start)\n\n        if 0 < time_overlap <= max_time_overlap:\n            for overlap_frame_id in xrange(time_overlap_start, time_overlap_end):\n                if overlap_frame_id not in anchor_track or overlap_frame_id not in candidate:\n                    continue\n\n                anchor_bbox = anchor_track[overlap_frame_id]\n                ref_bbox = candidate[overlap_frame_id]\n\n                bbox_overlap = _calc_iou(anchor_bbox, ref_bbox)\n                if bbox_overlap > best_bbox_overlap:\n                    best_bbox_overlap = bbox_overlap\n                    best_bbox_overlap_track_id = anchor_track_id\n\n    if best_bbox_overlap_track_id >= 0 and best_bbox_overlap > min_iou_overlap:\n        anchor_track = all_tracks[best_bbox_overlap_track_id]\n        for frame_id in candidate:\n            anchor_track[frame_id] = candidate[frame_id]\n            anchor_track[frame_id][\'track_id\'] = best_bbox_overlap_track_id + id_shift\n        return True\n    else:\n        return False\n\n\ndef _read_annotation(annotation_path, image_size, id_shift, action_names_map, ignore_occluded=False,\n                     valid_action_ids=None, min_iou_overlap=0.8, max_time_overlap=5, allow_extend_map=False):\n    """"""Loads annotation from the specified file and merges overlapped tracks.\n\n    :param annotation_path: Path to annotation file\n    :param image_size: Size of image\n    :param id_shift: ID shift\n    :param action_names_map: Map of input actions\n    :param ignore_occluded: Whether to ignore occluded boxes\n    :param valid_action_ids: List of valid action IDs\n    :param min_iou_overlap: Min IoU value to merge tracks\n    :param max_time_overlap: Max time overlap to merge\n    :param allow_extend_map: Whether to allow extending labels map\n    :return: Loaded annotation\n    """"""\n\n    tree = etree.parse(annotation_path)\n    root = tree.getroot()\n\n    converted_tracks = {}\n    for track_id, track in enumerate(root):\n        if \'label\' not in track.attrib or track.attrib[\'label\'] != \'person\':\n            continue\n\n        if \'id\' in track.attrib:\n            track_id = int(track.attrib[\'id\'])\n            assert track_id < IDS_SHIFT_SCALE, \'Invalid ID: {}\'.format(track_id)\n\n        is_new_track = track_id not in converted_tracks\n        converted_track = {} if is_new_track else converted_tracks[track_id]\n\n        for bbox in track:\n            if len(bbox) < 1:\n                continue\n\n            frame_id = int(bbox.attrib[\'frame\'])\n            if frame_id < 0:\n                continue\n\n            action_name = None\n            for bbox_attr_id, _ in enumerate(bbox):\n                attribute_name = bbox[bbox_attr_id].attrib[\'name\']\n                if attribute_name != \'action\':\n                    continue\n\n                action_name = bbox[bbox_attr_id].text\n\n            if action_name is None:\n                continue\n\n            if action_name not in action_names_map:\n                if allow_extend_map:\n                    action_names_map[action_name] = len(action_names_map)\n                else:\n                    continue\n\n            action_name_id = action_names_map[action_name]\n            if valid_action_ids is not None and action_name_id not in valid_action_ids:\n                continue\n\n            is_occluded = bbox.attrib[\'occluded\'] == \'1\'\n            if ignore_occluded and is_occluded:\n                action_name_id = action_names_map[\'__undefined__\']\n\n            bbox_desc = {\'label\': action_name_id,\n                         \'track_id\': track_id + id_shift,\n                         \'occluded\': is_occluded,\n                         \'xmin\': float(bbox.attrib[\'xtl\']) / float(image_size[1]),\n                         \'ymin\': float(bbox.attrib[\'ytl\']) / float(image_size[0]),\n                         \'xmax\': float(bbox.attrib[\'xbr\']) / float(image_size[1]),\n                         \'ymax\': float(bbox.attrib[\'ybr\']) / float(image_size[0])}\n\n            converted_track[frame_id] = bbox_desc\n\n        if is_new_track and len(converted_track) > 0:\n            is_merged = _try_to_merge(converted_tracks, converted_track, id_shift, min_iou_overlap, max_time_overlap)\n            if not is_merged:\n                converted_tracks[track_id] = converted_track\n\n    detections_by_frame_id = {}\n    for track_id in converted_tracks:\n        out_track = converted_tracks[track_id]\n        for frame_id in out_track:\n            detections_by_frame_id[frame_id] = detections_by_frame_id.get(frame_id, []) + [out_track[frame_id]]\n\n    return detections_by_frame_id\n\n\ndef create_dir(dir_path):\n    """"""Creates directory if needed.\n\n    :param dir_path: Path to new directory\n    """"""\n\n    if exists(dir_path):\n        rmtree(dir_path)\n\n    makedirs(dir_path)\n\n\ndef save_data_paths(data, out_path):\n    """"""Saves paths to data into specified file.\n\n    :param data: data to save\n    :param out_path: Path to save\n    """"""\n\n    with open(out_path, \'w\') as input_stream:\n        for image_path, annot_path in tqdm(data, desc=\'Dumping image paths\'):\n            input_stream.write(\'{} {}\\n\'.format(image_path, annot_path))\n\n\ndef save_class_mapping(class_mapping, out_path):\n    """"""Saves class mapping from ID to name onto disk.\n\n    :param class_mapping: Class mapping\n    :param out_path: path to save\n    """"""\n\n    with open(out_path, \'w\') as output_stream:\n        for class_name in class_mapping:\n            class_id = class_mapping[class_name]\n\n            output_stream.write(\'\\""{}\\"": {}\\n\'.format(class_name, class_id))\n\n\ndef main():\n    """"""Main function.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--tasks_path\', \'-t\', type=str, required=True, help=\'Path to source LMDB\')\n    parser.add_argument(\'--out_dir\', \'-o\', type=str, required=True, help=\'Path to save data\')\n    parser.add_argument(\'--input_map\', \'-i\', type=str, required=False, default=\'\', help=\'Path to class names\')\n    args = parser.parse_args()\n\n    assert exists(args.tasks_path)\n\n    if exists(args.input_map):\n        with open(args.input_map, \'r\') as config_file:\n            glob_action_names_map = yaml.load(config_file, Loader=yaml.FullLoader)\n        print(\'Loaded map with {} actions.\'.format(len(glob_action_names_map)))\n    else:\n        glob_action_names_map = {}\n        print(\'No class map is specified. Action names will be collected.\')\n    allow_extend_map = len(glob_action_names_map) == 0\n\n    annotation_dir = join(args.out_dir, \'annotation\')\n    create_dir(annotation_dir)\n\n    tasks = _parse_tasks(args.tasks_path)\n    print(\'Found {} tasks:\'.format(len(tasks)))\n    for task_id, task in enumerate(tasks):\n        print(\'   {}: {}\'.format(task_id, task[0]))\n\n    dumped_paths = []\n    total_num_frames = 0\n    total_num_bboxes = 0\n    max_num_bboxes = 0\n\n    for task_id, task in enumerate(tasks):\n        print(\'Loading task {}...\'.format(task[0]))\n\n        annotation_path = task[0]\n        images_dir = task[1]\n        video_resolution = task[2]\n\n        image_paths = _parse_images(images_dir)\n        if len(image_paths) == 0:\n            continue\n\n        task_annotation_dir = join(annotation_dir, basename(annotation_path.replace(\'.xml\', \'\')))\n        makedirs(task_annotation_dir)\n\n        id_shift = (task_id + 1) * IDS_SHIFT_SCALE\n        annotation = _read_annotation(annotation_path, video_resolution, id_shift, glob_action_names_map,\n                                      min_iou_overlap=0.5, max_time_overlap=5, allow_extend_map=allow_extend_map)\n\n        for frame_id in tqdm(annotation, desc=\'Dumping\'):\n            if frame_id not in image_paths:\n                continue\n\n            image_path = image_paths[frame_id]\n            gt_objects = annotation[frame_id]\n            if len(gt_objects) == 0:\n                continue\n\n            sample_name = \'sample_{:06}\'.format(frame_id)\n            annot_path = join(task_annotation_dir, \'{}.json\'.format(sample_name))\n            with open(annot_path, \'w\') as out_stream:\n                json_dump(gt_objects, out_stream)\n\n            dumped_paths.append((image_path, annot_path))\n            total_num_frames += 1\n            total_num_bboxes += len(gt_objects)\n            max_num_bboxes = max(max_num_bboxes, len(gt_objects))\n\n    out_data_path = join(args.out_dir, \'data.txt\')\n    save_data_paths(dumped_paths, out_data_path)\n    print(\'\\nLoaded frames: {} with {} boxes. Max number bboxes on image: {}\'\n          .format(total_num_frames, total_num_bboxes, max_num_bboxes))\n\n    if allow_extend_map:\n        out_class_map_path = join(args.out_dir, \'class_map.txt\')\n        save_class_mapping(glob_action_names_map, out_class_map_path)\n        print(\'\\nAction names map is stored to: {}\'.format(out_class_map_path))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/models/demo.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nfrom os.path import exists\nfrom argparse import ArgumentParser\n\nfrom action_detection.nn.monitors.factory import get_monitor\n\n\ndef main():\n    """"""Carry out model demonstration.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--config\', \'-c\', type=str, required=True, help=\'Path to config file\')\n    parser.add_argument(\'--input\', \'-i\', type=str, required=True, help=\'Path to video\')\n    parser.add_argument(\'--snapshot_path\', \'-s\', type=str, required=False, default=\'\', help=\'Path to snapshot\')\n    parser.add_argument(\'--out_scale\', type=float, default=1.0, help=\'Output frame scale\')\n    parser.add_argument(\'--deploy\', \'-d\', action=\'store_true\', help=\'Execute in deploy mode\')\n    args = parser.parse_args()\n\n    assert exists(args.config)\n    assert exists(args.input)\n    assert exists(args.snapshot_path + \'.index\')\n    assert args.out_scale > 0.0\n\n    task_monitor = get_monitor(args.config, snapshot_path=args.snapshot_path)\n    task_monitor.demo(args.input, args.out_scale, args.deploy)\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/models/eval.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom os.path import exists\nfrom argparse import ArgumentParser\n\nfrom action_detection.nn.monitors.factory import get_monitor\n\n\ndef main():\n    """"""Carry out model evaluation.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--config\', \'-c\', type=str, required=True, help=\'Path to config file\')\n    parser.add_argument(\'--val_data\', \'-v\', type=str, required=True, help=\'Path to file with annotated train images\')\n    parser.add_argument(\'--snapshot_path\', \'-s\', type=str, required=False, default=\'\', help=\'Path to snapshot\')\n    parser.add_argument(\'--batch_size\', \'-b\', type=int, required=False, default=8, help=\'Batch size\')\n    args = parser.parse_args()\n\n    assert exists(args.config)\n    assert exists(args.val_data)\n    assert exists(args.snapshot_path + \'.index\')\n    assert args.batch_size > 0\n\n    task_monitor = get_monitor(args.config, args.batch_size, snapshot_path=args.snapshot_path)\n    task_monitor.test(args.val_data)\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/models/export.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom os import makedirs\nfrom os.path import exists, basename, join\nfrom argparse import ArgumentParser\n\nfrom action_detection.nn.monitors.factory import get_monitor\n\nBASE_FILE_NAME = \'converted_model\'\nCKPT_FILE_NAME = \'{}.ckpt\'.format(BASE_FILE_NAME)\nPB_FILE_NAME = \'{}.pbtxt\'.format(BASE_FILE_NAME)\nFROZEN_FILE_NAME = \'frozen.pb\'\n\n\ndef main():\n    """"""Carry out model preparation for the export.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--config\', \'-c\', type=str, required=True, help=\'Path to config file\')\n    parser.add_argument(\'--snapshot_path\', \'-s\', type=str, required=True, default=\'\', help=\'Path to model snapshot\')\n    parser.add_argument(\'--output_dir\', \'-o\', type=str, required=True, default=\'\', help=\'Path to output directory\')\n    args = parser.parse_args()\n\n    assert exists(args.config)\n    assert exists(args.snapshot_path + \'.index\')\n\n    if not exists(args.output_dir):\n        makedirs(args.output_dir)\n\n    task_monitor = get_monitor(args.config, snapshot_path=args.snapshot_path)\n\n    converted_snapshot_path = join(args.output_dir, CKPT_FILE_NAME)\n    task_monitor.eliminate_train_ops(converted_snapshot_path)\n\n    converted_model_path = \'{}-{}\'.format(converted_snapshot_path,\n                                          int(basename(args.snapshot_path).split(\'-\')[-1]))\n    task_monitor.save_model_graph(converted_model_path, args.output_dir)\n\n    task_monitor.freeze_model_graph(converted_model_path,\n                                    join(args.output_dir, PB_FILE_NAME),\n                                    join(args.output_dir, FROZEN_FILE_NAME))\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/models/performance.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nfrom os.path import exists\nfrom argparse import ArgumentParser\n\nfrom action_detection.nn.monitors.factory import get_monitor\n\n\ndef main():\n    """"""Carry out model performance estimation.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--config\', \'-c\', type=str, required=True, help=\'Path to config file\')\n    args = parser.parse_args()\n\n    assert exists(args.config)\n\n    task_monitor = get_monitor(args.config)\n    task_monitor.performance()\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/action_detection/tools/models/train.py,0,"b'#!/usr/bin/env python2\n#\n# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom os import makedirs\nfrom os.path import exists\nfrom argparse import ArgumentParser\n\nfrom action_detection.nn.monitors.factory import get_monitor\n\n\ndef main():\n    """"""Carry out model training.\n    """"""\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--config\', \'-c\', type=str, required=True, help=\'Path to config file\')\n    parser.add_argument(\'--train_data\', \'-t\', type=str, required=True, help=\'Path to file with annotated train images\')\n    parser.add_argument(\'--log_dir\', \'-l\', type=str, required=True, help=\'Path to save logs\')\n    parser.add_argument(\'--snapshot_path\', \'-s\', type=str, required=False, default=\'\', help=\'Path to snapshot\')\n    parser.add_argument(\'--init_model_path\', \'-i\', type=str, required=False, default=\'\', help=\'Path to init weights\')\n    parser.add_argument(\'--src_scope\', type=str, required=False, default=\'\', help=\'Checkpoint scope name\')\n    parser.add_argument(\'--batch_size\', \'-b\', type=int, required=False, default=8, help=\'Batch size\')\n    parser.add_argument(\'--num_gpu\', \'-n\', type=int, required=False, default=1, help=\'Number of GPUs\')\n    args = parser.parse_args()\n\n    assert exists(args.config)\n    assert exists(args.train_data)\n    assert args.batch_size > 0\n    assert args.num_gpu > 0\n\n    if not exists(args.log_dir):\n        makedirs(args.log_dir)\n\n    task_monitor = get_monitor(args.config, args.batch_size, args.num_gpu, args.log_dir,\n                               args.src_scope, args.snapshot_path, args.init_model_path)\n    task_monitor.train(args.train_data)\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow_toolkit/lpr/lpr/networks/__init__.py,0,b''
tensorflow_toolkit/lpr/lpr/networks/lprnet.py,4,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\nclass LPRNet:\n  # Function for generation characters range\n\n  # Fire block\n  @staticmethod\n  def fire_block(block_input, outputs):\n    fire = slim.conv2d(block_input, outputs / 4, [1, 1])\n    fire = slim.conv2d(fire, outputs / 4, [3, 3])\n    fire = slim.conv2d(fire, outputs, [1, 1])\n    return fire\n\n  # Small Fire block\n  @staticmethod\n  def small_fire_block(block_input, outputs):\n    fire = slim.conv2d(block_input, outputs / 4, [1, 1])\n    fire = slim.conv2d(fire, outputs / 4, [3, 1])\n    fire = slim.conv2d(fire, outputs / 4, [1, 3])\n    fire = slim.conv2d(fire, outputs, [1, 1])\n    return fire\n\n  # Inception-ResNet like block\n  @staticmethod\n  def resinc_block(block_input, outputs):\n    inputs = int(block_input.get_shape()[3])\n    if inputs == outputs:\n      res = block_input\n    else:\n      res = slim.conv2d(block_input, outputs, [1, 1])\n    inc1 = slim.conv2d(block_input, outputs / 8, [1, 1])\n    inc2 = slim.conv2d(block_input, outputs / 8, [1, 1])\n    inc2 = slim.conv2d(inc2, outputs / 8, [3, 1])\n    inc2 = slim.conv2d(inc2, outputs / 8, [1, 3])\n    conc = tf.concat(3, [inc1, inc2])\n    inc = slim.conv2d(conc, outputs, [1, 1])\n    return res + inc\n\n  # basic_block = fire_block\n  basic_block = small_fire_block\n\n  # basic_block = resinc_block\n\n  # Convolution block for CNN\n  @staticmethod\n  def convolution_block(block_input, outputs, stride, **kwargs):\n    scope = kwargs.pop(\'scope\', None)\n    # cr = slim.conv2d(input, outputs, [3, 3], scope=scope)\n    b_block = LPRNet.basic_block(block_input, outputs)\n    max_pool = slim.max_pool2d(b_block, [3, 3], stride=(stride, 1), padding=\'VALID\', scope=scope)\n    return max_pool\n\n  @staticmethod\n  def enet_input_block(block_input, **kwargs):\n    scope = kwargs.pop(\'scope\', None)\n    input1 = slim.conv2d(block_input, 61, [3, 3], stride=(2, 1), padding=\'VALID\', scope=scope)\n    input2 = slim.avg_pool2d(block_input, [3, 3], stride=(2, 1), padding=\'VALID\', scope=scope)\n    step1 = tf.concat(3, [input1, input2])\n    step2 = LPRNet.basic_block(step1, 128)\n    step2 = slim.max_pool2d(step2, [2, 2], stride=(1, 1), padding=\'VALID\', scope=scope)\n    return step2\n\n  @staticmethod\n  def std_input_block(block_input):\n    return slim.stack(block_input, LPRNet.convolution_block, [(64, 1), (128, 2)])\n\n  @staticmethod\n  def mixed_input_block(block_input):\n    cnn = slim.conv2d(block_input, 64, [3, 3])\n    cnn = slim.max_pool2d(cnn, [3, 3], stride=(1, 1), padding=\'VALID\')\n    cnn = LPRNet.basic_block(cnn, 128)\n    cnn = slim.max_pool2d(cnn, [3, 3], stride=(2, 1), padding=\'VALID\')\n    return cnn\n\n  input_block = mixed_input_block\n\n  @staticmethod\n  def lprnet(net_input):\n    with slim.arg_scope([slim.fully_connected, slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm, weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n                        weights_regularizer=slim.l2_regularizer(0.0005)):\n      cnn = LPRNet.input_block(net_input)\n      cnn = LPRNet.basic_block(cnn, 256)\n      cnn = LPRNet.convolution_block(cnn, 256, 2)\n\n      cnn = slim.dropout(cnn)\n      cnn = slim.conv2d(cnn, 256, [4, 1], padding=\'VALID\')\n      cnn = slim.dropout(cnn)\n\n      return cnn\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/networks/__init__.py,0,b''
tensorflow_toolkit/ssd_detector/ssd_detector/networks/mobilenet_ssd.py,14,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n""""""\n  MobileNet + SSD.\n""""""\n\nimport math\n\nimport cv2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python import pywrap_tensorflow\nfrom slim.nets.mobilenet_v1 import mobilenet_v1_base\nfrom slim.nets.mobilenet.mobilenet_v2 import mobilenet_base as mobilenet_v2_base\nfrom slim.nets.mobilenet import mobilenet, mobilenet_v2\n\nfrom ssd_detector.toolbox.ssd_base import SSDBase\nfrom ssd_detector.toolbox.transformer import ResizeParameter, ExpansionParameter,\\\n  TransformationParameter, DistortionParameter\n\n\nclass MobileNetSSD(SSDBase):\n  # pylint: disable=dangerous-default-value,too-many-arguments,too-many-locals,too-many-statements\n  def __init__(self, num_classes, input_tensor, is_training=False, data_format=\'NHWC\',\n               priors_rule=\'object_detection_api\', priors=[],\n               mobilenet_version=\'v2\', depth_multiplier=1.0, min_depth=16,\n               weight_regularization=4e-5):\n    """"""\n\n    Args:\n      num_classes: Number of classes including a background class.\n      input_tensor: Input 4D tensor.\n      is_training: Is training or inference stage.\n      data_format: \'NHWC\' or \'NCHW\'.\n      priors_rule: \'caffe\', \'object_detection_api\', \'custom\'.\n      priors: List of list of prior sizes (relative sizes). Only for priors_rule=\'custom\'.\n      mobilenet_version: \'v1\' or \'v2\'.\n      depth_multiplier: MobileNet depth multiplier.\n      min_depth: Minimum channels count in MobileNet.\n      weight_regularization: l2 weight regularization scale.\n    """"""\n    assert data_format in [\'NHWC\', \'NCHW\']\n    assert priors_rule in [\'caffe\', \'object_detection_api\', \'custom\']\n\n    self.data_format = data_format\n    if self.data_format == \'NCHW\':\n      input_tensor = tf.transpose(input_tensor, [0, 3, 1, 2])\n    self.input_shape = input_tensor.get_shape().as_list()\n    self.input_tensor = input_tensor\n\n    if self.data_format == \'NCHW\':\n      spatial_dim_axis = [2, 3]\n    elif self.data_format == \'NHWC\':\n      spatial_dim_axis = [1, 2]\n\n    self.version = mobilenet_version\n\n    super(MobileNetSSD, self).__init__(num_classes=num_classes, input_shape=self.input_shape, data_format=data_format)\n\n    self.is_training = is_training\n\n    if mobilenet_version == \'v2\':\n      mobilenet_base = mobilenet_v2_base\n      base_scope = mobilenet_v2.training_scope\n      base_layers = [\'layer_7/output\', \'layer_15/expansion_output\', \'layer_19\']\n    elif mobilenet_version == \'v1\':\n      mobilenet_base = mobilenet_v1_base\n      base_scope = mobilenet.training_scope\n      base_layers = [\'Conv2d_5_pointwise\', \'Conv2d_11_pointwise\', \'Conv2d_13_pointwise\']\n    else:\n      tf.logging.error(\'Wrong MobileNet version = {}\'.format(mobilenet_version))\n      exit(0)\n\n    def scope_fn():\n      batch_norm_params = {\n        \'is_training\': self.is_training,\n        \'center\': True,\n        \'scale\': True,\n        \'decay\': 0.9997,\n        \'epsilon\': 0.001,\n        \'fused\': True,\n        \'data_format\': data_format\n      }\n      affected_ops = [slim.conv2d, slim.separable_conv2d, slim.conv2d_transpose]\n      with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n        with slim.arg_scope(affected_ops,\n                            weights_regularizer=slim.l2_regularizer(scale=float(weight_regularization)),\n                            weights_initializer=tf.truncated_normal_initializer(mean=0, stddev=0.03),\n                            activation_fn=tf.nn.relu6,\n                            normalizer_fn=slim.batch_norm) as scope:\n          return scope\n\n    with slim.arg_scope(base_scope(is_training=None)):\n      with slim.arg_scope(scope_fn()):\n        _, image_features = mobilenet_base(\n          self.input_tensor,\n          final_endpoint=base_layers[-1],\n          depth_multiplier=depth_multiplier,\n          min_depth=min_depth,\n          use_explicit_padding=False,\n          is_training=self.is_training)\n\n    head_feature_map_names = base_layers[-2:]\n    head_feature_map_tensors = [image_features[name] for name in head_feature_map_names]\n\n    feature_map = image_features[base_layers[-1]]\n\n    depths = [512, 256, 256, 128]\n    depths = [int(d * depth_multiplier) for d in depths]\n    with tf.variable_scope(\'extra_features\'):\n      with slim.arg_scope(scope_fn()):\n        for i, depth in enumerate(depths):\n          intermediate_layer = slim.conv2d(feature_map, int(depth / 2), [1, 1], stride=1,\n                                           scope=\'intermediate_{0}\'.format(i + 1))\n          feature_map = slim.separable_conv2d(\n            intermediate_layer,\n            None, [3, 3],\n            depth_multiplier=1,\n            padding=\'SAME\',\n            stride=2,\n            scope=\'feature_map_dw_{0}\'.format(i + 1))\n\n          output_feature_name = \'feature_map_{0}\'.format(i + 1)\n          feature_map = slim.conv2d(\n            feature_map,\n            int(depth), [1, 1],\n            padding=\'SAME\',\n            stride=1,\n            scope=output_feature_name)\n\n          head_feature_map_names.append(output_feature_name)\n          head_feature_map_tensors.append(feature_map)\n\n    variances = [0.1, 0.1, 0.2, 0.2]\n\n    if priors_rule == \'caffe\':\n      scale = [0.2, 0.35, 0.5, 0.65, 0.8, 0.95]\n      dicts = self._create_caffe_priors(self.input_shape, spatial_dim_axis, scale, variances,\n                                        head_feature_map_tensors, head_feature_map_names)\n    elif priors_rule == \'object_detection_api\':\n      scale = [0.2, 0.35, 0.5, 0.65, 0.8, 0.95, 1.]\n      dicts = self._create_obj_det_priors(self.input_shape, spatial_dim_axis, scale, variances,\n                                          head_feature_map_tensors, head_feature_map_names)\n    elif priors_rule == \'custom\':\n      assert len(priors) == len(head_feature_map_tensors)\n      dicts = self._create_custom_priors(self.input_shape, spatial_dim_axis, priors, variances,\n                                         head_feature_map_tensors, head_feature_map_names)\n\n    with slim.arg_scope(scope_fn()):\n      self.create_heads(head_feature_map_tensors, dicts)\n\n\n  @staticmethod\n  def _get_spatial_step(input_shape, tensor, spatial_dim_axis):\n    output_shape = tensor.get_shape().as_list()\n    return [float(input_shape[axis]) / float(output_shape[axis]) for axis in spatial_dim_axis]\n\n  @staticmethod\n  def _create_caffe_priors(input_shape, spatial_dim_axis, scale, variances, tensors, names):\n    width, _ = [input_shape[axis] for axis in spatial_dim_axis]\n    dicts = []\n    for i, scale_ in enumerate(scale):\n      dict_ = dict(prefix=names[i], step=MobileNetSSD._get_spatial_step(input_shape, tensors[i], spatial_dim_axis),\n                   variance=variances, clip=True)\n      if i == 0:\n        dict_.update(dict(min_sizes=[width * 0.1], max_sizes=[width * scale_], aspect_ratios=[2]))\n      else:\n        dict_.update(dict(min_sizes=[width * scale_], aspect_ratios=[2, 3]))\n      dicts.append(dict_)\n\n    return dicts\n\n  @staticmethod\n  def _create_obj_det_priors(input_shape, spatial_dim_axis, scale, variances, tensors, names):\n    width, _ = [input_shape[axis] for axis in spatial_dim_axis]\n\n    aspect_ratio = [2., 0.5, 3., 1. / 3.]\n    box_specs = []\n\n    for i in range(len(scale) - 1):\n      specs = []\n      if i == 0:\n        specs.append([scale[i] * 0.5, 1.])\n        specs.append([scale[i], 2.])\n        specs.append([scale[i], 0.5])\n      else:\n        specs.append([scale[i], 1.])\n        specs.append([math.sqrt(scale[i] * scale[i + 1]), 1.])\n        for asp_rat in aspect_ratio:\n          specs.append([scale[i], asp_rat])\n\n      specs = [[width * sc, ar] for sc, ar in specs]\n      box_specs.append(specs)\n\n    dicts = [dict(box_specs=box_specs[i], prefix=names[i], variance=variances, clip=True,\n                  step=MobileNetSSD._get_spatial_step(input_shape, tensors[i], spatial_dim_axis))\n             for i in range(len(scale) - 1)]\n\n    return dicts\n\n  @staticmethod\n  def _create_custom_priors(input_shape, spatial_dim_axis, priors, variances, tensors, names):\n    dicts = []\n    for (i, sizes) in enumerate(priors):\n      dict_ = dict(prefix=names[i], step=MobileNetSSD._get_spatial_step(input_shape, tensors[i], spatial_dim_axis),\n                   variance=variances, clustered_sizes=sizes, clip=True)\n      dicts.append(dict_)\n\n    return dicts\n\n  @staticmethod\n  def create_transform_parameters(height, width, fill_with_current_image_mean=True):\n    scale = 2.0 / 255.0\n    mean_value = 255.0 / 2.0\n    resize_param = ResizeParameter(height=height, width=width)\n    expand_param = ExpansionParameter(prob=0.5, max_expand_ratio=1.5,\n                                      fill_with_current_image_mean=fill_with_current_image_mean)\n    distort_param = DistortionParameter(brightness_prob=0.5, brightness_delta=32., contrast_prob=0.5,\n                                        contrast_lower=0.5, contrast_upper=1.5,\n                                        hue_prob=0.5, hue_delta=18, saturation_prob=0.5, saturation_lower=0.5,\n                                        saturation_upper=1.5, random_order_prob=0.5)\n    train_param = TransformationParameter(mirror=True, resize_param=resize_param,\n                                          expand_param=expand_param,\n                                          scale=scale, mean_value=mean_value,\n                                          noise_param=None, distort_param=distort_param)\n\n    val_resize_param = ResizeParameter(height=height, width=width, interp_mode=[cv2.INTER_LINEAR])\n    val_param = TransformationParameter(resize_param=val_resize_param, scale=scale, mean_value=mean_value)\n    return train_param, val_param\n\n  @staticmethod\n  def load_weights(weights_path):\n    variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n    reader = pywrap_tensorflow.NewCheckpointReader(weights_path)\n    ckp_vars_to_shape = reader.get_variable_to_shape_map()\n    if \'global_step\' in ckp_vars_to_shape:\n      del ckp_vars_to_shape[\'global_step\']  # Do not restore global_step\n\n    def __get_vars_to_restore(model_vars, ckp_vars_to_shape, prefixes):\n      vars_to_restore = dict()\n      skip_vars = []\n\n      stat = dict()\n      for prefix in prefixes:\n        stat[prefix] = 0\n\n      for var in model_vars:\n        var_name = var.name[:-2]\n        var_shape = var.shape.as_list()\n\n        skip_var = True\n        for prefix in prefixes:\n          if prefix + var_name in ckp_vars_to_shape and ckp_vars_to_shape[prefix + var_name] == var_shape:\n            vars_to_restore[prefix + var_name] = var_name\n            stat[prefix] += 1\n            skip_var = False\n            break\n\n        if skip_var:\n          skip_vars.append(var)\n\n      return vars_to_restore, skip_vars, stat\n\n    prefixes = [\'\',                   # For classification models and checkpoints\n                \'FeatureExtractor/\']  # For models from object detection API\n\n    vars_to_restore, skip_vars, stat = __get_vars_to_restore(variables, ckp_vars_to_shape, prefixes)\n\n    for prefix, restored_vars in stat.items():\n      tf.logging.info(""For the prefix \'{0}\' were found {1} weights"".format(prefix, restored_vars))\n\n    try:\n      with tf.name_scope(\'load_weights\'):\n        tf.train.init_from_checkpoint(weights_path, vars_to_restore)\n      tf.logging.info(""Values were loaded for {} tensors!"".format(len(vars_to_restore.keys())))\n      tf.logging.info(""Values were not loaded for {} tensors:"".format(len(skip_vars)))\n      for var in skip_vars:\n        tf.logging.info(""  {}"".format(var))\n    except ValueError as exception:\n      tf.logging.error(""Weights was not loaded at all!"")\n      tf.logging.error(exception)\n      exit(1)\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/readers/__init__.py,0,b''
tensorflow_toolkit/ssd_detector/ssd_detector/readers/object_detector_json.py,8,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n""""""\n  Dataset reader.\n""""""\n\nfrom __future__ import print_function\nimport os\nimport pickle\nimport sys\n\nimport cv2\nimport jpeg4py as jpeg\nimport matplotlib; matplotlib.use(\'Agg\')  # pylint: disable=multiple-statements\nimport numpy as np\nfrom pycocotools.coco import COCO\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom ssd_detector.toolbox.bounding_box import BoundingBox\n\n\ndef imread(im_path):\n  if os.path.splitext(im_path)[1].lower() in (\'.jpg\', \'.jpeg\'):\n    try:\n      img = jpeg.JPEG(im_path).decode()[..., ::-1]  # RGB -> BGR\n    # pylint: disable=broad-except\n    except Exception as ex:\n      tf.logging.warning(""Can\'t load {0} with jpeg4py (libjpeg-turbo): {1}. Will use OpenCV. ""\n                         ""Can be slower."".format(im_path, ex))\n      img = cv2.imread(im_path, cv2.IMREAD_COLOR)\n  else:\n    img = cv2.imread(im_path, cv2.IMREAD_COLOR)\n\n  return img\n\n\ndef imdecode(data):\n  try:\n    img = jpeg.JPEG(data).decode()[..., ::-1]  # RGB -> BGR\n  # pylint: disable=broad-except\n  except Exception as ex:\n    tf.logging.warning(""Can\'t decode with jpeg4py (libjpeg-turbo): {0}. Will use OpenCV."".format(ex))\n    img = cv2.imdecode(data, cv2.IMREAD_COLOR)\n\n  return img\n\n\nclass ObjectDetectorJson:\n  _cache = dict()\n\n  # pylint: disable=invalid-name\n  @staticmethod\n  def get_classes_from_coco_annotation(ann_path):\n    annotation = COCO(ann_path)\n    max_id = max(annotation.cats.keys())\n    classes = [\'None#{}\'.format(i) for i in range(max_id + 1)]\n    for class_id, class_item in annotation.cats.items():\n      classes[class_id] = class_item[\'name\']\n    return classes\n\n  # pylint: disable=too-many-locals\n  @staticmethod\n  def convert_coco_to_toolbox_format(coco_annotation, classes, annotation_directory=\'\'):\n    annotations = list(coco_annotation.anns.values())\n    converted_annotations = {}\n    images = list(coco_annotation.imgs.values())\n\n    images_id_to_name_map = {}\n    for image in images:\n      image_id = image[\'id\']\n      images_id_to_name_map[image_id] = image\n\n      image = images_id_to_name_map[image_id]\n      im_path = image[\'image\']\n      if not os.path.isabs(im_path):\n        im_path = os.path.join(annotation_directory, im_path)\n      image_size = [image[\'width\'], image[\'height\']]\n\n      converted_annotations[image_id] = {\'image_id\': image_id,\n                                         \'image\': im_path,\n                                         \'image_size\': image_size,\n                                         \'dataset\': image[\'dataset\'] if \'dataset\' in image else \'DATASET\',\n                                         \'objects\': []\n                                        }\n\n    for annotation in annotations:\n      xmin, ymin, width, height = annotation[\'bbox\']\n      xmax = xmin + width\n      ymax = ymin + height\n\n      obj = {}\n      image_id = annotation[\'image_id\']\n      obj[\'bbox\'] = [xmin, ymin, xmax, ymax]\n      obj[\'label\'] = classes[annotation[\'category_id\']]\n      obj[\'occluded\'] = annotation[\'is_occluded\'] if \'is_occluded\' in annotation else False\n      obj[\'attributes\'] = annotation[\'attributes\'] if \'attributes\' in annotation else dict()\n\n      if image_id in converted_annotations:\n        converted_annotations[image_id][\'objects\'].append(obj)\n      else:\n        tf.logging.error(\'Image with image_id = {} is absent, but was found in annotation\'.format(image_id))\n\n    images_without_annotation = [key for key, val in converted_annotations.items() if len(val[\'objects\']) == 0]\n    tf.logging.info(\'Images without annotation: {}\'.format(len(images_without_annotation)))\n    tf.logging.info(images_without_annotation)\n\n    return list(converted_annotations.values())\n\n  @staticmethod\n  def json_iterator(filename, classes):\n    coco_annotation = COCO(filename)\n    annotation_directory = os.path.join(os.getcwd(), os.path.dirname(filename))\n    converted_data = ObjectDetectorJson.convert_coco_to_toolbox_format(coco_annotation, classes, annotation_directory)\n\n    def generator():\n      for item in converted_data:\n        yield pickle.dumps(item)\n\n    return generator, len(converted_data)\n\n  @staticmethod\n  def init_cache(filename, cache_type, classes):\n    assert cache_type in (\'FULL\', \'ENCODED\', \'NONE\')\n    print(\'Load images in the cache: {}\'.format(cache_type))\n    generator, size = ObjectDetectorJson.json_iterator(filename, classes)\n    items = [pickle.loads(item) for item in generator()]\n\n    def _read_image_from_disk(im_path, cache_type):\n      assert cache_type in (\'ENCODED\', \'FULL\')\n      if cache_type == \'ENCODED\':\n        with open(im_path, \'rb\') as file:\n          encoded_image = file.read()\n          encoded_image = np.array(bytearray(encoded_image), dtype=np.uint8)\n        return encoded_image\n      if cache_type == \'FULL\':\n        image = imread(im_path)\n        return image\n\n      return None\n\n    items = tqdm(items, total=size, unit=\'images\')\n    total_cache_usage = 0\n    for item in items:\n      im_path = item[\'image\']\n      if cache_type != \'NONE\':\n        image = _read_image_from_disk(im_path, cache_type)\n      else:\n        image = None\n\n      annotation = ObjectDetectorJson._get_annotation(item, classes)\n      ObjectDetectorJson._cache[im_path] = [image, annotation]\n\n      if isinstance(image, np.ndarray):\n        total_cache_usage += image.nbytes\n      else:\n        total_cache_usage += sys.getsizeof(image)\n      total_cache_usage += sys.getsizeof(annotation)  # Bad estimation\n\n      items.set_postfix({\'cache usage (GB)\': total_cache_usage / 1024 ** 3})\n\n  @staticmethod\n  def _get_image_and_annotation(item, cache_type):\n    im_path = item[\'image\']\n\n    if im_path not in ObjectDetectorJson._cache:\n      tf.logging.error(""Image \'{0}\' is absent in the cache. Wrong path."".format(im_path))\n      exit(1)\n\n    if cache_type == \'NONE\':\n      _, annotation = ObjectDetectorJson._cache[im_path]\n      img = imread(im_path)\n    else:\n      img, annotation = ObjectDetectorJson._cache[im_path]\n\n      if cache_type == \'ENCODED\':\n        img = imdecode(img)\n\n    return img, annotation\n\n  @staticmethod\n  def _get_annotation(item, classes):\n    # Annotation is encoded in the original image size, actual image size may be smaller due to performance reasons\n    width, height = item[\'image_size\']\n    width, height = float(width), float(height)\n\n    annotation = {}\n    for obj in item[\'objects\']:\n      class_name = obj[\'label\']\n      if class_name == \'ignored\':\n        continue\n      try:\n        class_id = classes.index(class_name)\n      except ValueError:\n        tf.logging.warning(""Unknown label = \'{0}\', supported labels: {1}"".format(class_name, classes))\n        class_id = -1\n        continue\n\n      xmin, ymin, xmax, ymax = obj[\'bbox\']\n\n      xmin /= width\n      ymin /= height\n      xmax /= width\n      ymax /= height\n\n      annotation.setdefault(class_id, []).append(BoundingBox(xmin, ymin, xmax, ymax))\n\n    return annotation\n\n  @staticmethod\n  def json_decode_entry(value, cache_type):\n    value = pickle.loads(value)\n    img, annotation = ObjectDetectorJson._get_image_and_annotation(value, cache_type)\n    return img, annotation\n\n  @staticmethod\n  def create_dataset(json_file_path, classes):\n    gen, num = ObjectDetectorJson.json_iterator(json_file_path, classes)\n    return tf.data.Dataset.from_generator(gen, tf.string, tf.TensorShape([])), num\n\n  @staticmethod\n  def transform_fn(value, transformer, cache_type=\'NONE\', add_original_image=False):\n    image, annotation = ObjectDetectorJson.json_decode_entry(value, cache_type)\n    transformed_image, annotation = transformer.transform(image, annotation)\n    result = transformed_image.astype(np.float32), pickle.dumps(annotation)\n\n    if add_original_image:\n      result += (image,)\n\n    return result\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/__init__.py,0,b''
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/bounding_box.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport math\nimport random\n\n\nclass BoundingBox:\n  # pylint: disable=too-many-arguments\n  def __init__(self, xmin=0., ymin=0., xmax=0., ymax=0., difficult=False):\n    self.xmin = xmin\n    self.ymin = ymin\n    self.xmax = xmax\n    self.ymax = ymax\n\n    self.difficult = difficult\n\n  def width(self):\n    return self.xmax - self.xmin\n\n  def height(self):\n    return self.ymax - self.ymin\n\n  def to_list(self):\n    return [self.xmin, self.ymin, self.xmax, self.ymax]\n\n  def __str__(self):\n    return \'[xmin={} ymin={} xmax={} ymax={} diff={}]\'.format(self.xmin, self.ymin, self.xmax, self.ymax,\n                                                              self.difficult)\n\n  def size(self):\n    if self.xmax < self.xmin or self.ymax < self.ymin:\n      return 0\n\n    width = self.xmax - self.xmin\n    height = self.ymax - self.ymin\n    return width * height\n\n  def clip_box(self):\n    xmin = max(min(self.xmin, 1.), 0.)\n    xmax = max(min(self.xmax, 1.), 0.)\n    ymin = max(min(self.ymin, 1.), 0.)\n    ymax = max(min(self.ymax, 1.), 0.)\n    return BoundingBox(xmin, ymin, xmax, ymax, self.difficult)\n\n  def scale_box(self, height, width):\n    xmin = self.xmin * width\n    ymin = self.ymin * height\n    xmax = self.xmax * width\n    ymax = self.ymax * height\n    return BoundingBox(xmin, ymin, xmax, ymax, self.difficult)\n\n  def project_box(self, box):\n    if box.xmin >= self.xmax or box.xmax <= self.xmin or box.ymin >= self.ymax or box.ymax <= self.ymin:\n      return None\n\n    src_width = self.xmax - self.xmin\n    src_height = self.ymax - self.ymin\n    xmin = (box.xmin - self.xmin) / src_width\n    ymin = (box.ymin - self.ymin) / src_height\n    xmax = (box.xmax - self.xmin) / src_width\n    ymax = (box.ymax - self.ymin) / src_height\n    proj_bbox = BoundingBox(xmin, ymin, xmax, ymax, box.difficult).clip_box()\n    return proj_bbox if proj_bbox.size() > 0 else None\n\n  def locate_box(self, box):\n    src_width = self.xmax - self.xmin\n    src_height = self.ymax - self.ymin\n    xmin = self.xmin + box.xmin * src_width\n    ymin = self.ymin + box.ymin * src_height\n    xmax = self.xmin + box.xmax * src_width\n    ymax = self.ymin + box.ymax * src_height\n    return BoundingBox(xmin, ymin, xmax, ymax, box.difficult)\n\n  def is_cross_boundary(self):\n    return self.xmin < 0 or self.xmin > 1 or self.ymin < 0 or self.ymin > 1 or \\\n           self.xmax < 0 or self.xmax > 1 or self.ymax < 0 or self.ymax > 1\n\n\ndef intersect_box(box1, box2):\n  if box2.xmin > box1.xmax or box2.xmax < box1.xmin or box2.ymin > box1.ymax or box2.ymax < box1.ymin:\n    return BoundingBox()\n\n  xmin = max(box1.xmin, box2.xmin)\n  ymin = max(box1.ymin, box2.ymin)\n  xmax = min(box1.xmax, box2.xmax)\n  ymax = min(box1.ymax, box2.ymax)\n  return BoundingBox(xmin, ymin, xmax, ymax)\n\n\ndef box_coverage(box1, box2):\n  inter_box = intersect_box(box1, box2)\n  inter_size = inter_box.size()\n  return inter_size / box1.size() if inter_size > 0 else 0\n\n\ndef jaccard_overlap(box1, box2):\n  inter = intersect_box(box1, box2).size()\n  return inter / (box1.size() + box2.size() - inter) if inter > 0 else 0\n\n\ndef sample_box(sampler):\n  scale = random.uniform(sampler.min_scale, sampler.max_scale)\n  aspect_ratio = random.uniform(sampler.min_aspect_ratio, sampler.max_aspect_ratio)\n\n  aspect_ratio = max(aspect_ratio, scale ** 2.)\n  aspect_ratio = min(aspect_ratio, 1 / (scale ** 2.))\n\n  # Figure out bbox dimension.\n  bbox_width = scale * math.sqrt(aspect_ratio)\n  bbox_height = scale / math.sqrt(aspect_ratio)\n\n  # Figure out top left coordinates.\n  w_off = random.uniform(0., 1. - bbox_width)\n  h_off = random.uniform(0., 1. - bbox_height)\n\n  return BoundingBox(w_off, h_off, w_off + bbox_width, h_off + bbox_height)\n\n\ndef extrapolate_box(param, height, width, crop_bbox, bbox):\n  height_scale = param.height_scale\n  width_scale = param.width_scale\n\n  if height_scale > 0 and width_scale > 0 and param.resize_mode == \'FIT_SMALL_SIZE\':\n    orig_aspect = float(width) / height\n    resize_height = param.height\n    resize_width = param.width\n    resize_aspect = resize_width / resize_height\n    if orig_aspect < resize_aspect:\n      resize_height = resize_width / orig_aspect\n    else:\n      resize_width = resize_height * orig_aspect\n\n    crop_height = resize_height * (crop_bbox.ymax - crop_bbox.ymin)\n    crop_width = resize_width * (crop_bbox.xmax - crop_bbox.xmin)\n\n    result = BoundingBox(difficult=bbox.difficult)\n    result.xmin = bbox.xmin * crop_width / width_scale\n    result.xmax = bbox.xmax * crop_width / width_scale\n    result.ymin = bbox.ymin * crop_height / height_scale\n    result.ymax = bbox.ymax * crop_height / height_scale\n    return result\n\n  return bbox\n\n\ndef satisfy_sample_constraint(sampled_box, object_boxes, constr):\n  has_jaccard_overlap = constr.min_jaccard_overlap or constr.max_jaccard_overlap\n  has_sample_coverage = constr.min_sample_coverage or constr.max_sample_coverage\n  has_object_coverage = constr.min_object_coverage or constr.max_object_coverage\n  satisfy = not has_jaccard_overlap and not has_sample_coverage and not has_object_coverage\n  if satisfy:\n    return True\n\n  for box in object_boxes:\n    jaccard = jaccard_overlap(sampled_box, box)\n    sample_cov = box_coverage(sampled_box, box)\n    object_cov = box_coverage(box, sampled_box)\n\n    if constr.min_jaccard_overlap and jaccard < constr.min_jaccard_overlap:\n      continue\n\n    if constr.max_jaccard_overlap and jaccard > constr.max_jaccard_overlap:\n      continue\n\n    if constr.min_sample_coverage and sample_cov < constr.min_sample_coverage:\n      continue\n    if constr.max_sample_coverage and sample_cov > constr.max_sample_coverage:\n      continue\n\n    if constr.min_object_coverage and object_cov < constr.min_object_coverage:\n      continue\n\n    if constr.max_object_coverage and object_cov > constr.max_object_coverage:\n      continue\n\n    return True\n\n  return False\n\n\ndef generate_batch_samples(annotation, batch_samplers):\n  object_boxes = [b for _, boxes in annotation.items() for b in boxes]\n  sampled_boxes = []\n\n  for sampler in batch_samplers:\n    if not sampler.use_original_image:\n      continue\n\n    current_boxes = []\n    for _ in range(sampler.max_trials):\n      if len(current_boxes) >= sampler.max_sample:\n        break\n\n      # Generate sampled_bbox in the normalized space [0, 1].\n      sampled_bbox = sample_box(sampler)\n\n      # Determine if the sampled bbox is positive or negative by the constraint.\n      if satisfy_sample_constraint(sampled_bbox, object_boxes, sampler):\n        current_boxes.append(sampled_bbox)\n\n    sampled_boxes.extend(current_boxes)\n  return sampled_boxes\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/coco_metrics_eval.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom pycocotools.cocoeval import COCOeval\nfrom ssd_detector.readers.object_detector_json import ObjectDetectorJson\n\nMETRICS_NAMES = [""Average Precision(AP) @ [IoU = 0.50:0.95 | area = all | maxDets = 100]"",\n                 ""Average Precision(AP) @ [IoU = 0.50 | area = all | maxDets = 100]"",\n                 ""Average Precision(AP) @ [IoU = 0.75 | area = all | maxDets = 100]"",\n                 ""Average Precision(AP) @ [IoU = 0.50:0.95 | area = small | maxDets = 100]"",\n                 ""Average Precision(AP) @ [IoU = 0.50:0.95 | area = medium | maxDets = 100]"",\n                 ""Average Precision(AP) @ [IoU = 0.50:0.95 | area = large | maxDets = 100]"",\n                 ""Average Recall(AR) @ [IoU = 0.50:0.95 | area = all | maxDets = 1]"",\n                 ""Average Recall(AR) @ [IoU = 0.50:0.95 | area = all | maxDets = 10]"",\n                 ""Average Recall(AR) @ [IoU = 0.50:0.95 | area = all | maxDets = 100]"",\n                 ""Average Recall(AR) @ [IoU = 0.50:0.95 | area = small | maxDets = 100]"",\n                 ""Average Recall(AR) @ [IoU = 0.50:0.95 | area = medium | maxDets = 100]"",\n                 ""Average Recall(AR) @ [IoU = 0.50:0.95 | area = large | maxDets = 100]""]\n\n\n# pylint: disable=too-many-locals\ndef calc_coco_metrics(coco_annotations, predictions, classes):\n  annotations = ObjectDetectorJson.convert_coco_to_toolbox_format(coco_annotations, classes)\n  detections = []\n  for annotation, prediction in zip(annotations, predictions):\n    width, height = annotation[\'image_size\']\n    image_id = annotation[\'image_id\']\n\n    for obj_id, obj in enumerate(prediction):\n      label = int(obj[1])\n      score = float(obj[2])\n      if obj_id != 0 and score == 0:  # At least one prediction must be (COCO API issue)\n        continue\n      bbox = (obj[3:]).tolist()\n      bbox[::2] = [width * i for i in bbox[::2]]\n      bbox[1::2] = [height * i for i in bbox[1::2]]\n\n      xmin, ymin, xmax, ymax = bbox\n      w_bbox = round(xmax - xmin, 1)\n      h_bbox = round(ymax - ymin, 1)\n      xmin, ymin = round(xmin, 1), round(ymin, 1)\n\n      coco_det = {}\n      coco_det[\'image_id\'] = image_id\n      coco_det[\'category_id\'] = label\n      coco_det[\'bbox\'] = [xmin, ymin, w_bbox, h_bbox]\n      coco_det[\'score\'] = score\n      detections.append(coco_det)\n\n  coco_dt = coco_annotations.loadRes(detections)\n  img_ids = sorted(coco_annotations.getImgIds())\n  coco_eval = COCOeval(coco_annotations, coco_dt, \'bbox\')\n  coco_eval.params.imgIds = img_ids\n  coco_eval.evaluate()\n  coco_eval.accumulate()\n  coco_eval.summarize()\n\n  metrics = {}\n  for metric_name, value in zip(METRICS_NAMES, coco_eval.stats):\n    metrics[metric_name] = value\n\n  return metrics\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/layers.py,2,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\n@slim.add_arg_scope\ndef get_spatial_dims(tensor_or_shape, data_format=\'NHWC\'):\n  if isinstance(tensor_or_shape, (list, tuple)):\n    input_shape = tensor_or_shape\n  else:\n    input_shape = tensor_or_shape.get_shape().as_list()\n\n  assert data_format in (\'NCHW\', \'NHWC\')\n  assert len(input_shape) == 4\n\n  if data_format == \'NHWC\':\n    height = input_shape[1]\n    width = input_shape[2]\n  else:\n    height = input_shape[2]\n    width = input_shape[3]\n  return height, width\n\n\n@slim.add_arg_scope\ndef channel_to_last(inputs, data_format=\'NHWC\', scope=None):\n  assert data_format in (\'NCHW\', \'NHWC\')\n  with tf.name_scope(scope, \'channel_to_last\', [inputs]):\n    return inputs if data_format == \'NHWC\' else tf.transpose(inputs, perm=(0, 2, 3, 1))\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/loss.py,44,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n""""""\n  Loss functions.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.ops.control_flow_ops import with_dependencies\n\n\nclass MultiboxLoss:\n  def __init__(self, loc_weight=1.0, neg_pos_ratio=3.0):\n    """"""\n    Multibox loss.\n\n    Args:\n      loc_weight: Weight of localization loss.\n      neg_pos_ratio: Max ratio of negative to positive boxes in loss.\n    """"""\n    self.loc_weight = loc_weight\n    self.neg_pos_ratio = neg_pos_ratio\n    self.eval_tensors = {}\n\n  @staticmethod\n  def _localization_loss(ground_truth, prediction):\n    """"""\n    Compute L1-smooth loss.\n\n    Args:\n      ground_truth: Ground truth bounding boxes, shape: (?, #priors, 4).\n      prediction: Predicted bounding boxes, shape: (?, #priors, 4).\n\n    Returns:\n      L1-smooth loss, shape: (?, #priors).\n    """"""\n    l1_loss = tf.losses.huber_loss(ground_truth, prediction, reduction=tf.losses.Reduction.NONE)\n    return tf.reduce_sum(l1_loss, axis=-1)\n\n  @staticmethod\n  def _classification_loss(ground_truth, logits):\n    """"""\n      Compute sigmoid cross_entropy loss.\n\n    Args:\n      ground_truth: Ground truth targets, shape: (?, #priors, #classes).\n      logits: Predicted logits, shape: (?, #priors, #classes).\n\n    Returns:\n      Sigmoid cross_entropy loss, shape: (?, #priors).\n    """"""\n    # softmax_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth, logits=logits)\n    # return softmax_loss\n    per_entry_cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(labels=ground_truth, logits=logits)\n    return tf.reduce_sum(per_entry_cross_ent, axis=-1)\n\n  def __add_evaluation(self, evaluation_tensors):\n    """"""\n      Add tensors to a summary.\n    Args:\n      evaluation_tensors: Dictionary: evaluation name -> tensor scalar.\n    """"""\n    for log_name, log_value in evaluation_tensors.items():\n      tf.summary.scalar(log_name, log_value)\n      self.eval_tensors[log_name] = log_value\n\n  def eval_summary(self, ground_truth, prediction):\n    """"""\n      Compute evaluation metrics (for EVAL mode).\n\n    Args:\n      ground_truth: Ground truth, shape: (?, #priors, 4 + #classes).\n      prediction: Dictionary of predicted tensors, shape: {\'locs\'  : (?, #priors, 4), \\\n                                                           \'confs\' : (?, #priors, #classes), \\\n                                                           \'logits\': (?, #priors, #classes)}.\n    Returns:\n      Loss stub, shape: (1,).\n    """"""\n    localization_loss = self._localization_loss(ground_truth[:, :, :4],\n                                                prediction[\'locs\'])  # shape: (batch_size, num_priors)\n    classification_loss = self._classification_loss(ground_truth[:, :, 4:],\n                                                    prediction[\'logits\'])  # shape: (batch_size, num_priors)\n    positives = tf.reduce_max(ground_truth[:, :, 5:], axis=-1)  # shape: (batch_size, num_priors)\n    num_positives = tf.reduce_sum(positives)  # shape: (1,)\n    loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1)  # shape: (batch_size,)\n    classification_loss = tf.reduce_sum(classification_loss, axis=-1)  # shape: (batch_size,)\n\n    evaluation_tensors = {\n      \'total_classification_loss\':  tf.reduce_mean(classification_loss),\n      \'total_localization_loss\': tf.reduce_mean(loc_loss),\n    }\n\n    self.__add_evaluation(evaluation_tensors)\n\n    total_loss = tf.reduce_mean(classification_loss + self.loc_weight * loc_loss) / tf.maximum(1.0, num_positives)\n    return total_loss\n\n  # pylint: disable=too-many-locals\n  def loss(self, ground_truth, prediction, bboxes):\n    """"""\n      Compute multibox loss.\n\n    Args:\n      ground_truth: Ground truth, shape: (?, #priors, 4 + #classes).\n      prediction: Dictionary of predicted tensors, shape: {\'locs\'  : (?, #priors, 4), \\\n                                                           \'confs\' : (?, #priors, #classes), \\\n                                                           \'logits\': (?, #priors, #classes)}.\n    Returns:\n      Prediction loss, shape: (?,).\n    """"""\n    with tf.variable_scope(\'loss_function\'):\n      batch_size = tf.shape(prediction[\'locs\'])[0]\n      num_priors = tf.shape(prediction[\'locs\'])[1]\n\n      localization_loss = MultiboxLoss._localization_loss(ground_truth[:, :, :4],\n                                                          prediction[\'locs\'])  # shape: (batch_size, num_priors)\n      classification_loss = MultiboxLoss._classification_loss(ground_truth[:, :, 4:],\n                                                              prediction[\'logits\'])  # shape: (batch_size, num_priors)\n\n      ground_truth.set_shape([prediction[\'locs\'].shape[0]] + ground_truth.shape[1:].as_list())\n\n      negatives = ground_truth[:, :, 4]  # shape: (batch_size, num_priors)\n      positives = tf.reduce_max(ground_truth[:, :, 5:], axis=-1)  # shape: (batch_size, num_priors)\n      pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1)  # shape: (batch_size,)\n\n      num_positives = tf.reduce_sum(positives)  # shape: (1,)\n      neg_class_loss_all = classification_loss * negatives  # shape: (batch_size, num_priors)\n      n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.float32)  # shape: (1,)\n\n      def no_negatives():\n        return tf.zeros([batch_size], dtype=tf.float32), tf.constant(0, dtype=tf.int32)\n\n      def hard_negative_mining():\n        bboxes_per_batch = tf.unstack(bboxes)\n        classification_loss_per_batch = tf.unstack(classification_loss)\n        num_positives_per_batch = tf.unstack(tf.reduce_sum(positives, axis=-1))\n        neg_class_loss_per_batch = tf.unstack(neg_class_loss_all)\n\n        neg_class_losses = []\n        total_negatives = []\n\n        for bboxes_per_image, classification_loss_per_image, num_positives_per_image, neg_class_loss_per_image in \\\n            zip(bboxes_per_batch, classification_loss_per_batch, num_positives_per_batch, neg_class_loss_per_batch):\n          min_negatives_keep = tf.maximum(self.neg_pos_ratio * num_positives_per_image, 3)\n          num_negatives_keep = tf.minimum(min_negatives_keep,\n                                          tf.count_nonzero(neg_class_loss_per_image, dtype=tf.float32))\n\n          indices = tf.image.non_max_suppression(bboxes_per_image, classification_loss_per_image,\n                                                 tf.to_int32(num_negatives_keep), iou_threshold=0.99)\n          num_negatives = tf.size(indices)\n          total_negatives.append(num_negatives)\n          expanded_indexes = tf.expand_dims(indices, axis=1)  # shape: (num_negatives, 1)\n          negatives_keep = tf.scatter_nd(expanded_indexes, updates=tf.ones_like(indices, dtype=tf.int32),\n                                         shape=tf.shape(classification_loss_per_image))  # shape: (num_priors,)\n          negatives_keep = tf.to_float(tf.reshape(negatives_keep, [num_priors]))  # shape: (batch_size, num_priors)\n          neg_class_losses.append(tf.reduce_sum(classification_loss_per_image * negatives_keep, axis=-1))  # shape: (1,)\n\n        return tf.stack(neg_class_losses), tf.reduce_sum(tf.stack(total_negatives))\n\n      neg_class_loss, total_negatives = tf.cond(tf.equal(n_neg_losses, tf.constant(0.)),\n                                                no_negatives, hard_negative_mining)  # shape: (batch_size,)\n      class_loss = pos_class_loss + neg_class_loss  # shape: (batch_size,)\n      loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1)  # shape: (batch_size,)\n\n      total_loss = tf.reduce_sum(class_loss + self.loc_weight * loc_loss) / tf.maximum(1.0, num_positives)\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      if update_ops:\n        updates = tf.group(*update_ops)\n        total_loss = with_dependencies([updates], total_loss)\n\n      total_classification_loss = tf.reduce_mean(tf.reduce_sum(classification_loss, axis=-1))\n      total_localization_loss = tf.reduce_mean(loc_loss, axis=-1)\n\n      evaluation_tensors = {\n        \'total_classification_loss\': total_classification_loss,\n        \'total_localization_loss\': total_localization_loss,\n        \'num_positives_per_batch\': num_positives,\n        \'num_negatives_per_batch\': total_negatives\n      }\n\n      self.__add_evaluation(evaluation_tensors)\n\n      return total_loss\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/priors.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport math\nimport numpy as np\nfrom ssd_detector.toolbox.layers import get_spatial_dims\n\n\n# pylint: disable=too-many-arguments,too-many-locals\ndef prior_box_specs(blob, image_size, box_specs, step, clip=False, offset=0.5, variance=None, data_format=\'NHWC\'):\n  """"""\n  Generates numpy array of priors in caffe format\n\n\n  :param blob: input feature blob  (we only need spatial dimension from it)\n  :param image_size: input image size (height, width)\n  :param box_specs: list of pairs [size, aspect_ratio]\n  :param step:\n  :param flip: flip each aspect ration or not\n  :param clip: clip priors to image bounding box\n  :param offset: a subpixel offset for priors location\n  :param variance: optional array of lenghts 4 with variances to encode inpriors array\n  :param data_format: NHWC or NCHW\n  """"""\n  assert isinstance(box_specs, list)\n  assert variance is None or len(variance) == 4\n  assert data_format in [\'NHWC\', \'NCHW\']\n\n  if isinstance(step, (list, tuple)):\n    step_y, step_x = step\n  else:\n    step_y, step_x = step, step\n\n  if len(blob.get_shape()) == 2:\n    layer_height = layer_width = 1\n  else:\n    layer_height, layer_width = get_spatial_dims(blob, data_format)\n\n  anchors = []\n  for height in range(layer_height):\n    for width in range(layer_width):\n      center_y = (height + offset) * step_y\n      center_x = (width + offset) * step_x\n\n      for size, aspect_ratio in box_specs:\n        box_w = size * math.sqrt(aspect_ratio)\n        box_h = size / math.sqrt(aspect_ratio)\n        xmin = (center_x - box_w / 2.) / image_size[1]\n        ymin = (center_y - box_h / 2.) / image_size[0]\n        xmax = (center_x + box_w / 2.) / image_size[1]\n        ymax = (center_y + box_h / 2.) / image_size[0]\n        anchors.extend([xmin, ymin, xmax, ymax])\n\n  if clip:\n    anchors = np.clip(anchors, 0., 1.).tolist()\n\n  num_priors_per_pixel = len(anchors) // (layer_height * layer_width * 4)\n  num_priors_alt_formula = len(box_specs)\n  assert num_priors_per_pixel == num_priors_alt_formula\n\n  if variance:\n    anchors.extend(list(variance) * layer_height * layer_width * num_priors_per_pixel)\n\n  top_shape = 1, (2 if variance else 1), layer_height * layer_width * num_priors_per_pixel * 4\n\n  assert len(anchors) == np.prod(top_shape)\n  priors_array = np.array([anchors], dtype=np.float32).reshape(top_shape)\n  return priors_array, num_priors_per_pixel\n\n\n# pylint: disable=too-many-arguments,too-many-locals\ndef prior_box(blob, image_size, min_sizes, aspect_ratios, step, max_sizes=None, flip=True, clip=False, offset=0.5,\n              variance=None, data_format=\'NHWC\'):\n  """"""\n  Generates numpy array of priors in caffe format\n\n\n  :param blob: input feature blob  (we only need spatial dimension from it)\n  :param image_size: input image size (height, width)\n  :param min_sizes:\n  :param aspect_ratios:\n  :param step:\n  :param max_sizes:\n  :param flip: flip each aspect ration or not\n  :param clip: clip priors to image bounding box\n  :param offset: a subpixel offset for priors location\n  :param variance: optional array of lenghts 4 with variances to encode inpriors array\n  :param data_format: NHWC or NCHW\n  """"""\n  assert isinstance(min_sizes, list) and isinstance(aspect_ratios, list)\n  assert not max_sizes or len(max_sizes) == len(min_sizes)\n  assert variance is None or len(variance) == 4\n\n  max_sizes = max_sizes or []\n\n  ratios = []\n  for aspect_ratio in aspect_ratios:\n    close = [r for r in ratios if math.fabs(aspect_ratio - r) < 1e-6]\n    if not close:\n      ratios.append(aspect_ratio)\n      if flip:\n        ratios.append(1. / aspect_ratio)\n\n  box_specs = []\n\n  for (ind, min_size) in enumerate(min_sizes):\n    box_specs.append([min_size, 1.])\n\n    if max_sizes:\n      max_size = max_sizes[ind]\n      box_specs.append([math.sqrt(min_size * max_size), 1.])\n\n    # rest of priors\n    for aspect_ratio in ratios:\n      assert math.fabs(aspect_ratio - 1.) >= 1e-6\n      box_specs.append([min_size, math.sqrt(aspect_ratio)])\n\n  return prior_box_specs(blob, image_size, box_specs, step, clip, offset, variance, data_format)\n\n\n# pylint: disable=too-many-arguments,too-many-locals\ndef prior_box_clusterd(blob, image_size, clustered_sizes, step, clip=False, offset=0.5, variance=None,\n                       data_format=\'NHWC\'):\n  """"""\n  Generates numpy array of priors in caffe format\n\n  :param blob: input feature blob  (we only need spatial dimension from it)\n  :param image_size: input image size (height, width)\n  :param clustered_sizes: list of (height, width) tuples\n  :param step:\n  :param clip: clip priors to image bounding box\n  :param offset: a subpixel offset for priors location\n  :param variance: optional array of lenghts 4 with variances to encode inpriors array\n  :param data_format: NHWC or NCHW\n  """"""\n  assert variance is None or len(variance) == 4\n  assert data_format in [\'NHWC\', \'NCHW\']\n\n  if isinstance(step, (list, tuple)):\n    step_y, step_x = step\n  else:\n    step_y, step_x = step, step\n\n  if len(blob.get_shape()) == 2:\n    layer_height = layer_width = 1\n  else:\n    layer_height, layer_width = get_spatial_dims(blob, data_format)\n\n  num_priors_per_pixel = len(clustered_sizes)\n  top_shape = 1, (2 if variance else 1), layer_height * layer_width * num_priors_per_pixel * 4\n\n  anchors = []\n  for height in range(layer_height):\n    for width in range(layer_width):\n      center_x = (width + offset) * step_x / image_size[1]\n      center_y = (height + offset) * step_y / image_size[0]\n\n      for (box_rows, box_cols) in clustered_sizes:\n        xmin = center_x - box_cols / 2.\n        ymin = center_y - box_rows / 2.\n        xmax = center_x + box_cols / 2.\n        ymax = center_y + box_rows / 2.\n        anchors.extend([xmin, ymin, xmax, ymax])\n\n  if clip:\n    anchors = np.clip(anchors, 0., 1.).tolist()\n\n  if variance:\n    anchors.extend(list(variance) * layer_width * layer_height * num_priors_per_pixel)\n\n  assert len(anchors) == np.prod(top_shape)\n  priors_array = np.array([anchors], dtype=np.float32).reshape(top_shape)\n  return priors_array, num_priors_per_pixel\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/ssd_base.py,48,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport pickle\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nfrom ssd_detector.toolbox.layers import channel_to_last, get_spatial_dims\nfrom ssd_detector.toolbox.priors import prior_box, prior_box_clusterd, prior_box_specs\n\n\n# pylint: disable=too-many-instance-attributes\nclass SSDBase:\n  def __init__(self, input_shape, num_classes=2, overlap_threshold=0.5, data_format=\'NHWC\'):\n    assert len(input_shape) == 4\n    assert data_format in [\'NHWC\', \'NCHW\']\n\n    self.input_shape = input_shape\n    self.num_classes = num_classes\n    self.clip = False\n    self.overlap_threshold = overlap_threshold\n    self.mbox_loc = None\n    self.mbox_conf = None\n    self.mbox_priorbox = None\n    self.logits = None\n    self.predictions = None\n    self.detections = None\n    self.priors_array = None\n    self.priors = None\n    self.priors_info = []\n    self.flattens_for_tfmo = []\n    self.data_format = data_format\n\n  # pylint: disable=too-many-arguments\n  def _add_single_ssd_head(self, blob, num_classes, num_anchors, prefix, suffix=\'\'):\n    with slim.arg_scope([slim.conv2d], activation_fn=None, normalizer_fn=None, padding=\'SAME\', normalizer_params=None):\n      if len(blob.shape) == 4:\n        locs = slim.conv2d(blob, num_anchors * 4, (3, 3),\n                           scope=\'{}_mbox_loc{}\'.format(prefix, suffix), data_format=self.data_format)\n        locs = channel_to_last(locs, data_format=self.data_format)\n        locs = slim.flatten(locs)\n        conf = slim.conv2d(blob, num_anchors * num_classes, (3, 3), biases_initializer=tf.constant_initializer(0.0),\n                           scope=\'{}_mbox_conf{}\'.format(prefix, suffix), data_format=self.data_format)\n        conf = channel_to_last(conf, data_format=self.data_format)\n        conf = slim.flatten(conf)\n        self.flattens_for_tfmo.extend([locs, conf])\n      elif len(blob.shape) == 2:\n        locs = slim.fully_connected(blob, num_anchors * 4, activation_fn=None,\n                                    scope=\'{}_mbox_loc{}\'.format(prefix, suffix))\n        conf = slim.fully_connected(blob, num_anchors * num_classes, activation_fn=None,\n                                    scope=\'{}_mbox_conf{}\'.format(prefix, suffix))\n      else:\n        raise Exception(\'Unsupported input blob shape for SSD.\')\n      return conf, locs\n\n  # pylint: disable=too-many-locals\n  def create_heads(self, connections, params_dicts):\n    image_size = get_spatial_dims(self.input_shape, self.data_format)\n\n    with tf.variable_scope(\'ssd_heads\'):\n      scores, bboxes, priors = [], [], []\n      priors_array = []\n      for head_id, (tensor, params) in enumerate(zip(connections, params_dicts)):\n        with tf.variable_scope(\'head_{}\'.format(head_id)):\n          if \'clustered_sizes\' in params:\n            priors_fn = prior_box_clusterd\n          elif \'box_specs\' in params:\n            priors_fn = prior_box_specs\n          else:\n            priors_fn = prior_box\n          fn_params = {k: v for k, v in params.items() if not k == \'prefix\' and not k == \'suffix\'}\n          fn_params[\'data_format\'] = self.data_format\n\n          numpy_priors, num_priors_per_pixel = priors_fn(tensor, image_size, **fn_params)\n          assert np.prod(get_spatial_dims(tensor)) * num_priors_per_pixel == numpy_priors.shape[2] // 4\n          self.priors_info.append([get_spatial_dims(tensor), num_priors_per_pixel])\n          priors_array.append(numpy_priors)\n\n          priors_tensor = tf.convert_to_tensor(numpy_priors, name=\'{}_priorbox\'.format(params[\'prefix\']))\n          priors.append(priors_tensor)\n\n          score, bbox = self._add_single_ssd_head(tensor, self.num_classes, num_priors_per_pixel, params[\'prefix\'],\n                                                  params.get(\'suffix\', \'\'))\n          scores.append(score)\n          bboxes.append(bbox)\n\n      with tf.name_scope(\'concat_reshape_softmax\'):\n        # Gather all predictions\n        self.mbox_loc = tf.concat(bboxes, axis=-1, name=\'mbox_loc\') if len(connections) > 1 else bboxes[0]\n        self.mbox_conf = tf.concat(scores, axis=-1, name=\'mbox_conf\') if len(connections) > 1 else scores[0]\n        self.mbox_priorbox = tf.concat(priors, axis=-1, name=\'mbox_priorbox\') if len(connections) > 1 else priors[0]\n\n        total_priors = self.mbox_conf.get_shape()[-1] // self.num_classes\n        self.mbox_loc = tf.reshape(self.mbox_loc, shape=(-1, total_priors, 4), name=\'mbox_loc_final\')\n        self.logits = tf.reshape(self.mbox_conf, shape=(-1, total_priors, self.num_classes), name=\'mbox_conf_logits\')\n        self.mbox_conf = tf.sigmoid(self.logits, name=\'mbox_conf_final\')\n        # self.mbox_conf = tf.nn.softmax(self.logits, name=\'mbox_conf_final\')\n\n    self.priors_array = np.reshape(np.concatenate(priors_array, axis=-1), (2, -1, 4))\n    self.priors = tf.reshape(self.mbox_priorbox, shape=(1, 2, -1, 4), name=\'mbox_priorbox_final\')\n    assert self.priors_array.shape[1] == total_priors\n\n    self.predictions = dict(locs=self.mbox_loc, confs=self.mbox_conf, logits=self.logits)\n    return self.predictions\n\n  def _iou(self, box):\n    """"""Compute intersection over union for the box with all priors.\n\n        # Arguments\n            box: Box, numpy tensor of shape (4,).\n\n        # Return\n            iou: Intersection over union,\n                numpy tensor of shape (num_priors).\n        """"""\n    # compute intersection\n    inter_upleft = np.maximum(self.priors_array[0, :, :2], box[:2])\n    inter_botright = np.minimum(self.priors_array[0, :, 2:], box[2:])\n    inter_wh = np.maximum(inter_botright - inter_upleft, 0)\n    inter = inter_wh[:, 0] * inter_wh[:, 1]\n    # compute union\n    area_pred = (box[2] - box[0]) * (box[3] - box[1])\n    area_gt = (self.priors_array[0, :, 2] - self.priors_array[0, :, 0])\n    area_gt *= (self.priors_array[0, :, 3] - self.priors_array[0, :, 1])\n    union = area_pred + area_gt - inter\n    # compute iou\n    iou = inter / union\n    return iou\n\n  def _encode_box(self, box, return_iou=True):\n    """"""Encode box for training, do it only for assigned priors.\n\n        # Arguments\n            box: Box, numpy tensor of shape (4,).\n            return_iou: Whether to concat iou to encoded values.\n\n        # Return\n            encoded_box: Tensor with encoded box\n                numpy tensor of shape (num_priors, 4 + int(return_iou)).\n        """"""\n    iou = self._iou(box)\n    encoded_box = np.zeros((self.priors_array.shape[1], 4 + return_iou), np.float32)\n    assign_mask = iou > self.overlap_threshold\n    if not assign_mask.any():\n      assign_mask[iou.argmax()] = True\n    if return_iou:\n      encoded_box[:, -1][assign_mask] = iou[assign_mask]\n    assigned_priors = self.priors_array[:, assign_mask, :]\n    box_center = 0.5 * (box[:2] + box[2:])\n    box_wh = box[2:] - box[:2]\n    assigned_priors_center = 0.5 * (assigned_priors[0, :, :2] + assigned_priors[0, :, 2:])\n    assigned_priors_wh = (assigned_priors[0, :, 2:] - assigned_priors[0, :, :2])\n    # we encode variance\n    encoded_box[:, :2][assign_mask] = \\\n      (box_center - assigned_priors_center) / assigned_priors_wh / assigned_priors[1, :, :2]\n    encoded_box[:, 2:4][assign_mask] = np.log(box_wh / assigned_priors_wh) / assigned_priors[1, :, 2:]\n    return encoded_box.ravel()\n\n  def _assign_boxes(self, boxes):\n    """"""Assign boxes to priors for training.\n\n        # Arguments\n            boxes: Box, numpy tensor of shape (num_boxes, 4 + num_classes), num_classes without background.\n\n        # Return\n            assignment: Tensor with assigned boxes, numpy tensor of shape (num_priors, 4 + num_classes).\n        """"""\n\n    target_shape = self.priors_array.shape[1], 4 + self.num_classes\n    assignment = np.zeros(target_shape, dtype=np.float32)\n    assignment[:, 4] = 1.0  # mark all as background\n    # pylint: disable=len-as-condition\n    if len(boxes) == 0:\n      return assignment\n    encoded_boxes = np.apply_along_axis(self._encode_box, 1, boxes[:, :4])\n    encoded_boxes = encoded_boxes.reshape(-1, self.priors_array.shape[1], 5)\n    best_iou = encoded_boxes[:, :, -1].max(axis=0)\n    best_iou_idx = encoded_boxes[:, :, -1].argmax(axis=0)\n    best_iou_mask = best_iou > 0\n    best_iou_idx = best_iou_idx[best_iou_mask]\n    assign_num = len(best_iou_idx)\n    encoded_boxes = encoded_boxes[:, best_iou_mask, :]\n    assignment[:, :4][best_iou_mask] = encoded_boxes[best_iou_idx, np.arange(assign_num), :4]\n    assignment[:, 4][best_iou_mask] = 0\n    assignment[:, 5:][best_iou_mask] = boxes[best_iou_idx, 4:]\n    return assignment\n\n  def _compute_target(self, encoded_annotation):\n    annotations = [pickle.loads(ea) for ea in encoded_annotation]\n    assigns = []\n    for annotation in annotations:\n      one_hots, rects = [], []\n      for label, boxes in annotation.items():\n        one_hot = np.zeros((len(boxes), self.num_classes), dtype=np.float32)\n        one_hot[:, label] = 1\n        one_hots.extend(one_hot[:, 1:])\n        rects.extend([[bb.xmin, bb.ymin, bb.xmax, bb.ymax] for bb in boxes])\n      boxes_with_labels = np.hstack((np.asarray(rects), np.asarray(one_hots)))\n      assigns.append(self._assign_boxes(boxes_with_labels))\n    return np.array(assigns)\n\n  def create_targets(self, annoation_tensor):\n    assigns = tf.py_func(self._compute_target, [annoation_tensor], tf.float32, stateful=False, name=\'compute_target\')\n    assigns = tf.reshape(assigns, [-1, self.priors_array.shape[1], 4 + self.num_classes])\n    return assigns\n\n  def _decode_boxes(self, locs, priors, variance, scope=\'bboxes_decode\'):\n    with tf.variable_scope(scope):\n      # priors (1, #priors, 4) = #priors * [xmin, ymin, xmax, ymax]\n      prior_w = priors[:, 2] - priors[:, 0]\n      prior_h = priors[:, 3] - priors[:, 1]\n      prior_cx = 0.5 * (priors[:, 0] + priors[:, 2])\n      prior_cy = 0.5 * (priors[:, 1] + priors[:, 3])\n\n      # Compute center, height and width\n      center_x = locs[:, :, 0] * prior_w * variance[:, 0] + prior_cx\n      center_y = locs[:, :, 1] * prior_h * variance[:, 1] + prior_cy\n      width = prior_w * tf.exp(locs[:, :, 2] * variance[:, 2])\n      height = prior_h * tf.exp(locs[:, :, 3] * variance[:, 3])\n\n      xmin = center_x - 0.5 * width\n      ymin = center_y - 0.5 * height\n      xmax = center_x + 0.5 * width\n      ymax = center_y + 0.5 * height\n      bbox = tf.stack([xmin, ymin, xmax, ymax], axis=-1)\n      if self.clip:\n        bbox = tf.clip_by_value(bbox, 0., 1.)\n      return bbox\n\n  def detection_output(self, background_label=0, confidence_threshold=0.01, top_k=400, nms_threshold=0.45,\n                       keep_top_k=200, scope=\'DetectionOutput\', use_plain_caffe_format=True):\n    assert background_label == 0  # only such mode is supported\n\n    total_priors_count = self.priors.shape.as_list()[2]\n    if top_k > total_priors_count:\n      tf.logging.warning(\'detection_output::top_k value is greater than total priors count. \'\n                         \'Set top_k to total_priors_count = {}\'.format(total_priors_count))\n      top_k = total_priors_count\n\n    if keep_top_k > total_priors_count:\n      tf.logging.warning(\'detection_output::keep_top_k value is greater than total priors count. \'\n                         \'Set keep_top_k to total_priors_count = {}\'.format(total_priors_count))\n      keep_top_k = total_priors_count\n\n    with tf.variable_scope(scope):\n      bboxes = self._decode_boxes(self.mbox_loc, priors=self.priors[0, 0], variance=self.priors[0, 1])\n      scores = self.mbox_conf\n\n      batch_size = tf.shape(bboxes)[0]\n      num_classes = self.num_classes\n\n      def b_body(img_id, detection_batch):\n        b_scores = scores[img_id]\n        b_bboxes = bboxes[img_id]\n\n        def c_body(class_id, detection_array):\n          # Zeroing predictions below threshold\n          with tf.variable_scope(\'bboxes_c_select\', reuse=True):\n            c_scores = b_scores[:, class_id]\n            c_fmask = tf.cast(tf.greater(c_scores, confidence_threshold), scores.dtype)\n            c_scores = c_scores * c_fmask\n            c_bboxes = b_bboxes * tf.expand_dims(c_fmask, axis=-1)\n\n          # Apply NMS\n          with tf.variable_scope(\'bboxes_c_nms\', reuse=True):\n            c_indices = tf.image.non_max_suppression(c_bboxes, c_scores, top_k, nms_threshold)\n            size = tf.size(c_indices)\n            c_batch_ = tf.to_float(img_id) * tf.ones(shape=[top_k, 1], dtype=tf.float32)  # len(indices) x 1\n            c_labels = tf.to_float(class_id) * tf.ones(shape=[top_k, 1], dtype=tf.float32)  # len(indices) x 1\n\n            extra_size = top_k - size\n            c_scores = tf.expand_dims(tf.gather(c_scores, c_indices), axis=-1)  # len(indices) x 1\n            empty_c_scores = tf.zeros([extra_size, 1], dtype=tf.float32)\n            c_scores = tf.concat([c_scores, empty_c_scores], axis=0)\n\n            c_bboxes = tf.gather(c_bboxes, c_indices)  # len(indices) x 4\n            empty_c_bboxes = tf.zeros([extra_size, 4], dtype=tf.float32)\n            c_bboxes = tf.concat([c_bboxes, empty_c_bboxes], axis=0)\n            c_predictions = tf.concat([c_batch_, c_labels, c_scores, c_bboxes], axis=1)  # top_k x 7\n          return class_id + 1, detection_array.write(index=class_id - 1, value=c_predictions)\n\n        # loop over num_classes\n        class_id = 1  # c = 0 is a background, classes starts with index 1\n        detection_img = tf.TensorArray(tf.float32, size=num_classes - 1)\n        _, detection_img = tf.while_loop(lambda c, pa: tf.less(c, num_classes), c_body, [class_id, detection_img],\n                                         back_prop=False, parallel_iterations=1)\n        detection_img_flat = detection_img.concat()\n\n        # Select topmost \'keep_top_k\' predictions\n        with tf.variable_scope(\'bboxes_keep_top_k\', reuse=True):\n          k = tf.minimum(keep_top_k, tf.shape(detection_img_flat)[0])\n          _, indices = tf.nn.top_k(detection_img_flat[:, 2], k, sorted=True)\n          detection_img_flat = tf.gather(detection_img_flat, indices)\n\n        return img_id + 1, detection_batch.write(index=img_id, value=detection_img_flat)\n\n      # loop over batch\n      detection_batch = tf.TensorArray(tf.float32, size=batch_size)\n      _, detection_batch = tf.while_loop(lambda img_id, ra: tf.less(img_id, batch_size),\n                                         b_body, [0, detection_batch], back_prop=False)\n\n      self.detections = detection_batch.concat() if use_plain_caffe_format else detection_batch.stack()\n      self.detections = tf.reshape(self.detections, [-1, keep_top_k, self.detections.shape[-1]])\n      return self.detections\n\n  def get_config_for_tfmo(self, confidence_threshold=0.01, top_k=400, nms_threshold=0.45, keep_top_k=200):\n    locs, confs, priors = self.mbox_loc.name[:-2], self.mbox_conf.name[:-2], self.mbox_priorbox.name[:-2]\n\n    flatten_scopes = [f.name[:-2].replace(\'flatten/Reshape\', \'\') for f in self.flattens_for_tfmo]\n\n    json_lines = [\n      \'[\',\n      \'    {\',\n      \'        ""custom_attributes"": {\',\n      \'            ""code_type"": ""caffe.PriorBoxParameter.CENTER_SIZE"",\',\n      # \'            ""num_classes"": {},\'.format(self.num_classes),   # Now TF MO estimates this automatically\n      \'            ""confidence_threshold"": {},\'.format(confidence_threshold),\n      \'            ""keep_top_k"": {},\'.format(keep_top_k),\n      \'            ""nms_threshold"": {},\'.format(nms_threshold),\n      \'            ""top_k"": {},\'.format(top_k),\n      \'            ""pad_mode"": ""caffe.ResizeParameter.CONSTANT"",\',\n      \'            ""resize_mode"": ""caffe.ResizeParameter.WARP""\',\n      \'        },\',\n      \'        ""id"": ""SSDToolboxDetectionOutput"",\',\n      \'        ""include_inputs_to_sub_graph"": true,\',\n      \'        ""include_outputs_to_sub_graph"": true,\',\n      \'        ""instances"": {\',\n      \'            ""end_points"": [\',\n      \'                ""{}"",\'.format(locs),\n      \'                ""{}"",\'.format(confs),\n      \'                ""{}""\'.format(priors),\n      \'            ],\',\n      \'            ""start_points"": [\',\n      \'                ""{}"",\'.format(locs),\n      \'                ""{}"",\'.format(confs),\n      \'                ""{}""\'.format(priors),\n      \'            ]\',\n      \'        },\',\n      \'        ""match_kind"": ""points""\',\n      \'    }\']\n    json_nhwc_lines = [\n      \'    {\',\n      \'        ""custom_attributes"": {},\',\n      \'        ""id"": ""ConvFlatten"",\',\n      \'        ""inputs"": [\',\n      \'            [\',\n      \'                {\',\n      \'                    ""node"": ""flatten/Reshape$"",\',\n      \'                    ""port"": 0\',\n      \'                },\',\n      \'                {\',\n      \'                    ""node"": ""flatten/Shape$"",\',\n      \'                    ""port"": 0\',\n      \'                }\',\n      \'            ],\',\n      \'            [\',\n      \'                {\',\n      \'                    ""node"": ""flatten/Shape$"",\',\n      \'                    ""port"": 0\',\n      \'                },\',\n      \'                {\',\n      \'                    ""node"": ""flatten/Reshape$"",\',\n      \'                    ""port"": 0\',\n      \'                }\',\n      \'            ]\',\n      \'        ],\',\n      \'        ""instances"": [\',\n      \',\\n\'.join([\'            ""{}""\'.format(s) for s in flatten_scopes]),\n      \'        ],\',\n      \'        ""match_kind"": ""scope"",\',\n      \'        ""outputs"": [\',\n      \'            {\',\n      \'                ""node"": ""flatten/Reshape$"",\',\n      \'                ""port"": 0\',\n      \'            }\',\n      \'        ]\',\n      \'    }\',\n      \']\',\n    ]\n\n    json_config = \'\\n\'.join(json_lines)\n    if self.data_format == \'NHWC\':\n      json_config += \',\\n\' + \'\\n\'.join(json_nhwc_lines)\n\n    class_name = self.__class__.__name__\n    outputs = [locs, confs, priors] if self.detections is None else [self.detections.name[:-2]]\n\n    return dict(json=json_config, cut_points=[locs, confs, priors], output_nodes=outputs, class_name=class_name)\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/summary.py,22,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport pickle\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm, Normalize\nimport numpy as np\nimport tensorflow as tf\n\n\n# pylint: disable=invalid-name\ndef create_tensors_and_streaming_ops_for_assigned_priors(assigned_priors, priors_info, num_classes, weights=1.):\n  total_priors_number = np.sum(\n    [np.prod(priors_count) * num_priors_per_pixel for priors_count, num_priors_per_pixel in priors_info])\n  metric_ops = {}\n\n  with tf.name_scope(\'priors_streaming_summary\'):\n    assigned_priors_total = tf.reduce_mean(tf.reduce_max(assigned_priors[:, :, 5:], axis=-1), axis=0)\n    assigned_priors_total = tf.reshape(assigned_priors_total, [total_priors_number])  # For streaming operations\n    stream_assigned_priors_total = tf.metrics.mean_tensor(assigned_priors_total, weights=weights)\n    metric_ops[\'summary\'] = stream_assigned_priors_total\n\n    for i in range(num_classes):\n      if i == 0:  # Skip background class\n        continue\n      class_id = 4 + i  # 4 - localization\n      class_name = \'class_{0}\'.format(i)\n\n      assigned_priors_type = tf.reduce_mean(assigned_priors[:, :, class_id], axis=0)  # Mean by batch\n      assigned_priors_type = tf.reshape(assigned_priors_type, [total_priors_number])  # For streaming operations\n\n      stream_assigned_priors_type = tf.metrics.mean_tensor(assigned_priors_type, weights=weights)\n      metric_ops[class_name] = stream_assigned_priors_type\n\n  return metric_ops\n\n\n# pylint: disable=too-many-locals,invalid-name\ndef get_detailed_assigned_priors_summary(assigned_priors, priors_info, name):\n  """"""\n  Get assigned priors 1D tensors by SSD heads and priors type.\n\n  Args:\n    assigned_priors: Assigned priors, tensor of shape (num_priors).\n    priors_info: Information about priors, list of pairs for every ssd head: tensor_dimensions, num_priors_per_pixel.\n    name: Output name.\n\n  Returns:\n    detailed_assigned_priors: Dictionary with tensors for every SSD head and prior type.\n  """"""\n  assert len(assigned_priors.shape) == 1\n\n  detailed_assigned_priors = dict()\n  detailed_assigned_priors[\'priors/{0}\'.format(name)] = assigned_priors\n\n  start = 0\n  total_priors_number = int(assigned_priors.shape[0])\n\n  for head_id, (tensor_dimensions, num_priors_per_pixel) in enumerate(priors_info):\n    priors_per_type = np.prod(tensor_dimensions)\n    priors_count = np.prod(tensor_dimensions) * num_priors_per_pixel\n\n    prior_map = np.zeros(shape=total_priors_number, dtype=np.bool)\n    for i in range(priors_count):\n      prior_map[start + i] = True\n\n    if isinstance(assigned_priors, tf.Tensor):\n      assigned_priors_head = tf.boolean_mask(assigned_priors, prior_map)\n      assigned_priors_head = tf.reshape(assigned_priors_head, [priors_count])\n    else:\n      assigned_priors_head = assigned_priors[prior_map]\n\n    detailed_assigned_priors[\'priors_by_head/{0}/head_{1}\'.format(name, head_id)] = assigned_priors_head\n\n    for offset in range(num_priors_per_pixel):\n      prior_map = np.zeros(shape=total_priors_number, dtype=np.bool)\n      for i in range(priors_per_type):\n        prior_map[start + offset + i * num_priors_per_pixel] = True\n\n      if isinstance(assigned_priors, tf.Tensor):\n        assigned_priors_head_type = tf.boolean_mask(assigned_priors, prior_map)\n        assigned_priors_head_type = tf.reshape(assigned_priors_head_type, [priors_per_type])\n      else:\n        assigned_priors_head_type = assigned_priors[prior_map]\n\n      assigned_priors_head_type_name = \'priors_by_head_and_type/{0}/head_{1}/prior_{2}\'.format(name, head_id,\n                                                                                               offset)\n      detailed_assigned_priors[assigned_priors_head_type_name] = assigned_priors_head_type\n\n    start += priors_count\n\n  return detailed_assigned_priors\n\n\ndef get_detailed_assigned_priors_summary_tf(assigned_priors_dict, priors_info):\n  detailed_assigned_priors = dict()\n\n  with tf.name_scope(\'detailed_priors_summary\'):\n    for name, (assigned_priors, _) in assigned_priors_dict.items():\n      with tf.name_scope(name):\n        detailed_assigned_priors.update(\n          get_detailed_assigned_priors_summary(assigned_priors, priors_info, name))\n\n  return detailed_assigned_priors\n\n\ndef group_ssd_heads(assigned_priors, prefix=\'prior_histogram/\'):\n  groups = {}\n  priors_by_head_and_type = [(key.replace(prefix + \'priors_by_head_and_type/\', \'\', 1), val)\n                             for key, val in sorted(assigned_priors.items())\n                             if key.startswith(prefix + \'priors_by_head_and_type/\')]\n\n  if not priors_by_head_and_type:\n    return {}\n\n  def __extract_key(key):\n    keys = key.split(\'/\')\n    class_name = keys[0]\n    head_id = int(keys[1].split(\'_\')[1])\n    prior_id = int(keys[2].split(\'_\')[1])\n    return (class_name, head_id, prior_id)\n\n  priors_by_head_and_type = [(__extract_key(key), val) for key, val in priors_by_head_and_type]\n\n  max_head_id = np.max([head_id for (_, head_id, prior_id), val in priors_by_head_and_type]) + 1\n  max_prior_id = np.max([prior_id for (_, head_id, prior_id), val in priors_by_head_and_type]) + 1\n\n  for (class_name, _, _), _ in priors_by_head_and_type:\n    mat = [[None for _ in range(max_prior_id)] for __ in range(max_head_id)]\n    groups.setdefault(class_name, mat)\n\n  for (class_name, head_id, prior_id), val in priors_by_head_and_type:\n    groups[class_name][head_id][prior_id] = val\n\n  return groups\n\n\ndef fig_to_data(fig):\n  fig.canvas.draw()\n  width, height = fig.canvas.get_width_height()\n  buf = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n  buf.shape = (height, width, 3)\n  return buf\n\n\ndef write_histogram_2d_tf(assigned_priors, priors_info, name, step, log_dir):\n  try:\n    name = name.decode()  # Convert byte-string to str\n    log_dir = log_dir.decode()  # Convert byte-string to str\n  except AttributeError:\n    pass\n\n  priors_info = pickle.loads(priors_info)\n\n  detailed_assigned_priors = get_detailed_assigned_priors_summary(assigned_priors, priors_info, name)\n  group = group_ssd_heads(detailed_assigned_priors, \'\')\n  write_histogram_2d(group, step, log_dir, use_lognorm=False)\n  write_histogram_2d(group, step, log_dir, use_lognorm=True)\n\n  return True\n\n\n# pylint: disable=too-many-locals,invalid-name\ndef write_histogram_2d(assigned_priors_group, step, log_dir, use_lognorm=False):\n  summaries = []\n  for type_str, group in assigned_priors_group.items():\n    max_head_id = len(group)\n    max_prior_id = len(group[0])\n    fig, axes = plt.subplots(max_head_id, max_prior_id, figsize=(max_prior_id * 2.5, max_head_id * 2))\n    fig.tight_layout()\n\n    if use_lognorm:\n      name = \'priors_hist2d_log/\' + type_str\n    else:\n      name = \'priors_hist2d/\' + type_str\n\n    all_data = [prior for head in group for prior in head if prior is not None]\n\n    max_val = np.max([np.max(data) for data in all_data])\n    min_data = [np.min(data[data > 0]) for data in all_data if np.sum(data > 0) != 0]\n    min_val = np.min(min_data) if min_data else 0\n\n    if use_lognorm:\n      if min_val == 0:\n        continue\n\n    for i, head in enumerate(group):\n      for j, prior in enumerate(head):\n        if prior is None:\n          axes[i, j].axis(\'off\')\n        else:\n          prior = prior.copy()\n          prior[prior <= 0] = min_val * 0.01\n          prior_shape = int(np.sqrt(prior.shape))\n          prior = np.reshape(prior, (prior_shape, prior_shape))\n          dims = prior_shape, prior_shape\n          x_values, y_values = np.meshgrid(range(prior.shape[0]), range(prior.shape[1]))\n          ret = axes[i, j].hist2d(x_values.reshape((-1)), y_values.reshape((-1)), weights=prior.reshape((-1)),\n                                  bins=dims,\n                                  cmap=plt.cm.brg,\n                                  norm=LogNorm() if use_lognorm else Normalize(),\n                                  vmin=min_val * 0.95, vmax=max_val)\n          colorbar_map = ret[3]\n          axes[i, j].axis(\'on\')\n          axes[i, j].plot()\n\n    fig.subplots_adjust(right=0.9)\n    cbar_ax = fig.add_axes([0.93, 0.05, 0.02, 0.9])\n    fig.colorbar(colorbar_map, cax=cbar_ax)\n\n    img = fig_to_data(fig)\n    plt.close(fig)\n\n    encoded_image = cv2.imencode(\'.jpg\', img)[1].tostring()\n    img_sum = tf.Summary.Image(encoded_image_string=encoded_image, height=img.shape[0], width=img.shape[1])\n    summaries.append(tf.Summary.Value(tag=name, image=img_sum))\n\n  if summaries:\n    # pylint: disable=protected-access\n    with tf.summary.FileWriterCache._lock:\n      # pylint: disable=protected-access\n      if log_dir not in tf.summary.FileWriterCache._cache:\n        tf.summary.FileWriterCache._cache[log_dir] = tf.summary.FileWriter(log_dir,\n                                                                           graph=None)  # Don\'t store the graph\n      writer = tf.summary.FileWriterCache._cache[log_dir]\n\n    writer.add_summary(tf.Summary(value=summaries), step)\n'"
tensorflow_toolkit/ssd_detector/ssd_detector/toolbox/transformer.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\nimport bisect\nimport math\nimport random\n\nimport cv2\nimport numpy as np\n\nfrom ssd_detector.toolbox.bounding_box import BoundingBox, box_coverage, extrapolate_box, generate_batch_samples\n\n\n# #############################################################################\n# ################# Parameters and Transformer ################################\n# #############################################################################\n\n# pylint: disable=too-many-instance-attributes\nclass ResizeParameter:\n  WARP = \'WARP\'\n  FIT_SMALL_SIZE = \'FIT_SMALL_SIZE\'\n  FIT_LARGE_SIZE_AND_PAD = \'FIT_LARGE_SIZE_AND_PAD\'\n\n  # pylint: disable=too-many-arguments\n  def __init__(self, prob=1.0, resize_mode=WARP, height=0, width=0, height_scale=0., width_scale=0.,\n               pad_mode=cv2.BORDER_CONSTANT, pad_value=(0, 0, 0),\n               interp_mode=(cv2.INTER_LINEAR,\n                            cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_CUBIC, cv2.INTER_LANCZOS4)):\n    self.prob = prob\n    self.resize_mode = resize_mode\n    self.height = height\n    self.width = width\n    self.height_scale = height_scale\n    self.width_scale = width_scale\n    self.pad_mode = pad_mode\n    self.pad_value = pad_value\n    self.interp_mode = interp_mode or []\n\n\nclass NoiseParameter:\n  # pylint: disable=too-many-arguments\n  def __init__(self, prob=0., hist_eq=False, inverse=False, decolorize=False, gauss_blur=False, jpeg=-1,\n               posterize=False,\n               erode=False, clahe=False, saltpepper_fraction=0, saltpepper_values=None, convert_to_hsv=False,\n               convert_to_lab=False):\n    self.prob = prob\n    self.hist_eq = hist_eq\n    self.inverse = inverse\n    self.decolorize = decolorize\n    self.gauss_blur = gauss_blur\n    self.jpeg = jpeg\n    self.posterize = posterize\n    self.erode = erode\n    self.clahe = clahe\n    self.saltpepper_fraction = saltpepper_fraction\n    self.saltpepper_values = saltpepper_values\n    self.convert_to_hsv = convert_to_hsv\n    self.convert_to_lab = convert_to_lab\n\n\nclass DistortionParameter:\n  # pylint: disable=too-many-arguments\n  def __init__(self, brightness_prob=0., brightness_delta=0., contrast_prob=0., contrast_lower=0., contrast_upper=0.,\n               hue_prob=0., hue_delta=0., saturation_prob=0., saturation_lower=0., saturation_upper=0.,\n               random_order_prob=0.):\n    self.brightness_prob = brightness_prob\n    self.brightness_delta = brightness_delta\n    self.contrast_prob = contrast_prob\n    self.contrast_lower = contrast_lower\n    self.contrast_upper = contrast_upper\n    self.hue_prob = hue_prob\n    self.hue_delta = hue_delta\n    self.saturation_prob = saturation_prob\n    self.saturation_lower = saturation_lower\n    self.saturation_upper = saturation_upper\n    self.random_order_prob = random_order_prob\n\n\nclass ExpansionParameter:\n  def __init__(self, prob=1., max_expand_ratio=1., fill_with_current_image_mean=False):\n    self.prob = prob\n    self.max_expand_ratio = max_expand_ratio\n    self.fill_with_current_image_mean = fill_with_current_image_mean\n\n\nclass EmitConstraint:\n  CENTER = \'CENTER\'\n  MIN_OVERLAP = \'MIN_OVERLAP\'\n\n  def __init__(self, emit_type=CENTER, emit_overlap=0):\n    self.emit_type = emit_type\n    self.emit_overlap = emit_overlap\n\n\nclass BatchSampler:\n  # pylint: disable=too-many-arguments\n  def __init__(self, use_original_image=True, max_sample=0, max_trials=100,\n               # from sampler\n               min_scale=1., max_scale=1., min_aspect_ratio=1., max_aspect_ratio=1.,\n               # from sample_constraint\n               min_jaccard_overlap=None, max_jaccard_overlap=None, min_sample_coverage=None, max_sample_coverage=None,\n               min_object_coverage=None, max_object_coverage=None):\n    self.use_original_image = use_original_image\n    self.max_sample = max_sample\n    self.max_trials = max_trials\n    self.min_scale = min_scale\n    self.max_scale = max_scale\n    self.min_aspect_ratio = min_aspect_ratio\n    self.max_aspect_ratio = max_aspect_ratio\n\n    self.min_jaccard_overlap = min_jaccard_overlap\n    self.max_jaccard_overlap = max_jaccard_overlap\n    self.min_sample_coverage = min_sample_coverage\n    self.max_sample_coverage = max_sample_coverage\n    self.min_object_coverage = min_object_coverage\n    self.max_object_coverage = max_object_coverage\n\n\nclass TransformationParameter:\n  # pylint: disable=too-many-arguments\n  def __init__(self, scale=1., mirror=False, crop_size=(0, 0), mean_value=None,\n               resize_param=None, noise_param=None, distort_param=None, expand_param=None, emit_constraint=None):\n    self.scale = scale\n    self.mirror = mirror\n    self.crop_size = crop_size\n    self.mean_value = mean_value\n    self.resize_param = resize_param\n    self.noise_param = noise_param\n    self.distort_param = distort_param\n    self.expand_param = expand_param\n    self.emit_constraint = emit_constraint\n\n\nclass DataTransformer:\n  """"""\n  Data transformer that mimics caffe behaviour\n\n  """"""\n\n  def __init__(self, is_training=True, transform_param=None):\n    """"""\n    DataTransformer constructor\n\n    :type is_training: specified test or train phase\n    :type transform_param: Transformation parameter\n    """"""\n    self.is_training = is_training\n    self.transform_param = transform_param or TransformationParameter()\n\n  def _infer_top_shape(self, image):\n    """"""\n    Infers transformed blob shape from parameters\n\n    :type image: input image\n    """"""\n    image_h, image_w = image.shape[0:2]\n\n    if self.transform_param.resize_param:\n      original_aspect = float(image_w) / image_h\n      image_h = self.transform_param.resize_param.height\n      image_w = self.transform_param.resize_param.width\n      if self.transform_param.resize_param.resize_mode == ResizeParameter.FIT_SMALL_SIZE:\n        aspect = float(image_w) / image_h\n        if original_aspect < aspect:\n          image_h = int(image_w / original_aspect)\n        else:\n          image_w = int(original_aspect * image_h)\n\n    crop_w, crop_h = self.transform_param.crop_size\n    return (crop_h, crop_w) if crop_h * crop_w > 0 else (image_h, image_w)\n\n  # pylint: disable=too-many-locals\n  def _transform_image(self, image):\n    """"""\n    Geometrically transforms image according to parameters\n\n    :type image: input image\n    :rtype: transformed image\n    """"""\n    top_h, top_w = self._infer_top_shape(image)\n\n    scale = self.transform_param.scale\n    crop_w, crop_h = self.transform_param.crop_size\n    do_mirror = self.transform_param.mirror and random.randint(0, 1)\n\n    if self.transform_param.resize_param:\n      image = apply_resize(image, self.transform_param.resize_param)\n\n    if self.transform_param.noise_param:\n      image = apply_noise(image, self.transform_param.noise_param)\n\n    image_h, image_w = image.shape[0:2]\n\n    w_off, h_off = (0, 0)\n    if crop_h * crop_w > 0:\n      if self.is_training:\n        def exclusive_random(val):\n          return random.randint(0, val - 1)\n\n        h_off = exclusive_random(image_h - crop_h + 1)\n        w_off = exclusive_random(image_w - crop_w + 1)\n      else:\n        h_off = (image_h - crop_h) / 2\n        w_off = (image_w - crop_w) / 2\n      image = image[h_off:h_off + crop_h, w_off:w_off + crop_w]\n\n    xmin = float(w_off) / image_w\n    ymin = float(h_off) / image_h\n    xmax = float(w_off + top_w) / image_w\n    ymax = float(h_off + top_h) / image_h\n    crop_bbox = BoundingBox(xmin, ymin, xmax, ymax)\n\n    if do_mirror:\n      image = cv2.flip(image, 1)\n\n    if self.transform_param.mean_value:\n      image = image.astype(np.float32)\n      image -= self.transform_param.mean_value\n\n    if math.fabs(scale - 1) > 1e-2:\n      image = image.astype(np.float32)\n      image *= scale\n\n    return image, crop_bbox, do_mirror\n\n  def _meet_emit_constraint(self, src_bbox, bbox):\n    """"""\n    Checks if a bbox meet emit constraint w.r.t. src_bbox.\n    :type src_bbox: source box\n    :type bbox: input box\n    """"""\n    emit_constraint = self.transform_param.emit_constraint\n    if not emit_constraint:\n      return True  # satisfy if no constraint\n\n    assert emit_constraint.emit_type in (EmitConstraint.CENTER, EmitConstraint.MIN_OVERLAP)\n\n    if emit_constraint.emit_type == EmitConstraint.CENTER:\n      x_center = (bbox.xmin + bbox.xmax) / 2\n      y_center = (bbox.ymin + bbox.ymax) / 2\n      return src_bbox.xmin <= x_center <= src_bbox.xmax and src_bbox.ymin <= y_center <= src_bbox.ymax\n    if emit_constraint.emit_type == EmitConstraint.MIN_OVERLAP:\n      bbox_coverage = box_coverage(bbox, src_bbox)\n      return bbox_coverage > emit_constraint.emit_overlap\n\n    print(\'Unsupported emit_constraint.emit_type = {}\'.format(emit_constraint.emit_type))\n    return False\n\n  # pylint: disable=too-many-arguments\n  def _transform_annotation(self, original_shape, annotation, crop_bbox, do_mirror=False, do_resize=False):\n    """"""\n    Geometrically transforms annotation using parameters and crop_box used for similar image transformation\n\n    :type original_shape: original image shape before transform\n    :type annotation: annotation to transform\n    :type crop_bbox: crop box used to transform image\n    :type do_mirror: if mirror annotation using parameters\n    :type do_resize: if resize annotation using parameters\n    :rtype: transformed annotation\n    """"""\n    image_h, image_w = original_shape[0:2]\n    transformed_annotation = {}\n\n    for label, boxes in annotation.items():\n      transformed_boxes = []\n\n      for box in boxes:\n\n        if do_resize and self.transform_param.resize_param:\n          box = update_bbox_by_resize_policy(image_w, image_h, box, self.transform_param.resize_param)\n\n        if not self._meet_emit_constraint(crop_bbox, box):\n          continue\n\n        proj_bbox = crop_bbox.project_box(box)\n        if proj_bbox:\n          if do_mirror:\n            temp = proj_bbox.xmin\n            proj_bbox.xmin = 1 - proj_bbox.xmax\n            proj_bbox.xmax = 1 - temp\n\n          if do_resize and self.transform_param.resize_param:\n            proj_bbox = extrapolate_box(self.transform_param.resize_param, image_h, image_w, crop_bbox,\n                                        proj_bbox)\n\n          transformed_boxes.append(proj_bbox)\n\n      if transformed_boxes:\n        transformed_annotation[label] = transformed_boxes\n\n    return transformed_annotation\n\n  def transform(self, image, annotation):\n    """"""\n    Simultaneously transforms image and annotation according to parameters\n\n    :param image: image to transform\n    :param annotation: annotation to transform\n    :return: transformed (image, annotation) tuple\n    """"""\n    transformed_image, crop_bbox, do_mirror = self._transform_image(image)\n    transformed_annotation = self._transform_annotation(image.shape, annotation, crop_bbox, do_mirror,\n                                                        do_resize=True)\n\n    return transformed_image, transformed_annotation\n\n  def distort_image(self, image):\n    """"""\n    Distorts image photometrically according to parameters\n\n    :param image: image to transform\n    :return: transformed image\n    """"""\n    if self.transform_param.distort_param:\n      return apply_distort(image, self.transform_param.distort_param)\n    return image\n\n  def expand_image(self, image, annotation):\n    """"""\n    Expands image and annotaion according to parameters\n\n    :param image: image to expand\n    :param annotation: annotation to expand\n    :return: transformed (image, annotation) tuple\n    """"""\n    expand_param = self.transform_param.expand_param\n    if not expand_param:\n      return image, annotation\n\n    prob = random.random()\n    if prob > expand_param.prob:\n      return image, annotation\n\n    if math.fabs(expand_param.max_expand_ratio - 1.) < 1e-2:\n      return image, annotation\n\n    expand_ratio = random.uniform(1., expand_param.max_expand_ratio)\n    if expand_param.fill_with_current_image_mean:\n      mean = np.mean(image, axis=(0, 1))\n    else:\n      mean = self.transform_param.mean_value\n\n    expand_img, expand_bbox = expand_image(image, expand_ratio, mean)\n    expand_annotation = self._transform_annotation(image.shape, annotation, expand_bbox)\n\n    return expand_img, expand_annotation\n\n  def crop_image(self, image, annotation, bbox):\n    """"""\n    Cropts image and acnnotaion according to bbox\n\n    :param image: image to crop\n    :param annotation: annotaion to crop\n    :param bbox: crop box\n    :return: transformed (image, annotation) tuple\n    """"""\n    cropped_image = crop_image(image, bbox)\n\n    crop_bbox = bbox.clip_box()\n    cropped_annotation = self._transform_annotation(image.shape, annotation, crop_bbox)\n    return cropped_image, cropped_annotation\n\n\n# #############################################################################\n# ################# Transformer and Sampler ###################################\n# #############################################################################\n\n# pylint: disable=invalid-name\ndef create_default_transform_parameters(height=300, width=300):\n  resize_param = ResizeParameter(height=height, width=width)\n  emit_constraint = EmitConstraint(emit_type=\'CENTER\')\n  distort_param = DistortionParameter(brightness_prob=0.5, brightness_delta=32., contrast_prob=0.5,\n                                      contrast_lower=0.5, contrast_upper=1.5,\n                                      hue_prob=0.5, hue_delta=18, saturation_prob=0.5, saturation_lower=0.5,\n                                      saturation_upper=1.5, random_order_prob=0.5)\n\n  expand_param = ExpansionParameter(prob=0.5, max_expand_ratio=2.0)\n  train_param = TransformationParameter(mirror=True, resize_param=resize_param, distort_param=distort_param,\n                                        expand_param=expand_param, emit_constraint=emit_constraint)\n\n  val_param = TransformationParameter(resize_param=resize_param)\n  return train_param, val_param\n\n\ndef create_default_samplers():\n  samplers = [BatchSampler(max_sample=1, max_trials=1)]\n\n  for overlap in [0.1, 0.3, 0.5, 0.7, 0.9]:\n    sampler = BatchSampler(min_scale=0.3, max_scale=1.0, min_aspect_ratio=0.5, max_aspect_ratio=2.0, max_sample=1,\n                           max_trials=50)\n    sampler.min_jaccard_overlap = overlap\n    samplers.append(sampler)\n\n  sampler = BatchSampler(min_scale=0.3, max_scale=1.0, min_aspect_ratio=0.5, max_aspect_ratio=2.0, max_sample=1,\n                         max_trials=50)\n  sampler.max_jaccard_overlap = 1.0\n  samplers.append(sampler)\n  return samplers\n\n\nclass AnnotatedDataTransformer:\n  # pylint: disable=dangerous-default-value\n  def __init__(self, transform_param=None, is_training=True, train_samplers=create_default_samplers()):\n    assert not train_samplers or transform_param\n\n    self.data_transformer = None\n    self.batch_samplers = None\n\n    if is_training:\n      self.batch_samplers = train_samplers\n\n    if transform_param:\n      self.data_transformer = DataTransformer(is_training=is_training, transform_param=transform_param)\n\n  def transform(self, image, annotation):\n    if self.data_transformer:\n      image = self.data_transformer.distort_image(image)\n      image, annotation = self.data_transformer.expand_image(image, annotation)\n\n      if self.batch_samplers:\n        sampled_boxes = generate_batch_samples(annotation, self.batch_samplers)\n\n        if sampled_boxes:\n          # Randomly pick a sampled bbox and crop the expand_datum.\n          index = random.randint(0, len(sampled_boxes) - 1)\n          image, annotation = self.data_transformer.crop_image(image, annotation, sampled_boxes[index])\n\n      image, annotation = self.data_transformer.transform(image, annotation)\n\n    return image, annotation\n\n\n# #############################################################################\n# ############ Photometric image transforms ###################################\n# #############################################################################\n\ndef _random_brightness(image, brightness_prob, brightness_delta):\n  prob = random.random()\n  if prob < brightness_prob:\n    delta = random.uniform(-brightness_delta, brightness_delta)\n    delta_mat = np.full_like(image, abs(delta))\n    if delta > 0:\n      res = cv2.add(image, delta_mat)\n    else:\n      res = cv2.subtract(image, delta_mat)\n    return res\n  return image\n\n\ndef _random_contrast(image, contrast_prob, lower, upper):\n  prob = random.random()\n  if prob < contrast_prob:\n    delta = random.uniform(lower, upper)\n    res = cv2.addWeighted(image, delta, 0, 0, 0)\n    return res\n  return image\n\n\ndef _random_saturation(image, saturation_prob, lower, upper):\n  prob = random.random()\n  if prob < saturation_prob:\n    delta = random.uniform(lower, upper)\n    if math.fabs(delta - 1.) > 1e-3:\n      hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n      hsv[:, :, 1] = cv2.addWeighted(hsv[:, :, 1], delta, 0, 0, 0)\n      return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n  return image\n\n\ndef _random_hue(image, hue_prob, hue_delta):\n  prob = random.random()\n  if prob < hue_prob:\n    delta = random.uniform(-hue_delta, hue_delta)\n    if math.fabs(delta) > 0:\n      hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n      hsv[:, :, 0] = cv2.add(hsv[:, :, 0], delta)\n      return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n  return image\n\n\ndef _random_order_channels(image, random_order_prob):\n  prob = random.random()\n  if prob < random_order_prob:\n    perms = np.arange(len(image.shape))\n    np.random.shuffle(perms)\n    return image[..., perms]\n  return image\n\n\ndef apply_distort(image, distort_param):\n  prob = random.random()\n  if prob > 0.5:\n    image = _random_brightness(image, distort_param.brightness_prob, distort_param.brightness_delta)\n    image = _random_contrast(image, distort_param.contrast_prob, distort_param.contrast_lower,\n                             distort_param.contrast_upper)\n    image = _random_saturation(image, distort_param.saturation_prob, distort_param.saturation_lower,\n                               distort_param.saturation_upper)\n    image = _random_hue(image, distort_param.hue_prob, distort_param.hue_delta)\n    image = _random_order_channels(image, distort_param.random_order_prob)\n  else:\n    image = _random_brightness(image, distort_param.brightness_prob, distort_param.brightness_delta)\n    image = _random_saturation(image, distort_param.saturation_prob, distort_param.saturation_lower,\n                               distort_param.saturation_upper)\n    image = _random_hue(image, distort_param.hue_prob, distort_param.hue_delta)\n    image = _random_contrast(image, distort_param.contrast_prob, distort_param.contrast_lower,\n                             distort_param.contrast_upper)\n    image = _random_order_channels(image, distort_param.random_order_prob)\n  return image\n\n\n# pylint: disable=too-many-branches,too-many-statements\ndef apply_noise(image, noise_param):\n  if noise_param.decolorize:\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    image = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n\n  if noise_param.gauss_blur:\n    image = cv2.GaussianBlur(image, (7, 7), 1.5)\n\n  if noise_param.hist_eq:\n    if image.channels() > 1:\n      ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n      ycrcb[..., 0] = cv2.equalizeHist(ycrcb[..., 0])\n      image = cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2BGR)\n    else:\n      image = cv2.equalizeHist(image)\n\n  if noise_param.clahe:\n    clahe = cv2.createCLAHE(clipLimit=4)\n    if image.channels > 1:\n      ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n      ycrcb[..., 0] = clahe.apply(ycrcb[..., 0])\n      image = cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2BGR)\n    else:\n      image = clahe.apply(image)\n\n  if noise_param.jpeg > 0:\n    buf = cv2.imencode(\'.jpg\', image, [cv2.IMWRITE_JPEG_QUALITY, noise_param.jpeg])\n    image = cv2.imdecode(buf, cv2.IMREAD_COLOR)\n\n  if noise_param.erode:\n    elem = cv2.getStructuringElement(2, (3, 3), (1, 1))\n    image = cv2.erode(image, elem)\n\n  if noise_param.posterize:\n    def color_reduce(img, div=64):\n      div_2 = div / 2\n      table = (i / div * div + div_2 for i in range(256))\n      table = np.array(table).astype(np.uint8)\n      return cv2.LUT(img, table)\n\n    image = color_reduce(image)\n\n  if noise_param.inverse:\n    image = cv2.bitwise_not(image)\n\n  if noise_param.saltpepper_fraction > 0 and noise_param.saltpepper_values:\n    height, width = image.shape[0:2]\n    noise_pixels_num = int(noise_param.saltpepper_fraction * width * height)\n\n    def constant_noise(noise_pixels_num, val, img):\n      assert isinstance(val, list)\n      iheight, iwidth, channels = img.shape\n\n      for _ in range(noise_pixels_num):\n        i = random.randint(0, iwidth - 1)\n        j = random.randint(0, iheight - 1)\n\n        if channels == 1:\n          img[i, j] = val[0]\n        elif channels == 3:\n          img[i, j, :] = val\n\n    constant_noise(noise_pixels_num, noise_param.saltpepper_values, image)\n\n  if noise_param.convert_to_hsv:\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n  if noise_param.convert_to_lab:\n    lab_image = image.astype(np.float32) * (1.0 / 255)\n    image = cv2.cvtColor(lab_image, cv2.COLOR_BGR2Lab)\n\n  return image\n\n\n#  #############################################################################\n#  ############ Geometric image transforms #####################################\n#  #############################################################################\n\n# pylint: disable=too-many-locals\ndef expand_image(image, expand_ratio, mean_value=None):\n  height, width, chs = image.shape\n\n  # Get the bbox dimension.\n  new_h = int(height * expand_ratio)\n  new_w = int(width * expand_ratio)\n\n  h_off = int(math.floor(random.uniform(0., new_h - height)))\n  w_off = int(math.floor(random.uniform(0., new_w - width)))\n\n  xmin = -w_off / float(width)\n  ymin = -h_off / float(height)\n  xmax = (new_w - w_off) / float(width)\n  ymax = (new_h - h_off) / float(height)\n  expand_box = BoundingBox(xmin, ymin, xmax, ymax)\n\n  fill_value = mean_value if mean_value is not None else [0] * chs\n  expanded_image = np.full(shape=(new_h, new_w, chs), fill_value=fill_value, dtype=image.dtype)\n  expanded_image[h_off:h_off + height, w_off:w_off + width, :] = image\n  return expanded_image, expand_box\n\n\ndef crop_image(img, bbox):\n  img_height, img_width = img.shape[0:2]\n\n  # Get the bbox dimension.\n  clipped_bbox = bbox.clip_box()\n  scaled_bbox = clipped_bbox.scale_box(img_height, img_width)\n\n  # Crop the image using bbox.\n  w_off = int(scaled_bbox.xmin)\n  h_off = int(scaled_bbox.ymin)\n  width = int(scaled_bbox.xmax - scaled_bbox.xmin)\n  height = int(scaled_bbox.ymax - scaled_bbox.ymin)\n  return img[h_off:h_off + height, w_off:w_off + width]\n\n\n# pylint: disable=too-many-arguments\ndef aspect_keeping_resize_and_pad(image, new_width, new_height, pad_type, pad_val, interp_mode):\n  orig_aspect = float(image.cols) / image.rows\n  new_aspect = float(new_width) / new_height\n\n  if orig_aspect > new_aspect:\n    height = math.floor(float(new_width) / orig_aspect)\n    resized = cv2.resize(image, (new_width, height), interpolation=interp_mode)\n    h = resized.shape[0]\n    padding = math.floor((new_height - h) / 2.0)\n    resized = cv2.copyMakeBorder(resized, padding, new_height - h - padding, 0, 0, borderType=pad_type,\n                                 value=pad_val)\n  else:\n    width = math.floor(orig_aspect * new_height)\n    resized = cv2.resize(image, (width, new_height), 0, 0, interpolation=interp_mode)\n    w = resized.shape[1]\n    padding = math.floor((new_width - w) / 2.0)\n    resized = cv2.copyMakeBorder(resized, 0, 0, padding, new_width - w - padding, borderType=pad_type,\n                                 value=pad_val)\n  return resized\n\n\ndef aspect_keeping_resize_by_small(in_img, new_width, new_height, interp_mode):\n  orig_aspect = float(in_img.cols) / in_img.rows\n  new_aspect = float(new_width) / new_height\n\n  if orig_aspect < new_aspect:\n    height = math.floor(float(new_width) / orig_aspect)\n    resized = cv2.resize(in_img, (new_width, height), 0, 0, interpolation=interp_mode)\n  else:\n    width = math.floor(orig_aspect * new_height)\n    resized = cv2.resize(in_img, (width, new_height), 0, 0, interpolation=interp_mode)\n\n  return resized\n\n\ndef apply_resize(image, resize_param):\n  assert isinstance(resize_param.interp_mode, (list, tuple))\n\n  interp_mode = cv2.INTER_LINEAR\n  num_interp_mode = len(resize_param.interp_mode)\n  if num_interp_mode > 0:\n    probs = [1. / num_interp_mode] * num_interp_mode\n    cumulative = np.cumsum(probs)\n    val = random.uniform(0, cumulative[-1])\n    prob_num = bisect.bisect_left(cumulative, val)\n    interp_mode = resize_param.interp_mode[prob_num]\n\n  if resize_param.resize_mode == ResizeParameter.WARP:\n    return cv2.resize(image, dsize=(resize_param.width, resize_param.height), interpolation=interp_mode)\n  elif resize_param.resize_mode == ResizeParameter.FIT_LARGE_SIZE_AND_PAD:\n    return aspect_keeping_resize_and_pad(image, resize_param.width, resize_param.height, resize_param.pad_mode,\n                                         resize_param.pad_val, interp_mode)\n  elif resize_param.resize_mode == ResizeParameter.FIT_SMALL_SIZE:\n    return aspect_keeping_resize_by_small(image, resize_param.width, resize_param.height, interp_mode)\n  raise Exception()\n\n\ndef update_bbox_by_resize_policy(old_width, old_height, bbox, resize_param):\n  new_height = resize_param.height\n  new_width = resize_param.width\n  orig_aspect = float(old_width) / old_height\n  new_aspect = new_width / new_height\n\n  x_min = bbox.xmin * old_width\n  y_min = bbox.ymin * old_height\n  x_max = bbox.xmax * old_width\n  y_max = bbox.ymax * old_height\n\n  if resize_param.resize_mode == ResizeParameter.WARP:\n    x_min = max(0., x_min * new_width / old_width)\n    x_max = min(new_width, x_max * new_width / old_width)\n    y_min = max(0., y_min * new_height / old_height)\n    y_max = min(new_height, y_max * new_height / old_height)\n\n  elif resize_param.resize_mode == ResizeParameter.FIT_LARGE_SIZE_AND_PAD:\n    if orig_aspect > new_aspect:\n      padding = (new_height - new_width / orig_aspect) / 2\n      x_min = max(0., x_min * new_width / old_width)\n      x_max = min(new_width, x_max * new_width / old_width)\n      y_min = y_min * (new_height - 2 * padding) / old_height\n      y_min = padding + max(0., y_min)\n      y_max = y_max * (new_height - 2 * padding) / old_height\n      y_max = padding + min(new_height, y_max)\n    else:\n      padding = (new_width - orig_aspect * new_height) / 2\n      x_min = x_min * (new_width - 2 * padding) / old_width\n      x_min = padding + max(0., x_min)\n      x_max = x_max * (new_width - 2 * padding) / old_width\n      x_max = padding + min(new_width, x_max)\n      y_min = max(0., y_min * new_height / old_height)\n      y_max = min(new_height, y_max * new_height / old_height)\n\n  elif resize_param.resize_mode == ResizeParameter.FIT_SMALL_SIZE:\n    if orig_aspect < new_aspect:\n      new_height = new_width / orig_aspect\n    else:\n      new_width = orig_aspect * new_height\n\n    x_min = max(0., x_min * new_width / old_width)\n    x_max = min(new_width, x_max * new_width / old_width)\n    y_min = max(0., y_min * new_height / old_height)\n    y_max = min(new_height, y_max * new_height / old_height)\n\n  result = BoundingBox(difficult=bbox.difficult)\n  result.xmin = x_min / new_width\n  result.ymin = y_min / new_height\n  result.xmax = x_max / new_width\n  result.ymax = y_max / new_height\n  return result\n'"
tensorflow_toolkit/vehicle_attributes/vehicle_attributes/networks/__init__.py,0,b''
tensorflow_toolkit/vehicle_attributes/vehicle_attributes/networks/resnet_10_bn.py,17,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom nets import resnet_utils\nfrom nets.resnet_v1 import NoOpScope\nfrom nets.resnet_utils import resnet_arg_scope  # pylint: disable=unused-import\n\n# pylint: disable=too-many-locals, too-many-arguments, unused-argument, invalid-name\n@slim.add_arg_scope\ndef bottleneck(inputs,\n               depth,\n               depth_bottleneck,\n               stride,\n               rate=1,\n               outputs_collections=None,\n               scope=None,\n               use_bounded_activations=False):\n\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [3, 3], stride=stride,\n                           normalizer_fn=slim.batch_norm,\n                           normalizer_params={\'decay\': 0.99, \'zero_debias_moving_mean\': True},\n                           activation_fn=tf.nn.relu,\n                           scope=\'conv1\')\n    residual = slim.conv2d(residual, depth_bottleneck, 3, stride=1,\n                           normalizer_fn=None,\n                           activation_fn=None,\n                           rate=rate,\n                           biases_initializer=None,\n                           scope=\'conv2\')\n    if stride == 1:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(\n        inputs,\n        depth_bottleneck, [1, 1],\n        stride=stride,\n        normalizer_fn=None,\n        activation_fn=tf.nn.relu6 if use_bounded_activations else None,\n        biases_initializer=None,\n        scope=\'shortcut\')\n\n    if use_bounded_activations:\n      # Use clip_by_value to simulate bandpass activation.\n      residual = tf.clip_by_value(residual, -6.0, 6.0)\n      output = tf.nn.relu6(shortcut + residual)\n    else:\n      if tf.get_default_graph().get_name_scope().find(\'block3\') < 0:\n        output = slim.batch_norm(shortcut + residual, activation_fn=tf.nn.relu)\n      else:\n        output = shortcut + residual\n        output = slim.utils.collect_named_outputs(outputs_collections,\n                                                  sc.name,\n                                                  output)\n    return output\n\n# pylint: disable=unused-argument\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              store_non_strided_activations=False,\n              reuse=None,\n              scope=None):\n\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with (slim.arg_scope([slim.batch_norm], decay=0.99, zero_debias_moving_mean=True, is_training=is_training)\n            if is_training is not None else NoOpScope()):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = slim.batch_norm(net, decay=0.99, zero_debias_moving_mean=True, scale=True)\n          net = slim.conv2d(net, 64, 7, stride=2, padding=\'SAME\',\n                            normalizer_fn=slim.batch_norm,\n                            normalizer_params={\'decay\': 0.99, \'zero_debias_moving_mean\': True},\n                            activation_fn=tf.nn.relu,\n                            scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, padding=\'SAME\', scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n          end_points_collection)\n\n        num_types = 4\n        num_color = 3\n        types = slim.conv2d(net, num_types, [1, 1], activation_fn=tf.nn.relu,\n                            weights_initializer=tf.truncated_normal_initializer(mean=0, stddev=0.01),\n                            normalizer_fn=None, scope=\'logits_type\')\n        end_points[sc.name + \'/logits_type\'] = net\n\n        if global_pool:\n          types = tf.reduce_mean(types, [1, 2], name=\'pool5\', keepdims=True)\n          end_points[\'global_pool_types\'] = types\n\n        if spatial_squeeze:\n          types = tf.squeeze(types, [1, 2], name=\'type\')\n          end_points[sc.name + \'/type\'] = types\n        end_points[\'predictions_types\'] = types\n\n        color = slim.conv2d(net, num_color, [1, 1], activation_fn=tf.nn.relu,\n                            weights_initializer=tf.truncated_normal_initializer(mean=0, stddev=0.01),\n                            normalizer_fn=None, scope=\'logits\')\n        end_points[sc.name + \'/logits\'] = color\n\n        if global_pool:\n          color = tf.reduce_mean(color, [1, 2], name=\'pool6\', keepdims=True)\n          end_points[\'global_pool_color\'] = color\n\n        if spatial_squeeze:\n          color = tf.squeeze(color, [1, 2], name=\'color\')\n          end_points[sc.name + \'/color\'] = color\n        end_points[\'predictions_color\'] = color\n\n        return net, end_points\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n\n  return resnet_utils.Block(scope, bottleneck, [{\n    \'depth\': base_depth * 4,\n    \'depth_bottleneck\': base_depth,\n    \'stride\': 1\n  }] * (num_units - 1) + [{\n    \'depth\': base_depth * 4,\n    \'depth_bottleneck\': base_depth,\n    \'stride\': stride\n  }])\n\ndef resnet_v1_10(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 store_non_strided_activations=False,\n                 reuse=None,\n                 scope=\'resnet_v1_10\'):\n  blocks = [\n    resnet_v1_block(\'block1\', base_depth=64, num_units=1, stride=1),\n    resnet_v1_block(\'block2\', base_depth=128, num_units=1, stride=2),\n    resnet_v1_block(\'block3\', base_depth=128, num_units=1, stride=2)\n  ]\n\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\n'"
tensorflow_toolkit/vehicle_attributes/vehicle_attributes/readers/__init__.py,0,b''
tensorflow_toolkit/vehicle_attributes/vehicle_attributes/readers/vehicle_attributes_json.py,3,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\n""""""\n  Dataset reader.\n""""""\n\nimport json\nimport os\n\nimport sys\nimport threading\nfrom tqdm import tqdm\n\nimport cv2\nimport jpeg4py as jpeg\nimport numpy as np\nimport tensorflow as tf\n\ndef imread(im_path):\n  if os.path.splitext(im_path)[1].lower() in (\'.jpg\', \'.jpeg\'):\n    try:\n      img = jpeg.JPEG(im_path).decode()[..., ::-1]  # RGB -> BGR\n    # pylint: disable=broad-except\n    except Exception as ex:\n      tf.logging.warning(""Can\'t load {0} with jpeg4py (libjpeg-turbo): {1}. Will use OpenCV."".format(im_path, ex))\n      img = cv2.imread(im_path, cv2.IMREAD_COLOR)\n  else:\n    img = cv2.imread(im_path, cv2.IMREAD_COLOR)\n  return img\n\n\ndef imdecode(data):\n  try:\n    img = jpeg.JPEG(data).decode()[..., ::-1]  # RGB -> BGR\n  # pylint: disable=broad-except\n  except Exception as ex:\n    tf.logging.warning(""Can\'t decode with jpeg4py (libjpeg-turbo): {0}. Will use OpenCV."".format(ex))\n    img = cv2.imdecode(data, cv2.IMREAD_COLOR)\n  return img\n\n\nclass BarrierAttributesJson:\n  _cache = dict()\n  _lock = threading.RLock()\n\n  @staticmethod\n  def json_iterator(size):\n    def generator():\n      for i in range(size):\n        yield i\n\n    return generator\n\n  # pylint: disable=invalid-name\n  @staticmethod\n  def init_cache(filename, cache_type):\n    print(\'Load images in the cache: {}\'.format(cache_type))\n\n    with open(filename) as f:\n      items = json.load(f)\n\n    size = len(items)\n\n    id = 0\n\n    with BarrierAttributesJson._lock:\n      t = tqdm(items, total=size, unit=\'images\')\n      total_cache_usage = 0\n      for item in t:\n        images, annotations = BarrierAttributesJson._get_annotation(item)\n\n        for image, annotation in zip(images, annotations):\n          BarrierAttributesJson._cache[id] = [image, annotation]\n          id += 1\n\n          total_cache_usage += sys.getsizeof(image)\n          total_cache_usage += sys.getsizeof(annotation)\n\n        t.set_postfix({\'cache usage (GB)\': total_cache_usage / 1024 ** 3})\n\n    return id\n\n  @staticmethod\n  def _get_image_and_annotation(item):\n    image, annotation = BarrierAttributesJson._cache[item]\n    return image, annotation\n\n  @staticmethod\n  def type_annotation_to_one_hot(item):\n    vtype = np.zeros(4)\n    if item in (\'car\', \'mpv\', \'suv\', \'other\'):\n      vtype[0] = 1 #car\n    elif item == \'bus\':\n      vtype[1] = 1 #bus\n    elif item in (\'pickup\', \'truck\'):\n      vtype[2] = 1 #truck\n    elif item == \'van\':\n      vtype[3] = 1 #van\n    else:\n      print(\'Error of type recognition\')\n    return vtype\n\n  @staticmethod\n  def one_hot_annotation_to_type(t):\n    if t[0] == 1:\n      vehtype = \'car\'\n    elif t[1] == 1:\n      vehtype = \'bus\'\n    elif t[2] == 1:\n      vehtype = \'truck\'\n    elif t[3] == 1:\n      vehtype = \'van\'\n    else:\n      vehtype = \'undefined\'\n    return vehtype\n\n  # pylint: disable=len-as-condition\n  @staticmethod\n  def _get_annotation(frame):\n    images = []\n    annotations = []\n    for item in frame[\'objects\']:\n      if item[\'label\'] == \'vehicle\' and \\\n        \'color_bbox\' in item[\'attributes\'] and \\\n        len(item[\'attributes\'][\'color_bbox\']) != 0 and \\\n        \'type\' in item[\'attributes\'] and \\\n        len(item[\'attributes\'][\'type\']) != 0:\n        img = imread(frame[\'image\'])\n\n        vbbox = item[\'bbox\']\n        veh_image = img[int(vbbox[1]):int(vbbox[3]), int(vbbox[0]): int(vbbox[2])]\n        veh_image = cv2.resize(veh_image, (72, 72), interpolation=cv2.INTER_CUBIC)\n\n        #get color annotation\n        veh_color = np.zeros(3)\n        bbox = item[\'attributes\'][\'color_bbox\'][0]\n        if len(item[\'attributes\'][\'color_bbox\']) == 2:\n          if item[\'attributes\'][\'color_bbox\'][0][1] <= \\\n            item[\'attributes\'][\'color_bbox\'][1][1]:\n            bbox = item[\'attributes\'][\'color_bbox\'][0]\n          else:\n            bbox = item[\'attributes\'][\'color_bbox\'][1]\n\n        roi_color = img[int(bbox[1]):int(bbox[3]), int(bbox[0]): int(bbox[2])]\n        roi_color = cv2.cvtColor(roi_color, cv2.COLOR_BGR2LAB)\n        veh_color[0:3] = np.mean(roi_color.reshape(-1, 3), axis=0)\n\n        #get type annotation\n        veh_type = BarrierAttributesJson.type_annotation_to_one_hot(item[\'attributes\'][\'type\'])\n\n        images.append(veh_image.astype(np.float32) / 255.)\n        anno = np.concatenate((veh_type, veh_color.astype(np.float32) / 255.))\n        annotations.append(anno)\n\n    return images, annotations\n\n  @staticmethod\n  def json_decode_entry(value):\n    img, annotation = BarrierAttributesJson._get_image_and_annotation(value)\n    return img, annotation\n\n  @staticmethod\n  def create_dataset(size):\n    gen = BarrierAttributesJson.json_iterator(size)\n    return tf.data.Dataset.from_generator(gen, tf.int64, tf.TensorShape([]))\n\n  @staticmethod\n  def transform_fn(value):\n    image, annotation = BarrierAttributesJson.json_decode_entry(value)\n    result = image.astype(np.float32), annotation.astype(np.float32)\n    return result\n\n  @staticmethod\n  def get_annotations(item):\n    imgs, annotations = BarrierAttributesJson._get_annotation(item)\n    return imgs, annotations\n'"
pytorch_toolkit/action_recognition/action_recognition/models/backbone/__init__.py,0,"b'from collections import namedtuple\n\nfrom torch import nn\n\nfrom . import resnet\nfrom . import mobilenetv2\nfrom . import rmnet\n\nEncoder = namedtuple(\'Encoder\', (\'model\', \'features\', \'features_shape\'))\n\n\ndef make_encoder(name, input_size=224, input_channels=3, pretrained=None):\n    """"""Make encoder (backbone) with a given name and parameters""""""\n    \n    features_size = input_size // 32\n    num_features = 2048\n    if name.startswith(\'resnet\'):\n        model = getattr(resnet, name)(pretrained=pretrained, num_channels=input_channels)\n        features = nn.Sequential(*list(model.children())[:-2])\n        num_features = 512 if int(name[6:]) < 50 else 2048\n    elif name.startswith(\'mobilenetv2\'):\n        model = mobilenetv2.MobileNetV2(input_size=input_size, pretrained=None)\n        features = model.features\n        num_features = 1280\n    elif name.startswith(\'rmnet\'):\n        model = rmnet.RMNetClassifier(1000, pretrained=None)\n        features = nn.Sequential(*list(model.children())[:-2])\n        num_features = 512\n    elif name.startswith(\'se_res\'):\n        model = load_from_pretrainedmodels(name)(pretrained=\'imagenet\' if pretrained else None)\n        features = nn.Sequential(*list(model.children())[:-2])\n    else:\n        raise KeyError(""Unknown model name: {}"".format(name))\n\n    features_shape = (num_features, features_size, features_size)\n    return Encoder(model, features, features_shape)\n\n\ndef load_from_pretrainedmodels(model_name):\n    import pretrainedmodels\n    return getattr(pretrainedmodels, model_name)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/backbone/mobilenetv2.py,0,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU()\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU()\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1., pretrained=None):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        if pretrained:\n            checkpoint = torch.load(pretrained)\n            self.load_state_dict(checkpoint)\n        else:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = F.avg_pool2d(x, 7).view(-1, 1280)\n        # x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n'"
pytorch_toolkit/action_recognition/action_recognition/models/backbone/resnet.py,0,"b'import collections\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\nfrom ...utils import drop_last\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, num_channels=3):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef check_conv1_params(model, pretrained_weights):\n    if model.conv1.in_channels != pretrained_weights[\'conv1.weight\'].size(1):\n        # get mean over RGB channels weights\n        rgb_mean = torch.mean(pretrained_weights[\'conv1.weight\'], dim=1)\n\n        expand_ratio = model.conv1.in_channels // pretrained_weights[\'conv1.weight\'].size(1)\n        pretrained_weights[\'conv1.weight\'] = pretrained_weights[\'conv1.weight\'].repeat(1, expand_ratio, 1, 1)\n        # pretrained_weights[\'conv1.weight\'] = rgb_mean.unsqueeze(1).repeat(1, model.conv1.in_channels, 1, 1)\n\n\ndef average_conv1_weights(old_params, in_channels):\n    new_params = collections.OrderedDict()\n    layer_count = 0\n    all_key_list = old_params.keys()\n    for layer_key in drop_last(all_key_list, 2):\n        if layer_count == 0:\n            rgb_weight = old_params[layer_key]\n            rgb_weight_mean = torch.mean(rgb_weight, dim=1)\n            flow_weight = rgb_weight_mean.unsqueeze(1).repeat(1, in_channels, 1, 1)\n            if isinstance(flow_weight, torch.autograd.Variable):\n                new_params[layer_key] = flow_weight.data\n            else:\n                new_params[layer_key] = flow_weight\n            layer_count += 1\n        else:\n            new_params[layer_key] = old_params[layer_key]\n            layer_count += 1\n\n    return new_params\n\n\ndef load_pretrained_resnet(model, resnet_name=\'resnet34\', num_channels=3):\n    if num_channels == 3:\n        pretrained_weights = model_zoo.load_url(model_urls[resnet_name])\n        check_conv1_params(model, pretrained_weights)\n        model.load_state_dict(pretrained_weights)\n    else:\n        pretrained_dict = model_zoo.load_url(model_urls[resnet_name])\n        model_dict = model.state_dict()\n\n        new_pretrained_dict = average_conv1_weights(pretrained_dict, num_channels)\n\n        # 1. filter out unnecessary keys\n        new_pretrained_dict = {k: v for k, v in new_pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(new_pretrained_dict)\n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet18\', num_channels)\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet34\', num_channels)\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet50\', num_channels)\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet101\', num_channels)\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    num_channels = 3\n    if \'num_channels\' in kwargs:\n        num_channels = kwargs[\'num_channels\']\n    if pretrained:\n        model = load_pretrained_resnet(model, \'resnet152\', num_channels)\n    return model\n'"
pytorch_toolkit/action_recognition/action_recognition/models/backbone/rmnet.py,0,"b""from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ...utils import load_state\n\n\nclass RMBlock(nn.Module):\n    def __init__(self, input_planes, squeeze_planes, output_planes, downsample=False, dropout_ratio=0.1,\n                 activation=nn.ELU):\n        super(RMBlock, self).__init__()\n        self.downsample = downsample\n        self.input_planes = input_planes\n        self.output_planes = output_planes\n\n        self.squeeze_conv = nn.Conv2d(input_planes, squeeze_planes, kernel_size=1, bias=False)\n        self.squeeze_bn = nn.BatchNorm2d(squeeze_planes)\n\n        self.dw_conv = nn.Conv2d(squeeze_planes, squeeze_planes, groups=squeeze_planes, kernel_size=3, padding=1,\n                                 stride=2 if downsample else 1, bias=False)\n        self.dw_bn = nn.BatchNorm2d(squeeze_planes)\n\n        self.expand_conv = nn.Conv2d(squeeze_planes, output_planes, kernel_size=1, bias=False)\n        self.expand_bn = nn.BatchNorm2d(output_planes)\n\n        self.activation = activation(inplace=True)\n        self.dropout_ratio = dropout_ratio\n\n        if self.downsample:\n            self.skip_conv = nn.Conv2d(input_planes, output_planes, kernel_size=1, bias=False)\n            self.skip_conv_bn = nn.BatchNorm2d(output_planes)\n\n        self.init_weights()\n\n    def init_weights(self):\n        for m in self.children():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant(m.weight, 1)\n                nn.init.constant(m.bias, 0)\n\n    def forward(self, x):\n        residual = x\n        out = self.activation(self.squeeze_bn(self.squeeze_conv(x)))\n        out = self.activation(self.dw_bn(self.dw_conv(out)))\n        out = self.expand_bn(self.expand_conv(out))\n        if self.dropout_ratio > 0:\n            out = F.dropout(out, p=self.dropout_ratio, training=self.training, inplace=True)\n        if self.downsample:\n            residual = F.max_pool2d(x, kernel_size=2, stride=2, padding=0)\n            residual = self.skip_conv(residual)\n            residual = self.skip_conv_bn(residual)\n        out += residual\n        return self.activation(out)\n\n\nclass RMNetBody(nn.Module):\n    def __init__(self, block=RMBlock, blocks_per_stage=(None, 4, 8, 10, 11), trunk_width=(32, 32, 64, 128, 256),\n                 bottleneck_width=(None, 8, 16, 32, 64)):\n        super(RMNetBody, self).__init__()\n        assert len(blocks_per_stage) == len(trunk_width) == len(bottleneck_width)\n        self.dim_out = trunk_width[-1]\n\n        stages = [nn.Sequential(OrderedDict([\n            ('data_bn', nn.BatchNorm2d(3)),\n            ('conv1', nn.Conv2d(3, trunk_width[0], kernel_size=3, stride=2, padding=1, bias=False)),\n            ('bn1', nn.BatchNorm2d(trunk_width[0])),\n            ('relu1', nn.ReLU(inplace=True))\n        ])),\n        ]\n        for i, (blocks_num, w, wb) in enumerate(zip(blocks_per_stage, trunk_width, bottleneck_width)):\n            # Zeroth stage is already added.\n            if i == 0:\n                continue\n            stage = []\n            # Do not downscale input to the first stage.\n            if i > 1:\n                stage.append(block(trunk_width[i - 1], wb, w, downsample=True))\n            for _ in range(blocks_num):\n                stage.append(block(w, wb, w))\n            stages.append(nn.Sequential(*stage))\n\n        self.stages = nn.Sequential(OrderedDict([\n            ('stage_{}'.format(i), stage) for i, stage in enumerate(stages)\n        ]))\n\n        self.init_weights()\n\n    def init_weights(self):\n        m = self.stages[0][0]  # ['data_bn']\n        nn.init.constant(m.weight, 1)\n        nn.init.constant(m.bias, 0)\n        m = self.stages[0][1]  # ['conv1']\n        nn.init.kaiming_normal(m.weight, mode='fan_out')\n        m = self.stages[0][2]  # ['bn1']\n        nn.init.constant(m.weight, 1)\n        nn.init.constant(m.bias, 0)\n        # All other blocks should be initialized internally during instantiation.\n\n    def forward(self, x):\n        return self.stages(x)\n\n\nclass RMNetClassifier(nn.Module):\n    def __init__(self, num_classes, body=RMNetBody, dropout_ratio=0.1, pretrained=None):\n        super(RMNetClassifier, self).__init__()\n        self.dropout_ratio = dropout_ratio\n        self.backbone = body()\n        self.extra_conv_bn_relu = nn.Sequential(nn.Conv2d(256, 512, 3, stride=2, padding=1, bias=False),\n                                                nn.BatchNorm2d(512), nn.ELU())\n        self.extra_conv_bn_relu_2 = nn.Sequential(nn.Conv2d(512, 1024, 3, stride=2, padding=1, bias=False),\n                                                  nn.BatchNorm2d(1024), nn.ReLU())\n        self.fc = nn.Conv2d(1024, num_classes, 1, stride=1, padding=0)\n\n        if pretrained:\n            checkpoint = torch.load(pretrained)\n            load_state(self, checkpoint)\n            # self.load_state_dict(checkpoint)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.extra_conv_bn_relu(x)\n        x = self.extra_conv_bn_relu_2(x)\n        x = F.avg_pool2d(x, (4, 4))\n        x = self.fc(x)\n        x = x.view(-1, x.size(1))\n        return x\n"""
pytorch_toolkit/action_recognition/action_recognition/models/modules/__init__.py,0,"b""from .functional import unsquash_dim, squash_dims, reduce_tensor\nfrom .modules import Identity, AttentionLSTM, Attention, SEBlock, StateInitFC, StateInitZero\nfrom . import self_attention, bnlstm, tcn\n\n__all__ = ['unsquash_dim', 'squash_dims', 'reduce_tensor', 'self_attention', 'bnlstm', 'tcn', 'Identity',\n           'AttentionLSTM', 'Attention', 'SEBlock', 'StateInitFC', 'StateInitZero']\n"""
pytorch_toolkit/action_recognition/action_recognition/models/modules/bnlstm.py,0,"b'""""""Implementation of batch-normalized LSTM.""""""\nimport torch\nfrom torch import nn\nfrom torch.nn import functional, init\n\n\nclass SeparatedBatchNorm1d(nn.Module):\n\n    """"""\n    A batch normalization module which keeps its running mean\n    and variance separately per timestep.\n    """"""\n\n    def __init__(self, num_features, max_length, eps=1e-5, momentum=0.1,\n                 affine=True):\n        """"""\n        Most parts are copied from\n        torch.nn.modules.batchnorm._BatchNorm.\n        """"""\n\n        super(SeparatedBatchNorm1d, self).__init__()\n        self.num_features = num_features\n        self.max_length = max_length\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        if self.affine:\n            self.weight = nn.Parameter(torch.FloatTensor(num_features))\n            self.bias = nn.Parameter(torch.FloatTensor(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        for i in range(max_length):\n            self.register_buffer(\n                \'running_mean_{}\'.format(i), torch.zeros(num_features))\n            self.register_buffer(\n                \'running_var_{}\'.format(i), torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for i in range(self.max_length):\n            running_mean_i = getattr(self, \'running_mean_{}\'.format(i))\n            running_var_i = getattr(self, \'running_var_{}\'.format(i))\n            running_mean_i.zero_()\n            running_var_i.fill_(1)\n        if self.affine:\n            self.weight.data.uniform_()\n            self.bias.data.zero_()\n\n    def _check_input_dim(self, input_):\n        if input_.size(1) != self.running_mean_0.nelement():\n            raise ValueError(\'got {}-feature tensor, expected {}\'\n                             .format(input_.size(1), self.num_features))\n\n    def forward(self, input_, time):\n        self._check_input_dim(input_)\n        if time >= self.max_length:\n            time = self.max_length - 1\n        running_mean = getattr(self, \'running_mean_{}\'.format(time))\n        running_var = getattr(self, \'running_var_{}\'.format(time))\n        return functional.batch_norm(\n            input=input_, running_mean=running_mean, running_var=running_var,\n            weight=self.weight, bias=self.bias, training=self.training,\n            momentum=self.momentum, eps=self.eps)\n\n    def __repr__(self):\n        return (\'{name}({num_features}, eps={eps}, momentum={momentum},\'\n                \' max_length={max_length}, affine={affine})\'\n                .format(name=self.__class__.__name__, **self.__dict__))\n\n\nclass BNLSTMCell(nn.Module):\n\n    """"""A BN-LSTM cell.""""""\n\n    def __init__(self, input_size, hidden_size, max_length, use_bias=True):\n\n        super(BNLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.max_length = max_length\n        self.use_bias = use_bias\n        self.weight_ih = nn.Parameter(\n            torch.FloatTensor(input_size, 4 * hidden_size))\n        self.weight_hh = nn.Parameter(\n            torch.FloatTensor(hidden_size, 4 * hidden_size))\n        if use_bias:\n            self.bias = nn.Parameter(torch.FloatTensor(4 * hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n        # BN parameters\n        self.bn_ih = SeparatedBatchNorm1d(\n            num_features=4 * hidden_size, max_length=max_length)\n        self.bn_hh = SeparatedBatchNorm1d(\n            num_features=4 * hidden_size, max_length=max_length)\n        self.bn_c = SeparatedBatchNorm1d(\n            num_features=hidden_size, max_length=max_length)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters following the way proposed in the paper.\n        """"""\n\n        # The input-to-hidden weight matrix is initialized orthogonally.\n        init.orthogonal(self.weight_ih.data)\n        # The hidden-to-hidden weight matrix is initialized as an identity\n        # matrix.\n        weight_hh_data = torch.eye(self.hidden_size)\n        weight_hh_data = weight_hh_data.repeat(1, 4)\n        self.weight_hh.data.set_(weight_hh_data)\n        # The bias is just set to zero vectors.\n        init.constant(self.bias.data, val=0)\n        # Initialization of BN parameters.\n        self.bn_ih.reset_parameters()\n        self.bn_hh.reset_parameters()\n        self.bn_c.reset_parameters()\n        self.bn_ih.bias.data.fill_(0)\n        self.bn_hh.bias.data.fill_(0)\n        self.bn_ih.weight.data.fill_(0.1)\n        self.bn_hh.weight.data.fill_(0.1)\n        self.bn_c.weight.data.fill_(0.1)\n\n    def forward(self, input_, hx, time):\n        """"""\n        Args:\n            input_: A (batch, input_size) tensor containing input\n                features.\n            hx: A tuple (h_0, c_0), which contains the initial hidden\n                and cell state, where the size of both states is\n                (batch, hidden_size).\n            time: The current timestep value, which is used to\n                get appropriate running statistics.\n\n        Returns:\n            h_1, c_1: Tensors containing the next hidden and cell state.\n        """"""\n\n        h_0, c_0 = hx\n        batch_size = h_0.size(0)\n        bias_batch = (self.bias.unsqueeze(0)\n                      .expand(batch_size, *self.bias.size()))\n        wh = torch.mm(h_0, self.weight_hh)\n        wi = torch.mm(input_, self.weight_ih)\n        bn_wh = self.bn_hh(wh, time=time)\n        bn_wi = self.bn_ih(wi, time=time)\n        f, i, o, g = torch.split(bn_wh + bn_wi + bias_batch, self.hidden_size, dim=1)\n        c_1 = torch.sigmoid(f)*c_0 + torch.sigmoid(i)*torch.tanh(g)\n        h_1 = torch.sigmoid(o) * torch.tanh(self.bn_c(c_1, time=time))\n        return h_1, c_1\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/functional.py,0,"b'import torch\n\n\ndef squash_dims(tensor, dims):\n    """"""\n    Squashes dimension, given in dims into one, which equals to product of given.\n\n    Args:\n        tensor (Tensor): input tensor\n        dims: dimensions over which tensor should be squashed\n\n    """"""\n    assert len(dims) >= 2, ""Expected two or more dims to be squashed""\n\n    size = tensor.size()\n\n    squashed_dim = size[dims[0]]\n    for i in range(1, len(dims)):\n        assert dims[i] == dims[i - 1] + 1, ""Squashed dims should be consecutive""\n        squashed_dim *= size[dims[i]]\n\n    result_dims = size[:dims[0]] + (squashed_dim,) + size[dims[-1] + 1:]\n    return tensor.contiguous().view(*result_dims)\n\n\ndef unsquash_dim(tensor, dim, res_dim):\n    """"""\n    Unsquashes dimension, given in dim into separate dimensions given is res_dim\n    Args:\n        tensor (Tensor): input tensor\n        dim (int): dimension that should be unsquashed\n        res_dim (tuple): list of dimensions, that given dim should be unfolded to\n\n    """"""\n    size = tensor.size()\n    result_dim = size[:dim] + res_dim + size[dim + 1:]\n    return tensor.view(*result_dim)\n\n\ndef reduce_tensor(tensor, dims, reduction=torch.sum):\n    """"""Performs reduction over multiple dimensions at once""""""\n    permute_idx = [i for i, d in enumerate(tensor.size()) if i not in dims]\n    result_dims = [d for i, d in enumerate(tensor.size()) if i not in dims]\n    tensor = tensor.permute(*(permute_idx + list(dims))).contiguous()\n    return reduction(tensor.view(*result_dims, -1), -1)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/modules.py,0,"b'import torch\nfrom torch import nn\n\nfrom .functional import squash_dims\n\n\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass Identity(nn.Module):\n    def forward(self, input_):\n        return input_\n\n\nclass StateInitFC(nn.Module):\n    def __init__(self, init_size, hidden_size, activation=Identity):\n        super().__init__()\n\n        self.linear_h = nn.Linear(init_size, hidden_size)\n        self.linear_c = nn.Linear(init_size, hidden_size)\n        self.activation_h = activation()\n        self.activation_c = activation()\n\n        self.linear_h.weight.data.normal_(0.0, 0.02)\n        self.linear_h.bias.data.fill_(0)\n        self.linear_c.weight.data.normal_(0.0, 0.02)\n        self.linear_c.bias.data.fill_(0)\n\n    def forward(self, input_):\n        h0 = self.activation_h(self.linear_h(input_))\n        c0 = self.activation_c(self.linear_c(input_))\n        return h0, c0\n\n\nclass StateInitZero(nn.Module):\n    def __init__(self, hidden_size, num_layers=1, batch_first=False):\n        super(StateInitZero, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n\n    def forward(self, input: torch.Tensor):\n        h0 = input.new_zeros((self.num_layers, input.size(0 if self.batch_first else 1), self.hidden_size))\n        c0 = input.new_zeros((self.num_layers, input.size(0 if self.batch_first else 1), self.hidden_size))\n        return h0, c0\n\n\nclass Attention(nn.Module):\n    def __init__(self, q_size, k_size, v_size):\n        super().__init__()\n\n        self.softmax = nn.Softmax(dim=1)\n        self.linear_q = nn.Linear(q_size, v_size)\n\n        self.linear_q.weight.data.normal_(0.0, 0.02)\n        self.linear_q.bias.data.fill_(0)\n\n    def forward(self, q, k, v):\n        attn_scores = self.linear_q(q)\n        attn_map = self.softmax(attn_scores.view(-1, attn_scores.size(-1)))\n\n        return (v * attn_map).sum(-1), attn_map\n\n\nclass AttentionLSTM(nn.Module):\n    """"""LSTM with spatial attention """"""\n\n    def __init__(self, input_features, hidden_size, attention_size, batch_first=False, **kwargs):\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.lstm = nn.LSTM(input_features, hidden_size, batch_first=False, **kwargs)\n        self.attention = Attention(hidden_size, None, attention_size)\n\n    def forward(self, x, hidden):\n        hx, cx = hidden\n        if self.batch_first:\n            x = x.transpose(0, 1)\n\n        outputs = []\n        for i in range(x.size(0)):\n            # transpose in order to correctly broadcast while multiplying v\n            # squash dims in order to pull into vector (C x N x L)\n            v = squash_dims(x[i].transpose(0, 1), (2, 3))\n            feature, attention = self.attention(hx[-1], v, v)\n            feature = feature.transpose(0, 1)  # back to (N x C)\n\n            # unsqueeze to emulate sequence size = 1\n            _, (hx, cx) = self.lstm(feature.unsqueeze(0), (hx, cx))\n            outputs.append(hx)\n        ys = torch.cat(outputs, 0)\n\n        if self.batch_first:\n            ys = ys.transpose(0, 1)\n\n        return ys, (hx, cx)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/self_attention.py,0,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\nfrom .functional import unsquash_dim\nfrom .modules import Identity\n\n\nclass Linear(nn.Module):\n    """""" Simple Linear layer with xavier init """"""\n\n    def __init__(self, d_in, d_out, bias=True):\n        super(Linear, self).__init__()\n        self.linear = nn.Linear(d_in, d_out, bias=bias)\n        init.xavier_normal(self.linear.weight)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nclass Bottle(nn.Module):\n    """""" Perform the reshape routine before and after an operation """"""\n\n    def forward(self, input):\n        if len(input.size()) <= 2:\n            return super(Bottle, self).forward(input)\n        size = input.size()[:2]\n        out = super(Bottle, self).forward(input.view(size[0] * size[1], -1))\n        return out.view(size[0], size[1], -1)\n\n\nclass BottleLinear(Bottle, Linear):\n    """""" Perform the reshape routine before and after a linear projection """"""\n    pass\n\n\nclass BottleSoftmax(Bottle, nn.Softmax):\n    """""" Perform the reshape routine before and after a softmax operation""""""\n    pass\n\n\nclass LayerNormalization(nn.Module):\n    """""" Layer normalization module """"""\n\n    def __init__(self, d_hid, eps=1e-3):\n        super(LayerNormalization, self).__init__()\n\n        self.eps = eps\n        self.a_2 = nn.Parameter(torch.ones(d_hid), requires_grad=True)\n        self.b_2 = nn.Parameter(torch.zeros(d_hid), requires_grad=True)\n\n    def forward(self, z):\n        if z.size(1) == 1:\n            return z\n\n        mu = torch.mean(z, keepdim=True, dim=-1)\n        sigma = torch.std(z, keepdim=True, dim=-1)\n        ln_out = (z - mu.expand_as(z)) / (sigma.expand_as(z) + self.eps)\n        ln_out = ln_out * self.a_2.expand_as(ln_out) + self.b_2.expand_as(ln_out)\n\n        return ln_out\n\n\nclass ScaledDotProductAttention(nn.Module):\n    """""" Scaled Dot-Product Attention """"""\n\n    def __init__(self, d_model, attn_dropout=0.1):\n        super(ScaledDotProductAttention, self).__init__()\n        self.temper = np.power(d_model, 0.5)\n        self.dropout = nn.Dropout(attn_dropout)\n        self.softmax = BottleSoftmax()\n\n    def forward(self, q, k, v):\n        # q.size(): [nh*b x t x d_k]\n        # flops: b*nh * 2 * t * d_k * d_k = 8.4m\n        attn = torch.bmm(q, k.transpose(1, 2)) / self.temper\n\n        # flops: < 20k\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        # flops: 2*b*nh*t*t*d_k = 1.05m\n        output = torch.bmm(attn, v)\n\n        # flops: ~10m\n        return output, attn\n\n\nclass MultiHeadAttention(nn.Module):\n    """""" Multi-Head Attention module """"""\n\n    def __init__(self, n_head, input_size, output_size, d_k, d_v, dropout=0.1, use_proj=False, layer_norm=True):\n        """"""\n        Args:\n            n_head: Number of attention heads\n            input_size: Input feature size\n            output_size: Output feature size\n            d_k: Feature size for each head\n            d_v: Feature size for each head\n            dropout: Dropout rate after projection\n            use_proj: add additional projection to output feature space\n        """"""\n        super(MultiHeadAttention, self).__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n        self.use_proj = use_proj\n\n        self.w_qs = nn.Parameter(torch.FloatTensor(n_head, input_size, d_k))\n        self.w_ks = nn.Parameter(torch.FloatTensor(n_head, input_size, d_k))\n        self.w_vs = nn.Parameter(torch.FloatTensor(n_head, input_size, d_v))\n\n        self.attention = ScaledDotProductAttention(input_size)\n        self.layer_norm = LayerNormalization(input_size) if layer_norm else Identity()\n\n        if use_proj:\n            self.proj = Linear(n_head * d_v, output_size)\n\n        self.dropout = nn.Dropout(dropout)\n\n        init.xavier_normal_(self.w_qs)\n        init.xavier_normal_(self.w_ks)\n        init.xavier_normal_(self.w_vs)\n\n    def forward(self, q, k, v):\n        d_k, d_v = self.d_k, self.d_v\n        n_head = self.n_head\n\n        residual = q\n\n        mb_size, len_q, d_model = q.size()\n        mb_size, len_k, d_model = k.size()\n        mb_size, len_v, d_model = v.size()\n\n        # treat as a (n_head) size batch\n        q_s = q.repeat(n_head, 1, 1).view(n_head, -1, d_model)  # n_head x (mb_size*len_q) x d_model\n        k_s = k.repeat(n_head, 1, 1).view(n_head, -1, d_model)  # n_head x (mb_size*len_k) x d_model\n        v_s = v.repeat(n_head, 1, 1).view(n_head, -1, d_model)  # n_head x (mb_size*len_v) x d_model\n\n        # treat the result as a (n_head * mb_size) size batch\n        # flops: 3 * 2 * t * nh * d_k * c = 12.6m\n        q_s = torch.bmm(q_s, self.w_qs).view(-1, len_q, d_k)  # (n_head*mb_size) x len_q x d_k\n        k_s = torch.bmm(k_s, self.w_ks).view(-1, len_k, d_k)  # (n_head*mb_size) x len_k x d_k\n        v_s = torch.bmm(v_s, self.w_vs).view(-1, len_v, d_v)  # (n_head*mb_size) x len_v x d_v\n\n        # perform attention, result size = (n_head * mb_size) x len_q x d_v\n        # flops: 0.63m\n        outputs, attns = self.attention(q_s, k_s, v_s)\n\n        # back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)\n        split_size = mb_size.item() if isinstance(mb_size, torch.Tensor) else mb_size\n        outputs = unsquash_dim(outputs, 0, (-1, split_size))\n        outputs = outputs.permute(1, 2, 0, 3).contiguous().view(split_size, len_q, -1)\n        # outputs = torch.cat(outputs.split(split_size, dim=0), dim=-1)\n\n        if self.use_proj:\n            # project back to residual size\n            # flops: 2 * t * d_inner ^ 2 = 4.2m\n            outputs = self.proj(outputs)\n        outputs = self.dropout(outputs)\n\n        return self.layer_norm(outputs + residual), attns\n\n\nclass PositionwiseFeedForward(nn.Module):\n    """""" A two-feed-forward-layer module """"""\n\n    def __init__(self, d_hid, d_inner_hid, dropout=0.1, layer_norm=True):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1)  # position-wise\n        self.w_2 = nn.Conv1d(d_inner_hid, d_hid, 1)  # position-wise\n        self.layer_norm = LayerNormalization(d_hid) if layer_norm else Identity()\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        output = self.relu(self.w_1(x.transpose(1, 2)))\n        output = self.w_2(output).transpose(2, 1)\n        output = self.dropout(output)\n        return self.layer_norm(output + residual)\n\n\nclass DecoderBlock(nn.Module):\n    """""" Compose with two layers """"""\n\n    def __init__(self, input_size, hidden_size, inner_hidden_size, n_head, d_k, d_v, dropout=0.1, layer_norm=True):\n        super(DecoderBlock, self).__init__()\n        # flops: 17.5m\n        self.slf_attn = MultiHeadAttention(n_head, input_size, hidden_size, d_k, d_v, dropout=dropout,\n                                           layer_norm=layer_norm)\n        # flops: 0.528m\n        self.pos_ffn = PositionwiseFeedForward(hidden_size, inner_hidden_size, dropout=dropout, layer_norm=layer_norm)\n\n    def forward(self, enc_input):\n        enc_output, enc_slf_attn = self.slf_attn(\n            enc_input, enc_input, enc_input\n        )\n        enc_output = self.pos_ffn(enc_output)\n        return enc_output, enc_slf_attn\n\n\nclass PositionEncoding(nn.Module):\n    def __init__(self, n_positions, hidden_size):\n        super().__init__()\n        self.enc = nn.Embedding(n_positions, hidden_size, padding_idx=0)\n\n        position_enc = np.array([\n            [pos / np.power(10000, 2 * (j // 2) / hidden_size) for j in range(hidden_size)]\n            if pos != 0 else np.zeros(hidden_size) for pos in range(n_positions)])\n\n        position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n        position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n        self.enc.weight = torch.nn.Parameter(torch.from_numpy(position_enc).to(self.enc.weight.device, torch.float))\n\n    def forward(self, x):\n        indices = torch.arange(0, x.size(1)).to(self.enc.weight.device, torch.long)\n        encodings = self.enc(indices)\n        x += encodings\n        return x\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/tcn.py,0,"b'from torch import nn\nfrom torch.nn.utils import weight_norm\n\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        layers = []\n        num_levels = len(num_channels)\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i - 1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size - 1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.network(x)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/backbones/__init__.py,0,b''
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/backbones/backbone.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom ..base import FeatureExtractor\n\n\ndef freeze_params(module, freeze=True):\n    for p in module.parameters():\n        p.requires_grad = not freeze\n\n\ndef freeze_params_recursive(module, freeze=True, types=(nn.Module,)):\n    for x in module.modules():\n        if isinstance(x, types):\n            freeze_params(x, freeze)\n\n\ndef freeze_mode(module, train_mode=False):\n    module.train(train_mode)\n\n\ndef freeze_mode_recursive(module, train_mode=False, types=(nn.Module,)):\n    for x in module.modules():\n        if isinstance(x, types):\n            freeze_mode(x, train_mode)\n\n\ndef freeze_params_and_mode(module, train_mode=False, freeze=True):\n    module.train(train_mode)\n    freeze_params(module, freeze)\n\n\ndef freeze_params_and_mode_recursive(module, train_mode=False, freeze=True, types=(nn.Module,)):\n    for x in module.modules():\n        if isinstance(x, types):\n            freeze_params_and_mode(x, train_mode, freeze)\n\n\nclass Backbone(FeatureExtractor):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n        self.dims_in = (3,)\n        self.scales_in = (1,)\n        self.stages = nn.Sequential()\n        self._all_dims_out = tuple(1 for _ in range(len(self.stages)))\n        self._all_scales_out = tuple(2 ** (i + 1) for i in range(len(self.stages)))\n        self.output_stages = list(range(len(self.stages)))\n        self.dims_out = list(self._all_dims_out[i] for i in self.output_stages)\n        self.scales_out = list(self._all_scales_out[i] for i in self.output_stages)\n\n        self.stages_with_frozen_params = ()\n        self.stages_with_frozen_mode = ()\n        self.batch_norm_types = (nn.BatchNorm2d, )\n\n    def set_output_stages(self, output_stages):\n        self.output_stages = output_stages\n        self.dims_out = list(self._all_dims_out[i] for i in self.output_stages)\n        self.scales_out = list(self._all_scales_out[i] for i in self.output_stages)\n\n    def freeze_stages_params(self, stage_indices):\n        self.stages_with_frozen_params = stage_indices\n        # Unfreeze whole model first.\n        freeze_params_recursive(self, False)\n        # Freeze only relevant stages.\n        for stage_idx in stage_indices:\n            freeze_params(self.stages[stage_idx], True)\n\n    def freeze_stages_bns(self, stage_indices):\n        self.stages_with_frozen_mode = stage_indices\n        for stage_idx in range(len(self.stages)):\n            freeze_params_recursive(self.stages[stage_idx], stage_idx in stage_indices, self.batch_norm_types)\n        self.train(self.training)\n\n    def train(self, train_mode=True):\n        super().train(train_mode)\n        self.training = train_mode\n        for stage_idx, stage in enumerate(self.stages):\n            freeze_mode_recursive(stage, train_mode if stage_idx not in self.stages_with_frozen_mode else False,\n                                  types=self.batch_norm_types)\n\n    def forward(self, x):\n        src_width = x.shape[-1]\n        outputs_per_stage = []\n        for stage_idx, stage in enumerate(self.stages):\n            x = stage(x)\n            outputs_per_stage.append(x)\n        outputs = list(outputs_per_stage[i] for i in self.output_stages)\n        for output, scale in zip(outputs, self.scales_out):\n            assert output.shape[-1] * scale == src_width, \'{} * {} != {}\'.format(x.shape[-1], scale, src_width)\n        return outputs\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/backbones/resnet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import OrderedDict\n\nfrom torch import nn\n\nfrom .backbone import Backbone\nfrom ..group_norm import GroupNorm\nfrom ...utils.weights import get_group_gn\n\n\nclass ResBlock(nn.Module):\n    """""" Bottleneck Residual Block """"""\n\n    def __init__(self, inplanes, outplanes, innerplanes, stride=1, dilation=1, group=1,\n                 stride_1x1=True, track_running_batch_stat=True):\n        super().__init__()\n        # In original resnet, stride=2 is on 1x1.\n        # In fb.torch resnet, stride=2 is on 3x3.\n        (str1x1, str3x3) = (stride, 1) if stride_1x1 else (1, stride)\n\n        self.conv1 = nn.Conv2d(inplanes, innerplanes, kernel_size=1, stride=str1x1, bias=False)\n        self.bn1 = nn.BatchNorm2d(innerplanes, track_running_stats=track_running_batch_stat)\n\n        self.conv2 = nn.Conv2d(innerplanes, innerplanes, kernel_size=3, stride=str3x3, bias=False,\n                               padding=1 * dilation, dilation=dilation, groups=group)\n        self.bn2 = nn.BatchNorm2d(innerplanes, track_running_stats=track_running_batch_stat)\n\n        self.conv3 = nn.Conv2d(innerplanes, outplanes, kernel_size=1, stride=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes, track_running_stats=track_running_batch_stat)\n\n        self.downsample = None\n        if stride != 1 or inplanes != outplanes:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(outplanes, track_running_stats=track_running_batch_stat)\n            )\n        self.relu = nn.ReLU(inplace=True)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for submodule in self.modules():\n            if isinstance(submodule, nn.Conv2d):\n                nn.init.kaiming_uniform_(submodule.weight)\n                if submodule.bias is not None:\n                    nn.init.constant_(submodule.bias, 0)\n            elif isinstance(submodule, nn.BatchNorm2d):\n                if submodule.bias is not None:\n                    nn.init.constant_(submodule.bias, 0)\n                if submodule.weight is not None:\n                    nn.init.constant_(submodule.weight, 1)\n                if submodule.running_mean is not None:\n                    nn.init.constant_(submodule.running_mean, 0)\n                if submodule.running_var is not None:\n                    nn.init.constant_(submodule.running_var, 1)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResBlockWithGN(ResBlock):\n    def __init__(self, inplanes, outplanes, innerplanes, stride=1, dilation=1, group=1,\n                 stride_1x1=False, track_running_batch_stat=True):\n        super().__init__(inplanes, outplanes, innerplanes, stride, dilation, group,\n                         stride_1x1, track_running_batch_stat)\n        self.bn1 = GroupNorm(get_group_gn(innerplanes), innerplanes, eps=1e-5, affine=True)\n        self.bn2 = GroupNorm(get_group_gn(innerplanes), innerplanes, eps=1e-5, affine=True)\n        self.bn3 = GroupNorm(get_group_gn(outplanes), outplanes, eps=1e-5, affine=True)\n        if self.downsample is not None:\n            self.downsample[1] = GroupNorm(get_group_gn(outplanes), outplanes, eps=1e-5, affine=True)\n\n\nclass ResBlockWithFusedBN(nn.Module):\n    """""" Bottleneck Residual Block """"""\n\n    def __init__(self, inplanes, outplanes, innerplanes, stride=1, dilation=1, group=1, stride_1x1=True):\n        super().__init__()\n        # In original resnet, stride=2 is on 1x1.\n        # In fb.torch resnet, stride=2 is on 3x3.\n        (str1x1, str3x3) = (stride, 1) if stride_1x1 else (1, stride)\n\n        self.conv1 = nn.Conv2d(inplanes, innerplanes, kernel_size=1, stride=str1x1, bias=True)\n        self.conv2 = nn.Conv2d(innerplanes, innerplanes, kernel_size=3, stride=str3x3, bias=True,\n                               padding=1 * dilation, dilation=dilation, groups=group)\n        self.conv3 = nn.Conv2d(innerplanes, outplanes, kernel_size=1, stride=1, bias=True)\n\n        self.downsample = None\n        if stride != 1 or inplanes != outplanes:\n            self.downsample = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=stride, bias=True)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for submodule in self.modules():\n            if isinstance(submodule, nn.Conv2d):\n                nn.init.kaiming_uniform_(submodule.weight)\n                if submodule.bias is not None:\n                    nn.init.constant_(submodule.bias, 0)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetBody(Backbone):\n    def __init__(self, block_counts, res_block=ResBlock, num_groups=1, width_per_group=64, res5_dilation=1):\n        super().__init__()\n        self.block_counts = block_counts\n        self.num_groups = num_groups\n        self.width_per_group = width_per_group\n        self.res5_dilation = res5_dilation\n\n        self.convX = len(block_counts) + 1\n        self.num_layers = (sum(block_counts) + 3 * (self.convX == 4)) * 3 + 2\n        stage_dims_out = []\n\n        dim_in = 64\n        stages = [nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv2d(3, dim_in, 7, stride=2, padding=3, bias=False)),\n            (\'bn1\', nn.BatchNorm2d(dim_in)),\n            (\'relu\', nn.ReLU(inplace=True)),\n            (\'maxpool\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n            ])),\n        ]\n\n        stage_dims_out.append(dim_in)\n        dim_bottleneck = num_groups * width_per_group\n        stage, dim_in = self.add_stage(res_block, dim_in, 256, dim_bottleneck, block_counts[0],\n                                       dilation=1, stride_init=1, groups_num=num_groups)\n        stages.append(stage)\n        stage_dims_out.append(dim_in)\n\n        stage, dim_in = self.add_stage(res_block, dim_in, 512, dim_bottleneck * 2, block_counts[1],\n                                       dilation=1, stride_init=2, groups_num=num_groups)\n        stages.append(stage)\n        stage_dims_out.append(dim_in)\n\n        stage, dim_in = self.add_stage(res_block, dim_in, 1024, dim_bottleneck * 4, block_counts[2],\n                                       dilation=1, stride_init=2, groups_num=num_groups)\n        stages.append(stage)\n        stage_dims_out.append(dim_in)\n\n        if len(block_counts) == 4:\n            stride_init = 2 if res5_dilation == 1 else 1\n            stage, dim_in = self.add_stage(res_block, dim_in, 2048, dim_bottleneck * 8, block_counts[3],\n                                           dilation=res5_dilation, stride_init=stride_init, groups_num=num_groups)\n            stages.append(stage)\n            stage_dims_out.append(dim_in)\n\n        self.stages = nn.Sequential(OrderedDict([\n            (\'stage_{}\'.format(i), stage) for i, stage in enumerate(stages)\n        ]))\n\n        self.dims_in = (3,)\n        self.scales_in = (1,)\n        self._all_dims_out = tuple(stage_dims_out)\n        self._all_scales_out = (4, ) + tuple(2 ** (i + 1) for i in range(1, len(stages)))\n        self.set_output_stages(range(len(stages)))\n\n    @staticmethod\n    def add_stage(res_block, inplanes, outplanes, innerplanes, nblocks, stride_init=2, dilation=1, groups_num=1):\n        res_blocks = []\n        stride = stride_init\n        for _ in range(nblocks):\n            res_blocks.append(res_block(inplanes, outplanes, innerplanes, stride, dilation, groups_num))\n            inplanes = outplanes\n            stride = 1\n\n        return nn.Sequential(*res_blocks), outplanes\n\n\nclass ResNet(ResNetBody):\n    def __init__(self, base_arch=\'ResNet50\', fused_batch_norms=False, group_norm=False, stride_1x1=True,\n                 num_groups=1, width_per_group=64, res5_dilation=1):\n        if group_norm:\n            res_block_type = ResBlockWithGN\n        else:\n            res_block_type = ResBlockWithFusedBN if fused_batch_norms else ResBlock\n\n        def res_block(*args, **kwargs):\n            return res_block_type(*args, **kwargs, stride_1x1=stride_1x1)\n\n        if base_arch == \'ResNet50\':\n            block_counts = (3, 4, 6, 3)\n        elif base_arch == \'ResNet101\':\n            block_counts = (3, 4, 23, 3)\n        elif base_arch == \'ResNet152\':\n            block_counts = (3, 8, 36, 3)\n        else:\n            raise ValueError(\'Invalid ResNet architecture ""{}"".\'.format(base_arch))\n\n        super().__init__(block_counts=block_counts, res_block=res_block, num_groups=num_groups,\n                         width_per_group=width_per_group, res5_dilation=res5_dilation)\n        if group_norm:\n            dim_in = self.dims_out[0]\n            self.stages.stage_0.bn1 = GroupNorm(get_group_gn(dim_in), dim_in, eps=1e-5, affine=True)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/model_zoo/__init__.py,0,b''
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/model_zoo/fpn_mask_rcnn_base.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom collections import defaultdict, Mapping\n\nimport torch\nfrom torch import nn\n\nfrom ..detection_output import DetectionOutput\nfrom ..fpn import FPN\nfrom ..group_norm import GroupNorm\nfrom ..losses import rpn_loss_cls, rpn_loss_reg, detection_loss_cls, detection_loss_reg, accuracy, mask_loss\nfrom ..prior_box import PriorBox\nfrom ..proposal import generate_proposals\nfrom ..proposal_gt_matcher import ProposalGTMatcher\nfrom ..roi_feature_extractor import extract_roi_features, topk_rois\nfrom ..rpn import RPN\nfrom ..rpn_gt_matcher import RPNGTMatcher\nfrom ...utils.profile import timed, print_timing_stats, Timer\nfrom ...utils.weights import xavier_fill, msra_fill, get_group_gn\n\n\nclass BboxHead(nn.Module):\n    """"""Add a ReLU MLP with two hidden layers.""""""\n\n    def __init__(self, dim_in, dim_out, resolution_in, cls_num, cls_agnostic_bbox_regression=False, fc_as_conv=False):\n        super().__init__()\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        self.resolution_in = resolution_in\n        self.cls_num = cls_num\n        self.cls_agnostic_bbox_regression = cls_agnostic_bbox_regression\n\n        box_out_dims = 4 * (1 if cls_agnostic_bbox_regression else cls_num)\n        if fc_as_conv:\n            self.fc1 = nn.Conv2d(dim_in, dim_out, resolution_in)\n            self.fc2 = nn.Conv2d(dim_out, dim_out, 1)\n            self.cls_score = nn.Conv2d(dim_out, cls_num, 1)\n            self.bbox_pred = nn.Conv2d(dim_out, box_out_dims, 1)\n        else:\n            self.fc1 = nn.Linear(dim_in * resolution_in * resolution_in, dim_out)\n            self.fc2 = nn.Linear(dim_out, dim_out)\n            self.cls_score = nn.Linear(dim_out, cls_num)\n            self.bbox_pred = nn.Linear(dim_out, box_out_dims)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        xavier_fill(self.fc1.weight)\n        nn.init.constant_(self.fc1.bias, 0)\n        xavier_fill(self.fc2.weight)\n        nn.init.constant_(self.fc2.bias, 0)\n\n        nn.init.normal_(self.cls_score.weight, std=0.01)\n        nn.init.constant_(self.cls_score.bias, 0)\n        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n        nn.init.constant_(self.bbox_pred.bias, 0)\n\n    def get_score_and_prediction(self, x):\n        batch_size = int(x.size(0))\n        cls_score = self.cls_score(x).view(batch_size, -1)\n        if not self.training:\n            cls_score = nn.functional.softmax(cls_score, dim=1)\n        bbox_pred = self.bbox_pred(x).view(batch_size, -1)\n        return cls_score, bbox_pred\n\n    def forward(self, x):\n        if isinstance(self.fc1, nn.Linear):\n            batch_size = int(x.size(0))\n            x = x.view(batch_size, -1)\n        x = nn.functional.relu(self.fc1(x), inplace=True)\n        x = nn.functional.relu(self.fc2(x), inplace=True)\n        return self.get_score_and_prediction(x)\n\n\nclass BboxHeadWithGN(BboxHead):\n    """"""Replace linear layers by 2d convolutions with GroupNorm""""""\n\n    def __init__(self, dim_in, dim_out, resolution_in, cls_num, cls_agnostic_bbox_regression=False,\n                 conv_num=4, dim_internal=256):\n        super().__init__(dim_in, dim_out, resolution_in, cls_num, cls_agnostic_bbox_regression)\n        module_list = []\n        for i in range(conv_num):\n            module_list.extend([\n                nn.Conv2d(dim_in, dim_internal, kernel_size=3, stride=1, padding=1, bias=False),\n                GroupNorm(get_group_gn(dim_internal), dim_internal, eps=1e-5),\n                nn.ReLU(inplace=True)\n            ])\n        self.convs = nn.Sequential(*module_list)\n\n        self._init_weights()\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                msra_fill(m.weight)\n        self.fc2 = None\n\n    def forward(self, x):\n        batch_size = int(x.size(0))\n        x = self.convs(x)\n        x = nn.functional.relu(self.fc1(x.view(batch_size, -1)), inplace=True)\n        return self.get_score_and_prediction(x)\n\n\nclass MaskHead(nn.Module):\n    """"""X * (conv 3x3) -> convT 2x2.""""""\n\n    def __init__(self, dim_in, num_convs, num_cls, dim_internal=256, dilation=2, group_norm=False):\n        super().__init__()\n        self.dim_in = dim_in\n        self.num_convs = num_convs\n        self.dim_out = dim_internal\n        self.group_norm = group_norm\n\n        module_list = []\n        for i in range(num_convs):\n            if self.group_norm:\n                module_list.extend([\n                    nn.Conv2d(dim_in, dim_internal, kernel_size=3, stride=1, padding=dilation, dilation=dilation),\n                    GroupNorm(get_group_gn(dim_internal), dim_internal, eps=1e-5),\n                    nn.ReLU(inplace=True)\n                ])\n            else:\n                module_list.extend([\n                    nn.Conv2d(dim_in, dim_internal, kernel_size=3, stride=1, padding=dilation, dilation=dilation),\n                    nn.ReLU(inplace=True)\n                ])\n            dim_in = dim_internal\n        self.conv_fcn = nn.Sequential(*module_list)\n\n        self.upconv = nn.ConvTranspose2d(dim_internal, dim_internal, kernel_size=2, stride=2, padding=0)\n        self.segm = nn.Conv2d(dim_internal, num_cls, 1, 1, 0)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n                msra_fill(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.conv_fcn(x)\n        x = self.upconv(x)\n        x = nn.functional.relu(x, inplace=True)\n        x = self.segm(x)\n        if not self.training:\n            x = torch.sigmoid(x)\n        return x\n\n\nclass FPNMaskRCNN(nn.Module):\n    detection_roi_featuremap_resolution = 7\n    segmentation_roi_featuremap_resolution = 14\n    input_size_divisor = 32\n\n    def __init__(self, cls_num, backbone, force_max_output_size=False,\n                 group_norm=False, fc_detection_head=True, **kwargs):\n        super().__init__()\n        self.cls_num = cls_num\n        self.force_max_output_size = force_max_output_size\n\n        self.backbone = backbone\n\n        self.fpn = self.add_fpn(self.backbone.dims_out, self.backbone.scales_out, group_norm=group_norm)\n\n        self.prior_boxes, priors_num = self.add_priors_generator()\n        self.rpn = self.add_rpn(priors_num, self.fpn.dims_out)\n\n        self.detection_head, self.detection_output = self.add_detection_head(self.fpn.dims_out, self.cls_num,\n                                                                             group_norm=group_norm,\n                                                                             fc_detection_head=fc_detection_head)\n        self.mask_head = self.add_segmentation_head(self.fpn.dims_out, self.cls_num, group_norm=group_norm)\n\n        # For training only.\n        self.rpn_batch_size = 256\n        self.rpn_gt_matcher = RPNGTMatcher(straddle_threshold=0, positive_threshold=0.7,\n                                           negative_threshold=0.3, positive_fraction=0.5,\n                                           batch_size=self.rpn_batch_size)\n        self.mask_resolution = 2 * self.segmentation_roi_featuremap_resolution\n        self.proposal_gt_matcher = ProposalGTMatcher(positive_threshold=0.5, negative_threshold=0.5,\n                                                     positive_fraction=0.25, batch_size=512,\n                                                     target_mask_size=(self.mask_resolution, self.mask_resolution))\n\n        self._timers = defaultdict(Timer)\n\n    @property\n    def feature_pyramid_scales(self):\n        return self.fpn.scales_out\n\n    @property\n    def pre_nms_rois_count(self):\n        return 2000 if self.training else 1000\n\n    @property\n    def post_nms_rois_count(self):\n        return 2000 if self.training else 1000\n\n    @staticmethod\n    def add_priors_generator():\n        prior_boxes = nn.ModuleList()\n        widths = [[49.0, 33.0, 25.0], [89.0, 65.0, 49.0], [185.0, 129.0, 89.0], [361.0, 257.0, 185.0],\n                  [729.0, 513.0, 361.0]]\n        heights = [[25.0, 33.0, 49.0], [49.0, 65.0, 89.0], [97.0, 129.0, 147.0], [177.0, 257.0, 369.0],\n                   [369.0, 513.0, 721.0]]\n        for ws, hs in zip(widths, heights):\n            prior_boxes.append(PriorBox(widths=ws, heights=hs, flatten=True))\n        priors_per_level_num = list([priors.priors_num() for priors in prior_boxes])\n        assert priors_per_level_num[1:] == priors_per_level_num[:-1]\n        priors_num = priors_per_level_num[0]\n        return prior_boxes, priors_num\n\n    @staticmethod\n    def add_fpn(dims_in, scales_in, group_norm=False, **kwargs):\n        return FPN(dims_in, scales_in, 256, 256, group_norm=group_norm)\n\n    @staticmethod\n    def add_rpn(priors_num, features_dim_in):\n        # RPN is shared between FPN levels.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        rpn = RPN(features_dim_in[0], 256, priors_num, \'sigmoid\')\n        return rpn\n\n    @staticmethod\n    def add_detection_head(features_dim_in, cls_num, group_norm=False, fc_detection_head=True):\n        # ROI-wise detection part.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        dim_out = 1024\n        if group_norm:\n            detection_head = BboxHeadWithGN(features_dim_in[0], dim_out, 7, cls_num,\n                                            cls_agnostic_bbox_regression=False, conv_num=4, dim_internal=256)\n        else:\n            detection_head = BboxHead(features_dim_in[0], dim_out, 7, cls_num,\n                                      cls_agnostic_bbox_regression=False, fc_as_conv=not fc_detection_head)\n        detection_output = DetectionOutput(cls_num, nms_threshold=0.5, score_threshold=0.05)\n        return detection_head, detection_output\n\n    @staticmethod\n    def add_segmentation_head(features_dim_in, cls_num, group_norm):\n        # ROI-wise segmentation part.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        mask_head = MaskHead(features_dim_in[0], 4, cls_num, 256, 1, group_norm=group_norm)\n        return mask_head\n\n    @timed\n    def forward(self, im_data, im_info, gt_boxes=None, gt_labels=None, gt_is_ignored=None, gt_masks=None,\n                batch_idx=None, **kwargs):\n        if self.training:\n            # In case of training return a dict rather than a list.\n            return_values = {}\n        else:\n            return_values = []\n\n        im_data, im_info = self.preprocess_data(im_data, im_info, self.input_size_divisor)\n        batch_size = im_data.shape[0]\n\n        backbone_features = self.forward_backbone(im_data)\n        backbone_features = self.forward_fpn(backbone_features)\n        with torch.no_grad():\n            priors_pyramid = self.generate_priors(backbone_features, im_data)\n        rpn_cls_targets, rpn_reg_targets = self.get_rpn_targets(priors_pyramid, gt_boxes, gt_labels,\n                                                                gt_is_ignored, im_info)\n        rois, rois_probs, rpn_metrics, rpn_loss = self.forward_rpn(priors_pyramid, backbone_features, im_info,\n                                                                   rpn_cls_targets, rpn_reg_targets, batch_size)\n        return_values = self.update_return_values(return_values, rpn_metrics)\n        rois, rois_probs = self.process_proposals(rois, rois_probs, batch_size)\n        rois, roi_cls_targets, roi_reg_targets, roi_mask_targets = self.get_targets(rois, gt_boxes, gt_labels, gt_masks)\n\n        # Last pyramid level is used only for RPN part of the net, so, remove it now.\n        backbone_features = backbone_features[:-1]\n        instance_heads_output, instance_heads_loss = self.forward_instance_heads(im_info, backbone_features, rois,\n                                                                                 roi_cls_targets, roi_reg_targets,\n                                                                                 roi_mask_targets,\n                                                                                 batch_idx=batch_idx)\n        return_values = self.update_return_values(return_values, instance_heads_output)\n\n        if self.training:\n            total_loss = rpn_loss + instance_heads_loss\n            return_values[\'losses/TOTAL\'] = total_loss.detach()\n            return return_values, total_loss\n        else:\n            # Dummy operation on outputs for Model Optimizer.\n            for ret_val in return_values:\n                if ret_val is not None:\n                    ret_val += 0\n            return return_values\n\n    @timed\n    def forward_backbone(self, im_data):\n        return self.backbone(im_data)\n\n    @timed\n    def forward_fpn(self, feature_pyramid):\n        return self.fpn(feature_pyramid)\n\n    @staticmethod\n    def update_return_values(return_values, extra_values):\n        if isinstance(return_values, list):\n            return_values.extend(extra_values)\n        else:\n            for k, v in extra_values.items():\n                if isinstance(v, Mapping):\n                    return_values[k] = FPNMaskRCNN.update_return_values(return_values.get(k, {}), v)\n                else:\n                    return_values[k] = v\n        return return_values\n\n    @staticmethod\n    def pad_image_data(image_blobs, size_divisor):\n        target_height = max([entry.shape[-2] for entry in image_blobs])\n        target_width = max([entry.shape[-1] for entry in image_blobs])\n        target_height = (target_height + size_divisor - 1) // size_divisor * size_divisor\n        target_width = (target_width + size_divisor - 1) // size_divisor * size_divisor\n        for i, image_blob in enumerate(image_blobs):\n            image_blobs[i] = torch.nn.functional.pad(image_blob, (0, target_width - image_blob.shape[-1],\n                                                                  0, target_height - image_blob.shape[-2]))\n        return torch.stack(image_blobs, dim=0)\n\n    @timed\n    def preprocess_data(self, im_data, im_info, size_divisor=32):\n        with torch.no_grad():\n            if isinstance(im_data, list):\n                im_data = self.pad_image_data(im_data, size_divisor)\n            if isinstance(im_info, list):\n                im_info = torch.stack(im_info, dim=0)\n        if im_data.device != im_info.device:\n            im_info = im_info.to(im_data.device)\n        return im_data, im_info\n\n    @timed\n    def generate_priors(self, feature_pyramid, im_data):\n        priors_pyramid = []\n        for prior_boxes_generator, feature_map, scale in \\\n                zip(self.prior_boxes, feature_pyramid, self.feature_pyramid_scales):\n            priors_pyramid.append(prior_boxes_generator(feature_map, im_data,\n                                                        stride_x=scale, stride_y=scale))\n        return priors_pyramid\n\n    @timed\n    def get_rpn_targets(self, priors_pyramid, gt_boxes, gt_labels, gt_is_ignored, im_info):\n        rpn_cls_targets, rpn_reg_targets = None, None\n        if self.training:\n            with torch.no_grad():\n                all_priors = torch.cat(priors_pyramid, dim=0)\n                rpn_cls_targets, rpn_reg_targets = self.rpn_gt_matcher(all_priors, gt_boxes, gt_labels,\n                                                                       gt_is_ignored, im_info)\n                priors_per_level_num = list([len(p) for p in priors_pyramid])\n                rpn_cls_targets = list(zip(*[cls.split(priors_per_level_num) for cls in rpn_cls_targets]))\n                rpn_reg_targets = list(zip(*[reg.split(priors_per_level_num) for reg in rpn_reg_targets]))\n        return rpn_cls_targets, rpn_reg_targets\n\n    @timed\n    def forward_rpn(self, priors_pyramid, features_pyramid, im_info,\n                    rpn_cls_targets=None, rpn_reg_targets=None, batch_size=1):\n        rois = []\n        rois_probs = []\n        pyramid_level_cls_losses = []\n        pyramid_level_reg_losses = []\n        metrics = {}\n        for pyramid_level, (priors, feature_map) in enumerate(zip(priors_pyramid, features_pyramid)):\n            rpn_output = self.rpn(feature_map)\n            if self.training:\n                rpn_box_deltas, rpn_cls_scores = rpn_output[:2]\n                loss_rpn_cls, accuracy_rpn_cls, precision_rpn_cls, recall_rpn_cls = \\\n                    rpn_loss_cls(rpn_cls_targets[pyramid_level], rpn_cls_scores, reduction=\'sum\')\n                loss_rpn_cls /= self.rpn_batch_size * batch_size\n                loss_rpn_reg = rpn_loss_reg(rpn_cls_targets[pyramid_level], rpn_reg_targets[pyramid_level],\n                                            rpn_box_deltas, reduction=\'sum\')\n                loss_rpn_reg /= self.rpn_batch_size * batch_size\n\n                level = \'/{}\'.format(pyramid_level)\n                metrics.update({\n                    \'losses/rpn/cls\' + level: loss_rpn_cls.detach().unsqueeze(0),\n                    \'losses/rpn/reg\' + level: loss_rpn_reg.detach().unsqueeze(0),\n                    \'metrics/rpn/cls_accuracy\' + level: accuracy_rpn_cls.detach().unsqueeze(0),\n                    \'metrics/rpn/cls_precision\' + level: precision_rpn_cls.detach().unsqueeze(0),\n                    \'metrics/rpn/cls_recall\' + level: recall_rpn_cls.detach().unsqueeze(0)\n                })\n                pyramid_level_cls_losses.append(loss_rpn_cls)\n                pyramid_level_reg_losses.append(loss_rpn_reg)\n\n            with torch.no_grad():\n                proposal_rois, proposal_rois_probs = generate_proposals(priors, rpn_output, im_info,\n                                                                        pre_nms_count=self.pre_nms_rois_count,\n                                                                        post_nms_count=self.post_nms_rois_count,\n                                                                        force_max_output_size=self.force_max_output_size)\n                rois.append(proposal_rois)\n                rois_probs.append(proposal_rois_probs)\n\n        loss = None\n        if pyramid_level_cls_losses:\n            cls_loss = sum(pyramid_level_cls_losses)\n            reg_loss = sum(pyramid_level_reg_losses)\n            loss = cls_loss + reg_loss\n            metrics[\'losses/rpn/cls\'] = cls_loss.detach().unsqueeze(0)\n            metrics[\'losses/rpn/reg\'] = reg_loss.detach().unsqueeze(0)\n            metrics[\'losses/rpn\'] = loss.detach().unsqueeze(0)\n\n        return rois, rois_probs, metrics, loss\n\n    @timed\n    def process_proposals(self, rois, rois_probs, batch_size):\n        # Transpose from feature pyramid level -> batch element representation to\n        # batch element -> feature pyramid level one.\n        # And retain only post_nms_count top scoring proposals among all pyramid levels for one image.\n        rois = list(zip(*rois))\n        rois_probs = list(zip(*rois_probs))\n        assert len(rois) == batch_size, \'{} != {}\'.format(len(rois), batch_size)\n        assert len(rois_probs) == batch_size, \'{} != {}\'.format(len(rois_probs), batch_size)\n        for i, (image_rois, image_rois_probs) in enumerate(zip(rois, rois_probs)):\n            image_rois = torch.cat(image_rois, dim=0)\n            image_rois_probs = torch.cat(image_rois_probs, dim=0)\n            rois[i] = topk_rois(image_rois, image_rois_probs, max_rois=self.post_nms_rois_count,\n                                use_stub=not self.training)\n            rois_probs[i] = image_rois_probs\n        return rois, rois_probs\n\n    @timed\n    def get_targets(self, rois, gt_boxes, gt_labels, gt_masks):\n        roi_cls_targets, roi_reg_targets, roi_mask_targets = None, None, None\n        if self.training:\n            with torch.no_grad():\n                rois, roi_cls_targets, roi_reg_targets, roi_mask_targets = \\\n                    self.proposal_gt_matcher(rois, gt_boxes, gt_labels, gt_masks)\n                # Sanity checks.\n                assert len(rois) == len(roi_cls_targets)\n                assert len(rois) == len(roi_reg_targets)\n                for im_rois, im_roi_cls_targets, im_roi_reg_targets in zip(rois, roi_cls_targets, roi_reg_targets):\n                    assert im_rois.shape[0] == im_roi_cls_targets.shape[0]\n                    assert im_rois.shape[0] == im_roi_reg_targets.shape[0]\n        return rois, roi_cls_targets, roi_reg_targets, roi_mask_targets\n\n    @timed\n    def forward_detection_head(self, roi_features):\n        return self.detection_head(roi_features)\n\n    @timed\n    def forward_mask_head(self, roi_features):\n        return self.mask_head(roi_features)\n\n    @timed\n    def forward_detection_output(self, rois, raw_bbox_pred, raw_cls_score, im_info, batch_idx):\n        if len(rois) == 1:\n            rois = rois[0]\n        self.detection_output.force_max_output_size = self.force_max_output_size\n        boxes, classes, scores, batch_ids = self.detection_output(rois, raw_bbox_pred, raw_cls_score, im_info, batch_idx)\n        return boxes, classes, scores, batch_ids\n\n    def dummy_detections(self, device):\n        boxes = torch.zeros((1, 4), device=device, dtype=torch.float32)\n        classes = torch.zeros(1, device=device, dtype=torch.long)\n        scores = torch.zeros(1, device=device, dtype=torch.float32)\n        batch_ids = torch.zeros(1, device=device, dtype=torch.long)\n        raw_mask_output = torch.zeros((1, self.cls_num, self.mask_resolution, self.mask_resolution),\n                                      device=device, dtype=torch.float32)\n        return boxes, classes, scores, batch_ids, raw_mask_output\n\n    @timed\n    def forward_instance_heads(self, im_info, feature_pyramid, rois,\n                               roi_cls_targets=None, roi_reg_targets=None,\n                               roi_mask_targets=None, batch_size=1, batch_idx=None):\n        detection_roi_features, rois = self.extract_roi_features(rois, feature_pyramid,\n                                                                 output_size=self.detection_roi_featuremap_resolution)\n        raw_cls_score, raw_bbox_pred = self.forward_detection_head(detection_roi_features)\n\n        if self.training:\n            return_values = {}\n            loss_cls = detection_loss_cls(raw_cls_score, roi_cls_targets)\n            all_targets = torch.cat(roi_cls_targets)\n            valid_mask = all_targets >= 0\n            accuracy_cls = accuracy(torch.argmax(raw_cls_score[valid_mask], dim=1), all_targets[valid_mask])\n            loss_reg = detection_loss_reg(raw_bbox_pred, roi_cls_targets, roi_reg_targets,\n                                          self.detection_head.cls_agnostic_bbox_regression)\n            return_values[\'losses/detection/cls\'] = loss_cls.detach().unsqueeze(0)\n            return_values[\'losses/detection/reg\'] = loss_reg.detach().unsqueeze(0)\n            return_values[\'metrics/detection/cls_accuracy\'] = accuracy_cls.detach().unsqueeze(0)\n\n            with torch.no_grad():\n                positive_indices = (all_targets > 0).nonzero().view(-1)\n\n            if len(positive_indices) > 0:\n                with torch.no_grad():\n                    positive_mask_targets = torch.cat(roi_mask_targets, dim=0).index_select(0, positive_indices)\n                    positive_cls_targets = all_targets.index_select(0, positive_indices)\n                mask_roi_features, rois = self.extract_roi_features(rois, feature_pyramid,\n                                                                    output_size=self.segmentation_roi_featuremap_resolution)\n                mask_roi_features = mask_roi_features[positive_indices]\n                raw_mask_output = self.forward_mask_head(mask_roi_features)\n                assert raw_mask_output.shape[0] == positive_indices.shape[0]\n\n                assert raw_mask_output.shape[0] == positive_cls_targets.shape[0]\n                assert raw_mask_output.shape[0] == positive_mask_targets.shape[0]\n                loss_mask = mask_loss(raw_mask_output, positive_cls_targets, positive_mask_targets)\n                return_values[\'losses/mask\'] = loss_mask.detach().unsqueeze(0)\n                loss = (loss_cls + loss_reg + loss_mask).unsqueeze(0)\n            else:\n                return_values[\'losses/mask\'] = torch.zeros_like(loss_cls).detach().unsqueeze(0)\n                loss = (loss_cls + loss_reg).unsqueeze(0)\n        else:\n            return_values = []\n            with torch.no_grad():\n                boxes, classes, scores, batch_ids = self.forward_detection_output(rois, raw_bbox_pred,\n                                                                                  raw_cls_score, im_info,\n                                                                                  batch_idx)\n\n            if sum(len(im_boxes) for im_boxes in boxes) == 0:\n                return_values.extend(self.dummy_detections(im_info.device))\n            else:\n                if batch_idx is not None and len(batch_idx) > 1:\n                    rois = list([boxes.index_select(0, (batch_ids == image_id).nonzero().reshape(-1))\n                                 for image_id in batch_idx])\n                else:\n                    rois = [boxes, ]\n                # Extract features for every detected box preserving the order.\n                mask_roi_features, _ = self.extract_roi_features(rois, feature_pyramid,\n                                                                 output_size=self.segmentation_roi_featuremap_resolution)\n                raw_mask_output = self.forward_mask_head(mask_roi_features)\n                return_values.extend((boxes, classes, scores, batch_ids, raw_mask_output))\n            loss = None\n        return return_values, loss\n\n    @timed\n    def extract_roi_features(self, rois, feature_pyramid, output_size=7, preserve_order=True):\n        roi_features, rois = extract_roi_features(rois, feature_pyramid,\n                                                  pyramid_scales=tuple(self.feature_pyramid_scales),\n                                                  output_size=output_size,\n                                                  sampling_ratio=2,\n                                                  distribute_rois_between_levels=True,\n                                                  preserve_rois_order=preserve_order,\n                                                  use_stub=not self.training)\n        roi_features = torch.cat([x for x in roi_features if x is not None], dim=0)\n        return roi_features, rois\n\n    def print_timing(self):\n        print_timing_stats(self._timers)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/model_zoo/instance_segmentation_security_0050.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom .fpn_mask_rcnn_base import (MaskHead, BboxHead, PriorBox, DetectionOutput,\n                                 RPN, FPN, ProposalGTMatcher)\nfrom .resnet_fpn_mask_rcnn import ResNet50FPNMaskRCNN\nfrom ..panet import BottomUpPathAugmentation\nfrom ...utils.weights import xavier_fill\n\n\nclass BboxHead3FC(BboxHead):\n    def __init__(self, dim_in, dim_out, resolution_in, cls_num,\n                 cls_agnostic_bbox_regression=False, fc_as_conv=False, **kwargs):\n        super().__init__(dim_in, dim_out, resolution_in, cls_num, cls_agnostic_bbox_regression, fc_as_conv, **kwargs)\n        if fc_as_conv:\n            self.fc3 = nn.Conv2d(dim_out, dim_out, 1)\n        else:\n            self.fc3 = nn.Linear(dim_out, dim_out)\n\n        xavier_fill(self.fc3.weight)\n        nn.init.constant_(self.fc3.bias, 0)\n\n    def forward(self, x):\n        if isinstance(self.fc1, nn.Linear):\n            batch_size = int(x.size(0))\n            x = x.view(batch_size, -1)\n        x = nn.functional.relu(self.fc1(x), inplace=True)\n        x = nn.functional.relu(self.fc2(x), inplace=True)\n        x = nn.functional.relu(self.fc3(x), inplace=True)\n        return self.get_score_and_prediction(x)\n\n\nclass MaskHeadBN(MaskHead):\n    def __init__(self, dim_in, num_convs, num_cls, dim_internal=256, dilation=2, **kwargs):\n        super().__init__(dim_in, num_convs, num_cls, dim_internal, dilation, **kwargs)\n        self.dim_in = dim_in\n        self.num_convs = num_convs\n        self.dim_out = dim_internal\n\n        del self.conv_fcn\n        module_list = []\n        for i in range(num_convs):\n            module_list.extend([\n                nn.Conv2d(dim_in, dim_internal, kernel_size=3, stride=1, padding=dilation, dilation=dilation),\n                nn.BatchNorm2d(dim_internal),\n                nn.ReLU(inplace=True)\n            ])\n            dim_in = dim_internal\n        self.conv_fcn = nn.Sequential(*module_list)\n\n        self.upconv = nn.ConvTranspose2d(dim_internal, dim_internal, kernel_size=2, stride=2, padding=0)\n        self.segm = nn.Conv2d(dim_internal, num_cls, 1, 1, 0)\n\n        self._init_weights()\n\n\nclass BottomUpPathAugmentationBN(BottomUpPathAugmentation):\n\n    @staticmethod\n    def _conv2d_block(dim_in, dim_out, kernel, stride, padding, bias):\n        return nn.Sequential(\n            nn.Conv2d(dim_in, dim_out, kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n            nn.BatchNorm2d(dim_out),\n            nn.ReLU(inplace=True)\n        )\n\n\nclass RPNLite(RPN):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        del self.conv\n        self.conv = nn.Conv2d(self.dim_in, self.dim_internal, 1, 1, 0)\n\n        self.init_weights()\n\n\nclass InstanceSegmentationSecurity0050(ResNet50FPNMaskRCNN):\n    segmentation_roi_featuremap_resolution = 7\n\n    def __init__(self, cls_num, **kwargs):\n        super().__init__(cls_num, **kwargs)\n\n        self.bupa = BottomUpPathAugmentationBN(output_levels=5, dims_in=self.fpn.dims_out,\n                                               scales_in=self.fpn.scales_out, dim_out=128, group_norm=False)\n\n        r = self.segmentation_roi_featuremap_resolution\n        self.proposal_gt_matcher = ProposalGTMatcher(positive_threshold=0.5, negative_threshold=0.5,\n                                                     positive_fraction=0.25, batch_size=256,\n                                                     target_mask_size=(2 * r, 2 * r))\n\n    @staticmethod\n    def add_fpn(dims_in, scales_in, **kwargs):\n        return FPN(dims_in, scales_in, 128, 128, group_norm=False)\n\n    @staticmethod\n    def add_priors_generator():\n        prior_boxes = nn.ModuleList()\n        widths = [[18.00461443977467 + 1, 28.899706148116053 + 1, 73.42073611573349 + 1],\n                  [54.733722536651804 + 1, 136.73193415104578 + 1, 68.17445780221439 + 1],\n                  [99.90277461137683 + 1, 155.060481177927 + 1, 254.14803078894226 + 1],\n                  [143.22154707415015 + 1, 243.36151152306752 + 1, 416.3044920416812 + 1],\n                  [279.57615011598125 + 1, 410.15991631559905 + 1, 450.9415967854453 + 1]]\n\n        heights = [[22.358642713410788 + 1, 61.90766647668873 + 1, 46.83872274215618 + 1],\n                   [113.48241169265263 + 1, 89.23900340394223 + 1, 183.0552472694527 + 1],\n                   [280.45049255862403 + 1, 185.32713748322118 + 1, 123.3327609158835 + 1],\n                   [390.62539933985994 + 1, 270.83141023619 + 1, 163.83324079051005 + 1],\n                   [414.2871418830516 + 1, 308.4935963584353 + 1, 445.5373059473707 + 1]]\n\n        scale_factor = 1.0\n        for ws, hs in zip(widths, heights):\n            if scale_factor != 1.0:\n                for i in range(len(ws)):\n                    ws[i] *= scale_factor\n                for i in range(len(hs)):\n                    hs[i] *= scale_factor\n            prior_boxes.append(PriorBox(widths=ws, heights=hs, flatten=True, use_cache=True))\n        priors_per_level_num = list([priors.priors_num() for priors in prior_boxes])\n        assert priors_per_level_num[1:] == priors_per_level_num[:-1]\n        priors_num = priors_per_level_num[0]\n        return prior_boxes, priors_num\n\n    @staticmethod\n    def add_segmentation_head(features_dim_in, cls_num, **kwargs):\n        # ROI-wise segmentation part.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        mask_head = MaskHeadBN(features_dim_in[0], 6, cls_num, 128, 1)\n        return mask_head\n\n    @staticmethod\n    def add_rpn(priors_num, features_dim_in):\n        # RPN is shared between FPN levels.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        rpn = RPNLite(features_dim_in[0], 128, priors_num, \'sigmoid\')\n        return rpn\n\n    @staticmethod\n    def add_detection_head(features_dim_in, cls_num, fc_detection_head=True, **kwargs):\n        # ROI-wise detection part.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        dim_out = 512\n        detection_head = BboxHead3FC(features_dim_in[0], dim_out, 7, cls_num,\n                                     cls_agnostic_bbox_regression=False,\n                                     fc_as_conv=not fc_detection_head)\n        detection_output = DetectionOutput(cls_num, nms_threshold=0.5, score_threshold=0.05, post_nms_count=100)\n        return detection_head, detection_output\n\n    @property\n    def pre_nms_rois_count(self):\n        return 2000 if self.training else 100\n\n    @property\n    def post_nms_rois_count(self):\n        return 2000 if self.training else 100\n\n    def forward_fpn(self, feature_pyramid):\n        x = self.fpn(feature_pyramid)\n        return self.bupa(x)\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/model_zoo/panet_mask_rcnn_base.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\nfrom .fpn_mask_rcnn_base import FPNMaskRCNN\nfrom .. import panet\nfrom ..detection_output import DetectionOutput\nfrom ..losses import detection_loss_cls, detection_loss_reg, accuracy, mask_loss\nfrom ..roi_feature_extractor import extract_roi_features\nfrom ...utils.profile import timed\n\n\nclass PANetMaskRCNN(FPNMaskRCNN):\n    def __init__(self, cls_num, backbone, force_max_output_size=False, afp_levels_num=4, heavier_head=False,\n                 group_norm=False, fully_connected_fusion=True, deformable_conv=False, **kwargs):\n        super().__init__(cls_num, backbone, force_max_output_size, group_norm, **kwargs)\n\n        if deformable_conv:\n            self.bupa = panet.BottomUpPathAugmentationWithDeformConv(output_levels=5, dims_in=self.fpn.dims_out,\n                                                                     scales_in=self.fpn.scales_out,\n                                                                     dim_out=256, group_norm=True)\n        else:\n            self.bupa = panet.BottomUpPathAugmentation(output_levels=5, dims_in=self.fpn.dims_out,\n                                                       scales_in=self.fpn.scales_out, dim_out=256, group_norm=True)\n\n        self.detection_head, self.detection_output = self.add_detection_head(self.bupa.dims_out, self.cls_num,\n                                                                             afp_levels_num, heavier_head, group_norm)\n        self.mask_head = self.add_segmentation_head(self.bupa.dims_out, self.cls_num, afp_levels_num,\n                                                    fully_connected_fusion, group_norm)\n\n    @property\n    def feature_pyramid_scales(self):\n        return self.bupa.scales_out\n\n    @staticmethod\n    def add_detection_head(features_dim_in, cls_num, afp_levels_num=4, heavier_head=False, group_norm=False, **kwargs):\n        # ROI-wise detection part.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        detection_head = panet.BboxHead(features_dim_in[0], 1024, PANetMaskRCNN.detection_roi_featuremap_resolution,\n                                        cls_num,\n                                        cls_agnostic_bbox_regression=False,\n                                        afp_levels_num=afp_levels_num,\n                                        heavier_head=heavier_head, group_norm=group_norm)\n        detection_output = DetectionOutput(cls_num, nms_threshold=0.5, score_threshold=0.05)\n        return detection_head, detection_output\n\n    @staticmethod\n    def add_segmentation_head(features_dim_in, cls_num, afp_levels_num=4,\n                              fully_connected_fusion=False, group_norm=False):\n        # ROI-wise segmentation part.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        mask_head = panet.MaskHead(features_dim_in[0], cls_num, 256,\n                                   afp_levels_num=afp_levels_num,\n                                   fully_connected_fusion=fully_connected_fusion,\n                                   in_resolution=PANetMaskRCNN.segmentation_roi_featuremap_resolution,\n                                   group_norm=group_norm)\n        return mask_head\n\n    @timed\n    def forward(self, im_data, im_info, gt_boxes=None, gt_labels=None, gt_is_ignored=None, gt_masks=None,\n                batch_idx=None, **kwargs):\n        if self.training:\n            # In case of training return a dict rather than a list.\n            return_values = {}\n        else:\n            return_values = []\n\n        im_data, im_info = self.preprocess_data(im_data, im_info, self.input_size_divisor)\n        batch_size = im_data.shape[0]\n\n        backbone_features = self.forward_backbone(im_data)\n        backbone_features = self.forward_fpn(backbone_features)\n        backbone_features = self.forward_bupa(backbone_features)\n\n        with torch.no_grad():\n            priors_pyramid = self.generate_priors(backbone_features, im_data)\n        rpn_cls_targets, rpn_reg_targets = self.get_rpn_targets(priors_pyramid, gt_boxes, gt_labels,\n                                                                gt_is_ignored, im_info)\n        rois, rois_probs, rpn_metrics, rpn_loss = self.forward_rpn(priors_pyramid, backbone_features, im_info,\n                                                                   rpn_cls_targets, rpn_reg_targets, batch_size)\n        return_values = self.update_return_values(return_values, rpn_metrics)\n        rois, rois_probs = self.process_proposals(rois, rois_probs, batch_size)\n        rois, roi_cls_targets, roi_reg_targets, roi_mask_targets = self.get_targets(rois, gt_boxes, gt_labels, gt_masks)\n\n        # Last pyramid level is used only for RPN part of the net, so, remove it now.\n        backbone_features = backbone_features[:-1]\n        instance_heads_output, instance_heads_loss = self.forward_instance_heads(im_info, backbone_features, rois,\n                                                                                 roi_cls_targets, roi_reg_targets,\n                                                                                 roi_mask_targets,\n                                                                                 batch_idx=batch_idx)\n        return_values = self.update_return_values(return_values, instance_heads_output)\n\n        if self.training:\n            total_loss = rpn_loss + instance_heads_loss\n            return_values[\'losses/TOTAL\'] = total_loss.detach()\n            return return_values, total_loss\n        else:\n            # Dummy operation on outputs for Model Optimizer.\n            for ret_val in return_values:\n                if ret_val is not None:\n                    ret_val += 0\n            return return_values\n\n    @timed\n    def forward_bupa(self, feature_pyramid):\n        return self.bupa(feature_pyramid)\n\n    @timed\n    def forward_instance_heads(self, im_info, feature_pyramid, rois,\n                               roi_cls_targets=None, roi_reg_targets=None,\n                               roi_mask_targets=None, batch_size=1, batch_idx=None):\n        detection_roi_features, rois = self.extract_roi_features(rois, feature_pyramid,\n                                                                 output_size=self.detection_roi_featuremap_resolution)\n        raw_cls_score, raw_bbox_pred = self.forward_detection_head(detection_roi_features)\n\n        if self.training:\n            return_values = {}\n            loss_cls = detection_loss_cls(raw_cls_score, roi_cls_targets)\n            all_targets = torch.cat(roi_cls_targets)\n            valid_mask = all_targets >= 0\n            accuracy_cls = accuracy(torch.argmax(raw_cls_score[valid_mask], dim=1), all_targets[valid_mask])\n            loss_reg = detection_loss_reg(raw_bbox_pred, roi_cls_targets, roi_reg_targets,\n                                          self.detection_head.cls_agnostic_bbox_regression)\n            return_values[\'losses/detection/cls\'] = loss_cls.detach().unsqueeze(0)\n            return_values[\'losses/detection/reg\'] = loss_reg.detach().unsqueeze(0)\n            return_values[\'metrics/detection/cls_accuracy\'] = accuracy_cls.detach().unsqueeze(0)\n\n            with torch.no_grad():\n                positive_indices = (all_targets > 0).nonzero().view(-1)\n\n            if len(positive_indices) > 0:\n                with torch.no_grad():\n                    positive_mask_targets = torch.cat(roi_mask_targets, dim=0).index_select(0, positive_indices)\n                    positive_cls_targets = all_targets.index_select(0, positive_indices)\n                mask_roi_features, rois = self.extract_roi_features(rois, feature_pyramid,\n                                                                    output_size=self.segmentation_roi_featuremap_resolution)\n                for i, roi_feats in enumerate(mask_roi_features):\n                    mask_roi_features[i] = roi_feats[positive_indices]\n                # mask_roi_features = mask_roi_features[positive_indices]\n                raw_mask_output = self.forward_mask_head(mask_roi_features)\n                assert raw_mask_output.shape[0] == positive_indices.shape[0]\n\n                assert raw_mask_output.shape[0] == positive_cls_targets.shape[0]\n                assert raw_mask_output.shape[0] == positive_mask_targets.shape[0]\n                loss_mask = mask_loss(raw_mask_output, positive_cls_targets, positive_mask_targets)\n                return_values[\'losses/mask\'] = loss_mask.detach().unsqueeze(0)\n                loss = (loss_cls + loss_reg + loss_mask).unsqueeze(0)\n            else:\n                return_values[\'losses/mask\'] = torch.zeros_like(loss_cls).detach().unsqueeze(0)\n                loss = (loss_cls + loss_reg).unsqueeze(0)\n        else:\n            return_values = []\n            with torch.no_grad():\n                boxes, classes, scores, batch_ids = self.forward_detection_output(rois, raw_bbox_pred,\n                                                                                  raw_cls_score, im_info,\n                                                                                  batch_idx)\n\n            if sum(len(im_boxes) for im_boxes in boxes) == 0:\n                return_values.extend((None, None, None, None, None))\n            else:\n                if batch_idx is not None and len(batch_idx) > 1:\n                    rois = list([boxes.index_select(0, (batch_ids == image_id).nonzero().reshape(-1))\n                                 for image_id in batch_idx])\n                else:\n                    rois = [boxes, ]\n                # Extract features for every detected box preserving the order.\n                mask_roi_features, _ = self.extract_roi_features(rois, feature_pyramid,\n                                                                 output_size=self.segmentation_roi_featuremap_resolution)\n                raw_mask_output = self.forward_mask_head(mask_roi_features)\n                return_values.extend((boxes, classes, scores, batch_ids, raw_mask_output))\n            loss = None\n        return return_values, loss\n\n    @timed\n    def extract_roi_features(self, rois, feature_pyramid, output_size=7, preserve_order=True):\n        feature_map_roi_features = []\n        feature_map_rois = []\n        for feature_map, scale in zip(feature_pyramid, self.feature_pyramid_scales):\n            rf, r = extract_roi_features(rois, [feature_map, ],\n                                         pyramid_scales=(scale, ),\n                                         output_size=output_size,\n                                         sampling_ratio=2,\n                                         distribute_rois_between_levels=True,\n                                         preserve_rois_order=preserve_order,\n                                         use_stub=not self.training)\n            feature_map_roi_features.append(rf)\n            feature_map_rois.append(r)\n\n        # from  image -> level -> roi features  to  level -> image -> roi features\n        # roi_features = list(zip(*feature_map_roi_features))\n        roi_features = feature_map_roi_features\n        for i, level_roi_feats in enumerate(roi_features):\n            roi_features[i] = torch.cat(level_roi_feats, dim=0)\n\n        return feature_map_roi_features, rois\n\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/model_zoo/resnet50_c4_mask_rcnn.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom torch import nn\n\nfrom ..backbones.backbone import freeze_params_recursive, freeze_mode_recursive\nfrom ..backbones.resnet import ResNetBody, ResBlock\nfrom ..detection_output import DetectionOutput\nfrom ..losses import rpn_loss_cls, rpn_loss_reg, detection_loss_cls, detection_loss_reg, accuracy, mask_loss\nfrom ..prior_box import PriorBox\nfrom ..proposal import generate_proposals\nfrom ..proposal_gt_matcher import ProposalGTMatcher\nfrom ..roi_feature_extractor import extract_roi_features, topk_rois\nfrom ..rpn import RPN\nfrom ..rpn_gt_matcher import RPNGTMatcher\n\n\nclass BboxHead(nn.Module):\n    def __init__(self, dim_in, cls_num, cls_agnostic_bbox_regression=False):\n        super().__init__()\n        self.cls_num = cls_num\n        self.cls_agnostic_bbox_regression = cls_agnostic_bbox_regression\n\n        self.cls_score = nn.Conv2d(dim_in, cls_num, 1)\n        box_out_dims = 4 * (1 if cls_agnostic_bbox_regression else cls_num)\n        self.bbox_pred = nn.Conv2d(dim_in, box_out_dims, 1)\n        self._init_weights()\n        # Freeze parameters of an affine transforms in Batch Norm layers.\n        freeze_params_recursive(self, freeze=True, types=(nn.BatchNorm2d, ))\n\n    def _init_weights(self):\n        nn.init.normal_(self.cls_score.weight, std=0.01)\n        nn.init.constant_(self.cls_score.bias, 0)\n        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n        nn.init.constant_(self.bbox_pred.bias, 0)\n\n    def train(self, mode=True):\n        super().train(mode=mode)\n        # Always keep Batch Norms in eval mode.\n        freeze_mode_recursive(self, train_mode=False, types=(nn.BatchNorm2d, ))\n\n    def forward(self, x):\n        batch_size = int(x.size(0))\n        cls_score = self.cls_score(x).view(batch_size, -1)\n        if not self.training:\n            cls_score = nn.functional.softmax(cls_score, dim=1)\n        bbox_pred = self.bbox_pred(x).view(batch_size, -1)\n        return cls_score, bbox_pred\n\n\nclass MaskHead(nn.Module):\n    def __init__(self, dim_in, dim_internal, num_cls):\n        super().__init__()\n        self.upconv5 = nn.ConvTranspose2d(dim_in, dim_internal, 2, 2, 0)\n        self.segm = nn.Conv2d(dim_internal, num_cls, 1, 1, 0)\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.kaiming_uniform_(self.upconv5.weight)\n        nn.init.constant_(self.upconv5.bias, 0)\n        nn.init.kaiming_uniform_(self.segm.weight)\n        nn.init.constant_(self.segm.bias, 0)\n\n    def forward(self, x):\n        x = self.upconv5(x)\n        x = nn.functional.relu(x, inplace=True)\n        x = self.segm(x)\n        if not self.training:\n            x = torch.sigmoid(x)\n        return x\n\n\nclass ResNet50C4MaskRCNN(nn.Module):\n    roi_featuremap_resolution = 14\n\n    def __init__(self, cls_num, force_max_output_size=False, **kwargs):\n        super().__init__()\n        self.cls_num = cls_num\n        self.force_max_output_size = force_max_output_size\n\n        self.backbone = ResNetBody(block_counts=(3, 4, 6), res_block=ResBlock, num_groups=1,\n                                   width_per_group=64, res5_dilation=1)\n        self.backbone.freeze_stages_params(range(2))\n        self.backbone.freeze_stages_bns(range(5))\n        self.backbone.set_output_stages((3, ))\n\n        dim = self.backbone.dims_out[0]\n\n        widths = [46.0, 92.0, 184.0, 368.0, 736.0, 32.0, 64.0, 128.0, 256.0, 512.0, 22.0, 44.0, 88.0, 176.0, 352.0]\n        heights = [24.0, 48.0, 96.0, 192.0, 384.0, 32.0, 64.0, 128.0, 256.0, 512.0, 44.0, 88.0, 176.0, 352.0, 704.0]\n        self.prior_boxes = PriorBox(widths=widths, heights=heights, flatten=True)\n\n        self.rpn = RPN(dim, 1024, self.prior_boxes.priors_num(), \'sigmoid\')\n\n        self.common_detection_mask_head, head_dim = ResNetBody.add_stage(ResBlock, dim, 2048, 512, 3, stride_init=2)\n        self.global_pooling = nn.AvgPool2d(7)\n\n        self.cls_agnostic_bbox_regression = False\n        self.detection_head = BboxHead(head_dim, cls_num,\n                                       cls_agnostic_bbox_regression=self.cls_agnostic_bbox_regression)\n        self.detection_output = DetectionOutput(cls_num, nms_threshold=0.5, score_threshold=0.05)\n        self.mask_head = MaskHead(head_dim, 256, cls_num)\n\n        # For training only.\n        self.rpn_gt_matcher = RPNGTMatcher(straddle_threshold=0, positive_threshold=0.7,\n                                           negative_threshold=0.3, positive_fraction=0.5,\n                                           batch_size=256)\n        self.proposal_gt_matcher = ProposalGTMatcher(positive_threshold=0.5, negative_threshold=0.5,\n                                                     positive_fraction=0.25, batch_size=512, target_mask_size=(14, 14))\n\n    @property\n    def pre_nms_rois_count(self):\n        return 12000 if self.training else 6000\n\n    @property\n    def post_nms_rois_count(self):\n        return 2000 if self.training else 1000\n\n    def preprocess_data(self, im_data, im_info, size_divisor=1):\n        with torch.no_grad():\n            if isinstance(im_data, list):\n                im_data = self.pad_image_data(im_data, size_divisor)\n            if isinstance(im_info, list):\n                im_info = torch.stack(im_info, dim=0)\n        if im_data.device != im_info.device:\n            im_info = im_info.to(im_data.device)\n        return im_data, im_info\n\n    @staticmethod\n    def pad_image_data(image_blobs, size_divisor):\n        target_height = max([entry.shape[-2] for entry in image_blobs])\n        target_width = max([entry.shape[-1] for entry in image_blobs])\n        target_height = (target_height + size_divisor - 1) // size_divisor * size_divisor\n        target_width = (target_width + size_divisor - 1) // size_divisor * size_divisor\n        for i, image_blob in enumerate(image_blobs):\n            image_blobs[i] = torch.nn.functional.pad(image_blob, (0, target_width - image_blob.shape[-1],\n                                                                  0, target_height - image_blob.shape[-2]))\n        return torch.stack(image_blobs, dim=0)\n\n    def forward(self, im_data, im_info, gt_boxes=None, gt_labels=None, gt_is_ignored=None, gt_masks=None, **kwargs):\n        if self.training:\n            # In case of training return a dict rather than a list.\n            return_values = {}\n        else:\n            return_values = []\n\n        im_data, im_info = self.preprocess_data(im_data, im_info, 16)\n        batch_size = im_data.shape[0]\n\n        backbone_features = self.backbone(im_data)[0]\n\n        rpn_output = self.rpn(backbone_features)\n\n        with torch.no_grad():\n            priors = self.prior_boxes(backbone_features, im_data,\n                                      stride_x=self.backbone.scales_out[0],\n                                      stride_y=self.backbone.scales_out[0])\n\n        if self.training:\n            with torch.no_grad():\n                rpn_cls_targets, rpn_reg_targets = self.rpn_gt_matcher(priors, gt_boxes, gt_labels, gt_is_ignored, im_info)\n            rpn_box_deltas, rpn_cls_scores = rpn_output[:2]\n            loss_rpn_cls, accuracy_rpn_cls, precision_rpn_cls, recall_rpn_cls = rpn_loss_cls(rpn_cls_targets, rpn_cls_scores)\n            loss_rpn_reg = rpn_loss_reg(rpn_cls_targets, rpn_reg_targets, rpn_box_deltas)\n            return_values[\'losses/rpn/cls\'] = loss_rpn_cls.unsqueeze(0)\n            return_values[\'losses/rpn/reg\'] = loss_rpn_reg.unsqueeze(0)\n            return_values[\'metrics/rpn/cls_accuracy\'] = accuracy_rpn_cls.unsqueeze(0)\n            return_values[\'metrics/rpn/cls_precision\'] = precision_rpn_cls.unsqueeze(0)\n            return_values[\'metrics/rpn/cls_recall\'] = recall_rpn_cls.unsqueeze(0)\n            return_values[\'losses/rpn\'] = (loss_rpn_cls + loss_rpn_reg).unsqueeze(0)\n\n        with torch.no_grad():\n            rois, rois_probs = generate_proposals(priors, rpn_output, im_info,\n                                                  pre_nms_count=self.pre_nms_rois_count,\n                                                  post_nms_count=self.post_nms_rois_count,\n                                                  force_max_output_size=self.force_max_output_size)\n            if batch_size == 1:\n                rois[0] = topk_rois(rois[0], rois_probs[0], max_rois=self.post_nms_rois_count,\n                                    use_stub=not self.training)\n\n        if self.training:\n            with torch.no_grad():\n                rois, roi_cls_targets, roi_reg_targets, roi_mask_targets = \\\n                    self.proposal_gt_matcher(rois, gt_boxes, gt_labels, gt_masks)\n                # Sanity checks.\n                assert len(rois) == len(roi_cls_targets)\n                assert len(rois) == len(roi_reg_targets)\n                for im_rois, im_roi_cls_targets, im_roi_reg_targets in zip(rois, roi_cls_targets, roi_reg_targets):\n                    assert im_rois.shape[0] == im_roi_cls_targets.shape[0]\n                    assert im_rois.shape[0] == im_roi_reg_targets.shape[0]\n\n        roi_features, rois = self.extract_roi_features(rois, backbone_features)\n\n        common_roi_features = self.common_detection_mask_head(roi_features)\n        assert common_roi_features.shape[0] == roi_features.shape[0]\n\n        detection_roi_features = self.global_pooling(common_roi_features)\n        raw_cls_score, raw_bbox_pred = self.detection_head(detection_roi_features)\n        assert raw_cls_score.shape[0] == roi_features.shape[0]\n        assert raw_bbox_pred.shape[0] == roi_features.shape[0]\n\n        if self.training:\n            loss_cls = detection_loss_cls(raw_cls_score, roi_cls_targets)\n            all_targets = torch.cat(roi_cls_targets)\n            valid_mask = all_targets >= 0\n            accuracy_cls = accuracy(torch.argmax(raw_cls_score[valid_mask], dim=1), all_targets[valid_mask])\n            loss_reg = detection_loss_reg(raw_bbox_pred, roi_cls_targets, roi_reg_targets,\n                                          self.cls_agnostic_bbox_regression)\n            return_values[\'losses/detection/cls\'] = loss_cls.unsqueeze(0)\n            return_values[\'losses/detection/reg\'] = loss_reg.unsqueeze(0)\n            return_values[\'metrics/detection/cls_accuracy\'] = accuracy_cls.unsqueeze(0)\n\n            with torch.no_grad():\n                positive_indices = (all_targets > 0).nonzero().view(-1)\n                positive_mask_targets = torch.cat(roi_mask_targets, dim=0).index_select(0, positive_indices)\n                positive_cls_targets = all_targets.index_select(0, positive_indices)\n            mask_roi_features = common_roi_features.index_select(0, positive_indices)\n            raw_mask_output = self.mask_head(mask_roi_features)\n            assert raw_mask_output.shape[0] == positive_indices.shape[0]\n\n            loss_mask = mask_loss(raw_mask_output, positive_cls_targets, positive_mask_targets)\n            return_values[\'losses/mask\'] = loss_mask.unsqueeze(0)\n        else:\n            with torch.no_grad():\n                if len(rois) == 1:\n                    rois = rois[0]\n                self.detection_output.force_max_output_size = self.force_max_output_size\n                boxes, classes, scores, batch_ids = self.detection_output(rois, raw_bbox_pred, raw_cls_score, im_info)\n            if boxes.numel() > 0:\n                if batch_size > 1:\n                    rois_to_segment = list([boxes.index_select(0, (batch_ids == image_id).nonzero().reshape(-1))\n                                            for image_id in range(batch_size)])\n                else:\n                    rois_to_segment = [boxes, ]\n                roi_mask_features, rois_to_segment = self.extract_roi_features(rois_to_segment, backbone_features)\n                roi_mask_features = self.common_detection_mask_head(roi_mask_features)\n                raw_mask_output = self.mask_head(roi_mask_features)\n                return_values.extend((boxes, classes, scores, batch_ids, raw_mask_output))\n            else:\n                # Gathering empty tensors could be an issue in DataParallel, so explicitly return Nones.\n                return_values.extend((None, None, None, None, None))\n\n        if self.training:\n            loss_total = loss_rpn_cls + loss_rpn_reg + loss_cls + loss_reg + loss_mask\n            return_values[\'losses/TOTAL\'] = loss_total.unsqueeze(0)\n            return return_values, loss_total\n        else:\n            # Dummy operation on outputs for Model Optimizer.\n            for ret_val in return_values:\n                if ret_val is not None:\n                    ret_val += 0\n            return return_values\n\n    def extract_roi_features(self, rois, features):\n        roi_features, rois = extract_roi_features(rois, [features, ],\n                                                  pyramid_scales=tuple(self.backbone.scales_out),\n                                                  output_size=self.roi_featuremap_resolution,\n                                                  sampling_ratio=0,\n                                                  distribute_rois_between_levels=True,\n                                                  preserve_rois_order=True,\n                                                  use_stub=not self.training)\n        roi_features = roi_features[0]\n        rois = rois[0]\n        return roi_features, rois\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/model_zoo/resnet_fpn_mask_rcnn.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\n\nfrom .fpn_mask_rcnn_base import FPNMaskRCNN, BboxHead, DetectionOutput, MaskHead\nfrom ..backbones.resnet import ResNet\nfrom ..prior_box import PriorBox\nfrom ..proposal_gt_matcher import ProposalGTMatcher\n\n\nclass ResNet50FPNMaskRCNN(FPNMaskRCNN):\n    def __init__(self, cls_num, force_max_output_size=False, **kwargs):\n        backbone = ResNet(base_arch=\'ResNet50\')\n        backbone.freeze_stages_params(range(2))\n        backbone.freeze_stages_bns(range(5))\n        backbone.set_output_stages((1, 2, 3, 4))\n        super().__init__(cls_num, backbone, force_max_output_size=force_max_output_size, **kwargs)\n\n\nclass ResNet50FPNMaskRCNNDemo(ResNet50FPNMaskRCNN):\n    def __init__(self, cls_num, force_max_output_size=False, **kwargs):\n        super().__init__(cls_num, force_max_output_size=force_max_output_size, **kwargs)\n        self.data_normalizer = nn.BatchNorm2d(3, eps=0, affine=True, track_running_stats=True)\n        self.data_normalizer.weight[0] = 1.0\n        self.data_normalizer.weight[1] = 1.0\n        self.data_normalizer.weight[2] = 1.0\n        self.data_normalizer.bias[0] = -102.9801\n        self.data_normalizer.bias[1] = -115.9465\n        self.data_normalizer.bias[2] = -122.7717\n\n    @property\n    def pre_nms_rois_count(self):\n        return 2000 if self.training else 100\n\n    @property\n    def post_nms_rois_count(self):\n        return 2000 if self.training else 100\n\n    def preprocess_data(self, im_data, im_info, size_divisor=32):\n        im_data, im_info = super().preprocess_data(im_data, im_info, size_divisor)\n        im_data = self.data_normalizer(im_data)\n        return im_data, im_info\n\n\nclass ResNet101FPNMaskRCNN(FPNMaskRCNN):\n    def __init__(self, cls_num, **kwargs):\n        backbone = ResNet(base_arch=\'ResNet101\')\n        backbone.freeze_stages_params(range(2))\n        backbone.freeze_stages_bns(range(5))\n        backbone.set_output_stages((1, 2, 3, 4))\n        super().__init__(cls_num, backbone, **kwargs)\n\n\nclass ResNeXt101FPNMaskRCNN(FPNMaskRCNN):\n    def __init__(self, cls_num, **kwargs):\n        backbone = ResNet(base_arch=\'ResNet101\', num_groups=32, width_per_group=8, stride_1x1=False)\n        backbone.freeze_stages_params(range(2))\n        backbone.freeze_stages_bns(range(5))\n        backbone.set_output_stages((1, 2, 3, 4))\n        super().__init__(cls_num, backbone, **kwargs)\n\n\nclass ResNeXt10164x4dFPNMaskRCNN(FPNMaskRCNN):\n    def __init__(self, cls_num, **kwargs):\n        backbone = ResNet(base_arch=\'ResNet101\', num_groups=64, width_per_group=4, stride_1x1=False)\n        backbone.freeze_stages_params(range(2))\n        backbone.freeze_stages_bns(range(5))\n        backbone.set_output_stages((1, 2, 3, 4))\n        super().__init__(cls_num, backbone, **kwargs)\n\n\nclass ResNeXt152FPNMaskRCNN(FPNMaskRCNN):\n    def __init__(self, cls_num, **kwargs):\n        backbone = ResNet(base_arch=\'ResNet152\', num_groups=32, width_per_group=8, stride_1x1=False)\n        backbone.freeze_stages_params(range(2))\n        backbone.freeze_stages_bns(range(5))\n        backbone.set_output_stages((1, 2, 3, 4))\n        super().__init__(cls_num, backbone, **kwargs)\n\n\nclass ResNeXt152sFPNMaskRCNN(FPNMaskRCNN):\n    def __init__(self, cls_num, **kwargs):\n        backbone = ResNet(base_arch=\'ResNet152\', num_groups=32, width_per_group=8)\n        backbone.freeze_stages_params(range(2))\n        backbone.freeze_stages_bns(range(5))\n        backbone.set_output_stages((1, 2, 3, 4))\n        super().__init__(cls_num, backbone, **kwargs)\n\n\nclass ResNet50FPNGNMaskRCNN(FPNMaskRCNN):\n    def __init__(self, cls_num, **kwargs):\n        backbone = ResNet(base_arch=\'ResNet50\', group_norm=True)\n        backbone.freeze_stages_params(range(2))\n        backbone.set_output_stages((1, 2, 3, 4))\n        super().__init__(cls_num, backbone, group_norm=True, **kwargs)\n\n\nclass ResNet50FPNMaskRCNNLightSegmHead(ResNet50FPNMaskRCNN):\n    segmentation_roi_featuremap_resolution = 7\n\n    def __init__(self, cls_num, **kwargs):\n        super().__init__(cls_num, **kwargs)\n        r = self.segmentation_roi_featuremap_resolution\n        self.proposal_gt_matcher = ProposalGTMatcher(positive_threshold=0.5, negative_threshold=0.5,\n                                                     positive_fraction=0.25, batch_size=256,\n                                                     target_mask_size=(2 * r, 2 * r))\n\n    @staticmethod\n    def add_priors_generator():\n        prior_boxes = nn.ModuleList()\n        widths = [[49.0, 33.0, 25.0], [89.0, 65.0, 49.0], [185.0, 129.0, 89.0], [361.0, 257.0, 185.0],\n                  [729.0, 513.0, 361.0]]\n        heights = [[25.0, 33.0, 49.0], [49.0, 65.0, 89.0], [97.0, 129.0, 147.0], [177.0, 257.0, 369.0],\n                   [369.0, 513.0, 721.0]]\n        scale_factor = 0.4\n        for ws, hs in zip(widths, heights):\n            if scale_factor != 1.0:\n                for i in range(len(ws)):\n                    ws[i] *= scale_factor\n                for i in range(len(hs)):\n                    hs[i] *= scale_factor\n            prior_boxes.append(PriorBox(widths=ws, heights=hs, flatten=True, use_cache=True))\n        priors_per_level_num = list([priors.priors_num() for priors in prior_boxes])\n        assert priors_per_level_num[1:] == priors_per_level_num[:-1]\n        priors_num = priors_per_level_num[0]\n        return prior_boxes, priors_num\n\n    @staticmethod\n    def add_segmentation_head(features_dim_in, cls_num, group_norm):\n        # ROI-wise segmentation part.\n        assert features_dim_in[1:] == features_dim_in[:-1]\n        mask_head = MaskHead(features_dim_in[0], 4, cls_num, 128, 1, group_norm=group_norm)\n        return mask_head\n\n\nclass ResNet50FPNMaskRCNNLightSegmHeadDemo(ResNet50FPNMaskRCNNLightSegmHead):\n    def __init__(self, cls_num, **kwargs):\n        super().__init__(cls_num, **kwargs)\n        self.data_normalizer = nn.BatchNorm2d(3, eps=0, affine=True, track_running_stats=True)\n        self.data_normalizer.weight[0] = 1.0\n        self.data_normalizer.weight[1] = 1.0\n        self.data_normalizer.weight[2] = 1.0\n        self.data_normalizer.bias[0] = -102.9801\n        self.data_normalizer.bias[1] = -115.9465\n        self.data_normalizer.bias[2] = -122.7717\n\n    @property\n    def pre_nms_rois_count(self):\n        return 2000 if self.training else 100\n\n    @property\n    def post_nms_rois_count(self):\n        return 2000 if self.training else 100\n\n    def preprocess_data(self, im_data, im_info, size_divisor=32):\n        im_data, im_info = super().preprocess_data(im_data, im_info, size_divisor)\n        im_data = self.data_normalizer(im_data)\n        return im_data, im_info\n'"
pytorch_toolkit/instance_segmentation/segmentoly/rcnn/model_zoo/resnet_panet_mask_rcnn.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom .panet_mask_rcnn_base import PANetMaskRCNN\nfrom ..backbones.resnet import ResNet\nfrom ..panet import BboxHead as PANetBboxHead\n\n\nclass ResNeXt101PANetMaskRCNN(PANetMaskRCNN):\n    def __init__(self, cls_num, force_max_output_size=False, heavier_head=False, deformable_conv=True, **kwargs):\n        backbone = ResNet(base_arch=\'ResNet101\', num_groups=32, width_per_group=8)\n        backbone.freeze_stages_params(range(2))\n        backbone.freeze_stages_bns(range(5))\n        backbone.set_output_stages((1, 2, 3, 4))\n        super().__init__(cls_num, backbone, force_max_output_size=force_max_output_size,\n                         heavier_head=heavier_head,\n                         deformable_conv=deformable_conv, **kwargs)\n        self.mask_head = self.add_segmentation_head(self.bupa.dims_out, self.cls_num, afp_levels_num=4,\n                                                    fully_connected_fusion=True, group_norm=True)\n        self.detection_head = self.BboxHead(self.bupa.dims_out[0], 1024, PANetMaskRCNN.detection_roi_featuremap_resolution,\n                                            self.cls_num,\n                                            cls_agnostic_bbox_regression=False,\n                                            afp_levels_num=4,\n                                            heavier_head=heavier_head, group_norm=False)\n\n    class BboxHead(PANetBboxHead):\n        """"""BboxHead from PANet without ReLu after fc1""""""\n        def __init__(self, dim_in, dim_out, resolution_in, cls_num, cls_agnostic_bbox_regression=False,\n                     afp_levels_num=4, heavier_head=False, conv_head_dim=256, num_convs=4,\n                     group_norm=False):\n            super().__init__(dim_in, dim_out, resolution_in, cls_num, cls_agnostic_bbox_regression,\n                             afp_levels_num, heavier_head, conv_head_dim, num_convs, group_norm)\n\n        def forward(self, x):\n            batch_size = int(x[0].shape[0])\n            for i in range(self.levels_num):\n                if self.heavier_head:\n                    y = self.fc1[i](x[i])\n                else:\n                    y = self.fc1[i](x[i].view(batch_size, -1))\n\n                if i == 0:\n                    pooled_feature = y\n                else:\n                    pooled_feature = torch.max(pooled_feature, y)\n\n            x = self.fc2(pooled_feature)\n\n            if self.heavier_head:\n                x = nn.functional.relu(self.fc(x.view(batch_size, -1)), inplace=True)\n\n            cls_score = self.cls_score(x)\n            if not self.training:\n                cls_score = nn.functional.softmax(cls_score, dim=1)\n            bbox_pred = self.bbox_pred(x)\n\n            return cls_score, bbox_pred\n'"
pytorch_toolkit/nncf/examples/common/models/__init__.py,0,b'from examples.common.models.segmentation import *\nfrom examples.common.models.classification import *\n'
pytorch_toolkit/nncf/examples/object_detection/datasets/__init__.py,0,b''
pytorch_toolkit/nncf/examples/object_detection/datasets/coco.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport json\nfrom collections import OrderedDict\nfrom pathlib import Path\nimport sys\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nfrom examples.common.example_logger import logger\n\nCOCO_CLASSES = (  # always index 0\n    ""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"",\n    ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"", ""elephant"",\n    ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"",\n    ""sports ball"", ""kite"", ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"",\n    ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"", ""broccoli"",\n    ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"", ""bed"", ""dining table"", ""toilet"",\n    ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"",\n    ""book"", ""clock"", ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush"")\n\nCOCO_NAMES = (\n    ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"",\n    ""23"", ""24"", ""25"", ""27"", ""28"", ""31"", ""32"", ""33"", ""34"", ""35"", ""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", ""44"",\n    ""46"", ""47"", ""48"", ""49"", ""50"", ""51"", ""52"", ""53"", ""54"", ""55"", ""56"", ""57"", ""58"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"",\n    ""65"", ""67"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""80"", ""81"", ""82"", ""84"", ""85"", ""86"", ""87"", ""88"",\n    ""89"", ""90""\n)\n\n# for making bounding boxes pretty\nCOLORS = ((255, 0, 0, 128), (0, 255, 0, 128), (0, 0, 255, 128),\n          (0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128))\n\n\ndef _read_coco_annotation(annotation_file, images_folder):\n    images_folder = Path(images_folder)\n    anno_dict = OrderedDict()\n\n    with open(annotation_file) as data_file:\n        json_annotation = json.load(data_file)\n    annotation = json_annotation[""annotations""]\n\n    for imgAnnotation in annotation:\n        img_path = images_folder / ""{0:012d}.jpg"".format(imgAnnotation[\'image_id\'])\n\n        name = str(imgAnnotation[""category_id""])\n        label_idx = COCO_NAMES.index(name)\n        bbox = imgAnnotation[""bbox""]\n\n        if bbox is None or bbox == """":\n            raise ValueError(""No annotation for {}"".format(img_path))\n\n        bbox[2] = bbox[0] + bbox[2]\n        bbox[3] = bbox[1] + bbox[3]\n        anno_dict.setdefault(img_path.as_posix(), []).append({\'bbox\': bbox, \'label_idx\': label_idx})\n\n    return anno_dict\n\n\nclass COCODataset(data.Dataset):\n    classes = COCO_CLASSES\n    name = \'coco\'\n\n    def __init__(self, annotation_file, images_folder, transform=None, target_transform=None, scale_bboxes=True,\n                 return_image_info=False, rgb=True):\n        self.rgb = rgb\n        self.target_transform = target_transform\n        self.return_image_info = return_image_info\n        self.annotationFile = annotation_file\n        self.imagesFolder = images_folder\n        self.transform = transform\n        self.scale_bboxes = scale_bboxes\n        self.annotation = _read_coco_annotation(annotation_file, images_folder)\n\n    def __getitem__(self, index):\n        """"""\n        Returns image at index in torch tensor form (RGB) and\n        corresponding normalized annotation in 2d array [[xmin, ymin, xmax, ymax, label_ind],\n                                                         ... ]\n        """"""\n        im, gt, h, w = self.pull_item(index)\n        if self.return_image_info:\n            return im, gt, h, w\n        return im, gt\n\n    def __len__(self):\n        return len(self.annotation)\n\n    def pull_item(self, index):\n        """"""\n        Returns image at index in torch tensor form (RGB),\n        corresponding normalized annotation in 2d array [[xmin, ymin, xmax, ymax, label_ind],\n                                                          ... ],\n        height and width of image\n        """"""\n        img_path = list(self.annotation.keys())[index]\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        assert img is not None\n        height, width, _ = img.shape\n\n        boxes = np.asarray([anno[\'bbox\'] for anno in self.annotation[img_path]])\n        labels = np.asarray([anno[\'label_idx\'] for anno in self.annotation[img_path]])\n\n        if self.scale_bboxes:\n            boxes /= np.array([width, height, width, height])\n\n        if not boxes.size:\n            logger.error(""error: no annotation on image"")\n            sys.exit(-1)\n\n        if self.target_transform is not None:\n            annotation = self.target_transform(self.annotation, width, height)\n\n        if self.transform is not None:\n            img, boxes, labels = self.transform(img, boxes, labels)\n            if self.rgb:\n                img = img[:, :, (2, 1, 0)]\n            annotation = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n        return torch.from_numpy(img).permute(2, 0, 1), annotation, height, width\n\n    def pull_image(self, index):\n        return cv2.imread(list(self.annotation.keys())[index], cv2.IMREAD_COLOR)\n\n    def pull_anno(self, index):\n        """"""Returns the original annotation of image at index\n\n        eg: [\'001718\', [[xmin, ymin, xmax, ymax, label_ind], ... ]]\n        """"""\n        img_path = list(self.annotation.keys())[index]\n        return img_path[img_path.rfind(""/"") + 1: img_path.rfind(""."")], self.annotation[img_path]\n\n    def get_img_names(self):\n        img_names = []\n        for full_name in self.annotation.keys():\n            img_names.append(full_name[full_name.rfind(""/"") + 1: full_name.rfind(""."")])\n        return img_names\n'"
pytorch_toolkit/nncf/examples/object_detection/datasets/voc0712.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nimport os.path\nimport sys\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\nVOC_CLASSES = (  # always index 0\n    \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n# for making bounding boxes pretty\nCOLORS = ((255, 0, 0, 128), (0, 255, 0, 128), (0, 0, 255, 128),\n          (0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128))\n\n\nclass VOCAnnotationTransform:\n    """"""Transforms a VOC annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n\n    Args:\n        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n            (default: alphabetic indexing of VOC\'s 20 classes)\n        keep_difficult (bool, optional): keep difficult instances or not\n            (default: False)\n        height (int): height\n        width (int): width\n    """"""\n\n    def __init__(self, class_to_ind=None, keep_difficult=False):\n        self.class_to_ind = class_to_ind or dict(\n            zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n        self.keep_difficult = keep_difficult\n\n    def __call__(self, target, width, height):\n        """"""\n        Args:\n            target (annotation) : the target annotation to be made usable\n                will be an ET.Element\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class idx],\n            coordinates are normalized\n        """"""\n        res = []\n        for obj in target.iter(\'object\'):\n            difficult = int(obj.find(\'difficult\').text) == 1\n            if not self.keep_difficult and difficult:\n                continue\n            name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n\n            pts = [\'xmin\', \'ymin\', \'xmax\', \'ymax\']\n            bndbox = []\n            for i, pt in enumerate(pts):\n                cur_pt = int(bbox.find(pt).text) - 1\n                # scale height or width\n                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n                bndbox.append(cur_pt)\n            label_idx = self.class_to_ind[name]\n            res += [{\'bbox\': bndbox, \'label_idx\': label_idx, \'difficult\': difficult}]\n\n        return res\n\n\nclass VOCDetection(data.Dataset):\n    """"""VOC Detection Dataset Object\n\n    input is image, target is annotation\n\n    Args:\n        root (string): path to VOCdevkit folder.\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n    classes = VOC_CLASSES\n    name = \'voc\'\n\n    def __init__(self, root, image_sets=((\'2007\', \'trainval\'), (\'2012\', \'trainval\')), transform=None,\n                 target_transform=VOCAnnotationTransform(keep_difficult=False), return_image_info=False, rgb=True):\n        self.rgb = rgb\n        self.root = root\n        self.image_set = image_sets\n        self.transform = transform\n        self.target_transform = target_transform\n        self.return_image_info = return_image_info\n        self._annopath = os.path.join(\'%s\', \'Annotations\', \'%s.xml\')\n        self._imgpath = os.path.join(\'%s\', \'JPEGImages\', \'%s.jpg\')\n        self.ids = list()\n\n        for (year, name) in self.image_set:\n            rootpath = os.path.join(self.root, \'VOC\' + year)\n            for line in open(os.path.join(rootpath, \'ImageSets\', \'Main\', name + \'.txt\')):\n                self.ids.append((rootpath, line.strip()))\n\n    def __getitem__(self, index):\n        """"""\n        Returns image at index in torch tensor form (RGB) and\n        corresponding normalized annotation in 2d array [[xmin, ymin, xmax, ymax, label_ind],\n                                              ... ]\n        """"""\n        im, gt, h, w = self.pull_item(index)\n\n        if self.return_image_info:\n            return im, gt, h, w\n        return im, gt\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_item(self, index):\n        """"""\n        Returns image at index in torch tensor form (RGB),\n        corresponding normalized annotation in 2d array [[xmin, ymin, xmax, ymax, label_ind],\n                                                         ... ],\n        height and width of image\n        """"""\n        img_id = self.ids[index]\n\n        target = ET.parse(self._annopath % img_id).getroot()\n        img = cv2.imread(self._imgpath % img_id)\n\n        height, width, _ = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target, width, height)\n\n        if self.transform is not None:\n            boxes = np.asarray([x[\'bbox\'] for x in target])\n            labels = np.asarray([x[\'label_idx\'] for x in target])\n            img, boxes, labels = self.transform(img, boxes, labels)\n            if self.rgb:\n                img = img[:, :, (2, 1, 0)]\n            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n        return torch.from_numpy(img).permute(2, 0, 1), target, height, width\n\n    def pull_anno(self, index):\n        """"""Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n\n        Args:\n            index (int): index of img to get annotation of\n        Returns:\n            list:  img_id, [[bbox coords, label_idx],...]]\n                eg: [\'001718\', [[xmin, ymin, xmax, ymax, label_ind], ... ]]\n        """"""\n        img_name = self.ids[index]\n        anno = ET.parse(self._annopath % img_name).getroot()\n        gt = self.target_transform(anno, 1, 1)\n        return img_name[1], gt\n\n    def pull_image(self, index):\n        img_id = self.ids[index]\n        return cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n\n    def get_img_names(self):\n        img_names = []\n        for id_ in self.ids:\n            img_names.append(id_[1])\n        return img_names\n'"
pytorch_toolkit/nncf/examples/object_detection/layers/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
pytorch_toolkit/nncf/examples/object_detection/layers/box_utils.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\nfrom .extensions import EXTENSIONS\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:] / 2,  # xmin, ymin\n                      boxes[:, :2] + boxes[:, 2:] / 2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2]) / 2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) *\n              (box_a[:, 3] - box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2] - box_b[:, 0]) *\n              (box_b[:, 3] - box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef match(threshold, truths, priors, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers with variances, Shape: [2,n_priors,4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        priors[0]\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]  # Shape: [num_priors,4]\n    conf = labels[best_truth_idx] + 1  # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors)\n    loc_t[idx] = loc  # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\n\ndef encode(matched, priors):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in point-form with variances\n            Shape: [2, num_priors,4].\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n    variances = priors[1]\n    priors = priors[0]\n    g_cxcy = ((matched[:, :2] + matched[:, 2:]) / 2 - (priors[:, :2] + priors[:, 2:]) / 2) / (\n                (priors[:, 2:] - priors[:, :2]) * variances[:, :2])\n    g_wh = torch.log((matched[:, 2:] - matched[:, :2]) / (priors[:, 2:] - priors[:, :2])) / variances[:, 2:]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in point-form with variances.\n            Shape: [2, num_priors,4].\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    variances = priors[1].squeeze(0)\n    priors = priors[0].squeeze(0)\n    decoded_boxes_cx_cy = variances[:, :2] * loc[:, :2] * (priors[:, 2:] - priors[:, :2]) + (\n                (priors[:, :2] + priors[:, 2:]) / 2)\n    decoded_boxes_w_h = torch.exp(variances[:, 2:] * loc[:, 2:]) * (priors[:, 2:] - priors[:, :2])\n    decoded_boxes_xmin_ymin = decoded_boxes_cx_cy - (decoded_boxes_w_h / 2)\n    decoded_boxes_xmax_ymax = decoded_boxes_cx_cy + (decoded_boxes_w_h / 2)\n\n    encoded_boxes = torch.cat((decoded_boxes_xmin_ymin, decoded_boxes_xmax_ymax), 1)\n\n    return encoded_boxes\n\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x - x_max), 1, keepdim=True)) + x_max\n\n\nclass NMSFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, boxes, scores, threshold, top_k=200):\n        if scores.size(0) == 0:\n            return torch.tensor([], dtype=torch.int), torch.tensor(0)\n        if scores.dim() == 1:\n            scores = scores.unsqueeze(1)\n        keep = EXTENSIONS.nms(torch.cat((boxes, scores), dim=1), threshold, top_k)\n        return keep, torch.tensor(keep.size(0))\n\n    @staticmethod\n    def backward(ctx, boxes):\n        raise NotImplementedError\n\n\nnms = NMSFunction.apply\n'"
pytorch_toolkit/nncf/examples/object_detection/models/__init__.py,0,b''
pytorch_toolkit/nncf/examples/object_detection/models/ssd_mobilenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom examples.object_detection.layers.modules.ssd_head import MultiOutputSequential, SSDDetectionOutput\nfrom nncf.checkpoint_loading import load_state\n\nfrom examples.common.example_logger import logger\n\ndef conv_bn(inp, oup, kernel, stride, padding):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, kernel, stride, padding, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef conv_dw(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n        nn.BatchNorm2d(inp),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU(inplace=True),\n    )\n\n\ndef mobilenet(start_input_channels=3):\n    model = MultiOutputSequential(\n        [11, 13],\n        [\n            conv_bn(start_input_channels, 32, 3, 2, 1),\n            conv_dw(32, 64, 1),\n            conv_dw(64, 128, 2),\n            conv_dw(128, 128, 1),\n            conv_dw(128, 256, 2),\n            conv_dw(256, 256, 1),\n            conv_dw(256, 512, 2),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 1024, 2),\n            conv_dw(1024, 1024, 1)\n        ]\n    )\n    return model\n\n\ndef extra(start_input_channels):\n    return MultiOutputSequential(\n        [1, 3, 5, 7],\n        [\n            conv_bn(start_input_channels, 256, 1, 1, 0),\n            conv_bn(256, 512, 3, 2, 1),\n            conv_bn(512, 128, 1, 1, 0),\n            conv_bn(128, 256, 3, 2, 1),\n            conv_bn(256, 128, 1, 1, 0),\n            conv_bn(128, 256, 3, 2, 1),\n            conv_bn(256, 64, 1, 1, 0),\n            conv_bn(64, 128, 3, 2, 1)\n        ]\n    )\n\n\nclass MobileNetSSD(nn.Module):\n    def __init__(self, num_classes, cfg):\n        super(MobileNetSSD, self).__init__()\n        self.cfg = cfg\n        self.num_classes = num_classes\n\n        self.basenet = mobilenet()\n        self.extras = extra(1024)\n\n        NUM_INPUT_FEATURES = [512, 1024, 512, 256, 256, 128]\n        self.detection_head = SSDDetectionOutput(NUM_INPUT_FEATURES, num_classes, cfg)\n\n    def forward(self, x):\n        img_tensor = x[0].clone().unsqueeze(0)\n\n        sources, x = self.basenet(x)\n        extra_sources, x = self.extras(x)\n\n        return self.detection_head(sources + extra_sources, img_tensor)\n\n\ndef build_ssd_mobilenet(cfg, size, num_classes, config):\n    if size != 300:\n        raise ValueError(""Only Mobilenet-SSD with input size 300 is supported"")\n    mobilenet_ssd = MobileNetSSD(num_classes, cfg)\n\n    if config.basenet and (config.resuming_checkpoint is None) and (config.weights is None):\n        logger.debug(\'Loading base network...\')\n        basenet_weights = torch.load(config.basenet)[\'state_dict\']\n        new_weights = {}\n        for wn, wv in basenet_weights.items():\n            wn = wn.replace(\'model.\', \'\')\n            new_weights[wn] = wv\n\n        load_state(mobilenet_ssd.basenet, new_weights, is_resume=False)\n    return mobilenet_ssd\n'"
pytorch_toolkit/nncf/examples/object_detection/models/ssd_vgg.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\n\nimport torch\nimport torch.nn as nn\nfrom examples.object_detection.layers import L2Norm\nfrom examples.object_detection.layers.modules.ssd_head import MultiOutputSequential, SSDDetectionOutput\nfrom nncf.checkpoint_loading import load_state\n\nfrom examples.common.example_logger import logger\n\n\nBASE_NUM_OUTPUTS = {\n    300: [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\', 512, 512, 512],\n    512: [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512],\n}\nEXTRAS_NUM_OUTPUTS = {\n    300: [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n    512: [256, \'S\', 512, 128, \'S\', 256, 128, \'S\', 256, 128, \'S\', 256, 128, \'K\', 256],\n}\n\nBASE_OUTPUT_INDICES = {\n    300: [12],\n    512: [12],\n}\n\nEXTRA_OUTPUT_INDICES = {\n    300: [2, 5, 7, 9],\n    512: [2, 5, 8, 11, 14],\n}\n\n\nclass SSD_VGG(nn.Module):\n    def __init__(self, cfg, size, num_classes, batch_norm=False):\n        super(SSD_VGG, self).__init__()\n        self.config = cfg\n        self.num_classes = num_classes\n        self.size = size\n        self.enable_batchmorm = batch_norm\n\n        base_layers, base_outs, base_feats = build_vgg_ssd_layers(\n            BASE_NUM_OUTPUTS[size], BASE_OUTPUT_INDICES[size], batch_norm=batch_norm\n        )\n        extra_layers, extra_outs, extra_feats = build_vgg_ssd_extra(\n            EXTRAS_NUM_OUTPUTS[size], EXTRA_OUTPUT_INDICES[size], batch_norm=batch_norm\n        )\n        self.basenet = MultiOutputSequential(base_outs, base_layers)\n        self.extras = MultiOutputSequential(extra_outs, extra_layers)\n\n        self.detection_head = SSDDetectionOutput(base_feats + extra_feats, num_classes, cfg)\n        self.L2Norm = L2Norm(512, 20, 1e-10)\n\n    def forward(self, x):\n        img_tensor = x[0].clone().unsqueeze(0)\n\n        sources, x = self.basenet(x)\n        sources[0] = self.L2Norm(sources[0])\n\n        extra_sources, x = self.extras(x)\n\n        return self.detection_head(sources + extra_sources, img_tensor)\n\n    def load_weights(self, base_file):\n        _, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            logger.debug(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file,\n                                            map_location=lambda storage, loc: storage))\n            logger.debug(\'Finished!\')\n        else:\n            logger.error(\'Sorry only .pth and .pkl files supported.\')\n\n\ndef make_ssd_vgg_layer(input_features, output_features, kernel=3, padding=1, dilation=1, modifier=None,\n                       batch_norm=False):\n    stride = 1\n    if modifier == \'S\':\n        stride = 2\n        padding = 1\n    elif modifier == \'K\':\n        kernel = 4\n        padding = 1\n\n    layer = [nn.Conv2d(input_features, output_features, kernel_size=kernel, stride=stride, padding=padding,\n                       dilation=dilation)]\n    if batch_norm:\n        layer.append(nn.BatchNorm2d(output_features))\n    layer.append(nn.ReLU(inplace=True))\n    return layer\n\n\ndef build_vgg_ssd_layers(num_outputs, output_inddices, start_input_channels=3, batch_norm=False):\n    vgg_layers = []\n    output_num_features = []\n    source_indices = []\n    in_planes = start_input_channels\n    modifier = None\n    for i, out_planes in enumerate(num_outputs):\n        if out_planes in (\'M\', \'C\'):\n            vgg_layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=1 if modifier == \'C\' else 0))\n            continue\n        if isinstance(out_planes, str):\n            modifier = out_planes\n            continue\n        vgg_layers.extend(make_ssd_vgg_layer(in_planes, out_planes, modifier=modifier, batch_norm=batch_norm))\n        modifier = None\n        in_planes = out_planes\n        if i in output_inddices:\n            source_indices.append(len(vgg_layers) - 1)\n            output_num_features.append(out_planes)\n\n    vgg_layers.append(nn.MaxPool2d(kernel_size=3, stride=1, padding=1))\n    vgg_layers.extend(make_ssd_vgg_layer(in_planes, 1024, kernel=3, padding=6, dilation=6, batch_norm=batch_norm))\n    vgg_layers.extend(make_ssd_vgg_layer(1024, 1024, kernel=1, batch_norm=batch_norm))\n\n    source_indices.append(len(vgg_layers) - 1)\n    output_num_features.append(1024)\n    return vgg_layers, source_indices, output_num_features\n\n\ndef build_vgg_ssd_extra(num_outputs, output_indices, statrt_input_channels=1024, batch_norm=False):\n    extra_layers = []\n    output_num_features = []\n    source_indices = []\n    in_planes = statrt_input_channels\n    modifier = None\n    kernel_sizes = (1, 3)\n    for i, out_planes in enumerate(num_outputs):\n        if isinstance(out_planes, str):\n            modifier = out_planes\n            continue\n        kernel = kernel_sizes[len(extra_layers) % 2]\n        extra_layers.extend(make_ssd_vgg_layer(in_planes, out_planes, modifier=modifier, kernel=kernel, padding=0,\n                                               batch_norm=batch_norm))\n        modifier = None\n        in_planes = out_planes\n        if i in output_indices:\n            source_indices.append(len(extra_layers) - 1)\n            output_num_features.append(out_planes)\n\n    return extra_layers, source_indices, output_num_features\n\n\ndef build_ssd_vgg(cfg, size, num_classes, config):\n    ssd_vgg = SSD_VGG(cfg, size, num_classes, batch_norm=config.get(\'batchnorm\', False))\n\n    if config.basenet and (config.resuming_checkpoint is None) and (config.weights is None):\n        logger.debug(\'Loading base network...\')\n        basenet_weights = torch.load(config.basenet)\n        new_weights = {}\n        for wn, wv in basenet_weights.items():\n            wn = wn.replace(\'features.\', \'\')\n            new_weights[wn] = wv\n\n        load_state(ssd_vgg.basenet, new_weights, is_resume=False)\n    return ssd_vgg\n'"
pytorch_toolkit/nncf/examples/object_detection/utils/__init__.py,0,"b'from pathlib import Path\n\nimport cv2\nimport yaml\nimport sys\n\ndef mkdir_if_not_exists(path):\n    path = Path(path)\n    if not path.exists():\n        path.mkdir(parents=True)\n\n\ndef showAnnotation(dataset, class_names=None):\n    for image, target in dataset:\n        for anno in target:\n            xmin = int(anno[0])\n            ymin = int(anno[1])\n            xmax = int(anno[2])\n            ymax = int(anno[3])\n            if class_names:\n                name = class_names[int(anno[4])]\n                print(name)\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0))\n\n        cv2.imshow(""img"", image)\n        k = cv2.waitKey()\n        if k == 27:\n            sys.exit()\n\n\ndef read_config(path_to_config, img_size):\n    with open(str(path_to_config)) as f:\n        config = yaml.load(f)[\'CONFIG\']\n        return config[img_size]\n'"
pytorch_toolkit/nncf/examples/object_detection/utils/augmentations.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport types\n\nimport cv2\nimport numpy as np\nimport torch\nfrom numpy import random\n\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) *\n              (box_a[:, 3] - box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2] - box_b[0]) *\n              (box_b[3] - box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass Compose:\n    """"""Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels\n\n\nclass Lambda:\n    """"""Applies a lambda as a transform.""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, boxes=None, labels=None):\n        return self.lambd(img, boxes, labels)\n\n\nclass ConvertFromInts:\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass Normalize:\n    def __init__(self, mean, std, normalize_coef):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.normalize_coef = normalize_coef\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image /= self.normalize_coef\n        image -= self.mean\n        image /= self.std\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords:\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords:\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Resize:\n    def __init__(self, size=300):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                   self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation:\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue:\n    def __init__(self, delta=18.0):\n        assert 0 <= delta <= 360.\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise:\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            idx = int(random.randint(len(self.perms)))\n            swap = self.perms[idx]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor:\n    def __init__(self, current=\'BGR\', transform=\'HSV\'):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == \'BGR\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == \'HSV\' and self.transform == \'BGR\':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast:\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness:\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToCV2Image:\n    def __call__(self, tensor, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n\n\nclass ToTensor:\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\n\nclass RandomSampleCrop:\n    """"""Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    """"""\n\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float(\'-inf\')\n            if max_iou is None:\n                max_iou = float(\'inf\')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left + w), int(top + h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2], :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop\'s\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels\n\n\nclass Expand:\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, _ = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width * ratio - width)\n        top = random.uniform(0, height * ratio - height)\n\n        new_width = int(width * ratio)\n        new_height = int(height * ratio)\n        left = int(left)\n        top = int(top)\n        expand_image_cv = cv2.copyMakeBorder(\n            image,\n            top=top,\n            bottom=new_height - (top + height),\n            left=left,\n            right=new_width - (left + width),\n            borderType=cv2.BORDER_CONSTANT,\n            value=self.mean)\n        image = expand_image_cv\n\n        if boxes is not None:\n            boxes = boxes.copy()\n            boxes[:, :2] += (int(left), int(top))\n            boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass RandomMirror:\n    def __call__(self, image, boxes, classes):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass SwapChannels:\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort:\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),\n            ConvertColor(transform=\'HSV\'),\n            RandomSaturation(),\n            RandomHue(),\n            ConvertColor(current=\'HSV\', transform=\'BGR\'),\n            RandomContrast()\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n\n\nclass SSDAugmentation:\n    def __init__(self, size, mean, std, normalize_coef):\n        self.size = size\n        self.mean = mean\n        self.std = std\n        self.normalize_coef = normalize_coef\n        self.augment = Compose([\n            ToAbsoluteCoords(),\n            Expand(self.mean),\n            RandomSampleCrop(),\n            RandomMirror(),\n            ToPercentCoords(),\n            Resize(self.size),\n            ConvertFromInts(),\n            PhotometricDistort(),\n            Normalize(self.mean, self.std, self.normalize_coef)\n        ])\n\n    def __call__(self, img, boxes, labels):\n        return self.augment(img, boxes, labels)\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/datasets/__init__.py,0,"b""from .camvid import CamVid\nfrom .cityscapes import Cityscapes\nfrom .mapillary import Mapillary\n\n__all__ = ['CamVid', 'Cityscapes', 'Mapillary']\n"""
pytorch_toolkit/nncf/examples/semantic_segmentation/datasets/camvid.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nfrom collections import OrderedDict\nimport torch.utils.data as data\nimport examples.semantic_segmentation.utils.data as data_utils\n\n\nclass CamVid(data.Dataset):\n    """"""CamVid dataset loader where the dataset is arranged as in\n    https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid.\n\n\n    Keyword arguments:\n    - root_dir (``string``): Root directory path.\n    - mode (``string``): The type of dataset: \'train\' for training set, \'val\'\n    for validation set, and \'test\' for test set.\n    - transform (``callable``, optional): A function/transform that  takes in\n    an PIL image and returns a transformed version. Default: None.\n    - label_transform (``callable``, optional): A function/transform that takes\n    in the target and transforms it. Default: None.\n    - loader (``callable``, optional): A function to load an image given its\n    path. By default ``default_loader`` is used.\n\n    """"""\n    # Training dataset root folders\n    train_folder = \'train\'\n    train_lbl_folder = \'trainannot\'\n\n    # Validation dataset root folders\n    val_folder = \'val\'\n    val_lbl_folder = \'valannot\'\n\n    # Test dataset root folders\n    test_folder = \'test\'\n    test_lbl_folder = \'testannot\'\n\n    # Images extension\n    img_extension = \'.png\'\n\n    # Default encoding for pixel value, class name, and class color\n    color_encoding = OrderedDict([\n        (\'sky\', (128, 128, 128)),\n        (\'building\', (128, 0, 0)),\n        (\'pole\', (192, 192, 128)),\n        (\'road_marking\', (255, 69, 0)),\n        (\'road\', (128, 64, 128)),\n        (\'pavement\', (60, 40, 222)),\n        (\'tree\', (128, 128, 0)),\n        (\'sign_symbol\', (192, 128, 128)),\n        (\'fence\', (64, 64, 128)),\n        (\'car\', (64, 0, 128)),\n        (\'pedestrian\', (64, 64, 0)),\n        (\'bicyclist\', (0, 128, 192)),\n        (\'unlabeled\', (0, 0, 0))\n    ])\n\n    def __init__(self,\n                 root,\n                 image_set=\'train\',\n                 transforms=None,\n                 loader=data_utils.pil_loader):\n        self.root_dir = root\n        self.mode = image_set\n        self.transforms = transforms\n        self.loader = loader\n\n        if self.mode.lower() == \'train\':\n            # Get the training data and labels filepaths\n            self.train_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.train_folder),\n                extension_filter=self.img_extension)\n\n            self.train_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.train_lbl_folder),\n                extension_filter=self.img_extension)\n        elif self.mode.lower() == \'val\':\n            # Get the validation data and labels filepaths\n            self.val_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.val_folder),\n                extension_filter=self.img_extension)\n\n            self.val_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.val_lbl_folder),\n                extension_filter=self.img_extension)\n        elif self.mode.lower() == \'test\':\n            # Get the test data and labels filepaths\n            self.test_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.test_folder),\n                extension_filter=self.img_extension)\n\n            self.test_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.test_lbl_folder),\n                extension_filter=self.img_extension)\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n        - index (``int``): index of the item in the dataset\n\n        Returns:\n        A tuple of ``PIL.Image`` (image, label) where label is the ground-truth\n        of the image.\n\n        """"""\n        if self.mode.lower() == \'train\':\n            data_path, label_path = self.train_data[index], self.train_labels[\n                index]\n        elif self.mode.lower() == \'val\':\n            data_path, label_path = self.val_data[index], self.val_labels[\n                index]\n        elif self.mode.lower() == \'test\':\n            data_path, label_path = self.test_data[index], self.test_labels[\n                index]\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n        img, label = self.loader(data_path, label_path)\n\n        if self.transforms is not None:\n            img, label = self.transforms(img, label)\n\n        return img, label\n\n    def __len__(self):\n        """"""Returns the length of the dataset.""""""\n        if self.mode.lower() == \'train\':\n            return len(self.train_data)\n        if self.mode.lower() == \'val\':\n            return len(self.val_data)\n        if self.mode.lower() == \'test\':\n            return len(self.test_data)\n\n        raise RuntimeError(""Unexpected dataset mode. ""\n                           ""Supported modes are: train, val and test"")\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/datasets/cityscapes.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nfrom collections import OrderedDict\nimport torch.utils.data as data\nimport examples.semantic_segmentation.utils.data as data_utils\n\n\nclass Cityscapes(data.Dataset):\n    """"""Cityscapes dataset https://www.cityscapes-dataset.com/.\n\n    Keyword arguments:\n    - root_dir (``string``): Root directory path.\n    - mode (``string``): The type of dataset: \'train\' for training set, \'val\'\n    for validation set, and \'test\' for test set.\n    - transform (``callable``, optional): A function/transform that  takes in\n    an PIL image and returns a transformed version. Default: None.\n    - label_transform (``callable``, optional): A function/transform that takes\n    in the target and transforms it. Default: None.\n    - loader (``callable``, optional): A function to load an image given its\n    path. By default ``default_loader`` is used.\n\n    """"""\n    # Training dataset root folders\n    train_folder = ""leftImg8bit/train""\n    train_lbl_folder = ""gtFine/train""\n\n    # Validation dataset root folders\n    val_folder = ""leftImg8bit/val""\n    val_lbl_folder = ""gtFine/val""\n\n    # Test dataset root folders\n    test_folder = ""leftImg8bit/val""\n    test_lbl_folder = ""gtFine/val""\n\n    # Filters to find the images\n    img_extension = \'.png\'\n    lbl_name_filter = \'labelIds\'\n\n    # The values associated with the 35 classes\n    full_classes = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n                    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\n                    32, 33, -1)\n    # The values above are remapped to the following\n    new_classes = (0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 4, 5, 0, 0, 0, 6, 0, 7,\n                   8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 0, 17, 18, 19, 0)\n\n    # Default encoding for pixel value, class name, and class color\n    color_encoding = OrderedDict([\n        (\'unlabeled\', (0, 0, 0)),\n        (\'road\', (128, 64, 128)),\n        (\'sidewalk\', (244, 35, 232)),\n        (\'building\', (70, 70, 70)),\n        (\'wall\', (102, 102, 156)),\n        (\'fence\', (190, 153, 153)),\n        (\'pole\', (153, 153, 153)),\n        (\'traffic_light\', (250, 170, 30)),\n        (\'traffic_sign\', (220, 220, 0)),\n        (\'vegetation\', (107, 142, 35)),\n        (\'terrain\', (152, 251, 152)),\n        (\'sky\', (70, 130, 180)),\n        (\'person\', (220, 20, 60)),\n        (\'rider\', (255, 0, 0)),\n        (\'car\', (0, 0, 142)),\n        (\'truck\', (0, 0, 70)),\n        (\'bus\', (0, 60, 100)),\n        (\'train\', (0, 80, 100)),\n        (\'motorcycle\', (0, 0, 230)),\n        (\'bicycle\', (119, 11, 32))\n    ])\n\n    def __init__(self,\n                 root,\n                 image_set=\'train\',\n                 transforms=None,\n                 loader=data_utils.pil_loader):\n        self.root_dir = root\n        self.mode = image_set\n        self.transforms = transforms\n        self.loader = loader\n\n        if self.mode.lower() == \'train\':\n            # Get the training data and labels filepaths\n            self.train_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.train_folder),\n                extension_filter=self.img_extension)\n\n            self.train_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.train_lbl_folder),\n                name_filter=self.lbl_name_filter,\n                extension_filter=self.img_extension)\n        elif self.mode.lower() == \'val\':\n            # Get the validation data and labels filepaths\n            self.val_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.val_folder),\n                extension_filter=self.img_extension)\n\n            self.val_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.val_lbl_folder),\n                name_filter=self.lbl_name_filter,\n                extension_filter=self.img_extension)\n        elif self.mode.lower() == \'test\':\n            # Get the test data and labels filepaths\n            self.test_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.test_folder),\n                extension_filter=self.img_extension)\n\n            self.test_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.test_lbl_folder),\n                name_filter=self.lbl_name_filter,\n                extension_filter=self.img_extension)\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n        - index (``int``): index of the item in the dataset\n\n        Returns:\n        A tuple of ``PIL.Image`` (image, label) where label is the ground-truth\n        of the image.\n\n        """"""\n        if self.mode.lower() == \'train\':\n            data_path, label_path = self.train_data[index], self.train_labels[\n                index]\n        elif self.mode.lower() == \'val\':\n            data_path, label_path = self.val_data[index], self.val_labels[\n                index]\n        elif self.mode.lower() == \'test\':\n            data_path, label_path = self.test_data[index], self.test_labels[\n                index]\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n        img, label = self.loader(data_path, label_path)\n\n        # Remap class labels\n        label = data_utils.remap(label, self.full_classes, self.new_classes)\n\n        if self.transforms is not None:\n            img, label = self.transforms(img, label)\n\n        return img, label\n\n    def __len__(self):\n        """"""Returns the length of the dataset.""""""\n        if self.mode.lower() == \'train\':\n            return len(self.train_data)\n        if self.mode.lower() == \'val\':\n            return len(self.val_data)\n        if self.mode.lower() == \'test\':\n            return len(self.test_data)\n\n        raise RuntimeError(""Unexpected dataset mode. ""\n                           ""Supported modes are: train, val and test"")\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/datasets/mapillary.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nfrom collections import OrderedDict\nimport torch.utils.data as data\nimport examples.semantic_segmentation.utils.data as data_utils\n\n\nclass Mapillary(data.Dataset):\n    """"""Mapillary Vistas dataset\n\n    Keyword arguments:\n    - root (``string``): Root directory path.\n    - image_set (``string``): The type of dataset: \'train\' for training set, \'val\'\n    for validation set, and \'test\' for test set.\n    - transforms (``callable``, optional): A function/transform that  takes in\n    an PIL image and corresponding labels\n    - loader (``callable``, optional): A function to load an image given its\n    path. By default ``default_loader`` is used.\n\n    """"""\n    # Training dataset root folders\n    train_folder = ""training/images""\n    train_lbl_folder = ""training/labels""\n\n    # Validation dataset root folders\n    val_folder = ""validation/images""\n    val_lbl_folder = ""validation/labels""\n\n    # Test dataset root folders\n    test_folder = ""validation/images""\n    test_lbl_folder = ""validation/labels""\n\n    # Filters to find the images and labels\n    img_extension = \'.jpg\'\n    label_extension = \'.png\'\n\n    # Default encoding for pixel value, class name, and class color\n    # For Mapillary, the color values for classes below are the same\n    # as for Cityscapes, although the class names are not the same.\n    # Reusing Cityscapes names here.\n    # Any label colors encountered in the ground-truth .png file that are\n    # not in the dict below will be read as \'unlabeled\'\n    color_encoding = OrderedDict([\n        (\'unlabeled\', (0, 0, 0)),\n        (\'road\', (128, 64, 128)),\n        (\'sidewalk\', (244, 35, 232)),\n        (\'building\', (70, 70, 70)),\n        (\'wall\', (102, 102, 156)),\n        (\'fence\', (190, 153, 153)),\n        (\'pole\', (153, 153, 153)),\n        (\'traffic_light\', (250, 170, 30)),\n        (\'traffic_sign\', (220, 220, 0)),\n        (\'vegetation\', (107, 142, 35)),\n        (\'terrain\', (152, 251, 152)),\n        (\'sky\', (70, 130, 180)),\n        (\'person\', (220, 20, 60)),\n        (\'rider\', (255, 0, 0)),\n        (\'car\', (0, 0, 142)),\n        (\'truck\', (0, 0, 70)),\n        (\'bus\', (0, 60, 100)),\n        (\'train\', (0, 80, 100)),\n        (\'motorcycle\', (0, 0, 230)),\n        (\'bicycle\', (119, 11, 32))\n    ])\n\n    def __init__(self,\n                 root,\n                 image_set=\'train\',\n                 transforms=None,\n                 loader=data_utils.pil_loader):\n        self.root_dir = root\n        self.mode = image_set\n        self.transforms = transforms\n        self.loader = loader\n\n        if self.mode.lower() == \'train\':\n            # Get the training data and labels filepaths\n            self.train_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.train_folder),\n                extension_filter=self.img_extension)\n\n            self.train_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.train_lbl_folder),\n                extension_filter=self.label_extension)\n        elif self.mode.lower() == \'val\':\n            # Get the validation data and labels filepaths\n            self.val_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.val_folder),\n                extension_filter=self.img_extension)\n\n            self.val_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.val_lbl_folder),\n                extension_filter=self.label_extension)\n        elif self.mode.lower() == \'test\':\n            # Get the test data and labels filepaths\n            self.test_data = data_utils.get_files(\n                os.path.join(self.root_dir, self.test_folder),\n                extension_filter=self.img_extension)\n\n            self.test_labels = data_utils.get_files(\n                os.path.join(self.root_dir, self.test_lbl_folder),\n                extension_filter=self.label_extension)\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n        - index (``int``): index of the item in the dataset\n\n        Returns:\n        A tuple of ``PIL.Image`` (image, label) where label is the ground-truth\n        of the image.\n\n        """"""\n        if self.mode.lower() == \'train\':\n            data_path, label_path = self.train_data[index], self.train_labels[\n                index]\n        elif self.mode.lower() == \'val\':\n            data_path, label_path = self.val_data[index], self.val_labels[\n                index]\n        elif self.mode.lower() == \'test\':\n            data_path, label_path = self.test_data[index], self.test_labels[\n                index]\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n        img, color_labels = self.loader(data_path, label_path)\n        label = data_utils.color_to_label(color_labels, self.color_encoding)\n\n        if self.transforms is not None:\n            img, label = self.transforms(img, label)\n\n        return img, label\n\n    def __len__(self):\n        """"""Returns the length of the dataset.""""""\n        if self.mode.lower() == \'train\':\n            return len(self.train_data)\n        if self.mode.lower() == \'val\':\n            return len(self.val_data)\n        if self.mode.lower() == \'test\':\n            return len(self.test_data)\n\n        raise RuntimeError(""Unexpected dataset mode. ""\n                           ""Supported modes are: train, val and test"")\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/metric/__init__.py,0,"b""from .confusionmatrix import ConfusionMatrix\nfrom .iou import IoU\nfrom .metric import Metric\n\n__all__ = ['ConfusionMatrix', 'IoU', 'Metric']\n"""
pytorch_toolkit/nncf/examples/semantic_segmentation/metric/confusionmatrix.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nimport torch\nfrom .metric import Metric\n\n\nclass ConfusionMatrix(Metric):\n    """"""Constructs a confusion matrix for a multi-class classification problems.\n\n    Does not support multi-label, multi-class problems.\n\n    Keyword arguments:\n    - num_classes (int): number of classes in the classification problem.\n    - normalized (boolean, optional): Determines whether or not the confusion\n    matrix is normalized or not. Default: False.\n\n    Modified from: https://github.com/pytorch/tnt/blob/master/torchnet/meter/confusionmeter.py\n    """"""\n\n    def __init__(self, num_classes, normalized=False):\n        super().__init__()\n\n        self.conf = np.ndarray((num_classes, num_classes), dtype=np.int32)\n        self.normalized = normalized\n        self.num_classes = num_classes\n        self.reset()\n\n    def reset(self):\n        self.conf.fill(0)\n\n    def add(self, predicted, target):\n        """"""Computes the confusion matrix\n\n        The shape of the confusion matrix is K x K, where K is the number\n        of classes.\n\n        Keyword arguments:\n        - predicted (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        predicted scores obtained from the model for N examples and K classes,\n        or an N-tensor/array of integer values between 0 and K-1.\n        - target (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        ground-truth classes for N examples and K classes, or an N-tensor/array\n        of integer values between 0 and K-1.\n\n        """"""\n        # If target and/or predicted are tensors, convert them to numpy arrays\n        if torch.is_tensor(predicted):\n            predicted = predicted.cpu().numpy()\n        if torch.is_tensor(target):\n            target = target.cpu().numpy()\n\n        assert predicted.shape[0] == target.shape[0], \\\n            \'number of targets and predicted outputs do not match\'\n\n        if np.ndim(predicted) != 1:\n            assert predicted.shape[1] == self.num_classes, \\\n                \'number of predictions does not match size of confusion matrix\'\n            predicted = np.argmax(predicted, 1)\n        else:\n            assert (predicted.max() < self.num_classes) and (predicted.min() >= 0), \\\n                \'predicted values are not between 0 and k-1\'\n\n        if np.ndim(target) != 1:\n            assert target.shape[1] == self.num_classes, \\\n                \'Onehot target does not match size of confusion matrix\'\n            assert (target >= 0).all() and (target <= 1).all(), \\\n                \'in one-hot encoding, target values should be 0 or 1\'\n            assert (target.sum(1) == 1).all(), \\\n                \'multi-label setting is not supported\'\n            target = np.argmax(target, 1)\n\n        # Ignore out-of-bounds target labels\n        valid_indices = np.where((target >= 0) & (target < self.num_classes))\n        target = target[valid_indices]\n        predicted = predicted[valid_indices]\n\n        # hack for bincounting 2 arrays together\n        x = predicted + self.num_classes * target\n        bincount_2d = np.bincount(x.astype(np.int32), minlength=self.num_classes**2)\n\n        # See Pylint issue #2721\n        # pylint: disable=no-member\n        assert bincount_2d.size == self.num_classes**2\n        conf = bincount_2d.reshape((self.num_classes, self.num_classes))\n\n        self.conf += conf\n\n    def value(self):\n        """"""\n        Returns:\n            Confustion matrix of K rows and K columns, where rows corresponds\n            to ground-truth targets and columns corresponds to predicted\n            targets.\n        """"""\n        if self.normalized:\n            conf = self.conf.astype(np.float32)\n            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n\n        return self.conf\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/metric/iou.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\nfrom .metric import Metric\nfrom .confusionmatrix import ConfusionMatrix\n\n\nclass IoU(Metric):\n    """"""Computes the intersection over union (IoU) per class and corresponding\n    mean (mIoU).\n\n    Intersection over union (IoU) is a common evaluation metric for semantic\n    segmentation. The predictions are first accumulated in a confusion matrix\n    and the IoU is computed from it as follows:\n\n        IoU = true_positive / (true_positive + false_positive + false_negative).\n\n    Keyword arguments:\n    - num_classes (int): number of classes in the classification problem\n    - normalized (boolean, optional): Determines whether or not the confusion\n    matrix is normalized or not. Default: False.\n    - ignore_index (int or iterable, optional): Index of the classes to ignore\n    when computing the IoU. Can be an int, or any iterable of ints.\n    """"""\n\n    def __init__(self, num_classes, normalized=False, ignore_index=None):\n        super().__init__()\n        self.conf_metric = ConfusionMatrix(num_classes, normalized)\n\n        if ignore_index is None:\n            self.ignore_index = None\n        elif isinstance(ignore_index, int):\n            self.ignore_index = (ignore_index,)\n        else:\n            try:\n                self.ignore_index = tuple(ignore_index)\n            except TypeError:\n                raise ValueError(""\'ignore_index\' must be an int or iterable"")\n\n    def reset(self):\n        self.conf_metric.reset()\n\n    def add(self, predicted, target):\n        """"""Adds the predicted and target pair to the IoU metric.\n\n        Keyword arguments:\n        - predicted (Tensor): Can be a (N, K, H, W) tensor of\n        predicted scores obtained from the model for N examples and K classes,\n        or (N, H, W) tensor of integer values between 0 and K-1.\n        - target (Tensor): Can be a (N, K, H, W) tensor of\n        target scores for N examples and K classes, or (N, H, W) tensor of\n        integer values between 0 and K-1.\n\n        """"""\n        # Dimensions check\n        assert predicted.size(0) == target.size(0), \\\n            \'number of targets and predicted outputs do not match\'\n        assert predicted.dim() == 3 or predicted.dim() == 4, \\\n            ""predictions must be of dimension (N, H, W) or (N, K, H, W)""\n        assert target.dim() == 3 or target.dim() == 4, \\\n            ""targets must be of dimension (N, H, W) or (N, K, H, W)""\n\n        # If the tensor is in categorical format convert it to integer format\n        if predicted.dim() == 4:\n            _, predicted = predicted.max(1)\n        if target.dim() == 4:\n            _, target = target.max(1)\n\n        self.conf_metric.add(predicted.view(-1), target.view(-1))\n\n    def value(self):\n        """"""Computes the IoU and mean IoU.\n\n        The mean computation ignores NaN elements of the IoU array.\n\n        Returns:\n            Tuple: (IoU, mIoU). The first output is the per class IoU,\n            for K classes it\'s numpy.ndarray with K elements. The second output,\n            is the mean IoU.\n        """"""\n        conf_matrix = self.conf_metric.value()\n        if self.ignore_index is not None:\n            for _ in self.ignore_index:\n                conf_matrix[:, self.ignore_index] = 0\n                conf_matrix[self.ignore_index, :] = 0\n        true_positive = np.diag(conf_matrix)\n        false_positive = np.sum(conf_matrix, 0) - true_positive\n        false_negative = np.sum(conf_matrix, 1) - true_positive\n\n        # Just in case we get a division by 0, ignore/hide the error\n        with np.errstate(divide=\'ignore\', invalid=\'ignore\'):\n            iou = true_positive / (true_positive + false_positive + false_negative)\n\n        if self.ignore_index is not None:\n            iou_valid_cls = np.delete(iou, self.ignore_index)\n            miou = np.nanmean(iou_valid_cls)\n        else:\n            miou = np.nanmean(iou)\n        return iou, miou\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/metric/metric.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nclass Metric:\n    """"""Base class for all metrics.\n\n    From: https://github.com/pytorch/tnt/blob/master/torchnet/meter/meter.py\n    """"""\n    def reset(self):\n        pass\n\n    def add(self):\n        pass\n\n    def value(self):\n        pass\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/utils/checkpoint.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\n\nimport torch\n\nfrom nncf.checkpoint_loading import load_state\n\n\ndef save_checkpoint(model, optimizer, epoch, miou, compression_scheduler, config):\n    """"""Saves the model in a specified directory with a specified name.save\n\n    Keyword arguments:\n    - model (``nn.Module``): The model to save.\n    - optimizer (``torch.optim``): The optimizer state to save.\n    - epoch (``int``): The current epoch for the model.\n    - miou (``float``): The mean IoU obtained by the model.\n    - compression_scheduler: The compression scheduler associated with the model\n    - config: Model config"".\n\n    Returns:\n        The path to the saved checkpoint.\n    """"""\n    name = config.name\n    save_dir = config.checkpoint_save_dir\n\n    assert os.path.isdir(\n        save_dir), ""The directory \\""{0}\\"" doesn\'t exist."".format(save_dir)\n\n    # Save model\n    checkpoint_path = os.path.join(save_dir, name) + ""_last.pth""\n\n    checkpoint = {\n        \'epoch\': epoch,\n        \'miou\': miou,\n        \'state_dict\': model.state_dict(),\n        \'optimizer\': optimizer.state_dict(),\n        \'scheduler\': compression_scheduler.state_dict()\n    }\n    torch.save(checkpoint, checkpoint_path)\n    return checkpoint_path\n\n\ndef load_checkpoint(model, model_path, device_name, optimizer=None, compression_scheduler=None):\n    """"""Loads the model from a specified directory with a specified name\n\n    Keyword arguments:\n    - model (``nn.Module``): The stored model state is copied to this model\n    instance.\n    - model_path: The model filename.\n    - device_name: Device name for the model to be loaded into.\n    - is_ddp: If true, model will be treated as a DistributedDataParallel instance\n              and the actual model will be loaded into model.module\n    - optimizer (``torch.optim``): The stored optimizer state is copied to this\n    optimizer instance.\n    - compression_ctrl: The compression scheduler for the saved state\n                        to be loaded into\n\n    Returns:\n    The ``model``, ``optimizer``, epoch, mean IoU and ``compression_scheduler``, loaded from the\n    checkpoint.\n\n    """"""\n    assert os.path.isfile(\n        model_path), ""The model file \\""{0}\\"" doesn\'t exist."".format(model_path)\n\n    # Load the stored model parameters to the model instance\n    checkpoint = torch.load(model_path, map_location=device_name)\n    load_state(model, checkpoint[\'state_dict\'], is_resume=True)\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n    epoch = checkpoint[\'epoch\']\n    miou = checkpoint[\'miou\']\n\n    if ""scheduler"" in checkpoint and compression_scheduler is not None:\n        compression_scheduler.load_state_dict(checkpoint[\'scheduler\'])\n\n    return model, optimizer, epoch, miou, compression_scheduler\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/utils/data.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport os\nfrom collections import OrderedDict\nfrom tqdm import tqdm\n\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torchvision\nfrom torchvision.transforms import ToPILImage\nimport torchvision.transforms as T\n\n\ndef get_files(folder, name_filter=None, extension_filter=None):\n    """"""Helper function that returns the list of files in a specified folder\n    with a specified extension.\n\n    Keyword arguments:\n    - folder (``string``): The path to a folder.\n    - name_filter (```string``, optional): The returned files must contain\n    this substring in their filename. Default: None; files are not filtered.\n    - extension_filter (``string``, optional): The desired file extension.\n    Default: None; files are not filtered\n\n    """"""\n    if not os.path.isdir(folder):\n        raise RuntimeError(""\\""{0}\\"" is not a folder."".format(folder))\n\n    # Filename filter: if not specified don\'t filter (condition always true);\n    # otherwise, use a lambda expression to filter out files that do not\n    # contain ""name_filter""\n    if name_filter is None:\n        # This looks hackish...there is probably a better way\n        name_cond = lambda filename: True\n    else:\n        name_cond = lambda filename: name_filter in filename\n\n    # Extension filter: if not specified don\'t filter (condition always true);\n    # otherwise, use a lambda expression to filter out files whose extension\n    # is not ""extension_filter""\n    if extension_filter is None:\n        # This looks hackish...there is probably a better way\n        ext_cond = lambda filename: True\n    else:\n        ext_cond = lambda filename: filename.endswith(extension_filter)\n\n    filtered_files = []\n\n    # Explore the directory tree to get files that contain ""name_filter"" and\n    # with extension ""extension_filter""\n    for path, _, files in os.walk(folder):\n        files.sort()\n        for file in files:\n            if name_cond(file) and ext_cond(file):\n                full_path = os.path.join(path, file)\n                filtered_files.append(full_path)\n\n    return filtered_files\n\n\ndef pil_loader(data_path, label_path):\n    """"""Loads a sample and label image given their path as PIL images.\n\n    Keyword arguments:\n    - data_path (``string``): The filepath to the image.\n    - label_path (``string``): The filepath to the ground-truth image.\n\n    Returns the image and the label as PIL images.\n\n    """"""\n    data = Image.open(data_path)\n    label = Image.open(label_path)\n\n    return data, label\n\n\ndef remap(image, old_values, new_values):\n    assert isinstance(image, (Image.Image, np.ndarray)), ""image must be of type PIL.Image or numpy.ndarray""\n    assert isinstance(new_values, tuple), ""new_values must be of type tuple""\n    assert isinstance(old_values, tuple), ""old_values must be of type tuple""\n    assert len(new_values) == len(\n        old_values), ""new_values and old_values must have the same length""\n\n    # If image is a PIL.Image convert it to a numpy array\n    if isinstance(image, Image.Image):\n        image = np.array(image)\n\n    # Replace old values by the new ones\n    tmp = np.zeros_like(image)\n    for old, new in zip(old_values, new_values):\n        # Since tmp is already initialized as zeros we can skip new values\n        # equal to 0\n        if new != 0:\n            # See Pylint issue #2721\n            # pylint: disable=unsupported-assignment-operation\n            tmp[image == old] = new\n\n    return Image.fromarray(tmp)\n\n\ndef enet_weighing(dataloader, num_classes, c=1.02):\n    """"""Computes class weights as described in the ENet paper:\n\n        w_class = 1 / (ln(c + p_class)),\n\n    where c is usually 1.02 and p_class is the propensity score of that\n    class:\n\n        propensity_score = freq_class / total_pixels.\n\n    References: https://arxiv.org/abs/1606.02147\n\n    Keyword arguments:\n    - dataloader (``data.Dataloader``): A data loader to iterate over the\n    dataset.\n    - num_classes (``int``): The number of classes.\n    - c (``int``, optional): AN additional hyper-parameter which restricts\n    the interval of values for the weights. Default: 1.02.\n\n    """"""\n    class_count = np.zeros(num_classes)\n    total = np.zeros(num_classes)\n    for _, label in tqdm(dataloader):\n        label = label.cpu().numpy()\n\n        # Flatten label\n        flat_label = label.flatten()\n\n        # Ignore out-of-bounds target labels\n        valid_indices = np.where((flat_label >= 0) & (flat_label < num_classes))\n        flat_label = flat_label[valid_indices]\n\n        # Sum up the number of pixels of each class and the total pixel\n        # counts for each label\n        class_count += np.bincount(flat_label, minlength=num_classes)\n        total += flat_label.size\n\n    # Compute propensity score and then the weights for each class\n    propensity_score = class_count / total\n    class_weights = 1 / (np.log(c + propensity_score))\n\n    return class_weights\n\n\ndef median_freq_balancing(dataloader, num_classes):\n    """"""Computes class weights using median frequency balancing as described\n    in https://arxiv.org/abs/1411.4734:\n\n        w_class = median_freq / freq_class,\n\n    where freq_class is the number of pixels of a given class divided by\n    the total number of pixels in images where that class is present, and\n    median_freq is the median of freq_class.\n\n    Keyword arguments:\n    - dataloader (``data.Dataloader``): A data loader to iterate over the\n    dataset.\n    whose weights are going to be computed.\n    - num_classes (``int``): The number of classes\n\n    """"""\n    class_count = np.zeros(num_classes)\n    total = np.zeros(num_classes)\n    for _, label in tqdm(dataloader):\n        label = label.cpu().numpy()\n\n        # Flatten label\n        flat_label = label.flatten()\n\n        # Ignore out-of-bounds target labels\n        valid_indices = np.where((flat_label >= 0) & (flat_label < num_classes))\n        flat_label = flat_label[valid_indices]\n\n        # Sum up the class frequencies\n        bincount = np.bincount(flat_label, minlength=num_classes)\n\n        # Create of mask of classes that exist in the label\n        mask = bincount > 0\n        # Multiply the mask by the pixel count. The resulting array has\n        # one element for each class. The value is either 0 (if the class\n        # does not exist in the label) or equal to the pixel count (if\n        # the class exists in the label)\n        total += mask * flat_label.size\n\n        # Sum up the number of pixels found for each class\n        class_count += bincount\n\n    # Compute the frequency and its median\n\n    freq = class_count / total\n    freq = np.nan_to_num(freq)  # Guard against 0/0 divisions\n    med = np.median(freq)\n    class_weights = np.nan_to_num(med / freq)\n\n    return class_weights\n\n\nclass LongTensorToRGBPIL:\n    """"""Converts a ``torch.LongTensor`` to a ``PIL image``.\n\n    The input is a ``torch.LongTensor`` where each pixel\'s value identifies the\n    class.\n\n    Keyword arguments:\n    - rgb_encoding (``OrderedDict``): An ``OrderedDict`` that relates pixel\n    values, class names, and class colors.\n\n    """"""\n    def __init__(self, rgb_encoding):\n        self.rgb_encoding = rgb_encoding\n\n    def __call__(self, tensor):\n        """"""Performs the conversion from ``torch.LongTensor`` to a ``PIL image``\n\n        Keyword arguments:\n        - tensor (``torch.LongTensor``): the tensor to convert\n\n        Returns:\n        A ``PIL.Image``.\n\n        """"""\n        # Check if label_tensor is a LongTensor\n        if not isinstance(tensor, torch.LongTensor):\n            raise TypeError(""label_tensor should be torch.LongTensor. Got {}""\n                            .format(type(tensor)))\n        # Check if encoding is a ordered dictionary\n        if not isinstance(self.rgb_encoding, OrderedDict):\n            raise TypeError(""encoding should be an OrderedDict. Got {}"".format(\n                type(self.rgb_encoding)))\n\n        # label_tensor might be an image without a channel dimension, in this\n        # case unsqueeze it\n        if len(tensor.size()) == 2:\n            tensor.unsqueeze_(0)\n\n        color_tensor = torch.ByteTensor(3, tensor.size(1), tensor.size(2)).fill_(0)\n\n        for index, (_, color) in enumerate(self.rgb_encoding.items()):\n            # Get a mask of elements equal to index\n            mask = torch.eq(tensor, index).squeeze_()\n            # Fill color_tensor with corresponding colors\n            for channel, color_value in enumerate(color):\n                color_tensor[channel].masked_fill_(mask, color_value)\n\n        return ToPILImage()(color_tensor)\n\n\n\ndef batch_transform(batch, transform):\n    """"""Applies a transform to a batch of samples.\n\n    Keyword arguments:\n    - batch (): a batch os samples\n    - transform (callable): A function/transform to apply to ``batch``\n\n    """"""\n\n    # Convert the single channel label to RGB in tensor form\n    # 1. torch.unbind removes the 0-dimension of ""labels"" and returns a tuple of\n    # all slices along that dimension\n    # 2. the transform is applied to each slice\n    transf_slices = [transform(tensor) for tensor in torch.unbind(batch)]\n\n    return torch.stack(transf_slices)\n\n\ndef imshow_batch(images, labels):\n    """"""Displays two grids of images. The top grid displays ``images``\n    and the bottom grid ``labels``\n\n    Keyword arguments:\n    - images (``Tensor``): a 4D mini-batch tensor of shape\n    (B, C, H, W)\n    - labels (``Tensor``): a 4D mini-batch tensor of shape\n    (B, C, H, W)\n\n    """"""\n\n    # Make a grid with the images and labels and convert it to numpy\n    images = torchvision.utils.make_grid(images).numpy()\n    labels = torchvision.utils.make_grid(labels).numpy()\n\n    import matplotlib.pyplot as plt\n    _, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 7))\n    ax1.imshow(np.transpose(images, (1, 2, 0)))\n    ax2.imshow(np.transpose(labels, (1, 2, 0)))\n\n    plt.show()\n\n\ndef label_to_color(label, class_encoding):\n    label_to_rgb = T.Compose([\n        LongTensorToRGBPIL(class_encoding),\n        T.ToTensor()\n    ])\n    return batch_transform(label.cpu(), label_to_rgb)\n\n\ndef color_to_label(color_labels: Image, class_encoding: OrderedDict):\n    color_labels = np.array(color_labels.convert(\'RGB\'))\n    # pylint: disable=unsubscriptable-object\n    labels = np.zeros((color_labels.shape[0], color_labels.shape[1]), dtype=np.int64)\n\n    red = color_labels[..., 0]\n    green = color_labels[..., 1]\n    blue = color_labels[..., 2]\n\n    for index, (_, color) in enumerate(class_encoding.items()):\n        target_area = (red == color[0]) & (green == color[1]) & (blue == color[2])\n        labels[target_area] = index\n\n    labels = Image.fromarray(labels.astype(np.uint8), mode=\'L\')\n    return labels\n\ndef show_ground_truth_vs_prediction(images, gt_labels, color_predictions, class_encoding):\n    """"""Displays three grids of images. The top grid displays ``images``\n        the middle grid - ``gt_labels`` and the bottom grid - ``labels``\n\n        Keyword arguments:\n        - images (``Tensor``): a 4D mini-batch tensor of shape\n        (B, C, H, W)\n        - gt_labels (``Tensor``): a 4D mini-batch tensor of shape\n        (B, C, H, W)\n        - labels (``Tensor``): a 4D mini-batch tensor of shape\n        (B, C, H, W)\n\n        """"""\n\n    import matplotlib.pyplot as plt\n    # Make a grid with the images and labels and convert it to numpy\n    images = torchvision.utils.make_grid(images).numpy()\n    color_predictions = torchvision.utils.make_grid(color_predictions).numpy()\n\n    color_gt_labels = label_to_color(gt_labels, class_encoding)\n    color_gt_labels = torchvision.utils.make_grid(color_gt_labels).numpy()\n\n    plt.subplots(2, 3, figsize=(15, 7))\n    ax1 = plt.subplot2grid((3, 3), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 3), (0, 1), rowspan=2)\n    ax3 = plt.subplot2grid((3, 3), (0, 2), rowspan=2)\n    ax4 = plt.subplot2grid((3, 3), (2, 0), colspan=3)\n\n    ax1.imshow(np.transpose(images, (1, 2, 0)))\n    ax2.imshow(np.transpose(color_gt_labels, (1, 2, 0)))\n    ax3.imshow(np.transpose(color_predictions, (1, 2, 0)))\n\n    color_names = [k for k, _ in class_encoding.items()]\n    colors = [(v[0] / 255.0, v[1] / 255.0, v[2] / 255.0, 1.0) for _, v in class_encoding.items()]\n    ones = np.ones(len(colors))\n    ax4.bar(range(0, len(ones)), ones, color=colors, tick_label=color_names)\n\n    plt.show()\n\n\ndef downsample_labels(labels, target_size=None, downsample_factor=None):\n    H = labels.size()[1]\n    W = labels.size()[2]\n    if target_size is None and downsample_factor is None:\n        raise ValueError(""Either target_size or downsample_factor must be specified"")\n    if target_size is not None and downsample_factor is not None:\n        raise ValueError(""Only one of the target_size and downsample_factor must be specified"")\n\n    if downsample_factor is None:\n        h = target_size[0]\n        w = target_size[1]\n    else:\n        h = H // downsample_factor\n        w = W // downsample_factor\n    ih = torch.linspace(0, H - 1, h).long()\n    iw = torch.linspace(0, W - 1, w).long()\n\n    return labels[:, ih[:, None], iw]\n\n\ndef cat_list(images, fill_value=0):\n    max_size = tuple(max(s) for s in zip(*[img.shape for img in images]))\n    batch_shape = (len(images),) + max_size\n    batched_imgs = images[0].new(*batch_shape).fill_(fill_value)\n    for img, pad_img in zip(images, batched_imgs):\n        pad_img[..., :img.shape[-2], :img.shape[-1]].copy_(img)\n    return batched_imgs\n\n\ndef collate_fn(batch):\n    images, targets = list(zip(*batch))\n    batched_imgs = cat_list(images, fill_value=0)\n    batched_targets = cat_list(targets, fill_value=255)\n    return batched_imgs, batched_targets\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/utils/loss_funcs.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision\n\nfrom examples.semantic_segmentation.utils.data import downsample_labels\nfrom examples.common.models.segmentation.unet import center_crop\n\n\ndef cross_entropy_aux(inputs: dict, target: torch.Tensor, weight: list):\n    # This criterion is only suitable for `inputs` produced by the models\n    # adopting the torchvision.models.segmentation._utils._SimpleSegmentationModel\n    # contract - `inputs` shall be dicts of feature maps with keys corresponding\n    # to the classifier type (either ""out"" for the main classifier or ""aux"" for\n    # the auxiliary one)\n    losses = {}\n    for name, x in inputs.items():\n        losses[name] = F.cross_entropy(x, target,\n                                       ignore_index=255,\n                                       weight=weight)\n\n    if len(losses) == 1:\n        return losses[\'out\']\n\n    return losses[\'out\'] + 0.5 * losses[\'aux\']\n\n\ndef cross_entropy(inputs, target: torch.Tensor, weight: list):\n    # This criterion will support both the usual models producing\n    # tensors as outputs and the torchvision models producing dict\n    # of tensors, but without taking aux loss into account.\n    input_tensor = None\n    if isinstance(inputs, dict):\n        input_tensor = inputs[\'out\']\n    else:\n        input_tensor = inputs\n\n    return F.cross_entropy(input_tensor, target,\n                           ignore_index=255,\n                           weight=weight)\n\n\ndef cross_entropy_icnet(inputs, target: torch.Tensor, weight: list):\n    losses = {}\n\n    if isinstance(inputs, dict):\n        # Training - received a dict with auxiliary classifier outputs which\n        # are downsampled relative to the ground-truth labels and input image\n        target_ds4 = downsample_labels(target, downsample_factor=4)\n        target_ds8 = downsample_labels(target, downsample_factor=8)\n        target_ds16 = downsample_labels(target, downsample_factor=16)\n\n        losses[\'ds4\'] = F.cross_entropy(inputs[\'ds4\'], target_ds4,\n                                        ignore_index=255,\n                                        weight=weight)\n        losses[\'ds8\'] = F.cross_entropy(inputs[\'ds8\'], target_ds8,\n                                        ignore_index=255,\n                                        weight=weight)\n        losses[\'ds16\'] = F.cross_entropy(inputs[\'ds16\'], target_ds16,\n                                         ignore_index=255,\n                                         weight=weight)\n\n        return losses[\'ds4\'] + 0.4 * losses[\'ds8\'] + 0.4 * losses[\'ds16\']\n\n    # Testing - received classifier outputs with the same resolution as\n    # the input image\n    return F.cross_entropy(inputs, target, ignore_index=255, weight=weight)\n\n\ndef do_model_specific_postprocessing(model_name, labels, model_outputs):\n    # pylint:disable=no-member\n    metric_outputs = model_outputs\n    if model_name == \'unet\':\n        # UNet predicts center image crops\n        outputs_size_hw = (model_outputs.size()[2], model_outputs.size()[3])\n        labels = center_crop(labels, outputs_size_hw).contiguous()\n        metric_outputs = model_outputs\n    elif model_name == \'icnet\':\n        if isinstance(model_outputs, dict):\n            # Training - received a dict with auxiliary classifier outputs which\n            # are downsampled relative to the ground-truth labels and input image\n            # Will only calculate metric for the highest-res (1/4 size)\n            # output, upscaled to 1x size\n            metric_outputs = F.interpolate(model_outputs[\'ds4\'], scale_factor=4)\n    elif model_name in torchvision.models.segmentation.__dict__:\n        # Torchvision segmentation models may output a dict of labels\n        if isinstance(model_outputs, dict):\n            metric_outputs = model_outputs[\'out\']\n    return labels, model_outputs, metric_outputs\n'"
pytorch_toolkit/nncf/examples/semantic_segmentation/utils/transforms.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport random\nimport math\n\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms as T\nfrom torchvision.transforms import functional as F\n\n\n\ndef pad_if_smaller(img, size, fill=0):\n    min_size = min(img.size)\n    if min_size < size:\n        ow, oh = img.size\n        padh = size - oh if oh < size else 0\n        padw = size - ow if ow < size else 0\n        img = F.pad(img, (0, 0, padw, padh), fill=fill)\n    return img\n\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\n\nclass RandomResize:\n    def __init__(self, min_size, max_size=None):\n        self.min_size = min_size\n        if max_size is None:\n            max_size = min_size\n        self.max_size = max_size\n\n    def __call__(self, image, target):\n        size = random.randint(self.min_size, self.max_size)\n        image = F.resize(image, size)\n        target = F.resize(target, size, interpolation=Image.NEAREST)\n        return image, target\n\n\nclass RandomScaleAligned:\n    def __init__(self, min_scale, max_scale, alignment):\n        self.min_scale = min_scale\n        self.max_scale = max_scale\n        self.alignment = alignment\n\n    def __call__(self, image, target):\n        w, h = image.size\n        scale = random.uniform(self.min_scale, self.max_scale)\n        w_aligned = math.ceil(w * scale / self.alignment) * self.alignment\n        h_aligned = math.ceil(h * scale / self.alignment) * self.alignment\n        image = F.resize(image, (w_aligned, h_aligned))\n        target = F.resize(target, (w_aligned, h_aligned), interpolation=Image.NEAREST)\n        return image, target\n\n\nclass Resize:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image, target):\n        image = F.resize(image, self.size)\n        target = F.resize(target, self.size, interpolation=Image.NEAREST)\n        return image, target\n\n\nclass RandomHorizontalFlip:\n    def __init__(self, flip_prob):\n        self.flip_prob = flip_prob\n\n    def __call__(self, image, target):\n        if random.random() < self.flip_prob:\n            image = F.hflip(image)\n            target = F.hflip(target)\n        return image, target\n\n\nclass RandomCrop:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image, target):\n        image = pad_if_smaller(image, self.size)\n        target = pad_if_smaller(target, self.size, fill=0)\n        crop_params = T.RandomCrop.get_params(image, (self.size, self.size))\n        image = F.crop(image, *crop_params)\n        target = F.crop(target, *crop_params)\n        return image, target\n\n\nclass RandomSizedCrop:\n    """"""Note: Preserves image aspect ratio. The resulting crop size will differ from\n    the original image size by a random factor in the interval [min_scale; 1.0].""""""\n\n    def __init__(self, min_scale):\n        self.min_scale = min_scale\n\n    def __call__(self, image, target):\n        w, h = image.size\n        scale = random.uniform(self.min_scale, 1.0)\n        crop_w = math.ceil(w * scale)\n        crop_h = math.ceil(h * scale)\n        crop_params = T.RandomCrop.get_params(image, (crop_h, crop_w))\n        image = F.crop(image, *crop_params)\n        target = F.crop(target, *crop_params)\n        return image, target\n\nclass CenterCrop:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image, target):\n        image = F.center_crop(image, self.size)\n        target = F.center_crop(target, self.size)\n        return image, target\n\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        target = torch.as_tensor(np.asarray(target), dtype=torch.int64)\n        return image, target\n\n\nclass Normalize:\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image, target):\n        image = F.normalize(image, mean=self.mean, std=self.std)\n        return image, target\n'"
pytorch_toolkit/nncf/nncf/binarization/cpu/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/binarization/cuda/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/pruning/filter_pruning/algo.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom typing import List\n\nimport torch\n\nfrom nncf.algo_selector import COMPRESSION_ALGORITHMS\nfrom nncf.compression_method_api import CompressionAlgorithmController\nfrom nncf.layers import NNCF_CONV_MODULES_DICT\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.pruning.base_algo import BasePruningAlgoBuilder, PrunedModuleInfo, BasePruningAlgoController\nfrom nncf.pruning.filter_pruning.functions import calculate_binary_mask, FILTER_IMPORTANCE_FUNCTIONS, \\\n    tensor_l2_normalizer\nfrom nncf.pruning.filter_pruning.layers import FilterPruningBlock, inplace_apply_filter_binary_mask\nfrom nncf.pruning.schedulers import PRUNING_SCHEDULERS\nfrom nncf.pruning.utils import get_rounded_pruned_element_number\n\nfrom nncf.nncf_logger import logger as nncf_logger\n\n\n@COMPRESSION_ALGORITHMS.register(\'filter_pruning\')\nclass FilterPruningBuilder(BasePruningAlgoBuilder):\n    def __init__(self, config):\n        super().__init__(config)\n        self._params = self.config.get(""params"", {})\n\n    def create_weight_pruning_operation(self, module):\n        return FilterPruningBlock(module.weight.size(0))\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n        return FilterPruningController(target_model,\n                                       self._pruned_module_info,\n                                       self._params)\n\n    def _is_pruned_module(self, module):\n        # Currently prune only Convolutions\n        return isinstance(module, tuple(NNCF_CONV_MODULES_DICT.keys()))\n\n    @staticmethod\n    def get_types_of_pruned_modules():\n        types = [str.lower(v.__name__) for v in NNCF_CONV_MODULES_DICT.values()]\n        return types\n\n\nclass FilterPruningController(BasePruningAlgoController):\n    def __init__(self, target_model: NNCFNetwork,\n                 pruned_module_info: List[PrunedModuleInfo],\n                 params: dict):\n        super().__init__(target_model, pruned_module_info, params)\n        self.frozen = False\n        self.pruning_rate = 0\n\n        self.weights_normalizer = tensor_l2_normalizer  # for all weights in common case\n        self.filter_importance = FILTER_IMPORTANCE_FUNCTIONS.get(params.get(\'weight_importance\', \'L2\'))\n        self.all_weights = params.get(""all_weights"", False)\n        scheduler_cls = PRUNING_SCHEDULERS.get(params.get(""schedule"", ""baseline""))\n        self._scheduler = scheduler_cls(self, params)\n\n    @staticmethod\n    def _get_mask(minfo: PrunedModuleInfo):\n        return minfo.operand.binary_filter_pruning_mask\n\n    def statistics(self):\n        stats = super().statistics()\n        stats[\'pruning_rate\'] = self.pruning_rate\n        return stats\n\n    def freeze(self):\n        self.frozen = True\n\n    def set_pruning_rate(self, pruning_rate):\n        self.pruning_rate = pruning_rate\n        if not self.frozen:\n            if self.all_weights:\n                self._set_binary_masks_for_all_pruned_modules()\n            else:\n                self._set_binary_masks_for_filters()\n            if self.zero_grad:\n                self.zero_grads_for_pruned_modules()\n        self._apply_masks()\n\n    def _set_binary_masks_for_filters(self):\n        nncf_logger.debug(""Setting new binary masks for pruned modules."")\n\n        with torch.no_grad():\n            for minfo in self.pruned_module_info:\n                pruning_module = minfo.operand\n                # 1. Calculate importance for all filters in all weights\n                # 2. Calculate thresholds for every weight\n                # 3. Set binary masks for filter\n                filters_importance = self.filter_importance(minfo.module.weight)\n                num_of_sparse_elems = get_rounded_pruned_element_number(filters_importance.size(0),\n                                                                        self.pruning_rate)\n                threshold = sorted(filters_importance)[num_of_sparse_elems]\n                pruning_module.binary_filter_pruning_mask = calculate_binary_mask(filters_importance, threshold)\n\n    def _set_binary_masks_for_all_pruned_modules(self):\n        nncf_logger.debug(""Setting new binary masks for all pruned modules together."")\n\n        normalized_weights = []\n        filter_importances = []\n        for minfo in self.pruned_module_info:\n            pruning_module = minfo.operand\n            # 1. Calculate importance for all filters in all weights\n            # 2. Calculate thresholds for every weight\n            # 3. Set binary masks for filter\n            normalized_weight = self.weights_normalizer(minfo.module.weight)\n            normalized_weights.append(normalized_weight)\n\n            filter_importances.append(self.filter_importance(normalized_weight))\n        importances = torch.cat(filter_importances)\n        threshold = sorted(importances)[int(self.pruning_rate * importances.size(0))]\n\n        for i, minfo in enumerate(self.pruned_module_info):\n            pruning_module = minfo.operand\n            pruning_module.binary_filter_pruning_mask = calculate_binary_mask(filter_importances[i], threshold)\n\n    def _apply_masks(self):\n        nncf_logger.debug(""Applying pruning binary masks"")\n\n        def _apply_binary_mask_to_module_weight_and_bias(module, mask, module_name=""""):\n            with torch.no_grad():\n                # Applying mask to weights\n                inplace_apply_filter_binary_mask(mask, module.weight, module_name)\n                # Applying mask to bias too (if exists)\n                if module.bias is not None:\n                    inplace_apply_filter_binary_mask(mask, module.bias, module_name)\n\n        for minfo in self.pruned_module_info:\n            _apply_binary_mask_to_module_weight_and_bias(minfo.module, minfo.operand.binary_filter_pruning_mask,\n                                                         minfo.module_name)\n\n            # Applying mask to the BatchNorm node\n            related_modules = minfo.related_modules\n            if minfo.related_modules is not None and PrunedModuleInfo.BN_MODULE_NAME in minfo.related_modules \\\n                    and related_modules[PrunedModuleInfo.BN_MODULE_NAME] is not None:\n                bn_module = related_modules[PrunedModuleInfo.BN_MODULE_NAME]\n                _apply_binary_mask_to_module_weight_and_bias(bn_module, minfo.operand.binary_filter_pruning_mask)\n\n    def export_model(self, filename, *args, **kwargs):\n        """"""\n        This function saving model without actually pruning the layers, just nullifies the necessary filters by mask.\n        """"""\n        self._apply_masks()\n        super().export_model(filename, *args, **kwargs)\n'"
pytorch_toolkit/nncf/nncf/pruning/filter_pruning/functions.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport torch\n\n\ndef l1_filter_norm(weight_tensor):\n    """"""\n    Calculates L1 for weight_tensor for the first dimension.\n    """"""\n    return torch.norm(weight_tensor.view(weight_tensor.shape[0], -1), p=1, dim=1)\n\n\ndef l2_filter_norm(weight_tensor):\n    """"""\n    Calculates L2 for weight_tensor for the first dimension.\n    """"""\n    return torch.norm(weight_tensor.view(weight_tensor.shape[0], -1), p=2, dim=1)\n\n\ndef tensor_l2_normalizer(weight_tensor):\n    norm = torch.sqrt(torch.sum(torch.abs(weight_tensor) ** 2))\n    return weight_tensor / norm\n\n\ndef geometric_median_filter_norm(weight_tensor):\n    """"""\n    Compute geometric median norm for filters.\n    :param weight_tensor: tensor with weights\n    :return: metric value for every weight from weights_tensor\n    """"""\n    filters_count = weight_tensor.size(0)\n    weight_vec = weight_tensor.view(filters_count, -1)\n    similar_matrix = torch.zeros((filters_count, filters_count))\n    pdist_fn = torch.nn.PairwiseDistance(p=2)\n    for i in range(filters_count):\n        for j in range(i, filters_count):\n            similar_matrix[i, j] = pdist_fn(weight_vec[None, i], weight_vec[None, j])[0].item()\n            similar_matrix[j, i] = similar_matrix[i, j]\n    similar_sum = similar_matrix.sum(axis=0)\n    return similar_sum\n\n\nFILTER_IMPORTANCE_FUNCTIONS = {\n    \'L2\': l2_filter_norm,\n    \'L1\': l1_filter_norm,\n    \'geometric_median\': geometric_median_filter_norm\n}\n\n\ndef calculate_binary_mask(importance, threshold):\n    return (importance >= threshold).float()\n'"
pytorch_toolkit/nncf/nncf/pruning/filter_pruning/layers.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom nncf.layer_utils import COMPRESSION_MODULES\nfrom nncf.utils import is_tracing_state, no_jit_trace\n\n\n@COMPRESSION_MODULES.register()\nclass FilterPruningBlock(nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.register_buffer(""_binary_filter_pruning_mask"", torch.ones(size))\n\n    @property\n    def binary_filter_pruning_mask(self):\n        return self._binary_filter_pruning_mask\n\n    @binary_filter_pruning_mask.setter\n    def binary_filter_pruning_mask(self, mask):\n        with torch.no_grad():\n            self._binary_filter_pruning_mask.set_(mask)\n\n    def forward(self, conv_weight):\n        if is_tracing_state():\n            with no_jit_trace():\n                return conv_weight\n        return conv_weight\n\n\ndef broadcast_filter_mask(filter_mask, shape):\n    broadcasted_shape = np.ones(len(shape), dtype=np.int64)\n    broadcasted_shape[0] = filter_mask.size(0)\n    broadcasted_filter_mask = torch.reshape(filter_mask, tuple(broadcasted_shape))\n    return broadcasted_filter_mask\n\n\ndef inplace_apply_filter_binary_mask(filter_mask, conv_weight, module_name=""""):\n    """"""\n    Inplace applying binary filter mask to weight (or bias) of the convolution\n    (by first dim of the conv weight).\n    :param filter_mask: binary mask (should have the same shape as first dim of conv weight)\n    :param conv_weight: weight or bias of convolution\n    :return: result with applied mask\n    """"""\n    if filter_mask.size(0) != conv_weight.size(0):\n        raise RuntimeError(""Shape of mask = {} for module {} isn\'t broadcastable to weight shape={}.""\n                           "" "".format(filter_mask.shape, module_name, conv_weight.shape))\n    broadcasted_filter_mask = broadcast_filter_mask(filter_mask, conv_weight.shape)\n    return conv_weight.mul_(broadcasted_filter_mask)\n\n\ndef apply_filter_binary_mask(filter_mask, conv_weight, module_name=""""):\n    """"""\n    Applying binary filter mask to weight (or bias) of the convolution (applying by first dim of the conv weight)\n    without changing the weight.\n    :param filter_mask: binary mask (should have the same shape as first dim of conv weight)\n    :param conv_weight: weight or bias of convolution\n    :return: result with applied mask\n    """"""\n    if filter_mask.size(0) != conv_weight.size(0):\n        raise RuntimeError(""Shape of mask = {} for module {} isn\'t broadcastable to weight shape={}.""\n                           "" "".format(filter_mask.shape, module_name, conv_weight.shape))\n    broadcasted_filter_mask = broadcast_filter_mask(filter_mask, conv_weight.shape)\n    return broadcasted_filter_mask * conv_weight\n'"
pytorch_toolkit/nncf/nncf/quantization/cpu/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/quantization/cuda/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/sparsity/const/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/sparsity/const/algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom nncf.compression_method_api import CompressionAlgorithmController\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.sparsity.layers import BinaryMask\nfrom nncf.sparsity.base_algo import BaseSparsityAlgoBuilder, BaseSparsityAlgoController\nfrom nncf.algo_selector import COMPRESSION_ALGORITHMS\n\n\n@COMPRESSION_ALGORITHMS.register(\'const_sparsity\')\nclass ConstSparsityBuilder(BaseSparsityAlgoBuilder):\n    def create_weight_sparsifying_operation(self, module):\n        return BinaryMask(module.weight.size())\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n        return ConstSparsityController(target_model, self._sparsified_module_info)\n\n\nclass ConstSparsityController(BaseSparsityAlgoController):\n    def freeze(self):\n        pass\n\n    def set_sparsity_level(self, sparsity_level):\n        pass\n'"
pytorch_toolkit/nncf/nncf/sparsity/magnitude/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/sparsity/magnitude/algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\nfrom typing import List\n\nimport torch\n\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.compression_method_api import CompressionAlgorithmController\nfrom nncf.sparsity.magnitude.functions import WEIGHT_IMPORTANCE_FUNCTIONS, calc_magnitude_binary_mask\nfrom nncf.sparsity.layers import BinaryMask\nfrom nncf.sparsity.base_algo import BaseSparsityAlgoBuilder, BaseSparsityAlgoController, SparseModuleInfo\nfrom nncf.sparsity.schedulers import SPARSITY_SCHEDULERS\nfrom nncf.algo_selector import COMPRESSION_ALGORITHMS\n\n\n@COMPRESSION_ALGORITHMS.register(\'magnitude_sparsity\')\nclass MagnitudeSparsityBuilder(BaseSparsityAlgoBuilder):\n    def __init__(self, config):\n        super().__init__(config)\n        self._params = self.config.get(""params"", {})\n\n    def create_weight_sparsifying_operation(self, module):\n        return BinaryMask(module.weight.size())\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n\n        return MagnitudeSparsityController(target_model, self._sparsified_module_info,\n                                           self._params,\n                                           self.config.get(\'weight_importance\', \'normed_abs\'))\n\n\nclass MagnitudeSparsityController(BaseSparsityAlgoController):\n    def __init__(self, target_model: NNCFNetwork,\n                 sparsified_module_info: List[SparseModuleInfo],\n                 params, weight_importance: str):\n        super().__init__(target_model, sparsified_module_info)\n        self.sparsity_level = self.threshold = 0\n        self.weight_importance = WEIGHT_IMPORTANCE_FUNCTIONS.get(weight_importance)\n        scheduler_cls = SPARSITY_SCHEDULERS.get(params.get(""schedule"", ""polynomial""))\n        self._scheduler = scheduler_cls(self, params)\n\n    def statistics(self):\n        stats = super().statistics()\n        stats[\'sparsity_threshold\'] = self.threshold\n        stats[\'sparsity_level\'] = self.sparsity_level\n        return stats\n\n    def freeze(self):\n        pass\n\n    def set_sparsity_level(self, sparsity_level):\n        if sparsity_level >= 1 or sparsity_level < 0:\n            raise AttributeError(\n                \'Sparsity level should be within interval [0,1), actual value to set is: {}\'.format(sparsity_level))\n        self.sparsity_level = sparsity_level\n\n        self.threshold = self._select_threshold()\n        self._set_masks_for_threshold(self.threshold)\n\n    def _select_threshold(self):\n        all_weights = self._collect_all_weights()\n        if not all_weights:\n            return 0.0\n        all_weights_tensor, _ = torch.cat(all_weights).sort()\n        threshold = all_weights_tensor[int(all_weights_tensor.size(0) * self.sparsity_level)].item()\n        return threshold\n\n    def _set_masks_for_threshold(self, threshold_val):\n        for layer in self.sparsified_module_info:\n            layer.operand.binary_mask = calc_magnitude_binary_mask(layer.module.weight,\n                                                                   self.weight_importance,\n                                                                   threshold_val)\n\n    def _collect_all_weights(self):\n        all_weights = []\n        for minfo in self.sparsified_module_info:\n            all_weights.append(self.weight_importance(minfo.module.weight).view(-1))\n        return all_weights\n\n    def create_weight_sparsifying_operation(self, module):\n        return BinaryMask(module.weight.size())\n'"
pytorch_toolkit/nncf/nncf/sparsity/magnitude/functions.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\n\ndef abs_magnitude(weight):\n    return torch.abs(weight)\n\n\ndef normed_magnitude(weight):\n    return torch.abs(weight) / weight.norm(2)\n\n\nWEIGHT_IMPORTANCE_FUNCTIONS = {\n    \'abs\': abs_magnitude,\n    \'normed_abs\': normed_magnitude\n}\n\n\ndef calc_magnitude_binary_mask(weight, weight_importance, threshold):\n    return (weight_importance(weight) > threshold).float()\n'"
pytorch_toolkit/nncf/nncf/sparsity/rb/__init__.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n'"
pytorch_toolkit/nncf/nncf/sparsity/rb/algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom typing import List\n\nimport torch\nimport torch.distributed as dist\n\nfrom nncf.utils import get_world_size\nfrom nncf.nncf_network import NNCFNetwork\nfrom nncf.compression_method_api import CompressionAlgorithmController\nfrom nncf.sparsity.rb.layers import RBSparsifyingWeight\nfrom nncf.sparsity.rb.loss import SparseLoss\nfrom nncf.sparsity.base_algo import BaseSparsityAlgoBuilder, BaseSparsityAlgoController, SparseModuleInfo\nfrom nncf.sparsity.schedulers import SPARSITY_SCHEDULERS\nfrom nncf.algo_selector import COMPRESSION_ALGORITHMS\n\n\n@COMPRESSION_ALGORITHMS.register(\'rb_sparsity\')\nclass RBSparsityBuilder(BaseSparsityAlgoBuilder):\n    def apply_to(self, target_model: NNCFNetwork) -> NNCFNetwork:\n        target_model = super().apply_to(target_model)\n        return target_model\n\n    def create_weight_sparsifying_operation(self, module):\n        return RBSparsifyingWeight(module.weight.size(), sparsify=True)\n\n    def build_controller(self, target_model: NNCFNetwork) -> CompressionAlgorithmController:\n        params = self.config.get(""params"", {})\n        return RBSparsityController(target_model, self._sparsified_module_info,\n                                    params)\n\n\nclass RBSparsityController(BaseSparsityAlgoController):\n    def __init__(self, target_model: NNCFNetwork,\n                 sparsified_module_info: List[SparseModuleInfo],\n                 params):\n        super().__init__(target_model, sparsified_module_info)\n\n        self._distributed = False\n        self._loss = SparseLoss()  # type: SparseLoss\n        scheduler_cls = SPARSITY_SCHEDULERS.get(params.get(""schedule"", ""exponential""))\n        self._scheduler = scheduler_cls(self, params)\n        sparsify_operations = [m.operand for m in self.sparsified_module_info]\n        self._loss.set_layers(sparsify_operations)\n        self._check_sparsity_masks = params.get(""check_sparsity_masks"", False)\n\n    def set_sparsity_level(self, sparsity_level):\n        self._loss.target = 1 - sparsity_level\n\n    def freeze(self):\n        self._loss.disable()\n\n    def distributed(self):\n        if not dist.is_initialized():\n            raise KeyError(\'Could not set distributed mode for the compression algorithm \'\n                           \'because the default process group has not been initialized.\')\n\n        if next(self._model.parameters()).is_cuda:\n            state = torch.cuda.get_rng_state()\n            if dist.get_backend() == dist.Backend.NCCL:\n                state = state.cuda()\n            torch.distributed.broadcast(state, src=0)\n            torch.cuda.set_rng_state(state.cpu())\n        else:\n            state = torch.get_rng_state()\n            torch.distributed.broadcast(state, src=0)\n            torch.set_rng_state(state)\n\n        self._distributed = True\n\n    def check_distributed_masks(self):\n        if not self._distributed or get_world_size() == 1:\n            return 1\n\n        nvalues = 0\n        ncor_values = 0\n        eps = 1e-4\n        for minfo in self.sparsified_module_info:\n            mask = minfo.operand.mask\n\n            mask_list = [torch.empty_like(mask) for _ in range(get_world_size())]\n            # nccl does not support gather, send, recv operations\n            dist.all_gather(mask_list, mask)\n\n            for i in range(1, len(mask_list)):\n                rel_error = (mask_list[0] - mask_list[i]) / mask_list[0]\n                ncor_values = ncor_values + (rel_error.abs() < eps).sum(dtype=mask.dtype)\n                nvalues = nvalues + mask_list[i].numel()\n\n        return ncor_values / nvalues\n\n    def add_algo_specific_stats(self, stats):\n        stats[""target_sparsity_rate""] = self.loss.target_sparsity_rate\n        if self._distributed and self._check_sparsity_masks:\n            stats[""masks_consistents""] = self.check_distributed_masks()\n        return stats\n'"
pytorch_toolkit/nncf/nncf/sparsity/rb/functions.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\nfrom nncf.dynamic_graph.patch_pytorch import register_operator\nfrom nncf.functions import STThreshold, logit\n\n\ndef binary_mask(mask):\n    return STThreshold.apply(torch.sigmoid(mask))\n\n\n@register_operator()\ndef calc_rb_binary_mask(mask, uniform_buffer, eps):\n    if uniform_buffer is not None:\n        uniform_buffer.uniform_()\n        mask = mask + logit(uniform_buffer.clamp(eps, 1 - eps))\n    return binary_mask(mask)\n'"
pytorch_toolkit/nncf/nncf/sparsity/rb/layers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n\n\nimport torch\nimport torch.nn as nn\n\nfrom nncf.sparsity.layers import BinaryMask\nfrom .functions import calc_rb_binary_mask, binary_mask\nfrom ...functions import logit\nfrom ...layer_utils import COMPRESSION_MODULES\n\n\n\n@COMPRESSION_MODULES.register()\nclass RBSparsifyingWeight(BinaryMask):\n    def __init__(self, size, sparsify=False, eps=1e-6):\n        super().__init__(size)\n        self.sparsify = sparsify\n        self.eps = eps\n        self._mask = nn.Parameter(logit(torch.ones(size) * 0.99), requires_grad=self.sparsify)\n        self.binary_mask = binary_mask(self._mask)\n        self.register_buffer(""uniform"", torch.zeros(size))\n        self.mask_calculation_hook = MaskCalculationHook(self)\n\n    @property\n    def mask(self):\n        return self._mask\n\n    @mask.setter\n    def mask(self, tensor):\n        self._mask.data = tensor\n        self.binary_mask = binary_mask(self._mask)\n\n    def _calc_training_binary_mask(self, weight):\n        u = self.uniform if self.training and self.sparsify else None\n        return calc_rb_binary_mask(self._mask, u, self.eps)\n\n    def loss(self):\n        return binary_mask(self._mask)\n\n\nclass MaskCalculationHook():\n    def __init__(self, module):\n        # pylint: disable=protected-access\n        self.hook = module._register_state_dict_hook(self.hook_fn)\n\n    def hook_fn(self, module, destination, prefix, local_metadata):\n        module.binary_mask = binary_mask(module.mask)\n        destination[prefix + \'_binary_mask\'] = module.binary_mask\n        return destination\n\n    def close(self):\n        self.hook.remove()\n'"
pytorch_toolkit/nncf/nncf/sparsity/rb/loss.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\n\nfrom ...compression_method_api import CompressionLoss\n\n\n# Actually in responsible to lean density to target value\nclass SparseLoss(CompressionLoss):\n    def __init__(self, sparse_layers=None, target=1.0, p=0.05):\n        super().__init__()\n        self._sparse_layers = sparse_layers\n        self.target = target\n        self.p = p\n        self.disabled = False\n        self.current_sparsity = 0\n        self.mean_sparse_prob = 0\n\n    def set_layers(self, sparse_layers):\n        self._sparse_layers = sparse_layers\n\n    def disable(self):\n        if not self.disabled:\n            self.disabled = True\n\n            for sparse_layer in self._sparse_layers:\n                sparse_layer.sparsify = False\n\n    def forward(self):\n        if self.disabled:\n            return 0\n\n        params = 0\n        loss = 0\n        sparse_prob_sum = 0\n        for sparse_layer in self._sparse_layers:\n            if not self.disabled and not sparse_layer.sparsify:\n                raise AssertionError(\n                    ""Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss"")\n            if sparse_layer.sparsify:\n                sw_loss = sparse_layer.loss()\n                params = params + sw_loss.view(-1).size(0)\n                loss = loss + sw_loss.sum()\n                sparse_prob_sum += torch.sigmoid(sparse_layer.mask).sum()\n\n        self.mean_sparse_prob = (sparse_prob_sum / params).item()\n        self.current_sparsity = 1 - loss / params\n        return ((loss / params - self.target) / self.p).pow(2)\n\n    @property\n    def target_sparsity_rate(self):\n        rate = 1 - self.target\n        if rate < 0 or rate > 1:\n            raise IndexError(""Target is not within range(0,1)"")\n        return rate\n\n    def statistics(self):\n        return {\'mean_sparse_prob\': 1 - self.mean_sparse_prob}\n'"
pytorch_toolkit/nncf/tests/modules/seq2seq/__init__.py,0,b''
pytorch_toolkit/nncf/tests/modules/seq2seq/attention.py,1,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\n\nclass BahdanauAttention(nn.Module):\n    """"""\n    Bahdanau Attention (https://arxiv.org/abs/1409.0473)\n    Implementation is very similar to tf.contrib.seq2seq.BahdanauAttention\n    """"""\n\n    def __init__(self, query_size, key_size, num_units, normalize=False,\n                 batch_first=False, init_weight=0.1):\n        """"""\n        Constructor for the BahdanauAttention.\n\n        :param query_size: feature dimension for query\n        :param key_size: feature dimension for keys\n        :param num_units: internal feature dimension\n        :param normalize: whether to normalize energy term\n        :param batch_first: if True batch size is the 1st dimension, if False\n            the sequence is first and batch size is second\n        :param init_weight: range for uniform initializer used to initialize\n            Linear key and query transform layers and linear_att vector\n        """"""\n        super(BahdanauAttention, self).__init__()\n\n        self.normalize = normalize\n        self.batch_first = batch_first\n        self.num_units = num_units\n\n        self.linear_q = nn.Linear(query_size, num_units, bias=False)\n        self.linear_k = nn.Linear(key_size, num_units, bias=False)\n        nn.init.uniform_(self.linear_q.weight.data, -init_weight, init_weight)\n        nn.init.uniform_(self.linear_k.weight.data, -init_weight, init_weight)\n\n        self.linear_att = Parameter(torch.Tensor(num_units))\n\n        self.mask = None\n\n        if self.normalize:\n            self.normalize_scalar = Parameter(torch.Tensor(1))\n            self.normalize_bias = Parameter(torch.Tensor(num_units))\n        else:\n            self.register_parameter(\'normalize_scalar\', None)\n            self.register_parameter(\'normalize_bias\', None)\n\n        self.reset_parameters(init_weight)\n\n    def reset_parameters(self, init_weight):\n        """"""\n        Sets initial random values for trainable parameters.\n        """"""\n        stdv = 1. / math.sqrt(self.num_units)\n        self.linear_att.data.uniform_(-init_weight, init_weight)\n\n        if self.normalize:\n            self.normalize_scalar.data.fill_(stdv)\n            self.normalize_bias.data.zero_()\n\n    def set_mask(self, context_len, context):\n        """"""\n        sets self.mask which is applied before softmax\n        ones for inactive context fields, zeros for active context fields\n\n        :param context_len: b\n        :param context: if batch_first: (b x t_k x n) else: (t_k x b x n)\n\n        self.mask: (b x t_k)\n        """"""\n\n        if self.batch_first:\n            max_len = context.size(1)\n        else:\n            max_len = context.size(0)\n\n        indices = torch.arange(0, max_len, dtype=torch.int64,\n                               device=context.device)\n        self.mask = indices >= (context_len.unsqueeze(1))\n\n    def calc_score(self, att_query, att_keys):\n        """"""\n        Calculate Bahdanau score\n\n        :param att_query: b x t_q x n\n        :param att_keys: b x t_k x n\n\n        returns: b x t_q x t_k scores\n        """"""\n\n        b, t_k, n = att_keys.size()\n        t_q = att_query.size(1)\n\n        att_query = att_query.unsqueeze(2).expand(b, t_q, t_k, n)\n        att_keys = att_keys.unsqueeze(1).expand(b, t_q, t_k, n)\n        # TODO: can\'t match size??\n        sum_qk = att_query + att_keys\n\n        # if self.normalize:\n        #     sum_qk = sum_qk + self.normalize_bias\n        #     linear_att = self.linear_att / self.linear_att.norm()\n        #     linear_att = linear_att * self.normalize_scalar\n        # else:\n        linear_att = self.linear_att\n\n        out = torch.tanh(sum_qk).matmul(linear_att)\n        return out\n\n    def forward(self, query, keys):\n        """"""\n\n        :param query: if batch_first: (b x t_q x n) else: (t_q x b x n)\n        :param keys: if batch_first: (b x t_k x n) else (t_k x b x n)\n\n        :returns: (context, scores_normalized)\n        context: if batch_first: (b x t_q x n) else (t_q x b x n)\n        scores_normalized: if batch_first (b x t_q x t_k) else (t_q x b x t_k)\n        """"""\n\n        # first dim of keys and query has to be \'batch\', it\'s needed for bmm\n        if not self.batch_first:\n            keys = keys.transpose(0, 1)\n            if query.dim() == 3:\n                query = query.transpose(0, 1)\n\n        if query.dim() == 2:\n            single_query = True\n            query = query.unsqueeze(1)\n        else:\n            single_query = False\n\n        b = query.size(0)\n        t_k = keys.size(1)\n        t_q = query.size(1)\n\n        # FC layers to transform query and key\n        processed_query = self.linear_q(query)\n        processed_key = self.linear_k(keys)\n\n        # scores: (b x t_q x t_k)\n        scores = self.calc_score(processed_query, processed_key)\n\n        if self.mask is not None:\n            mask = self.mask.unsqueeze(1).expand(b, t_q, t_k)\n            # I can\'t use -INF because of overflow check in pytorch\n            scores.masked_fill_(mask, -65504.0)\n\n        # Normalize the scores, softmax over t_k\n        scores_normalized = F.softmax(scores, dim=-1)\n\n        # Calculate the weighted average of the attention inputs according to\n        # the scores\n        # context: (b x t_q x n)\n        context = torch.bmm(scores_normalized, keys)\n\n        if single_query:\n            context = context.squeeze(1)\n            scores_normalized = scores_normalized.squeeze(1)\n        elif not self.batch_first:\n            context = context.transpose(0, 1)\n            scores_normalized = scores_normalized.transpose(0, 1)\n\n        return context, scores_normalized\n'"
pytorch_toolkit/nncf/tests/modules/seq2seq/decoder.py,0,"b'import itertools\n\nimport torch\nimport torch.nn as nn\n\nfrom tests.modules.seq2seq.attention import BahdanauAttention\nfrom tests.modules.seq2seq.seq2seq_base import PAD\n\n\nclass RecurrentAttention(nn.Module):\n    """"""\n    LSTM wrapped with an attention module.\n    """"""\n\n    def __init__(self, input_size=1024, context_size=1024, hidden_size=1024,\n                 num_layers=1, batch_first=False, dropout=0.2,\n                 init_weight=0.1):\n        """"""\n        Constructor for the RecurrentAttention.\n\n        :param input_size: number of features in input tensor\n        :param context_size: number of features in output from encoder\n        :param hidden_size: internal hidden size\n        :param num_layers: number of layers in LSTM\n        :param batch_first: if True the model uses (batch,seq,feature) tensors,\n            if false the model uses (seq, batch, feature)\n        :param dropout: probability of dropout (on input to LSTM layer)\n        :param init_weight: range for the uniform initializer\n        """"""\n\n        super(RecurrentAttention, self).__init__()\n\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, bias=True,\n                           batch_first=batch_first)\n\n        self.attn = BahdanauAttention(hidden_size, context_size, context_size,\n                                      normalize=True, batch_first=batch_first)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, inputs, hidden, context, context_len):\n        """"""\n        Execute RecurrentAttention.\n\n        :param inputs: tensor with inputs\n        :param hidden: hidden state for LSTM layer\n        :param context: context tensor from encoder\n        :param context_len: vector of encoder sequence lengths\n\n        :returns (rnn_outputs, hidden, attn_output, attn_scores)\n        """"""\n        # set attention mask, sequences have different lengths, this mask\n        # allows to include only valid elements of context in attention\'s\n        # softmax\n        self.attn.set_mask(context_len, context)\n\n        inputs = self.dropout(inputs)\n        rnn_outputs, hidden = self.rnn(inputs, hidden)\n        attn_outputs, scores = self.attn(rnn_outputs, context)\n\n        return rnn_outputs, hidden, attn_outputs, scores\n\n\nclass Classifier(nn.Module):\n    """"""\n    Fully-connected classifier\n    """"""\n\n    def __init__(self, in_features, out_features, init_weight=0.1):\n        """"""\n        Constructor for the Classifier.\n\n        :param in_features: number of input features\n        :param out_features: number of output features (size of vocabulary)\n        :param init_weight: range for the uniform initializer\n        """"""\n        super(Classifier, self).__init__()\n        self.classifier = nn.Linear(in_features, out_features)\n        nn.init.uniform_(self.classifier.weight.data, -init_weight, init_weight)\n        nn.init.uniform_(self.classifier.bias.data, -init_weight, init_weight)\n\n    def forward(self, x):\n        """"""\n        Execute the classifier.\n\n        :param x: output from decoder\n        """"""\n        out = self.classifier(x)\n        return out\n\n\nclass ResidualRecurrentDecoder(nn.Module):\n    """"""\n    Decoder with Embedding, LSTM layers, attention, residual connections and\n    optinal dropout.\n\n    Attention implemented in this module is different than the attention\n    discussed in the GNMT arxiv paper. In this model the output from the first\n    LSTM layer of the decoder goes into the attention module, then the\n    re-weighted context is concatenated with inputs to all subsequent LSTM\n    layers in the decoder at the current timestep.\n\n    Residual connections are enabled after 3rd LSTM layer, dropout is applied\n    on inputs to LSTM layers.\n    """"""\n\n    def __init__(self, vocab_size, hidden_size=1024, num_layers=4, dropout=0.2,\n                 batch_first=False, embedder=None, init_weight=0.1):\n        """"""\n        Constructor of the ResidualRecurrentDecoder.\n\n        :param vocab_size: size of vocabulary\n        :param hidden_size: hidden size for LSMT layers\n        :param num_layers: number of LSTM layers\n        :param dropout: probability of dropout (on input to LSTM layers)\n        :param batch_first: if True the model uses (batch,seq,feature) tensors,\n            if false the model uses (seq, batch, feature)\n        :param embedder: instance of nn.Embedding, if None constructor will\n            create new embedding layer\n        :param init_weight: range for the uniform initializer\n        """"""\n        super(ResidualRecurrentDecoder, self).__init__()\n\n        self.num_layers = num_layers\n\n        self.att_rnn = RecurrentAttention(hidden_size, hidden_size,\n                                          hidden_size, num_layers=1,\n                                          batch_first=batch_first,\n                                          dropout=dropout)\n\n        self.rnn_layers = nn.ModuleList()\n        for _ in range(num_layers - 1):\n            self.rnn_layers.append(\n                nn.LSTM(2 * hidden_size, hidden_size, num_layers=1, bias=True,\n                        batch_first=batch_first))\n\n        if embedder is not None:\n            self.embedder = embedder\n        else:\n            self.embedder = nn.Embedding(vocab_size, hidden_size,\n                                         padding_idx=PAD)\n            nn.init.uniform_(self.embedder.weight.data, -init_weight,\n                             init_weight)\n\n        self.classifier = Classifier(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def init_hidden(self, hidden):\n        """"""\n        Converts flattened hidden state (from sequence generator) into a tuple\n        of hidden states.\n\n        :param hidden: None or flattened hidden state for decoder RNN layers\n        """"""\n        if hidden is not None:\n            # per-layer chunks\n            hidden = hidden.chunk(self.num_layers)\n            # (h, c) chunks for LSTM layer\n            hidden = tuple(i.chunk(2) for i in hidden)\n        else:\n            hidden = [None] * self.num_layers\n\n        self.next_hidden = []\n        return hidden\n\n    def append_hidden(self, h):\n        """"""\n        Appends the hidden vector h to the list of internal hidden states.\n\n        :param h: hidden vector\n        """"""\n        if self.inference:\n            self.next_hidden.append(h)\n\n    def package_hidden(self):\n        """"""\n        Flattens the hidden state from all LSTM layers into one tensor (for\n        the sequence generator).\n        """"""\n        if self.inference:\n            hidden = torch.cat(tuple(itertools.chain(*self.next_hidden)))\n        else:\n            hidden = None\n        return hidden\n\n    def forward(self, inputs, context, inference=False):\n        """"""\n        Execute the decoder.\n\n        :param inputs: tensor with inputs to the decoder\n        :param context: state of encoder, encoder sequence lengths and hidden\n            state of decoder\'s LSTM layers\n        :param inference: if True stores and repackages hidden state\n        """"""\n        self.inference = inference\n\n        enc_context, enc_len, hidden = context\n        hidden = self.init_hidden(hidden)\n\n        x = self.embedder(inputs)\n\n        x, h, attn, scores = self.att_rnn(x, hidden[0], enc_context, enc_len)\n        self.append_hidden(h)\n\n        x = torch.cat((x, attn), dim=2)\n        x = self.dropout(x)\n        x, h = self.rnn_layers[0](x, hidden[1])\n        self.append_hidden(h)\n\n        for i in range(1, len(self.rnn_layers)):\n            residual = x\n            x = torch.cat((x, attn), dim=2)\n            x = self.dropout(x)\n            x, h = self.rnn_layers[i](x, hidden[i + 1])\n            self.append_hidden(h)\n            x = x + residual\n\n        x = self.classifier(x)\n        hidden = self.package_hidden()\n\n        return x, scores, [enc_context, enc_len, hidden]\n'"
pytorch_toolkit/nncf/tests/modules/seq2seq/encoder.py,0,"b'import torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence\n\nfrom tests.modules.seq2seq.seq2seq_base import PAD\n\n\nclass ResidualRecurrentEncoder(nn.Module):\n    """"""\n    Encoder with Embedding, LSTM layers, residual connections and optional\n    dropout.\n\n    The first LSTM layer is bidirectional and uses variable sequence length\n    API, the remaining (num_layers-1) layers are unidirectional. Residual\n    connections are enabled after third LSTM layer, dropout is applied on\n    inputs to LSTM layers.\n    """"""\n\n    def __init__(self, vocab_size, hidden_size=1024, num_layers=4, dropout=0.2,\n                 batch_first=False, embedder=None, init_weight=0.1):\n        """"""\n        Constructor for the ResidualRecurrentEncoder.\n\n        :param vocab_size: size of vocabulary\n        :param hidden_size: hidden size for LSTM layers\n        :param num_layers: number of LSTM layers, 1st layer is bidirectional\n        :param dropout: probability of dropout (on input to LSTM layers)\n        :param batch_first: if True the model uses (batch,seq,feature) tensors,\n            if false the model uses (seq, batch, feature)\n        :param embedder: instance of nn.Embedding, if None constructor will\n            create new embedding layer\n        :param init_weight: range for the uniform initializer\n        """"""\n        super(ResidualRecurrentEncoder, self).__init__()\n        self.batch_first = batch_first\n        self.rnn_layers = nn.ModuleList()\n        # 1st LSTM layer, bidirectional\n        self.rnn_layers.append(\n            nn.LSTM(hidden_size, hidden_size, num_layers=1, bias=True,\n                    batch_first=batch_first, bidirectional=True))\n\n        # 2nd LSTM layer, with 2x larger input_size\n        self.rnn_layers.append(\n            nn.LSTM((2 * hidden_size), hidden_size, num_layers=1, bias=True,\n                    batch_first=batch_first))\n\n        # Remaining LSTM layers\n        for _ in range(num_layers - 2):\n            self.rnn_layers.append(\n                nn.LSTM(hidden_size, hidden_size, num_layers=1, bias=True,\n                        batch_first=batch_first))\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        if embedder is not None:\n            self.embedder = embedder\n        else:\n            self.embedder = nn.Embedding(vocab_size, hidden_size,\n                                         padding_idx=PAD)\n            nn.init.uniform_(self.embedder.weight.data, -init_weight,\n                             init_weight)\n\n    def forward(self, inputs, lengths):\n        """"""\n        Execute the encoder.\n\n        :param inputs: tensor with indices from the vocabulary\n        :param lengths: vector with sequence lengths (excluding padding)\n\n        returns: tensor with encoded sequences\n        """"""\n        x = self.embedder(inputs)\n\n        # bidirectional layer\n        x = self.dropout(x)\n        x = pack_padded_sequence(x, lengths.cpu().numpy(),\n                                 batch_first=self.batch_first)\n        x, _ = self.rnn_layers[0](x)\n        x, _ = pad_packed_sequence(x, batch_first=self.batch_first)\n\n        # 1st unidirectional layer\n        x = self.dropout(x)\n        x, _ = self.rnn_layers[1](x)\n\n        # the rest of unidirectional layers,\n        # with residual connections starting from 3rd layer\n        for i in range(2, len(self.rnn_layers)):\n            residual = x\n            x = self.dropout(x)\n            x, _ = self.rnn_layers[i](x)\n            x = x + residual\n\n        return x\n'"
pytorch_toolkit/nncf/tests/modules/seq2seq/gnmt.py,0,"b'import torch.nn as nn\n\nfrom tests.modules.seq2seq.decoder import ResidualRecurrentDecoder\nfrom tests.modules.seq2seq.encoder import ResidualRecurrentEncoder\nfrom tests.modules.seq2seq.seq2seq_base import Seq2Seq, PAD\n\n\nclass GNMT(Seq2Seq):\n    """"""\n    GNMT v2 model\n    """"""\n\n    def __init__(self, vocab_size, hidden_size=1024, num_layers=4, dropout=0.2,\n                 batch_first=False, share_embedding=True):\n        """"""\n        Constructor for the GNMT v2 model.\n\n        :param vocab_size: size of vocabulary (number of tokens)\n        :param hidden_size: internal hidden size of the model\n        :param num_layers: number of layers, applies to both encoder and\n            decoder\n        :param dropout: probability of dropout (in encoder and decoder)\n        :param batch_first: if True the model uses (batch,seq,feature) tensors,\n            if false the model uses (seq, batch, feature)\n        :param share_embedding: if True embeddings are shared between encoder\n            and decoder\n        """"""\n\n        super(GNMT, self).__init__(batch_first=batch_first)\n\n        if share_embedding:\n            embedder = nn.Embedding(vocab_size, hidden_size,\n                                    padding_idx=PAD)\n            nn.init.uniform_(embedder.weight.data, -0.1, 0.1)\n        else:\n            embedder = None\n\n        self.encoder = ResidualRecurrentEncoder(vocab_size, hidden_size,\n                                                num_layers, dropout,\n                                                batch_first, embedder)\n\n        self.decoder = ResidualRecurrentDecoder(vocab_size, hidden_size,\n                                                num_layers, dropout,\n                                                batch_first, embedder)\n\n    def forward(self, input_encoder, input_enc_len, input_decoder):\n        context = self.encode(input_encoder, input_enc_len)\n        context = (context, input_enc_len, None)\n        output, _, _ = self.decode(input_decoder, context)\n\n        return output\n'"
pytorch_toolkit/nncf/tests/modules/seq2seq/seq2seq_base.py,0,"b'import torch.nn as nn\nfrom torch.nn.functional import log_softmax\n\nPAD = 0\n\n\nclass Seq2Seq(nn.Module):\n    """"""\n    Generic Seq2Seq module, with an encoder and a decoder.\n    """"""\n\n    def __init__(self, encoder=None, decoder=None, batch_first=False):\n        """"""\n        Constructor for the Seq2Seq module.\n\n        :param encoder: encoder module\n        :param decoder: decoder module\n        :param batch_first: if True the model uses (batch, seq, feature)\n            tensors, if false the model uses (seq, batch, feature) tensors\n        """"""\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.batch_first = batch_first\n\n    def encode(self, inputs, lengths):\n        """"""\n        Applies the encoder to inputs with a given input sequence lengths.\n\n        :param inputs: tensor with inputs (batch, seq_len) if \'batch_first\'\n            else (seq_len, batch)\n        :param lengths: vector with sequence lengths (excluding padding)\n        """"""\n        return self.encoder(inputs, lengths)\n\n    def decode(self, inputs, context, inference=False):\n        """"""\n        Applies the decoder to inputs, given the context from the encoder.\n\n        :param inputs: tensor with inputs (batch, seq_len) if \'batch_first\'\n            else (seq_len, batch)\n        :param context: context from the encoder\n        :param inference: if True inference mode, if False training mode\n        """"""\n        return self.decoder(inputs, context, inference)\n\n    def generate(self, inputs, context, beam_size):\n        """"""\n        Autoregressive generator, works with SequenceGenerator class.\n        Executes decoder (in inference mode), applies log_softmax and topK for\n        inference with beam search decoding.\n\n        :param inputs: tensor with inputs to the decoder\n        :param context: context from the encoder\n        :param beam_size: beam size for the generator\n\n        returns: (words, logprobs, scores, new_context)\n            words: indices of topK tokens\n            logprobs: log probabilities of topK tokens\n            scores: scores from the attention module (for coverage penalty)\n            new_context: new decoder context, includes new hidden states for\n                decoder RNN cells\n        """"""\n        logits, scores, new_context = self.decode(inputs, context, True)\n        logprobs = log_softmax(logits, dim=-1)\n        logprobs, words = logprobs.topk(beam_size, dim=-1)\n        return words, logprobs, scores, new_context\n\n    def forward(self, input_encoder, input_enc_len, input_decoder):\n        raise NotImplementedError\n'"
pytorch_toolkit/nncf/tests/pruning/filter_pruning/__init__.py,0,b''
pytorch_toolkit/nncf/tests/pruning/filter_pruning/test_algo.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport pytest\nimport torch\n\nfrom examples.common.optimizer import make_optimizer, get_parameter_groups\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.pruning.filter_pruning.algo import FilterPruningController\nfrom nncf.pruning.filter_pruning.functions import l2_filter_norm\nfrom nncf.pruning.filter_pruning.layers import FilterPruningBlock, apply_filter_binary_mask\nfrom nncf.pruning.schedulers import BaselinePruningScheduler\nfrom tests.pruning.test_helpers import get_basic_pruning_config, PruningTestModel, \\\n    BigPruningTestModel, create_dataloader\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test, check_correct_nncf_modules_replacement\n\n\ndef create_pruning_algo_with_config(config):\n    """"""\n    Create filter_pruning with default params.\n    :param config: config for the algorithm\n    :return pruned model, pruning_algo, nncf_modules\n    """"""\n    config[\'compression\'][\'algorithm\'] = \'filter_pruning\'\n    model = BigPruningTestModel()\n    pruned_model, pruning_algo = create_compressed_model_and_algo_for_test(BigPruningTestModel(), config)\n\n    # Check that all modules was correctly replaced by NNCF modules and return this NNCF modules\n    _, nncf_modules = check_correct_nncf_modules_replacement(model, pruned_model)\n    return pruned_model, pruning_algo, nncf_modules\n\n\ndef test_check_default_algo_params():\n    """"""\n    Test for default algorithm params. Creating empty config and check for valid default\n    parameters.\n    """"""\n    # Creating algorithm with empty config\n    config = get_basic_pruning_config()\n    config[\'compression\'][\'algorithm\'] = \'filter_pruning\'\n    model = PruningTestModel()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    assert isinstance(compression_ctrl, FilterPruningController)\n    scheduler = compression_ctrl.scheduler\n    # Check default algo params\n    assert compression_ctrl.prune_first is False\n    assert compression_ctrl.prune_last is False\n    assert compression_ctrl.prune_batch_norms is False\n    assert compression_ctrl.filter_importance is l2_filter_norm\n\n    assert compression_ctrl.all_weights is False\n    assert compression_ctrl.zero_grad is True\n\n    # Check default scheduler params\n    assert isinstance(scheduler, BaselinePruningScheduler)\n\n\n@pytest.mark.parametrize(\n    (\'prune_first\', \'prune_last\'),\n    [\n        (False, False),\n        (False, True),\n        (True, False),\n        (True, True),\n    ]\n\n)\ndef test_valid_modules_replacement_and_pruning(prune_first, prune_last):\n    """"""\n    Test that checks that all conv modules in model was replaced by nncf modules and\n    pruning pre ops were added correctly.\n    :param prune_first: whether to prune first convolution or not\n    :param prune_last: whether to prune last convolution or not\n    """"""\n\n    def check_that_module_is_pruned(module):\n        assert len(module.pre_ops.values()) == 1\n        op = list(module.pre_ops.values())[0]\n        assert isinstance(op, UpdateWeight)\n        assert isinstance(op.operand, FilterPruningBlock)\n\n    config = get_basic_pruning_config(input_sample_size=(1, 1, 8, 8))\n    config[\'compression\'][\'params\'][\'prune_first_conv\'] = prune_first\n    config[\'compression\'][\'params\'][\'prune_last_conv\'] = prune_last\n\n    pruned_model, pruning_algo, nncf_modules = create_pruning_algo_with_config(config)\n    pruned_module_info = pruning_algo.pruned_module_info\n    pruned_modules = [minfo.module for minfo in pruned_module_info]\n\n    # Check for conv1\n    conv1 = pruned_model.conv1\n    if prune_first:\n        assert conv1 in pruned_modules\n        assert conv1 in nncf_modules.values()\n        check_that_module_is_pruned(conv1)\n\n    # Check for conv2\n    conv2 = pruned_model.conv2\n    assert conv2 in pruned_modules\n    assert conv2 in nncf_modules.values()\n    check_that_module_is_pruned(conv2)\n\n    # Check for conv3\n    conv3 = pruned_model.conv3\n    if prune_last:\n        assert conv3 in pruned_modules\n        assert conv3 in nncf_modules.values()\n        check_that_module_is_pruned(conv3)\n\n\n@pytest.mark.parametrize((\'all_weights\', \'prune_first\', \'ref_masks\'),\n                         [(False, True, [torch.tensor([0.0] * 8 + [1.0] * 8), torch.tensor([0.0] * 16 + [1.0] * 16)]),\n                          (True, True, [torch.tensor([0.0] * 7 + [1.0] * 9), torch.tensor([0.0] * 17 + [1.0] * 15)]),\n                          (False, False, [torch.tensor([0.0] * 16 + [1.0] * 16)]),\n                          (True, False, [torch.tensor([0.0] * 16 + [1.0] * 16)]),\n                          ]\n                         )\ndef test_pruning_masks_correctness(all_weights, prune_first, ref_masks):\n    """"""\n    Test for pruning masks check (_set_binary_masks_for_filters, _set_binary_masks_for_all_filters_together).\n    :param all_weights: whether mask will be calculated for all weights in common or not\n    :param prune_first: whether to prune first convolution or not\n    :param ref_masks: reference masks values\n    """"""\n\n    def check_mask(module, num):\n        op = list(module.pre_ops.values())[0]\n        assert hasattr(op.operand, \'binary_filter_pruning_mask\')\n        assert torch.allclose(op.operand.binary_filter_pruning_mask, ref_masks[num])\n\n    config = get_basic_pruning_config(input_sample_size=(1, 1, 8, 8))\n    config[\'compression\'][\'params\'][\'all_weights\'] = all_weights\n    config[\'compression\'][\'params\'][\'prune_first_conv\'] = prune_first\n\n    pruned_model, pruning_algo, _ = create_pruning_algo_with_config(config)\n    pruned_module_info = pruning_algo.pruned_module_info\n    pruned_modules = [minfo.module for minfo in pruned_module_info]\n    assert pruning_algo.pruning_rate == 0.5\n    assert pruning_algo.all_weights is all_weights\n\n    i = 0\n    # Check for conv1\n    conv1 = pruned_model.conv1\n    if prune_first:\n        assert conv1 in pruned_modules\n        check_mask(conv1, i)\n        i += 1\n\n    # Check for conv2\n    conv2 = pruned_model.conv2\n    assert conv2 in pruned_modules\n    check_mask(conv2, i)\n\n\n@pytest.mark.parametrize(\'prune_bn\',\n                         [False,\n                          True]\n                         )\ndef test_applying_masks(prune_bn):\n    config = get_basic_pruning_config(input_sample_size=(1, 1, 8, 8))\n    config[\'compression\'][\'params\'][\'prune_batch_norms\'] = prune_bn\n    config[\'compression\'][\'params\'][\'prune_first_conv\'] = True\n    config[\'compression\'][\'params\'][\'prune_last_conv\'] = True\n\n    pruned_model, pruning_algo, nncf_modules = create_pruning_algo_with_config(config)\n    pruned_module_info = pruning_algo.pruned_module_info\n    pruned_modules = [minfo.module for minfo in pruned_module_info]\n\n    assert len(pruned_modules) == len(nncf_modules)\n\n    for module in pruned_modules:\n        op = list(module.pre_ops.values())[0]\n        mask = op.operand.binary_filter_pruning_mask\n        masked_weight = apply_filter_binary_mask(mask, module.weight)\n        masked_bias = apply_filter_binary_mask(mask, module.bias)\n        assert torch.allclose(module.weight, masked_weight)\n        assert torch.allclose(module.bias, masked_bias)\n\n    # Have only one BN node in graph\n    bn_module = pruned_model.bn\n    conv_for_bn = pruned_model.conv2\n    bn_mask = list(conv_for_bn.pre_ops.values())[0].operand.binary_filter_pruning_mask\n    if prune_bn:\n        masked_bn_weight = apply_filter_binary_mask(bn_mask, bn_module.weight)\n        masked_bn_bias = apply_filter_binary_mask(bn_mask, bn_module.bias)\n        assert torch.allclose(bn_module.weight, masked_bn_weight)\n        assert torch.allclose(bn_module.bias, masked_bn_bias)\n\n\n@pytest.mark.parametrize(\'zero_grad\',\n                         [True, False])\ndef test_zeroing_gradients(zero_grad):\n    """"""\n    Test for zeroing gradients functionality (zero_grads_for_pruned_modules in base algo)\n    :param zero_grad: zero grad or not\n    """"""\n    config = get_basic_pruning_config(input_sample_size=(2, 1, 8, 8))\n    config[\'compression\'][\'params\'][\'prune_first_conv\'] = True\n    config[\'compression\'][\'params\'][\'prune_last_conv\'] = True\n    config[\'compression\'][\'params\'][\'zero_grad\'] = zero_grad\n\n    pruned_model, pruning_algo, _ = create_pruning_algo_with_config(config)\n    assert pruning_algo.zero_grad is zero_grad\n\n    pruned_module_info = pruning_algo.pruned_module_info\n    pruned_modules = [minfo.module for minfo in pruned_module_info]\n\n    device = next(pruned_model.parameters()).device\n    data_loader = create_dataloader(config)\n\n    pruning_algo.initialize(data_loader)\n\n    params_to_optimize = get_parameter_groups(pruned_model, config)\n    optimizer, lr_scheduler = make_optimizer(params_to_optimize, config)\n\n    lr_scheduler.step(0)\n\n    pruned_model.train()\n    for input_, target in data_loader:\n        input_ = input_.to(device)\n        target = target.to(device).view(1)\n\n        output = pruned_model(input_)\n\n        loss = torch.sum(target.to(torch.float32) - output)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        # In case of zero_grad = True gradients should be masked\n        if zero_grad:\n            for module in pruned_modules:\n                op = list(module.pre_ops.values())[0]\n                mask = op.operand.binary_filter_pruning_mask\n                grad = module.weight.grad\n                masked_grad = apply_filter_binary_mask(mask, grad)\n                assert torch.allclose(masked_grad, grad)\n'"
pytorch_toolkit/nncf/tests/pruning/filter_pruning/test_functions.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport pytest\nimport torch\n\nfrom nncf.pruning.filter_pruning.functions import FILTER_IMPORTANCE_FUNCTIONS, calculate_binary_mask\n\n\n@pytest.mark.parametrize((""norm_name"", ""input_tensor"", ""reference""),\n                         [(\'L1\', torch.arange(120.0).view(2, 3, 4, 5), torch.tensor([1770.0, 5370.0])),\n                          (\'L2\', torch.arange(120.0).view(2, 3, 4, 5), torch.tensor([264.9716966, 706.12321871])),\n                          (\'geometric_median\', torch.arange(120.0).view(3, 2, 4, 5),\n                           torch.tensor([758.94663844, 505.96442563, 758.94663844])),\n                         ])\ndef test_norms(norm_name, input_tensor, reference):\n    """"""\n    Test correctness of all norms calculations.\n    """"""\n    norm_fn = FILTER_IMPORTANCE_FUNCTIONS.get(norm_name)\n    result = norm_fn(input_tensor)\n    assert torch.allclose(result, reference)\n\n\n@pytest.mark.parametrize((""importance"", ""threshold"", ""reference""),\n                         [(torch.arange(20.), 10.0, torch.tensor([0.0]*10 + [1.0]*10))]\n                         )\ndef test_calculate_binary_mask(importance, threshold, reference):\n    """"""\n    Test correctness of binary mask calculation.\n    """"""\n    mask = calculate_binary_mask(importance, threshold)\n    assert torch.allclose(mask, reference)\n    assert isinstance(mask, torch.FloatTensor)\n'"
pytorch_toolkit/nncf/tests/pruning/filter_pruning/test_layers.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nimport pytest\nimport torch\nfrom torch import nn\n\nfrom nncf.layers import NNCFConv2d\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.pruning.filter_pruning.layers import FilterPruningBlock, inplace_apply_filter_binary_mask, \\\n    apply_filter_binary_mask\nfrom tests.test_helpers import fill_conv_weight, fill_bias\n\n\nclass TestFilterPruningBlockModel(nn.Module):\n    def __init__(self, layer):\n        super().__init__()\n        self.layer = layer\n        pruning_op = FilterPruningBlock(layer.weight.size(0))\n        self.op_key = self.layer.register_pre_forward_operation(UpdateWeight(pruning_op))\n\n    @property\n    def pruning_op(self):\n        return self.layer.get_pre_op(self.op_key).operand\n\n    def forward(self, x):\n        return self.layer(x)\n\n\n@pytest.mark.parametrize(\n    (\'weights_val\', \'bias_val\'),\n    (\n        (3, 0),\n        (9, 0),\n        (15, 1)\n    )\n)\ndef test_can_infer_magnitude_pruned_conv(weights_val, bias_val):\n    """"""\n    Check that NNCFConv2d with FilterPruningBlock as pre ops working exactly the same as\n    normal nn.Conv2d.\n    :param weights_val: value for filling weights\n    :param bias_val: value for filling biases\n    """"""\n    nncf_module = NNCFConv2d(1, 1, 2)\n    pytorch_module = nn.Conv2d(1, 1, 2)\n\n    sparse_model = TestFilterPruningBlockModel(nncf_module)\n\n    fill_conv_weight(nncf_module, weights_val)\n    fill_bias(nncf_module, bias_val)\n\n    fill_conv_weight(pytorch_module, weights_val)\n    fill_bias(pytorch_module, bias_val)\n\n    act_output = sparse_model(torch.ones([1, 1, 2, 2]))\n    ref_output = pytorch_module(torch.ones([1, 1, 2, 2]))\n    assert act_output.item() == ref_output\n\n\ndef test_assert_broadcastable_mask_and_weight_shape():\n    nncf_module = NNCFConv2d(1, 2, 2)\n    fill_conv_weight(nncf_module, 1)\n    fill_bias(nncf_module, 1)\n\n    mask = torch.zeros(10)\n\n    with pytest.raises(RuntimeError):\n        inplace_apply_filter_binary_mask(mask, nncf_module.weight.data)\n\n    with pytest.raises(RuntimeError):\n        apply_filter_binary_mask(mask, nncf_module.weight.data)\n\n\n@pytest.mark.parametrize((\'mask\', \'reference_weight\', \'reference_bias\'),\n                         [(torch.zeros(2), torch.zeros((2, 1, 2, 2)), torch.zeros(2)),\n                          (torch.ones(2), torch.ones((2, 1, 2, 2)) + torch.eye(2), torch.ones(2)),\n                          (torch.tensor([0, 1], dtype=torch.float32),\n                           torch.cat([torch.zeros((1, 1, 2, 2)), torch.ones((1, 1, 2, 2)) + torch.eye(2)]),\n                           torch.tensor([0, 1], dtype=torch.float32)),\n                          ])\nclass TestApplyMasks:\n    @staticmethod\n    def test_inplace_apply_filter_binary_mask(mask, reference_weight, reference_bias):\n        """"""\n        Test that inplace_apply_filter_binary_mask changes the input weight and returns valid result.\n        """"""\n        nncf_module = NNCFConv2d(1, 2, 2)\n        fill_conv_weight(nncf_module, 1)\n        fill_bias(nncf_module, 1)\n\n        result_weight = inplace_apply_filter_binary_mask(mask, nncf_module.weight.data)\n        assert torch.allclose(result_weight, reference_weight)\n        assert torch.allclose(nncf_module.weight, reference_weight)\n\n        result_bias = inplace_apply_filter_binary_mask(mask, nncf_module.bias.data)\n        assert torch.allclose(result_bias, reference_bias)\n        assert torch.allclose(nncf_module.bias, reference_bias)\n\n    @staticmethod\n    def test_apply_filter_binary_mask(mask, reference_weight, reference_bias):\n        """"""\n        Test that apply_filter_binary_mask not changes the input weight and returns valid result.\n        """"""\n        nncf_module = NNCFConv2d(1, 2, 2)\n        fill_conv_weight(nncf_module, 1)\n        fill_bias(nncf_module, 1)\n\n        original_weight = nncf_module.weight.data.detach().clone()\n        original_bias = nncf_module.bias.data.detach().clone()\n\n        result = apply_filter_binary_mask(mask, nncf_module.weight.data)\n        assert torch.allclose(nncf_module.weight, original_weight)\n        assert torch.allclose(result, reference_weight)\n\n        result_bias = apply_filter_binary_mask(mask, nncf_module.bias.data)\n        assert torch.allclose(result_bias, reference_bias)\n        assert torch.allclose(nncf_module.bias, original_bias)\n'"
pytorch_toolkit/nncf/tests/sparsity/const/__init__.py,0,b''
pytorch_toolkit/nncf/tests/sparsity/const/test_algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\nfrom copy import deepcopy\n\nimport torch\n\nfrom nncf.checkpoint_loading import load_state\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.sparsity.const.algo import ConstSparsityController\nfrom nncf.sparsity.layers import BinaryMask\nfrom tests.quantization.test_functions import check_equal\nfrom tests.sparsity.magnitude.test_helpers import MagnitudeTestModel\nfrom tests.test_helpers import BasicConvTestModel, get_empty_config, create_compressed_model_and_algo_for_test, \\\n    check_correct_nncf_modules_replacement\n\nsub_tensor = torch.tensor([[[[1., 0.],\n                             [0., 1.]]]])\nref_mask_1 = torch.cat((sub_tensor, sub_tensor), 0)\nsub_tensor = torch.tensor([[[[0., 1., 1.],\n                             [1., 0., 1.],\n                             [1., 1., 0.]]]])\nref_mask_2 = torch.cat((sub_tensor, sub_tensor), 1)\n\n\ndef test_can_create_const_sparse_algo__with_default():\n    model = BasicConvTestModel()\n    config = get_empty_config()\n    config[""compression""] = {""algorithm"": ""const_sparsity""}\n    sparse_model, compression_ctrl = create_compressed_model_and_algo_for_test(deepcopy(model), config)\n\n    assert isinstance(compression_ctrl, ConstSparsityController)\n    assert len(list(sparse_model.modules())) == 7\n\n    _, sparse_model_conv = check_correct_nncf_modules_replacement(model, sparse_model)\n\n    for sparse_module in sparse_model_conv.values():\n        store = []\n        for op in sparse_module.pre_ops.values():\n            if isinstance(op, UpdateWeight) and isinstance(op.operand, BinaryMask):\n                ref_mask = torch.ones_like(sparse_module.weight)\n                assert torch.allclose(op.operand.binary_mask, ref_mask)\n                assert op.__class__.__name__ not in store\n                store.append(op.__class__.__name__)\n\n\ndef test_can_restore_binary_mask_on_magnitude_algo_resume():\n    config = get_empty_config()\n    config[\'compression\'] = {""algorithm"": ""magnitude_sparsity"", ""weight_importance"": ""abs"",\n                             ""params"": {""schedule"": ""multistep"", ""sparsity_levels"": [0.3, 0.5]}}\n\n    sparse_model, _ = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n    with torch.no_grad():\n        sparse_model(torch.ones([1, 1, 10, 10]))\n\n    config = get_empty_config()\n    config[""compression""] = {""algorithm"": ""const_sparsity""}\n    const_sparse_model, _ = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n\n    load_state(const_sparse_model, sparse_model.state_dict())\n\n    op = const_sparse_model.conv1.pre_ops[\'0\']\n    check_equal(ref_mask_1, op.operand.binary_mask)\n\n    op = const_sparse_model.conv2.pre_ops[\'0\']\n    check_equal(ref_mask_2, op.operand.binary_mask)\n\n\ndef test_can_restore_binary_mask_on_magnitude_quant_algo_resume(tmp_path):\n    config = get_empty_config()\n    config[""compression""] = [\n        {""algorithm"": ""magnitude_sparsity"", ""weight_importance"": ""abs"",\n         ""params"": {""schedule"": ""multistep"", ""sparsity_levels"": [0.3, 0.5]}},\n        {""algorithm"": ""quantization""}]\n\n    sparse_model, _ = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n\n    # load_state doesn\'t support CPU + Quantization\n    sparse_model = torch.nn.DataParallel(sparse_model)\n    sparse_model.cuda()\n    with torch.no_grad():\n        sparse_model(torch.ones([1, 1, 10, 10]))\n\n    config = get_empty_config()\n    config[""compression""] = [{""algorithm"": ""const_sparsity""}, {""algorithm"": ""quantization""}]\n    const_sparse_model, _ = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n\n    load_state(const_sparse_model, sparse_model.state_dict())\n\n    op = const_sparse_model.get_nncf_wrapped_model().conv1.pre_ops[\'0\']\n    check_equal(ref_mask_1, op.operand.binary_mask)\n\n    op = const_sparse_model.get_nncf_wrapped_model().conv2.pre_ops[\'0\']\n    check_equal(ref_mask_2, op.operand.binary_mask)\n'"
pytorch_toolkit/nncf/tests/sparsity/magnitude/test_algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom copy import deepcopy\n\nimport pytest\nimport torch\nfrom pytest import approx\n\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.sparsity.layers import BinaryMask\nfrom nncf.sparsity.magnitude.algo import MagnitudeSparsityController\nfrom nncf.sparsity.magnitude.functions import normed_magnitude\nfrom tests.quantization.test_functions import check_equal\nfrom tests.sparsity.const.test_algo import ref_mask_2, ref_mask_1\nfrom tests.sparsity.magnitude.test_helpers import MagnitudeTestModel, get_basic_magnitude_sparsity_config\nfrom tests.test_helpers import create_compressed_model_and_algo_for_test, MockModel, BasicConvTestModel, \\\n    get_empty_config, check_correct_nncf_modules_replacement\n\n\ndef test_can_create_magnitude_sparse_algo__with_defaults():\n    model = MagnitudeTestModel()\n    config = get_basic_magnitude_sparsity_config()\n    config[\'compression\'][\'params\'] = \\\n        {\'schedule\': \'multistep\'}\n    sparse_model, compression_ctrl = create_compressed_model_and_algo_for_test(deepcopy(model), config)\n\n    assert isinstance(compression_ctrl, MagnitudeSparsityController)\n    assert compression_ctrl.sparsity_level == approx(0.1)\n    assert len(list(sparse_model.modules())) == 12\n\n    _, sparse_model_conv = check_correct_nncf_modules_replacement(model, sparse_model)\n\n    i = 0\n    for sparse_module in sparse_model_conv.values():\n        store = []\n        ref_mask = torch.ones_like(sparse_module.weight) if i == 0 else ref_mask_2\n        i += 1\n        for op in sparse_module.pre_ops.values():\n            if isinstance(op, UpdateWeight) and isinstance(op.operand, BinaryMask):\n                assert compression_ctrl.threshold == approx(0.24, 0.1)\n                assert torch.allclose(op.operand.binary_mask, ref_mask)\n                assert isinstance(compression_ctrl.weight_importance, type(normed_magnitude))\n                assert op.__class__.__name__ not in store\n                store.append(op.__class__.__name__)\n\n\n@pytest.mark.parametrize(\n    (\'weight_importance\', \'sparsity_level\', \'threshold\'),\n    (\n        (\'normed_abs\', None, 0.219),\n        (\'abs\', None, 9),\n        (\'normed_abs\', 0.5, 0.243),\n        (\'abs\', 0.5, 10),\n    )\n)\ndef test_magnitude_sparse_algo_sets_threshold(weight_importance, sparsity_level, threshold):\n    model = MagnitudeTestModel()\n    config = get_basic_magnitude_sparsity_config()\n    config[\'compression\'][\'weight_importance\'] = weight_importance\n    config[\'compression\'][\'params\'] = {\'schedule\': \'multistep\'}\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    if sparsity_level:\n        compression_ctrl.set_sparsity_level(sparsity_level)\n    assert compression_ctrl.threshold == pytest.approx(threshold, 0.01)\n\n\ndef test_can_not_set_sparsity_more_than_one_for_magnitude_sparse_algo():\n    config = get_basic_magnitude_sparsity_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n    with pytest.raises(AttributeError):\n        compression_ctrl.set_sparsity_level(1)\n        compression_ctrl.set_sparsity_level(1.2)\n\n\ndef test_can_not_create_magnitude_algo__without_steps():\n    config = get_basic_magnitude_sparsity_config()\n    config[\'compression\'][\'params\'] = {\'schedule\': \'multistep\', \'sparsity_levels\': [0.1]}\n    with pytest.raises(AttributeError):\n        _, _ = create_compressed_model_and_algo_for_test(MockModel(), config)\n\n\ndef test_can_create_magnitude_algo__without_levels():\n    config = get_basic_magnitude_sparsity_config()\n    config[\'compression\'][\'params\'] = {\'schedule\': \'multistep\', \'steps\': [1]}\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(MockModel(), config)\n    assert compression_ctrl.sparsity_level == approx(0.1)\n\n\ndef test_can_not_create_magnitude_algo__with_not_matched_steps_and_levels():\n    config = get_basic_magnitude_sparsity_config()\n    config[\'compression\'][\'params\'] = {\'schedule\': \'multistep\', \'sparsity_levels\': [0.1], \'steps\': [1, 2]}\n    with pytest.raises(AttributeError):\n        _, _ = create_compressed_model_and_algo_for_test(MockModel(), config)\n\n\ndef test_magnitude_algo_set_binary_mask_on_forward():\n    config = get_basic_magnitude_sparsity_config()\n    config[\'compression\'][\'weight_importance\'] = \'abs\'\n    sparse_model, compression_ctrl = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n    compression_ctrl.set_sparsity_level(0.3)\n    with torch.no_grad():\n        sparse_model(torch.ones([1, 1, 10, 10]))\n\n    op = sparse_model.conv1.pre_ops[\'0\']\n    check_equal(ref_mask_1, op.operand.binary_mask)\n\n    op = sparse_model.conv2.pre_ops[\'0\']\n    check_equal(ref_mask_2, op.operand.binary_mask)\n\n\ndef test_magnitude_algo_binary_masks_are_applied():\n    model = BasicConvTestModel()\n    config = get_empty_config()\n    config[\'compression\'][\'algorithm\'] = ""magnitude_sparsity""\n    compressed_model, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    minfo_list = compression_ctrl.sparsified_module_info  # type: List[SparseModuleInfo]\n    minfo = minfo_list[0]  # type: SparseModuleInfo\n\n    minfo.operand.binary_mask = torch.ones_like(minfo.module.weight)  # 1x1x2x2\n    input_ = torch.ones(size=(1, 1, 5, 5))\n    ref_output_1 = -4 * torch.ones(size=(2, 4, 4))\n    output_1 = compressed_model(input_)\n    assert torch.all(torch.eq(output_1, ref_output_1))\n\n    minfo.operand.binary_mask[0][0][0][1] = 0\n    minfo.operand.binary_mask[1][0][1][0] = 0\n    ref_output_2 = - 3 * torch.ones_like(ref_output_1)\n    output_2 = compressed_model(input_)\n    assert torch.all(torch.eq(output_2, ref_output_2))\n\n    minfo.operand.binary_mask[1][0][0][1] = 0\n    ref_output_3 = ref_output_2.clone()\n    ref_output_3[1] = -2 * torch.ones_like(ref_output_1[1])\n    output_3 = compressed_model(input_)\n    assert torch.all(torch.eq(output_3, ref_output_3))\n'"
pytorch_toolkit/nncf/tests/sparsity/magnitude/test_helpers.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom torch import nn\n\nfrom nncf.config import Config\nfrom tests.quantization.test_functions import check_equal\nfrom tests.test_helpers import create_conv\n\n\nclass MagnitudeTestModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = create_conv(1, 2, 2, 9, -2)\n        self.conv2 = create_conv(2, 1, 3, -10, 0)\n\n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n\n\ndef test_magnitude_model_has_expected_params():\n    model = MagnitudeTestModel()\n    act_weights_1 = model.conv1.weight.data\n    act_weights_2 = model.conv2.weight.data\n    act_bias_1 = model.conv1.bias.data\n    act_bias_2 = model.conv2.bias.data\n\n    sub_tensor = torch.tensor([[[[10., 9.],\n                                 [9., 10.]]]])\n    ref_weights_1 = torch.cat((sub_tensor, sub_tensor), 0)\n    sub_tensor = torch.tensor([[[[-9., -10., -10.],\n                                 [-10., -9., -10.],\n                                 [-10., -10., -9.]]]])\n    ref_weights_2 = torch.cat((sub_tensor, sub_tensor), 1)\n\n    check_equal(act_weights_1, ref_weights_1)\n    check_equal(act_weights_2, ref_weights_2)\n\n    check_equal(act_bias_1, torch.tensor([-2., -2]))\n    check_equal(act_bias_2, torch.tensor([0]))\n\n\ndef get_basic_magnitude_sparsity_config(input_sample_size=(1, 1, 4, 4)):\n    config = Config()\n    config.update({\n        ""model"": ""basic_sparse_conv"",\n        ""input_info"":\n            {\n                ""sample_size"": input_sample_size,\n            },\n        ""compression"":\n            {\n                ""algorithm"": ""magnitude_sparsity"",\n                ""params"": {}\n            }\n    })\n    return config\n'"
pytorch_toolkit/nncf/tests/sparsity/magnitude/test_modules.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport pytest\nimport torch\nfrom torch import nn\n\nfrom nncf.layers import NNCFConv2d, NNCFLinear\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.sparsity.layers import BinaryMask\nfrom nncf.sparsity.magnitude.functions import normed_magnitude, abs_magnitude, calc_magnitude_binary_mask\nfrom tests.test_helpers import fill_conv_weight, fill_linear_weight, fill_bias\n\n\nclass TestModel(nn.Module):\n    def __init__(self, layer):\n        super().__init__()\n        self.layer = layer\n        sparsifier = BinaryMask(layer.weight.size())\n        self.op_key = self.layer.register_pre_forward_operation(UpdateWeight(sparsifier))\n\n    @property\n    def sparsifier(self):\n        return self.layer.get_pre_op(self.op_key).operand\n\n    def forward(self, x):\n        return self.layer(x)\n\n\n@pytest.mark.parametrize(\n    (\'weight_importance\', \'threshold\', \'ref_output\'),\n    (\n        (None, None, 38),\n        (normed_magnitude, 10, 0),\n        (normed_magnitude, 9, 0),\n        (normed_magnitude, 0.5, 20),\n        (normed_magnitude, 0.4, 38),\n        (abs_magnitude, 10, 0),\n        (abs_magnitude, 9, 20),\n        (abs_magnitude, 0.5, 38),\n        (abs_magnitude, 0.4, 38)\n    )\n)\ndef test_can_infer_magnitude_sparse_conv(weight_importance, threshold, ref_output):\n    nncf_module = NNCFConv2d(1, 1, 2)\n    sparse_model = TestModel(nncf_module)\n    sparsifier = sparse_model.sparsifier\n    fill_conv_weight(nncf_module, 9)\n    fill_bias(nncf_module, 0)\n\n    if threshold is not None:\n        sparsifier.binary_mask = calc_magnitude_binary_mask(sparse_model.layer.weight,\n                                                            weight_importance,\n                                                            threshold)\n\n    act_output = sparse_model(torch.ones([1, 1, 2, 2]))\n    assert act_output.item() == ref_output\n\n\n@pytest.mark.parametrize(\n    (\'weight_importance\', \'threshold\', \'ref_output\'),\n    (\n        (None, None, 37),\n        (normed_magnitude, 10, 0),\n        (normed_magnitude, 9, 0),\n        (normed_magnitude, 0.5, 10),\n        (normed_magnitude, 0.4, 37),\n        (abs_magnitude, 10, 0),\n        (abs_magnitude, 9, 10),\n        (abs_magnitude, 0.5, 37),\n        (abs_magnitude, 0.4, 37)\n    )\n)\ndef test_can_infer_magnitude_sparse_linear(weight_importance, threshold, ref_output):\n    nncf_module = NNCFLinear(4, 1)\n    sparse_model = TestModel(nncf_module)\n    sparsifier = sparse_model.sparsifier\n    fill_linear_weight(nncf_module, 9)\n    fill_bias(nncf_module, 0)\n\n    if threshold is not None:\n        sparsifier.binary_mask = calc_magnitude_binary_mask(sparse_model.layer.weight,\n                                                            weight_importance,\n                                                            threshold)\n\n    act_output = sparse_model(torch.ones([1, 4]))\n    assert act_output.item() == ref_output\n'"
pytorch_toolkit/nncf/tests/sparsity/magnitude/test_scheduler.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport pytest\n\nfrom nncf.sparsity.schedulers import MultiStepSparsityScheduler\nfrom tests.sparsity.magnitude.test_helpers import MagnitudeTestModel, get_basic_magnitude_sparsity_config\nfrom tests.test_helpers import get_empty_config, create_compressed_model_and_algo_for_test\n\n\ndef get_multistep_normed_abs_config():\n    config = get_basic_magnitude_sparsity_config()\n    compression_config = config[\'compression\']\n    compression_config[\'weight_importance\'] = \'normed_abs\'\n    compression_config[\'params\'] = {\n        \'schedule\': \'multistep\',\n        \'steps\': [1, 3],\n        \'sparsity_levels\': [0.1, 0.5, 0.9]\n    }\n    return config\n\n\ndef test_magnitude_scheduler_can_do_epoch_step__with_norm():\n    _ = MagnitudeTestModel()\n    config = get_multistep_normed_abs_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n    scheduler = compression_ctrl.scheduler\n    assert isinstance(scheduler, MultiStepSparsityScheduler)\n\n    assert pytest.approx(compression_ctrl.sparsity_level) == 0.1\n    assert compression_ctrl.threshold == pytest.approx(0.219, 0.01)\n    assert scheduler.prev_ind == 0\n\n    scheduler.epoch_step()\n    assert compression_ctrl.sparsity_level == 0.5\n    assert compression_ctrl.threshold == pytest.approx(0.243, 0.01)\n    assert scheduler.prev_ind == 1\n\n    scheduler.epoch_step()\n    assert compression_ctrl.sparsity_level == 0.5\n    assert compression_ctrl.threshold == pytest.approx(0.243, 0.01)\n    assert scheduler.prev_ind == 1\n\n    scheduler.epoch_step()\n    assert compression_ctrl.sparsity_level == 0.9\n    assert compression_ctrl.threshold == pytest.approx(0.371, 0.01)\n    assert scheduler.prev_ind == 2\n\n\ndef test_magnitude_scheduler_can_do_epoch_step__with_last():\n    _ = MagnitudeTestModel()\n    config = get_multistep_normed_abs_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n    scheduler = compression_ctrl.scheduler\n\n    scheduler.epoch_step(3)\n    assert scheduler.prev_ind == 2\n    assert compression_ctrl.sparsity_level == 0.9\n    assert compression_ctrl.threshold == pytest.approx(0.371, 0.01)\n\n    scheduler.epoch_step()\n    assert scheduler.prev_ind == 2\n    assert compression_ctrl.sparsity_level == 0.9\n    assert compression_ctrl.threshold == pytest.approx(0.371, 0.01)\n\n\ndef test_magnitude_scheduler_can_do_epoch_step__with_multistep():\n    _ = MagnitudeTestModel()\n    config = get_empty_config()\n    config[""compression""] = {""algorithm"": ""magnitude_sparsity"", ""params"": {""schedule"": ""multistep"", \'steps\': [1]}}\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(MagnitudeTestModel(), config)\n    scheduler = compression_ctrl.scheduler\n    assert isinstance(scheduler, MultiStepSparsityScheduler)\n    assert pytest.approx(compression_ctrl.sparsity_level) == 0.1\n    assert scheduler.sparsity_levels == [0.1, 0.5]\n    scheduler.epoch_step()\n    assert compression_ctrl.sparsity_level == 0.5\n    scheduler.epoch_step()\n    assert compression_ctrl.sparsity_level == 0.5\n'"
pytorch_toolkit/nncf/tests/sparsity/rb/__init__.py,0,b''
pytorch_toolkit/nncf/tests/sparsity/rb/test_algo.py,0,"b'""""""\n Copyright (c) 2019-2020 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport pytest\nimport torch\nfrom copy import deepcopy\nfrom pytest import approx\nfrom torch import nn\n\nfrom nncf.config import Config\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.sparsity.rb.algo import RBSparsityController\nfrom nncf.sparsity.rb.layers import RBSparsifyingWeight\nfrom nncf.sparsity.rb.loss import SparseLoss\nfrom nncf.sparsity.schedulers import PolynomialSparseScheduler\nfrom tests.test_helpers import BasicConvTestModel, TwoConvTestModel, create_compressed_model_and_algo_for_test, \\\n    check_correct_nncf_modules_replacement\n\n\ndef get_basic_sparsity_config(model_size=4, input_sample_size=(1, 1, 4, 4),\n                              sparsity_init=0.02, sparsity_target=0.5, sparsity_steps=2, sparsity_training_steps=3):\n    config = Config()\n    config.update({\n        ""model"": ""basic_sparse_conv"",\n        ""model_size"": model_size,\n        ""input_info"":\n            {\n                ""sample_size"": input_sample_size,\n            },\n        ""compression"":\n            {\n                ""algorithm"": ""rb_sparsity"",\n                ""params"":\n                    {\n                        ""schedule"": ""polynomial"",\n                        ""sparsity_init"": sparsity_init,\n                        ""sparsity_target"": sparsity_target,\n                        ""sparsity_steps"": sparsity_steps,\n                        ""sparsity_training_steps"": sparsity_training_steps\n                    },\n\n                ""layers"":\n                    {\n                        ""conv"": {""sparsify"": True},\n                    }\n            }\n    })\n    return config\n\n\ndef test_can_load_sparse_algo__with_defaults():\n    model = BasicConvTestModel()\n    config = get_basic_sparsity_config()\n    sparse_model, compression_ctrl = create_compressed_model_and_algo_for_test(deepcopy(model), config)\n    assert isinstance(compression_ctrl, RBSparsityController)\n\n    _, sparse_model_conv = check_correct_nncf_modules_replacement(model, sparse_model)\n\n    for sparse_module in sparse_model_conv.values():\n        store = []\n        for op in sparse_module.pre_ops.values():\n            if isinstance(op, UpdateWeight) and isinstance(op.operand, RBSparsifyingWeight):\n                assert torch.allclose(op.operand.binary_mask, torch.ones_like(sparse_module.weight))\n                assert op.operand.sparsify\n                assert op.__class__.__name__ not in store\n                store.append(op.__class__.__name__)\n\n\ndef test_can_set_sparse_layers_to_loss():\n    model = BasicConvTestModel()\n    config = get_basic_sparsity_config()\n    config[\'compression\'][\'train_phase\'] = \'\'\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    loss = compression_ctrl.loss\n    assert isinstance(loss, SparseLoss)\n    #pylint: disable=protected-access\n    for layer in loss._sparse_layers:\n        assert isinstance(layer, RBSparsifyingWeight)\n\n\ndef test_sparse_algo_does_not_replace_not_conv_layer():\n    class TwoLayersTestModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = nn.Conv2d(1, 1, 1)\n            self.bn = nn.BatchNorm2d(1)\n\n        def forward(self, x):\n            return self.bn(self.conv(x))\n\n    model = TwoLayersTestModel()\n    config = get_basic_sparsity_config()\n    config[\'compression\'][\'train_phase\'] = \'\'\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n    assert isinstance(compression_ctrl, RBSparsityController)\n    for m in compression_ctrl.sparsified_module_info:\n        assert isinstance(m.operand, RBSparsifyingWeight)\n\n\ndef test_can_create_sparse_loss_and_scheduler():\n    model = BasicConvTestModel()\n\n    config = get_basic_sparsity_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    loss = compression_ctrl.loss\n    assert isinstance(loss, SparseLoss)\n    assert not loss.disabled\n    assert loss.target_sparsity_rate == approx(0.02)\n    assert loss.p == approx(0.05)\n\n    scheduler = compression_ctrl.scheduler\n    assert isinstance(scheduler, PolynomialSparseScheduler)\n    assert scheduler.current_sparsity_level == approx(0.02)\n    assert scheduler.max_sparsity == approx(0.5)\n    assert scheduler.max_step == 2\n    assert scheduler.sparsity_training_steps == 3\n\n\ndef test_sparse_algo_can_calc_sparsity_rate__for_basic_model():\n    model = BasicConvTestModel()\n\n    config = get_basic_sparsity_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    assert compression_ctrl.sparsified_weights_count == model.weights_num\n    assert compression_ctrl.sparsity_rate_for_model == (\n        1 - (model.nz_weights_num + model.nz_bias_num) / (model.weights_num + model.bias_num)\n    )\n    assert compression_ctrl.sparsity_rate_for_sparsified_modules == 1 - model.nz_weights_num / model.weights_num\n    assert len(compression_ctrl.sparsified_module_info) == 1\n\n\ndef test_sparse_algo_can_collect_sparse_layers():\n    model = TwoConvTestModel()\n\n    config = get_basic_sparsity_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    assert len(compression_ctrl.sparsified_module_info) == 2\n\n\ndef test_sparse_algo_can_calc_sparsity_rate__for_2_conv_model():\n    model = TwoConvTestModel()\n\n    config = get_basic_sparsity_config()\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(model, config)\n\n    assert compression_ctrl.sparsified_weights_count == model.weights_num\n    assert compression_ctrl.sparsity_rate_for_model == (\n        1 - (model.nz_weights_num + model.nz_bias_num) / (model.weights_num + model.bias_num)\n    )\n    assert compression_ctrl.sparsity_rate_for_sparsified_modules == 1 - model.nz_weights_num / model.weights_num\n\n\ndef test_scheduler_can_do_epoch_step__with_rb_algo():\n    config = Config()\n    config[\'input_info\'] = [{""sample_size"": [1, 1, 32, 32]}]\n    config[\'compression\'][\'algorithm\'] = \'rb_sparsity\'\n\n    config[\'compression\'][""params""] = {\n        \'schedule\': \'polynomial\',\n        \'power\': 1, \'sparsity_steps\': 2, \'sparsity_init\': 0.2, \'sparsity_target\': 0.6,\n        \'sparsity_training_steps\': 4\n    }\n\n    _, compression_ctrl = create_compressed_model_and_algo_for_test(BasicConvTestModel(), config)\n    scheduler = compression_ctrl.scheduler\n    loss = compression_ctrl.loss\n\n    assert pytest.approx(loss.target_sparsity_rate) == 0.2\n    assert not loss.disabled\n\n    for module_info in compression_ctrl.sparsified_module_info:\n        assert module_info.operand.sparsify\n    scheduler.epoch_step()\n    assert pytest.approx(loss.target_sparsity_rate, abs=1e-3) == 0.4\n    assert pytest.approx(loss().item(), abs=1e-3) == 64\n    assert not loss.disabled\n\n    scheduler.epoch_step()\n    assert pytest.approx(loss.target_sparsity_rate, abs=1e-3) == 0.6\n    assert pytest.approx(loss().item(), abs=1e-3) == 144\n    assert not loss.disabled\n\n    scheduler.epoch_step()\n    assert not loss.disabled\n    assert loss.target_sparsity_rate == 0.6\n    assert loss().item() == 144\n\n    scheduler.epoch_step()\n    assert loss.disabled\n    assert loss.target_sparsity_rate == 0.6\n    assert loss() == 0\n    for module_info in compression_ctrl.sparsified_module_info:\n        assert not module_info.operand.sparsify\n'"
pytorch_toolkit/nncf/tests/sparsity/rb/test_components.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport pytest\nimport torch\nfrom torch import nn\n\nfrom nncf.layers import NNCFConv2d, NNCFLinear, NNCFConvTranspose2d\nfrom nncf.module_operations import UpdateWeight\nfrom nncf.sparsity.rb.layers import RBSparsifyingWeight\nfrom nncf.sparsity.rb.loss import SparseLoss\n\n\nclass TestModel(nn.Module):\n    def __init__(self, layer, sparsify, size=1):\n        super().__init__()\n        self.size = size\n        self.layer = layer\n        if sparsify is None:\n            sparsifier = RBSparsifyingWeight(size=size)\n        else:\n            sparsifier = RBSparsifyingWeight(sparsify=sparsify, size=size)\n        self.op_key = self.layer.register_pre_forward_operation(UpdateWeight(sparsifier))\n\n    @property\n    def sparsifier(self):\n        return self.layer.get_pre_op(self.op_key).operand\n\n    def forward(self, x):\n        return self.layer(x)\n\n\ndef sparse_model(module, sparsify, size=1):\n    layer = module(size, size, size)\n    return TestModel(layer, sparsify, size)\n\n\n@pytest.mark.parametrize(\'module\',\n                         [NNCFLinear, NNCFConv2d, NNCFConvTranspose2d])\nclass TestSparseModules:\n    def test_create_loss__with_defaults(self, module):\n        model = sparse_model(module, None)\n        loss = SparseLoss([model.sparsifier])\n        assert not loss.disabled\n        assert loss.target_sparsity_rate == 0\n        assert loss.p == 0.05\n\n    @pytest.mark.parametrize((\'mask_value\', \'ref_loss\'),\n                             ((None, 1),\n                              (0, 0),\n                              (0.3, 1),\n                              (-0.3, 0)), ids=(\'default\', \'zero\', \'positive\', \'negative\'))\n    def test_can_forward_sparse_module__with_frozen_mask(self, module, mask_value, ref_loss):\n        model = sparse_model(module, False)\n        sm = model.layer\n        sm.weight.data.fill_(1)\n        sm.bias.data.fill_(0)\n        sw = model.sparsifier\n        if mask_value is not None:\n            new_mask = torch.zeros_like(sw.mask)\n            new_mask.fill_(mask_value)\n            sw.mask = new_mask\n        input_ = torch.ones([1, 1, 1, 1])\n        assert model(input_).item() == ref_loss\n\n    @pytest.mark.parametrize((\'sparsify\', \'raising\'), ((None, True), (True, False), (False, True)),\n                             ids=(\'default\', \'sparsify\', \'frozen\'))\n    def test_calc_loss(self, module, sparsify, raising):\n        model = sparse_model(module, sparsify)\n        sw = model.sparsifier\n        assert sw.sparsify is (False if sparsify is None else sparsify)\n        loss = SparseLoss([model.sparsifier])\n        try:\n            assert loss() == 0\n        except ZeroDivisionError:\n            pytest.fail(""Division by zero"")\n        except AssertionError:\n            if not raising:\n                pytest.fail(""Exception is not expected"")\n\n    @pytest.mark.parametrize(\'sparsify\', (None, True, False), ids=(\'default\', \'sparsify\', \'frozen\'))\n    class TestWithSparsify:\n        def test_can_freeze_mask(self, module, sparsify):\n            model = sparse_model(module, sparsify)\n            sw = model.sparsifier\n            if sparsify is None:\n                sparsify = False\n            assert sw.sparsify is sparsify\n            assert sw.mask.numel() == 1\n\n        def test_disable_loss(self, module, sparsify):\n            model = sparse_model(module, sparsify)\n            sw = model.sparsifier\n            assert sw.sparsify is (False if sparsify is None else sparsify)\n            loss = SparseLoss([model.sparsifier])\n            loss.disable()\n            assert not sw.sparsify\n\n    @pytest.mark.parametrize((\'target\', \'expected_rate\'),\n                             ((None, 0),\n                              (0, 1),\n                              (0.5, 0.5),\n                              (1, 0),\n                              (1.5, None),\n                              (-0.5, None)),\n                             ids=(\'default\', \'min\', \'middle\', \'max\', \'more_than_max\', \'less_then_min\'))\n    def test_get_target_sparsity_rate(self, module, target, expected_rate):\n        model = sparse_model(module, None)\n        loss = SparseLoss([model.sparsifier])\n        if target is not None:\n            loss.target = target\n        actual_rate = None\n        try:\n            actual_rate = loss.target_sparsity_rate\n            if expected_rate is None:\n                pytest.fail(""Exception should be raised"")\n        except IndexError:\n            if expected_rate is not None:\n                pytest.fail(""Exception is not expected"")\n        if expected_rate is not None:\n            assert actual_rate == expected_rate\n'"
pytorch_toolkit/nncf/tests/sparsity/rb/test_weights.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport pytest\nimport torch\n\nfrom nncf.functions import logit\nfrom nncf.sparsity.rb.layers import RBSparsifyingWeight\n\ndefault_mask = logit(torch.ones(1) * 0.99)\n\n\ndef test_can_create_sparse_weight__with_defaults():\n    sw = RBSparsifyingWeight(1)\n    assert not sw.sparsify\n    assert torch.allclose(default_mask, sw.mask)\n    assert not sw.mask.requires_grad\n    assert sw.eps == 1e-6\n\n\ndef test_can_freeze_mask():\n    sw = RBSparsifyingWeight(1, sparsify=True)\n    assert sw.sparsify\n    assert sw.mask.requires_grad\n    sw.sparsify = False\n    assert not sw.sparsify\n    # NOTE: should be False, but we didn\'t experiment on real models\n    assert sw.mask.requires_grad\n\n\n@pytest.mark.parametrize(\'sparsify\', (True, False), ids=(\'sparsify\', \'frozen\'))\nclass TestWithSparsify:\n    @pytest.mark.parametrize(\'is_train\', (True, False), ids=(\'train\', \'not_train\'))\n    def test_mask_is_not_updated_on_forward(self, sparsify, is_train):\n        sw = RBSparsifyingWeight(1, sparsify=sparsify)\n        if is_train:\n            sw.train()\n        assert torch.allclose(default_mask, sw.mask)\n        w = torch.ones(1)\n        sw.forward(w)\n        assert torch.allclose(default_mask, sw.mask)\n\n    @pytest.mark.parametrize((\'mask_value\', \'ref_loss\'),\n                             ((None, 1),\n                              (0, 0),\n                              (0.3, 1),\n                              (-0.3, 0)), ids=(\'default\', \'zero\', \'positive\', \'negative\'))\n    def test_loss_value(self, mask_value, ref_loss, sparsify):\n        sw = RBSparsifyingWeight(1, sparsify=sparsify)\n        if mask_value is not None:\n            tensor_to_set = torch.zeros_like(sw.mask)\n            tensor_to_set.fill_(mask_value)\n            sw.mask = tensor_to_set\n        assert sw.loss() == ref_loss\n        w = torch.ones(1)\n        assert sw.apply_binary_mask(w) == ref_loss\n        sw.sparsify = False\n        assert sw.forward(w) == ref_loss\n'"
pytorch_toolkit/text_spotting/text_spotting/models/backbones/__init__.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport logging\n\nfrom text_spotting.models.backbones.efficientnet import EfficientNet\nfrom text_spotting.models.backbones.mobilenet_v2 import MobileNetV2\nfrom text_spotting.models.backbones.mobilenet_v3 import mobilenetv3_large\nfrom text_spotting.models.backbones.resnet import resnet50\n\n\ndef get_backbone(name, **kwargs):\n    if name == \'mobilenet_v2\':\n        backbone = MobileNetV2()\n        # backbone.freeze_stages_params(range(2))\n        # backbone.freeze_stages_bns(range(19))\n        backbone.set_output_stages((3, 6, 13, 18))\n    elif name == \'resnet50\':\n        replace_stride_with_dilation = None\n        if \'replace_stride_with_dilation\' in kwargs:\n            replace_stride_with_dilation = kwargs[\'replace_stride_with_dilation\']\n\n        backbone = resnet50(pretrained=True, progress=True,\n                            replace_stride_with_dilation=replace_stride_with_dilation)\n        output_stages = (1, 2, 3, 4)\n\n    elif name == \'mobilenet_v3_large\':\n        backbone = mobilenetv3_large(shape=kwargs[\'shape\'])\n        # backbone.freeze_stages_params(range(2))\n        # backbone.freeze_stages_bns(range(16))\n        backbone.set_output_stages((3, 6, 12, 15))\n    elif name == \'efficientnet_b0\':\n        backbone = EfficientNet.from_name(\'efficientnet-b0\', shape=kwargs[\'shape\'])\n        backbone.freeze_stages_params(range(2))\n        backbone.freeze_stages_bns(range(17))\n        backbone.set_output_stages((3, 5, 11, 16))\n    else:\n        raise IOError(f\'Invalid backbone name {name}\')\n\n    if \'freeze_stages_params\' in kwargs:\n        logging.info(f\'Freezing stages params {kwargs[""freeze_stages_params""]}\')\n        backbone.freeze_stages_params(range(kwargs[\'freeze_stages_params\']))\n    if \'freeze_stages_bns\' in kwargs:\n        logging.info(f\'Freezing stages bns {kwargs[""freeze_stages_bns""]}\')\n        backbone.freeze_stages_bns(range(kwargs[\'freeze_stages_bns\']))\n\n    if \'output_stages\' in kwargs:\n        output_stages = kwargs[\'output_stages\']\n    logging.info(f\'Setting output stages {output_stages}\')\n    backbone.set_output_stages(output_stages)\n\n    return backbone\n'"
pytorch_toolkit/text_spotting/text_spotting/models/backbones/efficientnet.py,0,"b'# https://github.com/lukemelas/EfficientNet-PyTorch (Apache 2.0)\n\nimport re\nimport math\nimport collections\nfrom functools import partial\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import model_zoo\n\nfrom segmentoly.rcnn.backbones.backbone import Backbone\n\n\n########################################################################\n############### HELPERS FUNCTIONS FOR MODEL ARCHITECTURE ###############\n########################################################################\n\n\n# Parameters for the entire model (stem, all blocks, and head)\nGlobalParams = collections.namedtuple(\'GlobalParams\', [\n    \'batch_norm_momentum\', \'batch_norm_epsilon\', \'dropout_rate\',\n    \'num_classes\', \'width_coefficient\', \'depth_coefficient\',\n    \'depth_divisor\', \'min_depth\', \'drop_connect_rate\', \'image_size\'])\n\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\', \'num_repeat\', \'input_filters\', \'output_filters\',\n    \'expand_ratio\', \'id_skip\', \'stride\', \'se_ratio\'])\n\n\n# Change namedtuple defaults\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\ndef relu_fn(x):\n    """""" Swish activation function """"""\n    return x * torch.sigmoid(x)\n\ndef round_filters(filters, global_params):\n    """""" Calculate and round number of filters based on depth multiplier. """"""\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    """""" Round number of filters based on depth multiplier. """"""\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    """""" Drop connect. """"""\n    if not training: return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\ndef get_same_padding_conv2d(image_size=None):\n    """""" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n        Static padding is necessary for ONNX exporting of models. """"""\n    if image_size is None:\n        return Conv2dDynamicSamePadding\n    else:\n        return partial(Conv2dStaticSamePadding, image_size=image_size)\n\nclass Conv2dDynamicSamePadding(nn.Conv2d):\n    """""" 2D Convolutions like TensorFlow, for a dynamic image size """"""\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Conv2dStaticSamePadding(nn.Conv2d):\n    """""" 2D Convolutions like TensorFlow, for a fixed image size""""""\n    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n        # Calculate padding based on image size and save it\n        assert image_size is not None\n        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n        else:\n            self.static_padding = Identity()\n\n    def forward(self, x):\n        x = self.static_padding(x)\n        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        return x\n\n\nclass Identity(nn.Module):\n    def __init__(self,):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\n########################################################################\n############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n########################################################################\n\n\ndef efficientnet_params(model_name):\n    """""" Map EfficientNet model name to parameter coefficients. """"""\n    params_dict = {\n        # Coefficients:   width,depth,res,dropout\n        \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n        \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n        \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n        \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n        \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n        \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n        \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n        \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n    }\n    return params_dict[model_name]\n\n\nclass BlockDecoder(object):\n    """""" Block Decoder for readability, straight from the official TensorFlow repository """"""\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        """""" Gets a block through a string notation of arguments. """"""\n        assert isinstance(block_string, str)\n\n        ops = block_string.split(\'_\')\n        options = {}\n        for op in ops:\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        # Check stride\n        assert ((\'s\' in options and len(options[\'s\']) == 1) or\n                (len(options[\'s\']) == 2 and options[\'s\'][0] == options[\'s\'][1]))\n\n        return BlockArgs(\n            kernel_size=int(options[\'k\']),\n            num_repeat=int(options[\'r\']),\n            input_filters=int(options[\'i\']),\n            output_filters=int(options[\'o\']),\n            expand_ratio=int(options[\'e\']),\n            id_skip=(\'noskip\' not in block_string),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=[int(options[\'s\'][0])])\n\n    @staticmethod\n    def _encode_block_string(block):\n        """"""Encodes a block to a string.""""""\n        args = [\n            \'r%d\' % block.num_repeat,\n            \'k%d\' % block.kernel_size,\n            \'s%d%d\' % (block.strides[0], block.strides[1]),\n            \'e%s\' % block.expand_ratio,\n            \'i%d\' % block.input_filters,\n            \'o%d\' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append(\'se%s\' % block.se_ratio)\n        if block.id_skip is False:\n            args.append(\'noskip\')\n        return \'_\'.join(args)\n\n    @staticmethod\n    def decode(string_list):\n        """"""\n        Decodes a list of string notations to specify blocks inside the network.\n        :param string_list: a list of strings, each string is a notation of block\n        :return: a list of BlockArgs namedtuples of block args\n        """"""\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        """"""\n        Encodes a list of BlockArgs to a list of strings.\n        :param blocks_args: a list of BlockArgs namedtuples of block args\n        :return: a list of strings, each string is a notation of block\n        """"""\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n\n\ndef efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n    """""" Creates a efficientnet model. """"""\n\n    blocks_args = [\n        \'r1_k3_s11_e1_i32_o16_se0.25\', \'r2_k3_s22_e6_i16_o24_se0.25\',\n        \'r2_k5_s22_e6_i24_o40_se0.25\', \'r3_k3_s22_e6_i40_o80_se0.25\',\n        \'r3_k5_s11_e6_i80_o112_se0.25\', \'r4_k5_s22_e6_i112_o192_se0.25\',\n        \'r1_k3_s11_e6_i192_o320_se0.25\',\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args)\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        # data_format=\'channels_last\',  # removed, this is always true in PyTorch\n        num_classes=num_classes,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None,\n        image_size=image_size,\n    )\n\n    return blocks_args, global_params\n\n\ndef get_model_params(model_name, override_params):\n    """""" Get the block args and global params for a given model """"""\n    if model_name.startswith(\'efficientnet\'):\n        w, d, s, p = efficientnet_params(model_name)\n        # note: all models have drop connect rate = 0.2\n        blocks_args, global_params = efficientnet(\n            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n    else:\n        raise NotImplementedError(\'model name is not pre-defined: %s\' % model_name)\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included in global_params.\n        global_params = global_params._replace(**override_params)\n    return blocks_args, global_params\n\n\nurl_map = {\n    \'efficientnet-b0\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b0-355c32eb.pth\',\n    \'efficientnet-b1\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b1-f1951068.pth\',\n    \'efficientnet-b2\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b2-8bb594d6.pth\',\n    \'efficientnet-b3\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b3-5fb5a3c3.pth\',\n    \'efficientnet-b4\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b4-6ed6700e.pth\',\n    \'efficientnet-b5\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b5-b6417697.pth\',\n    \'efficientnet-b6\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b6-c76e70fd.pth\',\n    \'efficientnet-b7\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b7-dcc49843.pth\',\n}\n\ndef load_pretrained_weights(model, model_name, load_fc=True):\n    """""" Loads pretrained weights, and downloads if loading for the first time. """"""\n    state_dict = model_zoo.load_url(url_map[model_name])\n    if load_fc:\n        model.load_state_dict(state_dict)\n    else:\n        state_dict.pop(\'_fc.weight\')\n        state_dict.pop(\'_fc.bias\')\n        res = model.load_state_dict(state_dict, strict=False)\n        assert str(res.missing_keys) == str([\'_fc.weight\', \'_fc.bias\']), \'issue loading pretrained weights\'\n    print(\'Loaded pretrained weights for {}\'.format(model_name))\n\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self, kernel_size):\n        super().__init__()\n        self.pool = nn.AvgPool2d(kernel_size)\n\n    def forward(self, x):\n        return self.pool(x)\n\nclass MBConvBlock(nn.Module):\n    """"""\n    Mobile Inverted Residual Bottleneck Block\n    Args:\n        block_args (namedtuple): BlockArgs, see above\n        global_params (namedtuple): GlobalParam, see above\n    Attributes:\n        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n    """"""\n\n    def __init__(self, block_args, global_params, feature_map_size=None):\n        super().__init__()\n        self._block_args = block_args\n        self._bn_mom = 1 - global_params.batch_norm_momentum\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip  # skip connection and drop connect\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Expansion phase\n        inp = self._block_args.input_filters  # number of input channels\n        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Depthwise convolution phase\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        self._depthwise_conv = Conv2d(\n            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            self._se_avg_pool = GlobalAvgPool2d(feature_map_size)\n            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        # Output phase\n        final_oup = self._block_args.output_filters\n        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        self.feature_map_size = feature_map_size\n\n    def forward(self, inputs, drop_connect_rate=None):\n        """"""\n        :param inputs: input tensor\n        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n        :return: output of block\n        """"""\n\n        # Expansion and Depthwise Convolution\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n\n        # Squeeze and Excitation\n        if self.has_se:\n            #x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            if self.feature_map_size == x.shape[2:]:\n                x_squeezed = self._se_avg_pool(x)\n            else:\n                x_squeezed = torch.nn.functional.avg_pool2d(x, x.shape[2:])\n            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._bn2(self._project_conv(x))\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n\nclass EfficientNet(Backbone):\n    """"""\n    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n    Args:\n        blocks_args (list): A list of BlockArgs to construct blocks\n        global_params (namedtuple): A set of GlobalParams shared between blocks\n    Example:\n        model = EfficientNet.from_pretrained(\'efficientnet-b0\')\n    """"""\n\n    class Stages(nn.Module):\n        def __init__(self, obj):\n            super().__init__()\n            self._conv_stem = obj._conv_stem\n            self._bn0 = obj._bn0\n            self._blocks = obj._blocks\n\n        def __getitem__(self, item):\n            if item == 0:\n                return nn.Sequential(self._conv_stem, self._bn0)\n            elif item <= len(self._blocks):\n                return self._blocks[item - 1]\n\n        def __len__(self):\n            return 1 + len(self._blocks)\n\n        def __iter__(self):\n            self._iter = 0\n            return self\n\n        def __next__(self):\n            if self._iter == self.__len__():\n                raise StopIteration\n            value = self.__getitem__(self._iter)\n            self._iter += 1\n            return value\n\n\n    def __init__(self, blocks_args=None, global_params=None, shape=(768, 1280)):\n        super().__init__()\n        assert isinstance(blocks_args, list), \'blocks_args should be a list\'\n        assert len(blocks_args) > 0, \'block args must be greater than 0\'\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Batch norm parameters\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        # Stem\n        in_channels = 3  # rgb\n        out_channels = round_filters(32, self._global_params)  # number of output channels\n        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        stage_dims_out = [out_channels]\n        scales_out = [2]\n\n        # Build blocks\n        self._blocks = nn.ModuleList([])\n        for block_args in self._blocks_args:\n\n            # Update block input and output filters based on depth multiplier.\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters, self._global_params),\n                output_filters=round_filters(block_args.output_filters, self._global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            scale = scales_out[-1] * 2 if block_args.stride[0] == 2 else scales_out[-1]\n            self._blocks.append(MBConvBlock(block_args, self._global_params, (shape[0] // scale, shape[1] // scale)))\n            stage_dims_out.append(block_args.output_filters)\n            scales_out.append(scale)\n\n            if block_args.num_repeat > 1:\n                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n            for _ in range(block_args.num_repeat - 1):\n                scale = scales_out[-1] * 2 if block_args.stride == 2 else scales_out[-1]\n                self._blocks.append(MBConvBlock(block_args, self._global_params,\n                                                (shape[0] // scale, shape[1] // scale)))\n                stage_dims_out.append(block_args.output_filters)\n                scales_out.append(scale)\n\n        # Head\n        # in_channels = block_args.output_filters  # output of final block\n        # out_channels = round_filters(1280, self._global_params)\n        # self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        # self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # stage_dims_out.append(out_channels)\n        # scales_out.append(scales_out[-1])\n\n        self._all_dims_out = tuple(stage_dims_out)\n        self._all_scales_out = tuple(scales_out)\n\n\n        # Final linear layer\n        #self._dropout = self._global_params.dropout_rate\n        #self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n\n    def forward(self, x):\n        src_width = x.shape[-1]\n        x = self._conv_stem(x)\n        x = self._bn0(x)\n        x = relu_fn(x)\n\n        outputs_per_stage = [x]\n        for stage_idx, stage in enumerate(self._blocks):\n            x = stage(x)\n            outputs_per_stage.append(x)\n\n        # x = self._conv_head(x)\n        # x = self._bn1(x)\n        # x = relu_fn(x)\n        # outputs_per_stage.append(x)\n\n        outputs = list(outputs_per_stage[i] for i in self.output_stages)\n        for output, scale in zip(outputs, self.scales_out):\n            assert output.shape[-1] * scale == src_width, \'{} * {} != {}\'.format(x.shape[-1], scale, src_width)\n        return outputs\n\n    @property\n    def stages(self):\n        return self.Stages(self)\n\n    @classmethod\n    def from_name(cls, model_name, override_params=None, shape=None):\n        cls._check_model_name_is_valid(model_name)\n        blocks_args, global_params = get_model_params(model_name, override_params)\n        return cls(blocks_args, global_params, shape=shape)\n\n    @classmethod\n    def from_pretrained(cls, model_name, num_classes=1000):\n        model = cls.from_name(model_name, override_params={\'num_classes\': num_classes})\n        load_pretrained_weights(model, model_name, load_fc=(num_classes == 1000))\n        return model\n\n    @classmethod\n    def get_image_size(cls, model_name):\n        cls._check_model_name_is_valid(model_name)\n        _, _, res, _ = efficientnet_params(model_name)\n        return res\n\n    @classmethod\n    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n        """""" Validates model name. None that pretrained weights are only available for\n        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. """"""\n        num_models = 4 if also_need_pretrained_weights else 8\n        valid_models = [\'efficientnet-b\'+str(i) for i in range(num_models)]\n        if model_name not in valid_models:\n            raise ValueError(\'model_name should be one of: \' + \', \'.join(valid_models))\n'"
pytorch_toolkit/text_spotting/text_spotting/models/backbones/mobilenet_v2.py,0,"b'# https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py\n\nfrom torch import nn\nfrom segmentoly.rcnn.backbones.backbone import Backbone\n\ndef _make_divisible(v, divisor, min_value=None):\n    """"""\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    """"""\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU6(inplace=True)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        ])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(Backbone):\n    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n        """"""\n        MobileNet V2 main class\n        Args:\n            num_classes (int): Number of classes\n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n            inverted_residual_setting: Network structure\n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n            Set to 1 to turn off rounding\n        """"""\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(""inverted_residual_setting should be non-empty ""\n                             ""or a 4-element list, got {}"".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        stage_dims_out = [input_channel]\n        scales_out = [2]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                scales_out.append(scales_out[-1] if stride == 1 else scales_out[-1] * 2)\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                stage_dims_out.append(output_channel)\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        stage_dims_out.append(self.last_channel)\n        scales_out.append(scales_out[-1])\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n\n        self._all_dims_out = tuple(stage_dims_out)\n        self._all_scales_out = tuple(scales_out)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    @property\n    def stages(self):\n        return self.features\n'"
pytorch_toolkit/text_spotting/text_spotting/models/backbones/mobilenet_v3.py,0,"b'# https://github.com/d-li14/mobilenetv3.pytorch\n\n""""""\nCreates a MobileNetV3 Model as defined in:\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\nSearching for MobileNetV3\narXiv preprint arXiv:1905.02244.\n""""""\n\nimport torch.nn as nn\nimport math\n\nfrom segmentoly.rcnn.backbones.backbone import Backbone\n\n__all__ = [\'mobilenetv3_large\', \'mobilenetv3_small\']\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    """"""\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    """"""\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass h_sigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(h_sigmoid, self).__init__()\n        self.relu = nn.ReLU6(inplace=inplace)\n\n    def forward(self, x):\n        return self.relu(x + 3) / 6\n\n\nclass h_swish(nn.Module):\n    def __init__(self, inplace=True):\n        super(h_swish, self).__init__()\n        self.sigmoid = h_sigmoid(inplace=inplace)\n\n    def forward(self, x):\n        return x * self.sigmoid(x)\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, kernel_size, reduction=4):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.avg_pool_fixed = nn.AvgPool2d(kernel_size)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel // reduction),\n                nn.ReLU(inplace=True),\n                nn.Linear(channel // reduction, channel),\n                h_sigmoid()\n        )\n        self.kernel_size = kernel_size\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        if x.shape[2:] == self.kernel_size:\n            y = self.avg_pool_fixed(x).view(b, c)\n        else:\n            y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        h_swish()\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        h_swish()\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs, shape):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n\n        self.identity = stride == 1 and inp == oup\n\n        if inp == hidden_dim:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                h_swish() if use_hs else nn.ReLU(inplace=True),\n                # Squeeze-and-Excite\n                SELayer(hidden_dim, kernel_size=shape) if use_se else nn.Sequential(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                h_swish() if use_hs else nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                # Squeeze-and-Excite\n                SELayer(hidden_dim, kernel_size=shape) if use_se else nn.Sequential(),\n                h_swish() if use_hs else nn.ReLU(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV3(Backbone):\n    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1., shape=None):\n        super(MobileNetV3, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = cfgs\n        assert mode in [\'large\', \'small\']\n\n        # building first layer\n        input_channel = _make_divisible(16 * width_mult, 8)\n        layers = [conv_3x3_bn(3, input_channel, 2)]\n        dims_out = [input_channel]\n        scales_out = [2]\n        # building inverted residual blocks\n        block = InvertedResidual\n        for k, exp_size, c, use_se, use_hs, s in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 8)\n            scale = scales_out[-1] *  2 if s == 2 else scales_out[-1]\n            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs,\n                                shape=(shape[0] // scale, shape[1] // scale)))\n            scales_out.append(scale)\n            dims_out.append(output_channel)\n            input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n\n        #building last several layers\n        self.conv = nn.Sequential(\n            conv_1x1_bn(input_channel, _make_divisible(exp_size * width_mult, 8)),\n            SELayer(\n                _make_divisible(exp_size * width_mult, 8)) if mode == \'small\' else nn.Sequential()\n        )\n        self.avgpool = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            h_swish()\n        )\n        output_channel = _make_divisible(1280 * width_mult, 8) if width_mult > 1.0 else 1280\n        self.classifier = nn.Sequential(\n            nn.Linear(_make_divisible(exp_size * width_mult, 8), output_channel),\n            nn.BatchNorm1d(output_channel) if mode == \'small\' else nn.Sequential(),\n            h_swish(),\n            nn.Linear(output_channel, num_classes),\n            nn.BatchNorm1d(num_classes) if mode == \'small\' else nn.Sequential(),\n            h_swish() if mode == \'small\' else nn.Sequential()\n        )\n\n        self._initialize_weights()\n\n        self._all_dims_out = tuple(dims_out)\n        self._all_scales_out = tuple(scales_out)\n\n    @property\n    def stages(self):\n        return self.features\n\n    # def forward(self, x):\n    #     x = self.features(x)\n    #     x = self.conv(x)\n    #     x = self.avgpool(x)\n    #     x = x.view(x.size(0), -1)\n    #     x = self.classifier(x)\n    #     return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\ndef mobilenetv3_large(**kwargs):\n    """"""\n    Constructs a MobileNetV3-Large model\n    """"""\n    cfgs = [\n        # k, t, c, SE, NL, s\n        [3,  16,  16, 0, 0, 1],\n        [3,  64,  24, 0, 0, 2],\n        [3,  72,  24, 0, 0, 1],\n        [5,  72,  40, 1, 0, 2],\n        [5, 120,  40, 1, 0, 1],\n        [5, 120,  40, 1, 0, 1],\n        [3, 240,  80, 0, 1, 2],\n        [3, 200,  80, 0, 1, 1],\n        [3, 184,  80, 0, 1, 1],\n        [3, 184,  80, 0, 1, 1],\n        [3, 480, 112, 1, 1, 1],\n        [3, 672, 112, 1, 1, 1],\n        [5, 672, 160, 1, 1, 1],\n        [5, 672, 160, 1, 1, 2],\n        [5, 960, 160, 1, 1, 1]\n    ]\n    return MobileNetV3(cfgs, mode=\'large\', **kwargs)\n\n\ndef mobilenetv3_small(**kwargs):\n    """"""\n    Constructs a MobileNetV3-Small model\n    """"""\n    cfgs = [\n        # k, t, c, SE, NL, s\n        [3,  16,  16, 1, 0, 2],\n        [3,  72,  24, 0, 0, 2],\n        [3,  88,  24, 0, 0, 1],\n        [5,  96,  40, 1, 1, 2],\n        [5, 240,  40, 1, 1, 1],\n        [5, 240,  40, 1, 1, 1],\n        [5, 120,  48, 1, 1, 1],\n        [5, 144,  48, 1, 1, 1],\n        [5, 288,  96, 1, 1, 2],\n        [5, 576,  96, 1, 1, 1],\n        [5, 576,  96, 1, 1, 1],\n    ]\n\n    return MobileNetV3(cfgs, mode=\'small\', **kwargs)'"
pytorch_toolkit/text_spotting/text_spotting/models/backbones/resnet.py,0,"b'# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.model_zoo import load_url as load_state_dict_from_url\n\nfrom torch.nn.modules.upsampling import UpsamplingBilinear2d\nfrom segmentoly.rcnn.backbones.backbone import Backbone\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\', \'resnext50_32x4d\', \'resnext101_32x8d\',\n           \'wide_resnet50_2\', \'wide_resnet101_2\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'resnext50_32x4d\': \'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\',\n    \'resnext101_32x8d\': \'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\',\n    \'wide_resnet50_2\': \'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\',\n    \'wide_resnet101_2\': \'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    __constants__ = [\'downsample\']\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        if dilation > 1:\n            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    __constants__ = [\'downsample\']\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n        #\n        # self.layer1 = self._make_layer(block, 64, layers[0])\n        # self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n        #                                dilate=replace_stride_with_dilation[0])\n        # self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n        #                                dilate=replace_stride_with_dilation[1])\n        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n        #                                dilate=replace_stride_with_dilation[2])\n\n\nclass ResNet(Backbone):\n\n    class Stages(nn.Module):\n        def __init__(self, obj):\n            super().__init__()\n            self.conv1 = obj.conv1\n            self.bn1 = obj.bn1\n            self.relu = obj.relu\n            self.maxpool = obj.maxpool\n\n            self.layer1 = obj.layer1\n            self.layer2 = obj.layer2\n            self.layer3 = obj.layer3\n            self.layer4 = obj.layer4\n\n        def __getitem__(self, item):\n            if item == 0:\n                return nn.Sequential(\n                    self.conv1,\n                    self.bn1,\n                    self.relu,\n                    self.maxpool)\n            elif item == 1:\n                return self.layer1\n            elif item == 2:\n                return self.layer2\n            elif item == 3:\n                return self.layer3\n            elif item == 4:\n                return self.layer4\n\n        def __len__(self):\n            return 5\n\n        def __iter__(self):\n            self._iter = 0\n            return self\n\n        def __next__(self):\n            if self._iter == self.__len__():\n                raise StopIteration\n            value = self.__getitem__(self._iter)\n            self._iter += 1\n            return value\n\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(""replace_stride_with_dilation should be None ""\n                             ""or a 3-element tuple, got {}"".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n\n\n\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        stage_dims_out = [self.inplanes]\n        scales_out = [4]\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        scales_out.append(4)\n        stage_dims_out.append(64 * block.expansion)\n\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        scales_out.append(scales_out[-1] * 2 if not replace_stride_with_dilation[0] else scales_out[-1])\n        stage_dims_out.append(128 * block.expansion)\n\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n\n        scales_out.append(scales_out[-1] * 2 if not replace_stride_with_dilation[1] else scales_out[-1])\n        stage_dims_out.append(256 * block.expansion)\n\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        scales_out.append(scales_out[-1] * 2 if not replace_stride_with_dilation[2] else scales_out[-1])\n        stage_dims_out.append(512 * block.expansion)\n\n        self._all_dims_out = tuple(stage_dims_out)\n        self._all_scales_out = tuple(scales_out)\n\n        print(self._all_dims_out)\n        print(self._all_scales_out)\n\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    @property\n    def stages(self):\n        return self.Stages(self)\n\n\ndef _resnet(arch, block, layers, pretrained, progress, **kwargs):\n    model = ResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls[arch],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet18(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-18 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet18\', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet34(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-34 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet34\', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet50(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-50 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet50\', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet101(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-101 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet101\', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet152(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-152 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet152\', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n    r""""""ResNeXt-50 32x4d model from\n    `""Aggregated Residual Transformation for Deep Neural Networks"" <https://arxiv.org/pdf/1611.05431.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 4\n    return _resnet(\'resnext50_32x4d\', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n    r""""""ResNeXt-101 32x8d model from\n    `""Aggregated Residual Transformation for Deep Neural Networks"" <https://arxiv.org/pdf/1611.05431.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 8\n    return _resnet(\'resnext101_32x8d\', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n    r""""""Wide ResNet-50-2 model from\n    `""Wide Residual Networks"" <https://arxiv.org/pdf/1605.07146.pdf>`_\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'width_per_group\'] = 64 * 2\n    return _resnet(\'wide_resnet50_2\', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n    r""""""Wide ResNet-101-2 model from\n    `""Wide Residual Networks"" <https://arxiv.org/pdf/1605.07146.pdf>`_\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'width_per_group\'] = 64 * 2\n    return _resnet(\'wide_resnet101_2\', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, **kwargs)\n'"
pytorch_toolkit/text_spotting/text_spotting/models/mask_rcnn/__init__.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom text_spotting.models.mask_rcnn.fpn_text_mask_rcnn import FPNTextMaskRCNN, \\\n    FPNTextMaskRCNNWithBackboneTextFeatures\n\nstr_to_class = {\n    \'FPNTextMaskRCNN\': FPNTextMaskRCNN,\n    \'FPNTextMaskRCNNWithBackboneTextFeatures\': FPNTextMaskRCNNWithBackboneTextFeatures\n}\n'"
pytorch_toolkit/text_spotting/text_spotting/models/mask_rcnn/fpn_text_mask_rcnn.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport itertools\n\nimport torch\nimport torch.nn as nn\nfrom segmentoly.rcnn.losses import detection_loss_cls, detection_loss_reg, accuracy, mask_loss\nfrom segmentoly.rcnn.model_zoo.fpn_mask_rcnn_base import FPNMaskRCNN, timed, extract_roi_features\n\n\nclass FPNTextMaskRCNN(FPNMaskRCNN):\n    weight_text_loss = 0.25\n    mask_text = False\n\n    def __init__(self, cls_num, backbone, force_max_output_size=False,\n                 group_norm=False, fc_detection_head=True, num_chars=38, **kwargs):\n        super().__init__(cls_num, backbone, force_max_output_size,\n                         group_norm, fc_detection_head, **kwargs)\n\n        self.num_chars = num_chars\n        self.text_recogn_head = self.add_text_recogn_head()\n\n        self.export_mode = False\n\n    def add_text_recogn_head(*args, **kwargs):\n        raise NotImplementedError\n\n    @timed\n    def forward(self, im_data, im_info, gt_boxes=None, gt_labels=None, gt_is_ignored=None,\n                gt_masks=None,\n                batch_idx=None, gt_texts=None, **kwargs):\n        if self.training:\n            # In case of training return a dict rather than a list.\n            return_values = {}\n        else:\n            return_values = []\n\n        im_data, im_info = self.preprocess_data(im_data, im_info, self.input_size_divisor)\n        batch_size = im_data.shape[0]\n\n        backbone_features = self.forward_backbone(im_data)\n        fpn_features = self.forward_fpn(backbone_features)\n        with torch.no_grad():\n            priors_pyramid = self.generate_priors(fpn_features, im_data)\n        rpn_cls_targets, rpn_reg_targets = self.get_rpn_targets(priors_pyramid, gt_boxes, gt_labels,\n                                                                gt_is_ignored, im_info)\n        rois, rois_probs, rpn_metrics, rpn_loss = self.forward_rpn(priors_pyramid,\n                                                                   fpn_features, im_info,\n                                                                   rpn_cls_targets, rpn_reg_targets,\n                                                                   batch_size)\n        return_values = self.update_return_values(return_values, rpn_metrics)\n        rois, rois_probs = self.process_proposals(rois, rois_probs, batch_size)\n        rois, roi_cls_targets, roi_reg_targets, roi_mask_targets, roi_text_targets = self.get_targets(\n            rois, gt_boxes, gt_labels, gt_masks, gt_texts)\n\n        # Last pyramid level is used only for RPN part of the net, so, remove it now.\n        fpn_features = fpn_features[:-1]\n        instance_heads_output, instance_heads_loss = self.forward_instance_heads(im_info,\n                                                                                 fpn_features,\n                                                                                 backbone_features,\n                                                                                 rois,\n                                                                                 roi_cls_targets,\n                                                                                 roi_reg_targets,\n                                                                                 roi_mask_targets,\n                                                                                 roi_text_targets,\n                                                                                 batch_idx=batch_idx)\n        return_values = self.update_return_values(return_values, instance_heads_output)\n\n        if self.training:\n            total_loss = rpn_loss + instance_heads_loss\n            return_values[\'losses/TOTAL\'] = total_loss.detach()\n            return return_values, total_loss\n        else:\n            return return_values\n\n    def extract_text_features(self, **kwargs):\n        text_roi_features, rois = self.extract_roi_features(\n            kwargs[\'rois\'],\n            kwargs[\'feature_pyramid\'],\n            output_size=2 * self.segmentation_roi_featuremap_resolution\n        )\n        return text_roi_features, rois\n\n    @timed\n    def get_targets(self, rois, gt_boxes, gt_labels, gt_masks, gt_texts=None):\n        roi_cls_targets, roi_reg_targets, roi_mask_targets, roi_text_targets = None, None, None, None\n        if self.training:\n            with torch.no_grad():\n                rois, roi_cls_targets, roi_reg_targets, roi_mask_targets, roi_text_targets = self.proposal_gt_matcher(\n                    rois, gt_boxes, gt_labels, gt_masks, gt_texts)\n                # Sanity checks.\n                assert len(rois) == len(roi_cls_targets)\n                assert len(rois) == len(roi_reg_targets)\n                for im_rois, im_roi_cls_targets, im_roi_reg_targets in zip(rois, roi_cls_targets,\n                                                                           roi_reg_targets):\n                    assert im_rois.shape[0] == im_roi_cls_targets.shape[0]\n                    assert im_rois.shape[0] == im_roi_reg_targets.shape[0]\n\n        return rois, roi_cls_targets, roi_reg_targets, roi_mask_targets, roi_text_targets\n\n    @timed\n    def forward_text_recogn_head(self, roi_features, text_targets=None, masks=None):\n        return self.text_recogn_head(roi_features, text_targets, masks)\n\n    def dummy_detections(self, device):\n        boxes = torch.zeros((1, 4), device=device, dtype=torch.float32)\n        classes = torch.zeros(1, device=device, dtype=torch.long)\n        scores = torch.zeros(1, device=device, dtype=torch.float32)\n        batch_ids = torch.zeros(1, device=device, dtype=torch.long)\n        raw_mask_output = torch.zeros((1, self.cls_num, self.mask_resolution, self.mask_resolution),\n                                      device=device, dtype=torch.float32)\n        raw_text_output = self.text_recogn_head.dummy_forward()\n        raw_text_output = raw_text_output.to(raw_mask_output.device)\n        return boxes, classes, scores, batch_ids, raw_mask_output, raw_text_output\n\n    @timed\n    def forward_instance_heads(self, im_info, feature_pyramid, backbone_features, rois,\n                               roi_cls_targets=None, roi_reg_targets=None,\n                               roi_mask_targets=None, roi_text_targets=None,\n                               batch_size=1, batch_idx=None):\n        detection_roi_features, rois = self.extract_roi_features(rois, feature_pyramid,\n                                                                 output_size=self.detection_roi_featuremap_resolution)\n        raw_cls_score, raw_bbox_pred = self.forward_detection_head(detection_roi_features)\n\n        if self.training:\n            return_values = {}\n            loss_cls = detection_loss_cls(raw_cls_score, roi_cls_targets)\n            all_targets = torch.cat(roi_cls_targets)\n            valid_mask = all_targets >= 0\n            accuracy_cls = accuracy(torch.argmax(raw_cls_score[valid_mask], dim=1),\n                                    all_targets[valid_mask])\n            loss_reg = detection_loss_reg(raw_bbox_pred, roi_cls_targets, roi_reg_targets,\n                                          self.detection_head.cls_agnostic_bbox_regression)\n            return_values[\'losses/detection/cls\'] = loss_cls.detach().unsqueeze(0)\n            return_values[\'losses/detection/reg\'] = loss_reg.detach().unsqueeze(0)\n            return_values[\'metrics/detection/cls_accuracy\'] = accuracy_cls.detach().unsqueeze(0)\n\n            with torch.no_grad():\n                positive_indices = (all_targets > 0).nonzero().view(-1)\n\n            if len(positive_indices) > 0:\n                with torch.no_grad():\n                    positive_mask_targets = torch.cat(roi_mask_targets, dim=0).index_select(0,\n                                                                                            positive_indices)\n                    positive_cls_targets = all_targets.index_select(0, positive_indices)\n                    all_text_targets = list(itertools.chain.from_iterable(roi_text_targets))\n                    positive_text_indices = torch.tensor(\n                        [i for i in positive_indices if all_text_targets[i]])\n                    positive_text_targets = [all_text_targets[i] for i in positive_text_indices]\n\n                mask_roi_features, rois = self.extract_roi_features(rois, feature_pyramid,\n                                                                    output_size=self.segmentation_roi_featuremap_resolution)\n                mask_roi_features = mask_roi_features[positive_indices]\n                raw_mask_output = self.forward_mask_head(mask_roi_features)\n\n                loss_mask = mask_loss(raw_mask_output, positive_cls_targets, positive_mask_targets)\n                return_values[\'losses/mask\'] = loss_mask.detach().unsqueeze(0)\n\n                loss_text, accuracy_text = torch.tensor(0.0, device=im_info.device), torch.tensor(\n                    0.0, device=im_info.device)\n                if len(positive_text_indices) > 0:\n                    text_roi_features, rois = self.extract_text_features(rois=rois,\n                                                                         backbone_features=backbone_features,\n                                                                         feature_pyramid=feature_pyramid)\n                    text_roi_features = text_roi_features[positive_text_indices]\n\n                    text_mask = None\n                    if self.mask_text:\n                        text_mask = positive_mask_targets\n                        postitve_text_indices_in_positive_indices = torch.tensor(\n                            [i for i, j in enumerate(positive_indices) if all_text_targets[j]])\n                        text_mask = text_mask[postitve_text_indices_in_positive_indices]\n                        text_mask = text_mask.unsqueeze(1)\n\n                    loss_text, accuracy_text = self.forward_text_recogn_head(text_roi_features,\n                                                                             positive_text_targets,\n                                                                             masks=text_mask)\n                    loss_text *= self.weight_text_loss\n\n                return_values[\'losses/text\'] = loss_text.detach().unsqueeze(0)\n                return_values[\'metrics/TEXT/accuracy\'] = accuracy_text.unsqueeze(0)\n                loss = (loss_cls + loss_reg + loss_mask + loss_text).unsqueeze(0)\n            else:\n                return_values[\'losses/mask\'] = torch.zeros_like(loss_cls).detach().unsqueeze(0)\n                return_values[\'losses/text\'] = torch.zeros_like(loss_cls).detach().unsqueeze(0)\n                loss = (loss_cls + loss_reg).unsqueeze(0)\n        else:\n            return_values = []\n            with torch.no_grad():\n                boxes, classes, scores, batch_ids = self.forward_detection_output(rois,\n                                                                                  raw_bbox_pred,\n                                                                                  raw_cls_score,\n                                                                                  im_info,\n                                                                                  batch_idx)\n\n            if sum(len(im_boxes) for im_boxes in boxes) == 0:\n                return_values.extend(self.dummy_detections(im_info.device))\n            else:\n                if batch_idx is not None and len(batch_idx) > 1:\n                    rois = list(\n                        [boxes.index_select(0, (batch_ids == image_id).nonzero().reshape(-1))\n                         for image_id in batch_idx])\n                else:\n                    rois = [boxes, ]\n                # Extract features for every detected box preserving the order.\n                mask_roi_features, _ = self.extract_roi_features(rois, feature_pyramid,\n                                                                 output_size=self.segmentation_roi_featuremap_resolution)\n                raw_mask_output = self.forward_mask_head(mask_roi_features)\n                text_roi_features, _ = self.extract_text_features(rois=rois,\n                                                                  backbone_features=backbone_features,\n                                                                  feature_pyramid=feature_pyramid)\n\n                text_mask = None\n                if self.mask_text:\n                    text_mask = raw_mask_output[:, 1, :, :]\n                    text_mask = text_mask.unsqueeze(1)\n\n                if self.export_mode:\n                    raw_text_output = text_roi_features\n                else:\n                    raw_text_output = self.forward_text_recogn_head(text_roi_features,\n                                                                    masks=text_mask)\n                    raw_text_output = raw_text_output.permute((1, 0, 2))\n\n                # Following stuff is workaround for model optimizer.\n                delta = 0.0000000001\n                boxes = boxes + delta\n                classes = (classes.float() + delta).long()\n                scores = scores + delta\n                raw_text_output = raw_text_output + delta\n\n                return_values.extend(\n                    (boxes, classes, scores, batch_ids, raw_mask_output, raw_text_output))\n            loss = None\n        return return_values, loss\n\n\nclass FusedTextFeatures(nn.Module):\n\n    def __init__(self, input_dims, input_scales, out_dim, out_scale, indexes):\n        super().__init__()\n\n        assert len(input_dims) == len(input_scales)\n\n        input_dims = [input_dims[i] for i in indexes]\n        input_scales = [input_scales[i] for i in indexes]\n\n        self.layers = []\n        for i, (scale, dim) in enumerate(zip(input_scales, input_dims)):\n            if scale == out_scale:\n                self.layers.append(nn.Conv2d(dim, out_dim, 1, 1))\n            elif scale > out_scale:\n                assert scale % out_scale == 0\n                self.layers.append(nn.Sequential(\n                    nn.Conv2d(dim, out_dim, 1, 1),\n                    nn.UpsamplingBilinear2d(scale_factor=scale // out_scale)\n                ))\n            else:\n                raise NotImplementedError\n\n        self.layers = nn.ModuleList(self.layers)\n\n    def forward(self, inputs):\n        assert isinstance(inputs, list)\n        outputs_sum = 0\n\n        for i, input in enumerate(inputs):\n            outputs_sum += self.layers[i](input)\n\n        return outputs_sum\n\n\nclass FPNTextMaskRCNNWithBackboneTextFeatures(FPNTextMaskRCNN):\n\n    def __init__(self, cls_num, backbone, force_max_output_size=False,\n                 group_norm=False, fc_detection_head=True, num_chars=38, **kwargs):\n        super().__init__(cls_num, backbone, force_max_output_size,\n                         group_norm, fc_detection_head, num_chars, **kwargs)\n\n        self.backbone_outputs = [0, 1]\n        self.out_scale = 4\n\n        self.fused_text_features = FusedTextFeatures(backbone.dims_out, backbone.scales_out,\n                                                     self.text_recogn_head.encoder_dim_input,\n                                                     out_scale=self.out_scale,\n                                                     indexes=self.backbone_outputs)\n\n    def extract_text_features(self, **kwargs):\n        features = [kwargs[\'backbone_features\'][i] for i in self.backbone_outputs]\n        fused_text_features = self.fused_text_features(features)\n\n        text_roi_features, rois = extract_roi_features(\n            kwargs[\'rois\'],\n            [fused_text_features],\n            pyramid_scales=[self.out_scale],\n            output_size=self.segmentation_roi_featuremap_resolution * 2,\n            sampling_ratio=2,\n            distribute_rois_between_levels=True,\n            preserve_rois_order=True,\n            use_stub=not self.training\n\n        )\n        text_roi_features = torch.cat([x for x in text_roi_features if x is not None], dim=0)\n        return text_roi_features, rois\n'"
pytorch_toolkit/text_spotting/text_spotting/models/mask_rcnn/proposal_gt_matcher.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom segmentoly.utils.boxes import jaccard, bbox_transform_inv\nfrom segmentoly.utils.segms import polys_to_mask_wrt_box\n\n\nclass ProposalGTMatcher(nn.Module):\n    def __init__(self, positive_threshold=0.5, negative_threshold=0.3,\n                 positive_fraction=0.3, ignore_threshold=0.5, batch_size=512,\n                 target_mask_size=(14, 14)):\n        super().__init__()\n        self.positive_overlap_range = (positive_threshold, 1.1)\n        self.negative_overlap_range = (0.0, negative_threshold)\n        self.ignore_threshold = ignore_threshold\n        self.ensure_closest_box = True\n        self.fg_fraction = positive_fraction\n        self.batch_size = batch_size\n        self.target_mask_size = target_mask_size\n\n    def forward(self, boxes, gt_boxes, gt_labels, gt_masks=None, gt_texts=None):\n        batch_size = len(gt_boxes)\n        assert batch_size == len(gt_labels)\n\n        sampled_boxes = []\n        cls_targets = []\n        reg_targets = []\n        mask_targets = []\n        text_targets = []\n        for idx in range(batch_size):\n            im_gt_masks = gt_masks[idx] if gt_masks is not None else None\n            im_gt_texts = gt_texts[idx] if gt_texts is not None else None\n            image_sampled_boxes, image_cls_targets, image_reg_targets, image_mask_targets, image_text_targets = \\\n                self.forward_single_image(boxes[idx], gt_boxes[idx], gt_labels[idx], im_gt_masks,\n                                          im_gt_texts)\n            sampled_boxes.append(image_sampled_boxes)\n            cls_targets.append(image_cls_targets)\n            reg_targets.append(image_reg_targets)\n            mask_targets.append(image_mask_targets)\n            text_targets.append(image_text_targets)\n        if gt_texts is None:\n            return sampled_boxes, cls_targets, reg_targets, mask_targets\n        return sampled_boxes, cls_targets, reg_targets, mask_targets, text_targets\n\n    def forward_single_image(self, boxes, gt_boxes, gt_labels, gt_masks=None, gt_texts=None):\n        device = boxes.device\n\n        # Add ground truth boxes to also sample those as positives later.\n        boxes = torch.cat((gt_boxes, boxes.view(-1, 4)), dim=0)\n        boxes_num = boxes.shape[0]\n        box_to_label = torch.zeros(boxes_num, dtype=torch.long, device=device)\n\n        if len(gt_boxes) > 0:\n            # Compute overlaps between the boxes and the GT boxes.\n            box_by_gt_overlap = jaccard(boxes, gt_boxes)\n            # For each box, amount of overlap with most overlapping GT box\n            # and mapping from box to GT box that has highest overlap.\n            box_to_gt_max, box_to_gt_idx = box_by_gt_overlap.max(dim=1)\n            matched_boxes_mask = box_to_gt_max > 0\n            # Record max overlaps with the class of the appropriate gt box\n            matched_box_to_label = gt_labels[box_to_gt_idx[matched_boxes_mask]]\n            box_to_label.masked_scatter_(matched_boxes_mask, matched_box_to_label.long())\n\n        # Subsample positive labels if we have too many.\n        positive_mask = box_to_gt_max >= self.positive_overlap_range[0]\n        fg_num = int(positive_mask.sum().item())\n        target_fg_num = int(self.fg_fraction * self.batch_size)\n        # Due to issues with torch.multinomial do subsampling in a loop\n        # to ensure proper number of samples to be selected.\n        while target_fg_num < fg_num:\n            disable_inds = torch.multinomial(positive_mask.to(torch.float32),\n                                             fg_num - target_fg_num, replacement=False)\n            positive_mask[disable_inds] = 0\n            fg_num = int(positive_mask.sum().item())\n        target_fg_num = fg_num\n        assert target_fg_num == positive_mask.sum().item()\n\n        # Subsample negative labels if we have too many.\n        negative_mask = (self.negative_overlap_range[0] <= box_to_gt_max) & \\\n                        (box_to_gt_max < self.negative_overlap_range[1])\n        bg_num = int(negative_mask.sum().item())\n        assert bg_num > 0\n        target_bg_num = self.batch_size - target_fg_num\n        assert target_bg_num > 0\n        if target_bg_num < bg_num:\n            negative_indices = negative_mask.nonzero().reshape(-1)\n            assert negative_indices.numel() > 0\n            # torch.randperm tends to throw a SEGFAULT in a multi-GPU setup,\n            # so using numpy.random.permutation here as a workaround.\n            shuffled_order = torch.from_numpy(np.random.permutation(negative_indices.numel())).to(\n                device=device)\n            # shuffled_order = torch.randperm(negative_indices.numel(), device=device)\n            assert 0 < bg_num - target_bg_num < len(shuffled_order)\n            assert (shuffled_order < negative_indices.numel()).all()\n            negative_mask.index_fill_(0, negative_indices[shuffled_order[:bg_num - target_bg_num]],\n                                      0)\n        else:\n            target_bg_num = bg_num\n        assert target_bg_num == negative_mask.sum().item()\n\n        sampled_boxes = torch.cat((boxes[positive_mask, :], boxes[negative_mask, :]), dim=0)\n        sampled_labels = torch.cat((box_to_label[positive_mask],\n                                    torch.zeros(target_bg_num, dtype=torch.long, device=device)))\n\n        # Get target for bounding box regression.\n        sampled_gt_boxes = gt_boxes[box_to_gt_idx[positive_mask], :]\n        sampled_boxes_targets = torch.zeros((target_bg_num, 4), dtype=torch.float32, device=device)\n        if sampled_gt_boxes.numel() > 0:\n            sampled_boxes_targets = torch.cat((bbox_transform_inv(sampled_boxes[:target_fg_num, :],\n                                                                  sampled_gt_boxes),\n                                               sampled_boxes_targets), dim=0)\n\n        sampled_masks_targets = None\n        if gt_masks is not None:\n            sampled_boxes_cpu = sampled_boxes.cpu().detach().numpy()\n            # Get target for box segmentation.\n            sampled_gt_masks = [gt_masks[i] for i in box_to_gt_idx[positive_mask]]\n            sampled_masks_targets = []\n            for i in range(target_fg_num):\n                gt_polygon = sampled_gt_masks[i]\n                box = sampled_boxes_cpu[i]\n                mask = polys_to_mask_wrt_box(gt_polygon, box, self.target_mask_size)\n                sampled_masks_targets.append(\n                    torch.tensor(mask, dtype=torch.float32).reshape(*self.target_mask_size))\n            if len(sampled_masks_targets) > 0:\n                sampled_masks_targets = torch.cat((torch.stack(sampled_masks_targets, dim=0),\n                                                   torch.full(\n                                                       (target_bg_num, *self.target_mask_size), -1,\n                                                       dtype=torch.float32)),\n                                                  dim=0).to(device)\n            else:\n                sampled_masks_targets = torch.empty((0, *self.target_mask_size),\n                                                    dtype=torch.float32, device=device)\n\n        sampled_texts_targets = None\n        if gt_texts is not None:\n            sampled_gt_texts = [gt_texts[i] for i in box_to_gt_idx[positive_mask]]\n            sampled_gt_texts = [\n                sampled_gt_texts[i] if box_to_gt_max[positive_mask][i] >= 1.0 else [] for i in\n                range(len(sampled_gt_texts))]\n            sampled_texts_targets = sampled_gt_texts + [[] for _ in range(target_bg_num)]\n\n        return sampled_boxes, sampled_labels, sampled_boxes_targets, sampled_masks_targets, sampled_texts_targets\n'"
pytorch_toolkit/text_spotting/text_spotting/models/text_recognition_heads/__init__.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom text_spotting.models.text_recognition_heads.attention_based import TextRecognitionHeadAttention\n\nstr_to_class = {\n    \'TextRecognitionHeadAttention\': TextRecognitionHeadAttention\n}\n'"
pytorch_toolkit/text_spotting/text_spotting/models/text_recognition_heads/attention_based.py,0,"b'""""""\n Copyright (c) 2020 Intel Corporation\n\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport cv2\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom segmentoly.utils.weights import msra_fill\n\n\nclass Encoder(nn.Module):\n    def __init__(self, dim_input, dim_internal, num_layers):\n        super().__init__()\n\n        self.dim_input = dim_input\n\n        module_list = []\n        for i in range(num_layers):\n            module_list.extend([\n                nn.Conv2d(dim_input, dim_internal, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(dim_internal),\n                nn.ReLU(inplace=True)\n            ])\n            dim_input = dim_internal\n        self.layers = nn.Sequential(*module_list)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n                msra_fill(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, feature):\n        feature = self.layers(feature)\n        return feature\n\n\nclass DecoderAttention2d(nn.Module):\n    str_to_class = {\n        \'GRU\': nn.GRU,\n        \'LSTM\': nn.LSTM\n    }\n\n    def __init__(self, hidden_size, vocab_size, decoder_input_feature_size, rnn_type):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n\n        assert len(decoder_input_feature_size) == 2\n        self.flatten_feature_size = decoder_input_feature_size[0] * decoder_input_feature_size[1]\n\n        self.embedding = nn.Embedding(vocab_size, self.hidden_size)\n\n        self.decoder = self.str_to_class[rnn_type](\n            input_size=self.hidden_size,\n            hidden_size=self.hidden_size,\n            num_layers=1, bidirectional=False)\n\n        self.encoder_outputs_w = nn.Linear(self.hidden_size, self.hidden_size)\n        self.hidden_state_w = nn.Linear(self.hidden_size, self.hidden_size)\n        self.v = nn.Parameter(torch.Tensor(self.hidden_size, 1))  # context vector\n\n        self.attn = nn.Linear(self.hidden_size * 2, self.flatten_feature_size)\n\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n\n        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n\n        nn.init.normal_(self.v, 0, 0.1)\n\n    def forward(self, input, hidden, encoder_outputs, cell=None):\n        \'\'\'\n\n        :param input: Shape is [1, BATCH_SIZE]\n        :param hidden: Shape is [1, BATCH_SIZE, HIDDEN_DIM]\n        :param cell: Shape is [1, BATCH_SIZE, HIDDEN_DIM], it is used in case of LSTM\n        :param encoder_outputs: [BATCH_SIZE, T, HIDDEN_DIM]\n        :return:\n        \'\'\'\n        BATCH_SIZE = hidden.shape[1]\n        assert tuple(hidden.shape) == (1, BATCH_SIZE, self.hidden_size), f\'{hidden.shape}\'\n        assert tuple(input.shape) == (BATCH_SIZE,), f\'{input.shape} {input}\'\n        assert tuple(encoder_outputs.shape) == (\n            BATCH_SIZE, self.flatten_feature_size, self.hidden_size), f\'{encoder_outputs.shape}\'\n\n        input = input.long()\n\n        input = self.embedding(input)\n\n        encoder_outputs_w = self.encoder_outputs_w(encoder_outputs)\n        hidden_state_w = self.hidden_state_w(hidden[0]).unsqueeze(1)\n        assert tuple(hidden_state_w.shape) == (BATCH_SIZE, 1, self.hidden_size)\n        hidden_state_w = hidden_state_w.expand(\n            (BATCH_SIZE, encoder_outputs_w.shape[1], self.hidden_size))\n        assert tuple(hidden_state_w.shape) == (\n            BATCH_SIZE, encoder_outputs_w.shape[1], self.hidden_size)\n\n        s = torch.tanh(encoder_outputs_w + hidden_state_w)\n        assert tuple(s.shape) == (BATCH_SIZE, self.flatten_feature_size, self.hidden_size)\n        s = s.reshape(-1, self.hidden_size)\n        s = torch.matmul(s, self.v)\n        s = s.reshape(-1, self.flatten_feature_size)\n\n        attn_weights = F.softmax(s, dim=1)\n        # print(\'attn_weights.shape\', attn_weights.shape)\n\n        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n        # print(\'attn_applied.shape\', attn_applied.shape)\n\n        attn_applied = attn_applied.permute(1, 0, 2)\n        attn_applied = attn_applied.squeeze(0)\n\n        output = torch.cat((input, attn_applied), 1)\n\n        output = self.attn_combine(output).unsqueeze(0)\n        output = F.relu(output)\n        if isinstance(self.decoder, nn.GRU):\n            output, hidden = self.decoder(output, hidden)\n        elif isinstance(self.decoder, nn.LSTM):\n            output, (hidden, cell) = self.decoder(output, (hidden, cell))\n\n        # to avoid removing/renaming output by mo.py\n        hidden = torch.reshape(hidden, hidden.shape)\n\n        output = self.out(output[0])\n        if self.training:\n            output = F.log_softmax(output, dim=1)\n\n        if isinstance(self.decoder, nn.LSTM):\n            return output, hidden, cell, attn_weights\n        if isinstance(self.decoder, nn.GRU):\n            return output, hidden, attn_weights\n\n\nclass TextRecognitionHeadAttention(nn.Module):\n\n    def __init__(self,\n                 input_feature_size,\n                 encoder_dim_input,\n                 encoder_dim_internal,\n                 encoder_num_layers,\n                 decoder_input_feature_size,\n                 decoder_max_seq_len,\n                 decoder_vocab_size,\n                 decoder_dim_hidden,\n                 decoder_sos_index,\n                 decoder_rnn_type,\n                 visualize):\n        super().__init__()\n\n        self.input_feature_size = input_feature_size\n        self.encoder_dim_input = encoder_dim_input\n\n        self.encoder = Encoder(encoder_dim_input, encoder_dim_internal, encoder_num_layers)\n        self.dropout = nn.Dropout(0.5)\n        self.decoder = DecoderAttention2d(hidden_size=decoder_dim_hidden,\n                                          vocab_size=decoder_vocab_size,\n                                          decoder_input_feature_size=decoder_input_feature_size,\n                                          rnn_type=decoder_rnn_type)\n\n        self.decoder_input_feature_size = decoder_input_feature_size\n        self.decoder_max_seq_len = decoder_max_seq_len\n        self.decoder_sos_int = decoder_sos_index\n        self.decoder_dim_hidden = decoder_dim_hidden\n\n        self.visualize = visualize\n\n        self.criterion = nn.NLLLoss(reduction=\'none\')\n\n    def forward(self, features, target=None, masks=None):\n        features = self.encoder(features)\n        if masks is not None:\n            masks = masks.expand(-1, features.shape[1], -1, -1)\n            features = features * masks\n\n        if self.training:\n            if not all([len(t) <= self.decoder_max_seq_len for t in target if t is not None]):\n                return torch.tensor(0.0, device=features.device), torch.tensor(0.0,\n                                                                               device=features.device)\n\n            valid_targets_indexes = torch.tensor(\n                [ind for ind, tgt in enumerate(target) if tgt], device=features.device)\n\n            if len(valid_targets_indexes) == 0:\n                return torch.tensor(0.0, device=features.device), torch.tensor(0.0,\n                                                                               device=features.device)\n\n            target = [np.array(t) for t in target if t]\n            target = [np.pad(t, (0, self.decoder_max_seq_len - len(t))) for t in target]\n            target = np.array(target)\n\n            batch_size = target.shape[0]\n\n            features = features[valid_targets_indexes]\n            features = features.view(features.shape[0], features.shape[1], -1)  # B C H*W\n            features = features.permute(0, 2, 1)  # T=H*W B C\n\n            features = self.dropout(features)\n\n            decoder_hidden = torch.zeros([1, batch_size, self.decoder_dim_hidden],\n                                         device=features.device)\n\n            decoder_cell = torch.zeros([1, batch_size, self.decoder_dim_hidden],\n                                       device=features.device)\n\n            loss = 0\n            positive_counter = 0\n\n            decoder_input = torch.ones([batch_size], device=features.device,\n                                       dtype=torch.long) * self.decoder_sos_int\n\n            target = torch.tensor(target, device=features.device, dtype=torch.long)\n\n            for di in range(self.decoder_max_seq_len):\n                if isinstance(self.decoder.decoder, nn.GRU):\n                    decoder_output, decoder_hidden, decoder_attention = self.decoder(\n                        decoder_input, decoder_hidden, features)\n                elif isinstance(self.decoder.decoder, nn.LSTM):\n                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(\n                        decoder_input, decoder_hidden, features, decoder_cell)\n                mask = (target[:, di] != 0).float()\n                loss += self.criterion(decoder_output, target[:, di]) * mask\n                mask_sum = torch.sum(mask)\n                if mask_sum == 0:\n                    break\n\n                positive_counter += mask_sum\n                decoder_input = target[:, di]\n\n            assert positive_counter > 0\n            loss = torch.sum(loss) / positive_counter\n\n            return loss.to(features.device), torch.tensor(0.0, device=features.device)\n        else:\n            batch_size = features.shape[0]\n            features = features.view(features.shape[0], features.shape[1], -1)\n            features = features.permute(0, 2, 1)\n\n            decoder_hidden = torch.zeros([1, batch_size, self.decoder_dim_hidden],\n                                         device=features.device)\n\n            decoder_cell = torch.zeros([1, batch_size, self.decoder_dim_hidden],\n                                       device=features.device)\n\n            decoder_input = torch.ones([batch_size], device=features.device,\n                                       dtype=torch.long) * self.decoder_sos_int\n\n            decoder_outputs = []\n\n            if self.visualize:\n                full_attention_mask = np.zeros([112, 112], dtype=np.uint8)\n\n            for di in range(self.decoder_max_seq_len):\n                if isinstance(self.decoder.decoder, nn.GRU):\n                    decoder_output, decoder_hidden, decoder_attention = self.decoder(\n                        decoder_input, decoder_hidden, features)\n                elif isinstance(self.decoder.decoder, nn.LSTM):\n                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(\n                        decoder_input, decoder_hidden, features, decoder_cell)\n\n                if self.visualize:\n                    attention = decoder_attention.cpu().detach().numpy()\n                    attention = (np.reshape(attention, (-1, 28)) * 500).astype(np.uint8)\n                    attention = cv2.resize(attention, (112, 112))\n                    full_attention_mask += attention\n                    cv2.imshow(\'attention\', attention)\n                    cv2.waitKey(30)\n\n                topv, topi = decoder_output.topk(1)\n                decoder_outputs.append(decoder_output)\n                decoder_input = topi.detach().view(batch_size)\n\n            decoder_outputs = torch.stack(decoder_outputs)\n\n            if self.visualize:\n                cv2.imshow(\'full\', full_attention_mask)\n\n            return decoder_outputs\n\n    def dummy_forward(self):\n        return torch.zeros((1, self.decoder_max_seq_len, self.decoder.vocab_size),\n                           dtype=torch.float32)\n'"
tensorflow_toolkit/action_detection/action_detection/nn/backbones/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom action_detection.nn.backbones.common import get_backbone\nfrom action_detection.nn.backbones.common import get_orthogonal_scope_name\n'"
tensorflow_toolkit/action_detection/action_detection/nn/backbones/base_backbone.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom abc import ABCMeta\n\nfrom action_detection.nn.nodes.initializers import orthogonal_initializer\n\n\nclass BaseBackbone(object):\n    """"""Base class for backbones.\n    """"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, net_input, fn_activation, is_training, merge_bn, merge_bn_transition, name,\n                 use_extra_layers, keep_prob, reduced, norm_kernels):\n        """"""Constructor.\n\n        :param net_input: Network input.\n        :param fn_activation: Main activation function\n        :param is_training: Training indicator variable\n        :param merge_bn: Whether to run with merged BatchNorms\n        :param merge_bn_transition: Whether to run in BatchNorm merging mode\n        :param name: Name of output network\n        :param use_extra_layers: Whether to include extra layers if available\n        :param keep_prob: Probability to keep value in dropout\n        :param reduced: Whether to construct lightweight network variant\n        :param norm_kernels: Whether to normalize convolution kernels\n        """"""\n\n        self._is_training = is_training\n        self._merge_bn = merge_bn\n        self._merge_bn_transition = merge_bn_transition\n        self._fn_activation = fn_activation\n        self._reduced = reduced\n        self._norm_kernels = norm_kernels\n\n        self._model = {\'input\': net_input}\n\n        self._ort_init = orthogonal_initializer(mode=\'conv\', out_scale=0.1)\n        self._build(net_input, use_extra_layers, name, keep_prob)\n\n    def _build(self, input_value, use_extra_layers, name, keep_prob):\n        """"""Constructs target network.\n\n        :param input_value: Network input\n        :param use_extra_layers: Whether to enable extra layers\n        :param name: Name of output network\n        :param keep_prob: Probability to keep value in dropout\n        """"""\n\n    @property\n    def input(self):\n        """"""Returns network input.\n\n        :return: Network input\n        """"""\n\n        return self._model[\'input\']\n\n    @property\n    def output(self):\n        """"""Returns network output.\n\n        :return: Network output\n        """"""\n\n        return self._model[\'output\']\n\n    @property\n    def skeleton(self):\n        """"""Returns network skeleton.\n\n        :return: Network skeleton\n        """"""\n\n        return self._model\n'"
tensorflow_toolkit/action_detection/action_detection/nn/backbones/common.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom action_detection.nn.backbones.rmnet import RMNet\nfrom action_detection.nn.backbones.twinnet import TwinNet\n\n\ndef get_backbone(backbone_name, net_input, fn_activation, is_training, merge_bn, merge_bn_transition,\n                 name=None, use_extra_layers=False, keep_probe=0.9, norm_kernels=False):\n    """"""Returns backbone according specified parameters.\n\n    :param backbone_name: Name of backbone\n    :param net_input: Network input\n    :param fn_activation: Main activation function\n    :param is_training: Training indicator variable\n    :param merge_bn: Whether to run with merged BatchNorms\n    :param merge_bn_transition: Whether to run in BatchNorm merging mode\n    :param name: Name of node\n    :param use_extra_layers: Whether to include extra layers if available\n    :param keep_probe: Probability to keep value in dropout\n    :param norm_kernels: Whether to L2 normalize convolution weights\n    :return: Parameterised backbone\n    """"""\n\n    if backbone_name == \'rmnet\':\n        backbone = RMNet(net_input, fn_activation, is_training, merge_bn, merge_bn_transition, name,\n                         use_extra_layers, keep_probe, norm_kernels)\n    elif backbone_name == \'twinnet\':\n        backbone = TwinNet(net_input, fn_activation, is_training, merge_bn, merge_bn_transition, name,\n                           keep_probe, norm_kernels)\n    else:\n        raise Exception(\'Unknown backbone name: {}\'.format(backbone_name))\n\n    return backbone\n\n\ndef get_orthogonal_scope_name(backbone_name):\n    """"""Returns scope name of convolutions for orthogonal regularization.\n\n    :param backbone_name: Name of backbone\n    :return: Name of scope\n    """"""\n\n    if backbone_name == \'rmnet\' or backbone_name == \'twinnet\':\n        return \'dim_red\'\n    elif backbone_name == \'shufflenetv2\':\n        return \'inner_map\'\n    else:\n        raise Exception(\'Unknown backbone name: {}\'.format(backbone_name))\n'"
tensorflow_toolkit/action_detection/action_detection/nn/backbones/rmnet.py,13,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\n\nfrom action_detection.nn.backbones.base_backbone import BaseBackbone\nfrom action_detection.nn.nodes.ops import batch_norm, conv2d, max_pool, dropout\n\n\nclass RMNet(BaseBackbone):\n    """"""Base class for RMNet architecture.\n    """"""\n\n    def __init__(self, net_input, fn_activation, is_training, merge_bn, merge_bn_transition, name=\'rmnet\',\n                 use_extra_layers=False, keep_prob=0.9, reduced=False, norm_kernels=False):\n        """"""Constructor.\n\n        :param net_input: Network input.\n        :param fn_activation: Main activation function\n        :param is_training: Training indicator variable\n        :param merge_bn: Whether to run with merged BatchNorms\n        :param merge_bn_transition: Whether to run in BatchNorm merging mode\n        :param name: Name of output network\n        :param use_extra_layers: Whether to include extra layers if available\n        :param keep_prob: Probability to keep value in dropout\n        :param reduced: Whether to construct lightweight network variant\n        :param norm_kernels: Whether to normalize convolution kernels\n        """"""\n\n        super(RMNet, self).__init__(net_input, fn_activation, is_training, merge_bn, merge_bn_transition, name,\n                                    use_extra_layers, keep_prob, reduced, norm_kernels)\n\n    def _normalize_inputs(self, input_value, input_depth):\n        """"""Carry out normalization of network input.\n\n        :param input_value: Network input\n        :param input_depth: Number of input channels\n        :return: Normalized network input\n        """"""\n\n        with tf.variable_scope(\'init_norm\'):\n            out = batch_norm(input_value, \'bn\', self._is_training, num_channels=input_depth)\n        return out\n\n    def _init_block(self, input_value, num_channels, name):\n        """"""Converts network input in some internal representation.\n\n        :param input_value: Network input\n        :param num_channels: Number of input and output channels\n        :param name: Name of node\n        :return: Tensor\n        """"""\n\n        with tf.variable_scope(name):\n            out = conv2d(input_value, [3, 3, num_channels[0], num_channels[1]], \'dim_inc_conv\',\n                         stride=[1, 2, 2, 1], is_training=self._is_training, init=self._ort_init,\n                         use_bias=False, use_bn=True, fn_activation=self._fn_activation,\n                         merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                         add_summary=True, init_scale=0.1, norm_kernel=self._norm_kernels)\n\n        return out\n\n    def _bottleneck(self, input_value, num_channels, name, keep_prob=None, rate=None, factor=4., k=1):\n        """"""\n\n        :param input_value:\n        :param num_channels: Number of input and output channels\n        :param name: Name of node\n        :param keep_prob: Probability to keep value in dropout\n        :param rate: Rate value for dilated convolutions\n        :param factor: Factor to reduce number of channels in bottleneck\n        :param k: Stride of node\n        :return: Tensor\n        """"""\n\n        with tf.variable_scope(name):\n            internal_num_channels = int(num_channels[1] // factor)\n\n            conv1 = conv2d(input_value, [1, 1, num_channels[0], internal_num_channels], \'dim_red\',\n                           stride=[1, 1, 1, 1], is_training=self._is_training, init=self._ort_init,\n                           use_bias=False, use_bn=True, fn_activation=self._fn_activation,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                           norm_kernel=self._norm_kernels)\n\n            conv2 = conv2d(conv1, [3, 3, internal_num_channels, 1], \'inner_conv\', init_scale=0.1,\n                           stride=[1, k, k, 1], depth_wise=True, rate=rate, is_training=self._is_training,\n                           use_bias=False, use_bn=True, fn_activation=self._fn_activation,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                           norm_kernel=self._norm_kernels)\n\n            conv3 = conv2d(conv2, [1, 1, internal_num_channels, num_channels[1]], \'dim_inc\', init_scale=0.1,\n                           is_training=self._is_training, use_bias=False, use_bn=True, fn_activation=None,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                           norm_kernel=self._norm_kernels)\n\n            if keep_prob is not None:\n                conv3 = dropout(conv3, keep_prob, is_training=self._is_training)\n\n            skip_branch = input_value if k == 1 else max_pool(input_value, k=k)\n            if num_channels[0] != num_channels[1]:\n                skip_branch = conv2d(skip_branch, [1, 1, num_channels[0], num_channels[1]], \'skip_conv\',\n                                     stride=[1, 1, 1, 1], is_training=self._is_training, init_scale=0.1,\n                                     use_bias=False, use_bn=True, fn_activation=None,\n                                     merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                                     norm_kernel=self._norm_kernels)\n\n            out = tf.add(conv3, skip_branch)\n\n            out = self._fn_activation(out)\n\n        return out\n\n    def _build(self, input_value, use_extra_layers, name, keep_prob):\n        """"""Constructs target network.\n\n        :param input_value: Network input\n        :param use_extra_layers: Whether to enable extra layers\n        :param name: Name of output network\n        :param keep_prob: Probability to keep value in dropout\n        """"""\n\n        with tf.variable_scope(name):\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'data_stat\', input_value))\n\n            local_y = self._normalize_inputs(input_value, 3)\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'input_stat\', local_y))\n\n            # Init block: x1 -> x1/2\n            local_y = self._init_block(local_y, [3, 32], \'init_block\')\n            self._model[\'output_init\'] = local_y\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'init_block_stat\', local_y))\n\n            # Bottleneck1: x1/2 -> x1/2\n            local_y = self._bottleneck(local_y, [32, 32], \'bottleneck1_1\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [32, 32], \'bottleneck1_2\', keep_prob=keep_prob)\n            if not self._reduced:\n                local_y = self._bottleneck(local_y, [32, 32], \'bottleneck1_3\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [32, 32], \'bottleneck1_4\', keep_prob=keep_prob)\n            self._model[\'output_2x\'] = local_y\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'output_2x_stat\', local_y))\n\n            # Bottleneck2: x1/2 -> x1/4\n            local_y = self._bottleneck(local_y, [32, 64], \'bottleneck2_0\', keep_prob=keep_prob, k=2)\n            local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_1\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_2\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_3\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_4\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_5\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_6\', keep_prob=keep_prob)\n            if not self._reduced:\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_7\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_8\', keep_prob=keep_prob)\n            self._model[\'output_4x\'] = local_y\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'output_4x_stat\', local_y))\n\n            # Bottleneck3: x1/4 -> x1/8\n            local_y = self._bottleneck(local_y, [64, 128], \'bottleneck3_0\', keep_prob=keep_prob, k=2)\n            local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_1\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_2\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_3\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_4\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_5\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_6\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_7\', keep_prob=keep_prob)\n            if not self._reduced:\n                local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_8\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_9\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [128, 128], \'bottleneck3_10\', keep_prob=keep_prob)\n            self._model[\'output_8x\'] = local_y\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'output_8x_stat\', local_y))\n\n            # Bottleneck4: x1/8 -> x1/16\n            local_y = self._bottleneck(local_y, [128, 256], \'bottleneck4_0\', keep_prob=keep_prob, k=2)\n            local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_1\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_2\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_3\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_4\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_5\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_6\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_7\', keep_prob=keep_prob)\n            local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_8\', keep_prob=keep_prob)\n            if not self._reduced:\n                local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_9\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_10\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [256, 256], \'bottleneck4_11\', keep_prob=keep_prob)\n            self._model[\'output_16x\'] = local_y\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'output_16x_stat\', local_y))\n\n            if use_extra_layers:\n                # Bottleneck5: x1/16 -> x1/32\n                local_y = self._bottleneck(local_y, [256, 512], \'bottleneck5_0\', keep_prob=keep_prob, k=2)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_1\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_2\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_3\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_4\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_5\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_6\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_7\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_8\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_9\', keep_prob=keep_prob)\n                if not self._reduced:\n                    local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_10\', keep_prob=keep_prob)\n                    local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_11\', keep_prob=keep_prob)\n                    local_y = self._bottleneck(local_y, [512, 512], \'bottleneck5_12\', keep_prob=keep_prob)\n                self._model[\'output_32x\'] = local_y\n                tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'output_32x_stat\', local_y))\n\n            self._model[\'output\'] = local_y\n'"
tensorflow_toolkit/action_detection/action_detection/nn/backbones/twinnet.py,16,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\n\nfrom action_detection.nn.backbones.rmnet import RMNet\nfrom action_detection.nn.nodes.ops import conv2d\n\n\nclass TwinNet(RMNet):\n    """"""Base class for TwinNet architecture.\n    """"""\n\n    def __init__(self, net_input, fn_activation, is_training, merge_bn, merge_bn_transition,\n                 name=\'twinnet\', keep_prob=0.9, norm_kernels=False):\n        """"""Constructor.\n\n        :param net_input: Network input\n        :param fn_activation: Main activation function\n        :param is_training: Training indicator variable\n        :param merge_bn: Whether to run with merged BatchNorms\n        :param merge_bn_transition: Whether to run in BatchNorm merging mode\n        :param name: Name of output network\n        :param keep_prob: Probability to keep value in dropout\n        :param norm_kernels: Whether to normalize convolution kernels\n        """"""\n\n        super(TwinNet, self).__init__(net_input, fn_activation, is_training, merge_bn, merge_bn_transition,\n                                      name, False, keep_prob, False, norm_kernels)\n\n    def _bridge(self, source, target, num_channels, name):\n        """"""Constructs bridge between two input streams.\n\n        :param source: Source stream input\n        :param target: Target strean input\n        :param num_channels: Number of input and output channels\n        :param name: Name of block\n        :return: Output of merged streams\n        """"""\n\n        with tf.variable_scope(name):\n            out = conv2d(source, [1, 1, num_channels[0], num_channels[1]], \'mix\',\n                         is_training=self._is_training, use_bias=False, use_bn=True,\n                         merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                         norm_kernel=self._norm_kernels)\n            out = tf.add(out, target)\n            out = self._fn_activation(out)\n        return out\n\n    def _twin_bottleneck(self, left_x, right_x, num_channels, name, keep_prob=None, k=1, bridge=False):\n        """"""Constructs two stream bottlenecks.\n\n        :param left_x: Left stream input\n        :param right_x: Right stream input\n        :param num_channels: Number of input and output channels\n        :param name: Name of block\n        :param keep_prob: Probability to keep value in dropout\n        :param k: Stride of node\n        :param bridge: Whether to enable bridge between streams\n        :return: Tuple of bottleneck outputs\n        """"""\n\n        with tf.variable_scope(\'detection\'):\n            left_y = self._bottleneck(left_x, num_channels, name, keep_prob=keep_prob, k=k)\n\n        with tf.variable_scope(\'classification\'):\n            right_y = self._bottleneck(right_x, num_channels, name, keep_prob=keep_prob, k=k)\n\n        if bridge:\n            with tf.variable_scope(name + \'/bridge\'):\n                mixed_left_y = self._bridge(right_y, left_y, [num_channels[1], num_channels[1]], \'left_bridge\')\n                mixed_right_y = self._bridge(left_y, right_y, [num_channels[1], num_channels[1]], \'right_bridge\')\n                left_y, right_y = mixed_left_y, mixed_right_y\n\n        return left_y, right_y\n\n    def _build(self, input_value, use_extra_layers, name, keep_prob):\n        """"""Constructs target network.\n\n        :param input_value: Network input\n        :param use_extra_layers: Whether to enable extra layers\n        :param name: Name of output network\n        :param keep_prob: Probability to keep value in dropout\n        """"""\n\n        with tf.variable_scope(name):\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'data_stat\', input_value))\n\n            with tf.variable_scope(\'shared\'):\n                local_y = self._normalize_inputs(input_value, 3)\n                tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'input_stat\', local_y))\n\n                # Init block: x1 -> x1/2\n                local_y = self._init_block(local_y, [3, 32], \'init_block\')\n                self._model[\'output_init\'] = local_y\n                tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'init_block_stat\', local_y))\n\n                # Bottleneck1: x1/2 -> x1/2\n                local_y = self._bottleneck(local_y, [32, 32], \'bottleneck1_1\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [32, 32], \'bottleneck1_2\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [32, 32], \'bottleneck1_3\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [32, 32], \'bottleneck1_4\', keep_prob=keep_prob)\n                self._model[\'output_2x\'] = local_y\n                tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'output_2x_stat\', local_y))\n\n                # Bottleneck2: x1/2 -> x1/4\n                local_y = self._bottleneck(local_y, [32, 64], \'bottleneck2_0\', keep_prob=keep_prob, k=2)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_1\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_2\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_3\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_4\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_5\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_6\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_7\', keep_prob=keep_prob)\n                local_y = self._bottleneck(local_y, [64, 64], \'bottleneck2_8\', keep_prob=keep_prob)\n                shared_y = local_y\n                self._model[\'output_4x\'] = local_y\n                tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'output_4x_stat\', local_y))\n\n            # Bottleneck3: x1/4 -> x1/8\n            det_y, cl_y = self._twin_bottleneck(shared_y, shared_y, [64, 128], \'bottleneck3_0\',\n                                                keep_prob=keep_prob, k=2)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_1\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_2\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_3\', keep_prob=keep_prob,\n                                                bridge=True)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_4\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_5\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_6\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_7\', keep_prob=keep_prob,\n                                                bridge=True)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_8\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_9\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 128], \'bottleneck3_10\', keep_prob=keep_prob)\n            self._model[\'det_output_8x\'], self._model[\'cl_output_8x\'] = det_y, cl_y\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'det_output_8x_stat\', det_y))\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'cl_output_8x_stat\', cl_y))\n\n            # Bottleneck4: x1/8 -> x1/16\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [128, 256], \'bottleneck4_0\', keep_prob=keep_prob, k=2)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_1\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_2\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_3\', keep_prob=keep_prob,\n                                                bridge=True)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_4\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_5\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_6\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_7\', keep_prob=keep_prob,\n                                                bridge=True)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_8\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_9\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_10\', keep_prob=keep_prob)\n            det_y, cl_y = self._twin_bottleneck(det_y, cl_y, [256, 256], \'bottleneck4_11\', keep_prob=keep_prob)\n            self._model[\'det_output_16x\'], self._model[\'cl_output_16x\'] = det_y, cl_y\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'det_output_16x_stat\', det_y))\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'cl_output_16x_stat\', cl_y))\n\n            self._model[\'output\'] = self._model[\'det_output_16x\'], self._model[\'cl_output_16x\']\n'"
tensorflow_toolkit/action_detection/action_detection/nn/data/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/action_detection/nn/data/augmentation.py,103,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom functools import partial\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef _clip_to_unit(value):\n    """"""Clips input value into [0,1] range.\n\n    :param value: Input value\n    :return: Clipped value\n    """"""\n\n    return tf.maximum(0.0, tf.minimum(value, 1.0))\n\n\nclass AugmentFactory(object):\n    """"""Class to define the set of augmentors with specified probability.\n    """"""\n\n    def __init__(self, independent_probs=True, name=\'IndependentAugmentFactory\'):\n        """"""Constructor.\n\n        :param independent_probs: Whether to normalize input probabilities\n        :param name: Class name\n        """"""\n\n        self.name = name\n        self.independent_probs = independent_probs\n        self.augmentors = []\n        self.probs = []\n\n    def _augment_images(self, images):\n        """"""Carry out augmentation of image batch\n\n        :param images: Batch of images\n        :return: Augmented images\n        """"""\n\n        augm_images = images\n\n        if self.independent_probs:\n            for i in xrange(len(self.augmentors)):\n                augmentor = self.augmentors[i]\n                prob = self.probs[i]\n\n                if prob is None:\n                    augm_images = augmentor(augm_images)\n                else:\n                    uniform = tf.random_uniform([], 0., 1., dtype=tf.float32)\n                    augm_images = tf.cond(tf.less(uniform, prob),\n                                          lambda: augmentor(augm_images),  # pylint: disable=cell-var-from-loop\n                                          lambda: augm_images)\n        else:\n            sum_probs = float(sum(self.probs))\n            probs = [0.] + [float(p) / sum_probs for p in self.probs]\n            prob_sums = np.cumsum(probs)\n\n            uniform = tf.random_uniform([], 0., 1., dtype=tf.float32)\n            for i in xrange(len(self.augmentors)):\n                condition = tf.logical_and(tf.greater(uniform, prob_sums[i]),\n                                           tf.less(uniform, prob_sums[i + 1]))\n                augm_images = tf.cond(condition,\n                                      lambda: self.augmentors[i](augm_images),\n                                      lambda: augm_images)\n\n        return augm_images\n\n    def add(self, augmentor, prob=None):\n        """"""Adds new augmentor function into set of augmentors.\n\n        :param augmentor: New augmentor\n        :param prob: Probability of augmentor call\n        :return: New factory\n        """"""\n\n        if self.independent_probs:\n            if prob is not None:\n                assert 0. < prob < 1., \'prob must be in range (0; 1).\'\n        else:\n            assert prob is not None, \'prob must be specified.\'\n            assert prob > 0., \'prob must be positive.\'\n\n        self.augmentors.append(augmentor)\n        self.probs.append(prob)\n\n        return self\n\n    def __call__(self, float_images):\n        """"""Carry out augmentation.\n\n        :param float_images: Batch of images\n        :return: Augmented images\n        """"""\n\n        assert float_images.dtype == tf.float32\n\n        with tf.name_scope(self.name):\n            augm_images = self._augment_images(float_images)\n            augm_images = tf.clip_by_value(augm_images, 0.0, 1.0)\n            augm_images = tf.stop_gradient(augm_images)\n            return augm_images\n\n\nclass ContrastAugmentor(object):\n    """"""Carry out augmentation of image contrast.\n    """"""\n\n    def __init__(self, lower, upper):\n        """"""Constructor.\n\n        :param lower: Lower border >= 0\n        :param upper: Upper border\n        """"""\n\n        assert lower >= 0., \'lower must be non-negative.\'\n        assert upper > lower, \'upper must be > lower.\'\n\n        self.lower = lower\n        self.upper = upper\n\n    def __call__(self, float_images):\n        """"""Carry out augmentation.\n\n        :param float_images: Batch of images\n        :return: Augmented images\n        """"""\n\n        return tf.image.random_contrast(float_images, self.lower, self.upper)\n\n\nclass BrightnessAugmentor(object):\n    """"""Carry out augmentation of image brightness.\n    """"""\n\n    def __init__(self, delta):\n        """"""Constructor.\n\n        :param delta: Brightness delta parameter (>= 0)\n        """"""\n\n        assert delta >= 0., \'delta must be non-negative.\'\n\n        self.delta = delta\n\n    def __call__(self, float_images):\n        """"""Carry out augmentation.\n\n        :param float_images: Batch of images\n        :return: Augmented images\n        """"""\n\n        return tf.image.random_brightness(float_images, self.delta)\n\n\nclass SaturationAugmentor(object):\n    """"""Carry out augmentation of image saturation.\n    """"""\n\n    def __init__(self, limits):\n        """"""Constructor.\n\n        :param limits: List of limits in format [lower, upper]\n        """"""\n\n        assert len(limits) == 2\n        assert limits[0] >= 0., \'lower must be non-negative.\'\n        assert limits[1] > limits[0], \'upper must be > lower.\'\n\n        self.lower = limits[0]\n        self.upper = limits[1]\n\n    def __call__(self, float_images):\n        """"""Carry out augmentation.\n\n        :param float_images: Batch of images\n        :return: Augmented images\n        """"""\n\n        return tf.image.random_saturation(float_images, self.lower, self.upper)\n\n\nclass GammaAugmentor(object):\n    """"""Carry out Gamma augmentation of input image.\n    """"""\n\n    def __init__(self, delta, name=\'GammaAugmentor\'):\n        """"""Constructor.\n\n        :param delta: Positive parameter\n        :param name: Name of augmentor\n        """"""\n\n        assert delta > 0., \'delta must be positive.\'\n\n        self.delta = delta\n        self.name = name\n\n    def __call__(self, float_images):\n        """"""Carry out augmentation.\n\n        :param float_images: Batch of images\n        :return: Augmented images\n        """"""\n\n        with tf.name_scope(self.name):\n            uniform = tf.random_uniform([], -self.delta, self.delta)\n            gamma = tf.log(0.5 + (2 ** (-0.5)) * uniform) / tf.log(0.5 - (2 ** (-0.5)) * uniform)\n            return tf.pow(float_images, gamma)\n\n\nclass ClassifierAugmentation(object):\n    """"""Classification-specified set of augmentations.\n    """"""\n\n    def __init__(self, prob, scale_limits, var_limits, brightness_delta, saturation_limits, trg_aspect_ratio):\n        """"""Constructor.\n\n        :param prob: Crop probability\n        :param scale_limits: Limits of crop augmentation\n        :param var_limits: Limits to shake scale limits of crop augmentation\n        :param brightness_delta: Parameter for brightness augmentation\n        :param saturation_limits: Parameters for saturation augmentation\n        :param trg_aspect_ratio: Target image aspect ratio\n        """"""\n\n        self._prob = prob\n        self._scale_limits = scale_limits\n        self._var_limits = var_limits\n        self._brightness_delta = brightness_delta\n        self._saturation_limits = saturation_limits\n        self._trg_aspect_ratio = trg_aspect_ratio\n\n    @staticmethod\n    def _crop_augmentation(input_image, prob, scale_limits, var_limits, trg_aspect_ratio):\n        """"""Carry out crop augmentation.\n\n        :param input_image: Image to crop\n        :param prob: Probability to run crop augmnetation\n        :param scale_limits: Scale limits\n        :param var_limits: Variance limits\n        :param trg_aspect_ratio: Target image aspect ratio\n        :return: Augmented image\n        """"""\n\n        def _expand_to_aspect_ratio(in_height, in_width):\n            src_aspect_ratio = tf.divide(in_height, in_width)\n            out_height, out_width = tf.cond(tf.greater(src_aspect_ratio, trg_aspect_ratio),\n                                            lambda: (in_height, tf.divide(in_height, trg_aspect_ratio)),\n                                            lambda: (in_width * trg_aspect_ratio, in_width))\n            return out_height, out_width\n\n        def _crop_image():\n            im_float_height = tf.cast(tf.shape(input_image)[0], tf.float32)\n            im_float_width = tf.cast(tf.shape(input_image)[1], tf.float32)\n\n            crop_scale = tf.random_uniform([], scale_limits[0], scale_limits[1], dtype=tf.float32)\n            crop_height_var = tf.random_uniform([], var_limits[0], var_limits[1], dtype=tf.float32)\n            crop_width_var = tf.random_uniform([], var_limits[0], var_limits[1], dtype=tf.float32)\n\n            crop_height = im_float_height * tf.minimum(tf.maximum(0.0, crop_scale + crop_height_var), 1.0)\n            crop_width = im_float_width * tf.minimum(tf.maximum(0.0, crop_scale + crop_width_var), 1.0)\n\n            crop_height, crop_width = _expand_to_aspect_ratio(crop_height, crop_width)\n\n            crop_int_height = tf.minimum(tf.cast(crop_height, tf.int32), tf.shape(input_image)[0])\n            crop_int_width = tf.minimum(tf.cast(crop_width, tf.int32), tf.shape(input_image)[1])\n\n            cropped_im = tf.image.random_crop(input_image, [crop_int_height, crop_int_width, 3])\n\n            return cropped_im\n\n        uniform = tf.random_uniform([], 0., 1., dtype=tf.float32)\n        cropped_image = tf.cond(tf.less(uniform, prob), lambda: _crop_image(), lambda: input_image)\n\n        return cropped_image\n\n    @staticmethod\n    def _image_augmentation(input_image, brightness_delta, saturation_limits):\n        """"""Carry out set of image augmentations.\n\n        :param input_image: Input image\n        :param brightness_delta: Parameter for brightness augmentation\n        :param saturation_limits: Parameters for saturation augmentation\n        :return: Augmented image\n        """"""\n\n        blob = tf.image.random_flip_left_right(input_image)\n        blob = tf.image.random_brightness(blob, max_delta=brightness_delta)\n        blob = tf.image.random_saturation(blob, lower=saturation_limits[0], upper=saturation_limits[1])\n        blob = tf.clip_by_value(blob, 0.0, 1.0)\n        return blob\n\n    def __call__(self, src_image):\n        """"""Carry out augmentation\n\n        :param src_image: Input image\n        :return: Augmented image\n        """"""\n\n        image = self._crop_augmentation(src_image, self._prob, self._scale_limits,\n                                        self._var_limits, self._trg_aspect_ratio)\n        image = self._image_augmentation(image, self._brightness_delta, self._saturation_limits)\n        return image\n\n\nclass DetectionAugmentation(object):\n    """"""Detection-specified set of augmentations.\n    """"""\n\n    def __init__(self, free_prob, expand_prob, crop_prob, max_expand_ratio,\n                 crop_scale_delta, crop_scale_limits, crop_shift_delta, crop_aspect_ratio):\n        """"""Constructor.\n\n        :param free_prob: Probability to preserve original image\n        :param expand_prob: Probability to apply expand augmentation\n        :param crop_prob: Probability to apply crop augmentation\n        :param max_expand_ratio: Max ratio for expand augmentation\n        :param crop_scale_delta: Delta parameter for crop augmentation\n        :param crop_scale_limits: Scale limits for crop augmentation\n        :param crop_shift_delta: Shift parameter for crop augmentation\n        :param crop_aspect_ratio: Target aspect ratio for crop augmentation\n        """"""\n\n        assert 0.0 <= free_prob <= 1\n        assert 0.0 <= expand_prob <= 1\n        assert 0.0 <= crop_prob <= 1\n\n        self.free_prob = free_prob\n        self.expand_prob = expand_prob\n        self.crop_prob = crop_prob\n\n        assert max_expand_ratio > 1.0\n        assert len(crop_scale_limits) == 2\n        assert crop_scale_limits[0] < crop_scale_limits[1]\n        assert crop_shift_delta > 0.0\n\n        self.min_crop_size = 10\n\n        self.expand_augmentor = partial(self._expand, max_ratio=max_expand_ratio)\n        self.crop_augmentor = partial(self._crop, scale_delta=crop_scale_delta, scale_limits=crop_scale_limits,\n                                      shift_delta=crop_shift_delta, trg_aspect_ratio=crop_aspect_ratio,\n                                      min_crop_size=self.min_crop_size)\n\n    @staticmethod\n    def _left_right_flip(in_tuple, prob=0.5):\n        """"""Carry out horizontal flip image augmentation.\n\n        :param in_tuple: Image and annotation tuple\n        :param prob: Probability to apply augmentation\n        :return: Augmented image\n        """"""\n\n        def _process():\n            flipped_image = tf.image.flip_left_right(in_tuple[0])\n\n            bboxes = in_tuple[2]\n            flipped_bboxes = tf.stack([bboxes[:, 0],\n                                       1.0 - bboxes[:, 3],\n                                       bboxes[:, 2],\n                                       1.0 - bboxes[:, 1]], axis=1)\n\n            return flipped_image, in_tuple[1], flipped_bboxes\n\n        flipped_tuple = tf.cond(tf.less(tf.random_uniform([], 0., 1., dtype=tf.float32), prob),\n                                lambda: _process(),\n                                lambda: in_tuple)\n\n        return flipped_tuple\n\n    @staticmethod\n    def _crop(in_tuple, scale_limits, shift_delta, scale_delta, trg_aspect_ratio, min_crop_size):\n        """"""Carry out crop augmentation.\n\n        :param in_tuple: Image and annotation tuple\n        :param scale_limits: Crop limits\n        :param shift_delta: Delta to shift crop\n        :param scale_delta: Delta to shake crop\n        :param trg_aspect_ratio: Target aspect ratio of image\n        :param min_crop_size: Minimal size of cropped image\n        :return: Augmented image\n        """"""\n\n        def _estimate_similarity(anchor_bbox, list_bboxes):\n            anchor_center_y = 0.5 * (anchor_bbox[0] + anchor_bbox[2])\n            anchor_center_x = 0.5 * (anchor_bbox[1] + anchor_bbox[3])\n\n            rest_center_y = 0.5 * (list_bboxes[:, 0] + list_bboxes[:, 2])\n            rest_center_x = 0.5 * (list_bboxes[:, 1] + list_bboxes[:, 3])\n\n            distances = tf.squared_difference(anchor_center_y, rest_center_y) + \\\n                        tf.squared_difference(anchor_center_x, rest_center_x)\n\n            return tf.negative(distances)\n\n        def _estimate_box_limits(list_bboxes):\n            return tf.stack([tf.reduce_min(list_bboxes[:, 0]),\n                             tf.reduce_min(list_bboxes[:, 1]),\n                             tf.reduce_max(list_bboxes[:, 2]),\n                             tf.reduce_max(list_bboxes[:, 3])])\n\n        def _get_support_bbox(max_num_factor=5):\n            def _estimate_support_bbox(list_limits):\n                num_limits = tf.shape(list_limits)[0]\n                anchor_obj_id = tf.random_uniform([], 1, num_limits, tf.int32)\n                similarity_to_all = _estimate_similarity(list_limits[anchor_obj_id], list_limits)\n\n                max_support_size = tf.maximum(2, num_limits / max_num_factor)\n                support_size = tf.random_uniform([], 1, max_support_size, dtype=tf.int32)\n                _, support_obj_ids = tf.nn.top_k(similarity_to_all, k=support_size, sorted=False)\n\n                support_obj = tf.gather(list_limits, support_obj_ids)\n                support_limits = _estimate_box_limits(support_obj)\n\n                return support_limits\n\n            valid_obj_limits = tf.boolean_mask(in_tuple[2], tf.greater_equal(in_tuple[1], 0))\n            list_size = tf.shape(valid_obj_limits)[0]\n            return tf.cond(tf.equal(list_size, 1),\n                           lambda: valid_obj_limits[0],\n                           lambda: _estimate_support_bbox(valid_obj_limits))\n\n        def _expand_to_aspect_ratio(ymin, xmin, ymax, xmax):\n            height = ymax - ymin\n            width = xmax - xmin\n            src_aspect_ratio = tf.divide(height, width)\n\n            center_y = 0.5 * (ymin + ymax)\n            center_x = 0.5 * (xmin + xmax)\n\n            out_h, out_w = tf.cond(tf.greater(src_aspect_ratio, trg_aspect_ratio),\n                                   lambda: (height, tf.divide(height, trg_aspect_ratio)),\n                                   lambda: (width * trg_aspect_ratio, width))\n\n            out_ymin = _clip_to_unit(center_y - 0.5 * out_h)\n            out_xmin = _clip_to_unit(center_x - 0.5 * out_w)\n            out_ymax = _clip_to_unit(center_y + 0.5 * out_h)\n            out_xmax = _clip_to_unit(center_x + 0.5 * out_w)\n\n            return out_ymin, out_xmin, out_ymax, out_xmax\n\n        def _is_valid_box(ymin, xmin, ymax, xmax):\n            return tf.logical_and(tf.less(ymin, ymax), tf.less(xmin, xmax))\n\n        def _process(roi_ymin, roi_xmin, roi_height, roi_width):\n            src_image_height = tf.cast(tf.shape(in_tuple[0])[0], tf.float32)\n            src_image_width = tf.cast(tf.shape(in_tuple[0])[1], tf.float32)\n            cropped_image = tf.image.crop_to_bounding_box(in_tuple[0],\n                                                          tf.cast(roi_ymin * src_image_height, tf.int32),\n                                                          tf.cast(roi_xmin * src_image_width, tf.int32),\n                                                          tf.cast(roi_height * src_image_height, tf.int32),\n                                                          tf.cast(roi_width * src_image_width, tf.int32))\n\n            obj_bboxes = in_tuple[2]\n            cropped_obj_ymin = _clip_to_unit((obj_bboxes[:, 0] - roi_ymin) / roi_height)\n            cropped_obj_xmin = _clip_to_unit((obj_bboxes[:, 1] - roi_xmin) / roi_width)\n            cropped_obj_ymax = _clip_to_unit((obj_bboxes[:, 2] - roi_ymin) / roi_height)\n            cropped_obj_xmax = _clip_to_unit((obj_bboxes[:, 3] - roi_xmin) / roi_width)\n\n            valid_mask = tf.logical_and(_is_valid_box(cropped_obj_ymin, cropped_obj_xmin,\n                                                      cropped_obj_ymax, cropped_obj_xmax),\n                                        tf.greater_equal(in_tuple[1], 0))\n            valid_labels = tf.where(valid_mask, in_tuple[1], tf.fill(tf.shape(valid_mask), -1))\n            valid_cropped_obj_bboxes = tf.stack(\n                [tf.where(valid_mask, cropped_obj_ymin, tf.zeros_like(cropped_obj_ymin)),\n                 tf.where(valid_mask, cropped_obj_xmin, tf.zeros_like(cropped_obj_xmin)),\n                 tf.where(valid_mask, cropped_obj_ymax, tf.zeros_like(cropped_obj_ymax)),\n                 tf.where(valid_mask, cropped_obj_xmax, tf.zeros_like(cropped_obj_xmax))],\n                axis=1)\n\n            return cropped_image, valid_labels, valid_cropped_obj_bboxes\n\n        support_bbox = _get_support_bbox()\n        support_height = support_bbox[2] - support_bbox[0]\n        support_width = support_bbox[3] - support_bbox[1]\n        support_center_y = 0.5 * (support_bbox[0] + support_bbox[2])\n        support_center_x = 0.5 * (support_bbox[1] + support_bbox[3])\n\n        min_scale = tf.maximum(scale_limits[0] / support_height, scale_limits[0] / support_width)\n        max_scale = tf.minimum(scale_limits[1] / support_height, scale_limits[1] / support_width)\n        scale = tf.random_uniform([], min_scale, max_scale, dtype=tf.float32)\n        scale_y = scale * tf.random_uniform([], 1.0 - scale_delta, 1.0 + scale_delta, dtype=tf.float32)\n        scale_x = scale * tf.random_uniform([], 1.0 - scale_delta, 1.0 + scale_delta, dtype=tf.float32)\n\n        crop_candidate_height = scale_y * support_height\n        crop_candidate_width = scale_x * support_width\n\n        shift_delta_y = shift_delta * crop_candidate_height\n        shift_delta_x = shift_delta * crop_candidate_width\n\n        shift_y = tf.random_uniform([], -shift_delta_y, shift_delta_y, dtype=tf.float32)\n        shift_x = tf.random_uniform([], -shift_delta_x, shift_delta_x, dtype=tf.float32)\n\n        crop_ymin = _clip_to_unit(support_center_y + shift_y - 0.5 * crop_candidate_height)\n        crop_xmin = _clip_to_unit(support_center_x + shift_x - 0.5 * crop_candidate_width)\n        crop_ymax = _clip_to_unit(support_center_y + shift_y + 0.5 * crop_candidate_height)\n        crop_xmax = _clip_to_unit(support_center_x + shift_x + 0.5 * crop_candidate_width)\n\n        crop_ymin, crop_xmin, crop_ymax, crop_xmax = \\\n            _expand_to_aspect_ratio(crop_ymin, crop_xmin, crop_ymax, crop_xmax)\n\n        crop_height = _clip_to_unit(crop_ymax - crop_ymin)\n        crop_width = _clip_to_unit(crop_xmax - crop_xmin)\n\n        int_crop_height = tf.cast(crop_height * tf.cast(tf.shape(in_tuple[0])[0], tf.float32), tf.int32)\n        int_crop_width = tf.cast(crop_width * tf.cast(tf.shape(in_tuple[0])[1], tf.float32), tf.int32)\n        is_valid_crop = tf.logical_and(tf.greater(int_crop_height, min_crop_size),\n                                       tf.greater(int_crop_width, min_crop_size))\n        out_image, out_labels, out_objects = tf.cond(is_valid_crop,\n                                                     lambda: _process(crop_ymin, crop_xmin, crop_height, crop_width),\n                                                     lambda: in_tuple)\n\n        return out_image, out_labels, out_objects\n\n    @staticmethod\n    def _expand(in_tuple, max_ratio):\n        """"""Carry out expand augmentation.\n\n        :param in_tuple: Image and annotation tuple\n        :param max_ratio: Max ratio to expand image\n        :return: Augmented image\n        """"""\n\n        src_image_shape = tf.shape(in_tuple[0])\n        src_height = tf.cast(src_image_shape[0], tf.float32)\n        src_width = tf.cast(src_image_shape[1], tf.float32)\n\n        ratio = tf.random_uniform([], 1., float(max_ratio), dtype=tf.float32)\n        trg_height = src_height * ratio\n        trg_width = src_width * ratio\n\n        offset_height = tf.floor(tf.random.uniform([], 0., trg_height - src_height, tf.float32))\n        offset_width = tf.floor(tf.random.uniform([], 0., trg_width - src_width, tf.float32))\n\n        shift_y = offset_height / trg_height\n        shift_x = offset_width / trg_width\n        shift = tf.reshape([shift_y, shift_x, shift_y, shift_x], [1, 4])\n        scale = tf.reciprocal(ratio)\n\n        expanded_image = tf.image.pad_to_bounding_box(in_tuple[0],\n                                                      tf.cast(offset_height, tf.int32),\n                                                      tf.cast(offset_width, tf.int32),\n                                                      tf.cast(trg_height, tf.int32),\n                                                      tf.cast(trg_width, tf.int32))\n        expanded_bboxes = shift + scale * in_tuple[2]\n\n        return expanded_image, in_tuple[1], expanded_bboxes\n\n    def _spatial_transform(self, in_tuple, branch_prob):\n        """"""Carry out expand and crop augmentation according probability.\n\n        :param in_tuple: Image and annotation tuple\n        :param branch_prob: Overall probability to apply at least one augmentation\n        :return: Augmented image\n        """"""\n\n        expand_prob = self.expand_prob / branch_prob\n        expanded_tuple = tf.cond(tf.less(tf.random_uniform([], 0., 1., dtype=tf.float32), expand_prob),\n                                 lambda: self.expand_augmentor(in_tuple),\n                                 lambda: in_tuple)\n\n        crop_prob = self.crop_prob / branch_prob\n        cropped_tuple = tf.cond(tf.less(tf.random_uniform([], 0., 1., dtype=tf.float32), crop_prob),\n                                lambda: self.crop_augmentor(expanded_tuple),\n                                lambda: expanded_tuple)\n\n        return cropped_tuple\n\n    def __call__(self, src_images, src_labels, src_bboxes):\n        """"""Carry out image augmentation with according annotation.\n\n        :param src_images: Input images\n        :param src_labels: Input detection classes\n        :param src_bboxes: Input detection boxes\n        :return: Augmented images\n        """"""\n\n        augmented_tuple = src_images, src_labels, src_bboxes\n\n        augmented_tuple = tf.cond(tf.less(tf.random_uniform([], 0., 1., dtype=tf.float32), self.free_prob),\n                                  lambda: augmented_tuple,\n                                  lambda: self._spatial_transform(augmented_tuple, 1.0 - self.free_prob))\n\n        augmented_tuple = self._left_right_flip(augmented_tuple)\n\n        return augmented_tuple\n'"
tensorflow_toolkit/action_detection/action_detection/nn/data/core.py,5,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom os.path import exists, dirname, realpath, isabs, join\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nImageSize = namedtuple(\'ImageSize\', \'h, w, c\')\n\n\ndef parse_text_records(input_data_file_path, types):\n    """"""Converts input text records into tuple of specified types.\n\n    :param input_data_file_path: Input text file with records\n    :param types: Output record types\n    :return:\n    """"""\n\n    def _encode(input_value, dtype=None):\n        """"""Converts input value into specified data type.\n\n        :param input_value: Input value\n        :param dtype: Output data type\n        :return: Encoded value\n        """"""\n\n        if dtype is None:\n            return input_value\n        elif isinstance(dtype, str):\n            if dtype == \'path\':\n                return input_value if exists(input_value) else None\n            else:\n                raise Exception(\'Unknown dtype for record: {}\'.format(dtype))\n        elif callable(dtype):\n            return dtype(input_value)\n        else:\n            raise Exception(\'Cannot convert to dtype: {}\'.format(dtype))\n\n    assert len(types) > 0\n    data_dir = dirname(realpath(input_data_file_path))\n\n    out_data = [[] for _ in xrange(len(types))]\n    with open(input_data_file_path, \'r\') as input_stream:\n        for line in input_stream:\n            if line.endswith(\'\\n\'):\n                line = line[:-len(\'\\n\')]\n\n            if len(line) == 0:\n                continue\n\n            line_data = line.split(\' \')\n            assert len(line_data) == len(types)\n\n            encoded_data = []\n            valid_record = True\n            for i in xrange(len(types)):\n                if not isabs(line_data[i]):\n                    line_data[i] = join(data_dir, line_data[i])\n                encoded_value = _encode(line_data[i], types[i])\n                if encoded_value is None:\n                    valid_record = False\n                    break\n\n                encoded_data.append(encoded_value)\n\n            if valid_record:\n                for i in xrange(len(types)):\n                    out_data[i].append(encoded_data[i])\n\n    return out_data\n\n\ndef decode_jpeg(image_file_path, num_channels):\n    """"""Loads image tensor from file in .jpeg format\n\n    :param image_file_path: Path to image\n    :param num_channels: Input number of channels\n    :return:\n    """"""\n\n    image_bytes = tf.read_file(image_file_path)\n    image = tf.image.decode_jpeg(image_bytes, channels=num_channels)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    return image\n\n\ndef encode_image(src_image, out_height, out_width, to_bgr=True, scale=255.0):\n    """"""Converts raw image tensor into internal network format.\n\n    :param src_image: Source image tensor\n    :param out_height: Out image height\n    :param out_width: Out image width\n    :param to_bgr: Whether to convert into BGR channel format\n    :param scale: Scale parameter\n    :return:\n    """"""\n\n    blob = tf.image.resize_images(src_image, [out_height, out_width])\n\n    if to_bgr:\n        blob = tf.reverse(blob, [-1])  # Convert to BGR format\n\n    if scale != 1.0:\n        blob *= scale\n\n    return blob\n'"
tensorflow_toolkit/action_detection/action_detection/nn/data/dataset.py,13,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\n\nfrom functools import partial\nfrom os.path import exists\n\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import trange\n\nfrom action_detection.nn.data.core import parse_text_records, decode_jpeg, encode_image\n\n\ndef get_classification_dataset(data_file_path, num_classes, image_size, batch_size, do_shuffle, name,\n                               prefetch_size=1, num_threads=5, process_fn=None):\n    """"""Prepares classification dataset object.\n\n    :param data_file_path: Path to file with rows: [image_path label]\n    :param num_classes: Number of classification classes\n    :param image_size: Target image size\n    :param batch_size: Size of batch\n    :param do_shuffle: Whether shuffle records\n    :param name: Name of dataset\n    :param prefetch_size: Size of prefetch queue\n    :param num_threads: Number of threads to load images\n    :param process_fn: Function to transform each image\n    :return: Classification dataset\n    """"""\n\n    def _convert_label(input_value):\n        """"""Checks if input label valid and returns int value\n\n        :param input_value: Input value\n        :return: Int value if valid and None else\n        """"""\n\n        int_x = int(input_value)\n        return int_x if 0 <= int_x < num_classes else None\n\n    def _mapping_fn(filename, label):\n        """"""Loads image by the specified file path.\n\n        :param filename: Path to image\n        :param label: Image label\n        :return: Image and label tuple\n        """"""\n\n        image = decode_jpeg(filename, image_size.c)\n\n        if process_fn is not None:\n            image = process_fn(image)\n\n        image_blob = encode_image(image, image_size.h, image_size.w, True, 255.0)\n\n        return image_blob, label\n\n    image_paths, labels = parse_text_records(data_file_path, types=[\'path\', _convert_label])\n\n    with tf.name_scope(name):\n        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n        if do_shuffle:\n            dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(len(image_paths), count=-1))\n        else:\n            dataset = dataset.repeat(count=-1)\n        dataset = dataset.apply(tf.data.experimental.map_and_batch(_mapping_fn, batch_size, num_threads,\n                                                                   drop_remainder=True))\n\n        if prefetch_size is not None and prefetch_size > 0:\n            dataset = dataset.prefetch(prefetch_size)\n\n        return dataset\n\n\nclass DetectionClassBalancingGenerator(object):\n    """"""Functor which allow to load balanced data according bbox labels.\n    """"""\n\n    def __init__(self, image_paths, annotation_paths, labels_map=None, ignore_labels=None, min_class_queue_size=1):\n        """"""Constructor\n\n        :param image_paths: List of image paths\n        :param annotation_paths: List of annotation paths\n        :param labels_map: Map of input labels\n        :param ignore_labels: List of ignored labels\n        :param min_class_queue_size: Minimal size of queue\n        """"""\n\n        assert len(image_paths) == len(annotation_paths)\n        assert min_class_queue_size >= 1\n\n        class_queues = {}\n        for sample_id in trange(len(image_paths), desc=\'Reading data\'):\n            annotation_path = annotation_paths[sample_id]\n\n            with open(annotation_path, \'r\') as read_file:\n                annotation_data = json.load(read_file)\n\n            labels = [b[\'label\'] for b in annotation_data]\n            labels = np.unique(labels)\n            if labels_map is not None:\n                labels = [labels_map[l] for l in labels]\n\n            if ignore_labels is not None:\n                labels = [l for l in labels if l not in ignore_labels]\n\n            for label in labels:\n                if label not in class_queues:\n                    class_queues[label] = []\n\n                class_queues[label].append(sample_id)\n\n        for label in class_queues:\n            if len(class_queues[label]) < min_class_queue_size:\n                raise Exception(\'Error: Cannot find frames with {} label!\'.format(label))\n        print(\'Info: loaded {} frames with {} labels\'.format(len(image_paths), len(class_queues)))\n\n        min_queue_size = np.min([len(class_queues[label]) for label in class_queues])\n        print(\'Info: min class queue size: {}\'.format(min_queue_size))\n\n        self._image_paths = image_paths\n        self._annotation_paths = annotation_paths\n        self._class_queues = class_queues\n        self._min_queue_size = min_queue_size\n\n    def __call__(self):\n        """"""Generates next portion of data.\n\n        :return: Iterator object\n        """"""\n\n        class_subsets = []\n        for label in self._class_queues:\n            class_frame_ids = np.copy(self._class_queues[label]).astype(np.int32)\n            subset_ids = np.random.choice(class_frame_ids, self._min_queue_size, replace=False)\n            class_subsets.append(subset_ids)\n\n        final_num_classes = len(class_subsets)\n        final_ids = np.zeros([final_num_classes * self._min_queue_size], dtype=np.int32)\n        for i in range(final_num_classes):\n            final_ids[i::final_num_classes] = class_subsets[i]\n        np.random.shuffle(final_ids)\n\n        out_image_paths = []\n        out_annotation_paths = []\n        for sample_id in final_ids:\n            out_image_paths.append(self._image_paths[sample_id])\n            out_annotation_paths.append(self._annotation_paths[sample_id])\n\n        yield out_image_paths, out_annotation_paths\n\n    def get_chunk_size(self):\n        """"""Returns size of prepared chunk of data.\n\n        :return: Number of prepared images.\n        """"""\n\n        return len(self._class_queues) * self._min_queue_size\n\n\ndef get_detection_dataset(data_file_path, image_size, batch_size, do_shuffle, name, prefetch_size=1, num_threads=5,\n                          image_process_fn=None, tuple_process_fn=None, max_num_objects_per_image=None, cache=False,\n                          use_difficult=True, labels_map=None, output_labels_stat=False, ignore_classes=None,\n                          use_class_balancing=False):\n    """"""Prepares detection dataset object.\n\n    :param data_file_path: Path to file with data\n    :param image_size: Image size\n    :param batch_size: Size of batch\n    :param do_shuffle: Whether shuffle output records\n    :param name: Name of dataset\n    :param prefetch_size: Size of prefetch queue\n    :param num_threads: Num threads to load image\n    :param image_process_fn: Function to process image independently\n    :param tuple_process_fn: Function to process image and annotation simultaneously\n    :param max_num_objects_per_image: Max number of boxes on image\n    :param cache: Whether to enable dataset caching\n    :param use_difficult: Whether to include difficult samples\n    :param labels_map: Map of input labels\n    :param output_labels_stat: Whether to output label stats\n    :param ignore_classes: List of ignores classes\n    :param use_class_balancing: Enable class balancing scheme\n    :return: Detection dataset\n    """"""\n\n    def _get_max_num_objects(data_path):\n        """"""Returns mux number of boxes on single image.\n\n        :param data_path: Path to file with data\n        :return: Number of boxes\n        """"""\n\n        max_num_bboxes = 0\n        with open(data_path, \'r\') as input_stream:\n            for line in input_stream:\n                if line.endswith(\'\\n\'):\n                    line = line[:-len(\'\\n\')]\n\n                if len(line) == 0:\n                    continue\n\n                line_data = line.split(\' \')\n                assert len(line_data) == 2\n\n                annot_path = line_data[-1]\n                if exists(annot_path):\n                    with open(annot_path, \'r\') as read_file:\n                        bboxes = json.load(read_file)\n\n                    max_num_bboxes = np.maximum(max_num_bboxes, len(bboxes))\n\n        return max_num_bboxes\n\n    def _get_labels_stat(data_path):\n        """"""Returns label stats.\n\n        :param data_path: Path to file with data\n        :return: Dictionary with counts of each label\n        """"""\n\n        all_labels = []\n        with open(data_path, \'r\') as input_stream:\n            for line in input_stream:\n                if line.endswith(\'\\n\'):\n                    line = line[:-len(\'\\n\')]\n\n                if len(line) == 0:\n                    continue\n\n                line_data = line.split(\' \')\n                assert len(line_data) == 2\n\n                annot_path = line_data[-1]\n                if exists(annot_path):\n                    with open(annot_path, \'r\') as read_file:\n                        bboxes = json.load(read_file)\n\n                    if labels_map is not None:\n                        new_labels = [labels_map[b[\'label\']] for b in bboxes]\n                    else:\n                        new_labels = [b[\'label\'] for b in bboxes]\n\n                    all_labels.extend(new_labels)\n\n        unique_labels, unique_counts = np.unique(all_labels, return_counts=True)\n        out_counts = {l: c for l, c in zip(unique_labels, unique_counts)}\n\n        return out_counts\n\n    def _read_annot(annot_path, max_num_objects):\n        """"""Loads annotation from specified file.\n\n        :param annot_path: Path to annotation file\n        :param max_num_objects: Max number of boxes on single image\n        :return: Tensor with annotation\n        """"""\n\n        def _py_internal_fn(trg_path):\n            with open(trg_path, \'r\') as read_file:\n                bboxes = json.load(read_file)\n\n            out_labels = []\n            out_bboxes = []\n            for bbox in bboxes:\n                if not use_difficult and \'difficult\' in bbox and bbox[\'difficult\']:\n                    continue\n\n                decoded_label = labels_map[bbox[\'label\']] if labels_map is not None else bbox[\'label\']\n                out_labels.append(decoded_label)\n\n                out_bboxes.append([bbox[\'ymin\'], bbox[\'xmin\'], bbox[\'ymax\'], bbox[\'xmax\']])\n\n            assert len(out_labels) == len(out_bboxes)\n\n            out_labels = np.array(out_labels, dtype=np.int32)\n            out_bboxes = np.array(out_bboxes, dtype=np.float32)\n\n            if out_bboxes.shape[0] < max_num_objects:\n                pad_size = max_num_objects - out_bboxes.shape[0]\n                out_labels = np.pad(out_labels, (0, pad_size), \'constant\', constant_values=-1)\n                out_bboxes = np.pad(out_bboxes, ((0, pad_size), (0, 0)), \'constant\', constant_values=0.0)\n            elif out_bboxes.shape[0] > max_num_objects:\n                ids_subset = np.random.choice(np.arange(start=0, stop=len(bboxes)), max_num_objects, replace=False)\n                out_labels = out_labels[ids_subset]\n                out_bboxes = out_bboxes[ids_subset]\n\n            return out_labels, out_bboxes\n\n        return tf.py_func(_py_internal_fn, [annot_path], [tf.int32, tf.float32])\n\n    def _unpack_data(image_path, annot_path, size):\n        """"""Loads image and annotation from files.\n\n        :param image_path: Path to image\n        :param annot_path: Path to annotation file\n        :param size: Max number of boxes on single image\n        :return: Image and annotation tensors\n        """"""\n\n        image = decode_jpeg(image_path, image_size.c)\n        labels, bboxes = _read_annot(annot_path, size)\n\n        labels.set_shape([size])\n        bboxes.set_shape([size, 4])\n\n        if tuple_process_fn is not None:\n            image, labels, bboxes = tuple_process_fn(image, labels, bboxes)\n\n        if image_process_fn is not None:\n            image = image_process_fn(image)\n\n        encoded_image = encode_image(image, image_size.h, image_size.w, True, 255.0)\n\n        return encoded_image, labels, bboxes\n\n    if max_num_objects_per_image is None:\n        max_num_objects_per_image = _get_max_num_objects(data_file_path)\n\n    image_paths, annot_paths = parse_text_records(data_file_path, types=[\'path\', \'path\'])\n\n    with tf.name_scope(name):\n        if use_class_balancing:\n            data_generator = DetectionClassBalancingGenerator(image_paths, annot_paths, labels_map, ignore_classes)\n            data_chunk_size = data_generator.get_chunk_size()\n\n            chunk_dataset = tf.data.Dataset.from_generator(data_generator, (tf.string, tf.string),\n                                                           (tf.TensorShape([data_chunk_size]),\n                                                            tf.TensorShape([data_chunk_size])))\n            chunk_dataset = chunk_dataset.repeat(count=-1)\n            chunk_iterator = chunk_dataset.make_one_shot_iterator()\n            image_paths_chunk, annot_paths_chunk = chunk_iterator.get_next()\n\n            dataset = tf.data.Dataset.from_tensor_slices((image_paths_chunk, annot_paths_chunk))\n        else:\n            dataset = tf.data.Dataset.from_tensor_slices((image_paths, annot_paths))\n            data_chunk_size = len(image_paths)\n\n        if do_shuffle:\n            dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(data_chunk_size, count=-1))\n        else:\n            dataset = dataset.repeat(count=-1)\n\n        mapping_fn = partial(_unpack_data, size=max_num_objects_per_image)\n        dataset = dataset.apply(tf.data.experimental.map_and_batch(mapping_fn, batch_size, num_threads,\n                                                                   drop_remainder=True))\n\n        if prefetch_size is not None and prefetch_size > 0:\n            dataset = dataset.prefetch(prefetch_size)\n\n        if cache:\n            dataset = dataset.cache()\n\n    if output_labels_stat:\n        label_counts = _get_labels_stat(data_file_path)\n        return dataset, label_counts\n    else:\n        return dataset\n'"
tensorflow_toolkit/action_detection/action_detection/nn/data/preprocessing.py,15,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\n\n\nclass CentralCrop(object):\n    """"""Functor to extract central squared crop of image.\n    """"""\n\n    def __init__(self, side_size, trg_image_size):\n        """"""Constructor.\n\n        :param side_size: Size of crop side\n        :param trg_image_size: Target image size\n        """"""\n\n        self._side_size = side_size\n        self._trg_image_size = trg_image_size\n\n    def __call__(self, src_image):\n        """"""Carry out image cropping.\n\n        :param src_image: Input image\n        :return: Cropped image\n        """"""\n\n        src_image_size = tf.shape(src_image)\n        src_height = src_image_size[0]\n        src_width = src_image_size[1]\n        src_aspect_ratio = tf.cast(src_height, tf.float32) / tf.cast(src_width, tf.float32)\n\n        resized_image =\\\n            tf.cond(tf.less(src_height, src_width),\n                    lambda: tf.image.resize_images(\n                        src_image, [self._side_size, tf.cast(float(self._side_size) / src_aspect_ratio, tf.int32)]),\n                    lambda: tf.image.resize_images(\n                        src_image, [tf.cast(float(self._side_size) * src_aspect_ratio, tf.int32), self._side_size]))\n\n        resized_image_shape = tf.shape(resized_image)\n        offset_height =\\\n            tf.cast(0.5 * tf.cast(tf.abs(resized_image_shape[0] - self._trg_image_size.h), tf.float32), tf.int32)\n        offset_width =\\\n            tf.cast(0.5 * tf.cast(tf.abs(resized_image_shape[1] - self._trg_image_size.w), tf.float32), tf.int32)\n\n        trg_image = tf.image.crop_to_bounding_box(resized_image,\n                                                  offset_height, offset_width,\n                                                  self._trg_image_size.h, self._trg_image_size.w)\n\n        return trg_image\n\n\nclass ImageNetProcessFn(object):\n    """"""ImageNet-specific image pre-processing functor.\n    """"""\n\n    def __init__(self, central_fraction, trg_image_size):\n        """"""Constructor.\n\n        :param central_fraction: Fraction of central crop\n        :param trg_image_size: Target image size\n        """"""\n\n        self._central_fraction = central_fraction\n        self._trg_image_size = trg_image_size\n\n    def __call__(self, src_image):\n        """"""Carry out image pre-processing.\n\n        :param src_image: Input image\n        :return: Processed image\n        """"""\n\n        trg_image = tf.image.central_crop(src_image, central_fraction=self._central_fraction)\n\n        trg_image = tf.expand_dims(trg_image, 0)\n        trg_image = tf.image.resize_bilinear(trg_image, [self._trg_image_size.h, self._trg_image_size.w],\n                                             align_corners=False)\n        trg_image = tf.squeeze(trg_image, [0])\n\n        return trg_image\n'"
tensorflow_toolkit/action_detection/action_detection/nn/models/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom action_detection.nn.models.image_classifier import *\nfrom action_detection.nn.models.ssd import *\nfrom action_detection.nn.models.actionnet import *\n'"
tensorflow_toolkit/action_detection/action_detection/nn/models/actionnet.py,87,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom action_detection.nn.models import SSD\nfrom action_detection.nn.backbones import get_backbone\nfrom action_detection.nn.nodes.losses import adaptive_weighting, weighted_ce_loss, sampling_losses, \\\n    explicit_center_loss, explicit_pull_push_loss, local_push_loss\nfrom action_detection.nn.nodes.ops import conv2d\n\nclass ActionNet(SSD):\n    """"""Describes network for the person detection (PD) and action recognition (AR) problems.\n    """"""\n\n    def __init__(self, backbone_name, net_input, labels, annot, fn_activation, is_training,\n                 head_params, merge_bn, merge_bn_transition, lr_params, mbox_param, action_params,\n                 wd_weight=1e-2, global_step=None, name=\'actionnet\', use_nesterov=True, norm_kernels=False):\n        """"""Constructor.\n\n        :param backbone_name: Name of target backbone\n        :param net_input: Input images\n        :param labels: Bbox classes\n        :param annot: Bbox coordinates\n        :param fn_activation: Main activation function of network\n        :param is_training: Training indicator variable\n        :param head_params: Parameters of SSD heads\n        :param merge_bn: Whether to run with merged BatchNorms\n        :param merge_bn_transition: Whether to run in BatchNorm merging mode\n        :param lr_params: Learning rate parameters\n        :param mbox_param: Parameters for SSD-based PD training\n        :param action_params: Parameters for AR training\n        :param wd_weight: Weight decay value\n        :param global_step: Variable for counting the training steps if exists\n        :param name: Network name\n        :param use_nesterov: Whether to enable nesterov momentum calculation\n        :param norm_kernels: Whether to normalize convolution kernels\n        """"""\n\n        assert backbone_name == \'twinnet\'\n\n        self._action_params = action_params\n\n        super(ActionNet, self).__init__(backbone_name, net_input, labels, annot, fn_activation, is_training,\n                                        head_params, merge_bn, merge_bn_transition, lr_params, mbox_param, wd_weight,\n                                        global_step, name, use_nesterov, norm_kernels)\n\n    def _add_head_shared(self, input_value, out_size, name):\n        """"""Adds shared part of the action head.\n\n        :param input_value: Input tensor\n        :param out_size: Output number of channels\n        :param name: Name of block\n        :return: Output tensor\n        """"""\n\n        with tf.variable_scope(name):\n            conv1 = conv2d(input_value, [1, 1, input_value.get_shape()[3], out_size], \'conv1\',\n                           fn_activation=self._fn_activation, norm_kernel=self._norm_kernels,\n                           use_bias=False, use_bn=True, is_training=self._is_training,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition)\n            conv2 = conv2d(conv1, [3, 3, out_size, 1], \'conv2\', norm_kernel=self._norm_kernels,\n                           depth_wise=True, use_bias=False, use_bn=True, is_training=self._is_training,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition)\n\n        return conv2\n\n    def _add_anchor_specific_head(self, input_value, input_size, out_size, name):\n        """"""Adds anchor-specific part of the action head.\n\n        :param input_value: Input tensor\n        :param input_size: Input number of channels\n        :param out_size: Output number of channels\n        :param name: Name of block\n        :return: Output tensor\n        """"""\n\n        with tf.variable_scope(name):\n            conv1 = conv2d(input_value, [1, 1, input_size, out_size], \'conv1\',\n                           use_bias=False, use_bn=True, is_training=self._is_training,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                           add_summary=True, norm_kernel=self._norm_kernels)\n            action_branch = tf.nn.l2_normalize(conv1, axis=-1)\n\n        return action_branch\n\n    def _add_action_classifier_body(self, input_value, input_size, out_size, reuse=False):\n        """"""Adds action classifier operations.\n\n        :param input_value: Input tensor\n        :param input_size: Input number of channels\n        :param out_size: Embedding size\n        :param reuse: Whether to reuse variables\n        :return: Action logits\n        """"""\n\n        with tf.variable_scope(\'action_classifier\'):\n            conv1 = conv2d(input_value, [1, 1, input_size, input_size], \'conv1\',\n                           use_bias=False, use_bn=True, is_training=self._is_training,\n                           fn_activation=self._fn_activation,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                           norm_kernel=self._norm_kernels, reuse_var=reuse)\n            conv2 = conv2d(conv1, [1, 1, input_size, out_size], \'conv2\',\n                           use_bias=False, use_bn=True, is_training=self._is_training,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                           norm_kernel=self._norm_kernels, reuse_var=reuse)\n\n            action_embeddings = tf.nn.l2_normalize(conv2, axis=-1)\n\n        return action_embeddings\n\n    def _add_action_classifier(self, input_value, input_size, out_size, pr_product, add_summary=True):\n        """"""Adds embedding-based action classifier.\n\n        :param input_value: Input tensor\n        :param input_size: Input number of channels\n        :param out_size: Embedding size\n        :param pr_product: Whether to use PR-Product\n        :param add_summary: Whether to store summary info\n        :return: Action logits\n        """"""\n\n        action_embeddings = self._add_action_classifier_body(input_value, input_size, out_size)\n\n        with tf.variable_scope(\'action_classifier\'):\n            self._model[\'action_embeddings\'] = tf.reshape(action_embeddings, [-1, out_size])\n\n            assert self._action_params[\'num_centers_per_action\'] == 1\n            action_centers = tf.stack(self._action_moving_centers, axis=1)\n            self._model[\'action_centers\'] = action_centers\n\n            action_centers_kernel = tf.reshape(action_centers, [1, 1, out_size, self._action_params[\'num_actions\']])\n            all_action_logits = tf.nn.conv2d(action_embeddings, action_centers_kernel, [1, 1, 1, 1], \'SAME\')\n\n            if self._is_training is None and self._action_params[\'num_centers_per_action\'] == 1:\n                action_logits = tf.reshape(all_action_logits, [input_value.get_shape()[0], -1,\n                                                               self._action_params[\'num_actions\']],\n                                           name=\'out_action_logits\')\n            else:\n                if pr_product and self._is_training is not None:\n                    prod = all_action_logits\n                    alpha = tf.sqrt(1.0 - tf.square(prod))\n                    all_action_logits = tf.stop_gradient(alpha) * prod + tf.stop_gradient(prod) * (1.0 - alpha)\n\n                all_action_logits = tf.reshape(all_action_logits, [input_value.get_shape()[0], -1,\n                                                                   self._action_params[\'num_actions\'],\n                                                                   self._action_params[\'num_centers_per_action\']])\n                self._model[\'all_action_logits\'] = all_action_logits\n\n                action_logits = tf.reduce_max(all_action_logits, axis=-1, name=\'out_action_logits\')\n\n                if add_summary:\n                    action_center_ids = tf.reshape(tf.argmax(all_action_logits, axis=-1), [-1])\n                    tf.add_to_collection(\'accuracy_summary\',\n                                         tf.summary.histogram(\'action_center_ids\', action_center_ids))\n\n            return action_logits\n\n    def _build_train_action_heads(self, head_params, skeleton, action_prefix=\'cl_output_\'):\n        """"""Creates all action heads according twin detection branch.\n\n        :param head_params: Detection head parameters\n        :param skeleton: Network skeleton\n        :param action_prefix: Name prefix for the new created heads\n        """"""\n\n        with tf.variable_scope(\'action_heads\'):\n            action_head_inputs = []\n            for head_id, head_param in enumerate(head_params):\n                cl_place_name = \'{}{}x\'.format(action_prefix, head_param.scale)\n                action_x = skeleton[cl_place_name]\n\n                head_y = self._add_head_shared(action_x, head_param.internal_size, \'head_{}_shared\'.format(head_id + 1))\n\n                anchor_heads = []\n                for anchor_id in xrange(len(head_param.anchors)):\n                    anchor_head = self._add_anchor_specific_head(head_y, head_param.internal_size,\n                                                                 self._action_params[\'embedding_size\'],\n                                                                 \'head_{}_anchor_{}\'.format(head_id + 1, anchor_id + 1))\n                    anchor_head = tf.expand_dims(anchor_head, axis=3)\n                    anchor_heads.append(anchor_head)\n\n                anchor_heads = tf.concat(anchor_heads, axis=3)\n                anchor_heads = tf.reshape(anchor_heads,\n                                          [anchor_heads.get_shape()[0], -1, self._action_params[\'embedding_size\']])\n\n                action_head_inputs.append(anchor_heads)\n\n            action_head_inputs = tf.concat(action_head_inputs, axis=1)\n            action_head_inputs = tf.expand_dims(action_head_inputs, axis=1)\n\n            action_logits = self._add_action_classifier(action_head_inputs,\n                                                        self._action_params[\'embedding_size\'],\n                                                        self._action_params[\'embedding_size\'],\n                                                        pr_product=True)\n\n        self._model[\'action_logits\'] = tf.identity(action_logits, name=\'out_action_logits\')\n        self._model[\'pr_action_conf\'] = tf.nn.softmax(self._action_params[\'scale_end\'] * action_logits, axis=-1,\n                                                      name=\'out_action_probs\')\n\n    def _build_deploy_action_heads(self, head_params, skeleton, action_prefix=\'cl_output_\'):\n        """"""Creates all action heads according twin detection branch for deploying.\n\n        :param head_params: Detection head parameters\n        :param skeleton: Network skeleton\n        :param action_prefix: Name prefix for the new created heads\n        """"""\n\n        assert self._action_params[\'num_centers_per_action\'] == 1\n\n        with tf.variable_scope(\'action_heads\'):\n            action_centers = tf.stack(self._action_moving_centers, axis=1)\n            action_centers_kernel = tf.reshape(\n                action_centers, [1, 1, self._action_params[\'embedding_size\'], self._action_params[\'num_actions\']])\n\n            reuse_classifier_var = False\n            all_head_logits = []\n            for head_id, head_param in enumerate(head_params):\n                cl_place_name = \'{}{}x\'.format(action_prefix, head_param.scale)\n                action_x = skeleton[cl_place_name]\n\n                shared_head_y = self._add_head_shared(\n                    action_x, head_param.internal_size, \'head_{}_shared\'.format(head_id + 1))\n\n                all_anchor_logits = []\n                for anchor_id in xrange(len(head_param.anchors)):\n                    anchor_place_name = \'head_{}_anchor_{}\'.format(head_id + 1, anchor_id + 1)\n\n                    anchor_head = self._add_anchor_specific_head(shared_head_y, head_param.internal_size,\n                                                                 self._action_params[\'embedding_size\'],\n                                                                 anchor_place_name)\n\n                    anchor_embeddings = self._add_action_classifier_body(anchor_head,\n                                                                         self._action_params[\'embedding_size\'],\n                                                                         self._action_params[\'embedding_size\'],\n                                                                         reuse_classifier_var)\n                    reuse_classifier_var = True\n\n                    anchor_logits = tf.nn.conv2d(anchor_embeddings, action_centers_kernel, [1, 1, 1, 1], \'SAME\',\n                                                 name=\'out_{}\'.format(anchor_place_name))\n                    all_anchor_logits.append(anchor_logits)\n\n                all_head_logits.append(all_anchor_logits)\n\n        self._model[\'deploy_action_logits\'] = all_head_logits\n\n    def _build_action_moving_centers(self):\n        """"""Creates variables (not trainable by SGD) to store class centers.\n        """"""\n\n        with tf.variable_scope(\'action_moving_centers\'):\n            moving_centers = []\n            for center_class_id in xrange(self._action_params[\'num_actions\']):\n                init_center_value = tf.nn.l2_normalize(tf.random.normal(\n                    [self._action_params[\'embedding_size\']]), axis=-1)\n                moving_centers.append(tf.get_variable(\n                    \'moving_center_{}\'.format(center_class_id), initializer=init_center_value,\n                    synchronization=tf.VariableSynchronization.ON_READ, trainable=False,\n                    aggregation=tf.VariableAggregation.MEAN))\n\n            self._action_moving_centers = moving_centers\n\n    def _build_network(self, input_value, backbone_name):\n        """"""Creates parameterized network architecture.\n\n        :param input_value: Input tensor\n        :param backbone_name: Target name of backbone\n        """"""\n\n        with tf.variable_scope(self._name):\n            self._feature_extractor = get_backbone(backbone_name, input_value, self._fn_activation,\n                                                   self._is_training, self._merge_bn, self._merge_bn_transition,\n                                                   use_extra_layers=False, name=backbone_name,\n                                                   norm_kernels=self._norm_kernels)\n            skeleton = self._feature_extractor.skeleton\n\n            self._build_action_moving_centers()\n            self._build_detection_heads(self._head_params, skeleton, prefix=\'det_output_\')\n\n            if self._is_training is None and self._merge_bn:\n                self._build_deploy_action_heads(self._head_params, skeleton)\n            else:\n                self._build_train_action_heads(self._head_params, skeleton)\n\n    def _get_action_scale(self, start, end, num_steps, power):\n        """"""Creates scheduled scale to train AR model part.\n\n        :param start: Initial scale value\n        :param end: Target scale value\n        :param num_steps: Num steps for scale annealing\n        :param power: Power parameter\n        :return: Scale scalar value\n        """"""\n\n        if \'schedule\' not in self._lr_params or self._lr_params[\'schedule\'] == \'piecewise_constant\':\n            float_step = tf.cast(self._global_step, tf.float32)\n\n            factor = float(end - start) / float(1 - power)\n            var_a = factor / (float(num_steps) ** float(power))\n            var_b = -factor * float(power) / float(num_steps)\n\n            out_value = tf.cond(tf.less(self._global_step, num_steps),\n                                lambda: var_a * tf.pow(float_step, float(power)) + var_b * float_step + float(start),\n                                lambda: float(end))\n        elif self._lr_params[\'schedule\'] == \'cosine_decay_restarts\':\n            factor = start / self._lr_params[\'init_value\']\n            out_value = tf.maximum(factor * self._model[\'lr\'], end)\n        else:\n            raise Exception(\'Unknown lt schedule: {}\'.format(self._lr_params[\'schedule\']))\n\n        return out_value\n\n    def _filter_matches_by_instance(self, gt_ids, matched_scores, matches_logits, matches_embeddings, labels):\n        """"""Selects top-k matches per instance to train AR model part.\n\n        :param gt_ids: Ground truth IDs\n        :param matched_scores: Matched scores\n        :param matches_logits: Matched action logits\n        :param matches_embeddings: Matched action embedding vectors\n        :param labels: Action labels\n        :return: Filtered values\n        """"""\n\n        with tf.name_scope(\'instance_filtering_matches\'):\n            best_matches_mask, _ = self._get_top_matched_mask(\n                gt_ids, matched_scores, self._action_params[\'matches_threshold\'],\n                self._action_params[\'max_num_samples_per_gt\'], \'action_top\',\n                drop_ratio=self._action_params[\'sample_matches_drop_ratio\'], soft_num_samples=False)\n\n            matches_logits = tf.boolean_mask(matches_logits, best_matches_mask)\n            matches_embeddings = tf.boolean_mask(matches_embeddings, best_matches_mask)\n            matched_labels = tf.boolean_mask(labels, best_matches_mask)\n\n        return matches_logits, matches_embeddings, matched_labels\n\n    def _filter_matches_simple(self, matched_scores, matches_logits, matches_embeddings, labels):\n        """"""Filter matches by score.\n\n        :param matched_scores: Matched scores\n        :param matches_logits: Matched action logits\n        :param matches_embeddings: Matched action embedding vectors\n        :param labels: Action labels\n        :return: Filtered values\n        """"""\n\n        with tf.name_scope(\'simple_filtering_matches\'):\n            valid_matches_mask = tf.greater(matched_scores, self._action_params[\'matches_threshold\'])\n            self._add_loss_summary(tf.reduce_sum(tf.cast(valid_matches_mask, tf.int32)), \'num_matches\')\n\n            matches_logits = tf.boolean_mask(matches_logits, valid_matches_mask)\n            matches_embeddings = tf.boolean_mask(matches_embeddings, valid_matches_mask)\n            matched_labels = tf.boolean_mask(labels, valid_matches_mask)\n\n        return matches_logits, matches_embeddings, matched_labels\n\n    def _update_moving_centers(self, embeddings, labels, momentum=0.99, add_summary=True):\n        """"""Creates Op to update class centers according batch center of each class.\n\n        :param embeddings: Batch embeddings\n        :param labels: Action labels\n        :param momentum: Momentum scalar value\n        :param add_summary: Whether to add summary info\n        """"""\n\n        def _constant_top_triangle_mask(num_values):\n            mask = np.zeros([num_values, num_values], dtype=np.bool)\n            mask[np.triu_indices(num_values, 1)] = True\n            return mask\n\n        def _estimate_new_center(embd, old_center):\n            weights = 0.5 * (1.0 + tf.matmul(embd, tf.reshape(old_center, [-1, 1])))\n            current_center = tf.nn.l2_normalize(tf.reduce_mean(weights * embd, axis=0))\n            return tf.nn.l2_normalize(momentum * old_center + (1.0 - momentum) * current_center)\n\n        with tf.variable_scope(\'update_moving_centers\'):\n            for class_id in xrange(self._action_params[\'num_actions\']):\n                class_mask = tf.equal(labels, class_id)\n                class_embeddings = tf.boolean_mask(embeddings, class_mask)\n\n                moving_class_center = self._action_moving_centers[class_id]\n                new_center = tf.cond(tf.greater_equal(tf.reduce_sum(tf.cast(class_mask, tf.int32)), 1),\n                                     lambda: _estimate_new_center(class_embeddings, moving_class_center),  # pylint: disable=cell-var-from-loop\n                                     lambda: moving_class_center)  # pylint: disable=cell-var-from-loop\n\n                center_update_op = tf.assign(moving_class_center, new_center)\n                with tf.control_dependencies([center_update_op]):\n                    out_class_center = tf.identity(moving_class_center)\n\n                self._action_moving_centers[class_id] = out_class_center\n\n            if add_summary:\n                centers = tf.stack(self._action_moving_centers, axis=0)\n                center_distances = 1.0 - tf.matmul(centers, tf.transpose(centers))\n                top_triangle_distances = tf.boolean_mask(\n                    center_distances, _constant_top_triangle_mask(self._action_params[\'num_actions\']))\n\n                self._add_loss_summary(tf.reduce_min(top_triangle_distances), \'centers/min_dist\')\n                self._add_loss_summary(tf.reduce_mean(top_triangle_distances), \'centers/mean_dist\')\n                self._add_loss_summary(tf.reduce_max(top_triangle_distances), \'centers/max_dist\')\n\n                for k in xrange(self._action_params[\'num_actions\']):\n                    for action_id in xrange(k + 1, self._action_params[\'num_actions\']):\n                        # noinspection PyUnresolvedReferences\n                        self._add_loss_summary(center_distances[k, action_id],\n                                               \'centers/pair_{}_{}\'.format(k, action_id))\n\n    def _action_losses(self, matches_mask, matched_scores, gt_ids, labels):\n        """"""Creates action-specific losses.\n\n        :param matches_mask: Mask of matched priors\n        :param matched_scores: IoU score of matches\n        :param gt_ids: Ground truth IDs\n        :param labels: Action labels\n        :return: List of loss values\n        """"""\n\n        with tf.name_scope(\'action_loss\'):\n            matches_logits = tf.boolean_mask(self._model[\'action_logits\'], matches_mask)\n            matches_embeddings = tf.boolean_mask(self._model[\'action_embeddings\'], tf.reshape(matches_mask, [-1]))\n\n            # filter matched samples\n            if self._action_params[\'max_num_samples_per_gt\'] > 0:\n                matches_logits, matches_embeddings, matched_labels = tf.cond(\n                    tf.greater(tf.size(gt_ids), 0),\n                    lambda: self._filter_matches_by_instance(\n                        gt_ids, matched_scores, matches_logits, matches_embeddings, labels),\n                    lambda: (matches_logits, matches_embeddings, labels))\n            else:\n                matches_logits, matches_embeddings, matched_labels = tf.cond(\n                    tf.greater(tf.size(gt_ids), 0),\n                    lambda: self._filter_matches_simple(\n                        matched_scores, matches_logits, matches_embeddings, labels),\n                    lambda: (matches_logits, matches_embeddings, labels))\n\n            valid_action_mask = tf.not_equal(matched_labels, self._action_params[\'undefined_action_id\'])\n            valid_labels = tf.boolean_mask(matched_labels, valid_action_mask)\n            valid_logits = tf.boolean_mask(matches_logits, valid_action_mask)\n            valid_embeddings = tf.boolean_mask(matches_embeddings, valid_action_mask)\n            num_valid_labels = tf.size(valid_labels)\n\n            # add ops to update moving action centers\n            self._update_moving_centers(valid_embeddings, valid_labels)\n\n            # scheduled scale for logits\n            scale = self._get_action_scale(self._action_params[\'scale_start\'], self._action_params[\'scale_end\'],\n                                           self._action_params[\'scale_num_steps\'], self._action_params[\'scale_power\'])\n            # scale = self._get_action_scale(self._action_params[\'scale_start\'], valid_labels, valid_logits)\n            tf.add_to_collection(\'accuracy_summary\', tf.summary.scalar(\'action_scale\', scale))\n\n            # ce loss for valid action classes\n            action_ce_loss_value = weighted_ce_loss(\n                valid_labels, scale * valid_logits, self._action_params[\'num_actions\'], \'ce_loss\',\n                self._action_params[\'max_entropy_weight\'], limits=self._action_params[\'weight_limits\'],\n                alpha=self._action_params[\'focal_alpha\'], gamma=self._action_params[\'focal_gamma\'],\n                num_bins=self._action_params[\'num_bins\'])\n\n            # global pull-push loss for valid action classes with moving centers\n            glob_pull_push_loss_value = tf.cond(\n                tf.equal(num_valid_labels, 0),\n                lambda: 0.0,\n                lambda: explicit_pull_push_loss(valid_embeddings, valid_labels, self._action_moving_centers,\n                                                self._action_params[\'glob_pull_push_margin\'], \'glob_pull_push_loss\',\n                                                self._action_params[\'glob_pull_push_loss_top_k\']))\n\n            # center loss for valid action classes with moving centers\n            center_loss_value = tf.cond(tf.equal(num_valid_labels, 0),\n                                        lambda: 0.0,\n                                        lambda: explicit_center_loss(valid_embeddings, valid_labels,\n                                                                     self._action_moving_centers, \'glob_pull_push_loss\',\n                                                                     self._action_params[\'center_loss_top_k\']))\n\n            # local push loss for valid action classes\n            local_push_loss_value = tf.cond(tf.equal(num_valid_labels, 0),\n                                            lambda: 0.0,\n                                            lambda: local_push_loss(valid_embeddings, valid_labels,\n                                                                    self._action_params[\'num_actions\'],\n                                                                    self._action_params[\'local_push_margin\'],\n                                                                    \'local_push_loss\',\n                                                                    self._action_params[\'local_push_top_k\']))\n\n            # sampling loss for valid action classes\n            sampling_loss_value = tf.cond(tf.equal(num_valid_labels, 0),\n                                          lambda: 0.0,\n                                          lambda: sampling_losses(valid_embeddings, valid_labels,\n                                                                  self._action_moving_centers,\n                                                                  self._action_params[\'num_samples\'],\n                                                                  self._action_params[\'ce_loss_weight\'],\n                                                                  self._action_params[\'auxiliary_loss_weight\'],\n                                                                  \'sampling_loss\', scale,\n                                                                  self._action_params[\'max_entropy_weight\'],\n                                                                  limits=self._action_params[\'weight_limits\'],\n                                                                  alpha=self._action_params[\'focal_alpha\'],\n                                                                  gamma=self._action_params[\'focal_gamma\']))\n\n            return action_ce_loss_value, glob_pull_push_loss_value, center_loss_value, local_push_loss_value,\\\n                   sampling_loss_value\n\n    def _build_losses(self, labels, annot, backbone_name):\n        """"""Adds losses to the training graph.\n\n        :param labels: BBox labels\n        :param annot: BBox coordinates\n        :param backbone_name: Target backbone name\n        """"""\n\n        with tf.name_scope(self._name + \'_losses\'):\n            # Detection losses\n            det_conf_loss, det_loc_loss, matches_mask, matched_scores, gt_sample_ids, gt_labels =\\\n                self._multibox_loss(labels, annot, class_agnostic=True, output_original_labels=True)\n            self._add_loss_summary(det_conf_loss, \'conf_loss\')\n            self._add_loss_summary(det_loc_loss, \'loc_loss\')\n            total_detection_loss = adaptive_weighting([det_conf_loss, det_loc_loss], name=\'detection_loss\')\n            self._add_loss_summary(total_detection_loss, \'total_detection_loss\')\n\n            # Action losses\n            action_ce_loss_value, glob_pull_push_loss_value, center_loss_value, local_push_loss_value, \\\n            sampling_loss_value =\\\n                self._action_losses(matches_mask, matched_scores, gt_sample_ids, gt_labels)\n            self._add_loss_summary(action_ce_loss_value, \'action_ce_loss\')\n            self._add_loss_summary(glob_pull_push_loss_value, \'glob_pull_push_loss\')\n            self._add_loss_summary(center_loss_value, \'center_loss\')\n            self._add_loss_summary(local_push_loss_value, \'local_push_loss_value\')\n            self._add_loss_summary(sampling_loss_value, \'sampling_loss\')\n\n            # Total Action loss\n            auxiliary_action_loss = adaptive_weighting(\n                [glob_pull_push_loss_value, center_loss_value, local_push_loss_value, sampling_loss_value],\n                name=\'auxiliary_action_loss\')\n            self._add_loss_summary(auxiliary_action_loss, \'auxiliary_action_loss\')\n\n            total_action_loss = tf.add_n([self._action_params[\'ce_loss_weight\'] * action_ce_loss_value,\n                                          self._action_params[\'auxiliary_loss_weight\'] * auxiliary_action_loss],\n                                         name=\'total_action_loss\')\n            self._add_loss_summary(total_action_loss, \'total_action_loss\')\n\n            # Total loss\n            main_loss = adaptive_weighting([total_detection_loss, total_action_loss], name=\'main_loss\')\n            self._add_loss_summary(main_loss, \'main_loss\')\n            total_loss = tf.identity(main_loss, name=\'total_loss\')\n            self._add_loss_summary(total_loss, \'total_loss\')\n            self._model[\'total_loss\'] = total_loss\n\n    @property\n    def predictions(self):\n        """"""Returns model predictions Op\n\n        :return: Predicted locations, confidences and action classes Ops\n        """"""\n\n        return self._model[\'pr_loc\'], self._model[\'pr_det_conf\'], self._model[\'pr_action_conf\']\n'"
tensorflow_toolkit/action_detection/action_detection/nn/models/base_network.py,13,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nfrom abc import ABCMeta\n\nimport tensorflow as tf\n\n\nclass BaseNetwork(object):\n    """"""Base class for task-specific networks.\n    """"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, is_training, merge_bn, merge_bn_transition, lr_params, wd_weight, global_step,\n                 use_nesterov, norm_kernels):\n        """"""Constructor.\n\n        :param is_training: Training indicator variable\n        :param merge_bn: Whether to run with merged BatchNorms\n        :param merge_bn_transition: Whether to run in BatchNorm merging mode\n        :param lr_params: Learning rate parameters\n        :param wd_weight: Weight decay value\n        :param global_step: Variable for counting the training steps if exists\n        :param use_nesterov: Whether to enable nesterov momentum calculation\n        :param norm_kernels: Whether to normalize convolution kernels\n        """"""\n\n        self._is_training = is_training\n        self._merge_bn = merge_bn\n        self._merge_bn_transition = merge_bn_transition\n        self._weight_decay = wd_weight\n        self._lr_params = lr_params\n        self._use_nesterov = use_nesterov\n        self._norm_kernels = norm_kernels\n\n        self._max_grad_norm = 1.0\n        self._momentum = 0.9\n        self._global_step = tf.Variable(0, trainable=False, name=\'GlobalStep\') if global_step is None else global_step\n\n        self._model = {}\n\n    @staticmethod\n    def _add_loss_summary(loss, name):\n        """"""Adds summary of specified scalar loss into collection of summary.\n\n        :param loss: Scalar loss value\n        :param name: Name of loss in collection\n        """"""\n\n        tf.add_to_collection(\'loss_summary\', tf.summary.scalar(name, loss))\n\n    def _create_lr_schedule(self):\n        """"""Creates learning rate scheduled variable.\n\n        :return: Learning rate variable\n        """"""\n\n        with tf.name_scope(\'learning_rate\'):\n            if \'schedule\' not in self._lr_params or self._lr_params[\'schedule\'] == \'piecewise_constant\':\n                learning_rate = tf.train.piecewise_constant(\n                    self._global_step, self._lr_params[\'boundaries\'], self._lr_params[\'values\'], name=\'lr_value\')\n            elif self._lr_params[\'schedule\'] == \'cosine_decay_restarts\':\n                learning_rate = tf.train.cosine_decay_restarts(\n                    self._lr_params[\'init_value\'], self._global_step, self._lr_params[\'first_decay_steps\'],\n                    self._lr_params[\'t_mul\'], self._lr_params[\'m_mul\'], self._lr_params[\'alpha\'],\n                    name=\'lr_value\')\n            else:\n                raise Exception(\'Unknown lt schedule: {}\'.format(self._lr_params[\'schedule\']))\n\n            tf.add_to_collection(\'lr_summary\', tf.summary.scalar(\'lr\', learning_rate))\n            self._model[\'lr\'] = learning_rate\n\n    def create_optimizer(self):\n        """"""Create default optimizer.\n\n        :return: Optimizer\n        """"""\n\n        return tf.train.MomentumOptimizer(learning_rate=self._model[\'lr\'],\n                                          momentum=self._momentum,\n                                          use_nesterov=self._use_nesterov)\n\n    @staticmethod\n    def load_available_variables(checkpoint_path, sess, checkpoint_scope, model_scope,\n                                 print_vars=False, extra_ends=None):\n        """"""Loads all matched by name variables from the specified checkpoint.\n\n        :param checkpoint_path: Path to checkpoint\n        :param sess: Session to load in\n        :param checkpoint_scope: Name of scope to load from\n        :param model_scope: Name of scope to load in\n        :param print_vars: Whether to print loaded names of variables\n        :param extra_ends: List of name suffix to match too\n        """"""\n\n        def _print_variables(var_list, header):\n            """"""Prints variable names\n\n            :param var_list: List of variables or names\n            :param header: Header to print\n            """"""\n\n            if print_vars and len(var_list) > 0:\n                if isinstance(var_list[0], basestring):\n                    var_names = var_list\n                else:\n                    var_names = [var_name.name[:-2] for var_name in var_list]\n\n                sorted_var_names = var_names\n                sorted_var_names.sort()\n\n                print(\'\\n{}:\'.format(header))\n                for var_name in sorted_var_names:\n                    print(\'   {}\'.format(var_name))\n\n        reader = tf.train.NewCheckpointReader(checkpoint_path)\n        checkpoint_var_names = reader.get_variable_to_shape_map().keys()\n\n        valid_ends = [\'weights\', \'biases\', \'beta\', \'gamma\', \'moving_mean\', \'moving_variance\']\n        if extra_ends is not None:\n            valid_ends += extra_ends\n        valid_ends = tuple(valid_ends)\n\n        valid_checkpoint_var_names = [n for n in checkpoint_var_names\n                                      if n.startswith(checkpoint_scope) and n.endswith(valid_ends)]\n        _print_variables(valid_checkpoint_var_names, \'Checkpoint variables\')\n\n        all_model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=model_scope)\n        valid_model_vars = [v for v in all_model_vars if v.name[:-2].endswith(valid_ends)]\n        _print_variables(valid_model_vars, \'Session variables\')\n\n        valid_model_vars_map = {}\n        for model_var in valid_model_vars:\n            out_name = model_var.name[:-2].replace(model_scope, checkpoint_scope)\n            if out_name in valid_checkpoint_var_names:\n                valid_model_vars_map[out_name] = model_var\n            elif print_vars:\n                print(\'Unmatched variable: {}\'.format(out_name))\n        _print_variables(list(valid_model_vars_map), \'Matched variables\')\n\n        if len(valid_model_vars_map) == 0:\n            raise Exception(\'The provided checkpoint or source model scope does not contain valid variables\')\n\n        loader = tf.train.Saver(var_list=valid_model_vars_map)\n        loader.restore(sess, checkpoint_path)\n        print(\'{} / {} (from checkpoint\\\'s {}) model parameters have been restored.\'\n              .format(len(valid_model_vars_map), len(valid_model_vars), len(valid_checkpoint_var_names)))\n\n    @staticmethod\n    def get_merge_train_op():\n        """"""Creates Op to merge all internal update ops into single node.\n\n        :return: Merged Op\n        """"""\n\n        bn_merge_ops = tf.get_collection(\'bn_merge_ops\')\n\n        if len(bn_merge_ops) > 0:\n            out_op = tf.group(bn_merge_ops)\n        else:\n            out_op = tf.no_op()\n\n        return out_op\n\n    @property\n    def total_loss(self):\n        """"""Returns total loss Op.\n\n        :return: Total loss op\n        """"""\n\n        return self._model[\'total_loss\']\n'"
tensorflow_toolkit/action_detection/action_detection/nn/models/image_classifier.py,15,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\n\nfrom action_detection.nn.models.base_network import BaseNetwork\nfrom action_detection.nn.backbones import get_backbone, get_orthogonal_scope_name\nfrom action_detection.nn.nodes.ops import conv2d, glob_max_pool\nfrom action_detection.nn.nodes.losses import weight_decay, unit_gamma, orthogonal_conv, decorrelate_features\n\n\nclass ImageClassifier(BaseNetwork):\n    """"""Describes network for the image classification problem.\n    """"""\n\n    def __init__(self, backbone_name, net_input, labels, fn_activation, is_training, num_classes, merge_bn,\n                 merge_bn_transition, lr_params, wd_weight=1e-2, global_step=None, keep_probe=None,\n                 name=\'image_classifier\', use_nesterov=True, norm_kernels=False):\n        """"""Constructor.\n\n        :param backbone_name: Name of target backbone\n        :param net_input: Input images\n        :param labels: Labels of input images\n        :param fn_activation: Main activation function of network\n        :param is_training: Training indicator variable\n        :param num_classes: Number of image classes\n        :param merge_bn: Whether to run with merged BatchNorms\n        :param merge_bn_transition: Whether to run in BatchNorm merging mode\n        :param lr_params: Learning rate parameters\n        :param wd_weight: Weight decay value\n        :param global_step: Variable for counting the training steps if exists\n        :param keep_probe: Probability to keep values in dropout\n        :param name: Network name\n        :param use_nesterov: Whether to enable nesterov momentum calculation\n        :param norm_kernels: Whether to normalize convolution kernels\n        """"""\n\n        super(ImageClassifier, self).__init__(is_training, merge_bn, merge_bn_transition, lr_params, wd_weight,\n                                              global_step, use_nesterov, norm_kernels)\n\n        self._fn_activation = fn_activation\n        self._num_classes = num_classes\n        self._name = name\n        self._keep_probe = keep_probe\n\n        self._model[\'input\'] = net_input\n        self._model[\'labels\'] = labels\n\n        self._build_network(net_input, backbone_name)\n        if is_training is not None:\n            self._create_lr_schedule()\n            self._build_losses(labels, backbone_name)\n\n    def _classifier_layers(self, input_value, num_inputs):\n        """"""Create nodes to carry out classification into target number of classes.\n\n        :param input_value: Input features\n        :param num_inputs: Number of input channels\n        :param keep_prob: Probability to keep values in dropout\n        :return: Class logits\n        """"""\n\n        with tf.variable_scope(\'classifier_layer\'):\n            conv1 = conv2d(input_value, [1, 1, num_inputs, 1024], \'dim_inc\', fn_activation=self._fn_activation,\n                           use_bias=False, use_bn=True, is_training=self._is_training,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                           add_summary=True, norm_kernel=self._norm_kernels)\n\n            glob_pool = glob_max_pool(conv1, add_summary=True)\n\n            conv2 = conv2d(glob_pool, [1, 1, 1024, 1280], \'internal\', fn_activation=self._fn_activation,\n                           use_bias=False, use_bn=True, is_training=self._is_training,\n                           merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                           add_summary=True, norm_kernel=self._norm_kernels)\n            logits = conv2d(conv2, [1, 1, 1280, self._num_classes], \'logits\', fn_activation=None,\n                            use_bias=True, use_bn=False, is_training=self._is_training,\n                            merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                            add_summary=True, norm_kernel=self._norm_kernels)\n            logits = tf.reshape(logits, [-1, self._num_classes], name=\'output\')\n\n            return logits\n\n    def _build_network(self, input_value, backbone_name):\n        """"""Creates parameterized network architecture.\n\n        :param input_value: Input tensor\n        :param backbone_name: Target name of backbone\n        """"""\n\n        with tf.variable_scope(self._name):\n            self._feature_extractor = get_backbone(backbone_name, input_value, self._fn_activation, self._is_training,\n                                                   self._merge_bn, self._merge_bn_transition, use_extra_layers=True,\n                                                   name=backbone_name, keep_probe=self._keep_probe,\n                                                   norm_kernels=self._norm_kernels)\n            extracted_features = self._feature_extractor.output\n\n            logits = self._classifier_layers(extracted_features, num_inputs=extracted_features.get_shape()[3])\n            self._model[\'output\'] = logits\n            tf.add_to_collection(\'activation_summary\', tf.summary.histogram(\'logits\', logits))\n\n    def _build_losses(self, labels, backbone_name):\n        """"""Adds losses to the training graph.\n\n        :param labels: Labels of the image\n        :param backbone_name: Target backbone name\n        """"""\n\n        assert \'output\' in self._model.keys(), \'Call _build_network method before.\'\n\n        with tf.name_scope(self._name + \'_losses\'):\n            all_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self._name)\n\n            ce_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=labels, logits=self._model[\'output\'], name=\'ce_losses\'), name=\'ce_loss\')\n            self._add_loss_summary(ce_loss, \'ce_loss\')\n\n            wd_loss = weight_decay(all_trainable_vars, self._weight_decay, \'var_reg\')\n            self._add_loss_summary(wd_loss, \'wd_loss\')\n\n            unit_gamma_loss = unit_gamma(all_trainable_vars, 0.1 * self._weight_decay, \'gamma_reg\')\n            self._add_loss_summary(unit_gamma_loss, \'gamma_loss\')\n\n            orthogonal_loss = orthogonal_conv(all_trainable_vars, 0.25 * self._model[\'lr\'], \'ort_reg\',\n                                              key_scope=get_orthogonal_scope_name(backbone_name))\n            self._add_loss_summary(orthogonal_loss, \'orthogonal_loss\')\n\n            skeleton = self._feature_extractor.skeleton\n            feature_names = [\'output_init\', \'output_2x\', \'output_4x\', \'output_8x\', \'output_16x\', \'output_32x\']\n            feature_maps = [skeleton[n] for n in feature_names]\n            feature_map_weights = [1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6]\n            decorr_losses = [decorrelate_features(f, feature_map_weights[i] * self._model[\'lr\'],\n                                                  \'decor_{}\'.format(feature_names[i]))\n                             for i, f in enumerate(feature_maps)]\n            for i, feature_name in enumerate(feature_names):\n                self._add_loss_summary(decorr_losses[i], \'decor_{}\'.format(feature_name))\n\n            decorr_loss = tf.add_n(decorr_losses, name=\'fm_reg\')\n            self._add_loss_summary(decorr_loss, \'decorr_loss\')\n\n            total_loss = tf.add_n([ce_loss, wd_loss, unit_gamma_loss, orthogonal_loss, decorr_loss],\n                                  name=\'total_loss\')\n            self._add_loss_summary(total_loss, \'total_loss\')\n            self._model[\'total_loss\'] = total_loss\n\n    def num_valid_op(self, labels):\n        """"""Returns Op to calculate number of valid predictions.\n\n        :param labels: Target image labels\n        :return: Number of valid predictions Op\n        """"""\n\n        with tf.name_scope(self._name + \'_accuracy_op\'):\n            is_correct = tf.to_float(tf.equal(tf.argmax(self._model[\'output\'], axis=-1),\n                                              tf.cast(labels, tf.int64)))\n            return tf.reduce_sum(is_correct)\n\n    def predictions(self):\n        """"""Returns model predictions.\n\n        :return: Model predictions\n        """"""\n\n        with tf.name_scope(self._name + \'_prediction_op\'):\n            return tf.argmax(self._model[\'output\'], axis=-1)\n'"
tensorflow_toolkit/action_detection/action_detection/nn/models/ssd.py,167,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nfrom action_detection.nn.utils.detector.bounding_boxes import center_encode, center_decode\nfrom action_detection.nn.utils.detector.matchers import ssd_match\n\nfrom action_detection.nn.backbones import get_backbone, get_orthogonal_scope_name\nfrom action_detection.nn.models.base_network import BaseNetwork\nfrom action_detection.nn.nodes.losses import weight_decay, unit_gamma, orthogonal_conv, focal_ce_loss,\\\n    adaptive_weighting, balanced_l1_loss, gradient_harmonized_ce_loss, max_entropy_ce_loss\nfrom action_detection.nn.nodes.ops import conv2d, safe_reduce_op\nfrom action_detection.nn.utils.detector.priors import generate_clustered_prior_boxes\n\nSSDHeadDesc = namedtuple(\'SSDHeadDesc\', \'scale, anchors, num_classes, clip, offset, internal_size\')\nSSDHead = namedtuple(\'SSDHead\', \'name, loc, conf, priors, num_classes, num_priors\')\n\n\nclass SSD(BaseNetwork):\n    """"""Describes network for the general object detection (OD) problem.\n    """"""\n\n    def __init__(self, backbone_name, net_input, labels, annot, fn_activation, is_training, head_params, merge_bn,\n                 merge_bn_transition, lr_params, mbox_param, wd_weight=1e-2, global_step=None, name=\'ssd\',\n                 use_nesterov=True, norm_kernels=False):\n        """"""Constructor.\n\n        :param backbone_name: Name of target backbone\n        :param net_input: Input images\n        :param labels: Bbox classes\n        :param annot: Bbox coordinates\n        :param fn_activation: Main activation function of network\n        :param is_training: Training indicator variable\n        :param head_params: Parameters of SSD heads\n        :param merge_bn: Whether to run with merged BatchNorms\n        :param merge_bn_transition: Whether to run in BatchNorm merging mode\n        :param lr_params: Learning rate parameters\n        :param mbox_param: Parameters for training loss\n        :param wd_weight: Weight decay value\n        :param global_step: Variable for counting the training steps if exists\n        :param name: Network name\n        :param use_nesterov: Whether to enable nesterov momentum calculation\n        :param norm_kernels: Whether to normalize convolution kernels\n        """"""\n\n        super(SSD, self).__init__(is_training, merge_bn, merge_bn_transition, lr_params, wd_weight, global_step,\n                                  use_nesterov, norm_kernels)\n\n        self._fn_activation = fn_activation\n        self._head_params = head_params\n        self._name = name\n        self._mbox_param = mbox_param\n\n        self._model[\'input\'] = net_input\n        self._model[\'labels\'] = labels\n        self._model[\'annot\'] = annot\n        self._model[\'batch_size\'] = tf.shape(net_input)[0]\n\n        self._build_network(net_input, backbone_name)\n        if is_training is not None:\n            self._create_lr_schedule()\n            self._build_losses(labels, annot, backbone_name)\n\n    def _add_detection_head(self, input_value, input_size, internal_size, num_classes, num_anchors, name):\n        """"""Adds single SSD head on top of the specified input features\n\n        :param input_value: Input features\n        :param input_size: Number of input channels\n        :param internal_size: Number of channels for the internal representation\n        :param num_classes: Number of taget classes\n        :param num_anchors: Number of anchor boxes\n        :param name: Name of block\n        :return: Location and confidence tensors\n        """"""\n\n        with tf.variable_scope(name):\n            loc_conv1 = conv2d(input_value, [1, 1, input_size, internal_size], \'loc_conv1\',\n                               use_bias=False, use_bn=True, is_training=self._is_training,\n                               fn_activation=self._fn_activation, norm_kernel=self._norm_kernels,\n                               merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition)\n            loc_conv2 = conv2d(loc_conv1, [3, 3, internal_size, 1], \'loc_conv2\',\n                               depth_wise=True, use_bias=False, use_bn=True, is_training=self._is_training,\n                               merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                               norm_kernel=self._norm_kernels)\n            locations = conv2d(loc_conv2, [1, 1, internal_size, num_anchors * 4], \'loc\',\n                               use_bias=False, use_bn=True, is_training=self._is_training,\n                               merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                               pr_product=True, add_summary=True, norm_kernel=self._norm_kernels)\n\n            conf_conv1 = conv2d(input_value, [1, 1, input_size, internal_size], \'conf_conv1\',\n                                use_bias=False, use_bn=True, is_training=self._is_training,\n                                fn_activation=self._fn_activation, norm_kernel=self._norm_kernels,\n                                merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition)\n            conf_conv2 = conv2d(conf_conv1, [3, 3, internal_size, 1], \'conf_conv2\',\n                                depth_wise=True, use_bias=False, use_bn=True, is_training=self._is_training,\n                                merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                                norm_kernel=self._norm_kernels)\n            confidences = conv2d(conf_conv2, [1, 1, internal_size, num_anchors * num_classes], \'conf\',\n                                 use_bias=False, use_bn=True, is_training=self._is_training,\n                                 merge_op=self._merge_bn, merge_op_transit=self._merge_bn_transition,\n                                 pr_product=True, add_summary=True, norm_kernel=self._norm_kernels)\n\n        return locations, confidences\n\n    def _build_detection_heads(self, head_params, skeleton, prefix=\'output_\'):\n        """"""Creates all detection heads according parameters.\n\n        :param head_params: Parmaters of heads\n        :param skeleton: Network skeleton to add heads\n        :param prefix: Name prefix for the new created heads\n        """"""\n\n        with tf.variable_scope(\'detection_heads\'):\n            heads = []\n            all_loc = []\n            all_conf = []\n            all_priors = []\n            total_num_priors = 0\n\n            for i, head_param in enumerate(head_params):\n                place_name = \'{}{}x\'.format(prefix, head_param.scale)\n                network_input = skeleton[place_name]\n                anchors = head_param.anchors\n\n                loc, conf = self._add_detection_head(\n                    network_input, network_input.get_shape()[3], head_param.internal_size,\n                    head_param.num_classes, len(anchors), \'head_{}\'.format(i + 1))\n                all_loc.append(tf.reshape(loc, [self._model[\'batch_size\'], -1, 4]))\n                all_conf.append(tf.reshape(conf, [self._model[\'batch_size\'], -1, head_param.num_classes]))\n\n                feature_size = network_input.get_shape()[1:3]\n                image_size = self._model[\'input\'].get_shape().as_list()[1:3]\n                priors_array, num_priors =\\\n                    generate_clustered_prior_boxes(feature_size, image_size, anchors, head_param.scale,\n                                                   head_param.clip, head_param.offset)\n                priors = tf.constant(priors_array, name=\'head_{}/PriorBoxClustered\'.format(i))\n\n                all_priors.append(priors)\n                total_num_priors += num_priors\n\n                heads.append(SSDHead(name=place_name, loc=loc, conf=conf, priors=priors,\n                                     num_classes=head_param.num_classes, num_priors=num_priors))\n\n                tf.add_to_collection(\'activation_summary\', tf.summary.histogram(place_name + \'_loc\', loc))\n                tf.add_to_collection(\'activation_summary\', tf.summary.histogram(place_name + \'_conf\', conf))\n\n        self._model[\'heads\'] = heads\n        self._model[\'encoded_loc\'] = tf.concat(all_loc, axis=1, name=\'out_detection_loc\')\n        self._model[\'logits\'] = tf.concat(all_conf, axis=1, name=\'out_detection_logits\')\n        self._model[\'priors\'] = tf.concat(all_priors, axis=0, name=\'out_detection_priors\')\n        self._model[\'total_num_priors\'] = total_num_priors\n\n        self._model[\'pr_loc\'] = center_decode(self._model[\'encoded_loc\'], self._model[\'priors\'],\n                                              self._mbox_param[\'variance\'], clip=self._is_training is None)\n        self._model[\'pr_det_conf\'] = tf.nn.softmax(self._model[\'logits\'], axis=-1, name=\'out_detection_conf\')\n\n    def _build_network(self, input_value, backbone_name):\n        """"""Creates parameterized network architecture.\n\n        :param input_value: Input tensor\n        :param backbone_name: Target name of backbone\n        """"""\n\n        with tf.variable_scope(self._name):\n            self._feature_extractor = get_backbone(backbone_name, input_value, self._fn_activation,\n                                                   self._is_training, self._merge_bn, self._merge_bn_transition,\n                                                   use_extra_layers=False, name=backbone_name,\n                                                   norm_kernels=self._norm_kernels)\n            skeleton = self._feature_extractor.skeleton\n\n            self._build_detection_heads(self._head_params, skeleton)\n\n    @staticmethod\n    def _get_mean_matched_values(trg_ids, src_values, name=None):\n        """"""Calculates mean bbox for each instance.\n\n        :param trg_ids: Ground truth IDs\n        :param src_values: Values to average\n        :param name: Name of block\n        :return: GT IDs and averaged bboxes tuple\n        """"""\n\n        def _process():\n            unique_gt_ids, unique_gt_ids_idx, unique_gt_ids_counts = tf.unique_with_counts(trg_ids)\n\n            dense_shape = [tf.size(unique_gt_ids), tf.size(trg_ids)]\n            dense_size = dense_shape[0] * dense_shape[1]\n            range_ids = tf.range(0, tf.size(trg_ids), dtype=tf.int32)\n            grouped_sparse_idx = tf.expand_dims(tf.cast(unique_gt_ids_idx * tf.size(trg_ids) + range_ids, tf.int64), 1)\n\n            normalizer = tf.reciprocal(tf.cast(unique_gt_ids_counts, tf.float32))\n            unpacked_values = tf.unstack(src_values, axis=-1)\n\n            mean_slices = []\n            for values_slice in unpacked_values:\n                dense_values_slice = tf.reshape(tf.sparse_tensor_to_dense(\n                    tf.SparseTensor(grouped_sparse_idx, values_slice, [dense_size]),\n                    default_value=0.0, validate_indices=False), dense_shape)\n                mean_slice = normalizer * tf.reduce_sum(dense_values_slice, axis=-1)\n                mean_slices.append(tf.reshape(mean_slice, [-1, 1]))\n\n            return unique_gt_ids, tf.concat(tuple(mean_slices), axis=1)\n\n        with tf.name_scope(name, \'mean_matched_loc\'):\n            return tf.cond(tf.greater(tf.size(trg_ids), 0),\n                           lambda: _process(),\n                           lambda: (trg_ids, src_values))\n\n    def _get_top_matched_mask(self, gt_ids, matched_scores, threshold, max_num_samples_per_gt, name,\n                              drop_ratio=None, min_default_score=1e-3, soft_num_samples=True):\n        """"""Filters top-k matched prior boxes per instance.\n\n        :param gt_ids: Ground truth IDs\n        :param matched_scores: Score of matched priors\n        :param threshold: Threshold to filter low-confidence matches\n        :param max_num_samples_per_gt: Size of top-k queue\n        :param name: Name of block\n        :param drop_ratio: Ratio of matches per instance to drop\n        :param min_default_score: Min possible score\n        :param soft_num_samples: Whether to relax hard top-k border into soft one\n        :return: mask of filtered matches and per instance weights\n        """"""\n\n        assert threshold >= 0.0\n        assert max_num_samples_per_gt > 0\n        assert min_default_score > 0.0\n\n        def _process():\n            unique_gt_ids, unique_gt_ids_idx, unique_gt_ids_counts = tf.unique_with_counts(gt_ids)\n            dense_shape = [tf.size(unique_gt_ids), tf.size(gt_ids)]\n            dense_size = dense_shape[0] * dense_shape[1]\n            range_ids = tf.range(0, tf.size(gt_ids), dtype=tf.int32)\n            grouped_sparse_idx = tf.expand_dims(\n                tf.cast(unique_gt_ids_idx * tf.size(gt_ids) + range_ids, tf.int64), 1)\n\n            if drop_ratio is not None:\n                drop_mask = tf.less(tf.random_uniform([tf.size(matched_scores)], 0.0, 1.0), drop_ratio)\n                dropped_matched_scores = tf.where(\n                    drop_mask, tf.fill(tf.shape(matched_scores), min_default_score), matched_scores)\n            else:\n                dropped_matched_scores = matched_scores\n\n            grouped_matched_scores = tf.sparse_tensor_to_dense(\n                tf.SparseTensor(grouped_sparse_idx, dropped_matched_scores, [dense_size]),\n                default_value=0.0, validate_indices=False)\n            grouped_positions = tf.sparse_tensor_to_dense(\n                tf.SparseTensor(grouped_sparse_idx, range_ids, [dense_size]),\n                default_value=-1, validate_indices=False)\n\n            if soft_num_samples:\n                num_samples_per_gt = tf.maximum(max_num_samples_per_gt, tf.reduce_min(unique_gt_ids_counts))\n                tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'top_positives/num_per_gt\', num_samples_per_gt))\n            else:\n                num_samples_per_gt = max_num_samples_per_gt\n\n            top_values, top_indices = tf.nn.top_k(\n                tf.reshape(grouped_matched_scores, dense_shape), k=num_samples_per_gt, sorted=False)\n            valid_top_values_mask = tf.greater(top_values, threshold)\n\n            shifted_top_indices = top_indices + tf.reshape(\n                tf.range(0, tf.size(unique_gt_ids), dtype=tf.int32) * tf.size(gt_ids), [-1, 1])\n            filtered_top_indices = tf.boolean_mask(shifted_top_indices, valid_top_values_mask)\n            out_ids = tf.gather(grouped_positions, filtered_top_indices)\n            self._add_loss_summary(tf.size(out_ids), \'num_matches\')\n\n            num_valid_per_match = tf.reduce_sum(tf.cast(valid_top_values_mask, tf.float32), axis=-1, keepdims=True)\n            tiled_num_valid_per_match = tf.tile(num_valid_per_match, [1, num_samples_per_gt])\n            out_num_valid_per_match = tf.boolean_mask(tiled_num_valid_per_match, valid_top_values_mask)\n            out_instance_weights = tf.reciprocal(tf.maximum(1.0, out_num_valid_per_match))\n\n            sparse_to_dens_ids = tf.expand_dims(tf.cast(out_ids, tf.int64), 1)\n            out_matched_mask = tf.sparse_tensor_to_dense(\n                tf.SparseTensor(sparse_to_dens_ids, tf.ones([tf.size(out_ids)], dtype=tf.bool), [tf.size(gt_ids)]),\n                default_value=False, validate_indices=False)\n\n            return out_matched_mask, out_instance_weights\n\n        with tf.name_scope(name):\n            return tf.cond(tf.greater(tf.size(gt_ids), 0),\n                           lambda: _process(),\n                           lambda: (tf.zeros_like(gt_ids, dtype=tf.bool), tf.ones_like(gt_ids, dtype=tf.float32)))\n\n    def _multibox_loss(self, gt_labels, gt_bboxes, class_agnostic=False, output_original_labels=False):\n        """"""Creates main detector losses.\n\n        :param gt_labels: BBox labels\n        :param gt_bboxes: BBox coordinates\n        :param class_agnostic: Whether to interpret positive labels as single positive class\n        :param output_original_labels: Map of original class labels\n        :return: Tuple of confidence and location losses\n        """"""\n\n        def _default_loc_loss(pr_encoded_loc, gt_encoded_loc):\n            """"""Location loss\n\n            :param pr_encoded_loc: Encoded predicted locations\n            :param gt_encoded_loc: Encoded ground truth locations\n            :return: Loss value\n            """"""\n\n            with tf.name_scope(\'loc_loss\'):\n                diff = pr_encoded_loc - gt_encoded_loc\n                loss_values = balanced_l1_loss(diff, alpha=0.5, gamma=1.5)\n                return tf.reduce_sum(loss_values, axis=1)\n\n        def _conf_loss(predicted_conf, gt_classes, name, weights=None):\n            """"""Confidence loss\n\n            :param predicted_conf: Predicted class distributions\n            :param gt_classes: Ground truth classes\n            :param name: Name of block\n            :param weights: Instance weights\n            :return: Loss value\n            """"""\n\n            enable_max_entropy_loss = \'entropy_weight\' in self._mbox_param and self._mbox_param[\'entropy_weight\'] > 0.0\n            enable_focal_loss = \'focal_alpha\' in self._mbox_param and self._mbox_param[\'focal_alpha\'] > 0.0 and \\\n                                \'focal_gamma\' in self._mbox_param and self._mbox_param[\'focal_gamma\'] > 0.0\n            enable_gh_loss = \'gh_num_bins\' in self._mbox_param and self._mbox_param[\'gh_num_bins\'] > 0\n            if enable_max_entropy_loss and enable_focal_loss and enable_gh_loss:\n                raise Exception(\'Cannot enable different CE losses simultaneously\')\n\n            with tf.variable_scope(name):\n                if enable_max_entropy_loss:\n                    ce_losses = max_entropy_ce_loss(gt_classes, predicted_conf,\n                                                    self._mbox_param[\'entropy_weight\'], \'conf_ce_losses\',\n                                                    enable_gsoftmax=False)\n                elif enable_focal_loss:\n                    ce_losses = focal_ce_loss(gt_classes, predicted_conf,\n                                              self._mbox_param[\'focal_alpha\'], self._mbox_param[\'focal_gamma\'],\n                                              \'conf_ce_losses\')\n                elif enable_gh_loss:\n                    ce_losses = gradient_harmonized_ce_loss(gt_classes, predicted_conf, \'conf_ce_losses\',\n                                                            self._mbox_param[\'gh_num_bins\'])\n                else:\n                    ce_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=gt_classes, logits=predicted_conf,\n                                                                               name=\'ce_losses\')\n\n                if weights is not None:\n                    ce_losses = ce_losses * weights\n                ce_losses = tf.boolean_mask(ce_losses, tf.greater(ce_losses, 0.0))\n            return ce_losses\n\n        def _get_hard_samples(values, out_num):\n            """"""Safely extracts top-k values.\n\n            :param values: Input values\n            :param out_num: Number of values to extract\n            :return: Extracted values\n            """"""\n\n            def _hardest():\n                top_values, _ = tf.nn.top_k(values, out_num)\n                return top_values\n\n            with tf.name_scope(\'hard_sample_mining\'):\n                num_values = tf.size(values)\n                return tf.cond(tf.less_equal(num_values, out_num), lambda: values, _hardest)\n\n        def _giou_loss(pr_bboxes, input_gt_bboxes):\n            """"""Calculates General IoU loss directly over bboxes.\n\n            :param pr_bboxes: Predicted bbox coordinates\n            :param input_gt_bboxes: Ground truth bbox coordinates\n            :return: Loss value\n            """"""\n\n            with tf.name_scope(\'giou_loss\'):\n                fixed_pr_ymin = tf.minimum(pr_bboxes[:, 0], pr_bboxes[:, 2])\n                fixed_pr_xmin = tf.minimum(pr_bboxes[:, 1], pr_bboxes[:, 3])\n                fixed_pr_ymax = tf.maximum(pr_bboxes[:, 0], pr_bboxes[:, 2])\n                fixed_pr_xmax = tf.maximum(pr_bboxes[:, 1], pr_bboxes[:, 3])\n\n                intersect_ymin = tf.maximum(fixed_pr_ymin, input_gt_bboxes[:, 0])\n                intersect_xmin = tf.maximum(fixed_pr_xmin, input_gt_bboxes[:, 1])\n                intersect_ymax = tf.minimum(fixed_pr_ymax, input_gt_bboxes[:, 2])\n                intersect_xmax = tf.minimum(fixed_pr_xmax, input_gt_bboxes[:, 3])\n\n                intersect_height = tf.maximum(0.0, intersect_ymax - intersect_ymin)\n                intersect_width = tf.maximum(0.0, intersect_xmax - intersect_xmin)\n                intersect_areas = intersect_width * intersect_height\n\n                pr_areas = (fixed_pr_ymax - fixed_pr_ymin) * (fixed_pr_xmax - fixed_pr_xmin)\n                gt_areas = (input_gt_bboxes[:, 2] - input_gt_bboxes[:, 0]) *\\\n                           (input_gt_bboxes[:, 3] - input_gt_bboxes[:, 1])\n                union_areas = pr_areas + gt_areas - intersect_areas\n\n                overlaps = tf.where(tf.greater(union_areas, 0.0),\n                                    intersect_areas / union_areas,\n                                    tf.zeros_like(intersect_areas))\n\n                enclose_ymin = tf.minimum(fixed_pr_ymin, input_gt_bboxes[:, 0])\n                enclose_xmin = tf.minimum(fixed_pr_xmin, input_gt_bboxes[:, 1])\n                enclose_ymax = tf.maximum(fixed_pr_ymax, input_gt_bboxes[:, 2])\n                enclose_xmax = tf.maximum(fixed_pr_xmax, input_gt_bboxes[:, 3])\n\n                enclose_height = tf.maximum(0.0, enclose_ymax - enclose_ymin)\n                enclose_width = tf.maximum(0.0, enclose_xmax - enclose_xmin)\n                enclose_areas = enclose_width * enclose_height\n\n                enclose_ratio = tf.where(tf.greater(enclose_areas, 0.0),\n                                         (enclose_areas - union_areas) / enclose_areas,\n                                         tf.zeros_like(enclose_areas))\n                generalized_overlaps = overlaps - enclose_ratio\n                out_losses = 1.0 - generalized_overlaps\n\n                return out_losses\n\n        def _repulsion_loss(pr_bboxes, first_gt_ids, second_gt_ids, scores, eps=1e-5):\n            """"""Calculates Repulsion loss between current predicted bbox and top-2 anchor box.\n\n            :param pr_bboxes: Predicted bbox coordinates\n            :param first_gt_ids: Coordinates of matched GT bboxes\n            :param second_gt_ids: Coordinates of closest GT bboxes\n            :param scores: Matched scores\n            :param eps: Epsilon scalar value\n            :return: Loss value\n            """"""\n\n            def _process(mask):\n                valid_pr_bboxes = tf.boolean_mask(pr_bboxes, mask)\n                valid_first_gt_ids = tf.boolean_mask(first_gt_ids, mask)\n                valid_second_gt_ids = tf.boolean_mask(second_gt_ids, mask)\n\n                first_gt_bboxes = tf.gather(tf.reshape(gt_bboxes, [-1, 4]), valid_first_gt_ids)\n                second_gt_bboxes = tf.gather(tf.reshape(gt_bboxes, [-1, 4]), valid_second_gt_ids)\n\n                fixed_pr_ymin = tf.minimum(valid_pr_bboxes[:, 0], valid_pr_bboxes[:, 2])\n                fixed_pr_xmin = tf.minimum(valid_pr_bboxes[:, 1], valid_pr_bboxes[:, 3])\n                fixed_pr_ymax = tf.maximum(valid_pr_bboxes[:, 0], valid_pr_bboxes[:, 2])\n                fixed_pr_xmax = tf.maximum(valid_pr_bboxes[:, 1], valid_pr_bboxes[:, 3])\n\n                trg_intersect_ymin = tf.maximum(fixed_pr_ymin, second_gt_bboxes[:, 0])\n                trg_intersect_xmin = tf.maximum(fixed_pr_xmin, second_gt_bboxes[:, 1])\n                trg_intersect_ymax = tf.minimum(fixed_pr_ymax, second_gt_bboxes[:, 2])\n                trg_intersect_xmax = tf.minimum(fixed_pr_xmax, second_gt_bboxes[:, 3])\n\n                gt_intersect_ymin = tf.maximum(first_gt_bboxes[:, 0], second_gt_bboxes[:, 0])\n                gt_intersect_xmin = tf.maximum(first_gt_bboxes[:, 1], second_gt_bboxes[:, 1])\n                gt_intersect_ymax = tf.minimum(first_gt_bboxes[:, 2], second_gt_bboxes[:, 2])\n                gt_intersect_xmax = tf.minimum(first_gt_bboxes[:, 3], second_gt_bboxes[:, 3])\n\n                trg_intersect_areas = tf.maximum(0.0, trg_intersect_ymax - trg_intersect_ymin) *\\\n                                      tf.maximum(0.0, trg_intersect_xmax - trg_intersect_xmin)\n                gt_intersect_areas = tf.maximum(0.0, gt_intersect_ymax - gt_intersect_ymin) *\\\n                                     tf.maximum(0.0, gt_intersect_xmax - gt_intersect_xmin)\n                gt_areas = (second_gt_bboxes[:, 2] - second_gt_bboxes[:, 0]) * \\\n                           (second_gt_bboxes[:, 3] - second_gt_bboxes[:, 1])\n\n                shifted_intersect_over_gt = tf.where(tf.greater(gt_areas, 0.0),\n                                                     (trg_intersect_areas - gt_intersect_areas) / gt_areas,\n                                                     tf.zeros_like(trg_intersect_areas))\n\n                valid_intersections_mask = tf.greater(shifted_intersect_over_gt, 0.0)\n                valid_intersections = tf.boolean_mask(shifted_intersect_over_gt, valid_intersections_mask)\n                out_losses = tf.negative(tf.log(1.0 + eps - valid_intersections))\n\n                return out_losses\n\n            with tf.name_scope(\'repulsion_loss\'):\n                valid_mask = tf.greater(scores, 0.0)\n                num_valid_pairs = tf.reduce_sum(tf.cast(valid_mask, tf.int32))\n\n                return tf.cond(tf.greater(num_valid_pairs, 0),\n                               lambda: _process(valid_mask),\n                               lambda: tf.zeros([0], dtype=tf.float32))\n\n        def _compactness_loss(src_gt_idx, src_decoded_loc, src_gt_bboxes):\n            """"""Calculates location loss between GT and mean bbox coordinates.\n\n            :param src_gt_idx: Ground truth IDs\n            :param src_decoded_loc: Decoded predicted locations\n            :param src_gt_bboxes: Ground truth bbox coordinates\n            :return: Loss value\n            """"""\n\n            with tf.name_scope(\'compactness_loss\'):\n                trg_gt_idx, mean_decoded_loc = self._get_mean_matched_values(src_gt_idx, src_decoded_loc)\n                trg_gt_bboxes = tf.gather(tf.reshape(src_gt_bboxes, [-1, 4]), trg_gt_idx)\n                return _giou_loss(mean_decoded_loc, trg_gt_bboxes)\n\n        with tf.name_scope(\'multibox_loss\'):\n            enable_comp_loss =\\\n                \'comp_loss_max_num_samples\' in self._mbox_param and self._mbox_param[\'comp_loss_max_num_samples\'] > 0\n            enable_repulsion_loss = enable_comp_loss and self._mbox_param[\'repulsion_loss_weight\'] > 0.0\n\n            valid_data_mask = tf.greater_equal(gt_labels, 0)\n            matches_ids, matched_scores, second_matches_ids, second_matched_scores = ssd_match(\n                gt_bboxes, valid_data_mask, self._model[\'priors\'], self._mbox_param[\'threshold\'],\n                output_second_order=enable_repulsion_loss)\n            matches_mask = tf.greater_equal(matches_ids, 0)\n\n            valid_gt_ids = tf.boolean_mask(matches_ids, matches_mask)\n            valid_matched_scores = tf.boolean_mask(matched_scores, matches_mask)\n\n            if enable_repulsion_loss:\n                second_valid_gt_ids = tf.boolean_mask(second_matches_ids, matches_mask)\n                second_valid_matched_scores = tf.boolean_mask(second_matched_scores, matches_mask)\n\n            all_prior_ids = tf.tile(tf.reshape(tf.range(0, tf.shape(matches_ids)[1], dtype=tf.int32), [1, -1]),\n                                    [tf.shape(matches_ids)[0], 1])\n            valid_prior_ids = tf.boolean_mask(all_prior_ids, matches_mask)\n\n            positives_encoded_loc_data = tf.boolean_mask(self._model[\'encoded_loc\'], matches_mask)\n            positives_decoded_loc_data = tf.boolean_mask(self._model[\'pr_loc\'], matches_mask)\n            positives_conf_data = tf.boolean_mask(self._model[\'logits\'], matches_mask)\n            positives_gt_labels = tf.gather(tf.reshape(gt_labels, [-1]), valid_gt_ids)\n            positives_gt_loc = tf.gather(tf.reshape(gt_bboxes, [-1, 4]), valid_gt_ids)\n            positives_priors = tf.gather(self._model[\'priors\'], valid_prior_ids)\n\n            # Compactness location loss of extended set of matches\n            if enable_comp_loss:\n                best_matches_mask, _ = self._get_top_matched_mask(\n                    valid_gt_ids, valid_matched_scores, 1e-7, self._mbox_param[\'comp_loss_max_num_samples\'], \'det_top\')\n                self._add_loss_summary(tf.reduce_sum(tf.cast(best_matches_mask, tf.float32)) / tf.cast(\n                    tf.maximum(1, tf.size(best_matches_mask)), tf.float32), \'compactness/density\')\n\n                auxiliary_pos_loc_data = tf.boolean_mask(positives_decoded_loc_data, best_matches_mask)\n                auxiliary_valid_gt_ids = tf.boolean_mask(valid_gt_ids, best_matches_mask)\n\n                comp_losses = _compactness_loss(auxiliary_valid_gt_ids, auxiliary_pos_loc_data, gt_bboxes)\n                comp_loss = safe_reduce_op(comp_losses, tf.reduce_mean)\n                self._add_loss_summary(comp_loss, \'comp_loss\')\n\n                if enable_repulsion_loss:\n                    # noinspection PyUnboundLocalVariable\n                    repulsion_losses = _repulsion_loss(auxiliary_pos_loc_data, auxiliary_valid_gt_ids,\n                                                       tf.boolean_mask(second_valid_gt_ids, best_matches_mask),\n                                                       tf.boolean_mask(second_valid_matched_scores, best_matches_mask))\n                    repulsion_loss = safe_reduce_op(repulsion_losses, tf.reduce_mean)\n\n            out_positives_gt_labels = positives_gt_labels\n\n            do_instance_sampling =\\\n                \'max_num_samples_per_gt\' in self._mbox_param and self._mbox_param[\'max_num_samples_per_gt\'] > 0\n            if do_instance_sampling:\n                best_matches_mask, best_matches_weights = self._get_top_matched_mask(\n                    valid_gt_ids, valid_matched_scores, 1e-7, self._mbox_param[\'max_num_samples_per_gt\'], \'det_top\',\n                    drop_ratio=self._mbox_param[\'matches_drop_ratio\'])\n                self._add_loss_summary(tf.reduce_sum(tf.cast(best_matches_mask, tf.float32)) / tf.cast(\n                    tf.maximum(1, tf.size(best_matches_mask)), tf.float32), \'top_positives/density\')\n\n                positives_encoded_loc_data = tf.boolean_mask(positives_encoded_loc_data, best_matches_mask)\n                positives_decoded_loc_data = tf.boolean_mask(positives_decoded_loc_data, best_matches_mask)\n                positives_conf_data = tf.boolean_mask(positives_conf_data, best_matches_mask)\n                positives_gt_labels = tf.boolean_mask(positives_gt_labels, best_matches_mask)\n                positives_gt_loc = tf.boolean_mask(positives_gt_loc, best_matches_mask)\n                positives_priors = tf.boolean_mask(positives_priors, best_matches_mask)\n\n            # default L1 location loss\n            encoded_gt_loc_data = center_encode(positives_gt_loc, positives_priors, self._mbox_param[\'variance\'])\n            positive_loc_losses = _default_loc_loss(positives_encoded_loc_data, encoded_gt_loc_data)\n            default_loc_loss = safe_reduce_op(positive_loc_losses, tf.reduce_mean)\n            self._model[\'loc_loss\'] = default_loc_loss\n\n            # GIoU location loss\n            positive_giou_losses = _giou_loss(positives_decoded_loc_data, positives_gt_loc)\n            giou_loss = safe_reduce_op(positive_giou_losses, tf.reduce_mean)\n            self._model[\'giou_loss\'] = giou_loss\n\n            main_loc_loss_list = [default_loc_loss, giou_loss]\n            if enable_comp_loss:\n                # noinspection PyUnboundLocalVariable\n                main_loc_loss_list.append(comp_loss)\n            main_loc_loss = adaptive_weighting(main_loc_loss_list, name=\'gloc_loss\')\n            self._model[\'main_loc_loss\'] = main_loc_loss\n            self._add_loss_summary(main_loc_loss, \'main_loc_loss\')\n\n            if enable_repulsion_loss:\n                # noinspection PyUnboundLocalVariable\n                general_loc_loss = tf.add_n([main_loc_loss, self._mbox_param[\'repulsion_loss_weight\'] * repulsion_loss],\n                                            name=\'gloc_loss\')\n                self._model[\'general_loc_loss\'] = general_loc_loss\n                self._add_loss_summary(general_loc_loss, \'general_loc_loss\')\n                self._add_loss_summary(repulsion_loss, \'repulsion_loss\')\n            else:\n                general_loc_loss = main_loc_loss\n\n            fixed_positives_gt_labels = tf.ones_like(positives_gt_labels) if class_agnostic else positives_gt_labels\n            # noinspection PyUnboundLocalVariable\n            positive_conf_losses = _conf_loss(\n                positives_conf_data, fixed_positives_gt_labels, \'pos_conf_loss\',\n                best_matches_weights if do_instance_sampling and self._mbox_param[\'instance_normalization\'] else None)\n            self._model[\'positive_conf_losses\'] = positive_conf_losses\n\n            negatives_conf_data = tf.boolean_mask(self._model[\'logits\'], tf.logical_not(matches_mask))\n            negatives_conf_losses =\\\n                _conf_loss(negatives_conf_data, tf.fill([tf.shape(negatives_conf_data)[0]],\n                                                        self._mbox_param[\'bg_class\']), \'neg_conf_loss\')\n\n            num_positives = tf.cast(tf.shape(positive_conf_losses)[0], tf.float32)\n            num_negatives = tf.cast(float(self._mbox_param[\'neg_factor\']) * num_positives, tf.int32)\n            hard_negatives_conf_losses = _get_hard_samples(negatives_conf_losses, num_negatives)\n            self._model[\'hard_negatives_conf_losses\'] = hard_negatives_conf_losses\n\n            num_conf_losses = tf.maximum(1, tf.size(positive_conf_losses) + tf.size(hard_negatives_conf_losses))\n            conf_loss = tf.divide(tf.reduce_sum(positive_conf_losses) + tf.reduce_sum(hard_negatives_conf_losses),\n                                  tf.cast(num_conf_losses, tf.float32))\n\n        if output_original_labels:\n            return conf_loss, general_loc_loss, matches_mask,\\\n                   valid_matched_scores, valid_gt_ids, out_positives_gt_labels\n        else:\n            return conf_loss, general_loc_loss\n\n    def _build_losses(self, labels, annot, backbone_name):\n        """"""Adds losses to the training graph.\n\n        :param labels: BBox labels\n        :param annot: BBox coordinates\n        :param backbone_name: Target backbone name\n        """"""\n\n        with tf.name_scope(self._name + \'_losses\'):\n            all_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self._name)\n\n            conf_loss, loc_loss = self._multibox_loss(labels, annot)\n            self._add_loss_summary(conf_loss, \'conf_loss\')\n            self._add_loss_summary(loc_loss, \'loc_loss\')\n            mbox_loss = self._mbox_param[\'cl_weight\'] * conf_loss + loc_loss\n            self._add_loss_summary(mbox_loss, \'mbox_loss\')\n\n            wd_loss = weight_decay(all_trainable_vars, self._weight_decay, \'var_reg\')\n            self._add_loss_summary(wd_loss, \'wd_loss\')\n\n            unit_gamma_loss = unit_gamma(all_trainable_vars, 0.1 * self._weight_decay, \'gamma_reg\')\n            self._add_loss_summary(unit_gamma_loss, \'gamma_loss\')\n\n            orthogonal_loss = orthogonal_conv(all_trainable_vars, 0.25 * self._model[\'lr\'], \'ort_reg\',\n                                              key_scope=get_orthogonal_scope_name(backbone_name))\n            self._add_loss_summary(orthogonal_loss, \'orthogonal_loss\')\n\n            total_loss = tf.add_n([mbox_loss, wd_loss, unit_gamma_loss, orthogonal_loss], name=\'total_loss\')\n            self._add_loss_summary(total_loss, \'total_loss\')\n            self._model[\'total_loss\'] = total_loss\n\n    @property\n    def predictions(self):\n        """"""Returns model predictions Op\n\n        :return: Predicted locations and classes Ops\n        """"""\n\n        return self._model[\'pr_loc\'], self._model[\'pr_det_conf\']\n'"
tensorflow_toolkit/action_detection/action_detection/nn/monitors/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/action_detection/nn/monitors/action_monitor.py,1,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport time\nfrom functools import partial\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import trange, tqdm\n\nfrom action_detection.nn.data.dataset import get_detection_dataset\nfrom action_detection.nn.models import ActionNet\nfrom action_detection.nn.monitors.base_monitor import BaseMonitor\nfrom action_detection.postprocessing.detection_output import action_detection_output, action_warp_gt\nfrom action_detection.postprocessing.quality import calc_map_mr, calc_action_accuracy\n\n\nclass ActionMonitor(BaseMonitor):\n    """"""Class to control the behaviour of action network.\n    """"""\n\n    def _create_model(self, network_input, global_step=None, is_training=None,\n                      merge_train_ops=False, merging_transit=False):\n        if len(network_input) == 1:\n            images, labels, annotation = network_input[0], None, None\n        else:\n            images, labels, annotation = network_input\n\n        fn_activation = partial(tf.nn.leaky_relu, alpha=0.1)\n\n        return ActionNet(self._params.backbone, images, labels, annotation, fn_activation,\n                         is_training, self._params.head_params, merge_train_ops, merging_transit,\n                         self._params.lr_params, self._params.mbox_params, self._params.action_params,\n                         global_step=global_step, name=self._params.name,\n                         use_nesterov=self._params.use_nesterov, norm_kernels=self._params.norm_kernels)\n\n    def _transfer_train_parameters(self, sess, model):\n        trg_scope = \'{}/{}\'.format(self._params.name, self._params.backbone)\n\n        model.load_available_variables(self._init_model_path, sess, self._src_scope,\n                                       \'{}/shared\'.format(trg_scope))\n        model.load_available_variables(self._init_model_path, sess, self._src_scope,\n                                       \'{}/detection\'.format(trg_scope))\n        model.load_available_variables(self._init_model_path, sess, self._src_scope,\n                                       \'{}/classification\'.format(trg_scope))\n\n    def _get_extra_param_names(self):\n        return [\'moving_center_{}\'.format(i) for i in xrange(self._params.num_actions)]\n\n    def _create_train_dataset(self, data_path):\n        return get_detection_dataset(data_path, self._params.image_size, self._batch_size, True, \'train_data\',\n                                     self._train_prefetch_size, self._train_data_process_num_threads,\n                                     tuple_process_fn=self._params.tuple_augmentation,\n                                     image_process_fn=self._params.image_augmentation,\n                                     max_num_objects_per_image=self._params.max_num_objects_per_image,\n                                     labels_map=self._params.labels_map,\n                                     ignore_classes=self._params.ignore_classes,\n                                     use_class_balancing=self._params.use_class_balancing)\n\n    def _create_test_dataset(self, data_path):\n        return get_detection_dataset(data_path, self._params.image_size, self._batch_size, False, \'val_data\',\n                                     self._test_prefetch_size, self._test_data_process_num_threads,\n                                     max_num_objects_per_image=self._params.max_num_objects_per_image,\n                                     use_difficult=True, labels_map=self._params.labels_map)\n\n    def _get_test_ops(self, model, network_inputs):\n        return model.predictions\n\n    def _test(self, sess, network_inputs, test_ops):\n        def _normalize_confusion_matrix(input_cm):\n            """"""Normalizes by row the input confusion matrix.\n\n            :param input_cm: Input confusion matrix\n            :return: Normalized confusion matrix\n            """"""\n\n            assert len(input_cm.shape) == 2\n            assert input_cm.shape[0] == input_cm.shape[1]\n\n            row_sums = np.maximum(1, np.sum(input_cm, axis=1, keepdims=True)).astype(np.float32)\n            norm_cm = input_cm.astype(np.float32) / row_sums\n\n            return norm_cm\n\n        def _print_confusion_matrix(input_cm, name, classes):\n            """"""Prints values of the confusion matrix.\n\n            :param input_cm: Input confusion matrix\n            :param name: Header of the output\n            :param classes: List of class names\n            """"""\n\n            assert len(input_cm.shape) == 2\n            assert input_cm.shape[0] == input_cm.shape[1]\n            assert len(classes) == input_cm.shape[0]\n\n            max_class_name_length = max([len(cl) for cl in classes])\n\n            norm_cm = _normalize_confusion_matrix(input_cm)\n\n            self._log(\'{} CM:\'.format(name))\n            for i, class_name in enumerate(classes):\n                values = \'\'\n                for j in range(len(classes)):\n                    values += \'{:7.2f} |\'.format(norm_cm[i, j] * 100.)\n                self._log(\'   {0: <{1}}|{2}\'.format(class_name, max_class_name_length + 1, values))\n\n        def _calculate_accuracy(input_cm):\n            """"""Calculates the mean of diagonal elements of normalized confusion matrix.\n\n            :param input_cm: Input confusion matrix\n            :return: Accuracy value\n            """"""\n\n            assert len(input_cm.shape) == 2\n            assert input_cm.shape[0] == input_cm.shape[1]\n\n            base_acc = float(np.sum(input_cm.diagonal())) / float(np.maximum(1, np.sum(input_cm)))\n\n            norm_cm = _normalize_confusion_matrix(input_cm)\n            norm_acc = np.mean(norm_cm.diagonal())\n\n            return base_acc, norm_acc\n\n        all_detections = []\n        all_gt = []\n\n        for _ in trange(self._params.val_steps, desc=\'Dumping predictions\'):\n            gt_labels, gt_annot, pr_loc, pr_det_conf, pr_action_conf = sess.run(network_inputs[1:] + test_ops)\n\n            batch_detections = action_detection_output(pr_loc, pr_det_conf, pr_action_conf, self._params.bg_class,\n                                                       min_det_conf=self._params.det_conf,\n                                                       min_action_conf=self._params.action_conf)\n            batch_gt = action_warp_gt(gt_annot, gt_labels, self._params.bg_class)\n\n            all_detections.extend(batch_detections)\n            all_gt.extend(batch_gt)\n\n        sess.close()\n\n        action_cm = calc_action_accuracy(all_detections, all_gt, self._params.bg_class, self._params.num_actions)\n        base_accuracy, norm_accuracy = _calculate_accuracy(action_cm)\n        self._log(\'Task Action Accuracy. Base: {:.2f} Norm: {:.2f}\'.format(base_accuracy * 100., norm_accuracy * 100.))\n        _print_confusion_matrix(action_cm, \'Task\', self._params.valid_actions)\n\n        det_metrics_by_class = calc_map_mr(all_detections, all_gt, return_all=True)\n        ap_value, mr_value = det_metrics_by_class[(self._params.bg_class + 1) % 2]\n        self._log(\'Class-agnostic AP: {:.3f}%   mr@0.1: {:.3f}%\'.format(1e2 * ap_value, 1e2 * mr_value))\n\n    def _get_out_node_names(self):\n        det_name_templates = [\'{}/out_detection_loc\', \'{}/out_detection_conf\']\n        out_det_node_names = [t.format(self._params.name) for t in det_name_templates]\n\n        action_name_template = \'{}/action_heads/out_head_{}_anchor_{}\'\n        out_action_node_names = []\n        for head_id, head_desc in enumerate(self._params.head_params):\n            for anchor_id in xrange(head_desc.anchors.shape[0]):\n                out_action_node_names.append(action_name_template.format(self._params.name, head_id + 1, anchor_id + 1))\n\n        return out_det_node_names + out_action_node_names\n\n    def _demo(self, sess, data_path, input_image, inference_ops, out_scale):\n        def _draw_actions(image, detections, bg_label, min_det_score, min_action_score):\n            """"""Draws detected persons with action captions.\n\n            :param image: Input image\n            :param detections: Detected boxes\n            :param bg_label: Label of background class\n            :param min_det_score: Min detection score to show box\n            :param min_action_score: Min action score to show box\n            """"""\n\n            image = np.copy(image)\n            image_height, image_width = image.shape[:2]\n\n            det_label = (bg_label + 1) % 2\n            detections = detections[det_label]\n            if len(detections) == 0:\n                return image\n\n            for i in xrange(detections.loc.shape[0]):\n                if detections.scores[i] < min_det_score:\n                    continue\n\n                ymin = int(detections.loc[i, 0] * image_height)\n                xmin = int(detections.loc[i, 1] * image_width)\n                ymax = int(detections.loc[i, 2] * image_height)\n                xmax = int(detections.loc[i, 3] * image_width)\n\n                action_score = detections.action_scores[i]\n                action_color = self._params.action_colors_map[detections.action_labels[i]] \\\n                    if action_score > min_action_score else self._params.undefined_action_color\n                action_name = self._params.action_names_map[detections.action_labels[i]] \\\n                    if action_score > min_action_score else self._params.undefined_action_name\n\n                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), action_color, 1)\n\n                caption = \'{}: {:.1f} {:.1f}\'.format(action_name, 1e2 * action_score, 1e2 * detections.scores[i])\n                cv2.putText(image, caption, (xmin, ymin - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.4, action_color, 1)\n\n                head_x = int(0.5 * (xmin + xmax))\n                head_y = int(0.85 * ymin + 0.15 * ymax)\n                cv2.circle(image, (head_x, head_y), 4, (0, 0, 255), -1)\n\n            return image\n\n        vidcap = cv2.VideoCapture(data_path)\n        video_length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        pbar = tqdm(total=video_length, desc=\'Read video\')\n        success = True\n        while success:\n            success, frame = vidcap.read()\n            pbar.update(1)\n\n            if success:\n                float_image = cv2.resize(\n                    frame, (self._params.image_size.w, self._params.image_size.h)).astype(np.float32)\n                pr_loc, pr_det_conf, pr_action_conf = sess.run(inference_ops, feed_dict={input_image: float_image})\n\n                batch_detections = action_detection_output(pr_loc, pr_det_conf, pr_action_conf, self._params.bg_class,\n                                                           min_det_conf=self._params.det_conf,\n                                                           min_action_conf=self._params.action_conf)\n\n                if out_scale != 1.0:\n                    out_height = int(frame.shape[0] * out_scale)\n                    out_width = int(frame.shape[1] * out_scale)\n                    frame = cv2.resize(frame, (out_width, out_height))\n\n                annotated_frame = _draw_actions(frame, batch_detections[0], self._params.bg_class,\n                                                min_det_score=self._params.det_conf,\n                                                min_action_score=self._params.action_conf)\n\n                cv2.imshow(\'Demo\', annotated_frame)\n\n                key = cv2.waitKey(1)\n                if key == 27:\n                    break\n                elif key == ord(\'p\'):\n                    while True:\n                        new_key = cv2.waitKey(1)\n                        if new_key == ord(\'p\') or new_key == 27:\n                            break\n                        else:\n                            time.sleep(0.1)\n\n        pbar.close()\n        vidcap.release()\n        cv2.destroyAllWindows()\n'"
tensorflow_toolkit/action_detection/action_detection/nn/monitors/base_monitor.py,48,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nimport time\nfrom abc import ABCMeta, abstractmethod\nfrom datetime import datetime\nfrom os.path import exists, join, basename\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.tools import freeze_graph  # pylint: disable=no-name-in-module\n\nBASE_FILE_NAME = \'converted_model\'\nPB_FILE_NAME = \'{}.pbtxt\'.format(BASE_FILE_NAME)\n\n\nclass BaseMonitor(object):\n    """"""Base class for network monitors, which allows to control the network behaviour.\n    """"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, params, batch_size, num_gpu, log_dir, src_scope, snapshot_path, init_model_path):\n        """"""Constructor.\n\n        :param params: Dictionary with model parameters\n        :param batch_size: Size of batch\n        :param num_gpu: Number of GPU devices\n        :param log_dir: Path to directory for logging\n        :param src_scope: Name of source network scope to load variables from\n        :param snapshot_path: Path to model snapshot\n        :param init_model_path: Path to model weights to initialize from\n        """"""\n\n        self._params = params\n        self._batch_size = batch_size\n        self._num_gpu = num_gpu\n        self._log_dir = log_dir\n        self._src_scope = src_scope\n        self._snapshot_path = snapshot_path\n        self._init_model_path = init_model_path\n\n        self._log_iter = 100\n        self._save_iter = 1000\n        self._max_num_saves = 1000\n        self._train_prefetch_size = 1\n        self._train_data_process_num_threads = 5\n        self._test_prefetch_size = 1\n        self._test_data_process_num_threads = 5\n        self._max_grad_norm = 1.0\n\n    @abstractmethod\n    def _create_model(self, network_input, global_step=None, is_training=None,\n                      merge_train_ops=False, merging_transit=False):\n        """"""Abstract method for model creating.\n\n        :param network_input: Network input tensors\n        :param global_step: Training step variable\n        :param is_training: Training indicator\n        :param merge_train_ops: Whether to run with merged train Ops\n        :param merging_transit: Whether to run in train Ops merging mode\n        :return: Created model\n        """"""\n\n        pass\n\n    @abstractmethod\n    def _create_train_dataset(self, data_path):\n        """"""Abstract method for training dataset creation.\n\n        :param data_path: Path to data\n        :return: Created dataset\n        """"""\n\n        pass\n\n    @abstractmethod\n    def _create_test_dataset(self, data_path):\n        """"""Abstract method for evaluation dataset creation.\n\n        :param data_path:\n        :return: Created dataset\n        """"""\n\n        pass\n\n    def _transfer_train_parameters(self, sess, model):\n        """"""Loads available model params from the specified snapshot.\n\n        :param sess: Session to load in\n        :param model: Model to load variables\n        :return:\n        """"""\n\n        model.load_available_variables(self._init_model_path, sess, self._src_scope,\n                                       \'{}/{}\'.format(self._params.name, self._params.backbone))\n\n    @abstractmethod\n    def _get_out_node_names(self):\n        """"""Returns network output node names.\n\n        :return: List of names\n        """"""\n\n        pass\n\n    @abstractmethod\n    def _get_test_ops(self, model, network_inputs):\n        """"""Returns list of operations to carry out model evaluation.\n\n        :param model: Model for evaluation\n        :param network_inputs: Network input tensors\n        :return: List of operations\n        """"""\n\n        pass\n\n    @abstractmethod\n    def _test(self, sess, network_inputs, test_ops):\n        """"""Carry out internal testing step.\n\n        :param sess: Session\n        :param network_inputs: Network input tensors\n        :param test_ops: List of testing operations\n        """"""\n\n        pass\n\n    @abstractmethod\n    def _demo(self, sess, data_path, input_image, inference_ops, out_scale):\n        """"""Carry out internal demonstration step.\n\n        :param sess: Session\n        :param data_path: Path to data to demonstrate on\n        :param input_image: Input network tensor for images\n        :param inference_ops: List of operations to carry out inference\n        :param out_scale: Parameter to scale final image\n        """"""\n\n        pass\n\n    def _get_extra_param_names(self):\n        """"""Returns list of model-specific parameters names for restoring\n\n        :return: List of names\n        """"""\n\n        return []\n\n    @staticmethod\n    def _log(message):\n        """"""Prints logging message.\n\n        :param message: Message to print\n        """"""\n\n        print(message)\n\n    @staticmethod\n    def _start_session(log_device_placement=False, allow_growth=True, allow_soft_placement=True):\n        """"""Creates new session and initializes variables.\n\n        :param log_device_placement: Whether to log device placement for variables\n        :param allow_growth: Whether to allow growth of GPU memory usage\n        :param allow_soft_placement: Whether to allow soft placement of data on GPU\n        :return: New session\n        """"""\n\n        config = tf.ConfigProto()\n        config.log_device_placement = log_device_placement\n        config.gpu_options.allow_growth = allow_growth  # pylint: disable=no-member\n        config.allow_soft_placement = allow_soft_placement\n\n        sess = tf.Session(config=config)\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n\n        return sess\n\n    @staticmethod\n    def _close_session(sess):\n        """"""Close specified session.\n\n        :param sess: Session to close\n        """"""\n\n        sess.close()\n\n    @staticmethod\n    def _collect_summary():\n        """"""Creates Op to collect summary operations.\n\n        :return: Summary Op\n        """"""\n\n        train_summaries = \\\n            tf.get_collection(\'weights_summary\') + \\\n            tf.get_collection(\'activation_summary\') + \\\n            tf.get_collection(\'bn_summary\') + \\\n            tf.get_collection(\'loss_summary\') + \\\n            tf.get_collection(\'lr_summary\') + \\\n            tf.get_collection(\'grad_summary\') + \\\n            tf.get_collection(\'accuracy_summary\')\n        return tf.summary.merge(train_summaries)\n\n    def _restore_train_parameters(self, sess, model):\n        """"""Restores model parameters from the specified input for training.\n\n        :param sess: Session to load in\n        :param model: Model for parameters loading\n        """"""\n\n        self._log(\'\\nParameters restoring...\')\n        if exists(self._snapshot_path + \'.index\'):\n            loader = tf.train.Saver()\n            loader.restore(sess, self._snapshot_path)\n            self._log(\'Full model restored from: {}\'.format(self._snapshot_path))\n        elif exists(self._init_model_path + \'.index\') and self._src_scope != \'\':\n            self._transfer_train_parameters(sess, model)\n        else:\n            self._log(\'No source to load train parameters\')\n\n    def _restore_inference_parameters(self, sess, model, ckpt_path=\'\'):\n        """"""Restores model parameters from the specified input for inference.\n\n        :param sess: Session to load in\n        :param model: Model for parameters loading\n        :param ckpt_path: Extra parameter with snapshot path (can be empty)\n        """"""\n\n        model_path = ckpt_path if ckpt_path is not None and ckpt_path != \'\' else self._snapshot_path\n\n        self._log(\'\\nParameters restoring...\')\n        if exists(model_path + \'.index\'):\n            extra_param_ends = self._get_extra_param_names()\n\n            model.load_available_variables(model_path, sess, self._params.name, self._params.name,\n                                           extra_ends=extra_param_ends)\n        else:\n            self._log(\'No source to load test parameters\')\n\n    def _multi_gpu_training(self, global_step, is_training_var, data_iterator):\n        """"""Creates ops for multi-GPU training.\n\n        :param global_step: Iteration variable\n        :param is_training_var: Indicator variable of training mode\n        :param data_iterator: Network input data iterator\n        :return: Tuple of train and loss Ops and created models\n        """"""\n\n        def _average_gradients(tower_grads, max_grad_norm=None):\n            """"""Creates Op to merge list of gradients according its name.\n\n            :param tower_grads: List of gradients by tower\n            :param max_grad_norm: Max gradient norm to clip\n            :return: List of averaged gradients\n            """"""\n\n            averaged_grads = []\n            for grad_and_vars in zip(*tower_grads):\n                grads = []\n                for grad_value, _ in grad_and_vars:\n                    expanded_g = tf.expand_dims(grad_value, 0)\n\n                    grads.append(expanded_g)\n\n                grad = tf.concat(axis=0, values=grads)\n                grad = tf.reduce_mean(grad, 0)\n\n                if max_grad_norm is not None:\n                    grad = tf.clip_by_norm(grad, max_grad_norm)\n\n                var_name = grad_and_vars[0][1]\n\n                grad_and_var = (grad, var_name)\n                averaged_grads.append(grad_and_var)\n\n            return averaged_grads\n\n        optimizer = None\n        towers = []\n        tower_gradients = []\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in xrange(self._num_gpu):\n                with tf.device(\'/gpu:{}\'.format(i)):\n                    with tf.name_scope(\'tower_{}\'.format(i)):\n                        tower_inputs = data_iterator.get_next()\n\n                        tower = self._create_model(tower_inputs, global_step, is_training_var)\n                        towers.append(tower)\n\n                        tf.get_variable_scope().reuse_variables()\n\n                        if optimizer is None:\n                            optimizer = tower.create_optimizer()\n\n                        tower_gradients.append(optimizer.compute_gradients(tower.total_loss))\n\n        gradients = _average_gradients(tower_gradients, max_grad_norm=self._max_grad_norm)\n        optimizer_op = optimizer.apply_gradients(gradients, global_step=global_step)\n\n        extra_ops = tf.get_collection(\'train_extra_ops\')\n        with tf.control_dependencies(extra_ops):\n            with tf.control_dependencies([optimizer_op]):\n                train_op = tf.no_op(name=\'train_op\')\n\n        loss_op = tf.add_n([t.total_loss for t in towers]) / float(self._num_gpu)\n\n        return train_op, loss_op, towers\n\n    def _init_data(self, sess, data_init_op):\n        """"""Initializes network input data.\n\n        :param sess: Session to load\n        :param data_init_op: Initialization Op\n        """"""\n\n        self._log(\'\\nData loading...\')\n        sess.run(data_init_op)\n        self._log(\'Finished.\')\n\n    @staticmethod\n    def _get_iter_id(snapshot_path):\n        """"""Parse snapshot name to extract iteration number.\n\n        :param snapshot_path: Snapshot name\n        :return: Iteration number\n        """"""\n\n        return int(basename(snapshot_path).split(\'-\')[-1])\n\n    def _optimize(self, sess, global_step, train_op, loss_op, summary_op, is_training):\n        """"""Internal method to carry out model training procedure.\n\n        :param sess: Session to run in.\n        :param global_step: Step variable\n        :param train_op: Operation to carry out training step\n        :param loss_op: Operation to calculate loss scalar value\n        :param summary_op: Operation to dump summary Ops\n        :param is_training: Indicator of training mode\n        """"""\n\n        assert self._log_dir is not None and self._log_dir != \'\'\n\n        summary_writer = tf.summary.FileWriter(self._log_dir, sess.graph)\n        model_saver = tf.train.Saver(max_to_keep=self._max_num_saves)\n\n        logfile_name = \'{}_train_log.txt\'.format(datetime.now()).replace(\' \', \'-\').replace(\':\', \'-\')\n        log_stream = open(join(self._log_dir, logfile_name), \'w\')\n\n        start_time = time.time()\n        num_steps = 0\n\n        self._log(\'\\nOptimization...\')\n        init_train_step = sess.run(global_step)\n        for train_step in xrange(init_train_step, self._params.max_train_steps):\n            if train_step % self._log_iter == 0:\n                _, total_loss, train_summary = sess.run([train_op, loss_op, summary_op], feed_dict={is_training: True})\n                num_steps += 1\n\n                end_time = time.time()\n                mean_iter_time = float(end_time - start_time) / float(num_steps)\n                num_steps = 0\n\n                log_str = \\\n                    \'{}: {}: Train loss: {:.3f} Time: {:.3f} s/iter\' \\\n                        .format(datetime.now(), train_step, total_loss, mean_iter_time)\n                self._log(log_str)\n                log_stream.write(log_str + \'\\n\')\n                log_stream.flush()\n\n                assert not np.isnan(total_loss), \'Model diverged with loss = NaN, step= {}\'.format(train_step)\n\n                summary_writer.add_summary(train_summary, train_step)\n\n                start_time = time.time()\n            else:\n                _ = sess.run(train_op, feed_dict={is_training: True})\n                num_steps += 1\n\n            if train_step > 0 and train_step % self._save_iter == 0:\n                checkpoint_path = join(self._log_dir, \'model.ckpt\')\n                model_saver.save(sess, checkpoint_path, global_step=train_step)\n\n        log_stream.close()\n        self._log(\'Finished.\')\n\n    def train(self, data_path):\n        """"""Carry out model training procedure.\n\n        :param data_path: Path to load training data\n        """"""\n\n        with tf.Graph().as_default():\n            dataset = self._create_train_dataset(data_path)\n\n            data_iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n            data_init_op = data_iterator.make_initializer(dataset)\n\n            is_training = tf.placeholder(dtype=tf.bool, shape=(), name=\'is_training\')\n            global_step = tf.Variable(0, trainable=False, name=\'GlobalStep\')\n\n            train_op, loss_op, towers = self._multi_gpu_training(global_step, is_training, data_iterator)\n            tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'total_loss\', loss_op))\n\n            summary_op = self._collect_summary()\n\n            sess = self._start_session()\n            self._restore_train_parameters(sess, towers[0])\n            self._init_data(sess, data_init_op)\n            self._optimize(sess, global_step, train_op, loss_op, summary_op, is_training)\n            self._close_session(sess)\n\n    def test(self, data_path):\n        """"""Carry out model testing procedure.\n\n        :param data_path: Path to load training data\n        """"""\n\n        with tf.Graph().as_default():\n            dataset = self._create_test_dataset(data_path)\n\n            data_iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n            data_init_op = data_iterator.make_initializer(dataset)\n\n            network_inputs = data_iterator.get_next()\n            model = self._create_model(network_inputs)\n            test_ops = self._get_test_ops(model, network_inputs)\n\n            sess = self._start_session()\n            self._restore_inference_parameters(sess, model)\n            self._init_data(sess, data_init_op)\n            self._test(sess, network_inputs, test_ops)\n            self._close_session(sess)\n\n    def eliminate_train_ops(self, output_path):\n        """"""Carry out eliminating of all training nodes from the network.\n\n        :param output_path: Path to save variables\n        """"""\n\n        with tf.Graph().as_default():\n            input_shape = [1, self._params.image_size.h, self._params.image_size.w, self._params.image_size.c]\n            input_image = tf.placeholder(tf.float32, input_shape, name=\'input\')\n\n            model = self._create_model([input_image], None, None, True, True)\n            merge_op = model.get_merge_train_op()\n\n            sess = self._start_session()\n            self._restore_inference_parameters(sess, model)\n\n            sess.run(merge_op, feed_dict={input_image: np.zeros(input_shape, dtype=np.float32)})\n\n            saver = tf.train.Saver(max_to_keep=1)\n            saver.save(sess, output_path, global_step=self._get_iter_id(self._snapshot_path))\n            self._log(\'Model converted successfully.\')\n\n            self._close_session(sess)\n\n    def save_model_graph(self, input_snapshot_path, output_directory_path):\n        """"""Stores model graph to file.\n\n        :param input_snapshot_path: Path to load variables\n        :param output_directory_path: Path to save variables\n        """"""\n\n        with tf.Graph().as_default():\n            input_shape = [1, self._params.image_size.h, self._params.image_size.w, self._params.image_size.c]\n            input_image = tf.placeholder(tf.float32, input_shape, name=\'input\')\n\n            model = self._create_model([input_image], None, None, True, False)\n\n            sess = self._start_session()\n            self._restore_inference_parameters(sess, model, input_snapshot_path)\n\n            tf.train.write_graph(sess.graph.as_graph_def(), output_directory_path, PB_FILE_NAME, as_text=True)\n            self._log(\'Network graph is stored successfully.\')\n\n            self._close_session(sess)\n\n    def freeze_model_graph(self, checkpoint_path, network_path, out_path):\n        """"""Prepares model for inference and freezes it.\n\n        :param checkpoint_path: Path to model checkpoint\n        :param network_path: Path to network\n        :param out_path: path to save out model\n        """"""\n\n        out_node_names = \',\'.join(self._get_out_node_names())\n        freeze_graph.freeze_graph(network_path, \'\', False, checkpoint_path, out_node_names,\n                                  \'save/restore_all\', \'save/Const:0\', out_path, True, \'\')\n\n    def demo(self, data_path, out_scale=1.0, deploy=False):\n        """"""Outputs processed bt network image.\n\n        :param data_path: Path to data\n        :param out_scale: Parameter to scale output image\n        :param deploy: Whether to run network in deploy mode\n        """"""\n\n        with tf.Graph().as_default():\n            image_shape = [self._params.image_size.h, self._params.image_size.w, self._params.image_size.c]\n            input_image = tf.placeholder(tf.float32, image_shape)\n            network_inputs = [tf.expand_dims(input_image, axis=0)]\n\n            model = self._create_model(network_inputs, None, None, deploy, False)\n            inference_ops = self._get_test_ops(model, network_inputs)\n\n            sess = self._start_session()\n            self._restore_inference_parameters(sess, model)\n            self._demo(sess, data_path, input_image, inference_ops, out_scale)\n            self._close_session(sess)\n\n    def performance(self):\n        """"""Measures model properties like number of parameters and operations.\n        """"""\n\n        graph = tf.Graph()\n        with graph.as_default():\n            input_shape = [1, self._params.image_size.h, self._params.image_size.w, self._params.image_size.c]\n            input_image = tf.placeholder(tf.float32, input_shape, name=\'input\')\n\n            _ = self._create_model([input_image], None, None, True, False)\n\n            flops = \\\n                tf.profiler.profile(graph, options=tf.profiler.ProfileOptionBuilder.float_operation())\n            num_params = \\\n                tf.profiler.profile(graph, options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())\n\n            self._log(\'\\nTotal stat:\')\n            if num_params is not None:\n                self._log(\'MParams: {}\'.format(1e-6 * float(num_params.total_parameters)))  # pylint: disable=no-member\n            if flops is not None:\n                self._log(\'GFlop: {}\'.format(1e-9 * flops.total_float_ops))  # pylint: disable=no-member\n'"
tensorflow_toolkit/action_detection/action_detection/nn/monitors/classifier_monitor.py,1,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom datetime import datetime\nfrom functools import partial\n\nimport tensorflow as tf\nfrom tqdm import trange\n\nfrom action_detection.nn.data.dataset import get_classification_dataset\nfrom action_detection.nn.data.preprocessing import ImageNetProcessFn\nfrom action_detection.nn.models import ImageClassifier\nfrom action_detection.nn.monitors.base_monitor import BaseMonitor\n\n\nclass ClassifierMonitor(BaseMonitor):\n    """"""Class to control the behaviour of classification network.\n    """"""\n\n    def _create_model(self, network_input, global_step=None, is_training=None,\n                      merge_train_ops=False, merging_transit=False):\n        if len(network_input) == 1:\n            images, labels = network_input[0], None\n        else:\n            images, labels = network_input\n\n        fn_activation = partial(tf.nn.leaky_relu, alpha=0.1)\n\n        return ImageClassifier(self._params.backbone, images, labels, fn_activation, is_training,\n                               self._params.num_classes, merge_train_ops, merging_transit, self._params.lr_params,\n                               global_step=global_step, keep_probe=self._params.keep_probe, name=self._params.name,\n                               use_nesterov=self._params.use_nesterov, norm_kernels=self._params.norm_kernels)\n\n    def _create_train_dataset(self, data_path):\n        return get_classification_dataset(data_path, self._params.num_classes, self._params.image_size,\n                                          self._batch_size, True, \'train_data\',\n                                          self._train_prefetch_size, self._train_data_process_num_threads,\n                                          process_fn=self._params.image_augmentation)\n\n    def _create_test_dataset(self, data_path):\n        image_processing = ImageNetProcessFn(self._params.val_central_fraction, self._params.image_size)\n\n        return get_classification_dataset(data_path, self._params.num_classes, self._params.image_size,\n                                          self._batch_size, False, \'val_data\',\n                                          self._test_prefetch_size, self._test_data_process_num_threads,\n                                          process_fn=image_processing)\n\n    def _get_test_ops(self, model, network_inputs):\n        return [model.num_valid_op(network_inputs[1])]\n\n    def _test(self, sess, network_inputs, test_ops):\n        total_num_valid = 0\n        for _ in trange(self._params.val_steps, desc=\'Validation\'):\n            num_valid = sess.run(test_ops)\n            total_num_valid += num_valid\n\n        val_accuracy = float(total_num_valid) / float(self._params.val_steps * self._batch_size)\n        self._log(\'{}: Validation accuracy: {:.5f}%\'.format(datetime.now(), 1e2 * val_accuracy))\n\n    def _get_out_node_names(self):\n        name_templates = [\'{}/classifier_layer/output\']\n        return [t.format(self._params.name) for t in name_templates]\n\n    def _demo(self, sess, data_path, input_image, inference_ops, out_scale):\n        raise NotImplementedError()\n'"
tensorflow_toolkit/action_detection/action_detection/nn/monitors/detector_monitor.py,1,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom functools import partial\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import trange\n\nfrom action_detection.nn.data.dataset import get_detection_dataset\nfrom action_detection.nn.models import SSD\nfrom action_detection.nn.monitors.base_monitor import BaseMonitor\nfrom action_detection.postprocessing.detection_output import ssd_detection_output, ssd_warp_gt\nfrom action_detection.postprocessing.quality import calc_map_mr\n\n\nclass DetectorMonitor(BaseMonitor):\n    """"""Class to control the behaviour of detection network.\n    """"""\n\n    def _create_model(self, network_input, global_step=None, is_training=None,\n                      merge_train_ops=False, merging_transit=False):\n        if len(network_input) == 1:\n            images, labels, annotation = network_input[0], None, None\n        else:\n            images, labels, annotation = network_input\n\n        fn_activation = partial(tf.nn.leaky_relu, alpha=0.1)\n\n        return SSD(self._params.backbone, images, labels, annotation, fn_activation, is_training,\n                   self._params.head_params, merge_train_ops, merging_transit,\n                   self._params.lr_params, self._params.mbox_params,\n                   global_step=global_step, name=self._params.name,\n                   use_nesterov=self._params.use_nesterov, norm_kernels=self._params.norm_kernels)\n\n    def _create_train_dataset(self, data_path):\n        return get_detection_dataset(data_path, self._params.image_size, self._batch_size, True, \'train_data\',\n                                     self._train_prefetch_size, self._train_data_process_num_threads,\n                                     tuple_process_fn=self._params.tuple_augmentation,\n                                     image_process_fn=self._params.image_augmentation,\n                                     max_num_objects_per_image=self._params.max_num_objects_per_image,\n                                     labels_map=self._params.labels_map)\n\n    def _create_test_dataset(self, data_path):\n        return get_detection_dataset(data_path, self._params.image_size, self._batch_size, False, \'val_data\',\n                                     self._test_prefetch_size, self._test_data_process_num_threads,\n                                     max_num_objects_per_image=self._params.max_num_objects_per_image,\n                                     use_difficult=False,\n                                     labels_map=self._params.labels_map)\n\n    def _get_test_ops(self, model, network_inputs):\n        return model.predictions\n\n    def _test(self, sess, network_inputs, test_ops):\n        self._log(\'\\nValidation...\')\n        all_detections = []\n        all_gt = []\n        for _ in trange(self._params.val_steps, desc=\'Dumping predictions\'):\n            gt_labels, gt_annot, pr_loc, pr_conf = sess.run(network_inputs[1:] + test_ops)\n\n            batch_detections = ssd_detection_output(pr_loc, pr_conf, self._params.bg_class, min_conf=0.1)\n            batch_gt = ssd_warp_gt(gt_annot, gt_labels, self._params.bg_class)\n\n            all_detections.extend(batch_detections)\n            all_gt.extend(batch_gt)\n\n        metrics_by_class = calc_map_mr(all_detections, all_gt, return_all=True)\n\n        ap_values = []\n\n        self._log(\'\\nMetrics by class:\')\n        for class_id in xrange(self._params.num_classes):\n            if class_id == self._params.bg_class:\n                continue\n\n            if class_id in metrics_by_class:\n                ap_value, mr_value = metrics_by_class[class_id]\n            else:\n                ap_value, mr_value = 0.0, 1.0\n\n            ap_values.append(ap_value)\n\n            self._log(\'   {:>3}: AP: {:.3f}%   mr@0.1: {:.3f}%\'.format(class_id, 1e2 * ap_value, 1e2 * mr_value))\n\n        map_metric = np.mean(ap_values) if len(ap_values) > 0 else 0.0\n        self._log(\'Total. mAP: {:.3f}%\'.format(1e2 * map_metric))\n\n    def _get_out_node_names(self):\n        name_templates = [\'{}/out_detection_loc\', \'{}/out_detection_logits\', \'{}/out_detection_priors\']\n        return [t.format(self._params.name) for t in name_templates]\n\n    def _demo(self, sess, data_path, input_image, inference_ops, out_scale):\n        raise NotImplementedError()\n'"
tensorflow_toolkit/action_detection/action_detection/nn/monitors/factory.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom os.path import exists\n\nfrom action_detection.nn.monitors.classifier_monitor import ClassifierMonitor\nfrom action_detection.nn.monitors.detector_monitor import DetectorMonitor\nfrom action_detection.nn.monitors.action_monitor import ActionMonitor\nfrom action_detection.nn.parameters.classifier_parameters import ClassifierParams\nfrom action_detection.nn.parameters.detector_parameters import DetectorParams\nfrom action_detection.nn.parameters.action_parameters import ActionParams\nfrom action_detection.nn.parameters.common import load_config\n\n\ndef get_monitor(config_path, batch_size=1, num_gpu=1, log_dir=\'\', src_scope=\'\', snapshot_path=\'\', init_model_path=\'\'):\n    """"""Configures task-specific monitor to control the network.\n\n    :param config_path: Path to configuration file\n    :param batch_size: Size of batch\n    :param num_gpu: Number of GPUs\n    :param log_dir: Directory for lor logging\n    :param src_scope: Source network scope name to load from\n    :param snapshot_path: Path to snapshot\n    :param init_model_path: Path to model weights to initialize from\n    :return: Configured network monitor\n    """"""\n\n    assert exists(config_path)\n    assert batch_size > 0\n    assert num_gpu > 0\n\n    config_values = load_config(config_path)\n\n    if config_values.NETWORK_TYPE == \'classification\':\n        model_params = ClassifierParams(config_values, batch_size, num_gpu)\n        out_monitor = ClassifierMonitor(model_params, batch_size, num_gpu, log_dir,\n                                        src_scope, snapshot_path, init_model_path)\n    elif config_values.NETWORK_TYPE == \'detection\':\n        model_params = DetectorParams(config_values, batch_size, num_gpu)\n        out_monitor = DetectorMonitor(model_params, batch_size, num_gpu, log_dir,\n                                      src_scope, snapshot_path, init_model_path)\n    elif config_values.NETWORK_TYPE == \'action\':\n        model_params = ActionParams(config_values, batch_size, num_gpu)\n        out_monitor = ActionMonitor(model_params, batch_size, num_gpu, log_dir,\n                                    src_scope, snapshot_path, init_model_path)\n    else:\n        raise Exception(\'Invalid network type: {}\'.format(config_values.NETWORK_TYPE))\n\n    return out_monitor\n'"
tensorflow_toolkit/action_detection/action_detection/nn/nodes/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/action_detection/nn/nodes/initializers.py,2,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef orthogonal_initializer(mode=None, dtype=tf.float32, out_scale=1.0):\n    """"""Generates orthogonal initializer for convolution and dense layers.\n\n    :param mode: Type of operation: dense or conv\n    :param dtype: Type of data\n    :param out_scale: Scalar value tu multiply on\n    :return: Initializer function\n    """"""\n\n    def _generate_ort_vectors(num_in, num_out):\n        """"""Generates random vectors of the specified sizes.\n\n        :param num_in: Input size\n        :param num_out: Output size\n        :return: Orthogonal vectors\n        """"""\n\n        flat_shape = (num_in, num_out)\n\n        init_stddev = np.sqrt(1.0 / float(num_in)) / .87962566103423978\n        flatten_array = np.random.normal(0., init_stddev, flat_shape)\n\n        if num_out > num_in:\n            out_weights = flatten_array\n        else:\n            ort_matrix, _, _ = np.linalg.svd(flatten_array, full_matrices=False)\n            assert ort_matrix.shape == flat_shape\n\n            diff = np.matmul(ort_matrix.T, ort_matrix) - np.eye(ort_matrix.shape[1], ort_matrix.shape[1])\n            np.testing.assert_array_almost_equal(diff, np.zeros_like(diff))\n\n            out_weights = ort_matrix\n            out_weights *= out_scale * init_stddev / np.std(out_weights)\n\n        return out_weights\n\n    def _initializer(shape, dtype=dtype, partition_info=None):\n        """"""Returns orthogonal weights of the specified shape.\n\n        :param shape: Target shape of weights\n        :param dtype: Type of data\n        :param partition_info: Unused interface parameter\n        :return: Initialized weights\n        """"""\n\n        target_mode = mode\n        if target_mode is None:\n            if len(shape) == 2:\n                target_mode = \'dense\'\n            elif len(shape) == 4:\n                target_mode = \'conv\'\n            else:\n                raise Exception(\'Unsupported shape: {}\'.format(shape))\n        else:\n            assert target_mode in [\'dense\', \'conv\']\n\n        if target_mode == \'dense\':\n            num_in = shape[0]\n            num_out = shape[1]\n        else:\n            if shape[3] == 1:\n                num_in = shape[0] * shape[1]\n                num_out = shape[2]\n            else:\n                num_in = shape[0] * shape[1] * shape[2]\n                num_out = shape[3]\n\n        values = _generate_ort_vectors(num_in, num_out)\n        weights = tf.constant(values.reshape(shape), dtype=dtype)\n\n        return weights\n\n    return _initializer\n'"
tensorflow_toolkit/action_detection/action_detection/nn/nodes/losses.py,178,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom action_detection.nn.nodes.variables import create_variable\nfrom action_detection.nn.nodes.ops import safe_masked_reduce_op, sample_from_extreme_points, extract_gt_values,\\\n    gsoftmax, assign_moving_frequency\n\n\ndef weight_decay(variables, weight, name, key_word=\'weights\'):\n    """"""Creates Op to carry out weight decay.\n\n    :param variables: List of variables\n    :param weight: Weights of loss\n    :param name: Name of loss\n    :param key_word: Name to filter variables\n    :return: Loss value\n    """"""\n\n    with tf.name_scope(name):\n        vars_to_decay = [var for var in variables if key_word in var.name]\n        if len(vars_to_decay) > 0:\n            weight_decays = [tf.reduce_mean(tf.square(var)) for var in vars_to_decay]\n            normalizer = weight / float(len(vars_to_decay))\n            out_loss = tf.multiply(tf.add_n(weight_decays), normalizer, name=\'weight_decay\')\n            return out_loss\n        else:\n            return 0.0\n\n\ndef unit_gamma(variables, weight, name, key_word=\'gamma\'):\n    """"""Creates Op to preserve gamma parameter for Batch Norm to unit value.\n\n    :param variables: List of variables\n    :param weight: Weights of loss\n    :param name: Name of loss\n    :param key_word: Name to filter variables\n    :return: Loss value\n    """"""\n\n    with tf.name_scope(name):\n        vars_to_decay = [var for var in variables if key_word in var.name]\n        if len(vars_to_decay) > 0:\n            weight_decays = [tf.reduce_mean(tf.square(1.0 - var)) for var in vars_to_decay]\n            normalizer = weight / float(len(vars_to_decay))\n            out_loss = tf.multiply(tf.add_n(weight_decays), normalizer, name=\'unit_gamma\')\n            return out_loss\n        else:\n            return 0.0\n\n\ndef orthogonal_conv(variables, weight, name, key_word=\'weights\', key_scope=\'\'):\n    """"""Creates Op to fit kernel parameters onto orthogonal matrix.\n\n    :param variables: List of variables\n    :param weight: Weight of loss\n    :param name: Name of loss\n    :param key_word: Name to filter variables\n    :param key_scope: Name of scope to filter variables\n    :return: Loss value\n    """"""\n\n    with tf.name_scope(name):\n        conv_vars = [var for var in variables if key_word in var.name]\n        if key_scope is not None and key_scope != \'\':\n            conv_vars = [var for var in conv_vars if key_scope in var.name]\n\n        ort_losses = []\n        for var in conv_vars:\n            var_shape = var.get_shape()\n            if len(var_shape) != 4:\n                continue\n\n            kernel_h = int(var_shape[0])\n            kernel_w = int(var_shape[1])\n\n            input_size = kernel_h * kernel_w * int(var_shape[2])\n            output_size = int(var_shape[3])\n\n            model_weights = tf.reshape(var, [input_size, output_size])\n            coefficients = tf.matmul(tf.transpose(model_weights, perm=[1, 0]), model_weights)\n\n            ort_loss = tf.reduce_sum(tf.square(tf.matrix_set_diag(\n                coefficients, tf.zeros([output_size], dtype=tf.float32))))\n            ort_losses.append(ort_loss)\n\n        if len(ort_losses) > 0:\n            normalizer = weight / float(len(ort_losses))\n            out_loss = tf.multiply(tf.add_n(ort_losses), normalizer, name=\'orthogonal_conv\')\n            return out_loss\n        else:\n            return 0.0\n\n\ndef decorrelate_features(input_value, weight, name):\n    """"""Create Op to orthogonalize input features\n\n    :param input_value: Input features\n    :param weight: Weight of loss\n    :param name: Name of block\n    :return: Loss value\n    """"""\n\n    with tf.name_scope(name):\n        tensor_shape = tf.shape(input_value)\n\n        batch_size = tensor_shape[0]\n        input_size = tensor_shape[1] * tensor_shape[2]\n        output_size = tensor_shape[3]\n\n        batched_data = tf.reshape(input_value, [-1, input_size, output_size])\n        coefficients = tf.matmul(tf.transpose(batched_data, [0, 2, 1]), batched_data)\n\n        loss_value = tf.reduce_mean(tf.square(tf.matrix_set_diag(\n            coefficients, tf.zeros([batch_size, output_size], dtype=tf.float32))))\n        out_loss = tf.multiply(loss_value, weight, name=\'decorrelate_features\')\n\n    return out_loss\n\n\ndef adaptive_weighting(values, name, scale=2.0, add_summary=True, exclude_invalid=True, margin=0.1, init_value=2.5):\n    """"""Carry out adaptive weighting (with learnable parameter) for the input values.\n\n    :param values: Input values\n    :param name: Name of block\n    :param scale: Scale of target values\n    :param add_summary: Whether to add summary info\n    :param exclude_invalid: Whether to exclude invalid or negative values from output\n    :param margin: Margin for the output\n    :param init_value: Init value of parameters\n    :return: Adaptive weighted sum of input values\n    """"""\n\n    num_variables = len(values)\n\n    with tf.variable_scope(name):\n        init_params = np.full([num_variables], init_value, dtype=np.float32)\n        params = create_variable(\'params/weights\', init_params.shape, init_params)\n\n        clipped_values = tf.maximum(tf.stack(values), 0.0)\n        weights = tf.exp(tf.negative(params))\n        weighted_values = weights * clipped_values\n\n        out_values = params + scale * weighted_values\n\n        if exclude_invalid:\n            out_value = safe_masked_reduce_op(out_values, tf.greater(values, 0.0), tf.reduce_sum)\n        else:\n            out_value = tf.reduce_sum(out_values, 0.0)\n        out_value = tf.maximum(margin + out_value, 0.0)\n\n        if add_summary:\n            for var_id in xrange(num_variables):\n                tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'values/var_{}\'\n                                                                       .format(var_id), clipped_values[var_id]))\n                tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'weights/var_{}\'\n                                                                       .format(var_id), weights[var_id]))\n\n    return out_value\n\n\ndef frequencies_weighting(counts, values, decay, name, add_summary=True, limits=None):\n    """"""Carry out weighting of input values according their frequencies.\n\n    :param counts: Counts of af each value\n    :param values: Input values\n    :param decay: Decay value\n    :param name: Name of block\n    :param add_summary: Whether to add summary info\n    :param limits: Limits to restrict weights in format: [min, max]\n    :return: Weighted sum of input values\n    """"""\n\n    assert len(counts) == len(values)\n\n    num_values = len(counts)\n    assert num_values > 0\n\n    if limits is not None:\n        assert len(limits) == 2\n        assert 0.0 <= limits[0] < limits[1]\n\n    with tf.variable_scope(name):\n        values = tf.stack(values)\n        counts = tf.stack(counts)\n\n        sum_counts = tf.reduce_sum(counts)\n        normalizer = tf.cond(tf.greater(sum_counts, 0.0), lambda: tf.reciprocal(sum_counts), lambda: 1.0)\n        frequencies = normalizer * counts\n\n        smoothed_frequencies = tf.get_variable(\'smoothed_frequencies\', [num_values], tf.float32,\n                                               tf.initializers.constant(1.0 / num_values), trainable=False)\n        update_op = assign_moving_frequency(smoothed_frequencies, frequencies, 1.0 - decay)\n        with tf.control_dependencies([update_op]):\n            out_frequencies = tf.identity(smoothed_frequencies)\n\n        norm_factor = tf.reduce_sum(out_frequencies) / float(num_values)\n        weights = tf.where(tf.greater(out_frequencies, 0.0),\n                           norm_factor / out_frequencies,\n                           tf.zeros_like(out_frequencies))\n\n        if limits is not None:\n            weights = tf.maximum(limits[0], tf.minimum(weights, limits[1]))\n\n        weighted_values = normalizer * tf.stop_gradient(weights) * values\n        out_loss = tf.reduce_sum(weighted_values)\n\n        if add_summary:\n            for class_id in xrange(num_values):\n                tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'weight/var_{}\'.format(class_id),\n                                                                       weights[class_id]))\n                tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'freq/var_{}\'.format(class_id),\n                                                                       smoothed_frequencies[class_id]))\n                tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'in_val/var_{}\'.format(class_id),\n                                                                       values[class_id]))\n                tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'out_val/var_{}\'.format(class_id),\n                                                                       weighted_values[class_id]))\n\n    return out_loss\n\n\ndef class_frequencies_weighting(losses, labels, mask, num_classes, name, add_summary=True, decay=0.9, limits=None):\n    """"""Wrapper for class weighting loss according frequencies of classes.\n\n    :param losses: List of all losses\n    :param labels: List of labels\n    :param mask: Mask of valid values\n    :param num_classes: Number of classes\n    :param name: Name of block\n    :param add_summary: Whether to add summary info\n    :param decay: Decay scalar value\n    :param limits: Limits to restrict weights in format: [min, max]\n    :return: Loss value\n    """"""\n\n    with tf.variable_scope(name):\n        class_sum_losses = []\n        class_counts = []\n        for class_id in xrange(num_classes):\n            class_mask = tf.logical_and(tf.equal(labels, class_id), mask)\n\n            class_sum_losses.append(safe_masked_reduce_op(losses, class_mask, tf.reduce_sum))\n            class_counts.append(tf.reduce_sum(tf.cast(class_mask, tf.float32)))\n\n        out_loss = frequencies_weighting(class_counts, class_sum_losses, decay, \'balancing\',\n                                         add_summary=add_summary, limits=limits)\n\n    return out_loss\n\n\ndef max_entropy_ce_loss(labels, logits, entropy_weight, name, add_summary=True, eps=1e-12,\n                        enable_gsoftmax=False, gsoftmax_weight=1.0):\n    """"""Calculates Cross-Entropy loss which is regularized by Max-Entropy term.\n\n    :param labels: List of labels\n    :param logits: List of class logits\n    :param entropy_weight: Weight of Entropy term\n    :param name: Name of block\n    :param add_summary: Whether to add summary info\n    :param eps: Epsilon scalar value\n    :param enable_gsoftmax: Whether to enable General Softmax calculation\n    :param gsoftmax_weight: Weight parameter for the General Softmax\n    :return: List of loss values\n    """"""\n\n    with tf.name_scope(name):\n        if enable_gsoftmax:\n            probs = gsoftmax(logits, axis=-1, weight=gsoftmax_weight)\n            ce_losses = tf.negative(tf.log(extract_gt_values(probs, labels, \'gt_probs\')))\n        else:\n            probs = tf.nn.softmax(logits, axis=-1)\n            ce_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name=\'ce_losses\')\n\n        neg_entropy = tf.reduce_sum(probs * tf.log(probs + eps), axis=-1)\n        out_losses = tf.maximum(ce_losses + entropy_weight * neg_entropy, 0.0)\n\n        if add_summary:\n            sparsity = tf.nn.zero_fraction(out_losses)\n            tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'ce_losses/sparsity\', sparsity))\n\n    return out_losses\n\n\ndef focal_ce_loss(labels, logits, alpha, gamma, name, enable_gsoftmax=False, gsoftmax_weight=1.0):\n    """"""Calculates Focal+Cross-Entropy loss. See: https://arxiv.org/abs/1708.02002\n\n    :param labels: List of labels\n    :param logits: List of class logits\n    :param alpha: Loss scale\n    :param gamma: Power parameter\n    :param name: Name of block\n    :param enable_gsoftmax: Whether to enable General Softmax calculation\n    :param gsoftmax_weight: Weight parameter for the General Softmax\n    :return: List of loss values\n    """"""\n\n    with tf.name_scope(name):\n        if enable_gsoftmax:\n            probs = gsoftmax(logits, axis=-1, weight=gsoftmax_weight)\n        else:\n            probs = tf.nn.softmax(logits, axis=-1)\n\n        gt_probs = extract_gt_values(probs, labels, \'gt_probs\')\n        out_losses = float(-alpha) * tf.pow(1.0 - gt_probs, float(gamma)) * tf.log(gt_probs)\n\n    return out_losses\n\n\ndef gradient_harmonized_ce_loss(labels, logits, name, num_bins=20, momentum=0.75, smooth_param=0.1,\n                                add_summary=True, enable_gsoftmax=False, gsoftmax_weight=1.0):\n    """"""Calculates Cross-Entropy loss which is normalized by inversing gradient histogram.\n       See: https://arxiv.org/abs/1811.05181\n\n    :param labels: List of labels\n    :param logits: List of class logits\n    :param name: Name of block\n    :param num_bins: Number of histogram bins\n    :param momentum: Momentum scalar value\n    :param smooth_param: Parameter to smooth distribution\n    :param add_summary: Whether to add summary info\n    :param enable_gsoftmax: Whether to enable General Softmax calculation\n    :param gsoftmax_weight: Weight parameter for the General Softmax\n    :return: List of loss values\n    """"""\n\n    assert num_bins > 1\n    assert smooth_param >= 0.0\n\n    all_edges = np.array([float(x) / float(num_bins) for x in range(num_bins + 1)], dtype=np.float32)\n    all_edges[-1] += 1e-6\n    range_starts = tf.constant(all_edges[:-1].reshape([1, -1]), dtype=tf.float32)\n    range_ends = tf.constant(all_edges[1:].reshape([1, -1]), dtype=tf.float32)\n\n    with tf.variable_scope(name):\n        if enable_gsoftmax:\n            probs = gsoftmax(logits, axis=-1, weight=gsoftmax_weight)\n        else:\n            probs = tf.nn.softmax(logits, axis=-1)\n\n        gt_probs = extract_gt_values(probs, labels, \'gt_probs\')\n\n        errors = tf.stop_gradient(tf.reshape(1.0 - gt_probs, [-1, 1]))\n        mask = tf.logical_and(tf.greater_equal(errors, range_starts), tf.less(errors, range_ends))\n        float_mask = tf.cast(mask, tf.float32)\n        range_ids = tf.argmax(float_mask, axis=1)\n\n        bins_sizes = tf.reduce_sum(float_mask, axis=0)\n        total_num = tf.reduce_sum(bins_sizes)\n        frequencies = bins_sizes / tf.maximum(1.0, total_num)\n\n        smoothed_frequencies = tf.get_variable(\'smoothed_frequencies\', [num_bins], tf.float32,\n                                               tf.initializers.constant(1.0 / num_bins), trainable=False)\n        update_op = assign_moving_frequency(smoothed_frequencies, frequencies, momentum)\n        with tf.control_dependencies([update_op]):\n            out_frequencies = tf.identity(smoothed_frequencies)\n\n        bin_weights = tf.where(tf.greater(out_frequencies, 0.0),\n                               1.0 / (out_frequencies + smooth_param),\n                               tf.zeros_like(out_frequencies))\n        out_weights = tf.gather(bin_weights, range_ids)\n\n        out_losses = out_weights * tf.negative(tf.log(gt_probs))\n\n        if add_summary:\n            tf.add_to_collection(\'loss_summary\', tf.summary.histogram(\'gh_errors\', errors))\n\n    return out_losses\n\n\ndef adaptive_scale(scale, labels, logits, num_classes, name=None, add_summary=True):\n    """"""Adaptive scale Op to use with Cross-Entropy loss. See: https://arxiv.org/abs/1905.00292\n\n    :param scale: Input scale variable to update\n    :param labels: List of labels\n    :param logits:  List of class logits\n    :param num_classes: Number of classes\n    :param name: Name of block\n    :param add_summary: Whether to add summary info\n    :return: Scale scalar value\n    """"""\n\n    with tf.name_scope(name, \'adaptive_scale\'):\n        gt_cos_values = extract_gt_values(logits, labels, \'gt_cos_values\')\n        gt_angles = tf.acos(gt_cos_values)\n        median_gt_angle = tf.reduce_mean(gt_angles)\n\n        exp_values = tf.exp(scale * logits)\n\n        valid_mask = tf.one_hot(labels, num_classes, True, False, dtype=tf.bool)\n        invalid_exp_values = tf.where(valid_mask, tf.zeros_like(exp_values), exp_values)\n\n        mean_exp_value = tf.reduce_sum(invalid_exp_values) / tf.cast(tf.maximum(1, tf.size(labels)), tf.float32)\n\n        new_scale = tf.stop_gradient(tf.log(mean_exp_value) / tf.cos(tf.minimum(np.pi / 4.0, median_gt_angle)))\n        update_op = tf.assign(scale, new_scale)\n        with tf.control_dependencies([update_op]):\n            out_scale = tf.identity(scale)\n\n        if add_summary:\n            tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'adaptive_scale/angle\', median_gt_angle))\n            tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'adaptive_scale/mean_exp\', mean_exp_value))\n\n    return out_scale\n\n\ndef weighted_ce_loss(labels, logits, num_classes, name, max_entropy_weight=0.4, add_summary=True,\n                     decay=0.9, limits=None, alpha=None, gamma=None, num_bins=None):\n    """"""Wrapper to calculate Cross-Entropy loss with some regularization term.\n\n    :param labels: List of labels\n    :param logits: List of class logits\n    :param num_classes: Number of classes\n    :param name: Name of block\n    :param max_entropy_weight: Weight of Entropy term\n    :param add_summary: Whether to add summary info\n    :param decay: Decay scalar value\n    :param limits: Limits to restrict weights in format: [min, max]\n    :param alpha: Focal loss scale parameter\n    :param gamma: Focal loss power parameter\n    :param num_bins: GH loss number of bins\n    :return: Weighted sum of CE losses\n    """"""\n\n    enable_max_entropy_loss = max_entropy_weight is not None and max_entropy_weight > 0.0\n    enable_focal_loss = alpha is not None and alpha > 0.0 and gamma is not None and gamma > 0.0\n    enable_gradientharmonized_loss = num_bins is not None and num_bins > 0\n    if enable_max_entropy_loss and enable_focal_loss and enable_gradientharmonized_loss:\n        raise Exception(\'Cannot enable different CE losses simultaneously\')\n\n    with tf.variable_scope(name):\n        if enable_max_entropy_loss:\n            ce_losses = max_entropy_ce_loss(labels, logits, max_entropy_weight, \'me_ce_losses\', add_summary)\n        elif enable_focal_loss:\n            ce_losses = focal_ce_loss(labels, logits, alpha, gamma, \'fl_ce_losses\')\n        elif enable_gradientharmonized_loss:\n            ce_losses = gradient_harmonized_ce_loss(labels, logits, \'gh_ce_losses\', num_bins, add_summary=add_summary)\n        else:\n            ce_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name=\'ce_losses\')\n\n        valid_ce_losses_mask = tf.greater(ce_losses, 0.0)\n\n        out_loss = class_frequencies_weighting(ce_losses, labels, valid_ce_losses_mask, num_classes, \'by_class\',\n                                               add_summary=add_summary, decay=decay, limits=limits)\n\n    return out_loss\n\n\ndef class_balanced_hard_losses(all_losses, class_counts, top_k, name=None, add_summary=True):\n    """"""Carry out selection of hardest losses for each class.\n\n    :param all_losses: List of all losses by class\n    :param class_counts: Number of valid losses for each class\n    :param top_k: Max number of losses to select\n    :param name: Name of block\n    :param add_summary: Whether to add summary info\n    :return: Loss value\n    """"""\n\n    assert 0.0 < top_k < 1.0\n\n    with tf.name_scope(name, \'balanced_hard_losses\'):\n        class_counts = tf.stack(tuple(class_counts), axis=0)\n        valid_mask = tf.greater(class_counts, 0.0)\n        mean_valid_count = tf.maximum(1, tf.cast(safe_masked_reduce_op(class_counts, valid_mask, tf.reduce_mean),\n                                                 tf.int32))\n\n        top_losses = []\n        for class_losses in all_losses:\n            top_losses.append(\n                tf.cond(tf.greater(tf.size(class_losses), mean_valid_count),\n                        lambda: tf.nn.top_k(class_losses, mean_valid_count)[0],  # pylint: disable=cell-var-from-loop\n                        lambda: class_losses))  # pylint: disable=cell-var-from-loop\n\n        top_losses = tf.concat(tuple(top_losses), axis=0)\n        num_valid_classes = tf.reduce_sum(tf.cast(valid_mask, tf.float32))\n\n        out_losses = tf.cond(\n            tf.greater(num_valid_classes, 0),\n            lambda: tf.nn.top_k(\n                top_losses,\n                tf.maximum(1, tf.cast(float(top_k) * tf.cast(tf.size(top_losses), tf.float32), tf.int32)))[0],\n            lambda: tf.zeros([0], tf.float32))\n\n        out_loss = safe_masked_reduce_op(out_losses, tf.greater(out_losses, 0.0), tf.reduce_mean)\n\n        if add_summary:\n            tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'hard_losses/mean_num\',\n                                                                   mean_valid_count))\n            tf.add_to_collection(\'loss_summary\', tf.summary.scalar(\'hard_losses/sparsity\',\n                                                                   tf.nn.zero_fraction(top_losses)))\n\n    return out_loss\n\n\ndef explicit_center_loss(embeddings, labels, centers, name, top_k=0.5, min_class_size=2, add_summary=True):\n    """"""Calculates center loss for the specified embedding vectors.\n\n    :param embeddings: Embeddings vectors\n    :param labels: List of labels\n    :param centers: Centers of classes\n    :param name: Name of block\n    :param top_k: Max number of losses per class\n    :param min_class_size: Min number of samples per class\n    :param add_summary: Whether to add summary info\n    :return: Loss value\n    """"""\n\n    def _process(mask, center):\n        class_embeddings = tf.boolean_mask(embeddings, mask)\n        losses = 1.0 - tf.matmul(class_embeddings, tf.reshape(center, [-1, 1]))\n        return tf.reshape(losses, [-1])\n\n    with tf.variable_scope(name):\n        class_losses = []\n        class_counts = []\n        for class_id in xrange(len(centers)):\n            class_mask = tf.equal(labels, class_id)\n            class_size = tf.reduce_sum(tf.cast(class_mask, tf.int32))\n\n            is_valid_class = tf.greater_equal(class_size, min_class_size)\n\n            class_losses.append(tf.cond(is_valid_class,\n                                        lambda: _process(class_mask, centers[class_id]),  # pylint: disable=cell-var-from-loop\n                                        lambda: tf.zeros([0], tf.float32)))\n            class_counts.append(tf.cond(is_valid_class,\n                                        lambda: tf.cast(class_size, tf.float32),  # pylint: disable=cell-var-from-loop\n                                        lambda: 0.0))\n\n        out_loss = class_balanced_hard_losses(class_losses, class_counts, top_k, add_summary=add_summary)\n\n    return out_loss\n\n\ndef explicit_pull_push_loss(embeddings, labels, centers, margin, name, top_k=0.5, add_summary=True):\n    """"""Calculates Pull-Push loss with Smart margin. See: https://arxiv.org/abs/1812.02465\n\n    :param embeddings: Embedding vectors\n    :param labels: List of labels\n    :param centers: Centers of classes\n    :param margin: Margin scalar value\n    :param name: Name of block\n    :param top_k: Max number of losses per class\n    :param add_summary: Whether to add summary info\n    :return: Loss value\n    """"""\n\n    def _calculate_loss(embd, class_name):\n        valid_center = tf.reshape(centers[class_name], [-1, 1])\n        pos_dist = tf.reshape(1.0 - tf.matmul(embd, valid_center), [-1])\n\n        invalid_centers = tf.concat(tuple([tf.reshape(centers[i], [-1, 1]) for i in xrange(len(centers))\n                                           if i != class_name]), axis=1)\n        neg_dist = tf.reduce_min(1.0 - tf.matmul(embd, invalid_centers), axis=1)\n\n        return tf.maximum(0.0, margin + pos_dist - neg_dist)\n\n    with tf.variable_scope(name):\n        out_losses = []\n        class_counts = []\n        for class_id in xrange(len(centers)):\n            class_mask = tf.equal(labels, class_id)\n            class_embeddings = tf.boolean_mask(embeddings, class_mask)\n\n            class_losses = tf.cond(tf.greater_equal(tf.reduce_sum(tf.cast(class_mask, tf.int32)), 1),\n                                   lambda: _calculate_loss(class_embeddings, class_id),  # pylint: disable=cell-var-from-loop\n                                   lambda: tf.zeros([0], tf.float32))\n            out_losses.append(class_losses)\n\n            class_valid_losses_mask = tf.greater(class_losses, 0.0)\n            class_num_valid = tf.reduce_sum(tf.cast(class_valid_losses_mask, tf.int32))\n            class_counts.append(tf.cast(class_num_valid, tf.float32))\n\n        out_loss = class_balanced_hard_losses(out_losses, class_counts, top_k, add_summary=add_summary)\n\n    return out_loss\n\n\ndef sampling_losses(input_embeddings, input_labels, centers, num_samples, main_weight, auxiliary_weight, name,\n                    logit_scale=None, max_entropy_weight=0.4, add_summary=True, limits=None,\n                    alpha=None, gamma=None, num_bins=None):\n    """"""Calculates Cross_entropy, Center and Push losses for the sampled embeddings.\n\n    :param input_embeddings: Embedding vectors\n    :param input_labels: List labels\n    :param centers: Centers of classes\n    :param num_samples: Number of instances per class to sample\n    :param main_weight: CE loss weight\n    :param auxiliary_weight: Auxiliary loss weight\n    :param name: Name of block\n    :param logit_scale: Scale for CE loss\n    :param max_entropy_weight: Weight of Entropy term for CE loss\n    :param add_summary: Whether to add summary info\n    :param limits: Weight limits\n    :param alpha: Focal loss scale parameter\n    :param gamma: Focal loss power parameter\n    :param num_bins: Number of bins for GH loss\n    :return: Loss value\n    """"""\n\n    def _process(embd, labels):\n        sampled_logits = tf.matmul(embd, tf.stack(centers, axis=1))\n        scaled_logits = logit_scale * sampled_logits if logit_scale is not None else sampled_logits\n        ce_loss_value = weighted_ce_loss(labels, scaled_logits, len(centers), \'ce_loss\', max_entropy_weight,\n                                         limits=limits, alpha=alpha, gamma=gamma, num_bins=num_bins)\n\n        push_loss_value = local_push_loss(embd, labels, len(centers), 0.5, \'local_push_loss\')\n        center_loss_value = explicit_center_loss(embd, labels, centers, \'center_loss\', add_summary=add_summary)\n        auxiliary_loss = adaptive_weighting([center_loss_value, push_loss_value], \'auxiliary\', add_summary=add_summary)\n\n        return main_weight * ce_loss_value + auxiliary_weight * auxiliary_loss\n\n    with tf.variable_scope(name):\n        sampled_embeddings, sampled_labels = \\\n            sample_from_extreme_points(input_embeddings, input_labels, len(centers), num_samples)\n\n        num_embeddings = tf.shape(sampled_embeddings)[0]\n        is_valid = tf.greater(num_embeddings, 0)\n\n        return tf.cond(is_valid,\n                       lambda: _process(sampled_embeddings, sampled_labels),\n                       lambda: 0.0)\n\n\ndef local_push_loss(embeddings, labels, num_classes, margin, name, top_k=0.5):\n    """"""Calculates Push losses for each pair of classes and carry out two-stage hard sample mining over them.\n\n    :param embeddings: Embedding vectors\n    :param labels: List of labels\n    :param num_classes: Number of classes\n    :param margin: Margin scalar value\n    :param name: Name of block\n    :param top_k: Max number of losses\n    :return: Loss value\n    """"""\n\n    def _process(mask_a, mask_b):\n        embeddings_a = tf.boolean_mask(embeddings, mask_a)\n        embeddings_b = tf.boolean_mask(embeddings, mask_b)\n\n        distance_matrix = 1.0 - tf.matmul(embeddings_a, tf.transpose(embeddings_b))\n        losses = tf.reshape(margin - distance_matrix, [-1])\n\n        num_valid_values = tf.reduce_sum(tf.cast(tf.greater(losses, 0.0), tf.float32))\n\n        return losses, num_valid_values\n\n    class_pairs = [(i, j) for i in xrange(num_classes) for j in range(i + 1, num_classes)]\n\n    with tf.variable_scope(name):\n        all_losses = []\n        class_counts = []\n        for class_i, class_j in class_pairs:\n            class_i_mask = tf.equal(labels, class_i)\n            class_j_mask = tf.equal(labels, class_j)\n\n            is_valid = tf.logical_and(tf.reduce_any(class_i_mask), tf.reduce_any(class_j_mask))\n            class_losses, out_count = tf.cond(is_valid,\n                                              lambda: _process(class_i_mask, class_j_mask),  # pylint: disable=cell-var-from-loop\n                                              lambda: (tf.zeros([0], tf.float32), 0.0))\n\n            all_losses.append(class_losses)\n            class_counts.append(out_count)\n\n        out_loss = class_balanced_hard_losses(all_losses, class_counts, top_k)\n\n    return out_loss\n\n\ndef multi_similarity_loss(embeddings, labels, alpha=2.0, beta=50.0, gamma=1.0, similarity_delta=0.1, name=None):\n    """"""Calculates Multi-similarity loss. See: https://arxiv.org/abs/1904.06627\n\n    :param embeddings: Embedding vectors\n    :param labels: List of labels\n    :param alpha: Method parameter\n    :param beta: Method parameter\n    :param gamma: Shift parameter\n    :param similarity_delta: Delta parameter\n    :param name: Name of block\n    :return: Loss value\n    """"""\n\n    with tf.variable_scope(name, \'multi_similarity_loss\'):\n        similarity_matrix = tf.matmul(embeddings, tf.transpose(embeddings))\n        shifted_similarity_matrix = similarity_matrix - float(gamma)\n\n        same_label_mask = tf.equal(tf.reshape(labels, [-1, 1]), tf.reshape(labels, [1, -1]))\n        different_label_mask = tf.logical_not(same_label_mask)\n\n        pos_sim_matrix = tf.where(same_label_mask, similarity_matrix, tf.ones_like(similarity_matrix))\n        neg_sim_matrix = tf.where(different_label_mask, similarity_matrix, -1.0 * tf.ones_like(similarity_matrix))\n\n        hardest_pos_threshold = tf.reduce_min(pos_sim_matrix, axis=-1) - similarity_delta\n        hardest_neg_threshold = tf.reduce_max(neg_sim_matrix, axis=-1) + similarity_delta\n\n        positives_mask = tf.logical_and(tf.less(similarity_matrix, hardest_neg_threshold), same_label_mask)\n        negatives_mask = tf.logical_and(tf.greater(similarity_matrix, hardest_pos_threshold), different_label_mask)\n\n        positive_components = tf.where(positives_mask,\n                                       tf.exp(float(-alpha) * shifted_similarity_matrix),\n                                       tf.zeros_like(shifted_similarity_matrix))\n        negative_components = tf.where(negatives_mask,\n                                       tf.exp(float(beta) * shifted_similarity_matrix),\n                                       tf.zeros_like(shifted_similarity_matrix))\n\n        positive_values = float(1.0 / alpha) * tf.log(1.0 + tf.reduce_sum(positive_components, axis=-1))\n        negative_values = float(1.0 / beta) * tf.log(1.0 + tf.reduce_sum(negative_components, axis=-1))\n\n        out_loss = tf.reduce_mean(positive_values + negative_values)\n\n    return out_loss\n\n\ndef balanced_l1_loss(input_value, alpha, gamma):\n    """"""Calculates Balanced L1 loss. See: https://arxiv.org/abs/1904.02701\n\n    :param input_value: Input values\n    :param alpha: Method parameter\n    :param gamma: Method parameter\n    :return: Re-weighted loss values\n    """"""\n\n    betta = np.exp(float(gamma) / float(alpha)) - 1.0\n    shift = float(gamma) / float(betta) - float(alpha)\n\n    with tf.name_scope(\'balanced_l1_loss\'):\n        abs_x = tf.abs(input_value)\n        out_loss = tf.where(tf.less(abs_x, 1.0),\n                            alpha / betta * (betta * abs_x + 1.0) * tf.log(betta * abs_x + 1.0) - alpha * abs_x,\n                            gamma * abs_x + shift)\n\n    return out_loss\n'"
tensorflow_toolkit/action_detection/action_detection/nn/nodes/metrics.py,11,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\n\n\ndef iou_similarity(data_a, data_b):\n    """"""Computes IoU metric between two sets of boxes in matrix form.\n\n    :param data_a: First set of data of shape [M, 4]\n    :param data_b: Second set of data of shape [N, 4]\n    :return: IoU values of shape [M, N]\n    """"""\n\n    def _split_and_reshape(data, out_shape):\n        """"""Splits bounding box coordinates into vectors of coordinates.\n\n        :param data: Input bbox data of shape [N, 4]\n        :param out_shape: Shape of each vector\n        :return: List of vectors\n        """"""\n\n        return [tf.reshape(data[:, 0], out_shape),\n                tf.reshape(data[:, 1], out_shape),\n                tf.reshape(data[:, 2], out_shape),\n                tf.reshape(data[:, 3], out_shape)]\n\n    anchor_bboxes = _split_and_reshape(data_a, [-1, 1])\n    ref_bboxes = _split_and_reshape(data_b, [1, -1])\n\n    intersect_ymin = tf.maximum(anchor_bboxes[0], ref_bboxes[0])\n    intersect_xmin = tf.maximum(anchor_bboxes[1], ref_bboxes[1])\n    intersect_ymax = tf.minimum(anchor_bboxes[2], ref_bboxes[2])\n    intersect_xmax = tf.minimum(anchor_bboxes[3], ref_bboxes[3])\n\n    intersect_height = tf.maximum(0.0, intersect_ymax - intersect_ymin)\n    intersect_width = tf.maximum(0.0, intersect_xmax - intersect_xmin)\n    intersect_areas = intersect_width * intersect_height\n\n    areas1 = (anchor_bboxes[3] - anchor_bboxes[1]) * (anchor_bboxes[2] - anchor_bboxes[0])\n    areas2 = (ref_bboxes[3] - ref_bboxes[1]) * (ref_bboxes[2] - ref_bboxes[0])\n\n    union_areas = areas1 + areas2 - intersect_areas\n\n    out_values = tf.where(tf.greater(union_areas, 0.0), intersect_areas / union_areas, tf.zeros_like(intersect_areas))\n\n    return out_values\n'"
tensorflow_toolkit/action_detection/action_detection/nn/nodes/ops.py,134,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom action_detection.nn.nodes.variables import create_variable\n\n\ndef assign_moving_average(variable, value, momentum):\n    """"""Creates Op to assign exponentially decayed variable value.\n\n    :param variable: Input variable\n    :param value: New value\n    :param momentum: Momentum scalar value\n    :return: Update op\n    """"""\n\n    with tf.name_scope(None, \'AssignMovingAvg\'):\n        return tf.assign_sub(variable, (variable - value) * (1.0 - momentum))\n\n\ndef assign_moving_frequency(variable, value, momentum):\n    """"""Creates Op to assign normalized variable value\n\n    :param variable: Input normalized variable\n    :param value: New distribution value\n    :param momentum: Momentum scalar value\n    :return: Update op\n    """"""\n\n    with tf.name_scope(None, \'AssignMovingFreq\'):\n        averaged_value = float(momentum) * variable + value * (1.0 - momentum)\n        sum_values = tf.reduce_sum(averaged_value)\n        normalizer = tf.cond(tf.greater(sum_values, 0.0), lambda: tf.reciprocal(sum_values), lambda: 1.0)\n        return tf.assign(variable, normalizer * averaged_value)\n\n\ndef batch_norm(input_value, name, is_training, momentum=0.99, num_channels=None,\n               trg_weights=None, trg_weight_var=None, trg_bias=None,\n               reduction_axis=None, add_summary=False):\n    """"""Adds Batch Normalization (BN) after the specified input tensor.\n\n    :param input_value: Input tensor\n    :param name: Name of block\n    :param is_training: Inference mode\n    :param momentum: Momentum scalar value\n    :param num_channels: Input channels number\n    :param trg_weights: Dictionary with weight variables if needed\n    :param trg_weight_var: Dictionary with weight variables if needed\n    :param trg_bias: Dictionary with bias variables if needed\n    :param reduction_axis: List of axis for reduction\n    :param add_summary: Whether to add summary info\n    :return: Normalized tensor\n    """"""\n\n    if num_channels is None:\n        num_channels = [input_value.get_shape()[-1]]\n\n    if reduction_axis is None:\n        reduction_axis = [0, 1, 2]\n\n    if trg_weight_var is not None and trg_weights is not None and trg_bias is not None:\n        with tf.variable_scope(name):\n            beta = tf.get_variable(\'beta\', num_channels, tf.float32, tf.initializers.constant(0.0), trainable=True)\n            gamma = tf.get_variable(\'gamma\', num_channels, tf.float32, tf.initializers.constant(1.0), trainable=True)\n            mean = tf.get_variable(\n                \'moving_mean\', num_channels, tf.float32, tf.initializers.constant(0.0),\n                synchronization=tf.VariableSynchronization.ON_READ, trainable=False,\n                aggregation=tf.VariableAggregation.MEAN)\n            variance = tf.get_variable(\n                \'moving_variance\', num_channels, tf.float32, tf.initializers.constant(1.0),\n                synchronization=tf.VariableSynchronization.ON_READ, trainable=False,\n                aggregation=tf.VariableAggregation.MEAN)\n\n            scales = tf.multiply(tf.rsqrt(variance), gamma)\n            if trg_weights.get_shape()[-1] == num_channels:\n                # weights shape is [h, w, in, out]\n                new_weights = tf.multiply(trg_weights, tf.reshape(scales, [1, 1, 1, -1]))\n            elif trg_weights.get_shape()[-1] == 1:\n                # weights shape is [h, w, in, 1]\n                new_weights = tf.multiply(trg_weights, tf.reshape(scales, [1, 1, -1, 1]))\n            else:\n                # weights shape is [h, w, out, in]\n                trans_weights = tf.transpose(trg_weights, [0, 1, 3, 2])\n                scaled_trans_weights = tf.multiply(trans_weights, tf.reshape(scales, [1, 1, 1, -1]))\n                new_weights = tf.transpose(scaled_trans_weights, [0, 1, 3, 2])\n            new_bias = beta - tf.multiply(scales, mean)\n\n            weights_update_op = tf.assign(trg_weight_var, new_weights)\n            bias_update_op = tf.assign(trg_bias, new_bias)\n\n            tf.add_to_collection(\'bn_merge_ops\', weights_update_op)\n            tf.add_to_collection(\'bn_merge_ops\', bias_update_op)\n\n            return input_value\n    else:\n        with tf.variable_scope(name):\n            beta = tf.get_variable(\'beta\', num_channels, tf.float32, tf.initializers.constant(0.0), trainable=True)\n            gamma = tf.get_variable(\'gamma\', num_channels, tf.float32, tf.initializers.constant(1.0), trainable=True)\n\n            if add_summary:\n                tf.add_to_collection(\'bn_summary\', tf.summary.histogram(beta.op.name, beta))\n                tf.add_to_collection(\'bn_summary\', tf.summary.histogram(gamma.op.name, gamma))\n\n        def _get_train_values():\n            with tf.variable_scope(name):\n                batch_mean, batch_variance = tf.nn.moments(input_value, reduction_axis, name=\'moments\')\n\n                moving_mean = tf.get_variable(\n                    \'moving_mean\', num_channels, tf.float32, tf.initializers.constant(0.0),\n                    synchronization=tf.VariableSynchronization.ON_READ, trainable=False,\n                    aggregation=tf.VariableAggregation.MEAN)\n                moving_variance = tf.get_variable(\n                    \'moving_variance\', num_channels, tf.float32, tf.initializers.constant(1.0),\n                    synchronization=tf.VariableSynchronization.ON_READ, trainable=False,\n                    aggregation=tf.VariableAggregation.MEAN)\n\n                if add_summary:\n                    tf.add_to_collection(\'bn_summary\', tf.summary.histogram(\'moving_mean\', moving_mean))\n                    tf.add_to_collection(\'bn_summary\', tf.summary.histogram(\'moving_variance\', moving_variance))\n\n                mean_update_op = assign_moving_average(moving_mean, batch_mean, momentum)\n                var_update_op = assign_moving_average(moving_variance, batch_variance, momentum)\n\n                with tf.control_dependencies([mean_update_op, var_update_op]):\n                    return tf.identity(batch_mean), tf.identity(batch_variance)\n\n        def _get_test_values(reuse=True):\n            with tf.variable_scope(name, reuse=reuse):\n                out_mean = tf.get_variable(\n                    \'moving_mean\', num_channels, tf.float32, tf.initializers.constant(0.0),\n                    synchronization=tf.VariableSynchronization.ON_READ, trainable=False,\n                    aggregation=tf.VariableAggregation.MEAN)\n                out_variance = tf.get_variable(\n                    \'moving_variance\', num_channels, tf.float32, tf.initializers.constant(1.0),\n                    synchronization=tf.VariableSynchronization.ON_READ, trainable=False,\n                    aggregation=tf.VariableAggregation.MEAN)\n\n            return out_mean, out_variance\n\n        if is_training is not None:\n            mean, variance = tf.cond(is_training, _get_train_values, _get_test_values)\n        else:\n            mean, variance = _get_test_values(False)\n\n        output_value = tf.nn.batch_normalization(input_value, mean, variance, beta, gamma, 1e-3)\n        output_value.set_shape(input_value.get_shape())\n\n        return output_value\n\n\ndef kernel_norm(kernel, axis, merge_op=False, merge_op_transit=False):\n    """"""Adds Op to kerry out L2 normalization over model weights instead of weight decay.\n\n    :param kernel: Input weights\n    :param axis: List of axis to merge\n    :param merge_op: Whether to run with merged normalization\n    :param merge_op_transit: Whether to run in merging mode\n    :return: Normalized weights\n    """"""\n\n    if merge_op and not merge_op_transit:\n        out_kernel = kernel\n    else:\n        out_kernel = tf.nn.l2_normalize(kernel, axis=axis)\n\n    return out_kernel\n\n\ndef conv2d(input_value, shape, name, fn_activation=None, stride=None, rate=None, padding=\'SAME\', depth_wise=False,\n           use_bn=True, use_bias=False, use_dropout=None, dropout_shape=None, is_training=None, reuse_var=None,\n           init=None, init_scale=1.0, add_summary=False, merge_op=False, merge_op_transit=False,\n           norm_kernel=False, pr_product=False):\n    """"""Wrapper for Convolution layer.\n\n    :param input_value: Input tensor\n    :param shape: Kernel shape\n    :param name: Name of block\n    :param fn_activation: Output activation function if needed\n    :param stride: Stride size\n    :param rate: Dilation rate\n    :param padding: Padding size\n    :param depth_wise: Whether to carry out depth-wise convolution\n    :param use_bn: Whether to add Batch Normalization stage\n    :param use_bias: Whether to add bias term\n    :param use_dropout: Whether to use dropout regularization\n    :param dropout_shape: Shape of Bernoulli variable\n    :param is_training: Whether to run in training mode\n    :param reuse_var: Whether to reuse variables\n    :param init: Initialization type\n    :param init_scale: Scale of init weights\n    :param add_summary: Whether to add summary info\n    :param merge_op: Whether to run with merged normalization ops\n    :param merge_op_transit: Whether to run in merging mode\n    :param norm_kernel: Whether to normalize weights\n    :param pr_product: Whether to use PR-Product\n    :return: Output tensor\n    """"""\n\n    if init is None:\n        init = tf.initializers.variance_scaling(scale=init_scale, mode=\'fan_in\', distribution=\'truncated_normal\')\n\n    with tf.variable_scope(name, reuse=reuse_var) as scope:\n        if depth_wise:\n            out_num_channels = shape[-2]\n            kernel_var = create_variable(\'weights\', shape, init)\n            if norm_kernel:\n                kernel = kernel_norm(kernel_var, [0, 1], merge_op, merge_op_transit)\n            else:\n                kernel = kernel_var\n            conv = tf.nn.depthwise_conv2d(input_value, kernel, padding=padding,\n                                          strides=stride if stride is not None else [1, 1, 1, 1],\n                                          rate=rate if rate is not None else [1, 1])\n        else:\n            out_num_channels = shape[-1]\n            kernel_var = create_variable(\'weights\', shape, init)\n            if norm_kernel:\n                kernel = kernel_norm(kernel_var, [0, 1, 2], merge_op, merge_op_transit)\n            else:\n                kernel = kernel_var\n            conv = tf.nn.conv2d(input_value, kernel, padding=padding,\n                                strides=stride if stride is not None else [1, 1, 1, 1],\n                                dilations=rate if rate is not None else [1, 1, 1, 1])\n\n            if pr_product and is_training is not None:\n                if shape[0] != 1 or shape[1] != 1:\n                    raise Exception(\'PR Product support 1x1 conv only.\')\n\n                prod = conv\n                with tf.name_scope(\'pr_product\'):\n                    kernel_norms = tf.ones([1, 1, 1, out_num_channels], dtype=tf.float32) \\\n                        if norm_kernel else tf.reduce_sum(tf.square(kernel), axis=[0, 1, 2], keepdims=True)\n                    input_norms = tf.reduce_sum(tf.square(input_value), axis=-1, keepdims=True)\n\n                    prod_norms = kernel_norms * input_norms\n                    alpha = tf.sqrt(tf.square(prod_norms) - tf.square(prod))\n\n                    conv = tf.stop_gradient(alpha / prod_norms) * prod + \\\n                           tf.stop_gradient(prod / prod_norms) * (prod_norms - alpha)\n\n        if add_summary:\n            weights_summary_op = tf.summary.histogram(kernel.op.name + \'/values\', kernel)\n            tf.add_to_collection(\'weights_summary\', weights_summary_op)\n\n        if merge_op:\n            biases = create_variable(\'biases\', [out_num_channels], tf.initializers.constant(0.0))\n\n            if merge_op_transit:\n                conv = batch_norm(conv, \'bn\', is_training, num_channels=out_num_channels,\n                                  trg_weights=kernel, trg_weight_var=kernel_var, trg_bias=biases)\n            conv = tf.nn.bias_add(conv, biases)\n        else:\n            if use_bias:\n                biases = create_variable(\'biases\', [out_num_channels], tf.initializers.constant(0.0))\n                conv = tf.nn.bias_add(conv, biases)\n\n                if add_summary:\n                    biases_summary_op = tf.summary.histogram(biases.op.name, biases)\n                    tf.add_to_collection(\'weights_summary\', biases_summary_op)\n\n            if use_bn:\n                conv = batch_norm(conv, \'bn\', is_training, num_channels=out_num_channels)\n\n        if fn_activation is not None:\n            conv = fn_activation(conv, name=scope.name+\'_fn_activation\')\n\n        if use_dropout is not None and is_training is not None:\n            conv = tf.cond(is_training,\n                           lambda: tf.nn.dropout(conv, use_dropout, dropout_shape),\n                           lambda: conv)\n\n    return conv\n\n\ndef max_pool(input_value, kernel=None, stride=None, k=2, padding=\'SAME\', name=\'max_pool\'):\n    """"""Wrapper for max-pooling operation.\n\n    :param input_value: Input tensor\n    :param kernel: Kernel shape\n    :param stride: Stride sizes\n    :param k: Stride factor\n    :param padding: Padding sizes\n    :param name: Name of block\n    :return: Output tensor\n    """"""\n\n    assert k is not None or kernel is not None and stride is not None,\\\n        ""The kernel and stride should be specified if k is None""\n\n    if kernel is not None:\n        ksize = kernel\n    else:\n        ksize = [1, k, k, 1]\n\n    if stride is not None:\n        strides = stride\n    else:\n        strides = [1, k, k, 1]\n\n    with tf.name_scope(name):\n        pool = tf.nn.max_pool(input_value, ksize=ksize, strides=strides, padding=padding)\n\n    return pool\n\n\ndef glob_max_pool(input_value, name=\'glob_max_pool\', add_summary=False):\n    """"""Wrapper for global max-pooling operation.\n\n    :param input_value: Input tensor\n    :param name: Name of block\n    :param add_summary: Whether to add summary info\n    :return: Output tensor\n    """"""\n\n    with tf.name_scope(name):\n        pool = tf.reduce_max(input_value, axis=[1, 2], keepdims=True)\n        if add_summary:\n            biases_summary_op = tf.summary.histogram(pool.op.name, pool)\n            tf.add_to_collection(\'activation_summary\', biases_summary_op)\n\n        return pool\n\n\ndef dropout(input_value, keep_prob, noise_shape=None, is_training=None):\n    """"""Wrapper for dropout regularization\n\n    :param input_value: Input tensor\n    :param keep_prob: Probability to preserve value\n    :param noise_shape: Shape of dropout variable\n    :param is_training: Whether to run regularization\n    :return: Output tensor\n    """"""\n\n    if is_training is not None:\n        return tf.cond(is_training,\n                       lambda: tf.nn.dropout(input_value, keep_prob=keep_prob, noise_shape=noise_shape),\n                       lambda: input_value)\n    else:\n        return input_value\n\n\ndef extract_gt_values(values, labels, name):\n    """"""Selects values according specified labels\n\n    :param values: Input tensor of [n, m] shape\n    :param labels: Input tensor of [n] shape. Each value in [0, m - 1] range\n    :param name: Name of block\n    :return: Selected by ID values of [n] shape\n    """"""\n\n    with tf.name_scope(name, \'gt_values\'):\n        valid_logits = tf.batch_gather(values, tf.reshape(labels, [-1, 1]))\n        flat_logits = tf.reshape(valid_logits, [-1])\n    return flat_logits\n\n\ndef safe_masked_reduce_op(input_value, mask, reduce_op, default_value=0.0):\n    """"""Carry out safe (if empty) reduction operation over masked values.\n\n    :param input_value: Input tensor\n    :param mask: Mask of valid values\n    :param reduce_op: Reduction op\n    :param default_value: Value to return if all values are invalid\n    :return: Output scalar value\n    """"""\n\n    def _process():\n        masked_x = tf.boolean_mask(input_value, mask)\n        return reduce_op(masked_x)\n\n    input_size = tf.size(input_value)\n    num_masked = tf.reduce_sum(tf.cast(mask, tf.int32))\n    is_valid = tf.logical_and(tf.greater(input_size, 0),\n                              tf.greater(num_masked, 0))\n\n    return tf.cond(is_valid,\n                   lambda: _process(),\n                   lambda: default_value)\n\n\ndef safe_reduce_op(input_value, reduce_op, default_value=0.0):\n    """"""Carry out safe (if empty) reduction operation.\n\n    :param input_value: Input tensor\n    :param reduce_op: Reduction op\n    :param default_value: Value to return if all values are invalid\n    :return: Output scalar value\n    """"""\n\n    return tf.cond(tf.greater(tf.size(input_value), 0),\n                   lambda: reduce_op(input_value),\n                   lambda: default_value)\n\n\ndef interpolate_extreme_points(embeddings, num_samples, name, top_fraction=0.3):\n    """"""Extracts further from the geometric center points and samples interpolated new values.\n\n    :param embeddings: Input embedding vectors\n    :param num_samples: Number of samples to generate\n    :param name: Name of block\n    :param top_fraction: Number of further embeddings to interpolate between\n    :return: Interpolated embeddings\n    """"""\n\n    with tf.name_scope(name):\n        def _process():\n            center = tf.reduce_mean(embeddings, axis=0)\n            norm_center = tf.reshape(tf.nn.l2_normalize(center), [-1, 1])\n\n            dist_to_center = tf.reshape(1.0 - tf.matmul(embeddings, norm_center), [-1])\n\n            num_corner_points = tf.maximum(tf.cast(top_fraction * tf.cast(num_embeddings, tf.float32), tf.int32), 2)\n            _, corner_points_ids = tf.nn.top_k(dist_to_center, num_corner_points, sorted=False)\n\n            left_ids = tf.random.uniform([num_samples], minval=0, maxval=num_corner_points, dtype=tf.int32)\n            right_ids = tf.random.uniform([num_samples], minval=0, maxval=num_corner_points-1, dtype=tf.int32)\n            right_ids = tf.where(tf.less(right_ids, left_ids), right_ids, right_ids + 1)\n\n            glob_left_ids = tf.gather(corner_points_ids, left_ids)\n            glob_right_ids = tf.gather(corner_points_ids, right_ids)\n\n            left_embeddings = tf.gather(embeddings, glob_left_ids)\n            right_embeddings = tf.gather(embeddings, glob_right_ids)\n\n            cos_gamma = tf.reduce_sum(left_embeddings * right_embeddings, axis=1)\n            sin_gamma = tf.sqrt(1.0 - tf.square(cos_gamma))\n\n            ratio = tf.random.uniform([num_samples], minval=0.0, maxval=1.0, dtype=tf.float32)\n            alpha_angle = tf.acos((1.0 - ratio) / tf.sqrt(ratio * ratio - 2.0 * ratio * cos_gamma + 1.0)) + \\\n                          tf.atan(ratio * sin_gamma / (ratio * cos_gamma - 1.0))\n            gamma_angle = tf.acos(cos_gamma)\n            betta_angle = gamma_angle - alpha_angle\n\n            left_scale = tf.stop_gradient(tf.reshape(tf.sin(betta_angle) / sin_gamma, [-1, 1]))\n            right_scale = tf.stop_gradient(tf.reshape(tf.sin(alpha_angle) / sin_gamma, [-1, 1]))\n\n            return left_scale * left_embeddings + right_scale * right_embeddings\n\n        num_embeddings = tf.shape(embeddings)[0]\n        is_valid = tf.greater(num_embeddings, 1)\n\n        return tf.cond(is_valid,\n                       lambda: _process(),\n                       lambda: tf.zeros([0, embeddings.get_shape()[1]], dtype=tf.float32))\n\n\ndef sample_from_extreme_points(input_embeddings, input_labels, num_classes, num_samples):\n    """"""Carry out sampling of new embeddings for each class by interpolating existing border points.\n\n    :param input_embeddings: Input embeddings\n    :param input_labels: Class labels of embeddings\n    :param num_classes: Number of classes\n    :param num_samples: Number of samples per class\n    :return: Samples embeddings\n    """"""\n\n    out_embeddings = []\n    out_labels = []\n\n    for class_id in xrange(num_classes):\n        class_mask = tf.equal(input_labels, class_id)\n        class_embeddings = tf.boolean_mask(input_embeddings, class_mask)\n\n        sampled_embeddings = interpolate_extreme_points(class_embeddings, num_samples,\n                                                        \'sampling/class_{}\'.format(class_id))\n\n        out_embeddings.append(sampled_embeddings)\n        out_labels.append(tf.fill([tf.shape(sampled_embeddings)[0]], class_id))\n\n    return tf.concat(out_embeddings, axis=0), tf.concat(out_labels, axis=0)\n\n\ndef gsoftmax(logits, axis=-1, weight=1.0):\n    """"""Calculates Generalized Softmax over the input logits\n\n    :param logits: Input logits\n    :param axis: Axis to reduce\n    :param weight: Mixing weight\n    :return: Distribution tensor\n    """"""\n\n    assert axis == -1\n\n    num_classes = logits.get_shape()[-1]\n    factor = -0.5 * np.sqrt(2.0)\n\n    with tf.variable_scope(None, \'gsoftmax\'):\n        mean = tf.get_variable(\n            \'params/mean\', [num_classes], tf.float32, tf.initializers.constant(0.0), trainable=True)\n        log_sigma = tf.get_variable(\n            \'params/log_sigma\', [num_classes], tf.float32, tf.initializers.constant(0.0), trainable=True)\n\n        gaussian_cdf = 0.5 * (tf.erf(factor * (mean - logits) * tf.exp(tf.negative(log_sigma))) + 1.0)\n\n        return tf.nn.softmax(logits + weight * gaussian_cdf)\n'"
tensorflow_toolkit/action_detection/action_detection/nn/nodes/variables.py,6,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\n\n\ndef create_variable(name, shape, initializer, dtype=tf.float32, device=None, collections=None, trainable=True):\n    """"""Wrapper for the parameterized variable creation.\n\n    :param name: Name of variable\n    :param shape: Shape of variable\n    :param initializer: Initializer function if needed\n    :param dtype: Type of data\n    :param device: Target device if needed\n    :param collections: Target collection to add in\n    :param trainable: Whether to add variable set of trainable variables\n    :return: Specified variable\n    """"""\n\n    if device is None:\n        if not callable(initializer):\n            return tf.get_variable(\n                name, initializer=initializer, dtype=dtype, collections=collections, trainable=trainable)\n        else:\n            return tf.get_variable(\n                name, shape, initializer=initializer, dtype=dtype, collections=collections, trainable=trainable)\n    else:\n        with tf.device(device):\n            if not callable(initializer):\n                return tf.get_variable(\n                    name, initializer=initializer, dtype=dtype, collections=collections, trainable=trainable)\n            else:\n                return tf.get_variable(\n                    name, shape, initializer=initializer, dtype=dtype, collections=collections, trainable=trainable)\n'"
tensorflow_toolkit/action_detection/action_detection/nn/parameters/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/action_detection/nn/parameters/action_parameters.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom action_detection.nn.data.augmentation import AugmentFactory, BrightnessAugmentor, SaturationAugmentor, \\\n    DetectionAugmentation\nfrom action_detection.nn.data.core import ImageSize\nfrom action_detection.nn.models import SSDHeadDesc\nfrom action_detection.nn.parameters.common import AttributedDict as Dict\nfrom action_detection.nn.parameters.detector_parameters import DetectorParams\n\n\nclass ActionParams(DetectorParams):\n    """"""Class to control the action model specific parameters.\n    """"""\n\n    def _configure_params(self):\n        """"""Returns the parameters for action network.\n\n        :return: Parameters\n        """"""\n\n        lr_params = Dict(schedule=self._config_values.LR_SCHEDULE,\n                         boundaries=[int(e * self._epoch_num_steps) for e in self._config_values.LR_EPOCH_DROPS],\n                         values=self._config_values.LR_EPOCH_VALUES,\n                         init_value=self._config_values.LR_INIT_VALUE,\n                         first_decay_steps=int(self._config_values.LR_FIRST_DECAY_EPOCH * self._epoch_num_steps),\n                         t_mul=self._config_values.LR_T_MUL,\n                         m_mul=self._config_values.LR_M_MUL,\n                         alpha=self._config_values.LR_ALPHA)\n        mbox_params = Dict(threshold=self._config_values.MBOX_THRESHOLD,\n                           variance=self._config_values.VARIANCE,\n                           bg_class=self._config_values.DETECTION_BG_CLASS_ID,\n                           neg_factor=self._config_values.MBOX_NEG_FACTOR,\n                           cl_weight=self._config_values.MBOX_CL_WEIGHTS,\n                           entropy_weight=self._config_values.MBOX_ENTROPY_WEIGHT,\n                           max_num_samples_per_gt=self._config_values.MBOX_MAX_NUM_MATCHES_PER_GT,\n                           matches_drop_ratio=self._config_values.MBOX_MATCHES_DROP_RATIO,\n                           instance_normalization=self._config_values.MBOX_DO_INSTANCE_NORMALIZATION,\n                           comp_loss_max_num_samples=self._config_values.MBOX_COMPACTNESS_LOSS_MAX_NUM_SAMPLES,\n                           repulsion_loss_weight=self._config_values.MBOX_REPULSION_LOSS_WEIGHT,\n                           focal_alpha=self._config_values.MBOX_FOCAL_ALPHA,\n                           focal_gamma=self._config_values.MBOX_FOCAL_GAMMA,\n                           gh_num_bins=self._config_values.MBOX_GRADIENT_HARMONIZED_LOSS_NUM_BINS)\n\n        image_size = ImageSize(*self._config_values.IMAGE_SIZE)\n\n        tuple_augmentation = DetectionAugmentation(\n            self._config_values.FREE_PROB, self._config_values.EXPAND_PROB,\n            self._config_values.CROP_PROB, self._config_values.MAX_EXPAND_RATIO,\n            self._config_values.CROP_SCALE_DELTA, self._config_values.CROP_SCALE_LIMITS,\n            self._config_values.CROP_SHIFT_DELTA, float(image_size.h) / float(image_size.w))\n        image_augmentation = AugmentFactory() \\\n            .add(BrightnessAugmentor(self._config_values.BRIGHTNESS_DELTA)) \\\n            .add(SaturationAugmentor(self._config_values.SATURATION_LIMITS))\n\n        head_params = []\n        for scale in self._config_values.NORMALIZED_ANCHORS:\n            head_params.append(SSDHeadDesc(scale=scale,\n                                           internal_size=self._config_values.INTERNAL_HEAD_SIZES[scale],\n                                           num_classes=self._config_values.DETECTION_NUM_CLASSES,\n                                           anchors=self._scale_anchors(self._config_values.NORMALIZED_ANCHORS[scale],\n                                                                       image_size.h, image_size.w),\n                                           clip=False,\n                                           offset=0.5))\n\n        action_params = Dict(num_actions=len(self._config_values.VALID_ACTION_NAMES),\n                             embedding_size=self._config_values.ACTION_EMBEDDING_SIZE,\n                             undefined_action_id=self._config_values.UNDEFINED_ACTION_ID,\n                             num_centers_per_action=self._config_values.NUM_CENTERS_PER_ACTION,\n                             scale_start=self._config_values.SCALE_START_VALUE,\n                             scale_end=self._config_values.SCALE_END_VALUE,\n                             scale_num_steps=int(self._config_values.SCALE_NUM_EPOCHS * self._epoch_num_steps),\n                             scale_power=self._config_values.SCALE_POWER,\n                             max_entropy_weight=self._config_values.ACTION_ENTROPY_WEIGHT,\n                             focal_alpha=self._config_values.ACTION_FOCAL_ALPHA,\n                             focal_gamma=self._config_values.ACTION_FOCAL_GAMMA,\n                             glob_pull_push_margin=self._config_values.GLOB_PULL_PUSH_MARGIN,\n                             local_push_margin=self._config_values.LOCAL_PUSH_MARGIN,\n                             num_samples=self._config_values.NUM_SAMPLES_PER_CLASS,\n                             local_push_top_k=self._config_values.LOCAL_PUSH_LOSS_TOP_K,\n                             weight_limits=self._config_values.ADAPTIVE_WEIGHT_LIMITS,\n                             ce_loss_weight=self._config_values.CE_LOSS_WEIGHT,\n                             auxiliary_loss_weight=self._config_values.AUXILIARY_LOSS_WEIGHT,\n                             matches_threshold=self._config_values.MATCHES_THRESHOLD,\n                             max_num_samples_per_gt=self._config_values.MAX_NUM_MATCHES_PER_GT,\n                             sample_matches_drop_ratio=self._config_values.MATCHES_DROP_RATIO,\n                             glob_pull_push_loss_top_k=self._config_values.GLOB_PULL_PUSH_LOSS_TOP_K,\n                             center_loss_top_k=self._config_values.CENTER_LOSS_TOP_K,\n                             center_loss_weight=self._config_values.CENTER_LOSS_WEIGHT,\n                             num_bins=self._config_values.ACTION_GRADIENT_HARMONIZED_LOSS_NUM_BINS)\n\n        action_names_map = {i: v for i, v in enumerate(self._config_values.VALID_ACTION_NAMES)}\n\n        return Dict(max_num_objects_per_image=self._config_values.MAX_NUM_DETECTIONS_PER_IMAGE,\n                    num_classes=self._config_values.DETECTION_NUM_CLASSES,\n                    bg_class=self._config_values.DETECTION_BG_CLASS_ID,\n                    num_actions=len(self._config_values.VALID_ACTION_NAMES),\n                    tuple_augmentation=tuple_augmentation,\n                    image_augmentation=image_augmentation,\n                    lr_params=lr_params,\n                    mbox_params=mbox_params,\n                    head_params=head_params,\n                    action_params=action_params,\n                    labels_map=self._config_values.ACTIONS_MAP,\n                    valid_actions=self._config_values.VALID_ACTION_NAMES,\n                    ignore_classes=self._config_values.IGNORE_CLASSES,\n                    use_class_balancing=self._config_values.USE_CLASS_BALANCING,\n                    det_conf=self._config_values.DETECTION_CONFIDENCE,\n                    action_conf=self._config_values.ACTION_CONFIDENCE,\n                    action_colors_map=self._config_values.ACTION_COLORS_MAP,\n                    action_names_map=action_names_map,\n                    undefined_action_name=self._config_values.UNDEFINED_ACTION_NAME,\n                    undefined_action_color=self._config_values.UNDEFINED_ACTION_COLOR)\n'"
tensorflow_toolkit/action_detection/action_detection/nn/parameters/classifier_parameters.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom action_detection.nn.data.augmentation import ClassifierAugmentation\nfrom action_detection.nn.data.core import ImageSize\nfrom action_detection.nn.parameters.common import AttributedDict as Dict\nfrom action_detection.nn.parameters.model_parameters import ModelParams\n\n\nclass ClassifierParams(ModelParams):\n    """"""Class to control the classification model specific parameters.\n    """"""\n\n    def _configure_params(self):\n        """"""Returns the parameters for classification network.\n\n        :return: Model parameters\n        """"""\n\n        lr_params = Dict(boundaries=[int(e * self._epoch_num_steps) for e in self._config_values.LR_EPOCH_DROPS],\n                         values=self._config_values.LR_EPOCH_VALUES)\n\n        image_size = ImageSize(*self._config_values.IMAGE_SIZE)\n\n        augmentation = ClassifierAugmentation(self._config_values.CROP_PROB,\n                                              self._config_values.CROP_SCALE_LIMITS,\n                                              self._config_values.CROP_VAR_LIMITS,\n                                              self._config_values.BRIGHTNESS_DELTA,\n                                              self._config_values.SATURATION_LIMITS,\n                                              float(image_size.h) / float(image_size.w))\n\n        return Dict(val_image_side=self._config_values.VAL_IMAGE_SIDE_SIZE,\n                    val_central_fraction=self._config_values.VAL_CENTRAL_FRACTION,\n                    num_classes=self._config_values.NUM_CLASSES,\n                    image_augmentation=augmentation,\n                    lr_params=lr_params,\n                    keep_probe=self._config_values.DROPOUT_KEEP_PROBE)\n'"
tensorflow_toolkit/action_detection/action_detection/nn/parameters/common.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport yaml\n\n\nclass AttributedDict(dict):\n    """"""Class to simplify the access to dictionary fields.\n    """"""\n\n    def __getattr__(self, name):\n        return self[name]\n\n    def __setattr__(self, name, value):\n        self[name] = value\n\n\ndef load_config(config_path):\n    """"""Loads parameters into the dict from the specified path.\n\n    :param config_path: Path to config file\n    :return: Dictionary with parameters\n    """"""\n\n    with open(config_path, \'r\') as config_file:\n        config_values = AttributedDict(yaml.load(config_file, Loader=yaml.FullLoader))\n\n    return config_values\n'"
tensorflow_toolkit/action_detection/action_detection/nn/parameters/detector_parameters.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport numpy as np\n\nfrom action_detection.nn.data.augmentation import AugmentFactory, BrightnessAugmentor, SaturationAugmentor, \\\n    DetectionAugmentation\nfrom action_detection.nn.data.core import ImageSize\nfrom action_detection.nn.models import SSDHeadDesc\nfrom action_detection.nn.parameters.common import AttributedDict as Dict\nfrom action_detection.nn.parameters.model_parameters import ModelParams\n\n\nclass DetectorParams(ModelParams):\n    """"""Class to control the detection model specific parameters.\n    """"""\n\n    @staticmethod\n    def _scale_anchors(anchors, image_height, image_width):\n        """"""Converts anchors from normalized format to the target.\n\n        :param anchors: List of anchors\n        :param image_height: Target image height\n        :param image_width: Target image width\n        :return: List of converted anchors\n        """"""\n\n        anchors = np.array(anchors, dtype=np.float32)\n        return np.stack([anchors[:, 0] * image_height, anchors[:, 1] * image_width], axis=1)\n\n    def _configure_params(self):\n        """"""Returns the parameters for detection network.\n\n        :return: Parameters\n        """"""\n\n        lr_params = Dict(boundaries=[int(e * self._epoch_num_steps) for e in self._config_values.LR_EPOCH_DROPS],\n                         values=self._config_values.LR_EPOCH_VALUES)\n        mbox_params = Dict(threshold=self._config_values.MBOX_THRESHOLD,\n                           variance=self._config_values.VARIANCE,\n                           bg_class=self._config_values.BG_CLASS_ID,\n                           neg_factor=self._config_values.MBOX_NEG_FACTOR,\n                           cl_weight=self._config_values.MBOX_CL_WEIGHTS,\n                           entropy_weight=self._config_values.MBOX_ENTROPY_WEIGHT,\n                           max_num_samples_per_gt=self._config_values.MBOX_MAX_NUM_MATCHES_PER_GT,\n                           matches_drop_ratio=self._config_values.MBOX_MATCHES_DROP_RATIO,\n                           instance_normalization=self._config_values.MBOX_DO_INSTANCE_NORMALIZATION,\n                           comp_loss_max_num_samples=self._config_values.MBOX_COMPACTNESS_LOSS_MAX_NUM_SAMPLES,\n                           repulsion_loss_weight=self._config_values.MBOX_REPULSION_LOSS_WEIGHT,\n                           focal_alpha=self._config_values.MBOX_FOCAL_ALPHA,\n                           focal_gamma=self._config_values.MBOX_FOCAL_GAMMA,\n                           gh_num_bins=self._config_values.MBOX_GRADIENT_HARMONIZED_LOSS_NUM_BINS)\n\n        image_size = ImageSize(*self._config_values.IMAGE_SIZE)\n\n        tuple_augmentation = DetectionAugmentation(\n            self._config_values.FREE_PROB, self._config_values.EXPAND_PROB,\n            self._config_values.CROP_PROB, self._config_values.MAX_EXPAND_RATIO,\n            self._config_values.CROP_SCALE_DELTA, self._config_values.CROP_SCALE_LIMITS,\n            self._config_values.CROP_SHIFT_DELTA, float(image_size.h) / float(image_size.w))\n        image_augmentation = AugmentFactory() \\\n            .add(BrightnessAugmentor(self._config_values.BRIGHTNESS_DELTA)) \\\n            .add(SaturationAugmentor(self._config_values.SATURATION_LIMITS))\n\n        head_params = []\n        for scale in self._config_values.NORMALIZED_ANCHORS:\n            head_params.append(SSDHeadDesc(scale=scale,\n                                           internal_size=self._config_values.INTERNAL_HEAD_SIZES[scale],\n                                           num_classes=self._config_values.NUM_CLASSES,\n                                           anchors=self._scale_anchors(self._config_values.NORMALIZED_ANCHORS[scale],\n                                                                       image_size.h, image_size.w),\n                                           clip=False,\n                                           offset=0.5))\n\n        return Dict(max_num_objects_per_image=self._config_values.MAX_NUM_DETECTIONS_PER_IMAGE,\n                    num_classes=self._config_values.NUM_CLASSES,\n                    bg_class=self._config_values.BG_CLASS_ID,\n                    tuple_augmentation=tuple_augmentation,\n                    image_augmentation=image_augmentation,\n                    lr_params=lr_params,\n                    mbox_params=mbox_params,\n                    head_params=head_params,\n                    labels_map=self._config_values.LABELS_MAP)\n'"
tensorflow_toolkit/action_detection/action_detection/nn/parameters/model_parameters.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\n\nimport numpy as np\n\nfrom action_detection.nn.data.core import ImageSize\n\n\nclass ModelParams(object):\n    """"""Class to control the network parameters.\n    """"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, config_values, batch_size, num_gpu):\n        """"""Constructor.\n\n        :param config_values: Loaded from config parameters\n        :param batch_size: Target batch size\n        :param num_gpu: Target number of GPUs to compute on\n        """"""\n\n        assert batch_size > 0\n        assert num_gpu > 0\n\n        self._config_values = config_values\n        self._batch_size = batch_size\n        self._num_gpu = num_gpu\n\n        self._epoch_num_steps = int(float(config_values.TRAIN_DATA_SIZE) / float(batch_size * num_gpu))\n        self._num_train_steps = int(config_values.MAX_NUM_TRAIN_EPOCHS * self._epoch_num_steps)\n        self._num_val_steps = int(np.floor(float(config_values.VAL_DATA_SIZE) / float(batch_size * num_gpu)))\n\n        model_specific_params = self._configure_params()\n        self._params = self._add_shared_params(model_specific_params)\n\n    def __getattr__(self, name):\n        return self._params[name]\n\n    @abstractmethod\n    def _configure_params(self):\n        """"""Returns the parameters for classification network.\n\n        :return: Model parameters\n        """"""\n\n        pass\n\n    def _add_shared_params(self, model_params):\n        """"""Adds general parameters.\n\n        :param model_params: Model-specific parameters\n        :return: Updated model parameters\n        """"""\n\n        model_params.type = self._config_values.NETWORK_TYPE\n        model_params.backbone = self._config_values.NETWORK_BACKBONE\n        model_params.version = self._config_values.NETWORK_VERSION\n        model_params.name = self._config_values.NETWORK_NAME\n        model_params.image_size = ImageSize(*self._config_values.IMAGE_SIZE)\n        model_params.epoch_steps = self._epoch_num_steps\n        model_params.val_steps = self._num_val_steps\n        model_params.max_train_steps = self._num_train_steps\n        model_params.use_nesterov = self._config_values.USE_NESTEROV\n        model_params.norm_kernels = self._config_values.NORMALIZE_KERNELS\n\n        return model_params\n'"
tensorflow_toolkit/action_detection/action_detection/nn/utils/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/sync_batchnorm/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .replicate import DataParallelWithCallback, patch_replication_callback\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/sync_batchnorm/batchnorm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn.parallel._functions import Broadcast, ReduceAddCoalesced\n\nfrom .comm import SyncMaster\n\n__all__ = [\'SynchronizedBatchNorm1d\', \'SynchronizedBatchNorm2d\', \'SynchronizedBatchNorm3d\']\n\n\ndef _sum_ft(tensor):\n    """"""sum over the first and last dimension""""""\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    """"""add new dimensions at the front and the tail""""""\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple(\'_ChildMessage\', [\'sum\', \'ssum\', \'sum_size\'])\n_MasterMessage = collections.namedtuple(\'_MasterMessage\', [\'sum\', \'inv_std\'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch\'s implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        """"""Reduce the sum and square-sum, compute the statistics, and broadcast it.""""""\n\n        # Always using same ""device order"" makes the ReduceAdd operation faster.\n        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n\n        return outputs\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        """"""Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.""""""\n        assert size > 1, \'BatchNorm computes unbiased standard-deviation, which requires size > 1.\'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r""""""Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it\'s common terminology to call this Temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it\'s common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/sync_batchnorm/comm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : comm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\nimport queue\nimport threading\n\n__all__ = [\'FutureResult\', \'SlavePipe\', \'SyncMaster\']\n\n\nclass FutureResult(object):\n    """"""A thread-safe future implementation. Used only as one-to-one pipe.""""""\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, \'Previous result has\\\'t been fetched.\'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple(\'MasterRegistry\', [\'result\'])\n_SlavePipeBase = collections.namedtuple(\'_SlavePipeBase\', [\'identifier\', \'queue\', \'result\'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    """"""Pipe for master-slave communication.""""""\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    """"""An abstract `SyncMaster` object.\n\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    """"""\n\n    def __init__(self, master_callback):\n        """"""\n\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        """"""\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {\'master_callback\': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state[\'master_callback\'])\n\n    def register_slave(self, identifier):\n        """"""\n        Register an slave device.\n\n        Args:\n            identifier: an identifier, usually is the device id.\n\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n\n        """"""\n        if self._activated:\n            assert self._queue.empty(), \'Queue is not clean before next initialization.\'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        """"""\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n\n        Returns: the message to be sent back to the master device.\n\n        """"""\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, \'The first result should belongs to the master.\'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/sync_batchnorm/replicate.py,0,"b'# -*- coding: utf-8 -*-\n# File   : replicate.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport functools\n\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n__all__ = [\n    \'CallbackContext\',\n    \'execute_replication_callbacks\',\n    \'DataParallelWithCallback\',\n    \'patch_replication_callback\'\n]\n\n\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\nclass DataParallelWithCallback(DataParallel):\n    """"""\n    Data Parallel with a replication callback.\n\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    """"""\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate\n'"
pytorch_toolkit/action_recognition/action_recognition/models/modules/sync_batchnorm/unittest.py,0,"b""# -*- coding: utf-8 -*-\n# File   : unittest.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport unittest\n\nimport numpy as np\nfrom torch.autograd import Variable\n\n\ndef as_numpy(v):\n    if isinstance(v, Variable):\n        v = v.data\n    return v.cpu().numpy()\n\n\nclass TorchTestCase(unittest.TestCase):\n    def assertTensorClose(self, a, b, atol=1e-3, rtol=1e-3):\n        npa, npb = as_numpy(a), as_numpy(b)\n        self.assertTrue(\n                np.allclose(npa, npb, atol=atol),\n                'Tensor close check failed\\n{}\\n{}\\nadiff={}, rdiff={}'.format(a, b, np.abs(npa - npb).max(), np.abs((npa - npb) / np.fmax(npa, 1e-5)).max())\n        )\n"""
pytorch_toolkit/nncf/examples/common/models/classification/__init__.py,0,b'from .inceptionv3_cifar100 import *\nfrom .resnet_cifar import *\nfrom .squeezenet import *\nfrom .rmnet_cifar import *\nfrom .mobilenetv3 import *\n'
pytorch_toolkit/nncf/examples/common/models/classification/inceptionv3_cifar100.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n# Inceptionv3 implementation from:\n# torchvision/models/inception.py\n\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import model_zoo\n\n__all__ = [\'Inception3\', \'incept_v3_cifar100\']\n\nmodel_urls = {\n    # Inception v3 ported from TensorFlow\n    \'inception_v3_google\': \'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\',\n}\n\n_InceptionOuputs = namedtuple(\'InceptionOuputs\', [\'logits\', \'aux_logits\'])\n\n\ndef incept_v3_cifar100(pretrained=False, progress=True, **kwargs):\n    r""""""Inception v3 model architecture from\n    `""Rethinking the Inception Architecture for Computer Vision"" <http://arxiv.org/abs/1512.00567>`_.\n\n    .. note::\n        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n        aux_logits (bool): If True, add an auxiliary branch that can improve training.\n            Default: *True*\n        transform_input (bool): If True, preprocesses the input according to the method with which it\n            was trained on ImageNet. Default: *False*\n    """"""\n    if pretrained:\n        if \'transform_input\' not in kwargs:\n            kwargs[\'transform_input\'] = True\n        if \'aux_logits\' in kwargs:\n            original_aux_logits = kwargs[\'aux_logits\']\n            kwargs[\'aux_logits\'] = True\n        else:\n            original_aux_logits = True\n        model = Inception3(**kwargs)\n        state_dict = model_zoo.load_url(model_urls[\'inception_v3_google\'], progress=progress)\n        model.load_state_dict(state_dict)\n        if not original_aux_logits:\n            model.aux_logits = False\n            del model.AuxLogits\n        return model\n\n    return Inception3(**kwargs)\n\n\nclass Inception3(nn.Module):\n\n    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False):\n        super(Inception3, self).__init__()\n        self.aux_logits = aux_logits\n        self.transform_input = transform_input\n        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, padding=1)\n        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3, padding=1)\n        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n        self.Mixed_5b = InceptionA(192, pool_features=32)\n        self.Mixed_5c = InceptionA(256, pool_features=64)\n        self.Mixed_5d = InceptionA(288, pool_features=64)\n        self.Mixed_6a = InceptionB(288)\n        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n        if aux_logits:\n            self.AuxLogits = InceptionAux(768, num_classes)\n        self.Mixed_7a = InceptionD(768)\n        self.Mixed_7b = InceptionE(1280)\n        self.Mixed_7c = InceptionE(2048)\n        self.fc = nn.Linear(2048, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n                import scipy.stats as stats\n                stddev = m.stddev if hasattr(m, \'stddev\') else 0.1\n                X = stats.truncnorm(-2, 2, scale=stddev)\n                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n                values = values.view(m.weight.size())\n                with torch.no_grad():\n                    m.weight.copy_(values)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        if self.transform_input:\n            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n        # N x 3 x 299 x 299\n        x = self.Conv2d_1a_3x3(x)\n        # N x 32 x 149 x 149\n        x = self.Conv2d_2a_3x3(x)\n        # N x 32 x 147 x 147\n        x = self.Conv2d_2b_3x3(x)\n        # N x 64 x 147 x 147\n        # x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 64 x 73 x 73\n        x = self.Conv2d_3b_1x1(x)\n        # N x 80 x 73 x 73\n        x = self.Conv2d_4a_3x3(x)\n        # N x 192 x 71 x 71\n        # x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 192 x 35 x 35\n        x = self.Mixed_5b(x)\n        # N x 256 x 35 x 35\n        x = self.Mixed_5c(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_5d(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_6a(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6b(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6c(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6d(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6e(x)\n        # N x 768 x 17 x 17\n        if self.training and self.aux_logits:\n            aux = self.AuxLogits(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_7a(x)\n        # N x 1280 x 8 x 8\n        x = self.Mixed_7b(x)\n        # N x 2048 x 8 x 8\n        x = self.Mixed_7c(x)\n        # N x 2048 x 8 x 8\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 2048 x 1 x 1\n        x = F.dropout(x, training=self.training, p=0)\n        # N x 2048 x 1 x 1\n        x = x.view(x.size(0), -1)\n        # N x 2048\n        x = self.fc(x)\n        # N x 1000 (num_classes)\n        if self.training and self.aux_logits:\n            return _InceptionOuputs(x, aux)\n        return x\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, pool_features):\n        super(InceptionA, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionB(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionB, self).__init__()\n        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3(x)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n\n        outputs = [branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionC(nn.Module):\n\n    def __init__(self, in_channels, channels_7x7):\n        super(InceptionC, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(3, 0))\n        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(0, 3))\n\n        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionD(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionD, self).__init__()\n        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n\n        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n\n        branch7x7x3 = self.branch7x7x3_1(x)\n        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n        outputs = [branch3x3, branch7x7x3, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionE(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionE, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n\n    def __init__(self, in_channels, num_classes):\n        super(InceptionAux, self).__init__()\n        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01\n        self.fc = nn.Linear(768, num_classes)\n        self.fc.stddev = 0.001\n\n    def forward(self, x):\n        # N x 768 x 17 x 17\n        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n        # N x 768 x 5 x 5\n        x = self.conv0(x)\n        # N x 128 x 5 x 5\n        x = self.conv1(x)\n        # N x 768 x 1 x 1\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 768 x 1 x 1\n        x = x.view(x.size(0), -1)\n        # N x 768\n        x = self.fc(x)\n        # N x 1000\n        return x\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n'"
pytorch_toolkit/nncf/examples/common/models/classification/mobilenetv3.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n\n Creates a MobileNetV3 Model as defined in:\n Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,\n Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\n Searching for MobileNetV3\n arXiv preprint arXiv:1905.02244.\n\n MobileNetV3 implementation from:\n https://github.com/d-li14/mobilenetv3.pytorch\n\n""""""\n\nimport torch.nn as nn\nimport math\n\nfrom nncf.dynamic_graph.patch_pytorch import register_operator\n\n__all__ = [\'mobilenetv3_Large\', \'mobilenetv3\']\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    """"""\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    """"""\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\n@register_operator(\'h_sigmoid\')\ndef h_sigmoid_fn(x, inplace=True):\n    relu = nn.ReLU6(inplace=inplace)\n    return relu(x + 3) / 6\n\n\n@register_operator(\'h_swish\')\ndef h_swish_fn(x, inplace=True):\n    return x * h_sigmoid_fn(x, inplace=inplace)\n\n\nclass h_sigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(h_sigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return h_sigmoid_fn(x, inplace=self.inplace)\n\n\nclass h_swish(nn.Module):\n    def __init__(self, inplace=True):\n        super(h_swish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return h_swish_fn(x, inplace=self.inplace)\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n            h_sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        h_swish()\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        h_swish()\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, hidden_dim, oup,\n                 kernel_size, stride, use_se, use_hs):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n\n        self.identity = stride == 1 and inp == oup\n\n        if inp == hidden_dim:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride,\n                          (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                h_swish() if use_hs else nn.ReLU(inplace=True),\n                # Squeeze-and-Excite\n                SELayer(hidden_dim) if use_se else nn.Sequential(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                h_swish() if use_hs else nn.ReLU(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride,\n                          (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                # Squeeze-and-Excite\n                SELayer(hidden_dim) if use_se else nn.Sequential(),\n                h_swish() if use_hs else nn.ReLU(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        return self.conv(x)\n\n\nclass MobileNetV3(nn.Module):\n    def __init__(self, cfgs, mode, pretrained=False,\n                 num_classes=1000, width_mult=1.):\n        super(MobileNetV3, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = cfgs\n        assert mode in [\'large\', \'small\']\n        # building first layer\n        input_channel = _make_divisible(16 * width_mult, 8)\n        layers = [conv_3x3_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        block = InvertedResidual\n        for k, exp_size, c, use_se, use_hs, s in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 8)\n            layers.append(block(input_channel, exp_size,\n                                output_channel, k, s, use_se, use_hs))\n            input_channel = output_channel\n            last_exp_size = exp_size\n        self.features = nn.Sequential(*layers)\n        # building last several layers\n        self.conv = nn.Sequential(\n            conv_1x1_bn(input_channel, _make_divisible(\n                last_exp_size * width_mult, 8)),\n            SELayer(_make_divisible(last_exp_size * width_mult, 8)\n                    ) if mode == \'small\' else nn.Sequential()\n        )\n        self.avgpool = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            h_swish()\n        )\n        output_channel = _make_divisible(\n            1280 * width_mult, 8) if width_mult > 1.0 else 1280\n        self.classifier = nn.Sequential(\n            nn.Linear(_make_divisible(\n                last_exp_size * width_mult, 8), output_channel),\n            nn.BatchNorm1d(\n                output_channel) if mode == \'small\' else nn.Sequential(),\n            h_swish(),\n            nn.Linear(output_channel, num_classes),\n            nn.BatchNorm1d(\n                num_classes) if mode == \'small\' else nn.Sequential(),\n            h_swish() if mode == \'small\' else nn.Sequential()\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.conv(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\ndef mobilenetv3_Large(**kwargs):\n    """"""\n    Constructs a MobileNetV3-Large model\n    """"""\n    cfgs = [\n        # k, t, c, SE, NL, s\n        [3, 16, 16, 0, 0, 1],\n        [3, 64, 24, 0, 0, 2],\n        [3, 72, 24, 0, 0, 1],\n        [5, 72, 40, 1, 0, 2],\n        [5, 120, 40, 1, 0, 1],\n        [5, 120, 40, 1, 0, 1],\n        [3, 240, 80, 0, 1, 2],\n        [3, 200, 80, 0, 1, 1],\n        [3, 184, 80, 0, 1, 1],\n        [3, 184, 80, 0, 1, 1],\n        [3, 480, 112, 1, 1, 1],\n        [3, 672, 112, 1, 1, 1],\n        [5, 672, 160, 1, 1, 1],\n        [5, 672, 160, 1, 1, 2],\n        [5, 960, 160, 1, 1, 1]\n    ]\n    return MobileNetV3(cfgs, mode=\'large\', **kwargs)\n\n\ndef mobilenetv3(**kwargs):\n    """"""\n    Constructs a MobileNetV3-Small model\n    """"""\n    cfgs = [\n        # k, t, c, SE, NL, s\n        [3, 16, 16, 1, 0, 2],\n        [3, 72, 24, 0, 0, 2],\n        [3, 88, 24, 0, 0, 1],\n        [5, 96, 40, 1, 1, 2],\n        [5, 240, 40, 1, 1, 1],\n        [5, 240, 40, 1, 1, 1],\n        [5, 120, 48, 1, 1, 1],\n        [5, 144, 48, 1, 1, 1],\n        [5, 288, 96, 1, 1, 2],\n        [5, 576, 96, 1, 1, 1],\n        [5, 576, 96, 1, 1, 1],\n    ]\n\n    return MobileNetV3(cfgs, mode=\'small\', **kwargs)\n'"
pytorch_toolkit/nncf/examples/common/models/classification/resnet_cifar.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for block_stride in strides:\n            layers.append(block(self.in_planes, planes, block_stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18_cifar(num_classes=100, pretrained=False):\n    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n\n\ndef ResNet34_cifar(num_classes=100, pretrained=False):\n    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n\n\ndef ResNet50_cifar(num_classes=100, pretrained=False):\n    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n\n\ndef ResNet101_cifar(num_classes=100, pretrained=False):\n    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes)\n\n\ndef ResNet152_cifar(num_classes=100, pretrained=False):\n    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes)\n'"
pytorch_toolkit/nncf/examples/common/models/classification/rmnet_cifar.py,0,"b""from collections import OrderedDict\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RMBlock(nn.Module):\n    def __init__(self, input_planes, squeeze_planes, output_planes, downsample=False, dropout_ratio=0.1,\n                 activation=nn.ELU):\n        super(RMBlock, self).__init__()\n        self.downsample = downsample\n        self.input_planes = input_planes\n        self.output_planes = output_planes\n\n        self.squeeze_conv = nn.Conv2d(input_planes, squeeze_planes, kernel_size=1, bias=False)\n        self.squeeze_bn = nn.BatchNorm2d(squeeze_planes)\n\n        self.dw_conv = nn.Conv2d(squeeze_planes, squeeze_planes, groups=squeeze_planes, kernel_size=3, padding=1,\n                                 stride=2 if downsample else 1, bias=False)\n        self.dw_bn = nn.BatchNorm2d(squeeze_planes)\n\n        self.expand_conv = nn.Conv2d(squeeze_planes, output_planes, kernel_size=1, bias=False)\n        self.expand_bn = nn.BatchNorm2d(output_planes)\n\n        self.activation = activation(inplace=True)\n        self.dropout_ratio = dropout_ratio\n\n        if self.downsample:\n            self.skip_conv = nn.Conv2d(input_planes, output_planes, kernel_size=1, bias=False)\n            self.skip_conv_bn = nn.BatchNorm2d(output_planes)\n\n        self.init_weights()\n\n    def init_weights(self):\n        for m in self.children():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        residual = x\n        out = self.activation(self.squeeze_bn(self.squeeze_conv(x)))\n        out = self.activation(self.dw_bn(self.dw_conv(out)))\n        out = self.expand_bn(self.expand_conv(out))\n        if self.dropout_ratio > 0:\n            out = F.dropout(out, p=self.dropout_ratio, training=self.training, inplace=True)\n        if self.downsample:\n            residual = F.max_pool2d(x, kernel_size=2, stride=2, padding=0)\n            residual = self.skip_conv(residual)\n            residual = self.skip_conv_bn(residual)\n        out += residual\n        return self.activation(out)\n\n\nclass RMNetBody(nn.Module):\n    def __init__(self, block=RMBlock, blocks_per_stage=(None, 4, 8, 10, 11), trunk_width=(32, 32, 64, 128, 256),\n                 bottleneck_width=(None, 8, 16, 32, 64)):\n        super(RMNetBody, self).__init__()\n        assert len(blocks_per_stage) == len(trunk_width) == len(bottleneck_width)\n        self.dim_out = trunk_width[-1]\n\n        stages = [nn.Sequential(OrderedDict([\n            ('data_bn', nn.BatchNorm2d(3)),\n            ('conv1', nn.Conv2d(3, trunk_width[0], kernel_size=3, stride=2, padding=1, bias=False)),\n            ('bn1', nn.BatchNorm2d(trunk_width[0])),\n            ('relu1', nn.ReLU(inplace=True))\n        ]))]\n\n        for i, (blocks_num, w, wb) in enumerate(zip(blocks_per_stage, trunk_width, bottleneck_width)):\n            # Zeroth stage is already added.\n            if i == 0:\n                continue\n            stage = []\n            # Do not downscale input to the first stage.\n            if i > 1:\n                stage.append(block(trunk_width[i - 1], wb, w, downsample=True))\n            for _ in range(blocks_num):\n                stage.append(block(w, wb, w))\n            stages.append(nn.Sequential(*stage))\n\n        self.stages = nn.Sequential(OrderedDict([\n            ('stage_{}'.format(i), stage) for i, stage in enumerate(stages)\n        ]))\n\n        self.init_weights()\n\n    def init_weights(self):\n        m = self.stages[0][0]  # ['data_bn']\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n        m = self.stages[0][1]  # ['conv1']\n        nn.init.kaiming_normal_(m.weight, mode='fan_out')\n        m = self.stages[0][2]  # ['bn1']\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n        # All other blocks should be initialized internally during instantiation.\n\n    def forward(self, x):\n        return self.stages(x)\n\n\nclass RMNetClassifierCifar(nn.Module):\n    def __init__(self, num_classes, pretrained=False, body=RMNetBody, dropout_ratio=0.1):\n        super(RMNetClassifierCifar, self).__init__()\n        self.dropout_ratio = dropout_ratio\n        self.backbone = body()\n        self.extra_conv = nn.Conv2d(256, 512, 3, stride=2, padding=1, bias=False)\n        self.extra_conv_bn = nn.BatchNorm2d(512)\n        self.extra_conv_2 = nn.Conv2d(512, 1024, 3, stride=2, padding=1, bias=False)\n        self.extra_conv_2_bn = nn.BatchNorm2d(1024)\n        self.fc = nn.Conv2d(1024, num_classes, 1, stride=1, padding=0)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = F.elu(self.extra_conv_bn(self.extra_conv(x)))\n        x = F.relu(self.extra_conv_2_bn(self.extra_conv_2(x)))\n        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n        x = self.fc(x)\n        x = x.view(-1, x.size(1))\n        return x\n"""
pytorch_toolkit/nncf/examples/common/models/classification/squeezenet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n# SqueezeNet implementation from:\n# torchvision/models/squeezenet.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.utils.model_zoo as model_zoo\n\nmodel_urls = {\n    \'squeezenet1_0\': \'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\',\n    \'squeezenet1_1\': \'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n}\n\n\nclass Fire(nn.Module):\n\n    def __init__(self, inplanes, squeeze_planes,\n                 expand1x1_planes, expand3x3_planes):\n        super(Fire, self).__init__()\n        self.inplanes = inplanes\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n                                   kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n                                   kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n\nclass SqueezeNet(nn.Module):\n\n    def __init__(self, version=1.0, num_classes=1000, dropout=0.5):\n        super(SqueezeNet, self).__init__()\n        if version not in [1.0, 1.1]:\n            raise ValueError(""Unsupported SqueezeNet version {version}:""\n                             ""1.0 or 1.1 expected"".format(version=version))\n        self.num_classes = num_classes\n        if version == 1.0:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                Fire(128, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(256, 32, 128, 128),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(512, 64, 256, 256),\n            )\n        else:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256),\n            )\n        # Final convolution is initialized differently form the rest\n        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout),\n            final_conv,\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m is final_conv:\n                    init.normal_(m.weight, mean=0.0, std=0.01)\n                else:\n                    init.kaiming_uniform_(m.weight)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x.view(x.size(0), self.num_classes)\n\n\ndef squeezenet1_0_custom(pretrained=False, **kwargs):\n    r""""""SqueezeNet model architecture from the `""SqueezeNet: AlexNet-level\n    accuracy with 50x fewer parameters and <0.5MB model size""\n    <https://arxiv.org/abs/1602.07360>`_ paper.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = SqueezeNet(version=1.0, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'squeezenet1_0\']))\n    return model\n\n\ndef squeezenet1_1_custom(pretrained=False, **kwargs):\n    r""""""SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on ImageNet\n    """"""\n    model = SqueezeNet(version=1.1, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'squeezenet1_1\']))\n    return model\n'"
pytorch_toolkit/nncf/examples/common/models/segmentation/__init__.py,0,b'from .enet import *\nfrom .icnet import *\nfrom .unet import *\n'
pytorch_toolkit/nncf/examples/common/models/segmentation/enet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n# ENet implementation from:\n# https://github.com/davidtvs/PyTorch-ENet\n\nimport torch.nn as nn\nimport torch\n\nfrom examples.common.example_logger import logger\n\nclass InitialBlock(nn.Module):\n    """"""The initial block is composed of two branches:\n    1. a main branch which performs a regular convolution with stride 2;\n    2. an extension branch which performs max-pooling.\n\n    Doing both operations in parallel and concatenating their results\n    allows for efficient downsampling and expansion. The main branch\n    outputs 13 feature maps while the extension branch outputs 3, for a\n    total of 16 feature maps after concatenation.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number output channels.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer. Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 padding=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - As stated above the number of output channels for this\n        # branch is the total minus 3, since the remaining channels come from\n        # the extension branch\n        self.main_branch = nn.Conv2d(\n            in_channels,\n            out_channels - 3,\n            kernel_size=kernel_size,\n            stride=2,\n            padding=padding,\n            bias=bias)\n\n        # Extension branch\n        self.ext_branch = nn.MaxPool2d(kernel_size, stride=2, padding=padding)\n\n        # Initialize batch normalization to be used after concatenation\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        main = self.main_branch(x)\n        ext = self.ext_branch(x)\n\n        # Concatenate branches\n        out = torch.cat((main, ext), 1)\n\n        # Apply batch normalization\n        out = self.batch_norm(out)\n\n        return self.out_prelu(out)\n\n\nclass RegularBottleneck(nn.Module):\n    """"""Regular bottlenecks are the main building block of ENet.\n    Main branch:\n    1. Shortcut connection.\n\n    Extension branch:\n    1. 1x1 convolution which decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. regular, dilated or asymmetric convolution;\n    3. 1x1 convolution which increases the number of channels back to\n    ``channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - channels (int): the number of input and output channels.\n    - internal_ratio (int, optional): a scale factor applied to\n    ``channels`` used to compute the number of\n    channels after the projection. eg. given ``channels`` equal to 128 and\n    internal_ratio equal to 2 the number of channels after the projection\n    is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer described above in item 2 of the extension\n    branch. Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - dilation (int, optional): spacing between kernel elements for the\n    convolution described in item 2 of the extension branch. Default: 1.\n    asymmetric (bool, optional): flags if the convolution described in\n    item 2 of the extension branch is asymmetric or not. Default: False.\n    - dropout_prob (float, optional): probability of an element to be\n    zeroed. Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dilation=1,\n                 asymmetric=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}.""\n                               .format(channels, internal_ratio))\n\n        internal_channels = channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - shortcut connection\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution, and,\n        # finally, a regularizer (spatial dropout). Number of channels is constant.\n\n        # 1x1 projection convolution\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                channels,\n                internal_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # If the convolution is asymmetric we split the main convolution in\n        # two. Eg. for a 5x5 asymmetric convolution we have two convolution:\n        # the first is 5x1 and the second is 1x5.\n        if asymmetric:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(kernel_size, 1),\n                    stride=1,\n                    padding=(padding, 0),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation,\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(1, kernel_size),\n                    stride=1,\n                    padding=(0, padding),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation)\n        else:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=kernel_size,\n                    stride=1,\n                    padding=padding,\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after adding the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        # Main branch shortcut\n        main = x\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out)\n\n\nclass DownsamplingBottleneck(nn.Module):\n    """"""Downsampling bottlenecks further downsample the feature map size.\n\n    Main branch:\n    1. max pooling with stride 2; indices are saved to be used for\n    unpooling later.\n\n    Extension branch:\n    1. 2x2 convolution with stride 2 that decreases the number of channels\n    by ``internal_ratio``, also called a projection;\n    2. regular convolution (by default, 3x3);\n    3. 1x1 convolution which increases the number of channels to\n    ``out_channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number of output channels.\n    - internal_ratio (int, optional): a scale factor applied to ``channels``\n    used to compute the number of channels after the projection. eg. given\n    ``channels`` equal to 128 and internal_ratio equal to 2 the number of\n    channels after the projection is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer described above in item 2 of the extension branch.\n    Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - dilation (int, optional): spacing between kernel elements for the\n    convolution described in item 2 of the extension branch. Default: 1.\n    - asymmetric (bool, optional): flags if the convolution described in\n    item 2 of the extension branch is asymmetric or not. Default: False.\n    - return_indices (bool, optional):  if ``True``, will return the max\n    indices along with the outputs. Useful when unpooling later.\n    - dropout_prob (float, optional): probability of an element to be\n    zeroed. Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 return_indices=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Store parameters that are needed later\n        self.return_indices = return_indices\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}. ""\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_max1 = nn.MaxPool2d(\n            kernel_size,\n            stride=2,\n            padding=padding,\n            return_indices=return_indices)\n\n        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 2x2 projection convolution with stride 2\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                internal_channels,\n                kernel_size=2,\n                stride=2,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # Convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=padding,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                out_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(out_channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        # Main branch shortcut\n        if self.return_indices:\n            main, max_indices = self.main_max1(x)\n        else:\n            main = self.main_max1(x)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Main branch channel padding\n        n, ch_ext, h, w = ext.size()\n        ch_main = main.size()[1]\n        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n\n        # Before concatenating, check if main is on the CPU or GPU and\n        # convert padding accordingly\n        if main.is_cuda:\n            padding = padding.cuda()\n\n        # Concatenate\n        main = torch.cat((main, padding), 1)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out), max_indices\n\n\nclass UpsamplingBottleneck(nn.Module):\n    """"""The upsampling bottlenecks upsample the feature map resolution using max\n    pooling indices stored from the corresponding downsampling bottleneck.\n\n    Main branch:\n    1. 1x1 convolution with stride 1 that decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. max unpool layer using the max pool indices from the corresponding\n    downsampling max pool layer.\n\n    Extension branch:\n    1. 1x1 convolution with stride 1 that decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. transposed convolution (by default, 3x3);\n    3. 1x1 convolution which increases the number of channels to\n    ``out_channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number of output channels.\n    - internal_ratio (int, optional): a scale factor applied to ``in_channels``\n     used to compute the number of channels after the projection. eg. given\n     ``in_channels`` equal to 128 and ``internal_ratio`` equal to 2 the number\n     of channels after the projection is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in the\n    convolution layer described above in item 2 of the extension branch.\n    Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the input.\n    Default: 0.\n    - dropout_prob (float, optional): probability of an element to be zeroed.\n    Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if ``True``.\n    Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}. ""\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels))\n\n        # Remember that the stride is the same as the kernel_size, just like\n        # the max pooling layers\n        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 1x1 projection convolution with stride 1\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels, internal_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(internal_channels), activation)\n\n        # Transposed convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.ConvTranspose2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=kernel_size,\n                stride=2,\n                padding=padding,\n                output_padding=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x, max_indices):\n        # Main branch shortcut\n        main = self.main_conv1(x)\n        main = self.main_unpool1(main, max_indices)\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out)\n\n\nclass ENet(nn.Module):\n    """"""Generate the ENet model.\n\n    Keyword arguments:\n    - num_classes (int): the number of classes to segment.\n    - encoder_relu (bool, optional): When ``True`` ReLU is used as the\n    activation function in the encoder blocks/layers; otherwise, PReLU\n    is used. Default: False.\n    - decoder_relu (bool, optional): When ``True`` ReLU is used as the\n    activation function in the decoder blocks/layers; otherwise, PReLU\n    is used. Default: True.\n\n    """"""\n\n    def __init__(self, num_classes, encoder_relu=False, decoder_relu=True):\n        super().__init__()\n\n        self.initial_block = InitialBlock(3, 16, padding=1, relu=encoder_relu)\n\n        # Stage 1 - Encoder\n        self.downsample1_0 = DownsamplingBottleneck(\n            16,\n            64,\n            padding=1,\n            return_indices=True,\n            dropout_prob=0.01,\n            relu=encoder_relu)\n        self.regular1_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_3 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_4 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n\n        # Stage 2 - Encoder\n        self.downsample2_0 = DownsamplingBottleneck(\n            64,\n            128,\n            padding=1,\n            return_indices=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.regular2_1 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_2 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_3 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_4 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular2_5 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_6 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_7 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_8 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 3 - Encoder\n        self.regular3_0 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_1 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_2 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_3 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular3_4 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_5 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_6 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_7 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 4 - Decoder\n        self.upsample4_0 = UpsamplingBottleneck(\n            128, 64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n\n        # Stage 5 - Decoder\n        self.upsample5_0 = UpsamplingBottleneck(\n            64, 16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular5_1 = RegularBottleneck(\n            16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.transposed_conv = nn.ConvTranspose2d(\n            16,\n            num_classes,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            output_padding=1,\n            bias=False)\n\n    def forward(self, x):\n        # Initial block\n        x = self.initial_block(x)\n\n        # Stage 1 - Encoder\n        x, max_indices1_0 = self.downsample1_0(x)\n        x = self.regular1_1(x)\n        x = self.regular1_2(x)\n        x = self.regular1_3(x)\n        x = self.regular1_4(x)\n\n        # Stage 2 - Encoder\n        x, max_indices2_0 = self.downsample2_0(x)\n        x = self.regular2_1(x)\n        x = self.dilated2_2(x)\n        x = self.asymmetric2_3(x)\n        x = self.dilated2_4(x)\n        x = self.regular2_5(x)\n        x = self.dilated2_6(x)\n        x = self.asymmetric2_7(x)\n        x = self.dilated2_8(x)\n\n        # Stage 3 - Encoder\n        x = self.regular3_0(x)\n        x = self.dilated3_1(x)\n        x = self.asymmetric3_2(x)\n        x = self.dilated3_3(x)\n        x = self.regular3_4(x)\n        x = self.dilated3_5(x)\n        x = self.asymmetric3_6(x)\n        x = self.dilated3_7(x)\n\n        # Stage 4 - Decoder\n        x = self.upsample4_0(x, max_indices2_0)\n        x = self.regular4_1(x)\n        x = self.regular4_2(x)\n\n        # Stage 5 - Decoder\n        x = self.upsample5_0(x, max_indices1_0)\n        x = self.regular5_1(x)\n        x = self.transposed_conv(x)\n\n        return x\n\ndef enet(num_classes, pretrained=False, **kwargs):\n    model = ENet(num_classes, **kwargs)\n\n    if pretrained:\n        logger.warning(""ENet has no pretrained weights"")\n\n    return model\n'"
pytorch_toolkit/nncf/examples/common/models/segmentation/icnet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n# ICNet implementation attempting to closely follow the original authors\' model at:\n# https://github.com/hszhao/ICNet\n# Important differences:\n# 1) Upsampling is nearest-neighbour instead of bilinear since it is impossible\n#    to export bilinear upsampling to ONNX yet\n# 2) Weight initialization is omitted because it caused mIoU degradation on CamVid\n\nfrom collections import OrderedDict\nfrom pkg_resources import parse_version\n\nfrom numpy import lcm\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nfrom examples.common.example_logger import logger\nfrom nncf.utils import is_tracing_state\n\n\nclass ConvBN(nn.Module):\n    def __init__(self, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n        self.bn = nn.BatchNorm2d(out_channels, momentum=0.05)  # Corresponds to momentum 0.95 in Caffe notation\n\n    def forward(self, inputs):\n        x = self.conv(inputs)\n        x = self.bn(x)\n        return x\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):\n        super().__init__()\n        self.convbn = ConvBN(in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, inputs):\n        x = self.convbn(inputs)\n        x = self.relu(x)\n        return x\n\n\nclass ResNetBlock(nn.Module):\n    def __init__(self, in_channels, reduce_channels, increase_channels, dilation=1, stride=1):\n        super().__init__()\n        nonshrinking_padding = dilation\n        self.conv_1x1_reduce_bnrelu = ConvBNReLU(in_channels, out_channels=reduce_channels,\n                                                 kernel_size=1, stride=stride, padding=0, dilation=1, bias=False)\n        self.conv_3x3_bnrelu = ConvBNReLU(in_channels=reduce_channels, out_channels=reduce_channels,\n                                          kernel_size=3, stride=1, padding=nonshrinking_padding,\n                                          dilation=dilation, bias=False)\n        self.conv_1x1_increase_bn = ConvBN(in_channels=reduce_channels, out_channels=increase_channels,\n                                           kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n        self.need_proj = (in_channels != increase_channels)\n        if self.need_proj:\n            self.conv_1x1_proj_bn = ConvBN(in_channels, out_channels=increase_channels,\n                                           kernel_size=1, stride=stride, padding=0, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, inputs):\n        fx = self.conv_1x1_reduce_bnrelu(inputs)\n        fx = self.conv_3x3_bnrelu(fx)\n        fx = self.conv_1x1_increase_bn(fx)\n        x = inputs\n        if self.need_proj:\n            x = self.conv_1x1_proj_bn(x)\n        out = fx + x\n        out = self.relu(out)\n        return out\n\n\nclass ICNetBackbone(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        # Naming conventions below are chosen to correspond to the\n        # icnet_cityscapes_bnnomerge.prototxt file in the original ICNet Github\n        # repository. Although ICNet low-resolution branch layers \'conv3\', \'conv4\' and \'conv5\',\n        # are based upon ResNet50, they rather correspond to ResNet50 layers\n        # \'conv2\', \'conv3\' and \'conv4\' respectively.\n\n        self.conv1 = nn.Sequential(OrderedDict([\n            (\'conv1_1_3x3_s2\', ConvBNReLU(in_channels, out_channels=32, kernel_size=3,\n                                          stride=2, padding=1, dilation=1, bias=False)),\n            (\'conv1_2_3x3\', ConvBNReLU(in_channels=32, out_channels=32, kernel_size=3,\n                                       stride=1, padding=1, dilation=1, bias=False)),\n            (\'conv1_3_3x3\', ConvBNReLU(in_channels=32, out_channels=64, kernel_size=3,\n                                       stride=1, padding=1, dilation=1, bias=False)),\n            ]))\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Sequential(OrderedDict([\n            (\'conv2_1\', ResNetBlock(64, 32, 128)),\n            (\'conv2_2\', ResNetBlock(128, 32, 128)),\n            (\'conv2_3\', ResNetBlock(128, 32, 128))\n            ]))\n        self.conv3_1 = ResNetBlock(128, 64, 256, stride=2)\n        self.conv3_rest = nn.Sequential(OrderedDict([\n            (\'conv3_2\', ResNetBlock(256, 64, 256)),\n            (\'conv3_3\', ResNetBlock(256, 64, 256)),\n            (\'conv3_4\', ResNetBlock(256, 64, 256))\n            ]))\n        self.conv4 = nn.Sequential(OrderedDict([\n            (\'conv4_1\', ResNetBlock(256, 128, 512, dilation=2)),\n            (\'conv4_2\', ResNetBlock(512, 128, 512, dilation=2)),\n            (\'conv4_3\', ResNetBlock(512, 128, 512, dilation=2)),\n            (\'conv4_4\', ResNetBlock(512, 128, 512, dilation=2)),\n            (\'conv4_5\', ResNetBlock(512, 128, 512, dilation=2)),\n            (\'conv4_6\', ResNetBlock(512, 128, 512, dilation=2)),\n        ]))\n        self.conv4 = nn.Sequential(OrderedDict([\n            (\'conv4_1\', ResNetBlock(256, 128, 512, dilation=2)),\n            (\'conv4_2\', ResNetBlock(512, 128, 512, dilation=2)),\n            (\'conv4_3\', ResNetBlock(512, 128, 512, dilation=2)),\n            (\'conv4_4\', ResNetBlock(512, 128, 512, dilation=2)),\n            (\'conv4_5\', ResNetBlock(512, 128, 512, dilation=2)),\n            (\'conv4_6\', ResNetBlock(512, 128, 512, dilation=2)),\n        ]))\n        self.conv5 = nn.Sequential(OrderedDict([\n            (\'conv5_1\', ResNetBlock(512, 256, 1024, dilation=4)),\n            (\'conv5_2\', ResNetBlock(1024, 256, 1024, dilation=4)),\n            (\'conv5_3\', ResNetBlock(1024, 256, 1024, dilation=4)),\n        ]))\n\n    def forward(self):\n        pass\n\n\ndef get_backbone(backbone, in_channels):\n    if backbone == \'icnet\':\n        return ICNetBackbone(in_channels)\n    raise NotImplementedError\n\n\nclass PyramidPooling(nn.Module):\n    def __init__(self, input_size_hw, bin_dimensions=None, mode=\'sum\'):\n        super().__init__()\n\n        if mode not in [\'sum\', \'cat\']:\n            raise NotImplementedError\n\n        self.mode = mode\n        self.input_size_hw = input_size_hw\n        #self.sampling_params = {\'mode\': \'bilinear\', \'align_corners\': True}\n        self.sampling_params = {\'mode\': \'nearest\'}\n        if bin_dimensions is None:\n            self.bin_dimensions = [1, 2, 3, 6]\n        else:\n            self.bin_dimensions = bin_dimensions\n\n        # ONNX only supports exporting adaptive_avg_pool2d if the input tensor\n        # height and width are exact multiples of the output_size (i.e. bin dimensions).\n        # Inference-time pad calculation is also impossible to export to ONNX, therefore\n        # the required padding parameters are pre-calculated here, at init.\n        self.paddings = {}\n        for dim in self.bin_dimensions:\n            pad_h = (dim - (input_size_hw[0] % dim)) % dim\n            pad_w = (dim - (input_size_hw[1] % dim)) % dim\n            self.paddings[dim] = [0, pad_w, 0, pad_h]\n\n    def forward(self, inputs):\n        x = inputs.clone()\n\n        for dim in self.bin_dimensions:\n            padded_input = F.pad(inputs, self.paddings[dim], mode=\'constant\', value=0)\n            pooled_feature = F.adaptive_avg_pool2d(padded_input, dim)\n            pooled_feature = F.interpolate(pooled_feature, self.input_size_hw, **self.sampling_params)\n            if self.mode == \'sum\':\n                x += pooled_feature\n            elif self.mode == \'cat\':\n                x = torch.cat(pooled_feature)\n            else:\n                raise NotImplementedError\n\n        return x\n\n\nclass CascadeFeatureFusion(nn.Module):\n    def __init__(self, in_channels_lowres, in_channels_highres, highres_size_hw, num_classes):\n        super().__init__()\n        #self.sampling_params = {\'mode\': \'bilinear\', \'align_corners\': True}\n        self.sampling_params = {\'mode\': \'nearest\'}\n\n        self.conv = ConvBN(in_channels_lowres, out_channels=128,\n                           kernel_size=3, padding=2, dilation=2, bias=False)\n        self.conv_proj = ConvBN(in_channels_highres, out_channels=128,\n                                kernel_size=1, padding=0, dilation=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.classifier = nn.Conv2d(in_channels_lowres, out_channels=num_classes,\n                                    kernel_size=1, padding=0, dilation=1, bias=True)\n        self.highres_size_hw = highres_size_hw\n\n    def forward(self, lowres_input, highres_input):\n        upsampled = F.interpolate(lowres_input, self.highres_size_hw, **self.sampling_params)\n        lr = self.conv(upsampled)\n        hr = self.conv_proj(highres_input)\n        x = lr + hr\n        x = self.relu(x)\n        if self.training:\n            aux_labels = self.classifier(upsampled)\n            return x, aux_labels\n        return x\n\n\nclass ICNet(nn.Module):\n    def __init__(\n            self,\n            input_size_hw,\n            in_channels=3,\n            n_classes=20,\n            backbone=\'icnet\'\n    ):\n        super().__init__()\n        self._input_size_hw = input_size_hw\n\n        self._input_size_hw_ds2 = (self._input_size_hw[0] // 2, self._input_size_hw[1] // 2)\n        self._input_size_hw_ds4 = (self._input_size_hw[0] // 4, self._input_size_hw[1] // 4)\n        self._input_size_hw_ds8 = (self._input_size_hw[0] // 8, self._input_size_hw[1] // 8)\n        self._input_size_hw_ds16 = (self._input_size_hw[0] // 16, self._input_size_hw[1] // 16)\n        self._input_size_hw_ds32 = (self._input_size_hw[0] // 32, self._input_size_hw[1] // 32)\n\n\n        #self.sampling_params = {\'mode\': \'bilinear\', \'align_corners\': True}\n        self.sampling_params = {\'mode\': \'nearest\'}\n\n\n        self.backbone = get_backbone(backbone, in_channels)\n\n        self.highres_conv = nn.Sequential(OrderedDict([\n            (\'conv1_sub1\', ConvBNReLU(in_channels, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False)),\n            (\'conv2_sub1\', ConvBNReLU(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False)),\n            (\'conv3_sub1\', ConvBNReLU(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False))\n        ]))\n\n        # \'conv5_4_k1\' is applied immediately after pyramid pooling and before\n        # cascade feature fusion\n        self.conv5_4_k1 = ConvBNReLU(in_channels=1024, out_channels=256,\n                                     kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n\n        # Using pyramid pooling in \'sum\' mode instead of \'cat\' as in PSPNet,\n        # probably because in ICNet it is immediately followed by 1x1 reduce\n        # convolution anyway\n        self.ppm = PyramidPooling(self._input_size_hw_ds32)\n        self.cff42 = CascadeFeatureFusion(in_channels_lowres=256, in_channels_highres=256,\n                                          highres_size_hw=self._input_size_hw_ds16, num_classes=n_classes)\n        self.cff421 = CascadeFeatureFusion(in_channels_lowres=128, in_channels_highres=32,\n                                           highres_size_hw=self._input_size_hw_ds8, num_classes=n_classes)\n        self.conv6_cls = nn.Conv2d(128, out_channels=n_classes,\n                                   kernel_size=1, padding=0, dilation=1, bias=True)\n\n\n        required_alignment = 32\n        for bin_dim in self.ppm.bin_dimensions:\n            required_alignment = lcm(required_alignment, bin_dim)\n        if (input_size_hw[0] % required_alignment) or (input_size_hw[1] % required_alignment):\n            raise ValueError(""ICNet may only operate on {}-aligned input resolutions"".format(required_alignment))\n        # Weight initialization\n        # for module in self.modules():\n        #     if isinstance(module, nn.Conv2d):\n        #         nn.init.kaiming_normal_(module.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n        #         if module.bias is not None:\n        #             module.bias.data.zero_()\n        #     elif isinstance(module, nn.BatchNorm2d):\n        #         nn.init.constant_(module.weight, 1)\n        #         nn.init.constant_(module.bias, 0)\n\n    def highres_branch(self, inputs):\n        x = self.highres_conv(inputs)\n        return x\n\n    def mediumres_branch(self, inputs):\n        x = self.backbone.conv1(inputs)\n        x = self.backbone.maxpool(x)\n        x = self.backbone.conv2(x)\n        x = self.backbone.conv3_1(x)\n        return x\n\n    def lowres_branch(self, inputs):\n        x = self.backbone.conv3_rest(inputs)\n        x = self.backbone.conv4(x)\n        x = self.backbone.conv5(x)\n        x = self.ppm(x)\n        x = self.conv5_4_k1(x)\n        return x\n\n    def forward(self, inputs):\n        data_sub1 = inputs\n        features_sub1 = self.highres_branch(data_sub1)\n\n        data_sub2 = F.interpolate(data_sub1, self._input_size_hw_ds2, **self.sampling_params)\n        features_sub2 = self.mediumres_branch(data_sub2)\n\n        # Contrary to the ICNet paper Fig.2 , the low-resolution branch does not receive separate\n        # 4x-downsampled image input, but instead reuses feature maps from the medium-resolution\n        # branch.\n\n        data_sub4 = F.interpolate(features_sub2, self._input_size_hw_ds32, **self.sampling_params)\n        features_sub4 = self.lowres_branch(data_sub4)\n\n        if self.training:\n            fused_features_sub42, label_scores_ds16 = self.cff42(features_sub4, features_sub2)\n            fused_features_sub421, label_scores_ds8 = self.cff421(fused_features_sub42, features_sub1)\n\n            fused_features_ds4 = F.interpolate(fused_features_sub421, self._input_size_hw_ds4, **self.sampling_params)\n            label_scores_ds4 = self.conv6_cls(fused_features_ds4)\n\n            return OrderedDict([(""ds4"", label_scores_ds4),\n                                (""ds8"", label_scores_ds8),\n                                (""ds16"", label_scores_ds16)])\n\n        fused_features_sub42 = self.cff42(features_sub4, features_sub2)\n        fused_features_sub421 = self.cff421(fused_features_sub42, features_sub1)\n\n        fused_features_ds4 = F.interpolate(fused_features_sub421, self._input_size_hw_ds4, **self.sampling_params)\n        label_scores_ds4 = self.conv6_cls(fused_features_ds4)\n        label_scores = F.interpolate(label_scores_ds4, self._input_size_hw, **self.sampling_params)\n        if is_tracing_state() and parse_version(torch.__version__) >= parse_version(""1.1.0""):\n            # While exporting, add extra post-processing layers into the graph\n            # so that the model outputs class probabilities instead of class scores\n            softmaxed = F.softmax(label_scores, dim=1)\n            return softmaxed\n        return label_scores\n\n\ndef icnet(num_classes, pretrained=False, **kwargs):\n    model = ICNet(n_classes=num_classes, **kwargs)\n\n    if pretrained:\n        logger.warning(""ICNet has no pretrained weights"")\n\n    return model\n'"
pytorch_toolkit/nncf/examples/common/models/segmentation/unet.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\n# UNet implementation from:\n# https://github.com/jvanvugt/pytorch-unet\n\nfrom pkg_resources import parse_version\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nfrom nncf.utils import is_tracing_state\n\nfrom examples.common.example_logger import logger\n\nclass UNet(nn.Module):\n    def __init__(\n            self,\n            input_size_hw,\n            in_channels=3,\n            n_classes=2,\n            depth=5,\n            wf=6,\n            padding=True,\n            batch_norm=True,\n            up_mode=\'upconv\',\n    ):\n        """"""\n        Implementation of\n        U-Net: Convolutional Networks for Biomedical Image Segmentation\n        (Ronneberger et al., 2015)\n        https://arxiv.org/abs/1505.04597\n        Args:\n            in_channels (int): number of input channels\n            input_size_hw: a tuple of (height, width) of the input images\n            n_classes (int): number of output channels\n            depth (int): depth of the network\n            wf (int): number of filters in the first layer is 2**wf\n            padding (bool): if True, apply padding such that the input shape\n                            is the same as the output.\n                            This may introduce artifacts\n            batch_norm (bool): Use BatchNorm prior to layers with an\n                               activation function\n            up_mode (str): one of \'upconv\' or \'upsample\'.\n                           \'upconv\' will use transposed convolutions for\n                           learned upsampling.\n                           \'upsample\' will use bilinear upsampling.\n        """"""\n        super(UNet, self).__init__()\n        assert up_mode in (\'upconv\', \'upsample\')\n        if (input_size_hw[0] % 2**(depth - 1)) or (input_size_hw[1] % 2**(depth - 1)):\n            raise ValueError(""UNet may only operate on input resolutions aligned to 2**(depth - 1)"")\n        self.padding = padding\n        self.depth = depth\n        prev_channels = in_channels\n        self.down_path = nn.ModuleList()\n        for i in range(depth):\n            self.down_path.append(\n                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n            )\n            prev_channels = 2 ** (wf + i)\n\n        self.up_path = nn.ModuleList()\n        for i in reversed(range(depth - 1)):\n            self.up_path.append(\n                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n            )\n            prev_channels = 2 ** (wf + i)\n\n        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n\n    def forward(self, x):\n        blocks = []\n        for i, down in enumerate(self.down_path):\n            x = down(x)\n            if i != len(self.down_path) - 1:\n                blocks.append(x)\n                x = F.max_pool2d(x, 2)\n\n        for i, up in enumerate(self.up_path):\n            x = up(x, blocks[-i - 1])\n\n        x = self.last(x)\n        if is_tracing_state() and parse_version(torch.__version__) >= parse_version(""1.1.0""):\n            # While exporting, add extra post-processing layers into the graph\n            # so that the model outputs class probabilities instead of class scores\n            softmaxed = F.softmax(x, dim=1)\n            return softmaxed\n        return x\n\n\nclass UNetConvBlock(nn.Module):\n    def __init__(self, in_size, out_size, padding, batch_norm):\n        super(UNetConvBlock, self).__init__()\n        block = []\n\n        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n        block.append(nn.ReLU())\n\n        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n        block.append(nn.ReLU())\n\n        self.block = nn.Sequential(*block)\n\n    def forward(self, x):\n        out = self.block(x)\n        return out\n\n\ndef center_crop(layer, target_size):\n    if layer.dim() == 4:\n        # Cropping feature maps\n        _, _, layer_height, layer_width = layer.size()\n        diff_y = (layer_height - target_size[0]) // 2\n        diff_x = (layer_width - target_size[1]) // 2\n        return layer[\n            :, :, diff_y: (diff_y + target_size[0]), diff_x: (diff_x + target_size[1])\n            ]\n\n    # If dimension is not 4, assume that we are cropping ground truth labels\n    assert layer.dim() == 3\n    _, layer_height, layer_width = layer.size()\n    diff_y = (layer_height - target_size[0]) // 2\n    diff_x = (layer_width - target_size[1]) // 2\n    return layer[\n        :, diff_y: (diff_y + target_size[0]), diff_x: (diff_x + target_size[1])\n        ]\n\n\nclass UNetUpBlock(nn.Module):\n    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n        super(UNetUpBlock, self).__init__()\n        if up_mode == \'upconv\':\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n        elif up_mode == \'upsample\':\n            self.up = nn.Sequential(\n                nn.Upsample(mode=\'bilinear\', scale_factor=2),\n                nn.Conv2d(in_size, out_size, kernel_size=1),\n            )\n        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n        self.padding = padding\n\n    def forward(self, x, bridge):\n        up = self.up(x)\n        if self.padding:\n            out = torch.cat([up, bridge], 1)\n        else:\n            crop1 = center_crop(bridge, up.shape[2:])\n            out = torch.cat([up, crop1], 1)\n        out = self.conv_block(out)\n\n        return out\n\n\ndef unet(num_classes, pretrained=False, **kwargs):\n    model = UNet(n_classes=num_classes, **kwargs)\n\n    if pretrained:\n        logger.warning(""UNet has no pretrained weights"")\n\n    return model\n'"
pytorch_toolkit/nncf/examples/object_detection/layers/extensions/__init__.py,0,"b""import os.path\nfrom nncf.definitions import get_install_type\nfrom torch.utils.cpp_extension import load\n\next_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)))\nif get_install_type() == 'CPU':\n    EXTENSIONS = load(\n        'extensions', [\n            os.path.join(ext_dir, 'extensions.cpp'),\n            os.path.join(ext_dir, 'nms/nms_cpu.cpp'),\n            os.path.join(ext_dir, 'nms/nms_kernel.cpp'),\n        ],\n        verbose=False\n    )\nelse:\n    EXTENSIONS = load(\n        'extensions', [\n            os.path.join(ext_dir, 'extensions.cpp'),\n            os.path.join(ext_dir, 'nms/nms.cpp'),\n            os.path.join(ext_dir, 'nms/nms_kernel.cpp'),\n            os.path.join(ext_dir, 'nms/nms_kernel.cu'),\n        ],\n        verbose=False\n    )\n"""
pytorch_toolkit/nncf/examples/object_detection/layers/functions/__init__.py,0,"b""from .detection import DetectionOutput\nfrom .prior_box import PriorBox\n\n__all__ = ['DetectionOutput', 'PriorBox']\n"""
pytorch_toolkit/nncf/examples/object_detection/layers/functions/detection.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom torch import nn\n\nfrom nncf.utils import no_jit_trace\nfrom nncf.dynamic_graph.context import no_nncf_trace\n\nfrom ..box_utils import decode, nms\n\n\nclass DetectionOutput(nn.Module):\n    def __init__(self, num_classes, background_label_id, top_k, keep_top_k, confidence_threshold, nms_threshold,\n                 eta=1, share_location=1, code_type=\'CENTER_SIZE\', variance_encoded_in_target=0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.background_label_id = background_label_id\n        self.top_k = top_k\n        self.keep_top_k = keep_top_k\n        self.confidence_threshold = confidence_threshold\n        self.nms_threshold = nms_threshold\n        self.eta = eta\n        self.share_location = share_location\n        self.code_type = code_type\n        self.variance_encoded_in_target = variance_encoded_in_target\n\n    def forward(self, loc_data, conf_data, prior_data):\n        return DetectionOutputFunction.apply(loc_data, conf_data, prior_data, self)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        return grad_out\n\n\nclass DetectionOutputFunction(torch.autograd.Function):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations.\n    """"""\n\n    @staticmethod\n    def symbolic(g, loc_data, conf_data, prior_data, detection_output_params):\n        return g.op(""DetectionOutput"", loc_data, conf_data, prior_data,\n                    num_classes_i=detection_output_params.num_classes,\n                    background_label_id_i=detection_output_params.background_label_id,\n                    top_k_i=detection_output_params.top_k,\n                    keep_top_k_i=detection_output_params.keep_top_k,\n                    confidence_threshold_f=detection_output_params.confidence_threshold,\n                    nms_threshold_f=detection_output_params.nms_threshold, eta_f=detection_output_params.eta,\n                    share_location_i=detection_output_params.share_location,\n                    code_type_s=detection_output_params.code_type,\n                    variance_encoded_in_target_i=detection_output_params.variance_encoded_in_target)\n\n    @staticmethod\n    def forward(ctx, loc_data, conf_data, prior_data, detection_output_params):\n        """"""\n        Args:\n            loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch,num_priors*4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch,num_priors*num_classes]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [1,2,num_priors*4]\n        """"""\n        with no_jit_trace(), no_nncf_trace():\n            if detection_output_params.nms_threshold <= 0:\n                raise ValueError(\'nms_threshold must be non negative.\')\n            device = loc_data.device\n            batch_size = loc_data.size(0)  # batch size\n            num_priors = int(loc_data.size(1) / 4)\n            loc_data = loc_data.view(batch_size, num_priors, 4)\n            conf_data = conf_data.view(batch_size, num_priors, -1)\n            prior_data = prior_data.view(1, 2, num_priors, 4)\n            output = torch.zeros(batch_size, 1, detection_output_params.keep_top_k, 7).to(device)\n\n            conf_preds = conf_data.view(batch_size, num_priors,\n                                        detection_output_params.num_classes).transpose(2, 1)\n\n            # Decode predictions into bboxes.\n            for i in range(batch_size):\n                output_for_img = torch.zeros(0, 7).to(device)\n                decoded_boxes = decode(loc_data[i], prior_data[0])\n                # For each class, perform nms\n                conf_scores = conf_preds[i].clone()\n\n                total_detections_count = 0\n                all_indices = dict()  # indices of confident detections for each class\n                boxes = dict()\n                for cl in range(0, detection_output_params.num_classes):\n                    if cl == detection_output_params.background_label_id:\n                        continue\n                    c_mask = conf_scores[cl].gt(detection_output_params.confidence_threshold)\n                    scores = conf_scores[cl][c_mask]\n                    if scores.dim() == 0:\n                        continue\n                    conf_scores[cl, :scores.size()[0]] = scores\n                    conf_scores[cl, scores.size()[0]:] = 0\n                    l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n                    boxes[cl] = decoded_boxes[l_mask].view(-1, 4)\n                    # idx of highest scoring and non-overlapping boxes per class\n                    all_indices[cl], count = nms(boxes[cl], scores, detection_output_params.nms_threshold,\n                                                 detection_output_params.top_k)\n                    all_indices[cl] = all_indices[cl][:count]\n                    total_detections_count += count\n\n                score_index_pairs = list()  # list of tuples (score, label, idx)\n                for label, indices in all_indices.items():\n                    indices = indices.cpu().numpy()\n                    for idx in indices:\n                        score_index_pairs.append((conf_scores[label, idx], label, idx))\n\n                score_index_pairs.sort(key=lambda tup: tup[0], reverse=True)\n                score_index_pairs = score_index_pairs[:detection_output_params.keep_top_k]\n\n                all_indices_new = dict()\n                for _, label, idx in score_index_pairs:\n                    if label not in all_indices_new:\n                        all_indices_new[label] = [idx]\n                    else:\n                        all_indices_new[label].append(idx)\n\n                for label, indices in all_indices_new.items():\n                    out = torch.cat((\n                        torch.zeros((len(indices), 1), dtype=torch.float).new_full((len(indices), 1), i).to(device),\n                        torch.zeros((len(indices), 1), dtype=torch.float).new_full((len(indices), 1), label).to(device),\n                        conf_scores[label, indices].unsqueeze(1).to(device),\n                        boxes[label][indices].to(device)\n                    ), 1)\n                    output_for_img = torch.cat((output_for_img, out), 0)\n\n                output[i, 0, :output_for_img.size()[0]] = output_for_img\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        return grad_out\n'"
pytorch_toolkit/nncf/examples/object_detection/layers/functions/prior_box.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nfrom __future__ import division\n\nfrom itertools import product\nfrom math import sqrt\n\nimport torch\nfrom torch import nn\n\n\nclass PriorBox(nn.Module):\n    def __init__(self, min_size, max_size, aspect_ratio, flip, clip, variance, step, offset,\n                 step_h, step_w, img_size, img_h, img_w):\n        super().__init__()\n        self.min_size = min_size\n        self.max_size = max_size\n        self.aspect_ratio = aspect_ratio\n        self.flip = flip\n        self.clip = clip\n        self.variance = variance\n        self.step = step\n        self.offset = offset\n        self.step_h = step_h\n        self.step_w = step_w\n        self.img_size = img_size\n        self.img_h = img_h\n        self.img_w = img_w\n\n    def forward(self, input_fm, img_tensor):\n        return PriorBoxFunction.apply(input_fm, img_tensor, self)\n\n\nclass PriorBoxFunction(torch.autograd.Function):\n    """"""Compute priorbox coordinates in point form for each source\n    feature map.\n    """"""\n\n    @staticmethod\n    def symbolic(g, input_fm, img_tensor, priorbox_params):\n        return g.op(""PriorBox"", input_fm, img_tensor, min_size_f=[priorbox_params.min_size],\n                    max_size_f=[priorbox_params.max_size],\n                    aspect_ratio_f=priorbox_params.aspect_ratio, flip_i=priorbox_params.flip,\n                    clip_i=priorbox_params.clip,\n                    variance_f=priorbox_params.variance, step_f=priorbox_params.step,\n                    offset_f=priorbox_params.offset, step_h_f=priorbox_params.step_h, step_w_f=priorbox_params.step_w,\n                    img_size_i=priorbox_params.img_size, img_h_i=priorbox_params.img_h, img_w_i=priorbox_params.img_w)\n\n    @staticmethod\n    def forward(ctx, input_fm, img_tensor, priorbox_params):\n        for v in priorbox_params.variance:\n            if v <= 0:\n                raise ValueError(\'Variances must be greater than 0\')\n\n        mean = []\n        variance_channel = []\n        f_h = input_fm.size()[2]\n        f_w = input_fm.size()[3]\n        img_height = img_tensor.size()[2]\n        img_width = img_tensor.size()[3]\n\n        box_widths_heights = [(priorbox_params.min_size, priorbox_params.min_size),\n                              (sqrt(priorbox_params.min_size * priorbox_params.max_size),\n                               sqrt(priorbox_params.min_size * priorbox_params.max_size))]\n        for ar in priorbox_params.aspect_ratio:\n            box_widths_heights.append((priorbox_params.min_size * sqrt(ar), priorbox_params.min_size / sqrt(ar)))\n            if priorbox_params.flip:\n                box_widths_heights.append((priorbox_params.min_size / sqrt(ar), priorbox_params.min_size * sqrt(ar)))\n\n        for i, j in product(range(f_h), range(f_w)):\n            # unit center x,y\n            cx = (j + priorbox_params.offset) * priorbox_params.step\n            cy = (i + priorbox_params.offset) * priorbox_params.step\n\n            for box_width, box_height in box_widths_heights:\n                mean += [(cx - box_width / 2.) / img_width, (cy - box_height / 2.) / img_height,\n                         (cx + box_width / 2.) / img_width, (cy + box_height / 2.) / img_height]\n                variance_channel += priorbox_params.variance\n\n        # back to torch land\n        mean = torch.Tensor(mean).unsqueeze(0)\n        if priorbox_params.clip:\n            mean.clamp_(max=1, min=0)\n        variance_channel = torch.Tensor(variance_channel).unsqueeze(0)\n        output = torch.stack((mean, variance_channel), dim=1)\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n'"
pytorch_toolkit/nncf/examples/object_detection/layers/modules/__init__.py,0,"b""from .l2norm import L2Norm\nfrom .multibox_loss import MultiBoxLoss\n\n__all__ = ['L2Norm', 'MultiBoxLoss']\n"""
pytorch_toolkit/nncf/examples/object_detection/layers/modules/l2norm.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass L2Norm(nn.Module):\n    def __init__(self, n_channels, scale, eps, across_spatial=0, channel_shared=0):\n        super(L2Norm, self).__init__()\n        self.n_channels = n_channels\n        self.scale = scale or None\n        self.eps = eps\n        self.across_spatial = across_spatial\n        self.channel_shared = channel_shared\n        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init.constant_(self.weight, self.scale)\n\n    def forward(self, x):\n        if self.training:\n            norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps\n            x = torch.div(x, norm)\n            out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n            return out\n        return L2NormFunction.apply(x, self.weight, self)\n\n\nclass L2NormFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, l2NormParams):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + l2NormParams.eps\n        x = torch.div(x, norm)\n        out = weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        return grad_out\n\n    @staticmethod\n    def symbolic(g, x, weight, l2NormParams):\n        return g.op(""Normalize"", x, weight, eps_f=l2NormParams.eps,\n                    across_spatial_i=l2NormParams.across_spatial, channel_shared_i=l2NormParams.channel_shared)\n'"
pytorch_toolkit/nncf/examples/object_detection/layers/modules/multibox_loss.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..box_utils import match, log_sum_exp\n\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n    def __init__(self, cfg, num_classes, overlap_thresh, prior_for_matching,\n                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, device=None):\n        super(MultiBoxLoss, self).__init__()\n        self.device = device\n        self.num_classes = num_classes\n        self.threshold = overlap_thresh\n        self.background_label = bkg_label\n        self.encode_target = encode_target\n        self.use_prior_for_matching = prior_for_matching\n        self.do_neg_mining = neg_mining\n        self.negpos_ratio = neg_pos\n        self.neg_overlap = neg_overlap\n\n    def forward(self, predictions, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n\n            ground_truth (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n        loc_data, conf_data, priors = predictions\n        batch = loc_data.size(0)\n        num_priors = loc_data.size(1)\n\n        # match priors (default boxes) and ground truth boxes\n        loc_t = torch.Tensor(batch, num_priors, 4).to(self.device)\n        conf_t = torch.LongTensor(batch, num_priors).to(self.device)\n        for idx in range(batch):\n            truths = targets[idx][:, :-1].data\n            labels = targets[idx][:, -1].data\n            defaults = priors.data\n            match(self.threshold, truths, defaults[0], labels, loc_t, conf_t, idx)\n        pos = conf_t > 0\n        num_pos = pos.sum(dim=1, keepdim=True)\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1, 4)\n        loc_t = loc_t[pos_idx].view(-1, 4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction=\'sum\')\n\n        # Compute max conf across batch for hard negative mining\n        batch_conf = conf_data.view(-1, self.num_classes)\n\n        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n\n        # Hard Negative Mining\n        loss_c = loss_c.view(batch, -1)\n        loss_c[pos] = 0  # filter out pos boxes for now\n        _, loss_idx = loss_c.sort(1, descending=True)\n        _, idx_rank = loss_idx.sort(1)\n        num_pos = pos.long().sum(1, keepdim=True)\n        num_neg = torch.clamp(self.negpos_ratio * num_pos, max=pos.size(1) - 1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n\n        # Confidence Loss Including Positive and Negative Examples\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx + neg_idx).gt(0)].view(-1, self.num_classes)\n        targets_weighted = conf_t[(pos + neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction=\'sum\')\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n\n        N = num_pos.data.sum().to(torch.float)\n        loss_l /= N\n        loss_c /= N\n        return loss_l, loss_c\n'"
pytorch_toolkit/nncf/examples/object_detection/layers/modules/ssd_head.py,0,"b'""""""\n Copyright (c) 2019 Intel Corporation\n Licensed under the Apache License, Version 2.0 (the ""License"");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an ""AS IS"" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n""""""\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom examples.object_detection.layers import DetectionOutput, PriorBox\n\n\nclass SSDDetectionOutput(nn.Module):\n    def __init__(self, num_input_features, num_classes, config):\n        super().__init__()\n        self.config = config\n        self.num_classes = num_classes\n\n        self.heads = nn.ModuleList()\n        for i, num_features in enumerate(num_input_features):\n            self.heads.append(SSDHead(\n                num_features, num_classes, config.min_sizes[i], config.max_sizes[i],\n                config.aspect_ratios[i], config.steps[i], config.variance, config.flip, config.clip\n            ))\n\n        self.detection_output = DetectionOutput(\n            num_classes, 0, config.get(\'top_k\', 200), config.get(\'keep_top_k\', 200), 0.01, 0.45, 1, 1,\n            ""CENTER_SIZE"", 0\n        )\n\n    def forward(self, source_features, img_tensor):\n        locs = []\n        confs = []\n        priors = []\n        for features, head in zip(source_features, self.heads):\n            loc, conf, prior = head(features, img_tensor)\n            locs.append(loc)\n            confs.append(conf)\n            priors.append(prior)\n\n        batch = source_features[0].size(0)\n        loc = torch.cat([o.view(batch, -1) for o in locs], 1)\n        conf = torch.cat([o.view(batch, -1) for o in confs], 1)\n        conf_softmax = F.softmax(conf.view(conf.size(0), -1, self.num_classes), dim=-1)\n        priors = torch.cat(priors, dim=2)\n\n        if self.training:\n            return loc.view(batch, -1, 4), conf.view(batch, -1, self.num_classes), priors.view(1, 2, -1, 4)\n        return self.detection_output(loc, conf_softmax.view(batch, -1), priors)\n\n\nclass SSDHead(nn.Module):\n    def __init__(self, num_features, num_classes, min_size, max_size, aspect_ratios, steps, varience, flip, clip):\n        super().__init__()\n        self.num_classes = num_classes\n        self.clip = clip\n        self.flip = flip\n        self.varience = varience\n        self.steps = steps\n        self.aspect_ratios = aspect_ratios\n        self.max_size = max_size\n        self.min_size = min_size\n        self.input_features = num_features\n\n        num_prior_per_cell = 2 + 2 * len(aspect_ratios)\n        self.loc = nn.Conv2d(num_features, num_prior_per_cell * 4, kernel_size=3, padding=1)\n        self.conf = nn.Conv2d(num_features, num_prior_per_cell * num_classes, kernel_size=3, padding=1)\n        self.prior_box = PriorBox(min_size, max_size, aspect_ratios, flip, clip, varience, steps, 0.5, 0, 0,\n                                  0, 0, 0)\n\n    def forward(self, features, image_tensor):\n        loc = self.loc(features)\n        conf = self.conf(features)\n        priors = self.prior_box(features, image_tensor).to(loc.device)\n\n        loc = loc.permute(0, 2, 3, 1).contiguous()\n        conf = conf.permute(0, 2, 3, 1).contiguous()\n\n        return loc, conf, priors\n\n\nclass MultiOutputSequential(nn.Sequential):\n    def __init__(self, outputs, modules):\n        super().__init__(*modules)\n        self.outputs = [str(o) for o in outputs]\n\n    def forward(self, x):\n        outputs = []\n        for name, module in self._modules.items():\n            x = module(x)\n            if name in self.outputs:\n                outputs.append(x)\n        return outputs, x\n'"
tensorflow_toolkit/action_detection/action_detection/nn/utils/debug/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/action_detection/nn/utils/debug/debug.py,5,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef print_tensor(in_tensor, header=\'\', out_tensor=None):\n    """"""Debug function to incorporate print tensor op into the computation graph.\n\n    :param in_tensor: Tensor to print values\n    :param header: Text header to print\n    :param out_tensor: Tensor to attach in if needed\n    :return: Same tensor or output tensor\n    """"""\n\n    prefix = \'\\n{}: \'.format(header)\n    separator = \'\\n\'\n\n    in_shape = tf.shape(in_tensor)\n\n    if out_tensor is None:\n        with tf.control_dependencies([tf.print(prefix, in_shape, separator, in_tensor)]):\n            out = tf.identity(in_tensor)\n    else:\n        with tf.control_dependencies([tf.print(prefix, in_shape, separator, in_tensor)]):\n            out = tf.identity(out_tensor)\n    return out\n'"
tensorflow_toolkit/action_detection/action_detection/nn/utils/detector/__init__.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n'"
tensorflow_toolkit/action_detection/action_detection/nn/utils/detector/bounding_boxes.py,15,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\n\n\ndef center_encode(bboxes, priors, variance):\n    """"""Carry out encoding bounding box coordinate into SSD format.\n\n    :param bboxes: Input bounding boxes\n    :param priors: Set of prior boxes\n    :param variance: List of variances\n    :return: Encoded bounding box coordinates\n    """"""\n\n    priors_height = priors[:, 2] - priors[:, 0]\n    priors_width = priors[:, 3] - priors[:, 1]\n    priors_center_y = 0.5 * (priors[:, 0] + priors[:, 2])\n    priors_center_x = 0.5 * (priors[:, 1] + priors[:, 3])\n\n    bboxes_height = bboxes[:, 2] - bboxes[:, 0]\n    bboxes_width = bboxes[:, 3] - bboxes[:, 1]\n    bboxes_center_y = 0.5 * (bboxes[:, 0] + bboxes[:, 2])\n    bboxes_center_x = 0.5 * (bboxes[:, 1] + bboxes[:, 3])\n\n    encoded_bbox_ymin = (bboxes_center_y - priors_center_y) / (priors_height * variance[0])\n    encoded_bbox_xmin = (bboxes_center_x - priors_center_x) / (priors_width * variance[1])\n    encoded_bbox_ymax = tf.log(bboxes_height / priors_height) / variance[2]\n    encoded_bbox_xmax = tf.log(bboxes_width / priors_width) / variance[3]\n\n    return tf.stack([encoded_bbox_ymin, encoded_bbox_xmin, encoded_bbox_ymax, encoded_bbox_xmax], axis=1)\n\n\ndef center_decode(encoded_locs, priors, variance, clip=False):\n    """"""Carry out decoding bounding box coordinate from SSD format.\n\n    :param encoded_locs: Encoded bounding boxes\n    :param priors: Set of prior boxes\n    :param variance: List of variances\n    :param clip: Whether to clip coordinates into [0, 1] interval\n    :return: Decoded bounding boxes\n    """"""\n\n    output_shape = tf.shape(encoded_locs)\n    num_priors = tf.shape(priors)[0]\n    encoded_locs = tf.reshape(encoded_locs, [-1, num_priors, 4])\n\n    priors_height = priors[:, 2] - priors[:, 0]\n    priors_width = priors[:, 3] - priors[:, 1]\n    priors_center_y = 0.5 * (priors[:, 0] + priors[:, 2])\n    priors_center_x = 0.5 * (priors[:, 1] + priors[:, 3])\n\n    bboxes_center_y = encoded_locs[:, :, 0] * priors_height * variance[0] + priors_center_y\n    bboxes_center_x = encoded_locs[:, :, 1] * priors_width * variance[1] + priors_center_x\n    bboxes_height = tf.exp(encoded_locs[:, :, 2] * variance[2]) * priors_height\n    bboxes_width = tf.exp(encoded_locs[:, :, 3] * variance[3]) * priors_width\n\n    ymin = tf.expand_dims(bboxes_center_y - 0.5 * bboxes_height, axis=2)\n    xmin = tf.expand_dims(bboxes_center_x - 0.5 * bboxes_width, axis=2)\n    ymax = tf.expand_dims(bboxes_center_y + 0.5 * bboxes_height, axis=2)\n    xmax = tf.expand_dims(bboxes_center_x + 0.5 * bboxes_width, axis=2)\n\n    decoded_locs = tf.stack([ymin, xmin, ymax, xmax], axis=-1)\n    if clip:\n        decoded_locs = tf.clip_by_value(decoded_locs, 0., 1.)\n\n    decoded_locs = tf.reshape(decoded_locs, output_shape)\n\n    return decoded_locs\n'"
tensorflow_toolkit/action_detection/action_detection/nn/utils/detector/matchers.py,45,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport tensorflow as tf\n\nfrom action_detection.nn.nodes.metrics import iou_similarity\n\n\ndef ssd_match(gt_data, valid_gt_mask, priors_data, threshold, output_second_order=False):\n    """"""Carry out SSD-like two stage matching between sets of predicted and ground truth boxes.\n\n    :param gt_data: Ground truth set of boxes\n    :param valid_gt_mask: Mask of valid to match ground truth boxes\n    :param priors_data: Set of anchor boxes\n    :param threshold: Min IoU threshold to match boxes\n    :param output_second_order: Whether to output top-2 matches\n    :return: ID and IoU score of matched ground truth box or -1 for each prior box\n    """"""\n\n    def _match_gt_to_priors(sim_matrix):\n        """"""Carry out first matching stage: ground truth to priors.\n\n        :param sim_matrix: Similarity matrix\n        :return: Tuple of matched IDs and IoU scores\n        """"""\n\n        batch = tf.shape(gt_data)[0]\n        num_gt = tf.shape(gt_data)[1]\n        num_priors = tf.shape(priors_data)[0]\n\n        best_priors_local_ids = tf.reshape(tf.argmax(sim_matrix, axis=1), [batch, -1])\n        best_priors_scores = tf.reshape(tf.reduce_max(sim_matrix, axis=1), [-1])\n\n        best_priors_glob_ids = tf.reshape(\n            tf.cast(best_priors_local_ids, tf.int32) +\n            tf.reshape(tf.range(0, batch * num_priors, num_priors, dtype=tf.int32), [batch, 1]), [-1])\n\n        mask = tf.logical_and(tf.reshape(valid_gt_mask, [-1]), tf.greater(best_priors_scores, 0.0))\n        valid_priors_glob_ids = tf.boolean_mask(tf.cast(best_priors_glob_ids, tf.int64), mask)\n        valid_gt_glob_ids = tf.boolean_mask(tf.range(0, batch * num_gt, dtype=tf.int32), mask)\n        valid_matched_scores = tf.boolean_mask(best_priors_scores, mask)\n\n        sparse_indices = tf.expand_dims(valid_priors_glob_ids, 1)\n\n        sparse_matched_ids = tf.SparseTensor(sparse_indices, valid_gt_glob_ids, [batch * num_priors])\n        matched_ids = tf.reshape(\n            tf.sparse_tensor_to_dense(sparse_matched_ids, default_value=-1, validate_indices=False),\n            [batch, num_priors])\n        sparse_matched_scores = tf.SparseTensor(sparse_indices, valid_matched_scores, [batch * num_priors])\n\n        matched_scores = tf.reshape(\n            tf.sparse_tensor_to_dense(sparse_matched_scores, default_value=0.0, validate_indices=False),\n            [batch, num_priors])\n\n        return matched_ids, matched_scores\n\n    def _match_priors_to_gt(sim_matrix):\n        """"""Carry out second matching stage: priors to ground truth\n\n        :param sim_matrix: Similarity matrix\n        :return: Tuple of matched IDs and IoU scores\n        """"""\n\n        batch = tf.shape(gt_data)[0]\n        num_gt = tf.shape(gt_data)[1]\n        num_priors = tf.shape(priors_data)[0]\n\n        if output_second_order:\n            sim_matrix = tf.transpose(tf.reshape(sim_matrix, [batch, num_gt, num_priors]), [0, 2, 1])\n            top_gt_scores, top_gt_local_ids = tf.nn.top_k(sim_matrix, 2, sorted=True)\n        else:\n            sim_matrix = tf.reshape(sim_matrix, [batch, num_gt, num_priors])\n            top_gt_local_ids = tf.argmax(sim_matrix, axis=1)\n            top_gt_scores = tf.reduce_max(sim_matrix, axis=1)\n\n        return top_gt_local_ids, top_gt_scores\n\n    def _translate_matched_priors_to_gt(best_gt_local_ids, best_gt_scores):\n        """"""Converts matched IDs and score from internal format into output.\n\n        :param best_gt_local_ids: matched IDs\n        :param best_gt_scores: matched IoU scores\n        :return: Tuple of matched IDs and IoU scores\n        """"""\n\n        batch = tf.shape(gt_data)[0]\n        num_gt = tf.shape(gt_data)[1]\n\n        best_gt_glob_ids = tf.cast(best_gt_local_ids, tf.int32) + \\\n                           tf.reshape(tf.range(start=0, limit=batch * num_gt, delta=num_gt, dtype=tf.int32), [batch, 1])\n\n        valid_matches_mask = tf.greater(best_gt_scores, threshold)\n        matched_ids = tf.where(valid_matches_mask,\n                               tf.cast(best_gt_glob_ids, tf.int32),\n                               tf.fill(tf.shape(best_gt_glob_ids), -1))\n        matched_scores = tf.where(valid_matches_mask,\n                                  best_gt_scores,\n                                  tf.zeros_like(best_gt_scores))\n\n        return matched_ids, matched_scores\n\n    similarity_matrix = iou_similarity(tf.reshape(gt_data, [-1, 4]), priors_data)\n    similarity_matrix = tf.where(tf.reshape(valid_gt_mask, [-1]),\n                                 similarity_matrix,\n                                 tf.zeros_like(similarity_matrix))\n\n    gt_to_priors_matches, gt_to_priors_scores = _match_gt_to_priors(similarity_matrix)\n    top_priors_to_gt_matches, top_priors_to_gt_scores = _match_priors_to_gt(similarity_matrix)\n\n    if output_second_order:\n        best_priors_to_gt_matches, second_priors_to_gt_matches = tf.unstack(top_priors_to_gt_matches, axis=-1)\n        best_priors_to_gt_scores, second_priors_to_gt_scores = tf.unstack(top_priors_to_gt_scores, axis=-1)\n    else:\n        best_priors_to_gt_matches = top_priors_to_gt_matches\n        best_priors_to_gt_scores = top_priors_to_gt_scores\n\n    best_priors_to_gt_matches, best_priors_to_gt_scores = _translate_matched_priors_to_gt(\n        best_priors_to_gt_matches, best_priors_to_gt_scores)\n\n    gt_to_priors_matches_mask = tf.greater_equal(gt_to_priors_matches, 0)\n    best_matched_ids = tf.where(gt_to_priors_matches_mask, gt_to_priors_matches, best_priors_to_gt_matches)\n    best_matched_scores = tf.where(gt_to_priors_matches_mask, gt_to_priors_scores, best_priors_to_gt_scores)\n\n    if output_second_order:\n        # noinspection PyUnboundLocalVariable\n        second_priors_to_gt_matches, second_priors_to_gt_scores = _translate_matched_priors_to_gt(\n            second_priors_to_gt_matches, second_priors_to_gt_scores)\n\n        return best_matched_ids, best_matched_scores, second_priors_to_gt_matches, second_priors_to_gt_scores\n    else:\n        return best_matched_ids, best_matched_scores, None, None\n'"
tensorflow_toolkit/action_detection/action_detection/nn/utils/detector/priors.py,0,"b'# Copyright (C) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport numpy as np\n\n\ndef generate_clustered_prior_boxes(feature_size, image_size, anchor_sizes, step, clip=False, offset=0.5):\n    """"""Generates set of prior boxes according specified anchor sizes.\n\n    :param feature_size: Size of target feature map\n    :param image_size: Input size\n    :param anchor_sizes: Sizes of anchors in term of image size\n    :param step: Step of sliding window\n    :param clip: Whether to clip coordinates into [0, 1] range\n    :param offset: Parameter to shift coordinates\n    :return: Set of anchor boxes\n    """"""\n\n    if isinstance(step, (list, tuple)):\n        step_y, step_x = step\n    else:\n        step_y, step_x = step, step\n\n    height, width = feature_size\n\n    num_priors_per_pixel = len(anchor_sizes)\n    num_priors = height * width * num_priors_per_pixel\n    top_shape = num_priors, 4\n\n    anchors = []\n    for row in xrange(height):\n        for col in xrange(width):\n            center_x = (float(col) + float(offset)) * float(step_x)\n            center_y = (float(row) + float(offset)) * float(step_y)\n\n            for anchor_height, anchor_width in anchor_sizes:\n                ymin = (center_y - 0.5 * float(anchor_height)) / float(image_size[0])\n                xmin = (center_x - 0.5 * float(anchor_width)) / float(image_size[1])\n                ymax = (center_y + 0.5 * float(anchor_height)) / float(image_size[0])\n                xmax = (center_x + 0.5 * float(anchor_width)) / float(image_size[1])\n\n                anchors.append([ymin, xmin, ymax, xmax])\n\n    priors_array = np.array(anchors, dtype=np.float32).reshape(top_shape)\n\n    if clip:\n        priors_array = np.clip(priors_array, 0., 1.)\n\n    return priors_array, num_priors\n'"
