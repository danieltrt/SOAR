file_path,api_count,code
data_loader.py,0,"b'import os\nfrom os import listdir\nfrom os.path import isfile, join\nimport numpy as np\n\nclass Data_Loader:\n    def __init__(self, options):\n        if options[\'model_type\'] == \'translation\':\n            source_file = options[\'source_file\']\n            target_file = options[\'target_file\']\n\n            self.max_sentences = None\n            if \'max_sentences\' in options:\n                self.max_sentences = options[\'max_sentences\']\n\n            with open(source_file) as f:\n                self.source_lines = f.read().decode(""utf-8"").split(\'\\n\')\n            with open(target_file) as f:\n                self.target_lines = f.read().decode(""utf-8"").split(\'\\n\')\n\n            if self.max_sentences:\n                self.source_lines = self.source_lines[0:self.max_sentences]\n                self.target_lines = self.target_lines[0:self.max_sentences]\n\n            print ""Source Sentences"", len(self.source_lines)\n            print ""Target Sentences"", len(self.target_lines)\n\n            self.bucket_quant = options[\'bucket_quant\']\n            self.source_vocab = self.build_vocab(self.source_lines)\n            self.target_vocab = self.build_vocab(self.target_lines)\n\n            print ""SOURCE VOCAB SIZE"", len(self.source_vocab)\n            print ""TARGET VOCAB SIZE"", len(self.target_vocab)\n        \n        elif options[\'model_type\'] == \'generator\':\n            dir_name = options[\'dir_name\']\n            files = [ join(dir_name, f) for f in listdir(dir_name) if ( isfile(join(dir_name, f)) and (\'.txt\' in f) ) ]\n            text = []\n            for f in files:\n                text += list(open(f).read())\n            \n            vocab = {ch : True for ch in text}\n            print ""Bool vocab"", len(vocab)\n            self.vocab_list = [ch for ch in vocab]\n            print ""vocab list"", len(self.vocab_list)\n            self.vocab_indexed = {ch : i for i, ch in enumerate(self.vocab_list)}\n            print ""vocab_indexed"", len(self.vocab_indexed)\n\n            for index, item in enumerate(text):\n                text[index] = self.vocab_indexed[item]\n            self.text = np.array(text, dtype=\'int32\')\n\n    def load_generator_data(self, sample_size):\n        text = self.text\n        mod_size = len(text) - len(text)%sample_size\n        text = text[0:mod_size]\n        text = text.reshape(-1, sample_size)\n        return text, self.vocab_indexed\n\n\n    def load_translation_data(self):\n        source_lines = []\n        target_lines = []\n        for i in range(len(self.source_lines)):\n            source_lines.append( self.string_to_indices(self.source_lines[i], self.source_vocab) )\n            target_lines.append( self.string_to_indices(self.target_lines[i], self.target_vocab) )\n\n        buckets = self.create_buckets(source_lines, target_lines)\n\n        # frequent_keys = [ (-len(buckets[key]), key) for key in buckets ]\n        # frequent_keys.sort()\n\n        # print ""Source"", self.inidices_to_string( buckets[ frequent_keys[3][1] ][5][0], self.source_vocab)\n        # print ""Target"", self.inidices_to_string( buckets[ frequent_keys[3][1] ][5][1], self.target_vocab)\n        \n        return buckets, self.source_vocab, self.target_vocab\n\n\n\n    def create_buckets(self, source_lines, target_lines):\n        \n        bucket_quant = self.bucket_quant\n        source_vocab = self.source_vocab\n        target_vocab = self.target_vocab\n\n        buckets = {}\n        for i in xrange(len(source_lines)):\n            \n            source_lines[i] = np.concatenate( (source_lines[i], [source_vocab[\'eol\']]) )\n            target_lines[i] = np.concatenate( ([target_vocab[\'init\']], target_lines[i], [target_vocab[\'eol\']]) )\n            \n            sl = len(source_lines[i])\n            tl = len(target_lines[i])\n\n\n            new_length = max(sl, tl)\n            if new_length % bucket_quant > 0:\n                new_length = ((new_length/bucket_quant) + 1 ) * bucket_quant    \n            \n            s_padding = np.array( [source_vocab[\'padding\'] for ctr in xrange(sl, new_length) ] )\n\n            # NEED EXTRA PADDING FOR TRAINING.. \n            t_padding = np.array( [target_vocab[\'padding\'] for ctr in xrange(tl, new_length + 1) ] )\n\n            source_lines[i] = np.concatenate( [ source_lines[i], s_padding ] )\n            target_lines[i] = np.concatenate( [ target_lines[i], t_padding ] )\n\n            if new_length in buckets:\n                buckets[new_length].append( (source_lines[i], target_lines[i]) )\n            else:\n                buckets[new_length] = [(source_lines[i], target_lines[i])]\n\n            if i%1000 == 0:\n                print ""Loading"", i\n            \n        return buckets\n\n    def build_vocab(self, sentences):\n        vocab = {}\n        ctr = 0\n        for st in sentences:\n            for ch in st:\n                if ch not in vocab:\n                    vocab[ch] = ctr\n                    ctr += 1\n\n        # SOME SPECIAL CHARACTERS\n        vocab[\'eol\'] = ctr\n        vocab[\'padding\'] = ctr + 1\n        vocab[\'init\'] = ctr + 2\n\n        return vocab\n\n    def string_to_indices(self, sentence, vocab):\n        indices = [ vocab[s] for s in sentence ]\n        return indices\n\n    def inidices_to_string(self, sentence, vocab):\n        id_ch = { vocab[ch] : ch for ch in vocab } \n        sent = []\n        for c in sentence:\n            if id_ch[c] == \'eol\':\n                break\n            sent += id_ch[c]\n\n        return """".join(sent)\n\n    def get_batch_from_pairs(self, pair_list):\n        source_sentences = []\n        target_sentences = []\n        for s, t in pair_list:\n            source_sentences.append(s)\n            target_sentences.append(t)\n\n        return np.array(source_sentences, dtype = \'int32\'), np.array(target_sentences, dtype = \'int32\')\n\n\ndef main():\n    # FOR TESTING ONLY\n    trans_options = {\n        \'model_type\' : \'translation\',\n        \'source_file\' : \'Data/MachineTranslation/news-commentary-v11.de-en.de\',\n        \'target_file\' : \'Data/MachineTranslation/news-commentary-v11.de-en.en\',\n        \'bucket_quant\' : 25,\n    }\n    gen_options = {\n        \'model_type\' : \'generator\', \n        \'dir_name\' : \'Data\',\n    }\n\n    dl = Data_Loader(gen_options)\n    text_samples, vocab = dl.load_generator_data( 1000 )\n    print dl.inidices_to_string(text_samples[1], vocab)\n    print text_samples.shape\n    print np.max(text_samples)\n    # buckets, source_vocab, target_vocab = dl.load_translation_data()\n\nif __name__ == \'__main__\':\n    main()'"
generate.py,3,"b""import tensorflow as tf\nimport numpy as np\nimport argparse\nimport model_config\nimport data_loader\nfrom ByteNet import generator\nimport utils\nimport shutil\nimport time\n\ndef main():\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument('--sample_size', type=int, default=300,\n                       help='Sampled output size')\n    parser.add_argument('--top_k', type=int, default=5,\n                       help='Sample from top k predictions')\n    parser.add_argument('--model_path', type=str, default=None,\n                       help='Pre-Trained Model Path, to resume from')\n    parser.add_argument('--text_dir', type=str, default='Data/generator_training_data',\n                       help='Directory containing text files')\n    parser.add_argument('--data_dir', type=str, default='Data',\n                       help='Data Directory')\n    parser.add_argument('--seed', type=str, default='All',\n                       help='Seed for text generation')\n    \n\n\n    args = parser.parse_args()\n    \n    # model_config = json.loads( open('model_config.json').read() )\n    config = model_config.predictor_config\n\n    dl = data_loader.Data_Loader({'model_type' : 'generator', 'dir_name' : args.text_dir})\n    _, vocab = dl.load_generator_data(config['sample_size'])\n    \n    \n    model_options = {\n        'vocab_size' : len(vocab),\n        'residual_channels' : config['residual_channels'],\n        'dilations' : config['dilations'],\n        'filter_width' : config['filter_width'],\n    }\n\n    generator_model = generator.ByteNet_Generator( model_options )\n    generator_model.build_generator()\n    \n\n    sess = tf.InteractiveSession()\n    tf.initialize_all_variables().run()\n    saver = tf.train.Saver()\n    \n    if args.model_path:\n        saver.restore(sess, args.model_path)\n\n    seed_sentence = np.array([dl.string_to_indices(args.seed, vocab)], dtype = 'int32' )\n\n    for col in range(args.sample_size):\n        [probs] = sess.run([generator_model.g_probs], \n            feed_dict = {\n                generator_model.seed_sentence :seed_sentence \n            })\n\n        curr_preds = []\n        for bi in range(probs.shape[0]):\n            pred_word = utils.sample_top(probs[bi][-1], top_k = args.top_k )\n            curr_preds.append(pred_word)\n\n        seed_sentence = np.insert(seed_sentence, seed_sentence.shape[1], curr_preds, axis = 1)\n        print col, dl.inidices_to_string(seed_sentence[0], vocab)\n\n        f = open('Data/generator_sample.txt', 'wb')\n        f.write(dl.inidices_to_string(seed_sentence[0], vocab))\n        f.close()\n\nif __name__ == '__main__':\n    main()"""
model_config.py,0,"b'predictor_config = {\n\t""filter_width"": 3,\n\t""dilations"": [1, 2, 4, 8, 16,\n\t\t\t\t  1, 2, 4, 8, 16,\n\t\t\t\t  1, 2, 4, 8, 16,\n\t\t\t\t  1, 2, 4, 8, 16,\n\t\t\t\t  1, 2, 4, 8, 16,\n\t\t\t\t  ],\n\t""residual_channels"": 512,\n\t""n_target_quant"": 256,\n\t""n_source_quant"": 256,\n\t""sample_size"" : 1000\n}\n\ntranslator_config = {\n\t""decoder_filter_width"": 3,\n\t""encoder_filter_width"" : 5,\n\t""encoder_dilations"": [1, 2, 4, 8, 16,\n\t\t\t\t\t\t  1, 2, 4, 8, 16,\n\t\t\t\t\t\t  1, 2, 4, 8, 16,\n\t\t\t\t\t\t  ],\n\t""decoder_dilations"": [1, 2, 4, 8, 16,\n\t\t\t\t\t\t  1, 2, 4, 8, 16,\n\t\t\t\t\t\t  1, 2, 4, 8, 16,\n\t\t\t\t\t\t  ],\n\t""residual_channels"": 512,\n}'"
train_generator.py,6,"b'import tensorflow as tf\nimport numpy as np\nimport argparse\nimport model_config\nimport data_loader\nfrom ByteNet import generator\nimport utils\nimport shutil\nimport time\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--learning_rate\', type=float, default=0.001,\n                       help=\'Learning Rate\')\n    parser.add_argument(\'--batch_size\', type=int, default=1,\n                       help=\'Learning Rate\')\n    parser.add_argument(\'--sample_every\', type=int, default=500,\n                       help=\'Sample generator output evry x steps\')\n    parser.add_argument(\'--summary_every\', type=int, default=50,\n                       help=\'Sample generator output evry x steps\')\n    parser.add_argument(\'--save_model_every\', type=int, default=1500,\n                       help=\'Save model every\')\n    parser.add_argument(\'--sample_size\', type=int, default=300,\n                       help=\'Sampled output size\')\n    parser.add_argument(\'--top_k\', type=int, default=5,\n                       help=\'Sample from top k predictions\')\n    parser.add_argument(\'--max_epochs\', type=int, default=1000,\n                       help=\'Max Epochs\')\n    parser.add_argument(\'--beta1\', type=float, default=0.5,\n                       help=\'Momentum for Adam Update\')\n    parser.add_argument(\'--resume_model\', type=str, default=None,\n                       help=\'Pre-Trained Model Path, to resume from\')\n    parser.add_argument(\'--text_dir\', type=str, default=\'Data/generator_training_data\',\n                       help=\'Directory containing text files\')\n    parser.add_argument(\'--data_dir\', type=str, default=\'Data\',\n                       help=\'Data Directory\')\n    parser.add_argument(\'--seed\', type=str, default=\'All\',\n                       help=\'Seed for text generation\')\n    \n\n\n    args = parser.parse_args()\n    \n    # model_config = json.loads( open(\'model_config.json\').read() )\n    config = model_config.predictor_config\n\n    dl = data_loader.Data_Loader({\'model_type\' : \'generator\', \'dir_name\' : args.text_dir})\n    text_samples, vocab = dl.load_generator_data(config[\'sample_size\'])\n    print text_samples.shape\n    \n    model_options = {\n        \'vocab_size\' : len(vocab),\n        \'residual_channels\' : config[\'residual_channels\'],\n        \'dilations\' : config[\'dilations\'],\n        \'filter_width\' : config[\'filter_width\'],\n    }\n\n    generator_model = generator.ByteNet_Generator( model_options )\n    generator_model.build_model()\n    \n    optim = tf.train.AdamOptimizer(\n        args.learning_rate, \n        beta1 = args.beta1).minimize(generator_model.loss)\n\n    generator_model.build_generator(reuse = True)\n    merged_summary = tf.summary.merge_all()\n\n    sess = tf.InteractiveSession()\n    tf.initialize_all_variables().run()\n    saver = tf.train.Saver()\n    \n    if args.resume_model:\n        saver.restore(sess, args.resume_model)\n    \n    shutil.rmtree(\'Data/tb_summaries/generator_model\')\n    train_writer = tf.summary.FileWriter(\'Data/tb_summaries/generator_model\', sess.graph)\n\n    step = 0\n    for epoch in range(args.max_epochs):\n        batch_no = 0\n        batch_size = args.batch_size\n        while (batch_no+1) * batch_size < text_samples.shape[0]:\n\n            start = time.clock()\n\n            text_batch = text_samples[batch_no*batch_size : (batch_no + 1)*batch_size, :]\n            _, loss, prediction = sess.run( \n                [optim, generator_model.loss, \n                generator_model.arg_max_prediction], \n                feed_dict = {\n                    generator_model.t_sentence : text_batch\n                })\n            end = time.clock()\n            print ""-------------------------------------------------------""\n            print ""LOSS: {}\\tEPOCH: {}\\tBATCH_NO: {}\\t STEP:{}\\t total_batches:{}"".format(\n                loss, epoch, batch_no, step, text_samples.shape[0]/args.batch_size)\n            print ""TIME FOR BATCH"", end - start\n            print ""TIME FOR EPOCH (mins)"", (end - start) * (text_samples.shape[0]/args.batch_size)/60.0\n            \n            batch_no += 1\n            step += 1\n            \n            if step % args.summary_every == 0:\n                [summary] = sess.run([merged_summary], feed_dict = {\n                    generator_model.t_sentence : text_batch\n                })\n                train_writer.add_summary(summary, step)\n                print dl.inidices_to_string(prediction, vocab)\n            \n            print ""********************************************************""\n                \n            if step % args.sample_every == 0:\n                seed_sentence = np.array([dl.string_to_indices(args.seed, vocab)], dtype = \'int32\' )\n\n                for col in range(args.sample_size):\n                    [probs] = sess.run([generator_model.g_probs], \n                        feed_dict = {\n                            generator_model.seed_sentence :seed_sentence \n                        })\n\n                    curr_preds = []\n                    for bi in range(probs.shape[0]):\n                        pred_word = utils.sample_top(probs[bi][-1], top_k = args.top_k )\n                        curr_preds.append(pred_word)\n\n                    seed_sentence = np.insert(seed_sentence, seed_sentence.shape[1], curr_preds, axis = 1)\n                    print col, dl.inidices_to_string(seed_sentence[0], vocab)\n\n                f = open(\'Data/generator_sample.txt\', \'wb\')\n                f.write(dl.inidices_to_string(seed_sentence[0], vocab))\n                f.close()\n\n            if step % args.save_model_every == 0:\n                save_path = saver.save(sess, ""Data/Models/generation_model/model_epoch_{}_{}.ckpt"".format(epoch, step))\n\nif __name__ == \'__main__\':\n    main()'"
train_translator.py,6,"b'import tensorflow as tf\nimport numpy as np\nimport argparse\nimport model_config\nimport data_loader\nfrom ByteNet import translator\nimport utils\nimport shutil\nimport time\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--learning_rate\', type=float, default=0.001,\n                       help=\'Learning Rate\')\n    parser.add_argument(\'--batch_size\', type=int, default=8,\n                       help=\'Learning Rate\')\n    parser.add_argument(\'--bucket_quant\', type=int, default=50,\n                       help=\'Learning Rate\')\n    parser.add_argument(\'--max_epochs\', type=int, default=1000,\n                       help=\'Max Epochs\')\n    parser.add_argument(\'--beta1\', type=float, default=0.5,\n                       help=\'Momentum for Adam Update\')\n    parser.add_argument(\'--resume_model\', type=str, default=None,\n                       help=\'Pre-Trained Model Path, to resume from\')\n    parser.add_argument(\'--source_file\', type=str, default=\'Data/MachineTranslation/news-commentary-v11.de-en.de\',\n                       help=\'Source File\')\n    parser.add_argument(\'--target_file\', type=str, default=\'Data/MachineTranslation/news-commentary-v11.de-en.en\',\n                       help=\'Target File\')\n    parser.add_argument(\'--sample_every\', type=int, default=500,\n                       help=\'Sample generator output evry x steps\')\n    parser.add_argument(\'--summary_every\', type=int, default=50,\n                       help=\'Sample generator output evry x steps\')\n    parser.add_argument(\'--top_k\', type=int, default=5,\n                       help=\'Sample from top k predictions\')\n    parser.add_argument(\'--resume_from_bucket\', type=int, default=0,\n                       help=\'Resume From Bucket\')\n    args = parser.parse_args()\n    \n    data_loader_options = {\n        \'model_type\' : \'translation\',\n        \'source_file\' : args.source_file,\n        \'target_file\' : args.target_file,\n        \'bucket_quant\' : args.bucket_quant,\n    }\n\n    dl = data_loader.Data_Loader(data_loader_options)\n    buckets, source_vocab, target_vocab = dl.load_translation_data()\n    print ""Number Of Buckets"", len(buckets)\n\n    config = model_config.translator_config\n    model_options = {\n        \'source_vocab_size\' : len(source_vocab),\n        \'target_vocab_size\' : len(target_vocab),\n        \'residual_channels\' : config[\'residual_channels\'],\n        \'decoder_dilations\' : config[\'decoder_dilations\'],\n        \'encoder_dilations\' : config[\'encoder_dilations\'],\n        \'decoder_filter_width\' : config[\'decoder_filter_width\'],\n        \'encoder_filter_width\' : config[\'encoder_filter_width\'],\n    }\n\n    translator_model = translator.ByteNet_Translator( model_options )\n    translator_model.build_model()\n\n    optim = tf.train.AdamOptimizer(\n        args.learning_rate, \n        beta1 = args.beta1).minimize(translator_model.loss)\n\n    translator_model.build_translator(reuse = True)\n    merged_summary = tf.summary.merge_all()\n\n    sess = tf.InteractiveSession()\n    tf.initialize_all_variables().run()\n    saver = tf.train.Saver()\n\n    if args.resume_model:\n        saver.restore(sess, args.resume_model)\n\n    shutil.rmtree(\'Data/tb_summaries/translator_model\')\n    train_writer = tf.summary.FileWriter(\'Data/tb_summaries/translator_model\', sess.graph)\n    \n    bucket_sizes = [bucket_size for bucket_size in buckets]\n    bucket_sizes.sort()\n\n    step = 0\n    batch_size = args.batch_size\n    for epoch in range(args.max_epochs):\n        for bucket_size in bucket_sizes:\n            if epoch == 0 and bucket_size < args.resume_from_bucket:\n                continue\n\n            batch_no = 0\n            while (batch_no + 1) * batch_size < len(buckets[bucket_size]):\n                start = time.clock()\n                source, target = dl.get_batch_from_pairs( \n                    buckets[bucket_size][batch_no * batch_size : (batch_no+1) * batch_size] \n                )\n                \n                _, loss, prediction = sess.run( \n                    [optim, translator_model.loss, translator_model.arg_max_prediction], \n                    \n                    feed_dict = {\n                        translator_model.source_sentence : source,\n                        translator_model.target_sentence : target,\n                    })\n                end = time.clock()\n\n                print ""LOSS: {}\\tEPOCH: {}\\tBATCH_NO: {}\\t STEP:{}\\t total_batches:{}\\t bucket_size:{}"".format(\n                loss, epoch, batch_no, step, len(buckets[bucket_size])/args.batch_size, bucket_size)\n                print ""TIME FOR BATCH"", end - start\n                print ""TIME FOR BUCKET (mins)"", (end - start) * (len(buckets[bucket_size])/args.batch_size)/60.0\n\n                batch_no += 1\n                step += 1\n\n                if step % args.summary_every == 0:\n                    [summary] = sess.run([merged_summary], feed_dict = {\n                        translator_model.source_sentence : source,\n                        translator_model.target_sentence : target,\n                    })\n                    train_writer.add_summary(summary, step)\n\n                    print ""******""\n                    print ""Source "", dl.inidices_to_string(source[0], source_vocab)\n                    print ""---------""\n                    print ""Target "", dl.inidices_to_string(target[0], target_vocab)\n                    print ""----------""\n                    print ""Prediction "",dl.inidices_to_string(prediction[0:bucket_size], target_vocab)\n                    print ""******""\n\n                if step % args.sample_every == 0:\n                    log_file = open(\'Data/translator_sample.txt\', \'wb\')\n                    generated_target = target[:,0:1]\n                    for col in range(bucket_size):\n                        [probs] = sess.run([translator_model.t_probs], \n                            feed_dict = {\n                                translator_model.t_source_sentence : source,\n                                translator_model.t_target_sentence : generated_target,\n                            })\n\n                        curr_preds = []\n                        for bi in range(probs.shape[0]):\n                            pred_word = utils.sample_top(probs[bi][-1], top_k = args.top_k )\n                            curr_preds.append(pred_word)\n\n                        generated_target = np.insert(generated_target, generated_target.shape[1], curr_preds, axis = 1)\n                        \n\n                        for bi in range(probs.shape[0]):\n\n                            print col, dl.inidices_to_string(generated_target[bi], target_vocab)\n                            print col, dl.inidices_to_string(target[bi], target_vocab)\n                            print ""***************""\n\n                            if col == bucket_size - 1:\n                                try:\n                                    log_file.write(""Predicted: "" + dl.inidices_to_string(generated_target[bi], target_vocab) + \'\\n\')\n                                    log_file.write(""Actual Target: "" + dl.inidices_to_string(target[bi], target_vocab) + \'\\n\')\n                                    log_file.write(""Actual Source: "" + dl.inidices_to_string(source[bi], source_vocab) + \'\\n *******\')\n                                except:\n                                    pass\n                                print ""***************""\n                    log_file.close()\n\n            save_path = saver.save(sess, ""Data/Models/translation_model/model_epoch_{}_{}.ckpt"".format(epoch, bucket_size))\n\n\n\nif __name__ == \'__main__\':\n    main()\n\n\n\n'"
translate.py,3,"b'import tensorflow as tf\nimport numpy as np\nimport argparse\nimport model_config\nimport data_loader\nfrom ByteNet import translator\nimport utils\nimport shutil\nimport time\nimport random\n\ndef main():\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'--bucket_quant\', type=int, default=50,\n                       help=\'Learning Rate\')\n    parser.add_argument(\'--model_path\', type=str, default=None,\n                       help=\'Pre-Trained Model Path, to resume from\')\n    parser.add_argument(\'--source_file\', type=str, default=\'Data/MachineTranslation/news-commentary-v11.de-en.de\',\n                       help=\'Source File\')\n    parser.add_argument(\'--target_file\', type=str, default=\'Data/MachineTranslation/news-commentary-v11.de-en.en\',\n                       help=\'Target File\')\n    parser.add_argument(\'--top_k\', type=int, default=5,\n                       help=\'Sample from top k predictions\')\n    parser.add_argument(\'--batch_size\', type=int, default=16,\n                       help=\'Batch Size\')\n    parser.add_argument(\'--bucket_size\', type=int, default=None,\n                       help=\'Bucket Size\')\n    args = parser.parse_args()\n    \n    data_loader_options = {\n        \'model_type\' : \'translation\',\n        \'source_file\' : args.source_file,\n        \'target_file\' : args.target_file,\n        \'bucket_quant\' : args.bucket_quant,\n    }\n\n    dl = data_loader.Data_Loader(data_loader_options)\n    buckets, source_vocab, target_vocab = dl.load_translation_data()\n    print ""Number Of Buckets"", len(buckets)\n\n    config = model_config.translator_config\n    model_options = {\n        \'source_vocab_size\' : len(source_vocab),\n        \'target_vocab_size\' : len(target_vocab),\n        \'residual_channels\' : config[\'residual_channels\'],\n        \'decoder_dilations\' : config[\'decoder_dilations\'],\n        \'encoder_dilations\' : config[\'encoder_dilations\'],\n        \'decoder_filter_width\' : config[\'decoder_filter_width\'],\n        \'encoder_filter_width\' : config[\'encoder_filter_width\'],\n    }\n\n    translator_model = translator.ByteNet_Translator( model_options )\n    translator_model.build_translator()\n    \n    sess = tf.InteractiveSession()\n    tf.initialize_all_variables().run()\n    saver = tf.train.Saver()\n\n    if args.model_path:\n        saver.restore(sess, args.model_path)\n\n    \n    \n    bucket_sizes = [bucket_size for bucket_size in buckets]\n    bucket_sizes.sort()\n\n    if not args.bucket_size:\n        bucket_size = random.choice(bucket_sizes)\n    else:\n        bucket_size = args.bucket_size\n\n    source, target = dl.get_batch_from_pairs( \n        random.sample(buckets[bucket_size], args.batch_size)\n    )\n    \n    log_file = open(\'Data/translator_sample.txt\', \'wb\')\n    generated_target = target[:,0:1]\n    for col in range(bucket_size):\n        [probs] = sess.run([translator_model.t_probs], \n            feed_dict = {\n                translator_model.t_source_sentence : source,\n                translator_model.t_target_sentence : generated_target,\n            })\n\n        curr_preds = []\n        for bi in range(probs.shape[0]):\n            pred_word = utils.sample_top(probs[bi][-1], top_k = args.top_k )\n            curr_preds.append(pred_word)\n\n        generated_target = np.insert(generated_target, generated_target.shape[1], curr_preds, axis = 1)\n        \n\n        for bi in range(probs.shape[0]):\n\n            print col, dl.inidices_to_string(generated_target[bi], target_vocab)\n            print col, dl.inidices_to_string(target[bi], target_vocab)\n            print ""***************""\n\n            if col == bucket_size - 1:\n                try:\n                    log_file.write(""Predicted: "" + dl.inidices_to_string(generated_target[bi], target_vocab) + \'\\n\')\n                    log_file.write(""Actual Target: "" + dl.inidices_to_string(target[bi], target_vocab) + \'\\n\')\n                    log_file.write(""Actual Source: "" + dl.inidices_to_string(source[bi], source_vocab) + \'\\n *******\')\n                except:\n                    pass\n                \n    log_file.close()\n\n\n\nif __name__ == \'__main__\':\n    main()'"
utils.py,0,"b'import numpy as np\n\ndef sample_top(a=[], top_k=10):\n    idx = np.argsort(a)[::-1]\n    idx = idx[:top_k]\n    probs = a[idx]\n    probs = probs / np.sum(probs)\n    choice = np.random.choice(idx, p=probs)\n    return choice\n'"
ByteNet/__init__.py,0,b''
ByteNet/generator.py,18,"b'import tensorflow as tf\nimport ops\n\nclass ByteNet_Generator:\n    def __init__(self, options):\n        self.options = options\n        source_embedding_channels = 2 * options[\'residual_channels\']\n        self.w_sentence_embedding = tf.get_variable(\'w_sentence_embedding\', \n                [options[\'vocab_size\'], source_embedding_channels],\n                initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    def build_model(self):\n        options = self.options\n        self.t_sentence = tf.placeholder(\'int32\', \n            [None, None], name = \'t_sentence\')\n\n        source_sentence = self.t_sentence[:,0:-1]\n        target_sentence = self.t_sentence[:,1:]\n\n        source_embedding = tf.nn.embedding_lookup(self.w_sentence_embedding, \n            source_sentence, name = ""source_embedding"")\n\n        curr_input = source_embedding\n        for layer_no, dilation in enumerate(options[\'dilations\']):\n            curr_input = ops.byetenet_residual_block(curr_input, dilation, \n                layer_no, options[\'residual_channels\'], \n                options[\'filter_width\'], causal = True, train = True)\n\n        logits = ops.conv1d(tf.nn.relu(curr_input), \n            options[\'vocab_size\'], name = \'logits\')\n\n        logits_flat = tf.reshape(logits, [-1, options[\'vocab_size\']])\n        target_flat = tf.reshape(target_sentence, [-1])\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = target_flat, logits = logits_flat)\n        self.loss = tf.reduce_mean(loss)\n        \n        self.arg_max_prediction = tf.argmax(logits_flat, 1)\n        \n        tf.summary.scalar(\'loss\', self.loss)\n\n    def build_generator(self, reuse = False):\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n\n        options = self.options\n        self.seed_sentence = tf.placeholder(\'int32\', \n            [None, None], name = \'seed_sentence\')\n        \n        source_embedding = tf.nn.embedding_lookup(self.w_sentence_embedding, \n            self.seed_sentence, name = ""source_embedding"")\n\n        curr_input = source_embedding\n        for layer_no, dilation in enumerate(options[\'dilations\']):\n            curr_input = ops.byetenet_residual_block(curr_input, dilation, \n                layer_no, options[\'residual_channels\'], \n                options[\'filter_width\'], causal = True, train = False)\n\n        logits = ops.conv1d(tf.nn.relu(curr_input), \n            options[\'vocab_size\'], name = \'logits\')\n        logits_flat = tf.reshape(logits, [-1, options[\'vocab_size\']])\n        probs_flat = tf.nn.softmax(logits_flat)\n        \n        self.g_probs = tf.reshape(probs_flat, [-1, tf.shape(self.seed_sentence)[1], options[\'vocab_size\']])\n        \n\ndef main():\n    options = {\n        \'vocab_size\' : 250,\n        \'residual_channels\' : 512,\n        \'dilations\' : [ 1,2,4,8,16,\n                        1,2,4,8,16\n                       ],\n        \'filter_width\' : 3\n    }\n\n    model = ByteNet_Generator(options)\n    model.build_model()\n    model.build_generator(reuse = True)\n\nif __name__ == \'__main__\':\n    main()'"
ByteNet/ops.py,30,"b'import tensorflow as tf\nimport math\n\ndef fully_connected(input_, output_nodes, name, stddev=0.02):\n    with tf.variable_scope(name):\n        input_shape = input_.get_shape()\n        input_nodes = input_shape[-1]\n        w = tf.get_variable(\'w\', [input_nodes, output_nodes], \n            initializer=tf.truncated_normal_initializer(stddev=0.02))\n        biases = tf.get_variable(\'b\', [output_nodes], \n            initializer=tf.constant_initializer(0.0))\n        res = tf.matmul(input_, w) + biases\n        return res\n\n\n# 1d CONVOLUTION WITH DILATION\ndef conv1d(input_, output_channels, \n    dilation = 1, filter_width = 1, causal = False, \n    name = ""dilated_conv""):\n    with tf.variable_scope(name):\n        w = tf.get_variable(\'w\', [1, filter_width, input_.get_shape()[-1], output_channels ],\n            initializer=tf.truncated_normal_initializer(stddev=0.02))\n        b = tf.get_variable(\'b\', [output_channels ],\n           initializer=tf.constant_initializer(0.0))\n\n        if causal:\n            padding = [[0, 0], [(filter_width - 1) * dilation, 0], [0, 0]]\n            padded = tf.pad(input_, padding)\n            input_expanded = tf.expand_dims(padded, dim = 1)\n            out = tf.nn.atrous_conv2d(input_expanded, w, rate = dilation, padding = \'VALID\') + b\n        else:\n            input_expanded = tf.expand_dims(input_, dim = 1)\n            out = tf.nn.atrous_conv2d(input_expanded, w, rate = dilation, padding = \'SAME\') + b\n\n        return tf.squeeze(out, [1])\n\ndef layer_normalization(x, name, epsilon=1e-8, trainable = True):\n    with tf.variable_scope(name):\n        shape = x.get_shape()\n        beta = tf.get_variable(\'beta\', [ int(shape[-1])], \n            initializer=tf.constant_initializer(0), trainable=trainable)\n        gamma = tf.get_variable(\'gamma\', [ int(shape[-1])], \n            initializer=tf.constant_initializer(1), trainable=trainable)\n        \n        mean, variance = tf.nn.moments(x, axes=[len(shape) - 1], keep_dims=True)\n        \n        x = (x - mean) /  tf.sqrt(variance + epsilon)\n\n        return gamma * x + beta\n\ndef byetenet_residual_block(input_, dilation, layer_no, \n    residual_channels, filter_width,\n    causal = True, train = True):\n        block_type = ""decoder"" if causal else ""encoder""\n        block_name = ""bytenet_{}_layer_{}_{}"".format(block_type, layer_no, dilation)\n        with tf.variable_scope(block_name):\n            input_ln = layer_normalization(input_, name=""ln1"", trainable = train)\n            relu1 = tf.nn.relu(input_ln)\n            conv1 = conv1d(relu1, residual_channels, name = ""conv1d_1"")\n            conv1 = layer_normalization(conv1, name=""ln2"", trainable = train)\n            relu2 = tf.nn.relu(conv1)\n            \n            dilated_conv = conv1d(relu2, residual_channels, \n                dilation, filter_width,\n                causal = causal,\n                name = ""dilated_conv""\n                )\n            print dilated_conv\n            dilated_conv = layer_normalization(dilated_conv, name=""ln3"", trainable = train)\n            relu3 = tf.nn.relu(dilated_conv)\n            conv2 = conv1d(relu3, 2 * residual_channels, name = \'conv1d_2\')\n            return input_ + conv2\n\ndef init_weight(dim_in, dim_out, name=None, stddev=1.0):\n    return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n\ndef init_bias(dim_out, name=None):\n    return tf.Variable(tf.zeros([dim_out]), name=name)'"
ByteNet/translator.py,26,"b'import tensorflow as tf\nimport ops\n\nclass ByteNet_Translator:\n    def __init__(self, options):\n        self.options = options\n        embedding_channels = 2 * options[\'residual_channels\']\n\n        self.w_source_embedding = tf.get_variable(\'w_source_embedding\', \n                    [options[\'source_vocab_size\'], embedding_channels],\n                    initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n        self.w_target_embedding = tf.get_variable(\'w_target_embedding\', \n                    [options[\'target_vocab_size\'], embedding_channels],\n                    initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    def build_model(self):\n        options = self.options\n        self.source_sentence = tf.placeholder(\'int32\', \n            [None, None], name = \'source_sentence\')\n        self.target_sentence = tf.placeholder(\'int32\', \n            [None, None], name = \'target_sentence\')\n\n        target_1 = self.target_sentence[:,0:-1]\n        target_2 = self.target_sentence[:,1:]\n\n        source_embedding = tf.nn.embedding_lookup(self.w_source_embedding, \n            self.source_sentence, name = ""source_embedding"")\n        target_1_embedding = tf.nn.embedding_lookup(self.w_target_embedding, \n            target_1, name = ""target_1_embedding"")\n\n\n        curr_input = source_embedding\n        for layer_no, dilation in enumerate(options[\'encoder_dilations\']):\n            curr_input = ops.byetenet_residual_block(curr_input, dilation, \n                layer_no, options[\'residual_channels\'], \n                options[\'encoder_filter_width\'], causal = False, train = True)\n\n        encoder_output = curr_input\n        combined_embedding = target_1_embedding + encoder_output\n        curr_input = combined_embedding\n        for layer_no, dilation in enumerate(options[\'decoder_dilations\']):\n            curr_input = ops.byetenet_residual_block(curr_input, dilation, \n                layer_no, options[\'residual_channels\'], \n                options[\'decoder_filter_width\'], causal = True, train = True)\n\n        logits = ops.conv1d(tf.nn.relu(curr_input), \n            options[\'target_vocab_size\'], name = \'logits\')\n        print ""logits"", logits\n        logits_flat = tf.reshape(logits, [-1, options[\'target_vocab_size\']])\n        target_flat = tf.reshape(target_2, [-1])\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels = target_flat, logits = logits_flat)\n        \n        self.loss = tf.reduce_mean(loss)\n        self.arg_max_prediction = tf.argmax(logits_flat, 1)\n        tf.summary.scalar(\'loss\', self.loss)\n\n    def build_translator(self, reuse = False):\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n\n        options = self.options\n        self.t_source_sentence = tf.placeholder(\'int32\', \n            [None, None], name = \'source_sentence\')\n        self.t_target_sentence = tf.placeholder(\'int32\', \n            [None, None], name = \'target_sentence\')\n\n        source_embedding = tf.nn.embedding_lookup(self.w_source_embedding, \n            self.t_source_sentence, name = ""source_embedding"")\n        target_embedding = tf.nn.embedding_lookup(self.w_target_embedding, \n            self.t_target_sentence, name = ""target_embedding"")\n\n        curr_input = source_embedding\n        for layer_no, dilation in enumerate(options[\'encoder_dilations\']):\n            curr_input = ops.byetenet_residual_block(curr_input, dilation, \n                layer_no, options[\'residual_channels\'], \n                options[\'encoder_filter_width\'], causal = False, train = False)\n\n        encoder_output = curr_input[:,0:tf.shape(self.t_target_sentence)[1],:]\n\n        combined_embedding = target_embedding + encoder_output\n        curr_input = combined_embedding\n        for layer_no, dilation in enumerate(options[\'decoder_dilations\']):\n            curr_input = ops.byetenet_residual_block(curr_input, dilation, \n                layer_no, options[\'residual_channels\'], \n                options[\'decoder_filter_width\'], causal = True, train = False)\n\n        logits = ops.conv1d(tf.nn.relu(curr_input), \n            options[\'target_vocab_size\'], name = \'logits\')\n        logits_flat = tf.reshape(logits, [-1, options[\'target_vocab_size\']])\n        probs_flat = tf.nn.softmax(logits_flat)\n\n        self.t_probs = tf.reshape(probs_flat, \n            [-1, tf.shape(logits)[1], options[\'target_vocab_size\']])\n\ndef main():\n    options = {\n        \'source_vocab_size\' : 250,\n        \'target_vocab_size\' : 250,\n        \'residual_channels\' : 512,\n        \'encoder_dilations\' : [ 1,2,4,8,16,\n                        1,2,4,8,16\n                       ],\n        \'decoder_dilations\' : [ 1,2,4,8,16,\n            1,2,4,8,16\n        ],\n        \'encoder_filter_width\' : 3,\n        \'decoder_filter_width\' : 3\n    }\n    md = ByteNet_Translator(options)\n    md.build_model()\n    md.build_translator(reuse = True)\n\nif __name__ == \'__main__\':\n    main()'"
