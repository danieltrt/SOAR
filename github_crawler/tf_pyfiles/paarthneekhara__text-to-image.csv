file_path,api_count,code
data_loader.py,0,"b'import json\nimport os\nfrom os.path import join, isfile\nimport re\nimport numpy as np\nimport pickle\nimport argparse\nimport skipthoughts\nimport h5py\n\n# DID NOT TRAIN IT ON MS COCO YET\ndef save_caption_vectors_ms_coco(data_dir, split, batch_size):\n\tmeta_data = {}\n\tic_file = join(data_dir, \'annotations/captions_{}2014.json\'.format(split))\n\twith open(ic_file) as f:\n\t\tic_data = json.loads(f.read())\n\n\tmeta_data[\'data_length\'] = len(ic_data[\'annotations\'])\n\twith open(join(data_dir, \'meta_{}.pkl\'.format(split)), \'wb\') as f:\n\t\tpickle.dump(meta_data, f)\n\n\tmodel = skipthoughts.load_model()\n\tbatch_no = 0\n\tprint ""Total Batches"", len(ic_data[\'annotations\'])/batch_size\n\n\twhile batch_no*batch_size < len(ic_data[\'annotations\']):\n\t\tcaptions = []\n\t\timage_ids = []\n\t\tidx = batch_no\n\t\tfor i in range(batch_no*batch_size, (batch_no+1)*batch_size):\n\t\t\tidx = i%len(ic_data[\'annotations\'])\n\t\t\tcaptions.append(ic_data[\'annotations\'][idx][\'caption\'])\n\t\t\timage_ids.append(ic_data[\'annotations\'][idx][\'image_id\'])\n\n\t\tprint captions\n\t\tprint image_ids\n\t\t# Thought Vectors\n\t\ttv_batch = skipthoughts.encode(model, captions)\n\t\th5f_tv_batch = h5py.File( join(data_dir, \'tvs/\'+split + \'_tvs_\' + str(batch_no)), \'w\')\n\t\th5f_tv_batch.create_dataset(\'tv\', data=tv_batch)\n\t\th5f_tv_batch.close()\n\n\t\th5f_tv_batch_image_ids = h5py.File( join(data_dir, \'tvs/\'+split + \'_tv_image_id_\' + str(batch_no)), \'w\')\n\t\th5f_tv_batch_image_ids.create_dataset(\'tv\', data=image_ids)\n\t\th5f_tv_batch_image_ids.close()\n\n\t\tprint ""Batches Done"", batch_no, len(ic_data[\'annotations\'])/batch_size\n\t\tbatch_no += 1\n\n\ndef save_caption_vectors_flowers(data_dir):\n\timport time\n\t\n\timg_dir = join(data_dir, \'flowers/jpg\')\n\timage_files = [f for f in os.listdir(img_dir) if \'jpg\' in f]\n\tprint image_files[300:400]\n\tprint len(image_files)\n\timage_captions = { img_file : [] for img_file in image_files }\n\n\tcaption_dir = join(data_dir, \'flowers/text_c10\')\n\tclass_dirs = []\n\tfor i in range(1, 103):\n\t\tclass_dir_name = \'class_%.5d\'%(i)\n\t\tclass_dirs.append( join(caption_dir, class_dir_name))\n\n\tfor class_dir in class_dirs:\n\t\tcaption_files = [f for f in os.listdir(class_dir) if \'txt\' in f]\n\t\tfor cap_file in caption_files:\n\t\t\twith open(join(class_dir,cap_file)) as f:\n\t\t\t\tcaptions = f.read().split(\'\\n\')\n\t\t\timg_file = cap_file[0:11] + "".jpg""\n\t\t\t# 5 captions per image\n\t\t\timage_captions[img_file] += [cap for cap in captions if len(cap) > 0][0:5]\n\n\tprint len(image_captions)\n\n\tmodel = skipthoughts.load_model()\n\tencoded_captions = {}\n\n\n\tfor i, img in enumerate(image_captions):\n\t\tst = time.time()\n\t\tencoded_captions[img] = skipthoughts.encode(model, image_captions[img])\n\t\tprint i, len(image_captions), img\n\t\tprint ""Seconds"", time.time() - st\n\t\t\n\t\n\th = h5py.File(join(data_dir, \'flower_tv.hdf5\'))\n\tfor key in encoded_captions:\n\t\th.create_dataset(key, data=encoded_captions[key])\n\th.close()\n\t\t\t\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'--split\', type=str, default=\'train\',\n                       help=\'train/val\')\n\tparser.add_argument(\'--data_dir\', type=str, default=\'Data\',\n                       help=\'Data directory\')\n\tparser.add_argument(\'--batch_size\', type=int, default=64,\n                       help=\'Batch Size\')\n\tparser.add_argument(\'--data_set\', type=str, default=\'flowers\',\n                       help=\'Data Set : Flowers, MS-COCO\')\n\targs = parser.parse_args()\n\t\n\tif args.data_set == \'flowers\':\n\t\tsave_caption_vectors_flowers(args.data_dir)\n\telse:\n\t\tsave_caption_vectors_ms_coco(args.data_dir, args.split, args.batch_size)\n\nif __name__ == \'__main__\':\n\tmain()'"
download_datasets.py,0,"b'# downloads/extracts datasets described in the README.md\n\nimport os\nimport sys\nimport errno\nimport tarfile\n\nif sys.version_info >= (3,):\n    from urllib.request import urlretrieve\nelse:\n    from urllib import urlretrieve\n\nDATA_DIR = \'Data\'\n\n\n# http://stackoverflow.com/questions/273192/how-to-check-if-a-directory-exists-and-create-it-if-necessary\ndef make_sure_path_exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            raise\n\n\ndef create_data_paths():\n    if not os.path.isdir(DATA_DIR):\n        raise EnvironmentError(\'Needs to be run from project directory containing \' + DATA_DIR)\n    needed_paths = [\n        os.path.join(DATA_DIR, \'samples\'),\n        os.path.join(DATA_DIR, \'val_samples\'),\n        os.path.join(DATA_DIR, \'Models\'),\n    ]\n    for p in needed_paths:\n        make_sure_path_exists(p)\n\n\n# adapted from http://stackoverflow.com/questions/51212/how-to-write-a-download-progress-indicator-in-python\ndef dl_progress_hook(count, blockSize, totalSize):\n    percent = int(count * blockSize * 100 / totalSize)\n    sys.stdout.write(""\\r"" + ""...%d%%"" % percent)\n    sys.stdout.flush()\n\n\ndef download_dataset(data_name):\n    if data_name == \'flowers\':\n        print(\'== Flowers dataset ==\')\n        flowers_dir = os.path.join(DATA_DIR, \'flowers\')\n        flowers_jpg_tgz = os.path.join(flowers_dir, \'102flowers.tgz\')\n        make_sure_path_exists(flowers_dir)\n\n        # the original google drive link at https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view\n        # from https://github.com/reedscot/icml2016 is problematic to download automatically, so included\n        # the text_c10 directory from that archive as a bzipped file in the repo\n        captions_tbz = os.path.join(DATA_DIR, \'flowers_text_c10.tar.bz2\')\n        print(\'Extracting \' + captions_tbz)\n        captions_tar = tarfile.open(captions_tbz, \'r:bz2\')\n        captions_tar.extractall(flowers_dir)\n\n        flowers_url = \'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\'\n        print(\'Downloading \' + flowers_jpg_tgz + \' from \' + flowers_url)\n        urlretrieve(flowers_url, flowers_jpg_tgz,\n                    reporthook=dl_progress_hook)\n        print(\'Extracting \' + flowers_jpg_tgz)\n        flowers_jpg_tar = tarfile.open(flowers_jpg_tgz, \'r:gz\')\n        flowers_jpg_tar.extractall(flowers_dir)  # archive contains jpg/ folder\n\n    elif data_name == \'skipthoughts\':\n        print(\'== Skipthoughts models ==\')\n        SKIPTHOUGHTS_DIR = os.path.join(DATA_DIR, \'skipthoughts\')\n        SKIPTHOUGHTS_BASE_URL = \'http://www.cs.toronto.edu/~rkiros/models/\'\n        make_sure_path_exists(SKIPTHOUGHTS_DIR)\n\n        # following https://github.com/ryankiros/skip-thoughts#getting-started\n        skipthoughts_files = [\n            \'dictionary.txt\', \'utable.npy\', \'btable.npy\', \'uni_skip.npz\', \'uni_skip.npz.pkl\', \'bi_skip.npz\',\n            \'bi_skip.npz.pkl\',\n        ]\n        for filename in skipthoughts_files:\n            src_url = SKIPTHOUGHTS_BASE_URL + filename\n            print(\'Downloading \' + src_url)\n            urlretrieve(src_url, os.path.join(SKIPTHOUGHTS_DIR, filename),\n                        reporthook=dl_progress_hook)\n\n    elif data_name == \'nltk_punkt\':\n        import nltk\n        print(\'== NLTK pre-trained Punkt tokenizer for English ==\')\n        nltk.download(\'punkt\')\n\n    elif data_name == \'pretrained_model\':\n        print(\'== Pretrained model ==\')\n        MODEL_DIR = os.path.join(DATA_DIR, \'Models\')\n        pretrained_model_filename = \'latest_model_flowers_temp.ckpt\'\n        src_url = \'https://bitbucket.org/paarth_neekhara/texttomimagemodel/raw/74a4bbaeee26fe31e148a54c4f495694680e2c31/\' + pretrained_model_filename\n        print(\'Downloading \' + src_url)\n        urlretrieve(\n            src_url,\n            os.path.join(MODEL_DIR, pretrained_model_filename),\n            reporthook=dl_progress_hook,\n        )\n\n    else:\n        raise ValueError(\'Unknown dataset name: \' + data_name)\n\n\ndef main():\n    create_data_paths()\n    # TODO: make configurable via command-line\n    download_dataset(\'flowers\')\n    download_dataset(\'skipthoughts\')\n    download_dataset(\'nltk_punkt\')\n    download_dataset(\'pretrained_model\')\n    print(\'Done\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
generate_images.py,2,"b'import tensorflow as tf\nimport numpy as np\nimport model\nimport argparse\nimport pickle\nfrom os.path import join\nimport h5py\nfrom Utils import image_processing\nimport scipy.misc\nimport random\nimport json\nimport os\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\'--z_dim\', type=int, default=100,\n\t\t\t\t\t   help=\'Noise Dimension\')\n\n\tparser.add_argument(\'--t_dim\', type=int, default=256,\n\t\t\t\t\t   help=\'Text feature dimension\')\n\n\tparser.add_argument(\'--image_size\', type=int, default=64,\n\t\t\t\t\t   help=\'Image Size\')\n\n\tparser.add_argument(\'--gf_dim\', type=int, default=64,\n\t\t\t\t\t   help=\'Number of conv in the first layer gen.\')\n\n\tparser.add_argument(\'--df_dim\', type=int, default=64,\n\t\t\t\t\t   help=\'Number of conv in the first layer discr.\')\n\n\tparser.add_argument(\'--gfc_dim\', type=int, default=1024,\n\t\t\t\t\t   help=\'Dimension of gen untis for for fully connected layer 1024\')\n\n\tparser.add_argument(\'--caption_vector_length\', type=int, default=2400,\n\t\t\t\t\t   help=\'Caption Vector Length\')\n\t\n\tparser.add_argument(\'--data_dir\', type=str, default=""Data"",\n\t\t\t\t\t   help=\'Data Directory\')\n\n\tparser.add_argument(\'--model_path\', type=str, default=\'Data/Models/latest_model_flowers_temp.ckpt\',\n                       help=\'Trained Model Path\')\n\n\tparser.add_argument(\'--n_images\', type=int, default=5,\n                       help=\'Number of Images per Caption\')\n\n\tparser.add_argument(\'--caption_thought_vectors\', type=str, default=\'Data/sample_caption_vectors.hdf5\',\n                       help=\'Caption Thought Vector File\')\n\n\t\n\targs = parser.parse_args()\n\tmodel_options = {\n\t\t\'z_dim\' : args.z_dim,\n\t\t\'t_dim\' : args.t_dim,\n\t\t\'batch_size\' : args.n_images,\n\t\t\'image_size\' : args.image_size,\n\t\t\'gf_dim\' : args.gf_dim,\n\t\t\'df_dim\' : args.df_dim,\n\t\t\'gfc_dim\' : args.gfc_dim,\n\t\t\'caption_vector_length\' : args.caption_vector_length\n\t}\n\n\tgan = model.GAN(model_options)\n\t_, _, _, _, _ = gan.build_model()\n\tsess = tf.InteractiveSession()\n\tsaver = tf.train.Saver()\n\tsaver.restore(sess, args.model_path)\n\t\n\tinput_tensors, outputs = gan.build_generator()\n\n\th = h5py.File( args.caption_thought_vectors )\n\tcaption_vectors = np.array(h[\'vectors\'])\n\tcaption_image_dic = {}\n\tfor cn, caption_vector in enumerate(caption_vectors):\n\n\t\tcaption_images = []\n\t\tz_noise = np.random.uniform(-1, 1, [args.n_images, args.z_dim])\n\t\tcaption = [ caption_vector[0:args.caption_vector_length] ] * args.n_images\n\t\t\n\t\t[ gen_image ] = sess.run( [ outputs[\'generator\'] ], \n\t\t\tfeed_dict = {\n\t\t\t\tinput_tensors[\'t_real_caption\'] : caption,\n\t\t\t\tinput_tensors[\'t_z\'] : z_noise,\n\t\t\t} )\n\t\t\n\t\tcaption_images = [gen_image[i,:,:,:] for i in range(0, args.n_images)]\n\t\tcaption_image_dic[ cn ] = caption_images\n\t\tprint ""Generated"", cn\n\n\tfor f in os.listdir( join(args.data_dir, \'val_samples\')):\n\t\tif os.path.isfile(f):\n\t\t\tos.unlink(join(args.data_dir, \'val_samples/\' + f))\n\n\tfor cn in range(0, len(caption_vectors)):\n\t\tcaption_images = []\n\t\tfor i, im in enumerate( caption_image_dic[ cn ] ):\n\t\t\t# im_name = ""caption_{}_{}.jpg"".format(cn, i)\n\t\t\t# scipy.misc.imsave( join(args.data_dir, \'val_samples/{}\'.format(im_name)) , im)\n\t\t\tcaption_images.append( im )\n\t\t\tcaption_images.append( np.zeros((64, 5, 3)) )\n\t\tcombined_image = np.concatenate( caption_images[0:-1], axis = 1 )\n\t\tscipy.misc.imsave( join(args.data_dir, \'val_samples/combined_image_{}.jpg\'.format(cn)) , combined_image)\n\n\nif __name__ == \'__main__\':\n\tmain()\n'"
generate_thought_vectors.py,0,"b""import os\nfrom os.path import join, isfile\nimport re\nimport numpy as np\nimport pickle\nimport argparse\nimport skipthoughts\nimport h5py\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--caption_file', type=str, default='Data/sample_captions.txt',\n\t\t\t\t\t   help='caption file')\n\tparser.add_argument('--data_dir', type=str, default='Data',\n\t\t\t\t\t   help='Data Directory')\n\n\targs = parser.parse_args()\n\twith open( args.caption_file ) as f:\n\t\tcaptions = f.read().split('\\n')\n\n\tcaptions = [cap for cap in captions if len(cap) > 0]\n\tprint captions\n\tmodel = skipthoughts.load_model()\n\tcaption_vectors = skipthoughts.encode(model, captions)\n\n\tif os.path.isfile(join(args.data_dir, 'sample_caption_vectors.hdf5')):\n\t\tos.remove(join(args.data_dir, 'sample_caption_vectors.hdf5'))\n\th = h5py.File(join(args.data_dir, 'sample_caption_vectors.hdf5'))\n\th.create_dataset('vectors', data=caption_vectors)\t\t\n\th.close()\n\nif __name__ == '__main__':\n\tmain()"""
model.py,33,"b""import tensorflow as tf\nfrom Utils import ops\n\nclass GAN:\n\t'''\n\tOPTIONS\n\tz_dim : Noise dimension 100\n\tt_dim : Text feature dimension 256\n\timage_size : Image Dimension 64\n\tgf_dim : Number of conv in the first layer generator 64\n\tdf_dim : Number of conv in the first layer discriminator 64\n\tgfc_dim : Dimension of gen untis for for fully connected layer 1024\n\tcaption_vector_length : Caption Vector Length 2400\n\tbatch_size : Batch Size 64\n\t'''\n\tdef __init__(self, options):\n\t\tself.options = options\n\n\t\tself.g_bn0 = ops.batch_norm(name='g_bn0')\n\t\tself.g_bn1 = ops.batch_norm(name='g_bn1')\n\t\tself.g_bn2 = ops.batch_norm(name='g_bn2')\n\t\tself.g_bn3 = ops.batch_norm(name='g_bn3')\n\n\t\tself.d_bn1 = ops.batch_norm(name='d_bn1')\n\t\tself.d_bn2 = ops.batch_norm(name='d_bn2')\n\t\tself.d_bn3 = ops.batch_norm(name='d_bn3')\n\t\tself.d_bn4 = ops.batch_norm(name='d_bn4')\n\n\n\tdef build_model(self):\n\t\timg_size = self.options['image_size']\n\t\tt_real_image = tf.placeholder('float32', [self.options['batch_size'],img_size, img_size, 3 ], name = 'real_image')\n\t\tt_wrong_image = tf.placeholder('float32', [self.options['batch_size'],img_size, img_size, 3 ], name = 'wrong_image')\n\t\tt_real_caption = tf.placeholder('float32', [self.options['batch_size'], self.options['caption_vector_length']], name = 'real_caption_input')\n\t\tt_z = tf.placeholder('float32', [self.options['batch_size'], self.options['z_dim']])\n\n\t\tfake_image = self.generator(t_z, t_real_caption)\n\t\t\n\t\tdisc_real_image, disc_real_image_logits   = self.discriminator(t_real_image, t_real_caption)\n\t\tdisc_wrong_image, disc_wrong_image_logits   = self.discriminator(t_wrong_image, t_real_caption, reuse = True)\n\t\tdisc_fake_image, disc_fake_image_logits   = self.discriminator(fake_image, t_real_caption, reuse = True)\n\t\t\n\t\tg_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_image_logits, tf.ones_like(disc_fake_image)))\n\t\t\n\t\td_loss1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_real_image_logits, tf.ones_like(disc_real_image)))\n\t\td_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_wrong_image_logits, tf.zeros_like(disc_wrong_image)))\n\t\td_loss3 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_image_logits, tf.zeros_like(disc_fake_image)))\n\n\t\td_loss = d_loss1 + d_loss2 + d_loss3\n\n\t\tt_vars = tf.trainable_variables()\n\t\td_vars = [var for var in t_vars if 'd_' in var.name]\n\t\tg_vars = [var for var in t_vars if 'g_' in var.name]\n\n\t\tinput_tensors = {\n\t\t\t't_real_image' : t_real_image,\n\t\t\t't_wrong_image' : t_wrong_image,\n\t\t\t't_real_caption' : t_real_caption,\n\t\t\t't_z' : t_z\n\t\t}\n\n\t\tvariables = {\n\t\t\t'd_vars' : d_vars,\n\t\t\t'g_vars' : g_vars\n\t\t}\n\n\t\tloss = {\n\t\t\t'g_loss' : g_loss,\n\t\t\t'd_loss' : d_loss\n\t\t}\n\n\t\toutputs = {\n\t\t\t'generator' : fake_image\n\t\t}\n\n\t\tchecks = {\n\t\t\t'd_loss1': d_loss1,\n\t\t\t'd_loss2': d_loss2,\n\t\t\t'd_loss3' : d_loss3,\n\t\t\t'disc_real_image_logits' : disc_real_image_logits,\n\t\t\t'disc_wrong_image_logits' : disc_wrong_image,\n\t\t\t'disc_fake_image_logits' : disc_fake_image_logits\n\t\t}\n\t\t\n\t\treturn input_tensors, variables, loss, outputs, checks\n\n\tdef build_generator(self):\n\t\timg_size = self.options['image_size']\n\t\tt_real_caption = tf.placeholder('float32', [self.options['batch_size'], self.options['caption_vector_length']], name = 'real_caption_input')\n\t\tt_z = tf.placeholder('float32', [self.options['batch_size'], self.options['z_dim']])\n\t\tfake_image = self.sampler(t_z, t_real_caption)\n\t\t\n\t\tinput_tensors = {\n\t\t\t't_real_caption' : t_real_caption,\n\t\t\t't_z' : t_z\n\t\t}\n\t\t\n\t\toutputs = {\n\t\t\t'generator' : fake_image\n\t\t}\n\n\t\treturn input_tensors, outputs\n\n\t# Sample Images for a text embedding\n\tdef sampler(self, t_z, t_text_embedding):\n\t\ttf.get_variable_scope().reuse_variables()\n\t\t\n\t\ts = self.options['image_size']\n\t\ts2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n\t\t\n\t\treduced_text_embedding = ops.lrelu( ops.linear(t_text_embedding, self.options['t_dim'], 'g_embedding') )\n\t\tz_concat = tf.concat(1, [t_z, reduced_text_embedding])\n\t\tz_ = ops.linear(z_concat, self.options['gf_dim']*8*s16*s16, 'g_h0_lin')\n\t\th0 = tf.reshape(z_, [-1, s16, s16, self.options['gf_dim'] * 8])\n\t\th0 = tf.nn.relu(self.g_bn0(h0, train = False))\n\t\t\n\t\th1 = ops.deconv2d(h0, [self.options['batch_size'], s8, s8, self.options['gf_dim']*4], name='g_h1')\n\t\th1 = tf.nn.relu(self.g_bn1(h1, train = False))\n\t\t\n\t\th2 = ops.deconv2d(h1, [self.options['batch_size'], s4, s4, self.options['gf_dim']*2], name='g_h2')\n\t\th2 = tf.nn.relu(self.g_bn2(h2, train = False))\n\t\t\n\t\th3 = ops.deconv2d(h2, [self.options['batch_size'], s2, s2, self.options['gf_dim']*1], name='g_h3')\n\t\th3 = tf.nn.relu(self.g_bn3(h3, train = False))\n\t\t\n\t\th4 = ops.deconv2d(h3, [self.options['batch_size'], s, s, 3], name='g_h4')\n\t\t\n\t\treturn (tf.tanh(h4)/2. + 0.5)\n\n\t# GENERATOR IMPLEMENTATION based on : https://github.com/carpedm20/DCGAN-tensorflow/blob/master/model.py\n\tdef generator(self, t_z, t_text_embedding):\n\t\t\n\t\ts = self.options['image_size']\n\t\ts2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n\t\t\n\t\treduced_text_embedding = ops.lrelu( ops.linear(t_text_embedding, self.options['t_dim'], 'g_embedding') )\n\t\tz_concat = tf.concat(1, [t_z, reduced_text_embedding])\n\t\tz_ = ops.linear(z_concat, self.options['gf_dim']*8*s16*s16, 'g_h0_lin')\n\t\th0 = tf.reshape(z_, [-1, s16, s16, self.options['gf_dim'] * 8])\n\t\th0 = tf.nn.relu(self.g_bn0(h0))\n\t\t\n\t\th1 = ops.deconv2d(h0, [self.options['batch_size'], s8, s8, self.options['gf_dim']*4], name='g_h1')\n\t\th1 = tf.nn.relu(self.g_bn1(h1))\n\t\t\n\t\th2 = ops.deconv2d(h1, [self.options['batch_size'], s4, s4, self.options['gf_dim']*2], name='g_h2')\n\t\th2 = tf.nn.relu(self.g_bn2(h2))\n\t\t\n\t\th3 = ops.deconv2d(h2, [self.options['batch_size'], s2, s2, self.options['gf_dim']*1], name='g_h3')\n\t\th3 = tf.nn.relu(self.g_bn3(h3))\n\t\t\n\t\th4 = ops.deconv2d(h3, [self.options['batch_size'], s, s, 3], name='g_h4')\n\t\t\n\t\treturn (tf.tanh(h4)/2. + 0.5)\n\n\t# DISCRIMINATOR IMPLEMENTATION based on : https://github.com/carpedm20/DCGAN-tensorflow/blob/master/model.py\n\tdef discriminator(self, image, t_text_embedding, reuse=False):\n\t\tif reuse:\n\t\t\ttf.get_variable_scope().reuse_variables()\n\n\t\th0 = ops.lrelu(ops.conv2d(image, self.options['df_dim'], name = 'd_h0_conv')) #32\n\t\th1 = ops.lrelu( self.d_bn1(ops.conv2d(h0, self.options['df_dim']*2, name = 'd_h1_conv'))) #16\n\t\th2 = ops.lrelu( self.d_bn2(ops.conv2d(h1, self.options['df_dim']*4, name = 'd_h2_conv'))) #8\n\t\th3 = ops.lrelu( self.d_bn3(ops.conv2d(h2, self.options['df_dim']*8, name = 'd_h3_conv'))) #4\n\t\t\n\t\t# ADD TEXT EMBEDDING TO THE NETWORK\n\t\treduced_text_embeddings = ops.lrelu(ops.linear(t_text_embedding, self.options['t_dim'], 'd_embedding'))\n\t\treduced_text_embeddings = tf.expand_dims(reduced_text_embeddings,1)\n\t\treduced_text_embeddings = tf.expand_dims(reduced_text_embeddings,2)\n\t\ttiled_embeddings = tf.tile(reduced_text_embeddings, [1,4,4,1], name='tiled_embeddings')\n\t\t\n\t\th3_concat = tf.concat( 3, [h3, tiled_embeddings], name='h3_concat')\n\t\th3_new = ops.lrelu( self.d_bn4(ops.conv2d(h3_concat, self.options['df_dim']*8, 1,1,1,1, name = 'd_h3_conv_new'))) #4\n\t\t\n\t\th4 = ops.linear(tf.reshape(h3_new, [self.options['batch_size'], -1]), 1, 'd_h3_lin')\n\t\t\n\t\treturn tf.nn.sigmoid(h4), h4"""
skipthoughts.py,1,"b'\'\'\'\nSkip-thought vectors\nhttps://github.com/ryankiros/skip-thoughts\n\'\'\'\nimport os\n\nimport theano\nimport theano.tensor as tensor\n\nimport cPickle as pkl\nimport numpy\nimport copy\nimport nltk\n\nfrom collections import OrderedDict, defaultdict\nfrom scipy.linalg import norm\nfrom nltk.tokenize import word_tokenize\n\nprofile = False\n\n#-----------------------------------------------------------------------------#\n# Specify model and table locations here\n#-----------------------------------------------------------------------------#\npath_to_models = \'Data/skipthoughts/\'\npath_to_tables = \'Data/skipthoughts/\'\n#-----------------------------------------------------------------------------#\n\npath_to_umodel = path_to_models + \'uni_skip.npz\'\npath_to_bmodel = path_to_models + \'bi_skip.npz\'\n\n\ndef load_model():\n\t""""""\n\tLoad the model with saved tables\n\t""""""\n\t# Load model options\n\tprint \'Loading model parameters...\'\n\twith open(\'%s.pkl\'%path_to_umodel, \'rb\') as f:\n\t\tuoptions = pkl.load(f)\n\twith open(\'%s.pkl\'%path_to_bmodel, \'rb\') as f:\n\t\tboptions = pkl.load(f)\n\n\t# Load parameters\n\tuparams = init_params(uoptions)\n\tuparams = load_params(path_to_umodel, uparams)\n\tutparams = init_tparams(uparams)\n\tbparams = init_params_bi(boptions)\n\tbparams = load_params(path_to_bmodel, bparams)\n\tbtparams = init_tparams(bparams)\n\n\t# Extractor functions\n\tprint \'Compiling encoders...\'\n\tembedding, x_mask, ctxw2v = build_encoder(utparams, uoptions)\n\tf_w2v = theano.function([embedding, x_mask], ctxw2v, name=\'f_w2v\')\n\tembedding, x_mask, ctxw2v = build_encoder_bi(btparams, boptions)\n\tf_w2v2 = theano.function([embedding, x_mask], ctxw2v, name=\'f_w2v2\')\n\n\t# Tables\n\tprint \'Loading tables...\'\n\tutable, btable = load_tables()\n\n\t# Store everything we need in a dictionary\n\tprint \'Packing up...\'\n\tmodel = {}\n\tmodel[\'uoptions\'] = uoptions\n\tmodel[\'boptions\'] = boptions\n\tmodel[\'utable\'] = utable\n\tmodel[\'btable\'] = btable\n\tmodel[\'f_w2v\'] = f_w2v\n\tmodel[\'f_w2v2\'] = f_w2v2\n\n\treturn model\n\n\ndef load_tables():\n\t""""""\n\tLoad the tables\n\t""""""\n\twords = []\n\tutable = numpy.load(path_to_tables + \'utable.npy\')\n\tbtable = numpy.load(path_to_tables + \'btable.npy\')\n\tf = open(path_to_tables + \'dictionary.txt\', \'rb\')\n\tfor line in f:\n\t\twords.append(line.decode(\'utf-8\').strip())\n\tf.close()\n\tutable = OrderedDict(zip(words, utable))\n\tbtable = OrderedDict(zip(words, btable))\n\treturn utable, btable\n\n\ndef encode(model, X, use_norm=True, verbose=True, batch_size=128, use_eos=False):\n\t""""""\n\tEncode sentences in the list X. Each entry will return a vector\n\t""""""\n\t# first, do preprocessing\n\tX = preprocess(X)\n\n\t# word dictionary and init\n\td = defaultdict(lambda : 0)\n\tfor w in model[\'utable\'].keys():\n\t\td[w] = 1\n\tufeatures = numpy.zeros((len(X), model[\'uoptions\'][\'dim\']), dtype=\'float32\')\n\tbfeatures = numpy.zeros((len(X), 2 * model[\'boptions\'][\'dim\']), dtype=\'float32\')\n\n\t# length dictionary\n\tds = defaultdict(list)\n\tcaptions = [s.split() for s in X]\n\tfor i,s in enumerate(captions):\n\t\tds[len(s)].append(i)\n\n\t# Get features. This encodes by length, in order to avoid wasting computation\n\tfor k in ds.keys():\n\t\tif verbose:\n\t\t\tprint k\n\t\tnumbatches = len(ds[k]) / batch_size + 1\n\t\tfor minibatch in range(numbatches):\n\t\t\tcaps = ds[k][minibatch::numbatches]\n\n\t\t\tif use_eos:\n\t\t\t\tuembedding = numpy.zeros((k+1, len(caps), model[\'uoptions\'][\'dim_word\']), dtype=\'float32\')\n\t\t\t\tbembedding = numpy.zeros((k+1, len(caps), model[\'boptions\'][\'dim_word\']), dtype=\'float32\')\n\t\t\telse:\n\t\t\t\tuembedding = numpy.zeros((k, len(caps), model[\'uoptions\'][\'dim_word\']), dtype=\'float32\')\n\t\t\t\tbembedding = numpy.zeros((k, len(caps), model[\'boptions\'][\'dim_word\']), dtype=\'float32\')\n\t\t\tfor ind, c in enumerate(caps):\n\t\t\t\tcaption = captions[c]\n\t\t\t\tfor j in range(len(caption)):\n\t\t\t\t\tif d[caption[j]] > 0:\n\t\t\t\t\t\tuembedding[j,ind] = model[\'utable\'][caption[j]]\n\t\t\t\t\t\tbembedding[j,ind] = model[\'btable\'][caption[j]]\n\t\t\t\t\telse:\n\t\t\t\t\t\tuembedding[j,ind] = model[\'utable\'][\'UNK\']\n\t\t\t\t\t\tbembedding[j,ind] = model[\'btable\'][\'UNK\']\n\t\t\t\tif use_eos:\n\t\t\t\t\tuembedding[-1,ind] = model[\'utable\'][\'<eos>\']\n\t\t\t\t\tbembedding[-1,ind] = model[\'btable\'][\'<eos>\']\n\t\t\tif use_eos:\n\t\t\t\tuff = model[\'f_w2v\'](uembedding, numpy.ones((len(caption)+1,len(caps)), dtype=\'float32\'))\n\t\t\t\tbff = model[\'f_w2v2\'](bembedding, numpy.ones((len(caption)+1,len(caps)), dtype=\'float32\'))\n\t\t\telse:\n\t\t\t\tuff = model[\'f_w2v\'](uembedding, numpy.ones((len(caption),len(caps)), dtype=\'float32\'))\n\t\t\t\tbff = model[\'f_w2v2\'](bembedding, numpy.ones((len(caption),len(caps)), dtype=\'float32\'))\n\t\t\tif use_norm:\n\t\t\t\tfor j in range(len(uff)):\n\t\t\t\t\tuff[j] /= norm(uff[j])\n\t\t\t\t\tbff[j] /= norm(bff[j])\n\t\t\tfor ind, c in enumerate(caps):\n\t\t\t\tufeatures[c] = uff[ind]\n\t\t\t\tbfeatures[c] = bff[ind]\n\t\n\tfeatures = numpy.c_[ufeatures, bfeatures]\n\treturn features\n\n\ndef preprocess(text):\n\t""""""\n\tPreprocess text for encoder\n\t""""""\n\tX = []\n\tsent_detector = nltk.data.load(\'tokenizers/punkt/english.pickle\')\n\tfor t in text:\n\t\tsents = sent_detector.tokenize(t)\n\t\tresult = \'\'\n\t\tfor s in sents:\n\t\t\ttokens = word_tokenize(s)\n\t\t\tresult += \' \' + \' \'.join(tokens)\n\t\tX.append(result)\n\treturn X\n\n\ndef nn(model, text, vectors, query, k=5):\n\t""""""\n\tReturn the nearest neighbour sentences to query\n\ttext: list of sentences\n\tvectors: the corresponding representations for text\n\tquery: a string to search\n\t""""""\n\tqf = encode(model, [query])\n\tqf /= norm(qf)\n\tscores = numpy.dot(qf, vectors.T).flatten()\n\tsorted_args = numpy.argsort(scores)[::-1]\n\tsentences = [text[a] for a in sorted_args[:k]]\n\tprint \'QUERY: \' + query\n\tprint \'NEAREST: \'\n\tfor i, s in enumerate(sentences):\n\t\tprint s, sorted_args[i]\n\n\ndef word_features(table):\n\t""""""\n\tExtract word features into a normalized matrix\n\t""""""\n\tfeatures = numpy.zeros((len(table), 620), dtype=\'float32\')\n\tkeys = table.keys()\n\tfor i in range(len(table)):\n\t\tf = table[keys[i]]\n\t\tfeatures[i] = f / norm(f)\n\treturn features\n\n\ndef nn_words(table, wordvecs, query, k=10):\n\t""""""\n\tGet the nearest neighbour words\n\t""""""\n\tkeys = table.keys()\n\tqf = table[query]\n\tscores = numpy.dot(qf, wordvecs.T).flatten()\n\tsorted_args = numpy.argsort(scores)[::-1]\n\twords = [keys[a] for a in sorted_args[:k]]\n\tprint \'QUERY: \' + query\n\tprint \'NEAREST: \'\n\tfor i, w in enumerate(words):\n\t\tprint w\n\n\ndef _p(pp, name):\n\t""""""\n\tmake prefix-appended name\n\t""""""\n\treturn \'%s_%s\'%(pp, name)\n\n\ndef init_tparams(params):\n\t""""""\n\tinitialize Theano shared variables according to the initial parameters\n\t""""""\n\ttparams = OrderedDict()\n\tfor kk, pp in params.iteritems():\n\t\ttparams[kk] = theano.shared(params[kk], name=kk)\n\treturn tparams\n\n\ndef load_params(path, params):\n\t""""""\n\tload parameters\n\t""""""\n\tpp = numpy.load(path)\n\tfor kk, vv in params.iteritems():\n\t\tif kk not in pp:\n\t\t\twarnings.warn(\'%s is not in the archive\'%kk)\n\t\t\tcontinue\n\t\tparams[kk] = pp[kk]\n\treturn params\n\n\n# layers: \'name\': (\'parameter initializer\', \'feedforward\')\nlayers = {\'gru\': (\'param_init_gru\', \'gru_layer\')}\n\ndef get_layer(name):\n\tfns = layers[name]\n\treturn (eval(fns[0]), eval(fns[1]))\n\n\ndef init_params(options):\n\t""""""\n\tinitialize all parameters needed for the encoder\n\t""""""\n\tparams = OrderedDict()\n\n\t# embedding\n\tparams[\'Wemb\'] = norm_weight(options[\'n_words_src\'], options[\'dim_word\'])\n\n\t# encoder: GRU\n\tparams = get_layer(options[\'encoder\'])[0](options, params, prefix=\'encoder\',\n\t\t\t\t\t\t\t\t\t\t\t  nin=options[\'dim_word\'], dim=options[\'dim\'])\n\treturn params\n\n\ndef init_params_bi(options):\n\t""""""\n\tinitialize all paramters needed for bidirectional encoder\n\t""""""\n\tparams = OrderedDict()\n\n\t# embedding\n\tparams[\'Wemb\'] = norm_weight(options[\'n_words_src\'], options[\'dim_word\'])\n\n\t# encoder: GRU\n\tparams = get_layer(options[\'encoder\'])[0](options, params, prefix=\'encoder\',\n\t\t\t\t\t\t\t\t\t\t\t  nin=options[\'dim_word\'], dim=options[\'dim\'])\n\tparams = get_layer(options[\'encoder\'])[0](options, params, prefix=\'encoder_r\',\n\t\t\t\t\t\t\t\t\t\t\t  nin=options[\'dim_word\'], dim=options[\'dim\'])\n\treturn params\n\n\ndef build_encoder(tparams, options):\n\t""""""\n\tbuild an encoder, given pre-computed word embeddings\n\t""""""\n\t# word embedding (source)\n\tembedding = tensor.tensor3(\'embedding\', dtype=\'float32\')\n\tx_mask = tensor.matrix(\'x_mask\', dtype=\'float32\')\n\n\t# encoder\n\tproj = get_layer(options[\'encoder\'])[1](tparams, embedding, options,\n\t\t\t\t\t\t\t\t\t\t\tprefix=\'encoder\',\n\t\t\t\t\t\t\t\t\t\t\tmask=x_mask)\n\tctx = proj[0][-1]\n\n\treturn embedding, x_mask, ctx\n\n\ndef build_encoder_bi(tparams, options):\n\t""""""\n\tbuild bidirectional encoder, given pre-computed word embeddings\n\t""""""\n\t# word embedding (source)\n\tembedding = tensor.tensor3(\'embedding\', dtype=\'float32\')\n\tembeddingr = embedding[::-1]\n\tx_mask = tensor.matrix(\'x_mask\', dtype=\'float32\')\n\txr_mask = x_mask[::-1]\n\n\t# encoder\n\tproj = get_layer(options[\'encoder\'])[1](tparams, embedding, options,\n\t\t\t\t\t\t\t\t\t\t\tprefix=\'encoder\',\n\t\t\t\t\t\t\t\t\t\t\tmask=x_mask)\n\tprojr = get_layer(options[\'encoder\'])[1](tparams, embeddingr, options,\n\t\t\t\t\t\t\t\t\t\t\t prefix=\'encoder_r\',\n\t\t\t\t\t\t\t\t\t\t\t mask=xr_mask)\n\n\tctx = tensor.concatenate([proj[0][-1], projr[0][-1]], axis=1)\n\n\treturn embedding, x_mask, ctx\n\n\n# some utilities\ndef ortho_weight(ndim):\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin,nout=None, scale=0.1, ortho=True):\n\tif nout == None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = numpy.random.uniform(low=-scale, high=scale, size=(nin, nout))\n\treturn W.astype(\'float32\')\n\n\ndef param_init_gru(options, params, prefix=\'gru\', nin=None, dim=None):\n\t""""""\n\tparameter init for GRU\n\t""""""\n\tif nin == None:\n\t\tnin = options[\'dim_proj\']\n\tif dim == None:\n\t\tdim = options[\'dim_proj\']\n\tW = numpy.concatenate([norm_weight(nin,dim),\n\t\t\t\t\t\t   norm_weight(nin,dim)], axis=1)\n\tparams[_p(prefix,\'W\')] = W\n\tparams[_p(prefix,\'b\')] = numpy.zeros((2 * dim,)).astype(\'float32\')\n\tU = numpy.concatenate([ortho_weight(dim),\n\t\t\t\t\t\t   ortho_weight(dim)], axis=1)\n\tparams[_p(prefix,\'U\')] = U\n\n\tWx = norm_weight(nin, dim)\n\tparams[_p(prefix,\'Wx\')] = Wx\n\tUx = ortho_weight(dim)\n\tparams[_p(prefix,\'Ux\')] = Ux\n\tparams[_p(prefix,\'bx\')] = numpy.zeros((dim,)).astype(\'float32\')\n\n\treturn params\n\n\ndef gru_layer(tparams, state_below, options, prefix=\'gru\', mask=None, **kwargs):\n\t""""""\n\tForward pass through GRU layer\n\t""""""\n\tnsteps = state_below.shape[0]\n\tif state_below.ndim == 3:\n\t\tn_samples = state_below.shape[1]\n\telse:\n\t\tn_samples = 1\n\n\tdim = tparams[_p(prefix,\'Ux\')].shape[1]\n\n\tif mask == None:\n\t\tmask = tensor.alloc(1., state_below.shape[0], 1)\n\n\tdef _slice(_x, n, dim):\n\t\tif _x.ndim == 3:\n\t\t\treturn _x[:, :, n*dim:(n+1)*dim]\n\t\treturn _x[:, n*dim:(n+1)*dim]\n\n\tstate_below_ = tensor.dot(state_below, tparams[_p(prefix, \'W\')]) + tparams[_p(prefix, \'b\')]\n\tstate_belowx = tensor.dot(state_below, tparams[_p(prefix, \'Wx\')]) + tparams[_p(prefix, \'bx\')]\n\tU = tparams[_p(prefix, \'U\')]\n\tUx = tparams[_p(prefix, \'Ux\')]\n\n\tdef _step_slice(m_, x_, xx_, h_, U, Ux):\n\t\tpreact = tensor.dot(h_, U)\n\t\tpreact += x_\n\n\t\tr = tensor.nnet.sigmoid(_slice(preact, 0, dim))\n\t\tu = tensor.nnet.sigmoid(_slice(preact, 1, dim))\n\n\t\tpreactx = tensor.dot(h_, Ux)\n\t\tpreactx = preactx * r\n\t\tpreactx = preactx + xx_\n\n\t\th = tensor.tanh(preactx)\n\n\t\th = u * h_ + (1. - u) * h\n\t\th = m_[:,None] * h + (1. - m_)[:,None] * h_\n\n\t\treturn h\n\n\tseqs = [mask, state_below_, state_belowx]\n\t_step = _step_slice\n\n\trval, updates = theano.scan(_step,\n\t\t\t\t\t\t\t\tsequences=seqs,\n\t\t\t\t\t\t\t\toutputs_info = [tensor.alloc(0., n_samples, dim)],\n\t\t\t\t\t\t\t\tnon_sequences = [tparams[_p(prefix, \'U\')],\n\t\t\t\t\t\t\t\t\t\t\t\t tparams[_p(prefix, \'Ux\')]],\n\t\t\t\t\t\t\t\tname=_p(prefix, \'_layers\'),\n\t\t\t\t\t\t\t\tn_steps=nsteps,\n\t\t\t\t\t\t\t\tprofile=profile,\n\t\t\t\t\t\t\t\tstrict=True)\n\trval = [rval]\n\treturn rval\n\n'"
train.py,5,"b'import tensorflow as tf\nimport numpy as np\nimport model\nimport argparse\nimport pickle\nfrom os.path import join\nimport h5py\nfrom Utils import image_processing\nimport scipy.misc\nimport random\nimport json\nimport os\nimport shutil\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'--z_dim\', type=int, default=100,\n\t\t\t\t\t   help=\'Noise dimension\')\n\n\tparser.add_argument(\'--t_dim\', type=int, default=256,\n\t\t\t\t\t   help=\'Text feature dimension\')\n\n\tparser.add_argument(\'--batch_size\', type=int, default=64,\n\t\t\t\t\t   help=\'Batch Size\')\n\n\tparser.add_argument(\'--image_size\', type=int, default=64,\n\t\t\t\t\t   help=\'Image Size a, a x a\')\n\n\tparser.add_argument(\'--gf_dim\', type=int, default=64,\n\t\t\t\t\t   help=\'Number of conv in the first layer gen.\')\n\n\tparser.add_argument(\'--df_dim\', type=int, default=64,\n\t\t\t\t\t   help=\'Number of conv in the first layer discr.\')\n\n\tparser.add_argument(\'--gfc_dim\', type=int, default=1024,\n\t\t\t\t\t   help=\'Dimension of gen untis for for fully connected layer 1024\')\n\n\tparser.add_argument(\'--caption_vector_length\', type=int, default=2400,\n\t\t\t\t\t   help=\'Caption Vector Length\')\n\n\tparser.add_argument(\'--data_dir\', type=str, default=""Data"",\n\t\t\t\t\t   help=\'Data Directory\')\n\n\tparser.add_argument(\'--learning_rate\', type=float, default=0.0002,\n\t\t\t\t\t   help=\'Learning Rate\')\n\n\tparser.add_argument(\'--beta1\', type=float, default=0.5,\n\t\t\t\t\t   help=\'Momentum for Adam Update\')\n\n\tparser.add_argument(\'--epochs\', type=int, default=600,\n\t\t\t\t\t   help=\'Max number of epochs\')\n\n\tparser.add_argument(\'--save_every\', type=int, default=30,\n\t\t\t\t\t   help=\'Save Model/Samples every x iterations over batches\')\n\n\tparser.add_argument(\'--resume_model\', type=str, default=None,\n                       help=\'Pre-Trained Model Path, to resume from\')\n\n\tparser.add_argument(\'--data_set\', type=str, default=""flowers"",\n                       help=\'Dat set: MS-COCO, flowers\')\n\n\targs = parser.parse_args()\n\tmodel_options = {\n\t\t\'z_dim\' : args.z_dim,\n\t\t\'t_dim\' : args.t_dim,\n\t\t\'batch_size\' : args.batch_size,\n\t\t\'image_size\' : args.image_size,\n\t\t\'gf_dim\' : args.gf_dim,\n\t\t\'df_dim\' : args.df_dim,\n\t\t\'gfc_dim\' : args.gfc_dim,\n\t\t\'caption_vector_length\' : args.caption_vector_length\n\t}\n\t\n\t\n\tgan = model.GAN(model_options)\n\tinput_tensors, variables, loss, outputs, checks = gan.build_model()\n\t\n\td_optim = tf.train.AdamOptimizer(args.learning_rate, beta1 = args.beta1).minimize(loss[\'d_loss\'], var_list=variables[\'d_vars\'])\n\tg_optim = tf.train.AdamOptimizer(args.learning_rate, beta1 = args.beta1).minimize(loss[\'g_loss\'], var_list=variables[\'g_vars\'])\n\t\n\tsess = tf.InteractiveSession()\n\ttf.initialize_all_variables().run()\n\t\n\tsaver = tf.train.Saver()\n\tif args.resume_model:\n\t\tsaver.restore(sess, args.resume_model)\n\t\n\tloaded_data = load_training_data(args.data_dir, args.data_set)\n\t\n\tfor i in range(args.epochs):\n\t\tbatch_no = 0\n\t\twhile batch_no*args.batch_size < loaded_data[\'data_length\']:\n\t\t\treal_images, wrong_images, caption_vectors, z_noise, image_files = get_training_batch(batch_no, args.batch_size, \n\t\t\t\targs.image_size, args.z_dim, args.caption_vector_length, \'train\', args.data_dir, args.data_set, loaded_data)\n\t\t\t\n\t\t\t# DISCR UPDATE\n\t\t\tcheck_ts = [ checks[\'d_loss1\'] , checks[\'d_loss2\'], checks[\'d_loss3\']]\n\t\t\t_, d_loss, gen, d1, d2, d3 = sess.run([d_optim, loss[\'d_loss\'], outputs[\'generator\']] + check_ts,\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tinput_tensors[\'t_real_image\'] : real_images,\n\t\t\t\t\tinput_tensors[\'t_wrong_image\'] : wrong_images,\n\t\t\t\t\tinput_tensors[\'t_real_caption\'] : caption_vectors,\n\t\t\t\t\tinput_tensors[\'t_z\'] : z_noise,\n\t\t\t\t})\n\t\t\t\n\t\t\tprint ""d1"", d1\n\t\t\tprint ""d2"", d2\n\t\t\tprint ""d3"", d3\n\t\t\tprint ""D"", d_loss\n\t\t\t\n\t\t\t# GEN UPDATE\n\t\t\t_, g_loss, gen = sess.run([g_optim, loss[\'g_loss\'], outputs[\'generator\']],\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tinput_tensors[\'t_real_image\'] : real_images,\n\t\t\t\t\tinput_tensors[\'t_wrong_image\'] : wrong_images,\n\t\t\t\t\tinput_tensors[\'t_real_caption\'] : caption_vectors,\n\t\t\t\t\tinput_tensors[\'t_z\'] : z_noise,\n\t\t\t\t})\n\n\t\t\t# GEN UPDATE TWICE, to make sure d_loss does not go to 0\n\t\t\t_, g_loss, gen = sess.run([g_optim, loss[\'g_loss\'], outputs[\'generator\']],\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tinput_tensors[\'t_real_image\'] : real_images,\n\t\t\t\t\tinput_tensors[\'t_wrong_image\'] : wrong_images,\n\t\t\t\t\tinput_tensors[\'t_real_caption\'] : caption_vectors,\n\t\t\t\t\tinput_tensors[\'t_z\'] : z_noise,\n\t\t\t\t})\n\t\t\t\n\t\t\tprint ""LOSSES"", d_loss, g_loss, batch_no, i, len(loaded_data[\'image_list\'])/ args.batch_size\n\t\t\tbatch_no += 1\n\t\t\tif (batch_no % args.save_every) == 0:\n\t\t\t\tprint ""Saving Images, Model""\n\t\t\t\tsave_for_vis(args.data_dir, real_images, gen, image_files)\n\t\t\t\tsave_path = saver.save(sess, ""Data/Models/latest_model_{}_temp.ckpt"".format(args.data_set))\n\t\tif i%5 == 0:\n\t\t\tsave_path = saver.save(sess, ""Data/Models/model_after_{}_epoch_{}.ckpt"".format(args.data_set, i))\n\ndef load_training_data(data_dir, data_set):\n\tif data_set == \'flowers\':\n\t\th = h5py.File(join(data_dir, \'flower_tv.hdf5\'))\n\t\tflower_captions = {}\n\t\tfor ds in h.iteritems():\n\t\t\tflower_captions[ds[0]] = np.array(ds[1])\n\t\timage_list = [key for key in flower_captions]\n\t\timage_list.sort()\n\n\t\timg_75 = int(len(image_list)*0.75)\n\t\ttraining_image_list = image_list[0:img_75]\n\t\trandom.shuffle(training_image_list)\n\t\t\n\t\treturn {\n\t\t\t\'image_list\' : training_image_list,\n\t\t\t\'captions\' : flower_captions,\n\t\t\t\'data_length\' : len(training_image_list)\n\t\t}\n\t\n\telse:\n\t\twith open(join(data_dir, \'meta_train.pkl\')) as f:\n\t\t\tmeta_data = pickle.load(f)\n\t\t# No preloading for MS-COCO\n\t\treturn meta_data\n\ndef save_for_vis(data_dir, real_images, generated_images, image_files):\n\t\n\tshutil.rmtree( join(data_dir, \'samples\') )\n\tos.makedirs( join(data_dir, \'samples\') )\n\n\tfor i in range(0, real_images.shape[0]):\n\t\treal_image_255 = np.zeros( (64,64,3), dtype=np.uint8)\n\t\treal_images_255 = (real_images[i,:,:,:])\n\t\tscipy.misc.imsave( join(data_dir, \'samples/{}_{}.jpg\'.format(i, image_files[i].split(\'/\')[-1] )) , real_images_255)\n\n\t\tfake_image_255 = np.zeros( (64,64,3), dtype=np.uint8)\n\t\tfake_images_255 = (generated_images[i,:,:,:])\n\t\tscipy.misc.imsave(join(data_dir, \'samples/fake_image_{}.jpg\'.format(i)), fake_images_255)\n\n\ndef get_training_batch(batch_no, batch_size, image_size, z_dim, \n\tcaption_vector_length, split, data_dir, data_set, loaded_data = None):\n\tif data_set == \'mscoco\':\n\t\twith h5py.File( join(data_dir, \'tvs/\'+split + \'_tvs_\' + str(batch_no))) as hf:\n\t\t\tcaption_vectors = np.array(hf.get(\'tv\'))\n\t\t\tcaption_vectors = caption_vectors[:,0:caption_vector_length]\n\t\twith h5py.File( join(data_dir, \'tvs/\'+split + \'_tv_image_id_\' + str(batch_no))) as hf:\n\t\t\timage_ids = np.array(hf.get(\'tv\'))\n\n\t\treal_images = np.zeros((batch_size, 64, 64, 3))\n\t\twrong_images = np.zeros((batch_size, 64, 64, 3))\n\t\t\n\t\timage_files = []\n\t\tfor idx, image_id in enumerate(image_ids):\n\t\t\timage_file = join(data_dir, \'%s2014/COCO_%s2014_%.12d.jpg\'%(split, split, image_id) )\n\t\t\timage_array = image_processing.load_image_array(image_file, image_size)\n\t\t\treal_images[idx,:,:,:] = image_array\n\t\t\timage_files.append(image_file)\n\t\t\n\t\t# TODO>> As of Now, wrong images are just shuffled real images.\n\t\tfirst_image = real_images[0,:,:,:]\n\t\tfor i in range(0, batch_size):\n\t\t\tif i < batch_size - 1:\n\t\t\t\twrong_images[i,:,:,:] = real_images[i+1,:,:,:]\n\t\t\telse:\n\t\t\t\twrong_images[i,:,:,:] = first_image\n\n\t\tz_noise = np.random.uniform(-1, 1, [batch_size, z_dim])\n\n\n\t\treturn real_images, wrong_images, caption_vectors, z_noise, image_files\n\n\tif data_set == \'flowers\':\n\t\treal_images = np.zeros((batch_size, 64, 64, 3))\n\t\twrong_images = np.zeros((batch_size, 64, 64, 3))\n\t\tcaptions = np.zeros((batch_size, caption_vector_length))\n\n\t\tcnt = 0\n\t\timage_files = []\n\t\tfor i in range(batch_no * batch_size, batch_no * batch_size + batch_size):\n\t\t\tidx = i % len(loaded_data[\'image_list\'])\n\t\t\timage_file =  join(data_dir, \'flowers/jpg/\'+loaded_data[\'image_list\'][idx])\n\t\t\timage_array = image_processing.load_image_array(image_file, image_size)\n\t\t\treal_images[cnt,:,:,:] = image_array\n\t\t\t\n\t\t\t# Improve this selection of wrong image\n\t\t\twrong_image_id = random.randint(0,len(loaded_data[\'image_list\'])-1)\n\t\t\twrong_image_file =  join(data_dir, \'flowers/jpg/\'+loaded_data[\'image_list\'][wrong_image_id])\n\t\t\twrong_image_array = image_processing.load_image_array(wrong_image_file, image_size)\n\t\t\twrong_images[cnt, :,:,:] = wrong_image_array\n\n\t\t\trandom_caption = random.randint(0,4)\n\t\t\tcaptions[cnt,:] = loaded_data[\'captions\'][ loaded_data[\'image_list\'][idx] ][ random_caption ][0:caption_vector_length]\n\t\t\timage_files.append( image_file )\n\t\t\tcnt += 1\n\n\t\tz_noise = np.random.uniform(-1, 1, [batch_size, z_dim])\n\t\treturn real_images, wrong_images, captions, z_noise, image_files\n\nif __name__ == \'__main__\':\n\tmain()\n'"
Python 3 Codes/data_loader.py,0,"b'import json\nimport os\nfrom os.path import join, isfile\nimport re\nimport numpy as np\nimport pickle\nimport argparse\nimport skipthoughts\nimport h5py\n# DID NOT TRAIN IT ON MS COCO YET\ndef save_caption_vectors_ms_coco(data_dir, split, batch_size):\n\tmeta_data = {}\n\tic_file = join(data_dir, \'annotations/captions_{}2014.json\'.format(split))\n\twith open(ic_file) as f:\n\t\tic_data = json.loads(f.read())\n\n\tmeta_data[\'data_length\'] = len(ic_data[\'annotations\'])\n\twith open(join(data_dir, \'meta_{}.pkl\'.format(split)), \'wb\') as f:\n\t\tpickle.dump(meta_data, f)\n\n\tmodel = skipthoughts.load_model()\n\tbatch_no = 0\n\tprint(""Total Batches"", len(ic_data[\'annotations\'])/batch_size)\n\n\twhile batch_no*batch_size < len(ic_data[\'annotations\']):\n\t\tcaptions = []\n\t\timage_ids = []\n\t\tidx = batch_no\n\t\tfor i in range(batch_no*batch_size, (batch_no+1)*batch_size):\n\t\t\tidx = i%len(ic_data[\'annotations\'])\n\t\t\tcaptions.append(ic_data[\'annotations\'][idx][\'caption\'])\n\t\t\timage_ids.append(ic_data[\'annotations\'][idx][\'image_id\'])\n\n\t\tprint(captions)\n\t\tprint(image_ids)\n\t\t# Thought Vectors\n\t\ttv_batch = skipthoughts.encode(model, captions)\n\t\th5f_tv_batch = h5py.File( join(data_dir, \'tvs/\'+split + \'_tvs_\' + str(batch_no)), \'w\')\n\t\th5f_tv_batch.create_dataset(\'tv\', data=tv_batch)\n\t\th5f_tv_batch.close()\n\n\t\th5f_tv_batch_image_ids = h5py.File( join(data_dir, \'tvs/\'+split + \'_tv_image_id_\' + str(batch_no)), \'w\')\n\t\th5f_tv_batch_image_ids.create_dataset(\'tv\', data=image_ids)\n\t\th5f_tv_batch_image_ids.close()\n\n\t\tprint(""Batches Done"", batch_no, len(ic_data[\'annotations\'])/batch_size)\n\t\tbatch_no += 1\n\n\ndef save_caption_vectors_flowers(data_dir):\n\timport time\n\t\n\timg_dir = join(data_dir, \'flowers/jpg\')\n\timage_files = [f for f in os.listdir(img_dir) if \'jpg\' in f]\n\tprint(image_files[300:400])\n\tprint(len(image_files))\n\timage_captions = { img_file : [] for img_file in image_files }\n\n\tcaption_dir = join(data_dir, \'flowers/text_c10\')\n\tclass_dirs = []\n\tfor i in range(1, 103):\n\t\tclass_dir_name = \'class_%.5d\'%(i)\n\t\tclass_dirs.append( join(caption_dir, class_dir_name))\n\n\tfor class_dir in class_dirs:\n\t\tcaption_files = [f for f in os.listdir(class_dir) if \'txt\' in f]\n\t\tfor cap_file in caption_files:\n\t\t\twith open(join(class_dir,cap_file)) as f:\n\t\t\t\tcaptions = f.read().split(\'\\n\')\n\t\t\timg_file = cap_file[0:11] + "".jpg""\n\t\t\t# 5 captions per image\n\t\t\timage_captions[img_file] += [cap for cap in captions if len(cap) > 0][0:5]\n\n\tprint(len(image_captions))\n\n\tmodel = skipthoughts.load_model()\n\tencoded_captions = {}\n\n\n\tfor i, img in enumerate(image_captions):\n\t\tst = time.time()\n\t\tencoded_captions[img] = skipthoughts.encode(model, image_captions[img])\n\t\tprint(i, len(image_captions), img)\n\t\tprint(""Seconds"", time.time() - st)\n\t\t\n\t\n\th = h5py.File(join(data_dir, \'flower_tv.hdf5\'))\n\tfor key in encoded_captions:\n\t\th.create_dataset(key, data=encoded_captions[key])\n\th.close()\n\t\t\t\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'--split\', type=str, default=\'train\',\n                       help=\'train/val\')\n\tparser.add_argument(\'--data_dir\', type=str, default=\'Data\',\n                       help=\'Data directory\')\n\tparser.add_argument(\'--batch_size\', type=int, default=64,\n                       help=\'Batch Size\')\n\tparser.add_argument(\'--data_set\', type=str, default=\'flowers\',\n                       help=\'Data Set : Flowers, MS-COCO\')\n\targs = parser.parse_args()\n\t\n\tif args.data_set == \'flowers\':\n\t\tsave_caption_vectors_flowers(args.data_dir)\n\telse:\n\t\tsave_caption_vectors_ms_coco(args.data_dir, args.split, args.batch_size)\n\nif __name__ == \'__main__\':\n\tmain()\n'"
Python 3 Codes/download_datasets.py,0,"b'# downloads/extracts datasets described in the README.md\n\nimport os\nimport sys\nimport errno\nimport tarfile\n\nif sys.version_info >= (3,):\n    from urllib.request import urlretrieve\nelse:\n    from urllib.request import urlretrieve\n\nDATA_DIR = \'Data\'\n\n\n# http://stackoverflow.com/questions/273192/how-to-check-if-a-directory-exists-and-create-it-if-necessary\ndef make_sure_path_exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            raise\n\n\ndef create_data_paths():\n    if not os.path.isdir(DATA_DIR):\n        raise EnvironmentError(\'Needs to be run from project directory containing \' + DATA_DIR)\n    needed_paths = [\n        os.path.join(DATA_DIR, \'samples\'),\n        os.path.join(DATA_DIR, \'val_samples\'),\n        os.path.join(DATA_DIR, \'Models\'),\n    ]\n    for p in needed_paths:\n        make_sure_path_exists(p)\n\n\n# adapted from http://stackoverflow.com/questions/51212/how-to-write-a-download-progress-indicator-in-python\ndef dl_progress_hook(count, blockSize, totalSize):\n    percent = int(count * blockSize * 100 / totalSize)\n    sys.stdout.write(""\\r"" + ""...%d%%"" % percent)\n    sys.stdout.flush()\n\n\ndef download_dataset(data_name):\n    if data_name == \'flowers\':\n        print(\'== Flowers dataset ==\')\n        flowers_dir = os.path.join(DATA_DIR, \'flowers\')\n        flowers_jpg_tgz = os.path.join(flowers_dir, \'102flowers.tgz\')\n        make_sure_path_exists(flowers_dir)\n\n        # the original google drive link at https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view\n        # from https://github.com/reedscot/icml2016 is problematic to download automatically, so included\n        # the text_c10 directory from that archive as a bzipped file in the repo\n        captions_tbz = os.path.join(DATA_DIR, \'flowers_text_c10.tar.bz2\')\n        print((\'Extracting \' + captions_tbz))\n        captions_tar = tarfile.open(captions_tbz, \'r:bz2\')\n        captions_tar.extractall(flowers_dir)\n\n        flowers_url = \'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\'\n        print((\'Downloading \' + flowers_jpg_tgz + \' from \' + flowers_url))\n        urlretrieve(flowers_url, flowers_jpg_tgz,\n                    reporthook=dl_progress_hook)\n        print((\'Extracting \' + flowers_jpg_tgz))\n        flowers_jpg_tar = tarfile.open(flowers_jpg_tgz, \'r:gz\')\n        flowers_jpg_tar.extractall(flowers_dir)  # archive contains jpg/ folder\n\n    elif data_name == \'skipthoughts\':\n        print(\'== Skipthoughts models ==\')\n        SKIPTHOUGHTS_DIR = os.path.join(DATA_DIR, \'skipthoughts\')\n        SKIPTHOUGHTS_BASE_URL = \'http://www.cs.toronto.edu/~rkiros/models/\'\n        make_sure_path_exists(SKIPTHOUGHTS_DIR)\n\n        # following https://github.com/ryankiros/skip-thoughts#getting-started\n        skipthoughts_files = [\n            \'dictionary.txt\', \'utable.npy\', \'btable.npy\', \'uni_skip.npz\', \'uni_skip.npz.pkl\', \'bi_skip.npz\',\n            \'bi_skip.npz.pkl\',\n        ]\n        for filename in skipthoughts_files:\n            src_url = SKIPTHOUGHTS_BASE_URL + filename\n            print((\'Downloading \' + src_url))\n            urlretrieve(src_url, os.path.join(SKIPTHOUGHTS_DIR, filename),\n                        reporthook=dl_progress_hook)\n\n    elif data_name == \'nltk_punkt\':\n        import nltk\n        print(\'== NLTK pre-trained Punkt tokenizer for English ==\')\n        nltk.download(\'punkt\')\n\n    elif data_name == \'pretrained_model\':\n        print(\'== Pretrained model ==\')\n        MODEL_DIR = os.path.join(DATA_DIR, \'Models\')\n        pretrained_model_filename = \'latest_model_flowers_temp.ckpt\'\n        src_url = \'https://bitbucket.org/paarth_neekhara/texttomimagemodel/raw/74a4bbaeee26fe31e148a54c4f495694680e2c31/\' + pretrained_model_filename\n        print((\'Downloading \' + src_url))\n        urlretrieve(\n            src_url,\n            os.path.join(MODEL_DIR, pretrained_model_filename),\n            reporthook=dl_progress_hook,\n        )\n\n    else:\n        raise ValueError(\'Unknown dataset name: \' + data_name)\n\n\ndef main():\n    create_data_paths()\n    # TODO: make configurable via command-line\n    download_dataset(\'flowers\')\n    download_dataset(\'skipthoughts\')\n    download_dataset(\'nltk_punkt\')\n    download_dataset(\'pretrained_model\')\n    print(\'Done\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Python 3 Codes/generate_images.py,2,"b'import tensorflow as tf\nimport numpy as np\nimport model\nimport argparse\nimport pickle\nfrom os.path import join\nimport h5py\nfrom Utils import image_processing\nimport scipy.misc\nimport random\nimport json\nimport os\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\'--z_dim\', type=int, default=100,\n\t\t\t\t\t   help=\'Noise Dimension\')\n\n\tparser.add_argument(\'--t_dim\', type=int, default=256,\n\t\t\t\t\t   help=\'Text feature dimension\')\n\n\tparser.add_argument(\'--image_size\', type=int, default=64,\n\t\t\t\t\t   help=\'Image Size\')\n\n\tparser.add_argument(\'--gf_dim\', type=int, default=64,\n\t\t\t\t\t   help=\'Number of conv in the first layer gen.\')\n\n\tparser.add_argument(\'--df_dim\', type=int, default=64,\n\t\t\t\t\t   help=\'Number of conv in the first layer discr.\')\n\n\tparser.add_argument(\'--gfc_dim\', type=int, default=1024,\n\t\t\t\t\t   help=\'Dimension of gen untis for for fully connected layer 1024\')\n\n\tparser.add_argument(\'--caption_vector_length\', type=int, default=2400,\n\t\t\t\t\t   help=\'Caption Vector Length\')\n\t\n\tparser.add_argument(\'--data_dir\', type=str, default=""Data"",\n\t\t\t\t\t   help=\'Data Directory\')\n\n\tparser.add_argument(\'--model_path\', type=str, default=\'Data/Models/latest_model_flowers_temp.ckpt\',\n                       help=\'Trained Model Path\')\n\n\tparser.add_argument(\'--n_images\', type=int, default=5,\n                       help=\'Number of Images per Caption\')\n\n\tparser.add_argument(\'--caption_thought_vectors\', type=str, default=\'Data/sample_caption_vectors.hdf5\',\n                       help=\'Caption Thought Vector File\')\n\n\t\n\targs = parser.parse_args()\n\tmodel_options = {\n\t\t\'z_dim\' : args.z_dim,\n\t\t\'t_dim\' : args.t_dim,\n\t\t\'batch_size\' : args.n_images,\n\t\t\'image_size\' : args.image_size,\n\t\t\'gf_dim\' : args.gf_dim,\n\t\t\'df_dim\' : args.df_dim,\n\t\t\'gfc_dim\' : args.gfc_dim,\n\t\t\'caption_vector_length\' : args.caption_vector_length\n\t}\n\n\tgan = model.GAN(model_options)\n\t_, _, _, _, _ = gan.build_model()\n\tsess = tf.InteractiveSession()\n\tsaver = tf.train.Saver()\n\tsaver.restore(sess, args.model_path)\n\t\n\tinput_tensors, outputs = gan.build_generator()\n\n\th = h5py.File( args.caption_thought_vectors )\n\tcaption_vectors = np.array(h[\'vectors\'])\n\tcaption_image_dic = {}\n\tfor cn, caption_vector in enumerate(caption_vectors):\n\n\t\tcaption_images = []\n\t\tz_noise = np.random.uniform(-1, 1, [args.n_images, args.z_dim])\n\t\tcaption = [ caption_vector[0:args.caption_vector_length] ] * args.n_images\n\t\t\n\t\t[ gen_image ] = sess.run( [ outputs[\'generator\'] ], \n\t\t\tfeed_dict = {\n\t\t\t\tinput_tensors[\'t_real_caption\'] : caption,\n\t\t\t\tinput_tensors[\'t_z\'] : z_noise,\n\t\t\t} )\n\t\t\n\t\tcaption_images = [gen_image[i,:,:,:] for i in range(0, args.n_images)]\n\t\tcaption_image_dic[ cn ] = caption_images\n\t\tprint(""Generated"", cn)\n\n\tfor f in os.listdir( join(args.data_dir, \'val_samples\')):\n\t\tif os.path.isfile(f):\n\t\t\tos.unlink(join(args.data_dir, \'val_samples/\' + f))\n\n\tfor cn in range(0, len(caption_vectors)):\n\t\tcaption_images = []\n\t\tfor i, im in enumerate( caption_image_dic[ cn ] ):\n\t\t\t# im_name = ""caption_{}_{}.jpg"".format(cn, i)\n\t\t\t# scipy.misc.imsave( join(args.data_dir, \'val_samples/{}\'.format(im_name)) , im)\n\t\t\tcaption_images.append( im )\n\t\t\tcaption_images.append( np.zeros((64, 5, 3)) )\n\t\tcombined_image = np.concatenate( caption_images[0:-1], axis = 1 )\n\t\tscipy.misc.imsave( join(args.data_dir, \'val_samples/combined_image_{}.jpg\'.format(cn)) , combined_image)\n\n\nif __name__ == \'__main__\':\n\tmain()\n'"
Python 3 Codes/generate_thought_vectors.py,0,"b""import os\nfrom os.path import join, isfile\nimport re\nimport numpy as np\nimport pickle\nimport argparse\nimport skipthoughts\nimport h5py\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--caption_file', type=str, default='Data/sample_captions.txt',\n\t\t\t\t\t   help='caption file')\n\tparser.add_argument('--data_dir', type=str, default='Data',\n\t\t\t\t\t   help='Data Directory')\n\n\targs = parser.parse_args()\n\twith open( args.caption_file ) as f:\n\t\tcaptions = f.read().split('\\n')\n\n\tcaptions = [cap for cap in captions if len(cap) > 0]\n\tprint(captions)\n\tmodel = skipthoughts.load_model()\n\tcaption_vectors = skipthoughts.encode(model, captions)\n\n\tif os.path.isfile(join(args.data_dir, 'sample_caption_vectors.hdf5')):\n\t\tos.remove(join(args.data_dir, 'sample_caption_vectors.hdf5'))\n\th = h5py.File(join(args.data_dir, 'sample_caption_vectors.hdf5'))\n\th.create_dataset('vectors', data=caption_vectors)\t\t\n\th.close()\n\nif __name__ == '__main__':\n\tmain()"""
Python 3 Codes/image_processing.py,0,"b""import numpy as np\nfrom scipy import misc\nimport random\nimport skimage\nimport skimage.io\nimport skimage.transform\n\ndef load_image_array(image_file, image_size):\n\timg = skimage.io.imread(image_file)\n\t# GRAYSCALE\n\tif len(img.shape) == 2:\n\t\timg_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n\t\timg_new[:,:,0] = img\n\t\timg_new[:,:,1] = img\n\t\timg_new[:,:,2] = img\n\t\timg = img_new\n\n\timg_resized = skimage.transform.resize(img, (image_size, image_size))\n\n\t# FLIP HORIZONTAL WIRH A PROBABILITY 0.5\n\tif random.random() > 0.5:\n\t\timg_resized = np.fliplr(img_resized)\n\t\n\t\n\treturn img_resized.astype('float32')\n\nif __name__ == '__main__':\n\t# TEST>>>\n\tarr = load_image_array('sample.jpg', 64)\n\tprint(arr.mean())\n\t# rev = np.fliplr(arr)\n\tmisc.imsave( 'rev.jpg', arr)"""
Python 3 Codes/model.py,33,"b""import tensorflow as tf\nfrom Utils import ops\n\nclass GAN:\n\t'''\n\tOPTIONS\n\tz_dim : Noise dimension 100\n\tt_dim : Text feature dimension 256\n\timage_size : Image Dimension 64\n\tgf_dim : Number of conv in the first layer generator 64\n\tdf_dim : Number of conv in the first layer discriminator 64\n\tgfc_dim : Dimension of gen untis for for fully connected layer 1024\n\tcaption_vector_length : Caption Vector Length 2400\n\tbatch_size : Batch Size 64\n\t'''\n\tdef __init__(self, options):\n\t\tself.options = options\n\n\t\tself.g_bn0 = ops.batch_norm(name='g_bn0')\n\t\tself.g_bn1 = ops.batch_norm(name='g_bn1')\n\t\tself.g_bn2 = ops.batch_norm(name='g_bn2')\n\t\tself.g_bn3 = ops.batch_norm(name='g_bn3')\n\n\t\tself.d_bn1 = ops.batch_norm(name='d_bn1')\n\t\tself.d_bn2 = ops.batch_norm(name='d_bn2')\n\t\tself.d_bn3 = ops.batch_norm(name='d_bn3')\n\t\tself.d_bn4 = ops.batch_norm(name='d_bn4')\n\n\n\tdef build_model(self):\n\t\timg_size = self.options['image_size']\n\t\tt_real_image = tf.placeholder('float32', [self.options['batch_size'],img_size, img_size, 3 ], name = 'real_image')\n\t\tt_wrong_image = tf.placeholder('float32', [self.options['batch_size'],img_size, img_size, 3 ], name = 'wrong_image')\n\t\tt_real_caption = tf.placeholder('float32', [self.options['batch_size'], self.options['caption_vector_length']], name = 'real_caption_input')\n\t\tt_z = tf.placeholder('float32', [self.options['batch_size'], self.options['z_dim']])\n\n\t\tfake_image = self.generator(t_z, t_real_caption)\n\t\t\n\t\tdisc_real_image, disc_real_image_logits   = self.discriminator(t_real_image, t_real_caption)\n\t\tdisc_wrong_image, disc_wrong_image_logits   = self.discriminator(t_wrong_image, t_real_caption, reuse = True)\n\t\tdisc_fake_image, disc_fake_image_logits   = self.discriminator(fake_image, t_real_caption, reuse = True)\n\t\t\n\t\tg_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_image_logits, tf.ones_like(disc_fake_image)))\n\t\t\n\t\td_loss1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_real_image_logits, tf.ones_like(disc_real_image)))\n\t\td_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_wrong_image_logits, tf.zeros_like(disc_wrong_image)))\n\t\td_loss3 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_fake_image_logits, tf.zeros_like(disc_fake_image)))\n\n\t\td_loss = d_loss1 + d_loss2 + d_loss3\n\n\t\tt_vars = tf.trainable_variables()\n\t\td_vars = [var for var in t_vars if 'd_' in var.name]\n\t\tg_vars = [var for var in t_vars if 'g_' in var.name]\n\n\t\tinput_tensors = {\n\t\t\t't_real_image' : t_real_image,\n\t\t\t't_wrong_image' : t_wrong_image,\n\t\t\t't_real_caption' : t_real_caption,\n\t\t\t't_z' : t_z\n\t\t}\n\n\t\tvariables = {\n\t\t\t'd_vars' : d_vars,\n\t\t\t'g_vars' : g_vars\n\t\t}\n\n\t\tloss = {\n\t\t\t'g_loss' : g_loss,\n\t\t\t'd_loss' : d_loss\n\t\t}\n\n\t\toutputs = {\n\t\t\t'generator' : fake_image\n\t\t}\n\n\t\tchecks = {\n\t\t\t'd_loss1': d_loss1,\n\t\t\t'd_loss2': d_loss2,\n\t\t\t'd_loss3' : d_loss3,\n\t\t\t'disc_real_image_logits' : disc_real_image_logits,\n\t\t\t'disc_wrong_image_logits' : disc_wrong_image,\n\t\t\t'disc_fake_image_logits' : disc_fake_image_logits\n\t\t}\n\t\t\n\t\treturn input_tensors, variables, loss, outputs, checks\n\n\tdef build_generator(self):\n\t\timg_size = self.options['image_size']\n\t\tt_real_caption = tf.placeholder('float32', [self.options['batch_size'], self.options['caption_vector_length']], name = 'real_caption_input')\n\t\tt_z = tf.placeholder('float32', [self.options['batch_size'], self.options['z_dim']])\n\t\tfake_image = self.sampler(t_z, t_real_caption)\n\t\t\n\t\tinput_tensors = {\n\t\t\t't_real_caption' : t_real_caption,\n\t\t\t't_z' : t_z\n\t\t}\n\t\t\n\t\toutputs = {\n\t\t\t'generator' : fake_image\n\t\t}\n\n\t\treturn input_tensors, outputs\n\n\t# Sample Images for a text embedding\n\tdef sampler(self, t_z, t_text_embedding):\n\t\ttf.get_variable_scope().reuse_variables()\n\t\t\n\t\ts = self.options['image_size']\n\t\ts2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n\t\t\n\t\treduced_text_embedding = ops.lrelu( ops.linear(t_text_embedding, self.options['t_dim'], 'g_embedding') )\n\t\tz_concat = tf.concat(1, [t_z, reduced_text_embedding])\n\t\tz_ = ops.linear(z_concat, self.options['gf_dim']*8*s16*s16, 'g_h0_lin')\n\t\th0 = tf.reshape(z_, [-1, s16, s16, self.options['gf_dim'] * 8])\n\t\th0 = tf.nn.relu(self.g_bn0(h0, train = False))\n\t\t\n\t\th1 = ops.deconv2d(h0, [self.options['batch_size'], s8, s8, self.options['gf_dim']*4], name='g_h1')\n\t\th1 = tf.nn.relu(self.g_bn1(h1, train = False))\n\t\t\n\t\th2 = ops.deconv2d(h1, [self.options['batch_size'], s4, s4, self.options['gf_dim']*2], name='g_h2')\n\t\th2 = tf.nn.relu(self.g_bn2(h2, train = False))\n\t\t\n\t\th3 = ops.deconv2d(h2, [self.options['batch_size'], s2, s2, self.options['gf_dim']*1], name='g_h3')\n\t\th3 = tf.nn.relu(self.g_bn3(h3, train = False))\n\t\t\n\t\th4 = ops.deconv2d(h3, [self.options['batch_size'], s, s, 3], name='g_h4')\n\t\t\n\t\treturn (tf.tanh(h4)/2. + 0.5)\n\n\t# GENERATOR IMPLEMENTATION based on : https://github.com/carpedm20/DCGAN-tensorflow/blob/master/model.py\n\tdef generator(self, t_z, t_text_embedding):\n\t\t\n\t\ts = self.options['image_size']\n\t\ts2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n\t\t\n\t\treduced_text_embedding = ops.lrelu( ops.linear(t_text_embedding, self.options['t_dim'], 'g_embedding') )\n\t\tz_concat = tf.concat(1, [t_z, reduced_text_embedding])\n\t\tz_ = ops.linear(z_concat, self.options['gf_dim']*8*s16*s16, 'g_h0_lin')\n\t\th0 = tf.reshape(z_, [-1, s16, s16, self.options['gf_dim'] * 8])\n\t\th0 = tf.nn.relu(self.g_bn0(h0))\n\t\t\n\t\th1 = ops.deconv2d(h0, [self.options['batch_size'], s8, s8, self.options['gf_dim']*4], name='g_h1')\n\t\th1 = tf.nn.relu(self.g_bn1(h1))\n\t\t\n\t\th2 = ops.deconv2d(h1, [self.options['batch_size'], s4, s4, self.options['gf_dim']*2], name='g_h2')\n\t\th2 = tf.nn.relu(self.g_bn2(h2))\n\t\t\n\t\th3 = ops.deconv2d(h2, [self.options['batch_size'], s2, s2, self.options['gf_dim']*1], name='g_h3')\n\t\th3 = tf.nn.relu(self.g_bn3(h3))\n\t\t\n\t\th4 = ops.deconv2d(h3, [self.options['batch_size'], s, s, 3], name='g_h4')\n\t\t\n\t\treturn (tf.tanh(h4)/2. + 0.5)\n\n\t# DISCRIMINATOR IMPLEMENTATION based on : https://github.com/carpedm20/DCGAN-tensorflow/blob/master/model.py\n\tdef discriminator(self, image, t_text_embedding, reuse=False):\n\t\tif reuse:\n\t\t\ttf.get_variable_scope().reuse_variables()\n\n\t\th0 = ops.lrelu(ops.conv2d(image, self.options['df_dim'], name = 'd_h0_conv')) #32\n\t\th1 = ops.lrelu( self.d_bn1(ops.conv2d(h0, self.options['df_dim']*2, name = 'd_h1_conv'))) #16\n\t\th2 = ops.lrelu( self.d_bn2(ops.conv2d(h1, self.options['df_dim']*4, name = 'd_h2_conv'))) #8\n\t\th3 = ops.lrelu( self.d_bn3(ops.conv2d(h2, self.options['df_dim']*8, name = 'd_h3_conv'))) #4\n\t\t\n\t\t# ADD TEXT EMBEDDING TO THE NETWORK\n\t\treduced_text_embeddings = ops.lrelu(ops.linear(t_text_embedding, self.options['t_dim'], 'd_embedding'))\n\t\treduced_text_embeddings = tf.expand_dims(reduced_text_embeddings,1)\n\t\treduced_text_embeddings = tf.expand_dims(reduced_text_embeddings,2)\n\t\ttiled_embeddings = tf.tile(reduced_text_embeddings, [1,4,4,1], name='tiled_embeddings')\n\t\t\n\t\th3_concat = tf.concat( 3, [h3, tiled_embeddings], name='h3_concat')\n\t\th3_new = ops.lrelu( self.d_bn4(ops.conv2d(h3_concat, self.options['df_dim']*8, 1,1,1,1, name = 'd_h3_conv_new'))) #4\n\t\t\n\t\th4 = ops.linear(tf.reshape(h3_new, [self.options['batch_size'], -1]), 1, 'd_h3_lin')\n\t\t\n\t\treturn tf.nn.sigmoid(h4), h4\n"""
Python 3 Codes/ops.py,36,"b'# RESUED CODE FROM https://github.com/carpedm20/DCGAN-tensorflow/blob/master/ops.py\nimport math\nimport numpy as np \nimport tensorflow as tf\n\nfrom tensorflow.python.framework import ops\n\n\nclass batch_norm(object):\n\t""""""Code modification of http://stackoverflow.com/a/33950177""""""\n\tdef __init__(self, epsilon=1e-5, momentum = 0.9, name=""batch_norm""):\n\t\twith tf.variable_scope(name):\n\t\t\tself.epsilon = epsilon\n\t\t\tself.momentum = momentum\n\n\t\t\tself.ema = tf.train.ExponentialMovingAverage(decay=self.momentum)\n\t\t\tself.name = name\n\n\tdef __call__(self, x, train=True):\n\t\tshape = x.get_shape().as_list()\n\n\t\tif train:\n\t\t\twith tf.variable_scope(self.name) as scope:\n\t\t\t\tself.beta = tf.get_variable(""beta"", [shape[-1]],\n\t\t\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.))\n\t\t\t\tself.gamma = tf.get_variable(""gamma"", [shape[-1]],\n\t\t\t\t\t\t\t\t\tinitializer=tf.random_normal_initializer(1., 0.02))\n\t\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\tbatch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n\t\t\t\texcept:\n\t\t\t\t\tbatch_mean, batch_var = tf.nn.moments(x, [0, 1], name=\'moments\')\n\t\t\t\t\t\n\t\t\t\tema_apply_op = self.ema.apply([batch_mean, batch_var])\n\t\t\t\tself.ema_mean, self.ema_var = self.ema.average(batch_mean), self.ema.average(batch_var)\n\n\t\t\t\twith tf.control_dependencies([ema_apply_op]):\n\t\t\t\t\tmean, var = tf.identity(batch_mean), tf.identity(batch_var)\n\t\telse:\n\t\t\tmean, var = self.ema_mean, self.ema_var\n\n\t\tnormed = tf.nn.batch_norm_with_global_normalization(\n\t\t\t\tx, mean, var, self.beta, self.gamma, self.epsilon, scale_after_normalization=True)\n\n\t\treturn normed\n\ndef binary_cross_entropy(preds, targets, name=None):\n\t""""""Computes binary cross entropy given `preds`.\n\tFor brevity, let `x = `, `z = targets`.  The logistic loss is\n\t\tloss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n\tArgs:\n\t\tpreds: A `Tensor` of type `float32` or `float64`.\n\t\ttargets: A `Tensor` of the same type and shape as `preds`.\n\t""""""\n\teps = 1e-12\n\twith ops.op_scope([preds, targets], name, ""bce_loss"") as name:\n\t\tpreds = ops.convert_to_tensor(preds, name=""preds"")\n\t\ttargets = ops.convert_to_tensor(targets, name=""targets"")\n\t\treturn tf.reduce_mean(-(targets * tf.log(preds + eps) +\n\t\t\t\t\t\t\t  (1. - targets) * tf.log(1. - preds + eps)))\n\ndef conv_cond_concat(x, y):\n\t""""""Concatenate conditioning vector on feature map axis.""""""\n\tx_shapes = x.get_shape()\n\ty_shapes = y.get_shape()\n\treturn tf.concat(3, [x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])])\n\ndef conv2d(input_, output_dim, \n\t\t   k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n\t\t   name=""conv2d""):\n\twith tf.variable_scope(name):\n\t\tw = tf.get_variable(\'w\', [k_h, k_w, input_.get_shape()[-1], output_dim],\n\t\t\t\t\t\t\tinitializer=tf.truncated_normal_initializer(stddev=stddev))\n\t\tconv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=\'SAME\')\n\n\t\tbiases = tf.get_variable(\'biases\', [output_dim], initializer=tf.constant_initializer(0.0))\n\t\tconv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n\t\treturn conv\n\ndef deconv2d(input_, output_shape,\n\t\t\t k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n\t\t\t name=""deconv2d"", with_w=False):\n\twith tf.variable_scope(name):\n\t\t# filter : [height, width, output_channels, in_channels]\n\t\tw = tf.get_variable(\'w\', [k_h, k_h, output_shape[-1], input_.get_shape()[-1]],\n\t\t\t\t\t\t\tinitializer=tf.random_normal_initializer(stddev=stddev))\n\t\t\n\t\ttry:\n\t\t\tdeconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n\t\t\t\t\t\t\t\tstrides=[1, d_h, d_w, 1])\n\n\t\t# Support for verisons of TensorFlow before 0.7.0\n\t\texcept AttributeError:\n\t\t\tdeconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n\t\t\t\t\t\t\t\tstrides=[1, d_h, d_w, 1])\n\n\t\tbiases = tf.get_variable(\'biases\', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n\t\tdeconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n\n\t\tif with_w:\n\t\t\treturn deconv, w, biases\n\t\telse:\n\t\t\treturn deconv\n\ndef lrelu(x, leak=0.2, name=""lrelu""):\n\treturn tf.maximum(x, leak*x)\n\ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n\tshape = input_.get_shape().as_list()\n\n\twith tf.variable_scope(scope or ""Linear""):\n\t\tmatrix = tf.get_variable(""Matrix"", [shape[1], output_size], tf.float32,\n\t\t\t\t\t\t\t\t tf.random_normal_initializer(stddev=stddev))\n\t\tbias = tf.get_variable(""bias"", [output_size],\n\t\t\tinitializer=tf.constant_initializer(bias_start))\n\t\tif with_w:\n\t\t\treturn tf.matmul(input_, matrix) + bias, matrix, bias\n\t\telse:\n\t\t\treturn tf.matmul(input_, matrix) + bias\n'"
Python 3 Codes/skipthoughts.py,1,"b'\'\'\'\nSkip-thought vectors\nhttps://github.com/ryankiros/skip-thoughts\n\'\'\'\nimport os\n\nimport theano\nimport theano.tensor as tensor\n\nimport pickle as pkl\nimport numpy\nimport copy\nimport nltk\n\nfrom collections import OrderedDict, defaultdict\nfrom scipy.linalg import norm\nfrom nltk.tokenize import word_tokenize\n\nprofile = False\n\n#-----------------------------------------------------------------------------#\n# Specify model and table locations here\n#-----------------------------------------------------------------------------#\npath_to_models = \'Data/skipthoughts/\'\npath_to_tables = \'Data/skipthoughts/\'\n#-----------------------------------------------------------------------------#\n\npath_to_umodel = path_to_models + \'uni_skip.npz\'\npath_to_bmodel = path_to_models + \'bi_skip.npz\'\n\n\ndef load_model():\n\t""""""\n\tLoad the model with saved tables\n\t""""""\n\t# Load model options\n\tprint(\'Loading model parameters...\')\n\twith open(\'%s.pkl\'%path_to_umodel, \'rb\') as f:\n\t\tuoptions = pkl.load(f)\n\twith open(\'%s.pkl\'%path_to_bmodel, \'rb\') as f:\n\t\tboptions = pkl.load(f)\n\n\t# Load parameters\n\tuparams = init_params(uoptions)\n\tuparams = load_params(path_to_umodel, uparams)\n\tutparams = init_tparams(uparams)\n\tbparams = init_params_bi(boptions)\n\tbparams = load_params(path_to_bmodel, bparams)\n\tbtparams = init_tparams(bparams)\n\n\t# Extractor functions\n\tprint(\'Compiling encoders...\')\n\tembedding, x_mask, ctxw2v = build_encoder(utparams, uoptions)\n\tf_w2v = theano.function([embedding, x_mask], ctxw2v, name=\'f_w2v\')\n\tembedding, x_mask, ctxw2v = build_encoder_bi(btparams, boptions)\n\tf_w2v2 = theano.function([embedding, x_mask], ctxw2v, name=\'f_w2v2\')\n\n\t# Tables\n\tprint(\'Loading tables...\')\n\tutable, btable = load_tables()\n\n\t# Store everything we need in a dictionary\n\tprint(\'Packing up...\')\n\tmodel = {}\n\tmodel[\'uoptions\'] = uoptions\n\tmodel[\'boptions\'] = boptions\n\tmodel[\'utable\'] = utable\n\tmodel[\'btable\'] = btable\n\tmodel[\'f_w2v\'] = f_w2v\n\tmodel[\'f_w2v2\'] = f_w2v2\n\n\treturn model\n\n\ndef load_tables():\n\t""""""\n\tLoad the tables\n\t""""""\n\twords = []\n\tutable = numpy.load(path_to_tables + \'utable.npy\',encoding=\'latin1\')\n\tbtable = numpy.load(path_to_tables + \'btable.npy\',encoding=\'latin1\')\n\tf = open(path_to_tables + \'dictionary.txt\', \'rb\')\n\tfor line in f:\n\t\twords.append(line.decode(\'utf-8\').strip())\n\tf.close()\n\tutable = OrderedDict(list(zip(words, utable)))\n\tbtable = OrderedDict(list(zip(words, btable)))\n\treturn utable, btable\n\n\ndef encode(model, X, use_norm=True, verbose=True, batch_size=128, use_eos=False):\n\t""""""\n\tEncode sentences in the list X. Each entry will return a vector\n\t""""""\n\t# first, do preprocessing\n\tX = preprocess(X)\n\n\t# word dictionary and init\n\td = defaultdict(lambda : 0)\n\tfor w in list(model[\'utable\'].keys()):\n\t\td[w] = 1\n\tufeatures = numpy.zeros((len(X), model[\'uoptions\'][\'dim\']), dtype=\'float32\')\n\tbfeatures = numpy.zeros((len(X), 2 * model[\'boptions\'][\'dim\']), dtype=\'float32\')\n\n\t# length dictionary\n\tds = defaultdict(list)\n\tcaptions = [s.split() for s in X]\n\tfor i,s in enumerate(captions):\n\t\tds[len(s)].append(i)\n\n\t# Get features. This encodes by length, in order to avoid wasting computation\n\tfor k in list(ds.keys()):\n\t\tif verbose:\n\t\t\tprint(k)\n\t\tnumbatches = len(ds[k]) // batch_size + 1\n\t\tfor minibatch in range(numbatches):\n\t\t\tcaps = ds[k][minibatch::numbatches]\n\n\t\t\tif use_eos:\n\t\t\t\tuembedding = numpy.zeros((k+1, len(caps), model[\'uoptions\'][\'dim_word\']), dtype=\'float32\')\n\t\t\t\tbembedding = numpy.zeros((k+1, len(caps), model[\'boptions\'][\'dim_word\']), dtype=\'float32\')\n\t\t\telse:\n\t\t\t\tuembedding = numpy.zeros((k, len(caps), model[\'uoptions\'][\'dim_word\']), dtype=\'float32\')\n\t\t\t\tbembedding = numpy.zeros((k, len(caps), model[\'boptions\'][\'dim_word\']), dtype=\'float32\')\n\t\t\tfor ind, c in enumerate(caps):\n\t\t\t\tcaption = captions[c]\n\t\t\t\tfor j in range(len(caption)):\n\t\t\t\t\tif d[caption[j]] > 0:\n\t\t\t\t\t\tuembedding[j,ind] = model[\'utable\'][caption[j]]\n\t\t\t\t\t\tbembedding[j,ind] = model[\'btable\'][caption[j]]\n\t\t\t\t\telse:\n\t\t\t\t\t\tuembedding[j,ind] = model[\'utable\'][\'UNK\']\n\t\t\t\t\t\tbembedding[j,ind] = model[\'btable\'][\'UNK\']\n\t\t\t\tif use_eos:\n\t\t\t\t\tuembedding[-1,ind] = model[\'utable\'][\'<eos>\']\n\t\t\t\t\tbembedding[-1,ind] = model[\'btable\'][\'<eos>\']\n\t\t\tif use_eos:\n\t\t\t\tuff = model[\'f_w2v\'](uembedding, numpy.ones((len(caption)+1,len(caps)), dtype=\'float32\'))\n\t\t\t\tbff = model[\'f_w2v2\'](bembedding, numpy.ones((len(caption)+1,len(caps)), dtype=\'float32\'))\n\t\t\telse:\n\t\t\t\tuff = model[\'f_w2v\'](uembedding, numpy.ones((len(caption),len(caps)), dtype=\'float32\'))\n\t\t\t\tbff = model[\'f_w2v2\'](bembedding, numpy.ones((len(caption),len(caps)), dtype=\'float32\'))\n\t\t\tif use_norm:\n\t\t\t\tfor j in range(len(uff)):\n\t\t\t\t\tuff[j] /= norm(uff[j])\n\t\t\t\t\tbff[j] /= norm(bff[j])\n\t\t\tfor ind, c in enumerate(caps):\n\t\t\t\tufeatures[c] = uff[ind]\n\t\t\t\tbfeatures[c] = bff[ind]\n\t\n\tfeatures = numpy.c_[ufeatures, bfeatures]\n\treturn features\n\n\ndef preprocess(text):\n\t""""""\n\tPreprocess text for encoder\n\t""""""\n\tX = []\n\tsent_detector = nltk.data.load(\'tokenizers/punkt/english.pickle\')\n\tfor t in text:\n\t\tsents = sent_detector.tokenize(t)\n\t\tresult = \'\'\n\t\tfor s in sents:\n\t\t\ttokens = word_tokenize(s)\n\t\t\tresult += \' \' + \' \'.join(tokens)\n\t\tX.append(result)\n\treturn X\n\n\ndef nn(model, text, vectors, query, k=5):\n\t""""""\n\tReturn the nearest neighbour sentences to query\n\ttext: list of sentences\n\tvectors: the corresponding representations for text\n\tquery: a string to search\n\t""""""\n\tqf = encode(model, [query])\n\tqf /= norm(qf)\n\tscores = numpy.dot(qf, vectors.T).flatten()\n\tsorted_args = numpy.argsort(scores)[::-1]\n\tsentences = [text[a] for a in sorted_args[:k]]\n\tprint((\'QUERY: \' + query))\n\tprint(\'NEAREST: \')\n\tfor i, s in enumerate(sentences):\n\t\tprint((s, sorted_args[i]))\n\n\ndef word_features(table):\n\t""""""\n\tExtract word features into a normalized matrix\n\t""""""\n\tfeatures = numpy.zeros((len(table), 620), dtype=\'float32\')\n\tkeys = list(table.keys())\n\tfor i in range(len(table)):\n\t\tf = table[keys[i]]\n\t\tfeatures[i] = f / norm(f)\n\treturn features\n\n\ndef nn_words(table, wordvecs, query, k=10):\n\t""""""\n\tGet the nearest neighbour words\n\t""""""\n\tkeys = list(table.keys())\n\tqf = table[query]\n\tscores = numpy.dot(qf, wordvecs.T).flatten()\n\tsorted_args = numpy.argsort(scores)[::-1]\n\twords = [keys[a] for a in sorted_args[:k]]\n\tprint((\'QUERY: \' + query))\n\tprint(\'NEAREST: \')\n\tfor i, w in enumerate(words):\n\t\tprint(w)\n\n\ndef _p(pp, name):\n\t""""""\n\tmake prefix-appended name\n\t""""""\n\treturn \'%s_%s\'%(pp, name)\n\n\ndef init_tparams(params):\n\t""""""\n\tinitialize Theano shared variables according to the initial parameters\n\t""""""\n\ttparams = OrderedDict()\n\tfor kk, pp in list(params.items()):\n\t\ttparams[kk] = theano.shared(params[kk], name=kk)\n\treturn tparams\n\n\ndef load_params(path, params):\n\t""""""\n\tload parameters\n\t""""""\n\tpp = numpy.load(path)\n\tfor kk, vv in list(params.items()):\n\t\tif kk not in pp:\n\t\t\twarnings.warn(\'%s is not in the archive\'%kk)\n\t\t\tcontinue\n\t\tparams[kk] = pp[kk]\n\treturn params\n\n\n# layers: \'name\': (\'parameter initializer\', \'feedforward\')\nlayers = {\'gru\': (\'param_init_gru\', \'gru_layer\')}\n\ndef get_layer(name):\n\tfns = layers[name]\n\treturn (eval(fns[0]), eval(fns[1]))\n\n\ndef init_params(options):\n\t""""""\n\tinitialize all parameters needed for the encoder\n\t""""""\n\tparams = OrderedDict()\n\n\t# embedding\n\tparams[\'Wemb\'] = norm_weight(options[\'n_words_src\'], options[\'dim_word\'])\n\n\t# encoder: GRU\n\tparams = get_layer(options[\'encoder\'])[0](options, params, prefix=\'encoder\',\n\t\t\t\t\t\t\t\t\t\t\t  nin=options[\'dim_word\'], dim=options[\'dim\'])\n\treturn params\n\n\ndef init_params_bi(options):\n\t""""""\n\tinitialize all paramters needed for bidirectional encoder\n\t""""""\n\tparams = OrderedDict()\n\n\t# embedding\n\tparams[\'Wemb\'] = norm_weight(options[\'n_words_src\'], options[\'dim_word\'])\n\n\t# encoder: GRU\n\tparams = get_layer(options[\'encoder\'])[0](options, params, prefix=\'encoder\',\n\t\t\t\t\t\t\t\t\t\t\t  nin=options[\'dim_word\'], dim=options[\'dim\'])\n\tparams = get_layer(options[\'encoder\'])[0](options, params, prefix=\'encoder_r\',\n\t\t\t\t\t\t\t\t\t\t\t  nin=options[\'dim_word\'], dim=options[\'dim\'])\n\treturn params\n\n\ndef build_encoder(tparams, options):\n\t""""""\n\tbuild an encoder, given pre-computed word embeddings\n\t""""""\n\t# word embedding (source)\n\tembedding = tensor.tensor3(\'embedding\', dtype=\'float32\')\n\tx_mask = tensor.matrix(\'x_mask\', dtype=\'float32\')\n\n\t# encoder\n\tproj = get_layer(options[\'encoder\'])[1](tparams, embedding, options,\n\t\t\t\t\t\t\t\t\t\t\tprefix=\'encoder\',\n\t\t\t\t\t\t\t\t\t\t\tmask=x_mask)\n\tctx = proj[0][-1]\n\n\treturn embedding, x_mask, ctx\n\n\ndef build_encoder_bi(tparams, options):\n\t""""""\n\tbuild bidirectional encoder, given pre-computed word embeddings\n\t""""""\n\t# word embedding (source)\n\tembedding = tensor.tensor3(\'embedding\', dtype=\'float32\')\n\tembeddingr = embedding[::-1]\n\tx_mask = tensor.matrix(\'x_mask\', dtype=\'float32\')\n\txr_mask = x_mask[::-1]\n\n\t# encoder\n\tproj = get_layer(options[\'encoder\'])[1](tparams, embedding, options,\n\t\t\t\t\t\t\t\t\t\t\tprefix=\'encoder\',\n\t\t\t\t\t\t\t\t\t\t\tmask=x_mask)\n\tprojr = get_layer(options[\'encoder\'])[1](tparams, embeddingr, options,\n\t\t\t\t\t\t\t\t\t\t\t prefix=\'encoder_r\',\n\t\t\t\t\t\t\t\t\t\t\t mask=xr_mask)\n\n\tctx = tensor.concatenate([proj[0][-1], projr[0][-1]], axis=1)\n\n\treturn embedding, x_mask, ctx\n\n\n# some utilities\ndef ortho_weight(ndim):\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin,nout=None, scale=0.1, ortho=True):\n\tif nout == None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = numpy.random.uniform(low=-scale, high=scale, size=(nin, nout))\n\treturn W.astype(\'float32\')\n\n\ndef param_init_gru(options, params, prefix=\'gru\', nin=None, dim=None):\n\t""""""\n\tparameter init for GRU\n\t""""""\n\tif nin == None:\n\t\tnin = options[\'dim_proj\']\n\tif dim == None:\n\t\tdim = options[\'dim_proj\']\n\tW = numpy.concatenate([norm_weight(nin,dim),\n\t\t\t\t\t\t   norm_weight(nin,dim)], axis=1)\n\tparams[_p(prefix,\'W\')] = W\n\tparams[_p(prefix,\'b\')] = numpy.zeros((2 * dim,)).astype(\'float32\')\n\tU = numpy.concatenate([ortho_weight(dim),\n\t\t\t\t\t\t   ortho_weight(dim)], axis=1)\n\tparams[_p(prefix,\'U\')] = U\n\n\tWx = norm_weight(nin, dim)\n\tparams[_p(prefix,\'Wx\')] = Wx\n\tUx = ortho_weight(dim)\n\tparams[_p(prefix,\'Ux\')] = Ux\n\tparams[_p(prefix,\'bx\')] = numpy.zeros((dim,)).astype(\'float32\')\n\n\treturn params\n\n\ndef gru_layer(tparams, state_below, options, prefix=\'gru\', mask=None, **kwargs):\n\t""""""\n\tForward pass through GRU layer\n\t""""""\n\tnsteps = state_below.shape[0]\n\tif state_below.ndim == 3:\n\t\tn_samples = state_below.shape[1]\n\telse:\n\t\tn_samples = 1\n\n\tdim = tparams[_p(prefix,\'Ux\')].shape[1]\n\n\tif mask == None:\n\t\tmask = tensor.alloc(1., state_below.shape[0], 1)\n\n\tdef _slice(_x, n, dim):\n\t\tif _x.ndim == 3:\n\t\t\treturn _x[:, :, n*dim:(n+1)*dim]\n\t\treturn _x[:, n*dim:(n+1)*dim]\n\n\tstate_below_ = tensor.dot(state_below, tparams[_p(prefix, \'W\')]) + tparams[_p(prefix, \'b\')]\n\tstate_belowx = tensor.dot(state_below, tparams[_p(prefix, \'Wx\')]) + tparams[_p(prefix, \'bx\')]\n\tU = tparams[_p(prefix, \'U\')]\n\tUx = tparams[_p(prefix, \'Ux\')]\n\n\tdef _step_slice(m_, x_, xx_, h_, U, Ux):\n\t\tpreact = tensor.dot(h_, U)\n\t\tpreact += x_\n\n\t\tr = tensor.nnet.sigmoid(_slice(preact, 0, dim))\n\t\tu = tensor.nnet.sigmoid(_slice(preact, 1, dim))\n\n\t\tpreactx = tensor.dot(h_, Ux)\n\t\tpreactx = preactx * r\n\t\tpreactx = preactx + xx_\n\n\t\th = tensor.tanh(preactx)\n\n\t\th = u * h_ + (1. - u) * h\n\t\th = m_[:,None] * h + (1. - m_)[:,None] * h_\n\n\t\treturn h\n\n\tseqs = [mask, state_below_, state_belowx]\n\t_step = _step_slice\n\n\trval, updates = theano.scan(_step,\n\t\t\t\t\t\t\t\tsequences=seqs,\n\t\t\t\t\t\t\t\toutputs_info = [tensor.alloc(0., n_samples, dim)],\n\t\t\t\t\t\t\t\tnon_sequences = [tparams[_p(prefix, \'U\')],\n\t\t\t\t\t\t\t\t\t\t\t\t tparams[_p(prefix, \'Ux\')]],\n\t\t\t\t\t\t\t\tname=_p(prefix, \'_layers\'),\n\t\t\t\t\t\t\t\tn_steps=nsteps,\n\t\t\t\t\t\t\t\tprofile=profile,\n\t\t\t\t\t\t\t\tstrict=True)\n\trval = [rval]\n\treturn rval\n\n'"
Python 3 Codes/train.py,5,"b'import tensorflow as tf\nimport numpy as np\nimport model\nimport argparse\nimport pickle\nfrom os.path import join\nimport h5py\nfrom Utils import image_processing\nimport scipy.misc\nimport random\nimport json\nimport os\nimport shutil\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'--z_dim\', type=int, default=100,\n\t\t\t\t\t   help=\'Noise dimension\')\n\n\tparser.add_argument(\'--t_dim\', type=int, default=256,\n\t\t\t\t\t   help=\'Text feature dimension\')\n\n\tparser.add_argument(\'--batch_size\', type=int, default=64,\n\t\t\t\t\t   help=\'Batch Size\')\n\n\tparser.add_argument(\'--image_size\', type=int, default=64,\n\t\t\t\t\t   help=\'Image Size a, a x a\')\n\n\tparser.add_argument(\'--gf_dim\', type=int, default=64,\n\t\t\t\t\t   help=\'Number of conv in the first layer gen.\')\n\n\tparser.add_argument(\'--df_dim\', type=int, default=64,\n\t\t\t\t\t   help=\'Number of conv in the first layer discr.\')\n\n\tparser.add_argument(\'--gfc_dim\', type=int, default=1024,\n\t\t\t\t\t   help=\'Dimension of gen untis for for fully connected layer 1024\')\n\n\tparser.add_argument(\'--caption_vector_length\', type=int, default=2400,\n\t\t\t\t\t   help=\'Caption Vector Length\')\n\n\tparser.add_argument(\'--data_dir\', type=str, default=""Data"",\n\t\t\t\t\t   help=\'Data Directory\')\n\n\tparser.add_argument(\'--learning_rate\', type=float, default=0.0002,\n\t\t\t\t\t   help=\'Learning Rate\')\n\n\tparser.add_argument(\'--beta1\', type=float, default=0.5,\n\t\t\t\t\t   help=\'Momentum for Adam Update\')\n\n\tparser.add_argument(\'--epochs\', type=int, default=600,\n\t\t\t\t\t   help=\'Max number of epochs\')\n\n\tparser.add_argument(\'--save_every\', type=int, default=30,\n\t\t\t\t\t   help=\'Save Model/Samples every x iterations over batches\')\n\n\tparser.add_argument(\'--resume_model\', type=str, default=None,\n                       help=\'Pre-Trained Model Path, to resume from\')\n\n\tparser.add_argument(\'--data_set\', type=str, default=""flowers"",\n                       help=\'Dat set: MS-COCO, flowers\')\n\n\targs = parser.parse_args()\n\tmodel_options = {\n\t\t\'z_dim\' : args.z_dim,\n\t\t\'t_dim\' : args.t_dim,\n\t\t\'batch_size\' : args.batch_size,\n\t\t\'image_size\' : args.image_size,\n\t\t\'gf_dim\' : args.gf_dim,\n\t\t\'df_dim\' : args.df_dim,\n\t\t\'gfc_dim\' : args.gfc_dim,\n\t\t\'caption_vector_length\' : args.caption_vector_length\n\t}\n\t\n\t\n\tgan = model.GAN(model_options)\n\tinput_tensors, variables, loss, outputs, checks = gan.build_model()\n\t\n\td_optim = tf.train.AdamOptimizer(args.learning_rate, beta1 = args.beta1).minimize(loss[\'d_loss\'], var_list=variables[\'d_vars\'])\n\tg_optim = tf.train.AdamOptimizer(args.learning_rate, beta1 = args.beta1).minimize(loss[\'g_loss\'], var_list=variables[\'g_vars\'])\n\t\n\tsess = tf.InteractiveSession()\n\ttf.initialize_all_variables().run()\n\t\n\tsaver = tf.train.Saver()\n\tif args.resume_model:\n\t\tsaver.restore(sess, args.resume_model)\n\t\n\tloaded_data = load_training_data(args.data_dir, args.data_set)\n\t\n\tfor i in range(args.epochs):\n\t\tbatch_no = 0\n\t\twhile batch_no*args.batch_size < loaded_data[\'data_length\']:\n\t\t\treal_images, wrong_images, caption_vectors, z_noise, image_files = get_training_batch(batch_no, args.batch_size, \n\t\t\t\targs.image_size, args.z_dim, args.caption_vector_length, \'train\', args.data_dir, args.data_set, loaded_data)\n\t\t\t\n\t\t\t# DISCR UPDATE\n\t\t\tcheck_ts = [ checks[\'d_loss1\'] , checks[\'d_loss2\'], checks[\'d_loss3\']]\n\t\t\t_, d_loss, gen, d1, d2, d3 = sess.run([d_optim, loss[\'d_loss\'], outputs[\'generator\']] + check_ts,\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tinput_tensors[\'t_real_image\'] : real_images,\n\t\t\t\t\tinput_tensors[\'t_wrong_image\'] : wrong_images,\n\t\t\t\t\tinput_tensors[\'t_real_caption\'] : caption_vectors,\n\t\t\t\t\tinput_tensors[\'t_z\'] : z_noise,\n\t\t\t\t})\n\t\t\t\n\t\t\tprint(""d1"", d1)\n\t\t\tprint(""d2"", d2)\n\t\t\tprint(""d3"", d3)\n\t\t\tprint(""D"", d_loss)\n\t\t\t\n\t\t\t# GEN UPDATE\n\t\t\t_, g_loss, gen = sess.run([g_optim, loss[\'g_loss\'], outputs[\'generator\']],\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tinput_tensors[\'t_real_image\'] : real_images,\n\t\t\t\t\tinput_tensors[\'t_wrong_image\'] : wrong_images,\n\t\t\t\t\tinput_tensors[\'t_real_caption\'] : caption_vectors,\n\t\t\t\t\tinput_tensors[\'t_z\'] : z_noise,\n\t\t\t\t})\n\n\t\t\t# GEN UPDATE TWICE, to make sure d_loss does not go to 0\n\t\t\t_, g_loss, gen = sess.run([g_optim, loss[\'g_loss\'], outputs[\'generator\']],\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tinput_tensors[\'t_real_image\'] : real_images,\n\t\t\t\t\tinput_tensors[\'t_wrong_image\'] : wrong_images,\n\t\t\t\t\tinput_tensors[\'t_real_caption\'] : caption_vectors,\n\t\t\t\t\tinput_tensors[\'t_z\'] : z_noise,\n\t\t\t\t})\n\t\t\t\n\t\t\tprint(""LOSSES"", d_loss, g_loss, batch_no, i, len(loaded_data[\'image_list\'])/ args.batch_size)\n\t\t\tbatch_no += 1\n\t\t\tif (batch_no % args.save_every) == 0:\n\t\t\t\tprint(""Saving Images, Model"")\n\t\t\t\tsave_for_vis(args.data_dir, real_images, gen, image_files)\n\t\t\t\tsave_path = saver.save(sess, ""Data/Models/latest_model_{}_temp.ckpt"".format(args.data_set))\n\t\tif i%5 == 0:\n\t\t\tsave_path = saver.save(sess, ""Data/Models/model_after_{}_epoch_{}.ckpt"".format(args.data_set, i))\n\ndef load_training_data(data_dir, data_set):\n\tif data_set == \'flowers\':\n\t\th = h5py.File(join(data_dir, \'flower_tv.hdf5\'))\n\t\tflower_captions = {}\n\t\tfor ds in h.items():\n\t\t\tflower_captions[ds[0]] = np.array(ds[1])\n\t\timage_list = [key for key in flower_captions]\n\t\timage_list.sort()\n\n\t\timg_75 = int(len(image_list)*0.75)\n\t\ttraining_image_list = image_list[0:img_75]\n\t\trandom.shuffle(training_image_list)\n\t\t\n\t\treturn {\n\t\t\t\'image_list\' : training_image_list,\n\t\t\t\'captions\' : flower_captions,\n\t\t\t\'data_length\' : len(training_image_list)\n\t\t}\n\t\n\telse:\n\t\twith open(join(data_dir, \'meta_train.pkl\')) as f:\n\t\t\tmeta_data = pickle.load(f)\n\t\t# No preloading for MS-COCO\n\t\treturn meta_data\n\ndef save_for_vis(data_dir, real_images, generated_images, image_files):\n\t\n\tshutil.rmtree( join(data_dir, \'samples\') )\n\tos.makedirs( join(data_dir, \'samples\') )\n\n\tfor i in range(0, real_images.shape[0]):\n\t\treal_image_255 = np.zeros( (64,64,3), dtype=np.uint8)\n\t\treal_images_255 = (real_images[i,:,:,:])\n\t\tscipy.misc.imsave( join(data_dir, \'samples/{}_{}.jpg\'.format(i, image_files[i].split(\'/\')[-1] )) , real_images_255)\n\n\t\tfake_image_255 = np.zeros( (64,64,3), dtype=np.uint8)\n\t\tfake_images_255 = (generated_images[i,:,:,:])\n\t\tscipy.misc.imsave(join(data_dir, \'samples/fake_image_{}.jpg\'.format(i)), fake_images_255)\n\n\ndef get_training_batch(batch_no, batch_size, image_size, z_dim, \n\tcaption_vector_length, split, data_dir, data_set, loaded_data = None):\n\tif data_set == \'mscoco\':\n\t\twith h5py.File( join(data_dir, \'tvs/\'+split + \'_tvs_\' + str(batch_no))) as hf:\n\t\t\tcaption_vectors = np.array(hf.get(\'tv\'))\n\t\t\tcaption_vectors = caption_vectors[:,0:caption_vector_length]\n\t\twith h5py.File( join(data_dir, \'tvs/\'+split + \'_tv_image_id_\' + str(batch_no))) as hf:\n\t\t\timage_ids = np.array(hf.get(\'tv\'))\n\n\t\treal_images = np.zeros((batch_size, 64, 64, 3))\n\t\twrong_images = np.zeros((batch_size, 64, 64, 3))\n\t\t\n\t\timage_files = []\n\t\tfor idx, image_id in enumerate(image_ids):\n\t\t\timage_file = join(data_dir, \'%s2014/COCO_%s2014_%.12d.jpg\'%(split, split, image_id) )\n\t\t\timage_array = image_processing.load_image_array(image_file, image_size)\n\t\t\treal_images[idx,:,:,:] = image_array\n\t\t\timage_files.append(image_file)\n\t\t\n\t\t# TODO>> As of Now, wrong images are just shuffled real images.\n\t\tfirst_image = real_images[0,:,:,:]\n\t\tfor i in range(0, batch_size):\n\t\t\tif i < batch_size - 1:\n\t\t\t\twrong_images[i,:,:,:] = real_images[i+1,:,:,:]\n\t\t\telse:\n\t\t\t\twrong_images[i,:,:,:] = first_image\n\n\t\tz_noise = np.random.uniform(-1, 1, [batch_size, z_dim])\n\n\n\t\treturn real_images, wrong_images, caption_vectors, z_noise, image_files\n\n\tif data_set == \'flowers\':\n\t\treal_images = np.zeros((batch_size, 64, 64, 3))\n\t\twrong_images = np.zeros((batch_size, 64, 64, 3))\n\t\tcaptions = np.zeros((batch_size, caption_vector_length))\n\n\t\tcnt = 0\n\t\timage_files = []\n\t\tfor i in range(batch_no * batch_size, batch_no * batch_size + batch_size):\n\t\t\tidx = i % len(loaded_data[\'image_list\'])\n\t\t\timage_file =  join(data_dir, \'flowers/jpg/\'+loaded_data[\'image_list\'][idx])\n\t\t\timage_array = image_processing.load_image_array(image_file, image_size)\n\t\t\treal_images[cnt,:,:,:] = image_array\n\t\t\t\n\t\t\t# Improve this selection of wrong image\n\t\t\twrong_image_id = random.randint(0,len(loaded_data[\'image_list\'])-1)\n\t\t\twrong_image_file =  join(data_dir, \'flowers/jpg/\'+loaded_data[\'image_list\'][wrong_image_id])\n\t\t\twrong_image_array = image_processing.load_image_array(wrong_image_file, image_size)\n\t\t\twrong_images[cnt, :,:,:] = wrong_image_array\n\n\t\t\trandom_caption = random.randint(0,4)\n\t\t\tcaptions[cnt,:] = loaded_data[\'captions\'][ loaded_data[\'image_list\'][idx] ][ random_caption ][0:caption_vector_length]\n\t\t\timage_files.append( image_file )\n\t\t\tcnt += 1\n\n\t\tz_noise = np.random.uniform(-1, 1, [batch_size, z_dim])\n\t\treturn real_images, wrong_images, captions, z_noise, image_files\n\nif __name__ == \'__main__\':\n\tmain()\n'"
Utils/__init__.py,0,b''
Utils/image_processing.py,0,"b""import numpy as np\nfrom scipy import misc\nimport random\nimport skimage\nimport skimage.io\nimport skimage.transform\n\ndef load_image_array(image_file, image_size):\n\timg = skimage.io.imread(image_file)\n\t# GRAYSCALE\n\tif len(img.shape) == 2:\n\t\timg_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n\t\timg_new[:,:,0] = img\n\t\timg_new[:,:,1] = img\n\t\timg_new[:,:,2] = img\n\t\timg = img_new\n\n\timg_resized = skimage.transform.resize(img, (image_size, image_size))\n\n\t# FLIP HORIZONTAL WIRH A PROBABILITY 0.5\n\tif random.random() > 0.5:\n\t\timg_resized = np.fliplr(img_resized)\n\t\n\t\n\treturn img_resized.astype('float32')\n\nif __name__ == '__main__':\n\t# TEST>>>\n\tarr = load_image_array('sample.jpg', 64)\n\tprint arr.mean()\n\t# rev = np.fliplr(arr)\n\tmisc.imsave( 'rev.jpg', arr)"""
Utils/ops.py,36,"b'# RESUED CODE FROM https://github.com/carpedm20/DCGAN-tensorflow/blob/master/ops.py\nimport math\nimport numpy as np \nimport tensorflow as tf\n\nfrom tensorflow.python.framework import ops\n\n\nclass batch_norm(object):\n\t""""""Code modification of http://stackoverflow.com/a/33950177""""""\n\tdef __init__(self, epsilon=1e-5, momentum = 0.9, name=""batch_norm""):\n\t\twith tf.variable_scope(name):\n\t\t\tself.epsilon = epsilon\n\t\t\tself.momentum = momentum\n\n\t\t\tself.ema = tf.train.ExponentialMovingAverage(decay=self.momentum)\n\t\t\tself.name = name\n\n\tdef __call__(self, x, train=True):\n\t\tshape = x.get_shape().as_list()\n\n\t\tif train:\n\t\t\twith tf.variable_scope(self.name) as scope:\n\t\t\t\tself.beta = tf.get_variable(""beta"", [shape[-1]],\n\t\t\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.))\n\t\t\t\tself.gamma = tf.get_variable(""gamma"", [shape[-1]],\n\t\t\t\t\t\t\t\t\tinitializer=tf.random_normal_initializer(1., 0.02))\n\t\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\tbatch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n\t\t\t\texcept:\n\t\t\t\t\tbatch_mean, batch_var = tf.nn.moments(x, [0, 1], name=\'moments\')\n\t\t\t\t\t\n\t\t\t\tema_apply_op = self.ema.apply([batch_mean, batch_var])\n\t\t\t\tself.ema_mean, self.ema_var = self.ema.average(batch_mean), self.ema.average(batch_var)\n\n\t\t\t\twith tf.control_dependencies([ema_apply_op]):\n\t\t\t\t\tmean, var = tf.identity(batch_mean), tf.identity(batch_var)\n\t\telse:\n\t\t\tmean, var = self.ema_mean, self.ema_var\n\n\t\tnormed = tf.nn.batch_norm_with_global_normalization(\n\t\t\t\tx, mean, var, self.beta, self.gamma, self.epsilon, scale_after_normalization=True)\n\n\t\treturn normed\n\ndef binary_cross_entropy(preds, targets, name=None):\n\t""""""Computes binary cross entropy given `preds`.\n\n\tFor brevity, let `x = `, `z = targets`.  The logistic loss is\n\n\t\tloss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n\n\tArgs:\n\t\tpreds: A `Tensor` of type `float32` or `float64`.\n\t\ttargets: A `Tensor` of the same type and shape as `preds`.\n\t""""""\n\teps = 1e-12\n\twith ops.op_scope([preds, targets], name, ""bce_loss"") as name:\n\t\tpreds = ops.convert_to_tensor(preds, name=""preds"")\n\t\ttargets = ops.convert_to_tensor(targets, name=""targets"")\n\t\treturn tf.reduce_mean(-(targets * tf.log(preds + eps) +\n\t\t\t\t\t\t\t  (1. - targets) * tf.log(1. - preds + eps)))\n\ndef conv_cond_concat(x, y):\n\t""""""Concatenate conditioning vector on feature map axis.""""""\n\tx_shapes = x.get_shape()\n\ty_shapes = y.get_shape()\n\treturn tf.concat(3, [x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])])\n\ndef conv2d(input_, output_dim, \n\t\t   k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n\t\t   name=""conv2d""):\n\twith tf.variable_scope(name):\n\t\tw = tf.get_variable(\'w\', [k_h, k_w, input_.get_shape()[-1], output_dim],\n\t\t\t\t\t\t\tinitializer=tf.truncated_normal_initializer(stddev=stddev))\n\t\tconv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=\'SAME\')\n\n\t\tbiases = tf.get_variable(\'biases\', [output_dim], initializer=tf.constant_initializer(0.0))\n\t\tconv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n\t\treturn conv\n\ndef deconv2d(input_, output_shape,\n\t\t\t k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n\t\t\t name=""deconv2d"", with_w=False):\n\twith tf.variable_scope(name):\n\t\t# filter : [height, width, output_channels, in_channels]\n\t\tw = tf.get_variable(\'w\', [k_h, k_h, output_shape[-1], input_.get_shape()[-1]],\n\t\t\t\t\t\t\tinitializer=tf.random_normal_initializer(stddev=stddev))\n\t\t\n\t\ttry:\n\t\t\tdeconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n\t\t\t\t\t\t\t\tstrides=[1, d_h, d_w, 1])\n\n\t\t# Support for verisons of TensorFlow before 0.7.0\n\t\texcept AttributeError:\n\t\t\tdeconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n\t\t\t\t\t\t\t\tstrides=[1, d_h, d_w, 1])\n\n\t\tbiases = tf.get_variable(\'biases\', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n\t\tdeconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n\n\t\tif with_w:\n\t\t\treturn deconv, w, biases\n\t\telse:\n\t\t\treturn deconv\n\ndef lrelu(x, leak=0.2, name=""lrelu""):\n\treturn tf.maximum(x, leak*x)\n\ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n\tshape = input_.get_shape().as_list()\n\n\twith tf.variable_scope(scope or ""Linear""):\n\t\tmatrix = tf.get_variable(""Matrix"", [shape[1], output_size], tf.float32,\n\t\t\t\t\t\t\t\t tf.random_normal_initializer(stddev=stddev))\n\t\tbias = tf.get_variable(""bias"", [output_size],\n\t\t\tinitializer=tf.constant_initializer(bias_start))\n\t\tif with_w:\n\t\t\treturn tf.matmul(input_, matrix) + bias, matrix, bias\n\t\telse:\n\t\t\treturn tf.matmul(input_, matrix) + bias'"
