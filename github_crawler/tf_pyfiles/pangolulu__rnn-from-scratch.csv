file_path,api_count,code
activation.py,0,"b'import numpy as np\n\nclass Sigmoid:\n    def forward(self, x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def backward(self, x, top_diff):\n        output = self.forward(x)\n        return (1.0 - output) * output * top_diff\n\nclass Tanh:\n    def forward(self, x):\n        return np.tanh(x)\n\n    def backward(self, x, top_diff):\n        output = self.forward(x)\n        return (1.0 - np.square(output)) * top_diff'"
gate.py,0,"b'import numpy as np\n\nclass MultiplyGate:\n    def forward(self,W, x):\n        return np.dot(W, x)\n\n    def backward(self, W, x, dz):\n        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x)))\n        dx = np.dot(np.transpose(W), dz)\n        return dW, dx\n\nclass AddGate:\n    def forward(self, x1, x2):\n        return x1 + x2\n\n    def backward(self, x1, x2, dz):\n        dx1 = dz * np.ones_like(x1)\n        dx2 = dz * np.ones_like(x2)\n        return dx1, dx2'"
layer.py,0,"b'from activation import Tanh\nfrom gate import AddGate, MultiplyGate\n\nmulGate = MultiplyGate()\naddGate = AddGate()\nactivation = Tanh()\n\nclass RNNLayer:\n    def forward(self, x, prev_s, U, W, V):\n        self.mulu = mulGate.forward(U, x)\n        self.mulw = mulGate.forward(W, prev_s)\n        self.add = addGate.forward(self.mulw, self.mulu)\n        self.s = activation.forward(self.add)\n        self.mulv = mulGate.forward(V, self.s)\n\n    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n        self.forward(x, prev_s, U, W, V)\n        dV, dsv = mulGate.backward(V, self.s, dmulv)\n        ds = dsv + diff_s\n        dadd = activation.backward(self.add, ds)\n        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n        dU, dx = mulGate.backward(U, x, dmulu)\n        return (dprev_s, dU, dW, dV)'"
output.py,0,"b'import numpy as np\n\nclass Softmax:\n    def predict(self, x):\n        exp_scores = np.exp(x)\n        return exp_scores / np.sum(exp_scores)\n\n    def loss(self, x, y):\n        probs = self.predict(x)\n        return -np.log(probs[y])\n\n    def diff(self, x, y):\n        probs = self.predict(x)\n        probs[y] -= 1.0\n        return probs'"
preprocessing.py,0,"b'import csv\nimport numpy as np\nimport itertools\nimport nltk\n\ndef getSentenceData(path, vocabulary_size=8000):\n    unknown_token = ""UNKNOWN_TOKEN""\n    sentence_start_token = ""SENTENCE_START""\n    sentence_end_token = ""SENTENCE_END""\n\n    # Read the data and append SENTENCE_START and SENTENCE_END tokens\n    print(""Reading CSV file..."")\n    with open(path, \'r\', encoding=\'utf-8\') as f:\n        reader = csv.reader(f, skipinitialspace=True)\n        # Split full comments into sentences\n        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n        # Append SENTENCE_START and SENTENCE_END\n        sentences = [""%s %s %s"" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n    print(""Parsed %d sentences."" % (len(sentences)))\n\n    # Tokenize the sentences into words\n    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n    # Filter the sentences having few words (including SENTENCE_START and SENTENCE_END)\n    tokenized_sentences = list(filter(lambda x: len(x) > 3, tokenized_sentences))\n\n    # Count the word frequencies\n    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n    print(""Found %d unique words tokens."" % len(word_freq.items()))\n\n    # Get the most common words and build index_to_word and word_to_index vectors\n    vocab = word_freq.most_common(vocabulary_size-1)\n    index_to_word = [x[0] for x in vocab]\n    index_to_word.append(unknown_token)\n    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n\n    print(""Using vocabulary size %d."" % vocabulary_size)\n    print(""The least frequent word in our vocabulary is \'%s\' and appeared %d times."" % (vocab[-1][0], vocab[-1][1]))\n\n    # Replace all words not in our vocabulary with the unknown token\n    for i, sent in enumerate(tokenized_sentences):\n        tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n\n    print(""\\nExample sentence: \'%s\'"" % sentences[1])\n    print(""\\nExample sentence after Pre-processing: \'%s\'\\n"" % tokenized_sentences[0])\n\n    # Create the training data\n    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n\n    print(""X_train shape: "" + str(X_train.shape))\n    print(""y_train shape: "" + str(y_train.shape))\n\n    # Print an training data example\n    x_example, y_example = X_train[17], y_train[17]\n    print(""x:\\n%s\\n%s"" % ("" "".join([index_to_word[x] for x in x_example]), x_example))\n    print(""\\ny:\\n%s\\n%s"" % ("" "".join([index_to_word[x] for x in y_example]), y_example))\n\n    return X_train, y_train\n\nif __name__ == \'__main__\':\n    X_train, y_train = getSentenceData(\'data/reddit-comments-2015-08.csv\')'"
rnn.py,0,"b'from datetime import datetime\nimport numpy as np\nimport sys\nfrom layer import RNNLayer\nfrom output import Softmax\n\n\nclass Model:\n    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n        self.word_dim = word_dim\n        self.hidden_dim = hidden_dim\n        self.bptt_truncate = bptt_truncate\n        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (word_dim, hidden_dim))\n\n    \'\'\'\n        forward propagation (predicting word probabilities)\n        x is one single data, and a batch of data\n        for example x = [0, 179, 341, 416], then its y = [179, 341, 416, 1]\n    \'\'\'\n    def forward_propagation(self, x):\n        # The total number of time steps\n        T = len(x)\n        layers = []\n        prev_s = np.zeros(self.hidden_dim)\n        # For each time step...\n        for t in range(T):\n            layer = RNNLayer()\n            input = np.zeros(self.word_dim)\n            input[x[t]] = 1\n            layer.forward(input, prev_s, self.U, self.W, self.V)\n            prev_s = layer.s\n            layers.append(layer)\n        return layers\n\n    def predict(self, x):\n        output = Softmax()\n        layers = self.forward_propagation(x)\n        return [np.argmax(output.predict(layer.mulv)) for layer in layers]\n\n    def calculate_loss(self, x, y):\n        assert len(x) == len(y)\n        output = Softmax()\n        layers = self.forward_propagation(x)\n        loss = 0.0\n        for i, layer in enumerate(layers):\n            loss += output.loss(layer.mulv, y[i])\n        return loss / float(len(y))\n\n    def calculate_total_loss(self, X, Y):\n        loss = 0.0\n        for i in range(len(Y)):\n            loss += self.calculate_loss(X[i], Y[i])\n        return loss / float(len(Y))\n\n    def bptt(self, x, y):\n        assert len(x) == len(y)\n        output = Softmax()\n        layers = self.forward_propagation(x)\n        dU = np.zeros(self.U.shape)\n        dV = np.zeros(self.V.shape)\n        dW = np.zeros(self.W.shape)\n\n        T = len(layers)\n        prev_s_t = np.zeros(self.hidden_dim)\n        diff_s = np.zeros(self.hidden_dim)\n        for t in range(0, T):\n            dmulv = output.diff(layers[t].mulv, y[t])\n            input = np.zeros(self.word_dim)\n            input[x[t]] = 1\n            dprev_s, dU_t, dW_t, dV_t = layers[t].backward(input, prev_s_t, self.U, self.W, self.V, diff_s, dmulv)\n            prev_s_t = layers[t].s\n            dmulv = np.zeros(self.word_dim)\n            for i in range(t-1, max(-1, t-self.bptt_truncate-1), -1):\n                input = np.zeros(self.word_dim)\n                input[x[i]] = 1\n                prev_s_i = np.zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n                dprev_s, dU_i, dW_i, dV_i = layers[i].backward(input, prev_s_i, self.U, self.W, self.V, dprev_s, dmulv)\n                dU_t += dU_i\n                dW_t += dW_i\n            dV += dV_t\n            dU += dU_t\n            dW += dW_t\n        return (dU, dW, dV)\n\n    def sgd_step(self, x, y, learning_rate):\n        dU, dW, dV = self.bptt(x, y)\n        self.U -= learning_rate * dU\n        self.V -= learning_rate * dV\n        self.W -= learning_rate * dW\n\n    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n        num_examples_seen = 0\n        losses = []\n        for epoch in range(nepoch):\n            if (epoch % evaluate_loss_after == 0):\n                loss = self.calculate_total_loss(X, Y)\n                losses.append((num_examples_seen, loss))\n                time = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')\n                print(""%s: Loss after num_examples_seen=%d epoch=%d: %f"" % (time, num_examples_seen, epoch, loss))\n                # Adjust the learning rate if loss increases\n                if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n                    learning_rate = learning_rate * 0.5\n                    print(""Setting learning rate to %f"" % learning_rate)\n                sys.stdout.flush()\n            # For each training example...\n            for i in range(len(Y)):\n                self.sgd_step(X[i], Y[i], learning_rate)\n                num_examples_seen += 1\n        return losses'"
rnnlm.py,0,"b""import numpy as np\n\nfrom preprocessing import getSentenceData\nfrom rnn import Model\n\nword_dim = 8000\nhidden_dim = 100\nX_train, y_train = getSentenceData('data/reddit-comments-2015-08.csv', word_dim)\n\nnp.random.seed(10)\nrnn = Model(word_dim, hidden_dim)\n\nlosses = rnn.train(X_train[:100], y_train[:100], learning_rate=0.005, nepoch=10, evaluate_loss_after=1)"""
