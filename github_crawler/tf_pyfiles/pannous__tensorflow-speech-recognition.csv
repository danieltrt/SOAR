file_path,api_count,code
__init__.py,0,b'#!/usr/bin/env python\n# allow local includes\n'
bdlstm_utils.py,5,"b'#!/usr/bin/env python\nimport numpy as np\nimport tensorflow as tf\n\ndef target_list_to_sparse_tensor(targetList):\n\t\t\'\'\'make tensorflow SparseTensor from list of targets, with each element\n\t\t\t in the list being a list or array with the values of the target sequence\n\t\t\t (e.g., the integer values of a character map for an ASR target string)\n\t\t\t See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/ctc/ctc_loss_op_test.py\n\t\t\t for example of SparseTensor format\'\'\'\n\t\tindices = []\n\t\tvals = []\n\t\tfor tI, target in enumerate(targetList):\n\t\t\t\tfor seqI, val in enumerate(target):\n\t\t\t\t\t\tindices.append([tI, seqI])\n\t\t\t\t\t\tvals.append(val)\n\t\tshape = [len(targetList), np.asarray(indices).max(0)[1]+1]\n\t\treturn (np.array(indices), np.array(vals), np.array(shape))\n\ndef test_edit_distance():\n\t\tgraph = tf.Graph()\n\t\twith graph.as_default():\n\t\t\t\ttruth = tf.sparse_placeholder(tf.int32)\n\t\t\t\thyp = tf.sparse_placeholder(tf.int32)\n\t\t\t\teditDist = tf.edit_distance(hyp, truth, normalize=False)\n\n\t\twith tf.Session(graph=graph) as session:\n\t\t\t\ttruthTest = sparse_tensor_feed([[0,1,2], [0,1,2,3,4]])\n\t\t\t\thypTest = sparse_tensor_feed([[3,4,5], [0,1,2,2]])\n\t\t\t\tfeedDict = {truth: truthTest, hyp: hypTest}\n\t\t\t\tdist = session.run([editDist], feed_dict=feedDict)\n\t\t\t\tprint(dist)\n\ndef data_lists_to_batches(inputList, targetList, batchSize,swap_axes=True):\n\t\t\'\'\'Takes a list of input matrices and a list of target arrays and returns\n\t\t\t a list of batches, with each batch being a 3-element tuple of inputs,\n\t\t\t targets, and sequence lengths.\n\t\t\t inputList: list of 2-d numpy arrays with dimensions nFeatures x timesteps\n\t\t\t targetList: list of 1-d arrays or lists of ints\n\t\t\t batchSize: int indicating number of inputs/targets per batch\n\t\t\t returns: dataBatches: list of batch data tuples, where each batch tuple (inputs, targets, seqLengths) consists of\n\t\t\t\t\t\t\t\t\t\tinputs = 3-d array w/ shape nTimeSteps x batchSize x nFeatures\n\t\t\t\t\t\t\t\t\t\ttargets = tuple required as input for SparseTensor\n\t\t\t\t\t\t\t\t\t\tseqLengths = 1-d array with int number of timesteps for each sample in batch\n\t\t\t\t\t\t\t\tmaxSteps: maximum number of time steps across all samples\'\'\'\n\n\t\tassert len(inputList) == len(targetList), ""%d!=%d""%(len(inputList),len(targetList))\n\t\tnFeatures = inputList[0].shape[0]\n\t\tmaxSteps = 0\n\t\tfor inp in inputList:\n\t\t\tmaxSteps = max(maxSteps, inp.shape[1])\n\t\tprint(""nFeatures: %d    maxSteps: %d  ""%(nFeatures,maxSteps))\n\n\t\trandIxs = np.random.permutation(len(inputList))\n\t\tstart, end = (0, batchSize)\n\t\tdataBatches = []\n\n\t\twhile end <= len(inputList):\n\t\t\t\tbatchSeqLengths = np.zeros(batchSize)\n\t\t\t\tfor batchI, origI in enumerate(randIxs[start:end]):\n\t\t\t\t\t\tbatchSeqLengths[batchI] = inputList[origI].shape[-1]\n\t\t\t\tbatchInputs = np.zeros((maxSteps, batchSize, nFeatures))\n\t\t\t\tbatchTargetList = []\n\t\t\t\tfor batchI, origI in enumerate(randIxs[start:end]):\n\t\t\t\t\t\torig = inputList[origI]\n\t\t\t\t\t\t# print(""orig shape: ""+ str( orig.shape))\n\t\t\t\t\t\tpadSecs = maxSteps - orig.shape[1]\n\t\t\t\t\t\t# padFeatures = maxFeatures - orig.shape[0]\n\t\t\t\t\t\tpadded = np.pad(orig.T, ((0, padSecs), (0, 0)), \'constant\', constant_values=0)\n\t\t\t\t\t\t# print(""transposed + padded ""+str(padded.shape))\n\t\t\t\t\t\tbatchInputs[:,batchI,:] = padded\n\t\t\t\t\t\tbatchTargetList.append(targetList[origI])\n\t\t\t\tdataBatches.append((batchInputs, target_list_to_sparse_tensor(batchTargetList),\n\t\t\t\t\t\t\t\t\t\t\t\t\tbatchSeqLengths))\n\t\t\t\tstart += batchSize\n\t\t\t\tend += batchSize\n\t\treturn (dataBatches, maxSteps)\n\ndef load_batched_data(specPath, targetPath, batchSize, match=True):\n\t\timport os\n\t\t\'\'\'returns 4-element tuple: batched data (list), max # of time steps (int),\n\t\t\t total number of samples (int) and number of classes incl. noc (int)\'\'\'\n\t\tprint(""load_batched_data samples from "" + specPath)\n\t\tprint(""load_batched_data lables  from "" + targetPath)\n\t\ta = [np.load(os.path.join(specPath, fn)) for fn in os.listdir(specPath)]\n\t\tif match:b = [np.load(os.path.join(targetPath, fn)) for fn in os.listdir(specPath)]\n\t\telse: b = [np.load(os.path.join(targetPath, fn)) for fn in os.listdir(targetPath)]\n\t\tnSamples = len(os.listdir(specPath))\n\t\tnLables = len(os.listdir(targetPath))\n\t\tnClasses = -2359817587589759875328957 # doesn\'t matter\n\t\tprint(""#samples=?=#lables : %d=?=%d => %d=?=%d""%(nSamples,nLables,len(a),len(b)))\n\t\treturn data_lists_to_batches(a, b, batchSize) + (nSamples, nClasses)\n'"
densenet_layer.py,3,"b'#!/usr/bin/python\nimport tensorflow as tf\nimport layer\nimport speech_data\nfrom speech_data import Source,Target\n\n\nlearning_rate = 0.001\ntraining_iters = 300000\nbatch_size = 64\n\n\n# BASELINE toy net\ndef simple_dense(net): # best with lr ~0.001\n\t# type: (layer.net) -> None\n\t# net.dense(hidden=200,depth=8,dropout=False) # BETTER!!\n\tnet.dense(400, activation=tf.nn.tanh)# 0.99 YAY\n\t# net.denseNet(40, depth=4)\n\t# net.classifier() # auto classes from labels\n\treturn\n\n\ndef alex(net): # kinda\n\t# type: (layer.net) -> None\n\tprint(""Building Alex-net"")\n\tnet.reshape(shape=[-1, 64, 64, 1])  # Reshape input picture\n\t# net.batchnorm()\n\tnet.conv([3, 3, 1, 64]) # 64 filters\n\tnet.conv([3, 3, 64, 128])\n\tnet.conv([3, 3, 128, 256])\n\tnet.conv([3, 3, 256, 512])\n\tnet.conv([3, 3, 512, 1024])\n\tnet.dense(1024,activation=tf.nn.relu)\n\tnet.dense(1024,activation=tf.nn.relu)\n\n\n# Densely Connected Convolutional Networks https://arxiv.org/abs/1608.06993  # advanced ResNet\ndef denseConv(net):\n\t# type: (layer.net) -> None\n\tprint(""Building dense-net"")\n\tnet.reshape(shape=[-1, 64, 64, 1])  # Reshape input picture\n\tnet.buildDenseConv(nBlocks=1)  # increase nBlocks for real data\n\tnet.classifier() # auto classes from labels\n\n\ndef denseNet(net):\n\t# type: (layer.net) -> None\n\tprint(""Building dense-net"")\n\tnet.reshape(shape=[-1, 64, 64, 1])  # Reshape input picture\n\tnet.fullDenseNet()\n\tnet.classifier() # auto classes from labels\n\n\ntrain_digits=True\nif train_digits:\n\twidth= height=64 # for pcm baby data\n\tbatch=speech_data.spectro_batch_generator(1000,target=speech_data.Target.digits)\n\tclasses=10 # digits\nelse:\n\twidth=512 # for spoken_words overkill data\n\tclasses=74 #\n\tbatch=word_batch=speech_data.spectro_batch_generator(10, width, source_data=Source.WORD_SPECTROS, target=Target.first_letter)\n\traise Exception(""TODO"")\n\nX,Y=next(batch)\n\n# CHOOSE MODEL ARCHITECTURE HERE:\n# net = layer.net(simple_dense, data=batch, input_width=width, output_width=classes, learning_rate=0.01)\nnet = layer.net(simple_dense, data=batch, input_shape=(width,height), output_width=classes, learning_rate=0.01)\n# net=layer.net(model=alex,input_shape=(width, height),output_width=10, learning_rate=learning_rate)\n# net=layer.net(model=denseConv, input_shape=(width, height),output_width=10, learning_rate=learning_rate)\n\nnet.train(data=batch,batch_size=10,steps=500,dropout=0.6,display_step=1,test_step=1) # debug\n# net.train(data=batch,batch_size=10,steps=5000,dropout=0.6,display_step=5,test_step=20) # test\n# net.train(data=batch,batch_size=10,steps=5000,dropout=0.6,display_step=10,test_step=100) # run\n\n# net.predict() # nil=random\n# net.generate(3)  # nil=random\n\nprint (""Now try switching between model architectures in line 68-71"")\n'"
generate_speech_data.py,0,"b'#!/usr/bin/env python\nimport os\n\nimport librosa #mfcc 1.st lib\nfrom scikits.talkbox.features import mfcc # 2\'nd lib\n# import python_speech_features # 3rd lib\nimport os.path\nimport numpy as np\nimport subprocess\n\nAUTOMATIC_ALL_VOICES=False\n# list voices: `say -v ?`\n# good_voices = [] #AUTOMATIC !!\ngood_voices = """"""\nAgnes\nAlex\nAllison\nDaniel\nFred\nJunior\nKaren\nKate\nKathy\nLee\nMoira\nOliver\nPrincess\nRalph\nSamantha\nSerena\nTessa\nTom\nVeena\nVicki\nVictoria\n"""""".split()\n# Ava Susan not found.\n\n\nbad_voices = """"""\nAlbert\nBad\\ News\nBahh\nBells\nBoing\nBruce\nBubbles\nCellos\nDeranged\nGood\\ News\nHysterical\nPipe\\ Organ\nTrinoids\nWhisper\nZarvox\n"""""".split()\n\nlow_quality = """"""\nAgnes\nFred\nKathy\nPrincess\nRalph\n"""""".split()\n\n# Whisper + Albert Deranged Trinoids  >> later !\nno_rate = [""Bad News"", ""Bells"", ""Good News"", ""Cellos"", ""Pipe\\ Organ""]\n\nnum_characters = 32\nterminal_symbol = 0\noffset = 64  # starting with UPPER case characters ord(\'A\'):65->1\nmax_word_length = 78# 20\n\ndef pad(vec, pad_to=max_word_length,one_hot=False):\n\tfor i in range(0, pad_to - len(vec)):\n\t\tif one_hot: vec.append([terminal_symbol] * num_characters)\n\t\telse: vec.append(terminal_symbol)\n\treturn vec\n\n\ndef char_to_class(c):\n\t# type: (char|int) -> int\n\tif not isinstance(c,int): c=ord(c)\n\tclasse=(c - offset) % num_characters\n\tif c==\' \' or c==""_"": classe= terminal_symbol  # needed by ctc\n\treturn classe # A->1 ... Z->26\n\n\n# def achar_to_phoneme(c):\ndef pronounced_to_phoneme_class(pronounced):\n\t# type: (str) -> [int]\n\traise Exception(""TODO"")\n\tphonemes = map(char_to_class, pronounced)\n\tz = pad(z, pad_to)\n\treturn phonemes  # ""_hEllO""->[11,42,24,21,0,0,0,0] padded\n\n\ndef string_to_int_word(word, pad_to=max_word_length):\n\t# type: (str) -> [int]\n\tz = map(char_to_class, word)\n\tz = list(z)  # py3\n\tz = pad(z, pad_to)\n\t# z = np.array(z)\n\treturn z # ""abd""->[1,2,4,0,0,0,0,0] padded\n\n\ndef check_voices():\n\tvoice_infos=str(subprocess.check_output([""say"", ""-v?""])).split(""\\n"")[:-2]\n\tvoices=map(lambda x:x.split()[0],voice_infos)\n\tfor voice in good_voices:\n\t\tif voice in voices:\n\t\t\tprint (voice + "" FOUND!"")\n\tfor voice in good_voices:\n\t\tif not voice in voices:\n\t\t\tprint (voice+"" MISSING!"")\n\t\t\tgood_voices.remove(voice)\n\n\t# ADD ALL ACCENTS!! YAY!!\n\tif AUTOMATIC_ALL_VOICES: # takes looong to create and is harder to train (really?)\n\t\t#  todo: add trainig difficulty metadata to samples!\n\t\tfor voice in voices:\n\t\t\tgood_voices.append(voice)\n\n\ncheck_voices()\n\ndef generate_mfcc(voice, word, rate, path):\n\tfilename = path+""/ogg/{0}_{1}_{2}.ogg"".format(word, voice, rate)\n\tcmd = ""say \'{0}\' -v{1} -r{2}  -o \'{3}\'"".format(word, voice, rate, filename)\n\tos.system(cmd)  # ogg aiff m4a or caff\n\tsignal, sample_rate = librosa.load(filename, mono=True)\n\t# mel_features = librosa.feature.mfcc(signal, sample_rate)\n\t# sample_rate, wave = scipy.io.wavfile.read(filename) # 2nd lib\n\tmel_features, mspec, spec = mfcc(signal, fs=sample_rate, nceps=26)\n\t# mel_features=python_speech_features.mfcc(signal, numcep=26, nfilt=26*2,samplerate=sample_rate) # 3rd lib\n\t# print len(mel_features)\n\t# print len(mel_features[0])\n\t# print(""---"")\n\tmel_features=np.swapaxes(mel_features,0,1)# timesteps x nFeatures -> nFeatures x timesteps\n\tnp.save(path + ""/mfcc/%s_%s_%d.npy"" % (word,voice,rate), mel_features)\n\ndef generate_chars(voice, word, rate, path):\n\tchars = string_to_int_word(word) # todo : softlink!\n\t# os.symlink(""%d.npy"",""%s_%s_%d.npy"" % (word, voice, rate))\n\tnp.save(path + ""/chars/%s_%s_%d.npy"" % (word, voice, rate), chars)\n\ndef generate_phonemes(word,  path):\n\tpronounced=subprocess.check_output([""./word_to_phonemes.swift"", word]).decode(\'UTF-8\').strip()\n\tchars = string_to_int_word(pronounced, pad_to=max_word_length)  # hack for numbers!\n\t# chars = string_to_int_word(word, pad_to=max_word_length)\n\tnp.save(path + ""/chars/%s.npy""%word, chars)\n\t# phonemes= pronounced_to_phoneme_class(pronounced)\n\t# np.save(path + ""/phones/%s.npy""%word, phonemes)\n\n\ndef generate(words, path):\n\t# generate a bunch of files for each word (with many voices, nuances):\n\t# spoken wav/ogg\n\t# spectograph\n\t# mfcc Mel-frequency cepstrum\n\t# pronounced phonemes\n\tif not os.path.exists(path): os.mkdir(path)\n\tif not os.path.exists(path + ""/chars/""): os.mkdir(path + ""/chars/"")\n\tif not os.path.exists(path + ""/mfcc/""): os.mkdir(path + ""/mfcc/"")\n\tif not os.path.exists(path + ""/ogg/""): os.mkdir(path + ""/ogg/"")\n\tout=open(path + ""/words.list"", ""wt"")\n\tfor word in words:\n\t\tif isinstance(word, bytes):\n\t\t\tword=word.decode(\'UTF-8\').strip()\n\t\tprint(""generating %s""%word)\n\t\tout.write(""%s\\n""%word)\n\t\tgenerate_phonemes(word, path)\n\t\trate=120\n\t\t# for rate in range(80,360,step=20):\n\t\tfor voice in good_voices:\n\t\t\ttry:\n\t\t\t\tgenerate_chars(voice, word, rate, path)\n\t\t\t\tgenerate_mfcc(voice, word, rate, path)\n\t\t\texcept:\n\t\t\t\tpass  # ignore after debug!\n\n\n# generates\n# number/chars/1.npy\n# number/mfcc/1_Kathy_120.npy for each voice\ndef spoken_numbers():\n\tpath = ""number""\n\tnums = list(map(str, range(0, 10)))\n\tgenerate(nums, path)\n\n\ndef spoken_words():\n\tpath = ""words""\n\twordlist = ""wordlist.txt""\n\twords= open(wordlist).readlines()\n\tgenerate(words, path)\n\n\ndef spoken_sentence():\n\tpath = ""sentences""\n\twordlist = ""sentences.txt""\n\twords = open(wordlist).readlines()\n\tgenerate(words, path)\n\n\ndef extra():\n\tfor v in bad_voices:\n\t\tfor w in range(0,10):\n\t\t\tcmd = ""say \'{w}\' -v\'{v}\' -r120""  # -o \'spoken_numbers/{w}_{v}.ogg\'""\n\t\t\tos.system(cmd)\n\ndef main():\n\tspoken_numbers()\n\t# spoken_words()\n\t# spoken_sentence()\n\nif __name__ == \'__main__\':\n\tmain()\n\tprint(""DONE!"")\n'"
lstm-tflearn.py,1,"b'#!/usr/bin/env python\n#!/usr/bin/env python\nimport tensorflow as tf\nimport tflearn\n\nimport speech_data\n\nlearning_rate = 0.0001\ntraining_iters = 300000  # steps\nbatch_size = 64\n\nwidth = 20  # mfcc features\nheight = 80  # (max) length of utterance\nclasses = 10  # digits\n\nbatch = word_batch = speech_data.mfcc_batch_generator(batch_size)\n\n# Network building\nnet = tflearn.input_data([None, width, height])\nnet = tflearn.lstm(net, 128*4, dropout=0.5)\nnet = tflearn.fully_connected(net, classes, activation=\'softmax\')\nnet = tflearn.regression(net, optimizer=\'adam\', learning_rate=learning_rate, loss=\'categorical_crossentropy\')\nmodel = tflearn.DNN(net, tensorboard_verbose=0)\n\n## add this ""fix"" for tensorflow version errors\nfor x in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES): tf.add_to_collection(tf.GraphKeys.VARIABLES, x )\n\n# Training\n\nwhile --training_iters > 0:\n\ttrainX, trainY = next(batch)\n\ttestX, testY = next(batch)  # todo: proper ;)\n\tmodel.fit(trainX, trainY, n_epoch=10, validation_set=(testX, testY), show_metric=True, batch_size=batch_size)\n\nmodel.save(""tflearn.lstm.model"")\n_y = model.predict(next(batch)[0])  # << add your own voice here\nprint (_y)\n'"
lstm_ctc_to_chars.py,54,"b'#!/usr/bin/env python\n#!/usr/local/bin/python\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nimport speech_data\nfrom speech_data import Source, Target\nfrom tensorflow.python.ops import ctc_ops as ctc\n# import layer\n# from layer import net\nimport time\n\nstart = int(time.time())\ndisplay_step = 1\ntest_step = 10\nsave_step = 100\nlearning_rate = 0.0001\n# 0.0001 Step 300 Loss= 1.976625 Accuracy= 0.250 Time= 303s\n# Step 24261 Loss= 0.011786 Accuracy= 1.000 Time= 33762s takes time but works\n\ntraining_iters = 300000  # steps\nbatch_size = 64\n\nwidth = features = 20  # mfcc input features\nheight = max_input_length = 80  # (max) length of input utterance (mfcc slices)\nclasses = num_characters = 32\nmax_word_length = 20  # max length of output (characters per word)\n# classes=10 # digits\n\nkeep_prob = dropout = 0.7\n\n# batch = speech_data.mfcc_batch_generator(batch_size, target=Target.word)\nbatch = speech_data.mfcc_batch_generator(batch_size, source=Source.WORD_WAVES, target=Target.hotword)\nX, Y = next(batch)\nprint(""lable shape"", np.array(Y).shape)\n\n# inputs=tf.placeholder(tf.float32, shape=(batch_size,max_length,features))\nx = inputX = inputs = tf.placeholder(tf.float32, shape=(batch_size, features, max_input_length))\n# inputs = tf.transpose(inputs, [0, 2, 1]) #  inputs must be a `Tensor` of shape: `[batch_size, max_time, ...]`\ninputs = tf.transpose(inputs, [2, 0, 1])  # [max_time, batch_size, features] to split:\n# Split data because rnn cell needs a list of inputs for the RNN inner loop\ninputs = tf.split(axis=0, num_or_size_splits=max_input_length, value=inputs)  # n_steps * (batch_size, features)\n\nnum_hidden = 100  # features\ncell = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)\n# cell = tf.nn.rnn_cell.EmbeddingWrapper(num_hidden, state_is_tuple=True)\n\n# in many cases it may be more efficient to not use this wrapper,\n#   but instead concatenate the whole sequence of your outputs in time,\n#   do the projection on this batch-concatenated sequence, then split it\n#   if needed or directly feed into a softmax.\n# cell = tf.nn.rnn_cell.OutputProjectionWrapper(cell,)\n\n\ncell = tf.nn.rnn_cell.MultiRNNCell(num_hidden, state_is_tuple=True)\n# rnn=tf.nn.rnn(cell,inputs)\n# rnn=tf.nn.dynamic_rnn(cell,inputs)\n# manual:\n\nstate = cell.zero_state(batch_size, dtype=tf.float32)\nif ""manual"" == 0:\n\toutputs = []\n\tfor input_ in inputs:\n\t\tinput_ = tf.reshape(input_, [batch_size, features])\n\t\toutput, state = cell(input_, state)\n\t\toutputs.append(output)\n\ty_ = output\nelse:\n\t# inputs = tf.reshape(inputs, [-1, features])\n\tinputs = [tf.reshape(input_, [batch_size, features]) for input_ in inputs]\n\toutputs, states = tf.nn.rnn(cell, inputs, initial_state=state)\n# only last output as target for now\n# y_=outputs[-1]\n\n# optimize\ntarget_shape = (batch_size, max_word_length, classes)\ny = target = tf.placeholder(tf.float32, shape=target_shape)  # -> seq2seq!\n\n# dense\nlogits = []\ncosts = []\ni = 0\naccuracy = 0\n# for output in outputs:\nfor i in range(0, max_word_length):\n\toutput = outputs[-i - 1]\n\tuniform = tf.random_uniform([num_hidden, classes], minval=-1. / width, maxval=1. / width)\n\tweights = tf.Variable(uniform, name=""weights_%d"" % i)\n\tuniform_bias = tf.random_uniform([classes], minval=-1. / width, maxval=1. / width)\n\tbias = tf.Variable(uniform_bias, name=""bias_dense_%d"" % i)\n\ty_ = outputY = tf.matmul(output, weights, name=""dense_%d"" % i) + bias\n\n\tcost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y[:, i, :]), name=""cost"")  # prediction, target\n\tcosts.append(cost)\n\tlogits.append(y_)\n\n\tcorrect_pred = tf.equal(tf.argmax(outputY, 1), tf.argmax(y[:, i], 1))\n\taccuraci = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\taccuracy += accuraci\n\n# costs=tf.reduce_sum(costs)*10\n# y_ = outputY = tf.pack(logits)\n\n# targetIxs = tf.placeholder(tf.int64, shape=(batch_size, None),name=""indices"")\n# targetVals = tf.placeholder(tf.int32,name=""values"")\n# targetShape = tf.placeholder(tf.int64,name=""targetShape"")\n# targetY = tf.SparseTensor(targetIxs, targetVals, targetShape)\ntargetY = tf.SparseTensor()\n\n####Optimizing\nlogits = y_\nlogits3d = tf.stack(logits)\nseqLengths = [20] * batch_size\ncost = tf.reduce_mean(ctc.ctc_loss(logits3d, targetY, seqLengths))\n# CTCLoss op expects the reserved blank label to be the largest value! REALLY?\n\n# if 1D:\ntf.summary.scalar(\'cost\', cost)\ntf.summary.scalar(\'costs\', costs)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(costs)\n# prediction = y_\n\n# Evaluate model\n# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n# tf.scalar_summary(\'accuracy\', accuracy)\n# predictions = tf.to_int32(ctc.ctc_beam_search_decoder(logits3d, seqLengths)[0][0])\n# accuracy = tf.reduce_mean(tf.reduce_mean(logits))\n# reduced_sum = tf.reduce_sum(tf.edit_distance(predictions, targetY, normalize=False))\n# errorRate = reduced_sum / tf.to_float(tf.size(targetY.values))\n\nsteps = 9999999\nsession = tf.Session()\ntry:\n\tsaver = tf.train.Saver(tf.global_variables())\nexcept:\n\tsaver = tf.train.Saver(tf.global_variables())\nsnapshot = ""lstm_mfcc""\ncheckpoint = tf.train.latest_checkpoint(checkpoint_dir=""checkpoints"")\nif checkpoint:\n\tprint(""LOADING checkpoint "" + checkpoint + """")\n\ttry: saver.restore(session, checkpoint)\n\texcept: print(""incompatible checkpoint"")\ntry: session.run([tf.global_variables_initializer()])\nexcept: session.run([tf.global_variables_initializer()])  # tf <12\n\n# train\nstep = 0  # show first\ntry: summaries = tf.summary.merge_all()\nexcept: summaries = tf.summary.merge_all()  # tf<12\ntry: summary_writer = tf.summary.FileWriter(""logs"", session.graph)  #\nexcept: summary_writer = tf.summary.FileWriter(""logs"", session.graph)  # tf<12\nwhile step < steps:\n\tbatch_xs, batch_ys = next(batch)\n\n\t# tf.train.shuffle_batch_join(example_list, batch_size, capacity=min_queue_size + batch_size * 16, min_queue_size)\n\t# Fit training using batch data\n\tfeed_dict = {x: batch_xs, y: batch_ys}\n\t# feed_dict = {inputX: batch_xs, targetIxs: batch_ys.indices, targetVals: batch_ys.values,targetShape: 20}\n\t# , seqLengths: batchSeqLengths\n\tloss, _ = session.run([costs, optimizer], feed_dict=feed_dict)\n\tif step % display_step == 0:\n\t\tseconds = int(time.time()) - start\n\t\t# Calculate batch accuracy, loss\n\t\tfeed = {x: batch_xs, y: batch_ys}  # , keep_prob: 1., train_phase: False}\n\t\tacc, summary = session.run([accuracy, summaries], feed_dict=feed)\n\t\t# summary_writer.add_summary(summary, step) # only test summaries for smoother curve\n\t\tprint(""\\rStep {:d} Loss={:.4f} Fit={:.1f}% Time={:d}s"".format(step, loss, acc, seconds), end=\' \')\n\t\tif str(loss) == ""nan"":\n\t\t\tprint(""\\nLoss gradiant explosion, quitting!!!"")  # restore!\n\t\t\tquit(0)\n\t# if step % test_step == 0: test(step)\n\tif step % save_step == 0 and step > 0:\n\t\tprint(""SAVING snapshot %s"" % snapshot)\n\t\tsaver.save(session, ""checkpoints/"" + snapshot + "".ckpt"", step)\n\tstep = step + 1\n'"
lstm_mfcc_ctc_to_words.py,49,"b'#!/usr/bin/env python\n#!/usr/local/bin/python\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nimport speech_data\nfrom speech_data import Source,Target\nfrom tensorflow.python.ops import ctc_ops as ctc\n# import layer\n# from layer import net\nimport time\nstart=int(time.time())\ndisplay_step = 1\ntest_step = 10\nsave_step = 100\nlearning_rate = 0.0001\n# 0.0001 Step 300 Loss= 1.976625 Accuracy= 0.250 Time= 303s\n# Step 24261 Loss= 0.011786 Accuracy= 1.000 Time= 33762s takes time but works\n\ntraining_iters = 300000 #steps\nbatch_size = 64\n\nwidth=features=20 # mfcc input features\nheight=max_input_length=80 # (max) length of input utterance (mfcc slices)\nclasses = num_characters = 32\nmax_word_length = 20  # max length of output (characters per word)\n# classes=10 # digits\n\nkeep_prob=dropout=0.7\n\n# batch = speech_data.mfcc_batch_generator(batch_size, target=Target.word)\nbatch = speech_data.mfcc_batch_generator(batch_size, source=Source.WORD_WAVES, target=Target.hotword)\nX,Y=next(batch)\nprint(""lable shape"",np.array(Y).shape)\n\n# inputs=tf.placeholder(tf.float32, shape=(batch_size,max_length,features))\nx= inputX=inputs=tf.placeholder(tf.float32, shape=(batch_size, features, max_input_length))\n# inputs = tf.transpose(inputs, [0, 2, 1]) #  inputs must be a `Tensor` of shape: `[batch_size, max_time, ...]`\ninputs = tf.transpose(inputs, [2, 0, 1]) # [max_time, batch_size, features] to split:\n# Split data because rnn cell needs a list of inputs for the RNN inner loop\ninputs = tf.split(axis=0, num_or_size_splits=max_input_length, value=inputs)  # n_steps * (batch_size, features)\n\nnum_hidden = 100 #features\ncell = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)\n# rnn=tf.nn.rnn(cell,inputs)\n# rnn=tf.nn.dynamic_rnn(cell,inputs)\n# manual:\n\nstate = cell.zero_state(batch_size, dtype=tf.float32)\nif ""manual"" == 0:\n\toutputs = []\n\tfor input_ in inputs:\n\t\tinput_= tf.reshape(input_, [batch_size,features])\n\t\toutput, state = cell(input_, state)\n\t\toutputs.append(output)\n\ty_=output\nelse:\n\t# inputs = tf.reshape(inputs, [-1, features])\n\tinputs=[tf.reshape(input_, [batch_size,features]) for input_ in inputs]\n\toutputs, states = tf.nn.rnn(cell, inputs, initial_state=state)\n\t# only last output as target for now\n\t# y_=outputs[-1]\n\n# optimize\ny = target = tf.placeholder(tf.float32, shape=(batch_size, max_word_length, classes))  # -> seq2seq!\n\n# dense\nlogits=[]\ncosts=[]\ni=0\naccuracy=0\n# for output in outputs:\nfor i in range(0, max_word_length):\n\toutput=outputs[-i-1]\n\tweights = tf.Variable(tf.random_uniform([num_hidden, classes], minval=-1. / width, maxval=1./width), name=""weights_%d""%i)\n\tbias = tf.Variable(tf.random_uniform([classes], minval=-1. / width, maxval=1. / width), name=""bias_dense_%d""%i)\n\ty_ = outputY = tf.matmul(output, weights, name=""dense_%d"" % i) + bias\n\n\tcost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y[:,i,:]), name=""cost"")  # prediction, target\n\tcosts.append(cost)\n\tlogits.append(y_)\n\n\tcorrect_pred = tf.equal(tf.argmax(outputY, 1), tf.argmax(y[:,i], 1))\n\taccuraci = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\taccuracy+= accuraci\n\n# costs=tf.reduce_sum(costs)*10\n# y_ = outputY = tf.pack(logits)\n\n# targetIxs = tf.placeholder(tf.int64, shape=(batch_size, None),name=""indices"")\n# targetVals = tf.placeholder(tf.int32,name=""values"")\n# targetShape = tf.placeholder(tf.int64,name=""targetShape"")\n# targetY = tf.SparseTensor(targetIxs, targetVals, targetShape)\ntargetY = tf.SparseTensor()\n\n####Optimizing\nlogits=y_\nlogits3d = tf.stack(logits)\nseqLengths=[20]*batch_size\ncost = tf.reduce_mean(ctc.ctc_loss(logits3d, targetY, seqLengths))\n# CTCLoss op expects the reserved blank label to be the largest value! REALLY?\n\n# if 1D:\ntf.summary.scalar(\'cost\', cost)\ntf.summary.scalar(\'costs\', costs)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(costs)\n# prediction = y_\n\n# Evaluate model\n# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n# tf.scalar_summary(\'accuracy\', accuracy)\n# predictions = tf.to_int32(ctc.ctc_beam_search_decoder(logits3d, seqLengths)[0][0])\n# accuracy = tf.reduce_mean(tf.reduce_mean(logits))\n# reduced_sum = tf.reduce_sum(tf.edit_distance(predictions, targetY, normalize=False))\n# errorRate = reduced_sum / tf.to_float(tf.size(targetY.values))\n\nsteps = 9999999\nsession=tf.Session()\ntry:saver = tf.train.Saver(tf.global_variables())\nexcept:saver = tf.train.Saver(tf.global_variables())\nsnapshot = ""lstm_mfcc""\ncheckpoint = tf.train.latest_checkpoint(checkpoint_dir=""checkpoints"")\nif checkpoint:\n\tprint(""LOADING "" + checkpoint + "" !!!"")\n\ttry:saver.restore(session, checkpoint)\n\texcept: print(""incompatible checkpoint"")\ntry: session.run([tf.global_variables_initializer()])\nexcept: session.run([tf.global_variables_initializer()])# tf <12\n\n\n#train\nstep = 0  # show first\ntry:summaries = tf.summary.merge_all()\nexcept:summaries = tf.summary.merge_all() # tf<12\ntry:summary_writer = tf.summary.FileWriter(""logs"", session.graph)  #\nexcept:summary_writer = tf.summary.FileWriter(""logs"", session.graph)  # tf<12\nwhile step < steps:\n\tbatch_xs, batch_ys = next(batch)\n\n\t# tf.train.shuffle_batch_join(example_list, batch_size, capacity=min_queue_size + batch_size * 16, min_queue_size)\n\t# Fit training using batch data\n\tfeed_dict = {x: batch_xs, y: batch_ys}\n\t# feed_dict = {inputX: batch_xs, targetIxs: batch_ys.indices, targetVals: batch_ys.values,targetShape: 20}\n\t#, seqLengths: batchSeqLengths\n\tloss, _ = session.run([costs, optimizer], feed_dict=feed_dict)\n\tif step % display_step == 0:\n\t\tseconds = int(time.time()) - start\n\t\t# Calculate batch accuracy, loss\n\t\tfeed = {x: batch_xs, y: batch_ys} #, keep_prob: 1., train_phase: False}\n\t\tacc, summary = session.run([accuracy, summaries], feed_dict=feed)\n\t\t# summary_writer.add_summary(summary, step) # only test summaries for smoother curve\n\t\tprint(""\\rStep {:d} Loss={:.4f} Fit={:.1f}% Time={:d}s"".format(step, loss, acc, seconds), end=\' \')\n\t\tif str(loss) == ""nan"":\n\t\t\tprint(""\\nLoss gradiant explosion, quitting!!!"")  # restore!\n\t\t\tquit(0)\n\t# if step % test_step == 0: test(step)\n\tif step % save_step == 0 and step > 0:\n\t\tprint(""SAVING snapshot %s"" % snapshot)\n\t\tsaver.save(session, ""checkpoints/"" + snapshot + "".ckpt"", step)\n\tstep = step +1\n'"
lstm_mfcc_to_chars.py,32,"b'#!/usr/bin/env python\n#!/usr/local/bin/python\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nimport layer\nimport speech_data\nfrom speech_data import Source,Target\nfrom layer import net\nimport time\nstart=int(time.time())\ndisplay_step = 1\ntest_step = 10\nsave_step = 100\nlearning_rate = 0.0001\n# 0.0001 Step 300 Loss= 1.976625 Accuracy= 0.250 Time= 303s\n# Step 24261 Loss= 0.011786 Accuracy= 1.000 Time= 33762s takes time but works\n\ntraining_iters = 300000 #steps\nbatch_size = 64\n\nwidth=features=20 # mfcc features\nheight=max_length=80 # (max) length of utterance\nclasses=10 # digits\n\nkeep_prob=dropout=0.7\n\nbatch = speech_data.mfcc_batch_generator(batch_size,target=Target.digits) #\nX,Y=next(batch)\n# print(Y)\nprint(np.array(Y).shape)\n\n# inputs=tf.placeholder(tf.float32, shape=(batch_size,max_length,features))\nx=inputs=tf.placeholder(tf.float32, shape=(batch_size,features,max_length))\n# inputs = tf.transpose(inputs, [0, 2, 1]) #  inputs must be a `Tensor` of shape: `[batch_size, max_time, ...]`\ninputs = tf.transpose(inputs, [2, 0, 1]) # [max_time, batch_size, features] to split:\n# Split data because rnn cell needs a list of inputs for the RNN inner loop\ninputs = tf.split(axis=0, num_or_size_splits=max_length, value=inputs)  # n_steps * (batch_size, features)\n\nnum_hidden = 100 #features\ncell = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)\n# rnn=tf.nn.rnn(cell,inputs)\n# rnn=tf.nn.dynamic_rnn(cell,inputs)\n# manual:\n\nstate = cell.zero_state(batch_size, dtype=tf.float32)\nif ""manual"" == 0:\n\toutputs = []\n\tfor input_ in inputs:\n\t\tinput_= tf.reshape(input_, [batch_size,features])\n\t\toutput, state = cell(input_, state)\n\t\toutputs.append(output)\n\ty_=output\nelse:\n\t# inputs = tf.reshape(inputs, [-1, features])\n\tinputs=[tf.reshape(input_, [batch_size,features]) for input_ in inputs]\n\toutputs, states = tf.nn.rnn(cell, inputs, initial_state=state)\n\t# only last output as target for now\n\ty_=outputs[-1]\n\n# dense\nweights = tf.Variable(tf.random_uniform([num_hidden, classes], minval=-1. / width, maxval=1. / width), name=""weights_dense"")\nbias = tf.Variable(tf.random_uniform([classes], minval=-1. / width, maxval=1. / width), name=""bias_dense"")\ny_ = tf.matmul(y_, weights, name=\'dense\' ) + bias\n\n# optimize\n# if use_word: y=target=tf.placeholder(tf.float32, shape=(batch_size,(None,32))) # -> seq2seq!\ny=target=tf.placeholder(tf.float32, shape=(batch_size,classes))\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_,labels=y),name=""cost"")  # prediction, target\ntf.summary.scalar(\'cost\', cost)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\nprediction = y_\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(target, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\ntf.summary.scalar(\'accuracy\', accuracy)\n\nsteps = 9999999\nsession=tf.Session()\nsaver = tf.train.Saver(tf.global_variables())\nsnapshot = ""lstm_mfcc""\ncheckpoint = tf.train.latest_checkpoint(checkpoint_dir=""checkpoints"")\nif checkpoint:\n\tprint(""LOADING "" + checkpoint + "" !!!"")\n\ttry:saver.restore(session, checkpoint)\n\texcept: print(""incompatible checkpoint"")\ntry: session.run([tf.global_variables_initializer()])\nexcept: session.run([tf.global_variables_initializer()])\n\n\n#train\nstep = 0  # show first\nsummaries = tf.summary.merge_all()\nsummary_writer = tf.summary.FileWriter(""logs"", session.graph)  #\nwhile step < steps:\n\tbatch_xs, batch_ys = next(batch)\n\t# tf.train.shuffle_batch_join(example_list, batch_size, capacity=min_queue_size + batch_size * 16, min_queue_size)\n\t# Fit training using batch data\n\tfeed_dict = {x: batch_xs, y: batch_ys}\n\tloss, _ = session.run([cost, optimizer], feed_dict=feed_dict)\n\tif step % display_step == 0:\n\t\tseconds = int(time.time()) - start\n\t\t# Calculate batch accuracy, loss\n\t\tfeed = {x: batch_xs, y: batch_ys} #, keep_prob: 1., train_phase: False}\n\t\tacc, summary = session.run([accuracy, summaries], feed_dict=feed)\n\t\t# summary_writer.add_summary(summary, step) # only test summaries for smoother curve\n\t\tprint(""\\rStep {:d} Loss= {:.6f} Fit= {:.3f} Time= {:d}s"".format(step, loss, acc, seconds), end=\' \')\n\t\tif str(loss) == ""nan"":\n\t\t\tprint(""\\nLoss gradiant explosion, quitting!!!"")  # restore!\n\t\t\tquit(0)\n\t# if step % test_step == 0: test(step)\n\tif step % save_step == 0 and step > 0:\n\t\tprint(""SAVING snapshot %s"" % snapshot)\n\t\tsaver.save(session, ""checkpoints/"" + snapshot + "".ckpt"", step)\n\tstep = step +1\n'"
lstm_to_chars.py,41,"b'#!/usr/bin/env python\n#!/usr/bin/env python\n\'\'\'\nExample of a single-layer bidirectional long short-term memory network trained with\nconnectionist temporal classification to predict character sequences from nFeatures x nFrames\narrays of Mel-Frequency Cepstral Coefficients.  This is test code to run on the\n8-item data set in the ""sample_data"" directory, for those without access to TIMIT.\n\nOriginal Author: Jon Rein\n\'\'\'\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import ctc_ops as ctc\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops.rnn import bidirectional_rnn\nimport numpy as np\nimport re\n\nfrom bdlstm_utils import load_batched_data\n\nINPUT_PATH = \'/data/ctc/sample_data/mfcc\'  # directory of MFCC nFeatures x nFrames 2-D array .npy files\nTARGET_PATH = \'/data/ctc/sample_data/char_y/\'  # directory of nCharacters 1-D array .npy files\n# HUH?? >>> np.load(""/data/ctc/sample_data/char_y/0.npy"")\n# array([19, 15, 26, 2, 0, 18, 22, 18, 12, 2, 0, 18, 4, 16, 1, 0, 8,\n#        26, 17, 26, 14, 12, 2, 0, 8, 22, 13, 0, 15, 4, 14, 12, 19, 15,\n#        12, 0, 15, 5, 22, 15, 19, 25, 4, 0, 15, 26, 14, 12, 26, 1, 0,\n#        4, 14, 26, 0, 4, 14, 26, 0, 26, 22, 25, 5, 12, 0, 3, 4, 22,\n#        14, 12, 0, 12, 21, 4, 0, 12, 21, 4], dtype=uint8)\n# >> > map(lambda x: chr(x + 64), _)\n# [\'S\', \'O\', \'Z\', \'B\', \'@\', \'R\', \'V\', \'R\', \'L\', \'B\', \'@\', \'R\', \'D\', \'P\', \'A\', \'@\', \'H\', \'Z\', \'Q\', \'Z\', \'N\', \'L\', \'B\', \'@\',\n#  \'H\', \'V\', \'M\', \'@\', \'O\', \'D\', \'N\', \'L\', \'S\', \'O\', \'L\', \'@\', \'O\', \'E\', \'V\', \'O\', \'S\', \'Y\', \'D\', \'@\', \'O\', \'Z\', \'N\', \'L\',\n#  \'Z\', \'A\', \'@\', \'D\', \'N\', \'Z\', \'@\', \'D\', \'N\', \'Z\', \'@\', \'Z\', \'V\', \'Y\', \'E\', \'L\', \'@\', \'C\', \'D\', \'V\', \'N\', \'L\', \'@\', \'L\',\n#  \'U\', \'D\', \'@\', \'L\', \'U\', \'D\']\n# our_data=True\nour_data = False\nif our_data:\n\tprint(""Using our data"")\n\tINPUT_PATH = \'data/number/mfcc\'  # directory of MFCC nFeatures x nFrames 2-D array .npy files\n\tTARGET_PATH = \'data/number/chars/\'  # directory of nCharacters 1-D array .npy files\n\t# we have 0.npy : ~ zEeRo : array([31, 26, 17,  9, 25, 18, 15, 23, 14,  0, ... ]) should be fine?\n\n####Learning Parameters\nlearningRate = 0.001\nmomentum = 0.9\nnEpochs = 300\nSize = 4\n\n####Network Parameters\n\nnFeatures = 26  # 12 MFCC coefficients + energy, and derivatives\nnClasses = 28  # 27 characters, plus the ""blank"" for CTC\nif our_data:\n\tnFeatures = 26  # 20 MFCC coefficients + ^^ WHERE DID YOU GET THOSE?\n\tnClasses = 32 # forgot why ;)\n\nnHidden = 128\n####Load data\nprint(\'Loading data\')\nedData, maxTimeSteps, totalN, _ = load_batched_data(INPUT_PATH, TARGET_PATH, Size,match=our_data)\n\n####Define graph\nprint(\'Defining graph\')\nprint(\'This takes forever + some\')\nprint(\'Impossible to debug\')\nprint(\'It is slower than any other tensorflow graph, why?\')\n\ngraph = tf.Graph()\nwith graph.as_default():\n\t####NOTE: try variable-steps inputs and dynamic bidirectional rnn, when it\'s implemented in tensorflow\n\n\t####Graph input\n\tinputX = tf.placeholder(tf.float32, shape=(maxTimeSteps, Size, nFeatures))\n\t# Prep input data to fit requirements of rnn.bidirectional_rnn\n\t#  Reshape to 2-D tensor (nTimeSteps*Size, nfeatures)\n\tinputXrs = tf.reshape(inputX, [-1, nFeatures])\n\t#  Split to get a list of \'n_steps\' tensors of shape (_size, n_hidden)\n\tinputList = tf.split(axis=0, num_or_size_splits=maxTimeSteps, value=inputXrs)\n\ttargetIxs = tf.placeholder(tf.int64)\n\ttargetVals = tf.placeholder(tf.int32)\n\ttargetShape = tf.placeholder(tf.int64)\n\ttargetY = tf.SparseTensor(targetIxs, targetVals, targetShape)\n\tseqLengths = tf.placeholder(tf.int32, shape=Size)\n\n\t####Weights & biases\n\tstddev = np.sqrt(2.0 / (2 * nHidden))\n\ttruncated_normal = tf.truncated_normal([2, nHidden], stddev=stddev)\n\tweightsOutH1 = tf.Variable(truncated_normal)\n\tbiasesOutH1 = tf.Variable(tf.zeros([nHidden]))\n\tweightsOutH2 = tf.Variable(truncated_normal)\n\tbiasesOutH2 = tf.Variable(tf.zeros([nHidden]))\n\thalf_normal = tf.truncated_normal([nHidden, nClasses], stddev=np.sqrt(2.0 / nHidden))\n\tweightsClasses = tf.Variable(half_normal)\n\tbiasesClasses = tf.Variable(tf.zeros([nClasses]))\n\n\t####Network\n\tforwardH1 = rnn_cell.LSTMCell(nHidden, use_peepholes=True, state_is_tuple=True)\n\tbackwardH1 = rnn_cell.LSTMCell(nHidden, use_peepholes=True, state_is_tuple=True)\n\tprint(""building bidirectional_rnn ... SLOW!!!"")\n\tfbH1, _, _ = bidirectional_rnn(forwardH1, backwardH1, inputList, dtype=tf.float32, scope=\'BDLSTM_H1\')\n\tprint(""done building rnn"")\n\tprint(""building fbH1rs "")\n\tfbH1rs = [tf.reshape(t, [Size, 2, nHidden]) for t in fbH1]\n\tprint(""building outH1 "")\n\toutH1 = [tf.reduce_sum(tf.multiply(t, weightsOutH1), axis=1) + biasesOutH1 for t in fbH1rs]\n\tprint(""building logits "")\n\tlogits = [tf.matmul(t, weightsClasses) + biasesClasses for t in outH1]\n\tprint(""len(outH1) %d""% len(outH1))\n\t####Optimizing\n\tprint(""building loss"")\n\tlogits3d = tf.stack(logits)\n\tloss = tf.reduce_mean(ctc.ctc_loss(logits3d, targetY, seqLengths))\n\tout = tf.identity(loss, \'ctc_loss_mean\')\n\toptimizer = tf.train.MomentumOptimizer(learningRate, momentum).minimize(loss)\n\n\t####Evaluating\n\tprint(""building Evaluation"")\n\tlogitsMaxTest = tf.slice(tf.argmax(logits3d, 2), [0, 0], [seqLengths[0], 1])\n\tpredictions = tf.to_int32(ctc.ctc_beam_search_decoder(logits3d, seqLengths)[0][0])\n\treduced_sum = tf.reduce_sum(tf.edit_distance(predictions, targetY, normalize=False))\n\terrorRate = reduced_sum / tf.to_float(tf.size(targetY.values))\n\n\tcheck_op = tf.add_check_numerics_ops()\nprint(""done building graph"")\n\n####Run session\nwith tf.Session(graph=graph) as session:\n\ttry: merged = tf.summary.merge_all()\n\texcept: merged = tf.summary.merge_all()\n\ttry:writer = tf.summary.FileWriter(""/tmp/basic_new"", session.graph)\n\texcept: writer = tf.summary.FileWriter(""/tmp/basic_new"", session.graph)\n\ttry:saver = tf.train.Saver()  # defaults to saving all variables\n\texcept:\n\t\tprint(""tf.train.Saver() broken in tensorflow 0.12"")\n\t\tsaver = tf.train.Saver(tf.global_variables())# WTF stupid API breaking\n\tckpt = tf.train.get_checkpoint_state(\'./checkpoints\')\n\n\tstart = 0\n\tif ckpt and ckpt.model_checkpoint_path:\n\t\tp = re.compile(\'\\./checkpoints/model\\.ckpt-([0-9]+)\')\n\t\tm = p.match(ckpt.model_checkpoint_path)\n\t\ttry:start = int(m.group(1))\n\t\texcept:pass\n\tif saver and start > 0:\n\t\t# Restore variables from disk.\n\t\tsaver.restore(session, ""./checkpoints/model.ckpt-%d"" % start)\n\t\tprint(""Model %d restored."" % start)\n\telse:\n\t\tprint(\'Initializing\')\n\t\ttry: session.run(tf.global_variables_initializer())\n\t\texcept:session.run(tf.global_variables_initializer())\n\tfor epoch in range(nEpochs):\n\t\tprint(\'Epoch\', epoch + 1, \'...\')\n\t\terrors = np.zeros(len(edData))\n\t\tRandIxs = np.random.permutation(len(edData))  # randomize  order\n\t\tfor Nr, OrigI in enumerate(RandIxs):\n\t\t\tInputs, TargetSparse, SeqLengths = edData[OrigI]\n\t\t\tindices, values, shape = TargetSparse\n\t\t\tfeedDict = {inputX: Inputs, targetIxs: indices, targetVals: values, targetShape: shape, seqLengths: SeqLengths}\n\t\t\t_, l, er, lmt, ok = session.run([optimizer, loss, errorRate, logitsMaxTest, check_op], feed_dict=feedDict)\n\t\t\tprint(np.unique(lmt))\n\t\t\t# print unique argmax values of first sample in ; should be blank for a while, then spit out target values\n\t\t\tif (Nr % 1) == 0:\n\t\t\t\tprint(\'Mini\', Nr, \'/\', OrigI, \'loss:\', l)\n\t\t\t\tprint(\'Mini\', Nr, \'/\', OrigI, \'error rate:\', er)\n\t\t\terrors[Nr] = er * len(SeqLengths)\n\t\tepochErrorRate = errors.sum() / totalN\n\t\tprint(\'Epoch\', epoch + 1, \'error rate:\', epochErrorRate)\n\t\tif saver:saver.save(session, \'checkpoints/model.ckpt\', global_step=epoch + 1)\n\tprint(\'Learning finished\')\n\n'"
mfcc_feature_classifier.py,3,"b'#!/usr/bin/env python\n#!/usr/bin/env python\n#!/usr/bin/python\nimport numpy as np\nimport tensorflow as tf\n\nimport layer\nimport speech_data\nfrom speech_data import Source,Target\n\n#  LESS IS MORE! :\n# 0.001  Step 1000 Loss= 2.292103 Accuracy= 0.100 Time= 163s \t\t\tTest Accuracy:  0.1 too high vs\n# 0.0001  Step 1420 Loss= 1.794861 Accuracy= 0.600 Time= 231\n# 0.00001 Step 1700 Loss= 0.575172 Accuracy= 1.000 Time= 274s \t\t\tTest Accuracy:  0.8\nlearning_rate = 0.00001\ntraining_iters = 300000 #steps\nbatch_size = 64\n\n\nheight=20 # mfcc features\nwidth=80 # (max) length of utterance\nclasses=10 # digits\n\nbatch = word_batch = speech_data.mfcc_batch_generator(batch_size, source=Source.DIGIT_WAVES, target=Target.digits)\nX, Y = next(batch)\nprint(""batch shape "" + str(np.array(X).shape))\n\nshape=[-1, height, width, 1]\n# shape=[-1, width,height, 1]\n\n# BASELINE toy net\ndef simple_dense(net): # best with lr ~0.001\n\t# type: (layer.net) -> None\n\t# net.dense(hidden=200,depth=8,dropout=False) # BETTER!!\n\t# net.reshape(shape)  # Reshape input picture\n\tnet.dense(400, activation=tf.nn.tanh)# 0.99 YAY\n\t# net.denseNet(40, depth=4)\n\t# net.classifier() # auto classes from labels\n\treturn\n\ndef alex(net): # kinda\n\t# type: (layer.net) -> None\n\tprint(""Building Alex-net"")\n\tnet.reshape(shape)  # Reshape input picture\n\t# net.batchnorm()\n\tnet.conv([3, 3, 1, 64]) # 64 filters\n\tnet.conv([3, 3, 64, 128])\n\tnet.conv([3, 3, 128, 256])\n\tnet.conv([3, 3, 256, 512])\n\tnet.conv([3, 3, 512, 1024])\n\tnet.dense(1024,activation=tf.nn.relu)\n\tnet.dense(1024,activation=tf.nn.relu)\n\n\n# Densely Connected Convolutional Networks https://arxiv.org/abs/1608.06993  # advanced ResNet\ndef denseConv(net):\n\t# type: (layer.net) -> None\n\tprint(""Building dense-net"")\n\tnet.reshape(shape)  # Reshape input picture\n\tnet.buildDenseConv(nBlocks=1)\n\tnet.classifier() # auto classes from labels\n\n\ndef recurrent(net):\n\t# type: (layer.net) -> None\n\tnet.rnn()\n\tnet.classifier()\n\n\ndef denseNet(net):\n\t# type: (layer.net) -> None\n\tprint(""Building fully connected pyramid"")\n\tnet.reshape(shape)  # Reshape input picture\n\tnet.fullDenseNet()\n\tnet.classifier() # auto classes from labels\n\n# width=64 # for pcm baby data\n# batch=speech_data.spectro_batch_generator(1000,target=speech_data.Target.digits)\n# classes=10\n\n# CHOSE MODEL ARCHITECTURE HERE:\n# net=layer.net(simple_dense, data=batch,input_shape=[height,width],output_width=classes, learning_rate=learning_rate)\n# net=layer.net(model=alex,input_width= width*height,output_width=classes, learning_rate=learning_rate)\n# net=layer.net(model=denseConv,input_width= width*height,output_width=classes, learning_rate=learning_rate)\nnet = layer.net(recurrent, data=batch, input_shape=[height, width], output_width=classes, learning_rate=learning_rate)\n\n# net.train(data=batch,batch_size=10,steps=500,dropout=0.6,display_step=1,test_step=1) # debug\nnet.train(data=batch,batch_size=10,steps=training_iters,dropout=0.6,display_step=10,test_step=100) # test\n# net.train(data=batch,batch_size=batch_size,steps=training_iters,dropout=0.6,display_step=10,test_step=100) # run\n\n# net.predict() # nil=random\n# net.generate(3)  # nil=random\n\n'"
number_classifier_tflearn.py,0,"b'#!/usr/bin/env python\n#!/usr/bin/env PYTHONIOENCODING=""utf-8"" python\nimport tflearn\nimport pyaudio\nimport speech_data\nimport numpy\n\n# Simple spoken digit recognition demo, with 98% accuracy in under a minute\n\n# Training Step: 544  | total loss: 0.15866\n# | Adam | epoch: 034 | loss: 0.15866 - acc: 0.9818 -- iter: 0000/1000\n\nbatch=speech_data.wave_batch_generator(10000,target=speech_data.Target.digits)\nX,Y=next(batch)\n\nnumber_classes=10 # Digits\n\n# Classification\ntflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n\nnet = tflearn.input_data(shape=[None, 8192])\nnet = tflearn.fully_connected(net, 64)\nnet = tflearn.dropout(net, 0.5)\nnet = tflearn.fully_connected(net, number_classes, activation=\'softmax\')\nnet = tflearn.regression(net, optimizer=\'adam\', loss=\'categorical_crossentropy\')\n\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y,n_epoch=3,show_metric=True,snapshot_step=100)\n# Overfitting okay for now\n\ndemo_file = ""5_Vicki_260.wav""\ndemo=speech_data.load_wav_file(speech_data.path + demo_file)\nresult=model.predict([demo])\nresult=numpy.argmax(result)\nprint(""predicted digit for %s : result = %d ""%(demo_file,result))\n'"
number_gan_layer.py,0,"b'#!/usr/bin/env python\n#!/usr/bin/python\nimport speech_data\nimport layer\nfrom layer import *\n\n# Training Step: 544  | total loss: 0.15866\n# | Adam | epoch: 034 | loss: 0.15866 - acc: 0.9818 -- iter: 0000/1000\n# 98% Accuracy on training set in just a minute\n\n# audio = pyaudio.PyAudio()\n# # format=pyaudio.paFloat32\n# format=pyaudio.paInt8\n# # format=audio.get_format_from_width(f.getsampwidth())\n# # out_stream = audio.open( format=format,channels = f.getnchannels(), rate=f.getframerate(), output= True)\n# out_stream = audio.open( format=format,channels = 1, rate=48000, output= True)\n# out_stream.start_stream()\n# def play_pcm(data):\n#   out_stream.write(data)\n\nbatch=speech_data.wave_batch_generator(1000)\nX,Y=next(batch)\n\n# Classification\n# x = tflearn.input_data(shape=[None, 8192])\n# net = tflearn.fully_connected(x, 64)\n# net = tflearn.dropout(net, 0.5)\n# net = tflearn.fully_connected(net, 10, activation=\'softmax\')\n# net = tflearn.regression(net, optimizer=\'adam\', loss=\'categorical_crossentropy\')\n# y = net.placeholder\n# classifier = tflearn.DNN(net)\n\ndef model(net):\n\t# type: (layer.net) -> None\n\t# net.inputnet_data(shape=[None, 10])\n\tnet.fully_connected( 64)\n\tnet.dropout( 0.5)\n\tnet.fully_connected( 8192, activation=\'tanh\') #sigmoid\n\t# net.regression(gan, optimizer=\'adam\', loss=\'mean_square\')#,placeholder=__)\n# categorical_crossentropy, binary_crossentropy, softmax_categorical_crossentropy, hinge_loss, mean_square\n\n# print(""batch"",batch)\nnet=net(model,data=batch)\nnet.train(display_step=1,test_step=10)\n\t# model.fit(X, Y,n_epoch=10,show_metric=True,snapshot_step=1000)\n\t# net.fit(Y,X,n_epoch=100, show_metric=True, snapshot_step=1000)\n\t# gananlyzer.fit(Y, n_epoch=10, show_metric=True, snapshot_step=1000)\n\n\n'"
number_gan_tflearn.py,0,"b""#!/usr/bin/env python\nimport tflearn\nimport pyaudio\nimport speech_data\n\n\n# Training Step: 544  | total loss: 0.15866\n# | Adam | epoch: 034 | loss: 0.15866 - acc: 0.9818 -- iter: 0000/1000\n# 98% Accuracy on training set in just a minute\n\n# audio = pyaudio.PyAudio()\n# # format=pyaudio.paFloat32\n# format=pyaudio.paInt8\n# # format=audio.get_format_from_width(f.getsampwidth())\n# # out_stream = audio.open( format=format,channels = f.getnchannels(), rate=f.getframerate(), output= True)\n# out_stream = audio.open( format=format,channels = 1, rate=48000, output= True)\n# out_stream.start_stream()\n# def play_pcm(data):\n#   out_stream.write(data)\n\nbatch=speech_data.wave_batch(1000)\nX,Y=next(batch)\n\n# Classification\ntflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n#\n# x = tflearn.input_data(shape=[None, 8192])\n# net = tflearn.fully_connected(x, 64)\n# net = tflearn.dropout(net, 0.5)\n# net = tflearn.fully_connected(net, 10, activation='softmax')\n# net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n# y = net.placeholder\n# classifier = tflearn.DNN(net)\n\ngan = tflearn.input_data(shape=[None, 10])\ngan = tflearn.fully_connected(gan, 64)\ngan = tflearn.dropout(gan, 0.5)\ngan = tflearn.fully_connected(gan, 8192, activation='tanh') #sigmoid\ngan = tflearn.regression(gan, optimizer='adam', loss='mean_square')#,placeholder=__)\n# categorical_crossentropy, binary_crossentropy, softmax_categorical_crossentropy, hinge_loss, mean_square\n\ngan = tflearn.DNN(gan)\nwhile 1:\n\t# model.fit(X, Y,n_epoch=10,show_metric=True,snapshot_step=1000)\n\tgan.fit(Y,X,n_epoch=100, show_metric=True, snapshot_step=1000)\n\t# gananlyzer.fit(Y, n_epoch=10, show_metric=True, snapshot_step=1000)\n\n\n"""
record-autoencoder.py,0,"b""#!/usr/bin/env python\nimport numpy\n\nimport record\nimport layer\n\nframe = record.next_frame() # Generator\n\ndef baseline():\n\tpass\n\n# net=layer.net(baseline)\nnet = layer.identity()\n\ndef error(out, data):\n\treturn numpy.sum(numpy.abs(out-data))/len(data)\n\n\nif __name__ == '__main__':\n\tprediction=0\n\twhile 1:\n\t\tdata= next(frame)\n\t\tprint(error(prediction, data))\n\t\tprediction=net.predict(data)\n\n\n\n"""
record.py,0,"b'#!/usr/bin/env python\nimport subprocess\nimport skimage.io\nimport traceback\nimport numpy\nimport numpy as np\nimport os\nimport sys\nfrom os import system\nfrom platform import system as platform\nimport skimage.io\nimport wave\nimport pyaudio\nimport matplotlib.pyplot as plt\n\n\nplt.matshow([[1,0],[0,1]], fignum=1)\nplt.draw()\n\nif platform() == \'Darwin\':  # How Mac OS X is identified by Python\n    system(\'\'\'/usr/bin/osascript -e \'tell app ""Finder"" to set frontmost of process ""Python"" to true\' \'\'\')\n\ni = 0\nwidth=256\nheight=256\n\n# Number of bytes to be captured from audio stream\n# CHUNK = 512\n# CHUNK = 1024\n# CHUNK = 1024\n# CHUNK = 2048\nCHUNK = 4096\n# CHUNK = 9192\n\n# number of bytes used per FFT fourier slice\n# length=512\nlength = 1024\n# length=2048\n# length = 4096\n\n#  forward step in sliding window [ CHUNK    [[length]-> ]step      CHUNK   ]\n# step=32\n# step=64\n# step = 128\nstep=256\n# step=512\n# step<length : some overlap\n\nimage=numpy.array(bytearray(os.urandom(width*width)))\nimage=image.reshape(width,width)\n\ndef get_audio_input_stream():\n  INDEX = 0  # 1\n  # FORMAT = pyaudio.paInt8\n  FORMAT = pyaudio.paInt16\n  # FORMAT = pyaudio.paInt32\n  # FORMAT = pyaudio.paFloat32\n  CHANNELS = 1\n  # RATE = 22500\n  RATE = 48000 #* 2 = 96000Hz max on mac\n  INPUT_BLOCK_TIME = 0.05\n  # INPUT_BLOCK_TIME = 0.1\n  INPUT_FRAMES_PER_BLOCK = int(RATE * INPUT_BLOCK_TIME)\n\n  stream = pyaudio.PyAudio().open(\n    format=FORMAT,\n    channels=CHANNELS,\n    rate=RATE,\n    input=True,\n    frames_per_buffer=CHUNK,\n    input_device_index=INDEX)\n  return stream\n\n  \n\ndef next_frame():\n  stream = get_audio_input_stream()\n  while True:\n    try:\n      dataraw = stream.read(CHUNK)\n    except IOError as e:\n      print(e) # [Errno -9981] Input overflowed  WHY?\n      stream = get_audio_input_stream() # reset\n      continue\n    data0 = numpy.fromstring(dataraw, dtype=\'int16\')\n    yield data0\n\ndef record():\n  global i\n  global image\n  global winName\n  FILENAME = \'recording.wav\'\n  # r = numpy.array()\n  hamming_window = np.hamming(length) # minimize fourier frequency drain\n  #hamming hanning bartlett \'blackman\'\n  r = numpy.empty(length)\n  stream = get_audio_input_stream()\n  offset = 0\n  while True:\n    try:\n\t    dataraw = stream.read(CHUNK)\n    except IOError as e:\n\t    print(e) # [Errno -9981] Input overflowed  WHY?\n\t    stream=get_audio_input_stream()\n\t    pass\n    data0 = numpy.fromstring(dataraw, dtype=\'int16\')\n    # data0 = numpy.fromstring(dataraw, dtype=\'int8\')\n    if(i<20 and numpy.sum(np.abs(data0))<1000*width):\n      continue\n    r=numpy.append(r,data0)\n    while offset < r.size - length :\n      data = r[offset:offset+length]\n      data=data*hamming_window  # minimize fourier frequency drain\n      offset=offset + step\n\n      data = numpy.fft.fft(data)#.abs()\n      data = numpy.absolute(data)\n      data = data[0:height]/256.0#.split(data,512)\n      data = numpy.log2(data*0.05+1.0)#//*50.0;\n      numpy.putmask(data, data > 255, 255)\n\n      image[i] = data\n      i = i+1\n      if(i==width):\n        print(""i %d\\r""%i)\n        i=0\n        # image=image.T\n        image=numpy.rot90(image)\n        plt.matshow(image, fignum=1)\n        plt.draw()\n        plt.pause(0.01)\n        # result=spec2word(image) #todo: reconnect\n        # subprocess.call([""say"","" %s""%result])\n        # cv2.imshow(winName,image)\n        # if cv2.waitKey(10) == 27: BREAKS portAudio !!\n\nif __name__ == \'__main__\':\n  record()\n'"
speaker_classifier_tflearn.py,2,"b'#!/usr/bin/env python\n#!/usr/local/bin/python\n#!/usr/bin/env PYTHONIOENCODING=""utf-8"" python\nimport os\n\nimport tflearn\nimport speech_data as data\n\n# Simple speaker recognition demo, with 99% accuracy in under a minute ( on digits sample )\n\n# | Adam | epoch: 030 | loss: 0.05330 - acc: 0.9966 -- iter: 0000/1000\n# \x1b\'predicted speaker for 9_Vicki_260 : result = \', \'Vicki\'\nimport tensorflow as tf\nprint(""You are using tensorflow version ""+ tf.__version__) #+"" tflearn version ""+ tflearn.version)\nif tf.__version__ >= \'0.12\' and os.name == \'nt\':\n\tprint(""sorry, tflearn is not ported to tensorflow 0.12 on windows yet!(?)"")\n\tquit() # why? works on Mac?\n\nspeakers = data.get_speakers()\nnumber_classes=len(speakers)\nprint(""speakers"",speakers)\n\nbatch=data.wave_batch_generator(batch_size=1000, source=data.Source.DIGIT_WAVES, target=data.Target.speaker)\nX,Y=next(batch)\n\n\n# Classification\ntflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n\nnet = tflearn.input_data(shape=[None, 8192]) #Two wave chunks\nnet = tflearn.fully_connected(net, 64)\nnet = tflearn.dropout(net, 0.5)\nnet = tflearn.fully_connected(net, number_classes, activation=\'softmax\')\nnet = tflearn.regression(net, optimizer=\'adam\', loss=\'categorical_crossentropy\')\n\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y, n_epoch=100, show_metric=True, snapshot_step=100)\n\n# demo_file = ""8_Vicki_260.wav""\ndemo_file = ""8_Bruce_260.wav""\ndemo=data.load_wav_file(data.path + demo_file)\nresult=model.predict([demo])\nresult=data.one_hot_to_item(result,speakers)\nprint(""predicted speaker for %s : result = %s ""%(demo_file,result)) # ~ 97% correct\n'"
spectro_gan.py,29,"b'#!/usr/bin/env python\n#!/usr/local/bin/python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nimport speech_data\n\nsess = tf.InteractiveSession()\n\nbatch_size=100\nwidth=height=64\ndim=width*height\n# # Create the classifier model\nx2 = tf.placeholder(""float"", [batch_size, width, height], name=\'image_batch\')  # None~batch_size\nx = tf.reshape(x2, [batch_size, dim])  # flatten\nW = tf.Variable(tf.zeros([dim,10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n# # Define loss and optimizer\ny_ = tf.placeholder(""float"", [batch_size,10],name=\'label_batch\')\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n\n# GAN generative adversarial network:\n\n# Create the generator model\ny0 = y_ # share tf.placeholder(""float"", [10],name=""seed"")\nWg = tf.Variable(tf.zeros([10,dim]),name=\'W_generator\')\nxg=generated_x = tf.matmul(y0, Wg)\n\n# Create the discriminator model\nx0 = tf.Variable(tf.zeros([batch_size,dim]))\ndiscriminate = tf.assign(x0,x) # feed real data batch\ngenerate = tf.assign(x0,generated_x) # feed generated data batch\n\nWd = tf.Variable(tf.zeros([dim,1]))\nbd = tf.Variable(tf.zeros([1]))\nverdict = tf.sigmoid( tf.matmul(x, Wd) + bd)\n# Define loss and optimizer\nverdict_ = tf.placeholder(""float"", [batch_size], name=\'verdict\') # is this sample artificial \'0\' or real \'1\' ?\n\nlam=0.0000001\n# lam=0.01\n# discriminator_entropy = -tf.reduce_sum(verdict_ * tf.log(verdict))\ndiscriminator_entropy = tf.reduce_mean(tf.square(verdict_-verdict))\ngenerator_entropy = tf.reduce_mean(tf.square(x-generated_x))\n# cross_entropy = tf.reduce_sum(abs(y_-y))\n\ngan_entropy = discriminator_entropy + lam*generator_entropy\ngan_step  = tf.train.AdamOptimizer(learning_rate=0.04).minimize(gan_entropy) # 0.04=good #ANY VALUE WORKS!! WOW\n\n\nnegative=[0]*batch_size # input was fake\npositive=[1]*batch_size # input was real\n\ndef check_accuracy():\n  # Test trained model\n  prediction=tf.argmax(y,1)\n  probability=(y) #tf.div(y,tf.reduce_sum(y,0))\n  correct_prediction = tf.equal(prediction, tf.argmax(y_,1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n  batch_xs, batch_ys = next(batch)\n  feed_dict = {x2: batch_xs, y_: batch_ys}\n  best,p,a,verdict1= sess.run([prediction,probability,accuracy,verdict],feed_dict)\n  # print(best,a,list(map(lambda x:round(x,3),p[0])))\n  print(""overal accuracy "",a)\n\n\nbatch=speech_data.spectro_batch(batch_size)\n\ndraw=0\n\n# Train\ntf.global_variables_initializer().run()\nsteps=30000\ne=0\nfor i in range(steps):\n  batch_xs, batch_ys = next(batch)\n  # use x2 for matrix, x for flattened data\n  train_step.run({x2: batch_xs, y_: batch_ys})  # classical classifier\n  _, _, verdict1 = sess.run([discriminate, gan_step, verdict],\n                            {x2: batch_xs, y_: batch_ys, verdict_: positive})  # true examples\n  sampled, _, loss, verdict0 = sess.run([generate, gan_step, generator_entropy, verdict],\n                                        {x2: batch_xs, y_: batch_ys, verdict_: negative})  # generated samples\n  e+=loss\n  if(i%10==0):\n    print(""loss "",e)\n    e=0\n\n  if (i % 100 == 0):\n    # print(""Fool factor 0:  how often has it been fooled: %d %%""%(sum(verdict0)))\n    # print(""Fool factor 1:  how often did it identify true samples %d %%""%(sum(verdict1)))\n    print(""identified true samples %d%%  fooled: %d%%"" % (sum(verdict1),sum(verdict0)))\n    # imgs=np.reshape(batch_xs,(batch_size,28,28))[0]\n    check_accuracy()\n    if(draw):\n      imgs=np.reshape(sampled,(batch_size,64,64))[0]\n      plt.matshow(imgs,fignum=1)\n      # plt.matshow(sampled,fignum=1)\n      plt.draw()\n      plt.pause(0.01)\n\n\nsampled = sess.run(generated_x,{y_: [[0,0,0,3,0,0,0,0,0,0]]*batch_size})  # generated samples\nimgs = np.reshape(sampled, (batch_size, 64, 64))[0]\nplt.matshow(imgs, fignum=2)\nplt.show()\n'"
speech2text-seq2seq.py,16,"b'#!/usr/bin/env python\n#!/usr/local/bin/python\n\n\nprint(""""""\nUpdate:\ntf.nn.seq2seq doesn\'t work as hoped:\n\tIt needs a 1D Tensor (chars) as input, not 2D spectrogram/mfcc/...\n unless we feed it with very long 1D wave data,\n but that is probably not what seq2seq was intended to for.\n Fear not: 1D dilated convolution and LSTMs together with CTC are just fine.\n\n New dynamic seq2seq was recently added to the master and will probably be out with 1.0.0 release.I\n"""""")\n\n\nexit(0)\n\n""""""Sequence-to-sequence model with an attention mechanism.""""""\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\n# import sugartensor as tf\n# import sugartensor\nimport layer\nimport speech_data\nfrom speech_data import Source,Target\nfrom layer import net\n\nlearning_rate = 0.00001\ntraining_iters = 300000 #steps\nbatch_size = 64\n\n\n\ninput_classes=20 # mfcc features\nmax_input_length=80 # (max) length of utterance\nmax_output_length=20\noutput_classes=32 # dimensions: characters\n\n\n# Target.word here just returns the filename ""1_STEFFI_160.wav"" = digit_speaker_words-per-minute.wav nicely \'encoded\' ;)\nbatch=word_batch=speech_data.mfcc_batch_generator(batch_size, source=Source.DIGIT_WAVES, target=Target.hotword)\nX,Y=next(batch)\n\n# EOS=\'\\n\' # end of sequence symbol todo use how?\n# GO=1\t\t # start symbol 0x01 todo use how?\n# def decode(bytes):\n# \treturn """".join(map(chr, bytes)).replace(\'\\x00\', \'\').replace(\'\\n\', \'\')\n\nvocab_size=input_classes\ntarget_vocab_size=output_classes\nbuckets=[(max_input_length, max_output_length)] # our input and response words can be up to 10 characters long\n# (1000,1000) Takes 6 minutes on the Mac, half on Nvidia\nPAD=[0] # fill words shorter than 10 characters with \'padding\' zeroes\n\ninput_data    = x= X\ntarget_data   = y= Y\ntarget_weights= [[1.0]*50 + [0.0]*(max_input_length-50)] *batch_size # mask padding. todo: redundant --\nencoder_size = max_input_length\ndecoder_size = max_output_length #self.buckets[bucket_id]\n\nnum_dim=input_classes #?\n\n# residual block\ndef res_block(tensor, size, rate, dim=num_dim):\n    # filter convolution\n    conv_filter = tensor.sg_aconv1d(size=size, rate=rate, act=\'tanh\', bn=True)\n    # gate convolution\n    conv_gate = tensor.sg_aconv1d(size=size, rate=rate,  act=\'sigmoid\', bn=True)\n    # output by gate multiplying\n    out = conv_filter * conv_gate\n    # final output\n    out = out.sg_conv1d(size=1, dim=dim, act=\'tanh\', bn=True)\n    # residual and skip output\n    return out + tensor, out\n\n# expand dimension\nz = x.sg_conv1d(size=1, dim=num_dim, act=\'tanh\', bn=True)\n\n# dilated conv block loop\nskip = 0  # skip connections\nfor i in range(num_blocks):\n    for r in [1, 2, 4, 8, 16]:\n        z, s = res_block(z, size=7, rate=r)\n        skip += s\n\n# final logit layers\nlogit = (skip\n         .sg_conv1d(size=1, act=\'tanh\', bn=True)\n         .sg_conv1d(size=1, dim=voca_size))\n\n# CTC loss\nloss = logit.sg_ctc(target=y, seq_len=seq_len)\ntf.train.AdamOptimizer(learning_rate).minimize(loss)\nsaver = tf.train.Saver(tf.global_variables())\n\n# train\ntf.sg_train(log_interval=30, lr=0.0001, loss=loss, ep_size=1000, max_ep=200, early_stop=False)\n\n\n\n# tf.nn.seq2seq DOES\'T WORK: NEEDS 1D Tensor (chars) as input, not mfcc\n# class SpeechSeq2Seq(object):\n#\n# \tdef __init__(self,size, num_layers):\n#\n# \t\tcell = single_cell = tf.nn.rnn_cell.GRUCell(size)\n# \t\tif num_layers > 1:\n# \t\t cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n# \t\t# Feeds for inputs.\n# \t\tself.encoder_inputs = []\n# \t\tself.decoder_inputs = []\n# \t\tself.target_weights = []\n# \t\ti=0\n# \t\tself.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""encoder{0}"".format(i)))\n# \t\tself.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""decoder{0}"".format(i)))\n# \t\tself.target_weights.append(tf.placeholder(tf.float32, shape=[None], name=""weight{0}"".format(i)))\n# \t\t# tf.nn.rnn()\n# \t\t# targets = [self.decoder_inputs[i + 1] for i in xrange(len(self.decoder_inputs) - 1)]\n# \t\tself.outputs, self.losses = tf.nn.seq2seq.basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n# \t\ttf.train.AdamOptimizer(learning_rate).minimize(self.losses)\n# \t\tself.saver = tf.train.Saver(tf.all_variables())\n#\n# \t# def step(self, session, encoder_inputs, decoder_inputs, target_weights, test):\n# \t\tpass\n# def test():\n# \tperplexity, outputs = model.step(session, input_data, target_data, target_weights, test=True)\n# \twords = np.argmax(outputs, axis=2)  # shape (10, 10, 256)\n# \t# word = decode(words[0])\n# \tword = str(words[0])\n# \tprint(""step %d, perplexity %f, output: hello %s?"" % (step, perplexity, word))\n\n# def train():\n# \tstep=0\n# \ttest_step=1\n# \twith tf.Session() as session:\n# \t\tmodel= SpeechSeq2Seq(size=10, num_layers=1)\n# \t\tsession.run(tf.initialize_all_variables())\n# \t\twhile True:\n# \t\t\tmodel.step(session, input_data, target_data, target_weights, test=False) # no outputs in training\n# \t\t\tif step % test_step == 0:\n# \t\t\t\ttest()\n# \t\t\tstep=step+1\n'"
speech2text-tflearn.py,0,"b'#!/usr/bin/env python\n#!/usr/local/bin/python\n# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\nimport tflearn\n\nimport speech_data\n\nimport os\n\nlearning_rate = 0.0001\ntraining_iters = 300000  # steps\nbatch_size = 64\n\nwidth = 20  # mfcc features\nheight = 80  # (max) length of utterance\nclasses = 10  # digits\n\nbatch = word_batch = speech_data.mfcc_batch_generator(batch_size)\nX, Y = next(batch)\n\n# train, test, _ = ,X\ntrainX, trainY = X, Y\ntestX, testY = X, Y #overfit for now\n\n# Data preprocessing\n# Sequence padding\n# trainX = pad_sequences(trainX, maxlen=100, value=0.)\n# testX = pad_sequences(testX, maxlen=100, value=0.)\n# # Converting labels to binary vectors\n# trainY = to_categorical(trainY, nb_classes=2)\n# testY = to_categorical(testY, nb_classes=2)\n\n# Network building\nnet = tflearn.input_data([None, width, height])\n# net = tflearn.embedding(net, input_dim=10000, output_dim=128)\nnet = tflearn.lstm(net, 128, dropout=0.8)\nnet = tflearn.fully_connected(net, classes, activation=\'softmax\')\nnet = tflearn.regression(net, optimizer=\'adam\', learning_rate=learning_rate, loss=\'categorical_crossentropy\')\n# Training\nmodel = tflearn.DNN(net, tensorboard_verbose=0)\n# check model exists or not\nif os.path.isfile(""tflearn.lstm.model""):\n    print (""loading model tflearn.lstm.model"")\n    model.load(""tflearn.lstm.model"")\nwhile 1: #training_iters\n    model.fit(trainX, trainY, n_epoch=100, validation_set=(testX, testY), show_metric=True,\n          batch_size=batch_size)\n    _y=model.predict(X)\nmodel.save(""tflearn.lstm.model"")\nprint (_y)\nprint (Y)\n\n'"
speech_data.py,15,"b'#!/usr/bin/env python\n#!/usr/local/bin/python\n""""""Utilities for downloading and providing data from openslr.org, libriSpeech, Pannous, Gutenberg, WMT, tokenizing, vocabularies.""""""\n# TODO! see https://github.com/pannous/caffe-speech-recognition for some data sources\n\nimport os\nimport re\nimport sys\nimport wave\n\nimport numpy\nimport numpy as np\nimport skimage.io  # scikit-image\n\ntry:\n\timport librosa\nexcept:\n\tprint(""pip install librosa ; if you want mfcc_batch_generator"")\n# import extensions as xx\nfrom random import shuffle\ntry:\n\tfrom six.moves import urllib\n\tfrom six.moves import xrange  # pylint: disable=redefined-builtin\nexcept:\n\tpass # fuck 2to3\n\nspeech_commands=""sheila seven right one house down zero go yes wow six no three happy \\\nbird stop marvin two five on off four dog up tree cat bed nine eight left"".split("" "")\n\n# TRAIN_INDEX=\'train_words_index.txt\'\n# TEST_INDEX=\'test_words_index.txt\'\nSOURCE_URL = \'http://pannous.net/files/\' #spoken_numbers.tar\'\n\nDATA_DIR = \'data/\'\npcm_path = ""data/spoken_numbers_pcm/"" # 8 bit\nwav_path = ""data/spoken_numbers_wav/"" # 16 bit s16le\npath = pcm_path\nCHUNK = 4096\ntest_fraction=0.1 # 10% of data for test / verification\n\nclass Source:  # labels\n\tDIGIT_WAVES = \'./spoken_numbers_pcm.tar\'\n\t# DIGIT_SPECTROS = \'spoken_numbers_spectros_64x64.tar\'  # 64x64  baby data set, works astonishingly well\n\tDIGIT_SPECTROS = \'./spoken_numbers_spectros_64x64.tar\'\n\tNUMBER_WAVES = \'spoken_numbers_wav.tar\'\n\tNUMBER_IMAGES = \'spoken_numbers.tar\'  # width=256 height=256\n\tWORD_SPECTROS = \'https://dl.dropboxusercontent.com/u/23615316/spoken_words.tar\'  # width,height=512# todo: sliding window!\n\tSPEECH_COMMANDS = ""http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz"" # 16000Hz 1ch s16le (2 bytes per sample)\n\tWORD_WAVES = \'spoken_words_wav.tar\'\n\tTEST_INDEX = \'test_index.txt\'\n\tTRAIN_INDEX = \'train_index.txt\'\n\nfrom enum import Enum\nclass Target(Enum):  # labels\n\tdigits=1\n\tspeaker=2\n\twords_per_minute=3\n\tword_phonemes=4\n\tword = 5  # int vector as opposed to binary hotword\n\tsentence=6\n\tsentiment=7\n\tfirst_letter=8\n\thotword = 9\n\tspeech_commands = 10\n\t# test_word=9 # use 5 even for speaker etc\n\n\nnum_characters = 32\n# num_characters=60 #  only one case, Including numbers\n# num_characters=128 #\n# num_characters=256 #  including special characters\n# offset=0  # 1:1 mapping ++\n# offset=32 # starting with \' \' space\n# offset=48 # starting with  numbers\noffset = 64  # starting with characters\nmax_word_length = 20\nterminal_symbol = 0\n\ndef pad(vec, pad_to=max_word_length, one_hot=False,paddy=terminal_symbol):\n\tfor i in range(0, pad_to - len(vec)):\n\t\tif one_hot:\n\t\t\tvec.append([paddy] * num_characters)\n\t\telse:\n\t\t\tvec.append(paddy)\n\treturn vec\n\ndef char_to_class(c):\n\treturn (ord(c) - offset) % num_characters\n\ndef string_to_int_word(word, pad_to):\n\tz = map(char_to_class, word)\n\tz = list(z)\n\tz = pad(z)\n\treturn z\n\nclass SparseLabels:\n\tdef __init__(labels):\n\t\tlabels.indices = {}\n\t\tlabels.values = []\n\n\tdef shape(self):\n\t\treturn (len(self.indices),len(self.values))\n\n# labels: An `int32` `SparseTensor`.\n# labels.indices[i, :] == [b, t] means `labels.values[i]` stores the id for (batch b, time t).\n# labels.values[i]` must take on values in `[0, num_labels)`.\ndef sparse_labels(vec):\n\tlabels = SparseLabels()\n\tb=0\n\tfor lab in vec:\n\t\tt=0\n\t\tfor c in lab:\n\t\t\tlabels.indices[b, t] = len(labels.values)\n\t\t\tlabels.values.append(char_to_class(c))\n\t\t\t# labels.values[i] = char_to_class(c)\n\t\t\tt += 1\n\t\tb += 1\n\treturn labels\n\n\n\ndef progresshook(blocknum, blocksize, totalsize):\n\t\treadsofar = blocknum * blocksize\n\t\tif totalsize > 0:\n\t\t\t\tpercent = readsofar * 1e2 / totalsize\n\t\t\t\ts = ""\\r%5.1f%% %*d / %d"" % (\n\t\t\t\t\t\tpercent, len(str(totalsize)), readsofar, totalsize)\n\t\t\t\tsys.stderr.write(s)\n\t\t\t\tif readsofar >= totalsize: # near the end\n\t\t\t\t\t\tsys.stderr.write(""\\n"")\n\t\telse: # total size is unknown\n\t\t\t\tsys.stderr.write(""read %d\\n"" % (readsofar,))\n\ndef maybe_download(file, work_directory=DATA_DIR):\n\t""""""Download the data from Pannous\'s website, unless it\'s already here.""""""\n\tprint(""Looking for data %s in %s""%(file,work_directory))\n\tif not os.path.exists(work_directory):\n\t\ttry:\n\t\t\tos.mkdir(work_directory)\n\t\texcept:\n\t\t\tpass\n\tfilepath = os.path.join(work_directory, re.sub(\'.*\\/\',\'\',file))\n\tif not os.path.exists(filepath):\n\t\tif not file.startswith(""http""): url_filename = SOURCE_URL + file\n\t\telse: url_filename=file\n\t\tprint(\'Downloading from %s to %s\' % (url_filename, filepath))\n\t\tfilepath, _ = urllib.request.urlretrieve(url_filename, filepath,progresshook)\n\t\tstatinfo = os.stat(filepath)\n\t\tprint(\'Successfully downloaded\', file, statinfo.st_size, \'bytes.\')\n\t\t# os.system(\'ln -s \'+work_directory)\n\tif os.path.exists(filepath):\n\t\tprint(\'Extracting %s to %s\' % ( filepath, work_directory))\n\t\tos.system(\'tar xf \'+filepath+"" -C ""+work_directory)\n\t\tprint(\'Data ready!\')\n\treturn filepath.replace("".tar"","""")\n\ndef spectro_batch(batch_size=10):\n\treturn spectro_batch_generator(batch_size)\n\ndef speaker(filename):  # vom Dateinamen\n\t# if not ""_"" in file:\n\t#   return ""Unknown""\n\treturn filename.split(""_"")[1]\n\ndef get_speakers(path=pcm_path):\n\tmaybe_download(Source.DIGIT_SPECTROS)\n\tmaybe_download(Source.DIGIT_WAVES)\n\tfiles = os.listdir(path)\n\tdef nobad(name):\n\t\treturn ""_"" in name and not ""."" in name.split(""_"")[1]\n\tspeakers=list(set(map(speaker,filter(nobad,files))))\n\tprint(len(speakers),"" speakers: "",speakers)\n\treturn speakers\n\ndef load_wav_file(name):\n\tf = wave.open(name, ""rb"")\n\t# print(""loading %s""%name)\n\tchunk = []\n\tdata0 = f.readframes(CHUNK)\n\twhile data0:  # f.getnframes()\n\t\t# data=numpy.fromstring(data0, dtype=\'float32\')\n\t\t# data = numpy.fromstring(data0, dtype=\'uint16\')\n\t\tdata = numpy.fromstring(data0, dtype=\'uint8\')\n\t\tdata = (data + 128) / 255.  # 0-1 for Better convergence\n\t\t# chunks.append(data)\n\t\tchunk.extend(data)\n\t\tdata0 = f.readframes(CHUNK)\n\t# finally trim:\n\tchunk = chunk[0:CHUNK * 2]  # should be enough for now -> cut\n\tchunk.extend(numpy.zeros(CHUNK * 2 - len(chunk)))  # fill with padding 0\'s\n\t# print(""%s loaded""%name)\n\treturn chunk\n\n\ndef spectro_batch_generator(batch_size=10,width=64,source_data=Source.DIGIT_SPECTROS,target=Target.digits):\n\t# maybe_download(Source.NUMBER_IMAGES , DATA_DIR)\n\t# maybe_download(Source.SPOKEN_WORDS, DATA_DIR)\n\tpath=maybe_download(source_data, DATA_DIR)\n\tpath=path.replace(""_spectros"","""")# HACK! remove!\n\theight = width\n\tbatch = []\n\tlabels = []\n\tspeakers=get_speakers(path)\n\tif target==Target.digits: num_classes=10\n\tif target==Target.first_letter: num_classes=32\n\tif target==Target.speech_commands: num_classes=len(speech_commands)\n\tfiles = os.listdir(path)\n\t# shuffle(files) # todo : split test_fraction batch here!\n\t# files=files[0:int(len(files)*(1-test_fraction))]\n\tprint(""Got %d source data files from %s""%(len(files),path))\n\twhile True:\n\t\t# print(""shuffling source data files"")\n\t\tshuffle(files)\n\t\tfor image_name in files:\n\t\t\tif not ""_"" in image_name: continue # bad !?!\n\t\t\timage = skimage.io.imread(path + ""/"" + image_name).astype(numpy.float32)\n\t\t\t# image.resize(width,height) # lets see ...\n\t\t\tdata = image / 255.  # 0-1 for Better convergence\n\t\t\t# data = data.reshape([width * height])  # tensorflow matmul needs flattened matrices wtf\n\t\t\tbatch.append(list(data))\n\t\t\t# classe=(ord(image_name[0]) - 48)  # -> 0=0 .. A:65-48 ... 74 for \'z\'\n\t\t\tclasse = (ord(image_name[0]) - 48) % 32# -> 0=0  17 for A, 10 for z ;)\n\t\t\tlabels.append(dense_to_one_hot(classe,num_classes))\n\t\t\tif len(batch) >= batch_size:\n\t\t\t\tyield batch, labels\n\t\t\t\tbatch = []  # Reset for next batch\n\t\t\t\tlabels = []\n\ndef mfcc_batch_generator(batch_size=10, source=Source.DIGIT_WAVES, target=Target.digits):\n\tmaybe_download(source, DATA_DIR)\n\tif target == Target.speaker: speakers = get_speakers()\n\tbatch_features = []\n\tlabels = []\n\tfiles = os.listdir(path)\n\twhile True:\n\t\tprint(""loaded batch of %d files"" % len(files))\n\t\tshuffle(files)\n\t\tfor file in files:\n\t\t\tif not file.endswith("".wav""): continue\n\t\t\twave, sr = librosa.load(path+file, mono=True)\n\t\t\tmfcc = librosa.feature.mfcc(wave, sr)\n\t\t\tif target==Target.speaker: label=one_hot_from_item(speaker(file), speakers)\n\t\t\telif target==Target.digits:  label=dense_to_one_hot(int(file[0]),10)\n\t\t\telif target==Target.first_letter:  label=dense_to_one_hot((ord(file[0]) - 48) % 32,32)\n\t\t\telif target == Target.hotword: label = one_hot_word(file, pad_to=max_word_length)  #\n\t\t\telif target == Target.word: label=string_to_int_word(file, pad_to=max_word_length)\n\t\t\t\t# label = file  # sparse_labels(file, pad_to=20)  # max_output_length\n\t\t\telse: raise Exception(""todo : labels for Target!"")\n\t\t\tlabels.append(label)\n\t\t\t# print(np.array(mfcc).shape)\n\t\t\tmfcc=np.pad(mfcc,((0,0),(0,80-len(mfcc[0]))), mode=\'constant\', constant_values=0)\n\t\t\tbatch_features.append(np.array(mfcc))\n\t\t\tif len(batch_features) >= batch_size:\n\t\t\t\t# if target == Target.word:  labels = sparse_labels(labels)\n\t\t\t\t# labels=np.array(labels)\n\t\t\t\t# print(np.array(batch_features).shape)\n\t\t\t\t# yield np.array(batch_features), labels\n\t\t\t\t# print(np.array(labels).shape) # why (64,) instead of (64, 15, 32)? OK IFF dim_1==const (20)\n\t\t\t\tyield batch_features, labels  # basic_rnn_seq2seq inputs must be a sequence\n\t\t\t\tbatch_features = []  # Reset for next batch\n\t\t\t\tlabels = []\n\n\n# If you set dynamic_pad=True when calling tf.train.batch the returned batch will be automatically padded with 0s. Handy! A lower-level option is to use tf.PaddingFIFOQueue.\n# only apply to a subset of all images at one time\ndef wave_batch_generator(batch_size=10,source=Source.DIGIT_WAVES,target=Target.digits): #speaker\n\tmaybe_download(source, DATA_DIR)\n\tif target == Target.speaker: speakers=get_speakers()\n\tbatch_waves = []\n\tlabels = []\n\t# input_width=CHUNK*6 # wow, big!!\n\tfiles = os.listdir(path)\n\twhile True:\n\t\tshuffle(files)\n\t\tprint(""loaded batch of %d files"" % len(files))\n\t\tfor wav in files:\n\t\t\tif not wav.endswith("".wav""):continue\n\t\t\tif target==Target.digits: labels.append(dense_to_one_hot(int(wav[0])))\n\t\t\telif target==Target.speaker: labels.append(one_hot_from_item(speaker(wav), speakers))\n\t\t\telif target==Target.first_letter:  label=dense_to_one_hot((ord(wav[0]) - 48) % 32,32)\n\t\t\telse: raise Exception(""todo : Target.word label!"")\n\t\t\tchunk = load_wav_file(path+wav)\n\t\t\tbatch_waves.append(chunk)\n\t\t\t# batch_waves.append(chunks[input_width])\n\t\t\tif len(batch_waves) >= batch_size:\n\t\t\t\tyield batch_waves, labels\n\t\t\t\tbatch_waves = []  # Reset for next batch\n\t\t\t\tlabels = []\n\nclass DataSet(object):\n\n\tdef __init__(self, images, labels, fake_data=False, one_hot=False, load=False):\n\t\t""""""Construct a DataSet. one_hot arg is used only if fake_data is true.""""""\n\t\tif fake_data:\n\t\t\tself._num_examples = 10000\n\t\t\tself.one_hot = one_hot\n\t\telse:\n\t\t\tnum = len(images)\n\t\t\tassert num == len(labels), (\'images.shape: %s labels.shape: %s\' % (images.shape, labels.shape))\n\t\t\tprint(""len(images) %d"" % num)\n\t\t\tself._num_examples = num\n\t\tself.cache={}\n\t\tself._image_names = numpy.array(images)\n\t\tself._labels = labels\n\t\tself._epochs_completed = 0\n\t\tself._index_in_epoch = 0\n\t\tself._images=[]\n\t\tif load: # Otherwise loaded on demand\n\t\t\tself._images=self.load(self._image_names)\n\n\t@property\n\tdef images(self):\n\t\treturn self._images\n\n\t@property\n\tdef image_names(self):\n\t\treturn self._image_names\n\n\t@property\n\tdef labels(self):\n\t\treturn self._labels\n\n\t@property\n\tdef num_examples(self):\n\t\treturn self._num_examples\n\n\t@property\n\tdef epochs_completed(self):\n\t\treturn self._epochs_completed\n\n\t# only apply to a subset of all images at one time\n\tdef load(self,image_names):\n\t\tprint(""loading %d images""%len(image_names))\n\t\treturn list(map(self.load_image,image_names)) # python3 map object WTF\n\n\tdef load_image(self,image_name):\n\t\tif image_name in self.cache:\n\t\t\t\treturn self.cache[image_name]\n\t\telse:\n\t\t\timage = skimage.io.imread(DATA_DIR+ image_name).astype(numpy.float32)\n\t\t\t# images = numpy.multiply(images, 1.0 / 255.0)\n\t\t\tself.cache[image_name]=image\n\t\t\treturn image\n\n\n\tdef next_batch(self, batch_size, fake_data=False):\n\t\t""""""Return the next `batch_size` examples from this data set.""""""\n\t\tif fake_data:\n\t\t\tfake_image = [1] * width * height\n\t\t\tif self.one_hot:\n\t\t\t\tfake_label = [1] + [0] * 9\n\t\t\telse:\n\t\t\t\tfake_label = 0\n\t\t\treturn [fake_image for _ in xrange(batch_size)], [\n\t\t\t\t\tfake_label for _ in xrange(batch_size)]\n\t\tstart = self._index_in_epoch\n\t\tself._index_in_epoch += batch_size\n\t\tif self._index_in_epoch > self._num_examples:\n\t\t\t# Finished epoch\n\t\t\tself._epochs_completed += 1\n\t\t\t# Shuffle the data\n\t\t\tperm = numpy.arange(self._num_examples)\n\t\t\tnumpy.random.shuffle(perm)\n\t\t\t# self._images = self._images[perm]\n\t\t\tself._image_names = self._image_names[perm]\n\t\t\tself._labels = self._labels[perm]\n\t\t\t# Start next epoch\n\t\t\tstart = 0\n\t\t\tself._index_in_epoch = batch_size\n\t\t\tassert batch_size <= self._num_examples\n\t\tend = self._index_in_epoch\n\t\treturn self.load(self._image_names[start:end]), self._labels[start:end]\n\n\n# multi-label\ndef dense_to_some_hot(labels_dense, num_classes=140):\n\t""""""Convert class labels from int vectors to many-hot vectors!""""""\n\traise ""TODO dense_to_some_hot""\n\n\ndef one_hot_to_item(hot, items):\n\ti=np.argmax(hot)\n\titem=items[i]\n\treturn item\n\ndef one_hot_from_item(item, items):\n\t# items=set(items) # assure uniqueness\n\tx=[0]*len(items)# numpy.zeros(len(items))\n\ti=items.index(item)\n\tx[i]=1\n\treturn x\n\n\ndef one_hot_word(word,pad_to=max_word_length):\n\tvec=[]\n\tfor c in word:#.upper():\n\t\tx = [0] * num_characters\n\t\tx[(ord(c) - offset)%num_characters]=1\n\t\tvec.append(x)\n\tif pad_to:vec=pad(vec, pad_to, one_hot=True)\n\treturn vec\n\ndef many_hot_to_word(word):\n\ts=""""\n\tfor c in word:\n\t\tx=np.argmax(c)\n\t\ts+=chr(x+offset)\n\t\t# s += chr(x + 48) # numbers\n\treturn s\n\n\ndef dense_to_one_hot(batch, batch_size, num_labels):\n\tsparse_labels = tf.reshape(batch, [batch_size, 1])\n\tindices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n\tconcatenated = tf.concat(axis=1, values=[indices, sparse_labels])\n\tconcat = tf.concat(axis=0, values=[[batch_size], [num_labels]])\n\toutput_shape = tf.reshape(concat, [2])\n\tsparse_to_dense = tf.sparse_to_dense(concatenated, output_shape, 1.0, 0.0)\n\treturn tf.reshape(sparse_to_dense, [batch_size, num_labels])\n\n\ndef dense_to_one_hot(batch, batch_size, num_labels):\n\tsparse_labels = tf.reshape(batch, [batch_size, 1])\n\tindices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n\tconcatenated = tf.concat(axis=1, values=[indices, sparse_labels])\n\tconcat = tf.concat(axis=0, values=[[batch_size], [num_labels]])\n\toutput_shape = tf.reshape(concat, [2])\n\tsparse_to_dense = tf.sparse_to_dense(concatenated, output_shape, 1.0, 0.0)\n\treturn tf.reshape(sparse_to_dense, [batch_size, num_labels])\n\ndef dense_to_one_hot(labels_dense, num_classes=10):\n\t""""""Convert class labels from scalars to one-hot vectors.""""""\n\treturn numpy.eye(num_classes)[labels_dense]\n\ndef extract_labels(names_file,train, one_hot):\n\tlabels=[]\n\tfor line in open(names_file).readlines():\n\t\timage_file,image_label = line.split(""\\t"")\n\t\tlabels.append(image_label)\n\tif one_hot:\n\t\t\treturn dense_to_one_hot(labels)\n\treturn labels\n\ndef extract_images(names_file,train):\n\timage_files=[]\n\tfor line in open(names_file).readlines():\n\t\timage_file,image_label = line.split(""\\t"")\n\t\timage_files.append(image_file)\n\treturn image_files\n\n\ndef read_data_sets(train_dir,source_data=Source.NUMBER_IMAGES, fake_data=False, one_hot=True):\n\tclass DataSets(object):\n\t\tpass\n\tdata_sets = DataSets()\n\tif fake_data:\n\t\tdata_sets.train = DataSet([], [], fake_data=True, one_hot=one_hot)\n\t\tdata_sets.validation = DataSet([], [], fake_data=True, one_hot=one_hot)\n\t\tdata_sets.test = DataSet([], [], fake_data=True, one_hot=one_hot)\n\t\treturn data_sets\n\tVALIDATION_SIZE = 2000\n\tlocal_file = maybe_download(source_data, train_dir)\n\ttrain_images = extract_images(TRAIN_INDEX,train=True)\n\ttrain_labels = extract_labels(TRAIN_INDEX,train=True, one_hot=one_hot)\n\ttest_images = extract_images(TEST_INDEX,train=False)\n\ttest_labels = extract_labels(TEST_INDEX,train=False, one_hot=one_hot)\n\t# train_images = train_images[:VALIDATION_SIZE]\n\t# train_labels = train_labels[:VALIDATION_SIZE:]\n\t# test_images = test_images[VALIDATION_SIZE:]\n\t# test_labels = test_labels[VALIDATION_SIZE:]\n\tdata_sets.train = DataSet(train_images, train_labels , load=False)\n\tdata_sets.test = DataSet(test_images, test_labels, load=True)\n\t# data_sets.validation = DataSet(validation_images, validation_labels, load=True)\n\treturn data_sets\n\nif __name__ == ""__main__"":\n\tprint(""downloading speech datasets"")\n\tmaybe_download( Source.DIGIT_SPECTROS)\n\tmaybe_download( Source.DIGIT_WAVES)\n\tmaybe_download( Source.NUMBER_IMAGES)\n\tmaybe_download( Source.NUMBER_WAVES)\n'"
speech_encoder.py,36,"b'#!/usr/bin/env python\n""""""A simple speech classifer AND autoencoder\n\n1st step : classify+encode the spectogram of 10 spoken digits into one-hot-vector + \'flavor\'-vector of size 10\nThe \'flavor\' vector is meant to hold voice characteristics orthogonal to the \'number\' information: gender, speed, pitch, ...\n\nINPUT: spectogram\nENCODED: classifed digit + flavor\nOUTPUT: reconstructed spectogram\n\niteration 2000 speech_entropy  9.46917 overal accuracy:  0.9502\niteration 72600 cross_entropy  0.345358 overal accuracy:  0.9714\niteration 73100 cross_entropy  nan overal accuracy  0.098  WHY NAN??\n\n""""""\n\nimport sys\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nimport speech_data\nspeech = speech_data.read_data_sets(""data/"", one_hot=True)\n\n# width=256\n# height=256\nwidth=512\nheight=512\n\nn1=400\nn2=100\nn3=20\n\nprint(""Creating the model"")\n\n# input: spectogram (None: batch of variable size)\nx = tf.placeholder(""float"", [None, width * height])\n\n# l_rate = tf.placeholder(""float"", [1])# 0.01\nl_rate = tf.Variable(0.003)\n\nW1 = tf.Variable(tf.random_uniform([width * height,400],maxval=0.0001))\nb1 = tf.Variable(tf.random_uniform([400]))\nW2 = tf.Variable(tf.random_uniform([400,100],maxval=0.001))\nb2 = tf.Variable(tf.random_uniform([100]))\nW3 = tf.Variable(tf.random_uniform([100,n3],maxval=0.01))\nb3 = tf.Variable(tf.zeros([n3]))\nh= tf.nn.tanh( tf.matmul(x,W1)+b1) #\nh= tf.nn.dropout(h,keep_prob=.5) # THAT!\nh2= tf.nn.tanh( tf.matmul(h,W2)+b2) #\n# _y = tf.matmul(h,W2) + b2 # 10 Numbers + 10 \'styles\'\n_y = tf.matmul(h2,W3) + b3 # 10 Numbers + 10 \'styles\'\n# print(""_y "",tf.rank(_y))\nif n3==20:\n  y = tf.nn.softmax(tf.slice(_y,[0,0],[-1,10])) #  softmax of all batches(-1) only on the numbers(10)\nelse:\n  y = tf.nn.softmax(_y)\n\ne2= tf.matmul(_y,tf.transpose(W3)) #+b2\ne2=tf.nn.tanh(e2)\ne1= tf.matmul(e2,tf.transpose(W2)) #+b2  _y\ne1=tf.nn.tanh(e1)\n\nx_=x_reconstructed= tf.nn.sigmoid(tf.matmul(e1,tf.transpose(W1)))\n# measure reconstructed signal against input:\ny_ = tf.placeholder(""float"", [None,10])\n\n# Define loss and optimizer\nspeech_entropy = -tf.reduce_sum(y_*tf.log(y))\n\n# encod_entropy = -tf.reduce_sum(x_*tf.log(x))\nencod_entropy = tf.sqrt(tf.reduce_mean(tf.square(x - x_)))\n# encod_entropy = tf.reduce_mean(tf.square(x - x_))\n\ncross_entropy = encod_entropy * speech_entropy\n# cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\nspeech_step = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(speech_entropy)\nencod_step = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(encod_entropy)\ntrain_step = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(cross_entropy)\n\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y,1), tf.argmax(y_,1)), ""float""))\n\ndef flatten(matrix):\n  # print(""flattening %d""\n  # return itertools.chain.from_iterable(matrix)\n  return [item for vector in matrix for item in vector]\n\n\ntest_images=[flatten(matrix) for matrix in speech.test.images]\n\ndef eval(feed):\n    print(""cross_entropy "",cross_entropy.eval(feed))#, end=\' \')\n    print(""encod_entropy "",encod_entropy.eval(feed))#, end=\' \')\n    print(""speech_entropy "",speech_entropy.eval(feed))#, end=\' \')\n    print(""overal accuracy "",accuracy.eval({x: test_images, y_: speech.test.labels}))#, end=\'\\r\'WWWWAAA)\n\n\ndef train_spectrogram_encoder():\n  tf.global_variables_initializer().run()\n  print(""Pretrain"")\n  for i in range(6000-1):\n    batch_xs, batch_ys = speech.train.next_batch(100)\n    # WTF, tensorflow can\'t do 3D tensor operations?\n    # https://github.com/tensorflow/tensorflow/issues/406 =>\n    batch_xs=[flatten(matrix) for matrix in batch_xs]\n    #  you have to reshape to flat/matrix data? why didn\'t they call it matrixflow?\n    feed = {x: batch_xs, y_: batch_ys}\n    speech_step.run(feed) # better for encod_entropy too! (later)\n    if(i%100==0):\n        print(""iteration %d""%i)#, end=\' \')\n        eval(feed)\n    if((i+1)%7000==0):\n      print(""l_rate*=0.1"")\n      sess.run(tf.assign(l_rate,l_rate*0.1))\n  print(""Train"")\n  for i in range(100000):\n    batch_xs, batch_ys = speech.train.next_batch(100)\n    feed = {x: batch_xs, y_: batch_ys}\n    if((i+1)%9000==0):sess.run(tf.assign(l_rate,l_rate*0.3))\n    encod_step.run(feed) # alternating!\n    speech_step.run(feed)\n    train_step.run(feed)\n    if(i%100==0):\n      print(""iteration %d""%i)#, end=\' \')\n      eval(feed)\n\nif __name__ == \'__main__\':\n  train_spectrogram_encoder()\n\n'"
subtitle-downloader.py,0,"b'#!/usr/bin/env python\n#-------------------------------------------------------------------------------\n# Name      : subtitle downloader\n# Purpose   : One step subtitle download\n#\n# Authors   : manoj m j, arun shivaram p, Valentin Vetter, niroyb\n# Edited by : Valentin Vetter\n# Created   :\n# Copyright : (c) www.manojmj.com\n# Licence   : GPL v3\n#-------------------------------------------------------------------------------\n\n# TODO: use another DB if subs are not found on subDB\nimport hashlib\nimport os\nimport shutil \nimport sys\nimport logging\nimport requests,time,re,zipfile\nfrom bs4 import BeautifulSoup\nPY_VERSION = sys.version_info[0]\nif PY_VERSION == 2:\n    import urllib2\nif PY_VERSION == 3:\n    import urllib.request\n\n\ndef get_hash(file_path):\n    read_size = 64 * 1024\n    with open(file_path, \'rb\') as f:\n        data = f.read(read_size)\n        f.seek(-read_size, os.SEEK_END)\n        data += f.read(read_size)\n    return hashlib.md5(data).hexdigest()\n\n\ndef sub_downloader(file_path):\n    # Put the code in a try catch block in order to continue for other video files, if it fails during execution\n    try:\n        # Skip this file if it is not a video\n        root, extension = os.path.splitext(file_path)\n        if extension not in ["".avi"", "".mp4"", "".mkv"", "".mpg"", "".mpeg"", "".mov"", "".rm"", "".vob"", "".wmv"", "".flv"", "".3gp"","".3g2""]:\n            return\n\n        if not os.path.exists(root + "".srt""):\n            headers = {\'User-Agent\': \'SubDB/1.0 (subtitle-downloader/1.0; http://github.com/manojmj92/subtitle-downloader)\'}\n            url = ""http://api.thesubdb.com/?action=download&hash="" + get_hash(file_path) + ""&language=en""\n            if PY_VERSION == 3:\n                req = urllib.request.Request(url, None, headers)\n                response = urllib.request.urlopen(req).read()\n            if PY_VERSION == 2:\n                req = urllib2.Request(url, \'\', headers)\n                response = urllib2.urlopen(req).read()\n\n            with open(root + "".srt"", ""wb"") as subtitle:\n                subtitle.write(response)\n                logging.info(""Subtitle successfully downloaded for "" + file_path)\n    except:\n        #download subs from subscene if not found in subdb  \n        sub_downloader2(file_path)\ndef sub_downloader2(file_path):\n    try:\n        root, extension = os.path.splitext(file_path)\n        if extension not in ["".avi"", "".mp4"", "".mkv"", "".mpg"", "".mpeg"", "".mov"", "".rm"", "".vob"", "".wmv"", "".flv"", "".3gp"","".3g2""]:\n            return  \n        if os.path.exists(root + "".srt""):\n            return\n        j=-1\n        root2=root\n        for i in range(0,len(root)):\n            if(root[i]==""\\\\"" or root[i] ==""/""):\n                j=i\n                break\n        root=root2[j+1:]\n        root2=root2[:j+1]\n        r=requests.get(""http://subscene.com/subtitles/release?q=""+root);\n        soup=BeautifulSoup(r.content,""lxml"")\n        atags=soup.find_all(""a"")\n        href=""""\n        for i in range(0,len(atags)):\n            spans=atags[i].find_all(""span"")\n            if(len(spans)==2 and spans[0].get_text().strip()==""English""):\n                href=atags[i].get(""href"").strip()               \n        if(len(href)>0):\n            r=requests.get(""http://subscene.com""+href);\n            soup=BeautifulSoup(r.content,""lxml"")\n            lin=soup.find_all(\'a\',attrs={\'id\':\'downloadButton\'})[0].get(""href"")\n            r=requests.get(""http://subscene.com""+lin);\n            soup=BeautifulSoup(r.content,""lxml"")\n            subfile=open(root2+"".zip"", \'wb\')\n            for chunk in r.iter_content(100000):\n                subfile.write(chunk)\n                subfile.close()\n                time.sleep(1)\n                zip=zipfile.ZipFile(root2+"".zip"")\n                zip.extractall(root2)\n                zip.close()\n                os.unlink(root2+"".zip"")\n                shutil.move(root2+zip.namelist()[0], os.path.join(root2, root + "".srt""))\n    except:\n        #Ignore exception and continue\n        print(""Error in fetching subtitle for "" + file_path)\n        print(""Error"", sys.exc_info())\n        logging.error(""Error in fetching subtitle for "" + file_path + str(sys.exc_info()))\n\n\ndef main():\n    root, _ = os.path.splitext(sys.argv[0])\n    logging.basicConfig(filename=root + \'.log\', level=logging.INFO)\n    logging.info(""Started with params "" + str(sys.argv))\n\n    if len(sys.argv) == 1:\n        print(""This program requires at least one parameter"")\n        sys.exit(1)\n\n    for path in sys.argv:\n        if os.path.isdir(path):\n            # Iterate the root directory recursively using os.walk and for each video file present get the subtitle\n            for dir_path, _, file_names in os.walk(path):\n                for filename in file_names:\n                    file_path = os.path.join(dir_path, filename)\n                    sub_downloader(file_path)\n        else:\n            sub_downloader(path)\n\nif __name__ == \'__main__\':\n    main()\n'"
subtitle_srt_parser.py,0,"b'#!/usr/local/bin/python\nfrom __future__ import print_function\n\nimport wave\n\nimport numpy\nimport pyaudio\nfrom dateutil import parser\n\naudio = pyaudio.PyAudio()\n\nwaveFile = wave.open(\'qanda_2012_ep99_climate.wav\', \'r\')\nsubtitles = ""qanda_2012_ep99_climate.srt""\n\n# csvfile = open(\'qanda/qanda_2012_ep99_climate.csv\')  # , newline=\'\' is an invalid keyword\n# spamreader = csv.reader(csvfile, delimiter=\',\', quotechar=\'""\')\n# for row in spamreader:\n# \ttry:\n# \t\ttime, ida, kind, speaker, text = row\n# \t\tprint(speaker)\n# \texcept:\n# \t\tpass\n\nframeRate = waveFile.getframerate()\nout_stream = audio.open(format=pyaudio.paInt16, channels=waveFile.getnchannels(), rate=frameRate, output=True)\nout_stream.start_stream()\n\ndef play(fro, to):\n\tfro = fro* frameRate/1000\n\tto = to * frameRate /1000\n\twaveFile.setpos(fro)\n\tdataraw = waveFile.readframes(2 * (to - fro))  # 2 bytes per frame : 16bit little-endian mono @ 16khz\n\tdata0 = numpy.fromstring(dataraw, dtype=\'int16\')\n\tout_stream.write(data0)\n\n\n# yield data0\n\n\ndef milliseconds(offset):\n\ttime = parser.parse(offset).time()\n\treturn time.microsecond / 1000 + 1000 * (time.second + 60 * (time.minute + 60 * time.hour))\n\n\nlines = open(subtitles).readlines()\ni = 0\nwhile i<len(lines):\n\tnr,offsets,word=lines[i:i+3]\n\ti += 4\n\tprint(word)\n\tfro, to = offsets.split(""-->"")\n\tfro = milliseconds(fro)\n\tto = milliseconds(to)\n\tif fro > to or (to - fro) > 3 * 1000:\n\t\tprint(""segment too long:"")\n\t\tprint(word)\n\t\tprint(fro)\n\t\tprint(to)\n\t\tcontinue\n\tplay(fro,to)\n\n'"
wave_GANerate.py,33,"b'#!/usr/bin/env python\n#!/usr/bin/python \n# PYTHONUNBUFFERED=1\n""""""A simple GAN network and classifer.\n""""""\n# from __future__ import print_function\n# Import data\nimport matplotlib.pyplot as plt\nimport pyaudio\nimport wave\nimport sys\nimport skimage\nfrom skimage.transform import resize, rescale\nimport tensorflow as tf\nimport numpy as np\nsess = tf.InteractiveSession()\n\nimport speech_data\n\nnumber_of_classes=10 # 10 digits\ninput_width=4096*2 # CHUNK*6 vs width*height\nbatch_size=10\n\n# Create the classifier model\nx = tf.placeholder(""float"", [batch_size, input_width],name=\'wave_batch\') # None~batch_size\nx0 = tf.Variable(tf.zeros([batch_size,input_width]),name=\'classifier_input\')\nhidden1size=64 #number_of_classes\nW1 = tf.Variable(tf.truncated_normal([input_width,hidden1size]))\nb1 = tf.Variable(tf.zeros([hidden1size]))\ny1 = tf.nn.softmax(tf.matmul(x,W1) + b1)\ny1 = tf.nn.dropout(y1,0.5)\nW = tf.Variable(tf.truncated_normal([hidden1size,number_of_classes]))\nb = tf.Variable(tf.zeros([number_of_classes]))\ny = tf.nn.softmax(tf.matmul(y1,W) + b)\n# Define loss and optimizer\ny_ = tf.placeholder(""float"", [batch_size,number_of_classes],name=\'label_batch\')\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n# cross_entropy = tf.reduce_sum(abs(y_-y))\n# cross_entropy = tf.reduce_sum(tf.square(y_-y))\ntrain_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\nassign_batch = tf.assign(x0,x) # feed real data batch (vs gan_assign later)\n\n# GAN generative adversarial network:\n\n# Create the discriminator model\nWd = tf.Variable(tf.zeros([input_width,1]))\nbd = tf.Variable(tf.zeros([1]))\nverdict = tf.sigmoid( tf.matmul(x0, Wd) + bd)\nverdict_ = tf.placeholder(""float"", [batch_size], name=\'verdict\') # is this sample artificial \'0\' or real \'1\' ?\ndiscriminator_entropy = tf.reduce_mean(tf.square(verdict_-verdict))\n# discriminator_entropy = -tf.reduce_sum(verdict_ * tf.log(verdict))\n\n# Create the generator model\ny0 = y_ # share tf.placeholder(""float"", [10],name=""seed"")\nWg = tf.Variable(tf.zeros([10,input_width]),name=\'W_generator\')\nxg=generated_x = tf.matmul(y0, Wg)\ngenerator_entropy = tf.reduce_mean(tf.square(x-generated_x))\n\n#  evaluate and optimize the GAN\'s generator and discriminator\n# lam=0.0000001\nlam=0.01\ngan_assign= tf.assign(x0,generated_x) # feed generated data batch into classifier\ngan_entropy = discriminator_entropy + lam*generator_entropy\n# gan_entropy = lam*cross_entropy + generator_entropy\ngan_step  = tf.train.AdamOptimizer(learning_rate=0.04).minimize(gan_entropy) # 0.04=good #ANY VALUE WORKS!! WOW\n\n\n\ndef play_pcm(data):\n  print(""play_pcm"")\n  # f = wave.open(r""./test.wav"", ""rb"")\n  audio = pyaudio.PyAudio()\n  # format=pyaudio.paFloat32\n  format=pyaudio.paInt8\n  # format=audio.get_format_from_width(f.getsampwidth())\n  # out_stream = audio.open( format=format,channels = f.getnchannels(), rate=f.getframerate(), output= True)\n  out_stream = audio.open( format=format,channels = 1, rate=48000, output= True)\n  out_stream.start_stream()\n  out_stream.write(data)\n\n\n# Train\ntf.global_variables_initializer().run()\nsteps=3000#000\nbatch=speech_data.wave_batch_generator(target=speech_data.Target.digits)\nnegative=[0]*batch_size # input was fake\npositive=[1]*batch_size # input was real\n# print(next(batch))\nerr=0\nbatch_xs, batch_ys = next(batch) #  keep constant for overfitting\nfor i in range(steps):\n  # batch_xs, batch_ys = next(batch)\n  # batch_xs1 = np.reshape(batch_xs,[batch_size,width*height])\n  feed_dict={x: batch_xs, y_: batch_ys}\n  _,loss=sess.run([train_step,cross_entropy],feed_dict) # classical classifier\n  # _,_,loss=sess.run([assign_batch,train_step,cross_entropy],feed_dict) # classical classifier\n  # feed_dict[verdict_]=positive # true examples\n  # _, _, verdict1 =sess.run([assign_batch,gan_step,verdict],feed_dict)\n  #\n  # feed_dict[verdict_]=negative # generated samples\n  # sampled, _ , _, verdict0 =sess.run([generated_x,gan_step,gan_assign,verdict], feed_dict)\n  # sampled,_,_,loss=sess.run([generated_x,gan_step,gan_assign,gan_entropy],feed_dict) # gan classifier\n  err+=loss\n  if(i%20==0):\n    print ""%d loss %f\\r""%(i,err),\n    sys.stdout.flush()\n    # print(""%d loss %f\\r""%(i,err), end=\'\')#,flush=True) WTF PY3 \n    err=0\n\n  if(i%250==1):\n    # play_pcm(sampled)\n    # check_accuracy()\n    # Test trained model\n    prediction=tf.argmax(y,1)\n    probability=(y) #tf.div(y,tf.reduce_sum(y,0))\n    correct_prediction = tf.equal(prediction, tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n    # if not overfit: batch_xs, batch_ys = next(batch)\n    feed_dict={x: batch_xs, y_: batch_ys}\n    best,p,a,verdict1= sess.run([prediction,probability,accuracy,verdict],feed_dict)\n    # print(best,a,list(map(lambda x:round(x,3),p[0])))\n    print(""\\noveral accuracy %f""%a)\n\n\n\nprint(""FINAL TEST:"")\nsampled = sess.run(generated_x,{y_: [[0,0,0,3,0,0,0,0,0,0]]*batch_size})  # generated samples\nplay_pcm(sampled)\n'"
extra/cepstrum.py,0,"b'# PowerSpectrum = abs(fft(SpeechFrame,1024)).^2;\n# AutoCorrelation = ifft(PowerSpectrum,1024);\n# Cepstrum = ifft(log(PowerSpectrum),1024);\nimport numpy as np\nimport scipy.io.wavfile\nfrom scikits.talkbox.features import mfcc\nimport matplotlib.pyplot as plt\n\nsample_rate, X = scipy.io.wavfile.read(""0_Kathy_160.wav"")\nprint(""sample_rate"",sample_rate)\nceps, mspec, spec = mfcc(X,fs=sample_rate, nceps=10)\nprint(""mspec "",mspec)\nprint(""ceps "",ceps )\nprint(""num_ceps "",len(ceps))\nnum_ceps = len(ceps)\n\nnp.save(""0_Kathy_160.ceps"", ceps) # cache results so that ML becomes fast\n\nX = []\n# ceps = np.load(""0_Kathy_160.ceps"")\n# plt.matshow(ceps.transpose())\n# plt.matshow(X)\n# plt.matshow(mspec.transpose())\nspec=spec.transpose()\nspec=spec[len(spec)/2:]\na0=np.average(spec[0:len(spec)/4])\na1=np.average(spec[len(spec)/4:len(spec)/2])\na0=np.amax(spec[0:len(spec)/4],axis=0)\na1=np.amax(spec[len(spec)/4:len(spec)/2],axis=0)\na2=np.amin(spec[0:len(spec)/4],axis=0)\na3=np.amin(spec[len(spec)/4:len(spec)/2],axis=0)\na4=np.argmax(spec[0:len(spec)/2])*5\n# spec=spec[len(spec)/2:]\nspec[0]=a0\nspec[1]=a1\nspec[2]=a2\nspec[3]=a3\nspec[4]=a4\nplt.matshow(spec)\nplt.show()\nX.append(np.mean(ceps[int(num_ceps / 10):int(num_ceps * 9 / 10)], axis=0))\nVx = np.array(X)\nprint(Vx)\n# use Vx as input values vector for neural net, k-means, etc\n\n\n# OTHER: https://github.com/jameslyons/python_speech_features .\n#     features.mfcc() - Mel Frequency Cepstral Coefficients\n#     features.fbank() - Filterbank Energies\n#     features.logfbank() - Log Filterbank Energies\n#     features.ssc() - Spectral Subband Centroids\n'"
extra/generate_sound.py,36,"b'""""""A simple speech classifer AND autoencoder\n""""""\nfrom __future__ import print_function\nimport pyaudio\nimport wave\nimport numpy\nimport numpy as np\nimport pymouse\nimport pygame\nimport math\n\n# pygame.init()\n# pygame.mixer.init(frequency=22050, size=-16, channels=1, buffer=4096)\npygame.mixer.init(frequency=8000, size=-16, channels=1, buffer=4096)\nmouse = pymouse.PyMouse() # as sine input\n\n# import keyboard_listener\n# keyboard_listener.listen_keyboard(lambda x:print(""x""+x))\n#define stream chunk\nchunk = 1024\nCHUNK = 4096\n# CHUNK = 9192\n# length=512\nlength = 1024\nvolume = 0.2     # range [0.0, 1.0]\n# length=2048\n\n#open a wav format music\n# import wave\n# w = wave.open(\'/usr/share/sounds/ekiga/voicemail.wav\', \'r\')\n# for i in range(w.getnframes()):\n#   frame = w.readframes(i)\n#   print frame\n\n# import sounddevice as sd\n# CHANNELS = 2\n# RECORD_SECONDS = 5\n# myrecording = sd.rec(int(RECORD_SECONDS * RATE), samplerate=RATE, channels=CHANNELS, blocking=True, dtype=\'float32\')\n\n#instantiate PyAudio\naudio = pyaudio.PyAudio()\n#open stream\n\n# f = wave.open(r""./test.wav"", ""rb"")\nf = wave.open(r""./data/spoken_numbers_pcm/7_Vicki_160.wav"", ""rb"")\n\n# out_stream = audio.open(format = audio.get_format_from_width(f.getsampwidth()),\nout_stream = audio.open( format=pyaudio.paInt8, channels = f.getnchannels(), rate=f.getframerate(), output= True)\n# out_stream = audio.open( format=pyaudio.paFloat32, channels = f.getnchannels(), rate=f.getframerate(), output= True)\n\n# data0 = f.readframes(chunk)\n# data=numpy.fromstring(data0, dtype=\'uint8\') #// set in stream ^^ !!\n  # pygame.sndarray.make_sound(data).play()\n  # stream.write(data*volume*volume)\n  # data = np.reshape(data, (data.shape[0]/2, 2))# split stereo\n  # p(data.shape)\n  # x=numpy.float32(x)/255.0\n  # numpy.seterrcall(cat)\n  # numpy.seterrcall(lambda x: print(""XX""+x))\n  # numpy.seterr(divide=\'ignore\', invalid=\'ignore\')#// NO!\n  # numpy.seterr(all=\'raise\')# Does not help\n  # data0 = f.readframes(chunk)\n  # stream.write(data)\n\n\n\ndef gensin(frequency, duration, sampRate):\n    """""" Frequency in Hz, duration in seconds and sampleRate in Hz""""""\n    cycles = np.linspace(0,duration*2*np.pi,num=duration*sampRate)\n    wave = np.sin(cycles*frequency,dtype=\'float32\')\n    wave = wave*1000\n    wave = wave.astype(\'int16\')\n    # wave = np.sin(cycles*frequency,dtype=\'int16\')\n    # t = np.divide(cycles,2*np.pi)\n\n    return wave\n\nvolume = 0.5     # range [0.0, 1.0]\nfs = 44100       # sampling rate, Hz, must be integer\nduration = 4   # in seconds, may be float\nf = 441.0        # sine frequency, Hz, may be float\n# generate samples, note conversion to float32 array\nx0 = 0\n\nout_stream.start_stream()\nwav=0\nwhile 1:\n  x, y = mouse.position()\n  if x!=x0:\n    # samples = (np.sin(2*np.pi*np.arange(fs*duration)*x/fs)).astype(np.float32)\n    samples = gensin(x,duration,fs)\n    x0=x\n    if wav: wav.stop()\n    wav=pygame.sndarray.make_sound(samples)\n    # stream.write(volume * samples)\n    # t, samples = gensin(y, duration, fs)\n    # stream.write(volume * samples)\n    # wav.play()\n    # wav.play(loops=100, maxtime=duration, fade_ms=10)\n    wav.play(loops=100, fade_ms=30)\n    print(x,y)\n\n# data=np.array(range(chunk))\nwhile 1:\n  for i in range(len(data)):#range(chunk):\n    data[i] = math.sin(0.1* i / x)\n    if i > 40900: freq1 = int(freq1 * (len(data) - i) / 100.0) # avoid \'Popping\'\n  stream.write(volume*data)\n\n#stop stream\nstream.stop_stream()\nstream.close()\n\n#close PyAudio\n# p.terminate()\n\nwhile 1:\n  pass\nexit()\n\n\nimport sys\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nimport speech_data\nspeech = speech_data.read_data_sets(""/data/speech/"", one_hot=True)\n\n\n\nn1=400\nn2=100\nn3=20\n\n\nprint(""Creating the model"")\n\n# input: spectogram (None: batch of variable size)\nx = tf.placeholder(""float"", [None, width * height])\n\n# l_rate = tf.placeholder(""float"", [1])# 0.01\nl_rate = tf.Variable(0.003)\n\nW1 = tf.Variable(tf.random_uniform([width * height,400],maxval=0.0001))\nb1 = tf.Variable(tf.random_uniform([400]))\nW2 = tf.Variable(tf.random_uniform([400,100],maxval=0.001))\nb2 = tf.Variable(tf.random_uniform([100]))\nW3 = tf.Variable(tf.random_uniform([100,n3],maxval=0.01))\nb3 = tf.Variable(tf.zeros([n3]))\nh= tf.nn.tanh( tf.matmul(x,W1)+b1) #\nh= tf.nn.dropout(h,keep_prob=.5) # THAT!\nh2= tf.nn.tanh( tf.matmul(h,W2)+b2) #\n# _y = tf.matmul(h,W2) + b2 # 10 Numbers + 10 \'styles\'\n_y = tf.matmul(h2,W3) + b3 # 10 Numbers + 10 \'styles\'\n# print(""_y "",tf.rank(_y))\nif n3==20:\n  y = tf.nn.softmax(tf.slice(_y,[0,0],[-1,10])) #  softmax of all batches(-1) only on the numbers(10)\nelse:\n  y = tf.nn.softmax(_y)\n\ne2= tf.matmul(_y,tf.transpose(W3)) #+b2\ne2=tf.nn.tanh(e2)\ne1= tf.matmul(e2,tf.transpose(W2)) #+b2  _y\ne1=tf.nn.tanh(e1)\n\nx_=x_reconstructed= tf.nn.sigmoid(tf.matmul(e1,tf.transpose(W1)))\n# measure reconstructed signal against input:\ny_ = tf.placeholder(""float"", [None,10])\n\n# Define loss and optimizer\nspeech_entropy = -tf.reduce_sum(y_*tf.log(y))\n\n# encod_entropy = -tf.reduce_sum(x_*tf.log(x))\nencod_entropy = tf.sqrt(tf.reduce_mean(tf.square(x - x_)))\n# encod_entropy = tf.reduce_mean(tf.square(x - x_))\n\ncross_entropy = encod_entropy * speech_entropy\n# cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\nspeech_step = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(speech_entropy)\nencod_step = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(encod_entropy)\ntrain_step = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(cross_entropy)\n\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y,1), tf.argmax(y_,1)), ""float""))\n\ndef flatten(matrix):\n  # print(""flattening %d""\n  # return itertools.chain.from_iterable(matrix)\n  return [item for vector in matrix for item in vector]\n\n\ntest_images=[flatten(matrix) for matrix in speech.test.images]\n\ndef eval(feed):\n  print(""cross_entropy "",cross_entropy.eval(feed))#, end=\' \')\n  print(""encod_entropy "",encod_entropy.eval(feed))#, end=\' \')\n  print(""speech_entropy "",speech_entropy.eval(feed))#, end=\' \')\n  print(""overal accuracy "",accuracy.eval({x: test_images, y_: speech.test.labels}))#, end=\'\\r\'WWWWAAA)\n\n\ndef train_spectrogram_encoder():\n  tf.initialize_all_variables().run()\n  print(""Pretrain"")\n  for i in range(6000-1):\n    batch_xs, batch_ys = speech.train.next_batch(100)\n    # WTF, tensorflow can\'t do 3D tensor operations?\n    # https://github.com/tensorflow/tensorflow/issues/406 =>\n    batch_xs=[flatten(matrix) for matrix in batch_xs]\n    #  you have to reshape to flat/matrix data? why didn\'t they call it matrixflow?\n    feed = {x: batch_xs, y_: batch_ys}\n    speech_step.run(feed) # better for encod_entropy too! (later)\n    if(i%100==0):\n      print(""iteration %d""%i)#, end=\' \')\n      eval(feed)\n    if((i+1)%7000==0):\n      print(""l_rate*=0.1"")\n      sess.run(tf.assign(l_rate,l_rate*0.1))\n\n  print(""Train"")\n  for i in range(100000):\n    batch_xs, batch_ys = speech.train.next_batch(100)\n    feed = {x: batch_xs, y_: batch_ys}\n    if((i+1)%9000==0):sess.run(tf.assign(l_rate,l_rate*0.3))\n    encod_step.run(feed) # alternating!\n    speech_step.run(feed)\n    train_step.run(feed)\n    if(i%100==0):\n      print(""iteration %d""%i)#, end=\' \')\n      eval(feed)\n\nif __name__ == \'__main__\':\n  train_spectrogram_encoder()\n\n'"
layer/net.py,117,"b'from __future__ import print_function\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector  # for 3d PCA/ t-SNE\n\nfrom .tensorboard_util import *\n\nstart = int(time.time())\n\n# clear_tensorboard()\nset_tensorboard_run(auto_increment=True)\nrun_tensorboard(restart=False)\n\n# gpu = True\ngpu = False\ndebug = False  # True # summary.histogram  : \'module\' object has no attribute \'histogram\' WTF\ndebug = True  # histogram_summary ...\nvisualize_cluster = False  # NOT YET: \'ProjectorConfig\' object has no attribute \'embeddings\'\n\nslim = tf.contrib.slim\nweight_divider = 10.\ndefault_learning_rate = 0.001  # mostly overwritten, so ignore it\ndecay_steps = 100000\ndecay_size = 0.1\nsave_step = 10000  # if you don\'t want to save snapshots, set to -1\ncheckpoint_dir = ""checkpoints""\n\nif not os.path.exists(checkpoint_dir):\n\tos.makedirs(checkpoint_dir)\n\n\ndef nop(): return 0\n\n\ndef closest_unitary(A):\n\t"""""" Calculate the unitary matrix U that is closest with respect to the operator norm distance to the general matrix A. """"""\n\timport scipy\n\tV, __, Wh = scipy.linalg.svd(A)\n\treturn np.matrix(V.dot(Wh))\n\n\n_cpu = \'/cpu:0\'\n\n\nclass net():\n\tdef __init__(self, model, data=0, input_width=0, output_width=0, input_shape=0, name=0,\n\t             learning_rate=default_learning_rate):\n\t\tdevice = \'/GPU:0\' if gpu else \'/cpu:0\'\n\t\tdevice = None  # auto\n\t\tprint(""Using device "", device)\n\t\twith tf.device(device):\n\t\t\t# if True:\n\t\t\tself.session = sess = session = tf.Session()\n\t\t\t# self.session=sess=session=tf.Session(config=tf.ConfigProto(log_device_placement=True))\n\t\t\tself.model = model\n\t\t\tself.input_shape = input_shape or [input_width, input_width]\n\t\t\tself.data = data  # assigned to self.x=net.input via train\n\t\t\tif not input_width:\n\t\t\t\tinput_width = self.get_data_shape()\n\t\t\tself.input_width = input_width\n\t\t\tself.last_width = self.input_width\n\t\t\tself.output_width = output_width\n\t\t\tself.num_classes = output_width\n\t\t\t# self.batch_size=batch_size\n\t\t\tself.layers = []\n\t\t\tself.learning_rate = learning_rate\n\t\t\tif not name: name = model.__name__\n\t\t\tself.name = str(name)\n\t\t\tif name and os.path.exists(name + "".model""):\n\t\t\t\treturn self.load_model(name + "".model"")\n\t\t\tself.generate_model(model)\n\n\tdef get_data_shape(self):\n\t\tif self.input_shape:\n\t\t\treturn self.input_shape[0], self.input_shape[1]\n\t\ttry:\n\t\t\treturn self.data.shape[0], self.data.shape[-1]\n\t\texcept:\n\t\t\traise Exception(""Data does not have shape"")\n\n\tdef generate_model(self, model, name=\'\'):\n\t\tif not model: return self\n\t\twith tf.name_scope(\'state\'):\n\t\t\tself.keep_prob = tf.placeholder(tf.float32)  # 1 for testing! else 1 - dropout\n\t\t\tself.train_phase = tf.placeholder(tf.bool, name=\'train_phase\')\n\t\t\twith tf.device(_cpu): self.global_step = tf.Variable(\n\t\t\t\t0)  # dont set, feed or increment global_step, tensorflow will do it automatically\n\t\twith tf.name_scope(\'data\'):\n\t\t\tif len(self.input_shape) == 1:\n\t\t\t\tself.input_width = self.input_shape[0]\n\t\t\telif self.input_shape:\n\t\t\t\tself.x = x = self.input = tf.placeholder(tf.float32, [None, self.input_shape[0], self.input_shape[1]])\n\t\t\t\t# todo [None, self.input_shape]\n\t\t\t\tself.last_layer = x\n\t\t\t\tself.last_shape = x\n\t\t\telif self.input_width:\n\t\t\t\tself.x = x = self.target = tf.placeholder(tf.float32, [None, self.input_width])\n\t\t\t\tself.last_layer = x\n\t\t\telse:\n\t\t\t\traise Exception(""need input_shape or input_width by now"")\n\t\t\tself.y = y = self.target = tf.placeholder(tf.float32, [None, self.output_width])\n\t\twith tf.name_scope(\'model\'):\n\t\t\tmodel(self)\n\t\tif (self.last_width != self.output_width):\n\t\t\tself.classifier()  # 10 classes auto\n\n\tdef dropout(self, keep_rate=0.6):\n\t\tself.add(tf.nn.dropout(self.last_layer, keep_rate))\n\n\tdef fully_connected(self, hidden=1024, depth=1, activation=tf.nn.tanh, dropout=False, parent=-1, norm=None):  # ):\n\t\treturn self.dense()\n\n\t# Fully connected \'pyramid\' layer, allows very high learning_rate >0.1 (but don\'t abuse)\n\tdef denseNet(self, hidden=20, depth=3, act=tf.nn.tanh, dropout=True, norm=None):  #\n\t\tif (hidden > 100): print(""WARNING: denseNet uses quadratic mem for "" + str(hidden))\n\t\tif (depth < 3): print(\n\t\t\t""WARNING: did you mean to use Fully connected layer \'dense\'? Expecting depth>3 vs "" + str(depth))\n\t\tinputs = self.last_layer\n\t\tinputs_width = self.last_width\n\t\twidth = hidden\n\t\twhile depth > 0:\n\t\t\twith tf.name_scope(\'DenNet_{:d}\'.format(width)) as scope:\n\t\t\t\tprint(""dense width "", inputs_width, ""x"", width)\n\t\t\t\tnr = len(self.layers)\n\t\t\t\tweights = tf.Variable(tf.random_uniform([inputs_width, width], minval=-1. / width, maxval=1. / width),\n\t\t\t\t                      name=""weights"")\n\t\t\t\tbias = tf.Variable(tf.random_uniform([width], minval=-1. / width, maxval=1. / width),\n\t\t\t\t                   name=""bias"")  # auto nr + context\n\t\t\t\tdense1 = tf.matmul(inputs, weights, name=\'dense_\' + str(nr)) + bias\n\t\t\t\ttf.summary.histogram(\'dense_\' + str(nr), dense1)\n\t\t\t\ttf.summary.histogram(\'dense_\' + str(nr) + \'/sparsity\', tf.nn.zero_fraction(dense1))\n\t\t\t\ttf.summary.histogram(\'weights_\' + str(nr), weights)\n\t\t\t\ttf.summary.histogram(\'weights_\' + str(nr) + \'/sparsity\', tf.nn.zero_fraction(weights))\n\t\t\t\ttf.summary.histogram(\'bias_\' + str(nr), bias)\n\n\t\t\t\tif act: dense1 = act(dense1)\n\t\t\t\tif norm: dense1 = self.norm(dense1, lsize=1)  # SHAPE!\n\t\t\t\tif dropout: dense1 = tf.nn.dropout(dense1, self.keep_prob)\n\t\t\t\tself.add(dense1)\n\t\t\t\tself.last_width = width\n\t\t\t\tinputs = tf.concat(1, [inputs, dense1])\n\t\t\t\tinputs_width += width\n\t\t\t\tdepth = depth - 1\n\t\tself.last_width = width\n\n\tdef add(self, layer):\n\t\tself.layers.append(layer)\n\t\tself.last_layer = layer\n\t\tself.last_shape = layer.get_shape()\n\n\tdef reshape(self, shape):\n\t\tself.last_layer = tf.reshape(self.last_layer, shape)\n\t\tself.last_shape = shape\n\t\tself.last_width = shape[-1]\n\n\tdef batchnorm(self):\n\t\tfrom tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n\t\twith tf.name_scope(\'batchnorm\') as scope:\n\t\t\tinput = self.last_layer\n\t\t\t# mean, var = tf.nn.moments(input, axes=[0, 1, 2])\n\t\t\t# self.batch_norm = tf.nn.batch_normalization(input, mean, var, offset=1, scale=1, variance_epsilon=1e-6)\n\t\t\t# self.last_layer=self.batch_norm\n\t\t\ttrain_op = batch_norm(input, is_training=True, center=False, updates_collections=None, scope=scope)\n\t\t\ttest_op = batch_norm(input, is_training=False, updates_collections=None, center=False, scope=scope,\n\t\t\t                     reuse=True)\n\t\t\tself.add(tf.cond(self.train_phase, lambda: train_op, lambda: test_op))\n\n\tdef addLayer(self, nChannels, nOutChannels, do_dropout):\n\t\tident = self.last_layer\n\t\tself.batchnorm()\n\t\t# self.add(tf.nn.relu(ident)) # nChannels ?\n\t\tself.conv([3, 3, nChannels, nOutChannels], pool=False, dropout=do_dropout, norm=tf.nn.relu)  # None\n\t\tconcat = tf.concat(3, [ident, self.last_layer])\n\t\tprint(""concat "", concat.get_shape())\n\t\tself.add(concat)\n\n\tdef addTransition(self, nChannels, nOutChannels, do_dropout):\n\t\tself.batchnorm()\n\t\tself.add(tf.nn.relu(self.last_layer))\n\t\tself.conv([1, 1, nChannels, nOutChannels], pool=True, dropout=do_dropout, norm=None)  # pool (2, 2)\n\n\t# self.add(tf.nn.SpatialConvolution(nChannels, nOutChannels, 1, 1, 1, 1, 0, 0))\n\n\t# Densely Connected Convolutional Networks https://arxiv.org/abs/1608.06993\n\tdef buildDenseConv(self, N_blocks=3, magic_factor=1):\n\t\tdepth = 3 * N_blocks + 4\n\t\tif (depth - 4) % 3:  raise Exception(""Depth must be 3N + 4! (4,7,10,...) "")  # # layers in each denseblock\n\t\tN = (depth - 4) / 3\n\t\tdo_dropout = True  # None  nil to disable dropout, non - zero number to enable dropout and set drop rate\n\t\t# dropRate = self.keep_prob # nil to disable dropout, non - zero number to enable dropout and set drop rate\n\t\t# # channels before entering the first denseblock ??\n\t\t# set it to be comparable with growth rate ??\n\t\tnChannels = 64\n\t\t# nChannels = 16\n\t\tgrowthRate = 12\n\t\tself.conv([3, 3, 1, nChannels])\n\t\t# self.add(tf.nn.SpatialConvolution(3, nChannels, 3, 3, 1, 1, 1, 1))\n\n\t\tfor i in range(N):\n\t\t\tself.addLayer(nChannels, growthRate, do_dropout)\n\t\t\tnChannels = nChannels + growthRate\n\t\tself.addTransition(nChannels, nChannels, do_dropout)\n\n\t\tfor i in range(N):\n\t\t\tself.addLayer(nChannels, growthRate, do_dropout)\n\t\t\tnChannels = nChannels + growthRate\n\t\tself.addTransition(nChannels, nChannels, do_dropout)\n\n\t\tfor i in range(N):\n\t\t\tself.addLayer(nChannels, growthRate, do_dropout)\n\t\t\tnChannels = nChannels + growthRate\n\n\t\tself.batchnorm()\n\t\tself.add(tf.nn.relu(self.last_layer))\n\t\t# self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 8, 8, 1], strides=[1, 2, 2, 1], padding=\'SAME\'))\n\t\t# self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 8, 8, 1], strides=[1, 1, 1, 1], padding=\'SAME\'))\n\t\t# self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 1, 1, 1], padding=\'SAME\'))\n\t\tself.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding=\'SAME\'))\n\t\t# self.add(tf.nn.SpatialAveragePooling(8, 8)).add(nn.Reshape(nChannels))\n\t\tif magic_factor == 16:\n\t\t\tself.reshape([-1, nChannels * 16])  # ready for classification\n\t\telse:\n\t\t\tself.reshape([-1, nChannels * 4])  # ready for classification\n\n\t# Fully connected layer\n\tdef dense(self, hidden=1024, depth=1, activation=tf.nn.tanh, dropout=False, parent=-1, norm=None):  #\n\t\tif parent == -1: parent = self.last_layer\n\t\tshape = self.last_layer.get_shape()\n\t\tif shape and len(shape) > 2:\n\t\t\tif len(shape) == 3:\n\t\t\t\tself.last_width = int(shape[1] * shape[2])\n\t\t\telse:\n\t\t\t\tself.last_width = int(shape[1] * shape[2] * shape[3])\n\t\t\tprint(""reshapeing "", shape, ""to"", self.last_width)\n\t\t\tparent = tf.reshape(parent, [-1, self.last_width])\n\n\t\twidth = hidden\n\t\twhile depth > 0:\n\t\t\twith tf.name_scope(\'Dense_{:d}\'.format(hidden)) as scope:\n\t\t\t\tprint(""Dense "", self.last_width, width)\n\t\t\t\tnr = len(self.layers)\n\t\t\t\tif self.last_width == width:\n\t\t\t\t\tU = closest_unitary(\n\t\t\t\t\t\tnp.random.rand(self.last_width, width) / (self.last_width + width)) / weight_divider\n\t\t\t\t\tweights = tf.Variable(U, name=""weights_dense_"" + str(nr), dtype=tf.float32)\n\t\t\t\telse:\n\t\t\t\t\tweights = tf.Variable(\n\t\t\t\t\t\ttf.random_uniform([self.last_width, width], minval=-1. / width, maxval=1. / width),\n\t\t\t\t\t\tname=""weights_dense"")\n\t\t\t\tbias = tf.Variable(tf.random_uniform([width], minval=-1. / width, maxval=1. / width), name=""bias_dense"")\n\t\t\t\tdense1 = tf.matmul(parent, weights, name=\'dense_\' + str(nr)) + bias\n\t\t\t\ttf.summary.histogram(\'dense_\' + str(nr), dense1)\n\t\t\t\ttf.summary.histogram(\'weights_\' + str(nr), weights)\n\t\t\t\ttf.summary.histogram(\'bias_\' + str(nr), bias)\n\t\t\t\ttf.summary.histogram(\'dense_\' + str(nr) + \'/sparsity\', tf.nn.zero_fraction(dense1))\n\t\t\t\ttf.summary.histogram(\'weights_\' + str(nr) + \'/sparsity\', tf.nn.zero_fraction(weights))\n\t\t\t\tif activation: dense1 = activation(dense1)\n\t\t\t\tif norm: dense1 = self.norm(dense1, lsize=1)\n\t\t\t\tif dropout: dense1 = tf.nn.dropout(dense1, self.keep_prob)\n\t\t\t\tself.layers.append(dense1)\n\t\t\t\tself.last_layer = parent = dense1\n\t\t\t\tself.last_width = width\n\t\t\t\tdepth = depth - 1\n\t\t\t\tself.last_shape = [-1, width]  # dense\n\n\tdef conv2(self, shape, act=tf.nn.relu, pool=True, dropout=False, norm=True, name=None):\n\t\twith tf.name_scope(\'conv\'):\n\t\t\tprint(""input  shape "", self.last_shape)\n\t\t\tprint(""conv   shape "", shape)\n\t\t\t# padding=\'VALID\'\n\t\t\tconv = slim.conv2d(self.last_layer, shape[-1], [shape[1], shape[2]], 3, padding=\'SAME\', scope=name)\n\t\t\t# if pool: conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\t\t\t# if(pool): conv = slim.max_pool2d(conv, [2, 2], 1, scope=\'pool1\')\n\t\t\t# if(pool): conv = slim.max_pool2d(conv, [3, 3], 2, scope=\'pool1\')\n\t\t\tself.add(conv)\n\n\t# Convolution Layer\n\tdef conv(self, shape, act=tf.nn.relu, pool=True, dropout=False, norm=True,\n\t         name=None):  # True why dropout bad in tensorflow??\n\t\twith tf.name_scope(\'conv\'):\n\t\t\tprint(""input  shape "", self.last_shape)\n\t\t\tprint(""conv   shape "", shape)\n\t\t\twidth = shape[-1]\n\t\t\tfilters = tf.Variable(tf.random_normal(shape), name=""filters"")\n\t\t\t# filters = tf.Variable(tf.random_uniform(shape, minval=-1. / width, maxval=1. / width), name=""filters"")\n\t\t\t_bias = tf.Variable(tf.random_normal([shape[-1]]), name=""bias"")\n\n\t\t\t# # conv1 = conv2d(\'conv\', _X, _weights, _bias)\n\t\t\tconv1 = tf.nn.bias_add(tf.nn.conv2d(self.last_layer, filter=filters, strides=[1, 1, 1, 1], padding=\'SAME\'),\n\t\t\t                       _bias)\n\t\t\tif debug: tf.summary.histogram(\'conv_\' + str(len(self.layers)), conv1)\n\t\t\tif act: conv1 = act(conv1)\n\t\t\tif pool: conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\t\t\tif norm: conv1 = tf.nn.lrn(conv1, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n\t\t\tif debug: tf.summary.histogram(\'norm_\' + str(len(self.layers)), conv1)\n\t\t\tif dropout: conv1 = tf.nn.dropout(conv1, self.keep_prob)\n\t\t\tprint(""output shape "", conv1.get_shape())\n\t\t\tself.add(conv1)\n\n\tdef rnn(self):\n\t\t# data = tf.placeholder(tf.float32, [None, width, height])  # Number of examples, number of input, dimension of each input\n\t\t# target = tf.placeholder(tf.float32, [None, classes])\n\t\t# num_hidden = 24\n\t\tnum_hidden = 42\n\t\tcell = tf.nn.rnn_cell.LSTMCell(num_hidden)\n\t\tval, _ = tf.nn.dynamic_rnn(cell, self.last_layer, dtype=tf.float32)\n\t\tval = tf.nn.dropout(val, 0.8)\n\t\tval = tf.transpose(val, [1, 0, 2])\n\t\tself.last = tf.gather(val, int(val.get_shape()[0]) - 1)\n\n\t# weight = tf.Variable(tf.truncated_normal([num_hidden, int(target.get_shape()[1])]))\n\t# bias = tf.Variable(tf.constant(0.1, shape=[target.get_shape()[1]]))\n\t# mistakes = tf.not_equal(tf.argmax(target, 1), tf.argmax(prediction, 1))\n\t# error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n\n\tdef classifier(self, classes=0):  # Define loss and optimizer\n\t\tif not classes: classes = self.num_classes\n\t\twith tf.name_scope(\'prediction\'):  # prediction\n\t\t\tif self.last_width != classes:\n\t\t\t\t# print(""Automatically adding dense prediction"")\n\t\t\t\tself.dense(hidden=classes, activation=False, dropout=False)\n\t\t\t# cross_entropy = -tf.reduce_sum(y_*y)\n\t\twith tf.name_scope(\'classifier\'):\n\t\t\ty_ = self.target\n\t\t\tmanual = False  # True\n\t\t\tif classes > 100:\n\t\t\t\tprint(""using sampled_softmax_loss"")\n\t\t\t\ty = prediction = self.last_layer\n\t\t\t\tself.cost = tf.reduce_mean(tf.nn.sampled_softmax_loss(y, y_))  # for big vocab\n\t\t\telif manual:\n\t\t\t\t# prediction = y =self.last_layer=tf.nn.softmax(self.last_layer)\n\t\t\t\t# self.cost = cross_entropy = -tf.reduce_sum(y_ * tf.log(y+ 1e-10)) # against NaN!\n\t\t\t\tprediction = y = tf.nn.log_softmax(self.last_layer)\n\t\t\t\tself.cost = cross_entropy = -tf.reduce_sum(y_ * y)\n\t\t\telse:\n\t\t\t\ty = prediction = self.last_layer\n\t\t\t\tself.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))  # prediction, target\n\n\t\t\t# if not gpu:\n\t\t\twith tf.device(_cpu):\n\t\t\t\ttf.summary.scalar(\'cost\', self.cost)\n\t\t\t# self.cost = tf.Print(self.cost , [self.cost ], ""debug cost : "")\n\t\t\t# learning_scheme=self.learning_rate\n\t\t\tlearning_scheme = tf.train.exponential_decay(self.learning_rate, self.global_step, decay_steps, decay_size,\n\t\t\t                                             staircase=True)\n\t\t\twith tf.device(_cpu):\n\t\t\t\ttf.summary.scalar(\'learning_rate\', learning_scheme)\n\t\t\tself.optimizer = tf.train.AdamOptimizer(learning_scheme).minimize(self.cost)\n\t\t\t# self.optimizer = NeuralOptimizer(data=None, learning_rate=0.01, shared_loss=self.cost).minimize(self.cost) No good\n\n\t\t\t# Evaluate model\n\t\t\tcorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(self.target, 1))\n\t\t\tself.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\t\t\tif not gpu: tf.summary.scalar(\'accuracy\', self.accuracy)\n\t\t# Launch the graph\n\n\tdef next_batch(self, batch_size, session, test=False):\n\t\ttry:\n\t\t\tif test:\n\t\t\t\ttest_images = self.data.test.images[:batch_size]\n\t\t\t\ttest_labels = self.data.test.labels[:batch_size]\n\t\t\t\treturn test_images, test_labels\n\t\t\treturn self.data.train.next_batch(batch_size)\n\t\texcept:\n\t\t\ttry:\n\t\t\t\treturn next(self.data)\n\t\t\texcept:\n\t\t\t\treturn next(self.data.train)\n\n\tdef train(self, data=0, steps=-1, dropout=None, display_step=10, test_step=200, batch_size=10,\n\t          do_resume=False):  # epochs=-1,\n\t\tif data: self.data = data\n\t\tsteps = 9999999 if steps == -1 else steps\n\t\tsession = self.session\n\t\t# with tf.device(_cpu):\n\n\t\t# import tensorflow.contrib.layers as layers\n\t\t# t = tf.verify_tensor_all_finite(t, msg)\n\t\ttf.add_check_numerics_ops()\n\t\ttry:\n\t\t\tself.summaries = tf.summary.merge_all()\n\t\texcept:\n\t\t\tself.summaries = tf.merge_all_summaries()\n\t\ttry:\n\t\t\tself.summary_writer = tf.summary.FileWriter(current_logdir(), session.graph)  #\n\t\texcept:\n\t\t\tself.summary_writer = tf.train.SummaryWriter(current_logdir(), session.graph)  #\n\t\tif not dropout: dropout = 1.  # keep all\n\t\tx = self.x\n\t\ty = self.y\n\t\tkeep_prob = self.keep_prob\n\t\ttry:\n\t\t\tsaver = tf.train.Saver(tf.global_variables())\n\t\texcept:\n\t\t\tsaver = tf.train.Saver(tf.all_variables())\n\t\tsnapshot = self.name + str(get_last_tensorboard_run_nr())\n\t\tcheckpoint = tf.train.latest_checkpoint(checkpoint_dir)\n\t\tif do_resume and checkpoint:\n\t\t\tprint(""LOADING "" + checkpoint + "" !!!"")\n\t\t\tsaver.restore(session, checkpoint)\n\t\ttry:\n\t\t\tsession.run([tf.global_variables_initializer()])\n\t\texcept:\n\t\t\tsession.run([tf.initialize_all_variables()])\n\t\tstep = 0  # show first\n\t\twhile step < steps:\n\t\t\tbatch_xs, batch_ys = self.next_batch(batch_size, session)\n\t\t\t# print(""step %d \\r"" % step)# end=\' \')\n\n\t\t\t# tf.train.shuffle_batch_join(example_list, batch_size, capacity=min_queue_size + batch_size * 16, min_queue_size)\n\t\t\t# Fit training using batch data\n\t\t\tfeed_dict = {x: batch_xs, y: batch_ys, keep_prob: dropout, self.train_phase: True}\n\t\t\tloss, _ = session.run([self.cost, self.optimizer], feed_dict=feed_dict)\n\t\t\tif step % display_step == 0:\n\t\t\t\tseconds = int(time.time()) - start\n\t\t\t\t# Calculate batch accuracy, loss\n\t\t\t\tfeed = {x: batch_xs, y: batch_ys, keep_prob: 1., self.train_phase: False}\n\t\t\t\tacc, summary = session.run([self.accuracy, self.summaries], feed_dict=feed)\n\t\t\t\t# self.summary_writer.add_summary(summary, step) # only test summaries for smoother curve\n\t\t\t\tprint(""\\rStep {:d} Loss= {:.6f} Accuracy= {:.3f} Time= {:d}s"".format(step, loss, acc, seconds), end=\' \')\n\t\t\t\tif str(loss) == ""nan"": return print(""\\nLoss gradiant explosion, exiting!!!"")  # restore!\n\t\t\tif step % test_step == 0: self.test(step)\n\t\t\tif step % save_step == 0 and step > 0:\n\t\t\t\tprint(""SAVING snapshot %s"" % snapshot)\n\t\t\t\tsaver.save(session, checkpoint_dir + snapshot + "".ckpt"", self.global_step)\n\n\t\t\tstep += 1\n\t\tprint(""\\nOptimization Finished!"")\n\t\tself.test(step, number=10000)  # final test\n\n\tdef test(self, step, number=400):  # 256 self.batch_size\n\t\tsession = sess = self.session\n\t\tconfig = projector.ProjectorConfig()\n\t\tif visualize_cluster:\n\t\t\tembedding = config.embeddings.add()  # You can add multiple embeddings. Here just one.\n\t\t\tembedding.tensor_name = self.last_layer.name  # last_dense\n\t\t\t# embedding.tensor_path\n\t\t\t# embedding.tensor_shape\n\t\t\tembedding.sprite.image_path = PATH_TO_SPRITE_IMAGE\n\t\t\t# help(embedding.sprite)\n\t\t\tembedding.sprite.single_image_dim.extend([width, hight])  # if mnist   thumbnail\n\t\t\t# embedding.single_image_dim.extend([28, 28]) # if mnist   thumbnail\n\t\t\t# Link this tensor to its metadata file (e.g. labels).\n\t\t\tembedding.metadata_path = os.path.join(LOG_DIR, \'metadata.tsv\')\n\t\t\t# Saves a configuration file that TensorBoard will read during startup.\n\t\t\tprojector.visualize_embeddings(self.summary_writer, config)\n\n\t\trun_metadata = tf.RunMetadata()\n\t\trun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t# Calculate accuracy for 256 mnist test images\n\n\t\ttest_images, test_labels = self.next_batch(number, session, test=True)\n\n\t\tfeed_dict = {self.x: test_images, self.y: test_labels, self.keep_prob: 1., self.train_phase: False}\n\t\t# accuracy,summary= self.session.run([self.accuracy, self.summaries], feed_dict=feed_dict)\n\t\taccuracy, summary = session.run([self.accuracy, self.summaries], feed_dict, run_options, run_metadata)\n\t\tprint(\'\\t\' * 3 + ""Test Accuracy: "", accuracy)\n\t\tself.summary_writer.add_run_metadata(run_metadata, \'step #%03d\' % step)\n\t\tself.summary_writer.add_summary(summary, global_step=step)\n\n\t# def inputs(self,data):\n\t# \tself.inputs, self.labels = load_data()#...)\n'"
