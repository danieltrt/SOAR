file_path,api_count,code
setup.py,0,"b'""""""A setuptools based setup module.\n\nSee:\nhttps://packaging.python.org/guides/distributing-packages-using-setuptools/\nhttps://github.com/pypa/sampleproject\n""""""\n\nfrom setuptools import setup, find_packages\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'trpo\',\n    version=\'1.0.0\',\n    description=\'Audio representation learning.\',\n    packages=find_packages(exclude=[\'contrib\', \'docs\', \'tests\']),\n    python_requires=\'>=3.6\', install_requires=[\'tensorflow\', \'numpy\', \'pybullet\', \'gym\', \'scipy\']\n)\n'"
notebooks/plotting.py,0,"b'""""""\nShort Plotting Routine to Plot Pandas Dataframes by Column Label\n\n1. Takes list of dateframes to compare multiple trials\n2. Takes list of y-variables to combine on 1 plot\n3. Legend location and y-axis limits can be customized\n\nWritten by Patrick Coady (pat-coady.github.io)\n""""""\nimport matplotlib.pyplot as plt\n\n\ndef df_plot(dfs, x, ys, ylim=None, xlim=None, legend_loc=\'best\'):\n    """""" Plot y vs. x curves from pandas dataframe(s)\n\n    Args:\n        dfs: list of pandas dataframes\n        x: str column label for x variable\n        ys: list of str column labels for y variable(s)\n        ylim: tuple to override automatic y-axis limits\n        xlim: tuple to override automatic x-axis limits\n        legend_loc: str to override automatic legend placement:\n            \'upper left\', \'lower left\', \'lower right\' , \'right\' ,\n            \'center left\', \'center right\', \'lower center\',\n            \'upper center\', and \'center\'\n    """"""\n    if ylim is not None:\n        plt.ylim(ylim)\n    if xlim is not None:\n        plt.xlim(xlim)\n    for df, name in dfs:\n        if \'_\' in name:\n            name = name.split(\'_\')[1]\n        for y in ys:\n            plt.plot(df[x], df[y], linewidth=1,\n                     label=name + \' \' + y.replace(\'_\', \'\'))\n    plt.xlabel(x.replace(\'_\', \'\'))\n    plt.legend(loc=legend_loc)\n    plt.show()\n'"
trpo/archive.py,41,"b'""""""\nArchive of Procedures Not Used in Final Implementation\n\nWritten by Patrick Coady (pat-coady.github.io)\n""""""\nimport numpy as np\nimport tensorflow as tf\n\n\nclass ConstantScaler(object):\n    """""" Dumb scaler, scale and offset set at initialization """"""\n    def __init__(self, obs_dim, scale=1.0, offset=0.0):\n        self.scale = np.ones(obs_dim) * scale\n        self.offset = np.zeros(obs_dim) + offset\n\n    def update(self, x):\n        pass  # no updates for constant scaler\n\n    def get(self):\n        """""" returns 2-tuple: (scale, offset) """"""\n        return self.scale, self.offset\n\n\nclass LinearValueFunction(object):\n    """"""Simple linear regression value function, uses linear and squared features.\n\n    Mostly copied from: https://github.com/joschu/modular_rl\n    """"""\n    def __init__(self):\n        self.coef = None\n\n    def fit(self, x, y, logger):\n        """""" Fit model - (i.e. solve normal equations)\n\n        Args:\n            x: features\n            y: target\n            logger: logger to save training loss and % explained variance\n        """"""\n        y_hat = self.predict(x)\n        old_exp_var = 1-np.var(y-y_hat)/np.var(y)\n        xp = self.preproc(x)\n        a = xp.T.dot(xp)\n        nfeats = xp.shape[1]\n        a[np.arange(nfeats), np.arange(nfeats)] += 1e-3  # a little ridge regression\n        b = xp.T.dot(y)\n        self.coef = np.linalg.solve(a, b)\n        y_hat = self.predict(x)\n        loss = np.mean(np.square(y_hat-y))\n        exp_var = 1-np.var(y-y_hat)/np.var(y)\n\n        logger.log({\'LinValFuncLoss\': loss,\n                    \'LinExplainedVarNew\': exp_var,\n                    \'LinExplainedVarOld\': old_exp_var})\n\n    def predict(self, x):\n        """""" Predict method, predict zeros if model untrained """"""\n        if self.coef is None:\n            return np.zeros(x.shape[0])\n        else:\n            return self.preproc(x).dot(self.coef)\n\n    @staticmethod\n    def preproc(X):\n        """""" Adds squared features and bias term """"""\n\n        return np.concatenate([np.ones([X.shape[0], 1]), X, np.square(X)/2.0], axis=1)\n\n\ndef add_advantage(trajectories):\n    """""" Adds estimated advantage to all time steps of all trajectories\n\n    Args:\n        trajectories: as returned by run_policy(), must include \'values\'\n            key from add_value().\n\n    Returns:\n        None (mutates trajectories dictionary to add \'advantages\')\n    """"""\n    for trajectory in trajectories:\n        trajectory[\'advantages\'] = trajectory[\'disc_sum_rew\'] - trajectory[\'values\']\n\n\nclass PolicyWithVariance(object):\n    """""" Neural Net output means AND variance (had poor performance) """"""\n    def __init__(self, obs_dim, act_dim, kl_targ=0.003):\n        self.beta = 1.0\n        self.kl_targ = kl_targ\n        self._build_graph(obs_dim, act_dim)\n        self._init_session()\n\n    def _build_graph(self, obs_dim, act_dim):\n        """""" Build TensorFlow graph""""""\n        self.g = tf.Graph()\n        with self.g.as_default():\n            self._placeholders(obs_dim, act_dim)\n            self._policy_nn(obs_dim, act_dim)\n            self._logprob(act_dim)\n            self._kl_entropy(act_dim)\n            self._sample(act_dim)\n            self._loss_train_op()\n            self.init = tf.global_variables_initializer()\n\n    def _placeholders(self, obs_dim, act_dim):\n        """""" Input placeholders""""""\n        self.obs_ph = tf.placeholder(tf.float32, (None, obs_dim), \'obs\')\n        self.act_ph = tf.placeholder(tf.float32, (None, act_dim), \'act\')\n        self.advantages_ph = tf.placeholder(tf.float32, (None,), \'advantages\')\n        self.beta_ph = tf.placeholder(tf.float32, (), \'beta\')\n        self.eta_ph = tf.placeholder(tf.float32, (), \'eta\')\n        self.old_log_vars_ph = tf.placeholder(tf.float32, (None, act_dim,), \'old_log_vars\')\n        self.old_means_ph = tf.placeholder(tf.float32, (None, act_dim), \'old_means\')\n\n    def _policy_nn(self, obs_dim, act_dim):\n        """""" Neural net for policy approximation function """"""\n        out = tf.layers.dense(self.obs_ph, 200, tf.tanh,\n                              kernel_initializer=tf.random_normal_initializer(\n                                  stddev=np.sqrt(1 / obs_dim)),\n                              name=""h1"")\n        out = tf.layers.dense(out, 100, tf.tanh,\n                              kernel_initializer=tf.random_normal_initializer(\n                                  stddev=np.sqrt(1 / 200)),\n                              name=""h2"")\n        out = tf.layers.dense(out, 50, tf.tanh,\n                              kernel_initializer=tf.random_normal_initializer(\n                                  stddev=np.sqrt(1 / 100)),\n                              name=""h3"")\n        self.means = tf.layers.dense(out, act_dim,\n                                     kernel_initializer=tf.random_normal_initializer(\n                                         stddev=np.sqrt(1 / 50)),\n                                     name=""means"")\n        self.log_vars = tf.layers.dense(out, act_dim,\n                                        kernel_initializer=tf.random_normal_initializer(\n                                            stddev=np.sqrt(1 / 50)),\n                                        name=""log_vars"")\n\n    def _logprob(self, act_dim):\n        """""" Log probabilities of batch of states, actions""""""\n        logp = -0.5 * (np.log(np.sqrt(2.0 * np.pi)) * act_dim)\n        # logp += -0.5 * tf.reduce_sum(self.log_vars, axis=1)\n        logp += -0.5 * tf.reduce_sum(self.log_vars)\n        logp += -0.5 * tf.reduce_sum(tf.square(self.act_ph - self.means) /\n                                     tf.exp(self.log_vars), axis=1)\n        self.logp = logp\n\n        logp_old = -0.5 * (np.log(np.sqrt(2.0 * np.pi)) * act_dim)\n        logp_old += -0.5 * tf.reduce_sum(self.old_log_vars_ph, axis=1)\n        logp_old += -0.5 * tf.reduce_sum(tf.square(self.act_ph - self.old_means_ph) /\n                                         tf.exp(self.old_log_vars_ph), axis=1)\n        self.logp_old = logp_old\n\n    def _kl_entropy(self, act_dim):\n        """"""\n        Add KL divergence between old and new distributions\n        Add entropy of present policy given states and actions\n        """"""\n        log_det_cov_old = tf.reduce_sum(self.old_log_vars_ph, axis=1)\n        log_det_cov_new = tf.reduce_sum(self.log_vars, axis=1)\n        tr_old_new = tf.reduce_sum(tf.exp(self.old_log_vars_ph - self.log_vars), axis=1)\n\n        self.kl = 0.5 * tf.reduce_mean(log_det_cov_new - log_det_cov_old + tr_old_new +\n                                       tf.reduce_sum(tf.square(self.means - self.old_means_ph) /\n                                                     tf.exp(self.log_vars), axis=1) - act_dim)\n\n        self.entropy = 0.5 * (act_dim * (np.log(2 * np.pi) + 1) +\n                              tf.reduce_mean(tf.reduce_sum(self.log_vars, axis=1)))\n\n    def _sample(self, act_dim):\n        """""" Sample from distribution, given observation""""""\n        self.sampled_act = (self.means +\n                            tf.exp(self.log_vars / 2.0) * tf.random_normal(shape=(act_dim,)))\n\n    def _loss_train_op(self):\n        # TODO: use reduce_mean or reduce_sum?\n        loss1 = -tf.reduce_mean(self.advantages_ph *\n                                tf.exp(self.logp - self.logp_old))\n        loss2 = tf.reduce_mean(self.beta_ph * self.kl)\n        loss3 = self.eta_ph * tf.square(tf.maximum(0.0, self.kl - 2.0 * self.kl_targ))\n        self.loss = loss1 + loss2 + loss3\n        # optimizer = tf.train.AdamOptimizer(0.00003)\n        optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9, use_nesterov=True)\n        self.train_op = optimizer.minimize(self.loss)\n\n    def _init_session(self):\n        """"""Launch TensorFlow session and initialize variables""""""\n        self.sess = tf.Session(graph=self.g)\n        self.sess.run(self.init)\n\n    def sample(self, obs):\n        """"""Draw sample from policy distribution""""""\n        feed_dict = {self.obs_ph: obs}\n\n        return self.sess.run(self.sampled_act, feed_dict=feed_dict)\n\n    def update(self, observes, actions, advantages, logger, epochs=20):\n        feed_dict = {self.obs_ph: observes,\n                     self.act_ph: actions,\n                     self.advantages_ph: advantages,\n                     self.beta_ph: self.beta,\n                     self.eta_ph: 100}\n        old_means_np, old_log_vars_np = self.sess.run([self.means, self.log_vars],\n                                                      feed_dict)\n        feed_dict[self.old_log_vars_ph] = old_log_vars_np\n        feed_dict[self.old_means_ph] = old_means_np\n        for e in range(epochs):\n            self.sess.run(self.train_op, feed_dict)\n            loss, kl, entropy = self.sess.run([self.loss, self.kl, self.entropy], feed_dict)\n            if kl > self.kl_targ * 4:\n                break\n        if kl > self.kl_targ * 2:\n            self.beta *= 1.5\n        elif kl < self.kl_targ / 2:\n            self.beta /= 1.5\n\n        logger.log({\'PolicyLoss\': loss,\n                    \'PolicyEntropy\': entropy,\n                    \'KL\': kl,\n                    \'Beta\': self.beta})\n\n    def close_sess(self):\n        self.sess.close()'"
trpo/plotting.py,0,"b'""""""\nShort Plotting Routine to Plot Pandas Dataframes by Column Label\n\n1. Takes list of dateframes to compare multiple trials\n2. Takes list of y-variables to combine on 1 plot\n3. Legend location and y-axis limits can be customized\n\nWritten by Patrick Coady (pat-coady.github.io)\n""""""\nimport matplotlib.pyplot as plt\n\n\ndef df_plot(dfs, x, ys, ylim=None, legend_loc=\'best\'):\n    """""" Plot y vs. x curves from pandas dataframe(s)\n\n    Args:\n        dfs: list of pandas dataframes\n        x: str column label for x variable\n        ys: list of str column labels for y variable(s)\n        ylim: tuple to override automatic y-axis limits\n        legend_loc: str to override automatic legend placement:\n            \'upper left\', \'lower left\', \'lower right\' , \'right\' ,\n            \'center left\', \'center right\', \'lower center\',\n            \'upper center\', and \'center\'\n    """"""\n    if ylim is not None:\n        plt.ylim(ylim)\n    for df, name in dfs:\n        if \'_\' in name:\n            name = name.split(\'_\')[1]\n        for y in ys:\n            plt.plot(df[x], df[y], linewidth=1,\n                     label=name + \' \' + y.replace(\'_\', \'\'))\n    plt.xlabel(x.replace(\'_\', \'\'))\n    plt.legend(loc=legend_loc)\n    plt.show()\n'"
trpo/policy.py,0,"b'""""""\nNN Policy with KL Divergence Constraint\n\nWritten by Patrick Coady (pat-coady.github.io)\n""""""\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Layer\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\n\n\nclass Policy(object):\n    def __init__(self, obs_dim, act_dim, kl_targ, hid1_mult, init_logvar):\n        """"""\n        Args:\n            obs_dim: num observation dimensions (int)\n            act_dim: num action dimensions (int)\n            kl_targ: target KL divergence between pi_old and pi_new\n            hid1_mult: size of first hidden layer, multiplier of obs_dim\n            init_logvar: natural log of initial policy variance\n        """"""\n        self.beta = 1.0  # dynamically adjusted D_KL loss multiplier\n        eta = 50  # multiplier for D_KL-kl_targ hinge-squared loss\n        self.kl_targ = kl_targ\n        self.epochs = 20\n        self.lr_multiplier = 1.0  # dynamically adjust lr when D_KL out of control\n        self.trpo = TRPO(obs_dim, act_dim, hid1_mult, kl_targ, init_logvar, eta)\n        self.policy = self.trpo.get_layer(\'policy_nn\')\n        self.lr = self.policy.get_lr()  # lr calculated based on size of PolicyNN\n        self.trpo.compile(optimizer=Adam(self.lr * self.lr_multiplier))\n        self.logprob_calc = LogProb()\n\n    def sample(self, obs):\n        """"""Draw sample from policy.""""""\n        act_means, act_logvars = self.policy(obs)\n        act_stddevs = np.exp(act_logvars / 2)\n\n        return np.random.normal(act_means, act_stddevs).astype(np.float32)\n\n    def update(self, observes, actions, advantages, logger):\n        """""" Update policy based on observations, actions and advantages\n\n        Args:\n            observes: observations, shape = (N, obs_dim)\n            actions: actions, shape = (N, act_dim)\n            advantages: advantages, shape = (N,)\n            logger: Logger object, see utils.py\n        """"""\n        K.set_value(self.trpo.optimizer.lr, self.lr * self.lr_multiplier)\n        K.set_value(self.trpo.beta, self.beta)\n        old_means, old_logvars = self.policy(observes)\n        old_means = old_means.numpy()\n        old_logvars = old_logvars.numpy()\n        old_logp = self.logprob_calc([actions, old_means, old_logvars])\n        old_logp = old_logp.numpy()\n        loss, kl, entropy = 0, 0, 0\n        for e in range(self.epochs):\n            loss = self.trpo.train_on_batch([observes, actions, advantages,\n                                             old_means, old_logvars, old_logp])\n            kl, entropy = self.trpo.predict_on_batch([observes, actions, advantages,\n                                                      old_means, old_logvars, old_logp])\n            kl, entropy = np.mean(kl), np.mean(entropy)\n            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n                break\n        # TODO: too many ""magic numbers"" in next 8 lines of code, need to clean up\n        if kl > self.kl_targ * 2:  # servo beta to reach D_KL target\n            self.beta = np.minimum(35, 1.5 * self.beta)  # max clip beta\n            if self.beta > 30 and self.lr_multiplier > 0.1:\n                self.lr_multiplier /= 1.5\n        elif kl < self.kl_targ / 2:\n            self.beta = np.maximum(1 / 35, self.beta / 1.5)  # min clip beta\n            if self.beta < (1 / 30) and self.lr_multiplier < 10:\n                self.lr_multiplier *= 1.5\n\n        logger.log({\'PolicyLoss\': loss,\n                    \'PolicyEntropy\': entropy,\n                    \'KL\': kl,\n                    \'Beta\': self.beta,\n                    \'_lr_multiplier\': self.lr_multiplier})\n\n\nclass PolicyNN(Layer):\n    """""" Neural net for policy approximation function.\n\n    Policy parameterized by Gaussian means and variances. NN outputs mean\n     action based on observation. Trainable variables hold log-variances\n     for each action dimension (i.e. variances not determined by NN).\n    """"""\n    def __init__(self, obs_dim, act_dim, hid1_mult, init_logvar, **kwargs):\n        super(PolicyNN, self).__init__(**kwargs)\n        self.batch_sz = None\n        self.init_logvar = init_logvar\n        hid1_units = obs_dim * hid1_mult\n        hid3_units = act_dim * 10  # 10 empirically determined\n        hid2_units = int(np.sqrt(hid1_units * hid3_units))\n        self.lr = 9e-4 / np.sqrt(hid2_units)  # 9e-4 empirically determined\n        # heuristic to set learning rate based on NN size (tuned on \'Hopper-v1\')\n        self.dense1 = Dense(hid1_units, activation=\'tanh\', input_shape=(obs_dim,))\n        self.dense2 = Dense(hid2_units, activation=\'tanh\', input_shape=(hid1_units,))\n        self.dense3 = Dense(hid3_units, activation=\'tanh\', input_shape=(hid2_units,))\n        self.dense4 = Dense(act_dim, input_shape=(hid3_units,))\n        # logvar_speed increases learning rate for log-variances.\n        # heuristic sets logvar_speed based on network size.\n        logvar_speed = (10 * hid3_units) // 48\n        self.logvars = self.add_weight(shape=(logvar_speed, act_dim),\n                                       trainable=True, initializer=\'zeros\')\n        print(\'Policy Params -- h1: {}, h2: {}, h3: {}, lr: {:.3g}, logvar_speed: {}\'\n              .format(hid1_units, hid2_units, hid3_units, self.lr, logvar_speed))\n\n    def build(self, input_shape):\n        self.batch_sz = input_shape[0]\n\n    def call(self, inputs, **kwargs):\n        y = self.dense1(inputs)\n        y = self.dense2(y)\n        y = self.dense3(y)\n        means = self.dense4(y)\n        logvars = K.sum(self.logvars, axis=0, keepdims=True) + self.init_logvar\n        logvars = K.tile(logvars, (self.batch_sz, 1))\n\n        return [means, logvars]\n\n    def get_lr(self):\n        return self.lr\n\n\nclass KLEntropy(Layer):\n    """"""\n    Layer calculates:\n        1. KL divergence between old and new distributions\n        2. Entropy of present policy\n\n    https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback.E2.80.93Leibler_divergence\n    https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Entropy\n    """"""\n    def __init__(self, **kwargs):\n        super(KLEntropy, self).__init__(**kwargs)\n        self.act_dim = None\n\n    def build(self, input_shape):\n        self.act_dim = input_shape[0][1]\n\n    def call(self, inputs, **kwargs):\n        old_means, old_logvars, new_means, new_logvars = inputs\n        log_det_cov_old = K.sum(old_logvars, axis=-1, keepdims=True)\n        log_det_cov_new = K.sum(new_logvars, axis=-1, keepdims=True)\n        trace_old_new = K.sum(K.exp(old_logvars - new_logvars), axis=-1, keepdims=True)\n        kl = 0.5 * (log_det_cov_new - log_det_cov_old + trace_old_new +\n                    K.sum(K.square(new_means - old_means) /\n                          K.exp(new_logvars), axis=-1, keepdims=True) -\n                    np.float32(self.act_dim))\n        entropy = 0.5 * (np.float32(self.act_dim) * (np.log(2 * np.pi) + 1.0) +\n                         K.sum(new_logvars, axis=-1, keepdims=True))\n\n        return [kl, entropy]\n\n\nclass LogProb(Layer):\n    """"""Layer calculates log probabilities of a batch of actions.""""""\n    def __init__(self, **kwargs):\n        super(LogProb, self).__init__(**kwargs)\n\n    def call(self, inputs, **kwargs):\n        actions, act_means, act_logvars = inputs\n        logp = -0.5 * K.sum(act_logvars, axis=-1, keepdims=True)\n        logp += -0.5 * K.sum(K.square(actions - act_means) / K.exp(act_logvars),\n                             axis=-1, keepdims=True)\n\n        return logp\n\n\nclass TRPO(Model):\n    def __init__(self, obs_dim, act_dim, hid1_mult, kl_targ, init_logvar, eta, **kwargs):\n        super(TRPO, self).__init__(**kwargs)\n        self.kl_targ = kl_targ\n        self.eta = eta\n        self.beta = self.add_weight(\'beta\', initializer=\'zeros\', trainable=False)\n        self.policy = PolicyNN(obs_dim, act_dim, hid1_mult, init_logvar)\n        self.logprob = LogProb()\n        self.kl_entropy = KLEntropy()\n\n    def call(self, inputs):\n        obs, act, adv, old_means, old_logvars, old_logp = inputs\n        new_means, new_logvars = self.policy(obs)\n        new_logp = self.logprob([act, new_means, new_logvars])\n        kl, entropy = self.kl_entropy([old_means, old_logvars,\n                                       new_means, new_logvars])\n        loss1 = -K.mean(adv * K.exp(new_logp - old_logp))\n        loss2 = K.mean(self.beta * kl)\n        # TODO - Take mean before or after hinge loss?\n        loss3 = self.eta * K.square(K.maximum(0.0, K.mean(kl) - 2.0 * self.kl_targ))\n        self.add_loss(loss1 + loss2 + loss3)\n\n        return [kl, entropy]\n'"
trpo/train.py,0,"b'#! /usr/bin/env python3\n""""""\nTRPO: Trust Region Policy Optimization\n\nWritten by Patrick Coady (pat-coady.github.io)\n\nSee these papers for details:\n\nTRPO / PPO:\nhttps://arxiv.org/pdf/1502.05477.pdf (Schulman et al., 2016)\n\nDistributed PPO:\nhttps://arxiv.org/abs/1707.02286 (Heess et al., 2017)\n\nGeneralized Advantage Estimation:\nhttps://arxiv.org/pdf/1506.02438.pdf\n\nAnd, also, this GitHub repo which was helpful to me during\nimplementation:\nhttps://github.com/joschu/modular_rl\n\nThis implementation learns policies for continuous environments\nin the OpenAI Gym (https://gym.openai.com/). Testing was focused on\nthe MuJoCo control tasks.\n""""""\nimport gym\nimport pybullet\nimport pybullet_envs\nimport numpy as np\nfrom gym import wrappers\nfrom policy import Policy\nfrom value import NNValueFunction\nimport scipy.signal\nfrom utils import Logger, Scaler\nfrom datetime import datetime\nimport os\nimport argparse\nimport signal\n\n\nclass GracefulKiller:\n    """"""Gracefully exit program on CTRL-C.""""""\n    def __init__(self):\n        self.kill_now = False\n        signal.signal(signal.SIGINT, self.exit_gracefully)\n        signal.signal(signal.SIGTERM, self.exit_gracefully)\n\n    def exit_gracefully(self, signum, frame):\n        self.kill_now = True\n\n\ndef init_gym(env_name):\n    """"""\n    Initialize gym environment, return dimension of observation\n    and action spaces.\n\n    Args:\n        env_name: str environment name (e.g. ""Humanoid-v1"")\n\n    Returns: 3-tuple\n        gym environment (object)\n        number of observation dimensions (int)\n        number of action dimensions (int)\n    """"""\n    env = gym.make(env_name)\n    obs_dim = env.observation_space.shape[0]\n    act_dim = env.action_space.shape[0]\n\n    return env, obs_dim, act_dim\n\n\ndef run_episode(env, policy, scaler, animate=False):\n    """"""Run single episode with option to animate.\n\n    Args:\n        env: ai gym environment\n        policy: policy object with sample() method\n        scaler: scaler object, used to scale/offset each observation dimension\n            to a similar range\n        animate: boolean, True uses env.render() method to animate episode\n\n    Returns: 4-tuple of NumPy arrays\n        observes: shape = (episode len, obs_dim)\n        actions: shape = (episode len, act_dim)\n        rewards: shape = (episode len,)\n        unscaled_obs: useful for training scaler, shape = (episode len, obs_dim)\n    """"""\n    obs = env.reset()\n    observes, actions, rewards, unscaled_obs = [], [], [], []\n    done = False\n    step = 0.0\n    scale, offset = scaler.get()\n    scale[-1] = 1.0  # don\'t scale time step feature\n    offset[-1] = 0.0  # don\'t offset time step feature\n    while not done:\n        if animate:\n            env.render()\n        obs = np.concatenate([obs, [step]])  # add time step feature\n        obs = obs.astype(np.float32).reshape((1, -1))\n        unscaled_obs.append(obs)\n        obs = np.float32((obs - offset) * scale)  # center and scale observations\n        observes.append(obs)\n        action = policy.sample(obs)\n        actions.append(action)\n        obs, reward, done, _ = env.step(action.flatten())\n        rewards.append(reward)\n        step += 1e-3  # increment time step feature\n\n    return (np.concatenate(observes), np.concatenate(actions),\n            np.array(rewards, dtype=np.float32), np.concatenate(unscaled_obs))\n\n\ndef run_policy(env, policy, scaler, logger, episodes):\n    """""" Run policy and collect data for a minimum of min_steps and min_episodes\n\n    Args:\n        env: ai gym environment\n        policy: policy object with sample() method\n        scaler: scaler object, used to scale/offset each observation dimension\n            to a similar range\n        logger: logger object, used to save stats from episodes\n        episodes: total episodes to run\n\n    Returns: list of trajectory dictionaries, list length = number of episodes\n        \'observes\' : NumPy array of states from episode\n        \'actions\' : NumPy array of actions from episode\n        \'rewards\' : NumPy array of (un-discounted) rewards from episode\n        \'unscaled_obs\' : NumPy array of (un-discounted) rewards from episode\n    """"""\n    total_steps = 0\n    trajectories = []\n    for e in range(episodes):\n        observes, actions, rewards, unscaled_obs = run_episode(env, policy, scaler)\n        # print(observes.shape)\n        # print(actions.shape)\n        # print(rewards.shape)\n        # print(unscaled_obs.shape)\n        # print(observes.dtype)\n        # print(actions.dtype)\n        # print(rewards.dtype)\n        # print(unscaled_obs.dtype)\n        total_steps += observes.shape[0]\n        trajectory = {\'observes\': observes,\n                      \'actions\': actions,\n                      \'rewards\': rewards,\n                      \'unscaled_obs\': unscaled_obs}\n        trajectories.append(trajectory)\n    unscaled = np.concatenate([t[\'unscaled_obs\'] for t in trajectories])\n    scaler.update(unscaled)  # update running statistics for scaling observations\n    logger.log({\'_MeanReward\': np.mean([t[\'rewards\'].sum() for t in trajectories]),\n                \'Steps\': total_steps})\n\n    return trajectories\n\n\ndef discount(x, gamma):\n    """""" Calculate discounted forward sum of a sequence at each point """"""\n    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n\n\ndef add_disc_sum_rew(trajectories, gamma):\n    """""" Adds discounted sum of rewards to all time steps of all trajectories\n\n    Args:\n        trajectories: as returned by run_policy()\n        gamma: discount\n\n    Returns:\n        None (mutates trajectories dictionary to add \'disc_sum_rew\')\n    """"""\n    for trajectory in trajectories:\n        if gamma < 0.999:  # don\'t scale for gamma ~= 1\n            rewards = trajectory[\'rewards\'] * (1 - gamma)\n        else:\n            rewards = trajectory[\'rewards\']\n        disc_sum_rew = discount(rewards, gamma)\n        trajectory[\'disc_sum_rew\'] = disc_sum_rew\n\n\ndef add_value(trajectories, val_func):\n    """""" Adds estimated value to all time steps of all trajectories\n\n    Args:\n        trajectories: as returned by run_policy()\n        val_func: object with predict() method, takes observations\n            and returns predicted state value\n\n    Returns:\n        None (mutates trajectories dictionary to add \'values\')\n    """"""\n    for trajectory in trajectories:\n        observes = trajectory[\'observes\']\n        values = val_func.predict(observes)\n        trajectory[\'values\'] = values.flatten()\n\n\ndef add_gae(trajectories, gamma, lam):\n    """""" Add generalized advantage estimator.\n    https://arxiv.org/pdf/1506.02438.pdf\n\n    Args:\n        trajectories: as returned by run_policy(), must include \'values\'\n            key from add_value().\n        gamma: reward discount\n        lam: lambda (see paper).\n            lam=0 : use TD residuals\n            lam=1 : A =  Sum Discounted Rewards - V_hat(s)\n\n    Returns:\n        None (mutates trajectories dictionary to add \'advantages\')\n    """"""\n    for trajectory in trajectories:\n        if gamma < 0.999:  # don\'t scale for gamma ~= 1\n            rewards = trajectory[\'rewards\'] * (1 - gamma)\n        else:\n            rewards = trajectory[\'rewards\']\n        values = trajectory[\'values\']\n        # temporal differences\n        tds = rewards - values + np.append(values[1:] * gamma, 0)\n        advantages = discount(tds, gamma * lam)\n        trajectory[\'advantages\'] = advantages\n\n\ndef build_train_set(trajectories):\n    """"""\n\n    Args:\n        trajectories: trajectories after processing by add_disc_sum_rew(),\n            add_value(), and add_gae()\n\n    Returns: 4-tuple of NumPy arrays\n        observes: shape = (N, obs_dim)\n        actions: shape = (N, act_dim)\n        advantages: shape = (N,)\n        disc_sum_rew: shape = (N,)\n    """"""\n    observes = np.concatenate([t[\'observes\'] for t in trajectories])\n    actions = np.concatenate([t[\'actions\'] for t in trajectories])\n    disc_sum_rew = np.concatenate([t[\'disc_sum_rew\'] for t in trajectories])\n    advantages = np.concatenate([t[\'advantages\'] for t in trajectories])\n    # normalize advantages\n    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n\n    return observes, actions, advantages, disc_sum_rew\n\n\ndef log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode):\n    """""" Log various batch statistics """"""\n    logger.log({\'_mean_obs\': np.mean(observes),\n                \'_min_obs\': np.min(observes),\n                \'_max_obs\': np.max(observes),\n                \'_std_obs\': np.mean(np.var(observes, axis=0)),\n                \'_mean_act\': np.mean(actions),\n                \'_min_act\': np.min(actions),\n                \'_max_act\': np.max(actions),\n                \'_std_act\': np.mean(np.var(actions, axis=0)),\n                \'_mean_adv\': np.mean(advantages),\n                \'_min_adv\': np.min(advantages),\n                \'_max_adv\': np.max(advantages),\n                \'_std_adv\': np.var(advantages),\n                \'_mean_discrew\': np.mean(disc_sum_rew),\n                \'_min_discrew\': np.min(disc_sum_rew),\n                \'_max_discrew\': np.max(disc_sum_rew),\n                \'_std_discrew\': np.var(disc_sum_rew),\n                \'_Episode\': episode\n                })\n\n\ndef main(env_name, num_episodes, gamma, lam, kl_targ, batch_size, hid1_mult, init_logvar):\n    """""" Main training loop\n\n    Args:\n        env_name: OpenAI Gym environment name, e.g. \'Hopper-v1\'\n        num_episodes: maximum number of episodes to run\n        gamma: reward discount factor (float)\n        lam: lambda from Generalized Advantage Estimate\n        kl_targ: D_KL target for policy update [D_KL(pi_old || pi_new)\n        batch_size: number of episodes per policy training batch\n        hid1_mult: hid1 size for policy and value_f (multiplier of obs dimension)\n        init_logvar: natural log of initial policy variance\n    """"""\n    pybullet.connect(pybullet.DIRECT)\n    killer = GracefulKiller()\n    env, obs_dim, act_dim = init_gym(env_name)\n    obs_dim += 1  # add 1 to obs dimension for time step feature (see run_episode())\n    now = datetime.utcnow().strftime(""%b-%d_%H:%M:%S"")  # create unique directories\n    logger = Logger(logname=env_name, now=now)\n    aigym_path = os.path.join(\'/tmp\', env_name, now)\n    env = wrappers.Monitor(env, aigym_path, force=True)\n    scaler = Scaler(obs_dim)\n    val_func = NNValueFunction(obs_dim, hid1_mult)\n    policy = Policy(obs_dim, act_dim, kl_targ, hid1_mult, init_logvar)\n    # run a few episodes of untrained policy to initialize scaler:\n    run_policy(env, policy, scaler, logger, episodes=5)\n    episode = 0\n    while episode < num_episodes:\n        trajectories = run_policy(env, policy, scaler, logger, episodes=batch_size)\n        episode += len(trajectories)\n        add_value(trajectories, val_func)  # add estimated values to episodes\n        add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of Rs\n        add_gae(trajectories, gamma, lam)  # calculate advantage\n        # concatenate all episodes into single NumPy arrays\n        observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n        # add various stats to training log:\n        log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode)\n        policy.update(observes, actions, advantages, logger)  # update policy\n        val_func.fit(observes, disc_sum_rew, logger)  # update value function\n        logger.write(display=True)  # write logger results to file and stdout\n        if killer.kill_now:\n            if input(\'Terminate training (y/[n])? \') == \'y\':\n                break\n            killer.kill_now = False\n    logger.close()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=(\'Train policy on OpenAI Gym environment \'\n                                                  \'using Proximal Policy Optimizer\'))\n    parser.add_argument(\'env_name\', type=str, help=\'OpenAI Gym (PyBullet) environment name\')\n    parser.add_argument(\'-n\', \'--num_episodes\', type=int, help=\'Number of episodes to run\',\n                        default=1000)\n    parser.add_argument(\'-g\', \'--gamma\', type=float, help=\'Discount factor\', default=0.995)\n    parser.add_argument(\'-l\', \'--lam\', type=float, help=\'Lambda for Generalized Advantage Estimation\',\n                        default=0.98)\n    parser.add_argument(\'-k\', \'--kl_targ\', type=float, help=\'D_KL target value\',\n                        default=0.003)\n    parser.add_argument(\'-b\', \'--batch_size\', type=int,\n                        help=\'Number of episodes per training batch\',\n                        default=20)\n    parser.add_argument(\'-m\', \'--hid1_mult\', type=int,\n                        help=\'Size of first hidden layer for value and policy NNs\'\n                             \'(integer multiplier of observation dimension)\',\n                        default=10)\n    parser.add_argument(\'-v\', \'--init_logvar\', type=float,\n                        help=\'Initial policy log-variance (natural log of variance)\',\n                        default=-1.0)\n\n    args = parser.parse_args()\n    main(**vars(args))\n'"
trpo/utils.py,0,"b'""""""\nLogging and Data Scaling Utilities\n\nWritten by Patrick Coady (pat-coady.github.io)\n""""""\nimport numpy as np\nimport os\nimport shutil\nimport glob\nimport csv\n\n\nclass Scaler(object):\n    """""" Generate scale and offset based on running mean and stddev along axis=0\n\n        offset = running mean\n        scale = 1 / (stddev + 0.1) / 3 (i.e. 3x stddev = +/- 1.0)\n    """"""\n\n    def __init__(self, obs_dim):\n        """"""\n        Args:\n            obs_dim: dimension of axis=1\n        """"""\n        self.vars = np.zeros(obs_dim)\n        self.means = np.zeros(obs_dim)\n        self.m = 0\n        self.n = 0\n        self.first_pass = True\n\n    def update(self, x):\n        """""" Update running mean and variance (this is an exact method)\n        Args:\n            x: NumPy array, shape = (N, obs_dim)\n\n        see: https://stats.stackexchange.com/questions/43159/how-to-calculate-pooled-\n               variance-of-two-groups-given-known-group-variances-mean\n        """"""\n        if self.first_pass:\n            self.means = np.mean(x, axis=0)\n            self.vars = np.var(x, axis=0)\n            self.m = x.shape[0]\n            self.first_pass = False\n        else:\n            n = x.shape[0]\n            new_data_var = np.var(x, axis=0)\n            new_data_mean = np.mean(x, axis=0)\n            new_data_mean_sq = np.square(new_data_mean)\n            new_means = ((self.means * self.m) + (new_data_mean * n)) / (self.m + n)\n            self.vars = (((self.m * (self.vars + np.square(self.means))) +\n                          (n * (new_data_var + new_data_mean_sq))) / (self.m + n) -\n                         np.square(new_means))\n            self.vars = np.maximum(0.0, self.vars)  # occasionally goes negative, clip\n            self.means = new_means\n            self.m += n\n\n    def get(self):\n        """""" returns 2-tuple: (scale, offset) """"""\n        return 1/(np.sqrt(self.vars) + 0.1)/3, self.means\n\n\nclass Logger(object):\n    """""" Simple training logger: saves to file and optionally prints to stdout """"""\n    def __init__(self, logname, now):\n        """"""\n        Args:\n            logname: name for log (e.g. \'Hopper-v1\')\n            now: unique sub-directory name (e.g. date/time string)\n        """"""\n        path = os.path.join(\'log-files\', logname, now)\n        os.makedirs(path)\n        filenames = glob.glob(\'*.py\')  # put copy of all python files in log_dir\n        for filename in filenames:     # for reference\n            shutil.copy(filename, path)\n        path = os.path.join(path, \'log.csv\')\n\n        self.write_header = True\n        self.log_entry = {}\n        self.f = open(path, \'w\')\n        self.writer = None  # DictWriter created with first call to write() method\n\n    def write(self, display=True):\n        """""" Write 1 log entry to file, and optionally to stdout\n        Log fields preceded by \'_\' will not be printed to stdout\n\n        Args:\n            display: boolean, print to stdout\n        """"""\n        if display:\n            self.disp(self.log_entry)\n        if self.write_header:\n            fieldnames = [x for x in self.log_entry.keys()]\n            self.writer = csv.DictWriter(self.f, fieldnames=fieldnames)\n            self.writer.writeheader()\n            self.write_header = False\n        self.writer.writerow(self.log_entry)\n        self.log_entry = {}\n\n    @staticmethod\n    def disp(log):\n        """"""Print metrics to stdout""""""\n        log_keys = [k for k in log.keys()]\n        log_keys.sort()\n        print(\'***** Episode {}, Mean R = {:.1f} *****\'.format(log[\'_Episode\'],\n                                                               log[\'_MeanReward\']))\n        for key in log_keys:\n            if key[0] != \'_\':  # don\'t display log items with leading \'_\'\n                print(\'{:s}: {:.3g}\'.format(key, log[key]))\n        print(\'\\n\')\n\n    def log(self, items):\n        """""" Update fields in log (does not write to file, used to collect updates.\n\n        Args:\n            items: dictionary of items to update\n        """"""\n        self.log_entry.update(items)\n\n    def close(self):\n        """""" Close log file - log cannot be written after this """"""\n        self.f.close()\n'"
trpo/value.py,0,"b'""""""\nState-Value Function\n\nWritten by Patrick Coady (pat-coady.github.io)\n""""""\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n\nimport numpy as np\n\n\nclass NNValueFunction(object):\n    """""" NN-based state-value function """"""\n    def __init__(self, obs_dim, hid1_mult):\n        """"""\n        Args:\n            obs_dim: number of dimensions in observation vector (int)\n            hid1_mult: size of first hidden layer, multiplier of obs_dim\n        """"""\n        self.replay_buffer_x = None\n        self.replay_buffer_y = None\n        self.obs_dim = obs_dim\n        self.hid1_mult = hid1_mult\n        self.epochs = 10\n        self.lr = None  # learning rate set in _build_model()\n        self.model = self._build_model()\n\n    def _build_model(self):\n        """""" Construct TensorFlow graph, including loss function, init op and train op """"""\n        obs = Input(shape=(self.obs_dim,), dtype=\'float32\')\n        # hid1 layer size is 10x obs_dim, hid3 size is 10, and hid2 is geometric mean\n        hid1_units = self.obs_dim * self.hid1_mult\n        hid3_units = 5  # 5 chosen empirically on \'Hopper-v1\'\n        hid2_units = int(np.sqrt(hid1_units * hid3_units))\n        # heuristic to set learning rate based on NN size (tuned on \'Hopper-v1\')\n        self.lr = 1e-2 / np.sqrt(hid2_units)  # 1e-2 empirically determined\n        print(\'Value Params -- h1: {}, h2: {}, h3: {}, lr: {:.3g}\'\n              .format(hid1_units, hid2_units, hid3_units, self.lr))\n        y = Dense(hid1_units, activation=\'tanh\')(obs)\n        y = Dense(hid2_units, activation=\'tanh\')(y)\n        y = Dense(hid3_units, activation=\'tanh\')(y)\n        y = Dense(1)(y)\n        model = Model(inputs=obs, outputs=y)\n        optimizer = Adam(self.lr)\n        model.compile(optimizer=optimizer, loss=\'mse\')\n\n        return model\n\n    def fit(self, x, y, logger):\n        """""" Fit model to current data batch + previous data batch\n\n        Args:\n            x: features\n            y: target\n            logger: logger to save training loss and % explained variance\n        """"""\n        num_batches = max(x.shape[0] // 256, 1)\n        batch_size = x.shape[0] // num_batches\n        y_hat = self.model.predict(x)  # check explained variance prior to update\n        old_exp_var = 1 - np.var(y - y_hat)/np.var(y)\n        if self.replay_buffer_x is None:\n            x_train, y_train = x, y\n        else:\n            x_train = np.concatenate([x, self.replay_buffer_x])\n            y_train = np.concatenate([y, self.replay_buffer_y])\n        self.replay_buffer_x = x\n        self.replay_buffer_y = y\n        self.model.fit(x_train, y_train, epochs=self.epochs, batch_size=batch_size,\n                       shuffle=True, verbose=0)\n        y_hat = self.model.predict(x)\n        loss = np.mean(np.square(y_hat - y))         # explained variance after update\n        exp_var = 1 - np.var(y - y_hat) / np.var(y)  # diagnose over-fitting of val func\n\n        logger.log({\'ValFuncLoss\': loss,\n                    \'ExplainedVarNew\': exp_var,\n                    \'ExplainedVarOld\': old_exp_var})\n\n    def predict(self, x):\n        """""" Predict method """"""\n        return self.model.predict(x)\n'"
