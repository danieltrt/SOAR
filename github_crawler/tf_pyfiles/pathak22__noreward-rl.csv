file_path,api_count,code
doomFiles/__init__.py,0,"b'\'\'\'\nPlace this file in:\n/home/pathak/projects/unsup-rl/unsuprl/local/lib/python2.7/site-packages/ppaquette_gym_doom/__init__.py\n\'\'\'\n\nfrom gym.envs.registration import register\nfrom gym.scoreboard.registration import add_task, add_group\nfrom .package_info import USERNAME\nfrom .doom_env import DoomEnv, MetaDoomEnv\nfrom .doom_basic import DoomBasicEnv\nfrom .doom_corridor import DoomCorridorEnv\nfrom .doom_defend_center import DoomDefendCenterEnv\nfrom .doom_defend_line import DoomDefendLineEnv\nfrom .doom_health_gathering import DoomHealthGatheringEnv\nfrom .doom_my_way_home import DoomMyWayHomeEnv\nfrom .doom_predict_position import DoomPredictPositionEnv\nfrom .doom_take_cover import DoomTakeCoverEnv\nfrom .doom_deathmatch import DoomDeathmatchEnv\nfrom .doom_my_way_home_sparse import DoomMyWayHomeFixedEnv\nfrom .doom_my_way_home_verySparse import DoomMyWayHomeFixed15Env\n\n# Env registration\n# ==========================\n\nregister(\n    id=\'{}/meta-Doom-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:MetaDoomEnv\'.format(USERNAME),\n    timestep_limit=999999,\n    reward_threshold=9000.0,\n    kwargs={\n        \'average_over\': 3,\n        \'passing_grade\': 600,\n        \'min_tries_for_avg\': 3\n    },\n)\n\nregister(\n    id=\'{}/DoomBasic-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomBasicEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=10.0,\n)\n\nregister(\n    id=\'{}/DoomCorridor-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomCorridorEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=1000.0,\n)\n\nregister(\n    id=\'{}/DoomDefendCenter-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomDefendCenterEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=10.0,\n)\n\nregister(\n    id=\'{}/DoomDefendLine-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomDefendLineEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=15.0,\n)\n\nregister(\n    id=\'{}/DoomHealthGathering-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomHealthGatheringEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=1000.0,\n)\n\nregister(\n    id=\'{}/DoomMyWayHome-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomMyWayHomeEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=0.5,\n)\n\nregister(\n    id=\'{}/DoomPredictPosition-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomPredictPositionEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=0.5,\n)\n\nregister(\n    id=\'{}/DoomTakeCover-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomTakeCoverEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=750.0,\n)\n\nregister(\n    id=\'{}/DoomDeathmatch-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomDeathmatchEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=20.0,\n)\n\nregister(\n    id=\'{}/DoomMyWayHomeFixed-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomMyWayHomeFixedEnv\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=0.5,\n)\n\nregister(\n    id=\'{}/DoomMyWayHomeFixed15-v0\'.format(USERNAME),\n    entry_point=\'{}_gym_doom:DoomMyWayHomeFixed15Env\'.format(USERNAME),\n    timestep_limit=10000,\n    reward_threshold=0.5,\n)\n\n# Scoreboard registration\n# ==========================\nadd_group(\n    id= \'doom\',\n    name= \'Doom\',\n    description= \'Doom environments based on VizDoom.\'\n)\n\nadd_task(\n    id=\'{}/meta-Doom-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #1 to #9 - Beat all 9 Doom missions.\',\n    description=""""""\nThis is a meta map that combines all 9 Doom levels.\n\nLevels:\n    - #0 Doom Basic\n    - #1 Doom Corridor\n    - #2 Doom DefendCenter\n    - #3 Doom DefendLine\n    - #4 Doom HealthGathering\n    - #5 Doom MyWayHome\n    - #6 Doom PredictPosition\n    - #7 Doom TakeCover\n    - #8 Doom Deathmatch\n    - #9 Doom MyWayHomeFixed (customized)\n    - #10 Doom MyWayHomeFixed15 (customized)\n\nGoal: 9,000 points\n    - Pass all levels\n\nScoring:\n    - Each level score has been standardized on a scale of 0 to 1,000\n    - The passing score for a level is 990 (99th percentile)\n    - A bonus of 450 (50 * 9 levels) is given if all levels are passed\n    - The score for a level is the average of the last 3 tries\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomBasic-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #1 - Kill a single monster using your pistol.\',\n    description=""""""\nThis map is rectangular with gray walls, ceiling and floor.\nYou are spawned in the center of the longer wall, and a red\ncircular monster is spawned randomly on the opposite wall.\nYou need to kill the monster (one bullet is enough).\n\nGoal: 10 points\n    - Kill the monster in 3 secs with 1 shot\n\nRewards:\n    - Plus 101 pts for killing the monster\n    - Minus  5 pts for missing a shot\n    - Minus  1 pts every 0.028 secs\n\nEnds when:\n    - Monster is dead\n    - Player is dead\n    - Timeout (10 seconds - 350 frames)\n\nAllowed actions:\n    - ATTACK\n    - MOVE_RIGHT\n    - MOVE_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomCorridor-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #2 - Run as fast as possible to grab a vest.\',\n    description=""""""\nThis map is designed to improve your navigation. There is a vest\nat the end of the corridor, with 6 enemies (3 groups of 2). Your goal\nis to get to the vest as soon as possible, without being killed.\n\nGoal: 1,000 points\n    - Reach the vest (or get very close to it)\n\nRewards:\n    - Plus distance for getting closer to the vest\n    - Minus distance for getting further from the vest\n    - Minus 100 pts for getting killed\n\nEnds when:\n    - Player touches vest\n    - Player is dead\n    - Timeout (1 minutes - 2,100 frames)\n\nAllowed actions:\n    - ATTACK\n    - MOVE_RIGHT\n    - MOVE_LEFT\n    - MOVE_FORWARD\n    - TURN_RIGHT\n    - TURN_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomDefendCenter-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #3 - Kill enemies coming at your from all sides.\',\n    description=""""""\nThis map is designed to teach you how to kill and how to stay alive.\nYou will also need to keep an eye on your ammunition level. You are only\nrewarded for kills, so figure out how to stay alive.\n\nThe map is a circle with monsters. You are in the middle. Monsters will\nrespawn with additional health when killed. Kill as many as you can\nbefore you run out of ammo.\n\nGoal: 10 points\n    - Kill 11 monsters (you have 26 ammo)\n\nRewards:\n    - Plus 1 point for killing a monster\n    - Minus 1 point for getting killed\n\nEnds when:\n    - Player is dead\n    - Timeout (60 seconds - 2100 frames)\n\nAllowed actions:\n    - ATTACK\n    - TURN_RIGHT\n    - TURN_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomDefendLine-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #4 - Kill enemies on the other side of the room.\',\n    description=""""""\nThis map is designed to teach you how to kill and how to stay alive.\nYour ammo will automatically replenish. You are only rewarded for kills,\nso figure out how to stay alive.\n\nThe map is a rectangle with monsters on the other side. Monsters will\nrespawn with additional health when killed. Kill as many as you can\nbefore they kill you. This map is harder than the previous.\n\nGoal: 15 points\n    - Kill 16 monsters\n\nRewards:\n    - Plus 1 point for killing a monster\n    - Minus 1 point for getting killed\n\nEnds when:\n    - Player is dead\n    - Timeout (60 seconds - 2100 frames)\n\nAllowed actions:\n    - ATTACK\n    - TURN_RIGHT\n    - TURN_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomHealthGathering-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #5 - Learn to grad medkits to survive as long as possible.\',\n    description=""""""\nThis map is a guide on how to survive by collecting health packs.\nIt is a rectangle with green, acidic floor which hurts the player\nperiodically. There are also medkits spread around the map, and\nadditional kits will spawn at interval.\n\nGoal: 1000 points\n    - Stay alive long enough for approx. 30 secs\n\nRewards:\n    - Plus 1 point every 0.028 secs\n    - Minus 100 pts for dying\n\nEnds when:\n    - Player is dead\n    - Timeout (60 seconds - 2,100 frames)\n\nAllowed actions:\n    - MOVE_FORWARD\n    - TURN_RIGHT\n    - TURN_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomMyWayHome-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #6 - Find the vest in one the 4 rooms.\',\n    description=""""""\nThis map is designed to improve navigational skills. It is a series of\ninterconnected rooms and 1 corridor with a dead end. Each room\nhas a separate color. There is a green vest in one of the room.\nThe vest is always in the same room. Player must find the vest.\n\nGoal: 0.50 point\n    - Find the vest\n\nRewards:\n    - Plus 1 point for finding the vest\n    - Minus 0.0001 point every 0.028 secs\n\nEnds when:\n    - Vest is found\n    - Timeout (1 minutes - 2,100 frames)\n\nAllowed actions:\n    - MOVE_FORWARD\n    - TURN_RIGHT\n    - TURN_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomPredictPosition-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #7 - Learn how to kill an enemy with a rocket launcher.\',\n    description=""""""\nThis map is designed to train you on using a rocket launcher.\nIt is a rectangular map with a monster on the opposite side. You need\nto use your rocket launcher to kill it. The rocket adds a delay between\nthe moment it is fired and the moment it reaches the other side of the room.\nYou need to predict the position of the monster to kill it.\n\nGoal: 0.5 point\n    - Kill the monster\n\nRewards:\n    - Plus 1 point for killing the monster\n    - Minus 0.0001 point every 0.028 secs\n\nEnds when:\n    - Monster is dead\n    - Out of missile (you only have one)\n    - Timeout (20 seconds - 700 frames)\n\nHint: Wait 1 sec for the missile launcher to load.\n\nAllowed actions:\n    - ATTACK\n    - TURN_RIGHT\n    - TURN_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomTakeCover-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #8 - Survive as long as possible with enemies shooting at you.\',\n    description=""""""\nThis map is to train you on the damage of incoming missiles.\nIt is a rectangular map with monsters firing missiles and fireballs\nat you. You need to survive as long as possible.\n\nGoal: 750 points\n    - Survive for approx. 20 seconds\n\nRewards:\n    - Plus 1 point every 0.028 secs\n\nEnds when:\n    - Player is dead (1 or 2 fireballs is enough)\n    - Timeout (60 seconds - 2,100 frames)\n\nAllowed actions:\n    - MOVE_RIGHT\n    - MOVE_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomDeathmatch-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #9 - Kill as many enemies as possible without being killed.\',\n    description=""""""\nKill as many monsters as possible without being killed.\n\nGoal: 20 points\n    - Kill 20 monsters\n\nRewards:\n    - Plus 1 point for killing a monster\n\nEnds when:\n    - Player is dead\n    - Timeout (3 minutes - 6,300 frames)\n\nAllowed actions:\n    - ALL\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomMyWayHomeFixed-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #10 - Find the vest in one the 4 rooms.\',\n    description=""""""\nThis map is designed to improve navigational skills. It is a series of\ninterconnected rooms and 1 corridor with a dead end. Each room\nhas a separate color. There is a green vest in one of the room.\nThe vest is always in the same room. Player must find the vest.\nYou always start from fixed room (room no. 10 -- farthest).\n\nGoal: 0.50 point\n    - Find the vest\n\nRewards:\n    - Plus 1 point for finding the vest\n    - Minus 0.0001 point every 0.028 secs\n\nEnds when:\n    - Vest is found\n    - Timeout (1 minutes - 2,100 frames)\n\nAllowed actions:\n    - MOVE_FORWARD\n    - TURN_RIGHT\n    - TURN_LEFT\n""""""\n)\n\nadd_task(\n    id=\'{}/DoomMyWayHomeFixed15-v0\'.format(USERNAME),\n    group=\'doom\',\n    summary=\'Mission #11 - Find the vest in one the 4 rooms.\',\n    description=""""""\nThis map is designed to improve navigational skills. It is a series of\ninterconnected rooms and 1 corridor with a dead end. Each room\nhas a separate color. There is a green vest in one of the room.\nThe vest is always in the same room. Player must find the vest.\nYou always start from fixed room (room no. 10 -- farthest).\n\nGoal: 0.50 point\n    - Find the vest\n\nRewards:\n    - Plus 1 point for finding the vest\n    - Minus 0.0001 point every 0.028 secs\n\nEnds when:\n    - Vest is found\n    - Timeout (1 minutes - 2,100 frames)\n\nAllowed actions:\n    - MOVE_FORWARD\n    - TURN_RIGHT\n    - TURN_LEFT\n""""""\n)\n'"
doomFiles/action_space.py,0,"b'\'\'\'\nPlace this file in:\n/home/pathak/projects/unsup-rl/unsuprl/local/lib/python2.7/site-packages/ppaquette_gym_doom/wrappers/action_space.py\n\'\'\'\n\nimport gym\n\n# Constants\nNUM_ACTIONS = 43\nALLOWED_ACTIONS = [\n    [0, 10, 11],                                # 0 - Basic\n    [0, 10, 11, 13, 14, 15],                    # 1 - Corridor\n    [0, 14, 15],                                # 2 - DefendCenter\n    [0, 14, 15],                                # 3 - DefendLine\n    [13, 14, 15],                               # 4 - HealthGathering\n    [13, 14, 15],                               # 5 - MyWayHome\n    [0, 14, 15],                                # 6 - PredictPosition\n    [10, 11],                                   # 7 - TakeCover\n    [x for x in range(NUM_ACTIONS) if x != 33], # 8 - Deathmatch\n    [13, 14, 15],                               # 9 - MyWayHomeFixed\n    [13, 14, 15],                               # 10 - MyWayHomeFixed15\n]\n\n__all__ = [ \'ToDiscrete\', \'ToBox\' ]\n\ndef ToDiscrete(config):\n    # Config can be \'minimal\', \'constant-7\', \'constant-17\', \'full\'\n\n    class ToDiscreteWrapper(gym.Wrapper):\n        """"""\n            Doom wrapper to convert MultiDiscrete action space to Discrete\n\n            config:\n                - minimal - Will only use the levels\' allowed actions (+ NOOP)\n                - constant-7 - Will use the 7 minimum actions (+NOOP) to complete all levels\n                - constant-17 - Will use the 17 most common actions (+NOOP) to complete all levels\n                - full - Will use all available actions (+ NOOP)\n\n            list of commands:\n                - minimal:\n                    Basic:              NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT\n                    Corridor:           NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    DefendCenter        NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    DefendLine:         NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    HealthGathering:    NOOP, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    MyWayHome:          NOOP, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    PredictPosition:    NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    TakeCover:          NOOP, MOVE_RIGHT, MOVE_LEFT\n                    Deathmatch:         NOOP, ALL COMMANDS (Deltas are limited to [0,1] range and will not work properly)\n\n                - constant-7: NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, SELECT_NEXT_WEAPON\n\n                - constant-17: NOOP, ATTACK, JUMP, CROUCH, TURN180, RELOAD, SPEED, STRAFE, MOVE_RIGHT, MOVE_LEFT, MOVE_BACKWARD\n                                MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, LOOK_UP, LOOK_DOWN, SELECT_NEXT_WEAPON, SELECT_PREV_WEAPON\n        """"""\n        def __init__(self, env):\n            super(ToDiscreteWrapper, self).__init__(env)\n            if config == \'minimal\':\n                allowed_actions = ALLOWED_ACTIONS[self.unwrapped.level]\n            elif config == \'constant-7\':\n                allowed_actions = [0, 10, 11, 13, 14, 15, 31]\n            elif config == \'constant-17\':\n                allowed_actions = [0, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 31, 32]\n            elif config == \'full\':\n                allowed_actions = None\n            else:\n                raise gym.error.Error(\'Invalid configuration. Valid options are ""minimal"", ""constant-7"", ""constant-17"", ""full""\')\n            self.action_space = gym.spaces.multi_discrete.DiscreteToMultiDiscrete(self.action_space, allowed_actions)\n        def _step(self, action):\n            return self.env._step(self.action_space(action))\n\n    return ToDiscreteWrapper\n\ndef ToBox(config):\n    # Config can be \'minimal\', \'constant-7\', \'constant-17\', \'full\'\n\n    class ToBoxWrapper(gym.Wrapper):\n        """"""\n            Doom wrapper to convert MultiDiscrete action space to Box\n\n            config:\n                - minimal - Will only use the levels\' allowed actions\n                - constant-7 - Will use the 7 minimum actions to complete all levels\n                - constant-17 - Will use the 17 most common actions to complete all levels\n                - full - Will use all available actions\n\n            list of commands:\n                - minimal:\n                    Basic:              ATTACK, MOVE_RIGHT, MOVE_LEFT\n                    Corridor:           ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    DefendCenter        ATTACK, TURN_RIGHT, TURN_LEFT\n                    DefendLine:         ATTACK, TURN_RIGHT, TURN_LEFT\n                    HealthGathering:    MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    MyWayHome:          MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    PredictPosition:    ATTACK, TURN_RIGHT, TURN_LEFT\n                    TakeCover:          MOVE_RIGHT, MOVE_LEFT\n                    Deathmatch:         ALL COMMANDS\n\n                - constant-7: ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, SELECT_NEXT_WEAPON\n\n                - constant-17:  ATTACK, JUMP, CROUCH, TURN180, RELOAD, SPEED, STRAFE, MOVE_RIGHT, MOVE_LEFT, MOVE_BACKWARD\n                                MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, LOOK_UP, LOOK_DOWN, SELECT_NEXT_WEAPON, SELECT_PREV_WEAPON\n        """"""\n        def __init__(self, env):\n            super(ToBoxWrapper, self).__init__(env)\n            if config == \'minimal\':\n                allowed_actions = ALLOWED_ACTIONS[self.unwrapped.level]\n            elif config == \'constant-7\':\n                allowed_actions = [0, 10, 11, 13, 14, 15, 31]\n            elif config == \'constant-17\':\n                allowed_actions = [0, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 31, 32]\n            elif config == \'full\':\n                allowed_actions = None\n            else:\n                raise gym.error.Error(\'Invalid configuration. Valid options are ""minimal"", ""constant-7"", ""constant-17"", ""full""\')\n            self.action_space = gym.spaces.multi_discrete.BoxToMultiDiscrete(self.action_space, allowed_actions)\n        def _step(self, action):\n            return self.env._step(self.action_space(action))\n\n    return ToBoxWrapper\n'"
doomFiles/doom_env.py,0,"b'\'\'\'\nPlace this file in:\n/home/pathak/projects/unsup-rl/unsuprl/local/lib/python2.7/site-packages/ppaquette_gym_doom/doom_env.py\n\'\'\'\n\nimport logging\nimport os\nfrom time import sleep\nimport multiprocessing\n\nimport numpy as np\n\nimport gym\nfrom gym import spaces, error\nfrom gym.utils import seeding\n\ntry:\n    import doom_py\n    from doom_py import DoomGame, Mode, Button, GameVariable, ScreenFormat, ScreenResolution, Loader, doom_fixed_to_double\n    from doom_py.vizdoom import ViZDoomUnexpectedExitException, ViZDoomErrorException\nexcept ImportError as e:\n    raise gym.error.DependencyNotInstalled(""{}. (HINT: you can install Doom dependencies "" +\n                                           ""with \'pip install doom_py.)\'"".format(e))\n\nlogger = logging.getLogger(__name__)\n\n# Arguments:\nRANDOMIZE_MAPS = 80  # 0 means load default, otherwise randomly load in the id mentioned\nNO_MONSTERS = True  # remove monster spawning\n\n# Constants\nNUM_ACTIONS = 43\nNUM_LEVELS = 9\nCONFIG = 0\nSCENARIO = 1\nMAP = 2\nDIFFICULTY = 3\nACTIONS = 4\nMIN_SCORE = 5\nTARGET_SCORE = 6\n\n# Format (config, scenario, map, difficulty, actions, min, target)\nDOOM_SETTINGS = [\n    [\'basic.cfg\', \'basic.wad\', \'map01\', 5, [0, 10, 11], -485, 10],                               # 0 - Basic\n    [\'deadly_corridor.cfg\', \'deadly_corridor.wad\', \'\', 1, [0, 10, 11, 13, 14, 15], -120, 1000],  # 1 - Corridor\n    [\'defend_the_center.cfg\', \'defend_the_center.wad\', \'\', 5, [0, 14, 15], -1, 10],              # 2 - DefendCenter\n    [\'defend_the_line.cfg\', \'defend_the_line.wad\', \'\', 5, [0, 14, 15], -1, 15],                  # 3 - DefendLine\n    [\'health_gathering.cfg\', \'health_gathering.wad\', \'map01\', 5, [13, 14, 15], 0, 1000],         # 4 - HealthGathering\n    [\'my_way_home.cfg\', \'my_way_home_dense.wad\', \'\', 5, [13, 14, 15], -0.22, 0.5],                     # 5 - MyWayHome\n    [\'predict_position.cfg\', \'predict_position.wad\', \'map01\', 3, [0, 14, 15], -0.075, 0.5],      # 6 - PredictPosition\n    [\'take_cover.cfg\', \'take_cover.wad\', \'map01\', 5, [10, 11], 0, 750],                          # 7 - TakeCover\n    [\'deathmatch.cfg\', \'deathmatch.wad\', \'\', 5, [x for x in range(NUM_ACTIONS) if x != 33], 0, 20], # 8 - Deathmatch\n    [\'my_way_home.cfg\', \'my_way_home_sparse.wad\', \'\', 5, [13, 14, 15], -0.22, 0.5],               # 9 - MyWayHomeFixed\n    [\'my_way_home.cfg\', \'my_way_home_verySparse.wad\', \'\', 5, [13, 14, 15], -0.22, 0.5],               # 10 - MyWayHomeFixed15\n]\n\n# Singleton pattern\nclass DoomLock:\n    class __DoomLock:\n        def __init__(self):\n            self.lock = multiprocessing.Lock()\n    instance = None\n    def __init__(self):\n        if not DoomLock.instance:\n            DoomLock.instance = DoomLock.__DoomLock()\n    def get_lock(self):\n        return DoomLock.instance.lock\n\n\nclass DoomEnv(gym.Env):\n    metadata = {\'render.modes\': [\'human\', \'rgb_array\'], \'video.frames_per_second\': 35}\n\n    def __init__(self, level):\n        self.previous_level = -1\n        self.level = level\n        self.game = DoomGame()\n        self.loader = Loader()\n        self.doom_dir = os.path.dirname(os.path.abspath(__file__))\n        self._mode = \'algo\'                         # \'algo\' or \'human\'\n        self.no_render = False                      # To disable double rendering in human mode\n        self.viewer = None\n        self.is_initialized = False                 # Indicates that reset() has been called\n        self.curr_seed = 0\n        self.lock = (DoomLock()).get_lock()\n        self.action_space = spaces.MultiDiscrete([[0, 1]] * 38 + [[-10, 10]] * 2 + [[-100, 100]] * 3)\n        self.allowed_actions = list(range(NUM_ACTIONS))\n        self.screen_height = 480\n        self.screen_width = 640\n        self.screen_resolution = ScreenResolution.RES_640X480\n        self.observation_space = spaces.Box(low=0, high=255, shape=(self.screen_height, self.screen_width, 3))\n        self._seed()\n        self._configure()\n\n    def _configure(self, lock=None, **kwargs):\n        if \'screen_resolution\' in kwargs:\n            logger.warn(\'Deprecated - Screen resolution must now be set using a wrapper. See documentation for details.\')\n        # Multiprocessing lock\n        if lock is not None:\n            self.lock = lock\n\n    def _load_level(self):\n        # Closing if is_initialized\n        if self.is_initialized:\n            self.is_initialized = False\n            self.game.close()\n            self.game = DoomGame()\n\n        # Customizing level\n        if getattr(self, \'_customize_game\', None) is not None and callable(self._customize_game):\n            self.level = -1\n            self._customize_game()\n\n        else:\n            # Loading Paths\n            if not self.is_initialized:\n                self.game.set_vizdoom_path(self.loader.get_vizdoom_path())\n                self.game.set_doom_game_path(self.loader.get_freedoom_path())\n\n            # Common settings\n            self.game.load_config(os.path.join(self.doom_dir, \'assets/%s\' % DOOM_SETTINGS[self.level][CONFIG]))\n            self.game.set_doom_scenario_path(self.loader.get_scenario_path(DOOM_SETTINGS[self.level][SCENARIO]))\n            if DOOM_SETTINGS[self.level][MAP] != \'\':\n                if RANDOMIZE_MAPS > 0 and \'labyrinth\' in DOOM_SETTINGS[self.level][CONFIG].lower():\n                    if \'fix\' in DOOM_SETTINGS[self.level][SCENARIO].lower():\n                        # mapId = \'map%02d\'%np.random.randint(1, 23)\n                        mapId = \'map%02d\'%np.random.randint(4, 8)\n                    else:\n                        mapId = \'map%02d\'%np.random.randint(1, RANDOMIZE_MAPS+1)\n                    print(\'\\t=> Special Config: Randomly Loading Maps. MapID = \' + mapId)\n                    self.game.set_doom_map(mapId)\n                else:\n                    print(\'\\t=> Default map loaded. MapID = \' + DOOM_SETTINGS[self.level][MAP])\n                    self.game.set_doom_map(DOOM_SETTINGS[self.level][MAP])\n            self.game.set_doom_skill(DOOM_SETTINGS[self.level][DIFFICULTY])\n            self.allowed_actions = DOOM_SETTINGS[self.level][ACTIONS]\n            self.game.set_screen_resolution(self.screen_resolution)\n\n        self.previous_level = self.level\n        self._closed = False\n\n        # Algo mode\n        if \'human\' != self._mode:\n            if NO_MONSTERS:\n                print(\'\\t=> Special Config: Monsters Removed.\')\n                self.game.add_game_args(\'-nomonsters 1\')\n            self.game\n            self.game.set_window_visible(False)\n            self.game.set_mode(Mode.PLAYER)\n            self.no_render = False\n            try:\n                with self.lock:\n                    self.game.init()\n            except (ViZDoomUnexpectedExitException, ViZDoomErrorException):\n                raise error.Error(\n                    \'VizDoom exited unexpectedly. This is likely caused by a missing multiprocessing lock. \' +\n                    \'To run VizDoom across multiple processes, you need to pass a lock when you configure the env \' +\n                    \'[e.g. env.configure(lock=my_multiprocessing_lock)], or create and close an env \' +\n                    \'before starting your processes [e.g. env = gym.make(""DoomBasic-v0""); env.close()] to cache a \' +\n                    \'singleton lock in memory.\')\n            self._start_episode()\n            self.is_initialized = True\n            return self.game.get_state().image_buffer.copy()\n\n        # Human mode\n        else:\n            if NO_MONSTERS:\n                print(\'\\t=> Special Config: Monsters Removed.\')\n                self.game.add_game_args(\'-nomonsters 1\')\n            self.game.add_game_args(\'+freelook 1\')\n            self.game.set_window_visible(True)\n            self.game.set_mode(Mode.SPECTATOR)\n            self.no_render = True\n            with self.lock:\n                self.game.init()\n            self._start_episode()\n            self.is_initialized = True\n            self._play_human_mode()\n            return np.zeros(shape=self.observation_space.shape, dtype=np.uint8)\n\n    def _start_episode(self):\n        if self.curr_seed > 0:\n            self.game.set_seed(self.curr_seed)\n            self.curr_seed = 0\n        self.game.new_episode()\n        return\n\n    def _play_human_mode(self):\n        while not self.game.is_episode_finished():\n            self.game.advance_action()\n            state = self.game.get_state()\n            total_reward = self.game.get_total_reward()\n            info = self._get_game_variables(state.game_variables)\n            info[""TOTAL_REWARD""] = round(total_reward, 4)\n            print(\'===============================\')\n            print(\'State: #\' + str(state.number))\n            print(\'Action: \\t\' + str(self.game.get_last_action()) + \'\\t (=> only allowed actions)\')\n            print(\'Reward: \\t\' + str(self.game.get_last_reward()))\n            print(\'Total Reward: \\t\' + str(total_reward))\n            print(\'Variables: \\n\' + str(info))\n            sleep(0.02857)  # 35 fps = 0.02857 sleep between frames\n        print(\'===============================\')\n        print(\'Done\')\n        return\n\n    def _step(self, action):\n        if NUM_ACTIONS != len(action):\n            logger.warn(\'Doom action list must contain %d items. Padding missing items with 0\' % NUM_ACTIONS)\n            old_action = action\n            action = [0] * NUM_ACTIONS\n            for i in range(len(old_action)):\n                action[i] = old_action[i]\n        # action is a list of numbers but DoomGame.make_action expects a list of ints\n        if len(self.allowed_actions) > 0:\n            list_action = [int(action[action_idx]) for action_idx in self.allowed_actions]\n        else:\n            list_action = [int(x) for x in action]\n        try:\n            reward = self.game.make_action(list_action)\n            state = self.game.get_state()\n            info = self._get_game_variables(state.game_variables)\n            info[""TOTAL_REWARD""] = round(self.game.get_total_reward(), 4)\n\n            if self.game.is_episode_finished():\n                is_finished = True\n                return np.zeros(shape=self.observation_space.shape, dtype=np.uint8), reward, is_finished, info\n            else:\n                is_finished = False\n                return state.image_buffer.copy(), reward, is_finished, info\n\n        except doom_py.vizdoom.ViZDoomIsNotRunningException:\n            return np.zeros(shape=self.observation_space.shape, dtype=np.uint8), 0, True, {}\n\n    def _reset(self):\n        if self.is_initialized and not self._closed:\n            self._start_episode()\n            image_buffer = self.game.get_state().image_buffer\n            if image_buffer is None:\n                raise error.Error(\n                    \'VizDoom incorrectly initiated. This is likely caused by a missing multiprocessing lock. \' +\n                    \'To run VizDoom across multiple processes, you need to pass a lock when you configure the env \' +\n                    \'[e.g. env.configure(lock=my_multiprocessing_lock)], or create and close an env \' +\n                    \'before starting your processes [e.g. env = gym.make(""DoomBasic-v0""); env.close()] to cache a \' +\n                    \'singleton lock in memory.\')\n            return image_buffer.copy()\n        else:\n            return self._load_level()\n\n    def _render(self, mode=\'human\', close=False):\n        if close:\n            if self.viewer is not None:\n                self.viewer.close()\n                self.viewer = None      # If we don\'t None out this reference pyglet becomes unhappy\n            return\n        try:\n            if \'human\' == mode and self.no_render:\n                return\n            state = self.game.get_state()\n            img = state.image_buffer\n            # VizDoom returns None if the episode is finished, let\'s make it\n            # an empty image so the recorder doesn\'t stop\n            if img is None:\n                img = np.zeros(shape=self.observation_space.shape, dtype=np.uint8)\n            if mode == \'rgb_array\':\n                return img\n            elif mode is \'human\':\n                from gym.envs.classic_control import rendering\n                if self.viewer is None:\n                    self.viewer = rendering.SimpleImageViewer()\n                self.viewer.imshow(img)\n        except doom_py.vizdoom.ViZDoomIsNotRunningException:\n            pass  # Doom has been closed\n\n    def _close(self):\n        # Lock required for VizDoom to close processes properly\n        with self.lock:\n            self.game.close()\n\n    def _seed(self, seed=None):\n        self.curr_seed = seeding.hash_seed(seed) % 2 ** 32\n        return [self.curr_seed]\n\n    def _get_game_variables(self, state_variables):\n        info = {\n            ""LEVEL"": self.level\n        }\n        if state_variables is None:\n            return info\n        info[\'KILLCOUNT\'] = state_variables[0]\n        info[\'ITEMCOUNT\'] = state_variables[1]\n        info[\'SECRETCOUNT\'] = state_variables[2]\n        info[\'FRAGCOUNT\'] = state_variables[3]\n        info[\'HEALTH\'] = state_variables[4]\n        info[\'ARMOR\'] = state_variables[5]\n        info[\'DEAD\'] = state_variables[6]\n        info[\'ON_GROUND\'] = state_variables[7]\n        info[\'ATTACK_READY\'] = state_variables[8]\n        info[\'ALTATTACK_READY\'] = state_variables[9]\n        info[\'SELECTED_WEAPON\'] = state_variables[10]\n        info[\'SELECTED_WEAPON_AMMO\'] = state_variables[11]\n        info[\'AMMO1\'] = state_variables[12]\n        info[\'AMMO2\'] = state_variables[13]\n        info[\'AMMO3\'] = state_variables[14]\n        info[\'AMMO4\'] = state_variables[15]\n        info[\'AMMO5\'] = state_variables[16]\n        info[\'AMMO6\'] = state_variables[17]\n        info[\'AMMO7\'] = state_variables[18]\n        info[\'AMMO8\'] = state_variables[19]\n        info[\'AMMO9\'] = state_variables[20]\n        info[\'AMMO0\'] = state_variables[21]\n        info[\'POSITION_X\'] = doom_fixed_to_double(self.game.get_game_variable(GameVariable.USER1))\n        info[\'POSITION_Y\'] = doom_fixed_to_double(self.game.get_game_variable(GameVariable.USER2))\n        return info\n\n\nclass MetaDoomEnv(DoomEnv):\n\n    def __init__(self, average_over=10, passing_grade=600, min_tries_for_avg=5):\n        super(MetaDoomEnv, self).__init__(0)\n        self.average_over = average_over\n        self.passing_grade = passing_grade\n        self.min_tries_for_avg = min_tries_for_avg              # Need to use at least this number of tries to calc avg\n        self.scores = [[]] * NUM_LEVELS\n        self.locked_levels = [True] * NUM_LEVELS                # Locking all levels but the first\n        self.locked_levels[0] = False\n        self.total_reward = 0\n        self.find_new_level = False                             # Indicates that we need a level change\n        self._unlock_levels()\n\n    def _play_human_mode(self):\n        while not self.game.is_episode_finished():\n            self.game.advance_action()\n            state = self.game.get_state()\n            episode_reward = self.game.get_total_reward()\n            (reward, self.total_reward) = self._calculate_reward(episode_reward, self.total_reward)\n            info = self._get_game_variables(state.game_variables)\n            info[""SCORES""] = self.get_scores()\n            info[""TOTAL_REWARD""] = round(self.total_reward, 4)\n            info[""LOCKED_LEVELS""] = self.locked_levels\n            print(\'===============================\')\n            print(\'State: #\' + str(state.number))\n            print(\'Action: \\t\' + str(self.game.get_last_action()) + \'\\t (=> only allowed actions)\')\n            print(\'Reward: \\t\' + str(reward))\n            print(\'Total Reward: \\t\' + str(self.total_reward))\n            print(\'Variables: \\n\' + str(info))\n            sleep(0.02857)  # 35 fps = 0.02857 sleep between frames\n        print(\'===============================\')\n        print(\'Done\')\n        return\n\n    def _get_next_level(self):\n        # Finds the unlocked level with the lowest average\n        averages = self.get_scores()\n        lowest_level = 0                          # Defaulting to first level\n        lowest_score = 1001\n        for i in range(NUM_LEVELS):\n            if not self.locked_levels[i]:\n                if averages[i] < lowest_score:\n                    lowest_level = i\n                    lowest_score = averages[i]\n        return lowest_level\n\n    def _unlock_levels(self):\n        averages = self.get_scores()\n        for i in range(NUM_LEVELS - 2, -1, -1):\n            if self.locked_levels[i + 1] and averages[i] >= self.passing_grade:\n                self.locked_levels[i + 1] = False\n        return\n\n    def _start_episode(self):\n        if 0 == len(self.scores[self.level]):\n            self.scores[self.level] = [0] * self.min_tries_for_avg\n        else:\n            self.scores[self.level].insert(0, 0)\n            self.scores[self.level] = self.scores[self.level][:self.min_tries_for_avg]\n        self.is_new_episode = True\n        return super(MetaDoomEnv, self)._start_episode()\n\n    def change_level(self, new_level=None):\n        if new_level is not None and self.locked_levels[new_level] == False:\n            self.find_new_level = False\n            self.level = new_level\n            self.reset()\n        else:\n            self.find_new_level = False\n            self.level = self._get_next_level()\n            self.reset()\n        return\n\n    def _get_standard_reward(self, episode_reward):\n        # Returns a standardized reward for an episode (i.e. between 0 and 1,000)\n        min_score = float(DOOM_SETTINGS[self.level][MIN_SCORE])\n        target_score = float(DOOM_SETTINGS[self.level][TARGET_SCORE])\n        max_score = min_score + (target_score - min_score) / 0.99           # Target is 99th percentile (Scale 0-1000)\n        std_reward = round(1000 * (episode_reward - min_score) / (max_score - min_score), 4)\n        std_reward = min(1000, std_reward)                                  # Cannot be more than 1,000\n        std_reward = max(0, std_reward)                                     # Cannot be less than 0\n        return std_reward\n\n    def get_total_reward(self):\n        # Returns the sum of the average of all levels\n        total_score = 0\n        passed_levels = 0\n        for i in range(NUM_LEVELS):\n            if len(self.scores[i]) > 0:\n                level_total = 0\n                level_count = min(len(self.scores[i]), self.average_over)\n                for j in range(level_count):\n                    level_total += self.scores[i][j]\n                level_average = level_total / level_count\n                if level_average >= 990:\n                    passed_levels += 1\n                total_score += level_average\n        # Bonus for passing all levels (50 * num of levels)\n        if NUM_LEVELS == passed_levels:\n            total_score += NUM_LEVELS * 50\n        return round(total_score, 4)\n\n    def _calculate_reward(self, episode_reward, prev_total_reward):\n        # Calculates the action reward and the new total reward\n        std_reward = self._get_standard_reward(episode_reward)\n        self.scores[self.level][0] = std_reward\n        total_reward = self.get_total_reward()\n        reward = total_reward - prev_total_reward\n        return reward, total_reward\n\n    def get_scores(self):\n        # Returns a list with the averages per level\n        averages = [0] * NUM_LEVELS\n        for i in range(NUM_LEVELS):\n            if len(self.scores[i]) > 0:\n                level_total = 0\n                level_count = min(len(self.scores[i]), self.average_over)\n                for j in range(level_count):\n                    level_total += self.scores[i][j]\n                level_average = level_total / level_count\n                averages[i] = round(level_average, 4)\n        return averages\n\n    def _reset(self):\n        # Reset is called on first step() after level is finished\n        # or when change_level() is called. Returning if neither have been called to\n        # avoid resetting the level twice\n        if self.find_new_level:\n            return\n\n        if self.is_initialized and not self._closed and self.previous_level == self.level:\n            self._start_episode()\n            return self.game.get_state().image_buffer.copy()\n        else:\n            return self._load_level()\n\n    def _step(self, action):\n        # Changing level\n        if self.find_new_level:\n            self.change_level()\n\n        if \'human\' == self._mode:\n            self._play_human_mode()\n            obs = np.zeros(shape=self.observation_space.shape, dtype=np.uint8)\n            reward = 0\n            is_finished = True\n            info = self._get_game_variables(None)\n        else:\n            obs, step_reward, is_finished, info = super(MetaDoomEnv, self)._step(action)\n            reward, self.total_reward = self._calculate_reward(self.game.get_total_reward(), self.total_reward)\n            # First step() after new episode returns the entire total reward\n            # because stats_recorder resets the episode score to 0 after reset() is called\n            if self.is_new_episode:\n                reward = self.total_reward\n\n        self.is_new_episode = False\n        info[""SCORES""] = self.get_scores()\n        info[""TOTAL_REWARD""] = round(self.total_reward, 4)\n        info[""LOCKED_LEVELS""] = self.locked_levels\n\n        # Indicating new level required\n        if is_finished:\n            self._unlock_levels()\n            self.find_new_level = True\n\n        return obs, reward, is_finished, info\n'"
doomFiles/doom_my_way_home_sparse.py,0,"b'import logging\nfrom .doom_env import DoomEnv\n\nlogger = logging.getLogger(__name__)\n\n\nclass DoomMyWayHomeFixedEnv(DoomEnv):\n    """"""\n    ------------ Training Mission 10 - My Way Home Fixed ------------\n    Exactly same as Mission#6, but with fixed start from room-10 (farthest).\n    This map is designed to improve navigational skills. It is a series of\n    interconnected rooms and 1 corridor with a dead end. Each room\n    has a separate color. There is a green vest in one of the room.\n    The vest is always in the same room. Player must find the vest.\n\n    Allowed actions:\n        [13] - MOVE_FORWARD                     - Move forward - Values 0 or 1\n        [14] - TURN_RIGHT                       - Turn right - Values 0 or 1\n        [15] - TURN_LEFT                        - Turn left - Values 0 or 1\n    Note: see controls.md for details\n\n    Rewards:\n        +  1    - Finding the vest\n        -0.0001 - 35 times per second - Find the vest quick!\n\n    Goal: 0.50 point\n        Find the vest\n\n    Ends when:\n        - Vest is found\n        - Timeout (1 minutes - 2,100 frames)\n\n    Actions:\n        actions = [0] * 43\n        actions[13] = 0      # MOVE_FORWARD\n        actions[14] = 1      # TURN_RIGHT\n        actions[15] = 0      # TURN_LEFT\n\n    Configuration:\n        After creating the env, you can call env.configure() to configure some parameters.\n\n        - lock [e.g. env.configure(lock=multiprocessing_lock)]\n\n            VizDoom requires a multiprocessing lock when running across multiple processes, otherwise the vizdoom instance\n            might crash on launch\n\n            You can either:\n\n            1) [Preferred] Create a multiprocessing.Lock() and pass it as a parameter to the configure() method\n                [e.g. env.configure(lock=multiprocessing_lock)]\n\n            2) Create and close a Doom environment before running your multiprocessing routine, this will create\n                a singleton lock that will be cached in memory, and be used by all Doom environments afterwards\n                [e.g. env = gym.make(\'Doom-...\'); env.close()]\n\n            3) Manually wrap calls to reset() and close() in a multiprocessing.Lock()\n\n    Wrappers:\n\n        You can use wrappers to further customize the environment. Wrappers need to be manually copied from the wrappers folder.\n\n            theWrapperOne = WrapperOneName(init_options)\n            theWrapperTwo = WrapperTwoName(init_options)\n            env = gym.make(\'ppaquette/DoomMyWayHome-v0\')\n            env = theWrapperTwo(theWrapperOne((env))\n\n        - Observation space:\n\n            You can change the resolution by using the SetResolution wrapper.\n\n                wrapper = SetResolution(target_resolution)\n                env = wrapper(env)\n\n            The following are valid target_resolution that can be used:\n\n                \'160x120\', \'200x125\', \'200x150\', \'256x144\', \'256x160\', \'256x192\', \'320x180\', \'320x200\',\n                \'320x240\', \'320x256\', \'400x225\', \'400x250\', \'400x300\', \'512x288\', \'512x320\', \'512x384\',\n                \'640x360\', \'640x400\', \'640x480\', \'800x450\', \'800x500\', \'800x600\', \'1024x576\', \'1024x640\',\n                \'1024x768\', \'1280x720\', \'1280x800\', \'1280x960\', \'1280x1024\', \'1400x787\', \'1400x875\',\n                \'1400x1050\', \'1600x900\', \'1600x1000\', \'1600x1200\', \'1920x1080\'\n\n        - Action space:\n\n            You can change the action space by using the ToDiscrete or ToBox wrapper\n\n                wrapper = ToBox(config_options)\n                env = wrapper(env)\n\n            The following are valid config options (for both ToDiscrete and ToBox)\n\n                - minimal       - Only the level\'s allowed actions (and NOOP for discrete)\n                - constant-7    - 7 minimum actions required to complete all levels (and NOOP for discrete)\n                - constant-17   - 17 most common actions required to complete all levels (and NOOP for discrete)\n                - full          - All available actions (and NOOP for discrete)\n\n            Note: Discrete action spaces only allow one action at a time, Box action spaces support simultaneous actions\n\n        - Control:\n\n            You can play the game manually with the SetPlayingMode wrapper.\n\n                wrapper = SetPlayingMode(\'human\')\n                env = wrapper(env)\n\n            Valid options are \'human\' or \'algo\' (default)\n\n    -----------------------------------------------------\n    """"""\n    def __init__(self):\n        super(DoomMyWayHomeFixedEnv, self).__init__(9)\n'"
doomFiles/doom_my_way_home_verySparse.py,0,"b'import logging\nfrom .doom_env import DoomEnv\n\nlogger = logging.getLogger(__name__)\n\n\nclass DoomMyWayHomeFixed15Env(DoomEnv):\n    """"""\n    ------------ Training Mission 11 - My Way Home Fixed15 ------------\n    Exactly same as Mission#6, but with fixed start from room-10 (farthest).\n    This map is designed to improve navigational skills. It is a series of\n    interconnected rooms and 1 corridor with a dead end. Each room\n    has a separate color. There is a green vest in one of the room.\n    The vest is always in the same room. Player must find the vest.\n\n    Allowed actions:\n        [13] - MOVE_FORWARD                     - Move forward - Values 0 or 1\n        [14] - TURN_RIGHT                       - Turn right - Values 0 or 1\n        [15] - TURN_LEFT                        - Turn left - Values 0 or 1\n    Note: see controls.md for details\n\n    Rewards:\n        +  1    - Finding the vest\n        -0.0001 - 35 times per second - Find the vest quick!\n\n    Goal: 0.50 point\n        Find the vest\n\n    Ends when:\n        - Vest is found\n        - Timeout (1 minutes - 2,100 frames)\n\n    Actions:\n        actions = [0] * 43\n        actions[13] = 0      # MOVE_FORWARD\n        actions[14] = 1      # TURN_RIGHT\n        actions[15] = 0      # TURN_LEFT\n\n    Configuration:\n        After creating the env, you can call env.configure() to configure some parameters.\n\n        - lock [e.g. env.configure(lock=multiprocessing_lock)]\n\n            VizDoom requires a multiprocessing lock when running across multiple processes, otherwise the vizdoom instance\n            might crash on launch\n\n            You can either:\n\n            1) [Preferred] Create a multiprocessing.Lock() and pass it as a parameter to the configure() method\n                [e.g. env.configure(lock=multiprocessing_lock)]\n\n            2) Create and close a Doom environment before running your multiprocessing routine, this will create\n                a singleton lock that will be cached in memory, and be used by all Doom environments afterwards\n                [e.g. env = gym.make(\'Doom-...\'); env.close()]\n\n            3) Manually wrap calls to reset() and close() in a multiprocessing.Lock()\n\n    Wrappers:\n\n        You can use wrappers to further customize the environment. Wrappers need to be manually copied from the wrappers folder.\n\n            theWrapperOne = WrapperOneName(init_options)\n            theWrapperTwo = WrapperTwoName(init_options)\n            env = gym.make(\'ppaquette/DoomMyWayHome-v0\')\n            env = theWrapperTwo(theWrapperOne((env))\n\n        - Observation space:\n\n            You can change the resolution by using the SetResolution wrapper.\n\n                wrapper = SetResolution(target_resolution)\n                env = wrapper(env)\n\n            The following are valid target_resolution that can be used:\n\n                \'160x120\', \'200x125\', \'200x150\', \'256x144\', \'256x160\', \'256x192\', \'320x180\', \'320x200\',\n                \'320x240\', \'320x256\', \'400x225\', \'400x250\', \'400x300\', \'512x288\', \'512x320\', \'512x384\',\n                \'640x360\', \'640x400\', \'640x480\', \'800x450\', \'800x500\', \'800x600\', \'1024x576\', \'1024x640\',\n                \'1024x768\', \'1280x720\', \'1280x800\', \'1280x960\', \'1280x1024\', \'1400x787\', \'1400x875\',\n                \'1400x1050\', \'1600x900\', \'1600x1000\', \'1600x1200\', \'1920x1080\'\n\n        - Action space:\n\n            You can change the action space by using the ToDiscrete or ToBox wrapper\n\n                wrapper = ToBox(config_options)\n                env = wrapper(env)\n\n            The following are valid config options (for both ToDiscrete and ToBox)\n\n                - minimal       - Only the level\'s allowed actions (and NOOP for discrete)\n                - constant-7    - 7 minimum actions required to complete all levels (and NOOP for discrete)\n                - constant-17   - 17 most common actions required to complete all levels (and NOOP for discrete)\n                - full          - All available actions (and NOOP for discrete)\n\n            Note: Discrete action spaces only allow one action at a time, Box action spaces support simultaneous actions\n\n        - Control:\n\n            You can play the game manually with the SetPlayingMode wrapper.\n\n                wrapper = SetPlayingMode(\'human\')\n                env = wrapper(env)\n\n            Valid options are \'human\' or \'algo\' (default)\n\n    -----------------------------------------------------\n    """"""\n    def __init__(self):\n        super(DoomMyWayHomeFixed15Env, self).__init__(10)\n'"
src/a3c.py,52,"b'from __future__ import print_function\nfrom collections import namedtuple\nimport numpy as np\nimport tensorflow as tf\nfrom model import LSTMPolicy, StateActionPredictor, StatePredictor\nimport six.moves.queue as queue\nimport scipy.signal\nimport threading\nimport distutils.version\nfrom constants import constants\nuse_tf12_api = distutils.version.LooseVersion(tf.VERSION) >= distutils.version.LooseVersion(\'0.12.0\')\n\ndef discount(x, gamma):\n    """"""\n        x = [r1, r2, r3, ..., rN]\n        returns [r1 + r2*gamma + r3*gamma^2 + ...,\n                   r2 + r3*gamma + r4*gamma^2 + ...,\n                     r3 + r4*gamma + r5*gamma^2 + ...,\n                        ..., ..., rN]\n    """"""\n    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n\ndef process_rollout(rollout, gamma, lambda_=1.0, clip=False):\n    """"""\n    Given a rollout, compute its returns and the advantage.\n    """"""\n    # collecting transitions\n    if rollout.unsup:\n        batch_si = np.asarray(rollout.states + [rollout.end_state])\n    else:\n        batch_si = np.asarray(rollout.states)\n    batch_a = np.asarray(rollout.actions)\n\n    # collecting target for value network\n    # V_t <-> r_t + gamma*r_{t+1} + ... + gamma^n*r_{t+n} + gamma^{n+1}*V_{n+1}\n    rewards_plus_v = np.asarray(rollout.rewards + [rollout.r])  # bootstrapping\n    if rollout.unsup:\n        rewards_plus_v += np.asarray(rollout.bonuses + [0])\n    if clip:\n        rewards_plus_v[:-1] = np.clip(rewards_plus_v[:-1], -constants[\'REWARD_CLIP\'], constants[\'REWARD_CLIP\'])\n    batch_r = discount(rewards_plus_v, gamma)[:-1]  # value network target\n\n    # collecting target for policy network\n    rewards = np.asarray(rollout.rewards)\n    if rollout.unsup:\n        rewards += np.asarray(rollout.bonuses)\n    if clip:\n        rewards = np.clip(rewards, -constants[\'REWARD_CLIP\'], constants[\'REWARD_CLIP\'])\n    vpred_t = np.asarray(rollout.values + [rollout.r])\n    # ""Generalized Advantage Estimation"": https://arxiv.org/abs/1506.02438\n    # Eq (10): delta_t = Rt + gamma*V_{t+1} - V_t\n    # Eq (16): batch_adv_t = delta_t + gamma*delta_{t+1} + gamma^2*delta_{t+2} + ...\n    delta_t = rewards + gamma * vpred_t[1:] - vpred_t[:-1]\n    batch_adv = discount(delta_t, gamma * lambda_)\n\n    features = rollout.features[0]\n\n    return Batch(batch_si, batch_a, batch_adv, batch_r, rollout.terminal, features)\n\nBatch = namedtuple(""Batch"", [""si"", ""a"", ""adv"", ""r"", ""terminal"", ""features""])\n\nclass PartialRollout(object):\n    """"""\n    A piece of a complete rollout.  We run our agent, and process its experience\n    once it has processed enough steps.\n    """"""\n    def __init__(self, unsup=False):\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.values = []\n        self.r = 0.0\n        self.terminal = False\n        self.features = []\n        self.unsup = unsup\n        if self.unsup:\n            self.bonuses = []\n            self.end_state = None\n\n\n    def add(self, state, action, reward, value, terminal, features,\n                bonus=None, end_state=None):\n        self.states += [state]\n        self.actions += [action]\n        self.rewards += [reward]\n        self.values += [value]\n        self.terminal = terminal\n        self.features += [features]\n        if self.unsup:\n            self.bonuses += [bonus]\n            self.end_state = end_state\n\n    def extend(self, other):\n        assert not self.terminal\n        self.states.extend(other.states)\n        self.actions.extend(other.actions)\n        self.rewards.extend(other.rewards)\n        self.values.extend(other.values)\n        self.r = other.r\n        self.terminal = other.terminal\n        self.features.extend(other.features)\n        if self.unsup:\n            self.bonuses.extend(other.bonuses)\n            self.end_state = other.end_state\n\nclass RunnerThread(threading.Thread):\n    """"""\n    One of the key distinctions between a normal environment and a universe environment\n    is that a universe environment is _real time_.  This means that there should be a thread\n    that would constantly interact with the environment and tell it what to do.  This thread is here.\n    """"""\n    def __init__(self, env, policy, num_local_steps, visualise, predictor, envWrap,\n                    noReward):\n        threading.Thread.__init__(self)\n        self.queue = queue.Queue(5)  # ideally, should be 1. Mostly doesn\'t matter in our case.\n        self.num_local_steps = num_local_steps\n        self.env = env\n        self.last_features = None\n        self.policy = policy\n        self.daemon = True\n        self.sess = None\n        self.summary_writer = None\n        self.visualise = visualise\n        self.predictor = predictor\n        self.envWrap = envWrap\n        self.noReward = noReward\n\n    def start_runner(self, sess, summary_writer):\n        self.sess = sess\n        self.summary_writer = summary_writer\n        self.start()\n\n    def run(self):\n        with self.sess.as_default():\n            self._run()\n\n    def _run(self):\n        rollout_provider = env_runner(self.env, self.policy, self.num_local_steps,\n                                        self.summary_writer, self.visualise, self.predictor,\n                                        self.envWrap, self.noReward)\n        while True:\n            # the timeout variable exists because apparently, if one worker dies, the other workers\n            # won\'t die with it, unless the timeout is set to some large number.  This is an empirical\n            # observation.\n\n            self.queue.put(next(rollout_provider), timeout=600.0)\n\n\ndef env_runner(env, policy, num_local_steps, summary_writer, render, predictor,\n                envWrap, noReward):\n    """"""\n    The logic of the thread runner.  In brief, it constantly keeps on running\n    the policy, and as long as the rollout exceeds a certain length, the thread\n    runner appends the policy to the queue.\n    """"""\n    last_state = env.reset()\n    last_features = policy.get_initial_features()  # reset lstm memory\n    length = 0\n    rewards = 0\n    values = 0\n    if predictor is not None:\n        ep_bonus = 0\n        life_bonus = 0\n\n    while True:\n        terminal_end = False\n        rollout = PartialRollout(predictor is not None)\n\n        for _ in range(num_local_steps):\n            # run policy\n            fetched = policy.act(last_state, *last_features)\n            action, value_, features = fetched[0], fetched[1], fetched[2:]\n\n            # run environment: get action_index from sampled one-hot \'action\'\n            stepAct = action.argmax()\n            state, reward, terminal, info = env.step(stepAct)\n            if noReward:\n                reward = 0.\n            if render:\n                env.render()\n\n            curr_tuple = [last_state, action, reward, value_, terminal, last_features]\n            if predictor is not None:\n                bonus = predictor.pred_bonus(last_state, state, action)\n                curr_tuple += [bonus, state]\n                life_bonus += bonus\n                ep_bonus += bonus\n\n            # collect the experience\n            rollout.add(*curr_tuple)\n            rewards += reward\n            length += 1\n            values += value_[0]\n\n            last_state = state\n            last_features = features\n\n            timestep_limit = env.spec.tags.get(\'wrapper_config.TimeLimit.max_episode_steps\')\n            if timestep_limit is None: timestep_limit = env.spec.timestep_limit\n            if terminal or length >= timestep_limit:\n                # prints summary of each life if envWrap==True else each game\n                if predictor is not None:\n                    print(""Episode finished. Sum of shaped rewards: %.2f. Length: %d. Bonus: %.4f."" % (rewards, length, life_bonus))\n                    life_bonus = 0\n                else:\n                    print(""Episode finished. Sum of shaped rewards: %.2f. Length: %d."" % (rewards, length))\n                if \'distance\' in info: print(\'Mario Distance Covered:\', info[\'distance\'])\n                length = 0\n                rewards = 0\n                terminal_end = True\n                last_features = policy.get_initial_features()  # reset lstm memory\n                # TODO: don\'t reset when gym timestep_limit increases, bootstrap -- doesn\'t matter for atari?\n                # reset only if it hasn\'t already reseted\n                if length >= timestep_limit or not env.metadata.get(\'semantics.autoreset\'):\n                    last_state = env.reset()\n\n            if info:\n                # summarize full game including all lives (even if envWrap=True)\n                summary = tf.Summary()\n                for k, v in info.items():\n                    summary.value.add(tag=k, simple_value=float(v))\n                if terminal:\n                    summary.value.add(tag=\'global/episode_value\', simple_value=float(values))\n                    values = 0\n                    if predictor is not None:\n                        summary.value.add(tag=\'global/episode_bonus\', simple_value=float(ep_bonus))\n                        ep_bonus = 0\n                summary_writer.add_summary(summary, policy.global_step.eval())\n                summary_writer.flush()\n\n            if terminal_end:\n                break\n\n        if not terminal_end:\n            rollout.r = policy.value(last_state, *last_features)\n\n        # once we have enough experience, yield it, and have the ThreadRunner place it on a queue\n        yield rollout\n\n\nclass A3C(object):\n    def __init__(self, env, task, visualise, unsupType, envWrap=False, designHead=\'universe\', noReward=False):\n        """"""\n        An implementation of the A3C algorithm that is reasonably well-tuned for the VNC environments.\n        Below, we will have a modest amount of complexity due to the way TensorFlow handles data parallelism.\n        But overall, we\'ll define the model, specify its inputs, and describe how the policy gradients step\n        should be computed.\n        """"""\n        self.task = task\n        self.unsup = unsupType is not None\n        self.envWrap = envWrap\n        self.env = env\n\n        predictor = None\n        numaction = env.action_space.n\n        worker_device = ""/job:worker/task:{}/cpu:0"".format(task)\n\n        with tf.device(tf.train.replica_device_setter(1, worker_device=worker_device)):\n            with tf.variable_scope(""global""):\n                self.network = LSTMPolicy(env.observation_space.shape, numaction, designHead)\n                self.global_step = tf.get_variable(""global_step"", [], tf.int32, initializer=tf.constant_initializer(0, dtype=tf.int32),\n                                                   trainable=False)\n                if self.unsup:\n                    with tf.variable_scope(""predictor""):\n                        if \'state\' in unsupType:\n                            self.ap_network = StatePredictor(env.observation_space.shape, numaction, designHead, unsupType)\n                        else:\n                            self.ap_network = StateActionPredictor(env.observation_space.shape, numaction, designHead)\n\n        with tf.device(worker_device):\n            with tf.variable_scope(""local""):\n                self.local_network = pi = LSTMPolicy(env.observation_space.shape, numaction, designHead)\n                pi.global_step = self.global_step\n                if self.unsup:\n                    with tf.variable_scope(""predictor""):\n                        if \'state\' in unsupType:\n                            self.local_ap_network = predictor = StatePredictor(env.observation_space.shape, numaction, designHead, unsupType)\n                        else:\n                            self.local_ap_network = predictor = StateActionPredictor(env.observation_space.shape, numaction, designHead)\n\n            # Computing a3c loss: https://arxiv.org/abs/1506.02438\n            self.ac = tf.placeholder(tf.float32, [None, numaction], name=""ac"")\n            self.adv = tf.placeholder(tf.float32, [None], name=""adv"")\n            self.r = tf.placeholder(tf.float32, [None], name=""r"")\n            log_prob_tf = tf.nn.log_softmax(pi.logits)\n            prob_tf = tf.nn.softmax(pi.logits)\n            # 1) the ""policy gradients"" loss:  its derivative is precisely the policy gradient\n            # notice that self.ac is a placeholder that is provided externally.\n            # adv will contain the advantages, as calculated in process_rollout\n            pi_loss = - tf.reduce_mean(tf.reduce_sum(log_prob_tf * self.ac, 1) * self.adv)  # Eq (19)\n            # 2) loss of value function: l2_loss = (x-y)^2/2\n            vf_loss = 0.5 * tf.reduce_mean(tf.square(pi.vf - self.r))  # Eq (28)\n            # 3) entropy to ensure randomness\n            entropy = - tf.reduce_mean(tf.reduce_sum(prob_tf * log_prob_tf, 1))\n            # final a3c loss: lr of critic is half of actor\n            self.loss = pi_loss + 0.5 * vf_loss - entropy * constants[\'ENTROPY_BETA\']\n\n            # compute gradients\n            grads = tf.gradients(self.loss * 20.0, pi.var_list)  # batchsize=20. Factored out to make hyperparams not depend on it.\n\n            # computing predictor loss\n            if self.unsup:\n                if \'state\' in unsupType:\n                    self.predloss = constants[\'PREDICTION_LR_SCALE\'] * predictor.forwardloss\n                else:\n                    self.predloss = constants[\'PREDICTION_LR_SCALE\'] * (predictor.invloss * (1-constants[\'FORWARD_LOSS_WT\']) +\n                                                                    predictor.forwardloss * constants[\'FORWARD_LOSS_WT\'])\n                predgrads = tf.gradients(self.predloss * 20.0, predictor.var_list)  # batchsize=20. Factored out to make hyperparams not depend on it.\n\n                # do not backprop to policy\n                if constants[\'POLICY_NO_BACKPROP_STEPS\'] > 0:\n                    grads = [tf.scalar_mul(tf.to_float(tf.greater(self.global_step, constants[\'POLICY_NO_BACKPROP_STEPS\'])), grads_i)\n                                    for grads_i in grads]\n\n\n            self.runner = RunnerThread(env, pi, constants[\'ROLLOUT_MAXLEN\'], visualise,\n                                        predictor, envWrap, noReward)\n\n            # storing summaries\n            bs = tf.to_float(tf.shape(pi.x)[0])\n            if use_tf12_api:\n                tf.summary.scalar(""model/policy_loss"", pi_loss)\n                tf.summary.scalar(""model/value_loss"", vf_loss)\n                tf.summary.scalar(""model/entropy"", entropy)\n                tf.summary.image(""model/state"", pi.x)  # max_outputs=10\n                tf.summary.scalar(""model/grad_global_norm"", tf.global_norm(grads))\n                tf.summary.scalar(""model/var_global_norm"", tf.global_norm(pi.var_list))\n                if self.unsup:\n                    tf.summary.scalar(""model/predloss"", self.predloss)\n                    if \'action\' in unsupType:\n                        tf.summary.scalar(""model/inv_loss"", predictor.invloss)\n                        tf.summary.scalar(""model/forward_loss"", predictor.forwardloss)\n                    tf.summary.scalar(""model/predgrad_global_norm"", tf.global_norm(predgrads))\n                    tf.summary.scalar(""model/predvar_global_norm"", tf.global_norm(predictor.var_list))\n                self.summary_op = tf.summary.merge_all()\n            else:\n                tf.scalar_summary(""model/policy_loss"", pi_loss)\n                tf.scalar_summary(""model/value_loss"", vf_loss)\n                tf.scalar_summary(""model/entropy"", entropy)\n                tf.image_summary(""model/state"", pi.x)\n                tf.scalar_summary(""model/grad_global_norm"", tf.global_norm(grads))\n                tf.scalar_summary(""model/var_global_norm"", tf.global_norm(pi.var_list))\n                if self.unsup:\n                    tf.scalar_summary(""model/predloss"", self.predloss)\n                    if \'action\' in unsupType:\n                        tf.scalar_summary(""model/inv_loss"", predictor.invloss)\n                        tf.scalar_summary(""model/forward_loss"", predictor.forwardloss)\n                    tf.scalar_summary(""model/predgrad_global_norm"", tf.global_norm(predgrads))\n                    tf.scalar_summary(""model/predvar_global_norm"", tf.global_norm(predictor.var_list))\n                self.summary_op = tf.merge_all_summaries()\n\n            # clip gradients\n            grads, _ = tf.clip_by_global_norm(grads, constants[\'GRAD_NORM_CLIP\'])\n            grads_and_vars = list(zip(grads, self.network.var_list))\n            if self.unsup:\n                predgrads, _ = tf.clip_by_global_norm(predgrads, constants[\'GRAD_NORM_CLIP\'])\n                pred_grads_and_vars = list(zip(predgrads, self.ap_network.var_list))\n                grads_and_vars = grads_and_vars + pred_grads_and_vars\n\n            # update global step by batch size\n            inc_step = self.global_step.assign_add(tf.shape(pi.x)[0])\n\n            # each worker has a different set of adam optimizer parameters\n            # TODO: make optimizer global shared, if needed\n            print(""Optimizer: ADAM with lr: %f"" % (constants[\'LEARNING_RATE\']))\n            print(""Input observation shape: "",env.observation_space.shape)\n            opt = tf.train.AdamOptimizer(constants[\'LEARNING_RATE\'])\n            self.train_op = tf.group(opt.apply_gradients(grads_and_vars), inc_step)\n\n            # copy weights from the parameter server to the local model\n            sync_var_list = [v1.assign(v2) for v1, v2 in zip(pi.var_list, self.network.var_list)]\n            if self.unsup:\n                sync_var_list += [v1.assign(v2) for v1, v2 in zip(predictor.var_list, self.ap_network.var_list)]\n            self.sync = tf.group(*sync_var_list)\n\n            # initialize extras\n            self.summary_writer = None\n            self.local_steps = 0\n\n    def start(self, sess, summary_writer):\n        self.runner.start_runner(sess, summary_writer)\n        self.summary_writer = summary_writer\n\n    def pull_batch_from_queue(self):\n        """"""\n        Take a rollout from the queue of the thread runner.\n        """"""\n        # get top rollout from queue (FIFO)\n        rollout = self.runner.queue.get(timeout=600.0)\n        while not rollout.terminal:\n            try:\n                # Now, get remaining *available* rollouts from queue and append them into\n                # the same one above. If queue.Queue(5): len=5 and everything is\n                # superfast (not usually the case), then all 5 will be returned and\n                # exception is raised. In such a case, effective batch_size would become\n                # constants[\'ROLLOUT_MAXLEN\'] * queue_maxlen(5). But it is almost never the\n                # case, i.e., collecting  a rollout of length=ROLLOUT_MAXLEN takes more time\n                # than get(). So, there are no more available rollouts in queue usually and\n                # exception gets always raised. Hence, one should keep queue_maxlen = 1 ideally.\n                # Also note that the next rollout generation gets invoked automatically because\n                # its a thread which is always running using \'yield\' at end of generation process.\n                # To conclude, effective batch_size = constants[\'ROLLOUT_MAXLEN\']\n                rollout.extend(self.runner.queue.get_nowait())\n            except queue.Empty:\n                break\n        return rollout\n\n    def process(self, sess):\n        """"""\n        Process grabs a rollout that\'s been produced by the thread runner,\n        and updates the parameters.  The update is then sent to the parameter\n        server.\n        """"""\n        sess.run(self.sync)  # copy weights from shared to local\n        rollout = self.pull_batch_from_queue()\n        batch = process_rollout(rollout, gamma=constants[\'GAMMA\'], lambda_=constants[\'LAMBDA\'], clip=self.envWrap)\n\n        should_compute_summary = self.task == 0 and self.local_steps % 11 == 0\n\n        if should_compute_summary:\n            fetches = [self.summary_op, self.train_op, self.global_step]\n        else:\n            fetches = [self.train_op, self.global_step]\n\n        feed_dict = {\n            self.local_network.x: batch.si,\n            self.ac: batch.a,\n            self.adv: batch.adv,\n            self.r: batch.r,\n            self.local_network.state_in[0]: batch.features[0],\n            self.local_network.state_in[1]: batch.features[1],\n        }\n        if self.unsup:\n            feed_dict[self.local_network.x] = batch.si[:-1]\n            feed_dict[self.local_ap_network.s1] = batch.si[:-1]\n            feed_dict[self.local_ap_network.s2] = batch.si[1:]\n            feed_dict[self.local_ap_network.asample] = batch.a\n\n        fetched = sess.run(fetches, feed_dict=feed_dict)\n        if batch.terminal:\n            print(""Global Step Counter: %d""%fetched[-1])\n\n        if should_compute_summary:\n            self.summary_writer.add_summary(tf.Summary.FromString(fetched[0]), fetched[-1])\n            self.summary_writer.flush()\n        self.local_steps += 1\n'"
src/constants.py,0,"b""constants = {\n'GAMMA': 0.99,  # discount factor for rewards\n'LAMBDA': 1.0,  # lambda of Generalized Advantage Estimation: https://arxiv.org/abs/1506.02438\n'ENTROPY_BETA': 0.01,  # entropy regurarlization constant.\n'ROLLOUT_MAXLEN': 20, # 20 represents the number of 'local steps': the number of timesteps\n                    # we run the policy before we update the parameters.\n                    # The larger local steps is, the lower is the variance in our policy gradients estimate\n                    # on the one hand;  but on the other hand, we get less frequent parameter updates, which\n                    # slows down learning.  In this code, we found that making local steps be much\n                    # smaller than 20 makes the algorithm more difficult to tune and to get to work.\n'GRAD_NORM_CLIP': 40.0,   # gradient norm clipping\n'REWARD_CLIP': 1.0,       # reward value clipping in [-x,x]\n'MAX_GLOBAL_STEPS': 100000000,  # total steps taken across all workers\n'LEARNING_RATE': 1e-4,  # learning rate for adam\n\n'PREDICTION_BETA': 0.01,  # weight of prediction bonus\n                          # set 0.5 for unsup=state\n'PREDICTION_LR_SCALE': 10.0,  # scale lr of predictor wrt to policy network\n                              # set 30-50 for unsup=state\n'FORWARD_LOSS_WT': 0.2,  # should be between [0,1]\n                          # predloss = ( (1-FORWARD_LOSS_WT) * inv_loss + FORWARD_LOSS_WT * forward_loss) * PREDICTION_LR_SCALE\n'POLICY_NO_BACKPROP_STEPS': 0,  # number of global steps after which we start backpropagating to policy\n}\n"""
src/demo.py,10,"b'#!/usr/bin/env python\nfrom __future__ import print_function\nimport tensorflow as tf\nimport gym\nimport numpy as np\nimport argparse\nimport logging\nfrom envs import create_env\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\ndef inference(args):\n    """"""\n    It restore policy weights, and does inference.\n    """"""\n    # define environment\n    env = create_env(args.env_id, client_id=\'0\', remotes=None, envWrap=True,\n                        acRepeat=1, record=args.record, outdir=args.outdir)\n    numaction = env.action_space.n\n\n    with tf.device(""/cpu:0""):\n        config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n        with tf.Session(config=config) as sess:\n            logger.info(""Restoring trainable global parameters."")\n            saver = tf.train.import_meta_graph(args.ckpt+\'.meta\')\n            saver.restore(sess, args.ckpt)\n\n            probs = tf.get_collection(""probs"")[0]\n            sample = tf.get_collection(""sample"")[0]\n            vf = tf.get_collection(""vf"")[0]\n            state_out_0 = tf.get_collection(""state_out_0"")[0]\n            state_out_1 = tf.get_collection(""state_out_1"")[0]\n\n            last_state = env.reset()\n            if args.render or args.record:\n                env.render()\n            last_features = np.zeros((1, 256), np.float32); last_features = [last_features, last_features]\n            length = 0\n            rewards = 0\n            mario_distances = np.zeros((args.num_episodes,))\n            for i in range(args.num_episodes):\n                print(""Starting episode %d"" % (i + 1))\n\n                if args.random:\n                    print(\'I am a random policy!\')\n                else:\n                    if args.greedy:\n                        print(\'I am a greedy policy!\')\n                    else:\n                        print(\'I am a sampled policy!\')\n                while True:\n                    # run policy\n                    fetched = sess.run([probs, sample, vf, state_out_0, state_out_1] ,\n                                {""global/x:0"": [last_state], ""global/c_in:0"": last_features[0], ""global/h_in:0"": last_features[1]})\n                    prob_action, action, value_, features = fetched[0], fetched[1], fetched[2], fetched[3:]\n\n                    # run environment\n                    if args.random:\n                        stepAct = np.random.randint(0, numaction)  # random policy\n                    else:\n                        if args.greedy:\n                            stepAct = prob_action.argmax()  # greedy policy\n                        else:\n                            stepAct = action.argmax()\n                    state, reward, terminal, info = env.step(stepAct)\n\n                    # update stats\n                    length += 1\n                    rewards += reward\n                    last_state = state\n                    last_features = features\n                    if args.render or args.record:\n                        env.render()\n\n                    timestep_limit = env.spec.tags.get(\'wrapper_config.TimeLimit.max_episode_steps\')\n                    if timestep_limit is None: timestep_limit = env.spec.timestep_limit\n                    if terminal or length >= timestep_limit:\n                        if length >= timestep_limit or not env.metadata.get(\'semantics.autoreset\'):\n                            last_state = env.reset()\n                        last_features = np.zeros((1, 256), np.float32); last_features = [last_features, last_features]\n                        print(""Episode finished. Sum of rewards: %.2f. Length: %d."" % (rewards, length))\n                        length = 0\n                        rewards = 0\n                        if args.render or args.record:\n                            env.render()\n                        break\n\n        logger.info(\'Finished %d true episodes.\', args.num_episodes)\n        env.close()\n\n\ndef main(_):\n    parser = argparse.ArgumentParser(description=None)\n    parser.add_argument(\'--ckpt\', default=""../models/doom/doom_ICM"", help=\'checkpoint name\')\n    parser.add_argument(\'--outdir\', default=""../models/output"", help=\'Output log directory\')\n    parser.add_argument(\'--env-id\', default=""doom"", help=\'Environment id\')\n    parser.add_argument(\'--record\', action=\'store_true\', help=""Record the policy running video"")\n    parser.add_argument(\'--render\', action=\'store_true\',\n                        help=""Render the gym environment video online"")\n    parser.add_argument(\'--num-episodes\', type=int, default=2, help=""Number of episodes to run"")\n    parser.add_argument(\'--greedy\', action=\'store_true\',\n                        help=""Default sampled policy. This option does argmax."")\n    parser.add_argument(\'--random\', action=\'store_true\',\n                        help=""Default sampled policy. This option does random policy."")\n    args = parser.parse_args()\n    inference(args)\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
src/env_wrapper.py,0,"b'""""""\nAuthor: Deepak Pathak\n\nAcknowledgement:\n    - The wrappers (BufferedObsEnv, SkipEnv) were originally written by\n        Evan Shelhamer and modified by Deepak. Thanks Evan!\n    - This file is derived from\n        https://github.com/shelhamer/ourl/envs.py\n        https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers_deprecated.py\n""""""\nfrom __future__ import print_function\nimport numpy as np\nfrom collections import deque\nfrom PIL import Image\nfrom gym.spaces.box import Box\nimport gym\nimport time, sys\n\n\nclass BufferedObsEnv(gym.ObservationWrapper):\n    """"""Buffer observations and stack e.g. for frame skipping.\n\n    n is the length of the buffer, and number of observations stacked.\n    skip is the number of steps between buffered observations (min=1).\n\n    n.b. first obs is the oldest, last obs is the newest.\n         the buffer is zeroed out on reset.\n         *must* call reset() for init!\n    """"""\n    def __init__(self, env=None, n=4, skip=4, shape=(84, 84),\n                    channel_last=True, maxFrames=True):\n        super(BufferedObsEnv, self).__init__(env)\n        self.obs_shape = shape\n        # most recent raw observations (for max pooling across time steps)\n        self.obs_buffer = deque(maxlen=2)\n        self.maxFrames = maxFrames\n        self.n = n\n        self.skip = skip\n        self.buffer = deque(maxlen=self.n)\n        self.counter = 0  # init and reset should agree on this\n        shape = shape + (n,) if channel_last else (n,) + shape\n        self.observation_space = Box(0.0, 255.0, shape)\n        self.ch_axis = -1 if channel_last else 0\n        self.scale = 1.0 / 255\n        self.observation_space.high[...] = 1.0\n\n    def _step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        return self._observation(obs), reward, done, info\n\n    def _observation(self, obs):\n        obs = self._convert(obs)\n        self.counter += 1\n        if self.counter % self.skip == 0:\n            self.buffer.append(obs)\n        obsNew = np.stack(self.buffer, axis=self.ch_axis)\n        return obsNew.astype(np.float32) * self.scale\n\n    def _reset(self):\n        """"""Clear buffer and re-fill by duplicating the first observation.""""""\n        self.obs_buffer.clear()\n        obs = self._convert(self.env.reset())\n        self.buffer.clear()\n        self.counter = 0\n        for _ in range(self.n - 1):\n            self.buffer.append(np.zeros_like(obs))\n        self.buffer.append(obs)\n        obsNew = np.stack(self.buffer, axis=self.ch_axis)\n        return obsNew.astype(np.float32) * self.scale\n\n    def _convert(self, obs):\n        self.obs_buffer.append(obs)\n        if self.maxFrames:\n            max_frame = np.max(np.stack(self.obs_buffer), axis=0)\n        else:\n            max_frame = obs\n        intensity_frame = self._rgb2y(max_frame).astype(np.uint8)\n        small_frame = np.array(Image.fromarray(intensity_frame).resize(\n            self.obs_shape, resample=Image.BILINEAR), dtype=np.uint8)\n        return small_frame\n\n    def _rgb2y(self, im):\n        """"""Converts an RGB image to a Y image (as in YUV).\n\n        These coefficients are taken from the torch/image library.\n        Beware: these are more critical than you might think, as the\n        monochromatic contrast can be surprisingly low.\n        """"""\n        if len(im.shape) < 3:\n            return im\n        return np.sum(im * [0.299, 0.587, 0.114], axis=2)\n\n\nclass NoNegativeRewardEnv(gym.RewardWrapper):\n    """"""Clip reward in negative direction.""""""\n    def __init__(self, env=None, neg_clip=0.0):\n        super(NoNegativeRewardEnv, self).__init__(env)\n        self.neg_clip = neg_clip\n\n    def _reward(self, reward):\n        new_reward = self.neg_clip if reward < self.neg_clip else reward\n        return new_reward\n\n\nclass SkipEnv(gym.Wrapper):\n    """"""Skip timesteps: repeat action, accumulate reward, take last obs.""""""\n    def __init__(self, env=None, skip=4):\n        super(SkipEnv, self).__init__(env)\n        self.skip = skip\n\n    def _step(self, action):\n        total_reward = 0\n        for i in range(0, self.skip):\n            obs, reward, done, info = self.env.step(action)\n            total_reward += reward\n            info[\'steps\'] = i + 1\n            if done:\n                break\n        return obs, total_reward, done, info\n\n\nclass MarioEnv(gym.Wrapper):\n    def __init__(self, env=None, tilesEnv=False):\n        """"""Reset mario environment without actually restarting fceux everytime.\n        This speeds up unrolling by approximately 10 times.\n        """"""\n        super(MarioEnv, self).__init__(env)\n        self.resetCount = -1\n        # reward is distance travelled. So normalize it with total distance\n        # https://github.com/ppaquette/gym-super-mario/blob/master/ppaquette_gym_super_mario/lua/super-mario-bros.lua\n        # However, we will not use this reward at all. It is only for completion.\n        self.maxDistance = 3000.0\n        self.tilesEnv = tilesEnv\n\n    def _reset(self):\n        if self.resetCount < 0:\n            print(\'\\nDoing hard mario fceux reset (40 seconds wait) !\')\n            sys.stdout.flush()\n            self.env.reset()\n            time.sleep(40)\n        obs, _, _, info = self.env.step(7)  # take right once to start game\n        if info.get(\'ignore\', False):  # assuming this happens only in beginning\n            self.resetCount = -1\n            self.env.close()\n            return self._reset()\n        self.resetCount = info.get(\'iteration\', -1)\n        if self.tilesEnv:\n            return obs\n        return obs[24:-12,8:-8,:]\n\n    def _step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        # print(\'info:\', info)\n        done = info[\'iteration\'] > self.resetCount\n        reward = float(reward)/self.maxDistance # note: we do not use this rewards at all.\n        if self.tilesEnv:\n            return obs, reward, done, info\n        return obs[24:-12,8:-8,:], reward, done, info\n\n    def _close(self):\n        self.resetCount = -1\n        return self.env.close()\n\n\nclass MakeEnvDynamic(gym.ObservationWrapper):\n    """"""Make observation dynamic by adding noise""""""\n    def __init__(self, env=None, percentPad=5):\n        super(MakeEnvDynamic, self).__init__(env)\n        self.origShape = env.observation_space.shape\n        newside = int(round(max(self.origShape[:-1])*100./(100.-percentPad)))\n        self.newShape = [newside, newside, 3]\n        self.observation_space = Box(0.0, 255.0, self.newShape)\n        self.bottomIgnore = 20  # doom 20px bottom is useless\n        self.ob = None\n\n    def _observation(self, obs):\n        imNoise = np.random.randint(0,256,self.newShape).astype(obs.dtype)\n        imNoise[:self.origShape[0]-self.bottomIgnore, :self.origShape[1], :] = obs[:-self.bottomIgnore,:,:]\n        self.ob = imNoise\n        return imNoise\n\n    # def render(self, mode=\'human\', close=False):\n    #     temp = self.env.render(mode, close)\n    #     return self.ob\n'"
src/envs.py,0,"b'from __future__ import print_function\nfrom PIL import Image\nfrom gym.spaces.box import Box\nimport numpy as np\nimport gym\nfrom gym import spaces\nimport logging\nimport universe\nfrom universe import vectorized\nfrom universe.wrappers import BlockingReset, GymCoreAction, EpisodeID, Unvectorize, Vectorize, Vision, Logger\nfrom universe import spaces as vnc_spaces\nfrom universe.spaces.vnc_event import keycode\nimport env_wrapper\nimport time\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nuniverse.configure_logging()\n\ndef create_env(env_id, client_id, remotes, **kwargs):\n    if \'doom\' in env_id.lower() or \'labyrinth\' in env_id.lower():\n        return create_doom(env_id, client_id, **kwargs)\n    if \'mario\' in env_id.lower():\n        return create_mario(env_id, client_id, **kwargs)\n\n    spec = gym.spec(env_id)\n    if spec.tags.get(\'flashgames\', False):\n        return create_flash_env(env_id, client_id, remotes, **kwargs)\n    elif spec.tags.get(\'atari\', False) and spec.tags.get(\'vnc\', False):\n        return create_vncatari_env(env_id, client_id, remotes, **kwargs)\n    else:\n        # Assume atari.\n        assert ""."" not in env_id  # universe environments have dots in names.\n        return create_atari_env(env_id, **kwargs)\n\ndef create_doom(env_id, client_id, envWrap=True, record=False, outdir=None,\n                    noLifeReward=False, acRepeat=0, **_):\n    from ppaquette_gym_doom import wrappers\n    if \'labyrinth\' in env_id.lower():\n        if \'single\' in env_id.lower():\n            env_id = \'ppaquette/LabyrinthSingle-v0\'\n        elif \'fix\' in env_id.lower():\n            env_id = \'ppaquette/LabyrinthManyFixed-v0\'\n        else:\n            env_id = \'ppaquette/LabyrinthMany-v0\'\n    elif \'very\' in env_id.lower():\n        env_id = \'ppaquette/DoomMyWayHomeFixed15-v0\'\n    elif \'sparse\' in env_id.lower():\n        env_id = \'ppaquette/DoomMyWayHomeFixed-v0\'\n    elif \'fix\' in env_id.lower():\n        if \'1\' in env_id or \'2\' in env_id:\n            env_id = \'ppaquette/DoomMyWayHomeFixed\' + str(env_id[-2:]) + \'-v0\'\n        elif \'new\' in env_id.lower():\n            env_id = \'ppaquette/DoomMyWayHomeFixedNew-v0\'\n        else:\n            env_id = \'ppaquette/DoomMyWayHomeFixed-v0\'\n    else:\n        env_id = \'ppaquette/DoomMyWayHome-v0\'\n\n    # VizDoom workaround: Simultaneously launching multiple vizdoom processes\n    # makes program stuck, so use the global lock in multi-threading/processing\n    client_id = int(client_id)\n    time.sleep(client_id * 10)\n    env = gym.make(env_id)\n    modewrapper = wrappers.SetPlayingMode(\'algo\')\n    obwrapper = wrappers.SetResolution(\'160x120\')\n    acwrapper = wrappers.ToDiscrete(\'minimal\')\n    env = modewrapper(obwrapper(acwrapper(env)))\n    # env = env_wrapper.MakeEnvDynamic(env)  # to add stochasticity\n\n    if record and outdir is not None:\n        env = gym.wrappers.Monitor(env, outdir, force=True)\n\n    if envWrap:\n        fshape = (42, 42)\n        frame_skip = acRepeat if acRepeat>0 else 4\n        env.seed(None)\n        if noLifeReward:\n            env = env_wrapper.NoNegativeRewardEnv(env)\n        env = env_wrapper.BufferedObsEnv(env, skip=frame_skip, shape=fshape)\n        env = env_wrapper.SkipEnv(env, skip=frame_skip)\n    elif noLifeReward:\n        env = env_wrapper.NoNegativeRewardEnv(env)\n\n    env = Vectorize(env)\n    env = DiagnosticsInfo(env)\n    env = Unvectorize(env)\n    return env\n\ndef create_mario(env_id, client_id, envWrap=True, record=False, outdir=None,\n                    noLifeReward=False, acRepeat=0, **_):\n    import ppaquette_gym_super_mario\n    from ppaquette_gym_super_mario import wrappers\n    if \'-v\' in env_id.lower():\n        env_id = \'ppaquette/\' + env_id\n    else:\n        env_id = \'ppaquette/SuperMarioBros-1-1-v0\'  # shape: (224,256,3)=(h,w,c)\n\n    # Mario workaround: Simultaneously launching multiple vizdoom processes makes program stuck,\n    # so use the global lock in multi-threading/multi-processing\n    # see: https://github.com/ppaquette/gym-super-mario/tree/master/ppaquette_gym_super_mario\n    client_id = int(client_id)\n    time.sleep(client_id * 50)\n    env = gym.make(env_id)\n    modewrapper = wrappers.SetPlayingMode(\'algo\')\n    acwrapper = wrappers.ToDiscrete()\n    env = modewrapper(acwrapper(env))\n    env = env_wrapper.MarioEnv(env)\n\n    if record and outdir is not None:\n        env = gym.wrappers.Monitor(env, outdir, force=True)\n\n    if envWrap:\n        frame_skip = acRepeat if acRepeat>0 else 6\n        fshape = (42, 42)\n        env.seed(None)\n        if noLifeReward:\n            env = env_wrapper.NoNegativeRewardEnv(env)\n        env = env_wrapper.BufferedObsEnv(env, skip=frame_skip, shape=fshape, maxFrames=False)\n        if frame_skip > 1:\n            env = env_wrapper.SkipEnv(env, skip=frame_skip)\n    elif noLifeReward:\n        env = env_wrapper.NoNegativeRewardEnv(env)\n\n    env = Vectorize(env)\n    env = DiagnosticsInfo(env)\n    env = Unvectorize(env)\n    # env.close() # TODO: think about where to put env.close !\n    return env\n\ndef create_flash_env(env_id, client_id, remotes, **_):\n    env = gym.make(env_id)\n    env = Vision(env)\n    env = Logger(env)\n    env = BlockingReset(env)\n\n    reg = universe.runtime_spec(\'flashgames\').server_registry\n    height = reg[env_id][""height""]\n    width = reg[env_id][""width""]\n    env = CropScreen(env, height, width, 84, 18)\n    env = FlashRescale(env)\n\n    keys = [\'left\', \'right\', \'up\', \'down\', \'x\']\n    if env_id == \'flashgames.NeonRace-v0\':\n        # Better key space for this game.\n        keys = [\'left\', \'right\', \'up\', \'left up\', \'right up\', \'down\', \'up x\']\n    logger.info(\'create_flash_env(%s): keys=%s\', env_id, keys)\n\n    env = DiscreteToFixedKeysVNCActions(env, keys)\n    env = EpisodeID(env)\n    env = DiagnosticsInfo(env)\n    env = Unvectorize(env)\n    env.configure(fps=5.0, remotes=remotes, start_timeout=15 * 60, client_id=client_id,\n                  vnc_driver=\'go\', vnc_kwargs={\n                    \'encoding\': \'tight\', \'compress_level\': 0,\n                    \'fine_quality_level\': 50, \'subsample_level\': 3})\n    return env\n\ndef create_vncatari_env(env_id, client_id, remotes, **_):\n    env = gym.make(env_id)\n    env = Vision(env)\n    env = Logger(env)\n    env = BlockingReset(env)\n    env = GymCoreAction(env)\n    env = AtariRescale42x42(env)\n    env = EpisodeID(env)\n    env = DiagnosticsInfo(env)\n    env = Unvectorize(env)\n\n    logger.info(\'Connecting to remotes: %s\', remotes)\n    fps = env.metadata[\'video.frames_per_second\']\n    env.configure(remotes=remotes, start_timeout=15 * 60, fps=fps, client_id=client_id)\n    return env\n\ndef create_atari_env(env_id, record=False, outdir=None, **_):\n    env = gym.make(env_id)\n    if record and outdir is not None:\n        env = gym.wrappers.Monitor(env, outdir, force=True)\n    env = Vectorize(env)\n    env = AtariRescale42x42(env)\n    env = DiagnosticsInfo(env)\n    env = Unvectorize(env)\n    return env\n\ndef DiagnosticsInfo(env, *args, **kwargs):\n    return vectorized.VectorizeFilter(env, DiagnosticsInfoI, *args, **kwargs)\n\nclass DiagnosticsInfoI(vectorized.Filter):\n    def __init__(self, log_interval=503):\n        super(DiagnosticsInfoI, self).__init__()\n\n        self._episode_time = time.time()\n        self._last_time = time.time()\n        self._local_t = 0\n        self._log_interval = log_interval\n        self._episode_reward = 0\n        self._episode_length = 0\n        self._all_rewards = []\n        self._num_vnc_updates = 0\n        self._last_episode_id = -1\n\n    def _after_reset(self, observation):\n        logger.info(\'Resetting environment logs\')\n        self._episode_reward = 0\n        self._episode_length = 0\n        self._all_rewards = []\n        return observation\n\n    def _after_step(self, observation, reward, done, info):\n        to_log = {}\n        if self._episode_length == 0:\n            self._episode_time = time.time()\n\n        self._local_t += 1\n        if info.get(""stats.vnc.updates.n"") is not None:\n            self._num_vnc_updates += info.get(""stats.vnc.updates.n"")\n\n        if self._local_t % self._log_interval == 0:\n            cur_time = time.time()\n            elapsed = cur_time - self._last_time\n            fps = self._log_interval / elapsed\n            self._last_time = cur_time\n            cur_episode_id = info.get(\'vectorized.episode_id\', 0)\n            to_log[""diagnostics/fps""] = fps\n            if self._last_episode_id == cur_episode_id:\n                to_log[""diagnostics/fps_within_episode""] = fps\n            self._last_episode_id = cur_episode_id\n            if info.get(""stats.gauges.diagnostics.lag.action"") is not None:\n                to_log[""diagnostics/action_lag_lb""] = info[""stats.gauges.diagnostics.lag.action""][0]\n                to_log[""diagnostics/action_lag_ub""] = info[""stats.gauges.diagnostics.lag.action""][1]\n            if info.get(""reward.count"") is not None:\n                to_log[""diagnostics/reward_count""] = info[""reward.count""]\n            if info.get(""stats.gauges.diagnostics.clock_skew"") is not None:\n                to_log[""diagnostics/clock_skew_lb""] = info[""stats.gauges.diagnostics.clock_skew""][0]\n                to_log[""diagnostics/clock_skew_ub""] = info[""stats.gauges.diagnostics.clock_skew""][1]\n            if info.get(""stats.gauges.diagnostics.lag.observation"") is not None:\n                to_log[""diagnostics/observation_lag_lb""] = info[""stats.gauges.diagnostics.lag.observation""][0]\n                to_log[""diagnostics/observation_lag_ub""] = info[""stats.gauges.diagnostics.lag.observation""][1]\n\n            if info.get(""stats.vnc.updates.n"") is not None:\n                to_log[""diagnostics/vnc_updates_n""] = info[""stats.vnc.updates.n""]\n                to_log[""diagnostics/vnc_updates_n_ps""] = self._num_vnc_updates / elapsed\n                self._num_vnc_updates = 0\n            if info.get(""stats.vnc.updates.bytes"") is not None:\n                to_log[""diagnostics/vnc_updates_bytes""] = info[""stats.vnc.updates.bytes""]\n            if info.get(""stats.vnc.updates.pixels"") is not None:\n                to_log[""diagnostics/vnc_updates_pixels""] = info[""stats.vnc.updates.pixels""]\n            if info.get(""stats.vnc.updates.rectangles"") is not None:\n                to_log[""diagnostics/vnc_updates_rectangles""] = info[""stats.vnc.updates.rectangles""]\n            if info.get(""env_status.state_id"") is not None:\n                to_log[""diagnostics/env_state_id""] = info[""env_status.state_id""]\n\n        if reward is not None:\n            self._episode_reward += reward\n            if observation is not None:\n                self._episode_length += 1\n            self._all_rewards.append(reward)\n\n        if done:\n            logger.info(\'True Game terminating: env_episode_reward=%s episode_length=%s\', self._episode_reward, self._episode_length)\n            total_time = time.time() - self._episode_time\n            to_log[""global/episode_reward""] = self._episode_reward\n            to_log[""global/episode_length""] = self._episode_length\n            to_log[""global/episode_time""] = total_time\n            to_log[""global/reward_per_time""] = self._episode_reward / total_time\n            self._episode_reward = 0\n            self._episode_length = 0\n            self._all_rewards = []\n\n        if \'distance\' in info: to_log[\'distance\'] = info[\'distance\']  # mario\n        if \'POSITION_X\' in info:  # doom\n            to_log[\'POSITION_X\'] = info[\'POSITION_X\']\n            to_log[\'POSITION_Y\'] = info[\'POSITION_Y\']\n        return observation, reward, done, to_log\n\ndef _process_frame42(frame):\n    frame = frame[34:34+160, :160]\n    # Resize by half, then down to 42x42 (essentially mipmapping). If\n    # we resize directly we lose pixels that, when mapped to 42x42,\n    # aren\'t close enough to the pixel boundary.\n    frame = np.asarray(Image.fromarray(frame).resize((80, 80), resample=Image.BILINEAR).resize(\n                        (42,42), resample=Image.BILINEAR))\n    frame = frame.mean(2)  # take mean along channels\n    frame = frame.astype(np.float32)\n    frame *= (1.0 / 255.0)\n    frame = np.reshape(frame, [42, 42, 1])\n    return frame\n\nclass AtariRescale42x42(vectorized.ObservationWrapper):\n    def __init__(self, env=None):\n        super(AtariRescale42x42, self).__init__(env)\n        self.observation_space = Box(0.0, 1.0, [42, 42, 1])\n\n    def _observation(self, observation_n):\n        return [_process_frame42(observation) for observation in observation_n]\n\nclass FixedKeyState(object):\n    def __init__(self, keys):\n        self._keys = [keycode(key) for key in keys]\n        self._down_keysyms = set()\n\n    def apply_vnc_actions(self, vnc_actions):\n        for event in vnc_actions:\n            if isinstance(event, vnc_spaces.KeyEvent):\n                if event.down:\n                    self._down_keysyms.add(event.key)\n                else:\n                    self._down_keysyms.discard(event.key)\n\n    def to_index(self):\n        action_n = 0\n        for key in self._down_keysyms:\n            if key in self._keys:\n                # If multiple keys are pressed, just use the first one\n                action_n = self._keys.index(key) + 1\n                break\n        return action_n\n\nclass DiscreteToFixedKeysVNCActions(vectorized.ActionWrapper):\n    """"""\n    Define a fixed action space. Action 0 is all keys up. Each element of keys can be a single key or a space-separated list of keys\n\n    For example,\n       e=DiscreteToFixedKeysVNCActions(e, [\'left\', \'right\'])\n    will have 3 actions: [none, left, right]\n\n    You can define a state with more than one key down by separating with spaces. For example,\n       e=DiscreteToFixedKeysVNCActions(e, [\'left\', \'right\', \'space\', \'left space\', \'right space\'])\n    will have 6 actions: [none, left, right, space, left space, right space]\n    """"""\n    def __init__(self, env, keys):\n        super(DiscreteToFixedKeysVNCActions, self).__init__(env)\n\n        self._keys = keys\n        self._generate_actions()\n        self.action_space = spaces.Discrete(len(self._actions))\n\n    def _generate_actions(self):\n        self._actions = []\n        uniq_keys = set()\n        for key in self._keys:\n            for cur_key in key.split(\' \'):\n                uniq_keys.add(cur_key)\n\n        for key in [\'\'] + self._keys:\n            split_keys = key.split(\' \')\n            cur_action = []\n            for cur_key in uniq_keys:\n                cur_action.append(vnc_spaces.KeyEvent.by_name(cur_key, down=(cur_key in split_keys)))\n            self._actions.append(cur_action)\n        self.key_state = FixedKeyState(uniq_keys)\n\n    def _action(self, action_n):\n        # Each action might be a length-1 np.array. Cast to int to\n        # avoid warnings.\n        return [self._actions[int(action)] for action in action_n]\n\nclass CropScreen(vectorized.ObservationWrapper):\n    """"""Crops out a [height]x[width] area starting from (top,left) """"""\n    def __init__(self, env, height, width, top=0, left=0):\n        super(CropScreen, self).__init__(env)\n        self.height = height\n        self.width = width\n        self.top = top\n        self.left = left\n        self.observation_space = Box(0, 255, shape=(height, width, 3))\n\n    def _observation(self, observation_n):\n        return [ob[self.top:self.top+self.height, self.left:self.left+self.width, :] if ob is not None else None\n                for ob in observation_n]\n\ndef _process_frame_flash(frame):\n    frame = np.array(Image.fromarray(frame).resize((200, 128), resample=Image.BILINEAR))\n    frame = frame.mean(2).astype(np.float32)\n    frame *= (1.0 / 255.0)\n    frame = np.reshape(frame, [128, 200, 1])\n    return frame\n\nclass FlashRescale(vectorized.ObservationWrapper):\n    def __init__(self, env=None):\n        super(FlashRescale, self).__init__(env)\n        self.observation_space = Box(0.0, 1.0, [128, 200, 1])\n\n    def _observation(self, observation_n):\n        return [_process_frame_flash(observation) for observation in observation_n]\n'"
src/inference.py,19,"b'#!/usr/bin/env python\nfrom __future__ import print_function\nimport go_vncdriver\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport logging\nimport os\nimport gym\nfrom envs import create_env\nfrom worker import FastSaver\nfrom model import LSTMPolicy\nimport utils\nimport distutils.version\nuse_tf12_api = distutils.version.LooseVersion(tf.VERSION) >= distutils.version.LooseVersion(\'0.12.0\')\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\ndef inference(args):\n    """"""\n    It only restores LSTMPolicy architecture, and does inference using that.\n    """"""\n    # get address of checkpoints\n    indir = os.path.join(args.log_dir, \'train\')\n    outdir = os.path.join(args.log_dir, \'inference\') if args.out_dir is None else args.out_dir\n    with open(indir + \'/checkpoint\', \'r\') as f:\n        first_line = f.readline().strip()\n    ckpt = first_line.split(\' \')[-1].split(\'/\')[-1][:-1]\n    ckpt = ckpt.split(\'-\')[-1]\n    ckpt = indir + \'/model.ckpt-\' + ckpt\n\n    # define environment\n    if args.record:\n        env = create_env(args.env_id, client_id=\'0\', remotes=None, envWrap=args.envWrap, designHead=args.designHead,\n                            record=True, noop=args.noop, acRepeat=args.acRepeat, outdir=outdir)\n    else:\n        env = create_env(args.env_id, client_id=\'0\', remotes=None, envWrap=args.envWrap, designHead=args.designHead,\n                            record=True, noop=args.noop, acRepeat=args.acRepeat)\n    numaction = env.action_space.n\n\n    with tf.device(""/cpu:0""):\n        # define policy network\n        with tf.variable_scope(""global""):\n            policy = LSTMPolicy(env.observation_space.shape, numaction, args.designHead)\n            policy.global_step = tf.get_variable(""global_step"", [], tf.int32, initializer=tf.constant_initializer(0, dtype=tf.int32),\n                                               trainable=False)\n\n        # Variable names that start with ""local"" are not saved in checkpoints.\n        if use_tf12_api:\n            variables_to_restore = [v for v in tf.global_variables() if not v.name.startswith(""local"")]\n            init_all_op = tf.global_variables_initializer()\n        else:\n            variables_to_restore = [v for v in tf.all_variables() if not v.name.startswith(""local"")]\n            init_all_op = tf.initialize_all_variables()\n        saver = FastSaver(variables_to_restore)\n\n        # print trainable variables\n        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n        logger.info(\'Trainable vars:\')\n        for v in var_list:\n            logger.info(\'  %s %s\', v.name, v.get_shape())\n\n        # summary of rewards\n        action_writers = []\n        if use_tf12_api:\n            summary_writer = tf.summary.FileWriter(outdir)\n            for ac_id in range(numaction):\n                action_writers.append(tf.summary.FileWriter(os.path.join(outdir,\'action_{}\'.format(ac_id))))\n        else:\n            summary_writer = tf.train.SummaryWriter(outdir)\n            for ac_id in range(numaction):\n                action_writers.append(tf.train.SummaryWriter(os.path.join(outdir,\'action_{}\'.format(ac_id))))\n        logger.info(""Inference events directory: %s"", outdir)\n\n        config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n        with tf.Session(config=config) as sess:\n            logger.info(""Initializing all parameters."")\n            sess.run(init_all_op)\n            logger.info(""Restoring trainable global parameters."")\n            saver.restore(sess, ckpt)\n            logger.info(""Restored model was trained for %.2fM global steps"", sess.run(policy.global_step)/1000000.)\n            # saving with meta graph:\n            # metaSaver = tf.train.Saver(variables_to_restore)\n            # metaSaver.save(sess, \'models/doomICM\')\n\n            last_state = env.reset()\n            if args.render or args.record:\n                env.render()\n            last_features = policy.get_initial_features()  # reset lstm memory\n            length = 0\n            rewards = 0\n            mario_distances = np.zeros((args.num_episodes,))\n            for i in range(args.num_episodes):\n                print(""Starting episode %d"" % (i + 1))\n                if args.recordSignal:\n                    from PIL import Image\n                    signalCount = 1\n                    utils.mkdir_p(outdir + \'/recordedSignal/ep_%02d/\'%i)\n                    Image.fromarray((255*last_state[..., -1]).astype(\'uint8\')).save(outdir + \'/recordedSignal/ep_%02d/%06d.jpg\'%(i,signalCount))\n\n                if args.random:\n                    print(\'I am random policy!\')\n                else:\n                    if args.greedy:\n                        print(\'I am greedy policy!\')\n                    else:\n                        print(\'I am sampled policy!\')\n                while True:\n                    # run policy\n                    fetched = policy.act_inference(last_state, *last_features)\n                    prob_action, action, value_, features = fetched[0], fetched[1], fetched[2], fetched[3:]\n\n                    # run environment: sampled one-hot \'action\' (not greedy)\n                    if args.random:\n                        stepAct = np.random.randint(0, numaction)  # random policy\n                    else:\n                        if args.greedy:\n                            stepAct = prob_action.argmax()  # greedy policy\n                        else:\n                            stepAct = action.argmax()\n                    # print(stepAct, prob_action.argmax(), prob_action)\n                    state, reward, terminal, info = env.step(stepAct)\n\n                    # update stats\n                    length += 1\n                    rewards += reward\n                    last_state = state\n                    last_features = features\n                    if args.render or args.record:\n                        env.render()\n                    if args.recordSignal:\n                        signalCount += 1\n                        Image.fromarray((255*last_state[..., -1]).astype(\'uint8\')).save(outdir + \'/recordedSignal/ep_%02d/%06d.jpg\'%(i,signalCount))\n\n                    # store summary\n                    summary = tf.Summary()\n                    summary.value.add(tag=\'ep_{}/reward\'.format(i), simple_value=reward)\n                    summary.value.add(tag=\'ep_{}/netreward\'.format(i), simple_value=rewards)\n                    summary.value.add(tag=\'ep_{}/value\'.format(i), simple_value=float(value_[0]))\n                    if \'NoFrameskip-v\' in args.env_id:  # atari\n                        summary.value.add(tag=\'ep_{}/lives\'.format(i), simple_value=env.unwrapped.ale.lives())\n                    summary_writer.add_summary(summary, length)\n                    summary_writer.flush()\n                    summary = tf.Summary()\n                    for ac_id in range(numaction):\n                        summary.value.add(tag=\'action_prob\', simple_value=float(prob_action[ac_id]))\n                        action_writers[ac_id].add_summary(summary, length)\n                        action_writers[ac_id].flush()\n\n                    timestep_limit = env.spec.tags.get(\'wrapper_config.TimeLimit.max_episode_steps\')\n                    if timestep_limit is None: timestep_limit = env.spec.timestep_limit\n                    if terminal or length >= timestep_limit:\n                        if length >= timestep_limit or not env.metadata.get(\'semantics.autoreset\'):\n                            last_state = env.reset()\n                        last_features = policy.get_initial_features()  # reset lstm memory\n                        print(""Episode finished. Sum of rewards: %.2f. Length: %d."" % (rewards, length))\n                        if \'distance\' in info:\n                            print(\'Mario Distance Covered:\', info[\'distance\'])\n                            mario_distances[i] = info[\'distance\']\n                        length = 0\n                        rewards = 0\n                        if args.render or args.record:\n                            env.render()\n                        if args.recordSignal:\n                            signalCount += 1\n                            Image.fromarray((255*last_state[..., -1]).astype(\'uint8\')).save(outdir + \'/recordedSignal/ep_%02d/%06d.jpg\'%(i,signalCount))\n                        break\n\n        logger.info(\'Finished %d true episodes.\', args.num_episodes)\n        if \'distance\' in info:\n            print(\'Mario Distances:\', mario_distances)\n            np.save(outdir + \'/distances.npy\', mario_distances)\n        env.close()\n\n\ndef main(_):\n    parser = argparse.ArgumentParser(description=None)\n    parser.add_argument(\'--log-dir\', default=""tmp/doom"", help=\'input model directory\')\n    parser.add_argument(\'--out-dir\', default=None, help=\'output log directory. Default: log_dir/inference/\')\n    parser.add_argument(\'--env-id\', default=""PongDeterministic-v3"", help=\'Environment id\')\n    parser.add_argument(\'--record\', action=\'store_true\',\n                        help=""Record the gym environment video -- user friendly"")\n    parser.add_argument(\'--recordSignal\', action=\'store_true\',\n                        help=""Record images of true processed input to network"")\n    parser.add_argument(\'--render\', action=\'store_true\',\n                        help=""Render the gym environment video online"")\n    parser.add_argument(\'--envWrap\', action=\'store_true\',\n                        help=""Preprocess input in env_wrapper (no change in input size or network)"")\n    parser.add_argument(\'--designHead\', type=str, default=\'universe\',\n                        help=""Network deign head: nips or nature or doom or universe(default)"")\n    parser.add_argument(\'--num-episodes\', type=int, default=2,\n                        help=""Number of episodes to run"")\n    parser.add_argument(\'--noop\', action=\'store_true\',\n                        help=""Add 30-noop for inference too (recommended by Nature paper, don\'t know?)"")\n    parser.add_argument(\'--acRepeat\', type=int, default=0,\n                        help=""Actions to be repeated at inference. 0 means default. applies iff envWrap is True."")\n    parser.add_argument(\'--greedy\', action=\'store_true\',\n                        help=""Default sampled policy. This option does argmax."")\n    parser.add_argument(\'--random\', action=\'store_true\',\n                        help=""Default sampled policy. This option does random policy."")\n    parser.add_argument(\'--default\', action=\'store_true\', help=""run with default params"")\n    args = parser.parse_args()\n    if args.default:\n        args.envWrap = True\n        args.acRepeat = 1\n    if args.acRepeat <= 0:\n        print(\'Using default action repeat (i.e. 4). Min value that can be set is 1.\')\n    inference(args)\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
src/mario.py,0,"b""'''\nScript to test if mario installation works fine. It\ndisplays the game play simultaneously.\n'''\n\nfrom __future__ import print_function\nimport gym, universe\nimport env_wrapper\nimport ppaquette_gym_super_mario\nfrom ppaquette_gym_super_mario import wrappers\nimport numpy as np\nimport time\nfrom PIL import Image\nimport utils\n\noutputdir = './gray42/'\nenv_id = 'ppaquette/SuperMarioBros-1-1-v0'\nenv = gym.make(env_id)\nmodewrapper = wrappers.SetPlayingMode('algo')\nacwrapper = wrappers.ToDiscrete()\nenv = modewrapper(acwrapper(env))\nenv = env_wrapper.MarioEnv(env)\n\nfreshape = fshape = (42, 42)\nenv.seed(None)\nenv = env_wrapper.NoNegativeRewardEnv(env)\nenv = env_wrapper.DQNObsEnv(env, shape=freshape)\nenv = env_wrapper.BufferedObsEnv(env, n=4, skip=1, shape=fshape, channel_last=True)\nenv = env_wrapper.EltwiseScaleObsEnv(env)\n\nstart = time.time()\nepisodes = 0\nmaxepisodes = 1\nenv.reset()\nimCount = 1\nutils.mkdir_p(outputdir + '/ep_%02d/'%(episodes+1))\nwhile(1):\n    obs, reward, done, info = env.step(env.action_space.sample())\n    Image.fromarray((255*obs).astype('uint8')).save(outputdir + '/ep_%02d/%06d.jpg'%(episodes+1,imCount))\n    imCount += 1\n    if done:\n        episodes += 1\n        print('Ep: %d, Distance: %d'%(episodes, info['distance']))\n        if episodes >= maxepisodes:\n            break\n        env.reset()\n        imCount = 1\n        utils.mkdir_p(outputdir + '/ep_%02d/'%(episodes+1))\nend = time.time()\nprint('\\nTotal Time spent: %0.2f seconds'% (end-start))\nenv.close()\nprint('Done!')\nexit(1)\n"""
src/model.py,87,"b'from __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.rnn as rnn\nfrom constants import constants\n\n\ndef normalized_columns_initializer(std=1.0):\n    def _initializer(shape, dtype=None, partition_info=None):\n        out = np.random.randn(*shape).astype(np.float32)\n        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n        return tf.constant(out)\n    return _initializer\n\n\ndef cosineLoss(A, B, name):\n    \'\'\' A, B : (BatchSize, d) \'\'\'\n    dotprod = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(A,1), tf.nn.l2_normalize(B,1)), 1)\n    loss = 1-tf.reduce_mean(dotprod, name=name)\n    return loss\n\n\ndef flatten(x):\n    return tf.reshape(x, [-1, np.prod(x.get_shape().as_list()[1:])])\n\n\ndef conv2d(x, num_filters, name, filter_size=(3, 3), stride=(1, 1), pad=""SAME"", dtype=tf.float32, collections=None):\n    with tf.variable_scope(name):\n        stride_shape = [1, stride[0], stride[1], 1]\n        filter_shape = [filter_size[0], filter_size[1], int(x.get_shape()[3]), num_filters]\n\n        # there are ""num input feature maps * filter height * filter width""\n        # inputs to each hidden unit\n        fan_in = np.prod(filter_shape[:3])\n        # each unit in the lower layer receives a gradient from:\n        # ""num output feature maps * filter height * filter width"" /\n        #   pooling size\n        fan_out = np.prod(filter_shape[:2]) * num_filters\n        # initialize weights with random weights\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n\n        w = tf.get_variable(""W"", filter_shape, dtype, tf.random_uniform_initializer(-w_bound, w_bound),\n                            collections=collections)\n        b = tf.get_variable(""b"", [1, 1, 1, num_filters], initializer=tf.constant_initializer(0.0),\n                            collections=collections)\n        return tf.nn.conv2d(x, w, stride_shape, pad) + b\n\n\ndef deconv2d(x, out_shape, name, filter_size=(3, 3), stride=(1, 1), pad=""SAME"", dtype=tf.float32, collections=None, prevNumFeat=None):\n    with tf.variable_scope(name):\n        num_filters = out_shape[-1]\n        prevNumFeat = int(x.get_shape()[3]) if prevNumFeat is None else prevNumFeat\n        stride_shape = [1, stride[0], stride[1], 1]\n        # transpose_filter : [height, width, out_channels, in_channels]\n        filter_shape = [filter_size[0], filter_size[1], num_filters, prevNumFeat]\n\n        # there are ""num input feature maps * filter height * filter width""\n        # inputs to each hidden unit\n        fan_in = np.prod(filter_shape[:2]) * prevNumFeat\n        # each unit in the lower layer receives a gradient from:\n        # ""num output feature maps * filter height * filter width""\n        fan_out = np.prod(filter_shape[:3])\n        # initialize weights with random weights\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n\n        w = tf.get_variable(""W"", filter_shape, dtype, tf.random_uniform_initializer(-w_bound, w_bound),\n                            collections=collections)\n        b = tf.get_variable(""b"", [num_filters], initializer=tf.constant_initializer(0.0),\n                            collections=collections)\n        deconv2d = tf.nn.conv2d_transpose(x, w, tf.pack(out_shape), stride_shape, pad)\n        # deconv2d = tf.reshape(tf.nn.bias_add(deconv2d, b), deconv2d.get_shape())\n        return deconv2d\n\n\ndef linear(x, size, name, initializer=None, bias_init=0):\n    w = tf.get_variable(name + ""/w"", [x.get_shape()[1], size], initializer=initializer)\n    b = tf.get_variable(name + ""/b"", [size], initializer=tf.constant_initializer(bias_init))\n    return tf.matmul(x, w) + b\n\n\ndef categorical_sample(logits, d):\n    value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keep_dims=True), 1), [1])\n    return tf.one_hot(value, d)\n\n\ndef inverseUniverseHead(x, final_shape, nConvs=4):\n    \'\'\' universe agent example\n        input: [None, 288]; output: [None, 42, 42, 1];\n    \'\'\'\n    print(\'Using inverse-universe head design\')\n    bs = tf.shape(x)[0]\n    deconv_shape1 = [final_shape[1]]\n    deconv_shape2 = [final_shape[2]]\n    for i in range(nConvs):\n        deconv_shape1.append((deconv_shape1[-1]-1)/2 + 1)\n        deconv_shape2.append((deconv_shape2[-1]-1)/2 + 1)\n    inshapeprod = np.prod(x.get_shape().as_list()[1:]) / 32.0\n    assert(inshapeprod == deconv_shape1[-1]*deconv_shape2[-1])\n    # print(\'deconv_shape1: \',deconv_shape1)\n    # print(\'deconv_shape2: \',deconv_shape2)\n\n    x = tf.reshape(x, [-1, deconv_shape1[-1], deconv_shape2[-1], 32])\n    deconv_shape1 = deconv_shape1[:-1]\n    deconv_shape2 = deconv_shape2[:-1]\n    for i in range(nConvs-1):\n        x = tf.nn.elu(deconv2d(x, [bs, deconv_shape1[-1], deconv_shape2[-1], 32],\n                        ""dl{}"".format(i + 1), [3, 3], [2, 2], prevNumFeat=32))\n        deconv_shape1 = deconv_shape1[:-1]\n        deconv_shape2 = deconv_shape2[:-1]\n    x = deconv2d(x, [bs] + final_shape[1:], ""dl4"", [3, 3], [2, 2], prevNumFeat=32)\n    return x\n\n\ndef universeHead(x, nConvs=4):\n    \'\'\' universe agent example\n        input: [None, 42, 42, 1]; output: [None, 288];\n    \'\'\'\n    print(\'Using universe head design\')\n    for i in range(nConvs):\n        x = tf.nn.elu(conv2d(x, 32, ""l{}"".format(i + 1), [3, 3], [2, 2]))\n        # print(\'Loop{} \'.format(i+1),tf.shape(x))\n        # print(\'Loop{}\'.format(i+1),x.get_shape())\n    x = flatten(x)\n    return x\n\n\ndef nipsHead(x):\n    \'\'\' DQN NIPS 2013 and A3C paper\n        input: [None, 84, 84, 4]; output: [None, 2592] -> [None, 256];\n    \'\'\'\n    print(\'Using nips head design\')\n    x = tf.nn.relu(conv2d(x, 16, ""l1"", [8, 8], [4, 4], pad=""VALID""))\n    x = tf.nn.relu(conv2d(x, 32, ""l2"", [4, 4], [2, 2], pad=""VALID""))\n    x = flatten(x)\n    x = tf.nn.relu(linear(x, 256, ""fc"", normalized_columns_initializer(0.01)))\n    return x\n\n\ndef natureHead(x):\n    \'\'\' DQN Nature 2015 paper\n        input: [None, 84, 84, 4]; output: [None, 3136] -> [None, 512];\n    \'\'\'\n    print(\'Using nature head design\')\n    x = tf.nn.relu(conv2d(x, 32, ""l1"", [8, 8], [4, 4], pad=""VALID""))\n    x = tf.nn.relu(conv2d(x, 64, ""l2"", [4, 4], [2, 2], pad=""VALID""))\n    x = tf.nn.relu(conv2d(x, 64, ""l3"", [3, 3], [1, 1], pad=""VALID""))\n    x = flatten(x)\n    x = tf.nn.relu(linear(x, 512, ""fc"", normalized_columns_initializer(0.01)))\n    return x\n\n\ndef doomHead(x):\n    \'\'\' Learning by Prediction ICLR 2017 paper\n        (their final output was 64 changed to 256 here)\n        input: [None, 120, 160, 1]; output: [None, 1280] -> [None, 256];\n    \'\'\'\n    print(\'Using doom head design\')\n    x = tf.nn.elu(conv2d(x, 8, ""l1"", [5, 5], [4, 4]))\n    x = tf.nn.elu(conv2d(x, 16, ""l2"", [3, 3], [2, 2]))\n    x = tf.nn.elu(conv2d(x, 32, ""l3"", [3, 3], [2, 2]))\n    x = tf.nn.elu(conv2d(x, 64, ""l4"", [3, 3], [2, 2]))\n    x = flatten(x)\n    x = tf.nn.elu(linear(x, 256, ""fc"", normalized_columns_initializer(0.01)))\n    return x\n\n\nclass LSTMPolicy(object):\n    def __init__(self, ob_space, ac_space, designHead=\'universe\'):\n        self.x = x = tf.placeholder(tf.float32, [None] + list(ob_space), name=\'x\')\n        size = 256\n        if designHead == \'nips\':\n            x = nipsHead(x)\n        elif designHead == \'nature\':\n            x = natureHead(x)\n        elif designHead == \'doom\':\n            x = doomHead(x)\n        elif \'tile\' in designHead:\n            x = universeHead(x, nConvs=2)\n        else:\n            x = universeHead(x)\n\n        # introduce a ""fake"" batch dimension of 1 to do LSTM over time dim\n        x = tf.expand_dims(x, [0])\n        lstm = rnn.rnn_cell.BasicLSTMCell(size, state_is_tuple=True)\n        self.state_size = lstm.state_size\n        step_size = tf.shape(self.x)[:1]\n\n        c_init = np.zeros((1, lstm.state_size.c), np.float32)\n        h_init = np.zeros((1, lstm.state_size.h), np.float32)\n        self.state_init = [c_init, h_init]\n        c_in = tf.placeholder(tf.float32, [1, lstm.state_size.c], name=\'c_in\')\n        h_in = tf.placeholder(tf.float32, [1, lstm.state_size.h], name=\'h_in\')\n        self.state_in = [c_in, h_in]\n\n        state_in = rnn.rnn_cell.LSTMStateTuple(c_in, h_in)\n        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n            lstm, x, initial_state=state_in, sequence_length=step_size,\n            time_major=False)\n        lstm_c, lstm_h = lstm_state\n        x = tf.reshape(lstm_outputs, [-1, size])\n        self.vf = tf.reshape(linear(x, 1, ""value"", normalized_columns_initializer(1.0)), [-1])\n        self.state_out = [lstm_c[:1, :], lstm_h[:1, :]]\n\n        # [0, :] means pick action of first state from batch. Hardcoded b/c\n        # batch=1 during rollout collection. Its not used during batch training.\n        self.logits = linear(x, ac_space, ""action"", normalized_columns_initializer(0.01))\n        self.sample = categorical_sample(self.logits, ac_space)[0, :]\n        self.probs = tf.nn.softmax(self.logits, dim=-1)[0, :]\n\n        self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n        # tf.add_to_collection(\'probs\', self.probs)\n        # tf.add_to_collection(\'sample\', self.sample)\n        # tf.add_to_collection(\'state_out_0\', self.state_out[0])\n        # tf.add_to_collection(\'state_out_1\', self.state_out[1])\n        # tf.add_to_collection(\'vf\', self.vf)\n\n    def get_initial_features(self):\n        # Call this function to get reseted lstm memory cells\n        return self.state_init\n\n    def act(self, ob, c, h):\n        sess = tf.get_default_session()\n        return sess.run([self.sample, self.vf] + self.state_out,\n                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n\n    def act_inference(self, ob, c, h):\n        sess = tf.get_default_session()\n        return sess.run([self.probs, self.sample, self.vf] + self.state_out,\n                        {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})\n\n    def value(self, ob, c, h):\n        sess = tf.get_default_session()\n        return sess.run(self.vf, {self.x: [ob], self.state_in[0]: c, self.state_in[1]: h})[0]\n\n\nclass StateActionPredictor(object):\n    def __init__(self, ob_space, ac_space, designHead=\'universe\'):\n        # input: s1,s2: : [None, h, w, ch] (usually ch=1 or 4)\n        # asample: 1-hot encoding of sampled action from policy: [None, ac_space]\n        input_shape = [None] + list(ob_space)\n        self.s1 = phi1 = tf.placeholder(tf.float32, input_shape)\n        self.s2 = phi2 = tf.placeholder(tf.float32, input_shape)\n        self.asample = asample = tf.placeholder(tf.float32, [None, ac_space])\n\n        # feature encoding: phi1, phi2: [None, LEN]\n        size = 256\n        if designHead == \'nips\':\n            phi1 = nipsHead(phi1)\n            with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n                phi2 = nipsHead(phi2)\n        elif designHead == \'nature\':\n            phi1 = natureHead(phi1)\n            with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n                phi2 = natureHead(phi2)\n        elif designHead == \'doom\':\n            phi1 = doomHead(phi1)\n            with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n                phi2 = doomHead(phi2)\n        elif \'tile\' in designHead:\n            phi1 = universeHead(phi1, nConvs=2)\n            with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n                phi2 = universeHead(phi2, nConvs=2)\n        else:\n            phi1 = universeHead(phi1)\n            with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n                phi2 = universeHead(phi2)\n\n        # inverse model: g(phi1,phi2) -> a_inv: [None, ac_space]\n        g = tf.concat(1,[phi1, phi2])\n        g = tf.nn.relu(linear(g, size, ""g1"", normalized_columns_initializer(0.01)))\n        aindex = tf.argmax(asample, axis=1)  # aindex: [batch_size,]\n        logits = linear(g, ac_space, ""glast"", normalized_columns_initializer(0.01))\n        self.invloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n                                        logits, aindex), name=""invloss"")\n        self.ainvprobs = tf.nn.softmax(logits, dim=-1)\n\n        # forward model: f(phi1,asample) -> phi2\n        # Note: no backprop to asample of policy: it is treated as fixed for predictor training\n        f = tf.concat(1, [phi1, asample])\n        f = tf.nn.relu(linear(f, size, ""f1"", normalized_columns_initializer(0.01)))\n        f = linear(f, phi1.get_shape()[1].value, ""flast"", normalized_columns_initializer(0.01))\n        self.forwardloss = 0.5 * tf.reduce_mean(tf.square(tf.subtract(f, phi2)), name=\'forwardloss\')\n        # self.forwardloss = 0.5 * tf.reduce_mean(tf.sqrt(tf.abs(tf.subtract(f, phi2))), name=\'forwardloss\')\n        # self.forwardloss = cosineLoss(f, phi2, name=\'forwardloss\')\n        self.forwardloss = self.forwardloss * 288.0  # lenFeatures=288. Factored out to make hyperparams not depend on it.\n\n        # variable list\n        self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n\n    def pred_act(self, s1, s2):\n        \'\'\'\n        returns action probability distribution predicted by inverse model\n            input: s1,s2: [h, w, ch]\n            output: ainvprobs: [ac_space]\n        \'\'\'\n        sess = tf.get_default_session()\n        return sess.run(self.ainvprobs, {self.s1: [s1], self.s2: [s2]})[0, :]\n\n    def pred_bonus(self, s1, s2, asample):\n        \'\'\'\n        returns bonus predicted by forward model\n            input: s1,s2: [h, w, ch], asample: [ac_space] 1-hot encoding\n            output: scalar bonus\n        \'\'\'\n        sess = tf.get_default_session()\n        # error = sess.run([self.forwardloss, self.invloss],\n        #     {self.s1: [s1], self.s2: [s2], self.asample: [asample]})\n        # print(\'ErrorF: \', error[0], \' ErrorI:\', error[1])\n        error = sess.run(self.forwardloss,\n            {self.s1: [s1], self.s2: [s2], self.asample: [asample]})\n        error = error * constants[\'PREDICTION_BETA\']\n        return error\n\n\nclass StatePredictor(object):\n    \'\'\'\n    Loss is normalized across spatial dimension (42x42), but not across batches.\n    It is unlike ICM where no normalization is there across 288 spatial dimension\n    and neither across batches.\n    \'\'\'\n\n    def __init__(self, ob_space, ac_space, designHead=\'universe\', unsupType=\'state\'):\n        # input: s1,s2: : [None, h, w, ch] (usually ch=1 or 4)\n        # asample: 1-hot encoding of sampled action from policy: [None, ac_space]\n        input_shape = [None] + list(ob_space)\n        self.s1 = phi1 = tf.placeholder(tf.float32, input_shape)\n        self.s2 = phi2 = tf.placeholder(tf.float32, input_shape)\n        self.asample = asample = tf.placeholder(tf.float32, [None, ac_space])\n        self.stateAenc = unsupType == \'stateAenc\'\n\n        # feature encoding: phi1: [None, LEN]\n        if designHead == \'universe\':\n            phi1 = universeHead(phi1)\n            if self.stateAenc:\n                with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n                    phi2_aenc = universeHead(phi2)\n        elif \'tile\' in designHead:  # for mario tiles\n            phi1 = universeHead(phi1, nConvs=2)\n            if self.stateAenc:\n                with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n                    phi2_aenc = universeHead(phi2)\n        else:\n            print(\'Only universe designHead implemented for state prediction baseline.\')\n            exit(1)\n\n        # forward model: f(phi1,asample) -> phi2\n        # Note: no backprop to asample of policy: it is treated as fixed for predictor training\n        f = tf.concat(1, [phi1, asample])\n        f = tf.nn.relu(linear(f, phi1.get_shape()[1].value, ""f1"", normalized_columns_initializer(0.01)))\n        if \'tile\' in designHead:\n            f = inverseUniverseHead(f, input_shape, nConvs=2)\n        else:\n            f = inverseUniverseHead(f, input_shape)\n        self.forwardloss = 0.5 * tf.reduce_mean(tf.square(tf.subtract(f, phi2)), name=\'forwardloss\')\n        if self.stateAenc:\n            self.aencBonus = 0.5 * tf.reduce_mean(tf.square(tf.subtract(phi1, phi2_aenc)), name=\'aencBonus\')\n        self.predstate = phi1\n\n        # variable list\n        self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n\n    def pred_state(self, s1, asample):\n        \'\'\'\n        returns state predicted by forward model\n            input: s1: [h, w, ch], asample: [ac_space] 1-hot encoding\n            output: s2: [h, w, ch]\n        \'\'\'\n        sess = tf.get_default_session()\n        return sess.run(self.predstate, {self.s1: [s1],\n                                            self.asample: [asample]})[0, :]\n\n    def pred_bonus(self, s1, s2, asample):\n        \'\'\'\n        returns bonus predicted by forward model\n            input: s1,s2: [h, w, ch], asample: [ac_space] 1-hot encoding\n            output: scalar bonus\n        \'\'\'\n        sess = tf.get_default_session()\n        bonus = self.aencBonus if self.stateAenc else self.forwardloss\n        error = sess.run(bonus,\n            {self.s1: [s1], self.s2: [s2], self.asample: [asample]})\n        # print(\'ErrorF: \', error)\n        error = error * constants[\'PREDICTION_BETA\']\n        return error\n'"
src/train.py,0,"b'import argparse\nimport os\nimport sys\nfrom six.moves import shlex_quote\n\nparser = argparse.ArgumentParser(description=""Run commands"")\nparser.add_argument(\'-w\', \'--num-workers\', default=20, type=int,\n                    help=""Number of workers"")\nparser.add_argument(\'-r\', \'--remotes\', default=None,\n                    help=\'The address of pre-existing VNC servers and \'\n                         \'rewarders to use (e.g. -r vnc://localhost:5900+15900,vnc://localhost:5901+15901).\')\nparser.add_argument(\'-e\', \'--env-id\', type=str, default=""doom"",\n                    help=""Environment id"")\nparser.add_argument(\'-l\', \'--log-dir\', type=str, default=""tmp/doom"",\n                    help=""Log directory path"")\nparser.add_argument(\'-n\', \'--dry-run\', action=\'store_true\',\n                    help=""Print out commands rather than executing them"")\nparser.add_argument(\'-m\', \'--mode\', type=str, default=\'tmux\',\n                    help=""tmux: run workers in a tmux session. nohup: run workers with nohup. child: run workers as child processes"")\nparser.add_argument(\'--visualise\', action=\'store_true\',\n                    help=""Visualise the gym environment by running env.render() between each timestep"")\nparser.add_argument(\'--envWrap\', action=\'store_true\',\n                    help=""Preprocess input in env_wrapper (no change in input size or network)"")\nparser.add_argument(\'--designHead\', type=str, default=\'universe\',\n                    help=""Network deign head: nips or nature or doom or universe(default)"")\nparser.add_argument(\'--unsup\', type=str, default=None,\n                    help=""Unsup. exploration mode: action or state or stateAenc or None"")\nparser.add_argument(\'--noReward\', action=\'store_true\', help=""Remove all extrinsic reward"")\nparser.add_argument(\'--noLifeReward\', action=\'store_true\',\n                    help=""Remove all negative reward (in doom: it is living reward)"")\nparser.add_argument(\'--expName\', type=str, default=\'a3c\',\n                    help=""Experiment tmux session-name. Default a3c."")\nparser.add_argument(\'--expId\', type=int, default=0,\n                    help=""Experiment Id >=0. Needed while runnig more than one run per machine."")\nparser.add_argument(\'--savio\', action=\'store_true\',\n                    help=""Savio or KNL cpu cluster hacks"")\nparser.add_argument(\'--default\', action=\'store_true\', help=""run with default params"")\nparser.add_argument(\'--pretrain\', type=str, default=None, help=""Checkpoint dir (generally ..../train/) to load from."")\n\ndef new_cmd(session, name, cmd, mode, logdir, shell):\n    if isinstance(cmd, (list, tuple)):\n        cmd = "" "".join(shlex_quote(str(v)) for v in cmd)\n    if mode == \'tmux\':\n        return name, ""tmux send-keys -t {}:{} {} Enter"".format(session, name, shlex_quote(cmd))\n    elif mode == \'child\':\n        return name, ""{} >{}/{}.{}.out 2>&1 & echo kill $! >>{}/kill.sh"".format(cmd, logdir, session, name, logdir)\n    elif mode == \'nohup\':\n        return name, ""nohup {} -c {} >{}/{}.{}.out 2>&1 & echo kill $! >>{}/kill.sh"".format(shell, shlex_quote(cmd), logdir, session, name, logdir)\n\n\ndef create_commands(session, num_workers, remotes, env_id, logdir, shell=\'bash\',\n                    mode=\'tmux\', visualise=False, envWrap=False, designHead=None,\n                    unsup=None, noReward=False, noLifeReward=False, psPort=12222,\n                    delay=0, savio=False, pretrain=None):\n    # for launching the TF workers and for launching tensorboard\n    py_cmd = \'python\' if savio else sys.executable\n    base_cmd = [\n        \'CUDA_VISIBLE_DEVICES=\',\n        py_cmd, \'worker.py\',\n        \'--log-dir\', logdir,\n        \'--env-id\', env_id,\n        \'--num-workers\', str(num_workers),\n        \'--psPort\', psPort]\n\n    if delay > 0:\n        base_cmd += [\'--delay\', delay]\n    if visualise:\n        base_cmd += [\'--visualise\']\n    if envWrap:\n        base_cmd += [\'--envWrap\']\n    if designHead is not None:\n        base_cmd += [\'--designHead\', designHead]\n    if unsup is not None:\n        base_cmd += [\'--unsup\', unsup]\n    if noReward:\n        base_cmd += [\'--noReward\']\n    if noLifeReward:\n        base_cmd += [\'--noLifeReward\']\n    if pretrain is not None:\n        base_cmd += [\'--pretrain\', pretrain]\n\n    if remotes is None:\n        remotes = [""1""] * num_workers\n    else:\n        remotes = remotes.split(\',\')\n        assert len(remotes) == num_workers\n\n    cmds_map = [new_cmd(session, ""ps"", base_cmd + [""--job-name"", ""ps""], mode, logdir, shell)]\n    for i in range(num_workers):\n        cmds_map += [new_cmd(session,\n            ""w-%d"" % i, base_cmd + [""--job-name"", ""worker"", ""--task"", str(i), ""--remotes"", remotes[i]], mode, logdir, shell)]\n\n    # No tensorboard or htop window if running multiple experiments per machine\n    if session == \'a3c\':\n        cmds_map += [new_cmd(session, ""tb"", [""tensorboard"", ""--logdir"", logdir, ""--port"", ""12345""], mode, logdir, shell)]\n    if session == \'a3c\' and mode == \'tmux\':\n        cmds_map += [new_cmd(session, ""htop"", [""htop""], mode, logdir, shell)]\n\n    windows = [v[0] for v in cmds_map]\n\n    notes = []\n    cmds = [\n        ""mkdir -p {}"".format(logdir),\n        ""echo {} {} > {}/cmd.sh"".format(sys.executable, \' \'.join([shlex_quote(arg) for arg in sys.argv if arg != \'-n\']), logdir),\n    ]\n    if mode == \'nohup\' or mode == \'child\':\n        cmds += [""echo \'#!/bin/sh\' >{}/kill.sh"".format(logdir)]\n        notes += [""Run `source {}/kill.sh` to kill the job"".format(logdir)]\n    if mode == \'tmux\':\n        notes += [""Use `tmux attach -t {}` to watch process output"".format(session)]\n        notes += [""Use `tmux kill-session -t {}` to kill the job"".format(session)]\n    else:\n        notes += [""Use `tail -f {}/*.out` to watch process output"".format(logdir)]\n    notes += [""Point your browser to http://localhost:12345 to see Tensorboard""]\n\n    if mode == \'tmux\':\n        cmds += [\n        ""kill -9 $( lsof -i:12345 -t ) > /dev/null 2>&1"",  # kill any process using tensorboard\'s port\n        ""kill -9 $( lsof -i:{}-{} -t ) > /dev/null 2>&1"".format(psPort, num_workers+psPort), # kill any processes using ps / worker ports\n        ""tmux kill-session -t {}"".format(session),\n        ""tmux new-session -s {} -n {} -d {}"".format(session, windows[0], shell)\n        ]\n        for w in windows[1:]:\n            cmds += [""tmux new-window -t {} -n {} {}"".format(session, w, shell)]\n        cmds += [""sleep 1""]\n    for window, cmd in cmds_map:\n        cmds += [cmd]\n\n    return cmds, notes\n\n\ndef run():\n    args = parser.parse_args()\n    if args.default:\n        args.envWrap = True\n        args.savio = True\n        args.noLifeReward = True\n        args.unsup = \'action\'\n\n    # handling nuances of running multiple jobs per-machine\n    psPort = 12222 + 50*args.expId\n    delay = 220*args.expId if \'doom\' in args.env_id.lower() or \'labyrinth\' in args.env_id.lower() else 5*args.expId\n    delay = 6*delay if \'mario\' in args.env_id else delay\n\n    cmds, notes = create_commands(args.expName, args.num_workers, args.remotes, args.env_id,\n                                    args.log_dir, mode=args.mode, visualise=args.visualise,\n                                    envWrap=args.envWrap, designHead=args.designHead,\n                                    unsup=args.unsup, noReward=args.noReward,\n                                    noLifeReward=args.noLifeReward, psPort=psPort,\n                                    delay=delay, savio=args.savio, pretrain=args.pretrain)\n    if args.dry_run:\n        print(""Dry-run mode due to -n flag, otherwise the following commands would be executed:"")\n    else:\n        print(""Executing the following commands:"")\n    print(""\\n"".join(cmds))\n    print("""")\n    if not args.dry_run:\n        if args.mode == ""tmux"":\n            os.environ[""TMUX""] = """"\n        os.system(""\\n"".join(cmds))\n    print(\'\\n\'.join(notes))\n\n\nif __name__ == ""__main__"":\n    run()\n'"
src/utils.py,0,"b'from __future__ import print_function\nimport sys\nimport os\nimport errno\n\n\ndef mkdir_p(path):\n    """"""\n    It creates directory recursively if it does not already exist\n    """"""\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n'"
src/worker.py,22,"b'#!/usr/bin/env python\nimport go_vncdriver\nimport tensorflow as tf\nimport argparse\nimport logging\nimport sys, signal\nimport time\nimport os\nfrom a3c import A3C\nfrom envs import create_env\nfrom constants import constants\nimport distutils.version\nuse_tf12_api = distutils.version.LooseVersion(tf.VERSION) >= distutils.version.LooseVersion(\'0.12.0\')\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n# Disables write_meta_graph argument, which freezes entire process and is mostly useless.\nclass FastSaver(tf.train.Saver):\n    def save(self, sess, save_path, global_step=None, latest_filename=None,\n             meta_graph_suffix=""meta"", write_meta_graph=True):\n        super(FastSaver, self).save(sess, save_path, global_step, latest_filename,\n                                    meta_graph_suffix, False)\n\ndef run(args, server):\n    env = create_env(args.env_id, client_id=str(args.task), remotes=args.remotes, envWrap=args.envWrap, designHead=args.designHead,\n                        noLifeReward=args.noLifeReward)\n    trainer = A3C(env, args.task, args.visualise, args.unsup, args.envWrap, args.designHead, args.noReward)\n\n    # logging\n    if args.task == 0:\n        with open(args.log_dir + \'/log.txt\', \'w\') as fid:\n            for key, val in constants.items():\n                fid.write(\'%s: %s\\n\'%(str(key), str(val)))\n            fid.write(\'designHead: %s\\n\'%args.designHead)\n            fid.write(\'input observation: %s\\n\'%str(env.observation_space.shape))\n            fid.write(\'env name: %s\\n\'%str(env.spec.id))\n            fid.write(\'unsup method type: %s\\n\'%str(args.unsup))\n\n    # Variable names that start with ""local"" are not saved in checkpoints.\n    if use_tf12_api:\n        variables_to_save = [v for v in tf.global_variables() if not v.name.startswith(""local"")]\n        init_op = tf.variables_initializer(variables_to_save)\n        init_all_op = tf.global_variables_initializer()\n    else:\n        variables_to_save = [v for v in tf.all_variables() if not v.name.startswith(""local"")]\n        init_op = tf.initialize_variables(variables_to_save)\n        init_all_op = tf.initialize_all_variables()\n    saver = FastSaver(variables_to_save)\n    if args.pretrain is not None:\n        variables_to_restore = [v for v in tf.trainable_variables() if not v.name.startswith(""local"")]\n        pretrain_saver = FastSaver(variables_to_restore)\n\n    var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n    logger.info(\'Trainable vars:\')\n    for v in var_list:\n        logger.info(\'  %s %s\', v.name, v.get_shape())\n\n    def init_fn(ses):\n        logger.info(""Initializing all parameters."")\n        ses.run(init_all_op)\n        if args.pretrain is not None:\n            pretrain = tf.train.latest_checkpoint(args.pretrain)\n            logger.info(""==> Restoring from given pretrained checkpoint."")\n            logger.info(""    Pretraining address: %s"", pretrain)\n            pretrain_saver.restore(ses, pretrain)\n            logger.info(""==> Done restoring model! Restored %d variables."", len(variables_to_restore))\n\n    config = tf.ConfigProto(device_filters=[""/job:ps"", ""/job:worker/task:{}/cpu:0"".format(args.task)])\n    logdir = os.path.join(args.log_dir, \'train\')\n\n    if use_tf12_api:\n        summary_writer = tf.summary.FileWriter(logdir + ""_%d"" % args.task)\n    else:\n        summary_writer = tf.train.SummaryWriter(logdir + ""_%d"" % args.task)\n\n    logger.info(""Events directory: %s_%s"", logdir, args.task)\n    sv = tf.train.Supervisor(is_chief=(args.task == 0),\n                             logdir=logdir,\n                             saver=saver,\n                             summary_op=None,\n                             init_op=init_op,\n                             init_fn=init_fn,\n                             summary_writer=summary_writer,\n                             ready_op=tf.report_uninitialized_variables(variables_to_save),\n                             global_step=trainer.global_step,\n                             save_model_secs=30,\n                             save_summaries_secs=30)\n\n    num_global_steps = constants[\'MAX_GLOBAL_STEPS\']\n\n    logger.info(\n        ""Starting session. If this hangs, we\'re mostly likely waiting to connect to the parameter server. "" +\n        ""One common cause is that the parameter server DNS name isn\'t resolving yet, or is misspecified."")\n    with sv.managed_session(server.target, config=config) as sess, sess.as_default():\n        # Workaround for FailedPreconditionError\n        # see: https://github.com/openai/universe-starter-agent/issues/44 and 31\n        sess.run(trainer.sync)\n\n        trainer.start(sess, summary_writer)\n        global_step = sess.run(trainer.global_step)\n        logger.info(""Starting training at gobal_step=%d"", global_step)\n        while not sv.should_stop() and (not num_global_steps or global_step < num_global_steps):\n            trainer.process(sess)\n            global_step = sess.run(trainer.global_step)\n\n    # Ask for all the services to stop.\n    sv.stop()\n    logger.info(\'reached %s steps. worker stopped.\', global_step)\n\ndef cluster_spec(num_workers, num_ps, port=12222):\n    """"""\nMore tensorflow setup for data parallelism\n""""""\n    cluster = {}\n\n    all_ps = []\n    host = \'127.0.0.1\'\n    for _ in range(num_ps):\n        all_ps.append(\'{}:{}\'.format(host, port))\n        port += 1\n    cluster[\'ps\'] = all_ps\n\n    all_workers = []\n    for _ in range(num_workers):\n        all_workers.append(\'{}:{}\'.format(host, port))\n        port += 1\n    cluster[\'worker\'] = all_workers\n    return cluster\n\ndef main(_):\n    """"""\nSetting up Tensorflow for data parallel work\n""""""\n\n    parser = argparse.ArgumentParser(description=None)\n    parser.add_argument(\'-v\', \'--verbose\', action=\'count\', dest=\'verbosity\', default=0, help=\'Set verbosity.\')\n    parser.add_argument(\'--task\', default=0, type=int, help=\'Task index\')\n    parser.add_argument(\'--job-name\', default=""worker"", help=\'worker or ps\')\n    parser.add_argument(\'--num-workers\', default=1, type=int, help=\'Number of workers\')\n    parser.add_argument(\'--log-dir\', default=""tmp/doom"", help=\'Log directory path\')\n    parser.add_argument(\'--env-id\', default=""doom"", help=\'Environment id\')\n    parser.add_argument(\'-r\', \'--remotes\', default=None,\n                        help=\'References to environments to create (e.g. -r 20), \'\n                             \'or the address of pre-existing VNC servers and \'\n                             \'rewarders to use (e.g. -r vnc://localhost:5900+15900,vnc://localhost:5901+15901)\')\n    parser.add_argument(\'--visualise\', action=\'store_true\',\n                        help=""Visualise the gym environment by running env.render() between each timestep"")\n    parser.add_argument(\'--envWrap\', action=\'store_true\',\n                        help=""Preprocess input in env_wrapper (no change in input size or network)"")\n    parser.add_argument(\'--designHead\', type=str, default=\'universe\',\n                        help=""Network deign head: nips or nature or doom or universe(default)"")\n    parser.add_argument(\'--unsup\', type=str, default=None,\n                        help=""Unsup. exploration mode: action or state or stateAenc or None"")\n    parser.add_argument(\'--noReward\', action=\'store_true\', help=""Remove all extrinsic reward"")\n    parser.add_argument(\'--noLifeReward\', action=\'store_true\',\n                        help=""Remove all negative reward (in doom: it is living reward)"")\n    parser.add_argument(\'--psPort\', default=12222, type=int, help=\'Port number for parameter server\')\n    parser.add_argument(\'--delay\', default=0, type=int, help=\'delay start by these many seconds\')\n    parser.add_argument(\'--pretrain\', type=str, default=None, help=""Checkpoint dir (generally ..../train/) to load from."")\n    args = parser.parse_args()\n\n    spec = cluster_spec(args.num_workers, 1, args.psPort)\n    cluster = tf.train.ClusterSpec(spec).as_cluster_def()\n\n    def shutdown(signal, frame):\n        logger.warn(\'Received signal %s: exiting\', signal)\n        sys.exit(128+signal)\n    signal.signal(signal.SIGHUP, shutdown)\n    signal.signal(signal.SIGINT, shutdown)\n    signal.signal(signal.SIGTERM, shutdown)\n\n    if args.job_name == ""worker"":\n        server = tf.train.Server(cluster, job_name=""worker"", task_index=args.task,\n                                 config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=2))\n        if args.delay > 0:\n            print(\'Startup delay in worker: {}s\'.format(args.delay))\n            time.sleep(args.delay)\n            print(\'.. wait over !\')\n        run(args, server)\n    else:\n        server = tf.train.Server(cluster, job_name=""ps"", task_index=args.task,\n                                 config=tf.ConfigProto(device_filters=[""/job:ps""]))\n        while True:\n            time.sleep(1000)\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
