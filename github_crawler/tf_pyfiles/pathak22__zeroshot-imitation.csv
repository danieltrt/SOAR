file_path,api_count,code
train.py,89,"b'import sys\nimport os\nPD = os.getcwd() + \'/caffe/python/\'\nif PD not in sys.path:\n    sys.path.append(PD)\nfrom data import rope_data\nimport numpy as np\nimport subprocess\nimport collections\nimport copy\nimport tensorflow as tf\nimport time\nimport matplotlib.pyplot as plt\n\nslim = tf.contrib.slim\nfrom nets import alexnet_geurzhoy\n\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import embedding_ops\n\nCONFIG = tf.ConfigProto()\nCONFIG.gpu_options.allow_growth = True\n\nGRAD_CLIP_NORM = 40\n\n# from original poke paper\nFEAT_SIZE = 400\nBATCH_SIZE = 64\nENCODING_SIZE = 200 # latent feature space representation of image\nFEATURE_SIZE = 2 * ENCODING_SIZE\nLOCATION_BINS = 400 # number of possible grasp locations\nLOCATION_EMBEDDING_SIZE = 50 # discrete to continuous representation\nTHETA_BINS = 36 # discretization of angle bins\nTHETA_EMBEDDING_SIZE = 36 # discrete to cintinusous representation\nLENGTH_BINS = 10 # 1-10 cm movement\n\ndef init_weights(name, shape):\n    return tf.get_variable(name, shape=shape, initializer=tf.random_normal_initializer(0, 0.01))\n\ndef make_network(x, network_size):\n    """"""Makes fully connected network with input x and given layer sizes.\n    Assume len(network_size) >= 2\n    """"""\n    input_size = network_size[0]\n    output_size = network_size.pop()\n    a = input_size\n    cur = x\n    i = 0\n    for a, b in zip(network_size, network_size[1:]):\n        W = init_weights(""W"" + str(i), [a, b])\n        B = init_weights(""B"" + str(i), [1, b])\n        cur = tf.nn.elu(tf.matmul(cur, W) + B)\n        i += 1\n    W = init_weights(""W"" + str(i), [b, output_size])\n    B = init_weights(""B"" + str(i), [1, output_size])\n    prediction = tf.matmul(cur, W) + B\n    return prediction\n\ndef leaky_relu(x, alpha):\n    return tf.maximum(x, alpha * x)\n\nclass RopeImitator():\n\n    def __init__(self, name, unfreeze_time=30000, autoencode=False,\n        action_lr=1e-4, deconv_lr=1e-3, fwd_consist=False, baseline_reg=False, softmaxBackprop=True,\n        gtAction=False):\n        self.unfreeze_time = unfreeze_time\n        self.autoencode = autoencode\n        self.gtAction = gtAction\n        self.name = \'{0}_{1}_{2}_{3}_{4}_{5}K_{6}_{7}\'.format(name, \'fwdconsist\' + str(fwd_consist), \'baselinereg\' + str(baseline_reg), \n            \'deconv_lr\' + str(deconv_lr), \'autoencode\' + str(autoencode),\n            \'unfreeze\' + str(int(unfreeze_time/1000.)), \'softmax\' + str(softmaxBackprop),\n            \'gtAction\' + str(gtAction))\n        self.fwd_consist = fwd_consist\n        self.start = 0\n\n        self.batch_loader = rope_data\n\n        self.image_ph = tf.placeholder(tf.float32, [None, 200, 200, 3], name=\'image_ph\')\n        self.goal_image_ph = tf.placeholder(tf.float32, [None, 200, 200, 3], name=\'goal_image_ph\')\n        self.location_ph = tf.placeholder(tf.float32, [None, LOCATION_BINS], name=\'location_ph\')\n        self.theta_ph = tf.placeholder(tf.float32, [None, THETA_BINS], name=\'theta_ph\')\n        self.length_ph = tf.placeholder(tf.float32, [None, LENGTH_BINS], name=\'length_ph\')\n        self.ignore_flag_ph = tf.placeholder(tf.float32, [None], name=\'ignore_flag_ph\')\n        self.is_training_ph = tf.placeholder(tf.bool, name=\'is_training_ph\')\n        self.autoencode_ph = tf.placeholder(tf.bool)\n        self.gtAction_ph = tf.placeholder(tf.bool)\n\n        # get latent representations for both the images\n        latent_image, latent_conv5_image = alexnet_geurzhoy.network(self.image_ph, trainable=True, num_outputs=ENCODING_SIZE)\n        latent_goal_image, latent_conv5_goal_image = alexnet_geurzhoy.network(self.goal_image_ph, trainable=True, num_outputs=ENCODING_SIZE, reuse=True)\n\n        # concatenate the latent representations and share information\n        features = tf.concat(1, [latent_image, latent_goal_image])\n\n        with tf.variable_scope(""concat_fc""):\n            x = tf.nn.relu(features)\n            x = slim.fully_connected(x, FEAT_SIZE, scope=""concat_fc"")\n\n        #################################\n        # ACTION PREDICTION\n        #################################\n        location_embedding = init_weights(\'location_embedding\', [LOCATION_BINS, LOCATION_EMBEDDING_SIZE])\n        theta_embedding = init_weights(\'theta_embedding\', [THETA_BINS, THETA_EMBEDDING_SIZE])\n\n        # layer for predicting X, Y\n        with tf.variable_scope(\'location_pred\'):\n            loc_network_layers = [FEATURE_SIZE, 200, 200, LOCATION_BINS]\n            location_pred = make_network(x, loc_network_layers)\n            location_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(location_pred, self.location_ph))\n\n            location_sample = math_ops.argmax(tf.cond(self.is_training_ph, lambda: self.location_ph, lambda: location_pred), 1)\n            location_embed = embedding_ops.embedding_lookup(location_embedding, location_sample)\n\n        # layer for predicting theta\n        with tf.variable_scope(\'theta_pred\'):\n            x_with_loc = tf.concat(1, [x, location_embed])\n            theta_network_layers = [FEATURE_SIZE + LOCATION_EMBEDDING_SIZE, 200, 200, THETA_BINS]\n            theta_pred = make_network(x_with_loc, theta_network_layers)\n            theta_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(theta_pred, self.theta_ph))\n\n            theta_sample = math_ops.argmax(tf.cond(self.is_training_ph, lambda: self.theta_ph, lambda: theta_pred), 1)\n            theta_embed = embedding_ops.embedding_lookup(theta_embedding, theta_sample)\n\n        # layer for predicting length of movement\n        with tf.variable_scope(\'length_pred\'):\n            x_with_loc_theta = tf.concat(1, [x_with_loc, theta_embed])\n            length_network_layers = [FEATURE_SIZE + LOCATION_EMBEDDING_SIZE + THETA_EMBEDDING_SIZE, 200, 200, LENGTH_BINS]\n            length_pred = make_network(x_with_loc_theta, length_network_layers)\n            length_softmax = tf.nn.softmax_cross_entropy_with_logits(length_pred, self.length_ph)\n            length_loss = tf.reduce_mean(length_softmax * self.ignore_flag_ph)\n\n        # add to collections for retrieval\n        tf.add_to_collection(\'location_logit\', location_pred)\n        tf.add_to_collection(\'theta_logit\', theta_pred)\n        tf.add_to_collection(\'len_logit\', length_pred)\n\n        # variables of only inverse model without features\n        inv_vars_no_alex = [v for v in tf.trainable_variables() if \'alexnet\' not in v.name]\n        print(\'Action prediction tensors consist {0} out of {1}\'.format(len(inv_vars_no_alex), len(tf.trainable_variables())))\n\n\n        total_loss = location_loss + theta_loss + length_loss\n\n        action_optimizer = tf.train.AdamOptimizer(action_lr)\n\n        action_grads, _ = zip(*action_optimizer.compute_gradients(total_loss, inv_vars_no_alex))\n        action_grads, _ = tf.clip_by_global_norm(action_grads, GRAD_CLIP_NORM)\n        action_grads = zip(action_grads, inv_vars_no_alex)\n\n        action_grads_full, _ = zip(*action_optimizer.compute_gradients(total_loss, tf.trainable_variables()))\n        action_grads_full, _ = tf.clip_by_global_norm(action_grads_full, GRAD_CLIP_NORM)\n        action_grads_full = zip(action_grads_full, tf.trainable_variables())\n\n        #################################\n        # FORWARD CONSISTENCY\n        #################################\n        if self.fwd_consist:\n            with tf.variable_scope(\'fwd_consist\'):\n                if softmaxBackprop:\n                    location_pred = tf.nn.softmax(location_pred)\n                    theta_pred = tf.nn.softmax(theta_pred)\n                    length_pred = tf.nn.softmax(length_pred)\n\n                # baseline regularization => gradients flow only to alexnet, not action pred\n                if baseline_reg:\n                    print(\'baseline\')\n                    action_embed = tf.concat(1, [self.location_ph, self.theta_ph, self.length_ph])\n                else:\n                    # fwd_consist => gradients flow through action prediction\n                    latent_conv5_image = tf.stop_gradient(latent_conv5_image)\n                    action_embed = tf.cond(self.gtAction_ph,\n                        lambda: tf.concat(1, [self.location_ph, self.theta_ph, self.length_ph]),\n                        lambda: tf.concat(1, [location_pred, theta_pred, length_pred]))\n\n                action_embed = slim.fully_connected(action_embed, 363)\n                action_embed = tf.reshape(action_embed, [-1, 11, 11, 3])\n                # concat along depth\n                fwd_features = tf.concat(3, [latent_conv5_image, action_embed])\n                # deconvolution\n                batch_size = tf.shape(fwd_features)[0]\n\n                wt1 = tf.Variable(tf.truncated_normal([5, 5, 64, 259], stddev=0.1))\n                deconv1 = tf.nn.conv2d_transpose(fwd_features, wt1, [batch_size, 22, 22, 64], [1, 2, 2, 1])\n                deconv1 = leaky_relu(deconv1, 0.2)\n                wt2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n                deconv2 = tf.nn.conv2d_transpose(deconv1, wt2, [batch_size, 44, 44, 32], [1, 2, 2, 1])\n                deconv2 = leaky_relu(deconv2, 0.2)\n                wt3 = tf.Variable(tf.truncated_normal([5, 5, 3, 32], stddev=0.1))\n                deconv3 = tf.nn.conv2d_transpose(deconv2, wt3, [batch_size, 88, 88, 3], [1, 2, 2, 1])\n                deconv3 = tf.nn.tanh(deconv3)\n                # loss from upsampled deconvolution and goal image\n                upsampled_deconv_img = tf.image.resize_images(deconv3, [200, 200])\n                tf.add_to_collection(\'upsampled_deconv_img\', upsampled_deconv_img)\n\n                # image inputs are -255 to 255 ??? for some reason\n                # whether to autoencode or not\n\n                normalized_goal_img = tf.cond(self.autoencode_ph, lambda: self.image_ph / 255.0, lambda: self.goal_image_ph / 255.0)\n\n                # just to visualize\n                deconv_log_img = (upsampled_deconv_img + 1.0) * 127.5\n\n                # variables of only forward model\n                fwd_vars = [v for v in tf.trainable_variables() if \'fwd_consist\' in v.name]\n                print(\'Forward consistency tensors consist {0} out of {1}\'.format(len(fwd_vars), len(tf.trainable_variables())))\n\n                fwd_consist_loss = tf.reduce_mean(tf.abs(upsampled_deconv_img - normalized_goal_img))\n                deconv_optimizer = tf.train.AdamOptimizer(deconv_lr)\n\n                fwd_consist_grads, _ = zip(*deconv_optimizer.compute_gradients(fwd_consist_loss, fwd_vars))\n                fwd_consist_grads, _ = tf.clip_by_global_norm(fwd_consist_grads, GRAD_CLIP_NORM)\n                fwd_consist_grads = zip(fwd_consist_grads, fwd_vars)\n\n                fwd_consist_grads_full, _ = zip(*deconv_optimizer.compute_gradients(fwd_consist_loss, tf.trainable_variables()))\n                fwd_consist_grads_full, _ = tf.clip_by_global_norm(fwd_consist_grads_full, GRAD_CLIP_NORM)\n                fwd_consist_grads_full = zip(fwd_consist_grads_full, tf.trainable_variables())\n\n                self.optimize_fwd_freeze = deconv_optimizer.apply_gradients(fwd_consist_grads)\n\n                with tf.control_dependencies([fwd_consist_grads_full[0][0][0], action_grads_full[0][0][0]]):\n                    self.optimize_fwd_full = deconv_optimizer.apply_gradients(fwd_consist_grads_full)\n                    self.optimize_action_full = action_optimizer.apply_gradients(action_grads_full)\n\n        self.optimize_action_no_alex = action_optimizer.apply_gradients(action_grads)\n        self.optimize_action_alex = action_optimizer.apply_gradients(action_grads_full)\n\n        #################################\n        # LOGGING AND SAVING OPERATIONS\n        #################################\n        loc_correct_pred = tf.equal(tf.argmax(location_pred, 1), tf.argmax(self.location_ph, 1))\n        self.loc_accuracy = tf.reduce_mean(tf.cast(loc_correct_pred, tf.float32))\n\n        theta_correct_pred = tf.equal(tf.argmax(theta_pred, 1), tf.argmax(self.theta_ph, 1))\n        self.theta_accuracy = tf.reduce_mean(tf.cast(theta_correct_pred, tf.float32))\n\n        length_correct_pred = tf.equal(tf.argmax(length_pred, 1), tf.argmax(self.length_ph, 1))\n        self.length_accuracy = tf.reduce_mean(tf.cast(length_correct_pred, tf.float32))\n\n        # logging\n        tf.summary.scalar(\'model/location_loss\', location_loss, collections=[\'train\'])\n        tf.summary.scalar(\'model/theta_loss\', theta_loss, collections=[\'train\'])\n        tf.summary.scalar(\'model/length_loss\', length_loss, collections=[\'train\'])\n        if self.fwd_consist:\n            tf.summary.scalar(\'model/fwd_consist_loss\', fwd_consist_loss, collections=[\'train\'])\n            tf.summary.image(\'upsampled_deconv_image\', deconv_log_img, max_outputs=5, collections=[\'train\'])\n\n        tf.summary.image(\'before\', (self.image_ph + 255.0) / 2.0, max_outputs=5, collections=[\'train\'])\n        tf.summary.image(\'after\', (self.goal_image_ph + 255.0) / 2.0, max_outputs=5, collections=[\'train\'])\n\n        self.train_summaries = tf.summary.merge_all(\'train\')\n\n        self.writer = tf.summary.FileWriter(\'./results/{0}/logs/{1}\'.format(self.name, time.time()))\n\n        self.saver = tf.train.Saver(max_to_keep=None)\n\n        self.sess = tf.Session(config=CONFIG)\n        self.sess.run(tf.global_variables_initializer())\n\n        self.model_directory = \'./results/{0}/models/\'.format(self.name)\n        if not os.path.exists(self.model_directory):\n            os.makedirs(self.model_directory)\n\n    def get_batch(self, batch_size, is_training):\n        dataset = \'train\' if is_training else \'val\'\n\n        image, goal_image, location, theta, length, d, c, ignore_flag = self.batch_loader.get_batch(dataset, batch_size)\n        print(dataset, location.shape, theta.shape, length.shape)\n\n        feed_dict = {\n            self.image_ph: image,\n            self.goal_image_ph: goal_image,\n            self.location_ph: location,\n            self.theta_ph: theta,\n            self.length_ph: length,\n            self.ignore_flag_ph: ignore_flag,\n            self.is_training_ph: is_training,\n            self.autoencode_ph: False,\n            self.gtAction_ph: False\n        }\n\n        return feed_dict\n\n    def train(self, iterations):\n        for i in range(self.start, iterations):\n            print(i)\n            feed_dict = self.get_batch(BATCH_SIZE, True)\n\n            ops_to_run = []\n            if i < self.unfreeze_time:\n                ops_to_run.append(self.optimize_action_no_alex)\n                if self.fwd_consist:\n                    ops_to_run.append(self.optimize_fwd_freeze)\n                    if self.autoencode and i < self.unfreeze_time * (2/3):\n                        feed_dict[self.autoencode_ph] = True\n                if self.gtAction:\n                    feed_dict[self.gtAction_ph] = True\n            else:\n                if self.fwd_consist:\n                    ops_to_run.append(self.optimize_fwd_full)\n                    ops_to_run.append(self.optimize_action_full)\n                else:\n                    ops_to_run.append(self.optimize_action_alex)\n\n\n            ops_to_run.append(self.train_summaries)\n            op_results = self.sess.run(ops_to_run, feed_dict=feed_dict)\n            train_summaries = op_results[-1]\n\n            if i % 100 == 0:\n                self.writer.add_summary(train_summaries, i)\n\n            # validate on 1000 images\n            # split into batches of 100 because of memory issues\n            if i % 1000 == 0:\n                self.saver.save(self.sess, self.model_directory + \'inverse\', global_step=i)\n                print(\'Saved at timestep {0}\'.format(i))\n\n                cum_loc_acc, cum_theta_acc, cum_len_acc = 0, 0, 0\n\n                for _ in range(10):\n                    val_dict = self.get_batch(100, False)\n                    loc_acc, theta_acc, len_acc = self.sess.run([self.loc_accuracy, self.theta_accuracy, self.length_accuracy], feed_dict=val_dict)\n\n                    cum_loc_acc += loc_acc\n                    cum_theta_acc += theta_acc\n                    cum_len_acc += len_acc\n\n                cum_loc_acc, cum_theta_acc, cum_len_acc = cum_loc_acc / 10.0, cum_theta_acc / 10.0, cum_len_acc / 10.0\n\n                summaries = tf.Summary(value=[tf.Summary.Value(tag=\'val/loc_acc\', simple_value=cum_loc_acc), tf.Summary.Value(tag=\'val/theta_acc\', simple_value=cum_theta_acc), tf.Summary.Value(tag=\'val/len_acc\', simple_value=cum_len_acc)])\n                self.writer.add_summary(summaries, i)\n\n            self.writer.flush()\n\n    def restore(self, iteration, model_name=None):\n        if model_name == None:\n            model_name = self.name\n        self.start = iteration\n\n        saved_model_directory = \'./results/{0}/models/\'.format(model_name)\n        self.saver.restore(self.sess, saved_model_directory + \'inverse-{0}\'.format(iteration))\n        print(\'Loaded model {0} at iteration {1}\'.format(model_name, iteration))\n\n    # print statistics of data\n    # use to check to see if you\'ve downloaded the correct dataset\n    def stats(self):\n        # validation data\n        v_loc, v_theta, v_len, t_loc, t_theta, t_len = [], [], [], [], [], []\n        for i in range(3):\n            val_dict = self.get_batch(1000, False)\n            v_loc.append(np.argmax(val_dict[self.location_ph], axis=1))\n            v_theta.append(np.argmax(val_dict[self.theta_ph], axis=1))\n            v_len.append(np.argmax(val_dict[self.length_ph], axis=1))\n\n        for i in range(10):\n            train_dict = self.get_batch(1000, True)\n            t_loc.append(np.argmax(train_dict[self.location_ph], axis=1))\n            t_theta.append(np.argmax(train_dict[self.theta_ph], axis=1))\n            t_len.append(np.argmax(train_dict[self.length_ph], axis=1))\n\n        fig, axes = plt.subplots(2, 3)\n        axes[0, 0].set_title(\'val_locs\')\n        axes[0, 0].hist(np.concatenate(v_loc))\n        axes[0, 1].set_title(\'val_theta\')\n        axes[0, 1].hist(np.concatenate(v_theta))\n        axes[0, 2].set_title(\'val_lens\')\n        axes[0, 2].hist(np.concatenate(v_len))\n        axes[1, 0].set_title(\'train_locs\')\n        axes[1, 0].hist(np.concatenate(t_loc))\n        axes[1, 1].set_title(\'train_theta\')\n        axes[1, 1].hist(np.concatenate(t_theta))\n        axes[1, 2].set_title(\'train_lens\')\n        axes[1, 2].hist(np.concatenate(t_len))\n        plt.show()\n'"
data/__init__.py,0,b''
data/cropping_utils.py,0,"b'""""""Cropping utils""""""\n\nimport itertools\nimport window\nimport numpy as np\nimport cv2\nimport random\n\nCROP_SIZE = 200\ndef crop(img, x, y):\n    """"""Crop img with offsets x and y""""""\n    if len(img.shape) == 3:\n        new_img = img[y:y+CROP_SIZE, x:x+CROP_SIZE, :]\n    if len(img.shape) == 2:\n        new_img = img[y:y+CROP_SIZE, x:x+CROP_SIZE]\n    return new_img\n\nclass RandomCropper:\n    def __init__(self, ox, oy, square, r, dx=None, dy=None):\n        """"""Params of random cropping:\n        ox = offset x\n        oy = offset y\n        square = initial square crop size\n        r = resize square size\n        dx, dy = constant offset\n        Then each image is further randomly cropped to CROP_SIZE.\n        """"""\n        self.params = [ox, oy, square, r]\n        self.dx = dx\n        self.dy = dy\n\n    def random_crop(self, images, poke):\n        interior = False\n        for _ in range(10):\n            dx = self.dx or random.randint(0, 40)\n            dy = self.dy or random.randint(0, 40)\n            poke_cropped = self.resized_cropped_poke(poke, dx, dy)\n            if self.is_interior_poke(poke_cropped):\n                cropped_images = []\n                for image in images:\n                    cropped_images.append(crop(image, dx, dy))\n                poke_cropped = self.resized_cropped_poke(poke, dx, dy)\n                return cropped_images, poke_cropped\n        return None\n\n    def resize(self, img):\n        """"""Resizes img into r x r by first cropping a square x square offset by (ox, oy)""""""\n        ox, oy, square, r = self.params\n        return cv2.resize(img[oy:oy+square, ox:ox+square, :], (r, r))\n\n    def resized_cropped_poke(self, poke, dx, dy):\n        ox, oy, square, r = self.params\n        x, y, theta, length = poke\n        x_new = ((x - ox) / float(square) * r - dx) / float(CROP_SIZE)\n        y_new = ((y - oy) / float(square) * r - dy) / float(CROP_SIZE)\n        return x_new, y_new, theta, length\n\n\n    def cropped_poke_to_real(self, poke, dx, dy):\n        ox, oy, square, r = self.params\n        x, y, theta, length = poke\n        x_new = ((x * CROP_SIZE) + dx) * square / float(r) + ox\n        y_new = ((y * CROP_SIZE) + dy) * square / float(r) + oy\n        return x_new, y_new, theta, length\n\n    def is_interior_poke(self, poke):\n        x, y, _, _ = poke\n        return all([x < 0.9, x > 0.1, y < 0.9, y > 0.1])\n'"
data/database.py,0,"b'import cv2\n#from cv_bridge import CvBridge, CvBridgeError\nimport lmdb\nimport caffe\nimport numpy as np\nimport scipy\nimport itertools\n\n#bridge = CvBridge()\n\ndef save_vector(vector, label, db):\n    x = vector\n    if len(vector.shape) != 3:\n        x = vector[:, np.newaxis, np.newaxis]\n    datum = caffe.io.array_to_datum(x, long(label))\n    str_id = \'{:08}\'.format(label)\n    with db.begin(write=True) as txn:\n        txn.put(str_id.encode(\'ascii\'), datum.SerializeToString())\n\ndef save_img(data, label, db, size = [227, 227, 3], transpose = True, scale = 1.0, convert_from_ros = False):\n    if convert_from_ros:\n        cv_image = bridge.imgmsg_to_cv2(data).astype(int)\n    else:\n        cv_image = data\n    x = np.asarray(cv_image) * scale\n    x = x.astype(np.uint8)\n    if len(x.shape) == 2:\n        x = x[:, :, None]\n    if size:\n        # n = x.shape[2]\n        # s = size[:].append(n)\n        x = scipy.misc.imresize(x, s)\n    if transpose:\n        x = x.transpose()\n    datum = caffe.io.array_to_datum(x, 0)\n    str_id = \'{:08}\'.format(label)\n    with db.begin(write=True) as txn:\n        txn.put(str_id.encode(\'ascii\'), datum.SerializeToString())\n\nclass ImageLMDB(object):\n    def __init__(self, name, size = None, scale = 1, convert_from_ros = True):\n        self.name = name\n        self.DB = lmdb.open(name, 99999999999)\n        self.i = self.get_num_elements()\n        self.size = size\n        self.scale = scale\n        self.convert_from_ros = convert_from_ros\n\n    def save(self, data, label = None):\n        if not label:\n            label = self.i\n        save_img(data, label, self.DB, self.size, scale = self.scale, convert_from_ros = self.convert_from_ros)\n        self.i += 1\n\n    def get_num_elements(self):\n        return int(self.DB.stat()[\'entries\'])\n\n    def play(self, millis = 20, transpose = True, start = 0, end = 0, key = 0):\n        cv2.namedWindow(self.name)\n        img_size = None\n        with self.DB.begin() as txn:\n            cursor = txn.cursor()\n            # print ""got here""\n            # label = \'{:08}\'.format(start).encode(\'ascii\')\n            # cursor.set_key(label)\n            if start:\n                for _ in xrange(start):\n                    cursor.next()\n            elif end:\n                cursor.last()\n                for _ in xrange(end):\n                    cursor.prev()\n            elif key:\n                cursor = txn.cursor()\n                label = \'{:08}\'.format(key).encode(\'ascii\')\n                print ""key"", cursor.set_key(label)\n            for key, img_datum in cursor:\n                datum = caffe.proto.caffe_pb2.Datum()\n                datum.ParseFromString(img_datum)\n                img = caffe.io.datum_to_array(datum) # .astype(np.uint8)\n                if not img_size:\n                    print img.shape\n                    img_size = img.shape\n                if transpose:\n                    img = img.transpose()\n                cv2.imshow(self.name, img)\n                cv2.waitKey(millis)\n        return img_size\n\n    def info(self):\n        print self.DB.stat()\n\n    def images(self, start = 0, transpose = True):\n        with self.DB.begin() as txn:\n            cursor = txn.cursor()\n            label = \'{:08}\'.format(start).encode(\'ascii\')\n            cursor.set_key(label)\n            for key, img_datum in cursor:\n                datum = caffe.proto.caffe_pb2.Datum()\n                datum.ParseFromString(img_datum)\n                img = caffe.io.datum_to_array(datum).astype(np.uint8)\n                if transpose:\n                    img = img.transpose()\n                yield img\n\n    def iterator(self):\n        return self.images()\n\n    def rollback_once(self):\n        self.i = self.i - 1\n        with self.DB.begin(write=True) as txn:\n            label = \'{:08}\'.format(self.i).encode(\'ascii\')\n            txn.delete(label)\n\nclass SensorLMDB(object):\n    def __init__(self, name):\n        self.name = name\n        self.DB = lmdb.open(name, 99999999999)\n        self.i = self.get_num_elements()\n\n    def save(self, data, label = None):\n        if not label:\n            label = self.i\n        save_vector(data, label, self.DB)\n        self.i += 1\n\n    def get_num_elements(self):\n        return int(self.DB.stat()[\'entries\'])\n\n    def play(self):\n        with self.DB.begin() as txn:\n            cursor = txn.cursor()\n            for key, sensor_datum in cursor:\n                datum = caffe.proto.caffe_pb2.Datum()\n                datum.ParseFromString(sensor_datum)\n                vec = caffe.io.datum_to_array(datum)\n                print vec\n\n    def readings(self, start = 0):\n        with self.DB.begin() as txn:\n            cursor = txn.cursor()\n            label = \'{:08}\'.format(start).encode(\'ascii\')\n            cursor.set_key(label)\n            for key, sensor_datum in cursor:\n                datum = caffe.proto.caffe_pb2.Datum()\n                datum.ParseFromString(sensor_datum)\n                vec = caffe.io.datum_to_array(datum)\n                yield vec\n\n    def iterator(self):\n        return self.readings()\n\n    def info(self):\n        print self.DB.stat()\n        print self.DB.info()\n\n    def rollback_once(self):\n        self.i = self.i - 1\n        with self.DB.begin(write=True) as txn:\n            label = \'{:08}\'.format(self.i).encode(\'ascii\')\n            txn.delete(label)\n\ndef test():\n    db = ImageLMDB(""/home/ashvin/data/static/arm_motion"", transpose=False)\n    db.play()\n\nif __name__ == ""__main__"":\n    # db = ImageLMDB(""/home/ashvin/data/poke/depth_after"")\n    db = ImageLMDB(""rope9/train/image_after"")\n    db2 = SensorLMDB(\'rope9/train/poke\')\n    db.info()\n    db2.info()\n    # db.play(40)\n\n    # vel_db = SensorLMDB(""/home/ashvin/data/poke/"")\n    # pos_db = SensorLMDB(""/home/ashvin/data/poke/positions"")\n    # vel_db.play()\n    # pos_db.play()\n'"
data/rope_data.py,0,"b'""""""Interface to the baxter data (todo)""""""\n\nimport sys\nimport os\nsys.path.insert(1, os.path.join(sys.path[0], \'..\'))\n\nimport database\nimport itertools\nimport window\nimport numpy as np\nimport scipy\nimport random\nimport math\nfrom cropping_utils import RandomCropper\n\npoke_data_dir = os.path.dirname(os.path.abspath(__file__)) + \'/\'\nDATASET_LOCATION = poke_data_dir + \'datasets/\'\n\nRESIZE_SIZE = 240\n# X_MIN = 150\n# Y_MIN = 110\n# SQUARE_SIZE = 300\nX_MIN = 150\nY_MIN = 80\nSQUARE_SIZE = 400\nrc = RandomCropper(X_MIN, Y_MIN, SQUARE_SIZE, RESIZE_SIZE)\n\nimg_mean = np.load(open(poke_data_dir + ""img_mean.npy""))\ncropped_mean = rc.resize(img_mean)\n\n# create a filter to filter out the non-table parts\ntable_filter = np.zeros((240, 240))\nfor y in range(240):\n    for x in range(240):\n        if y > 25 and y > 2.5 * x - 450:\n            table_filter[y, x] = 1\n\nSEGMENTATION_SIZE = 20\ndef resize_segmentation(img):\n    return scipy.misc.imresize(img, (SEGMENTATION_SIZE, SEGMENTATION_SIZE))\n\ndef rectify(img):\n    """"""Transpose database image to [Y, X, channels] (480x640x3)""""""\n    return np.transpose(img, [1, 2, 0])\n\ndef get_runs(runs, stop=True):\n    while True:\n        random.shuffle(runs)\n        for r in runs:\n            db_images = database.ImageLMDB(r + ""/image"")\n            db_pokes = database.SensorLMDB(r + ""/poke"")\n            print ""loading from"", r, ""size"", db_pokes.i,\n            yield db_images.images(), db_pokes.readings()\n        if stop:\n            break\n\ndef get_rope_segmentation(normalized_image):\n    im = np.sum(abs(normalized_image), 2) > 100\n    return np.logical_and(im, table_filter)\n\ndef get_data(dataset, shuffle=True):\n    """"""dataset: name\n    """"""\n    location = DATASET_LOCATION + dataset\n    db_image_before = database.ImageLMDB(location + ""/image_before"", convert_from_ros = False)\n    db_image_after = database.ImageLMDB(location + ""/image_after"", convert_from_ros = False)\n    db_poke = database.SensorLMDB(location + ""/poke"")\n\n    while True:\n        iterators = [db_image_before.images(), db_image_after.images(), db_poke.readings()]\n        for image_before, image_after, poke in itertools.izip(*iterators):\n            n_before = normalize(image_before)\n            n_after = normalize(image_after)\n            before_segment = get_rope_segmentation(n_before)\n            after_segment = get_rope_segmentation(n_after)\n\n            imgs = [n_before, n_after, before_segment, after_segment]\n            c_data = rc.random_crop(imgs, poke[:4, 0, 0])\n            if c_data:\n                cropped_images, cropped_poke = c_data\n                img_before, img_after, s_before, s_after = cropped_images\n                s_b = resize_segmentation(s_before)\n                s_a = resize_segmentation(s_after)\n                im = np.sum(abs(img_before.astype(float) - img_after.astype(float)), 2) > 100\n                diff = np.sum(im) # rough measure of number of pixels different between the two images\n                if diff > 1000:\n                    yield img_before, img_after, s_b, s_a, cropped_poke\n\n# up until Sep 8\n# TRAIN_DATA = [DATA + ""run_rope_"" + str(i) for i in range(3, 10)] # 9832\n# TEST_DATA = [DATA + ""run_rope_"" + str(i) for i in range(10, 11)] # 579\n\nTRAIN_DATA = ""rope9/train""\nTEST_DATA = ""rope9/test""\n\nif os.path.exists(DATASET_LOCATION): # don\'t fail import on machine where data not present\n    print(\'Data loaded\')\n    print(TRAIN_DATA)\n    SOURCES = {""train"": get_data(TRAIN_DATA), ""val"": get_data(TEST_DATA)}\n\ndef load_dataset(name):\n    print ""Loading dataset:"", name\n    SOURCES[""train""] = get_data(name + ""/train"")\n    # SOURCES[""val""] = get_data(name + ""/test"")\n\ndef get_size(dataset):\n    s = 0\n    for run in dataset:\n        db1 = database.ImageLMDB(run + ""/image"")\n        db2 = database.ImageLMDB(run + ""/poke"")\n        print run, db1.i, db2.i\n        s += db1.i\n    return s\n\ndef normalize(image):\n    # return (image.astype(float) - 127.0) / 100.0\n    return image.astype(float) - cropped_mean\n\ndistrib = np.zeros((11))\ndef get_batch(source_name, batch_size = 1):\n    """"""X is the input to the network, Y is the supervision/ground truth""""""\n    X1 = np.zeros((batch_size, 200, 200, 3))\n    X2 = np.zeros((batch_size, 200, 200, 3))\n    Y1 = np.zeros((batch_size, 400))\n    Y2 = np.zeros((batch_size, 36))\n    Y3 = np.zeros((batch_size, 10))\n    Z  = np.ones((batch_size)) # ignore length flag\n    S1 = np.zeros((batch_size, SEGMENTATION_SIZE, SEGMENTATION_SIZE))\n    S2 = np.zeros((batch_size, SEGMENTATION_SIZE, SEGMENTATION_SIZE))\n    # Y = np.zeros((batch_size, max_len))\n    for example, i in itertools.izip(SOURCES[source_name], xrange(batch_size)):\n        image_before, image_after, s_before, s_after, poke = example\n        X1[i, :, :, :] = image_before\n        X2[i, :, :, :] = image_after\n        S1[i, :, :] = s_before\n        S2[i, :, :] = s_after\n        x, y, theta, length = poke\n        # if length > 0.1005 and length < 0.1006:\n        if length > 0.1:\n            Z[i] = 0\n            distrib[10] += 1\n        Y1[i, :] = window.encode_pixel(x, y)[0]\n        Y2[i, :] = window.encode_theta(theta)[0]\n        l_arr, l_ind = window.encode_length(length)\n        Y3[i, :] = l_arr\n        distrib[l_ind] += 1\n    # print distrib\n    return X1, X2, Y1, Y2, Y3, S1, S2, Z\n\nif __name__ == ""__main__"":\n    # for example, i in itertools.izip(SOURCES[""train""], xrange(30000)):\n    #     image_before, image_after, length, pixel, theta = example\n    #     print pixel, window.encode_pixel(*pixel)[1]\n    #     if i % 1000 == 0:\n    #         print i\n    t = 0\n    # for image_before, image_after, poke in get_data(TRAIN_DATA):\n    #     t += 1\n    #     print poke\n    # print t\n\n    # for i, _ in enumerate(get_data(TEST_DATA, True)):\n    #     pass\n    s = get_size(TRAIN_DATA)\n    print ""train data size"", s\n    # for i, _ in enumerate(get_data(TRAIN_DATA, True)):\n    #     pass\n    s = get_size(TEST_DATA)\n    print ""test data size"", s\n'"
data/window.py,0,"b'import numpy as np\nimport random\nimport math\nimport bisect\n\nIN = 240\nOUT = 240\n\ndef crop(img, x, y):\n    new_img = img[:, y:y+OUT, x:x+OUT]\n    return new_img\n\ndef shift_and_normalize(action, dx, dy):\n    action[0, 0, 0] *= IN\n    action[0, 0, 0] -= dx\n    action[1, 0, 0] *= IN\n    action[1, 0, 0] -= dy\n\n    x, y = action\n    ret = encode_pixel(x, y)\n\n    action[0, 0, 0] /= OUT\n    action[1, 0, 0] /= OUT\n    return ret, action, ind\n\nXY_BINS = 20\ndx = 1.0 / XY_BINS\ndef encode_pixel(x, y):\n    """"""Expect x, y from 0-1""""""\n    # TODO: this should really be an assert\n    x = min(0.9999, max(0, x))\n    y = min(0.9999, max(0, y))\n\n    arr = np.zeros((XY_BINS * XY_BINS))\n    x, y = int(x/dx), int(y/dx)\n    x, y = min(max(0, x), XY_BINS-1), min(max(0, y), XY_BINS-1)\n    ind = XY_BINS * x + y\n    arr[ind] = 1\n    return arr, ind\n\n# LENGTH_BINS = 10\n# dl = 0.04 / LENGTH_BINS\n# def encode_length(l):\n#     """"""Length in meters""""""\n#     l = min(0.04999, max(0.01, l))\n\n#     arr = np.zeros((LENGTH_BINS))\n#     ind = int((l - 0.01)/dl)\n#     arr[ind] = 1\n#     return arr, ind\n\n# LENGTH_BINS = 10\n# MIN_L = 0.01\n# MAX_L = 0.149999\n# dl = (MAX_L - MIN_L) / LENGTH_BINS\n# def encode_length(l):\n#     """"""Length in meters""""""\n#     l = min(MAX_L, max(MIN_L, l))\n\n#     arr = np.zeros((LENGTH_BINS))\n#     ind = int((l - MIN_L)/dl)\n#     arr[ind] = 1\n#     return arr, ind\n\nLENGTH_BINS = 10\n# LENGTH_BINS_SORTED = [0.031354602426290512, 0.05083392933011055, 0.059834115207195282, 0.074937090277671814, 0.090499997138977051, 0.1005, 0.1006, 0.11, 0.11050000041723251, 0.15] # obtained by binning the first 10000 pokes uniformly\nLENGTH_BINS_SORTED = [0.020790038630366325, 0.03206624835729599, 0.04270794615149498, 0.051202502101659775, 0.056311529129743576, 0.062374196946620941, 0.072047881782054901, 0.081927493214607239, 0.091911002993583679, 0.15]\ndef encode_length(l):\n    """"""Length in meters binned by a fixed array""""""\n    arr = np.zeros((LENGTH_BINS))\n    ind = bisect.bisect_right(LENGTH_BINS_SORTED, l)\n    if ind == 10:\n        # print ""UH OH length"", l\n        ind = 9\n\n    arr[ind] = 1\n    return arr, ind\n\nTHETA_BINS = 36\ndt = 2 * math.pi / THETA_BINS\ndef encode_theta(t):\n    arr = np.zeros((THETA_BINS))\n    ind = int(t / dt)\n    arr[ind] = 1\n    return arr, ind\n\n# class WindowLayer(caffe.Layer):\n\n#     def setup(self, bottom, top):\n#         # check input pair\n#         pass\n\n#     def reshape(self, bottom, top):\n#         N = bottom[0].num # batch size\n\n#         top[0].reshape(2 * N, 3, OUT, OUT)\n#         top[1].reshape(N, XY_BINS * XY_BINS, 1, 1)\n#         top[2].reshape(N, 2, 1, 1)\n#         top[3].reshape(N, 2, 1, 1)\n#         top[4].reshape(N, LENGTH_BINS)\n#         top[5].reshape(N, THETA_BINS)\n#         top[6].reshape(N, 1)\n#         top[7].reshape(N, 1)\n#         top[8].reshape(N, 1)\n\n#     def forward(self, bottom, top):\n#         N = bottom[0].num # batch size\n#         for i in range(N):\n#             img_before = bottom[0].data[i, :, :, :]\n#             img_after = bottom[1].data[i, :, :, :]\n#             action = np.zeros((2, 1, 1))\n#             action[:2, :, :] = np.copy(bottom[2].data[i, :2, :, :])\n#             x, y = bottom[2].data[i, :, 0, 0]\n#             length = bottom[3].data[i, 0, 0, 0]\n#             theta = bottom[4].data[i, 0, 0, 0]\n\n#             # dx = random.randint(max(1, int(x) - 200), max(1, min(250, int(x-30))))\n#             # dy = random.randint(max(1, int(y) - 200), max(1, min(250, int(y-30))))\n\n#             dx = random.randint(0, 40)\n#             dy = random.randint(0, 40)\n\n#             top[0].data[i, :, :, :] = crop(img_before, dx, dy)\n#             top[0].data[N + i, :, :, :] = crop(img_after, dx, dy)\n#             array, action, xy_ind = shift_and_normalize(action, dx, dy)\n#             top[1].data[i, :, 0, 0] = array\n\n#             jitter = np.zeros((2, 1, 1))\n#             jitter[0, 0, 0] = dx\n#             jitter[1, 0, 0] = dy\n#             top[2].data[i, :, :, :] = jitter\n\n#             top[3].data[i, :, :, :] = action\n\n#             top[4].data[i, :], length_ind = encode_length(length)\n\n#             top[5].data[i, :], theta_ind = encode_theta(theta)\n\n#             top[6].data[i, 0] = xy_ind\n#             top[7].data[i, 0] = length_ind\n#             top[8].data[i, 0] = theta_ind\n\n#     def backward(self, top, propagate_down, bottom):\n#         pass\n'"
nets/__init__.py,0,b''
nets/alexnet_geurzhoy.py,36,"b'################################################################################\n#Michael Guerzhoy and Davi Frossard, 2016\n#AlexNet implementation in TensorFlow, with weights\n#Details:\n#http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n#\n#With code from https://github.com/ethereon/caffe-tensorflow\n#Model from  https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet\n#Weights from Caffe converted using https://github.com/ethereon/caffe-tensorflow\n#\n#\n################################################################################\n\nimport sys\nimport os\nsys.path.insert(1, os.path.join(sys.path[0], \'..\'))\nthis_path = os.path.dirname(os.path.abspath(__file__))\nfrom numpy import *\nimport os\nfrom pylab import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cbook as cbook\nimport time\nfrom scipy.misc import imread\nfrom scipy.misc import imresize\nimport matplotlib.image as mpimg\nfrom scipy.ndimage import filters\nimport urllib\nfrom numpy import random\n\nimport tensorflow as tf\nslim = tf.contrib.slim\n\ntrain_x = zeros((1, 227,227,3)).astype(float32)\ntrain_y = zeros((1, 1000))\nxdim = train_x.shape[1:]\nydim = train_y.shape[1]\n\n\n# ################################################################################\n# #Read Image\n\n\n# im1 = (imread(""poodle.png"")[:,:,:3]).astype(float32)\n# im1 = im1 - mean(im1)\n\n# im2 = (imread(""laska.png"")[:,:,:3]).astype(float32)\n# im2 = im2 - mean(im2)\n\n################################################################################\n\n# (self.feed(\'data\')\n#         .conv(11, 11, 96, 4, 4, padding=\'VALID\', name=\'conv1\')\n#         .lrn(2, 2e-05, 0.75, name=\'norm1\')\n#         .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool1\')\n#         .conv(5, 5, 256, 1, 1, group=2, name=\'conv2\')\n#         .lrn(2, 2e-05, 0.75, name=\'norm2\')\n#         .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n#         .conv(3, 3, 384, 1, 1, name=\'conv3\')\n#         .conv(3, 3, 384, 1, 1, group=2, name=\'conv4\')\n#         .conv(3, 3, 256, 1, 1, group=2, name=\'conv5\')\n#         .fc(4096, name=\'fc6\')\n#         .fc(4096, name=\'fc7\')\n#         .fc(1000, relu=False, name=\'fc8\')\n#         .softmax(name=\'prob\'))\n\nnet_data = load(this_path + ""/bvlc_alexnet.npy"").item()\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\ndef conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=""VALID"", group=1):\n    \'\'\'From https://github.com/ethereon/caffe-tensorflow\n    \'\'\'\n    c_i = input.get_shape()[-1]\n    assert c_i%group==0\n    assert c_o%group==0\n    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n\n\n    if group==1:\n        conv = convolve(input, kernel)\n    else:\n        input_groups = tf.split(3, group, input)\n        kernel_groups = tf.split(3, group, kernel)\n        output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n        conv = tf.concat(3, output_groups)\n    return tf.reshape(tf.nn.bias_add(conv, biases), [-1]+conv.get_shape().as_list()[1:])\n\n\n\n# x = tf.placeholder(tf.float32, (None,) + xdim)\n\ndef var(name, data, trainable):\n    return tf.get_variable(name, initializer=tf.constant(data), trainable=trainable)\n    # return tf.get_variable(name, shape=data.shape, initializer=trunc_normal(0.01), trainable=trainable)\n\ndef network(x, trainable=False, reuse=None, num_outputs=100):\n    with tf.variable_scope(""alexnet"", reuse=reuse) as sc:\n        print ""REUSE"", reuse\n        #conv1\n        #conv(11, 11, 96, 4, 4, padding=\'VALID\', name=\'conv1\')\n        k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n        conv1W = var(""conv1w"", net_data[""conv1""][0], trainable)\n        conv1b = var(""conv1b"", net_data[""conv1""][1], trainable)\n        conv1_in = conv(x, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=""SAME"", group=1)\n        conv1 = tf.nn.relu(conv1_in)\n\n        #lrn1\n        #lrn(2, 2e-05, 0.75, name=\'norm1\')\n        radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n        lrn1 = tf.nn.local_response_normalization(conv1,\n                                                          depth_radius=radius,\n                                                          alpha=alpha,\n                                                          beta=beta,\n                                                          bias=bias)\n\n        #maxpool1\n        #max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool1\')\n        k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = \'VALID\'\n        maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n\n\n        #conv2\n        #conv(5, 5, 256, 1, 1, group=2, name=\'conv2\')\n        k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n        conv2W = var(""conv2w"", net_data[""conv2""][0], trainable)\n        conv2b = var(""conv2b"", net_data[""conv2""][1], trainable)\n        conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=""SAME"", group=group)\n        conv2 = tf.nn.relu(conv2_in)\n\n\n        #lrn2\n        #lrn(2, 2e-05, 0.75, name=\'norm2\')\n        radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n        lrn2 = tf.nn.local_response_normalization(conv2,\n                                                          depth_radius=radius,\n                                                          alpha=alpha,\n                                                          beta=beta,\n                                                          bias=bias)\n\n        #maxpool2\n        #max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n        k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = \'VALID\'\n        maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n\n        #conv3\n        #conv(3, 3, 384, 1, 1, name=\'conv3\')\n        k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n        conv3W = var(""conv3w"", net_data[""conv3""][0], trainable)\n        conv3b = var(""conv3b"", net_data[""conv3""][1], trainable)\n        conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=""SAME"", group=group)\n        conv3 = tf.nn.relu(conv3_in)\n\n        #conv4\n        #conv(3, 3, 384, 1, 1, group=2, name=\'conv4\')\n        k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n        conv4W = var(""conv4w"", net_data[""conv4""][0], trainable)\n        conv4b = var(""conv4b"", net_data[""conv4""][1], trainable)\n        conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=""SAME"", group=group)\n        conv4 = tf.nn.relu(conv4_in)\n\n\n        #conv5\n        #conv(3, 3, 256, 1, 1, group=2, name=\'conv5\')\n        k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n        conv5W = var(""conv5w"", net_data[""conv5""][0], trainable)\n        conv5b = var(""conv5b"", net_data[""conv5""][1], trainable)\n        conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=""SAME"", group=group)\n        conv5 = tf.nn.relu(conv5_in)\n\n        # #maxpool5\n        #max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool5\')\n        k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = \'VALID\'\n        maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n\n        with slim.arg_scope([slim.conv2d],\n                              weights_initializer=trunc_normal(0.005),\n                              biases_initializer=tf.constant_initializer(0.1)):\n            net = slim.conv2d(maxpool5, num_outputs, [5, 5], padding=\'VALID\', scope=\'fc6\', reuse=reuse)\n            # net = tf.nn.relu(net)\n            net = tf.reshape(net, [-1, num_outputs])\n\n    filters = [conv1W, ]\n    return net, conv5#, filters\n# #fc6\n# #fc(4096, name=\'fc6\')\n# fc6W = tf.Variable(net_data[""fc6""][0])\n# fc6b = tf.Variable(net_data[""fc6""][1])\n# fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n\n# #fc7\n# #fc(4096, name=\'fc7\')\n# fc7W = tf.Variable(net_data[""fc7""][0])\n# fc7b = tf.Variable(net_data[""fc7""][1])\n# fc7 = tf.nn.relu_layer(fc6, fc7W, fc7b)\n\n# #fc8\n# #fc(1000, relu=False, name=\'fc8\')\n# fc8W = tf.Variable(net_data[""fc8""][0])\n# fc8b = tf.Variable(net_data[""fc8""][1])\n# fc8 = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n\n\n# #prob\n# #softmax(name=\'prob\'))\n# prob = tf.nn.softmax(fc8)\n\n# init = tf.initialize_all_variables()\n# sess = tf.Session()\n# sess.run(init)\n\n# t = time.time()\n# output = sess.run(prob, feed_dict = {x:[im1,im2]})\n# ################################################################################\n\n# #Output:\n\n\n# for input_im_ind in range(output.shape[0]):\n#     inds = argsort(output)[input_im_ind,:]\n#     print ""Image"", input_im_ind\n#     for i in range(5):\n#         print class_names[inds[-1-i]], output[input_im_ind, inds[-1-i]]\n\n# print time.time()-t\n'"
