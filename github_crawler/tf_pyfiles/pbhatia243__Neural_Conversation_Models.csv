file_path,api_count,code
__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.models.rnn.translate import data_utils\nfrom tensorflow.models.rnn.translate import seq2seq_model\n\n\n'
data_utils.py,0,"b'# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nimport gzip\nimport os\nimport re\nimport tarfile\n\nfrom six.moves import urllib\n\nfrom tensorflow.python.platform import gfile\n\n# Special vocabulary symbols - we always put them at the start.\n_PAD = ""_PAD""\n_GO = ""_GO""\n_EOS = ""_EOS""\n_UNK = ""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\n\n# Regular expressions used to tokenize.\n# _WORD_SPLIT = re.compile(""([.,!?\\""\':;)(])"")\n_WORD_SPLIT = re.compile(""([.,!/?\\"":;)(])"")\n_DIGIT_RE = re.compile(r""\\d"")\n\n\n\ndef gunzip_file(gz_path, new_path):\n  """"""Unzips from gz_path into new_path.""""""\n  print(""Unpacking %s to %s"" % (gz_path, new_path))\n  with gzip.open(gz_path, ""rb"") as gz_file:\n    with open(new_path, ""w"") as new_file:\n      for line in gz_file:\n        new_file.write(line)\n\n\ndef basic_tokenizer(sentence):\n  """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n  words = []\n  for space_separated_fragment in sentence.strip().split():\n    words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n  return [w for w in words if w]\n\n\ndef create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n                      tokenizer=None, normalize_digits=True):\n  """"""Create vocabulary file (if it does not exist yet) from data file.\n\n  Data file is assumed to contain one sentence per line. Each sentence is\n  tokenized and digits are normalized (if normalize_digits is set).\n  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n  We write it to vocabulary_path in a one-token-per-line format, so that later\n  token in the first line gets id=0, second line gets id=1, and so on.\n\n  Args:\n    vocabulary_path: path where the vocabulary will be created.\n    data_path: data file that will be used to create vocabulary.\n    max_vocabulary_size: limit on the size of the created vocabulary.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n    vocab = {}\n    with gfile.GFile(data_path, mode=""r"") as f:\n      counter = 0\n      for line in f:\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  processing line %d"" % counter)\n        text_conversation =line.strip().lower().split(""\\t"")\n        if len(text_conversation) == 2:\n          txt  = text_conversation[0] + "" "" + text_conversation[1]\n          tokens = tokenizer(txt) if tokenizer else basic_tokenizer(txt)\n          for w in tokens:\n            # word = re.sub(_DIGIT_RE, ""0"", w) if normalize_digits else w\n            word = w\n            if word in vocab:\n              vocab[word] += 1\n            else:\n              vocab[word] = 1\n      vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n      print len(vocab_list)\n      if len(vocab_list) > max_vocabulary_size:\n        vocab_list = vocab_list[:max_vocabulary_size]\n      with gfile.GFile(vocabulary_path, mode=""w"") as vocab_file:\n        for w in vocab_list:\n          vocab_file.write(w + ""\\n"")\n\ndef initialize_vocabulary(vocabulary_path):\n  """"""Initialize vocabulary from file.\n\n  We assume the vocabulary is stored one-item-per-line, so a file:\n    dog\n    cat\n  will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n  also return the reversed-vocabulary [""dog"", ""cat""].\n\n  Args:\n    vocabulary_path: path to the file containing the vocabulary.\n\n  Returns:\n    a pair: the vocabulary (a dictionary mapping string to integers), and\n    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n  Raises:\n    ValueError: if the provided vocabulary_path does not exist.\n  """"""\n  if gfile.Exists(vocabulary_path):\n    rev_vocab = []\n    with gfile.GFile(vocabulary_path, mode=""r"") as f:\n      rev_vocab.extend(f.readlines())\n    rev_vocab = [line.strip() for line in rev_vocab]\n    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n    return vocab, rev_vocab\n  else:\n    raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary,\n                          tokenizer=None, normalize_digits=True):\n  """"""Convert a string to list of integers representing token-ids.\n\n  For example, a sentence ""I have a dog"" may become tokenized into\n  [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n  ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n  Args:\n    sentence: a string, the sentence to convert to token-ids.\n    vocabulary: a dictionary mapping tokens to integers.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n  Returns:\n    a list of integers, the token-ids for the sentence.\n  """"""\n  if tokenizer:\n    words = tokenizer(sentence)\n  else:\n    words = basic_tokenizer(sentence)\n  # if not normalize_digits:\n  return [vocabulary.get(w, UNK_ID) for w in words]\n  # Normalize digits by 0 before looking words up in the vocabulary.\n  # return [vocabulary.get(re.sub(_DIGIT_RE, ""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary_path,\n                      tokenizer=None, normalize_digits=True):\n  """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n  This function loads data line-by-line from data_path, calls the above\n  sentence_to_token_ids, and saves the result to target_path. See comment\n  for sentence_to_token_ids on the details of token-ids format.\n\n  Args:\n    data_path: path to the data file in one-sentence-per-line format.\n    target_path: path where the file with token-ids will be created.\n    vocabulary_path: path to the vocabulary file.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(target_path):\n    print(""Tokenizing data in %s"" % data_path)\n    vocab, _ = initialize_vocabulary(vocabulary_path)\n    with gfile.GFile(data_path, mode=""r"") as data_file:\n      with gfile.GFile(target_path, mode=""w"") as tokens_file:\n        counter = 0\n        for line in data_file:\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  tokenizing line %d"" % counter)\n          token_ids = sentence_to_token_ids(line, vocab, tokenizer,\n                                            normalize_digits)\n          tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n'"
my_seq2seq.py,20,"b'\n\n""""""Library for creating sequence-to-sequence models in TensorFlow.\n\nSequence-to-sequence recurrent neural networks can learn complex functions\nthat map input sequences to output sequences. These models yield very good\nresults on a number of tasks, such as speech recognition, parsing, machine\ntranslation, or even constructing automated replies to emails.\n\n\n* Full sequence-to-sequence models.\n\n  - embedding_rnn_seq2seq: The basic model with input embedding.\n  - embedding_attention_seq2seq: Advanced model with input embedding and\n      the neural attention mechanism; recommended for complex tasks.\n\n\n* Decoders\n  - rnn_decoder: The basic decoder based on a pure RNN.\n  - attention_decoder: A decoder that uses the attention mechanism.\n\n* Losses.\n  - sequence_loss: Loss for a sequence model returning average log-perplexity.\n  - sequence_loss_by_example: As above, but not averaging over all examples.\n\n* model_with_buckets: A convenience function to create models with bucketing\n    (see the tutorial above for an explanation of why and how to use it).\n""""""\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom six.moves import zip     # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import variable_scope\nimport tensorflow as tf\n\ntry:\n  linear = tf.nn.rnn_cell.linear\nexcept:\n  from tensorflow.python.ops.rnn_cell import _linear as linear\n  \n\ndef _extract_argmax_and_embed(embedding, output_projection=None,\n                              update_embedding=True):\n  """"""Get a loop_function that extracts the previous symbol and embeds it.\n  Args:\n    embedding: embedding tensor for symbols.\n    output_projection: None or a pair (W, B). If provided, each fed previous\n      output will first be multiplied by W and added B.\n    update_embedding: Boolean; if False, the gradients will not propagate\n      through the embeddings.\n  Returns:\n    A loop function.\n  """"""\n  def loop_function(prev, _):\n    if output_projection is not None:\n      prev = nn_ops.xw_plus_b(\n          prev, output_projection[0], output_projection[1])\n    prev_symbol = math_ops.argmax(prev, 1)\n    # Note that gradients will not propagate through the second parameter of\n    # embedding_lookup.\n    emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n    if not update_embedding:\n      emb_prev = array_ops.stop_gradient(emb_prev)\n    return emb_prev\n  return loop_function\n\ndef _extract_beam_search(embedding, beam_size, num_symbols, embedding_size,  output_projection=None,\n                              update_embedding=True):\n  """"""Get a loop_function that extracts the previous symbol and embeds it.\n\n  Args:\n    embedding: embedding tensor for symbols.\n    output_projection: None or a pair (W, B). If provided, each fed previous\n      output will first be multiplied by W and added B.\n    update_embedding: Boolean; if False, the gradients will not propagate\n      through the embeddings.\n\n  Returns:\n    A loop function.\n  """"""\n  def loop_function(prev, i, log_beam_probs, beam_path, beam_symbols):\n    if output_projection is not None:\n      prev = nn_ops.xw_plus_b(\n          prev, output_projection[0], output_projection[1])\n    # prev= prev.get_shape().with_rank(2)[1]\n\n    probs  = tf.log(tf.nn.softmax(prev))\n\n    if i > 1:\n\n        probs = tf.reshape(probs + log_beam_probs[-1],\n                               [-1, beam_size * num_symbols])\n\n    best_probs, indices = tf.nn.top_k(probs, beam_size)\n    indices = tf.stop_gradient(tf.squeeze(tf.reshape(indices, [-1, 1])))\n    best_probs = tf.stop_gradient(tf.reshape(best_probs, [-1, 1]))\n\n    symbols = indices % num_symbols # Which word in vocabulary.\n    beam_parent = indices // num_symbols # Which hypothesis it came from.\n\n\n    beam_symbols.append(symbols)\n    beam_path.append(beam_parent)\n    log_beam_probs.append(best_probs)\n\n    # Note that gradients will not propagate through the second parameter of\n    # embedding_lookup.\n\n    emb_prev = embedding_ops.embedding_lookup(embedding, symbols)\n    emb_prev  = tf.reshape(emb_prev,[beam_size,embedding_size])\n    # emb_prev = embedding_ops.embedding_lookup(embedding, symbols)\n    if not update_embedding:\n      emb_prev = array_ops.stop_gradient(emb_prev)\n    return emb_prev\n  return loop_function\n\n\ndef rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  """"""RNN decoder for the sequence-to-sequence model.\n\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    loop_function: If not None, this function will be applied to the i-th output\n      in order to generate the i+1-st input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    scope: VariableScope for the created subgraph; defaults to ""rnn_decoder"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing generated outputs.\n      state: The state of each cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n        (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n         states can be the same. They are different for LSTM cells though.)\n  """"""\n  with variable_scope.variable_scope(scope or ""rnn_decoder""):\n    state = initial_state\n    outputs = []\n    prev = None\n    for i, inp in enumerate(decoder_inputs):\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      output, state = cell(inp, state)\n\n      outputs.append(output)\n      if loop_function is not None:\n        prev = output\n  return outputs, state\n\ndef beam_rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None,output_projection=None, beam_size=10):\n  """"""RNN decoder for the sequence-to-sequence model.\n\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    loop_function: If not None, this function will be applied to the i-th output\n      in order to generate the i+1-st input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    scope: VariableScope for the created subgraph; defaults to ""rnn_decoder"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing generated outputs.\n      state: The state of each cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n        (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n         states can be the same. They are different for LSTM cells though.)\n  """"""\n  with variable_scope.variable_scope(scope or ""rnn_decoder""):\n    state = initial_state\n    outputs = []\n    prev = None\n    log_beam_probs, beam_path, beam_symbols = [],[],[]\n    state_size = int(initial_state.get_shape().with_rank(2)[1])\n\n    for i, inp in enumerate(decoder_inputs):\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i,log_beam_probs, beam_path, beam_symbols)\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n\n      input_size = inp.get_shape().with_rank(2)[1]\n      print input_size\n      x = inp\n      output, state = cell(x, state)\n\n      if loop_function is not None:\n        prev = output\n      if  i ==0:\n          states =[]\n          for kk in range(beam_size):\n                states.append(state)\n          state = tf.reshape(tf.concat(0, states), [-1, state_size])\n\n      outputs.append(tf.argmax(nn_ops.xw_plus_b(\n          output, output_projection[0], output_projection[1]), dimension=1))\n  return outputs, state, tf.reshape(tf.concat(0, beam_path),[-1,beam_size]), tf.reshape(tf.concat(0, beam_symbols),[-1,beam_size])\n\n\ndef embedding_rnn_decoder(decoder_inputs, initial_state, cell, num_symbols,\n                          embedding_size, output_projection=None,\n                          feed_previous=False,\n                          update_embedding_for_previous=True, scope=None, beam_search=True, beam_size=10 ):\n  """"""RNN decoder with embedding and a pure-decoding option.\n\n  Args:\n    decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    cell: rnn_cell.RNNCell defining the cell function.\n    num_symbols: Integer, how many symbols come into the embedding.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has\n      shape [num_symbols]; if provided and feed_previous=True, each fed\n      previous output will first be multiplied by W and added B.\n    feed_previous: Boolean; if True, only the first of decoder_inputs will be\n      used (the ""GO"" symbol), and all other decoder inputs will be generated by:\n        next = embedding_lookup(embedding, argmax(previous_output)),\n      In effect, this implements a greedy decoder. It can also be used\n      during training to emulate http://arxiv.org/abs/1506.03099.\n      If False, decoder_inputs are used as given (the standard decoder case).\n    update_embedding_for_previous: Boolean; if False and feed_previous=True,\n      only the embedding for the first symbol of decoder_inputs (the ""GO""\n      symbol) will be updated by back propagation. Embeddings for the symbols\n      generated from the decoder itself remain unchanged. This parameter has\n      no effect if feed_previous=False.\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_rnn_decoder"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell in each time-step. This is a list\n        with length len(decoder_inputs) -- one item for each time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  if output_projection is not None:\n    proj_weights = ops.convert_to_tensor(output_projection[0],\n                                         dtype=dtypes.float32)\n    proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])\n    proj_biases = ops.convert_to_tensor(\n        output_projection[1], dtype=dtypes.float32)\n    proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n  with variable_scope.variable_scope(scope or ""embedding_rnn_decoder""):\n    with ops.device(""/cpu:0""):\n      embedding = variable_scope.get_variable(""embedding"",\n                                              [num_symbols, embedding_size])\n\n    if beam_search:\n        loop_function = _extract_beam_search(\n        embedding, beam_size,num_symbols,embedding_size,  output_projection,\n        update_embedding_for_previous)\n    else:\n        loop_function = _extract_argmax_and_embed(\n        embedding, output_projection,\n        update_embedding_for_previous) if feed_previous else None\n\n    emb_inp = [\n        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n\n\n    if beam_search:\n        return beam_rnn_decoder(emb_inp, initial_state, cell,\n                       loop_function=loop_function,output_projection=output_projection, beam_size=beam_size)\n\n    else:\n        return  rnn_decoder(emb_inp, initial_state, cell,\n                       loop_function=loop_function)\n\n\n\ndef embedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n                          num_encoder_symbols, num_decoder_symbols,\n                          embedding_size, output_projection=None,\n                          feed_previous=False, dtype=dtypes.float32,\n                          scope=None, beam_search=True, beam_size=10):\n  """"""Embedding RNN sequence-to-sequence model.\n\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\n  embedded encoder_inputs into a state vector. Next, it embeds decoder_inputs\n  by another newly created embedding (of shape [num_decoder_symbols x\n  input_size]). Then it runs RNN decoder, initialized with the last\n  encoder state, on embedded decoder_inputs.\n\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols: Integer; number of symbols on the decoder side.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_decoder_symbols] and B has\n      shape [num_decoder_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial state for both the encoder and encoder\n      rnn cells (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_rnn_seq2seq""\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x num_decoder_symbols] containing the generated\n        outputs.\n      state: The state of each decoder cell in each time-step. This is a list\n        with length len(decoder_inputs) -- one item for each time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(scope or ""embedding_rnn_seq2seq""):\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # Decoder.\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n\n\n    return embedding_rnn_decoder(\n          decoder_inputs, encoder_state, cell, num_decoder_symbols,\n          embedding_size, output_projection=output_projection,\n          feed_previous=feed_previous, beam_search=beam_search, beam_size=beam_size)\n\n\n\n\n\ndef attention_decoder(decoder_inputs, initial_state, attention_states, cell,\n                      output_size=None, num_heads=1, loop_function=None,\n                      dtype=dtypes.float32, scope=None,\n                      initial_state_attention=False):\n  """"""RNN decoder with attention for the sequence-to-sequence model.\n\n  In this context ""attention"" means that, during decoding, the RNN can look up\n  information in the additional tensor attention_states, and it does this by\n  focusing on a few entries from the tensor. This model has proven to yield\n  especially good results in a number of sequence-to-sequence tasks. This\n  implementation is based on http://arxiv.org/abs/1412.7449 (see below for\n  details). It is recommended for complex sequence-to-sequence tasks.\n\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    output_size: Size of the output vectors; if None, we use cell.output_size.\n    num_heads: Number of attention heads that read from attention_states.\n    loop_function: If not None, this function will be applied to i-th output\n      in order to generate i+1-th input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    dtype: The dtype to use for the RNN initial state (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors of\n        shape [batch_size x output_size]. These represent the generated outputs.\n        Output i is computed from input i (which is either the i-th element\n        of decoder_inputs or loop_function(output {i-1}, i)) as follows.\n        First, we run the cell on a combination of the input and previous\n        attention masks:\n          cell_output, new_state = cell(linear(input, prev_attn), prev_state).\n        Then, we calculate new attention masks:\n          new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))\n        and then we calculate the output:\n          output = linear(cell_output, new_attn).\n      state: The state of each decoder cell the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: when num_heads is not positive, there are no inputs, shapes\n      of attention_states are not set, or input size cannot be inferred\n      from the input.\n  """"""\n  if not decoder_inputs:\n    raise ValueError(""Must provide at least 1 input to attention decoder."")\n  if num_heads < 1:\n    raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n  if not attention_states.get_shape()[1:2].is_fully_defined():\n    raise ValueError(""Shape[1] and [2] of attention_states must be known: %s""\n                     % attention_states.get_shape())\n  if output_size is None:\n    output_size = cell.output_size\n\n  with variable_scope.variable_scope(scope or ""attention_decoder""):\n    batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n    attn_length = attention_states.get_shape()[1].value\n    attn_size = attention_states.get_shape()[2].value\n\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n    hidden = array_ops.reshape(\n        attention_states, [-1, attn_length, 1, attn_size])\n    hidden_features = []\n    v = []\n    attention_vec_size = attn_size  # Size of query vectors for attention.\n    for a in xrange(num_heads):\n      k = variable_scope.get_variable(""AttnW_%d"" % a,\n                                      [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n      v.append(variable_scope.get_variable(""AttnV_%d"" % a,\n                                           [attention_vec_size]))\n\n    state = initial_state\n    def attention(query):\n      """"""Put attention masks on hidden using hidden_features and query.""""""\n      ds = []  # Results of attention reads will be stored here.\n      for a in xrange(num_heads):\n        with variable_scope.variable_scope(""Attention_%d"" % a):\n          y = linear(query, attention_vec_size, True)\n          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n          # Attention mask is a softmax of v^T * tanh(...).\n          s = math_ops.reduce_sum(\n              v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n          a = nn_ops.softmax(s)\n          # Now calculate the attention-weighted vector d.\n          d = math_ops.reduce_sum(\n              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n              [1, 2])\n          ds.append(array_ops.reshape(d, [-1, attn_size]))\n      return ds\n\n    outputs = []\n    prev = None\n    batch_attn_size = array_ops.pack([batch_size, attn_size])\n    attns = [array_ops.zeros(batch_attn_size, dtype=dtype)\n             for _ in xrange(num_heads)]\n    for a in attns:  # Ensure the second shape of attention vectors is set.\n      a.set_shape([None, attn_size])\n    if initial_state_attention:\n      attns = attention(initial_state)\n    for i, inp in enumerate(decoder_inputs):\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      # If loop_function is set, we use it instead of decoder_inputs.\n      if loop_function is not None :\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n            if prev is not None:\n                inp = loop_function(prev, i)\n\n      input_size = inp.get_shape().with_rank(2)[1]\n\n      x = linear([inp] + attns, input_size, True)\n      # Run the RNN.\n      cell_output, state = cell(x, state)\n      # Run the attention mechanism.\n      if i == 0 and initial_state_attention:\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                           reuse=True):\n          attns = attention(state)\n      else:\n        attns = attention(state)\n\n      with variable_scope.variable_scope(""AttnOutputProjection""):\n        output = linear([cell_output] + attns, output_size, True)\n      if loop_function is not None:\n        prev = output\n      outputs.append(output)\n\n  return outputs, state\n\n\ndef beam_attention_decoder(decoder_inputs, initial_state, attention_states, cell,\n                      output_size=None, num_heads=1, loop_function=None,\n                      dtype=dtypes.float32, scope=None,\n                      initial_state_attention=False, output_projection=None, beam_size=10):\n  """"""RNN decoder with attention for the sequence-to-sequence model.\n\n  In this context ""attention"" means that, during decoding, the RNN can look up\n  information in the additional tensor attention_states, and it does this by\n  focusing on a few entries from the tensor. This model has proven to yield\n  especially good results in a number of sequence-to-sequence tasks. This\n  implementation is based on http://arxiv.org/abs/1412.7449 (see below for\n  details). It is recommended for complex sequence-to-sequence tasks.\n\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    output_size: Size of the output vectors; if None, we use cell.output_size.\n    num_heads: Number of attention heads that read from attention_states.\n    loop_function: If not None, this function will be applied to i-th output\n      in order to generate i+1-th input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    dtype: The dtype to use for the RNN initial state (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors of\n        shape [batch_size x output_size]. These represent the generated outputs.\n        Output i is computed from input i (which is either the i-th element\n        of decoder_inputs or loop_function(output {i-1}, i)) as follows.\n        First, we run the cell on a combination of the input and previous\n        attention masks:\n          cell_output, new_state = cell(linear(input, prev_attn), prev_state).\n        Then, we calculate new attention masks:\n          new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))\n        and then we calculate the output:\n          output = linear(cell_output, new_attn).\n      state: The state of each decoder cell the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: when num_heads is not positive, there are no inputs, shapes\n      of attention_states are not set, or input size cannot be inferred\n      from the input.\n  """"""\n  if not decoder_inputs:\n    raise ValueError(""Must provide at least 1 input to attention decoder."")\n  if num_heads < 1:\n    raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n  if not attention_states.get_shape()[1:2].is_fully_defined():\n    raise ValueError(""Shape[1] and [2] of attention_states must be known: %s""\n                     % attention_states.get_shape())\n  if output_size is None:\n    output_size = cell.output_size\n\n  with variable_scope.variable_scope(scope or ""attention_decoder""):\n    batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n    attn_length = attention_states.get_shape()[1].value\n    attn_size = attention_states.get_shape()[2].value\n\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n    hidden = array_ops.reshape(\n        attention_states, [-1, attn_length, 1, attn_size])\n    hidden_features = []\n    v = []\n    attention_vec_size = attn_size  # Size of query vectors for attention.\n    for a in xrange(num_heads):\n      k = variable_scope.get_variable(""AttnW_%d"" % a,\n                                      [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n      v.append(variable_scope.get_variable(""AttnV_%d"" % a,\n                                           [attention_vec_size]))\n\n    print ""Initial_state""\n\n    state_size =  int(initial_state.get_shape().with_rank(2)[1])\n    states =[]\n    for kk in range(1):\n        states.append(initial_state)\n    state = tf.reshape(tf.concat(0, states), [-1, state_size])\n    def attention(query):\n      """"""Put attention masks on hidden using hidden_features and query.""""""\n      ds = []  # Results of attention reads will be stored here.\n      for a in xrange(num_heads):\n        with variable_scope.variable_scope(""Attention_%d"" % a):\n          y = linear(query, attention_vec_size, True)\n          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n          # Attention mask is a softmax of v^T * tanh(...).\n          s = math_ops.reduce_sum(\n              v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n          a = nn_ops.softmax(s)\n          # Now calculate the attention-weighted vector d.\n          d = math_ops.reduce_sum(\n              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n              [1, 2])\n          # for c in range(ct):\n          ds.append(array_ops.reshape(d, [-1, attn_size]))\n      return ds\n\n    outputs = []\n    prev = None\n    batch_attn_size = array_ops.pack([batch_size, attn_size])\n    attns = [array_ops.zeros(batch_attn_size, dtype=dtype)\n             for _ in xrange(num_heads)]\n    for a in attns:  # Ensure the second shape of attention vectors is set.\n      a.set_shape([None, attn_size])\n\n    if initial_state_attention:\n       attns = []\n       attns.append(attention(initial_state))\n       tmp = tf.reshape(tf.concat(0, attns), [-1, attn_size])\n       attns = []\n       attns.append(tmp)\n\n    log_beam_probs, beam_path, beam_symbols = [],[],[]\n    for i, inp in enumerate(decoder_inputs):\n\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      # If loop_function is set, we use it instead of decoder_inputs.\n      if loop_function is not None :\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n            if prev is not None:\n                inp = loop_function(prev, i,log_beam_probs, beam_path, beam_symbols)\n\n      input_size = inp.get_shape().with_rank(2)[1]\n      x = linear([inp] + attns, input_size, True)\n      cell_output, state = cell(x, state)\n\n      # Run the attention mechanism.\n      if i == 0 and initial_state_attention:\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                           reuse=True):\n          attns = attention(state)\n      else:\n          attns = attention(state)\n\n      with variable_scope.variable_scope(""AttnOutputProjection""):\n        output = linear([cell_output] + attns, output_size, True)\n      if loop_function is not None:\n        prev = output\n      if  i ==0:\n          states =[]\n          for kk in range(beam_size):\n                states.append(state)\n          state = tf.reshape(tf.concat(0, states), [-1, state_size])\n          with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n                attns = attention(state)\n\n      outputs.append(tf.argmax(nn_ops.xw_plus_b(\n          output, output_projection[0], output_projection[1]), dimension=1))\n\n  return outputs, state, tf.reshape(tf.concat(0, beam_path),[-1,beam_size]), tf.reshape(tf.concat(0, beam_symbols),[-1,beam_size])\n\ndef embedding_attention_decoder(decoder_inputs, initial_state, attention_states,\n                                cell, num_symbols, embedding_size, num_heads=1,\n                                output_size=None, output_projection=None,\n                                feed_previous=False,\n                                update_embedding_for_previous=True,\n                                dtype=dtypes.float32, scope=None,\n                                initial_state_attention=False, beam_search=True, beam_size=10):\n  """"""RNN decoder with embedding and attention and a pure-decoding option.\n\n  Args:\n    decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: rnn_cell.RNNCell defining the cell function.\n    num_symbols: Integer, how many symbols come into the embedding.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_heads: Number of attention heads that read from attention_states.\n    output_size: Size of the output vectors; if None, use output_size.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has shape\n      [num_symbols]; if provided and feed_previous=True, each fed previous\n      output will first be multiplied by W and added B.\n    feed_previous: Boolean; if True, only the first of decoder_inputs will be\n      used (the ""GO"" symbol), and all other decoder inputs will be generated by:\n        next = embedding_lookup(embedding, argmax(previous_output)),\n      In effect, this implements a greedy decoder. It can also be used\n      during training to emulate http://arxiv.org/abs/1506.03099.\n      If False, decoder_inputs are used as given (the standard decoder case).\n    update_embedding_for_previous: Boolean; if False and feed_previous=True,\n      only the embedding for the first symbol of decoder_inputs (the ""GO""\n      symbol) will be updated by back propagation. Embeddings for the symbols\n      generated from the decoder itself remain unchanged. This parameter has\n      no effect if feed_previous=False.\n    dtype: The dtype to use for the RNN initial states (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  if output_size is None:\n    output_size = cell.output_size\n  if output_projection is not None:\n    proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n    proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n  with variable_scope.variable_scope(scope or ""embedding_attention_decoder""):\n    with ops.device(""/cpu:0""):\n      embedding = variable_scope.get_variable(""embedding"",\n                                              [num_symbols, embedding_size])\n    print ""Check number of symbols""\n    print num_symbols\n    if beam_search:\n        loop_function = _extract_beam_search(\n        embedding, beam_size,num_symbols, embedding_size, output_projection,\n        update_embedding_for_previous)\n    else:\n        loop_function = _extract_argmax_and_embed(\n        embedding, output_projection,\n        update_embedding_for_previous) if feed_previous else None\n    emb_inp = [\n        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n    if beam_search:\n        return beam_attention_decoder(\n            emb_inp, initial_state, attention_states, cell, output_size=output_size,\n            num_heads=num_heads, loop_function=loop_function,\n            initial_state_attention=initial_state_attention, output_projection=output_projection, beam_size=beam_size)\n    else:\n\n        return attention_decoder(\n            emb_inp, initial_state, attention_states, cell, output_size=output_size,\n            num_heads=num_heads, loop_function=loop_function,\n            initial_state_attention=initial_state_attention)\n\n\ndef embedding_attention_seq2seq(encoder_inputs, decoder_inputs, cell,\n                                num_encoder_symbols, num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1, output_projection=None,\n                                feed_previous=False, dtype=dtypes.float32,\n                                scope=None, initial_state_attention=False, beam_search =True, beam_size = 10 ):\n  """"""Embedding sequence-to-sequence model with attention.\n\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\n  embedded encoder_inputs into a state vector. It keeps the outputs of this\n  RNN at every step to use for attention later. Next, it embeds decoder_inputs\n  by another newly created embedding (of shape [num_decoder_symbols x\n  input_size]). Then it runs attention decoder, initialized with the last\n  encoder state, on embedded decoder_inputs and attending to encoder outputs.\n\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols: Integer; number of symbols on the decoder side.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_heads: Number of attention heads that read from attention_states.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_decoder_symbols] and B has\n      shape [num_decoder_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial RNN state (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_attention_seq2seq"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x num_decoder_symbols] containing the generated\n        outputs.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(scope or ""embedding_attention_seq2seq""):\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    encoder_outputs, encoder_state = rnn.rnn(\n        encoder_cell, encoder_inputs, dtype=dtype)\n    print ""Symbols""\n    print num_encoder_symbols\n    print num_decoder_symbols\n    # First calculate a concatenation of encoder outputs to put attention on.\n    top_states = [array_ops.reshape(e, [-1, 1, cell.output_size])\n                  for e in encoder_outputs]\n    attention_states = array_ops.concat(1, top_states)\n    print attention_states\n    # Decoder.\n    output_size = None\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n\n    return embedding_attention_decoder(\n          decoder_inputs, encoder_state, attention_states, cell,\n          num_decoder_symbols, embedding_size, num_heads=num_heads,\n          output_size=output_size, output_projection=output_projection,\n          feed_previous=feed_previous,\n          initial_state_attention=initial_state_attention, beam_search=beam_search, beam_size=beam_size)\n\n\n\n\ndef sequence_loss_by_example(logits, targets, weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None, name=None):\n  """"""Weighted cross-entropy loss for a sequence of logits (per example).\n\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, default: ""sequence_loss_by_example"".\n\n  Returns:\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\n\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  """"""\n  if len(targets) != len(logits) or len(weights) != len(logits):\n    raise ValueError(""Lengths of logits, weights, and targets must be the same ""\n                     ""%d, %d, %d."" % (len(logits), len(weights), len(targets)))\n  with ops.op_scope(logits + targets + weights, name,\n                    ""sequence_loss_by_example""):\n    log_perp_list = []\n    for logit, target, weight in zip(logits, targets, weights):\n      if softmax_loss_function is None:\n        target = array_ops.reshape(target, [-1])\n        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n            logit, target)\n      else:\n        crossent = softmax_loss_function(logit, target)\n      log_perp_list.append(crossent * weight)\n    log_perps = math_ops.add_n(log_perp_list)\n    if average_across_timesteps:\n      total_size = math_ops.add_n(weights)\n      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n      log_perps /= total_size\n  return log_perps\n\n\ndef sequence_loss(logits, targets, weights,\n                  average_across_timesteps=True, average_across_batch=True,\n                  softmax_loss_function=None, name=None):\n  """"""Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    average_across_batch: If set, divide the returned cost by the batch size.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, defaults to ""sequence_loss"".\n\n  Returns:\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\n\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  """"""\n  with ops.op_scope(logits + targets + weights, name, ""sequence_loss""):\n    cost = math_ops.reduce_sum(sequence_loss_by_example(\n        logits, targets, weights,\n        average_across_timesteps=average_across_timesteps,\n        softmax_loss_function=softmax_loss_function))\n    if average_across_batch:\n      batch_size = array_ops.shape(targets[0])[0]\n      return cost / math_ops.cast(batch_size, dtypes.float32)\n    else:\n      return cost\n\n\ndef model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n                       buckets, seq2seq, softmax_loss_function=None,\n                       per_example_loss=False, name=None):\n  """"""Create a sequence-to-sequence model with support for bucketing.\n\n  The seq2seq argument is a function that defines a sequence-to-sequence model,\n  e.g., seq2seq = lambda x, y: basic_rnn_seq2seq(x, y, rnn_cell.GRUCell(24))\n\n  Args:\n    encoder_inputs: A list of Tensors to feed the encoder; first seq2seq input.\n    decoder_inputs: A list of Tensors to feed the decoder; second seq2seq input.\n    targets: A list of 1D batch-sized int32 Tensors (desired output sequence).\n    weights: List of 1D batch-sized float-Tensors to weight the targets.\n    buckets: A list of pairs of (input size, output size) for each bucket.\n    seq2seq: A sequence-to-sequence model function; it takes 2 input that\n      agree with encoder_inputs and decoder_inputs, and returns a pair\n      consisting of outputs and states (as, e.g., basic_rnn_seq2seq).\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    per_example_loss: Boolean. If set, the returned loss will be a batch-sized\n      tensor of losses for each sequence in the batch. If unset, it will be\n      a scalar with the averaged loss from all examples.\n    name: Optional name for this operation, defaults to ""model_with_buckets"".\n\n  Returns:\n    A tuple of the form (outputs, losses), where:\n      outputs: The outputs for each bucket. Its j\'th element consists of a list\n        of 2D Tensors of shape [batch_size x num_decoder_symbols] (jth outputs).\n      losses: List of scalar Tensors, representing losses for each bucket, or,\n        if per_example_loss is set, a list of 1D batch-sized float Tensors.\n\n  Raises:\n    ValueError: If length of encoder_inputsut, targets, or weights is smaller\n      than the largest (last) bucket.\n  """"""\n  if len(encoder_inputs) < buckets[-1][0]:\n    raise ValueError(""Length of encoder_inputs (%d) must be at least that of la""\n                     ""st bucket (%d)."" % (len(encoder_inputs), buckets[-1][0]))\n  if len(targets) < buckets[-1][1]:\n    raise ValueError(""Length of targets (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n  if len(weights) < buckets[-1][1]:\n    raise ValueError(""Length of weights (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(weights), buckets[-1][1]))\n\n  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n  losses = []\n  outputs = []\n  with ops.op_scope(all_inputs, name, ""model_with_buckets""):\n    for j, bucket in enumerate(buckets):\n      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                         reuse=True if j > 0 else None):\n\n        bucket_outputs, _ = seq2seq(encoder_inputs[:bucket[0]],\n                                    decoder_inputs[:bucket[1]])\n\n        outputs.append(bucket_outputs)\n        if per_example_loss:\n          losses.append(sequence_loss_by_example(\n              outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n              softmax_loss_function=softmax_loss_function))\n        else:\n          losses.append(sequence_loss(\n              outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n              softmax_loss_function=softmax_loss_function))\n\n  return outputs, losses\n\ndef decode_model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n                       buckets, seq2seq, softmax_loss_function=None,\n                       per_example_loss=False, name=None):\n  """"""Create a sequence-to-sequence model with support for bucketing.\n\n  The seq2seq argument is a function that defines a sequence-to-sequence model,\n  e.g., seq2seq = lambda x, y: basic_rnn_seq2seq(x, y, rnn_cell.GRUCell(24))\n\n  Args:\n    encoder_inputs: A list of Tensors to feed the encoder; first seq2seq input.\n    decoder_inputs: A list of Tensors to feed the decoder; second seq2seq input.\n    targets: A list of 1D batch-sized int32 Tensors (desired output sequence).\n    weights: List of 1D batch-sized float-Tensors to weight the targets.\n    buckets: A list of pairs of (input size, output size) for each bucket.\n    seq2seq: A sequence-to-sequence model function; it takes 2 input that\n      agree with encoder_inputs and decoder_inputs, and returns a pair\n      consisting of outputs and states (as, e.g., basic_rnn_seq2seq).\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    per_example_loss: Boolean. If set, the returned loss will be a batch-sized\n      tensor of losses for each sequence in the batch. If unset, it will be\n      a scalar with the averaged loss from all examples.\n    name: Optional name for this operation, defaults to ""model_with_buckets"".\n\n  Returns:\n    A tuple of the form (outputs, losses), where:\n      outputs: The outputs for each bucket. Its j\'th element consists of a list\n        of 2D Tensors of shape [batch_size x num_decoder_symbols] (jth outputs).\n      losses: List of scalar Tensors, representing losses for each bucket, or,\n        if per_example_loss is set, a list of 1D batch-sized float Tensors.\n\n  Raises:\n    ValueError: If length of encoder_inputsut, targets, or weights is smaller\n      than the largest (last) bucket.\n  """"""\n  if len(encoder_inputs) < buckets[-1][0]:\n    raise ValueError(""Length of encoder_inputs (%d) must be at least that of la""\n                     ""st bucket (%d)."" % (len(encoder_inputs), buckets[-1][0]))\n  if len(targets) < buckets[-1][1]:\n    raise ValueError(""Length of targets (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n  if len(weights) < buckets[-1][1]:\n    raise ValueError(""Length of weights (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(weights), buckets[-1][1]))\n\n  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n  losses = []\n  outputs = []\n  beam_paths = []\n  beam_symbols = []\n  with ops.op_scope(all_inputs, name, ""model_with_buckets""):\n    for j, bucket in enumerate(buckets):\n      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                         reuse=True if j > 0 else None):\n        bucket_outputs, _, beam_path, beam_symbol = seq2seq(encoder_inputs[:bucket[0]],\n                                    decoder_inputs[:bucket[1]])\n        outputs.append(bucket_outputs)\n        beam_paths.append(beam_path)\n        beam_symbols.append(beam_symbol)\n  print ""End**********""\n\n  return outputs, beam_paths, beam_symbols'"
neural_conversation_model.py,32,"b'\n\n""""""Most of the code comes from seq2seq tutorial. Binary for training conversation models and decoding from them.\n\nRunning this program without --decode will  tokenize it in a very basic way,\nand then start training a model saving checkpoints to --train_dir.\n\nRunning with --decode starts an interactive loop so you can see how\nthe current checkpoint performs\n\nSee the following papers for more information on neural translation models.\n * http://arxiv.org/abs/1409.3215\n * http://arxiv.org/abs/1409.0473\n * http://arxiv.org/abs/1412.2007\n""""""\n\n\nimport math\nimport os\nimport random\nimport sys\nimport time\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom  data_utils import *\nfrom  seq2seq_model import *\nimport codecs\n\ntf.app.flags.DEFINE_float(""learning_rate"", 0.5, ""Learning rate."")\ntf.app.flags.DEFINE_float(""learning_rate_decay_factor"", 0.99,\n                          ""Learning rate decays by this much."")\ntf.app.flags.DEFINE_float(""max_gradient_norm"", 5.0,\n                          ""Clip gradients to this norm."")\ntf.app.flags.DEFINE_integer(""batch_size"", 64,\n                            ""Batch size to use during training."")\ntf.app.flags.DEFINE_integer(""size"", 512, ""Size of each model layer."")\ntf.app.flags.DEFINE_integer(""num_layers"", 3, ""Number of layers in the model."")\ntf.app.flags.DEFINE_integer(""en_vocab_size"", 40000, ""English vocabulary size."")\ntf.app.flags.DEFINE_string(""train_dir"", ""./tmp/"", ""Training directory."")\ntf.app.flags.DEFINE_string(""vocab_path"", ""./tmp/"", ""Data directory"")\ntf.app.flags.DEFINE_string(""data_path"", ""./tmp/"", ""Training directory."")\ntf.app.flags.DEFINE_string(""dev_data"", ""./tmp/"", ""Data directory"")\ntf.app.flags.DEFINE_integer(""max_train_data_size"", 0,\n                            ""Limit on the size of training data (0: no limit)."")\ntf.app.flags.DEFINE_integer(""steps_per_checkpoint"", 400,\n                            ""How many training steps to do per checkpoint."")\ntf.app.flags.DEFINE_integer(""beam_size"", 100,\n                            ""How many training steps to do per checkpoint."")\ntf.app.flags.DEFINE_boolean(""beam_search"", False,\n                            ""Set to True for beam_search."")\ntf.app.flags.DEFINE_boolean(""decode"", False,\n                            ""Set to True for interactive decoding."")\ntf.app.flags.DEFINE_boolean(""attention"", False,\n                            ""Set to True for interactive decoding."")\ntf.app.flags.DEFINE_boolean(""self_test"", False,\n                            ""Run a self-test if this is set to True."")\n\nFLAGS = tf.app.flags.FLAGS\n\n# We use a number of buckets and pad to the closest one for efficiency.\n# See seq2seq_model.Seq2SeqModel for details of how they work.\n_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n\n\n\n\ndef read_chat_data(data_path,vocabulary_path, max_size=None):\n    counter = 0\n    vocab, _ = initialize_vocabulary(vocabulary_path)\n    print len(vocab)\n    print max_size\n    data_set = [[] for _ in _buckets]\n    with codecs.open(data_path, ""rb"") as fi:\n        for line in fi.readlines():\n            counter += 1\n            if max_size!=0 and counter > max_size:\n                break\n            if counter % 10000 == 0:\n              print(""  reading data line %d"" % counter)\n              sys.stdout.flush()\n            entities = line.lower().split(""\\t"")\n            # print entities\n            if len(entities) == 2:\n                source = entities[0]\n                target = entities[1]\n                source_ids = [int(x) for x in sentence_to_token_ids(source,vocab)]\n                target_ids = [int(x) for x in sentence_to_token_ids(target,vocab)]\n                target_ids.append(EOS_ID)\n                for bucket_id, (source_size, target_size) in enumerate(_buckets):\n                  if len(source_ids) < source_size and len(target_ids) < target_size:\n                    data_set[bucket_id].append([source_ids, target_ids])\n                    break\n    return data_set\n\ndef create_model(session, forward_only, beam_search, beam_size = 10, attention = True):\n  """"""Create translation model and initialize or load parameters in session.""""""\n  model = Seq2SeqModel(\n      FLAGS.en_vocab_size, FLAGS.en_vocab_size, _buckets,\n      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\n      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,\n      forward_only=forward_only, beam_search=beam_search, beam_size=beam_size, attention=attention)\n  print FLAGS.train_dir\n  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n\n  # ckpt.model_checkpoint_path =""./big_models/chat_bot.ckpt-183600""\n  # print ckpt.model_checkpoint_path\n  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n    print(""Reading model parameters from %s"" % ckpt.model_checkpoint_path)\n    model.saver.restore(session, ckpt.model_checkpoint_path)\n  else:\n    print(""Created model with fresh parameters."")\n    session.run(tf.initialize_all_variables())\n  return model\n\ndef create_models(path, en_vocab_size, session, forward_only, beam_search, beam_size = 10, attention = True):\n  """"""Create translation model and initialize or load parameters in session.""""""\n  model = Seq2SeqModel(\n      en_vocab_size, en_vocab_size, _buckets,\n      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\n      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,\n      forward_only=forward_only, beam_search=beam_search, beam_size=beam_size, attention=attention)\n  print FLAGS.train_dir\n  ckpt = tf.train.get_checkpoint_state(path)\n\n  # ckpt.model_checkpoint_path =""./big_models/chat_bot.ckpt-183600""\n  # print ckpt.model_checkpoint_path\n  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n    print(""Reading model parameters from %s"" % ckpt.model_checkpoint_path)\n    model.saver.restore(session, ckpt.model_checkpoint_path)\n  else:\n    print(""Created model with fresh parameters."")\n    session.run(tf.initialize_all_variables())\n  return model\n\ndef train():\n\n  data_path =FLAGS.data_path\n  dev_data = FLAGS.dev_data\n  vocab_path =FLAGS.vocab_path\n  # Beam search is false during training operation and usedat inference .\n  beam_search = False\n  beam_size =10\n  attention = FLAGS.attention\n\n  normalize_digits=True\n  create_vocabulary(vocab_path, data_path, FLAGS.en_vocab_size )\n\n\n  with tf.Session() as sess:\n    # Create model.\n    print(""Creating %d layers of %d units."" % (FLAGS.num_layers, FLAGS.size))\n    model = create_model(sess, False,beam_search=beam_search, beam_size=beam_size, attention=attention)\n\n    # Read data into buckets and compute their sizes.\n    print (""Reading development and training data (limit: %d).""\n           % FLAGS.max_train_data_size)\n    train_set =read_chat_data(data_path,vocab_path, FLAGS.max_train_data_size)\n    dev_set =read_chat_data(dev_data,vocab_path, FLAGS.max_train_data_size)\n\n    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n    train_total_size = float(sum(train_bucket_sizes))\n\n    # A bucket scale is a list of increasing numbers from 0 to 1 that we\'ll use\n    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n    # the size if i-th training bucket, as used later.\n    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                           for i in xrange(len(train_bucket_sizes))]\n\n    # This is the training loop.\n    step_time, loss = 0.0, 0.0\n    current_step = 0\n    previous_losses = []\n    while True:\n      # Choose a bucket according to data distribution. We pick a random number\n      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n      # print ""Started""\n      random_number_01 = np.random.random_sample()\n      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                       if train_buckets_scale[i] > random_number_01])\n\n      # Get a batch and make a step.\n      start_time = time.time()\n      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n          train_set, bucket_id)\n\n      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                   target_weights, bucket_id, False, beam_search)\n      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n      loss += step_loss / FLAGS.steps_per_checkpoint\n      current_step += 1\n\n      # Once in a while, we save checkpoint, print statistics, and run evals.\n      if current_step % FLAGS.steps_per_checkpoint == 0:\n        # Print statistics for the previous epoch.\n        print ""Running epochs""\n        perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\n        print (""global step %d learning rate %.4f step-time %.2f perplexity ""\n                ""%.2f"" % (model.global_step.eval(), model.learning_rate.eval(),\n                          step_time, perplexity))\n        # # Decrease learning rate if no improvement was seen over last 3 times.\n        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n           sess.run(model.learning_rate_decay_op)\n        previous_losses.append(loss)\n        # # Save checkpoint and zero timer and loss.\n        checkpoint_path = os.path.join(FLAGS.train_dir, ""chat_bot.ckpt"")\n        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n        step_time, loss = 0.0, 0.0\n        # # Run evals on development set and print their perplexity.\n        for bucket_id in xrange(len(_buckets)):\n            if len(dev_set[bucket_id]) == 0:\n              print(""  eval: empty bucket %d"" % (bucket_id))\n              continue\n            encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n                dev_set, bucket_id)\n            _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                         target_weights, bucket_id, True, beam_search)\n            eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float(\'inf\')\n            print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\n        sys.stdout.flush()\n\ndef decode():\n  with tf.Session() as sess:\n    # Create model and load parameters.\n    beam_size = FLAGS.beam_size\n    beam_search = FLAGS.beam_search\n    attention = FLAGS.attention\n    model = create_model(sess, True, beam_search=beam_search, beam_size=beam_size, attention=attention)\n    model.batch_size = 1  # We decode one sentence at a time.\n\n    # Load vocabularies.\n    vocab_path = FLAGS.vocab_path\n    vocab, rev_vocab = initialize_vocabulary(vocab_path)\n\n    # Decode from standard input.\n    if beam_search:\n        sys.stdout.write(""> "")\n        sys.stdout.flush()\n        sentence = sys.stdin.readline()\n        while sentence:\n          # Get token-ids for the input sentence.\n          token_ids = sentence_to_token_ids(tf.compat.as_bytes(sentence), vocab)\n          # Which bucket does it belong to?\n          bucket_id = min([b for b in xrange(len(_buckets))\n                           if _buckets[b][0] > len(token_ids)])\n          # Get a 1-element batch to feed the sentence to the model.\n          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n              {bucket_id: [(token_ids, [])]}, bucket_id)\n          # Get output logits for the sentence.\n          # print bucket_id\n          path, symbol , output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n                                           target_weights, bucket_id, True,beam_search )\n\n          k = output_logits[0]\n          paths = []\n          for kk in range(beam_size):\n              paths.append([])\n          curr = range(beam_size)\n          num_steps = len(path)\n          for i in range(num_steps-1, -1, -1):\n              for kk in range(beam_size):\n                paths[kk].append(symbol[i][curr[kk]])\n                curr[kk] = path[i][curr[kk]]\n          recos = set()\n          print ""Replies --------------------------------------->""\n          for kk in range(beam_size):\n              foutputs = [int(logit)  for logit in paths[kk][::-1]]\n\n          # If there is an EOS symbol in outputs, cut them at that point.\n              if EOS_ID in foutputs:\n          #         # print outputs\n                   foutputs = foutputs[:foutputs.index(EOS_ID)]\n              rec = "" "".join([tf.compat.as_str(rev_vocab[output]) for output in foutputs])\n              if rec not in recos:\n                      recos.add(rec)\n                      print rec\n\n          print(""> "", """")\n          sys.stdout.flush()\n          sentence = sys.stdin.readline()\n    else:\n        sys.stdout.write(""> "")\n        sys.stdout.flush()\n        sentence = sys.stdin.readline()\n\n        while sentence:\n              # Get token-ids for the input sentence.\n              token_ids = sentence_to_token_ids(tf.compat.as_bytes(sentence), vocab)\n              # Which bucket does it belong to?\n              bucket_id = min([b for b in xrange(len(_buckets))\n                               if _buckets[b][0] > len(token_ids)])\n              # for loc in locs:\n                  # Get a 1-element batch to feed the sentence to the model.\n              encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n                      {bucket_id: [(token_ids, [],)]}, bucket_id)\n\n              _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n                                                   target_weights, bucket_id, True,beam_search)\n              # This is a greedy decoder - outputs are just argmaxes of output_logits.\n\n              outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n              # If there is an EOS symbol in outputs, cut them at that point.\n              if EOS_ID in outputs:\n                  # print outputs\n                  outputs = outputs[:outputs.index(EOS_ID)]\n\n              print("" "".join([tf.compat.as_str(rev_vocab[output]) for output in outputs]))\n              print(""> "", """")\n              sys.stdout.flush()\n              sentence = sys.stdin.readline()\n\n\n\ndef main(_):\n  if FLAGS.decode:\n    decode()\n  else:\n    train()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
seq2seq_model.py,21,"b'\n""""""Sequence-to-sequence model with an attention mechanism.""""""\n\n\nimport random\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom  data_utils import *\nfrom my_seq2seq import *\n\nclass Seq2SeqModel(object):\n  """"""Sequence-to-sequence model with attention and for multiple buckets.\n\n  This class implements a multi-layer recurrent neural network as encoder,\n  and an attention-based decoder. This is the same as the model described in\n  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n  or into the seq2seq library for complete model implementation.\n  This class also allows to use GRU cells in addition to LSTM cells, and\n  sampled softmax to handle large output vocabulary size. A single-layer\n  version of this model, but with bi-directional encoder, was presented in\n    http://arxiv.org/abs/1409.0473\n  and sampled softmax is described in Section 3 of the following paper.\n    http://arxiv.org/abs/1412.2007\n  """"""\n\n  def __init__(self, source_vocab_size, target_vocab_size, buckets, size,\n               num_layers, max_gradient_norm, batch_size, learning_rate,\n               learning_rate_decay_factor, use_lstm=False,\n               num_samples=1024, forward_only=False, beam_search = True, beam_size=10, attention=True):\n    """"""Create the model.\n\n    Args:\n      source_vocab_size: size of the source vocabulary.\n      target_vocab_size: size of the target vocabulary.\n      buckets: a list of pairs (I, O), where I specifies maximum input length\n        that will be processed in that bucket, and O specifies maximum output\n        length. Training instances that have inputs longer than I or outputs\n        longer than O will be pushed to the next bucket and padded accordingly.\n        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n      size: number of units in each layer of the model.\n      num_layers: number of layers in the model.\n      max_gradient_norm: gradients will be clipped to maximally this norm.\n      batch_size: the size of the batches used during training;\n        the model construction is independent of batch_size, so it can be\n        changed after initialization if this is convenient, e.g., for decoding.\n      learning_rate: learning rate to start with.\n      learning_rate_decay_factor: decay learning rate by this much when needed.\n      use_lstm: if true, we use LSTM cells instead of GRU cells.\n      num_samples: number of samples for sampled softmax.\n      forward_only: if set, we do not construct the backward pass in the model.\n    """"""\n    self.source_vocab_size = source_vocab_size\n    self.target_vocab_size = target_vocab_size\n    self.buckets = buckets\n    self.batch_size = batch_size\n    self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n    self.learning_rate_decay_op = self.learning_rate.assign(\n        self.learning_rate * learning_rate_decay_factor)\n    self.global_step = tf.Variable(0, trainable=False)\n\n    # If we use sampled softmax, we need an output projection.\n    output_projection = None\n    softmax_loss_function = None\n    # Sampled softmax only makes sense if we sample less than vocabulary size.\n    if num_samples > 0 and num_samples < self.target_vocab_size:\n      with tf.device(""/cpu:0""):\n        w = tf.get_variable(""proj_w"", [size, self.target_vocab_size])\n        w_t = tf.transpose(w)\n        b = tf.get_variable(""proj_b"", [self.target_vocab_size])\n      output_projection = (w, b)\n\n      def sampled_loss(inputs, labels):\n        with tf.device(""/cpu:0""):\n          labels = tf.reshape(labels, [-1, 1])\n          return tf.nn.sampled_softmax_loss(w_t, b, inputs, labels, num_samples,\n                                            self.target_vocab_size)\n      softmax_loss_function = sampled_loss\n    # Create the internal multi-layer cell for our RNN.\n    single_cell = tf.nn.rnn_cell.GRUCell(size)\n    if use_lstm:\n      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n    cell = single_cell\n    if num_layers > 1:\n      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers, state_is_tuple=False)\n\n    # The seq2seq function: we use embedding for the input and attention.\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n        if attention:\n            print ""Attention Model""\n            return embedding_attention_seq2seq(\n               encoder_inputs, decoder_inputs, cell,\n               num_encoder_symbols=source_vocab_size,\n               num_decoder_symbols=target_vocab_size,\n               embedding_size=size,\n               output_projection=output_projection,\n               feed_previous=do_decode,\n               beam_search=beam_search,\n               beam_size=beam_size )\n        else:\n            print ""Simple Model""\n            return embedding_rnn_seq2seq(\n              encoder_inputs, decoder_inputs, cell,\n              num_encoder_symbols=source_vocab_size,\n              num_decoder_symbols=target_vocab_size,\n              embedding_size=size,\n              output_projection=output_projection,\n              feed_previous=do_decode,\n              beam_search=beam_search,\n              beam_size=beam_size )\n\n\n    # Feeds for inputs.\n    self.encoder_inputs = []\n    self.decoder_inputs = []\n    self.target_weights = []\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""encoder{0}"".format(i)))\n    for i in xrange(buckets[-1][1] + 1):\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""decoder{0}"".format(i)))\n      self.target_weights.append(tf.placeholder(tf.float32, shape=[None],\n                                                name=""weight{0}"".format(i)))\n\n    # Our targets are decoder inputs shifted by one.\n    targets = [self.decoder_inputs[i + 1]\n               for i in xrange(len(self.decoder_inputs) - 1)]\n\n    # Training outputs and losses.\n    if forward_only:\n        if beam_search:\n              self.outputs, self.beam_path, self.beam_symbol = decode_model_with_buckets(\n                  self.encoder_inputs, self.decoder_inputs, targets,\n                  self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n                  softmax_loss_function=softmax_loss_function)\n        else:\n              # print self.decoder_inputs\n              self.outputs, self.losses = model_with_buckets(\n                  self.encoder_inputs, self.decoder_inputs, targets,\n                  self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n                  softmax_loss_function=softmax_loss_function)\n              # If we use output projection, we need to project outputs for decoding.\n              if output_projection is not None:\n                    for b in xrange(len(buckets)):\n                      self.outputs[b] = [\n                          tf.matmul(output, output_projection[0]) + output_projection[1]\n                          for output in self.outputs[b]\n                      ]\n\n\n    else:\n      self.outputs, self.losses = model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    if not forward_only:\n      self.gradient_norms = []\n      self.updates = []\n      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(self.losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                         max_gradient_norm)\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n            zip(clipped_gradients, params), global_step=self.global_step))\n\n    self.saver = tf.train.Saver(tf.all_variables())\n\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n           bucket_id, forward_only, beam_search):\n    """"""Run a step of the model feeding the given inputs.\n\n    Args:\n      session: tensorflow session to use.\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n      target_weights: list of numpy float vectors to feed as target weights.\n      bucket_id: which bucket of the model to use.\n      forward_only: whether to do the backward step or only forward.\n\n    Returns:\n      A triple consisting of gradient norm (or None if we did not do backward),\n      average perplexity, and the outputs.\n\n    Raises:\n      ValueError: if length of encoder_inputs, decoder_inputs, or\n        target_weights disagrees with bucket size for the specified bucket_id.\n    """"""\n    # Check if the sizes match.\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    if len(encoder_inputs) != encoder_size:\n      raise ValueError(""Encoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(encoder_inputs), encoder_size))\n    if len(decoder_inputs) != decoder_size:\n      raise ValueError(""Decoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(decoder_inputs), decoder_size))\n    if len(target_weights) != decoder_size:\n      raise ValueError(""Weights length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(target_weights), decoder_size))\n\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n    input_feed = {}\n    for l in xrange(encoder_size):\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n    for l in xrange(decoder_size):\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n      input_feed[self.target_weights[l].name] = target_weights[l]\n\n    # Since our targets are decoder inputs shifted by one, we need one more.\n    last_target = self.decoder_inputs[decoder_size].name\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n    # Output feed: depends on whether we do a backward step or not.\n    if not forward_only:\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                     self.gradient_norms[bucket_id],  # Gradient norm.\n                     self.losses[bucket_id]]  # Loss for this batch.\n    else:\n        if beam_search:\n              output_feed = [self.beam_path[bucket_id]]  # Loss for this batch.\n              output_feed.append(self.beam_symbol[bucket_id])\n        else:\n            output_feed = [self.losses[bucket_id]]\n\n        for l in xrange(decoder_size):  # Output logits.\n            output_feed.append(self.outputs[bucket_id][l])\n    # print bucket_id\n    outputs = session.run(output_feed, input_feed)\n    if not forward_only:\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n    else:\n      if beam_search:\n          return outputs[0], outputs[1], outputs[2:]  # No gradient norm, loss, outputs.\n      else:\n          return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\n  def get_batch(self, data, bucket_id):\n    """"""Get a random batch of data from the specified bucket, prepare for step.\n\n    To feed data in step(..) it must be a list of batch-major vectors, while\n    data here contains single length-major cases. So the main logic of this\n    function is to re-index data cases to be in the proper format for feeding.\n\n    Args:\n      data: a tuple of size len(self.buckets) in which each element contains\n        lists of pairs of input and output data that we use to create a batch.\n      bucket_id: integer, which bucket to get the batch for.\n\n    Returns:\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\n      the constructed batch that has the proper format to call step(...) later.\n    """"""\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    encoder_inputs, decoder_inputs = [], []\n\n    # Get a random batch of encoder and decoder inputs from data,\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\n    for _ in xrange(self.batch_size):\n      encoder_input, decoder_input = random.choice(data[bucket_id])\n\n      # Encoder inputs are padded and then reversed.\n      encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n      # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\n      decoder_inputs.append([GO_ID] + decoder_input +\n                            [PAD_ID] * decoder_pad_size)\n\n    # Now we create batch-major vectors from the data selected above.\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n    # Batch encoder inputs are just re-indexed encoder_inputs.\n    for length_idx in xrange(encoder_size):\n      batch_encoder_inputs.append(\n          np.array([encoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n    for length_idx in xrange(decoder_size):\n      batch_decoder_inputs.append(\n          np.array([decoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n      # Create target_weights to be 0 for targets that are padding.\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n      for batch_idx in xrange(self.batch_size):\n        # We set weight to 0 if the corresponding target is a PAD symbol.\n        # The corresponding target is decoder_input shifted by 1 forward.\n        if length_idx < decoder_size - 1:\n          target = decoder_inputs[batch_idx][length_idx + 1]\n        if length_idx == decoder_size - 1 or target == PAD_ID:\n          batch_weight[batch_idx] = 0.0\n      batch_weights.append(batch_weight)\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n'"
